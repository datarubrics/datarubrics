{"id": "emnlp-2022-main-590", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present Bloom Library, a linguistically diverse set of multimodal and multilingual datasets for language modeling, image captioning, visual storytelling, and speech synthesis/recognition. These datasets represent either the most, or among the most, multilingual datasets for each of the included downstream tasks. In total, the initial release of the Bloom Library datasets covers 363 languages across 32 language families. We train downstream task models for various languages represented in the data, showing the viability of the data for future work in low-resource, multimodal NLP and establishing the first known baselines for these downstream tasks in certain languages (e.g., Bisu [bzi], with an estimated population of 700 users). Some of these first-of-their-kind baselines are comparable to state-of-the-art performance for higher-resourced languages. The Bloom Library datasets are released under Creative Commons licenses on the Hugging Face datasets hub to catalyze more linguistically diverse research in the included downstream tasks.\\n\\n1 Introduction\\n\\nOnly a negligible fraction of the 7100+ living languages (Eberhard et al., 2021) have sufficient, publicly available text, audio, and image data to train state-of-the-art language/speech models and/or models for downstream tasks like Named Entity Recognition (NER) or image captioning. This data scarcity results in systematic inequalities in the performance of NLP tasks across the world's languages (Blasi et al., 2021). Indigenous language ecologies also represent profoundly different understandings of the nature and function of language (Bird, 2022, 2020), which might prioritize orality or translanguaging (Quakenbush and Simons, 2018), for example, above a single, written mode of communication in all domains.\\n\\nThe Bloom Library is a web-based platform that is attempting to facilitate an increase in the amount of multimodal materials available to communities speaking non-dominant languages. The Bloom Library holds over 12,400 books in 545 languages (at the time this paper is published), covering subjects including agriculture, business, culture, math, science, religion, and health. Many of these books include images aligned with text, and 1,600+ of the books have corresponding audio recordings (called \\\"talking books\\\"). Language communities can create new books, create audio recordings, download existing books, and translate existing books using the open-source \\\"Bloom\\\" software.\\n\\nTo boost language diversity and indigenous perspectives in the NLP research community, we present multimodal datasets post-processed out of the Bloom Library. We anticipate that more task-specific datasets will be created from the Bloom Library. However, as a starting point, we are presenting the following datasets: (1) bloom-lm for language modeling in 351 languages; (2) bloom-captioning for image-to-text or text-to-image tasks in 351 languages; (3) bloom-vist for visual storytelling in 351 languages; and (4) bloom-speech for speech-to-text and text-to-speech tasks in 56 languages. The languages in these datasets correspond to 32 language families, and many of the included languages are in extremely low-resource settings. Furthermore, to the authors' knowledge, bloom-vist represents the first (and certainly most) multilingual visual storytelling dataset, and bloom-speech includes more languages in the following language families than any other aligned speech dataset (number of languages in parenthesis): Austronesian (8), Mayan (6), Niger-Congo (7), Sepik (2), Tequistlatecan (2), and Trans-New Guinea (3).\\n\\n1 https://bloomlibrary.org/\\n2 https://github.com/BloomBooks/\\nBloomDesktop\"}"}
{"id": "emnlp-2022-main-590", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To assess the difficulty of language modeling, image captioning, and automatic speech recognition (ASR) with the Bloom Library datasets, we trained baseline models on each of these tasks. For certain languages, the Bloom Library datasets facilitate the first known baselines with comparable to state-of-the-art performance for higher-resource languages. We achieve a BLEU score on image captioning of above 10.0 for 10 languages using only data from bloom-captioning. For ASR, we demonstrate a Word Error Rate (WER) below 0.5 for 18 languages and a Character Error Rate (CER) below 0.2 for 21 languages.\\n\\n2 Related Work\\n\\nIn terms of language coverage, various multilingual and single modality datasets have emerged recently. These include, by way of example, the JHU Bible Corpus (McCarthy et al., 2020), the CMU Wilderness Multilingual Speech dataset (Black, 2019), Common Voice (Ardila et al., 2019), Multilingual BABEL (Consortium, 2022), and MARSIVE (FitzGerald et al., 2022). The number of languages in these datasets is impressive. However, many are limited in domain (e.g., only including Bible data), accessibility, licensing, or modality (e.g., only focusing on text or read speech). These datasets are also primarily rooted in content from large, dominant languages, like English, and are translated or adapted to other fairly large languages. Bloom Library data, in contrast, originates from local language communities, which are producing Bloom Books to fit their own local language ecology and perspectives. As a result, the data presented here covers languages, language families, and topics that are not covered by any other aligned and prepared datasets.\\n\\nIn terms of modality, the research community is presenting an increasing number of intriguing multimodal datasets. These include, by way of example, Pano-A VQA (Yun et al., 2021), which facilitates question answering regarding various objects, sounds, and their associations in videos, and VIST (Huang et al., 2016), which facilitates sequential vision-to-language tasks. However, recent multimodal datasets are overwhelmingly monolingual. Datasets representing both multiple modalities and many languages include Multi30k, which is one of the few multimodal, multilingual datasets in existence, with ~30k images and corresponding text descriptions in several languages including English, German (Elliott et al., 2016), French (Elliott et al., 2017), and Czech (Barrault et al., 2018). One listing can be found in K\u00e1d\u00e1r (2019), which provides a helpful (and comprehensive) table of multilingual, multimodal resources, dividing them into two categories: (i) \u201ctranslation\u201d (with captions translated into another language); and (ii) \u201cdescription\u201d (with annotations independently created for each language). The table reveals that Multi30k was, at the time, the largest translation dataset available in terms of image count, at approximately 31k images and 31k sentences covering 4 languages. Bloom Library datasets fit into the \u201cdescription\u201d category of K\u00e1d\u00e1r (2019). However, with over 90k+ images and 110k+ captions covering 351 languages and additional speech data in 56 languages, Bloom Library represents a massive increase in language and modality coverage (up to two orders of magnitude wider than previous multilingual, multimodal datasets). Further, the existing datasets referenced by K\u00e1d\u00e1r (2019) focus on large languages in high-resource settings, with no representation of local languages in low resource settings. In contrast, our datasets include languages in extremely low resource and non-dominant settings like Bisu [bzi] and Kagayanen [cgc], with estimated populations of 700 and 30,000 users, respectively.\\n\\n3 Constructing the Datasets\\n\\nThe authors worked directly with the Bloom Library developers to gain access to and understand the raw data behind the Bloom Library website. We parse, clean, deduplicate, and publicly release this data for research use on the Hugging Face Hub in formats compatible with the Hugging Face datasets Python package. bloom-lm, bloom-captioning, and bloom-vist are created using one data pipeline starting with bloom-vist, because each of these datasets use some or all of the images and corresponding text within the Bloom Library.\"}"}
{"id": "emnlp-2022-main-590", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"separate data pipeline is used for bloom-speech to process only \u201ctalking books.\u201d\\n\\n3.1 bloom-vist\\nThe Bloom Library books offer the rare possibility of leveraging sequential images for language understanding across many languages. Thus, we first process the Bloom Library data into a format consistent with the VIST task published by Huang et al. (2016). VIST is a dataset made by creating collections of sequential image-caption pairs which form short \u201cstories\u201d, collaboratively set up by researchers at Google Research, CMU, and JHU, we structure our data to match this. We hope this release of VIST-formatted data from Bloom Library catalyzes techniques in both multilingual and multimodal storytelling.\\n\\nThe raw Bloom Library data we received from the Bloom Library team consisted of a folder of files for each \u201cbook,\u201d which corresponds to one of the pages on the Bloom Library website. The relevant files in this folder include: (1) meta.json, containing important metadata such as the book\u2019s translation lineage, alternative titles, copyright, etc.; (2) an *.htm file containing the actual data, particularly text and image links for each page of the book; (3) in certain cases, a number of image files of various types including *.jpg and *.png; and (4) in certain cases (for talking books), a number of *.mp3 audio files. In order to construct the sequential VIST-type data, we parse the *.htm file with BeautifulSoup to associate images files with captions and sequence these according to the sequence of pages in the book. We use meta.json to pull out relevant metadata (book title, topics, etc.) and to filter out any books not released under a Creative Commons license.\\n\\nFigure 1 includes some example data included in bloom-vist by way of example. The dataset includes \u201calbums,\u201d which are ordered sequences of images. An album may be associated with multiple \u201cstories,\u201d where each story is an ordered sequence of text captions.\\n\\nOnce in the appropriate format, we take various steps to clean up and filter the data. We check for, among other things, irreconcilable inconsistencies in metadata (like conflicting titles or book IDs), duplicate books, duplicate stories, duplicate albums, and similar or identical image-caption pairs. To account for image size or brightness variations during deduplication, we utilize a perceptual hash to identify albums sharing at least 80% of images. We also filter out stories where the writing system script (e.g., Latn or Thai) does not match the majority writing system script used for that language.\\n\\nOf the 14,095 stories in the raw data, 2,547 were duplicates and 155 are filtered due to script mismatch.\\n\\nFinally, we follow Kreutzer et al. (2022) and conduct a manual inspection for every language, rejecting any with obvious quality issues \u201cat a glance.\u201d As in that work, some of the authors conducted manual inspections on languages they were familiar with (e.g. Mandarin, German), but also languages they had no familiarity with. These checks provide a \u201cfloor\u201d on data quality, allowing the detection of extremely low-quality data that is quite obviously wrong even at a glance even by those who do not speak the language.\\n\\nFor example, in this manual review, we detected a number of books having captions in the wrong language (e.g. \u201cEnglish\u201d text in Devanagari or Arabic script) or obvious \u201ctest\u201d stories containing the verbatim phrases \u201ctext in a block\u201d or the English text \u201cTHIS IS ALSO IN FALI.\u201d Manual inspection was conducted on at least 50 random stories per language - or fewer if there were fewer stories in a language overall. 85 stories did not pass this manual inspection, some of which were also filtered out by the other quality checks.\\n\\nStories which failed any of the checks above are marked as \u201cquarantined\u201d in the JSON file. Downstream data loading scripts can then filter these when loading the data.\\n\\nAfter all filtering and \u201cquarantining\u201d of items in the JSON, we are left with 11,407 stories containing a total of 112,080 image/caption pairs in our dataset listed on HuggingFace. The bloom-vist dataset is listed in the Hugging Face Hub as bloom-vist.\\n\\n3.2 bloom-captioning\\nBuilding off of the data produced for bloom-vist, we further process the VIST JSON...\"}"}
{"id": "emnlp-2022-main-590", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Several examples from the bloom-vist dataset. Three stories (sets of captions) associated with the same album (set of images) are shown, in this case corresponding with the book titled \u201cMepu\u201d.\\n\\nData into a format useful for image-captioning (and other text-to-image or image-to-text) tasks. More specifically, we post-process the images and corresponding captions into a format consistent with that of the Red Caps dataset (Desai et al., 2021). This format includes (for each sample in each language) the text caption, the album ID, the image ID, and a static URL to a publicly accessible image file. Additionally and in contrast to the Red Caps data, we include language metadata for each set of captions including a normalized ISO639-3 language code as well as the original language code parsed from the Bloom Library files.\\n\\nIn order to prevent data leakage, all image-caption pairs from one story are put into the same split. If there are fewer than 50 stories for a language, we only provide a test split for that language. If there are more than 50 stories for a language, the validation split receives 20% of the next 250 stories and the train split receives 80%. Any stories above 300 are split between train (90%) and test (10%).\\n\\nIn bloom-captioning, a total of 112,080 image-caption pairs are included, with 157 languages have training splits. For the other languages, the data may be used for testing, or for zero-shot experiments. The bloom-captioning dataset is listed in the Hugging Face Hub as bloom-captioning.\\n\\nBuilding off of the data produced for bloom-captioning, we further process the captioning data into a format useful for language modeling (and other written language NLP) tasks. More specifically, we concatenate all of the captions from each story into a single story text. This results in an array of texts per language. For each language, this array is randomized and split with 80% going into the train set and the remaining 20% split evenly between test and validation.\\n\\nThe bloom-captioning dataset is listed in the Hugging Face Hub as bloom-lm.\\n\\nWe construct the bloom-speech dataset separately from the data pipeline described above for image and caption related data. Bloom \u201cTalking Books\u201d also have an *.htm file associated with them. We parse the HTML tags within this file that contains the information about the paired audio file and text. The language for each audio file was present in either the same tag, or in one of the parent tags (for example, a tag for the whole page of a book with individual sentences underneath). As there is some re-use of audio between books, only one audio segment for each language was downloaded per unique text string.\\n\\nFor all files that were successfully parsed and downloaded, additional checks were performed to ensure that these files matched the tagged language.\\n\\n12 https://huggingface.co/datasets/red_caps\\n13 https://huggingface.co/datasets/sil-ai/bloom-captioning\"}"}
{"id": "emnlp-2022-main-590", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"First, we evaluate the writing system scripts for each language using a mapping of unicode character ranges to script types. Text strings that did not match the script typed used by the majority of records for that language were thrown out. For some languages, this filtering may exclude known alternate script types. However, based on our manual review, the filtering primarily removed records tagged with incorrect language tags. Additionally, we applied FastText language identification (Joulin et al., 2016) to each text string. If the book appeared to be in English or Spanish, when it was supposed to be in another language, we set a flag called \u201cquarantine\u201d that prevented it from being released as part of the public dataset. Similarly, text strings that contain digits instead of spelled out numbers are flagged, such that they are not included by default.\\n\\nThe records output from the dataset are designed to be consistent with other speech recognition datasets on Hugging Face, with file, audio, and text fields. A field for credits was to comply with the Creative Commons attribution sharing requirements, and a field for license was added to identify subsets of the dataset that are licensed under specific Creative Commons licenses (such as cc-by-nc-nd and cc-by-sa). The book, instance, and original_lang_tag provide a means of matching up content with other Bloom Library datasets. The training, test, and validation splits for each language followed the same methodology as that described for bloom-captioning.\\n\\nThe bloom-speech dataset is listed in the Hugging Face Hub as bloom-speech.\\n\\n4 Dataset Analysis\\n\\nThe final statistics per language family for all the datasets are presented in Table 1. In total, the datasets include 11,407 text stories, 112,080 image-caption pairs, 25,680 audio files, and 2,873 minutes of audio across 363 languages and 32 language families. A full list of included languages is provided in the Appendix.\\n\\nOur bloom-vist, bloom-lm, and bloom-captioning datasets include data from 351 of these languages across 31 language families. There is a mean of 32 stories and 319 image-caption pairs and median of 2 stories and 22 image-caption pairs per language. Our bloom-speech dataset comprises 428 hours of total audio in 56 languages. There is a mean of 458 and median of 138 records per language. 18 language families are represented, with a mean of 159 minutes and median of 31 minutes of audio per language family. A full breakdown of the composition of the dataset is available in the Appendix (Table 3).\\n\\nNotably for bloom-speech, among some of the languages of wider use in the dataset (e.g. French, Spanish, and English) the accent of the speaker is often not the same as in other public datasets. In Common Voice, a large majority of French data is labelled as coming from speakers in France. Most of the French data in bloom-speech is from Francophone Africa. The Spanish is primarily from Central America, and the English includes many places where English is a language of commerce.\\n\\n5 Baseline Experiments\\n\\nTo establish some of the first known baselines in languages included in Bloom Library and to assess the difficulty of language modeling, image captioning, and automatic speech recognition with the Bloom Library datasets, we trained baseline models for 44 included languages. The summarized results are included in Table 2.\\n\\n5.1 Language Modeling\\n\\nWe used bloom-lm to fine-tune the DistilBERT base multilingual cased model (Sanh et al., 2019) (available on the Hugging Face hub) for 32 languages. These particular languages were chosen because they each had more than 500 stories in the Bloom Library. We fine-tuned these models on the training split of bloom-lm and tested on the test set for the masked language modeling task. Training was implemented using the Hugging Face Trainer API from the Python transformers package version 4.20.1 (Wolf et al., 2020), with a training and evaluation batch size of 8 and a random seed of 1022. The default Trainer API configurations were used for all other hyperparameters and training arguments. The models each ran for 3 epochs on P100 GPUs.\\n\\n17https://huggingface.co/distilbert-base-multilingual-cased\"}"}
{"id": "emnlp-2022-main-590", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language Family | Languages | Stories | Audio Files | Audio Minutes | Image-Caption Pairs |\\n|-----------------|-----------|---------|-------------|---------------|--------------------|\\n| Afro-Asiatic    | 19        | 304     | 799         | 111           | 2329               |\\n| Algic           | 3         | 6       | 85          | 36            | 80                 |\\n| Austro-Asiatic  | 17        | 195     | 0           | 0             | 2219               |\\n| Austronesian    | 59        | 1560    | 1660        | 160           | 12593              |\\n| Chibchan        | 1         | 1       | 0           | 0             | 7                  |\\n| Chocoan         | 1         | 0       | 19          | 1             | 0                  |\\n| Creole          | 7         | 532     | 1280        | 137           | 5209               |\\n| Dravidian       | 8         | 56      | 818         | 251           | 410                |\\n| Eyak-Athabaskan | 1         | 1       | 14          | 13            | 13                 |\\n| Hmong-Mien      | 1         | 17      | 0           | 0             | 212                |\\n| Indo-European   | 43        | 5961    | 7399        | 829           | 60735              |\\n| Japonic         | 1         | 1       | 0           | 0             | 2                  |\\n| Koreanic        | 1         | 132     | 0           | 0             | 2773               |\\n| Kra-Dai         | 6         | 322     | 0           | 0             | 3229               |\\n| Maipurean       | 2         | 3       | 0           | 0             | 31                 |\\n| Mayan           | 6         | 438     | 7212        | 641           | 4556               |\\n| Niger-Congo     | 101       | 409     | 2704        | 402           | 3703               |\\n| Nilo-Saharan    | 13        | 74      | 15          | 1             | 871                |\\n| North Bougainville | 1       | 1       | 0           | 0             | 9                  |\\n| Otomanguean     | 6         | 28      | 89          | 9             | 245                |\\n| Panoan          | 1         | 13      | 0           | 0             | 103                |\\n| Quechuan        | 8         | 18      | 0           | 0             | 154                |\\n| Ramu-Lower Sepik| 1         | 1       | 0           | 0             | 7                  |\\n| Sepik           | 4         | 16      | 420         | 37            | 194                |\\n| Sino-Tibetan    | 25        | 768     | 2234        | 195           | 6579               |\\n| South Bougainville | 3        | 7       | 63          | 6             | 53                 |\\n| South-Central Papuan | 1    | 7       | 275         | 15            | 113                |\\n| Tequistlatecan  | 2         | 2       | 505         | 16            | 109                |\\n| Torricelli      | 1         | 1       | 0           | 0             | 11                 |\\n| Trans-New Guinea| 15        | 103     | 89          | 13            | 930                |\\n| Turkic          | 2         | 411     | 0           | 0             | 4332               |\\n| Uto-Aztecan     | 3         | 19      | 0           | 0             | 269                |\\n\\nTable 1: Language coverage and dataset statistics by language family. In total, 363 language families are represented.\"}"}
{"id": "emnlp-2022-main-590", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Images | Stories | PPL | ACC | Pairs | BLEU | chrF2 | TER | Files | Min. WER | CER |\\n|----------|--------|---------|-----|-----|-------|------|-------|-----|-------|----------|-----|\\n| ahk      |        |         | 3.06| 0.75| 907   | 0.7  | 16.6  | 115.1| 0     | 0        | 0   |\\n| awa      |        |         | 10.54| 0.54| 1200  | 0.1  | 5.4   | 103.4| 0     | 0        | 0   |\\n| bam      |        |         |     |     | 86    | 52   | 1.06  | 0.09 | 1.04  | 0.09    | *   |\\n| ben      |        |         | 4.5 | 0.66| 2235  | 7.1  | 13.9  | 112.8| 0     | 0        | 0   |\\n| bho      |        |         | 8.56| 0.56| 1172  | 0.1  | 6.1   | 150.7| 0     | 0        | 0   |\\n| boz      |        |         |     |     | 102   | 53   | 0.35  | 0.11 | 0.09  | 0.05    | *   |\\n| bzi      |        |         | 66  |     | 497   | 123  | 0.11  | 0.02 | 1.04  | 0.04    | *   |\\n| cak      |        |         | 13.41| 0.55| 817   | 8.6  | 19.2  | 138.6| 1154  | 0.21    | *   |\\n| ceb      |        |         | 16.78| 0.51| 2953  | -    | -     | -    | 670   | 0.18    | \u2020   |\\n| cgc      |        |         | 33.59| 0.44| 1638  | 0.0  | 6.7   | 103.1| 0     | 0        | 0   |\\n| chd      |        |         | 1  |     | 84    | 7    | 1.06  | 0.57 | 0.94  | 0.1     | \u2020   |\\n| dty      |        |         | 10.84| 0.52| 1310  | 0.8  | 7.6   | 114.0| 0     | 0        | 0   |\\n| eng      |        |         | 2633 | 0.6 | 28618 | 13.9 | 25.4  | 126.0| 4646  | 0.27    | \u2020   |\\n| fas      |        |         | 129 | 0.45| 631   | 0.2  | 6.4   | 140.9| 0     | 0        | 0   |\\n| fra      |        |         | 403 | 0.63| 5278  | 14.8 | 25.0  | 113.0| 360   | 0.29    | \u2020   |\\n| hat      |        |         | 260 | 0.51| 2411  | 1.6  | 15.7  | 111.3| 0     | 0        | 0   |\\n| hau      |        |         | 256 | 0.54| 1865  | 19.6 | 32.8  | 93.2 | 0     | 0        | 0   |\\n| hbb      |        |         | 27  |     | 273   | 95   | 0.27  | 0.06 | 0.11  | 0.06    | *   |\\n| ind      |        |         | 259 | 0.6 | 2177  | 3.8  | 16.6  | 110.0| 14    | 1        | -   |\\n| jra      |        |         | 139 | 0.67| 1423  | 0.2  | 6.3   | 132.1| 303   | 0.12    | 0.03|\\n| kak      |        |         | 195 | 0.52| 1416  | 0.2  | 7.9   | 107.1| 0     | 0        | 0   |\\n| kan      |        |         | 21  |     | 168   | -    | -     | -    | 374   | 0.47    | \u2020   |\\n| kek      |        |         | 36  | 0.59| 621   | 0.4  | 11.9  | 145.7| 1915  | 0.51    | *   |\\n| kir      |        |         | 382 | 0.62| 4026  | 20.9 | 26.0  | 126.7| 0     | 0        | 0   |\\n| kjb      |        |         | 102 | 0.56| 984   | 0.2  | 13.4  | 180.4| 911   | 0.33    | *   |\\n| kor      |        |         | 132 | 0.56| 2773  | 10.1 | 14.5  | 104.1| 0     | 0        | 0   |\\n| mai      |        |         | 180 | 0.59| 1211  | 0.8  | 10.0  | 103.1| 11  | -        | -   |\\n| mam      |        |         | 134 | 0.58| 1317  | 8.0  | 17.3  | 189.1| 1514  | 0.32    | *   |\\n| mhx      |        |         | 98  | 0.6 | 945   | 3.5  | 14.1  | 175.1| 0     | 0        | 0   |\\n| mya      |        |         | 38  |     | 421   | 60   | 0.8   | 0.09 | 0.09  | 0.09    | \u2020   |\\n| myk      |        |         | 34  |     | 341   | -    | -     | -    | 799   | 0.21    | *   |\\n| nep      |        |         | 200 | 0.63| 1507  | 0.8  | 7.9   | 106.2| 0     | 0        | 0   |\\n| new      |        |         | 177 | 0.57| 1225  | 0.0  | 6.4   | 108.7| 0     | 0        | 0   |\\n| por      |        |         | 163 | 0.6 | 3101  | 11.9 | 22.9  | 117.5| 34    | 0.3    | -   |\\n| quc      |        |         | 99  | 0.48| 817   | 3.4  | 12.4  | 179.2| 1677  | 0.31    | *   |\\n| rus      |        |         | 353 | 0.6 | 3933  | 13.3 | 24.7  | 187.9| 0     | 0        | 0   |\\n| sdk      |        |         | 11  |     | 153   | -    | -     | -    | 412   | 0.28    | *   |\\n| snk      |        |         | 35  |     | 356   | -    | -     | -    | 662   | 0.3    | 0.07|\\n| spa      |        |         | 528 | 0.59| 6111  | 10.2 | 18.7  | 131.7| 2073  | 0.24    | \u2020   |\\n| stk      |        |         | 7   |     | 113   | -    | -     | -    | 275   | 0.51    | *   |\\n| tgl      |        |         | 0  |     | 0     | -    | -     | -    | 450   | 0.18    | \u2020   |\\n| tha      |        |         | 285 | 0.67| 3023  | 31.2 | 34.0  | 101.5| 0     | 0        | 0   |\\n| thl      |        |         | 185 | 0.55| 1464  | 5.1  | 10.2  | 135.0| 0     | 0        | 0   |\\n| tpi      |        |         | 201 | 0.75| 2162  | 29.2 | 38.8  | 102.1| 1234  | 0.09    | \u2020   |\\n\\nTable 2: Our complete baseline results for all tasks.\\n\\n* Languages from language families not represented at all in XLS-R\\n\u2020 Languages explicitly included in the training for XLS-R, which we fine-tuned for our baselines. We may expect these scores to benefit compared to languages which were not.\"}"}
{"id": "emnlp-2022-main-590", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Perplexity and accuracy were used as evaluation metrics for the masked language modeling task. The full results can be seen in Table 2. The mean perplexity for all the languages was 10.29, with a maximum of 33.59 (for Kagayanen [cgc]) and minimum of 3.06 (for Akha [ahk]). For reference, the RoBERTa base model trained over BOOKCORPUS and WIKIPEDIA achieves a perplexity of 3.68 (Liu et al., 2019). Thus, bloom-lm seems to show promise for kickstarting language modeling tasks in many new languages, especially when fine-tuning from existing multilingual language models.\\n\\n5.2 Image Captioning\\nWe used bloom-captioning to train image captioning models inspired by Xu et al. (2015) for 31 languages. For each language, we first downloaded the image-caption pairs and performed some pre-processing on the data. For the images, we resized each image to 299x299 pixels and normalized the images so that they contained pixels in the range of -1 to 1. We then extracted image features for each of the images using a version of InceptionV3 (Szegedy et al., 2016) pretrained on Imagenet and available in TensorFlow version 2.8.0. For the captions, we encoded the text into integer sequences with the TextVectorization layer within TensorFlow Keras, keeping a vocabulary of the top 5,000 words.\\n\\nThe image captioning model used a Convolutional encoder network (CNN) followed by an Recurrent decoder network (RNN). The shape of the features from InceptionV3 was 2,048, and we used an embedding dimension of 256 along with a hidden attention layer (Bahdanau Attention) having a dimension of (batch size, 64, 512) in the RNN decoder. Each image captioning model was trained for 50 epochs on A100 GPUs using a random seed of 1022, the default settings of the TensorFlow Adam optimizer, and Sparse Categorical Cross-entropy loss. BLEU score, chrF2, and Translation Error Rate (TER) were used as evaluation metrics for the image captioning task. The full results can be seen in Table 2. The mean BLEU score for all languages was 7.12, with a maximum of 31.2 (for Thai [tha]) and a minimum of 0.0 (for Kagayanen [cgc] and Newar [new]). For reference, the state-of-the-art result on the COCO captions dataset (Chen et al., 2015) (at the time of this paper was drafted) is a BLEU score of 44.9 (Wang et al., 2022). Thai, which has the best captioning performance, also shows good results in language modeling having a perplexity of 3.81. This trend is not generally true, however, with Akha achieving a BLEU score of 0.7 despite having the best language modeling performance. This is likely due to the number of stories and captions available in each language (3k+ image-caption pairs for Thai and only around 900 for Akha) and the diversity of those stories and captions.\\n\\n5.3 Speech Recognition\\nWe used bloom-speech to fine-tune Wav2Vec2 XLS-R model (Babu et al., 2021) (available on the Hugging Face hub) for 23 languages \u2014 any language with at least 275 audio files. Training was implemented using the Hugging Face Trainer API from the Python transformers package installed from source at a particular GitHub commit ID (Wolf et al., 2020).\\n\\nFor the text strings, we normalized white-space characters (eliminating carriage returns, non-breaking spaces, etc.) and removed special characters. For a full list of the special characters, see the source code.\\n\\nNo adjustments were needed for audio processing, as the Hugging Face code and libraries standardize the audio to 16khz sampling rate, as required by Wav2Vec2-based models. All languages were trained with the same parameter settings. Except otherwise noted, all parameters and configuration were taken from the \u201cSingle GPU CTC\u201d settings in the Hugging Face Transformers speech recognition example (von Platten et al., 2022). Early stopping was used with a patience of three, tracking the Word Error Rate (WER) metric. max_duration_in_seconds was set to 25, approximately the maximum that the 20GB GPU partitions we were using could support. eval_steps and warmup_steps were set to 250, with save_steps set to 500. Because of filtering for max duration and special characters, the size of the train, validation, and test sets is reduced in the experiment. For purposes of reproducibility and comparison, we provide the before and after split sizes in the Appendix, Table 3.\\n\\nThe complete baseline data for all 23 languages is available in Table 2. The mean WER across all sets is 0.37 and for all sets with over one hour of training audio, the mean WER is 0.25. Tok Pisin [tpi], Bisu [bzi], and Jarai [jra] achieve the...\"}"}
{"id": "emnlp-2022-main-590", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The top three scores with 0.09, 0.11, and 0.12 WER, in a range that we consider among the state-of-the-art for languages in low resource settings. Tok Pisin is one of the languages explicitly included in the training data for XLS-R, while data from Bisu and Jarai were not included. Of the subset of languages not found in XLS-R, Kaqchikel [cak] (of the Mayan language family) achieves the best score with a WER of 0.21, which may be practically useful for certain implementations.\\n\\nWe believe the case of the Bisu language is particularly worth noting as the language has a population of as few as 700 users (Eberhard et al., 2021). The efforts of the Bisu language community to preserve their language (Person, 2002), particularly through the creation of Bloom Books, and the work that has gone into making large multilingual speech models like XLS-R puts advanced language technology within reach using our datasets.\\n\\n6 Conclusion\\n\\nWe present 4 datasets processed out of the Bloom Library books. These datasets represent a massive increase in language and modality coverage for the tasks of language modeling, image captioning, visual storytelling, and speech recognition. In total, the datasets cover 363 languages across 32 language families.\\n\\nWe demonstrated an ability to make use of the data as-is, and we foresee this dataset being used by researchers to fine-tune multilingual models (as recommended by, e.g., Babu et al. (2021)), benchmark zero-shot performance for linguistically diverse languages, or train new models from scratch in combination with other datasets.\\n\\nIn future work, we would like to continue to improve the size and quality of the dataset. We would also like to explore and understand baseline performance in the various tasks. Some of the baseline performance numbers are quite low compared to numbers on established datasets, and it would be interesting to further understand the characteristics of the Bloom data that make it challenging for various tasks (e.g., lexical diversity). Finally, we would like to prepare aligned multilingual versions of the dataset that would be immediately useful for cross-lingual and multilingual tasks that require parallel corpora.\\n\\n7 Limitations\\n\\n7.1 Data quality\\n\\nThe Bloom Library is populated primarily by community submissions. This increases the linguistic and topic diversity of the set, but it also leads to issues with user-submitted books that have inconsistent metadata, varying quality, images not matching captions, etc. While many of these issues can be detected during the parsing process (e.g. if the number of images does not match the number of captions), it is likely that some issues persist and will need to be addressed in future releases.\\n\\n7.2 Sources of bias\\n\\nOf the religious books included in the datasets, there is a bias towards Christian books and Bible stories. However, the datasets also cover a wide variety of non-religious topics of interest to local language communities and fitting their local language ecology. Some of the non-religious topics included in the datasets are STEM (307 books), COVID-19 (370 books), and Agriculture (87 books).\\n\\nGenerally, there may be few individuals or organizations producing the underlying content for a given language. Bias should be expected and appropriate steps should be taken to evaluate and counteract biases depending on how the data is used.\\n\\n7.3 Cross-lingual alignment\\n\\nWhile our datasets cover many languages, all of the data (for any one particular language) should be considered to be monolingual and unaligned. Bloom books are, by their nature, community-submitted books. The format, count, or even order of image-caption pairs is not guaranteed to match across books within the same translation \u201clineage.\u201d Some translations may, for example, lack captions for certain images. In this release of the datasets, we therefore release each dataset in monolingual form without attempting to align for tasks such as machine translation.\\n\\n7.4 Audio and speakers\\n\\nFor many languages, we have few audio files. Practically, some of these may only serve as a supplement to another dataset by providing a separate domain test set. We have not assessed the variety of speakers in any language, but assume that in some languages there will only be one or a few speakers. In training speech recognition systems, this may\"}"}
{"id": "emnlp-2022-main-590", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"create problems in recognizing different speakers (also see bias in 8). Fine-tuning from a large multi-lingual speech language model, as we have done, may offset some of these concerns. This may also make our metrics (WER and CER) less comparable with other datasets having more variety. While comparison between different datasets/domains is always a concern with these metrics, it may be amplified here. Our models are intended only as baselines and have not been customized to the particularities of each individual language.\\n\\n7.5 Reproducibility\\nAll of the source code for dataset preparation and baseline model training/evaluation is included in this GitHub repository. All of the data used for baseline experiments has been released on the Hugging Face hub.\\n\\n8 Ethical considerations\\nBooks in the released datasets are restricted to those available on the Bloom Library under a Creative Commons (CC) license. The original creators of the material chose these CC licenses, and we received expressed permission for this use and access from Bloom Library team consistent with their terms of use. The original creators were aware at the time of creating the materials that their material would be published publicly on the Internet, and are presumed to be aware that this data would be widely available for uses beyond those originally envisioned.\\n\\nIn this paper we aim to demonstrate the technical feasibility of applications of this dataset to creating NLP tools in local languages. This should not be taken to suggest that we recommend using these tools in local contexts. The normative ethics around creating a tool for African French speakers, for example, do not necessarily apply for local languages like Bisu. We would recommend a consideration of the language ecology (Bird, 2022) and designing tools for \u201cconviviality\u201d (Voinea, 2018), with the goal of sustaining language use (Lewis and Simons, 2016). That is, we urge the reader to consider the impact of the tools they are creating on local language communities, enhance the agency of the users and community who use (or do not use) the tool, and support the flourishing of the community through the use of its own language.\\n\\nWe assess the ecological impact of our dataset creation and model training to be minimal. The dataset creation made minimal use of energy hungry GPUs, and was not measured. Total training time for bloom-speech based models was 86 hours, an average of 3 hours and 45 minutes per model trained. Each training was conducted on a 20G or 40G partition of an NVIDIA A100. Each of the image captioning baseline models and language modeling baseline models required even less training time than bloom-speech.\\n\\nIn terms of equitable access, the hardware needed to replicate the experiments is available through cloud services at a moderate price. The data is being released and will be made publicly available under CC licenses for primarily non-commercial use.\\n\\nAcknowledgments\\nThe Masakhane community has been a great source of help, feedback, and inspiration. Colin Leong would like to thank the University of Dayton Research Institute for their support and encouragement, as well as Dr. Vijayan Asari and the University of Dayton Vision Lab for support and guidance. Further, the authors appreciate important feedback on the paper draft and draft version of the datasets from Sebastian Ruder, Genie Razumovskaia, Paul Frank, and Gary Simons.\\n\\nReferences\\nRosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2019. Common voice: A massively-multilingual speech corpus.\\nArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2021. Xls-r: Self-supervised cross-lingual speech representation learning at scale.\\nLo\u00efc Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the third shared task on multimodal machine translation. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 304\u2013323.\\nSteven Bird. 2020. Decolonising speech and language technology. In Proceedings of the 28th International\"}"}
{"id": "emnlp-2022-main-590", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-590", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of bert: smaller, faster, cheaper and lighter. ArXiv, abs/1910.01108.\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. 2016. Rethinking the inception architecture for computer vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2818\u20132826.\\n\\nCristina Voinea. 2018. Designing for conviviality. Technology in Society, 52:70\u201378. Technology and the Good Society.\\n\\nPatrick von Platten, Lysandre Debut, Sylvain Gugger, Anton Lozhkov, Fran\u00e7ois REMY, Suraj Patil, and Matthew Goldey. 2022. Fine-tuning a huggingface transformers ctc model for automatic speech recognition.\\n\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jingren Zhou, and Hongxia Yang. 2022. Ofa: Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier-ric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nKelvin Xu, Jimmy Lei Ba, Ryan Kiros, Kyunghyun Cho, Aaron Courville, Ruslan Salakhutdinov, Richard S. Zemel, and Yoshua Bengio. 2015. Show, attend and tell: Neural image caption generation with visual attention. In Proceedings of the 32nd International Conference on International Conference on Machine Learning - Volume 37, ICML'15, page 2048\u20132057. JMLR.org.\\n\\nHeeseung Yun, Youngjae Yu, Wonsuk Yang, Kangil Lee, and Gunhee Kim. 2021. Pano-avqa: Grounded audio-visual question answering on 360\u00b0 videos. In ICCV.\\n\\nA Appendix\\nThe full list of languages included in this initial release of our datasets is as follows:\\n\\nGhotuo[aaa]; Ayta, Ambala[abc]; Dangme[ada]; Adangbe[adq]; Akeu[aeu]; Afrikaans[afr]; Aghem[agq]; Esimbi[ags]; Akha[ahk]; Arosi[aia]; Amri Karbi[ajz]; Akan[aka]; Yanesha'[ame]; Amharic[amh]; Alamblak[amp]; Amuzgo, Guerrero[amu]; Obolo[ann]; Athpariya[aph]; Awadhi[awa]; Awa[awb]; Nahuatl, Western Durango[azn]; Awing[azo]; Tuki[bag]; Bamanankan[bam]; Bambili-Bambui[baw]; Bamun[bax]; Babanki[bbk]; Balochi, Southern[bcc]; Bamenyam[bce]; Iceve-Maci[bec]; Benabena[bef]; Bengali[ben]; Bafut[bfd]; Mmen[bfm]; Bunak[bfn]; Bangandu/bgf]; Bhojpuri[bho]; Buwal[bhs]; Bislama[bis]; Banjar[bjn]; Binumarien[bjr]; Baka[bkc]; Bakoko[bkh]; Kom[bkm]; Baikeno[bkx]; Aweer[bob]; Tibetan, Central[bod]; Bozo, Tieyaxo[boz]; Wumboko[bqm]; Braj Bhasha[bra]; Lave[brb]; Mokpwe[bri]; Bru, Western[brv]; Akoose[bss]; Ntcham[bud]; Terei[buo]; Bafaw-Balong[bwt]; Bunu, Bu-Nao[bwx]; Tairaha[bxa]; Bukusu[bxk]; Batak[bya]; Bozo, Jenaama[bze]; Bisu[bzi]; Kaqchikel[cak]; Kakataibo-Kashibo[cbr]; Cebuano[ceb]; Kagayanen[cgc]; Chontal, Highland Oaxaca[chd]; Dene[chp]; Cimbrian[cim]; Kurdish, Central[ckb]; Chontal, Lowland Oaxaca[clo]; Chinese, Mandarin[cmn]; Chinese, Mandarin[cmn]; Mnong, Central[cmo]; Cree, Swampy[csw]; Gichuka[cuh]; Cuvok[cuv]; Dagbani[dag]; Fataluku[ddg]; Deduga[ded]; German, Standard[deu]; Chidigo[dig]; Zarma[dje]; Kinabatangan, Upper[dmg]; Dani, Western[dnw]; Kadazan Dusun[dtp]; Lotud[dtr]; Dotyali[dty]; Chiduruma[dug]; Elip[ekm]; Markweeta[enb]; En[enc]; English[eng]; Ewondo[ewo]; Filipino[fil]; Fali[fli]; Fon[fon]; French[fra]; Fulfulde, Adamawa[fub]; Fulfulde, Western Niger[fuh]; Galolen[gal]; Gadaba, Bodo[gbj]; Gavar[gou]; German, Swiss[gsw]; Wayuu[guc]; Gujarati[guj]; Ekegusii[guz]; Gawri[gwc]; Hak\u00f6[hao]; Haitian Creole[hat]; Hausa[hau]; Nya Huba[hbb]; Kamwe[hig]; Hiligaynon[hil]; Hindi[hin]; Halia[hla]; Mina[hna]; Hre[hre]; Haroi[hro]; Idat\u00e9[idt]; Ilocano[ilo]; Indo-\"}"}
{"id": "emnlp-2022-main-590", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-590", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Total Train Cuts Used | Total Validation Cuts Used | Total Test Cuts Used |\\n|----------|----------------------|---------------------------|---------------------|\\n| bam      | 179 / 203            | 43 / 50                   | 45 / 50             |\\n| boz      | 425 / 427            | 50 / 50                   | 51 / 52             |\\n| bzi      | 1363 / 1363          | 50 / 50                   | 157 / 157           |\\n| cak      | 989 / 989            | 50 / 50                   | 115 / 115           |\\n| ceb      | 553 / 553            | 50 / 50                   | 67 / 67             |\\n| chd      | 205 / 205            | 50 / 50                   | 50 / 50             |\\n| eng      | 3979 / 4143          | 46 / 48                   | 445 / 455           |\\n| fra      | 208 / 261            | 42 / 49                   | 42 / 50             |\\n| hbb      | 546 / 558            | 49 / 50                   | 66 / 67             |\\n| jra      | 203 / 203            | 49 / 50                   | 50 / 50             |\\n| kan      | 232 / 281            | 34 / 43                   | 40 / 50             |\\n| kek      | 1675 / 1676          | 49 / 49                   | 189 / 190           |\\n| kjb      | 767 / 770            | 50 / 50                   | 91 / 91             |\\n| mam      | 1312 / 1313          | 50 / 50                   | 151 / 151           |\\n| mya      | 299 / 321            | 49 / 50                   | 48 / 50             |\\n| myk      | 635 / 669            | 48 / 50                   | 76 / 80             |\\n| quc      | 1449 / 1460          | 50 / 50                   | 166 / 167           |\\n| sdk      | 312 / 312            | 50 / 50                   | 50 / 50             |\\n| snk      | 517 / 546            | 49 / 50                   | 64 / 66             |\\n| spa      | 1807 / 1816          | 50 / 50                   | 207 / 207           |\\n| stk      | 180 / 180            | 45 / 45                   | 50 / 50             |\\n| tgl      | 352 / 352            | 48 / 48                   | 50 / 50             |\\n| tpi      | 1044 / 1061          | 48 / 50                   | 121 / 123           |\\n\\nTable 3: Bloom-speech comparison of cuts used in training versus total available. In cases where the total validation cuts is not 50, it indicates a file(s) which failed to download.\"}"}
