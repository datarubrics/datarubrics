{"id": "emnlp-2023-main-40", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstracts derived from biomedical literature possess distinct domain-specific characteristics, including specialised writing styles and biomedical terminologies, which necessitate a deep understanding of the related literature. As a result, existing language models struggle to generate technical summaries that are on par with those produced by biomedical experts, given the absence of domain-specific background knowledge. This paper aims to enhance the performance of language models in biomedical abstractive summarisation by aggregating knowledge from external papers cited within the source article. We propose a novel attention-based citation aggregation model that integrates domain-specific knowledge from citation papers, allowing neural networks to generate summaries by leveraging both the paper content and relevant knowledge from citation papers. Furthermore, we construct and release a large-scale biomedical summarisation dataset that serves as a foundation for our research. Extensive experiments demonstrate that our model outperforms state-of-the-art approaches and achieves substantial improvements in abstractive biomedical text summarisation.\\n\\n1 Introduction\\n\\nBiomedical text summarisation plays a pivotal role in facilitating the comprehension of the vast and constantly expanding body of biomedical literature (Xie et al., 2022), which poses a significant challenge for clinicians and domain experts who strive to remain well-informed in their respective fields. To address this challenge, the generation of high-quality summaries from the extensive corpus of biomedical literature holds immense potential in supporting research and advancements within the biomedical domain (DeYoung et al., 2021).\\n\\nOne of the key challenges in biomedical natural language generation (NLG) lies in effectively handling domain-specific terminologies that are prevalent in biomedical texts. Consequently, a plethora of research studies have been conducted with a primary focus on enhancing language quality by better integrating domain-specific knowledge in the biomedicine domain (Sotudeh Gharbagh et al., 2020; Tangsali et al., 2022; An et al., 2021; Tang et al., 2023b). However, most prior works have predominantly attempted to incorporate knowledge by leveraging additional annotations within the paper content. These annotations include frequent items (Givchi et al., 2022), named entities (Schulze and Neves, 2016; Peng et al., 2021), entity relations (Shang et al., 2011), as well as external knowledge systems such as biomedical ontologies (Chandu et al., 2017) and external terminology...\"}"}
{"id": "emnlp-2023-main-40", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"searching tools (Gigioli et al., 2018). Surprisingly, the inclusion of external knowledge derived from citation papers has been rarely explored in previous biomedical studies. Existing corpora for biomedical text summarisation are typically constructed in a manner that models solely rely on the source article when generating a summary. However, as shown in Figure 1, there exists strong connections among papers in the citation network with shared research backgrounds, terminologies, and abstract styles, which will be a useful source of knowledge for improving biomedical abstractive summarisation but not captured in existing datasets.\\n\\nTo address this gap in the existing biomedical summarisation dataset, we construct a novel biomedical summarisation dataset utilising an open-source biomedical literature corpus provided by the Allen Institute. During the dataset construction process, we applied rigorous filtering criteria to eliminate low-quality samples. Specifically, we discarded samples with an insufficient number of citations (less than three distinct citations), as well as unqualified papers whose unique identifiers (UIDs) or citation UIDs were inaccessible within the corpus. Additionally, we designed heuristic rules to select and transform the unstructured raw data corpus into a structured dataset in JsonL format. The final dataset comprises over 10,000 instances, with each instance having an average of 16 citations. To the best of our knowledge, this is the largest biomedical literature dataset specifically tailored for citation paper-enhanced biomedical text summarisation. Furthermore, we provide the corresponding methods for collecting the citation network, including cited papers and their associations.\\n\\nFacilitated by our biomedical summarisation dataset, we further propose a novel approach to biomedical document summarisation whereby we enhance neural models with external domain-specific knowledge in the form of the abstracts of cited papers. Accordingly, we introduce an attention-based network (Vaswani et al., 2017) that dynamically aggregates features extracted from the citation abstracts with the encoded content features of the main paper. This aggregation is achieved by applying attention mechanisms to the associated abstracts of all cited papers, which provides the subsequent summary decoding process with additional features derived from abstracts of the citation papers. Within this framework, the base language model can effectively leverage both the features of the main paper and the additional domain-specific knowledge obtained from cited papers. Consequently, this integration leads to enhanced performance in text summarisation. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines in abstractive biomedical text summarisation. We also conducted an in-depth quantitative analysis to verify the performance gain obtained by our attention-based citation knowledge enhancement framework. Our contributions are summarised as follows:\\n\\n\u2022 We construct a large-scale biomedical literature dataset, which can be used for enhancing biomedical text summarisation with the extracted external knowledge from cited papers.\\n\u2022 We propose a novel framework that can effectively leverage citation papers to enhance the performance of large-scale language models on abstractive summarisation of biomedical literature.\\n\u2022 We conduct extensive experiments to evaluate the effectiveness of our proposed framework, including comparisons with SOTA models and an in-depth analysis of the performance gain achieved by aggregating different quantities of citations.\\n\\n2 Related Work\\n\\nIn recent years, a variety of large-scale pre-trained models (PLMs), such as BART (Lewis et al., 2019); T5 (Raffel et al., 2020); GPT-2 (Radford et al., 2019), have demonstrated remarkable performance improvements across various tasks (Loakman et al., 2023; Zhang et al., 2023; Zhao et al., 2023; Tang et al., 2022b) in the Natural Language Generation (NLG) Domain. These PLMs have also been widely applied to biomedical text summarisation. These models, e.g. BioBERT (Lee et al., 2020) and BioBART (Yuan et al., 2022), have achieved remarkable performance by training on extensive biomedical literature corpora, such as Pubmed and MIMIC-III. However, certain high-level knowledge, e.g., the understanding of medical terminologies, cannot be adequately captured solely...\"}"}
{"id": "emnlp-2023-main-40", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"through the implicit modeling of word probabilities. To address this limitation, the improvement of biomedical background knowledge understanding is able to necessitate the integration of additional knowledge systems, such as conceptual ontologies. These ontologies explicitly model representations of domain-specific knowledge learned by neural networks. Recent studies have proposed incorporating biomedical knowledge, including terminologies (Tang et al., 2023b) and concepts (Chandu et al., 2017), to enhance the performance of these language models and bridge the gap between language understanding and specialized biomedical knowledge. Indeed, several notable works have focused on enhancing summarisation through citations in the open domain, such as An et al. (2021) and Yasunaga et al. (2019). However, it is important to highlight that the progress of language models in the biomedical domain has been hindered by the limited availability of datasets and resources. This scarcity has impeded the further advancement and improvement of pre-trained language models (PLMs) specifically tailored for biomedical applications. In this study, we require a dataset that contains retrievable citation papers, making traditional raw data corpora such as Pubmed and MIMIC-III inadequate. To date, the sole public dataset we could find is the Text Analysis Conference (TAC) 2014 Biomedical Summarization track (Cohan et al., 2014). However, this dataset is limited in size comprising merely 313 instances, and is somewhat outdated. Therefore, we construct a novel dataset for investigating biomedical citation-enhanced summarisation.\\n\\n3 Dataset Construction\\n\\n3.1 Construction Process\\n\\nIn order to create a dataset containing biomedical literature and its associated citations, we process a semi-structured raw corpus released by Allen Institute. We refer to this dataset as BioCiteDB throughout the paper. The construction process of the dataset is outlined in algorithm 1, where $C$ represents the raw corpus, and $D$ represents the processed dataset. To ensure the quality and relevance of the data, the selected papers have to meet the following requirements: (1) The papers must include the \u201cIntroduction\u201d section, as it is considered the most crucial part for generating abstracts; (2) The papers must have at least three distinct citations to ensure the quality of curated data; (3) The essential elements of the papers, including UID (Pubmed id), Title, Abstract, Sections, and Citations, must be accessible within the raw corpus. As a result of this construction process, the dataset $D$ comprises structured data in JsonL format, with each sample representing an individual paper.\\n\\n3.2 Data Statistics\\n\\nThe statistical analysis of our processed dataset is presented in Table 1. Additionally, the distribution of the dataset is shown in Table 2.\\n\\n### Table 1: Data statistics of Biomed Ref dataset\\n\\n|          | Train | Val  | Test |\\n|----------|-------|------|------|\\n| # Samples| 9144  | 1143 | 1143 |\\n| # Papers | 18194 | 4762 | 4618 |\\n| Avg. # Distinct Citations of Doc | 6.28 | 6.23 | 6.26 |\\n| Avg. # Total Citations of Doc | 16.56 | 16.85 | 16.96 |\\n| Avg. # Chunks in Doc | 37.33 | 37.33 | 37.13 |\\n| Avg. # Sentences in Doc | 529.94 | 526.8 | 529.11 |\\n| Avg. # Words in Doc | 9858.09 | 9901.96 | 9907.31 |\\n| Avg. # Sentences in Summary | 13.96 | 13.98 | 13.69 |\\n| Avg. # Words in Summary | 220.25 | 222.04 | 217.28 |\\n\\nDatasets\\n\\n|          | Train | Val  | Test |\\n|----------|-------|------|------|\\n| # Samples| 9144  | 1143 | 1143 |\\n| # Papers | 18194 | 4762 | 4618 |\\n| Avg. # Distinct Citations of Doc | 6.28 | 6.23 | 6.26 |\\n| Avg. # Total Citations of Doc | 16.56 | 16.85 | 16.96 |\\n| Avg. # Chunks in Doc | 37.33 | 37.33 | 37.13 |\\n| Avg. # Sentences in Doc | 529.94 | 526.8 | 529.11 |\\n| Avg. # Words in Doc | 9858.09 | 9901.96 | 9907.31 |\\n| Avg. # Sentences in Summary | 13.96 | 13.98 | 13.69 |\\n| Avg. # Words in Summary | 220.25 | 222.04 | 217.28 |\\n\\nDatasets\\n\\n3.2 Data Statistics\\n\\nThe statistical analysis of our processed dataset is presented in Table 1. Additionally, the distribution of the dataset is shown in Table 2.\"}"}
{"id": "emnlp-2023-main-40", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: A visualization of the distribution of citations per paper and the data size associated with each split. To avoid clutter and maintain clarity, we have excluded papers with over 50 citations from the visualization, as they constitute a relatively small proportion of the dataset and fall into the long tail category. Some papers in our corpus are only used as citations other than the papers in data splits, so we categorize them as \u201cothers.\u201d\\n\\nThe results obtained from both the statistical analysis and visual representations in Table 1 and Figure 2 both validate the data quality of the constructed dataset, thus indicating the effectiveness of our data construction process and the consistency of the dataset splits. This validation supports the notion that training and inference tasks conducted on this dataset can be regarded as fair and reliable.\\n\\nAlgorithm 2: Extracting Citation Graph $G$\\n\\nInput: $d_i \\\\in D$; hop$_{\\\\text{max}}$\\n\\nOutput: The set of related papers $P$\\n\\n1. Initialise current hop $n = 0$; a double-ended queue $DQ \\\\leftarrow (d_i.\\\\text{uid}, n)$; a queue recording visited nodes $VQ$.\\n2. while $s_i$ in $S$ do\\n3. pop $uid$ and $n$ from $DQ$\\n4. if $n > \\\\text{hop}_\\\\text{max}$ then\\n5. return $P$\\n6. end\\n7. $P \\\\leftarrow (uid, n)$\\n8. if $|P| > N$ then\\n9. return $P$\\n10. end\\n11. get $d_j$ by $uid$\\n12. $VQ \\\\leftarrow uid$\\n13. foreach $r_n \\\\in d_j.\\\\text{citations}$ do\\n14. if $r_n.\\\\text{uid} \\\\notin VQ$ and $r_n \\\\in D$ then\\n15. $P \\\\leftarrow r_n.\\\\text{uid}$ and $VQ \\\\leftarrow r_n.\\\\text{uid}$\\n16. end\\n17. end\\n18. end\\n\\n3.3 Extract Citation Graph\\n\\nScientific papers are intricately connected through citation relationships, forming a network of interconnected nodes. This citation graph provides valuable insights into the relatedness of papers. In order to retrieve relevant papers within this citation graph, we propose an algorithm outlined in Algorithm 2. hop$_{\\\\text{max}}$ defines the maximum number of hops between papers that the algorithm can traverse, and $N$ specifies the maximum number of retrieved papers at each hop. As output, $P$ represents papers as nodes, while citation relationships are represented as edges in the network. Due to the high computational cost of processing long documents for summarisation, we set hop$_{\\\\text{max}}$ to 1 and neighbor$_{\\\\text{max}}$ to 12, taking into account the limitations of our available computing resources. However, it is worth noting that the attention-based citation aggregation module can be extended to incorporate Graph Attention Networks (Zhou et al., 2020), which have the capability to integrate multi-layer citation graphs (Zhang et al., 2023).\\n\\n4 Methodology\\n\\nAs illustrated in Figure 3, our proposed framework is designed to enhance the performance of the base language model by leveraging the collective knowledge from a set of citation papers. For our experiments, we select BART (Lewis et al., 2019), a widely-used summarization model that has demonstrated promising results in the biomedical domain (Goldsack et al., 2022, 2023), as the base model. In this study, we adopt a strategy where we concatenate the abstracts of the citation papers with the input document to form the model\u2019s input. This approach is motivated by the goal of enabling the model to capture and emulate the writing style present in relevant papers. By incorporating this additional information, we aim to improve the model\u2019s ability to generate high-quality summaries that align with the conventions and patterns observed in the domain-specific literature.\"}"}
{"id": "emnlp-2023-main-40", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Task Definition\\n\\nThe task is formulated as follows: Given a paper document \\\\( d_i \\\\in D \\\\) as the input, where \\\\( D \\\\) represents the paper corpus, and \\\\( d_i \\\\) denotes the \\\\( i \\\\)-th paper. In addition, the citations papers \\\\( D_c = \\\\{ d_{c1}, d_{c2}, \\\\ldots, d_{ck} \\\\} \\\\) are also provided as the input. The abstracts of \\\\( d_{ck} \\\\in D_c \\\\) are denoted \\\\( \\\\text{abs}_{c} \\\\). Either \\\\( d_i \\\\) or \\\\( d_{cj} \\\\) consists of a sequence of words represented as \\\\( X = \\\\{ x_1, x_2, \\\\ldots, x_t \\\\} \\\\) where \\\\( x_t \\\\) denotes the \\\\( t \\\\)-th word in \\\\( X \\\\). The goal is to generate a summary \\\\( Y = \\\\{ y_1, y_2, \\\\ldots, y_t \\\\} \\\\) by modeling the conditional probability distribution \\\\( P(Y | X \\\\in d_i, X \\\\in D_c) \\\\).\\n\\n4.2 Knowledge Aggregation from Citations\\n\\nInput\\n\\nAt the initial stage, both the input document \\\\( d_i \\\\) and its retrieved \\\\( N \\\\) citation abstracts \\\\( \\\\text{abs}_{c} \\\\) are concatenated and encoded by language models. Byte-Pair Encoding (Radford et al., 2019) is implemented in the transformation from text into fixed word embeddings:\\n\\n\\\\[\\nE_{\\\\text{doc}} = \\\\text{LMemb}(\\\\[\\\\text{Tok}_{\\\\text{CLS}}, x_t \\\\in d_i\\\\]) \\\\tag{1}\\n\\\\]\\n\\n\\\\[\\nE_{\\\\text{abs}_{c} j} = \\\\text{LMemb}(\\\\[\\\\text{Tok}_{\\\\text{ABS}}, x_t \\\\in \\\\text{abs}_{c} j\\\\]) \\\\tag{2}\\n\\\\]\\n\\n\\\\[\\nE_{Q j} = \\\\text{concat}(E_{\\\\text{doc}}, E_{\\\\text{abs}_{c} j}) \\\\tag{3}\\n\\\\]\\n\\nwhere \\\\( \\\\text{LMemb} \\\\) represents the module responsible for tokenising and converting words into subword embeddings. \\\\( \\\\text{Tok}_{\\\\text{CLS}} \\\\) is a special token that signifies the global context tag in the input text. \\\\( \\\\text{Tok}_{\\\\text{ABS}} \\\\) is a special token used to indicate the separation between the input document and the cited abstracts. \\\\( E_{Q j} \\\\) denotes the embeddings generated for the \\\\( j \\\\)-th (\\\\( j \\\\in [1, N] \\\\)) document abstract pair.\\n\\nEncoding\\n\\nIn order to capture the relevance of each cited abstract, we employ an attention mechanism to measure the importance of \\\\( d_i \\\\) with respect to \\\\( \\\\text{abs}_{c} j \\\\). The attention score is denoted as \\\\( \\\\text{attn}_{ij} \\\\), and the process of aggregating knowledge is illustrated as follows:\\n\\n\\\\[\\nE_{Q} = \\\\text{concat}(E_{Q1}, \\\\ldots, E_{QN}) \\\\tag{4}\\n\\\\]\\n\\n\\\\[\\nQ = \\\\text{LMenc}(E_{Q}), Q \\\\in \\\\mathbb{R}^{N \\\\times L \\\\times M} \\\\tag{5}\\n\\\\]\\n\\nwhere \\\\( E_{Q} \\\\) denotes the matrix of embeddings for all composed \\\\( Q_j \\\\), and it is encoded by the language model encoder to generate the encoded features \\\\( Q \\\\).\\n\\n\\\\[\\nQ_{CLS} = \\\\text{FirstPool}(Q), Q_{CLS} \\\\in \\\\mathbb{R}^{N \\\\times M} \\\\tag{6}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Attn}_{\\\\text{logits}} = Q_{CLS} W_Q, \\\\text{Attn}_{\\\\text{logits}} \\\\in \\\\mathbb{R}^{N \\\\times 1} \\\\tag{7}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Attn} = \\\\text{softmax}(\\\\text{Attn}_{\\\\text{logits}}), \\\\text{Attn} \\\\in \\\\mathbb{R}^{N \\\\times 1} \\\\tag{8}\\n\\\\]\\n\\n\\\\[\\nF = \\\\text{Attn}^T Q, F \\\\in \\\\mathbb{R}^{L \\\\times M} \\\\tag{9}\\n\\\\]\\n\\nIn the above equations, \\\\( \\\\text{FirstPool} \\\\) collects features that represent the global context of the input \\\\( d_i \\\\) and \\\\( \\\\text{abs}_{c} j \\\\) pairs. As the hidden states of the neural encoder, \\\\( Q \\\\) incorporates features from both the documents and the abstracts. Therefore, the representations of the first position in \\\\( Q \\\\) (represented as \\\\( Q_{CLS} \\\\)) correspond to the global context token \\\\( \\\\text{Tok}_{\\\\text{CLS}} \\\\). The attention logits matrix is obtained by applying a trainable parameter \\\\( W_Q \\\\in \\\\mathbb{R}^{M \\\\times 1} \\\\) to the features of \\\\( Q_{CLS} \\\\). After applying the softmax function for normalization, \\\\( \\\\text{Attn} \\\\) represents the importance of the input features and is used to reweight the original encoded features \\\\( Q \\\\), resulting in the final features \\\\( F \\\\).\"}"}
{"id": "emnlp-2023-main-40", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 Summary Generation\\n\\nIn line with other abstractive summarization systems, we employ an auto-regressive decoder to generate summary tokens $y_t$ in a sequential manner. The process is described as follows:\\n\\n\\\\[\\nH_t = \\\\text{Decoder}(y_{<t}, F) \\\\tag{10}\\n\\\\]\\n\\n\\\\[\\nP(y_t | y_{<t}, X) = \\\\text{softmax}(H_t W_D) \\\\tag{11}\\n\\\\]\\n\\n\\\\[\\ny_t \\\\overset{\\\\text{sampling}}{\\\\leftarrow} P(y_t | y_{<t}, F) \\\\tag{12}\\n\\\\]\\n\\nwhere $t$ represents the current time step. $X$ corresponds to the input, consisting of the words from $d$ and $\\\\text{abs}_c_1, ..., \\\\text{abs}_c_j$, provided to the neural model. $H_t$ refers to the hidden state of the decoder module at time step $t$. This state is computed by the language models using the infused features $F$, which encapsulate the information from the input document and its cited abstracts, along with the previously predicted tokens $y_{<t}$. $W_D$ denotes a trainable parameter, and $P(y_t | y_{<t}, F)$ represents the probability distribution over the vocabulary, which includes special tokens. Employing a sampling strategy, such as $\\\\text{argmax}$, we obtain the predicted token $y_t$.\\n\\n4.4 Training and Inference\\n\\nFinally, as shown in Figure 3, the neural model is trained to fit on the citation-enhanced training set by the following objective function:\\n\\n\\\\[\\nL = -\\\\frac{1}{N} \\\\sum_{t=1}^{N} \\\\log P(y_t | y_{<t}, X) \\\\tag{13}\\n\\\\]\\n\\nwhere $L$ is the cross-entropy loss employed to train the model in modeling the conditional probabilities over the token sequence $P(y_t | y_{<t}, F)$. By minimizing $L$, the language model learns to predict the referenced abstract corresponding to the input document.\\n\\n5 Experiment\\n\\n5.1 Experimental Setup\\n\\nBaselines\\n\\nWe include a range of competitive PLM models as our baselines. We also provide the results of two rule-based systems, namely LEAD-3 and ORACLE, which serve as benchmarks representing the upper and lower bounds of model performance. LEAD-3 extracts the first 3 sentences from the input as the summary, which can be considered as the lower bound of the performance. ORACLE selects sentences from the input document and composes a summary with the highest score, which is the upper bound of extractive summarisation systems. The PLM models serve as baselines for abstractive biomedical summarisation. The Long-Document Transformer (LED) is a Transformer-based model which is able to process long sequences due to their self-attention operation (Beltagy et al., 2020). PE-GASUS (Zhang et al., 2020) is pre-training large Transformer-based encoder-decoder models on massive text corpora with a new self-supervised objective, which is tailored for abstractive text summarisation. BART (Lewis et al., 2019) is a widely used PLM model based on a denoising autoencoder that has proved effective for long text generation tasks. Pubmed-X refers to several PLM-based baselines that have been pre-trained on a large-scale biomedical literature corpus Pubmed (Cohan et al., 2018) (the size of the dataset is 215k), where $X$ denotes the name of a PLM. Additionally, we include ChatGPT for comparison. However, as it is closed-source and very expensive for training, we were unable to use ChatGPT as the base language model to fine-tune on our dataset. Instead, we compare the outputs of ChatGPT in a zero-shot setting.\\n\\nEvaluation Metrics\\n\\nIn the domain of text summarisation (Sun et al., 2021; Tang et al., 2022a; Xie et al., 2022), ROUGE (Lin, 2004) is the most used metric for the evaluation of generated summaries. For evaluating the quality of the generated summaries, we implement the ROUGE metric with the python package of rouge_score. Specifically, we report the unigram and bigram overlaps (ROUGE-1 and ROUGE-2, respectively) between the generated summaries and the reference (golden) summaries. Additionally, we include the longest common subsequence (ROUGE-L) metric to evaluate the fluency of the generated summaries. For each ROUGE metric, we provide fine-grained measurements of precision, recall, and F-values, offering a comprehensive assessment of the summarisation performance.\\n\\nIn addition to the ROUGE metric, we conduct an extensive automatic evaluation utilising a broader range of evaluation metrics. Specifically, we implement a set of automatic evaluation metrics which also include ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2, ROUGE-L, ROUGE-1, ROUGE-2"}
{"id": "emnlp-2023-main-40", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Automatic evaluation based on ROUGE scores. LEAD-3, ORACLE, and ChatGPT were excluded from the performance comparisons as they were not trained on the datasets. However, we use them as reference models to provide insights into the potential performance achievable on our datasets. For each metric, the best overall score is highlighted in bold, and the baseline score is underlined. \u2191/\u2193 indicates the higher/lower the better, respectively.\\n\\n| Model            | PPL\u2193 | ROUGE-1\u2191 | ROUGE-2\u2191 | ROUGE-L\u2191 |\\n|------------------|------|----------|----------|----------|\\n| LEAD-3           |      |          |          |          |\\n| ORACLE           |      |          |          |          |\\n| ChatGPT          |      | 0.2039   | 0.0647   | 0.0941   |\\n| LED              | 0.5669 | 0.4121   | 0.4676   |\\n| PEGASUS          | 0.5669 | 0.4121   | 0.4676   |\\n| BART             | 0.5669 | 0.4121   | 0.4676   |\\n| Pubmed-LED       | 0.5669 | 0.4121   | 0.4676   |\\n| Pubmed-PEGASUS   | 0.5669 | 0.4121   | 0.4676   |\\n| Pubmed-BART      | 0.5669 | 0.4121   | 0.4676   |\\n| Pubmed-BART - w one citation | 0.5669 | 0.4121   | 0.4676   |\\n| Pubmed-BART - w citation agg. | 0.5669 | 0.4121   | 0.4676   |\\n\\nTable 3: Automatic evaluation on more metrics. BeS and BaS denote the F1 values of BERTScore and BartScore, respectively. FLK and CLI denote the readability scores of Flesch-Kincaid and Coleman-Liau Index, respectively.\\n\\n| Model            | BeS\u2191 | BaS\u2191 | FLK\u2193 | CLI\u2193 |\\n|------------------|------|------|------|------|\\n| Golden           | 100  | -0.27| 16.49| 15.69|\\n| ChatGPT          | 85.56| -3.11| 16.19| 16.31|\\n| Pubmed-LED       | 82.85| -3.37| 16.33| 15.31|\\n| Pubmed-PEGASUS   | 81.97| -3.42| 15.78| 14.52|\\n| Pubmed-BART      | 83.34| -3.36| 14.45| 13.32|\\n| Pubmed-BART - w one citation | 83.34| -3.36| 14.78| 13.51|\\n| Pubmed-BART - w citation agg. | 83.60| -3.35| 13.82| 13.20|\\n\\n5.2 Implementation Details\\n\\nAll of the pre-trained models used are restored from the publicly available checkpoints on Hugging Face. The checkpoints we selected include: LED, PEGASUS, BART, Pubmed-LED, Pubmed-PEGASUS, and Pubmed-BART.\\n\\nTo make the comparison fair, all input text is chunked according to the minimal input size limitation of selected language models. In our experiments, it is BART (1024 tokens). Models are trained for up to 10 epochs on a Tesla A40 machine, which has 40 GB GPU memory, and the best checkpoints are kept based on the perplexity of generated responses during validation for the testset. The batch size is set to 16, and the learning rate is $1 \\\\times 10^{-4}$, with the Adam optimizer selected for training. For more details, please refer to the Appendix A.1.\\n\\n5.3 Automatic Evaluation\\n\\nThe results of all experiments are presented in Table 2. It can be observed that our proposed framework (-w citation agg.) significantly outperforms all baseline models across all ROUGE scores (F1 scores), indicating substantial enhancements in the summarisation capability of biomedical papers. To be more specific, the incorporation of citation knowledge has contributed to a substantial improvement in recall, with ROUGE-1 exhibiting a 5.7% increase and ROUGE-2 demonstrating an 8.1% increase. This suggests that the integration of citation knowledge helps in capturing more relevant information from the source text, thereby improving the summarisation quality.\\n\\nModel Referenced Unreferenced\\nBeS\u2191 BaS\u2191 FLK\u2193 CLI\u2193\\nGolden 100 -0.27 16.49 15.69\\nChatGPT 85.56 -3.11 16.19 16.31\\nPubmed-LED 82.85 -3.37 16.33 15.31\\nPubmed-PEGASUS 81.97 -3.42 15.78 14.52\\nPubmed-BART 83.34 -3.36 14.45 13.32\\nPubmed-BART - w one citation 83.34 -3.36 14.78 13.51\\nPubmed-BART - w citation agg. 83.60 -3.35 13.82 13.20\\n\\nModel Referenced Unreferenced\\nBeS\u2191 BaS\u2191 FLK\u2193 CLI\u2193\\nGolden 100 -0.27 16.49 15.69\\nChatGPT 85.56 -3.11 16.19 16.31\\nPubmed-LED 82.85 -3.37 16.33 15.31\\nPubmed-PEGASUS 81.97 -3.42 15.78 14.52\\nPubmed-BART 83.34 -3.36 14.45 13.32\\nPubmed-BART - w one citation 83.34 -3.36 14.78 13.51\\nPubmed-BART - w citation agg. 83.60 -3.35 13.82 13.20\"}"}
{"id": "emnlp-2023-main-40", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In addition, within our framework, the language model achieves substantially lower perplexity (PPL) and ROUGE-L scores, signifying an improvement in the language quality and reduced confusion during summary generation. We hypothesise that the decrease in PPL and ROUGE-L indicates that the language model has learned writing styles and relevant biomedical terminologies by referring to the abstracts of cited papers.\\n\\nRegarding the ablation study, -w one citation yields a slight improvement compared to the baseline model Pubmed-BART but exhibits a higher perplexity. This observation suggests that the direct inclusion of random citation content may introduce certain noise. In contrast, our attention-based mechanism enables the neural networks to dynamically select and aggregate important information from multiple citations, effectively addressing confusion issues associated with additional inputs.\\n\\nIn Table 3, we present the results of additional evaluation metrics. BertScore and BartScore, as machine learning-based metrics, measure the semantic similarity between the generated summaries and the reference abstracts. Flesch-Kincaid and Coleman-Liau metrics assess text readability on a vocabulary level. Across all these metrics, -w citation agg. outperforms all baseline models, showcasing the advantages of introducing citation knowledge with our framework. Further analysis within the \\\"Pubmed-BART\\\" model reveals that using a single citation results in a slight decrease in BERTScore and BartScore, along with a slightly higher Flesch-Kincaid score (14.78) and Coleman-Liau Index score (13.51). However, employing citation aggregation leads to improvements across all metrics, with BERTScore (83.60), BartScore (-3.35), Flesch-Kincaid score (13.82), and Coleman-Liau Index score (13.20). This analysis confirms our initial hypothesis, that directly introducing a random citation may introduce noise that hampers model performance, while our aggregation model comprehensively considers all citation papers, effectively reducing the random noise introduced by a single citation.\\n\\n5.4 Human Evaluation\\n\\nIn order to obtain a more comprehensive evaluation of the generated summaries, we also incorporate human evaluation. This evaluation focuses on four key aspects: fluency, readability, relevance, and informativeness. Fluency assessment aims to measure the overall quality of the language used in the summaries. Readability evaluation determines the extent to which the summaries are easily understandable by readers. Relevance assessment examines whether the content of the summaries is pertinent and aligned with the content of the input document. Informativeness measurement evaluates the extent to which the generated summaries provide sufficient and meaningful information derived from the given input. By incorporating human evaluation, we can assess subjective aspects of summary quality that automated metrics may not fully capture.\\n\\nConsidering the difficulty of evaluating generated summaries, which requires a thorough understanding of the content in both the source papers and the summaries, it is imperative that human evaluators possess a strong background in academic writing and biomedical knowledge. We invite 3 qualified evaluators by snowball sampling to rate 30 randomly sampled instances from the testset. In order to minimise biases and increase inter-annotator agreement, the evaluators were provided with the same annotation guide (see Appendix A.3). The results of the human evaluation are presented in Table 4. It can be observed that both the -w one citation and -w citation agg. models exhibit superior performance compared to other baseline models, thereby affirming the effectiveness of our proposed framework.\"}"}
{"id": "emnlp-2023-main-40", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: (a), (b) and (c) illustrate the bar charts depicting the performance enhancement achieved by the -w citation agg. method over the PubmedBART model. The improvement is calculated as the difference between the scores obtained by -w citation agg. and PubmedBART. In these bar charts, we report the ROUGE F1 scores. Additionally, Figure 1(d) exhibits the smoothed curve of the ROUGE scores data, obtained using Gaussian kernel smoothing technique. Due to the limit of GPU memory, we limit the input of the citation aggregation network to 12 citation papers.\\n\\nTo delve further into the evaluation, the metrics of Relevance and Informativeness underscore the improved capability to extract relevant information from the input content and generate comprehensive abstracts. Additionally, the fluency and readability metrics assess the language quality, indicating that the language model generates abstracts that are more coherent and natural. However, it is important to note that the tested Pretrained Language Models (PLMs) exhibited a notable disparity in language quality when compared to the performance of ChatGPT. This discrepancy can be attributed to the substantial difference in model size, with ChatGPT having 130 billion parameters, whereas the tested PLMs have less than 5 billion parameters.\\n\\n5.5 In-depth Analysis\\n\\nTo further investigate the impact of the citation knowledge aggregation module, we conduct an evaluation to assess the improvement in the generated abstracts. This evaluation involves comparing the performance of our proposed framework, denoted as -w citation agg., against the base model Pubmed-BART using ROUGE scores. The results, presented in Figure 4 as (a), (b), and (c), illustrate the increase in ROUGE scores (F value) for different numbers of citations. The inclusion of citations is shown to have a positive effect on the abstract generation process. The Gaussian kernel smoothed increasing curve, depicted in Figure 4 (d), indicates a clear trend: as more citation abstracts are introduced, the language model exhibits greater improvements. The results highlight the potential of leveraging citation information to enhance the quality of generated abstracts.\\n\\n6 Conclusion\\n\\nIn conclusion, we proposed a novel attention-based citation aggregation model that incorporates domain-specific knowledge from citation papers. By integrating this additional information, our model enables neural networks to generate summaries that benefit from both the paper content and the associated knowledge extracted from citation papers. Furthermore, we introduced a specialized biomedical summarisation dataset, which served as a valuable resource for evaluating and advancing our research. The effectiveness of our approach was demonstrated through extensive experiments, where our model consistently outperformed state-of-the-art methods in biomedical text summarisation. The results highlight the significant improvements achieved by leveraging knowledge from citation papers and the potential for our model to enhance the understanding of biomedical literature through natural language generation techniques.\\n\\nAcknowledgements\\n\\nChen Tang is supported by the China Scholarship Council (CSC) for his doctoral study (File No.202006120039). We also gratefully acknowledge the anonymous reviewers for their insightful comments.\\n\\nLimitations\\n\\nIn the field of text summarisation, two main approaches are commonly employed: extractive summarisation and abstractive summarisation. While extractive summarisation composes summaries by directly selecting sentences from the input content, abstractive summarisation generates summaries...\"}"}
{"id": "emnlp-2023-main-40", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that are not bound to the input content, providing greater flexibility but posing challenges in management and control. In this work, due to resource and time constraints, we focused on implementing an abstractive summarisation model and did not further conduct experiments to develop an extractive summarisation counterpart using our proposed algorithm. However, it is worth noting that our proposed approach has shown promising results, emphasizing the importance of leveraging citation papers to enhance the performance of language models in generating high-quality biomedical summaries. Theoretically, the aggregation of knowledge from citation papers can also be beneficial for extractive summarization approaches.\\n\\nEthics Statement\\n\\nOur new dataset is derived from an existing publicly available corpus released by the Allen Institute, which is a comprehensive biomedical literature corpus licensed under the Apache License 2.0. We have diligently adhered to the terms and conditions of the license and followed all provided instructions. Furthermore, we have familiarized ourselves with and acknowledged the ACM Code of Ethics and Professional Conduct. We approach our professional responsibilities with utmost seriousness, ensuring that our study upholds ethical principles in every aspect.\\n\\nReferences\\n\\nChenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2021. Enhancing scientific papers summarization with citation graph. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pages 12498\u201312506.\\n\\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.\\n\\nKhyathi Chandu, Aakanksha Naik, Aditya Chandrasekar, Zi Yang, Niloy Gupta, and Eric Nyberg. 2017. Tackling biomedical text summarization: OAQA at BioASQ 5B. In BioNLP 2017, pages 58\u201366, Vancouver, Canada. Association for Computational Linguistics.\\n\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A discourse-aware attention model for abstractive summarization of long documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615\u2013621, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nArman Cohan, Luca Soldaini, and Nazli Goharian. 2014. Towards citation-based summarization of biomedical literature. In Proceedings of the Text Analysis Conference (TAC'14).\\n\\nJay DeYoung, Iz Beltagy, Madeleine van Zuylen, Bailey Kuehl, and Lucy Lu Wang. 2021. MS\\\\(^2\\\\): Multi-document summarization of medical studies. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7494\u20137513, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nPaul Gigioli, Nikhita Sagar, Anand Rao, and Joseph Voyle. 2018. Domain-aware abstractive text summarization for medical documents. In 2018 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 2338\u20132343. IEEE.\\n\\nAzadeh Givchi, Reza Ramezani, and Ahmad Baraani-Dastjerdi. 2022. Graph-based abstractive biomedical text summarization. Journal of Biomedical Informatics, 132:104099.\\n\\nTomas Goldsack, Zheheng Luo, Qianqian Xie, Carolina Scarton, Matthew Shardlow, Sophia Ananiadou, and Chenghua Lin. 2023. Overview of the biolaysumm 2023 shared task on lay summarization of biomedical research articles. In The 22nd Workshop on Biomedical Natural Language Processing and BioNLP Shared Tasks, pages 468\u2013477.\\n\\nTomas Goldsack, Zhihao Zhang, Chenghua Lin, and Carolina Scarton. 2022. Making science simple: Corpora for the lay summarisation of scientific literature. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10589\u201310604, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nHenglin Huang, Chen Tang, Tyler Loakman, Frank Guerin, and Chenghua Lin. 2022. Improving Chinese story generation via awareness of syntactic dependencies and semantics. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 178\u2013185, Online only. Association for Computational Linguistics.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, and Veselin Stoyanov. 2020. Mawcc: Masked language modeling with cross-contrastive objectives. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1329\u20131339, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-40", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-40", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of table-based reasoning ability from llms. arXiv\\npreprint arXiv:2309.13182.\\nMichihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev. 2019. Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7386\u20137393.\\n\\nHongyi Yuan, Zheng Yuan, Ruyi Gan, Jiaxing Zhang, Yutao Xie, and Sheng Yu. 2022. Biobart: pretraining and evaluation of a biomedical generative language model. arXiv preprint arXiv:2204.03905.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263\u201327277.\\n\\nHongbo Zhang, Chen Tang, Tyler Loakman, Chenghua Lina, and Stefan Goetze. 2023. Cadge: Context-aware dialogue generation enhanced with graph-structured knowledge aggregation. arXiv preprint arXiv:2305.06294.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328\u201311339. PMLR.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nKun Zhao, Bohao Yang, Chenghua Lin, Wenge Rong, Aline Villavicencio, and Xiaohui Cui. 2023. Evaluating open-domain dialogues in latent space with next sentence prediction and mutual information. arXiv preprint arXiv:2305.16967.\\n\\nJie Zhou, Ganqu Cui, Shengding Hu, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, Lifeng Wang, Changcheng Li, and Maosong Sun. 2020. Graph neural networks: A review of methods and applications. AI open, 1:57\u201381.\\n\\nA Appendices\\nA.1 Implementation Details\\nChatGPT Prompts\\nThe performance of ChatGPT is highly reliable on the quality of input prompts. We manually design and test prompts of abstract summarisation, and select the best cases as the experimental results.\\n\\nOthers\\nThe Gaussian kernel smoothing used in Figure 4 is implemented with the gaussian_filter1d function from the python package of scipy.ndimage. The ROUGE score evaluation is implemented with the python package rouge_score. The readability scores such as Flesch-Kincaid (FLK) and Coleman-Liau Index (CLI), are implemented with the python package py-readability-metrics. BertScore is bert_score, and BartScore is from the GitHub repository of https://github.com/neulab/BARTScore.\\n\\nA.2 Automatic Evaluation\\nTable 6 shows the full results of BertScore and BartScore.\\n\\nA.3 Human Evaluation\\nIn addition to automatic evaluation metrics, we conducted a comprehensive human evaluation to assess the quality of biomedical summarization generated by the different models. The human evaluation aimed to capture important aspects of summarization, including fluency, readability, and relevance.\\n\\nFor the human evaluation, we recruited a group of expert annotators with a strong background in biomedical research. The annotators were provided with a set of summaries generated by each model and were asked to rate them on a Likert scale ranging from 1 to 5. The Likert scale allowed annotators to provide a subjective assessment of the summaries based on their expertise and judgment. The four aspects evaluated in the human evaluation were as follows:\\n\\nFluency: Annotators assessed the language quality and coherence of the summaries. They considered factors such as grammar, sentence structure, and overall fluency of the generated text. Higher ratings on the Likert scale indicated better fluency.\\n\\nReadability: Evaluators focused on the readability and comprehensibility of the summaries. They assessed whether the generated summaries were clear, concise, and understandable to a non-expert audience. Higher ratings indicated better readability.\\n\\nRelevance: An important criterion was the relevance of the summaries to the original input documents. Annotators evaluated whether the summaries captured the main ideas, key findings, and important concepts present in the source documents. Higher ratings indicated greater relevance.\\n\\nInformativeness: Evaluate the extent to which the generated summaries provide sufficient and meaningful information derived from the given input. Assess the comprehensiveness and completeness of table-based reasoning ability from llms.\"}"}
{"id": "emnlp-2023-main-40", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generate Summaries\\n\\nWrite a summary according to given paper content, which is part of a medical scientific paper (A). The length of generated summary is expected to be larger than 130 words and less than \\\\(\\\\text{MAX} \\\\cdot n\\\\) words.\\n\\nThe paper content of (A) is: \\\\(\\\\text{DOC}\\\\).\\n\\nOutput:\\n\\nGenerate the Human Evaluation Guideline\\n\\nWrite a detailed human evaluation guideline based on the following content: \\\\(\\\\{X\\\\}\\\\).\\n\\nTable 5: The examples of ChatGPT prompt templates. The variable in the brackets \\\\(\\\\{}\\\\) will be replaced by the actual text during the utilisation.\\n\\n| Model          | BerS-P | BerS-R | BartS-F1 | BartS |\\n|----------------|--------|--------|----------|-------|\\n| Golden         | 100    | 100    | 100      | -0.26 |\\n| ChatGPT        | 86.46  | 84.71  | 85.56    | -3.11 |\\n| Pubmed-LED     | 82.91  | 82.83  | 82.85    | -3.37 |\\n| Pubmed-PEGASUS | 82.55  | 81.43  | 81.97    | -3.42 |\\n| Pubmed-BART    | 84.26  | 82.49  | 83.35    | -3.38 |\\n| Pubmed-BART w one citation | 84.20 | 82.54 | 83.34 | -3.36 |\\n| Pubmed-BART w citation agg. | 84.33 | 82.92 | 83.60 | -3.35 |\\n\\nTable 6: BeS and BaS denote BERTScore and BartScore, respectively. P, R, F1 represents the precision, recall and F1 values, respectively.\\n\\nA.4 Future Works\\n\\nIn this paper, we propose a novel framework designed to enhance the performance of biomedical text summarisation using pre-trained language models. Recent years have witnessed the emergence of increasingly potent open-source language models, exemplified by Llama 2\\\\(^{17}\\\\) and Baichuan\\\\(^{18}\\\\).\\n\\nHowever, the practical implementation of our approach on these immensely large-scale models has been constrained by high computational demands. Consequently, we anticipate the need for more advanced GPU hardware or optimised models, such as distilled models (Yang et al., 2023b), to render the training of these models feasible. Presently, there are two primary ways for advancing our approach: (1) The development of a more efficient neural network that can effectively incorporate the graph-based features derived from citations (Tang et al., 2023a; Yang et al., 2023a). (2) The identification and extraction of key information from both the input document and its associated citations to enhance language understanding (Huang et al., 2022; Tang et al., 2022c). We defer the exploration of these directions to future research endeavors.\\n\\n---\\n\\n\\\\(^{17}\\\\) https://ai.meta.com/llama/\\n\\n\\\\(^{18}\\\\) https://github.com/baichuan-inc/Baichuan2\"}"}
