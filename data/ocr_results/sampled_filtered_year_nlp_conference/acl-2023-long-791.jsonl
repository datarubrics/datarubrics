{"id": "acl-2023-long-791", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe focus on the novel problem of persona based dialogue generation for comic strips. Dialogs in comic strips is a unique and unexplored area where every strip contains utterances from various characters with each one building upon the previous utterances and the associated visual scene. Previous works like DialogGPT, PersonaGPT and other dialog generation models encode two-party dialogues and do not account for the visual information. To the best of our knowledge we are the first to propose the paradigm of multimodal persona based dialogue generation. We contribute a novel dataset, COMSET, consisting of 54K strips, harvested from 13 popular comics available online. Further, we propose a multimodal persona-based architecture, MPDIALOG, to generate dialogues for the next panel in the strip which decreases the perplexity score by $\\\\sim 10$ points over strong dialogue generation baseline models. We demonstrate that there is still ample opportunity for improvement, highlighting the importance of building stronger dialogue systems that are able to generate persona-consistent dialogues and understand the context through various modalities.\\n\\n1 Introduction\\n\\nMultimodal conversational agents build dialog systems that engage with modalities beyond text, in constructing next responses. They open up a novel direction of text-vision multimodality, where the agent is part of the scene, rather than being a distant observer. This facilitates research and creation of support based multimodal agents. These agents could be critical for various applications such as assistants for visually impaired, conversations with robots in physical settings, instruction following by a digital agent that is manipulating images, clarification discussions during a presentation and so on. Such agents can help to promote literacy and language skills, as users engage with the generated dialogue to create their own stories. In all such cases, a natural conversational experience will be emulated better if visual or other modal elements get incorporated in the AI models.\\n\\nThere is substantial recent research in building neural conversational AI systems for text-only task-oriented dialogues (Eric et al., 2017; Madotto et al., 2018; Wu et al., 2018, 2021; Hosseini-Asl et al., 2020; He et al., 2022) as well as open domain conversations (Gao et al., 2020; Zhang et al., 2020; Santra et al., 2021; Shuster et al., 2022). On the other hand, research on multimodal conversation is still in its early stages. A key exception is Visual Dialog (Das et al., 2017), where an agent answers multi-turn questions about a single static image. However, to the best of our knowledge, there is little work that builds dialog systems with multiple evolving images.\\n\\nOur goal is to advance research in such multi-modal dialog systems. A particular domain that enables us to study this is that of comic books. In contrast with Visual Dialog, a comic strip has several images with temporal progression and an aligned dialog. Building an effective comic dialog would require understanding the visual context and the temporal progression of the strip. This is a challenging task that requires the integration of visual and textual information. We believe that comic strips provide a rich and interesting domain for studying multimodal dialogue systems.\"}"}
{"id": "acl-2023-long-791", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"system necessitates understanding the visual narrative, in addition to the textual context, making it a good testbed for multimodal dialog.\\n\\nIn addition to multimodality, comics have several other unique characteristics that make the domain challenging for AI systems. For instance, comic conversations are often multiparty, whereas most existing dialog agents assume dyadic (two party) conversations. Moreover, each character in a comic has a distinctive persona and style, and the dialog agent has to learn to follow the right style for each speaker. Finally, many comics are humorous, necessitating the model to be funny in its responses.\\n\\nTo study dialog systems in the comics domain, we first curate a novel dataset, COMSET, which consists of $\\\\sim 54$K strips from 13 comics. Each strip is associated with the visual panel (with text masked), along with the text transcript. We harvest strips from a publicly available online collection, GoComics. Panel and dialogue segmentation on the visual scene data in these strips leads to a dataset with 200+ characters. To describe the distinctive persona of each lead character, we also curate a set of persona facts (inspired by Zhang et al. (2018)) from popular fandom websites.\\n\\nWe define the novel task of next utterance generation for comics conditioned on the textual dialog history, visual scene history, and the persona facts for the comic characters. Fig. 1 shows an example. Since existing dialogue generation models do not handle multi-image multimodal context along with persona, we implement a novel method (MPDIALOG) for the task, as illustrated in Fig. 4. Text utterances, persona facts, and visual scenes are passed into the MultiModal Embedding (MME) module which encodes them into tokens each of $D=768$ dimensions. These embeddings are then passed on to a language decoder to produce the output tokens. MME module (i) computes the text encodings using a text embedding (TE) layer, (ii) computes visual token embeddings of panel images using CLIP Vision encoder (VE), linearly projects (LP) each embedding of size $D$ to $n \\\\times D$ and reshaping it to $n$ tokens each of size $D$, (iii) interleaves text and visual token embeddings. Interleaving occurs such that the dialogues of a panel are preceded by the respective panel embedding. Extensive comparisons show that MPDIALOG outperforms multiple text-only dialogue generation systems as well as those systems that do not use persona facts.\\n\\nOverall, we make the following main contributions in this work. (1) We contribute a novel multimodal comics dataset, COMSET, containing $\\\\sim 54$K strips and persona facts for 200+ characters. (2) We propose a multimodal persona-based dialog generation baseline, MPDIALOG, which incorporates both the modalities and generates the next utterances effectively. (3) We demonstrate empirically that multimodality and persona orientation leads to better dialogues. This paper adds interesting questions around multimodal persona-based dialogue generation modeling and we hope that our study motivates more work in this area. We make code and dataset publicly available.\\n\\n### Related Work\\n\\nOur work is related to the following three areas: dialogue generation, multimodal models, and multimodal datasets for dialogue generation.\\n\\n**Dialogue Generation**: Recently, several neural dialog generation models have been proposed (Gao et al., 2018; Ni et al., 2022); we discuss a few here. DialoGPT (Zhang et al., 2020) uses a GPT-2 (Radford et al., 2019) decoder pretrained on Reddit conversations and can effectively capture the contextual information in dialogues, thereby generating interesting and human-like responses. However, DialoGPT does not allow explicit style control over the generated responses. EDGE (Gupta et al., 2021) allows for controlled response generation by conditioning on semantic frames of exemplar responses. A particular kind of style control models are persona-based models which use \u201cpersona\u201d information for personalized dialog generation. Bert-over-Bert (Song et al., 2021) disentangles persona-based dialogue generation into two tasks: dialogue generation and consistency understanding; the model uses a shared BERT encoder but has two task-specific decoders. PersonaGPT (Tang et al., 2021) uses GPT-2 finetuned on PersonaChat (Zhang et al., 2018) dataset, with added persona fact tokens for personalized generation and question control codes for controlled generation. None of these models capture the multimodal multi-party context which is the setting for comic dialogues.\\n\\n**Multimodal Datasets for Dialogue Generation**: The COMICS (Iyyer et al., 2017) dataset contains scanned images of comic strips but it does not contain manually extracted transcript information or...\"}"}
{"id": "acl-2023-long-791", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Information about comic characters. Further, as the authors mention, the dataset is unsuitable for generation tasks due to OCR detection inaccuracies. PersonaChat (Zhang et al., 2018) has conversations between two agents and their corresponding persona facts but it has no images. Other multimodal datasets include ImageChat (Shuster et al., 2020), PhotoChat (Zang et al., 2021) and VisualDialog (Das et al., 2017) which have a conversation between speakers about a single reference image. They differ from our setting, where the speakers are themselves a part of the image, and we have multiple panels (images).\\n\\nMultimodal Models: Recently, several types of multimodal encoders and generators have been proposed for a variety of applications. Models like CLIP (Radford et al., 2021) and ALIGN (Jia et al., 2021) are based on alignment of visual and textual embedding spaces. Frozen (Tsimpoukelli et al., 2021) and ClipCap (Mokady et al., 2021) also align text and visual embedding by projecting visual embeddings onto the textual embedding space. Text-image cross attention is used in VisualGPT (Chen et al., 2021), VC-GPT (Luo et al., 2022), CoCa (Yu et al., 2022). Perceiver-IO (Jaegle et al., 2021) is a fully attentional read-process-write architecture with variants like Uni-Perciever-MOE (Zhu et al., 2022) which use Mixture of Experts for response selection. In SimVLM (Wang et al., 2021) and VisualBERT (Li et al., 2019) the Visual and Textual Models are jointly trained on the task itself. Given its significant zero-shot image classification capabilities, we use CLIP as the image encoder for our MPDIALOG architecture.\\n\\nThe CCOMSET Dataset\\nWe contribute a novel comics dataset, CCOMSET, containing 13 popular English comic strips, obtained from GoComics. Each comic strip contains transcription and an image. We remove duplicate strips (re-broadcasts with minor modifications) based on Levenshtein distance between transcripts. For each comic, we also obtained persona facts (representative personality traits) for each character by manually curating such information from websites like Fandom, Wikipedia, and TV Tropes, and paraphrasing all collected persona facts into first person English sentences. We describe data pre-processing and analysis in this section.\\n\\n3.1 Dataset Pre-processing\\nThe raw dataset was pre-processed as follows. Parsing Transcripts: Parsing transcripts involves parsing speaker (character) and utterance pairs from unstructured conversation transcripts. We first obtained a list of comic characters (for our 13 comics) from the same websites that were used to gather character personas. We also added character aliases to this list. Further, we mined frequent proper nouns with PERSON entity tag from all transcripts to search for all potential speaker candidates. We reduced infrequent characters into a catch-all character OTHER. Around 17% utterances in our corpus are attributed to OTHER. Further, there were some frequent speakers which were not named entities, for example, Man, Woman, Stranger, Voice, Noise, Sound. We conflated Voice, Noise, Sound into a single speaker (Voice) and added all such characters to list of characters. Finally, for all comics except Doonesbury and Cleats, we used list lookup for extraction of mention spans for character named entities. Using basic heuristics like word followed by colon or quotation characters, we could also do a fuzzy character name match to handle spelling errors in transcripts.\\n\\nTranscripts for Doonesbury and Cleats contain free-form text like Bucky is holding Smacky and says ... Typically each sentence contains four parts: character/speaker name (Bucky), action or attribute phrase (is holding Smacky and), speaking verb (says, replies, asks, proclaims, etc.), and utterance. To obtain these parts from transcripts, we first perform part-of-speech tagging, named entity recognition, and dependency parsing using spaCy (Honnibal et al., 2020). Then we use heuristics like (a) speaker name should have the POS tag PROPN, must be the nominal subject (nsubj) and have the NER tag as PERSON, (b) The speaker should have a direct/indirect relation to the speaking verb.\\n\\nPanel Segmentation: Each strip image had several panels and utterances across panels. Classical vision methods like Hough Transform (Duda and Hart, 1972), polygon detection (Li et al., 2014), recursive cuts (Pang et al., 2014) and density gradients (Tanaka et al., 2007) led to poor panel segmentation due to their assumptions about uniform white background and clean gutters. Inspired\\n\\n6https://archiewahwah.wordpress.com/speech-verbs-list/\"}"}
{"id": "acl-2023-long-791", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Few examples of Panel Segmentation by Iyyer et al. (2017), we model the panel segmentation as an object detection problem. We used the 500 manually annotated panel bounding boxes out of comic strips provided by them to train a Faster-RCNN (Ren et al., 2015) architecture with a ResNet-50 (He et al., 2016) backbone, and used it to segment panels from our comic strips. Some segmentation results are shown in Fig. 2.\\n\\nDialogue Text Detection and Masking: While predicting the next utterance for a character in the current panel, the ground truth utterance in the panel image could lead to a label leak. Hence, to eliminate redundancies and to avoid possibilities of label leak, we mask the utterance text from panel images. Iyyer et al. (2017) detect utterance text on images by training a Faster-RCNN model on 1500 manually annotated panels to detect text boxes. This approach led to poor results for our dataset since text box structure is not consistent across comics, and often there is no explicit text box or bubble to encapsulate the dialogue, also evident from Figs. 2 and 3. Hence, we used off-the-shelf OCR, specifically EasyOCR, to extract the text and bounding boxes from each segmented panel. We filled bounding boxes with random noise so as to not bias the model towards any color at utterance positions, as shown in Fig. 3.\\n\\nMultimodal Alignment: For each comic strip $c$, panel segmentation yields a sequence of $n_c$ panel images along with OCR text $\\\\{P_j\\\\}_{j=1}^{n_c}$ for each panel $j$, and transcript parsing yields a sequence of $m_c$ utterances $\\\\{D_i\\\\}_{i=1}^{m_c}$ along with speaker labels. For next utterance prediction, the model needs both text and visual context aligned with each other. For each $(D_i, P_j)$ pair, we calculate a string fuzzy Levenshtein distance-based similarity score $S_{ij}$ which determines the extent to which $D_i$ matches with text $P_j$. The panel index for the $i$th utterance is then calculated as $\\\\sigma_i = \\\\arg \\\\max_j S_{ij}$. The matched panel sequence can be written as $\\\\Sigma = \\\\{\\\\sigma_1, \\\\sigma_2, \\\\ldots , \\\\sigma_{m_c}\\\\}$. Due to inaccurate OCR, $\\\\Sigma$ may not be monotonically increasing. We handle this inconsistency by transforming $\\\\Sigma$ to a sorted sequence $\\\\Sigma = DP(\\\\Sigma)$ where $DP$ is a dynamic programming method to sort an input sequence with minimum number of edits. We found that the DP filter was needed for only 2% of all the utterances.\\n\\n3.2 Dataset Statistics and Quality: Across 13 comics, COMSET contains 53,903 strips covering a total of 159,610 panels and 238,484 utterances. Thus, there are 2.96 images per strip. On average, a dialogue contains 16.09 tokens. Each strip has 2.98 characters on average. The dataset contains 6.66 persona facts per character on average across 202 characters. Each persona fact contains 12.23 tokens on average. Table 1 shows key statistics for COMSET. Table 2 shows distribution of number of strips, panels, utterances and characters across the 13 comics. We split the 13 comics into a seen set of 8 comics and unseen set of 5 comics. Seen set was further split randomly 70:10:20 into train:val:test stratified by comic name.\\n\\nWe manually inspected our dataset quality using 50 randomly chosen examples. We found that our scripts for parsing speaker from transcripts had an accuracy of $\\\\sim 98\\\\%$. Some comics had bad transcripts, and speaker information was completely missing ($<1\\\\%$). In $\\\\sim 2\\\\%$ of utterances, there were some parts of the speaker overflowing into the previous utterance due to whitespace in speaker names.\"}"}
{"id": "acl-2023-long-791", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Key statistics of the proposed COMSET dataset.\\n\\n| Metric                          | Value          |\\n|--------------------------------|----------------|\\n| Avg Unique characters (per strip) | 2.98           |\\n| Avg dialogue length (tokens)   | 16.09          |\\n| Avg persona facts (per comic)  | 57.39          |\\n| Avg persona facts (per character) | 6.66           |\\n| Avg persona fact length (tokens) | 12.23          |\\n| Image per Utterance            | 0.671          |\\n| Image per dialog               | 2.96           |\\n\\nNumber of Strips/dialogs: 53,903\\nNumber of Panels/Images: 159,610\\nNumber of Utterances: 238,484\\nNumber of Characters(Personas): 202\\n\\nTable 2: Comic wise distribution of dataset statistics.\\n\\n| Comic                | # strips | # panels | # utterances | # characters |\\n|----------------------|----------|----------|--------------|--------------|\\n| cleats               | 2588     | 5580     | 10064        | 33           |\\n| bigtop               | 1752     | 5457     | 8977         | 11           |\\n| heartofthecity       | 6544     | 14117    | 23499        | 14           |\\n| garfield             | 10295    | 24578    | 27731        | 21           |\\n| peanuts              | 2623     | 10612    | 7069         | 15           |\\n| riphaywire           | 2730     | 7815     | 14638        | 18           |\\n| bignate              | 5446     | 21339    | 32380        | 11           |\\n| inkpen               | 2205     | 6722     | 9736         | 12           |\\n| getfuzzy             | 2383     | 6630     | 12080        | 11           |\\n| familytree           | 362      | 1119     | 2112         | 11           |\\n| calvinandhobbes      | 2557     | 8120     | 10120        | 11           |\\n| doonesbury           | 13821    | 45401    | 77329        | 21           |\\n| cathy                | 597      | 2120     | 2749         | 13           |\\n| Total                | 53903    | 159610   | 238484       | 202          |\\n\\n4 Methodology\\n\\nIn this section we formalize the next utterance prediction task in the multi-modal persona-based dialogue setting for benchmarking COMSET and propose a novel baseline architecture MPDIALOG.\\n\\n4.1 Next Utterance Prediction Task\\n\\nFor a comic strip, consider a conversation history with utterances \\\\( \\\\{C_i\\\\}_{i=1}^n \\\\) and an aligned sequence of images \\\\( \\\\{I_j\\\\}_{m=1}^j \\\\). At any time step \\\\( t \\\\), the objective is to generate \\\\( C_t \\\\) given the textual conversation history \\\\( \\\\{C_i\\\\}_{t-1}^i \\\\) and the corresponding image history sequence \\\\( \\\\{I_j\\\\}_{k=1}^j \\\\) where \\\\( C_t \\\\) is aligned with \\\\( I_k \\\\), \\\\( t \\\\leq n \\\\), and \\\\( k \\\\leq m \\\\).\\n\\nIn practice, it may be useful to limit historical context to a history size \\\\( h \\\\) of past utterances and their corresponding panel images. While this problem formulation is generally applicable to any setting with multimodal conversation history, we propose a model for next utterance prediction for comics in this work.\\n\\n4.2 Baseline Methods\\n\\nWe first describe our adaptation to the existing language model (LM) only methods, as well as LM+persona based methods.\\n\\nLM only: LM only methods use only the text part of the conversations. We experiment with DialoGPT (Zhang et al., 2020) and EDGE (Gupta et al., 2021). DialoGPT is trained on a 147M multi-turn dialogue dataset from Reddit, and conditions response generation on the previous conversation context. EDGE (Gupta et al., 2021) allows controlling dialogue response generation based on the semantic structure of exemplar responses. During inference, EDGE retrieves the exemplar responses of the test set context with train set dialogues as the candidate set using a ParlAI Polyencoder (Humeau et al., 2019) model pretrained on the ConvAI2 dataset. EDGE then uses the opensesame (Swayamdipta et al., 2017) frame extraction model, which is a frame-semantic parser for automatically detecting FrameNet (Baker et al., 1998) frames and their frame-elements from sentences.\\n\\nWe adapt these models to COMSET by extracting the conversation history \\\\( C_{t'}:_{t-1} \\\\) and finetune the model to predict \\\\( C_t \\\\), where \\\\( t' = \\\\max(0, t - h) \\\\). We set the maximum history size \\\\( h = 5 \\\\).\\n\\nLM+Persona: These baselines utilize the conversation context along with persona facts for each character to generate persona consistent responses. Models evaluated include PersonaGPT (Tang et al., 2021) and BoB (Song et al., 2021). These models assume a dyadic conversation and require persona facts of both the speakers as input to generate responses. PersonaGPT is finetuned on the PersonaChat (Zhang et al., 2018) dataset, with added spe-\\n\\n8https://parl.ai/projects/polyencoder\"}"}
{"id": "acl-2023-long-791", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tyr: So you won't go out with me?\\n\\nJenn Erica: Don't you have a wife back in Asgard?\\n\\nTyr: AHHH, that don't mean nothing! It's just a marriage of convenience.\\n\\nJenn Erica: Your wife is on a completely different plane of existence! What's so convenient about that?\"}"}
{"id": "acl-2023-long-791", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We design extensive experiments to answer the following questions: (1) Can existing dialogue generation language models adapt their knowledge to the comic setting? (2) To what extent does persona orientation of language models help in generating comics? (3) Does adding multimodality help the language model in better understanding the context and thereby generating coherent responses? (4) How does the generalizability to unseen comics (zero-shot setting) vary across architectures. To answer these questions we finetune each of the baseline language-only models and those with persona alignment on only the textual component and later train MPDIALOG on the multimodal dataset.\\n\\nWe generate response for each of the trained models using nucleus sampling. This was done for both the seen (finetuned) and unseen (zero-shot) splits of our dataset. The results for these experiments are shown in Table 3.\\n\\nPerformance on Seen Dataset:\\nWe observe that the proposed model, MPDIALOG, outperforms both the language model only as well as persona-based baselines. Language only models (like DialoGPT and EDGE) cannot generate coherent responses (high perplexity and low MaUde) in the comic setting. This is expected as it is very hard to understand the context of a comic without any information about the characters or the visual scene. We observe that adding persona information of the characters significantly boosts performance as is evident from the perplexity scores, BLEURT and MaUde, of PersonaGPT-base. We conducted a Welch-T (Welch, 1947) test on results of MPDialog with other baselines for precision, recall, F1, MaUde, BLEURT and we got max (p) < 0.025 indicating statistical significance. Persona information delivers meaningful insights into the context and helps the model in understanding the conversation better. Moreover, adding visual scene information along with the persona also boosts performance as the model has now access to the actual scene of the comic in which the conversation is happening. As an ablation we also trained a model with language and visual components but without the persona information. Its perplexity came out to be 23.76 on the seen set, which is better than just the language part (DialoGPT) but worse than MPDialog (which also incorporates persona). For illustration, we show a cherry-picked example in Fig. 5 where PersonaGPT-base and MPDialog are able to align their responses with the character persona, whereas responses for other language only models are either too banal (EDGE and BoB) or completely nonsensical (DialoGPT).\\n\\nPerformance on Unseen Dataset:\\nWe also show results on unseen comics set, for various models in Table 3. Again, MPDIALOG outperforms baselines across most metrics. Perplexity scores of language only models (both DialoGPT and EDGE) are quite high. Adding persona information and evaluating PersonaGPT we find that it significantly boosts the models' ability to generate responses with more certainty, as the perplexity score drops by a factor of \u223c12 points. It is worthwhile to note that PersonaGPT performs much better on neural metrics like BertScore and MauDe as compared to language only baselines. Our multimodal model (MPDIALOG) further improves these neural metrics and shows that it is able to produce much more coherent responses by taking into account the associated visual information. Thus the trend (LM + persona + images > LM + persona > LM) can still be observed as in the \u201cseen comics\u201d setting.\"}"}
{"id": "acl-2023-long-791", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Persona Facts (Character: Arlene, Comic: Garfield)\\n\\n\u2022 I am Garfield's on-and-off girlfriend\\n\u2022 I can unsuccessfully crack jokes at the expense of Garfield on a regular basis.\\n\u2022 I love to make smart remarks about Garfield, most likely due to his enormous ego.\\n\u2022 I am often interested in a deeper relationship with Garfield\\n\\nContext:\\n\\nI adore you, I worship you! I love you more than life itself. I can't live without you.\\n\\nGold Response:\\n\\nUh.. Garfield?\\n\\nMPDialog (our work):\\n\\nYou're not alone...you've been there for years and your whole life has just come full circle around me!! And now that we have each other again..we'll be together foreverrrr!!! We will rule this world!!\\n\\nPersonaGPT (LM + persona):\\n\\nYou're not alone??!!...you know what this is like?? It must be so much better when we get back together and have some sorta conversation over it all....but that doesn't amaze me too hard for no reason.\\n\\nEDGE (LM + frames):\\n\\nI can't live without you\\n\\nBoB (LM + persona):\\n\\nI'm going to have to eat.\\n\\nDialoGPT (LM only):\\n\\nI'm sobsomn'ing to the world is a great place, and it's not just for me?!??!!! what do we have in common with each other??? oh my god....what are they doing ' 'out here again\\n\\n5.4 Comic-wise Quantitative Analysis\\n\\nTable 4 shows comic-level BLEURT scores for both the seen as well as unseen test sets. We also show MaUde and perplexity scores in Tables 5 and 6 respectively. For most comics across all the three metrics, MPDIALOG performs better than other models. Unlike most comic strips, Cleats comic focuses on the relationships between the characters, their sportsmanship and the challenges of being part of a team. We believe that images in Cleats do not contain much additional information and hence multi-modality of MPDIALOG does not lead to improved results.\\n\\n5.5 Qualitative Analysis\\n\\nThe proposed method, MPDIALOG, is persona-based. How well does it capture the persona style in the generations, compared to other persona-based baselines? To answer this question, we perform the following experiment. For every character $c$ in the train set, we obtain its unigram vocabulary distribution $Train_c$. Given a model, over the entire test set, we also compute unigram vocabulary distribution $Outputs_c$ from combined text of all generations for character $c$. If the model has captured persona for character $c$ well, the symmetric KL-divergence between $Train_c$ and $Outputs_c$ should be small. Hence, we compare MPDIALOG with other persona-based baseline models (PersonaGPT and BoB) using the symmetric KL divergence metric. We observe that symmetric KL divergence is 4.41, 4.56 and 3.36 for PersonaGPT, BoB and MPDIALOG respectively. Thus, we infer that MPDIALOG is the best at capturing the persona information.\\n\\nWe also attempt to understand the image patch attribution for a generated dialogue by our model as applied on Fig. 5. We conducted a Grad-CAM (Selvaraju et al., 2017) analysis to check where the model \\\"looks\\\" while generating its utterances. Since generation is stochastic and dependent on nucleus sampling, we cannot attribute the model's output to a particular attention map over the image. As a surrogate, we calculate the attention map over the visual panels when the model generates the last \\\\([eot] \\\\) token. In Fig. 5, we were able to observe that the model does indeed look at Arlene's face and Garfield's face (as indicated in Fig. 6) and gives less relative importance to the background and the bubble above it. It helped us confirm that our model is able to contextualize within the images as well and generates tokens...\"}"}
{"id": "acl-2023-long-791", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.6 Human Evaluation Results\\n\\nWe obtain manual annotations for the utterances generated by various models on fluency, engagingness, dialog-consistency, scene-consistency and persona-detection. Four annotators performed judgments on a set of 65 examples, randomly sampled from the test set. We compute inter-annotator agreement as pairwise correlation between method rankings, averaged across the five criteria. It was found to be 0.318 (Kendall\u2019s Tau \u2018B\u2019) which is considered as strong agreement. Detailed annotation guidelines are mentioned in the appendix. Specifically, we measure persona detection as follows. Given persona facts of two characters, and a response, the annotator is asked to guess which of the two personas the response matches to. Table 7 shows that MPDIALOG performs best on all measures except for dialog consistency where EDGE performs the best. EDGE uses semantic frame exemplars to guide a structure for the utterance leading to better consistency. All the other models do not make use of this extra structural input, and amongst them, MPDIALOG performs best. On persona detection, MPDIALOG performs comparably to PersonaGPT. Overall, MPDIALOG performs quite well on human perceived quality of generated comic dialogues.\\n\\nAs an additional qualitative analysis for the proposed model, we performed the following experiment. We considered examples, where in the multimodal input context, we changed the last character prompt to some other character from the same or other comic. The goal was to check how another character (say \u201cTyr\u201d) would respond in a situation in a comic (say \u201cGarfield\u201d). We found that the generated responses often reflect the persona of the injected character. For example, in Garfield, we found for the same situation: (1) Mom\u2019s response showing her down-to-earth, exasperated and sensitive nature who loves her son dearly, and (2) Susie\u2019s response to be teasing Calvin, thereby showing her love-hate relationship with Calvin. Thus, our model seems to be capturing the persona behavior somewhat, but we feel there is much more work to be done to generate responses that are contextually more coherent, and at the level of human skill.\\n\\n6 Conclusions and Future Work\\n\\nWe propose a novel problem of next utterance prediction for comics given historical multimodal context consisting of previous utterances and panel images. We contribute a novel dataset, COMSET, which contains 53,903 strips, 159,610 panels and 238,484 utterances from 13 comics. We also propose a multimodal persona-based baseline model, MPDIALOG, which performs better compared to strong language-only and persona-based dialogue generation models, both in the seen comic and the unseen comic settings. We make our code and dataset publicly available. In the future we plan to (1) focus on generation of humor-focused text, and (2) explore generation of next utterances and panel images together.\\n\\nAcknowledgements\\n\\nThis work is supported by grants by Google, Verisk, and 1MG, an IBM SUR award, and the Jai Gupta chair fellowship by IIT Delhi. We also acknowledge travel support from Google and Yardi School of AI travel grants. We thank the IIT Delhi HPC facility for its computational resources. We also thank Rocktim Jyoti Das for his help with the code for MPDialog.\\n\\nLimitations\\n\\nIn this paper, we focused on English comics only because of their ease of availability. Although we have not experimented with non-English text, we\"}"}
{"id": "acl-2023-long-791", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"expect the proposed model to work well in multi-lingual settings if we replace GPT-2 decoder with other decoders like BLOOM (Scao et al., 2022).\\n\\nEthics Statement\\n\\nMost of our dataset has been obtained from GoComics (https://gocomics.com/). The website allows downloads of comic images for research purposes. However, they do not allow redistribution of images. Hence, in our dataset release, we have only provided links to images on GoComics website.\\n\\nProviding links to images or webpages is a common trend (e.g., Google Landmarks, GoogleConceptualCaptions, WIT datasets). That said, our code base provides all the scripts needed to (1) do pre-processing and modeling based on this images (2) gather transcripts and align with the panels in comic strips. Thus, overall, all steps in the paper are reproducible. Further, we have also provided character identification annotations that we perform on these images as part of the dataset.\\n\\nNatural language generation is in general prone to issues like biased, offensive, harmful, misinformative text generation. Fortunately, in this work, we finetune our models using relatively clean comics dataset. Also, given that these generations are meant to be consumed in a humorous form, we do not foresee the bias (if at all) generated by our model to be hurtful. To the extent we browsed over the generations produced by our model, we did not observe any biased, offensive, harmful, misinformative text getting generated.\\n\\nReferences\\n\\nCollin F Baker, Charles J Fillmore, and John B Lowe. 1998. The berkeley framenet project. In COLING 1998 Volume 1: The 17th International Conference on Computational Linguistics.\\n\\nJun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed Elhoseiny. 2021. Visualgpt: Data-efficient image captioning by balancing visual input and linguistic knowledge from pretraining. arXiv preprint arXiv:2102.10407.\\n\\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 326\u2013335.\\n\\nRichard O Duda and Peter E Hart. 1972. Use of the hough transformation to detect lines and curves in pictures. Communications of the ACM, 15(1):11\u201315.\\n\\nMihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D Manning. 2017. Key-value retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue, pages 37\u201349.\\n\\nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational ai. In The 41st international ACM SIGIR conference on research & development in information retrieval, pages 1371\u20131374.\\n\\nXiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, and William B Dolan. 2020. Dialogue response ranking training with large-scale human feedback data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 386\u2013395.\\n\\nPrakhar Gupta, Jeffrey P Bigham, Yulia Tsvetkov, and Amy Pavel. 2021. Controlling dialogue generation with semantic exemplars. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3018\u20133029.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778.\\n\\nWanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, et al. 2022. Galaxy: A generative pre-trained model for task-oriented dialog with semi-supervised learning and explicit policy injection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 10749\u201310757.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Langedem, and Adriane Boyd. 2020. spacy: Industrial-strength natural language processing in python. To appear.\\n\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. Advances in Neural Information Processing Systems, 33:20179\u201320191.\\n\\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux, and Jason Weston. 2019. Poly-encoders: Architectures and pre-training strategies for fast and accurate multi-sentence scoring. In International Conference on Learning Representations.\\n\\nMohit Iyyer, Varun Manjunatha, Anupam Guha, Yogarshi Vyas, Jordan Boyd-Graber, Hal Daume, and Larry S Davis. 2017. The amazing mysteries of the gutter: Drawing inferences between panels in comic strips.\"}"}
{"id": "acl-2023-long-791", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"book narratives. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 7186\u20137195.\\n\\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. 2021. Perceiver io: A general architecture for structured inputs & outputs. In International Conference on Learning Representations.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR.\\n\\nNitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher. 2019. Ctrl: A conditional transformer language model for controllable generation. arXiv preprint arXiv:1909.05858.\\n\\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh, and Kai-Wei Chang. 2019. Visualbert: A simple and performant baseline for vision and language. arXiv preprint arXiv:1908.03557.\\n\\nLuyuan Li, Yongtao Wang, Zhi Tang, and Liangcai Gao. 2014. Automatic comic page segmentation based on polygon detection. Multimedia Tools and Applications, 69(1):171\u2013197.\\n\\nZiyang Luo, Yadong Xi, Rongsheng Zhang, and Jing Ma. 2022. Vc-gpt: Visual conditioned gpt for end-to-end generative vision-and-language pre-training. arXiv preprint arXiv:2201.12723.\\n\\nAndrea Madotto, Chien-sheng Wu, and Pascale Ngan Fung. 2018. Mem2seq: Effectively incorporating knowledge bases into end-to-end task-oriented dialog systems. In ACL 2018-56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers), page 1468.\\n\\nRon Mokady, Amir Hertz, and Amit H Bermano. 2021. Clipcap: Clip prefix for image captioning. arXiv preprint arXiv:2111.09734.\\n\\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial intelligence review, pages 1\u2013101.\\n\\nXufang Pang, Ying Cao, Rynson WH Lau, and Antoni B Chan. 2014. A robust panel extraction method for manga. In Proceedings of the 22nd ACM international conference on Multimedia, pages 1125\u20131128.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sathyam, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models are unsupervised multitask learners. https://github.com/openai/gpt-2.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. 2015. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28.\\n\\nBishal Santra, Sumegh Roychowdhury, Aishik Mandal, Vasu Gurram, Atharva Naik, Manish Gupta, and Pawan Goyal. 2021. Representation learning for conversational data using discourse mutual information maximization. arXiv preprint arXiv:2112.05787.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Elodie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\\n\\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. Bleurt: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892.\\n\\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. 2017. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626.\\n\\nKurt Shuster, Samuel Humeau, Antoine Bordes, and Jamie Weston. 2020. Image-chat: Engaging grounded conversations. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2414\u20132429.\\n\\nKurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller, Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al. 2022. Blenderbot 3: a deployed conversational agent that continually learns to responsibly engage. arXiv preprint arXiv:2208.03188.\\n\\nKoustuv Sinha, Prasanna Parthasarathi, Jasmine Wang, Ryan Lowe, William L Hamilton, and Joelle Pineau. 2020. Learning an unreferenced metric for online dialogue evaluation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2430\u20132441.\\n\\nHaoyu Song, Yan Wang, Kaiyan Zhang, Weinan Zhang, and Ting Liu. 2021. Bob: Bert over bert for training persona-based dialogue models from limited personalized data. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 167\u2013177.\"}"}
{"id": "acl-2023-long-791", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our method produced errors where the demarcation between frames was not very clear as shown in a few examples in Fig. 7.\\n\\nFigure 7: Panel Segmentation Error Analysis: The erroneous segmentations are colored separately from red.\\n\\nAnnotation details\\n\\nHuman annotations were done by four undergraduate Computer Science students (3 male, 1 female) with an interest in comics in the age group 21-22 years. They were paid as per the rules of our institute for the task. The annotators were informed that this data will be used for research on dialogue generation for comics.\\n\\nThe following guidelines were provided to the annotators for evaluation.\\n\\n1. Panel Segmentation Errors\\n   - Our method produced errors where the demarcation between frames was not very clear as shown in a few examples in Fig. 7.\\n\\n2. Annotation details\\n   - Human annotations were done by four undergraduate Computer Science students (3 male, 1 female) with an interest in comics in the age group 21-22 years. They were paid as per the rules of our institute for the task. The annotators were informed that this data will be used for research on dialogue generation for comics.\\n\\n   The following guidelines were provided to the annotators for evaluation.\"}"}
{"id": "acl-2023-long-791", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Fluency: How fluent is the response on its own? (1-5), where 1 is \u201cnot fluent at all\u201d, 5 is \u201cextremely fluent\u201d. Fluency encompasses how easy to understand the response is.\\n\\n\u2022 Engagingness: How much engaging is the response on its own? (1-5), where 1 is \u201cnot engaging at all\u201d or \u201cgeneric\u201d, 5 is \u201cextremely engaging\u201d or \u201cunique\u201d. Engagingness is defined as how interesting and unique the response is. Repetition and generic responses are scored low and highly detailed and attention grabbing responses are scored high.\\n\\n\u2022 Dialog Consistency: How consistent is the response to the dialogue history? (1-5) 1 is \u201ctotally unrelated\u201d and 5 is \u201cFully consistent\u201d.\\n\\n\u2022 Scene Consistency: How much consistent is the response to the image history? (1-5) 1 is \u201ctotally unrelated\u201d and 5 is \u201cFully consistent\u201d and 3 is \u201cOK\u201d.\\n\\n\u2022 Persona Detection: Given persona facts of two characters, which persona does the response match to?\"}"}
{"id": "acl-2023-long-791", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/face anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-791", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? There is no personally identifiable information in the dataset. Hence, no specific ethics review was needed.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
