{"id": "emnlp-2022-main-594", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 7: Steps 1 and 2 of the Agent view of the annotation HIT. The Patient view for these steps is similar except for the title of the HIT.\\n\\nFigure 8: Steps 3a & 3b of the Agent view of the annotation HIT.\\n\\nFigure 9: Steps 3a & 3b of the Patient view of the annotation HIT.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Step 3c of the Agent view of the annotation HIT.\\nFigure 11: Step 3c of the Patient view of the annotation HIT.\\nFigure 12: Step 4 of the Agent view of the annotation HIT. The Patient view for this step is similar except for the title of the HIT.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-594", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: HIT for Validating Factors\\n\\nIn task 2, when evaluating factors, we check whether each factor contains story related information. For task 3, we check whether the summary mentions a change that is consistent with the details in the story. A blank summary is valid if the story does not contain any changes.\\n\\n3) Salience (Tasks 2 and 3):\\nWhether the summary is a valid ending for the situation described in the story for task 2. In task 2, when evaluating factors, we check whether each factor is necessary for understanding the situation. For task 3, we check whether the summary mentions a change that is consistent with the details in the story. When a story does not contain any changes, the annotation of no-changes is considered salient.\\n\\nE Model Training\\nTo perform early development experiments, we used 5-fold cross validation for 2 epochs. We found that while the evaluation loss plateaued after training for an epoch, recall improved with further training for another epoch. Classification results are the average of the 5-fold cross validation after 2 epochs for T5 and 1 epoch for BART for the single label classification in Task 5. The Multi-label binary classification for Task 4 was trained for 5 epochs for the BART models, 10 epochs for the T5 Encoder Only and the Encoder-Decoder model.\\n\\nFor the summarization tasks, we trained models on the entire training set without subsequent hyperparameter tuning for 2 epochs for all tasks.\\n\\nF Expanded Results\\nIn this section we present expanded results from \u00a76. In addition to average number of tokens in a summary or factor ($\\\\text{Len}$) and percentage of extractive trigrams from the story text ($\\\\text{Ext}$), ROUGE-L (longest word sequences) and BertScore that were reported in the paper, we use the ROUGE scores based on unigrams (ROUGE-1) and bigrams (ROUGE-2), corpus and sentence level BLEU, and METEOR. Unlike the lexical- and ontological-based metrics of ROUGE, BLEU and METEOR, BertScore aims to provide a modern, embedding-based approach for handling semantic equivalence/similarity even when the texts being compared have different surface forms (e.g., different words are used). These automated scores are calculated using all the test data whereas the human evaluation scores are for the 50 randomly selected data items for evaluation. The automated scores of ROUGE, BLEU, METEOR and BertScore are not reported for the reference summaries, as these summaries were used as gold references when calculating automated scores for the baseline and model generations. For the human evaluation we report the IAA by crowd workers. The difference in IAA for the crowd + expert scores and the crowd scores was < 0.1. The metrics reported on the left of the double line are calculated for all the test data. The best value for each score category are bolded and those that are significantly higher than all other values are marked with a *.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-594", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11: Generating Process Summaries for stories and factors\\n\\n| Model   | Rouge | MET- | BLEU | Bert | Factuality | Salience |\\n|---------|-------|------|------|------|------------|----------|\\n|         |      |      |      |      |            |          |\\n| Reference | 3.6  | .13* | -    | 3.57 | 72         |          |\\n| Last sent | 23.3 | .79  | 24.08| 12.33| 21.82      | 24.62    |\\n|          |      |      |      |      |            |          |\\n| Story   |      |      |      |      |            |          |\\n| Bart-base | 11   | .72  | 27.83| 13.05| 25.43      | 22.88    |\\n|          |      |      |      |      |            |          |\\n| Bart-large | 10.3 | .63  | 27.00| 12.51| 24.74      | 21.44    |\\n|          |      |      |      |      |            |          |\\n| T5-base |      |      |      | 13.6 | .70        | 26.66    |\\n|          |      |      |      |      |            |          |\\n| T5-large | 12.9 | .67  | 28.36| 13.15| 25.71      | 24.31    |\\n|          |      |      |      |      |            |          |\\n| Fact.   |      |      |      |      |            |          |\\n| Bart-base | 7.3  | .49  | 26.26| 10.86| 24.09      | 18.28    |\\n|          |      |      |      |      |            |          |\\n| T5-base |      |      |      | 10.4 | .47        | 24.74    |\\n\\n* indicates the Reference value for Abstractness is significantly higher than other values in the column with a p value between 0.001 - 0.0001 (except for Bart-base trained on Factors where the p value is 0.13).\\n\\nThe column AvgLS is the average of 3 crowd worker 1-5 Likert scores, and the column % Abs is percentage of instances with a score $\\\\geq 3$.\\n\\n### Table 12: Generating Endpoints for stories and factors (Task 2a)\\n\\n| Model   | Rouge | MET- | BLEU | Bert | Factuality | Salience |\\n|---------|-------|------|------|------|------------|----------|\\n|         |      |      |      |      |            |          |\\n| Reference | 7.9  | .27  | -    | 4.15 | .87        | 80       |\\n| Last sent | 23.3 | .79  | 24.08| 12.33| 21.82      | 24.62    |\\n|          |      |      |      |      |            |          |\\n| Story   |      |      |      |      |            |          |\\n| Bart-base | 11   | .72  | 27.83| 13.05| 25.43      | 22.88    |\\n| Bart-large | 10.3 | .63  | 27.00| 12.51| 24.74      | 21.44    |\\n| T5-base |      |      |      | 13.6 | .70        | 26.66    |\\n| T5-large | 12.9 | .67  | 28.36| 13.15| 25.71      | 24.31    |\\n| Fact.   |      |      |      |      |            |          |\\n| Bart-base | 7.3  | .49  | 26.26| 10.86| 24.09      | 18.28    |\\n| T5-base |      |      |      | 10.4 | .47        | 24.74    |\\n\\n* indicates the value is significantly higher than the Reference with a p value of .002 for Factuality and .0008 for Salience. The column AvgLS is the average of 3 crowd worker 1-5 Likert scores, and the column % Abs is percentage of instances with a score $\\\\geq 3$. \"}"}
{"id": "emnlp-2022-main-594", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience | Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience |\\n|----------------|--------|----------|------|------------|----------|----------------|--------|----------|------|------------|----------|\\n| Reference      | -      | 3.36     | 0.78 | 60         | 3.32     | 0.76           | 64     |          |      |            |          |\\n| Bart-base      | 3.6    | 8.3      | 3.35 | 0.73       | 90       | 3.25           | 0.67   | 52       | 3.04 | 0.73       | 42       |\\n| Bart-large     | 3.5    | 14.0     | 4.9  | 0.42       | 25.18    | 88.06          | 2.46   | 59       | 3.57 | 0.69       | 59       |\\n| T5-base        | 2.6    | 19.6     | 47.89| 0.38       | 24.61    | 87.74          | 1.77   | 38       | 3.69 | 0.76       | 68       |\\n| T5-large       | 3.7    | 13.9     | 52.26| 0.40       | 26.8     | 88.44          | 2.15   | 48       | 3.80*| 0.65       | 48       |\\n\\nTable 13: Generating Factors from stories and their endpoints (Task 2b). The best scores are bolded. The * for factuality and salience indicates the value is significantly higher than the Reference with a p value of .0001. The Reference value for Brevity is significantly higher than all values in the column with a p value of .0001. The column AvgLS is the average of 3 crowd workers' 1-5 Likert scores, and the column % Abs is percentage of instances with a score $\\\\geq 3$. \\n\\n| Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience | Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience |\\n|----------------|--------|----------|------|------------|----------|----------------|--------|----------|------|------------|----------|\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 5.6    | 0.55     | 42.54| 21.13      | 39.16    | 31.20          | 18.54  | 12.37    | 91.46| 4.43       | 4.25     |\\n| T5-base        | 8.4    | 0.55     | 43.13| 20.66      | 38.53    | 33.35          | 16.56  | 9.77     | 90.83| 4.43       | 4.24     |\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 9.8    | 0.39     | 22.53| 8.25       | 20.49    | 16.27          | 6.46   | 4.97     | 87.17| 3.98       | 3.59     |\\n| T5-base        | 23.8   | 0.64     | 23.60| 10.03      | 20.60    | 24.54          | 6.99   | 4.64     | 86.44| 4.46       | 4.01     |\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 6.2    | 0.89     | 39.39| 18.80      | 36.41    | 29.64          | 15.55  | 10.95    | 90.46| 4.55       | 4.09     |\\n| T5-base        | 9.0    | 0.74     | 35.42| 14.41      | 31.39    | 26.94          | 11.75  | 7.01     | 90.26| 4.45       | 3.43     |\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 11.5   | 0.77     | 25.28| 11.58      | 23.12    | 20.77          | 10.16  | 7.78     | 86.95| 4.62       | 4.01     |\\n| T5-base        | 15.5   | 0.75     | 22.94| 9.69       | 20.51    | 20.22          | 8.05   | 5.48     | 86.51| 4.59       | 3.95     |\\n\\nTable 14: Generating changes resulting from a complex event (Task 3). The best scores are bolded. The column AvgLS is the average of 3 crowd workers' 1-5 Likert scores, and the column % Abs is percentage of instances with a score $\\\\geq 3$. \\n\\n| Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience | Model          | Rouge  | MET-BLEU | Bert | Factuality | Salience |\\n|----------------|--------|----------|------|------------|----------|----------------|--------|----------|------|------------|----------|\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 6.2    | 0.89     | 39.39| 18.80      | 36.41    | 29.64          | 15.55  | 10.95    | 90.46| 4.55       | 4.09     |\\n| T5-base        | 9.0    | 0.74     | 35.42| 14.41      | 31.39    | 26.94          | 11.75  | 7.01     | 90.26| 4.45       | 3.43     |\\n| ROC            |        |          |      |            |          |                |        |          |      |            |          |\\n| Bart-base      | 11.5   | 0.77     | 25.28| 11.58      | 23.12    | 20.77          | 10.16  | 7.78     | 86.95| 4.62       | 4.01     |\\n| T5-base        | 15.5   | 0.75     | 22.94| 9.69       | 20.51    | 20.22          | 8.05   | 5.48     | 86.51| 4.59       | 3.95     |\\n\\nTable 15: Comparing Endpoint generations of models trained on ROCStories and models trained on Newswire stories.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We trained our models on a single RTX 8000 with 48GB of GPU memory. Approximate run time was 1 hour.\\n\\nModel Parameters\\nIn addition to the number of parameters in each of the models we consider (e.g., Bart-Base, T5-base, T5-large), each of our fine-tuned classification models has a separate classifier layer. This layer takes in a $D$-dimension embedding from the encoder and uses a single linear layer to compute $K$-dimensional logits (therefore, an additional $D \\\\times K$ parameters). The value of $D$ will depend on the model, and the value of $K$ will depend on the number of label types that could be predicted. For the generation tasks, we do not add any additional parameters to the models.\\n\\nHyperparameters\\nFor all experiments we used AdamW (Loshchilov and Hutter, 2017) optimizer, a learning rate of $10^{-4}$, a weight decay of $10^{-4}$ and a random seed of 11. We applied manual tuning and tried various learning rates from .001 to .00001 as suggested for the BART and T5 models. For the generation we used Top-K sampling with a beam size of 2. These parameters worked well for all the models and this was selected based on accuracy for the classification tasks and Rouge scores for the summarization tasks.\\n\\nResults statistics\\nWith our 5-fold cross-validation, the automated metrics for summarization and the accuracy/F1 values for classification varied by less than 3 percent.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"POQue: Asking Participant-specific Outcome Questions for a Deeper Understanding of Complex Events\\n\\nSai Vallurupalli1, Sayontan Ghosh2, Katrin Erk3, Niranjan Balasubramanian2, Francis Ferraro1\\n\\n1 University of Maryland, Baltimore County, 2 Stony Brook University, 3 University of Texas, Austin\\n\\nkolli@umbc.edu, sagghosh@cs.stonybrook.edu, katrin.erk@utexas.edu, niranjan@cs.stonybrook.edu, ferraro@umbc.edu\\n\\nAbstract\\n\\nKnowledge about outcomes is critical for complex event understanding but is hard to acquire. We show that by pre-identifying a participant in a complex event, crowdworkers are able to (1) infer the collective impact of salient events that make up the situation, (2) annotate the volitional engagement of participants in causing the situation, and (3) ground the outcome of the situation in state changes of the participants. By creating a multi-step interface and a careful quality control strategy, we collect a high quality annotated dataset of 8K short newswire narratives and ROCStories with high inter-annotator agreement (0.74-0.96 weighted Fleiss Kappa). Our dataset, POQue (Participant Outcome Questions), enables the exploration and development of models that address multiple aspects of semantic understanding. Experimentally, we show that current language models lag behind human performance in subtle ways through our task formulations that target abstract and specific comprehension of a complex event, its outcome, and a participant\u2019s influence over the event culmination.\\n\\n1 Introduction\\n\\nSituations that people experience or describe can be complex, and developing a computational understanding of these situations is not straightforward. Consider the short narrative from Fig. 1:\\n\\nAfter a decade as renters, [the Brofmans] were finally able to buy a small house here four years ago. But if the Argentine government yields to [IMF] pressure to rescind emergency legislation meant to protect ordinary families like the Brofman, the couple stand to lose their home and the $32,000 they have paid for it so far.\\n\\nAcross multiple, interwoven events with multiple participants, this narrative describes part of the process of losing one\u2019s house. A possible ending (the loss of a home) is suggested, which is the result of a confluence of these events. This ending can be semantically grounded in various changes of state that the participants experience, though note how the use of counterfactual considerations, conditional statements (\u201cif the Argentine government...\u201d), and varying levels of certainty over whether events have actually happened (e.g., realis vs. irrealis) contribute to the difficulty in understanding this complex event (Herman, 2002; Ryan, 1991).\\n\\nKnowledge about how event outcomes affect individual participants can help identify salient events in a narrative, fill in implicit missing information (LoBue and Yates, 2011) and chain events that lead to improved understanding of complex events (Graesser et al., 1994). To infuse AI models with similar knowledge, narrative comprehension research has focused on learning event relationships (Mostafazadeh et al., 2016; Chambers and Jurafsky, 2008; O\u2019Gorman et al., 2016; Caselli and Vossen, 2016), using temporal (Pustejovsky et al., 2003), causal (Mirza et al., 2014) and discourse (Prasad et al., 2008) relationships in text. However, as Dunietz et al. (2020) and Piper et al. (2021) argue, for a more useful, generalizable, and robust comprehension, we need to take a holistic view of complex events. In this paper, we tackle an understudied notion of this holistic view and examine knowledge of post-conditions based in states to support inferences of the form \u201cwho did what to whom and with what end result.\u201d\\n\\nA core insight we make is that viewing complex events through the lens of a single participant at a time, either from an agent (how the participant affects others) or a patient (how the participant is affected) view, can help mitigate the complexities we have discussed. In this we build on cross-disciplinary research that shows that humans mentally structure events along single participants (Black and Bower, 1980; Morrow et al., 1989), and that participant-based event and outcome analysis improves complex event understanding (Dijk and Kintsch, 1983; Liveley, 2019). We...\"}"}
{"id": "emnlp-2022-main-594", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Our approach to understanding state change outcome of complex events. Our four step annotation process involves describing abstractly what a story is about; writing an endpoint for the story; identifying and describing the changes that are a result of the endpoint, and identifying the salient sub-events that lead or support to those changes. To mitigate this complexity, we focus annotator's attention on one particular participant, and how that participant either causes or experiences the identified changes. This process has resulted in 4k annotated documents.\\n\\nAlso note that a complex event is not exhaustively described by what is stated in the text: it is well known that speakers often omit narrative steps that can be inferred (Grice, 1975), including outcomes and effects of a narrative that are often left implicit.\\n\\nWe present POQue, a dataset with post-conditional knowledge about complex events. We identify cumulative outcome-oriented endpoints of the stories caused by related events and the consequences or post-conditions of those events as state-based changes in participants. Seen in Fig. 1, in a storyline involving a participant (\\\"the Brofmans\\\"), we identify an ending outcome for the complex event (the Endpoint, \\\"the brofmans might lose their home and payments\\\"), with salient events that lead to this (Factors Leading to the Endpoint). We relate a participant's involvement in the complex event (as a \\\"patient\\\" who \\\"Very Likely\\\" experienced the ending outcome) and the changes of state occurring as a result of the complex event (the changes in possession and location experienced by the Brofmans and other families, and the change in possession by the Argentine government and IMF).\\n\\nTo facilitate high quality annotations we designed a multi-stage crowd sourcing solution to acquire, monitor, assess and curate annotations at scale. We collected 7772 annotations across 4001 stories and assessed a random 1545 annotations (20%) in a multistage pipeline to obtain a highly curated test set. Using POQue, we test current language models on reasoning about complex events in narratives: we formulate challenge tasks to identify and generate post-conditions from a story, and evaluate how well trained models predict a participant's involvement in enabling a complex event.\\n\\nWe summarize our contributions as follows: (1) we introduce a new annotation scheme focusing on complex events from the point of view of a single participant. (2) We create a new dataset of complex events from three collections of everyday stories, using free form text to obtain insight into implied outcomes. (3) We obtain high quality annotations from crowd workers without the use of requester generated qualification tests. (4) We formulate challenge tasks aimed at evaluating the ability of language models to perform richer complex event comprehension, specifically: a) generating a process summary of the complex event b) generating an endpoint of the complex event, c) generating the outcome of a complex event based on a participant's semantic role d) identifying a participant's involvement in a complex event, and e) generating post-conditions or changes caused by a complex event. Our dataset and code are publicly available at https://github.com/saiumbc/POQue.\\n\\n2 Related Work\\nNarrative texts communicate experiences and situations by connecting related events (Brooks, 1984; Mateas and Sengers, 1999) through events involv-\"}"}
{"id": "emnlp-2022-main-594", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing participants (Bal, 1997; Eisenberg and Finlayson, 2017; Liveley, 2019). Previous works, viewing narratives as sequences of events, annotated event pairs for event coreference, temporal, and causal relationships (O\u2019Gorman et al., 2016; Caselli and Vossen, 2016; Mirza and Tonelli, 2016; Mostafazadeh et al., 2016). Newer works have studied event groups using predicate hierarchies (Qi et al., 2022) and temporal graphs (Li et al., 2021). However, these approaches focus on event-event relationships, without diving deeply into participant or entity analysis. Unsupervised methods assume narratives are coherent and learn partially ordered event chains (Chambers and Jurafsky, 2008; Balasubramanian et al., 2013) or sub event relationships (Yao et al., 2020) but these are limited to what occurs in the text itself, which can lead to well-known issues of bias or evaluation limitations (Gordon and Van Durme, 2013; Rudinger et al., 2015).\\n\\nCaselli and Inel (2018) obtain crowd annotations of causal relationships between events and assess their quality by relating them to expert annotations. PeKo (Kwon et al., 2020) uses crowd annotations of precondition relationships and fine tunes a language model for finding such relationships. However, both works limit their study to event pairs in short text snippets. The ESTER dataset (Han et al., 2021) consists of more comprehensive relationships in a story, though limited to within-text (i.e., stated) mentions of events only. GLUCOSE (Mostafazadeh et al., 2020) provides elaborate causal relationships for several event dimensions for each event in a story. However, in contrast to our effort, these works address direct causal relationships between within-text events and do not focus on participants.\\n\\nUnderstanding complex events has long attracted cross-disciplinary attention. For example, theoretical linguistics and cognitive science work has shown that humans understand a narrative text using simulative inference (Kaplan and Schubert, 2000; Boella et al., 1999; Schubert and Hwang, 2000). Prior work has also shown how observing participants\u2019 events and the resulting consequences can lead to improved understanding of events (Dijk and Kintsch, 1983; Zwaan and Radvansky, 1998).\\n\\n3 Knowledge Representation\\n\\nAs an underlying motivation for our efforts, we posit that for language models to be able to reason about complex events from narratives, they should be able to identify a likely ending of that complex event, component events that lead to that ending, and the state changes that result from that ending. However, this type of knowledge is complex and has been computationally understudied, leading to a scarcity of sizable datasets. In our efforts to correct this, we appeal to classic, cognitively- and linguistically-backed results.\\n\\nFirst, inspired by the idea from Kintsch and van Dijk (1978) that text comprehension involves reducing relevant details into an abstract coherent semantic text, our targeted annotations include an abstract high level summary and the minimal set of salient events that make up the complex event. Second, we extend the idea of thematic roles for verbal arguments (Dowty, 1991) to a generalized semantic role for the complex event. Specifically, Dowty showed that easily verifiable characteristics and properties, such as volitional participation in an event or whether a participant underwent a change of state because of a particular event, can be used to define predicate-level prototypical semantic roles. Inspired by this, we characterize the roles of complex events through the intentional engagement in changes of state of participants. As such, our targeted annotations account for both the intentional involvement of the participants in the complex event and the cumulative impact of all the events that make up the complex event.\\n\\nStory and Participant\\n\\nStories in our dataset are either a ROCStory or heuristically salient portions of newswire (first 100-150 tokens). For more information on story processing see \u00a74.1. We define a participant as an entity that was mentioned several times in the story. See \u00a74.1 for more on participant selection. Multiple entities are considered a single Participant if these entities are mentioned together and participate together in all the events. In Fig. 1, \u201cthe Brofmans\u201d are a Participant.\\n\\n3.1 Targeted Knowledge Annotations\\n\\nGiven a story $S$, a participant $P$, and $P$ taking on a agent-like or patient-like cumulative semantic role, $PR$, we obtain the following annotations.\\n\\nProcess Summary (PS):\\nA high-level, free-form description of the situation, which provides the topical context for the complex event. For example, \u201cLosing home and payments\u201d for the story in Fig. 1.\\n\\n1While we acknowledge they have important differences, we use \u201cnarrative\u201d and \u201cstory\u201d interchangeably.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Crowd Annotations\\n\\nWe created a human intelligence task (HIT), deployed on Amazon Mechanical Turk (AMT). The HIT consists of a story with highlighted participant mentions displayed in the left column and four annotation steps in the right column which vary slightly for the agent and patient views. The protocol was IRB approved.\\n\\nCrowd workers were instructed to read the story, focus on the highlighted participant, and provide annotations. We provide several annotated examples, general instructions for completing the HIT, and specific instructions for each step suggesting a template to follow for some steps. More details and the layout of the HIT are in Appendix B.\\n\\nThe annotation task consists of 4 steps and each story is assigned to two workers, one where the highlighted participant is assigned the role of \u201cagent\u201d and another with the assigned role as \u201cpatient.\u201d Step 1 asks for a high level description of the story, a process summary of the situation described. Step 2 of the HIT asks for a description of an endpoint in the story. We assume a story\u2019s endpoint typically signifies a state change caused by a complex event. Step 3 asks for a summary of changes caused by the complex event in the story participants and also asks to identify the type of changes. Step 4 of the HIT asks an annotator to identify the salient events, or factors, that lead up to the complex event and changes from it.\\n\\n4 Dataset\\n\\nWe selected stories from three narrative English language datasets \u2013 the ESTER dataset (Han et al., 2021), the ROC stories dataset (Mostafazadeh et al., 2016) and passages from the Annotated New York Times newswire dataset (Sandhaus, 2008). We selected these given the prominence the underlying documents have in the broader NLP community (the ESTER documents are a subset of the TempEval3 (TE3) workshop dataset (UzZaman et al., 2013). We included ROC stories because they often contain a single situation with mostly salient information. We noticed these stories help crowd workers easily focus on salient events, providing cues for factors and state changes. Meanwhile, ESTER and the selected Newswire stories provide a variety of complex situations and discourse text. Additionally, by selecting subsets of these well-known datasets, we hope that future efforts may be able to aggregate our annotations with existing ones, enabling richer phenomena to be examined.\\n\\n4.1 Story Preparation\\n\\nWe sampled stories from the Annotated New York Times (ANYT) corpus, ROCStories, and ESTER. We then identified participants via an automatic entity coreference system. We heuristically selected relevant and annotable excerpts of the document by identifying \u201ccontinuant story lines\u201d (see Appendix A). After identifying a participant and a\"}"}
{"id": "emnlp-2022-main-594", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Stories\\n\\n|        | Total | ROC  | ESTER | NYT  |\\n|--------|-------|------|-------|------|\\n| Stories| 4001  | 1383 | 1275  | 1343 |\\n| Agent  | 3896  | 1364 | 1237  | 1295 |\\n| Patient| 3876  | 1356 | 1218  | 1302 |\\n\\nTable 1: Document-level data statistics. Note that the number of stories refers to the number of unique stories annotated, while the agent and patient numbers refer to the number of instances annotated on those documents. Additionally, 260 of the ROCStories are from the CATERS (Mostafazadeh et al., 2016) collection. CATERS stories and ESTER stories containing subevents are useful for relating causal and compositional events in a complex event.\\n\\n### Avg. process summary length\\n\\n5.7 words\\n\\n### Avg. endpoint length\\n\\n9.4 words\\n\\n### Endpoint stated/implied/unsure %\\n\\n68.5%/28.9%/2.6%\\n\\n### Avg. change descr. length\\n\\n8.9 words\\n\\n### Avg. likelihood of causing change (agent)\\n\\n219 (likely)\\n\\n### Avg. likelihood of experiencing change (patient)\\n\\n219 (likely)\\n\\n### Avg. # of factors\\n\\n3.7\\n\\n### Avg. factor length\\n\\n8.0 words\\n\\nTable 2: Additional statistics about POQue.\\n\\n### 4.2 Dataset Annotation & Pricing\\n\\nOur dataset has 4001 stories, annotated by 163 different crowd workers. The average number of stories annotated per worker is 43. When possible, we annotated from both an agent and a patient view for a participant, so in total we obtained 7773 annotations. Workers were paid an average of $0.50 for annotating a single HIT, either an agent or a patient view of the story. For more detailed information on payment and training see Appendix B.3. We tackle the positivity bias in AMT work (Matherly, 2018) using a thorough initial verification and training (see Appendix B) and ensured workers understood the task and provided quality annotations.\\n\\n### 4.3 Dataset Statistics\\n\\nFor most stories we obtained two annotations, with the two participant semantic roles. We show high-level document statistics in Table 1. Annotations for the two different roles of the highlighted participant are shown in separate columns. We show more detailed annotation statistics in Table 2, and examine the frequency of change modes in Fig. 2.\\n\\n### Figure 2: Count of change modes, shown for each of the agent and patient roles, and broken out across the originating datasets our annotated narratives come from.\\n\\n#### 4.4 Dataset Quality and Analysis\\n\\nWe noticed that the nature of the stories and the task steps elicit a variation in the text style and format, even from the same worker. Our experiments (\u00a76) and ablation studies (\u00a77) did not uncover any easy biases attributable to a small number of workers producing most of the annotations. Due to space limitations we explain our process for evaluating 1545 random annotations in Appendix C.2.\\n\\n### 5 Tasks for State Change Knowledge\\n\\nBased upon our collected dataset, we propose several tasks. These tasks are designed to test various aspects of comprehension involving complex events, their participants, and outcomes.\\n\\n#### 5.1 Task 1: Generating Process Summaries\\n\\nCategorizing stories based on the type of situation they describe is necessary for generalization. For this, we fine-tune models to generate an abstract and high level process summary of the complex action described in a story. Because we annotated salient events for the story, i.e., the factors, we have two task formulations. We fine-tune models to generate $PS$ either given $S$ or $(f_1, f_2, ..., f_n)$. Both are standard summarization tasks which we compare with a baseline where the process summary is assumed to be \\\"About $P$.\\\"\\n\\n#### 5.2 Task 2: Generating Complex Event Endpoints and Salient Events\\n\\nUnderstanding a story involves the identification and decomposition of salient events that lead to an endpoint, for the described complex event. We test this understanding with two complementary formulations where we generate either the endpoint description or the salient events, i.e., factors. For generating $ED$ we have two sub formulations where...\"}"}
{"id": "emnlp-2022-main-594", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we fine-tune models either on $S$ or $(f_1, f_2, \\\\ldots, f_n)$. For generating $(f_1, f_2, \\\\ldots, f_n)$ we fine-tune models on $(S, ED)$. These are all standard summarization tasks which we compare with a baseline where $ED$ is assumed to be the last sentence of the story.\\n\\n5.3 Task 3: Generating Changes Resulting from a Complex Event\\nKnowing the changes caused by a complex event gives us an insight into its importance and the intentions (addressed in Task 5) behind it. In this task, we generate changes caused by a complex event through the lens of the semantic role tracking we have employed throughout our effort. Using standard summarization, we fine-tune models to generate $CS$ given $(S, ED, PR)$.\\n\\n5.4 Task 4: Identifying Types of Changes\\nGrounding the impact of a complex event in the various change modes a participant undergoes helps in understanding the importance of new situations by relating them to known situations with similar post-conditions. We formulate this as a multi-label binary classification and fine-tune models to identify $k=5$ change modes $(c_1, c_2, \\\\ldots, c_k)$ given $S$.\\n\\n5.5 Task 5: Assessing Participant's Involvement in the Complex Event\\nBesides the story context, the participant's semantic role heavily influences our decision of whether the participant intended or enabled the complex event or the changes caused by it. In this binary classification task, we predict the participant involvement rating $PI$ given $(S, ED, PR)$.\\n\\n6 State Change Benchmark Experiments\\nWe benchmark the performance of current encoder-decoder transformer language models, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020), which are effective for both text generation and classification. We compare fine-tuned base and large models with multiple automated metrics and crowdsourced human evaluation. We use bootstrap for calculating statistical significance via the mlxtend library (Raschka, 2018).\\n\\n6.1 Automated Evaluation\\nWe use the classic metrics of ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and METEOR (Lavie and Agarwal, 2007), and the more recent BertScore (Zhang et al., 2020). Due to space limitations, we present ROUGE-L and BertScore in the main paper, and additional ROUGE-1, ROUGE-2, METEOR, and BLEU scores in the appendix (Appendix F). We use standard metrics used for single and multi-label classification: Accuracy and macro F1. In multi-label classification, we calculate Subset Accuracy and macro F1 using sklearn and a Hamming Score which is computed as $1/n \\\\sum_{i=1}^{n} Y_i \\\\cap Z_i / Y_i \\\\cup Z_i$, where $Y$ and $Z$ are true and predicted labels for $n$ examples.\\n\\n6.2 Human Study of Model Generations\\nWe perform a human evaluation of the generation tasks (1, 2, and 3) using 50 randomly selected generations for each model and the corresponding human annotations. We obtained qualitative ratings from 3 crowd workers experienced in annotating our HITs and measured IAA using a weighted Fleiss's Kappa as in Appendix C. For each summary, workers are presented with the story and the summary and asked to rate the summary on aspects that relate to the task such as abstractness, factuality and salience using a 5-point Likert scale. See Appendix D for more information on these aspects and the HITs used for evaluation.\\n\\n6.3 Task 1: Generating Process Summaries\\nTo test whether a model generates a more focused process summary when trained on salient information, we compare pre-trained models fine-tuned on $S$ and $(f_1, f_2, \\\\ldots, f_n)$ with an easy baseline process summary of \\\"About $P$\\\", where $P$ is the participant. Less than 1% of the process summaries in the dataset and model generations contain this baseline format. Results from this task training are listed partially in Table 3 and fully in Table 11. For all models, Rouge, BLEU and METEOR scores show less lexical overlap, but BertScore indicates a high similarity between the model generated and reference summaries. Inspired by previous work in measuring abstractiveness (See et al., 2017; Dreyer et al., 2021; Gao et al., 2019; Narayan et al., 2018), we compare average number of tokens ($Len$) across all summaries, the percentage of exactly matched trigram spans in the story ($Ext$), and the average of Abstractness Likert scores ($Abstr.$) from the evaluation.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Generating Process Summaries (Task 1). See appendix Table 11 for the full results. Bart-large is not included because we were unable to get it to properly converge. The best scores are bolded. We use * to indicate a significantly higher value than other values in the column with a p value between 0.001 and 0.0001 (except for Bart-base trained on Factors where the p value is 0.13).\\n\\n| Model          | Len  | Ext. | RougeL | BertScr | Fact. Sal. |\\n|----------------|------|------|--------|---------|------------|\\n| Reference      | 3.6  | 0.13*| -      | 3.57*   |            |\\n| About          | 1.7  | 0.27 | 10.43  | 83.86   | 2.37       |\\n| Story          |      |      |        |         |            |\\n| Bart-base      | 4.0  | 4.46 | 21.43  | 86.70   | 2.77       |\\n| T5-base        | 10.0 | 6.60 | 19.50  | 85.99   | 2.32       |\\n| T5-large       | 6.9  | 6.63 | 20.30  | 86.15   | 2.13       |\\n| Fact.          |      |      |        |         |            |\\n| Bart-base      | 4.2  | 4.33 | 23.81  | 87.66   | 3.22       |\\n| T5-base        | 9.9  | 5.60 | 18.29  | 86.10   | 2.73       |\\n\\nDiscussion:\\nBART generations are brief, less extractive and more abstractive, whereas T5 generations are longer, less abstractive and more directly drawing upon spans of story text. The Reference summaries are brief, significantly less extractive and at a significantly higher abstractness score compared to all the models. Models fine-tuned only on the factors produce more abstractive summaries. However, this increase in the abstractness for the BART model increased the factual errors, in line with previous observations (Cao et al., 2017; Kryscinski et al., 2019; Dreyer et al., 2021). The significantly higher brevity and abstractness of the Reference summaries point to a substantial gap between human and LMs' ability at capturing complex actions in a brief, high-level, abstract phrase.\\n\\n6.4 Task 2: Generating Endpoints & Factors\\nTo test how well models generate endpoints, models are fine-tuned to generate $ED$, given $S$ or ($f_1, f_2, \\\\ldots, f_n$) and compared with the baseline version where the $ED$ is assumed to be the story's last sentence. Partial results for this task formulation are listed in Table 4 and the full results in Table 12 for the trained models. We also compare models trained on the complementary task of generating ($f_1, f_2, \\\\ldots, f_n$) given ($S$, $ED$). A special token separates factors in all the task formulations involving factors. Partial results for this complementary task are listed in Table 5 with the full results in Table 13.\\n\\nFor all models, Rouge, BLEU and METEOR scores show higher lexical overlap, and BertScore shows higher similarity between the generated and reference summaries. We compare the average of the Factuality and Salience scores ($Fact.$ and $Sal.$, resp.) from the endpoint summary evaluation HIT (see Fig. 18) along with the average number of tokens ($Len$) across all summaries and the percentage of exactly matched trigram spans in the story ($Ext$). We also compare the average of the Brevity, Factuality and Salience scores ($Brev.$, $Fact.$, and $Sal.$) from the evaluation HIT (see Fig. 20) for factors along with the average number of factors ($# fact.$) across all stories and the average number of tokens ($Len$) across all factors and stories.\\n\\nDiscussion:\\nScores are significantly higher for LM generations than Reference endpoint descriptions on both Factuality and Salience. We looked at a random 100 Reference endpoint descriptions and the corresponding model generations. The first author of this paper identified which of the endpoints were not directly stated in the story, but rather implied by the story, and which ones were not-factual. As shown in Fig. 3, very few of the model generations are implied endpoints and a third of the Reference endpoints contain implied descriptions. Our HIT instructions asked workers to annotate not only explicit endpoints but also the ones implied by the story and they identified 29% of them as implied. Evaluators lowered their scores both...\"}"}
{"id": "emnlp-2022-main-594", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Non-factual and implied endpoint types.\\n\\nfor factuality and salience for these implied endpoints as the description may be a possible but not strictly entailed outcome. Models trained on factors generated more implied endpoint descriptions but these implied endpoints contained more factual errors possibly because of less available context. We conclude that LMs try to generate stated endpoint descriptions unless challenged by limited context.\\n\\nFine-tuning on stories and endpoints generates salient factors, indicated by the high assessment scores. However, the generated factors on average contain multiple facts making them less concise and focused than human written factors.\\n\\n6.5 Task 3: Generating Change Summaries\\n\\nWe compare model generations of state changes, by fine-tuning models to generate $CS_{conditioned}$ on $(ED, PR)$. Given the pair $(S, t)$, where $t$ is either \\\"P caused this: $ED$\\\" if $PR$ = \\\"agent\\\" or \\\"P experienced this: $ED$\\\" if $PR$ = \\\"patient\\\", fine-tuned models generate $CS$. Partial results for this task are listed in Table 6, and the full results in Table 14. For all models, Rouge, BLEU and METEOR scores show high lexical overlap and BertScore indicates a high similarity between model generations and Reference summaries. We compare the average of the Factuality and Salience Likert scores (Fact. and Sal. resp.) from the evaluation HIT (see Fig. 19) which measures whether the generated text contains change(s) resulting from the complex action.\\n\\nDiscussion: T5 models generate change summaries that are significantly more factual and salient than the BART models. While T5 generations score higher than Reference summaries, the difference is not significant. To explain these results, we inspected the 50 evaluated stories and found that less than 10% of the stories have no changes even though annotators indicated \\\"no changes\\\" for 25% of the stories in these 50 (and in the entire dataset). To see if crowd workers can identify these no-change stories, we ran a HIT where the change summary for these 50 stories was set to no-changes. From the results of this HIT we found that human evaluators also think there are 3 times as many stories with no-changes. Workers missed subtle changes in a story especially when they relate to changes in cognition. T5 was able to identify story text that contained subtle changes while the BART models seem to be learning the data distribution from the training data. BART generations also contain a higher number of factual errors leading to its subpar performance.\\n\\n6.6 Task 4: Identifying Types of Changes\\n\\nWe fine-tune base models on a multi-label (n=5) binary classification task and assign change mode labels, $(c_1, c_2, .., c_k)$, for an input context consisting of two sentences: story $S$, and $(PI, c, ED)$ where $c$ is a connector phrase. The value of $c$ is \\\"caused this:\\\" when $PR$ = \\\"agent,\\\" and \\\"experienced this:\\\" when $PR$ = \\\"patient.\\\" The results from this classification are reported in Table 7 and consist of Subset accuracy, Hamming Score and Macro F1. The Enc Only models consists of a T5 encoder model with a classification head on top. The classification head consists of the following sequence of transformations: Dropout (p = 0.3) \u2013> Linear(768x 512) \u2013> tanh() \u2013> Dropout (p = 0.3) \u2013> Linear(512 x 5) \u2013> sigmoid(). We also fine-tuned a pretrained T5 encoder-decoder model in a text-to-text multi-label RTE setting.\\n\\nDiscussion: These results indicate that while the fine-tuned models are good at generating change summaries, assigning the various change model labels is a challenging task for these LMs.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Identifying Participant\u2019s involvement (Task 5).\\n\\nThe best results are bolded. The different folds of the Bart-large models converge at different checkpoints resulting in lower average scores but the best scores for any fold are comparable to the Bart-base model.\\n\\n6.7 Task 5: Assessing Participant Involvement\\n\\nWe turn the 5-point Likert scale for PI into a binary class: the first two options (unlikely to be involved) make up the negative class and the latter three (neutral to likely to be involved) are the positive class. We formulate participant involvement and enablement of changes as entailment: the story S is the premise and the hypothesis is framed as the P\u2019s involvement in the changes indicated by CS.\\n\\nWe fine-tuned models on all story annotations; only annotations where P\u2019s semantic role is \\\"agent\\\"; and only the annotations where P\u2019s semantic role is \\\"patient.\\\" Results are in Table 8.\\n\\nDiscussion: While all the models are able to classify a participant\u2019s involvement and enablement of changes with high accuracy there is still room for improvement. Error analysis indicated models are not able to identify enablement when there are no state changes. This usually happens when the complex action is a hypothetical situation or the changes involve subtle cognition (discussed in Task 3). In T5 models, we noticed some errors contradicted the hypothesis statement; these may be due to the model\u2019s external knowledge from pre-training, but this requires further study.\\n\\n7 Effect of Discourse Text on Models\\n\\nWe study the effect of discourse text on model generations of endpoint descriptions using the two story types we annotated: ROCStories and newswire stories. ROCStories are simpler with short, concise and focused salient events, while newswire are more complex, containing more text not always salient to the complex action we annotated. We wish to answer the following questions:\\n\\n1) How does training domain affect endpoint generation?\\n2) Are the endpoint generations more/less concise, varied, focused and factual for the story context?\\n3) Do models trained on one type of stories transfer their learnt knowledge to generate equally good endpoints for the other type?\\n\\nWe fine-tuned BART and T5 base models separately on ROCStories vs. Newswire, and evaluated them on test sets for both story types. We calculated human scores from the endpoint evaluation HIT.\\n\\nFrom Table 9, we observe the following: (1) ROC-Stories models generate shorter, more varied and abstract descriptions. (2) Newswire generations are longer and more extractive. (3) ROC-trained BART has significantly lower salience when tested on Newswire stories. News-trained BART does not suffer from poorer salience. News-trained T5 has lower salience when tested on ROCStories, while ROC-trained T5 does not result in significantly lower salience. (4) BART generations are less factual than T5, possibly because of higher abstractness (Dreyer et al., 2021). (5) ROC-trained T5 and News-trained BART obtain similar high scores for factuality and salience.\\n\\n8 Conclusions\\n\\nWe have argued that a deeper understanding of complex events can be achieved by examining their cumulative outcomes, grounded as changes of state. By focusing on a specific participant in a complex event, and a broad notion of its semantic role, we developed a crowdsourcing protocol to obtain 7.7k annotations about complex events and participant state change across 4k stories. We validated 20% of the annotations, with high inter-annotator agreement. We have formulated five challenge tasks that stress model\u2019s understanding of story outcomes, state changes and complex event understanding.\\n\\nOur evaluations suggest that additional modeling advances are needed to achieve this understanding; we hope that our dataset spurs this future work.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nWe acknowledge the following limitations of our approach:\\n\\n\u2022 Though the documents we base our annotations on come from well known data sources, our efforts focus on more formal levels of written English. Generation and classification abilities can vary as the formality, style, or language change.\\n\\n\u2022 Though our work is heavily grounded in interdisciplinary literature, we adopt a limited two-argument view of complex event participants: either they are an \\\"agent\\\" or a \\\"patient.\\\" Expanding to other types, or finer-grained notions, of arguments requires more investigation.\\n\\n\u2022 We use large, pre-trained language models in our experiments. While powerful, they can echo biases, either implicitly or explicitly. We do not attempt to control for these in this work.\\n\\nAcknowledgements\\n\\nWe would like to thank the anonymous reviewers for their comments, questions, and suggestions. This material is based in part upon work supported by the National Science Foundation under Grant No. IIS-2024878. Some experiments were conducted on the UMBC HPCF, supported by the National Science Foundation under Grant No. CNS-1920079. This material is also based on research that is in part supported by the Army Research Laboratory, Grant No. W911NF212-20076, and by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either express or implied, of the Air Force Research Laboratory (AFRL), DARPA, or the U.S. Government.\\n\\nReferences\\n\\nM. Bal. 1997. Narratology: Introduction to the Theory of Narrative. University of Toronto Press.\\n\\nNiranjan Balasubramanian, Stephen Soderland, Mausam, and Oren Etzioni. 2013. Generating Coherent Event Schemas at Scale. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1721\u20131731, Seattle, Washington, USA. Association for Computational Linguistics.\\n\\nMohaddeseh Bastan, Mahnaz Koupaee, Youngseo Son, Richard Sicoli, and Niranjan Balasubramanian. 2020. Author's Sentiment Prediction. In Proceedings of the 28th International Conference on Computational Linguistics, pages 604\u2013615, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nJohn B. Black and Gordon H. Bower. 1980. Story understanding as problem-solving. Poetics, 9(1):223\u2013250. Special Issue Story Comprehension.\\n\\nGuido Boella, Rossana Damiano, and Leonardo Lesmo. 1999. Understanding narrative is like observing agents. AAAI Technical Report FS-99-01.\\n\\nPeter Brooks. 1984. Reading for the Plot: Design and Intention in Narrative. Knopf Doubleday Publishing Group.\\n\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2017. Faithful to the Original: Fact Aware Neural Abstractive Summarization. CoRR, abs/1711.04434.\\n\\nTommaso Caselli and Oana Inel. 2018. Crowdsourcing StoryLines: Harnessing the Crowd for Causal Relation Annotation. In Proceedings of the Workshop Events and Stories in the News 2018, pages 44\u201354, Santa Fe, New Mexico, U.S.A. Association for Computational Linguistics.\\n\\nTommaso Caselli and Piek Vossen. 2016. The Storyline Annotation and Representation Scheme (StaR): A Proposal. In Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016), pages 67\u201372, Austin, Texas. Association for Computational Linguistics.\\n\\nNathanael Chambers and Dan Jurafsky. 2008. Unsupervised Learning of Narrative Event Chains. In Proceedings of ACL-08: HLT, pages 789\u2013797, Columbus, Ohio. Association for Computational Linguistics.\\n\\nT. A. Dijk and W. Kintsch. 1983. Strategies of Discourse Comprehension. In Psychology.\\n\\nDavid Dowty. 1991. Thematic Proto-Roles and Argument Selection. In Language, volume 67, pages 547\u2013619, USA. Linguistic Society of America.\\n\\nMarkus Dreyer, Mengwen Liu, Feng Nan, Sandeep Atluri, and Sujith Ravi. 2021. Analyzing the Abstractiveness-Factuality Tradeoff With Nonlinear Abstractiveness Constraints. CoRR, abs/2108.02859.\\n\\nJesse Dunietz, Greg Burnham, Akash Bharadwaj, Owen Rambow, Jennifer Chu-Carroll, and Dave Ferrucci. 2020. To Test Machine Comprehension, Start by Defining Comprehension. In Proceedings of the...\"}"}
{"id": "emnlp-2022-main-594", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-594", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Donata Marasini, Piero Quatto, and Enrico Ripamonti. 2016. Assessing the inter-rater agreement for ordinal data through weighted indexes. *Statistical Methods in Medical Research*, 25:2611 \u2013 2633.\\n\\nMichael Mateas and Phoebe Sengers. 1999. Narrative Intelligence. *AAAI Technical Report FS-99-01*.\\n\\nTed Matherly. 2018. A Panel For Lemons? Positivity bias, reputation systems and data quality on MTurk. *European Journal of Marketing*, 53.\\n\\nParamita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating Causality in the TempEval-3 Corpus. In *Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL)*, pages 10\u201319, Gothenburg, Sweden. Association for Computational Linguistics.\\n\\nParamita Mirza and Sara Tonelli. 2016. CATENA: CAusal and TEmporal relation extraction from NAtural language texts. In *Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers*, pages 64\u201375, Osaka, Japan. The COLING 2016 Organizing Committee.\\n\\nDaniel G Morrow, Gordon H Bower, and Steven L Greenspan. 1989. Updating situation models during narrative comprehension. *Journal of Memory and Language*, 28(3):292\u2013312.\\n\\nNasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Vanderwende. 2016. CaTeRS: Causal and Temporal Relation Scheme for Semantic Annotation of Event Structures. In *Proceedings of the Fourth Workshop on Events*, pages 51\u201361, San Diego, California. Association for Computational Linguistics.\\n\\nNasrin Mostafazadeh, Aditya Kalyanpur, Lori Moon, David Buchanan, Lauren Berkowitz, Or Biran, and Jennifer Chu-Carroll. 2020. GLUCOSE: GeneraLized and COntextualized Story Explanations. In *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)*, pages 4569\u20134586, Online. Association for Computational Linguistics.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization. In *Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\\n\\nTim O'Gorman, Kristin Wright-Bettner, and Martha Palmer. 2016. Richer Event Description: Integrating event coreference with temporal, causal and bridging annotation. In *Proceedings of the 2nd Workshop on Computing News Storylines (CNS 2016)*, pages 47\u201356, Austin, Texas. Association for Computational Linguistics.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: A Method for Automatic Evaluation of Machine Translation. In *Proceedings of the 40th Annual Meeting on Association for Computational Linguistics*, ACL '02, page 311\u2013318, USA. Association for Computational Linguistics.\\n\\nAndrew Piper, Richard Jean So, and David Bamman. 2021. Narrative Theory for Computational Narrative Understanding. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*, pages 298\u2013311, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nRashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltisasaki, Livio Robaldo, Aravind Joshi, and Bonnie Webber. 2008. The Penn Discourse TreeBank 2.0. In *Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08)*, Marrakech, Morocco. European Language Resources Association (ELRA).\\n\\nJames Pustejovsky, Robert Ingria, Robert Gaizauskas, Andrea Setzer, and Graham Katz. 2003. TimeML: Robust specification of event and temporal expressions in text. In *In New Directions in Question Answering*.\\n\\nZheng Qi, Elior Sulem, Haoyu Wang, Xiaodong Yu, and Dan Roth. 2022. Capturing the Content of a Document through Complex Event Identification. In *Proceedings of the 11th Joint Conference on Lexical and Computational Semantics*, pages 331\u2013340, Seattle, Washington. Association for Computational Linguistics.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer. *Journal of Machine Learning Research*, 21(140):1\u201367.\\n\\nSebastian Raschka. 2018. MLxtend: Providing machine learning and data science utilities and extensions to Python's scientific computing stack. *The Journal of Open Source Software*, 3(24).\\n\\nRachel Rudinger, Pushpendre Rastogi, Francis Ferraro, and Benjamin Van Durme. 2015. Script Induction as Language Modeling. In *Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing*, pages 1681\u20131686, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nMarie-Laure Ryan. 1991. Possible worlds, artificial intelligence, and narrative theory. *Indiana University Press, Bloomington*.\\n\\nEvan Sandhaus. 2008. The new york times annotated corpus. *Linguistic Data Consortium, Philadelphia*, 6(12):e26752.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Additional Details on Data Preparation\\n\\nIn this section, we expand on data processing described in \u00a74.1.\\n\\nDocument Selection\\n\\nFrom the Annotated New York Times (ANYT) newswire articles, we found that stories from the Financial, National and Foreign desks contained the type of complex events that were most reliable to annotate: those with focused discourse text that required less external, societal, or cultural knowledge to understand the story. We did not specifically target obituaries as they could lead to less varied endpoint and cumulative state changes. The ROCStories were randomly sampled, and we subsampled stories from ESTER that had subevent annotations in that dataset.\\n\\nParticipant Identification\\n\\nWe used spanBERT (Joshi et al., 2019) to resolve coreferent mentions in the text, and selected the largest cluster of mentions. To find clusters containing a valid participant, we selected the shortest text span from all the mentions in the cluster making sure that it is at least 3 characters long and matched it with the names database published by the SSA. This ensured that the \u201cparticipant or prop\u201d we selected is a person, place, group or organization. In ROCStories and ESTER, the largest cluster is always a person, place or group and did not require this name filtering.\\n\\nContinuant Story Lines\\n\\nWe selected the first few lines containing approximately 100 tokens, which resulted in stories similar in length to previous work (Han et al., 2021; Glava\u0161 et al., 2014; O\u2019Gorman et al., 2016). We highlighted mentions of the participant to outline a \u201ccontinuant\u201d story-line, i.e., a set of related events that lead to a coherent story involving the participant. Focusing on the events in a continuant story line helps an annotator observe a complex action and its effects. By identifying and highlighting a single participant we limit the scope of possible valid endpoints an annotator might consider. Assigning a semantic role to the participant, of an agent, or a patient, helps cue the annotator to identify a participant\u2019s role in the complex action, and the changes resulting from it.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.1 Worker Qualifications\\nWe did not use requester generated qualification tests to filter out workers because we target the understanding of everyday reported events, not domain or expert-level matter. However, we used community standard quality criteria, such as requiring a 98% or greater HIT acceptance rate and the completion of 1000 approved HITs. In addition, we required the worker's stated location to be in the USA, UK, Canada, Australia, or New Zealand. Given the language-dependent semantic phenomena we pursue in this work, this location requirement was used for avoiding language-based artifacts. While qualification tests can filter for spam, initial misunderstandings could exclude capable workers who benefit from additional feedback. By providing positive and constructive feedback to ensure workers understood the task, we were able to retain workers who improved over time and provided quality annotations, a requirement for any crowdsourced task.\\n\\nB.2 Annotation HIT Streamlining\\nOur initial development tested selection of text spans vs. free form text and noticed workers preferred one over the other for some of the steps. To reduce annotation time, we refined the HIT to prime workers to hone in on the salient information in the story, provided functionality that allowed for a quick highlight and paste of relevant text when needed. We encouraged free form text in steps 1 and 2, an easy to fill in template for step 3, and highlight and paste for step 4. Despite instructions to be concise, early annotations suggested that some workers would try to include as much information as possible into the free form text fields, resulting in lengthy descriptions that provided too much detail (e.g., going beyond immediate outcomes, or providing explanation/justification for why those changes happened). To address this issue, we implemented two-tiered length limitations on the free form text. The first tier was a \u201csoft constraint\u201d: if, e.g., a worker typed in a endpoint greater than 8 words, they were prompted to consider revising, but they did not have to. The second tier was a \u201chard constraint\u201d: if, e.g., the endpoint was greater than 15 words, they were prevented from submitting until they reparsed and satisfied the hard constraint limit. These limits were set based on the examination of early annotations.\\n\\nIn addition, in each HIT batch, we included a mix of the lengthier Newswire and short ROCStory texts to reduce the monotony of annotation. From the alpha run annotation times, and internal annotation timing, we estimated the average annotation time for a HIT completion to be under 2 minutes. The bulk of annotations for our HITs were completed within 5 minutes, with a median and mean of approximately 2.5 minutes.\\n\\nB.3 Worker Training and Pay\\nOur HITs were priced to target an hourly pay of $10-$12. We carefully tracked and analyzed the user response times across pilot runs to arrive at the HIT pricing. For each worker, we carefully examined the first 10-30 annotations to check task understanding, providing feedback and a bonus as appropriate to compensate for the time spent on communication. We initially had an additional 26 crowd workers who attempted the HIT, but we removed their annotations from the dataset for obvious bad-faith efforts (10 workers) and for benefit-of-the-doubt good faith efforts but where the workers (16 workers) did not follow instructions even with repeated feedback. Anyone construed to have completed the annotation in good faith was paid, even when their responses were not included in our dataset.\\n\\nB.4 Annotation Quality Checking\\nOur cursory visual check of the annotation and an automatic lexical check of a list of novel unigrams that are not part of the story ensured the annotation content was focused on the story. We gave iterative feedback to workers not following task instructions and excluded them with a qualification type when there was no improvement.\\n\\nAdditional Data Analysis\\nIn Fig. 4 we show the distribution of crowd workers for our annotation effort.\\n\\nB.5 Annotation HITs\\nThe HIT for acquiring story annotations from crowd workers displays two different views based on participant\u2019s semantic role in the story. General and specific instructions for Step 1-3 are different in two views.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Quality Assessment of Crowd Annotations\\n\\nIn the initial phases of data collection the first two authors of this paper evaluated both the agent and patient views of 50 random stories (= 100 annotations) to ensure annotator responsiveness and quality. After collecting all the data, 3 crowd workers evaluated a random 1545 annotations and an expert evaluated a random 100 of these annotations (equal agent and patient views) for comparison. Table 10 lists the inter-rater reliability (IRR) measured using weighted Fleiss\u2019s Kappa (Marasini et al., 2016) with the weighting scheme used by (Bastan et al., 2020), which penalises each dissimilar class by an amount based on the distance between classes (e.g., an item with responses of \u201cvery likely\u201d and \u201cvery unlikely\u201d will be penalized more heavily than with responses of \u201cvery likely\u201d and \u201csomewhat likely\u201d).\\n\\nOur evaluation consisted of 4 validation HITs, where 3 crowd workers rated the various annotation steps (see Appendix C.2). The results from this evaluation are listed in Table 10. The IRR scores suggest substantial-to-high agreement. Notably, these demonstrate that we can obtain high quality process and change of state summaries, endpoint descriptions and enabling sub-event factors.\\n\\nC.1 Evaluation Set Curation\\n\\nFrom the 1545 validated annotations we selected annotations where the average score of the crowd workers for each of the 4 validation HITs is at least 3.0. This curation resulted in 1196 carefully produced annotations for a given story. The test data set is made up of these curated annotations and the training data set is made up of the remaining validated and unvalidated annotations.\\n\\nC.2 Validation HITs\\n\\nThe various annotation steps are validated using 4 HITs. 3 workers evaluate the following using a 1-5 Likert scale with the options: Strongly Disagree, Somewhat Disagree, Neutral, Somewhat Agree and Strongly Agree.\\n\\n1. Whether the Process Summary is a valid high level summary of the story.\\n2. Whether the endpoint description describes a valid endpoint for the complex action in the story.\\n3. Whether the change summary describes changes that happened as result of the complex event described in the story.\\n4. Whether the categorization of changes into the five change modes is consistent with the changes inferred from the story.\\n5. Whether the factors are salient to the complex event\u2019s endpoint.\"}"}
{"id": "emnlp-2022-main-594", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Instructions provided for the Agent view of the annotation HIT. The distinguishing aspect that makes it the Agent view is in step 3, where changes are attributable to what the participant or prop caused. In 7, 8, 10 and 12 we show the interface for each of the steps.\\n\\nFigure 6: Instructions provided for the Patient view of the annotation HIT. The distinguishing aspect that makes it the Patient view is in step 3, where we ask about changes the participant or prop likely experienced. In 7, 9, 11 and 12 we show the interface for each of the steps.\"}"}
