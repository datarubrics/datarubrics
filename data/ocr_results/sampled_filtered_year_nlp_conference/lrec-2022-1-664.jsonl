{"id": "lrec-2022-1-664", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MAKED: Multi-lingual Automatic Keyword Extraction Dataset\\n\\nYash Verma, Anubhav Jangra, Sriparna Saha, Adam Jatowt, Dwaipayan Roy\\n\\nIndian Institute of Science Education and Research Kolkata, India\\nIndian Institute of Technology Patna, India\\nUniversity of Innsbruck, Austria\\n\\n{yashv7523, anubhav0603, sriparna.saha, jatowt, dwaipayan.roy}@gmail.com\\n\\nAbstract\\n\\nKeyword extraction is an integral task for many downstream problems like clustering, recommendation, search and classification. Development and evaluation of keyword extraction techniques require an exhaustive dataset; however, currently, the community lacks large-scale multi-lingual datasets. In this paper, we present MAKED, a large-scale multi-lingual keyword extraction dataset comprising of 540K+ news articles from British Broadcasting Corporation News (BBC News) spanning 20 languages. It is the first keyword extraction dataset for 11 of these 20 languages. The quality of the dataset is examined by experimentation with several baselines. We believe that the proposed dataset will help advance the field of automatic keyword extraction given its size, diversity in terms of languages used, topics covered and time periods as well as its focus on under-studied languages.\\n\\nKeywords:\\n\\nkeyword extraction, text processing, multi-lingual dataset.\\n\\n1. Introduction\\n\\nThe amount of information being uploaded on internet each day is increasing over time, making it harder to filter out relevant information tailored to an individual. The information over the web spans multiple languages, making the task of multi-lingual keyword extraction quite useful. At the same time the diversity of these languages makes the task also challenging. For example, Le (2015) explains that if we consider Japanese (ja) for the task of keyphrase extraction in the legal context, the candidates of interest are words, chunks, and clauses. However, for the same task in English (en) language, utilizing similar structural information will lead to a less optimal solution or may not improve the performance since chunks can lead to a noisy output for keyword extraction.\\n\\nExtracting representative words or phrases from a document is essential to quickly summarize and understand the topics covered within the text. Keyphrases are word(s) that convey the essence or the main topics of the document, and their extraction is essential for supporting or enhancing many downstream tasks in the domain of information retrieval (Medelyan and Witten, 2008), text representation and summarization (Litvak and Last, 2008), document clustering (Han et al., 2007) and so on. Although many scientific articles and news articles are already associated with keywords, most documents are not; hence the development of dedicated models for the keyword extraction task is necessary. A large and diverse annotated corpus will then motivate and foster the development of supervised techniques and evaluation of various keyword extraction methods.\\n\\nNews articles are one of the most consumed and readily available types of documents and have been explored in many state-of-the-art transformer-based models (Zhang et al., 2020; Lewis et al., 2019; Raffel et al., 2019) for tasks like summarization, question generation and answering etc. Keywords are crucial for the news domain and can help in tasks like clustering articles based on keywords, enhancing the search for specific events presented as keywords, and obtaining temporal changes for event recommendation systems.\\n\\nPrevious works introducing datasets for keyword extraction (see Tab. 1. for an overview) rely on small-scale uni-lingual data from the scientific domain. Current deep neural network-based models require however considerable amount of data for training purpose. Yet, to the best of our knowledge, there exist only 3 mono-lingual datasets in the news domain that have up to 500 documents.\\n\\nHence, to fill this gap, in this work we propose a multi-lingual keyword extraction dataset from the news domain. It comprises of more than 540K documents and spans over 20 languages: English (en), Chinese (zh), Spanish (es), Russian (ru), French (fr), Ukrainian (uk), Portuguese (pt), Japanese (jp), Tamil (ta), Hindi (hi), Marathi (mr), Gujarati (gu), Bengali (bn), Sinhala (si), Urdu (ur), Pashto (ps), Indonesian (id), Telugu (te), Punjabi (pa), and Nepali (ne). This makes it the largest keyword extraction dataset with the highest number of supported languages so far. Note that our dataset also contains images in many documents which have the potential to foster the research in multi-modal keyword extraction.\\n\\nThe major contributions of this work are as follows:\\n\\n\u2022 We release the first ever large-scale multi-lingual keyword extraction dataset covering 20 languages and comprising of 540K+ news articles.\\n\\n\u2022 The performance of various baselines on the proposed dataset, including statistical, graph-based, and supervised keyword extraction methods, is reported.\\n\\n\u2022 It is the largest mono-lingual news keyword extraction dataset for each proposed language, where at least 14 of the covered languages are under-studied and categorized as low-resourced.\\n\\n\u2022 To the best of our knowledge, this is the first cross-lingual keyword extraction dataset for English-Japanese (en-ja) pair.\\n\\n1 A sample dataset can be found in the project repository at https://github.com/zenquiorra/MAKED. The complete dataset will be released in the camera-ready version.\"}"}
{"id": "lrec-2022-1-664", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related Work\\n\\nIn this section we discuss some existing keyword extraction datasets that are frequently used in the community. A large percentage of these datasets are based on scientific publications since these already contain manually-added keywords.\\n\\nKrapivin2009: Krapivin et al. (2009) proposed a dataset consisting of 2304 scientific papers from the computer science domain published by ACM. Every article has keyphrases assigned by the authors, and parts of each paper such as abstract and title are separated, enabling extraction based on a given part of an article\u2019s text.\\n\\nInspec: Hulth (2003) proposed a dataset consisting of 2000 abstracts of scientific journals from the computer science domain, and it has a temporal span from the year 1998 to 2002. For every document, the ground truth keywords are assembled by taking the union of controlled keywords, which are available in the Inspec thesaurus (these may not appear in the document content), and the uncontrolled keywords assigned by the authors.\\n\\nWWW: Gollapalli and Caragea (2014) proposed a graph-based algorithm CiteTextRank for automatic keyphrase extraction. It utilizes the context in which a document is referred to within a citation network and the content of the document. WWW is constructed as a gold standard annotated dataset to test the performance of CiteTextRank. It has been obtained from the proceedings of the last ten years of the World Wide Web Conference (WWW) and consists of 1330 documents.\\n\\nSchutz2008: Schutz and others (2008) released a collection of scientific papers collected from PubMed Central, which consists of publications from biomedical literature. The dataset contains 1231 documents; the authors provided the gold keywords for the corresponding documents.\\n\\nWikiNews: Bougouin et al. (2013) proposed a graph-based algorithm called TopicRank for automatic keyphrase extraction. It relies on the topical representation of the document, such that the vertices in a complete graph are key-words clustered into topics that are ranked using a graph-based ranking model. A French (fr) corpus has been created using the French version of WikiNews that contains 100 news articles with manual annotations added by students.\\n\\n110-PT-BN-KP: Marujo et al. (2013) proposed a dataset made from 8 TV Broadcast News (BN) programs in Portuguese (pt) language containing 110 news, derived from the European Portuguese ALERT BN database. In-house manual examination produced transcriptions, including punctuation, capitalization, and segmentation. Keyphrases were then manually annotated with the objective to capture keyphrases that summarize each news.\\n\\n500N-KP-Crowd-v1.1: Marujo et al. (2013) proposed 500N-KP-Crowd-v1.1 consisting of 500 news articles in English (en) language across various categories. Ground truth keywords have been developed through the Amazon Mechanical Turk service, using multiple annotators, and keywords were chosen if these were provided by over 90% of the annotators.\\n\\nMost of the keyword extraction works have created corpora suitable for testing their proposed keyword extraction methods, and they utilize different parts of text across certain domains. In general, the field lacks a large-scale benchmark dataset that could be used to evaluate existing methods across multiple topics with varying document sizes. Previous works lack the required size to train modern neural-based models or to evaluate the actual performance of unsupervised techniques over a large real-world corpus instead of a small sample space, which may not be representative. The existing datasets are also mono-lingual, and most of them are limited to English language (refer to Appendix A). Since different languages have varying writing styles, there is a need for robust keyword extraction techniques that could handle such variations. The MAKED corpora that we release can act as the benchmark for evaluation, as well as a source for training such robust models. It consists of 20 languages, out of which 9 are among the top 10 most spoken languages globally, and they belong to 5 language families namely Indo-European, Dravidian, Austronesian, Sino-Tibetan and Japanic. A detailed comparison of our dataset with prior works can be found in Table 1.\"}"}
{"id": "lrec-2022-1-664", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Dataset Statistics.\\n\\n| Language | \\n|----------|----------|----------|-------------------|----------|----------|\\n|          | \\n| si       | 2,590    | 660.80   | 32.89            | 9,987   | 1.65     | 55.26   |\\n| pt       | 4,307    | 5,809.21 | 221.61          | 17,848  | 1.39     | 68.16   |\\n| fr       | 6,689    | 753.80   | 25.70           | 22,929  | 1.83     | 71.07   |\\n| ja       | 6,845    | 1,083.04 | 34.69           | 32,346  | 1.55     | 48.28   |\\n| ps       | 10,140   | 605.76   | 20.89           | 41,757  | 1.67     | 55.60   |\\n| ne       | 10,933   | 449.09   | 27.68           | 37,334  | 1.46     | 47.51   |\\n| pa       | 11,364   | 848.87   | 39.06           | 58,841  | 1.54     | 53.20   |\\n| gu       | 11,682   | 873.04   | 50.32           | 62,485  | 1.71     | 57.47   |\\n| zh       | 12,926   | 1,436.83 | 45.36           | 60,364  | 1.90     | 51.04   |\\n| bn       | 13,226   | 618.85   | 38.10           | 45,272  | 1.32     | 39.53   |\\n| id       | 13,642   | 907.74   | 44.64           | 41,467  | 1.39     | 65.67   |\\n| te       | 15,061   | 631.52   | 52.15           | 77,430  | 1.39     | 62.36   |\\n| mr       | 15,736   | 873.84   | 63.54           | 82,331  | 1.52     | 56.66   |\\n| ur       | 19,835   | 998.33   | 1.12            | 76,481  | 1.43     | 42.66   |\\n| ta       | 20,835   | 495.63   | 33.87           | 85,830  | 1.40     | 61.25   |\\n| hi       | 22,286   | 1,144.29 | 54.65           | 95,163  | 1.56     | 43.77   |\\n| uk       | 25,905   | 659.61   | 33.84           | 83,976  | 1.26     | 80.62   |\\n| es       | 31,782   | 3,985.67 | 127.76          | 127,237 | 1.63     | 67.08   |\\n| ru       | 36,654   | 881.56   | 36.53           | 129,075 | 1.34     | 81.13   |\\n| en       | 249,696  | 677.96   | 23.05           | 730,12  | 1.72     | 51.28   |\\n| Total    | 542,134  | 1,219.77 | 50.37           | 95,914.05 | 1.53 | 57.98  |\\n\\n\"A.tk\" denotes the average number of tokens in a document for a given language, \"Avg. Sent\" denotes the average number of sentences in the document for a given language, \"# Gold Key\" denotes the number of gold keywords in the whole corpus for a language, and \"A. K. tk\" denotes the average number of tokens in given keywords for a language, and \"Abs. Go.%\" denotes the percent of tokens in keywords absent from the input text.\\n\\nLanguages follow language codes defined by the ISO 639-1 standard.\\n\\nRegions in various languages. We have selected 20 such languages with different language roots, targeting many regions worldwide and many under-researched languages.\\n\\nObtaining Articles: For every language, we collect links to articles from publicly available corresponding BBC Twitter accounts. To extend the data, we scrape 6 valid links obtained from the parsed articles for each language. We use Scrapy as our primary tool for crawling news articles and obtaining chunks of data with identification labels.\\n\\nSelecting Keywords: We select articles in BBC News which have keywords associated with them. Further to validate the quality of keywords, we manually verify 100 instances of randomly selected articles against the given keywords in English, Hindi, and Bengali languages confirming their correctness. We assume that this validation results held also for other languages in our corpus due to the uniformity across BBC News for all languages.\\n\\nOrdering Data: These chunks are processed further in Python for ordering and clustering tasks. All chunks are ordered using the identification tags assigned to them during scraping, and elements of articles are accumulated in a hash map based on their identification tags. We design the hash map with tags to optimally obtain specific modalities. The same structure is further written in a JSON file to be ready for use with a dedicated parser designed for our JSON structure.\\n\\nFinal Data: The final dataset consists of text documents in 20 languages saved in JSON format and a parser to access various modalities from each document, with every article having keyword(s) in the corresponding language for that document.\\n\\n3.2. Analysis of Dataset Features\\n\\nMAKED spans over 20 languages, and contains over 540K documents. Within this corpus, English (en) accounts for 250K documents. The dataset contains documents in 6 languages which have more than 20,000 instances, while the smallest corpora consists of 2,590 instances (Sinhala (si) language). We also did a survey on the number of available keyword extraction datasets for each language from various sources including Papers With Code, Metatext, Kaggle, and investigating top results from Web search engines. The survey containing the detailed statistics (no. of speakers, language family, no. of existing datasets) for each language in our corpus can be found in Appendix A.\\n\\nThis is also evident from the uniform placement of keywords across various languages a BBC News article webpage. The parser can be found in our project repository at https://github.com/zenquiorra/MAKED.\"}"}
{"id": "lrec-2022-1-664", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Domains covered in the English (en) corpus of MAKED has a temporal span of half a decade for most languages, while some of the languages span for over a decade.\\n\\nFigure 8. shows the yearly density of articles published for every language in our corpus.\\n\\nTo explore the diversity further, we analyze the domains span of our dataset. To investigate the documents' distribution over corresponding domains, we manually annotate a set of randomly chosen 1000 instances from the English (en) corpus. We classify these sample documents into 8 categories: \\\"Sports\\\", \\\"Business & Finance\\\", \\\"Food & Travel\\\", \\\"Entertainment\\\", \\\"Science & Tech\\\", \\\"Politics\\\", \\\"Health & Medicine\\\" and \\\"Education\\\". We keep an extra category labeled \\\"Others\\\" for domains that do not belong to one of the 8 above categories. The results of this study can be found in Figure 2.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nWe do a train-test-validation split with a ratio of 80:10:10 for every language in our dataset. To obtain unbiased results across different languages, we combine publicly available tokenizers and sentence segmenters for multiple languages in a single package. We also define a set of rules for segmentation tasks by analyzing languages that have no such support in external packages.\\n\\nWe use segtok for certain Indo-European languages, IndicTokenize for Indian languages, fugashi for Japanese and chinese for Chinese. To analyze our data and run certain baselines, apart from tokenization and segmentation, we also obtain stop words from nltk. For languages not supported in nltk, we collect stop words available in spaCy repository.\\n\\n16 We restrict ourselves to the English language as it was understandable by all our annotators.\\n\\n17 The package can be found in the project repository https://github.com/zenquiorra/MAKED\\n\\n18 To reduce complexity and size of our implementation, we use only those external packages which offer most functionality.\\n\\n19 https://pypi.org/project/segtok/1.1.0/\\n\\n20 https://github.com/anoopkunchukuttan/indic_nlp_library\\n\\n21 https://pypi.org/project/fugashi/\\n\\n22 https://pypi.org/project/chinese/\\n\\n23 https://nltk.org\\n\\n24 https://github.com/explosion/spaCy/tree/master/spacy/lang\\n\\n4.2. Baselines\\n\\nWe evaluate the performance of various keyword extraction techniques on our dataset, including two statistical, two graph-based, and one semi-supervised technique. For statistical methods, we employ TF-IDF (Salton and Buckley, 1988) as it is one of the most basic statistical methods to capture the importance of words and is also a basis for many statistics based techniques for keyword extraction. We also evaluate the performance of YAKE (Campos et al., 2020) on our dataset - one of the most recent statistical methods that offers superior performance compared to other techniques.\\n\\nFor graph-based approaches, we employ TextRank (Mihalcea and Tarau, 2004) being one of the simplest and most well-known graph-based techniques, often serving as a basis for other graph-oriented techniques. We also explore MultiPartiteRank (Boudin, 2018) as its one of the most recent and better performing graph-based models.\\n\\nFinally, we explore a semi-supervised way to extract keywords using a multilingual embedding and a classification mechanism on top of it. We utilize the pretrained checkpoint mT5multilingualXLSum (Hasan et al., 2021) of the MT5 encoderdecoder model. We supplement this model to obtain keywords by using the KeyBERT (Grootendorst, 2020) package, which uses an embedding model and classifies words from the text document into keywords.\\n\\n4.2.1. Statistical Approaches\\n\\nYAKE!: YAKE! (Campos et al., 2020) is an automatic keyword extraction technique that utilizes multiple statistical features from the text to assign scores to words and phrases. It ranks the candidates to obtain keywords for a given text.\\n\\nTF-IDF: Term frequency inverse-document frequency (Salton and Buckley, 1988), is a statistical measure that determines how important a word is within a document given a collection. TF determines how often a word occurs in a document, and IDF determines the significance of a word, given a corpus. Words with high TF-IDF scores are considered as candidates for keywords.\\n\\n4.2.2. Graph-based Approaches\\n\\nTextRank: TextRank (Mihalcea and Tarau, 2004) is a graph-based keyword extraction technique. It utilizes the structure of the text, taking the co-occurrence of words into account to create a graph structure, and then it further determines keyphrases that are the most central to the target document.\\n\\nMultiPartiteRank (MPR): MultiPartiteRank (Boudin, 2018) is a graph-based keyword extraction technique; it utilizes a multipartite graph structure to represent candidates and topics within a single graph and exploits relationships to improve candidate selection. It determines keyphrase preference using a novel selection mechanism.\\n\\n4.2.3. Semi-Supervised Approach\\n\\nWe denote the semi-supervised approach we study here as MT5 throughout the paper. In this approach, we utilize a...\"}"}
{"id": "lrec-2022-1-664", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pre-trained model mT5 which is a multi-lingual model pre-trained on the XL-Sum (Hasan et al., 2021) dataset. Note that, this dataset obtained its data from the BBC News domain, same source as ours. We pass this model along with a text document inside the publicly available KeyBERT (Groothuis, 2020) package which utilizes cosine similarity based on embeddings to obtain keywords.\\n\\n5. Results and Discussion\\n\\n5.1. Evaluation Metrics\\n\\nWe use the following metrics to evaluate the performance of tested models:\\n\\n\u2022 We generate a confusion matrix based on predicted keywords and gold keywords based on keyword overlap. We do not segment keywords containing more than one word and consider the actual match to evaluate recall scores. We denote this metric by \u201cem-R\u201d (exact match Recall).\\n\\n\u2022 Levenshtein Distance: We use the Levenshtein distance (Levenshtein and others, 1966) to evaluate edit distance between gold keywords and predicted keywords by taking an average of top-n scores as discussed further. We normalize the Levenshtein Distance between 0 and 1 by dividing the distance by the maximum length of the two compared strings. We then subtract it from 1 to obtain a similarity score; a perfect match implies a Levenshtein distance similarity score of 1. We denote this metric by \u201clev\u201d.\\n\\n\u2022 Jaro-Winkler Distance: We use the metric proposed by Winkler (1990) to evaluate the similarity between two gold and extracted keywords; we employ it similar to the way we did with Levenshtein distance similarity. To compute this similarity, we subtract the distance from 1, to have an even comparison across all metrics such that a perfect match implies a Jaro-Winkler similarity score of 1. We denote this metric by \u201cjar\u201d.\\n\\nWe analyze the performance of the baselines on the MAKED dataset using these metrics over the top 5, top 10, and top 15 extracted keywords against the gold keywords. The first metric relies on the actual match and captures the performance of baselines across all languages; however, for many languages, the ground-truth keywords cannot be verbatim located in the input document. This can be observed for Indic Languages such as Hindi (hi) and Tamil (ta), for which the \u201cem-R\u201d scores (refer to top of Table 3) are considerably lower across all baselines compared to other languages like English (en). Note that we have not implemented stemming for evaluation by the exact match. Extracted keywords can be composed of multiple words depending on the model used. Hence, \u201cem-R\u201d alone is insufficient to evaluate the performance of a baseline across various languages. To capture the semantics, we employ Levenshtein and Jaro-Winkler distance as our metrics. We assume that, since these two metrics rely on the edit distance between two strings, that any two words having a lower edit distance (having similar terms in them) will imply that the words are likely similar and hence semantically close. Jaro-Winkler additionally puts more weight on matching prefix, which further enhances the evaluation of extracted keywords where the representation is different. In regards to this, employing these similarity metrics, we observe that languages for which \u201cem-R\u201d scores are extremely low can still capture some meaning, as shown in Table 3.\\n\\n5.2. Top n scores\\n\\nAs we increase the number of extracted keywords and evaluate the exact match, in general, there is a rise in \u201cem-R\u201d scores, sometimes by a large magnitude. This is intuitive since the increase in \\\\( n \\\\) increases the sample size of extracted keywords to be matched against the gold keywords. Overall, we do not observe significant change in the Levenshtein scores and Jaro-Winkler scores as we increase the size of \\\\( n \\\\). For some cases, we even observe a drop in scores (e.g. the Jaro-Winkler score for TextRank on ja dataset drops from 0.025 to 0.022 when \\\\( n \\\\) is changed from 5 to 10 as can be seen in the bottom section of Table 3). This may imply that the corresponding baseline has extracted semantically best possible keywords for smaller \\\\( n \\\\) values, but it may still give a better \u201cem-R\u201d score if an exact match is found further. The significantly low MPR scores for some of the languages can be explained by the lack of parts of speech available in the nltk toolkit, which is implemented by many of the packages used for executing our baselines. Moving from top-5 to top-10 extracted keywords changes the trend for the best performing baseline across different metrics. However, this depends on multiple factors, including the average number of sentences within the corpus for a given language and the number of gold keywords. We further discuss other such features in Section 8..\\n\\n6. Sources of Errors\\n\\nWe discuss the potential sources of errors in our results in three broad categories:\\n\\n1. Software induced errors: We utilize various publicly available packages for analysis and technique evaluation. A major part of the experimentation section includes techniques like Tokenization, Segmentation, Parts of Speech tagging, Stop Words filtering, etc. which are implemented in multiple layers within such packages. We have attempted to make the baselines compatible and uniform across languages. However, even these systems are not perfect, and some error could be credited to such technical inadequacies.\\n\\n2. Author/Editor Bias in articles: The scraped dataset consists of manually written keywords for news articles, and hence some human error could have been inculcated in the process. For example, in the document presented in the case study (refer to Fig. 3), another potential keyword could have been \u201cBrexit\u201d, however, Levenshtein distance between the word \u201cbooking\u201d and \u201cbook\u201d is 3, while between \u201cbooking\u201d and \u201cback\u201d is 5. \u201cBooking\u201d and \u201cbook\u201d are semantically closer compared to \u201cback\u201d\"}"}
{"id": "lrec-2022-1-664", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Baselines | YAKE! | TF-IDF | TextRank | MPR | MT5 |\\n|-----------|-------|--------|----------|-----|-----|\\n| Languages | Top 5 | Top 10 | Top 15 | Top 5 | Top 10 | Top 15 |\\n| si        | 0.002 | 0.002  | 0.002   | 0.002 | 0.002 | 0.002 |\\n| pt        | 0.123 | 0.177  | 0.208   | 0.121 | 0.177 | 0.177 |\\n| fr        | 0.077 | 0.121  | 0.149   | 0.100 | 0.133 | 0.133 |\\n| ja        | 0.145 | 0.190  | 0.211   | 0.020 | 0.029 | 0.029 |\\n| ps        | 0.096 | 0.142  | 0.172   | 0.151 | 0.198 | 0.198 |\\n| ne        | 0.000 | 0.000  | 0.000   | 0.000 | 0.000 | 0.000 |\\n| pa        | 0.001 | 0.002  | 0.002   | 0.001 | 0.002 | 0.002 |\\n| gu        | 0.000 | 0.000  | 0.000   | 0.000 | 0.000 | 0.000 |\\n| zh        | 0.051 | 0.058  | 0.061   | 0.038 | 0.046 | 0.046 |\\n| bn        | 0.000 | 0.000  | 0.000   | 0.000 | 0.001 | 0.000 |\\n| id        | 0.152 | 0.223  | 0.268   | 0.192 | 0.251 | 0.251 |\\n| te        | 0.001 | 0.001  | 0.001   | 0.001 | 0.001 | 0.001 |\\n| mr        | 0.001 | 0.002  | 0.002   | 0.002 | 0.002 | 0.002 |\\n| ur        | 0.005 | 0.008  | 0.009   | 0.100 | 0.162 | 0.162 |\\n| ta        | 0.000 | 0.000  | 0.000   | 0.000 | 0.000 | 0.000 |\\n| hi        | 0.004 | 0.004  | 0.004   | 0.003 | 0.004 | 0.004 |\\n| uk        | 0.011 | 0.026  | 0.039   | 0.046 | 0.069 | 0.069 |\\n| es        | 0.098 | 0.145  | 0.175   | 0.121 | 0.156 | 0.156 |\\n| ru        | 0.175 | 0.326  | 0.436   | 0.049 | 0.074 | 0.074 |\\n| en        | 0.086 | 0.146  | 0.196   | 0.157 | 0.218 | 0.218 |\\n\\nTable 3: Performance of various baselines against the MAKED dataset. Top table presents the exact match recall (em-) scores, middle table shows the Levenshtein Distance Similarity and bottom table displays the Jaro-Winkler similarity scores. Top 5, Top 10, and Top 15 refer to the Top n number of extracted keywords from the corresponding baselines taken into consideration for each evaluation metric.\"}"}
{"id": "lrec-2022-1-664", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Intel not considering UK chip factory after Brexit\\n\\nPat Gelsinger told the BBC that before the UK left the EU, the country \u201cwould have been a site that we would have considered\u201d. But he added: \u201cPost-Brexit... we\u2019re looking at EU countries and getting support from the EU\u201d. Intel wants to boost its output amid a global chip shortage that has hit the supply of cars and other goods. The firm \u2013 which is one of the world\u2019s largest makers of semiconductors \u2013 says the crisis has shown that the US and Europe are too reliant on Asia for its chip-making needs. Intel is investing up to $95bn (\u00a370bn) on opening and upgrading semiconductor plants in Europe over the next 10 years, as well as boosting its US output. But while Mr Gelsinger said the firm \u201cabsolutely would have been seeking sites for consideration\u201d in the UK, he said Brexit had changed this. \u201cI have no idea whether we would have had a superior site from the UK,\u201d he said. \u201cBut we now have about 70 proposals for sites across Europe from maybe 10 different countries. \u201cWe\u2019re hopeful that we\u2019ll get to agreement on a site, as well as support from the EU... before the end of this year.\u201d Microchips are vital components in millions of products from cars to washing machines, but they have been in short supply this year due to surging demand and supply chain issues. It has led to shortages of popular goods like cars and computers and driven up prices - issues Mr Gelsinger said were set to continue into Christmas. \u201cThere is some possibility that there may be a few IOUs under the Christmas trees around the world this year,\u201d he said. \u201cJust everything is short right now. And even as I and my peers in the industry are working like crazy to catch up, it\u2019s going to be a while.\u201d He said things would \u201cincrementally\u201d improve next year but were unlikely to stabilise until 2023. Nobody should be too dependent.\\n\\nIntel's expansion comes as the overall market for semi-conductors is set more than double in the next seven years to around $800bn. The firm also hopes to secure subsidies from US and European politicians, who feel their reliance on Asia for chips could threaten national security. Today the US only produces around 12% of the world's semiconductors, while Korea's Samsung and Taiwan Semiconductor Manufacturing Company (TSMC) account for 70% of global supply. \u201cIt is clearly part of the motivation of a globally balanced supply chain that nobody should be too dependent on somebody else,\u201d Mr Gelsinger told the BBC. Intel will continue outsourcing some of its chip-making but eventually hopes to make most of its products in-house. Competing won\u2019t be easy, though. Chip-making is still far cheaper in Asia and Intel\u2019s rivals continue to expand. TSMC, the world's largest contract maker of semi-conductors, will spend $100bn on increasing capacity over the next three years while Samsung invests $205bn. Mr Gelsinger said he is confident Intel can still regain its leading edge. \u201cThis is an industry that we created in the US, Intel's the company that puts silicon into Silicon Valley,\u201d he said. \u201cBut we realise these are good companies, they're well capitalised, they're investing, they're innovating together. So we have to re-earn that right of unquestioned leadership.\u201d\"}"}
{"id": "lrec-2022-1-664", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cent of keywords that are not verbatim present in our text (\\\"abs\\\\verb+gold\\\\\"), the average number of tokens in the keywords (\\\"avg\\\\verb+tok\\\\k\\\"), the average number of sentences (\\\"avg\\\\verb+sent\\\\\\\") and the average number of tokens in the text (\\\"avg\\\\verb+tok\\\\\\\") for all languages.\\n\\nFigure 4: Correlation of various baselines for the top-15 exact match Recall scores with different features.\\n\\nFigure 5: Correlation of various baselines for the top-15 Levenshtein scores with different features.\\n\\nWe use the top-15 \\\"em-R\\\" scores and \\\"lev\\\" scores, which refers to exact match recall and Levenshtein distance respectively. The correlation was calculated using the Pearson correlation.\\n\\nFor both cases, we observe that all baselines are negatively correlated with the average number of tokens; as the average number of tokens increases, the extraction tasks get difficult, given that the number of \\\"candidate keywords\\\" also increases. This is more prominent in the case of statistical approaches since they are affected by the frequency of terms occurring within a document, as indicated in the Fig. 4 and Fig. 5.\\n\\nWe also observe a performance drop across all five baselines with an increase in the average number of sentences within a document. This drop is even more prevalent for the semantic metrics (Fig. 5) since similar meaning can be conveyed by multiple sentences in larger documents.\\n\\nWe don't observe any significant correlation between the average number of tokens within a keyword. It can also be noted that TextRank negatively correlates with the number of absent gold tokens for the exact match recall score (refer to Table 4), while it correlates exceptionally well for the Levenshtein scores. We believe that the tendency of Text Rank to generate bi-grams/tri-grams over uni-grams can explain this phenomenon to some extent. The model implementation only considers co-occurrence among the input document words, making it a simple model (refer to Section 7. for the case study).\\n\\n9. Dataset Applications\\n\\nMAKED is primarily designed for developing techniques for automatic keyword extraction and technique evaluation. It can be also be used in the following ways:\\n\\n1. Development of large deep learning frameworks: MAKED can be used for training and development of deep neural networks for keyword extraction, utilizing the size of the corpus and the diversity of the topics covered within the news domain.\\n\\n2. Word Embeddings: Leveraging the size of data available in MAKED, the dataset can be used to develop large-scale word embeddings for low resource languages. Since it also captures temporal data of over a decade, creating word embeddings on this data is likely to generalize different writing styles and capture changes in words over time.\\n\\n3. Parallel Model: The dataset consists of a cross-lingual corpus of Japanese-English (\\\\texttt{ja-en}) document pairs. It can then be utilized to create cross-lingual embeddings to further the development of neural networks based cross-lingual keyword extraction.\\n\\n4. Multi-modal keyword extraction: The dataset can be used to explore and motivate multi-modal keyword extraction. Note that due to the lack of existing works in multi-modal keyword extraction, we cannot provide any baselines for it. However, we plan to explore this as a problem in our future work.\\n\\n10. Conclusion\\n\\nIn this work, we present the largest multi-lingual keyword extraction dataset in 20 languages, as well as a cross-lingual dataset for a pair of languages obtained from the British Broadcasting Corporation (BBC News). We study the performance of various techniques on our dataset and report their results. In future works, we plan to explore various neural based models for keyword extraction on our dataset with the motivation of developing new word embeddings for low resource languages.\"}"}
{"id": "lrec-2022-1-664", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"11. Bibliographical References\\n\\nBhowmik, R. (2008). Keyword extraction from abstracts and titles. In IEEE SoutheastCon 2008, pages 610\u2013617. IEEE.\\n\\nBoudin, F. (2018). Unsupervised keyphrase extraction with multipartite graphs. arXiv preprint arXiv:1803.08721.\\n\\nBougouin, A., Boudin, F., and Daille, B. (2013). Topicrank: Graph-based topic ranking for keyphrase extraction. In International joint conference on natural language processing (IJCNLP), pages 543\u2013551.\\n\\nCampos, R., Mangaravite, V., Pasquali, A., Jorge, A., Nunes, C., and Jatowt, A. (2020). Yake! keyword extraction from single documents using multiple local features. Information Sciences, 509:257\u2013289.\\n\\nGollapalli, S. D. and Caragea, C. (2014). Extracting keyphrases from research papers using citation networks. In Twenty-eighth AAAI conference on artificial intelligence.\\n\\nGrootendorst, M. (2020). Keybert: Minimal keyword extraction with bert.\\n\\nHan, J., Kim, T., and Choi, J. (2007). Web document clustering by using automatic keyphrase extraction. In 2007 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology-Workshops, pages 56\u201359. IEEE.\\n\\nHasan, T., Bhattacharjee, A., Islam, M. S., Mubasshir, K., Li, Y.-F., Kang, Y.-B., Rahman, M. S., and Shahriyar, R. (2021). XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4693\u20134703, Online, August. Association for Computational Linguistics.\\n\\nHulth, A. (2003). Improved automatic keyword extraction given more linguistic knowledge. In Proceedings of the 2003 conference on Empirical methods in natural language processing, pages 216\u2013223.\\n\\nKrapivin, M., Autaeu, A., and Marchese, M. (2009). Large dataset for keyphrases extraction.\\n\\nLe, T. N. T. (2015). Study on language processing methods for supporting understanding and using multiple legal documents.\\n\\nLevenshtein, V. I. et al. (1966). Binary codes capable of correcting deletions, insertions, and reversals. In Soviet physics doklady, volume 10, pages 707\u2013710. Soviet Union.\\n\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2019). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. arXiv preprint arXiv:1910.13461.\\n\\nLitvak, M. and Last, M. (2008). Graph-based keyword extraction for single-document summarization. In Coling 2008: Proceedings of the workshop Multi-source Multilingual Information Extraction and Summarization, pages 17\u201324.\\n\\nMarujo, L., Gershman, A., Carbonell, J., Frederking, R., and Neto, J. P. (2013). Supervised topical key phrase extraction of news stories using crowdsourcing, light filtering and co-reference normalization. arXiv preprint arXiv:1306.4886.\\n\\nMedelyan, O. and Witten, I. H. (2008). Domain-independent automatic keyphrase indexing with small training sets. Journal of the American Society for Information Science and Technology, 59(7):1026\u20131040.\\n\\nMeng, R., Zhao, S., Han, S., He, D., Brusilovsky, P., and Chi, Y. (2017). Deep keyphrase generation. arXiv preprint arXiv:1704.06879.\\n\\nMihalcea, R. and Tarau, P. (2004). Textrank: Bringing order into text. In Proceedings of the 2004 conference on empirical methods in natural language processing, pages 404\u2013411.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2019). Exploring the limits of transfer learning with a unified text-to-text transformer. arXiv preprint arXiv:1910.10683.\\n\\nSalton, G. and Buckley, C. (1988). Term-weighting approaches in automatic text retrieval. Information processing & management, 24(5):513\u2013523.\\n\\nSchutz, A. T. et al. (2008). Keyphrase extraction from single documents in the open domain exploiting linguistic and statistical methods. M. App. Sc Thesis.\\n\\nWinkler, W. E. (1990). String comparator metrics and enhanced decision rules in the fellegi-sunter model of record linkage.\\n\\nZhang, J., Zhao, Y., Saleh, M., and Liu, P. (2020). Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328\u201311339. PMLR.\"}"}
{"id": "lrec-2022-1-664", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We perform a literature survey for keyword extraction datasets, covering up to 20 languages present in the MAKED dataset. We also collect other relevant information like the total number of speakers across the globe and the Family for each of these languages. We used visualcapitalist which has data sourced from Ethnologue to obtain the total number of speakers and the parent in language family tree for each of these 20 languages. We found that there are no existing keyword extraction datasets for 11 out of the 20 languages proposed in our dataset. For example, there is no dedicated keyword extraction dataset for Dravidian languages. The one that exists for Telugu (te) consists of news article snippets where the headlines are considered gold keywords.\\n\\n| Language | Language Code | No. Speakers | Family | No. Datasets |\\n|----------|---------------|--------------|--------|-------------|\\n| English  | en            | 1,132M       | Indo-European | 26          |\\n| Chinese  | zh            | 1,117M       | Sino-Tibetan | 1           |\\n| Hindi    | hi            | 615M         | Indo-European | 0           |\\n| Spanish  | es            | 534M         | Indo-European | 2           |\\n| French   | fr            | 280M         | Indo-European | 3           |\\n| Bengali  | bn            | 265M         | Indo-European | 2           |\\n| Russian  | ru            | 258M         | Indo-European | 1           |\\n| Portuguese | pt           | 234M         | Indo-European | 1           |\\n| Indonesian | id           | 199M         | Austronesian | 1           |\\n| Urdu     | ur            | 170M         | Indo-European | 0           |\\n| Japanese | ja            | 128M         | Japanic    | 0           |\\n| Marathi  | mr            | 95M          | Indo-European | 0           |\\n| Telugu   | te            | 93M          | Dravidan   | 1           |\\n| Tamil    | ta            | 81M          | Dravidan   | 0           |\\n| Gujarati | gu            | 61M          | Indo-European | 0           |\\n| Ukrainian| pa            | 33M          | Indo-European | 0           |\\n| Punjabi  | pa            | 33M          | Indo-European | 0           |\\n| Nepali   | ne            | 25M          | Indo-European | 0           |\\n| Pashto   | pa            | 21M          | Indo-European | 0           |\\n| Sinhala  | si            | 17M          | Indo-European | 0           |\\n\\nTable 4: Survey of available datasets in comparison with the number of speakers.\\n\\nFor the English (en) language, the only large-scale dataset available belongs to the academic domain (Meng et al., 2017), where abstracts are typically considered as the input text. However, Bhowmik (2008) showed that using just an abstract is not sufficient for keyword extraction, making the existing datasets insufficient either in terms of quality or quantity. The rest of the datasets for keyword extraction in the English language are several magnitudes smaller in scale.\\n\\nWe note that roughly 80% languages covered in MAKED are low resource languages, even though some of them belong to the top-10/20 most spoken languages in the world. On the other hand, MAKED also includes some languages which are among the less spoken languages globally and are not yet explored for keyword extraction tasks.\\n\\n30 https://www.visualcapitalist.com/100-most-spoken-languages/\\n31 Obtained from 22nd edition published in the year 2019 https://www.ethnologue.com/world.\"}"}
