{"id": "acl-2023-long-593", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWhen communicating with elders with cognitive impairment, cognitive stimulation (CS) help to maintain the cognitive health of elders. Data sparsity is the main challenge in building CS-based dialogue systems, particularly in the Chinese language. To fill this gap, we construct a Chinese CS conversation (CSConv) dataset, which contains about 2.6K groups of dialogues with therapy principles and emotional support strategy labels. Making chit chat while providing emotional support is overlooked by the majority of existing cognitive dialogue systems. In this paper, we propose a multi-source knowledge fusion method for CS dialogue (CSD), to generate open-ended responses guided by the therapy principle and emotional support strategy. We first use a progressive mask method based on external knowledge to learn encoders as effective classifiers, which is the prerequisite to predict the therapy principle and emotional support strategy of the target response. Then a decoder interacts with the perceived therapy principle and emotional support strategy to generate responses. Extensive experiments conducted on the CSConv dataset demonstrate the effectiveness of the proposed method, while there is still a large space for improvement compared to human performance.\\n\\n1 Introduction\\n\\nDialogue systems have enjoyed rapid progress in recent years, through communication with humans to satisfy diverse needs (Liu et al., 2021; Kann et al., 2022). Cognition stimulation of elders is a critical psychological therapy where dialogue systems serve as effective tools for restoring the cognition of older adults (De Oliveira et al., 2014; Park et al., 2019; Tokunaga et al., 2021). Some studies have shown that chit-chat can help older people with cognitive restoration (van Rijn et al., 2010; Garcia, 2022). Meanwhile, several studies have shown that emotional support is beneficial for maintaining or even increasing cognitive function in elders (Ellwardt et al., 2013; Liu et al., 2020; Sharma et al., 2020). Nonetheless, there remains an open question on how to introduce emotional support and therapy principles simultaneously into chit-chat dialogue systems to provide cognitive recovery training for elders with cognitive impairment.\\n\\nOne main obstacle to building cognitive dialogue is the lack of training corpora, especially in the Chinese language. Therefore, we first construct a Chinese CS Conversation (CSConv) dataset, containing about 2.6K groups of dialogue data where each utterance is annotated with three types of labels, i.e., therapy principle, emotional labels, and emotional support strategy labels. To generate open-ended responses with emotional support strategies, we propose a multi-source knowledge fusion method for CS dialogue (CSD), to generate open-ended responses guided by the therapy principle and emotional support strategy. We first use a progressive mask method based on external knowledge to learn encoders as effective classifiers, which is the prerequisite to predict the therapy principle and emotional support strategy of the target response. Then a decoder interacts with the perceived therapy principle and emotional support strategy to generate responses. Extensive experiments conducted on the CSConv dataset demonstrate the effectiveness of the proposed method, while there is still a large space for improvement compared to human performance.\"}"}
{"id": "acl-2023-long-593", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we propose a multi-source knowledge fusion in a Chinese CS Dialogue (CSD) system. We use Jiagu2, a Chinese NLP toolkit, to extract emotional words and keywords to form knowledge source and progressively mask the extracted knowledge on the encoder side, to increase the generalizability of the model. Meanwhile, we adopt Chinese EmoBank (Lee et al., 2022) to calculate the weight value of each word in the utterance, so that the model pays more attention to words with high values. By introducing multiple sources of external knowledge, we greatly enrich the content of the conversation. Moreover, we judge the content and emotions that elders express which is critical to generate satisfied responses, matching them with the cognitive therapeutic principles, and coming up with corresponding supporting strategies. At last, we design a multi-source interactive mechanism so that emotional support strategies and cognitive stimulation therapies can be reasonably combined to generate responses benefiting to mental health. Figure 1 shows an example of a conversation with an elder based on the therapy principle.\\n\\nIn summary, our contributions are as follows: (1) We construct a Chinese CS-based conversation dataset to facilitate the following research; (2) We propose a progressive mask method for encoder modules, which enhances the generalizability on emotional knowledge and the applicability of the therapeutic conversations with elders; (3) We design a multi-source interactive method to model the interaction among encoder modules, decoder modules and external knowledge; (4) We conduct extensive experiments to demonstrate the effectiveness of the proposed CSD.\\n\\n2 Dataset\\n\\n2.1 Data Collection\\n\\nThere is no publicly available CS-based Chinese conversation dataset to enable a cognitively restorative dialogue system for elders with cognitive impairment. We introduce a Chinese one-to-one open-domain CS Conversation dataset, (CSConv), which is collected and created via cognitive stimulation therapy videos and handbook, and the ratio of conversation data from videos to those from the handbook is approximately 2:1.\\n\\nAs high-quality conversation examples are needed for building Chinese CS-based dialogue system, our efforts include the following. (1) The videos are Cantonese. We first translate the Cantonese conversations in the videos into Mandarin Chinese, in a format suitable for CS model training. (2) We make Mandarin conversations artificially based on the eighteen therapy principles in the handbook. (3) We clean the dataset based on rules (e.g., truncating excessively long utterances, removing the multiple consecutive symbols in the utterance). (4) We manually annotate whether each utterance is spoken by the SPEAKER or the LISTENER (SPEAKER for elder, LISTENER for smart speaker or health care worker). (5) We use BERT-based text classification models to annotate the emotion label, strategy label, therapy label of each utterance, and then conduct manual review and modification. (6) All the data are professionally produced and reviewed twice. (7) We test our CSConv dataset on some text classification models and text generation models, which can directly reflect the performance differences between models.\\n\\n| Therapy Labels | Interpretation |\\n|----------------|----------------|\\n| None           | Neutral        |\\n| Neutral        | Inquiry        |\\n| None           | Respect        |\\n| None           | Reminiscence   |\\n| None           | Expression     |\\n| None           | Enjoyment      |\\n| None           | Comfort        |\\n\\nTable 1: Therapy Labels and their interpretation.\"}"}
{"id": "acl-2023-long-593", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gestions, Information, Others), which are similar to\\nthe strategies in (Liu et al., 2021). There are seven\\ntypes of therapy labels. Table 1 shows the name of\\nexplanation of each therapy label.\\n\\n2.2 Data Statistics\\nStatistics of the CSConv dataset are given in Ta-\\nble 2. The number and proportion of therapy labels,\\nemotion labels and strategy labels are shown in\\nTable 3.\\n\\n| Categories         | Number | Proportion |\\n|--------------------|--------|------------|\\n| Conversations      | 2643   |            |\\n| Utterances         | 16845  |            |\\n| SPEAKER Utterances | 8363   | 31.44      |\\n| LISTENER Utterances| 8480   | 24.67      |\\n| Average token per conversation | 60.39 | |\\n| Average utterance per conversation | 6.37 | |\\n| Average token per utterance | 9.48 | |\\n\\nTable 2: Statistics of the CSConv dataset.\\n\\n| Therapy Labels | Number | Proportion |\\n|----------------|--------|------------|\\n| None           | 5296   | 31.44      |\\n| Inquiry        | 4156   | 24.67      |\\n| Respect        | 2134   | 12.70      |\\n| Reminiscence   | 464    | 2.76       |\\n| Expression     | 2651   | 15.74      |\\n| Enjoyment      | 1862   | 11.05      |\\n| Comfort        | 281    | 1.67       |\\n\\nTable 3: Number and proportion of therapy, emotion,\\nstrategy labels.\\n\\n3 Method\\n3.1 Overview\\nFigure 2 gives an overview of our Chinese CSD\\narchitecture, which consists of two stages: (1) Pro-\\ngressive mask encoder; (2) Multi-source interactive\\ndecoder. The first stage is divided into two mod-\\nules: progressive mask encoder for context training\\nand encoders for text classification.\\n\\n3.2 Progressive Mask Encoder\\nProgressive Mask Encoder for Context Train-\\ning.\\nLike the traditional BERT pre-training task,\\nin order to better represent information of the ut-\\nterances and evaluate the Next Sentence Prediction\\n(NSP) task, the utterances of the SPEAKER and\\nLISTENER are used to generate three types of\\nembeddings (Vaswani et al., 2017), namely word\\nembedding, position embedding and segment em-\\nbedding.\\n\\nDuring training, the encoder randomly masks\\ntokens to improve generalizability. We first use\\nJiagu's sentiment analysis function to extract en-\\ntities (i.e., one and multiple words) and sentences\\nwith positive or negative values generated by Ji-\\nagu greater than the $\\\\lambda_{emo}$ threshold, and Jiagu's\\nkeyword extraction function to extract keywords in\\nthe utterances. Eventually, emotion and keyword\\ndictionaries are constructed. Through the emotion\\nand keyword dictionaries, the data during training\\nis masked in pre-defined proportions. As the train-\\ning progresses, the span of a single mask gradually\\nincreases (i.e., from one word to multiple words,\\nand finally to a sentence), the ratios of masking\\none-word entities, two-word entities, three-word\\nentities, four-word entities and sentences are\\n$\\\\lambda_1$, $\\\\lambda_2$, $\\\\lambda_3$, $\\\\lambda_4$ and $\\\\lambda_5$, respectively. In order to further\\nimprove the encoder's generalization through the\\nprogressive mask method, we retain a certain pro-\\nportion of the traditional BERT mask method. To\\nbe more specific, 5% of the entities in the utterances\\nare randomly masked, of which 80% proceed mask\\nprocessing, 10% proceed random replacement pro-\\ncessing, and 10% remain unchanged.\\n\\nAfter the progressive mask operation, encoders\\nare used to encode context information for the ut-\\nterances (i.e., context learning) and finally the pre-\\ntrained models are obtained.\\n\\nEncoders of context training based on the emo-\\ntion dictionary are used for utterance emotion clas-\\nsification. Encoders based on the keyword dictio-\\nnary are used to classify the therapy principle and\\nsupport strategy of the utterances.\\n\\nEncoders for Text Classification\\nA multi-\\nturn dialogue context consists of $M$ utterances\\nemitted by SPEAKER and LISTENER in turn.\\nThe context $U_{ref}$ refers to the sequence of utterance,\"}"}
{"id": "acl-2023-long-593", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We take the therapy label $l_{cs}$, emotional label $l_{emo}$, and strategy label $l_{str}$ that encoder classification models generate as three tokens ($t_{emo}$, $t_{cs}$, $t_{str}$) and append them at the end of each utterance. We can then obtain decoder input tokens $Y = [y_1, ..., y_j, t_{emo}, t_{cs}, t_{str}]$. To represent sentences and knowledge, we first use a word embedding layer, a positional embedding layer to convert each token into vectors (Vaswani et al., 2017), i.e., $E_W(y_j) \\\\in \\\\mathbb{R}^d$, $E_P(y_j) \\\\in \\\\mathbb{R}^d$, where $d$ is the dimensionality of embeddings. $y_j$ is computed as follows:\\n\\n$$[y_1, ..., y_j, t_{emo}, t_{cs}, t_{str}]$$\\n\\nis the composition of two types of embeddings.\\n\\nCross-Attention Mechanism. We first train an extra encoder that flattens the input data (the format of the data is the same as that of the decoder input), and get the corresponding hidden states $h_{e,j}$:\\n\\n$$h_{e,j} = LN(y_{l-1,j} + MHAtt(y_{l-1,j}))$$\\n\\nIn order to more reasonably embed the representation of SPEAKER/LISTENER's utterances generated by encoders into the decoder through cross-attention mechanism, we extract the hidden states from the encoder classification models to replace the hidden states of the labels position ($h_{emo}$, $h_{cs}$, $h_{str}$) generated by extra encoder, forming new encoder hidden states embedded in the cross attention of decoder.\\n\\nAttention Loss. Since humans naturally pay extra attention to emotional support and therapy information during a conversation, we enforce an emotional attention loss and keyword attention loss in order to focus on those words with higher emotion intensity values and keyword intensity values. Emotional intensity values and keyword intensity values are obtained from Chinese Emobank and Jiagu, respectively.\\n\\nTo highlight emotional information, we compute emotion intensity values for dialogue words and\"}"}
{"id": "acl-2023-long-593", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\eta_{\\\\text{emo}}(y_j) = (V_a(y_j) + A_r(y_j)) - 2 \\\\times R_{\\\\text{min}} \\\\frac{R_{\\\\text{max}} - R_{\\\\text{min}}}{(5)} \\\\]\\n\\nwhere \\\\( V_a(y_j) \\\\) and \\\\( A_r(y_j) \\\\) denote the mean values of valence and arousal dimensions of word \\\\( y_j \\\\), respectively. \\\\( R_{\\\\text{min}} \\\\) and \\\\( R_{\\\\text{max}} \\\\) represent the minimal and maximal values of the value range, respectively.\\n\\nIf \\\\( y_j \\\\) is not in Chinese EmoBank, \\\\( \\\\eta_{\\\\text{emo}}(y_j) \\\\) will be set to 0.\\n\\nTo highlight keyword information, keyword intensity values for dialogue words \\\\( y_j \\\\) are used based on Jiagu's keyword extraction function:\\n\\\\[ \\\\eta_{\\\\text{kw}}(y_j) = \\\\text{softmax}(y_j) \\\\quad (6) \\\\]\\n\\nwhere the softmax operation calculates a probability for every word and the probabilities of all the words add up to one.\\n\\nEmotion loss \\\\( L_{\\\\text{emo}} \\\\) and keywords loss \\\\( L_{\\\\text{kw}} \\\\) are calculated using Mean Square Error (MSE).\\n\\\\[ L_{\\\\text{emo}} = \\\\frac{1}{e} \\\\times \\\\frac{1}{e} \\\\sum_{i=1}^{e} (\\\\eta_{\\\\text{emo}}(y_j) - a_j)^2 \\\\quad (7) \\\\]\\n\\\\[ L_{\\\\text{kw}} = \\\\frac{1}{e} \\\\times \\\\frac{1}{e} \\\\sum_{i=1}^{e} (\\\\eta_{\\\\text{kw}}(y_j) - a_j)^2 \\\\quad (8) \\\\]\\n\\nwhere \\\\( a_j \\\\) is the attention weight of each word in the utterance calculated by the attention output tensors.\\n\\nWhen the model generates the response, we use a sampling method to generate the next \\\\( j \\\\)-th token. Given \\\\( U \\\\) and tokens \\\\( t_{\\\\text{emo}}, t_{\\\\text{cs}}, t_{\\\\text{str}} \\\\), our multi-source interactive decoder aims to generate a \\\\( n \\\\)-length response \\\\( Y = \\\\{y_1, \\\\ldots, y_n\\\\} \\\\) through maximizing the probability\\n\\\\[ P(Y|U, t_{\\\\text{emo}}, t_{\\\\text{cs}}, t_{\\\\text{str}}) = \\\\prod_{N=1}^{N} P(y_n|y_{<n}, U, t_{\\\\text{emo}}, t_{\\\\text{cs}}, t_{\\\\text{str}}) \\\\]\\n\\nLike most dialogue generation tasks, standard maximum likelihood estimator (MLE) is used as the optimization objective:\\n\\\\[ L_{\\\\text{gen}} = -\\\\log(P(Y|U, t_{\\\\text{emo}}, t_{\\\\text{cs}}, t_{\\\\text{str}})) \\\\quad (9) \\\\]\\n\\nEventually, a joint loss function is defined to jointly minimize the emotion attention loss (Eq. 7), the keywords attention loss (Eq. 8) and the generation loss (Eq. 9) as follows:\\n\\\\[ L = \\\\gamma_1 \\\\times L_{\\\\text{gen}} + \\\\gamma_2 \\\\times L_{\\\\text{emo}} + \\\\gamma_3 \\\\times L_{\\\\text{kw}} \\\\quad (10) \\\\]\\n\\nwhere \\\\( \\\\gamma_1, \\\\gamma_2 \\\\) and \\\\( \\\\gamma_3 \\\\) are hyper-parameters.\\n\\n### 3.4 Training\\nWe divide training into three phases as follows: (1) Encoders are used for context training based on the progressive mask method. Two pre-trained encoder models are trained based on sentiment dictionary and keyword dictionary, respectively. (2) Therapy classification and strategy classification tasks are realized on the basis of the encoder trained according to the keyword dictionary. The task of emotion classification is realized based on the encoder trained according to the emotion dictionary. (3) We use the flatten data as the training data of the encoder, making the batch size and input data consistent with the decoder. Then the hidden state of the last layer of the encoder is interacted with the decoder through the cross attention mechanism.\\n\\n### 4 Experiments\\n\\n#### 4.1 Implementation Details\\nWe conduct experiments on the CSConv dataset. For the encoder module of the CSD, the pre-trained model is bert-base-chinese, and the decoder module is gpt2-chinese-cluecorpussmall (Du, 2019). Most of the hyperparameters are the same as those in decoder chitchat. In the progressive mask encoder trained based on the keyword dictionary, the ratios of masked entities and sentences (i.e., \\\\( \\\\lambda_1, \\\\lambda_2, \\\\lambda_3, \\\\lambda_4 \\\\) and \\\\( \\\\lambda_5 \\\\)) are set as 0.9, 0.9, 0.9, 0.9 and 0.4, respectively. Based on the emotion dictionary, \\\\( \\\\lambda_1, \\\\lambda_2, \\\\lambda_3, \\\\lambda_4 \\\\) and \\\\( \\\\lambda_5 \\\\) are set as 0.5, 0.5, 0.4, 0.3 and 0.2, respectively. Loss weights, namely \\\\( \\\\gamma_1, \\\\gamma_2 \\\\) and \\\\( \\\\gamma_3 \\\\), are set as 1, 0.5 and 0.5, respectively. We implement all models with PyTorch (Paszke et al., 2019) on four NVIDIA A100 GPUs, and train the models using AdamW optimizer (Loshchilov and Hutter, 2017) with a batch size of 4. We vary the learning rate during training following (Vaswani et al., 2017). For inference, we set the temperature as 0.7, top-k as 8 and top-p as 0.5. The training time for the encoder of the CSD is about 2 minutes and that for the decoder is about 33 minutes. In testing different models, we use NLTK packages to compute the Bleu metric and bert-score package to compute BERTScore. We set the smooth function of NLTK to method 7, and the model used in computing the bert-score is bert-base-chinese.\\n\\n4https://huggingface.co/bert-base-chinese\\n5https://github.com/yangjianxin1/GPT2-chitchat\"}"}
{"id": "acl-2023-long-593", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Automatic Evaluation\\nFor encoder classification, to evaluate the model at the emotional level, we adopt **Emotion Accuracy** as the evaluation metric between the ground truth emotion labels and the predicted emotion labels. **Therapy Accuracy** and **Strategy Accuracy** are similar evaluation metrics to emotion accuracy.\\n\\nFor decoder generation, we employ **BLEU** (Papineni et al., 2002), an algorithm for evaluating the text quality, as the metric. Since BLEU cannot perfectly reflect the quality of generated results (Liu et al., 2016), we adopt **BERTScore** (Zhang et al., 2019a) to compare the similarity between embeddings of a generated sentence and the reference sentence. **Distinct-1** / **Distinct-2** (Li et al., 2016) is the proportion of the distinct uni / bi-grams in all the generated results, that indicate the diversity.\\n\\n4.3 Human Evaluation\\nTo qualitatively examine model performance, we also conduct human evaluations. We sample some dialogues from the CSD and the baselines. We find 6 elders and their relatives to evaluate the responses generated by different models. All models are evaluated in terms of **Empathy**, **Support** and **Fluency**. Empathy measures whether LISTENER understands SPEAKER's feelings. Support measures whether LISTENER gives SPEAKER reasonable help and comfort. Fluency measures the grammatical correctness and readability of the SPEAKER's responses. Each metric is rated on five-scale, where 1, 3 and 5 indicate unacceptable, moderate and excellent performance, respectively.\\n\\n4.4 Baselines for Comparison\\nWe conduct extensive experiments to compare the encoder module of the CSD against the following representative baselines: (1) **Transformer** (Vaswani et al., 2017): A transformer-based encoder-decoder model. (2) **BERT** (Kenton and Toutanova, 2019): BERT is a context-aware encoder, and is good at processing downstream tasks, like classification. (3) **BERT+CNN** 6: The model is the embedding with contextual meaning output by BERT, which is input into a CNN classifier for classification.\\n\\nWe conduct extensive experiments to compare the decoder generation module of CSD against the following representative baselines: (1) **CDialGPT-base** (Wang et al., 2020a): The model is a 12-layer GPT model trained on the LCCC-base dataset. (2) **CDialGPT-large** (Wang et al., 2020a): The model is a 12-layer GPT model trained on the LCCC-large dataset. (3) **GPT2-chitchat** 7: The model is a 10-layer GPT-2 trained on 500,000 chitchat corpus. (4) **Distil-cluecorpussmall** (Radford et al., 2019): The model is a 6-layer GPT-2 trained on the CLUECorpusSmall (Xu et al., 2020; Du, 2019) corpus. (5) **Cluecorpussmall** (Radford et al., 2019; Du, 2019): The model is a 12-layer GPT-2 trained on the CLUECorpusSmall corpus.\\n\\nTo better analyze the influence of different components in the CSD, we also conduct an ablation study.\"}"}
{"id": "acl-2023-long-593", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"study as follows: (1) w/o NM: The CSD model uses only traditional BERT instead of BERT trained using the progressive mask method. (2) w/o IL: The CSD model only splices three classification result labels after utterance as the train data. (3) w/o CA: The CSD model only interacts with encoder through the cross-attention mechanism. (4) w/o AL: The CSD model only adds the attention loss to embed external knowledge.\\n\\nTable 6: Ablation test of different components.\\n\\n| Models          | Bleu-2 | Bleu-4 | BERTScore | Distinct-2 |\\n|-----------------|--------|--------|-----------|------------|\\n| CSD             | 45.53  | 30.90  | 74.61     | 27.04      |\\n| w/o NM          | 44.75  | 30.42  | 74.27     | 26.77      |\\n| w/o IL          | 42.88  | 30.53  | 73.22     | 22.71      |\\n| w/o CA          | 43.39  | 28.73  | 72.79     | 29.54      |\\n| w/o AL          | 43.66  | 28.91  | 70.97     | 23.20      |\\n\\nTable 7: Result of human A/B test.\\n\\n| Models          | Win   | Loss  | Tie    |\\n|-----------------|-------|-------|--------|\\n| CSD vs CDialGPT base | 69.0  | 20.7  | 10.3   |\\n| CSD vs CDialGPT large | 65.5  | 20.7  | 13.8   |\\n| CSD vs GPT2-chitchat | 55.2  | 17.2  | 27.6   |\\n| CSD vs Distil-cluecorpussmall | 48.3  | 27.6  | 24.1   |\\n| CSD vs Cluecorpussmall | 41.4  | 31.0  | 27.6   |\\n\\n4.5 Experimental Results and Analysis\\n\\nAutomatic evaluations. In Table 4, we observe that the encoder module of the CSD is better than the other baselines in therapy, emotion, support strategy recognition accuracy. In Table 5, we observe that the CSD outperforms strong baselines in terms of Bleu and BERTScore. Because CSD models extensive therapy principle and emotional support strategy and there is less language diversity associated with therapy principle and emotional support strategy, the diversity of response is weaker than that of CDialGPT base and CDialGPT large.\\n\\nWe also perform an ablation study for better understanding the contributions of the main modules of the CSD model. As shown in Table 6, CSD outperforms all other models (w/o NM, w/o IL, w/o CA, w/o AL) in Bleu and BERTScore. However, due to therapy principle and emotional support strategy intervening in the generation of decoders, the diversity of response generation decreases. Only the case of w/o CA model involving a small number of therapies and support strategies achieves high diversity of generated responses.\\n\\nHuman evaluation. Table 5 illustrates that CSD obtains the best performance on Empathy, Support and Fluency scores. Additionally, we carry out pairwise response comparison to directly compare the dialogue quality gains in Table 7. The results confirm that the responses from CSD are more preferred by human judges.\\n\\n4.6 External Knowledge Analysis\\n\\nWe introduce external knowledge in three ways: training encoders by using external knowledge to progressively mask entities and sentences (w/o NM), intervening GPT-2 generation by classification labels (w/o IL), and paying more attention to emotional words and keywords by calculating the weight of words (w/o AL). To further investigate the impact of introduced knowledge, we test different components of CSD as shown in Table 6. However, the distinct metrics of these models are lower than models without embedded knowledge (w/o CA). Because w/o NM has more knowledge embedded than w/o IL and w/o AL and distinct metric of w/o NM is also significantly improved compared with w/o IL and w/o AL, it concluded that the generated response diversity decreases when little external knowledge is embedded, but with the increase of embedded knowledge, diversity of the generated response also increases.\\n\\n4.7 Case Study\\n\\nFor decoder generation evaluation, Table 8 shows two examples generated by CSD and other baselines. In the first case, CSD generates an informative response with proper therapy principle and emotional support, which stimulates thinking of the elder through implicit empathy and further questioning. However, baselines with only the decoder part fail to express responses with the therapy principle and emotional support. In the second case, CSD generates a response with continuous questions, which further stimulates thinking of elder. Both cases show that CSD can generate responses with therapy principle and emotional support.\\n\\n5 Related Work\\n\\n5.1 Cognitive Training Dialogue System\\n\\nWith the increasing popularity of NLP, dialogue systems have progressed from exploiting simple neural networks (Lee et al., 2016) to large-scale pre-trained models (Vlasov et al., 2019; Zhang et al., 2019b; Ni et al., 2022). Currently, while English dialogue systems dominate, there also exist Chinese dialogue systems...\"}"}
{"id": "acl-2023-long-593", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Where did you get your hair cut? (Inquiry, None, Question)\\n\\nAt the community center. (Expression, None, None)\\n\\nWow, is there anyone at the community center who cuts hair? (Inquiry, Surprise, Question)\\n\\nYes, it's very cheap, five dollars. (Expression, None, None)\\n\\nI've just heard \u201cNanping Evening Bells\u201d. (Expression, None, Self-disclosure)\\n\\nThis song seems very old. Have you heard this song before?\\n\\nI just listened to it. (Expression, None, Self-disclosure)\\n\\nHa ha, I just heard that too. (Expression, None, Self-disclosure)\\n\\nHave you ever heard the song \u201cNanping Evening Bells\u201d?\\n\\nHave you heard this song?\\n\\nDo you seem to have heard this song before?\\n\\nWho sings this song? Why is it called \u201cNanping Evening Bells\u201d?\\n\\nTable 8: Generated responses (translated from Chinese to English) from CSD and baseline models. The emotion classification, therapy principle, support strategy are labeled in the parentheses after the utterances.\"}"}
{"id": "acl-2023-long-593", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CS conversations to train models, and address the issue of therapy principle, emotional support recognition in reference context in dialogue.\\n\\nLimitations\\nThe current dialogue system is mainly based on deep neural network, like transformer structure, which often requires a large number of data sets for training model. However, there are still some deficiencies in our dataset. We will further label and create more dataset to train model. In addition, in order to improve the quality of dialogue, our model parameters are relatively large, which affect the speed of dialogue generation to some extent. We will explore some methods, such as knowledge distillation, to reduce model parameters to improve the speed of dialogue generation on the premise of keeping the quality of dialogue generation unchanged.\\n\\nEthics Statement\\nWe have sought to ethically conduct this study, including transparently communicating with data annotators about data use and study intent, and finding suitable elders to conduct human tests of the dialogue systems, compensating workers and elders at a reasonable hourly wage. We have obtained study approval from the ethics review board.\\n\\nAcknowledgements\\nWe want to thank our anonymous AC and reviewers for their feedback. This work was supported in part by grants from Hong Kong Research Grants Council (RGC) under the contracts HKU 17203522 and 17207621.\\n\\nReferences\\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normalization. arXiv preprint arXiv:1607.06450.\\n\\nTha\u00eds Cristina Galdino De Oliveira, Fernanda Cabral Soares, Liliane Dias E Dias De Macedo, Domingos Luiz Wanderley Pican\u00e7o Diniz, Nat\u00e1li Valim Oliver Bento-Torres, and Cristovam Wanderley Pican\u00e7o-Diniz. 2014. Beneficial effects of multisensory and cognitive stimulation on age-related cognitive decline in long-term-care institutions. Clinical Interventions in Aging, pages 309\u2013321.\\n\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2018. Wizard of wikipedia: Knowledge-powered conversational agents. CoRR, abs/1811.01241.\\n\\nZeyao Du. 2019. Gpt2-chinese: Tools for training gpt2 model in chinese language. https://github.com/Morizeyao/GPT2-Chinese.\\n\\nLea Ellwardt, Marja Aartsen, Dorly Deeg, and Nardi Steverink. 2013. Does loneliness mediate the relation between social support and cognitive functioning in later life? Social science & medicine, 98:116\u2013124.\\n\\nJun Gao, Yuhan Liu, Haolin Deng, Wei Wang, Yu Cao, Jiachen Du, and Ruifeng Xu. 2021. Improving empathetic response generation by recognizing emotion cause in conversations. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 807\u2013819.\\n\\nLinda J Garcia. 2022. The usefulness of useless conversation: An avenue for connection and autonomy for older adults. In Well-being In Later Life, pages 53\u201364. Routledge.\\n\\nYuxian Gu, Jiaxin Wen, Hao Sun, Yi Song, Pei Ke, Chujie Zheng, Zheng Zhang, Jianzhu Yao, Xiaoyan Zhu, Jie Tang, et al. 2022. Eva2.0: Investigating open-domain chinese dialogue systems with large-scale pre-training. arXiv preprint arXiv:2203.09313.\\n\\nKatharina Kann, Abteen Ebrahimi, Joewie Koh, Shiran Dudy, and Alessandro Roncone. 2022. Open-domain dialogue generation: What we can do, cannot do, and should do next. In Proceedings of the 4th Workshop on NLP for Conversational AI, pages 148\u2013165.\\n\\nJacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of naacL-HLT, pages 4171\u20134186.\\n\\nHyunwoo Kim, Byeongchang Kim, and Gunhee Kim. 2021a. Perspective-taking and pragmatics for generating empathetic responses focused on emotion causes. EMNLP.\\n\\nJeongsim Kim, EunJi Shin, KyungHwa Han, Soowon Park, Jung Hae Youn, Guixiang Jin, Jun-Young Lee, et al. 2021b. Efficacy of smart speaker\u2013based metamemory training in older adults: Case-control cohort study. Journal of medical Internet research, 23(2):e20177.\\n\\nYoon Kim. 2014. Convolutional neural networks for sentence classification. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1746\u20131751, Doha, Qatar. Association for Computational Linguistics.\\n\\nHanbit Lee, Yeonchan Ahn, Haejun Lee, Seungdo Ha, and Sang-goo Lee. 2016. Quote recommendation in dialogue using deep neural network. In Proceedings of the 39th International ACM SIGIR conference on Research and Development in Information Retrieval, pages 957\u2013960.\"}"}
{"id": "acl-2023-long-593", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lung-Hao Lee, Jian-Hong Li, and Liang-Chih Yu. 2022. Chinese emobank: Building valence-arousal resources for dimensional sentiment analysis. Transactions on Asian and Low-Resource Language Information Processing, 21(4):1\u201318.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2016. A diversity-promoting objective function for neural conversation models. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 110\u2013119, San Diego, California. Association for Computational Linguistics.\\n\\nQintong Li, Hongshen Chen, Zhaochun Ren, Pengjie Ren, Zhaopeng Tu, and Zhumin Chen. 2020. Empdg: Multiresolution interactive empathetic dialogue generation. COLING.\\n\\nQintong Li, Piji Li, Zhaochun Ren, Pengjie Ren, and Zhumin Chen. 2022. Knowledge bridging for empathetic dialogue generation. 36th Association for the Advancement of Artificial Intelligence.\\n\\nZhaojiang Lin, Andrea Madotto, Jamin Shin, Peng Xu, and Pascale Fung. 2019. Moel: Mixture of empathetic listeners. CoRR, abs/1908.07687.\\n\\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How NOT to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122\u20132132, Austin, Texas. Association for Computational Linguistics.\\n\\nSiyang Liu, Chujie Zheng, Orianna Demasi, Sahand Sabour, Yu Li, Zhou Yu, Yong Jiang, and Minlie Huang. 2021. Towards emotional support dialog systems. pages 3469\u20133483.\\n\\nYingxu Liu, Shu Zhang, Yasutake Tomata, Tatsui Otosuka, Dieta Nurrika, Yumi Sugawara, and Ichiro Tsuji. 2020. Emotional support (giving or receiving) and risk of incident dementia: The ohsaki cohort 2006 study. Archives of Gerontology and Geriatrics, 86:103964.\\n\\nIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.\\n\\nJavier Navarro, Faiyaz Doctor, V\u00edctor Zamudio, Rizhat Iqbal, Arun Kumar Sangaiah, and Carlos Lino. 2018. Fuzzy adaptive cognitive stimulation therapy generation for alzheimer\u2019s sufferers: Towards a pervasive dementia care monitoring platform. Future Generation Computer Systems, 88:479\u2013490.\\n\\nJinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent advances in deep learning based dialogue systems: A systematic survey. Artificial Intelligence Review, pages 1\u2013101.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nJeong-Mo Park, Mi-Won Kim, and Hee-Young Shim. 2019. Effects of a multicomponent cognitive stimulation program on cognitive function improvement among elderly women. Asian Nursing Research, 13(5):306\u2013312.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.\\n\\nWei Peng, Ziyuan Qin, Yue Hu, Yuqiang Xie, and Yunpeng Li. 2022. Fado: Feedback-aware double controlling network for emotional support conversation. arXiv preprint arXiv:2211.00250.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.\\n\\nSahand Sabour, Chujie Zheng, and Minlie Huang. 2022. Cem: Commonsense-aware empathetic response generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11229\u201311237.\\n\\nAshish Sharma, Inna W. Lin, Adam S. Miner, David C. Atkins, and Tim Althoff. 2021. Towards facilitating empathic conversations in online mental health support: A reinforcement learning approach. In Proceedings of the Web Conference 2021, WWW \u201921, page 194\u2013205, New York, NY, USA. Association for Computing Machinery.\\n\\nAshish Sharma, Adam Miner, David Atkins, and Tim Althoff. 2020. A computational approach to understanding empathy expressed in text-based mental health support. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5263\u20135276, Online. Association for Computational Linguistics.\\n\\nLei Shen, Jinchao Zhang, Jiao Ou, Xiaofang Zhao, and Jie Zhou. 2021. Constructing emotion consensus and utilizing unpaired data for empathetic dialogue generation. EMNLP.\"}"}
{"id": "acl-2023-long-593", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Seiki Tokunaga, Katie Seaborn, Kazuhiro Tamura, and Mihoko Otake-Matsuura. 2019. Cognitive training for older adults with a dialogue-based, robot-facilitated storytelling system. In International Conference on Interactive Digital Storytelling, pages 405\u2013409. Springer.\\n\\nSeiki Tokunaga, Kazuhiro Tamura, and Mihoko Otake-Matsuura. 2021. A dialogue-based system with photo and storytelling for older adults: toward daily cognitive training. Frontiers in Robotics and AI, page 179.\\n\\nQuan Tu, Yanran Li, Jianwei Cui, Bin Wang, Ji-Rong Wen, and Rui Yan. 2022. Misc: A mixed strategy-aware model integrating comet for emotional support conversation. 60th Annual Meeting of the Association for Computational Linguistics.\\n\\nHelma van Rijn, Joost van Hoof, and Pieter Jan Stappers. 2010. Designing leisure products for people with dementia: Developing \u201cthe chitchatters\u201d game. American Journal of Alzheimer's Disease & Other Dementias\u00ae, 25(1):74\u201389.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nVladimir Vlasov, Johannes E. M. Mosig, and Alan Nichol. 2019. Dialogue transformers. CoRR, abs/1910.00486.\\n\\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020a. A large-scale chinese short-text conversation dataset. In NLPCC.\\n\\nYida Wang, Pei Ke, Yinhe Zheng, Kaili Huang, Yong Jiang, Xiaoyan Zhu, and Minlie Huang. 2020b. A large-scale chinese short-text conversation dataset. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 91\u2013103. Springer.\\n\\nAnuradha Welivita and Pearl Pu. 2020. A taxonomy of empathetic response intents in human social conversations. CoRR, abs/2012.04080.\\n\\nLiang Xu, Xuanwei Zhang, and Qianqian Dong. 2020. Cluecorpus2020: A large-scale chinese corpus for pre-training language model. ArXiv, abs/2003.01355.\\n\\nXiaohan Xu, Xuying Meng, and Yequan Wang. 2022. Poke: Prior knowledge enhanced emotional support conversation with latent variable. arXiv preprint arXiv:2210.12640.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019a. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2019b. Dialogpt: Large-scale generative pre-training for conversational response generation. CoRR, abs/1911.00536.\\n\\nHao Zhou, Pei Ke, Zheng Zhang, Yuxian Gu, Yinhe Zheng, Chujie Zheng, Yida Wang, Chen Henry Wu, Hao Sun, Xiaocong Yang, et al. 2021. Eva: An open-domain chinese dialogue system with large-scale generative pre-training. arXiv preprint arXiv:2108.01547.\"}"}
{"id": "acl-2023-long-593", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work? \\n\\nLimitation part.\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\nThere is no potential risk in our work.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\\nAbstract part and part 6.\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nLeft blank.\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n\\nLeft blank.\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\nNo response.\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\nNo response.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\nNo response.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\nNo response.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\nNo response.\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nNo response.\\n\\nC \u25a1 Did you run computational experiments?\\n\\nSection 5.1 of Part 5.\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nSection 5.1 of Part 5.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-593", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 5.1 of Part 5.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSection 5.1 of Part 5.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nSection 5.1 of Part 5.\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nEthnics statement part.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nEthnics statement part.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nEthnics statement part.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nEthnics statement part.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nEthnics statement part.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNot applicable. Left blank.\"}"}
