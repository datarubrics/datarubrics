{"id": "acl-2023-long-154", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? \\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We report both descriptive statistics and exact per-model performance.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? This information is proprietary to the language service providers we relied upon.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? Data comes from Wikipedia.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board? Not applicable. Left blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data? Such information was not available to us due to privacy regulations.\"}"}
{"id": "acl-2023-long-154", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Small Data, Big Impact: Leveraging Minimal Data for Effective Machine Translation\\n\\nJean Maillard\u2217\\nMeta AI\\nCynthia Gao\\nMeta AI\\nElahe Kalbassi\\nMeta AI\\nKaushik Ram Sadagopan\\nMeta AI\\nVedanuj Goswami\u2020\\nMeta AI\\nPhilipp Koehn\\nJohns Hopkins University\\nAngela Fan\\nMeta AI\\nFrancisco Guzm\u00e1n\\nMeta AI\\n\\nAbstract\\nFor many languages, machine translation progress is hindered by the lack of reliable training data. Models are trained on whatever pre-existing datasets may be available and then augmented with synthetic data, because it is often not economical to pay for the creation of large-scale datasets. But for the case of low-resource languages, would the creation of a few thousand professionally translated sentence pairs give any benefit? In this paper, we show that it does.\\n\\nWe describe a broad data collection effort involving around 6k professionally translated sentence pairs for each of 39 low-resource languages, which we make publicly available. We analyse the gains of models trained on this small but high-quality data, showing that it has significant impact even when larger but lower quality pre-existing corpora are used, or when data is augmented with millions of sentences through backtranslation.\\n\\n1 Introduction\\nState of the art machine translation models are able to cover hundreds of languages (Ma et al., 2021; Wang et al., 2022; Siddhant et al., 2022; NLLB Team et al., 2022) by relying on large amounts of annotated (Skad\u0131n, \u0161 et al., 2014; Lison and Tiedemann, 2016; Agi\u0107 and Vuli\u0107, 2019) and unannotated web crawled data (Schwenk et al., 2021; Hefner et al., 2022). Translation for low-resource languages still faces significant challenges related to data availability, since many of these languages have neither large-scale parallel corpora nor a big presence on the web (Adelani et al., 2022b).\\n\\nTechniques such as self-supervised learning (Ma et al., 2021; Liu et al., 2021) and backtranslation (Sennrich et al., 2016; Edunov et al., 2018; Fan et al., 2020) can be effective tools to reduce the reliance on annotation for translation models. In some cases, these techniques can be combined or even be applied iteratively (Hoang et al., 2018), leading to a feedback loop that can generate increasingly better translations. In order to be effective, however, such methods still require a certain amount of seed parallel data, which can be used to kickstart the process.\\n\\nAs a result, researchers and communities looking to train translation systems for low-resource languages may find themselves wondering how much parallel data is required to achieve a given performance target level.\\n\\nIn this paper, we describe a data collection effort for 39 low-resource languages, involving the creation of over 6k seed sentence pairs per language by professional translators, which we make publicly available with an open license. We analyse the behaviour of bilingual translation systems trained on varying amounts of this data, with and without the addition of pre-existing publicly available parallel datasets, and find that even comparatively small amounts of professionally produced parallel sentences can have an outsized impact. We find that gains coming from high quality data are further enhanced when training multilingual models of closely related high- and low-resource languages, and even more so when augmenting the dataset via backtranslation.\\n\\nOverall, our results show that employing relatively small but high-quality, professionally translated datasets constitutes a promising and viable way towards achieving performant machine translation for low-resource languages, especially for those with high-resource relatives. This holds true even for languages for which some pre-existing data might already be publicly available, further highlighting the importance of high-quality training datasets.\\n\\nNotably, parallel datasets of the scale discussed here are compact enough that coverage for a new\"}"}
{"id": "acl-2023-long-154", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"language could plausibly be collected by a relatively small group of volunteers in a week, making these results relevant for the usage of machine translation technologies in crisis situations (Lewis et al., 2011).\\n\\nOur main contributions are:\\n\\n1. The creation and public release of a professionally translated seed dataset for 39 low-resource languages.\\n2. An analysis of the impact of this high-quality data, both in isolation and also when combined with pre-existing datasets, based on hundreds of trained models.\\n3. A study of how gains from high-quality parallel data compound when using multilingual training and backtranslation, showing that benefits from high-quality data do not get washed away when using stronger models or data augmentation.\\n\\nBackground\\n\\nLow-resource language translation\\n\\nDespite very successful recent advances in neural machine translation, most of the gains have only benefited a handful of so-called high-resource languages, which have enough textual resources to satisfy the substantial data requirements of state-of-the-art techniques. The vast majority of the world\u2019s languages are low-resource, and researchers have increasingly been focusing on evaluating performance in this challenging setting (Wenzek et al., 2021).\\n\\nBenchmarks\\n\\nTraditionally, one of the biggest challenges to the development of low-resource translation systems has been the lack of high-quality evaluation data. Several benchmarks focus on specific sets of languages, such as the MADAR dataset for Arabic dialects (Bouamor et al., 2018), the Autshumato benchmark covering 11 South African languages (McKellar, 2017), or the TICO-19 benchmark covering 35 languages for the domain of medical information related to the COVID-19 pandemic (Anastasopoulos et al., 2020). More recently, the FLORES-101 dataset (Goyal et al., 2022) and its expansion to over 200 languages (NLLB Team et al., 2022) has enabled multilingual evaluation across tens of thousands of directions, including many low-resource languages. Its domain is composed of an even mixture of travel guides (Wikitravel), children\u2019s literature (Wikijunior), and news content (Wikinews).\\n\\nTraining corpora\\n\\nMuch important work has gone towards the development of parallel corpora for low-resource languages, most of which focusing on individual language pairs (Tapo et al., 2021; Ali et al., 2021; Adelani et al., 2021; Azunre et al., 2021, inter alia). Adelani et al. (2022a) study the case of 15 low-resource African languages, most of which already have tens or hundreds of thousands of parallel sentences in the religious domain, and investigate how combining pre-trained models and a newly created corpus can lead to effective domain transfer.\\n\\nLow-resource training\\n\\nAmongst the techniques that can be used to decrease the reliance on manually annotated data, bitext mining (Schwenk et al., 2021; Ramesh et al., 2022) enables finding pairs of translations among large collections of unannotated monolingual text. Heffernan et al. (2022) show its effectiveness for low-resource languages, but point out that it can be limited for the most data scarce languages. Backtranslation (Sennrich et al., 2016; Edunov et al., 2018) can be used to create pseudo-parallel data from monolingual data in a target language. It relies on an initial, potentially low-quality translation model \u2013 thereby having some requirements on annotated data \u2013 and can also be applied iteratively for improved performance (Hoang et al., 2018). Self-supervision (Siddhant et al., 2020) is a method employing monolingual text denoising as a joint training objective, and its use has been suggested as a way of kick-starting an iterative backtranslation pipeline. Finally, multilingual translation, which is often combined with one or more of the above techniques, has been shown to improve low-resource translation performance via cross-lingual transfer (Firat et al., 2016; Fan et al., 2020; Ma et al., 2021; Wang et al., 2022; Siddhant et al., 2022; NLLB Team et al., 2022).\\n\\nTraining without parallel data\\n\\nWithin the area of low-resource translation, Bapna et al. (2022) describe the development of translation systems for low-resource languages without using any parallel data at all, relying instead on crawled monolingual data and language transfer. Methods which don\u2019t require parallel data are likely complementary to the seed data approach proposed in this...\"}"}
{"id": "acl-2023-long-154", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paper. However, the over-reliance on cross-lingual\\ntransfer from a high-resource language opens up\\nthe risk of a translation system flattening the differ-\\nences between related languages, as observed by\\nNLLB Team et al. (2022) for Arabic dialects. This\\nis a particularly thorny issue for communities of\\nspeakers of endangered languages which are at risk\\nof being displaced by a related higher-resource lan-\\nguage \u2013 as is the case for several of the languages\\ncovered in this paper. In such cases, we recommend\\nthe seed data approach, which opens the door for\\nthe communities to take ownership in preserving\\ntheir languages, and aligns well with their desire\\nto preserve the distinctiveness of their language in\\ntechnological applications.\\n\\nCrisis MT\\nLow-resource machine translation has\\nbeen studied in the context of crisis events, and has\\nbeen proposed as a component of a rapid response\\ninfrastructure (Lewis et al., 2011). In particular,\\nLewis (2010) describe the creation of a system for\\nHaitian Creole after the devastating 2010 earth-\\nquake, and Anastasopoulos et al. (2020) built a\\ndataset to facilitate access to information related to\\nthe COVID-19 pandemic.\\n\\n3 Data collection\\nRegardless of the many modelling improvements\\naimed at reducing the amount of required supervi-\\nsion, it is likely impossible for translation models\\nto reach acceptable levels of quality without even\\nsmall amounts of parallel data. This is especially\\ntrue for approaches that explicitly rely on the pre-\\nexistence of parallel corpora, such as backtrans-\\nlation. As a result, low-resource languages with\\ncorpora that are too small to enable the use of these\\ntechniques are cut off from the improvements they\\nbring. With this in mind, we set up a data collec-\\ntion effort for a number of low-resource languages\\nwhich fit this criterion, resulting in a dataset of\\naround six thousand English sentences translated\\ninto each of 39 low-resource languages.\\n\\nLanguage selection\\nIn order to choose which\\nlanguages to collect data for, we took several fac-\\ntors into account. First, we looked at the list\\nof languages supported by Wikipedia. The user-\\ngenerated encyclopedia is one of the most visited\\nwebsites in the world, and constitutes an impor-\\ntant means of knowledge dissemination for many\\nlow-resource language communities. Crucially,\\nWikipedia has an open process towards support-\\nning new languages, which has led to the platform\\nsupporting over 300 languages in 2022. This\\nlist of languages was cross-referenced with those\\ncurrently supported by machine translation bench-\\nmarks, including the large FLORES-200 dataset.\\n\\nWe then focussed our attention to those languages\\nfor which not enough high quality data was cur-\\ntently publicly available for large-scale training,\\nlooking in particular at those languages with fewer\\nthan 100,000 parallel training sentences and priori-\\ntising those with the least amount of high quality\\ndata (as determined by automatic metrics such as\\nlanguage identification). Finally, we partnered with\\nlinguists and identified those languages for which\\nprofessional translators would be available.\\n\\nSource sentence selection\\nThe dataset consists\\nof English sentences translated into a number of\\nlow-resource languages. The source data was\\nsampled from Wikimedia\u2019s List of articles every\\nWikipedia should have, a collection of 10,000\\nWikidata IDs corresponding to notable topics in\\ndifferent fields of knowledge and human activity.\\nThese are split into 11 categories such as People,\\nHistory, Philosophy and Religion, Geography. We\\nuniformly sampled a subset of IDs from which we\\nwould draw data, and mapped these to the corre-\\nsponding English Wikipedia articles. From each of\\nthese articles we then sampled triplets of contigu-\\nous sentences, such that some amount of context\\nwould be provided, and ensured a maximum of one\\ntriplet would be sampled per article to guarantee a\\nrelatively uniform coverage of topics.\\n\\nFinding translators\\nThe parallel dataset was cre-\\nated through human translation. We identified\\ntranslators through various specialised language\\nservice providers. Through a vetting process, we\\nselected translators that were native speakers in the\\ntarget language, with a minimum of two years of\\nprofessional experience and a degree in a relevant\\nfield of studies, such as translation or linguistics.\\nAll translators were additionally required to have\\na high level of English fluency, and had to pass an\\ninitial test to assess their translation proficiency.\\n\\n2 https://meta.wikimedia.org/wiki/\\nLanguage_proposal_policy\\n3 https://meta.wikimedia.org/wiki/List_\\nof_Wikipedias\\n4 https://github.com/facebookresearch/\\nflores\\n5 https://meta.wikimedia.org/wiki/List_\\nof_articles_every_Wikipedia_should_have/\\nExpanded\"}"}
{"id": "acl-2023-long-154", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Translation workflow\\n\\nTranslators were provided with a clear set of instructions for the project, which can be seen in Appendix B. In addition to these general instructions, in order to avoid issues of mismatching script, spelling system or dialect with the available evaluation benchmarks, we established a set of linguistic guidelines to match the data that was collected for the FLORES-200 dataset. Translators referenced these guidelines while working on the creation of the dataset. The source sentences were translated directly from English for most languages. The only exceptions were Acehnese and Banjar in the Arabic script and Tamasheq in the Tifinagh script, which were transliterated from their respective Latin script datasets, that had in turn first been translated from English. Following this process we conducted a linguistic quality assessment phase in which all translations were checked for conformance with the linguistic guidelines, and automatic quality control checks were performed.\\n\\nCompensation range\\n\\nThe hourly compensation for translators averaged 25.80 US dollars, with a median of 25.60. The productivity rate generally ranged between 200-250 words per hour, with the exception of the Acehnese and Banjar transcriptions into Arabic which required less effort. Transcription of Tamashek into Tifinagh proved to be more difficult, and had a productivity rate close to that of translation. The full costs for the project also included quality assurance as well as other various expenses incurred by the language providers we partnered with.\\n\\nFinal dataset\\n\\nThe final dataset size was chosen in order to obtain at least 6,000 parallel sentences per direction, while simultaneously maximising language coverage. Given the available budget, this resulted in a final dataset of 6,193 sentences translated into 39 languages, including three transcribed directions. The dataset is released under the open CC-BY-SA 4.0 license. A full list of the languages can be found in Appendix A.\\n\\n4 Experimental Setup\\n\\n4.1 Data\\n\\nBilingual models\\n\\nOur first set of experiments focuses on bilingual machine translation, both into and out of English. Beyond our newly developed seed corpus described in Section 3, we sourced additional pre-existing parallel sentences with English through the OpenSubtitles corpus (Lison and Tiedemann, 2016), the QCRI educational domain corpus (Abdelali et al., 2014), the PMIndia corpus (Haddow and Kirefu, 2020), the MultiIndicMT corpus (Nakazawa et al., 2021) as well as the GlobalVoices, Gnome, KDE, Sftware, Tatoeba, Ubuntu and Wikimedia corpora available through the OPUS repository (Tiedemann, 2012). The parallel sentences were obtained through the mtdata tool (Gowda et al., 2021).\\n\\nMultilingual models\\n\\nOur second set of experiments involves training multilingual machine translation models for two clusters of related languages: an Italic model, trained on six low-resource languages (fur_Latn, lij_Latn, lmo_Latn, scn_Latn, srd_Latn, vec_Latn) and three related high-resource languages (cat_Latn, ita_Latn, spa_Latn) along with English; and an Indo-Aryan model, with four low-resource (bho_Deva, hne_Deva, kas_Deva, mag_Deva) and two related high-resource languages (hin_Deva, ben_Beng), together with English. For these experiments, we collected additional parallel sentences between any two of the languages within each group. On top of the corpora mentioned in the previous paragraph, we also used the EU Bookshop (Tiedemann, 2012) and Europarl (Koehn, 2005) corpora for certain high-resource directions.\\n\\nBacktranslation\\n\\nFor the backtranslation experiments of Section 4.4, we sourced monolingual data from the Common Crawl project, and filtered it with the LID model provided by NLLB Team et al. (2022) in order to obtain a maximum of 2M sentences per language.\\n\\nAll models are evaluated on the devtest split of the FLORES-200 benchmark.\"}"}
{"id": "acl-2023-long-154", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to study the data scaling properties, we randomly partition each seed dataset into three chunks: one consisting of 1k seed parallel sentences, one consisting of 2k, and the final one consisting of the remaining 3k sentences.\\n\\nFor each unresourced language, we consider two directions, into and out of English. For each direction, we train three models: on the first, the first two, and all three chunks of the seed data (training corpus sizes of 1k, 3k and 6k sentences respectively). This results in 162 models overall.\\n\\nFor the barely-resourced languages, we take the same basic approach, but always include the pre-existing publicly available data. In addition, we also train models using the whole seed dataset only, and the publicly available data only. This results in 120 models.\\n\\nAll bilingual models use a transformer architecture (Vaswani et al., 2017) with 6 encoder layers and 6 decoder layers, 8 attention heads, 512-dimensional embeddings, 0.3 dropout, an effective batch size of 130k tokens, and are trained with an inverse square root learning rate schedule with warmup. Data for each model is tokenised with a language pair specific sentencepiece model (Kudo and Richardson, 2018). Training is conducted with fairseq (Ott et al., 2019), with each model being trained on a machine with 8 NVIDIA Tesla V100 Volta 32GB GPUs for at most 12 hours.\\n\\n| Language Code | Script | Existing data |\\n|---------------|--------|---------------|\\n| Friulian      | fur Latn | 2k            |\\n| Nigerian Fulfulde | fuv Latn | 2k            |\\n| Chhattisgarhi | hne Deva | 35k           |\\n| Ligurian      | lij Latn | 1k            |\\n| Limburgish    | lim Latn | 3k            |\\n| Magahi        | mag Deva | 14k           |\\n| Meitei        | mni Beng | 6k            |\\n| Nuer          | nus Latn | 23k           |\\n| Dari          | prs Arab | 1k            |\\n| Southern Pashto | pbt Arab | 26k           |\\n| Sardinian     | srd Latn | 2k            |\\n| Tamasheq (Latin scr.) | taq Latn | 27k           |\\n\\nTable 1: List of the 12 barely-resourced languages, for which some data (parallel sentences) was already publicly available.\\n\\n4.3 Multilingual Experiments\\n\\nLow-resource languages have been shown to significantly benefit from multilingual transfer (Arivazhagan et al., 2019; Bapna et al., 2022; NLLB Team et al., 2022), so it is reasonable to expect that any attempts at boosting low-resource translation performance would also involve multilingual training. In order to evaluate the data scaling and language transfer properties in this useful setting, we design an additional set of experiments focusing on two groups of languages.\\n\\n\u2022 We train an Italic model on the low-resource Friulian, Ligurian, Lombard, Sicilian, Sardinian and Venetian, combined with the related high-resource Catalan, Italian and Spanish, plus English.\\n\\n\u2022 We train an Indo-Aryan model on the low-resource Bhojpuri, Chhattisgarhi, Kashmiri (Devanagari script) and Magahi, combined with the related high-resource Hindi and Bengali, plus English.\\n\\nEach model is trained on all available parallel data between any of its languages. We further conduct an ablation experiment for each model, by removing all seed data and training on the publicly available data only. The training setup is analogous to that of the bilingual experiments, but the architecture is scaled up to 12 layers and 8 attention heads for both encoder and decoder, 1024-dimensional embeddings, 0.1 dropout, and an effective batch size of 524k tokens. Multilingual models are trained on four machines, each with 8 NVIDIA Tesla V100 Volta 32GB GPUs, for a maximum of 48 hours.\\n\\n4.4 Backtranslation\\n\\nOur final set of experiments involves generating backtranslation data with the multilingual models, and training new multilingual models with this additional data. As discussed in Section 2, this technique can be particularly effective for improving low-resource translation performance. The unlabelled monolingual data it relies upon is more easily obtainable than parallel sentences (Heffernan et al., 2022), making this technique particularly important to boost performance for particularly data scarce settings. We run this experiment both using pre-existing data only, as well as with the addition of all seed data.\\n\\nDespite monolingual data taking centre stage in backtranslation, the technique still depends on the existence of a seed translation model to augment the unannotated sentences with synthetic translations. We experiment with generating backtranslation data for the two multilingual models of Section 4.3, using both the full and ablated models.\"}"}
{"id": "acl-2023-long-154", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the Italic model, we provide backtranslations from the six low-resource languages into both eng_Latn and ita_Latn, and vice versa. For the Indo-Aryan model, we provide backtranslations from the four low-resource languages into both eng_Latn and hin_Deva, and vice versa.\\n\\n5 Results and Analysis\\nWe report all results using automatic evaluation metrics against the FLORES-200 benchmark. We rely on the chrF++ score (Popovi\u0107, 2017), which is based on character-level n-gram overlap, and is complemented by unigram and bigram features. This score overcomes the limitations inherent to the more commonly used BLEU metric (Papineni et al., 2002), which relies on the availability of tokenization tools for all languages and fails to accurately account for highly agglutinative languages.\\n\\n5.1 Bilingual Experiments\\nA summary of bilingual translation performance on the unresourced languages is reported in Figures 1a and 1b. At the lowest training data level, consisting of 1k sentences, we obtain an average chrF++ score of 12.6 eng-xxx and 13.9 xxx-eng. Moving to the 3k-sized corpus, the average increases to 19.9 eng-xxx and 20.6 xxx-eng. Training on the full seed corpus, this further increases to 22.9 eng-xxx and 23.7 xxx-eng. On the whole, models perform at a similar level on the two translation directions, with a slightly larger spread on the eng-xxx direction.\\n\\nResults on languages that already had some amount of parallel data publicly available \u2013 which we call barely-resourced \u2013 are reported separately, in Figures 1c and 1d. We find that, even though these languages already have pre-existing training data (accounting for 12k sentences per language, on average) the addition of a mere 1k parallel sentences from our high-quality dataset brings the average performance up from 12.9 to 19.0 chrF++ in the eng-xxx direction, and from 16.0 to 20.9 chrF++ in the xxx-eng direction. Notably, we see that training without the publicly available data has little effect. Indeed, the removal of all public data accounts for a mere average chrF++ drop of 0.7 eng-xxx and 1.1 xxx-eng, underlining the fundamental role that high quality annotated data can play in improving performance for data-scarce languages.\\n\\n5.2 Multilingual Experiments\\nResults for the multilingual experiments on the Italic and Indo-Aryan language clusters are reported in Table 2. For the xxx-eng directions, which target high-resource English, we see that gains from multilingual training are substantial, averaging 25.6 chrF++ for the Italic model and 20.2 chrF++ for the Indo-Aryan model when compared to their respective bilingual versions (Appendix D). The multilingual model sees a lot more English data as target, and performs better on it. Gains are still sizable but relatively smaller for the eng-xxx directions, into low-resource languages. In this case, the average performance difference is of 13.6 and 16.2 chrF++ for the Italic and Indo-Aryan models, respectively.\\n\\nFor a comparison of the effects of seed data collection, column \u2206 in Table 2 measures the performance difference of the P+6k and P multilingual models. For the eng-xxx direction the average difference is 14.0 and 12.9 chrF++ for the Italic and Indo-Aryan models respectively; in the reverse directions, the difference is 14.6 and 9.8. This confirms that the beneficial effects of cross-lingual transfer do not compensate for the gains achieved by higher quality data.\\n\\n5.3 Backtranslation Performance for the two multilingual models keeps steadily improving when adding backtranslation. By looking at column \u2206 of Table 3, which compares multilingual models with and without backtranslated data, we see that all models trained with backtranslated data outperform their base counterparts for every single direction. Gains from backtranslation are generally more pronounced for the P models, which are trained without seed data. Overall, the same trend as in previous experiments holds true: as revealed by column \u2206, which compares the P+6k and P backtranslation-augmented models, the models trained with seed data achieve the best performance for every direction.\\n\\n6 Analysis\\nFigure 2 brings together the average performance of all models trained on the Italic and Indo-Aryan language clusters \u2013 bilingual, multilingual, and multilingual with backtranslation \u2013 both when trained only on pre-existing data alone (first set of bars), and when trained with the addition of high-quality seed data (hatched bars).\"}"}
{"id": "acl-2023-long-154", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Average bilingual translation performance (chrF++).\\n\\nUnresourced languages are trained on increasing amounts of seed data (1k, 3k, 6k sentences).\\n\\nBarely-resourced languages are trained on pre-existing data (P), plus increasing amounts of seed data (P+1k, P+3k, P+6k), and seed data alone (6k). Full results in Appendix D.\\n\\n| Language | eng-xxx | xxx-eng |\\n|----------|---------|---------|\\n| P        | P+1k    | \u2206       |\\n| fur_Latn | 33.2    | 51.1    | 17.9  |\\n| lij_Latn | 33.8    | 50.0    | 16.2  |\\n| lmo_Latn | 26.6    | 32.6    | 6.0   |\\n| scn_Latn | 25.6    | 41.8    | 16.2  |\\n| srd_Latn | 36.4    | 50.0    | 13.6  |\\n| vec_Latn | 35.4    | 49.5    | 14.1  |\\n| Average  | 31.8    | 45.8    | 14.0  |\\n\\n| Language | eng-xxx | xxx-eng |\\n|----------|---------|---------|\\n| P        | P+6k    | \u2206       |\\n| bho_Deva | 24.3    | 36.3    | 12.0  |\\n| hne_Deva | 33.4    | 47.1    | 13.7  |\\n| kas_Deva | 10.3    | 15.5    | 5.2   |\\n| mag_Deva | 30.6    | 51.1    | 20.5  |\\n| Average  | 24.7    | 37.5    | 12.9  |\\n\\nTable 2: Performance of the Italic and Indo-Aryan multilingual models (chrF++) when trained on pre-existing data only (P) and both pre-existing and seed data (P+6k).\\n\\n\u2206 measures the impact of adding seed data to multilingual models, measured as the difference between the P+6k and P multilingual models.\\n\\nThe same trends hold throughout our experiments: even with modelling improvements that aim to reduce the amount of required supervision, such as multilingual training and backtranslation, we observe that models trained on as little as 6k high-quality seed parallel sentences always come out ahead. This is true even for languages such as mag_Deva and hne_Deva, for which tens of thousands of pre-existing parallel sentences are publicly available.\\n\\nCrucially, we see that the multilingual model with seed data (\u201cMultilingual, P+6k\u201d in the graph) outperforms in all but one case the version without seed data but with backtranslation (\u201cMultilingual+BT, P\u201d). In other words, even adding vast amounts of monolingual data (as much as 2M sentences for xxx-eng) cannot make up the difference that 6k high-quality parallel sentences make.\"}"}
{"id": "acl-2023-long-154", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of the backtranslation-augmented Italic and Indo-Aryan multilingual models (chrF++).\\n\\n\\\\[ \\\\text{Average} \\\\begin{array}{cccccc}\\n\\\\text{fur} & 0.3 & 47.7 & 14.5 & 56.4 & 5.3 & 8.7 \\\\\\\\\\n\\\\text{lij} & 0.1 & 48.7 & 14.9 & 53.0 & 3.0 & 4.3 \\\\\\\\\\n\\\\text{lmo} & 0.1 & 27.5 & 0.9 & 33.7 & 1.1 & 6.2 \\\\\\\\\\n\\\\text{scn} & 1.9 & 28.8 & 3.2 & 45.1 & 3.3 & 16.3 \\\\\\\\\\n\\\\text{srd} & 0.2 & 49.5 & 13.1 & 55.7 & 5.7 & 6.2 \\\\\\\\\\n\\\\text{vec} & 1.5 & 41.8 & 6.4 & 50.7 & 1.2 & 8.9 \\\\\\\\\\n\\\\hline\\n\\\\text{bho} & 0.9 & 33.7 & 9.4 & 38.5 & 2.2 & 4.8 \\\\\\\\\\n\\\\text{hne} & 0.4 & 45.1 & 11.7 & 48.2 & 1.1 & 3.1 \\\\\\\\\\n\\\\text{kas} & 0.6 & 14.2 & 3.9 & 15.8 & 0.3 & 1.6 \\\\\\\\\\n\\\\text{mag} & 0.5 & 45.1 & 14.5 & 52.4 & 1.3 & 7.3 \\\\\\\\\\n\\\\hline\\n\\\\text{Average} & 40.7 & 8.8 & 49.1 & 3.3 & 8.4 & 51.6 \\\\\\\\\\n\\\\end{array} \\\\]\\n\\n\\\\[ \\\\text{Average} \\\\begin{array}{cccccc}\\n\\\\text{bho} & 0.9 & 33.7 & 9.4 & 38.5 & 2.2 & 4.8 \\\\\\\\\\n\\\\text{hne} & 0.4 & 45.1 & 11.7 & 48.2 & 1.1 & 3.1 \\\\\\\\\\n\\\\text{kas} & 0.6 & 14.2 & 3.9 & 15.8 & 0.3 & 1.6 \\\\\\\\\\n\\\\text{mag} & 0.5 & 45.1 & 14.5 & 52.4 & 1.3 & 7.3 \\\\\\\\\\n\\\\hline\\n\\\\text{Average} & 34.5 & 9.9 & 38.7 & 1.2 & 4.2 & 46.2 \\\\\\\\\\n\\\\end{array} \\\\]\\n\\nFigure 2: Comparison of average performance (chrF++) on the Italic and Indo-Aryan languages for all model types trained, both with pre-existing data only (P) and with the addition of all seed data (P+6k, hatched).\\n\\n7 Conclusions\\n\\nIn this paper, we have described a parallel data collection effort involving 6k seed parallel sentences for 39 languages, and investigated the effects of this relatively small but high-quality dataset on machine translation performance. By training hundreds of bilingual translation models, we have looked at the data scaling properties, and found that even when several thousand pre-existing sentences are already available, adding as little as a thousand high-quality parallel sentences can significantly boost performance.\\n\\nTo answer the question of whether stronger models can compensate for the lack of high-quality data, we moved beyond simple bilingual models and introduced two modelling improvements: multilingual training of closely related low- and high-resource languages, and backtranslation. We found that models trained with the additional high-quality data performed consistently better. Even when augmenting the models with vast amounts of monolingual data via backtranslation, the beneficial effects of seed data were still present.\\n\\nOverall, the results show that collecting high-quality parallel data, produced by native speakers and manually aligned, is a fundamentally important investment for training machine translation models.\"}"}
{"id": "acl-2023-long-154", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOther ways of reducing the amount of required supervision could be attempted, but we do not expect that these would change the outcomes significantly. Self-supervised learning via masking/denoising objectives, either in the form of an auxiliary task or via the use of pretrained models, is one such approach. This however generally underperforms backtranslation, which can utilise the same monolingual data to more effect (NLLB Team et al., 2022), as we see in the experiments of Appendix E. Iterative backtranslation might offer an additional boost for data-scarce settings, but is very computationally intensive, complex, and any gains would almost certainly apply to models trained with the addition of seed data too.\\n\\nThe seed datasets that we release bring about large translation performance gains for a number of low-resource languages. We note that, due to budgetary and complexity constraints, the source data we used was sourced from English Wikipedia only. This is likely to have two effects. First, translating English-original data leads to so-called translationese effects on the low-resource side (Volansky et al., 2015), leading to decreased effectiveness for directions that target low-resource languages. Second, the data is unlikely to adequately cover diverse content from multiple cultures. An interesting avenue for future research would therefore involve studying the effects of seed parallel data that is originally translated from low-resource languages.\\n\\nReferences\\n\\nAhmed Abdelali, Francisco Guzman, Hassan Sajjad, and Stephan Vogel. 2014. The AMARA corpus: Building parallel language resources for the educational domain. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 1856\u20131862, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajudeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valeria Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022a. A few thousand translations go a long way! leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3053\u20133070, Seattle, United States. Association for Computational Linguistics.\\n\\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Ayoadele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. 2021. The effect of domain and diacritics in Yoruba\u2013English neural machine translation. In Proceedings of the 18th Biennial Machine Translation Summit (Volume 1: Research Track), pages 61\u201375, Virtual. Association for Machine Translation in the Americas.\\n\\nDavid Ifeoluwa Adelani, Jesujoba Oluwadara Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajudeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Chinenye Emezue, Colin Leong, Michael Beukman, Shamsuddin Hassan Muhammad, Guyo Dub Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ayoade Ajibade, Tunde Oluwaseyi Ajayi, Yvonne Wambui Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, Anuoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Koffi Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valencia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022b. A few thousand translations go a long way! leveraging pre-trained models for African news translation. CoRR, abs/2205.02022.\\n\\n\u017deljko Agi\u0107 and Ivan Vuli\u0107. 2019. JW300: A wide-coverage parallel corpus for low-resource languages. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3204\u20133210, Florence, Italy. Association for Computational Linguistics.\\n\\nFelermino D. M. A. Ali, Andrew Caines, and Jaimito L. A. Malavi. 2021. Towards a parallel corpus of Portuguese and the Bantu language Emakhuwa of Mozambique.\\n\\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Francisco Guzm\u00e1n, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp \u00d6ktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020. TICO-19: the translation initiative for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 1), pages 2739\u20132748.\"}"}
{"id": "acl-2023-long-154", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Naveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, Wolfgang Macherey, Zhifeng Chen, and Yonghui Wu. 2019. Massively multilingual neural machine translation in the wild: Findings and challenges.\\n\\nPaul Azunre, Salomey Osei, Salomey Addo, Lawrence Asamoah Adu-Gyamfi, Stephen Moore, Bernard Adabankah, Bernard Opoku, Clara Asare-Nyarko, Samuel Nyarko, Cynthia Amoaba, Esther Dansoa Appiah, Felix Akwerh, Richard Nii Lante Lawson, Joel Budu, Emmanuel Debrah, Nana Boateng, Wisdom Ofori, Edwin Buabeng-Munkoh, Franklin Adjei, Isaac Kojo Essel Ampomah, Joseph Otoo, Reindorf Borkor, Standylove Birago Mensah, Lucien Mensah, Mark Amoako Marcel, Anokye Acheampong Amponsah, and James Ben Hayfron-Acquah. 2021. English-twi parallel corpus for machine translation.\\n\\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, Theresa Breiner, Vera Axelrod, Jason Riesa, Yuan Cao, Mia Xu Chen, Klaus Macherey, Maxim Krikun, Pidong Wang, Alexander Gutkin, Apurva Shah, Yanping Huang, Zhifeng Chen, Yonghui Wu, and Macduff Hughes. 2022. Building machine translation systems for the next thousand languages.\\n\\nHouda Bouamor, Nizar Habash, Mohammad Salameh, Wajdi Zaghouani, Owen Rambow, Dana Abdulrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani, Alexander Erdmann, and Kemal Oflazer. 2018. The MADAR Arabic dialect corpus and lexicon. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. 2018. Understanding back-translation at scale. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 489\u2013500, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2020. Beyond english-centric multilingual machine translation. The Journal of Machine Learning Research.\\n\\nOrhan Firat, Kyunghyun Cho, and Yoshua Bengio. 2016. Multi-way, multilingual neural machine translation with a shared attention mechanism. In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 866\u2013875, San Diego, California. Association for Computational Linguistics.\\n\\nThamme Gowda, Zhao Zhang, Chris Mattmann, and Jonathan May. 2021. Many-to-English machine translation tools, data, and pretrained models. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 306\u2013316, Online. Association for Computational Linguistics.\\n\\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Pengjen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzm\u00e1n, and Angela Fan. 2022. The Flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522\u2013538.\\n\\nBarry Haddow and Faheem Kirefu. 2020. Pmindia \u2013 a collection of parallel corpora of languages of india.\\n\\nKevin Heffernan, Onur \u00c7elebi, and Holger Schwenk. 2022. Bitext mining using distilled sentence representations for low-resource languages.\\n\\nVu Cong Duy Hoang, Philipp Koehn, Gholamreza Haffari, and Trevor Cohn. 2018. Iterative back-translation for neural machine translation. In Proceedings of the 2nd Workshop on Neural Machine Translation and Generation, pages 18\u201324, Melbourne, Australia. Association for Computational Linguistics.\\n\\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 79\u201386, Phuket, Thailand.\\n\\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.\\n\\nWilliam Lewis. 2010. Haitian Creole: How to build and ship an MT engine from scratch in 4 days, 17 hours, & 30 minutes. In Proceedings of the 14th Annual conference of the European Association for Machine Translation, Saint Rapha\u00ebl, France. European Association for Machine Translation.\\n\\nWilliam Lewis, Robert Munro, and Stephan Vogel. 2011. Crisis MT: Developing a cookbook for MT in crisis situations. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 501\u2013511, Edinburgh, Scotland. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-154", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-154", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Full language list\\n\\nThe full list of languages covered by the seed dataset is shown in Table 4.\\n\\nB Translation instructions\\n\\nWe include below the instructions that were shared with translators participating in this project.\\n\\nImportant note\\n\\nYour translations will be used to help train a Machine Translation engine. For this reason, this project requires Human Translation. The use of Machine Translation is strictly prohibited. Please read the section on Machine Translation for more details.\\n\\nGeneral instructions\\n\\n1. You will be translating different contents from Wikipedia pages. The source URL is available for more context. Please refer to it.\\n2. Do not convert any units of measurement. Translate them exactly as noted in the source content.\\n3. As the source material is Wikipedia pages, translations should use a formal tone.\\n4. Provide fluent translations without deviating too much from the source structure. Only allow necessary changes.\\n5. Do not expand or replace information compared to what is present in the source documents. Do not add any explanatory or parenthetical information, definitions, etc.\\n6. Do not ignore any meaningful text that was present in the source.\\n7. In case of multiple possible translations, please pick the one that makes the most sense (e.g., for gender concordance, cultural fit in the target language, level of formality, etc.).\\n8. Translations must be faithful to the source in terms of pragmatics such as (if applicable) level of hedging/modality, sentiment and its intensity, negation, speech effects (disfluencies), etc.\\n9. For proper nouns and common abbreviations, please see the guidelines on Named Entities below.\\n10. Idiomatic expressions should not be translated word for word. Use an equivalent idiom, if one exists. If no equivalent idiom exists, use an idiom of similar meaning. If no similar expressions exist in the target language, paraphrase the idiom such that the meaning is retained in the target language.\\n11. When a pronoun to be translated is ambiguous (for instance, when it could be interpreted as either him/her or he/she), opt for gender neutral pronouns (such as them/they) if those exist in the target language. However, when a pronoun to be translated is clearly marked for gender, you should follow the source material and continue to mark for gender.\\n\\nMachine translation\\n\\nThe translations you will provide are going to be used to train new Machine Translation engines. For this reason, the translations you provide should not be biased by existing Machine Translation providers. Therefore:\\n\\n1. Translators should not reference any Machine Translation engine at all when translating, to avoid being biased by it.\"}"}
{"id": "acl-2023-long-154", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language Name   | Code | Script | Family          | Subgrouping          |\\n|-----------------|------|--------|-----------------|----------------------|\\n| Acehnese        | ace  | Arab   | Austronesian    | Malayo-Polynesian    |\\n| Moroccan Arabic | ary  | Arab   | Afro-Asiatic    | Semitic              |\\n| Egyptian Arabic | arz  | Arab   | Afro-Asiatic    | Semitic              |\\n| Bambara         | bam  | Latn   | Mande Western   | Mande               |\\n| Balinese        | ban  | Latn   | Austronesian    | Malayo-Polynesian    |\\n| Bhojpuri        | bho  | Deva   | Indo-European   | Indo-Iranian         |\\n| Banjar          | bjn  | Arab   | Austronesian    | Malayo-Polynesian    |\\n| Buginese        | bug  | Latn   | Austronesian    | Malayo-Polynesian    |\\n| Crimean Tatar   | crh  | Latn   | Turkic          | Southern Turkic      |\\n| Southwestern Dinka | dik | Latn  | Nilo-Saharan    | Western Saharan      |\\n| Dzongkha        | dzo  | Tibt   | Sino-Tibetian   | Bodic                |\\n| Friulian        | fur  | Latn   | Indo-European   | Italic               |\\n| Nigerian Fulfulde | fuv | Latn   | Atlantic-Congo  | North-Central Atlantic |\\n| Guarani         | grn  | Latn   | Tupian          | Maweti-Guarani       |\\n| Chhattisgarhi   | hne  | Deva   | Indo-European   | Indo-Iranian         |\\n| Kashmiri        | kas  | Arab   | Indo-European   | Indo-Aryan           |\\n| Central Kanuri  | knc  | Arab   | Nilo-Saharan    | Western Saharan      |\\n| Latgalian       | ltg  | Latn   | Indo-European   | Balto-Slavic         |\\n| Magahi          | mag  | Deva   | Indo-European   | Indo-Iranian         |\\n| Meitei          | mni  | Beng   | Sino-Tibetian   | Kuki-Chin-Naga       |\\n| Maori           | mri  | Latn   | Austronesian    | Malayo-Polynesian    |\\n| Nuer            | nus  | Latn   | Nilo-Saharan    | Western Saharan      |\\n| Dari            | prs  | Arab   | Indo-European   | Indo-Iranian         |\\n| Southern Pashto | pbt  | Arab   | Indo-European   | Indo-Iranian         |\\n| Sicilian        | scn  | Latn   | Indo-European   | Italic               |\\n| Shan            | shn  | Mymr   | Tai-Kadai       | Kam-Tai              |\\n| Sardinian       | srd  | Latn   | Indo-European   | Italic               |\\n| Silesian        | szl  | Latn   | Indo-European   | Balto-Slavic         |\\n| Tamasheq        | taq  | Tfng   | Afro-Asiatic    | Berber               |\\n| Central Atlas Tamasheq | tzm  | Tfng  | Afro-Asiatic    | Berber               |\\n\\nTable 4: Focus languages for which seed data was collected. We adopt the same language subgrouping approach as NLLB Team et al. (2022).\\n\\n2. All translations will be inspected, and those that are found to be too close to Machine Translation output will be returned to the translator. These will need to be revised, or the translator will be required to provide a quick explanation as to why the translation cannot be modified further without affecting its meaning.\\n\\nNamed Entities\\n\\nNamed Entities are people, places, organisations, etc., that are commonly referred to using a proper noun. This section provides guidance on how to...\"}"}
{"id": "acl-2023-long-154", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"handle Named Entities. Please review the following guidelines carefully:\\n\\n1. If there is a commonly used term in the target language for the Named Entity:\\n   (a) If the most commonly used term is the same as in the source language, then keep it as it is.\\n   (b) If the most commonly used term is a translation or a transliteration, then use that.\\n\\n2. If there is no commonly used term:\\n   (a) If possible, a transliteration of the original term should be used.\\n   (b) If a transliteration would not be commonly understood in the context, and the source term would be more acceptable, you may retain it.\\n\\nC Experimental details\\n\\nWe compute ChrF++ scores using the sacrebleu implementation, with the following signature:\\nchrF2++|nrefs:1|case:mixed|eff:yes|nc:6|nw:2|space:no|version:2.1.0.\\n\\nTraining is conducted via the fairseq framework; example training configurations for both bilingual and multilingual models are made available.\\n\\nD Performance of bilingual models\\n\\nThe full results of bilingual translation experiments for unresourced and barely-resourced languages is reported in Tables 5 and 6 respectively.\\n\\nE Self-supervised learning\\n\\nIn order to evaluate the effectiveness of self-supervised learning on monolingual data (SSL), we conduct a series of experiments with our two multilingual models of Sections 4.3 and 4.4. The setup of these experiments follows the denoising autoencoder technique of Liu et al. (2021).\\n\\nOne possible approach would be to pre-train on a denoising task, and subsequently fine-tune on...\"}"}
{"id": "acl-2023-long-154", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: Pre-existing data availability (#P, thousands of sentences) and performance (chrF++) of bilingual barely-resourced models using increasing amounts of seed data with (P+{1,3,6}k) and without (6k) pre-existing data.\\n\\n| Language | eng-xxx | xxx-eng |\\n|----------|---------|---------|\\n| BT SSL | BT SSL | |\\n| fur_Latn | 56.4 | 50.4 | 61.9 | 59.3 |\\n| lij_Latn | 53.0 | 49.8 | 64.7 | 62.2 |\\n| lmo_Latn | 33.7 | 32.5 | 55.5 | 53.1 |\\n| scn_Latn | 45.1 | 41.9 | 57.1 | 53.8 |\\n| srd_Latn | 55.7 | 49.9 | 61.3 | 58.7 |\\n| vec_Latn | 50.7 | 49.1 | 62.3 | 60.5 |\\n| bho_Deva | 38.5 | 36.9 | 50.4 | 46.3 |\\n| hne_Deva | 48.2 | 46.6 | 62.4 | 55.5 |\\n| kas_Deva | 15.8 | 13.9 | 38.1 | 33.7 |\\n| mag_Deva | 52.4 | 49.6 | 62.7 | 58.3 |\\n\\nTable 7: Performance (chrF++) of the Italic and Indo-Aryan multilingual models augmented with either back-translation (BT) or self-supervision (SSL), when using all available training data (P+6k).\"}"}
{"id": "acl-2023-long-154", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\nLeft blank.\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nNot applicable. Left blank.\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\nLeft blank.\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\n\\nB Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\nsection 3\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nsection 3\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nNot applicable. All datasets used were intended for machine translation\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nNot applicable. Data collected from Wikipedia\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nsection 3 \u2013 demographic information not available due to privacy regulations\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nFor the created dataset: section 3 and Appendix A.\\n\\nC Did you run computational experiments?\\nsections 4, 5\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nsection 4 (we report the size of the model in terms of layers, embedding size, etc.)\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
