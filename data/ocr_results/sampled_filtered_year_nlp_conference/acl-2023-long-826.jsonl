{"id": "acl-2023-long-826", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Personality Understanding of Fictional Characters during Book Reading\\nMo Yu1\u2217 Jiangnan Li2\u2217 Shunyu Yao3 Wenjie Pang1 Xiaochen Zhou4 Xiao Zhou1 Fandong Meng1 Jie Zhou1\\n\\n1 Pattern Recognition Center, WeChat AI\\n2 Institute of Information Engineering, Chinese Academy of Sciences\\n3 Princeton University\\n4 Syracuse University\\n\\nmoyumyu@global.tencent.com lijiangnan@iie.ac.cn\\n\\nAbstract\\nComprehending characters' personalities is a crucial aspect of story reading. As readers engage with a story, their understanding of a character evolves based on new events and information; and multiple fine-grained aspects of personalities can be perceived. This leads to a natural problem of situated and fine-grained personality understanding. The problem has not been studied in the NLP field, primarily due to the lack of appropriate datasets mimicking the process of book reading. We present the first labeled dataset PERSNET for this problem. Our novel annotation strategy involves annotating user notes from online reading apps as a proxy for the original books. Experiments and human studies indicate that our dataset construction is both efficient and accurate; and our task heavily relies on long-term context to achieve accurate predictions for both machines and humans.\\n\\n1 Introduction\\nLively fictional characters with distinct personalities are the first drive of the plotline developments. The authors shape the characters with various personality types, which distinguish a character from others and explain the motivations and behaviors of the characters. As a reverse process, the readers grasp the characters' personalities during reading a story, which helps to understand the logics of a plot and predict its future developments.\\n\\nThe NLP community has also recognized the values of personality understanding; and conducted studies (Bamman et al., 2013; Flekova and Gurevych, 2015; Sang et al., 2022b) along this direction. In the problem definition of the existing tasks, the input is an entire book. By construction, they ask for the general impression of character personalities. Also for this reason, they only focus on coarse-grained personality types, e.g., the four coarse MBTI types (Myers and McCaulley, 1988).\\n\\n\u2217 Authors contributed equally to this paper.\\n1 Available at https://github.com/Gorov/personet_acl23.\\n\\nFigure 1: Illustration of the WeRead App interface enabling our dataset construction (user IDs masked for privacy); and an example of our task, situated personality prediction.\\n\\nTo make a personality prediction task more practical and useful, we consider two important aspects of character understanding in real life that have not been studied in the context of machine reading. First, we aim at predicting fine-grained personality types, with an exhaustive vocabulary of personality traits as the targets. Second and more importantly, we study the continuous-process nature of story reading \u2014 As people read, they form dynamic impressions of the characters and plots. We name this process situated comprehension. Specific to personality understanding, a character may have multi-faced personalities. In a certain point of the story, the character's behaviors can reflect one of them when faced the situation and events at the time. Human readers have the ability to use their knowledge of what has happened so far (i.e., the history) to understand the character in the current situation. We hence propose to study situated personality prediction, which differs from the static prediction problem studied before.\\n\\nWhile the aforementioned two problems are practical and common in real life, they create new\"}"}
{"id": "acl-2023-long-826", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"challenges in dataset creation, especially the latter. To accurately mimic the human reading process, annotators would need to read entire books, which is not practical due to the significant time required. We overcome this annotation difficulty and create a large-scale dataset for personality prediction in the reading processes. To achieve this goal, we propose a novel annotation strategy that utilizes publicly available book notes. Recent online reading apps such as WeRead (shown in Figure 1) allow users to take notes while reading a book. As users read, they can add notes at the current reading position. These notes are linked to specific text in the book, which is highlighted with a dotted underline, referred to as underlined texts throughout this paper. This mechanism ensures that the notes accurately reflect the thoughts the user had while reading the surrounding text of the underlined text. Based on this resource, we propose our strategy of annotating user notes as a delegate of the book reading process. Specific to our task of personality prediction, this corresponds to (1) identifying if a user note discusses the personality trait of a character; and (2) associating the trait label to the context at the note location. We take user notes that contain at least a character name and a personality trait word, and ask human annotators to confirm if the trait is a modifier of the character in the note text (i.e., the user note mentions that the character has the trait). The verified notes serve as natural labels of character personalities reflected by the surroundings of the underlined texts. By using this approach, we collect labeled data that only requires annotators to read short notes, without the need for knowledge about the books themselves.\\n\\nWith our new strategy, we create our situated personality prediction dataset, PERSONET, that contains \u223c32K instances from 33 books in the classic literature domain. We prove that our annotation strategy is efficient as each worker only requires a median of 23.7s to finish one sample. The whole annotation process costed in total $2,400 and 471.8 hours (distributed to 20 working days by 11 annotators). It is also accurate evidenced by both the over 88% inter-annotator agreement. In addition, we make the dataset bilingual in both English and Chinese with automatic book sentence alignment and manual character alignment.\\n\\nWe conduct experiments with our dataset in two folds. First, we develop various improvement over the standard pre-trained models, including enabling the models to use different types of long contexts, equipping the models with oracle history trait information, and task-oriented unsupervised training. Second, we conduct extensive human studies with people who have read the books (i.e., with the knowledge of the book history) and not. Our results show that (1) our task is challenging as humans with knowledge of book history can achieve more than 70% accuracy, compared to the best model accuracy of \u223c45%; (2) our task heavily requires the long context modeling, as introducing characters' history information significantly improves the model accuracy; and humans without the book history can only perform on par with models.\\n\\nWe make the following contributions:\\n\\n\u2022 A dataset, PERSONET, that is the first benchmark of situated reading comprehension and of fine-grained personality prediction on books. We prove that our dataset is a valid assessment to long context understanding for both machines and humans without significant shortcuts.\\n\u2022 A novel dataset creation approach for book comprehension problems based on user notes, which is efficient and accurate.\\n\u2022 Task-oriented unsupervised training and character history enhancement methods that improve on our task, providing insights to future work.\\n\\nRelated Work\\nStory book understanding has been recognized as a challenging and rewarding direction (Piper et al., 2021; Sang et al., 2022a). Many evaluation benchmarks on various narrative understanding tasks have been developed, such as plot structure analysis (Saldias and Roy, 2020; Papalampidi et al., 2019), question answering (Richardson et al., 2013; Ko\u010disk\u00fd et al., 2018; Xu et al., 2022), summarization (Ladhak et al., 2020; Kryscinski et al., 2021; Chen et al., 2022), character identification and relationship extraction (Elson et al., 2010; Elsner, 2012; Elangovan and Eisenstein, 2015; Iyyer et al., 2016; Chaturvedi et al., 2016; Kim and Klinger, 2019; Sang et al., 2022c).\\n\\nAll of the prior work takes the entire long story as input to a model for predictions. None of them considers the situated reading process like ours. Existing strategies of dataset construction over long stories fall into the following categories: \u2022 A straightforward way is to have labelers read the entire stories. Because of the huge efforts, it only...\"}"}
{"id": "acl-2023-long-826", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"works for short stories for young children (Xu et al., 2022) or simpler tasks like coref (Bamman et al., 2019), NER (Bamman, 2020) and quotation attribution (Vishnubhotla et al., 2022).\\n\\nUsing the book summaries as proxy of the original stories, e.g., the creation of book-level question answering task (Ko\u02c7cisk`y et al., 2018). The created data usually only covers abstract and major events in the book, as shown in (Mou et al., 2021). Thus the types of comprehension skills that can be assessed with this strategy are limited.\\n\\nExploiting Web resources created by fans or experts. Flekova and Gurevych (2015) used fans' rated MBTI types to create a classification task for book characters; Ladhak et al. (2020); Kry\u00b4sci\u00b4nski et al. (2021) created a book chapter summarization task based on summaries on the English learning websites; and Thai et al. (2022) created a book retrieval task based on quotes in literature reviews. The drawback of this strategy is that the tasks can be supported are limited by the available resources.\\n\\nAutomatically created cloze tests is a traditional strategy. With specifically designed techniques, the clozes can be made resolvable only with global context, e.g., (Rae et al., 2020; Sang et al., 2022c; Yu et al., 2022). The problem of this method is that the created datasets usually have unclear assessment goals.\\n\\nThe limitations of these strategies make them insufficient to create datasets for our task of situated personality understanding.\\n\\n3 Problem Definition\\n\\nOur PERSONET is the first task on situated prediction of characters' personality traits in book contexts. That is, we aim to predict the traits reflected by a local snippet of book, given all the previous book content as the background (Figure 1).\\n\\nFormally, we consider a local book snippet $S(i) = \\\\{s_k(i)_1, s_k(i)_2, ..., s_k(i)_J\\\\}$. Each $s_k(i)_j$ is a sentence from the book, with $k(i)_j$ the absolute position of the sentence in the book. Each $S$ in our task depicts a character's personality. Therefore, it is associated with a pair of $(c, t)$, where $c$ is a character name or alias and $t$ is the personality trait of $c$ that reflected by $S$. Note that different pairs may share a same snippet. Our task is then to predict:\\n\\n$$P(y = t | c, S(i), H(i) = s_1:k(i)_1)$$\\n\\nwhere $s_1:k(i)_1$ refers to all the sentences before $S(i)$ in the book. We split the books into training, dev and test sets, so that the evaluation characters are unseen during training. For evaluation, we adopt a multi-choice setting. For each instance, we sampled 4 negative candidates, two from the top-20 most frequent traits and the rest from the whole list. Combining the negative choices with $t$, we have a candidate set $T$. Our data thus form a tuple $(S, H = s_1:k(i)_1, c, t, T)$.\\n\\n4 Our PERSONET Dataset\\n\\n4.1 Data Source\\n\\nList of Personality Traits\\n\\nFollowing previous work (Shuster et al., 2019), we use the list of 818 English personality traits from the MIT Ideonomy project. We translate the traits into Chinese with Youdao dictionary, then ask human annotators to select all the translated meanings that depict personality in Chinese. There are 499 English traits and 565 Chinese traits left that are bilingually aligned.\\n\\nBooks and Notes\\n\\nWe collect 100 public books available in the Gutenberg project. For each book, we find all its Chinese-translated versions on the WeRead app and collect all their user notes. We kept notes that (1) contain any traits, (2) contain any person names and (3) with lengths less than 100 words (relatively shorter notes can improve human annotation efficiency). We filtered out the books with less than 100 notes left, leaving 33 books and 194 of their Chinese translations. These books have 110,114 notes that contain 140,268 traits in total.\\n\\nNote Clustering\\n\\nIt is common for multiple users to comment on the same part of a book, discussing the same character. When these users express similar opinions about a character, it leads to duplication. To remove this duplication for data annotating, we group the notes according to their positions, defined as the center token offset of its associated snippet $S(i)$ (i.e., its underlined text). Notes with distances smaller than 100 tokens are grouped, leading to 27,678 note clusters. We take the unique traits within each cluster for human labeling, which corresponds to 113,026 samples as defined in Section 3. The notes are anonymized for human annotation.\\n\\nExtension of the Snippets\\n\\nThe lengths of underlined texts can vary significantly, which means they may not always provide a representative context...\"}"}
{"id": "acl-2023-long-826", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for reflecting a character's personality, particularly when the texts are very short. We address this issue by extending each $S_i$ from the underlined text to a window of 480 tokens. This window is generally large enough to encompass a scene and ensures that the context relevant to the user note is included. The reason for choosing this window size is that it is typically longer than one page displayed by the WeRead App (as shown in Figure 1) \u2014 users often write notes on the same page while reading the context, rather than flipping through previous or subsequent pages.\\n\\n4.2 Dataset Construction\\n\\nOur dataset construction consists of two major steps: (1) human annotation of user notes; (2) projection of labeled data from Chinese to English. In addition, we show that (3) our data construction strategy enables to build an accurate note classifier for automatic weakly-supervised data labeling.\\n\\nStep 1: Human Annotation\\n\\nThis step requires the annotators to read each user note, and determine if it discusses the personality of a character. We present the annotators with notes that contain at least one trait word in our vocabulary in Section 4.1. The note is paired with the underlined book content, which is optional to read, if they think the note itself is ambiguous. The annotators are then asked to (1) judge if the note is indeed about a certain character's trait; then (2) marked the target character name with the trait from the note.\\n\\nThe first step takes most of the human efforts. We wrote concrete guidelines (Figure 4 in Appendix A) for the decision making process. The annotators are citizens in China who have received at least high school education (which, in the Chinese education system, covers most of the general knowledge about classic literature). Therefore it is more convenient for them to work in Chinese; and Figure 4 lists both the original guidelines in Chinese and their English translations.\\n\\nOur annotation interface (with English translations) is shown in Appendix A. Once the annotators confirm that the given trait word describes some characters, they are required to annotate the character name by dragging from the note text. If not, the character name will be left empty.\\n\\nStep 2: Bilingual Projection\\n\\nThe human annotation step has created a personality prediction dataset in Chinese. Next we project the data to English. Since the same English book may have multiple translated books in Chinese, their labeled data scattered. By projecting the labeled data to English books, the book version is unified and the annotations become dense.\\n\\nAccording to Section 3, to create an English version of our dataset, we only need to project the traits $t$, the characters $c$ and the snippets (positions) $S$. The trait $t$ is already from a bilingual vocabulary, so we only need to focus on the latter two. \\n\\n- Book Alignment\\n\\nThe projection of $S$ is equivalent to finding each labeled instance\u2019s sentence positions in the English book, which is essentially a sentence alignment problem. Specifically, we sentencize the books firstly with Spacy; then utilize the vecalign (Thompson and Koehn, 2019) toolkit to achieve sentence alignments among books. We represent each sentence with the default number (10) of its consecutive sentences, and employ the multilingual sentence embedding LASER 7 to embed the sentences. After that, vecalign performs sentence alignments with dynamic programming based on the embeddings.\\n\\nWith bilingual sentence alignment, the position of each labeled instance can be mapped to the corresponding position in the English book, i.e. $S_{en} = \\\\{a(s) | \\\\forall s \\\\in S\\\\}$, where $a(s)$ refers to aligned position of the Chinese sentence $s$ in the English book. For most of the $S$ in our dataset, we can find consecutive $S_{en}$ as the aligned results. There are a few instances mapped to empty. We excluded these cases in our English dataset. There are also a few instances mapped to inconsecutive English sentences, sometimes in a wide range. For this situation, we take the median position of the mapped English sentences and include the consecutive context in a window as the projection.\\n\\n- Character Name Projection\\n\\nWe manually project the list of 377 frequent (appear > 10 times in our labeled data) character names to English. We asked two annotators to find the English names of these characters; and resolved all the inconsistency after they complete their own annotation jobs.\\n\\nStep 3: Weakly-Supervised Data\\n\\nOur method reduces the problem of annotation over books to annotation over notes. This makes it possible to build a note classifier for automatic data augmentation. We collect another 65,521 notes from the same book collection that consists of at least one trait\\n\\n7https://github.com/facebookresearch/LASER.\"}"}
{"id": "acl-2023-long-826", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"word and one person name. By pairing traits with names within the same notes, we create 154,030 examples. Then we train a binary \\\\textit{roberta-wwm} (Cui et al., 2020) classifier over our human-labeled data to determine if the note discusses the character\u2019s trait, i.e., the same task in human annotation but without the need of marking target characters. For each human annotated note, if the note is recognized as describing a trait of a character, it is used as a positive example. For those labeled as irrelevant to character traits, i.e., no characters are annotated, we denote them as negative examples. Cross-validation on the human-labeled data shows that our classifier is accurate: 91.1% and 90.2% on the dev and test set. Applying our classifier to these unlabeled examples, we recognize 31,346 examples as describing characters\u2019 traits.\\n\\n### 4.3 Quality of the Annotated Data\\n\\nThis section proves the accuracy of our data construction method via human study.\\n\\n#### Correctness of Book Notes\\n\\nFirst of all, we need to prove that the user notes are indeed an accurate delegate of books. That is, when a note mentions a personality of a character, whether it is highly consistent to what the book content reflects. This study requires annotators who have read the books to make the correct judgement. We selected four books with two annotators who have read and are familiar with them. Each annotator labeled two books. We sampled in total 431 notes from these books. The annotators are required to judge if the note is accurate about the character or not. We present the corresponding underlined content along with the note, so that the annotators can identify which part the note is commenting.\\n\\nThe results in Table 1 show that 89.1% of the notes are accurate understanding of the books. There are 9.7% ambiguous examples, meaning the annotated traits are implied by the current place of the books, but might be falsified later, e.g., the authors may intend to mislead the readers to create surprisal or tension. These ambiguous labels give valid data for our problem of dynamic personality prediction, according to our description at the beginning of Section 1 and Eq. (1).\\n\\n#### Accuracy of Human Labels\\n\\nNext, we proved that our annotation process leads to accurate human labels. This accuracy is verified in two ways. First, we compute the inter-annotator agreement, with a duplicated set of 3,000 notes during annotation. 88.67% of the duplicated samples receive consistent labels. The Cohen\u2019s Kappa (Cohen, 1960) is 0.849, which indicates nearly perfect agreement (Viera et al., 2005). Second, as shown in the Step 3 in Section 4.2, a fairly accurate note classifier can be trained on our human-labeled data (91.1% and 90.2% accuracy on dev and test). Both tests confirm the accuracy of our annotation strategy. Considering the relevance of the book notes (Table 1), this gives an estimation of overall accuracy around 87.6 \u2013 89.1%. The two endpoints are computed with inter-annotator agreement and classifier accuracy accordingly. It confirms that our dataset is overall accurate.\\n\\nTable 7 in Appendix B gives some difficult examples that created disagreements. There are two major sources of difficulties: (1) the trait word has multiple meanings in Chinese and the usage does not represent the sense of the trait; (2) a trait word is used to recall the general impression or history behavior of a character in an implicit way.\\n\\n#### Accuracy of Cross-Lingual Alignment\\n\\nFinally, we evaluate the quality of the bilingual alignment. We randomly sampled 200 labeled instances for human study. We present to the annotators the snippet $S$ of each instance in the Chinese book and their aligned sentences from the English books. The human annotators were asked to rate the alignments into four grades: \\\\textit{perfect}/\\\\textit{high overlap}/\\\\textit{low overlap}/\\\\textit{no match}, i.e., all/\\\\textgreater 50%/\\\\textless 50%/none of the Chinese sentences have their translations in the paired English sentences. Table 2 show that >97% of the cases fall into the \\\\textit{perfect} and \\\\textit{high overlap} categories. When taking texts from the median\"}"}
{"id": "acl-2023-long-826", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4 Data Statistics and Visualization\\n\\nData Statistics\\n\\nTable 3 shows the statistics of our PERSONET. We give the full list of books in Appendix C. We can also see that our dataset contains a wide range of book characters. In the annotated training set, approximately 41% of the notes are about positive traits, 36% are about negative traits, and 23% are about neutral traits. This distribution reveals a slight bias, which can be attributed to the fact that users are more inclined to write notes when they have strong sentiments or opinions about a character.\\n\\nVisualization of Our Dataset\\n\\nFigure 2 visualizes the major traits and the polarity of traits over time for two of the most popular characters. It can be found that the major traits match readers' common impressions; and the trends well align with the common feelings of readers during reading. This further confirms the quality of our data.\\n\\n5 Models for Persona Prediction\\n\\nWe design models based on two different types of pre-trained models, BERT (Devlin et al., 2019) and Longformer (Beltagy et al., 2020). We use the latter model to investigate the strength of models that are pre-trained to handle long contexts.\\n\\n5.1 Input to the Reader Models\\n\\nOur data instance consists of a tuple $(S, H, c, t, T)$. Here $S$ is a book snippet that expresses a personality trait $t$ of a character $c$. $H$ is the previous history of $S$ in the book. $T$ is a set of candidate traits with $t$ as an element. The task is to rank $t$ to the top within $T$ given $(S, H)$ and $c$. We represent the input $(S, H, c)$ with the following format options:\\n\\n- No history: represents the input as $x = [c[SEP]S]$, i.e., does not use the history $H$.\\n- Extended history: $x = [c[SEP]S[SEP]H_{prev}]$, where $H_{prev} \\\\subset H$ includes sentences that are adjacent to $S$, truncated by models' length limited.\\n- Character history: $x = [c[SEP]S[SEP]S_c]$. $S_c \\\\subset H$ includes snippets to the left of $S$ that contains the character $c$ in our dataset.\\n\\n5.2 Model Architectures\\n\\nOur methods compute the score of an input $x$ having a trait $t$, based on the siamese model.\\n\\nText Encoding\\n\\nFirstly, we use a pre-trained LM (PLM, either BERT or Longformer) to encode $x$ and $t$ to the embedding space. The contextualized embeddings of input and output are denoted as $X = PLM(x) \\\\in \\\\mathbb{R}^{l_x \\\\times d}$, where $l_x$ is the length of $x$ and $d$ is the size of hidden states; and $T = PLM(t) \\\\in \\\\mathbb{R}^{l_t \\\\times d}$, with $l_t$ the length of $t$.\\n\\nBaseline Siamese Model\\n\\nAs our baseline models, we compute a weighted sum over $X$ to get a vector representation of the input. Specifically, we use a linear model to compute the attention score over each token of $x$:\\n\\n$$A = \\\\text{Attention}(H), \\\\quad \\\\alpha = \\\\text{Softmax}(A).$$\\n\\nThe attention $\\\\alpha$ is then used to summarize the hidden states into a vector $x = X^T \\\\alpha$.\\n\\nThe sequence of a trait $t$ is usually short (e.g., a single word's BPE tokenization). Therefore we simply take the average $t = \\\\text{mean}(T)$. The model makes prediction with $t = \\\\arg \\\\max_{t \\\\in T} \\\\langle x, t \\\\rangle$. Contextualization with History\\n\\nWhen the input $x$ contains the extended or character history as defined in Section 5.1, we need to separate the information of the history from the current context. We maintain a mask $H \\\\in \\\\mathbb{R}^{l_x \\\\times 1}$, such that $H[j] = 1$ if the $j$-th word belongs to the appended history and 0 otherwise. Two attention vectors are computed for the current snippet and the history:\\n\\n$$\\\\alpha_s = \\\\text{Softmax}(A \\\\odot (1 - H)), \\\\quad \\\\alpha_h = \\\\text{Softmax}(A \\\\odot H).$$\"}"}
{"id": "acl-2023-long-826", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The corresponding summarized vectors are $s = X^T \\\\alpha s$ and $h = X^T \\\\alpha h$. The prediction function is then modified with a gating function $\\\\sigma(s)$ added:\\n\\n$$t = \\\\arg \\\\max_{t \\\\in T} \\\\sigma(s) < s, t > + (1 - \\\\sigma(s)) < h, t >.$$  \\n\\n(2)\\n\\n5.3 Unsupervised Training\\n\\nFinally, we propose an unsupervised training task to improve personality prediction. The unsupervised task is used to pre-train the classifiers, before they are fine-tuned on our labeled data. The task mimics the problem definition in Section 3 and constructs tuples of $(S, t)$. We first extract sentences that contain trait words. If a sentence $s_j$ contains a trait, we keep a local window of it as the book snippet, with the sentence itself removed. That is, $S(i) = \\\\{s_j - w, \\\\ldots, s_j - 1, s_j + 1, \\\\ldots, s_j + w\\\\}$. Intuitively, since $S$ provides the context of $s_j$, it is informative for inferring the appearance of the trait described in $s_j$. Therefore this unsupervised task helps to find narrative clues of traits thus can help to better pre-train the encoders.\\n\\nThe method has the limitation of not being character-specific, hence not compatible with our character-history-based models. We leave it to future work.\\n\\n6 Experiments\\n\\n6.1 Experimental Settings\\n\\nWe use bert-base-uncased and longformer-base-4096 as backbones for English experiments; and Roberta-wwm-ext for the Chinese experiments. Hyperparameters For our siamese models with and without history, the most important hyperparameter is the lengths of $S$ and $H$. We set the maximal length of $S$ to 480 tokens for most of the models. For models with history we set the maximum of $|S| + |H| = 1,600$. To show the better performance of our usage of history, we also compare with Longformer with a maximum $|S| = 2K$ tokens (the best a single A100 GPU can handle). The batch size is 40 for BERT-based models; and 8 for Longformer-based models with gradient accumulation every 5 batches. Each epoch of BERT and Longformer models takes $\\\\sim 7$ and $\\\\sim 40$ minutes respectively on a single A100 GPU. We set the learning rate to $2 \\\\times 10^{-5}$. We conduct early-stopping on the dev set; and run 5 times to compute the average and stand derivation for all the methods.\\n\\nAdditional Baselines\\n\\nBesides the models in Section 5, we further compared with the follows:\\n\\n\u2022 Models with Oracle Traits in History, which uses the character\u2019s history traits in replace of the history texts. For each instance, we take its target character $c$\u2019s other instances prior to it, and concatenate their groundtruth traits as a sequence to replace $H$ in the model of Eq. (2).\\n\\n\u2022 Char-Majority, which always predicts the most frequent trait for a character. This is used to show the diversity of traits for the same character (i.e., necessity of situated prediction).\\n\\n\u2022 GPT-davinci (text-davinci-003), the few-shot instruct-GPT (Ouyang et al., 2022).\\n\\n\u2022 ChatGPT, which conduct zero-shot prediction on our task thus can take longer inputs. We test $|S| = 480$ and 1.6K as in our experiments with trained models.\\n\\n\u2022 Humans: we present the same format of our instances with maximal $|S| = 480$ to humans to get their performance.\\n\\nFurthermore, we added LoRA (Hu et al., 2022) fine-tuning of the LLaMA (Touvron et al., 2023) and WeLM (Su et al., 2022) on our PERSO NET as additional baselines. The fine-tuning of large language models and the usage of ChatGPT reflect the latest state-of-the-arts in concurrence with our work.\\n\\n6.2 Overall Results\\n\\nOur main results are shown in Table 4. First, all the three models without the usages of history achieve similar results. The Longformer with a 2K window does not give better performance, showing that simply increasing the length of input without including useful history information is not helpful for our task. Second, our model with character history achieves the best results. Replacing the character history with extended history slightly reduces the dev performance but lead to significant test performance drop (according to the standard derivation). Among all the supervised-only methods, this model is the only on that maintains consistent dev and test accuracy. Third, our unsupervised training significantly improve the accuracy for all the models. Fourth, the oracle history traits improve the supervised accuracy with a large margin. Yet for Longformer, adding character history and unsupervised training makes the gap smaller. Finally, the best human performance with knowledge of story history greatly outperforms all the models with and without oracle information with 20 $\\\\sim 23\\\\%$, showing the challenges and great potential of our PERSO NET. These results highlight the importance of incorporating history information in solving our...\"}"}
{"id": "acl-2023-long-826", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Overall performance (%) on our PERSON-EN task.\\n\\nTable 5: 5-choice accuracy (%) on our PERSON-ZH task.\\n\\nHyperparameter tuning, we specifically adjust the rank $r$, weight $\\\\alpha$, and number of training epochs. For model selection, we rely on the accuracy on the development subset utilized in our human study, which sets $r = 8$, $\\\\alpha = 1$, and 10 training epochs.\\n\\nThe results in Table 4 and 5 show that the fine-tuned LLM achieves slightly better results compared to our proposed baselines. However, it still significantly lags behind human performance by a considerable margin. Interestingly, unlike the other models and humans, the fine-tuned LLM perform better on the testing subset compared to the development one. Our hypothesis is that the testing book Notre-Dame de Paris is more popular on the Internet, thus may be more sufficiently trained during the pre-training stages of LLaMA and WeLM.\\n\\nThe LLM fine-tuning results can be potentially improved by employing a contrastive training approach similar to our proposed models. We leave this to future study.\\n\\n6.3 Human Study\\n\\nWe conduct human study to understand the challenges of our task. We sampled instances from the two books that have most instances from the development and testing sets; and asked human annotators (who are co-authors of the paper but have not seen the labeled data before) to complete our multi-choice task. There are two types of annotators: Type-I who have not read the books before (human w/o history); and Type-II who have read the books (human w/ history).\\n\\nWe have annotated in total 160 samples. Each sample is guaranteed to be annotated by two humans, one with history and one without history.\\n\\nRatio of Ambiguous Instances\\n\\nSometimes an event in a book can depict multiple aspects of personality. When the sampled negative choices share similarity to these personality traits, it leads to ambiguous cases with more than one correct answers. To investigate these cases, we require the Type-II...\"}"}
{"id": "acl-2023-long-826", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are 41 ambiguous samples recognized, i.e., \u223c25% of the cases have more than one correct answer. This indicates an \u223c87.2% approximated upper bound accuracy of our task, if we consider each ambiguous instance has two choices that are correct. In the future, we can leverage our note clusters to mitigate this ambiguity by ensuring that negative candidates do not appear in the cluster from which the snippet originates.\\n\\n**Main Findings**\\n\\nThe knowledge of book history is not only important to models, but also to humans. Table 6 compares humans performance with and without history. There is a \u223c25% performance gap. Furthermore, human performance without history is only comparable to the best model performance (selected according to dev accuracy, which performs 47.18% and 47.21% on the full dev and test data). These results confirm that our task raises the core challenge of long context understanding.\\n\\nDetailed results show that the Type-I annotators labeled \u223c35% of cases that they believe unsolvable because of their lacking of the book history. After verification by Type-II annotators, there are 37 cases left for close examination. It reveals that the history information is critical for these cases for two major reasons: (1) there are multiple possible answers given the snippets but with the knowledge of the characters' history behavior the incorrect traits can be resolved (17 of 37); (2) the plots in the snippets cannot be understood and linked to any personality without book history (11 of 37). There is a third difficult category (9 of 37), where reasoning is required to draw connections, e.g., consequence or analogy between the current snippet and a character's previously demonstrated personality. Examples of these categories can be found in Table 10 in Appendix E.\\n\\n6.4 Analysis\\n\\n**Learning Curve**\\n\\nFigure 3 plots the learning curve of our PERSO NET task. The curves shows that the size of our dataset is large enough as the curves become flat after the point of 30K. More importantly, the results justify the accuracy of our data construction strategy. As adding weak supervision (all) significantly outperforms training with only human-labeled data (dotted lines).\\n\\n---\\n\\n### Table 6: Comparison of performance among models and humans\\n\\n| Model                  | Unambiguous | Best model      |\\n|------------------------|-------------|-----------------|\\n| GPT-davinci 5-shot     | 33.33       | 38.46           |\\n| ChatGPT zero-shot      | 37.74       | 41.03           |\\n| LLaMA + LoRA-sft       | 48.43       | 52.63           |\\n| Human w/o history      | 42.50       | 50.42           |\\n| Human w/ history       | 67.92       | 73.50           |\\n\\nThe Unambiguous subset is annotated by annotators who have read the books.\\n\\n---\\n\\n### Difficult Trait Types\\n\\nWe examine the traits that appear more than 20 times in the dev set. The most difficult types include Confident (0.00%), Mature (5.56%), Liberal (7.41%), Humorous (7.69%), Impressionable (8.82%), Gentle (9.09%), Optimistic (10.81%), Rational (11.36%), Imprudent (14.29%), and Insincere (16.00%). It can be found that most of the difficulty types are abstract, which are usually not explicit depicted in the books but require reasoning from characters' behaviors.\\n\\n---\\n\\n7 Conclusion\\n\\nWe propose a dataset PERSO NET for the new problem of situated personality understanding of book characters. We overcome the difficulty in dataset construction with a new strategy of annotating the user notes as a proxy for the original books. Our dataset construction method maintains both efficiency and accuracy. Experiments show that the task raised challenges of long-text understanding for both humans and machines.\\n\\n**Limitations**\\n\\nOur propose annotation strategy can be applied to labeling other MRC problems, no matter situated comprehension ones or not. However, when generalizing to other problems other than personality prediction we studied here, the accuracy of the user notes may vary with the difficulty of tasks. Additional human verification on the correctness of annotators to mark the instances that they believe have ambiguous labels.8 Because these people have memory of the books, they can accurately distinguish the ambiguous cases from those can be disambiguated by the history.\"}"}
{"id": "acl-2023-long-826", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"notes like in our Section 4.3 need to be conducted.\\n\\nOur unsupervised training technique does not support the Longformer reader with character history (Char-Hist Longformer) yet. Therefore, the improvement from unsupervised training for our model is smaller.\\n\\nWhile Longformer is common in benchmarking for long story understanding tasks. There are other families of models (Rae et al., 2020; Izacard and Grave, 2021; Ainslie et al., 2020; Xiong et al., 2021; Pang et al., 2022) handling long text encoding. We leave the comparison with these models to future work.\\n\\nPotential Risks\\n\\nLike the other work that based on the similar set of books (Bamman et al., 2019; Bamman, 2020; Vishnubhotla et al., 2022; Thai et al., 2022), the classic literature may be limited by the time of writing, thus raise fairness considerations. However, please note that our dataset construction strategy is not limited to these books, but can work with any books on WeRead to create a sampled book set without such biases. The main reason we stick with the current list of books is for reproducibility since they are publicly available.\\n\\nReferences\\n\\nJoshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, and Li Yang. 2020. Etc: Encoding long and structured inputs in transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 268\u2013284.\\n\\nDavid Bamman. 2020. Litbank: Born-literary natural language processing. Computational Humanities, Debates in Digital Humanities (2020, preprint).\\n\\nDavid Bamman, Brendan O\u2019Connor, and Noah A Smith. 2013. Learning latent personas of film characters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 352\u2013361.\\n\\nDavid Bamman, Sejal Popat, and Sheng Shen. 2019. An annotated dataset of literary entities. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2138\u20132144.\\n\\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.\\n\\nSnigdha Chaturvedi, Shashank Srivastava, Hal Daume III, and Chris Dyer. 2016. Modeling evolving relationships between characters in literary novels. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 30.\\n\\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2022. Summscreen: A dataset for abstractive screenplay summarization. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8602\u20138615.\\n\\nJacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1):37\u201346.\\n\\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Shijin Wang, and Guoping Hu. 2020. Revisiting pre-trained models for Chinese natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 657\u2013668, Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT 2019, pages 4171\u20134186.\\n\\nVinodh Krishnan Elangovan and Jacob Eisenstein. 2015. \u201cyou\u2019re mr. lebowski, i\u2019m the dude\u201d: Inducing address term formality in signed social networks. In The 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1616\u20131626.\\n\\nMicha Elsner. 2012. Character-based kernels for novelistic plot structure. In Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 634\u2013644.\\n\\nDavid K. Elson, Nicholas Dames, and Kathleen R. McKeeown. 2010. Extracting social networks from literary fiction. In ACL 2010, Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 138\u2013147. The Association for Computer Linguistics.\\n\\nLucie Flekova and Iryna Gurevych. 2015. Personality profiling of fictional characters using sense-level links between lexical resources. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1805\u20131816.\\n\\nEdward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. Lora: Low-rank adaptation of large language models. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.\\n\\nMohit Iyyer, Anupam Guha, Snigdha Chaturvedi, Jordan Boyd-Graber, and Hal Daum\u00e9 III. 2016. Feuding families and former friends: Unsupervised learning for dynamic fictional relationships. In Proceedings.\"}"}
{"id": "acl-2023-long-826", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-826", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"literary claims. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers).\\n\\nBrian Thompson and Philipp Koehn. 2019. Vecalign: Improved sentence alignment in linear time and space. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1342\u20131348, Hong Kong, China. Association for Computational Linguistics.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aur\u00e9lien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. CoRR, abs/2302.13971.\\n\\nAnthony J Viera, Joanne M Garrett, et al. 2005. Understanding interobserver agreement: the kappa statistic. Fam med, 37(5):360\u2013363.\\n\\nKrishnapriya Vishnubhotla, Adam Hammond, and Graeme Hirst. 2022. The project dialogism novel corpus: A dataset for quotation attribution in literary texts. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 5838\u20135848. European Language Resources Association.\\n\\nYunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li, and Vikas Singh. 2021. Nystr\u00f6mformer: A nystr\u00f6m-based algorithm for approximating self-attention. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14138\u201314148.\\n\\nYing Xu, Dakuo Wang, Mo Yu, Daniel Ritchie, Bing-sheng Yao, Tongshuang Wu, Zheng Zhang, Toby Jia-Jun Li, Nora Bradford, Branda Sun, Tran Hoang, Yisi Sang, Yufang Hou, Xiaojuan Ma, Diyi Yang, Nanyun Peng, Zhou Yu, and Mark Warschauer. 2022. Fantastic questions and where to find them: Fairytaleqa\u2014an authentic dataset for narrative comprehension. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 447\u2013460. Association for Computational Linguistics.\\n\\nMo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Jing Li, Yue Yu, and Jie Zhou. 2022. Few-shot character understanding in movies as an assessment to meta-learning of theory-of-mind. arXiv preprint arXiv:2211.04684.\"}"}
{"id": "acl-2023-long-826", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Annotation Guidelines and Interface\\n\\nWe show our guidelines in Figure 4; and the annotation interface with translations in Figure 5.\\n\\nB Notes that Are Difficult or Ambiguous to Label\\n\\nTable 7 in Appendix B gives some difficult examples that created disagreements. There are two major sources of difficulties. First, the trait word has multiple meanings in Chinese and the usage does not represent the sense of the trait. In the first example, \\\"\u53ef\u6015\u7684\u654c\u4eba\\\" (frightening enemy) in Chinese usually means \\\"an very-strong enemy that is hard/impossible to beat\\\", i.e., a terrible enemy. The enemy, here refers to the protagonist Dant\u00e8s, does not necessary has the frightening personality.\\n\\nSimilarly, in the second example, the annotators have disagreement because some people believe in Chinese, \\\"\u975e\u51e1\\\" (extraordinary) can be used as a personality trait only when a person possesses exceptional characteristics. While some annotators think the trait can also describe a person with exceptional abilities.\\n\\nSecond, a trait word is used to recall the general impression or history behavior of a character in an implicit way. In the third example, the user wanted to express that Elizabeth used to be clear-headed but becomes a fool at the dance party. This recall of the general impression clear-headed is not explicit, but can be inferred from the next sentence that this note is commenting on a snippet of the dance party. Therefore the user aims to comment the foolish trait on the snippet instead of clear-headed.\\n\\nC Full Book List\\n\\nTable 9 shows the detailed information of each book included in our PERSO NET.\\n\\nD Visualization\\n\\nTrait Clouds\\n\\nFigure 6 include more word clouds for different characters.\\n\\nSentiment Plots\\n\\nOur trait vocabulary contains in total 818 traits with polarity annotations. Specifically, there are 234 positive traits, 292 neutral traits and 292 negative traits. Figure 7 visualizes readers' sentiments towards four popular characters through the lens of traits. We map the labeled traits to their sentiments, i.e., positive or negative, and then plot the sentiment along time. Here the x-axis is the position of an note with trait label, normalized by...\"}"}
{"id": "acl-2023-long-826", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Our annotation guidelines. Top: the original Chinese guidelines. Bottom: the English translation.\\n\\nFigure 5: Our annotation interface (with English translations in blue words).\"}"}
{"id": "acl-2023-long-826", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Case Study\\nWe assessed the model performance on the points where people's ratings of Albert have dramatic fluctuations (around $x = 0.8$).\\n\\nSpecifically, we compared three models: the baseline BERT model without any history, the BERT model enhanced with our unsupervised objective, and the Char-Hist Longformer, which can leverage longer historical information. The results are shown in Table 8.\\n\\nOur findings revealed that both the enhanced models\u2014BERT with the unsupervised objective and Char-Hist Longformer\u2014achieved a similar level of improvement over the BERT baseline when considering the entire evaluation set of Albert. These results align with our experimental observations from the comprehensive evaluation data. However, it is noteworthy that the model incorporating the unsupervised objective exhibited a significantly greater enhancement at the slump of the curve. As mentioned earlier in this section, the author explicitly portrayed Albert's reckless personality through his actions and dialogues in this particular case. Even without prior knowledge of the events leading up to this point, humans can intuitively grasp Albert's personality traits. Our unsupervised task aims to capture the correlation between personality and the external expressions manifested within the narrative. This is why it proves to be more effective in this specific case.\\n\\nExamples of Cases that Require History Information\\nThe cases where history information is necessary to solve can be roughly categorized into three types according to our human study in Section 6.3. We include examples for each type in Table 10.\"}"}
{"id": "acl-2023-long-826", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Book Name #Characters | #Instances | #Sentences |\\n|----------------------|------------|------------|\\n| Training Books       |            |            |\\n| Of Human Bondage     | 18         | 6539       | 16542     |\\n| Pride and Prejudice  | 21         | 8632       | 5954      |\\n| Madame Bovary        | 10         | 5440       | 6952      |\\n| Anna Karenina        | 14         | 8204       | 20898     |\\n| Anne Of Green Gables | 8          | 3755       | 6834      |\\n| Little Women         | 7          | 2726       | 9409      |\\n| War and Peace        | 21         | 3448       | 31784     |\\n| Don Quixote          | 3          | 767        | 9384      |\\n| Wuthering Heights    | 13         | 2471       | 6753      |\\n| Jane Eyre            | 10         | 1048       | 9692      |\\n| Twenty Thousand Leagues under the Sea | 4 | 471 | 6614 |\\n| Jude the Obscure     | 3          | 174        | 9191      |\\n| The Sorrows of Young Werther | 1 | 74 | 2400 |\\n| Father Goriot        | 5          | 198        | 6678      |\\n| Uncle Tom's Cabin     | 5          | 138        | 10122     |\\n| Vanity Fair          | 5          | 171        | 13125     |\\n| Oliver Twist         | 5          | 178        | 9166      |\\n| Development Books    |            |            |\\n| The Red and the Black | 6     | 721        | 11061     |\\n| The Count of Monte Cristo | 27 | 2488 | 26437     |\\n| The Adventures of Tom Sawyer Complete | 2 | 115 | 4913 |\\n| David Copperfield    | 15         | 312        | 19195     |\\n| The Gadfly           | 1          | 76         | 6875      |\\n| A Tale of Two Cities | 3          | 33         | 7757      |\\n| Testing Books        |            |            |\\n| Crime and Punishment | 18         | 1498       | 14347     |\\n| The Brothers Karamazov | 12  | 638 | 24101     |\\n| Les Miserables       | 14         | 557        | 35139     |\\n| Eugenie Grandet      | 4          | 217        | 3797      |\\n| Tess of the d'Urbervilles | 3 | 162 | 8074 |\\n| Notre-Dame de Paris  | 8          | 206        | 11278     |\\n| The Call of the Wild | 1          | 42         | 1696      |\\n| The Idiot            | 9          | 215        | 16072     |\\n| Moby Dick; or The Whale | 2 | 51 | 9911 |\\n| Resurrection         | 2          | 38         | 9760      |\"}"}
{"id": "acl-2023-long-826", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"i am the abbe faria, and have been imprisoned as you know in this chateau d'if since the year 1811; previously to which i had been confined for three years in the fortress of fenestrelle. in the year 1811 i was transferred to piedmont in france. it was at this period i learned that the destiny which seemed subservient to every wish formed by napoleon, had bestowed on him a son, named king of rome even in his cradle. i was very far then from expecting the change you have just informed me of; namely, that four years afterwards, this colossus of power would be overthrown. then who reigns in france at this moment\u2014napoleon ii.? \u201cno, louis xviii.\u201d ... dantes' whole attention was riveted on a man who could thus forget his own misfortunes while occupying himself with the destinies of others. \u201cyes, yes,\u201d continued he, \u201c\u2019twill be the same as it was in england. after charles i., cromwell; after cromwell, charles ii., and then james ii., and then some son-in-law or relation, some prince of orange, a stadtholder who becomes a king. then new concessions to the people, then a constitution, then liberty. ah, my friend!\u201d said the abbe, turning towards dantes, and surveying him with the kindling gaze of a prophet, \u201cyou are young, you will see all this come to pass. ...\u201d\\n\\nthe servant soon returned. the decanter and the glass were completely empty. noirtier made a sign that he wished to speak. \u201cwhy are the glass and decanter empty?\u201d asked he; \u201cvalentine said she only drank half the glassful.\u201d the translation of this new question occupied another five minutes. \u201ci do not know,\u201d said the servant, \u201cbut the housemaid is in mademoiselle valentine\u2019s room: perhaps she has emptied them.\u201d \u201cask her,\u201d said morrel, translating noirtier\u2019s thought this time by his look. the servant went out, but returned almost immediately. \u201cmademoiselle valentine passed through the room to go to madame de villefort\u2019s, \u201d said he; \u201cand in passing, as she was thirsty, she drank what remained in the glass; as for the decanter, master edward had emptied that to make a pond for his ducks.\u201d noirtier raised his eyes to heaven, as a gambler does who stakes his all on one stroke. from that moment the old man\u2019s eyes were fixed on the door, and did not quit it. it was indeed madame danglars and her daughter whom valentine had seen; they had been ushered into madame de villefort\u2019s room, who had said she would receive them there. that is why valentine passed through her room, which was on a level with valentine\u2019s, and only separated from it by edward\u2019s. the two ladies entered the drawing-room with that sort of official stiffness which preludes a formal communication. among worldly people manner is contagious. madame de villefort received them with equal solemnity. valentine entered at this moment, and the formalities were resumed. ...\\n\\nis she to be hung yonder? \u201cfool! t\u2019is here that she is to make her apology in her shift! the good god is going to cough latin in her face! that is always done here, at midday. if\u2019tis the gallows that you wish, go to the greve.\u201d \u201ci will go there, afterwards.\u201d \u201ctell me, la boucanbry? is it true that she has refused a confessor?\u201d \u201cit appears so, la bechaigne.\u201d \u201cyou see what a pagan she is!\u201d \u201c\u2019tis the custom, monsieur. the bailiff of the courts is bound to deliver the malefactor ready judged for execution if he be a layman, to the provost of paris; if a clerk, to the official of the bishopric.\u201d \u201cthank you, sir.\u201d \u201coh, god!\u201d said fleur-de-lys, \u201cthe poor creature!\u201d this thought filled with sadness the glance which she cast upon the populace. the captain, much more occupied with her than with that pack of the rabble, was amorously rumpling her girdle behind. she turned round, entreating and smiling. \u201cplease let me alone, phoebus! if my mother were to return, she would see your hand!\u201d at that moment, midday rang slowly out from the clock of notre-dame. a murmur of satisfaction broke out in the crowd. the last vibration of the twelfth stroke had hardly died away when all heads surged like the waves beneath a squall, and an immense shout went up from the pavement, the windows, and the roofs, \u201cthere she is!\u201d fleur-de-lys pressed her hands to her eyes, that she might not see. \u201ccharming girl,\u201d said phoebus, \u201cdo you wish to withdraw?\u201d \u201cno,\u201d she replied ...\\n\\nTable 10:\\n\\nExample of cases that require history information to solve. The red texts are the underlined text of the notes that used to construct the labeled instance. In the first example, according to the snippet, both simple and impressionable are possible traits to explain the character's behavior. Only from the history that Dant\u00e8s is a brave and determined person, we can select simple as the correct answer. In the second example, only when the readers know that Noirtier (The elder) aims to help Valentine get immunity from the poisoned juicy, they can understand the character's wisdom. In the third example, Esmeralda is not present. However, the scene of love between Phoebus and Fleur-de-Lys is quite similar to her story with Phoebus, illustrating that she was easily deceived by the man.\"}"}
{"id": "acl-2023-long-826", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nDid you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\nNot applicable. We create artifacts by ourselves.\\n\\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\\n\\nSection 7. We will release our dataset for public research with CC-BY 4.0.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\nNot applicable. We create artifacts by ourselves.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n\\nSection 4.1. We anonymized the data with user information.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\nSection 4.1 and Appendix C.\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nSection 4.4.\\n\\nDid you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nSection 6.1.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-826", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 6.1.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSection 6.2. We report mean-std results with 5 runs.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nSection 4.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nAppendix A.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nSection 1 and Section 4.2 Step 1.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nSection 4.2 Step 1.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNot applicable. The data is from social network thus the study follows IRB.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nSection 4.2 Step 1.\"}"}
