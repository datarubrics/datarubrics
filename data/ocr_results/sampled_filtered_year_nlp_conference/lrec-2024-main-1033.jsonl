{"id": "lrec-2024-main-1033", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multilinguality or Back-translation? A Case Study with Estonian\\nElizaveta Korotkova, Taido Purason, Agnes Luhtaru, Mark Fishel\\nInstitute of Computer Science\\nUniversity of Tartu, Estonia\\n{elizaveta.korotkova,taido.purason,agnes.luhtaru,mark.fisel}@ut.ee\\n\\nAbstract\\nMachine translation quality is highly reliant on large amounts of training data, and, when a limited amount of parallel\\ndata is available, synthetic back-translated or multilingual data can be used in addition. In this work, we introduce\\nSynEst, a synthetic corpus of translations from 11 languages into Estonian which totals over 1 billion sentence pairs.\\nUsing this corpus, we investigate whether adding synthetic or English-centric additional data yields better translation\\nquality for translation directions that do not include English. Our results show that while both strategies are effective,\\nsynthetic data gives better results. Our final models improve the performance of the baseline No Language Left\\nBehind model while retaining its source-side multilinguality.\\n\\nKeywords: machine translation, synthetic corpus, less-resourced languages\\n\\n1. Introduction\\nThe quality of neural machine translation systems\\nheavily depends on the availability and quality of\\ntraining data. While for some languages (first\\nand foremost English) vast amounts of suitable\\nresources are often readily available, for less-\\nresourced languages that is not often the case\\n(Joshi et al., 2020). In such cases, one can re-\\nsort to generating synthetic data and/or leveraging\\nmultilingual resources for transfer learning in order\\nto mitigate the lack of parallel data.\\n\\nIn this work, we directly compare these two data\\naugmentation approaches for machine translation\\n(MT). We focus on Estonian, a mid-resourced Euro-\\npean language of the Finno-Ugric language group,\\nwith no genealogically or geographically close lan-\\nguages that are particularly resource-rich. We intro-\\nduce a novel large-scale synthetic parallel corpus,\\nSynEst, consisting of translations from 11 other\\nlanguages into Estonian. The choice of source lan-\\nguages is motivated both globally, with languages\\nsuch as English or Chinese, and regionally, for e.g.\\nFinnish, Latvian, and Lithuanian. The resulting\\ncorpus contains over 1 billion parallel sentences\\nand is 6 times larger than the monolingual national\\ncorpus of Estonian (Koppel and Kallas, 2022) and\\nmore than twice the size of the Estonian part of the\\nCulturaX corpus (Nguyen et al., 2023).\\n\\nWith the help of this new resource, we carry out\\na pilot experiment focused on machine translation\\nfrom Estonian into other languages, intentionally\\nexploring non-English-centric translation directions.\\nWe aim to determine whether a more substantial\\ngain in translation quality can be achieved by using\\nsynthetic Estonian\u2013other (ET\u2013X) data or multilin-\\ngual data, specifically, English\u2013other (EN\u2013X), and\\nshow that, while both approaches are successful,\\naugmenting with synthetic data leads to better per-\\nformance. The final result is an MT system which\\nuses our new synthetic dataset for augmentation\\nand surpasses the baseline No Language Left Be-\\nhind (NLLB Team et al., 2022) model in quality\\nwhile increasing its inference speed and retaining\\nits support of multilingual input.\\n\\nWe first briefly outline related work in Section 2,\\nthen describe our novel synthetic corpus in Sec-\\ntion 3, present the pilot experiments in Section 4,\\ndescribe their empirical results in Section 5, and\\ndiscuss them in Section 6. Section 7 concludes the\\npaper.\\n\\nThis work's main contributions are:\\n\u2022 we release a synthetic parallel corpus of over\\n1 billion sentence pairs with translations from\\n11 languages into Estonian (Section 3);\\n\u2022 we directly compare two data augmentation\\nmethods, namely, leveraging synthetic back-\\ntranslated data and English-centric data, by\\nperforming experiments focused on training\\nmachine translation systems for translation\\nfrom Estonian with limited parallel resources\\n(Section 4);\\n\u2022 we empirically show the usefulness and satis-\\nfactory quality of our synthetic dataset for out-\\nof-Estonian machine translation (Section 5).\\n\\n2. Related Work\\nNon-English-centric MT\\nhas been underexplored\\nin machine translation research compared to lan-\\nguage pairs involving English. Recently, however,\\nthere has been some shift towards including more\\npairs without English. For instance, the general MT\\ntask at WMT 2020 included 2 translation directions\\nwithout English out of 22 in total (Barrault et al.,\\n2020), while in 2021 6 out of 20, and in 2022 6 out\\n\"}"}
{"id": "lrec-2024-main-1033", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of 21 directions did not include English (Akhbardeh et al., 2021; Kocmi et al., 2022). In the space of multilingual MT, works such as Fan et al. (2020) have stressed the utility of many-to-many training data as opposed to purely English-centric. In our work, we intentionally focus on experiments with non-English-centric translation directions.\\n\\nMethods for low-resource MT can be used when a limited amount of parallel data for a translation direction is available. Haddow et al. (2022) outline using back-translated data (Sennrich et al., 2016) and multilingual models (Dong et al., 2015; Johnson et al., 2017) as two such methods. While the language pairs in our experiments are not low-resource but rather mid-resource, in the absence of abundant parallel data, we draw inspiration from low-resource MT, and use both back-translated data and a multilingual model to improve MT performance in our experiments.\\n\\nSynthetic parallel corpora have proven effective, but can be costly to produce, especially on a massive scale. Thus, efforts similar to ours have published back-translated corpora to be reused. CzEng 2.0 (Kocmi et al., 2020) is a Czech-English parallel corpus that includes automatic translations of 127M total sentences crawled from news servers. S\u00edmonarson et al. (2021) create an English-Icelandic parallel corpus of 76M sentences. In this work, we also focus on one relatively under-resourced language and produce a massive synthetic corpus, including 11 translation directions.\\n\\nModular architectures for MT were introduced by Escolano et al. (2021) and Lyu et al. (2020). Our choice of a modular architecture with a fixed encoder and language-specific decoders draws inspiration from these works.\\n\\n### 3. SynEst: Synthetic Corpus of Parallel Estonian\\n\\nTo create synthetic back-translated data for augmenting our parallel corpus, we translate the whole NewsCrawl monolingual corpus (Kocmi et al., 2022; Haddow et al., 2022) up to year 2021 into Estonian. The NewsCrawl corpus contains monolingual text extracted from online newspapers and released for the WMT series of shared tasks. We select 11 languages to translate from: 6 globally wide-spread languages (English, German, Spanish, French, Chinese, Arabic), Finnish as a language closely related to Estonian, and 4 regionally important languages of neighbors and Estonian minorities (Latvian, Lithuanian, Ukrainian, Russian).\\n\\n| Language | Code | Code Source | Count (Millions) | Word Count (Billions) |\\n|----------|------|-------------|------------------|-----------------------|\\n| Arabic   | AR   |             | 42.3             | 1.0                   |\\n| German   | DE   |             | 427.1            | 6.0                   |\\n| English  | EN   |             | 314.3            | 5.3                   |\\n| Spanish  | ES   |             | 72.1             | 1.3                   |\\n| Finnish  | FI   |             | 28.8             | 0.3                   |\\n| French   | FR   |             | 104.8            | 1.5                   |\\n| Lithuanian | LT  |             | 7.6              | 0.1                   |\\n| Latvian  | LV   |             | 14.9             | 0.2                   |\\n| Russian  | RU   |             | 126.6            | 1.6                   |\\n| Ukrainian| UK   |             | 2.3              | 0.03                  |\\n| Chinese  | ZH   |             | 13.9             | 0.3                   |\\n\\nTable 1: Sizes of the synthetic back-translation corpora (unfiltered): snt count gives the number of sentences, and word count gives the number of words in the Estonian output.\\n\\nThe number of the resulting translated sentences and words is shown in Table 1.\\n\\n7. We translate from English, German, and Russian with the MTee general-domain model as the most high-performing MT model for these language pairs (T\u00e4ttar et al., 2022). For all other source languages we use the M2M-100 1.2B-parameter model (Fan et al., 2020). In all cases, we use beam search with beam size 5.\\n\\nThe dataset is available to download via MetaShare under the CC BY license. For each language pair, we provide an unfiltered and filtered corpus (data filtering details are described in Appendix A). Each unfiltered corpus is a file in tab-separated format with three columns: the original sentence, the translation, and the translation\u2019s log-probability score. The filtered corpora are provided as .tar archives which contain parallel text files.\\n\\n### 4. Experiments\\n\\nIn this section, we present a pilot study that uses 3 translation directions from the SynEst corpus. The aim is to see if an existing massively multilingual MT system (NLLB: NLLB Team et al., 2022) can be efficiently improved for the chosen translation directions without losing its multilinguality using modular MT (Escolano et al., 2021; Lyu et al., 2020).\\n\\nBelow we describe the parallel data used in addition to SynEst, the modular approach, and evaluation details. Results can be found in the next section.\\n\\n#### 4.1. Training Data\\n\\nWe focus on translation from Estonian into three target languages: Finnish (closely related to Estonian), German (resource-rich, unrelated language but has some similarities with Estonian on lexical...\"}"}
{"id": "lrec-2024-main-1033", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Sizes (in millions of sentence pairs) of original parallel ET\u2013X corpora used for training and total sizes of available augmentation corpora (after filtering). In our experiments, we always use all original ET\u2013X data and mix it 1:7 with augmentation data, under/oversampling additional data as needed.\\n\\n| Language Pair | Original Size | Augmented Size EN\u2013X | Augmented Size ET\u2013X |\\n|---------------|---------------|----------------------|---------------------|\\n| ET\u2013FI         | 15.0          | 23.5                 | 80.3                |\\n| ET\u2013DE         | 9.3           | 332.6                | 398.6               |\\n| ET\u2013ZH         | 5.8           | 10.4                 | 63.8                |\\n\\nWe use the concatenation of 10 parallel corpora in our experiments: CCMatrix (Schwenk et al., 2021b), WikiMatrix (Schwenk et al., 2021a), MultiParaCrawl (Ba\u00f1\u00f3n et al., 2020), Europarl (Koehn, 2005), OpenSubtitles (Lison and Tiedemann, 2016), JRC-Acquis (Steinberger et al., 2006), TED2020 (Reimers and Gurevych, 2020), EMEA, infopankki, and DGT (Tiedemann, 2012). Each corpus is used whenever it is available for a particular translation direction.\\n\\nFor all three translation directions, there is not an overwhelming amount of parallel data available. To mitigate this, we explore augmenting our parallel data with two types of additional data:\\n\\n- the synthetic back-translation data of SynEst, making use of available monolingual data in the target languages,\\n- parallel data between English and the target languages (EN\u2013X), leveraging the relative abundance of English-centric data and the base model\u2019s ability to translate from multiple source languages.\\n\\nAs sources of EN\u2013X data, we use the same 10 corpora for EN\u2013X as for the original parallel ET\u2013X data, except that MultiParaCrawl is replaced by ParaCrawl (Ba\u00f1\u00f3n et al., 2020) in this case. The total sizes of our parallel and additional training corpora are shown in Table 2.\\n\\nUsing the original ET\u2013X parallel data and the two varieties of additional data, we obtain 4 different training datasets:\\n\\n1. only original parallel ET\u2013X data,\\n2. parallel ET\u2013X data mixed with SynEst back-translation data,\\n3. parallel ET\u2013X data mixed with EN\u2013X English-centric data,\\n4. parallel ET\u2013X data mixed with both SynEst and EN\u2013X data.\\n\\nIn augmentation scenarios 2 and 3, we mix the original parallel and additional data 1:7, always using all available original data. When using all types of data (scenario 4), we mix the original parallel, SynEst, and EN\u2013X data 1:7:7. For information on preliminary experiments with other original to additional data proportions, see Appendix E.\\n\\n4.2. Models\\n\\nAs the base model, we use the multilingual NLLB-1.3B dense model (NLLB Team et al., 2022). We freeze the parameters of the original model\u2019s encoder, retaining the multilinguality of the model on the source side. While focusing primarily on one input language, this allows us to not lose, and, in some cases, improve translation quality from other source languages, while also reducing the training-time costs. This also contributes to improved inference-time efficiency, as the same encoder is reused for multiple language pairs, and the models can be built in a modular fashion (Lyu et al., 2020; Escolano et al., 2021).\\n\\nFor each target language, we train a new randomly initialized decoder with 6 transformer layers of the same dimensions as in the original NLLB-1.3B. (For details on the decoder size choice, see Appendix D.) This allows us to train specialized decoders for each target language while making them more lightweight and reducing the training and inference costs. We also reduce the target vocabulary size to 32k (from 256k in NLLB). We use FairSeq (Ott et al., 2019) to train the models. For further model training details, see Appendix B.\\n\\n4.3. Hyperparameter Search\\n\\nWe perform grid search for data mixing proportions and decoder size.\\n\\nWe experiment with 2:1, 1:1, 1:3, and 1:7 parallel to augmentation data proportions for SynEst and EN\u2013X data, and find 1:7 to be the best performing on average. Details of this experiment are shown in Appendix E.\\n\\nTo choose the number of decoder layers, we train models on parallel ET\u2013FI data for 200k updates. The model fails to train with 18- and 24-layer decoders due to the amount of training data being insufficient to match the large number of parameters; 3, 6, and 12 layers show results comparable to each other, with 6 slightly outperforming the others; having 1 layer leads to noticeably worse performance. See Figure 3 in Appendix D for more detailed results of this experiment.\"}"}
{"id": "lrec-2024-main-1033", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: chrF++ curves of models trained only using ET-X parallel data (purple), with added EN\u2013X data (yellow), with added synthetic data (red), and with both types of data added (dark green) on Flores-dev. Dashed lines (black) show original NLLB-1.3B scores. Parallel ET\u2013X to additional data proportions are 1:7 (1:7:7 when both synthetic and EN\u2013X data are added).\\n\\n4.4. Evaluation\\nWe evaluate the performance of our models using the Flores benchmark dataset (Goyal et al., 2022). The dev split of Flores is also used as the development set during training. Following NLLB Team et al. (2022) and the recommendations of Kocmi et al. (2021), we report both BLEU (Papineni et al., 2002) and chrF++ (Popovi\u0107, 2017) scores for evaluation. Specifically, we use the sacreBLEU implementation (Post, 2018) for both metrics.\\n\\n5. Results\\nFor each translation direction, we compare four models: one trained using only ET\u2013X parallel data, the second augmented with SynEst as back-translation data, the third augmented with EN\u2013X parallel data and, finally, a model trained using parallel and both types of augmentation data. The original NLLB-1.3B model serves as the baseline. Table 3 shows each scenario\u2019s BLEU and chrF++ scores on the devtest split of the Flores benchmark dataset for the three translation directions. To calculate the scores of our models, we use the checkpoint with the best score on Flores-dev for each model.\\n\\nFor all three translation directions, both augmentation strategies prove useful. The performance when adding synthetic and English-centric data is similar, with models trained with added synthetic data showing slightly better scores. This is also evident in Figure 1, which shows the chrF++ scores of our models on Flores-dev as the training progresses. Models trained with added synthetic data (shown in red) consistently show better results and less stagnation in the later stages of training than those trained with added EN\u2013X data (yellow). This confirms that both multilingual and synthetic data can be used to support translation in directions with limited parallel resources, and shows the practical usefulness of the introduced synthetic corpus. Augmenting with both kinds of data at the same time improves the results slightly for ET\u2013FI and more noticeably for ET\u2013ZH, while for ET\u2013DE using both...\"}"}
{"id": "lrec-2024-main-1033", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: BLEU/chrF++ scores on Flores-dev (for our models, we report the score of the best checkpoint). Note that the FR\u2013FI dataset was translated with the model trained on ET\u2013FI data, and RU\u2013DE with the model trained on ET\u2013DE data, which makes the shown translation directions zero-shot.\\n\\nTypes of augmentation data does not yield a better result than adding only SynEst data. While the scores of the models augmented with SynEst synthetic data and with English-centric data are mostly very close, for synthetic data the result is reached with fewer unique sentence pairs in the training set. The original and augmentation data are always mixed 1:7, using all of the original data once and under- or oversampling augmentation data as needed. While the augmentation data for ET\u2013DE is always undersampled, for ET\u2013FI and ET\u2013ZH the SynEst data is oversampled more than the EN\u2013X data, since EN\u2013X is more abundant. A SynEst sentence pair occurs, on average, around 4.5 times in the ET\u2013FI training corpus, while each EN\u2013FI sentence pair occurs only 1.3 times. For ET\u2013ZH, the figure is 3.9 for SynEst, while the EN\u2013ZH corpus is undersampled and thus its sentence pairs are not repeated at all. This shows that unique SynEst data is likely more valuable for translation performance than English-centric data.\\n\\nAlthough the models are trained with only one source language (or two, in the case when EN\u2013X data is added), they also maintain or even improve NLLB\u2019s translation quality when translating from other languages, due to the encoder being frozen and the decoder being focused on a specific target language. Table 4 shows two examples of this, namely, the scores of the model trained on Estonian\u2013Finnish parallel data translating from French into Finnish, and the Estonian\u2013German model translating from Russian into German. While translation from Russian, which uses a different script than Estonian, has a lower BLEU but slightly higher chrF++ score than NLLB, translation from French into Finnish is improved compared to the baseline NLLB performance.\\n\\n6. Discussion\\n\\nWhile for ET\u2013FI and ET\u2013DE the non-augmented models trained only on ET\u2013X parallel data easily outperform the multilingual NLLB-1.3B baseline, for ET\u2013ZH that is not the case, with the new model trailing behind the baseline. The augmented models manage to beat the baseline, which suggests the need for further exploration in this direction. For further context, we also apply GPT-4 (OpenAI, 2023) to translating Flores-dev-test. The evaluation setup is described in Appendix F. While GPT-4 outperforms our models on translation into German and Chinese, it shows a similar BLEU score for Finnish. This suggests that it may not be optimal for less represented languages, given that, to the best of our understanding, GPT-4 uses several orders of magnitude more parameters and training data than our models. The main issue, however, is the instability of its content moderation system: GPT-4 refused to translate around 1.7% of Flores-devtest sentences, including ones of innocent nature, such as \u201cToday, the only insects that cannot fold back their wings are dragon flies and mayflies\u201d (reference English translation). This presents a significant challenge for using closed models.\\n\\n7. Conclusion\\n\\nIn this work, we present two main contributions. First, we release SynEst, a large synthetic corpus comprised of translations from 11 languages into Estonian and totaling over 1 billion parallel sentence pairs. Second, we perform experiments with this corpus, training translation systems based on the NLLB multilingual model for three language pairs, with a focus on translation directions which do not involve English and for which limited resources are available. Our models retain NLLB\u2019s multilinguality on the source side while improving translation quality for the translation directions of interest. We compare three data augmentation methods, namely, using our novel synthetic data, using English-centric parallel data, and a combination of the two. We demonstrate the usefulness of our corpus for training out-of-Estonian MT systems.\\n\\n8. Bibliographical References\\n\\nFarhad Akhbardeh, Arkady Arkhangorodsky, Magdalena Biesialska, Ond\u0159ej Bojar, Rajen Chatterjee, Vishrav Chaudhary, Marta R. Costa-jussa, Cristina Espa\u00f1a-Bonet, Angela Fan, Christian Federmann, Markus Freitag, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Leonie Harter, Kenneth Heafield, Christopher Homan, Matthias Huck, Kwabena Amponsah-Kaakyire, Jungo Kasai, Daniel Khashabi, Kevin Knight, Tom Kocmi, Philipp Koehn, Nicholas Lourie, Christof Monz, Makoto Morishita, Masaaki Nagata, Ajay Nagesh, Toshiaki Nakazawa, Matteo Negri, Santanu Pal, Allahsera Auguste Tapo, Marco Turchi, Valentin Vydrin, and Marcos...\"}"}
{"id": "lrec-2024-main-1033", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zampieri. 2021. Findings of the 2021 conference on machine translation (WMT21). In Proceedings of the Sixth Conference on Machine Translation, pages 1\u201388, Online. Association for Computational Linguistics.\\n\\nMikko Aulamo, Sami Virpioja, and J\u00f6rg Tiedemann. 2020. OpusFilter: A configurable parallel corpus filtering toolbox. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 150\u2013156, Online. Association for Computational Linguistics.\\n\\nLo\u00efc Barrault, Magdalena Biesialska, Ond\u0159ej Bojar, Marta R. Costa-juss\u00e0, Christian Federmann, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Matthias Huck, Eric Joanis, Tom Kocmi, Philipp Koehn, Chi-kiu Lo, Nikola Ljube\u0161i\u0107, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Santanu Pal, Matt Post, and Marcos Zampieri. 2020. Findings of the 2020 conference on machine translation (WMT20). In Proceedings of the Fifth Conference on Machine Translation, pages 1\u201355, Online. Association for Computational Linguistics.\\n\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146.\\n\\nDaxiang Dong, Hua Wu, Wei He, Dianhai Yu, and Haifeng Wang. 2015. Multi-task learning for multiple language translation. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1723\u20131732, Beijing, China. Association for Computational Linguistics.\\n\\nCarlos Escolano, Marta R. Costa-juss\u00e0, Jos\u00e9 A. R. Fonollosa, and Mikel Artetxe. 2021. Multilingual machine translation: Closing the gap between shared and language-specific encoder-decoders. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 944\u2013948, Online. Association for Computational Linguistics.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur \u00c7elebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Edouard Grave, Michael Auli, and Armand Joulin. 2020. Beyond english-centric multilingual machine translation. J. Mach. Learn. Res., 22:107:1\u2013107:48.\\n\\nBarry Haddow, Rachel Bawden, Antonio Vale-r\u00edo Miceli Barone, Jind\u0159ich Helcl, and Alexandra Birch. 2022. Survey of low-resource machine translation. Computational Linguistics, 48(3):673\u2013732.\\n\\nMelvin Johnson, Mike Schuster, Quoc V. Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jeffrey Dean. 2017. Google\u2019s multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351.\\n\\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kaliha Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online.\\n\\nDiederik Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), San Diego, CA, USA.\\n\\nTom Kocmi, Rachel Bawden, Ond\u0159ej Bojar, Anton Dvorkovich, Christian Federmann, Mark Fishel, Thamme Gowda, Yvette Graham, Roman Grundkiewicz, Barry Haddow, Rebecca Knowles, Philipp Koehn, Christof Monz, Makoto Morishita, Masaaki Nagata, Toshiaki Nakazawa, Michal Nov\u00e1k, Martin Popel, and Maja Popovi\u0107. 2022. Findings of the 2022 conference on machine translation (WMT22). In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 1\u201345, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\\n\\nTom Kocmi, Christian Federmann, Roman Grundkiewicz, Marcin Junczys-Dowmunt, Hitokazu Matsushita, and Arul Menezes. 2021. To ship or not to ship: An extensive evaluation of automatic metrics for machine translation. In Proceedings of the Sixth Conference on Machine Translation, pages 478\u2013494, Online. Association for Computational Linguistics.\\n\\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-1033", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-1033", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pierre Lison and J\u00f6rg Tiedemann. 2016. OpenSubtitles2016: Extracting large parallel corpora from movie and TV subtitles. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 923\u2013929, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nThuat Nguyen, Chien Van Nguyen, Viet Dac Lai, Hieu Man, Nghia Trung Ngo, Franck Dernoncourt, Ryan A. Rossi, and Thien Huu Nguyen. 2023. Culturax: A cleaned, enormous, and multilingual dataset for large language models in 167 languages.\\n\\nNils Reimers and Iryna Gurevych. 2020. Making monolingual sentence embeddings multilingual using knowledge distillation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\\n\\nHolger Schwenk, Vishrav Chaudhary, Shuo Sun, Hongyu Gong, and Francisco Guzm\u00e1n. 2021a. WikiMatrix: Mining 135M parallel sentences in 1620 language pairs from Wikipedia. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 1351\u20131361, Online. Association for Computational Linguistics.\\n\\nHolger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela Fan. 2021b. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6490\u20136500, Online. Association for Computational Linguistics.\\n\\nHaukur Barri S\u00edmonarson, V\u00e9steinn Sn\u00e6bjarnarson, P\u00e9tur Orri Ragnarson, Haukur J\u00f3nsson, and Vilhjalmur Thorsteinsson. 2021. Mi\u00f0eind's WMT 2021 submission. In Proceedings of the Sixth Conference on Machine Translation, pages 136\u2013139, Online. Association for Computational Linguistics.\\n\\nRalf Steinberger, Bruno Pouliquen, Anna Widiger, Camelia Ignat, Toma\u017e Erjavec, Dan Tufi\u015f, and D\u00e1niel Varga. 2006. The JRC-Acquis: A multilingual aligned parallel corpus with 20+ languages. In Proceedings of the Fifth International Conference on Language Resources and Evaluation (LREC'06), Genoa, Italy. European Language Resources Association (ELRA).\\n\\nJ\u00f6rg Tiedemann. 2012. Parallel data, tools and interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association (ELRA).\"}"}
{"id": "lrec-2024-main-1033", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The synthetic dataset was first filtered based on log probability of the generated translations. We only keep the examples that where log probability is higher than $\\\\mu - 1.5\\\\sigma$ where $\\\\mu$ is the mean and $\\\\sigma$ is the standard deviation over all translation log probabilities for a given language and corpus.\\n\\nBoth synthetic and parallel data are normalized with \\\\textit{MTee} normalization script (T\u00e4ttar et al., 2022) and filtered with OpusFilter (Aulamo et al., 2020). The OpusFilter configuration is a modified version of filters used in \\\\textit{MTee}. The following filters are used:\\n\\n1. \\\\textit{LongWordFilter}: filter examples with words longer than 40 characters (default).\\n2. \\\\textit{LengthFilter}: filter examples longer than 1000 characters or shorter than 10 characters.\\n3. \\\\textit{LengthFilter}: filter examples longer than 100 words.\\n4. \\\\textit{LengthRatioFilter}: filter examples where the source and target sentence lengths differ more than 3 times in terms of number of words.\\n5. \\\\textit{CharacterScoreFilter} with threshold 1 (default) for the respective scripts.\\n6. \\\\textit{LanguageIDFilter} with fastText (Bojanowski et al., 2017) language identification model.\\n7. \\\\textit{LanguageIDFilter} with CLD2 language identification.\\n8. \\\\textit{TerminalPunctuationFilter} with the default parameters.\\n9. \\\\textit{NonZeroNumeralsFilter} with the default parameters.\\n\\nThis configuration is applied to all the language-pairs with the following exceptions:\\n\\n- Arabic\u2013Estonian which uses filters 1 \u2013 6 and uses minimal sentence length of 3 characters in filter 2;\\n- Chinese\u2013Estonian, which only uses \\\\textit{LengthFilter} with maximal sentence length of 750 characters (no minimal length), \\\\textit{CharacterScoreFilter}, and \\\\textit{LanguageIDFilter} with fastText as language identification model.\\n\\nFurthermore, duplicates and test set overlaps are removed from the training dataset.\"}"}
{"id": "lrec-2024-main-1033", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Training\\n\\nThe models in the main experiments use the NLLB-1.3B encoder, which has 24 transformer layers with embedding dimension 1024, feed-forward dimension 8192, and 16 attention heads. The decoders are randomly initialized and have 6 transformer layers with the same dimensions as the encoder. The input and output embeddings of the decoder are shared. The vocabulary size is 256,000 for the encoder and 32,000 for the decoder. Model size is approximately 950M parameters; only 184M of these parameters are trained, the rest are not updated and are re-used in all our models. The original NLLB SentencePiece (Kudo and Richardson, 2018) model and vocabulary are used for the encoder, and a new model is trained for the decoder for each target language. The new subword models are trained using the non-augmented parallel data for each translation direction, and are re-used for all models for that translation direction.\\n\\nWe use FairSeq to train the models (Ott et al., 2019). All models are trained on 8 GPUs (4 AMD MI250x 128GB GPU modules, each acting as 2 GPUs). The batch size is 4,096 tokens per GPU. FP16 floating-point format is used. All models are trained for 2,000,000 updates. The initial learning rate is $1 \\\\times 10^{-7}$, with inverse square root learning rate scheduler with 4,000 warm-up updates to a maximum learning rate of $5 \\\\times 10^{-4}$. We use Adam optimizer (Kingma and Ba, 2015) with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.999$. Dropout probability is set to 0.1, attention dropout to 0.1, and activation dropout is not used. The loss function is cross-entropy.\\n\\nC. Retaining Source-Side Multilinguality\\n\\nWhile we focus on Estonian as the main source language, freezing the parameters of the NLLB-1.3B encoder allows us to retain the model's multilinguality on the source side. Figure 2 shows how the chrF++ score on the dev split of the Flores dataset progresses during training. In Figure 2a, we show scores on the French-Finnish translation direction when the model is trained using only Estonian-Finnish parallel data; similarly, Figure 2b demonstrates results of the model trained on Estonian-German parallel data when applied to Russian-German translation. While for RU\u2013DE the ET\u2013DE model starts lagging behind the baseline NLLB-1.3B after around 250,000 updates, the ET\u2013FI model consistently outperforms the baseline on FR\u2013FI translation.\\n\\nD. Decoder Size\\n\\nTo explore possible choices of decoder size, we first use the parallel ET\u2013FI data to train models with different number of transformer layers in the decoder (1, 3, 6, 12, 18, and 24 layers). The decoder\"}"}
{"id": "lrec-2024-main-1033", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"layers have the same dimensions as the encoder layers (embedding dimension 1024, feed-forward dimension 8192, 16 attention heads). The models are trained for 200,000 updates. Figure 3 shows the progress of BLEU score (Papineni et al., 2002) on the Flores dev set during training. 18- and 24-layer decoders fail to train with the amount of parallel data available; the model with 1 decoder layer slightly outperforms the original NLLB-1.3B model, while 3-, 6-, and 12-layer decoders show comparable results. In subsequent experiments, we train models with 6 layers in decoder.\\n\\nE. Data Proportions\\nFigures 4 and 5 show Flores-dev chrF++ scores during training of models with different proportions of original and augmentation data (EN\u2013X data in Figure 4, and synthetic data in Figure 5). We mix original and additional data 2:1, 1:1, 1:3, and 1:7, the latter being the main experiments described in Section 4.\\n\\nF. GPT-4 Evaluation\\nTo evaluate translation performance of GPT-4 (OpenAI, 2023), we follow Zhang et al. (2023) and choose a simple prompt template:\\n\\n```\\n[src]: [input]\\n[tgt]: [input]\\n```\\n\\n[src] and [tgt] denote source and target language names, respectively, and [input] denotes the input test sentence. The translations were retrieved on 16 October 2023.\\n\\nThe main issue with using the GPT-4 API for translation is that some prompts trigger the content management policy, and no translation is provided at all. This moderation system seems to be unstable; the reference English translation of one of the Flores-devtest sentences which it refused to translate is \u201cToday, the only insects that cannot fold back their wings are dragonflies and mayflies.\u201d This presents a significant challenge for using closed models. Where a translation could not be generated, it was replaced with an empty line.\\n\\nFor translation into Chinese, GPT-4 noticeably outperforms all our models and NLLB-1.3B. At the same time, ET-ZH is also the weakest translation for our models in comparison with the original NLLB model. There is also some difference in favor of GPT-4 in translation into German, while for Finnish our augmented models and GPT-4 show the same BLEU score, with our models, to the best of our understanding, having orders of magnitude fewer parameters and training data examples. This suggests that, while GPT-4 shows high quality of translation when generating widespread languages, for less represented languages it might not be optimal. It also not possible to establish whether the model has encountered the Flores test set before.\"}"}
