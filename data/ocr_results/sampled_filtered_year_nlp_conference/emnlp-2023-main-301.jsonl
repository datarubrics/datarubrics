{"id": "emnlp-2023-main-301", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Performant vision-language (VL) models like CLIP represent captions using a single vector. How much information about language is lost in this bottleneck? We first curate CompPrompts, a set of increasingly compositional image captions that VL models should be able to capture (e.g., single object, to object+property, to multiple interacting objects). Then, we train text-only recovery probes that aim to reconstruct captions from single-vector text representations produced by several VL models. This approach does not require images, allowing us to test on a broader range of scenes compared to prior work. We find that: 1) CLIP's text encoder falls short on more compositional inputs, including object relationships, attribute-object association, counting, and negations; 2) some text encoders work significantly better than others; and 3) text-only recovery performance predicts multimodal matching performance on ControlledImCaps: a new evaluation benchmark we collect and release consisting of fine-grained compositional images and captions. Specifically, our results suggest text-only recoverability is a necessary (but not sufficient) condition for modeling compositional factors in contrastive VL models. We release our datasets and code.\\n\\n1 Introduction\\n\\n\\\"A penguin on Mars wearing a spacesuit and walking a robot dog next to Santa Claus.\\\" Riedl (2022)'s text-to-image query is the type that modern multimodal models should be able to support. It is spatially precise (the dog is next to Santa, not in front), compositional (robot dog, but not robot Santa), and imaginative (it is unlikely such an image exists already). However, several recent works have shown that a variety of multimodal models (despite achieving strong benchmark performance) are frequently unable to reason about even simple spatial relations or attribute attachments (Gokhale et al., 2022; Thrush et al., 2022; Yuksekgonul et al., 2023).\\n\\nVerbs \u2026 can the model do image-text matching? (\u00a74.1)\\n\\nIncorrect\\n\\nA person after opening an umbrella\\nA person before opening an umbrella\\n\\nCan a probe reconstruct the input? (\u00a73)\\n\\nPredicts (\u00a74.2-4.5)\\n\\nA cheetah before eating\\nA bride before flying\\nA businessperson after driving\\nAn instructor after sitting\\nA president after helping\\nA politician after helping\\nA rider after helping\\nA stallion after carrying\\n\\nContrastive VL model\\nContrastive VL text encoder\\nTemporal Relations\\nSpatial Relations\\nCompPrompts\\nControlledImCaps\\n\\nFigure 1: We present CompPrompts, a dataset of 18,100 text prompts, and ControlledImCaps, a dataset of 600 image pairs+captions that differ by only one word. The two datasets are grouped by the same set of caption properties, e.g., temporal/spatial relations. Experiments on CompPrompts quantify the information loss of a text encoder; experiments on ControlledImCaps illustrate that information loss correlates with multimodal errors.\\n\\nUnderlying several popular multimodal models like CLIP (Radford et al., 2021), DALL-E 2 (Ramesh et al., 2022) and ALIGN (Jia et al., 2021) is a pooled text encoder, i.e., a text representation model that outputs a single vector for a given input caption. In this work, we use this representational bottleneck as an interface to ask: how precise are textual representations of visually-descriptive language in these modern multimodal models? Pooled text encoders (c.f., bidirectional multimodal encoders) are used for a variety of practical reasons: e.g., for guided diffusion (Dhariwal and Nichol, 2021), for fast k-NN queries over billions of images (Schuhmann et al., 2022), for contrastive objectives dependent on large batch sizes like Radford et al. (2021)'s 32K example \\\"mini\\\"-batch, etc.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: We probe the representations of single-vector text encoders used in popular VL models. Using a corpus of increasingly compositional image captions, CompPrompts, we attempt to generatively decode the original input sentence. Text encoders of popular models like CLIP fail to effectively encode precise aspects of their captions like attribute attachments and object relationships (real examples shown, as in Figure 1).\\n\\nOur strategy is as follows: for a fixed pooled text encoder $T: x \\\\rightarrow y$, which maps from captions $x$ to vectors $y \\\\in \\\\mathbb{R}^d$, we test how accurately $x$ can be recovered by an expressive generative decoder given $y$, i.e., $P(x|T(x))$. In an ideal case, $T$ should result in no information loss, i.e., an exact reconstruction of the original caption should be possible, to account for specific visual factors.\\n\\nHowever, we hypothesize that if a specific visually descriptive property (e.g., a spatial relation) cannot be accurately decoded from $y$ (using a decoder trained with significant supervision), then it is unlikely a multimodal model can effectively use that property of $x$ using $T$. Different from existing probes, ours does not require images, enabling exploration of a broader range of captions, e.g., creative text-to-image queries for which there may be no associated image (like \\\"A penguin on Mars... \\\").\\n\\nWe execute our probe using an increasingly-compositional hierarchy of image captions we curate, CompPrompts, which covers cases ranging from a single object with no attributes (e.g. \\\"a cat\\\") to multiple objects with attributes and relations (e.g. \\\"an orange cat to the left of a dog\\\"). We also test counting (e.g. \\\"three cats\\\") (Segu\u00ed et al., 2015; Parcalabescu et al., 2021) and negations (e.g. \\\"a cat that is not yawning\\\"). We compare five text encoders, and find that top contrastive VL models: (1) are broadly ineffective at textually encoding spatial relations, numbers, and negations; (2) frequently cannot match attributes to their corresponding objects; and (3) fail more as inputs grow more compositional. While some text encoders perform significantly better than others, all underperform a proof-of-concept model which demonstrates that our prompts can indeed be compressed into single vectors with little information loss.\\n\\nIn order to verify that our text-only probe predicts performance in multimodal settings, we curate an evaluation set of image-caption pairs, ControlledImCaps, which operationalizes the compositional factors of CompPrompts in a multimodal setting. Results on this corpus suggest our text-only probe gives a necessary condition: if the text-only recovery probe fails to recover a text-only property on CompPrompts, then the associated multimodal model also performs poorly for that property on ControlledImCaps. However, our results also suggest that text-only recoverability is not a sufficient condition: a model can achieve low text-only information loss on a particular prompt type but not fully solve it on ControlledImCaps.\\n\\nTo facilitate future probing experiments, we release our code alongside the newly collected CompPrompts and ControlledImCaps corpora at https://github.com/amitakamath/vl_text_encoders_are_bottlenecks. \\n\\n2 Evaluation Corpora\\n\\n2.1 CompPrompts\\n\\nWe create an evaluation dataset of 18,100 text prompts describing potential visual scenes with varying degrees of specificity and composition. Our starting point is animate nouns with corresponding verbs and adjectives from the Web10K dataset (Kamath et al., 2022). We remove some synonyms to prevent ambiguity in the prompt (e.g. \\\"a rhino to the left of a rhinoceros\\\").\\n\\nThe prompts are increasingly compositional: They have 1 or 2 unique nouns, and 0, 1, or 2 attributes, of which there are 4 types: adjective, verb, spatial, and temporal. Nouns are randomly matched to generate prompts with two unique nouns \u2014 this results in unusual and imaginative text inputs that cannot be guessed based on priors learned during model pre-training (e.g., \\\"a crab lifting... \\\").\"}"}
{"id": "emnlp-2023-main-301", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The verb and spatial attributes can have either one associated noun (i.e. intransitive, e.g. \u201ca koala yawning\u201d, \u201ca policeman on the left\u201d) or two (i.e. transitive, e.g. \u201ca poet chasing a rabbit\u201d, \u201ca dinosaur left of a tiger\u201d). We also test multiples and negations in the one-attribute setting.\\n\\n2.2 ControlledImCaps\\n\\nWe create a second evaluation dataset to evaluate the overall vision-language model, rather than the text encoder specifically: where CompPrompts contains text prompts alone, ControlledImCaps contains 600 pairs of images, along with a corresponding caption for each image.\\n\\nThe images are sourced from the COCO validation set (Lin et al., 2014), and the captions are handwritten to study one of six specific fine-grained textual properties: spatial relations with one associated noun, spatial relations with two associated nouns, temporal relations, verbs with one associated noun, verbs with two associated nouns, or adjectives. For spatial relations, we evaluate only \u201cleft\u201d and \u201cright\u201d (unlike CompPrompts, which evaluates also \u201cabove\u201d, \u201cunder\u201d, \u201cin front of\u201d, and \u201cbehind\u201d), due to insufficient presence of other spatial relations clearly depicted in the COCO data.\\n\\nA key property of ControlledImCaps is that only one word changes between the two captions associated with a given image pair, such that the relation is changed or inverted: e.g., the caption pair \u201ca person before opening an umbrella\u201d, \u201ca person after opening an umbrella\u201d, along with the corresponding images for each (as in Figure 1) tests the overall model\u2019s understanding of temporal relations alone, without conflating any other types of reasoning.\\n\\n3 Text-only Recovery\\n\\nFor a given text encoder $T$, our first step is to obtain a training corpus of representations to fit a decoding probe $P(x|T(x))$. We use (just the text of) Conceptual Captions 3M (Sharma et al., 2018) (CC3M) split into a 90/10 train/val set; this corpus consists of cleaned alt-texts from web images, and thus is similar to the pretraining corpora of many VL models. For $P$, we use T5-large: specifically, we condition the decoder on $T(x)$, followed by a linear transformation and layer normalization. We train using Adafactor (Shazeer and Stern, 2018) with $T(x)$ Embed. size Avg. EM (%)\\n\\n| Model                        | Embed. Size | EM   |\\n|------------------------------|-------------|------|\\n| CLIP ViT-B/32                | 512         | 13.2 |\\n| CLIP ViT-L/14                | 768         | 28.5 |\\n| negCLIP ViT-B/32             | 512         | 28.6 |\\n| RoBERTaCLIP ViT-B/32         | 512         | 28.9 |\\n| SBERT                        | 768         | 41.6 |\\n| Proof-of-concept T5          | 1024        | 92.9 |\\n\\nTable 1: Average EM performance of each text encoder on CompPrompts, not including multiples and negations (reported in Table 3).\\n\\nA batch size of 512 for 4 epochs over CC3M; we select checkpoints with the lowest val loss. Models are trained using 4xA6000 GPUs with 48GB of memory using Transformers (Wolf et al., 2019) and accelerate 2. At evaluation time, we generate captions for CompPrompts set using beam=5 search.\\n\\nText Models.\\n\\nWe evaluate several $T$ models: CLIP ViT-B/32 (12 layers, 512 dim) and ViT-L/14 (12 layers, 768 dim) (Radford et al., 2021), CLIP with a RoBERTa-pretrained text encoder (Liu et al., 2019; Ilharco et al., 2021), and Yuksekgonul et al. (2023)\u2019s more-order-aware CLIP encoder finetuned with hard negatives, negCLIP. For comparison, we also consider the uni-modal SentenceBERT (Reimers and Gurevych, 2019) model all-mpnet-base-v2, which is trained on several sentence similarity datasets including COCO captions (Lin et al., 2014).\\n\\nProof-of-concept T5 We also consider a T5-large text encoder that produces a single vector output via mean pooling over the token embeddings. In contrast to the other fixed encoders, we fine-tune this model on CC3M, like an autoencoder 3. Then, we use the resulting encoder as a feature extractor, and hand a dimension-shuffled version of the resulting embeddings to the probe. This \u201cproof of concept\u201d encoder is specifically optimized to generate a vector from which a T5 model can decode the full sentence, and serves to validate that our probe setup is even possible.\\n\\nEvaluation.\\n\\nWe evaluate using exact match (EM). While we report BLEU scores in the Appendix, for our high-precision setting, partial credit metrics are too generous, e.g., generating \u201ca re-\\n\\n2https://github.com/huggingface/accelerate\\n3There is no overlap between CC3M and CompPrompts.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Prompt example and exact match (% EM) score of reconstruction from all models, averaged over several hundred instances each. As inputs become more compositional, vision-language text encoders perform increasingly poorly on text reconstruction.\\n\\n porter on top of a penguin\u201d as \u201ca penguin on top of a hill\u201d scores 48 BLEU-4 points. Similarly for BERT-Score (Zhang et al., 2020), where generating \u201ctwo rabbits and three shrimps\u201d as \u201cfour of the shrimps and a rabbit\u201d scores 0.91 F1.\\n\\n3.1 Text-only Recovery Results\\n\\nTable 1 presents the average exact match of each model over the corpus of prompts in CompPrompts, excluding negations and multiples, which are reported in Table 3. The proof-of-concept5 model\u2019s high performance illustrates that it is possible in theory to nearly exactly decode all captions in CompPrompts using T5-large, given the \u201cright\u201d encoding. Beyond proof-of-concept5, the best performing model is SBERT; and the best performing multimodal model is RoBERTa-CLIP.\\n\\n3.2 Fine-Grained Results on Different Prompt Types\\n\\nTable 2 contains EM results of all models on the various types of prompts in CompPrompts. A separate study on multiples and negations in Table 3 shows that text encoders struggle to encode those as well. These results show that it is fairly difficult to decode input sentences from text representations for most VL models, with increasingly compositional categories proving more difficult (e.g., \u201can orange cat\u201d to \u201can orange cat yawning\u201d to \u201can orange cat chasing a dog\u201d).\\n\\nSpatial relations. Text encoders of VL models struggle to represent spatial relations (average 23.7 EM), particularly those between two objects (average 13.8 EM). SBERT, in comparison, scores 36.9 and 22.3 EM, respectively.\\n\\nTemporal relations. VL models perform poorly on temporal relations, scoring on average 17.1 EM. In comparison, SBERT scores 29.6 EM \u2014 likely because temporal relations appear more frequently in language than in web alt-text.\\n\\nTransitive vs intransitive verbs and prepositions. On transitive verbs (e.g., \u201cchasing\u201d), CLIP ViT-B/32 and ViT-L/14 do worse by an average of 21 EM than vs. intransitive verbs (e.g., \u201cyawning\u201d), whereas negCLIP and RoBERTa-CLIP do better by an averaged 18.7 points. On transitive prepositions (\u201cto the left of\u201d) instead of intransitive (\u201con the left\u201d), all models do worse by an averaged 35 EM.\\n\\nNegations and multiples. Models perform poorly on negations (average EM 13.0) and multiples (average EM 5.1). This agrees with previous observations that VL models struggle with counting (Segu\u00ed et al., 2015; Parcalabescu et al., 2021).\\n\\nPrompts where word order matters. VL text encoders struggle to capture word order: on prompts where word order matters less (e.g., \u201ca cat and a dog\u201d), they score an average of 34 EM, but where word order matters more, they score an average of 15.8 EM. The failure cases are often caused by assigning attributes to nouns incorrectly, as highlighted in the Appendix. This extends Thrush et al. (2022)\u2019s and Yuksekgonul et al. (2023)\u2019s finding that contrastive VL models can behave like bags-of-words \u2014 this issue manifests just in the text encoder as well.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adjectives and verbs. VL models perform relatively well in the basic one-object, one-attribute setting on both adjectives (average EM 44.8) and verbs (average EM 34.5): even higher than the zero-attribute setting, where error analysis reveals they tend to hallucinate information (\u201ca tarantula\u201d \u2192 \u201ca tarantula in a hand\u201d). While these numbers are well behind SBERT (EM 91.8 and 78.4 respectively), they agree with previous observations that VL models exhibit good visual recognition of basic adjectives and actions (Radford et al., 2021).\\n\\nCompositionality. Text encoders struggle with increasingly compositional information, e.g., the probe decodes SBERT (\u201ca dentist after examining an ape\u201d) \u2192 \u201can ape after examining a dentist\u201d. On average, performance on two unique objects drops by 49% from their performance on one unique object (for CLIP ViT-B/32, it drops 71%). VL model performance drops on average by 35% when the prompt contains two attributes compared to one.\\n\\n3.3 Fine-Grained Results for Different Model Designs\\nPre-training the text encoder helps, especially on negations. The average EM of RoBERTa-CLIP on prompts without multiples or negations is 15.7 points higher than CLIP ViT-B/32. However, on the prompts that do include negations, its average EM is 29 points higher. This provides evidence that text pre-training the text encoder helps negations, presumably because negations are less likely in alt-texts compared to other settings.\\n\\nIncreasing model size helps overall, but not on spatial relations. The average EM of CLIP ViT-L/14 on prompts that do not include spatial relations is 20.7 points higher than CLIP ViT-B/32. However, on the prompts that do include spatial relations, its average EM is only 4 points higher. The modest increase of text encoder size in the CLIP training regime appear less reliable for encoding spatial relations than text pre-training or hard negatives (though, more significant scaling could be beneficial, as in Imagen (Saharia et al., 2022)).\\n\\nHard negatives from Yuksekgonul et al. (2023) help, especially where word order matters. On average, negCLIP does 15.4 points better than CLIP. On prompts where word order matters (e.g. \u201ca cat chasing a dog\u201d), it scores 16.3 points higher; on prompts where word order does not matter (e.g. \u201ca cat and a dog\u201d), it scores 12.8 points higher.\\n\\n3.4 Incorrect Model Predictions\\nWe manually inspect models\u2019 incorrect predictions. Decoded VL text encoder predictions often come close (e.g. \u201cthree shrimps\u201d \u2192 \u201cthree of shrimp\u201d is a pattern shown by CLIP ViT-B/32, CLIP ViT-L/14 and negCLIP), whereas SBERT\u2019s incorrect decodings fall further afield (e.g. \u201cthree gardeners\u201d \u2192 \u201cthree gardeners and a third man.\u201d). Thus, while the superior results of the unimodal SBERT compared to the VL text encoders when evaluated in the same frozen-encoder setting (including CLIP ViT-L/14, which has the same text embedding size) show that there is significant room for improvement for VL text encoders, the types of errors made by each model may not be fully captured by EM. Nonetheless, EM remains an appropriate metric for our high-precision setting, as discussed in Section 3.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Each attribute in \\\\textit{ControlledImCaps}, with comparable prompts in \\\\textit{CompPrompts} and an example.\\n\\nEncoder performance of a VL model on a particular prompt type in \\\\textit{CompPrompts} with the performance of the overall VL model on that prompt type in \\\\textit{ControlledImCaps}. As discussed in Section 2.2, the two captions in every example differ by only one word which changes or inverts the relation, allowing us to perform fine-grained analyses in controlled settings without conflating multiple types of compositional reasoning. Figure 3 depicts the six types of attributes studied in \\\\textit{ControlledImCaps}, their corresponding prompt type in \\\\textit{CompPrompts}, and an example of each.\\n\\n\\\\textbf{VL Models.} We evaluate the same VL models as in Section 3: CLIP ViT-B/32, CLIP ViT-L/14, CLIP with a RoBERTa-pretrained text encoder (Liu et al., 2019; Ilharco et al., 2021), and negCLIP (Yuksekgonul et al., 2023). Each of these models can return a score when given an image and a caption, representing how well they match.\\n\\n\\\\textbf{Evaluation.} We follow the evaluation scheme from Winoground (Thrush et al., 2022): for a given pair of images with corresponding captions, we measure both a text score, the fraction of instances where a model scores the correct caption higher than the incorrect caption when given an image, and an image score, the fraction of instances where a model scores the correct image higher than the incorrect image when given a caption.\\n\\n| Type   | Example                  | Text score | Image score |\\n|--------|--------------------------|------------|-------------|\\n|   1-obj L/R | a cat on the right       | 28.5       | 4.0         |\\n|   2-obj L/R | a person to the right of a horse | 4.4        | 4.0         |\\n|   Temporal | a person feeding an elephant | 26.8       | 7.0         |\\n|   Verb 1-obj | a dog before catching a frisbee | 71.6       | 84.0        |\\n|   Verb 2-obj | a dog after catching a frisbee | 37.2       | 46.0        |\\n|   Adjectives | a bird sitting          | 81.8       | 65.0        |\\n|   Adjectives | a bird flying           |            |             |\\n\\nTable 4: Performance of CLIP ViT-L/14 text encoder (%) on the equivalent prompts in \\\\textit{CompPrompts}, and performance of CLIP ViT-L/14 full model on \\\\textit{ControlledImCaps}(CIC). On the types of prompts where the text encoder performs poorly, so too does the overall model. We see similar findings per prompt type and model design as those discussed in Section 3.3.\\n\\n4.1 Multi-modal Results\\n\\nTable 4 presents the results of CLIP ViT-L/14 on both \\\\textit{CompPrompts} and \\\\textit{ControlledImCaps} (all model results in Appendix). The \\\\textit{CompPrompts} results correspond to the prompt type(s) most closely matching the captions in \\\\textit{ControlledImCaps} (specified in Figure 3). For the spatial relations, for this table alone, we calculate the EM on the data points in \\\\textit{CompPrompts} containing \u201cleft\u201d and \u201cright\u201d spatial relations only due to lack of sufficient support in COCO for other spatial relations, as discussed in Section 2.2. On prompt types where the text encoder performance on \\\\textit{CompPrompts} is poor, the overall model performance on \\\\textit{ControlledImCaps} is also poor: showing that the text encoder does indeed bottleneck VL models' compositionality.\\n\\n4.2 Fine-Grained Results on Different Prompt Types\\n\\nWe discuss findings on the prompt types in \\\\textit{ControlledImCaps}, with 95% confidence intervals. Models do poorly on spatial relations. On average, VL models perform poorly on spatial relations, achieving an average image/text score of $2.5 \\\\pm 12.4$ ($\\\\pm 2.2 \\\\pm 3.7$). Their text encoder performance on the corresponding prompts in \\\\textit{CompPrompts} was similarly poor, with an average EM of 27.8. This agrees with Kamath et al. (2023), which shows that VL models struggle with spatial relations.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Models do poorly on temporal relations. VL performs poorly on temporal relations, with an average image |text score of 5.3 |30.8 (\u00b1 2.7 |4.8). Their text encoder performance on CompPrompts temporal reasoning was similarly low at 18.9 EM.\\n\\nModels do well on verbs and adjectives. VL models perform well on verbs (average image |text score 65.4 |78.1, \u00b1 5.0 |4.8) and even better on adjectives (average image |text score 78.5 |89.0, \u00b1 7.0 |3.5), mirroring their text encoder performance on CompPrompts, where the average EM for verbs and adjectives were 33.7 and 44.8 respectively.\\n\\nTwo-object verbs are more difficult than one-object verbs. We find that for all models, two-object verbs are harder than one-object verbs, with the former achieving an image |text score of 52.3 |68.5 and the latter 78.5 |87.8 (with p < 0.05 under the Wilcoxon signed-rank test). This follows performance on CompPrompts for ViT-B/32 and ViT-L/14, but not for negCLIP and RoBERTa-CLIP, hinting that ability to reconstruct is necessary but not sufficient, as discussed in Section 4.5.\\n\\n4.3 Fine-grained results on different model design choices\\n\\nWe discuss findings on the model designs in ControlledImCaps. All findings are statistically significant at p < 0.05 using the Wilcoxon signed-rank test to compare models.\\n\\nPre-training the text encoder improves text score on verbs. RoBERTa-CLIP obtains a higher text score than CLIP ViT-B/32 (78.0 vs 68.0), as well as a higher EM on the prompts in CompPrompts corresponding to verbs (39.4 vs 11.0).\\n\\nIncreasing model size does not help on spatial or temporal reasoning. On both spatial and temporal reasoning inputs, ViT-L/14 performance on ControlledImCaps was not statistically significantly higher than that of ViT-B/32.\\n\\nHard negatives from Yuksekgonul et al. (2023) help where word order matters. On prompts where word order matters, negCLIP scores an image |text score of 36.5 |50.0 and a CompPrompts EM of 27.2, and other models score an average image |text score of 24.8 |37.5 and a CompPrompts EM of 21.0. negCLIP also outperforms ViT-B/32 on all prompts on average.\\n\\n4.4 Text reconstruction appears to be necessary...\\n\\nTo study the relationship between text reconstruction and overall model performance beyond Table 4, we evaluate text reconstruction on ControlledImCaps. Specifically, we use the trained T5 decoders from Section 3 and try to reconstruct the input when ControlledImCaps text inputs are evaluated.\\n\\nOn the cases where the reconstruction is incorrect according to human evaluation on either of the two text inputs, the overall model Image Score on ControlledImCaps for CLIP ViT-L/14 is zero 96% of the time, and the Text Score is zero 83% of the time. This text reconstruction vs. multimodal matching correlation is more direct compared to the similar correlation reported in Table 4 because we compare on the same instances.\\n\\n4.5 ... but insufficient.\\n\\nConversely, just because a model performs well on the CompPrompts probe does not mean it will perform well on ControlledImCaps. For example, ViT-L/14 outperformed ViT-B/32 overall on CompPrompts, but not (statistically significantly) on ControlledImCaps. Also, RoBERTa-CLIP outperforms ViT-B/32 on temporal relations on CompPrompts, but achieves a worse text score on ControlledImCaps. When we evaluate text reconstruction on ControlledImCaps, on cases where the reconstruction is correct for both text inputs, the overall model Image Score on ControlledImCaps for ViT-L/14 is zero 59% of the time, and the Text Score is zero 47% of the time. This suggests that text recoverability is a necessary but insufficient condition for overall model performance. The insufficiency is intuitive, as multimodal errors could potentially stem from the image encoder.\\n\\n4.6 A Note on Winoground\\n\\nWe evaluate our four VL models on the Winoground dataset (Thrush et al., 2022). They perform poorly, with an average image |text score of 10.3 |30.8, where random chance is 25.0 |25.0. However, on shorter inputs (5 words or less) which exhibit fewer compositional concepts on average, e.g., \\\"a bird eats a snake\\\" |\\\"a snake eats a bird\\\", the four models achieve higher scores of 20.4 |47.2 on average. On longer (over 10 words), more\"}"}
{"id": "emnlp-2023-main-301", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"compositional inputs, e.g., \u201cin the stadium, the person wearing gray outperformed the one wearing blue\u201d vs. \u201cin the stadium, the person wearing blue outperformed the one wearing gray\u201d, models achieve a much lower score of 3.4 vs. 18.5. This mirrors our finding on CompPrompts that VL text encoders struggle with increasingly compositional inputs.\\n\\n5 Related work\\n\\nBuilding models capable of reasoning jointly about visual and textual inputs is a long-standing goal of AI (Winograd, 1971), with potential applications in the fields of vision-language navigation (Anderson et al., 2018), human-robot interaction (Matuszek et al., 2012), accessible image captioning (Gurari et al., 2020), etc.\\n\\nRecent challenge datasets have been designed to probe the capacity of multimodal models to represent descriptions of precise visual compositions (Johnson et al., 2017; Suhr et al., 2019; Hudson and Manning, 2019; Thrush et al., 2022). Yuksekgonul et al. (2023) and Yamada et al. (2022) study CLIP specifically, demonstrating its shortcomings (and some potential fixes) in terms of modeling syntax. Ma et al. (2022) study OpenCLIP models for various types of compositional reasoning, with programmatically sourced hard negatives. Different from these works, our textual probe does not require access to images.\\n\\nOur image-and-text evaluation most closely resembles Thrush et al. (2022). However, we stratify the examples based on type of input (e.g., temporal relations) to provide more detailed insights. We also keep our prompts relatively simple, never having more than two objects or two attributes in the input. We believe this is a more realistic goal for our current vision-language models. The word order shuffling aspect is also discussed in Yuksekgonul et al. (2023). However, as their proposed benchmark does not provide pairs of images with corresponding captions, it is possible to achieve state-of-the-art with a text-only model (specifically, 2-shot ChatGPT [6](Ouyang et al., 2022), details in Appendix and the recent Hsieh et al. (2023)). While this does not detract from their finding that vision-language models ignore word order, our benchmarks have an additional advantage of being insensitive to text-only priors.\\n\\n6 Conclusion and Discussion\\n\\nWe present probing results that suggest significant information loss upon text encoding of compositional inputs in vision and language models. This information loss is quantified using CompPrompts, a test set of increasingly compositional image descriptions, and ControlledImCaps, a test set that we use to verify that this information loss affects the performance of multimodal models on compositional inputs. Harder negatives, more text pretraining, and larger models all improve encoder quality, but information is still lost even for the most performant models, compared to the uni-modal SBERT as well as a T5-based auto-encoder.\\n\\nGoing forward, even more difficult test sets than CompPrompts and ControlledImCaps might be required to analyze and evaluate vision-language model capabilities. Returning to Riedl (2022)\u2019s tweet from the intro, \u201cA penguin on Mars wearing a spacesuit and walking a robot dog next to Santa Claus.\u201d, even our highly accurate proof-of-concept T5 model struggles, predicting: \u201ccompulsory penguin onexposition wearing a spacesuit and walking a dog robot next to hoc.\u201d To support imaginative text-to-image generation queries (for images that may not exist yet), future work would be well-suited to design text encoders that can generalize to captions that contain compositions never-before-seen in web alt-text corpora.\\n\\nOur probing results suggest two future modeling directions: (1) Modifying contrastive VL models\u2019 training objectives to additionally encourage ability to reconstruct the text input, either through an additional reconstruction loss on the text encoder during finetuning, or through the addition of even harder negatives than Yuksekgonul et al. (2023) and Ma et al. (2022), would be an exciting avenue for future work. Alternatives to contrastive training, such as captioning, have also shown promise in recent work (Tschannen et al., 2023); and (2) explicitly encouraging linear recovery with a modified loss function: while the gap between SBERT and the VL Text encoders can be partially explained by the superior pooling method and training data, SBERT\u2019s training objective does not require linear recoverability (whereas CLIP\u2019s dot product interaction term might): explicitly encouraging linear text-text recoverability might improve multimodal performance. Finally, we hope that ControlledImCaps can facilitate research beyond single-vector bottleneck VL models.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nFirst, our probing method involves a pre-trained T5 decoder. It is possible that language biases from the pre-training emerge while decoding from the VL text embedding, e.g., predicting \\\"a dog chasing a cat\\\" instead of \\\"a cat chasing a dog\\\" because the former is more likely under the T5 decoder's priors from pre-training. However, as the methodology is the same across all models we evaluate, we believe that the evaluation is fair. Second, we evaluate with only one probe, whereas probing with complementary methods (e.g., especially deterministic ones, like a convex linear probe) could reveal more insights. Third, text encoders that do well on our evaluation may not perform well if directly plugged into a contrastive VL model like CLIP, if the text encoders were not trained to encode the information in a manner that is linearly recoverable.\\n\\nAcknowledgements\\n\\nThe authors thank John Hewitt, Akhila Yerukola, and anonymous reviewers for useful discussion and feedback. This work was funded by the Allen Institute for AI. AK was additionally supported by the UCLA Computer Science Department First-Year Fellowship. KC was supported in part by U.S. DARPA MCS Program under contract number N660011924032, U.S. DARPA ECOLE Program No. HR00112390060, and ONR N00014-23-1-2780, and a Sloan Fellowship. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of DARPA, or the U.S. Government.\\n\\nReferences\\n\\nPeter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S\u00fcnderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. 2018. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In CVPR.\\n\\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. NeurIPS.\\n\\nTejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. 2022. Benchmarking spatial relationships in text-to-image generation. arXiv preprint arXiv:2212.10015.\\n\\nDanna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. 2020. Captioning images taken by people who are blind. In ECCV. Springer.\\n\\nCheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. 2023. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. In Thirty-Seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\\n\\nDrew A Hudson and Christopher D Manning. 2019. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, pages 6700\u20136709.\\n\\nGabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. 2021. Openclip.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning.\\n\\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR.\\n\\nAmita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and Aniruddha Kembhavi. 2022. Webly supervised concept expansion for general purpose vision models. ECCV.\\n\\nAmita Kamath, Jack Hessel, and Kai-Wei Chang. 2023. What's \\\"up\\\" with vision-language models? investigating their struggle with spatial reasoning. In EMNLP.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manhardt Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.\\n\\nZixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. 2022. Crepe: Can vision-language foundation models reason compositionally? In CVPR.\\n\\nCynthia Matuszek, Nicholas FitzGerald, Luke Zettlemoyer, Liefeng Bo, and Dieter Fox. 2012. A joint model of language and perception for grounded attribute learning. In ICML.\"}"}
{"id": "emnlp-2023-main-301", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-301", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5 contains average BLEU-4 scores of the models. Table 6 contains a study of model performance on object-attribute association in CompPrompts. Table 7, Table 8 and Table 9 contain results of other models on ControlledImCaps in comparison to CompPrompts (ViT-L/14 is discussed in Table 4). Table 10 discusses text-only results on the ARO benchmark (Yuksekgonul et al., 2023).\\n\\n| Embed. size | Avg. BLEU-4 |\\n|------------|-------------|\\n| CLIP ViT-B/32 512 | 33.3 |\\n| CLIP ViT-L/14 768 | 46.0 |\\n| negCLIP ViT-B/32 512 | 50.2 |\\n| RoBERTaCLIP ViT-B/32 512 | 52.0 |\\n| SBERT 768 | 56.7 |\\n\\nTable 5: Average BLEU-4 performance of each text encoder on CompPrompts, not including multiples and negations. The trend correlates with EM %, but the evaluation itself is too lenient for our purposes, as described in the main text.\\n\\n| Embed. size | Shuffled % (\u2193) |\\n|------------|----------------|\\n| CLIP ViT-B/32 512 | 51.8 |\\n| CLIP ViT-L/14 768 | 55.5 |\\n| negCLIP ViT-B/32 512 | 37.2 |\\n| RoBERTaCLIP ViT-B/32 512 | 62.8 |\\n| SBERT 768 | 44.2 |\\n\\nTable 6: Of the times the model gets the words in the prediction correct, Shuffled % is the percentage of when it gets the word order incorrect (in the prompts where word order matters, unlike \u201ccat and dog\u201d \u2014 specifically, where attributes must be associated with the correct object). Clearly, negCLIP having been trained with hard negatives involving word order shuffling allows it to perform the best. All models suffer from poor object attribute association.\\n\\n| Prompt Type | EM on CompPrompts | CIC Image score | CIC Text score |\\n|-------------|--------------------|-----------------|----------------|\\n| Spatial 1-obj L/R | 27.2 | 1.0 | 10.0 |\\n| Spatial 2-obj L/R | 0.6 | 4.0 | 10.0 |\\n| Temporal | 7.3 | 4.0 | 35.0 |\\n| Verb 1-obj | 15.4 | 71.0 | 77.0 |\\n| Verb 2-obj | 6.6 | 43.0 | 59.0 |\\n| Adjectives | 38.2 | 74.0 | 92.0 |\\n\\nTable 7: Performance of CLIP ViT-B/32 text encoder (%) on the equivalent prompts in CompPrompts, and performance of CLIP ViT-B/32 full model on ControlledImCaps (CIC).\\n\\n| Prompt Type | EM on CompPrompts | CIC Image score | CIC Text score |\\n|-------------|--------------------|-----------------|----------------|\\n| Spatial 1-obj L/R | 26.0 | 1.0 | 13.0 |\\n| Spatial 2-obj L/R | 15.0 | 3.0 | 13.0 |\\n| Temporal | 23.0 | 8.0 | 35.0 |\\n| Verb 1-obj | 20.6 | 84.0 | 94.0 |\\n| Verb 2-obj | 39.4 | 70.0 | 87.0 |\\n| Adjectives | 42.2 | 95.0 | 92.0 |\\n\\nTable 8: Performance of negCLIP ViT-B/32 text encoder (%) on the equivalent prompts in CompPrompts, and performance of negCLIP ViT-B/32 full model on ControlledImCaps (CIC).\\n\\n| Prompt Type | EM on CompPrompts | CIC Image score | CIC Text score |\\n|-------------|--------------------|-----------------|----------------|\\n| Spatial 1-obj L/R | 92.3 | 1.0 | 10.0 |\\n| Spatial 2-obj L/R | 28.3 | 2.0 | 20.0 |\\n| Temporal | 18.5 | 2.0 | 23.0 |\\n| Verb 1-obj | 30 | 75.0 | 91.0 |\\n| Verb 2-obj | 48.8 | 50.0 | 65.0 |\\n| Adjectives | 17 | 80.0 | 87.0 |\\n\\nTable 9: Performance of RoBERTa-CLIP ViT-B/32 text encoder (%) on the equivalent prompts in CompPrompts, and performance of RoBERTa-CLIP ViT-B/32 full model on ControlledImCaps (CIC).\"}"}
{"id": "emnlp-2023-main-301", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Performance of ChatGPT 2-shot on the ARO benchmark (Yuksekgonul et al., 2023). While the dataset was designed to test VL models' sensitivity to word order shuffling (which is orthogonal to text-only performance on the same data), the textual priors that exist in ARO (e.g., \u201chorse eating grass\u201d is more likely than \u201cgrass eating horse\u201d) are less relevant to the probing experiments for CompPrompts (because the probe must reconstruct any given caption in CompPrompts, including unusual ones, e.g., \u201cfive teenagers riding three butterflies\u201d) and do not exist in ControlledImCaps due to the paired-image construction.\"}"}
