{"id": "emnlp-2022-main-593", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EDIN: An End-to-end Benchmark and Pipeline for Unknown Entity Discovery and Indexing\\n\\nNora Kassner, Fabio Petroni, Mikhail Plekhanov, Sebastian Riedel, Nicola Cancedda\\n\\nMeta AI\\nkassner@meta.com\\n\\nAbstract\\nExisting work on Entity Linking mostly assumes that the reference knowledge base is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as knowledge bases are incomplete and because novel concepts arise constantly. We introduce the temporally segmented Unknown Entity Discovery and Indexing (EDIN) benchmark where unknown entities, that is entities not part of the knowledge base and without descriptions and labeled mentions, have to be integrated into an existing entity linking system. By contrasting EDIN with zero-shot entity linking, we provide insight on the additional challenges it poses. Building on dense-retrieval based entity linking, we introduce the end-to-end EDIN-pipeline that detects, clusters, and indexes mentions of unknown entities in context. Experiments show that indexing a single embedding per entity unifying the information of multiple mentions works better than indexing mentions independently.\\n\\n1 Introduction\\nMost existing works on Entity linking (EL) \u2013 the fundamental task of detecting mentions of entities in context and disambiguating them against a reference knowledge base (KB) \u2013 assume that such KB is complete, and therefore all mentions can be linked. In practice this is hardly ever the case, as KBs are incomplete when they are created and because novel concepts arise constantly. For example, English Wikipedia, often used as the reference KB for large scale linking, is growing by more than 17k entities every month. Consequently, at the time of deployment EL systems are quickly outdated and static evaluation overestimates performance. But as these systems play significant role in many real world industry applications, e.g., moderating discussions around recent events, a dynamic look on EL is crucial. Nonetheless, related work on this problem is sparse. Available datasets (Ji et al., 2015; Derczynski et al., 2017; Nakashole et al., 2013) and models (Hoffart et al., 2014) are outdated and/or small scale and use features which are not read-ily available (Nakashole et al., 2013; Wu et al., 2016). Most importantly, they approach the problem only in parts. We revisit this problem in context of dense-retrieval and large-scale EL, e.g., EL relying on bi-encoder architecture that runs a nearest neighbor search between mention encoding and a large-scale index of entity encodings. To this end, we introduce EDIN-benchmark and EDIN-pipeline where unknown entities, that is entities with no available canonical names, descriptions and labeled mentions, have to be integrated into an existing EL model in an end-to-end fashion. To the best of our knowledge, EDIN-pipeline is the first end-to-end pipeline tackling this problem.\\n\\nNote that this setting is strictly more demanding than zero-shot (zs) entity linking (Logeswaran et al., 2019), where a textual description of the zs entities is available at the time of training.\\n\\nThe EDIN-benchmark is temporally segmented into two parts, one preceding time \\\\( t_1 \\\\) and one preceding time \\\\( t_2 \\\\). With current approaches, an EL system created at \\\\( t_1 \\\\) is unable to create a dense-index entry \u2013 and therefore successfully link \u2013 unknown entities introduced after \\\\( t_1 \\\\). The task that we propose consists in adapting a model trained at \\\\( t_1 \\\\) using only an adaptation dataset \u2013 a set of new documents also mentioning unknown entities \u2013 and unsupervised techniques. There are therefore two parts to this task: i) Discovery, which consists in detecting mentions of unknown entities in the adaptation dataset and classifying them as unknown and ii) Indexing, consisting in mapping co-referring mentions of unknown entities to a single representation compatible with the entity index.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"By introducing a clear-cut temporal segmentation, EDIN-benchmark targets unknown entities which are truly novel/unseen to all parts of an EL system, specifically the pre-trained language model (PLM). Therefore, the EL system cannot rely on implicit knowledge captured by the PLM. This is, to the best of our knowledge, a setting that has not been explored before in the context of dense-retrieval based EL.\\n\\nTemporal segmentation also lets us study effects of entity encoder and PLM degradation. We observe that precision drops for known entities in novel contexts which points to a large problem of PLM staleness also discussed by (Agarwal and Nenkova, 2021; Dhingra et al., 2022; Lazaridou et al., 2021).\\n\\nWe show that distinguishing known from unknown entities, arguably a key feature of an intelligent system, poses a major challenge to dense-retrieval based EL systems, as a model has to strike a delicate balance between relying on mention vs. context: context is crucial to distinguish unknown entities carrying the same name as known entities and to co-refer different mentions of the same unknown entities, while mentions are essential to distinguish unknown entities with different name but semantic similarity to existing ones.\\n\\nOn the side of indexing, inserting unknown entities into a space of known entities poses problems of interference with known entities in their close proximity. For instance, when first encountering mentions of BioNTech we want to create an index entry in proximity of other biotech companies but in a way that linking can still differentiate between them. We find that adapting the EL model to the updated index, is essential.\\n\\nWe experiment with different indexing methods. In particular, we contrast single mention-level indexing (FitzGerald et al., 2021) with indexing clusters of mentions. We find that unifying the information of multiple mentions into a single embedding is beneficial.\\n\\nWe summarize our contributions as follows: i) We introduce the EDIN-benchmark, a large scale end-to-end EL dataset where unknown entities need to be discovered and integrated into an existing entity index in an unsupervised fashion. ii) We propose the EDIN-pipeline in the form of an extension of existing dense-retrieval architectures. iii) We contrast this task with zs EL, and provide insight on the challenges it poses. iv) We show that indexing a single embedding per entity, unifying the information of multiple mentions, works better than indexing mentions independently.\\n\\nData and evaluation code is located here: https://github.com/facebookresearch/EDIN\\n\\nTask definition\\n\\nWe formally define end-to-end EL as follows: Given a paragraph $p$ and a set of known entities $E_K = \\\\{e_i\\\\}$, each with canonical name, the title, $t(e_i)$ and textual description $d(e_i)$, our goal is to output a list of tuples, $(e, [i,j])$, where $e \\\\in E_K$ is the entity corresponding to the mention $m_{i,j}$ spanning from the $i$th to $j$th token in $p$. We call a system that solves this task based on $d(e_i)$ a Description-based entity linking system $L$.\\n\\nFor EDIN-benchmark, after training a model $L_{t_1}$ at time step $t_1$, a set of unknown entities $E_U = \\\\{e_i\\\\}$ with $E_U \\\\cap E_K = \\\\emptyset$ and no available canonical names, descriptions and labeled mentions is introduced between $t_1$ and $t_2 > t_1$. The task is to adapt $L_{t_1}$ in an unsupervised fashion such that it can successfully link mentions of $E_U \\\\cup E_K$.\\n\\nWe use three dataset splits: the training set $D_{train}$ to train $L_{t_1}$, the adaptation dataset $D_{adapt}$ used to adapt $L_{t_1}$ and the test set $D_{test}$ to evaluate. Both $D_{adapt}$ and $D_{test}$ include mentions between $t_1$ and $t_2$. The model relies on $D_{adapt}$ to discover $E_U$ and extract representations to integrate $E_U$ into the entity index. We ensure that $D_{adapt}$ and $D_{test}$ are disjoint to prevent leakage of test mentions into entity representations extracted from $D_{adapt}$.\\n\\n3 EDIN-pipeline\\n\\nOur EDIN-pipeline is built on top an end-to-end extension of the dense-retrieval based model BLINK (Ledell Wu, 2020) and is similar to (Li et al., 2020). It is composed of a Mention Detection (MD), Entity Disambiguation (ED) and Rejection (R) components. MD detects entity mention spans $[i,j]$ in context relying on BERT (Devlin et al., 2019). ED links these mentions to $e \\\\in E_K$. It relies on bi-encoder architecture running a k-nearest-neighbor (kNN) search between mention encoding and candidate entity encodings (the entity index). Mention encodings are pooled from BERT-encoded paragraph tokens $p_1..n$:\\n\\n$$m_{i,j} = FFL(BERT([CLS]p_1 ... p_n[SEP])_{i...j})$$\"}"}
{"id": "emnlp-2022-main-593", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: EDIN-pipeline: In the adaptation phase, detected mentions in $D_{adapt}$ are mapped into a joint dense space with $E_K$ representations. A clustering algorithm groups mentions and entities based on kNN-similarity. Clusters of mentions without entity encoding are collected in $E_U'$. To integrate these into the index of $E_K$, mentions in single-sentence contexts are concatenated and mapped to a single embedding using the entity encoder. After adaptation, the updated entity index is used for standard EL in an inductive setting.\\n\\nEntities are represented using BLINK's frozen entity encoder:\\n\\n$$e = \\\\text{BERT}[\\\\text{CLS}](\\\\text{CLS}t(e)[\\\\text{SEP}d(e)[\\\\text{SEP}])$$\\n\\nMention-entity candidates are passed to $R$ that controls precision-recall trade-off by thresholding a learned candidate score.\\n\\nMore information about architecture and training are detailed in appendix A.\\n\\n4 Unknown Entity Discovery and Indexing\\n\\nWe introduce an end-to-end pipeline to encode $E_U$ into $L_1$'s entity index. The process is depicted in Figure 1. This pipeline is fully unsupervised and only relies on $D_{adapt}$. It follows a two-step process: i) in Discovery the EL system detects mentions of unknown entities and recognises them as being unknown; ii) during Indexing, co-referring mentions of unknown entities are mapped to a single embedding compatible with the entity index. After adaptation the updated model is tested on $D_{test}$.\\n\\n4.1 Unknown Entity Discovery\\n\\nFirst, $L_1$ detects and encodes mentions part of $D_{adapt}$. The MD head is trained to detect mentions leveraging the context around them, and can therefore detect mentions of both $E_K$ and $E_U$. Encoded mentions $M = \\\\{m_1, \\\\ldots, m_M\\\\}$ are then input to a clustering algorithm that partitions $M$ into disjoint clusters $C = \\\\{c_1, \\\\ldots, c_C\\\\}$. We adopt the same greedy NN clustering algorithm as Logan IV et al. (2021) where $m_i$ is assigned to cluster $c_k$ if $m_j \\\\in c_k$ is NN mention to $m_i$ and $\\\\text{sim}(m_i, m_j) > \\\\delta$.\\n\\nNext, entity encodings of $e \\\\in E_K$ are assigned to these clusters if $\\\\sum_{j=0}^{J}(\\\\text{sim}(e_i, m_j))/J > \\\\tau$ holds for $m_j \\\\in c_i$ with $e_i$ being the nearest entity of $m_j \\\\in c_i$. $\\\\delta$ and $\\\\tau$ are tuned on $D_{adapt}$-dev to optimize for recall. For more details see appendix C. Following Agarwal et al. (2021), all clusters not containing any entity representation are deemed to refer to entities in $E_U$. We refer to this subset of automatically identified unknown entities as $E_U'$.\\n\\n4.2 Unknown Entity Indexing\\n\\nNext, clusters identified as $E_U'$ are integrated into the EL index of $L_1$. We explore two different methods of indexing:\\n\\nCluster-based: We concatenate all mentions part of a cluster, each with the sentence they occur in, and use the entity encoder to map to a single entity encodings. We pool over all $m_i \\\\in c_i$ and select the most occurring mention as canonical name $t(e)$.\\n\\nMention-based: Mentions in single-sentence contexts are indexed individually using the entity encoder. Individual mentions are used as $t(e)$.\\n\\n5 Evaluation\\n\\nAs mentions of type $E_U$ are significantly less frequent than mentions of type $E_K$, we report results on these two types separately. For Discovery, we report precision and recall of $E_U$ classification and clustering metrics.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1:\\n\\n| Bin       | Number of samples | Number of mentions |\\n|-----------|-------------------|--------------------|\\n| [0)       | 68,241            | 21.1               |\\n| [1, 10)   | 59,227            | 29.1               |\\n| [10, 100) | 313,232           | 45.6               |\\n| [100, 1k) | 901,857           | 65.7               |\\n| [1k, +)   | 2,860,880         | 76.9               |\\n\\nTable 2:\\n\\nFrequency effects:\\n\\nEnd-to-end EL performance of upper baseline model $L_t^2$ per frequency bins.\\n\\nTo evaluate end-to-end EL, we compute precision (P) and recall (R) following Li et al. (2020) but using a hard matching criteria.\\n\\nTo do so for cluster-based discovery, canonical names of indexed clusters need to be consistent with the set of test labels. Our method of assigning canonical names to clusters based on pooling over mentions is not. To resolve this mismatch we pool over the gold labels associated with these mentions instead of the mentions themselves. This is only done for evaluation.\\n\\nUnsupervised clustering of mentions in $D_{adapt}$ may suffer from two kinds of errors: i) Clusters can be incomplete, e.g., mentions of a single entity can be split into multiple clusters which can lead to indexing the same entity multiple times and ii) Clusters can be impure, e.g., mentions of different entities end in the same cluster, which leads to conflation of multiple entities into one representation.\\n\\nIn our evaluation we use the gold labels for computing standard EL metrics by associating possibly more than one cluster to each $E_U$, and consider a prediction correct if a mention is linked to any of the clusters associated with the correct entity. EL metrics could fail to capture shortcomings in establishing co-references between mentions though, therefore we report clustering metrics alongside EL metrics. We follow Agarwal et al. (2021) and report normalized mutual information (NMI).\\n\\nWikipedia benchmark\\nTo construct the entity index, we download Wikipedia dumps from $t_1$ and $t_2$ and extract entity titles and descriptions. Setting $t_1$ to September 2019 (the date when BLINK was trained) the KB consists of 5.9M entities, setting $t_2$ to March 2022 an additional set of 0.7M entities is introduced.\\n\\nWikipedia and Oscar data is created as follows.\\n\\nWikipedia:\\nSince usually only the first mention of an entity inside a Wikipedia article is hyperlinked, we annotate a subset of Wikipedia. We use a version of $L$ that was trained at $t_2$ on a labelled non-public dataset. While noisy, these predictions are significantly better than what our best discovery and indexing methods can achieve, therefore we adopt them as pseudo-labels for the purpose of comparing approaches. As discovery and indexing methods improve, manual labelling of the evaluation data will afford more accurate measures.\\n\\nWikipedia provides time stamps which enables us to separate two time splits.\\n\\nOSCAR news:\\nThis dataset is based on the common-crawl dataset OSCAR (Abadji et al., 2021). We select a subset of English language news pages which we label automatically as described above. The dataset consists of 797k samples, which we split based on their publication date. We publish this dataset using stand-off annotations and code to download the relevant raw data. To enable evaluation of future versions of PLMs and EL systems, we also publish our data processing scripts.\\n\\nFor both types of datasets we publish two time splits: $D_1$, containing samples preceding $t_1$, which is used to train model $L_{t_1}$ and $D_2$, with samples preceding $t_2$, which is used to train an upper bound model $L_{t_2}$. To adapt $L_{t_1}$, we hold out a subset of data from between $t_1$ and $t_2$ to construct $D_{adapt}$ ($D_{adapt} \\\\cap D_2 = \\\\emptyset$). Remaining samples are randomly split into train, dev, test. Figure 2 illustrates the different data splits. Overall dataset statistics are listed in Table 1.\\n\\nTo construct $D_{adapt}$, we follow Agarwal et al. (2021), and set the ratio of mentions of type $E_U$ to $E_K$ to 0.1. Naturally this ratio would lie at 0.02. We made this artificial adjustment to reduce the strong class imbalance and obtain more interpretable and statistically stable results. Such adjustment could be lifted once considerably more precise unknown entity discovery components become available.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"times lower than for EK. COVID-19 is the most occurring unknown entity with 12k mentions. 638k EU are not mentioned at all and only 733 are mentioned more than ten times.\\n\\n7. Results and Discussion\\n\\nIn the following sections, we discuss results for OSCAR data. Results on Wikipedia data are consistent but lower and shown in appendix F. Our main findings are shown in Table 3 where we report end-to-end performance on OSCAR D2-test.\\n\\nOverall, our results show:\\n\\n\u2022 EDIN-benchmark is challenging. Particularly attributed to imperfect discovery, end-to-end performance in terms of recall lacks significantly behind the upper bound, see 7.5.\\n\\n\u2022 When contrasting with zs EL, we find that i) adapting the model to the updated index by re-training the model after indexing is crucial, see 7.2 and ii) entity encodings relying on clusters of mentions in context instead of human crafted descriptions have high potential but discovering these clusters is challenging, see 7.4.1.\\n\\n\u2022 Our best performing system relies on Cluster-based indexing, with the advantage of attending to and unifying the information of multiple mentions, see 7.4. We call this version the EDIN-pipeline.\\n\\nIn what\u2019s to come, we first discuss upper and lower performance bounds. Then, we follow our two-step pipeline where we first present results on discovery and indexing separately and then assemble the full end-to-end pipeline.\\n\\nRecall our terminology:\\n\\n\u2022 Cluster-based: EU encodings rely on mentions in context which are concatenated and embedded into a single encoding.\\n\\n\u2022 Mention-based: EU encodings rely on individually indexed mentions in context.\\n\\n\u2022 Description-based: EU encodings rely on human crafted descriptions. This type of indexing is used in the zs setting.\\n\\n7.1 Lower and upper bounds\\n\\nOur starting point, and an obvious lower performance bound, is given by model Lt1 trained at Dt1. This model lacks representations of EU and its training data does not contain any corresponding mentions. Therefore, performance on the subset of EU is 0 for all metrics.\\n\\nFor an upper performance bound we take model Lt2 trained at Dt2. The entities in EU were introduced to Wikipedia past t1 but before t2, meaning that to Lt2 these entities are actually known: labeled mentions of EU are part of the training data and entity representations are part of the index. Lt2 reaches similar performance as Lt1 for EK. We suspect performance differences can be attributed to the difference in training data.\\n\\nPerformance of Lt2 on mentions of EU is lower than on mentions of EK. The performance discrepancy between EU and EK is largely due to frequency differences, see Table 2. We suspect that the remaining difference can be attributed to the degradation of PLM and entity encoder. Note that while labelled mentions of EU were seen during the training phase of Lt2, BLINK\u2019s entity encoder was not re-trained. To investigate this hypothesis further, we test Lt1 on mentions of EK that meet two conditions: i) time stamps of these samples are posterior to t1 and ii) two or more mentions of EU occur in their context. Thus, we target mentions of EK in novel contexts to which neither BLINK nor the PLM have been exposed. The total number of entities that meet these conditions are 40,055.\\n\\nWe find that recall drops only slightly from 80.1 to 79.9 but precision drops from 82.0 to 75.9. This result indicates that EU are also a source of noise when trying to link mentions of EK.\\n\\n7.2 Additional upper bound: Zero-shot EL\\n\\nZs EL relies on Description-based indexing. It may be a valid option in some practical settings, where we may e.g. be able to frequently download fresh Wikipedia snapshots and rerun all or part of the training, but it does not meet the conditions for being a valid entry to EDIN-benchmark, because it relies on supervision for deciding what novel entities to add to the index, and because it requires manually written descriptions for such entities. For these reasons, we present it here as an additional upper bound comparison point.\\n\\nWe note that in the zs problem, all entities are part of the index at training time. In the setting...\"}"}
{"id": "emnlp-2022-main-593", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: EL performance on OSCAR $D_t^2$-test for unknown entities $E_U$ and known entities $E_K$.\\n\\nTable 4: Adapting the model to the updated index: End-to-end EL performance on OSCAR $D_t^2$-test when adding Description-based representation of unknown entities $E_U$ to the entity index with (Re-trained) and without (Not re-trained) re-training of $L_t^1$.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We measure clustering quality of 91.2% NMI on Dadapt. We evaluate discovery based on these clusters by evaluating whether a discovered cluster is indeed referring to an EU. Note, that here duplicated discovery of the same entity is not penalized. We set the minimum number of mentions per cluster to 3 and report low discovery precision (10%) but relatively high recall (86%). Overall, this results in detecting 71% of all unknown entities part of Dadapt.\\n\\nWe find that the constraint requiring that most mentions in a cluster are within a region controlled by hyper-parameter $\\\\tau$, as described in 4.1, is crucial. In an ablation study we drop this condition and greedily assign EU to clusters if $\\\\text{sim}(e_i, m_i) > \\\\tau$ holds for any $m_i \\\\in c_i$. This setting is similar to Agarwal et al. (2021) where a single entity-mention link is sufficient for cluster assignment. Discovery dropped to 49% recall and 8% precision.\\n\\nA qualitative error analysis reveals that false negatives are mostly caused by the problem that mention embeddings of EU (e.g. BioNTech) can have high similarity with entity embeddings of EU (e.g. of other biotechnology companies). We suspect that this problem is particularly pronounced in our setting because EDIN-benchmark is large scale (up to 6 times more entities in the reference KB and up to 36 times more mentions in the clustering set compared to Agarwal et al. (2021)) with many tail entities.\\n\\nConversely, false positives are mostly due to known entities being misclassified unknown when occurring in novel contexts, e.g., \u201cblood tests\u201d or \u201cvaccine\u201d in context of COVID form distinct clusters. But, low precision in discovery is less problematic than low recall as re-training after indexing gives the ability to learn to ignore clusters of EU.\\n\\n7.4 EDIN indexing\\n\\nAfter discovery, we need mention clusters of EU to be integrated into the entity index. We compare Mention-based and Cluster-based indexing. To isolate discovery and indexing performance, we first evaluate indexing using oracle clusters, where we replace the discovery method run on Dadapt with an oracle where mentions of EU are discovered and clustered perfectly. Mention-based indexing performs worse than Cluster-based indexing with a gap of around 5% points, see Table 3 (left), $L_{1}$-Mention-Oracle vs. $L_{1}$-Cluster-Oracle.\\n\\nWhen reducing the test set to mentions of entities that were actually discoverable, the difference in recall becomes even more pronounced: 41% for Mention-based vs. 52% for Cluster-based indexing, see Table 3 (right).\\n\\nInterestingly, this means that the ability to attend over multiple mentions in context and unify their information into a single embedding leads to superior representations. Note that here the entity encoder was neither trained to deal with the style of individual mentions in context nor with clusters of mentions in context. For future work, it would be interesting to see if Cluster-based indexing can be generally beneficial to EL, outside of the context of EDIN-pipeline.\\n\\n7.4.1 Cluster-based vs. Description-based\\n\\nAs an upper baseline, we compare Cluster-based indexing with the zs setting which uses Description-based indexing. Zs EL does not rely on Dadapt but on a human\u2019s decision to add an entry to the index and therefore discovery is perfect. To isolate indexing from discovery, we again filter the test set to actually discoverable entities and assume perfect oracle clusters.\\n\\nIn this setting, see Table 3 (right), we find that Cluster-based-Oracle indexing performs 7% points lower than Description-based indexing in recall but 26% better in terms of precision.\\n\\nThe take-away is that when discovery is perfect, Cluster-based indexing relying on concatenated mentions in context instead of manually crafted descriptions has high potential. In the end-to-end setting, we see that assembling these perfect clusters is challenging.\\n\\nWe also want to emphasise that results in Table 3 show that EL performance on EU is not affected by this adaptation process. Recall and precision remain, with 80.3 and 81.9, stable. We also test if this finding also holds on standard EL datasets. We compare performance on AIDA test before and after adaptation and report no difference in performance on EU.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When reducing the test set to mentions of entities that were discoverable, thus part of $D_{\\\\text{adapt}}$, Cluster-based indexing is 1% point better in terms of recall and 0.4% worse in precision, Table 3 (right). When reducing the test set further to mentions of entities that were in fact discovered, recall of Cluster-based indexing is, with 58.4%, better than that of Mention-based indexing (55.5%).\\n\\nWe also report performance of ED with oracle mention detection in Table 6 in the appendix E. Here, we find that Cluster-based indexing is performing better than Mention-based indexing across all metrics.\\n\\nWe conclude that Cluster-based indexing performs better than Mention-based indexing. We call this version the EDIN-pipeline.\\n\\nBesides yielding an index that scales in memory with the number of entities rather than the number of mentions \u2013 a significant advantage when the number of entities is already large and in view of a streaming extension \u2013 Cluster-based indexing generates fixed-size entity embeddings as a by-product that can have applications of their own and can be used to enhance PLMs (e.g., Peters et al. (2019)).\\n\\nOverall, EDIN-pipeline performance shows that EDIN-benchmark is challenging. In terms of recall, end-to-end performance lacks 26% points behind the upper bound $L_2$. In this setting, errors in discovery propagate. Most notably, we see this manifest when i) comparing Table 3 unfiltered and filtered where the recall problem of $E_{\\\\text{U}}$ becomes apparent and ii) comparing performance of oracle and automatic clusters where precision drops by 10% points.\\n\\nIn future work, we want to explore a setting where $E_{\\\\text{U}}$ are discovered in a streaming fashion, thus scaling up $D_{\\\\text{adapt}}$ and dropping the artificially imposed ratio of $E_K$ vs. $E_{\\\\text{U}}$. This would pose challenges in terms of scale and precision in discovery. Here, a human in the loop approach, as proposed by Hoffart et al. (2016) in the context of keeping KBs fresh, to introduce a component of supervision, might be needed.\\n\\n8 Related work\\nEL is an extensively studied task. Prior to the introduction of PLMs, EL systems used frequency and typing information, alias tables, TF-IDF-based methods and neural networks to model context, mention and entity (Cucerzan, 2007; Bunescu and Pa\u0161ca, 2006; Milne and Witten, 2008; He et al., 2013; Sun et al., 2015a; Lazic et al., 2015; Raiman and Raiman, 2018; Kolitsas et al., 2018; Gupta et al., 2017; Ganea and Hofmann, 2017; Khalife and Vazirgiannis, 2018; Onoe and Durrett, 2019).\\n\\nGillick et al. (2019) present a PLM-based dual encoder architecture that encodes mentions and entities in the same dense vector space and performs EL via kNN search. Logeswaran et al. (2019) proposed the $zs$ EL task and show that domain adaptive training can address the domain shift problem. Subsequently, Wu et al. (2020) showed that pre-trained $zs$ architectures are both highly accurate and computationally efficient at scale. None of these works tackle the problem of unknown entities.\\n\\nRecently, FitzGerald et al. (2021) model EL entirely as mappings between mentions, where inference involves a NN search against all known mentions of all entities in the training set. In this setting mentions need to be labeled. They do not explore their approach in the setting of unknown entities.\\n\\nPrior to dense retrieval-based EL, unknown entity discovery work includes: Ratinov et al. (2011) train a classifier to determine whether the top ranked EL candidate is unknown relying on local context, global Wikipedia coherence, and additional manually crafted features. Nakashole et al. (2013) introduce a model for unknown entity discovery and typing leveraging incompatibilities and correlations among entity types. Hoffart et al. (2014); Wu et al. (2016) study a variety of features for unknown entity discovery: Hoffart et al. (2014) use perturbation-based confidence measures and key-phrase representations and Wu et al. (2016) explore different feature spaces, e.g., topical and search engine features. These features are not readily available and incorporating them into PLM-based approaches is not straightforward; Ji et al. (2015); Derczynski et al. (2017) introduce shared tasks for discovery. These tasks are defined on comparatively small datasets and target only named entities; Akasaki et al. (2019) introduces a time sensitive method of discovering emerging entities relying on Twitter data.\\n\\nNone of these works consider unknown entities in an end-to-end setting including mention detection, unknown entity discovery and indexing. Also, we cannot use their datasets to evaluate as these entities were part of training the PLM.\\n\\nIn the context of named entity tagging, Mota and\"}"}
{"id": "emnlp-2022-main-593", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Grishman (2009) showed that entity taggers can be effectively updated by incorporating contemporary unlabeled data using semi-supervised learning. Closely related to EL is the task of cross-document entity co-reference (CDC), where no reference KB is present (Bagga and Baldwin, 1998; Gooi and Allan, 2004; Singh et al., 2011; Dutta and Weikum, 2015; Barhom et al., 2019; Cattan et al., 2021a; Caciularu et al., 2021; Cattan et al., 2021b). Most recently, Logan IV et al. (2021) benchmark methods for streaming CDC, where mentions are disambiguated in a scalable manner via incremental clustering. Our work can be seen as bridging between the world of CDC and EL.\\n\\nMost recently, Angell et al. (2021) introduce a new EL method using document-level supervised graph-based clustering. Agarwal et al. (2021) extend this work to cross-document EL and entity discovery. In this work, we adopt a more standard bi-encoder architecture (i.e. BLINK), with better EL scalability potential (memory linear in the number of entities and not in the number of mentions) and an existing end-to-end extension. We use a modified version of their discovery method.\\n\\n**9 Conclusion**\\n\\nThis work introduced EDIN-benchmark and EDIN-pipeline. EDIN-benchmark is a large-scale, end-to-end EL benchmark with a clear cut temporal segmentation for Unknown Entity Discovery and Indexing. EDIN-pipeline detects and clusters mentions of unknown entities in context. These clusters of unknown mentions are then collapsed into single embeddings and integrated into the entity index of the original EL system.\\n\\n**Limitations**\\n\\nThe main limitations of EDIN-benchmark are: i) The dataset is not human-annotated. Instead we used an upper-bound model to label data automatically. ii) We limit \\\\( D_{\\\\text{adapt}} \\\\) in size and artificially adjust class imbalance between mentions of type \\\\( E_U \\\\) to \\\\( E_K \\\\). The limited size of \\\\( D_{\\\\text{adapt}} \\\\) in turn limits the discoverability of unknown entities, specifically low-frequency ones. Once progress is made in the accuracy and scalability of entity discovery, EDIN-benchmark can be modified to a truly dynamic setting where unknown entities are continuously discovered in a stream of incoming documents and integrated into the EL system.\\n\\nEDIN-pipeline is tailored to dense-retrieval based EL and adapting it to different EL approaches, e.g., to generative EL systems De Cao et al. (2021), is not straightforward. We study EDIN-benchmark and -pipeline in a monolingual setting using English language only. EDIN-benchmark's extension to a multilingual setting is straightforward. OSCAR and Wikipedia data are available in 166 different languages but coverage will be a problem. EDIN-pipeline can be extended to more languages by following (Botha et al., 2020) but EDIN performance is expected to vary across languages as it does for standard EL.\\n\\nEDIN-benchmark covers news and Wikipedia domain entities only, and we have not evaluated the EDIN-pipeline on other domains. The overall performance of EDIN-pipeline has ample margins for improvement, with the precision of clustering-based discovery as the main bottleneck at present. The significant number of false positives (mentions of known entities classified as unknown) is still a barrier to deployment in most real-world settings.\\n\\n**Ethical Considerations**\\n\\nEL is a standard NLP task. Outside of academia EL can be deployed in both non-problematic (e.g., content understanding for hate speech detection) and problematic (e.g., surveillance) settings. Independent of the use-case, potential bias that these models could exhibit needs to be evaluated. EL relies on human curated knowledge bases (here Wikipedia) which could carry bias e.g. in terms of language, genders and races, see for example Sun and Peng (2021). Another source of bias in the context of dense-retrieval based EL, is the bias of the underlying language model (here BERT). Both potential sources of bias could be propagated to the downstream task. To mitigate biases, we refer to Goldfarb-Tarrant et al. (2021); Steed et al. (2022) that show bias mitigation needs to be done on the side of the downstream task rather than the language model. Rudinger et al. (2018); Zhao et al. (2018) introduce methods of downstream bias mitigation, here in the context of co-reference resolution.\\n\\nWe publish our dataset/scripts that generate the datasets. Our dataset is based on English Wikipedia and a subset of English online news pages extracted from OSCAR. All Wikipedia based data is made fully available. OSCAR is common-crawl based data and only available to researchers upon request.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"request. We release code and stand-off annotations which enables researchers to reproduce the dataset. Our EL annotations rely on an upper bound model which is due to the performance gap sufficient for EDIN but should not be considered gold data for general EL tasks. We will indicate this prominently on the website we use to host the data.\\n\\nAcknowledgements\\nWe thank our colleagues Louis Martin and Frederic Dreyer for their valuable feedback.\\n\\nReferences\\nJulien Abadji, Pedro Javier Ortiz Su\u00e1rez, Laurent Ro-mary, and Beno\u00eet Sagot. 2021. Ungoliant: An optimized pipeline for the generation of a very large-scale multilingual web corpus. Proceedings of the Workshop on Challenges in the Management of Large Corpora (CMLC-9) 2021. Limerick, 12 July 2021 (Online-Event), pages 1 \u2013 9, Mannheim. Leibniz-Institut f\u00fcr Deutsche Sprache.\\n\\nDhruv Agarwal, Rico Angell, Nicholas Monath, and Andrew McCallum. 2021. Entity linking and discovery via arborescence-based supervised clustering. Oshin Agarwal and Ani Nenkova. 2021. Temporal effects on pre-trained models for language processing tasks.\\n\\nSatoshi Akasaki, Naoki Yoshinaga, and Masashi Toyoda. 2019. Early discovery of emerging entities in microblogs. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4882\u20134889. ijcai.org.\\n\\nRico Angell, Nicholas Monath, Sunil Mohan, Nishant Yadav, and Andrew McCallum. 2021. Clustering-based inference for biomedical entity linking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2598\u20132608, Online. Association for Computational Linguistics.\\n\\nAmit Bagga and Breck Baldwin. 1998. Entity-based cross-document coreferencing using the vector space model. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL '98/COLING '98, page 79\u201385, USA. Association for Computational Linguistics.\\n\\nShany Barhom, Vered Shwartz, Alon Eirew, Michael Bugert, Nils Reimers, and Ido Dagan. 2019. Revisiting joint modeling of cross-document entity and event coreference resolution. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4179\u20134189, Florence, Italy. Association for Computational Linguistics.\\n\\nJan A. Botha, Zifei Shan, and Daniel Gillick. 2020. Entity Linking in 100 Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7833\u20137845, Online. Association for Computational Linguistics.\\n\\nRazvan Bunescu and Marius Pa\u00b8 sca. 2006. Using encyclopedic knowledge for named entity disambiguation. In 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 9\u201316, Trento, Italy. Association for Computational Linguistics.\\n\\nAvi Caciularu, Arman Cohan, Iz Beltagy, Matthew Peters, Arie Cattan, and Ido Dagan. 2021. CDLM: Cross-document language modeling. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2648\u20132662, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nYixin Cao, Lei Hou, Juan-Zi Li, and Zhiyuan Liu. 2018. Neural collective entity linking. In COLING.\\n\\nArie Cattan, Alon Eirew, Gabriel Stanovsky, Mandar Joshi, and Ido Dagan. 2021a. Realistic evaluation principles for cross-document coreference resolution. In Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics, pages 143\u2013151, Online. Association for Computational Linguistics.\\n\\nArie Cattan, Sophie Johnson, Daniel S. Weld, Ido Dagan, Iz Beltagy, Doug Downey, and Tom Hope. 2021b. Scico: Hierarchical cross-document coreference for scientific concepts. In 3rd Conference on Automated Knowledge Base Construction.\\n\\nSilviu Cucerzan. 2007. Large-scale named entity disambiguation based on Wikipedia data. In Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pages 708\u2013716, Prague, Czech Republic. Association for Computational Linguistics.\\n\\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and Fabio Petroni. 2021. Autoregressive entity retrieval. In International Conference on Learning Representations.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-593", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-593", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Model\\n\\nIn the following sections, we explain our model's architecture in detail. It relies on Blink's bi-encoder architecture (680M parameters). The model can be downloaded from: https://github.com/facebookresearch/BLINK\\n\\nThe code for clustering is located here: https://github.com/rloganiv/streaming-cdc\\n\\nA.1 Mention Detection\\n\\nFor every span \\\\([i,j]\\\\), the MD head calculates the probability of \\\\([i,j]\\\\) being the mention of an entity by scoring whether \\\\(i\\\\) is the start of the mention, \\\\(j\\\\) is the end of the mention, and the tokens between \\\\(i\\\\) and \\\\(j\\\\) are the insides:\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{start}(i) &= w^T \\\\text{start} p_i \\\\\\\\\\n\\\\text{end}(j) &= w^T \\\\text{end} p_j \\\\\\\\\\n\\\\text{mention}(t) &= w^T \\\\text{mention} p_t\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\(w_{\\\\text{start}}, w_{\\\\text{end}}, w_{\\\\text{mention}}\\\\) are learnable vectors and \\\\(p_i\\\\) paragraph token representations based on BERT:\\n\\n\\\\[\\n[p_1 \\\\ldots p_n] = \\\\text{BERT}(\\\\left[p_1 \\\\ldots p_n\\\\right]_{\\\\text{CLS}}) \\\\ldots \\\\text{CLS} \\\\ldots \\\\text{SEP} \\\\ldots d(e) \\\\ldots \\\\text{SEP}\\n\\\\]\\n\\nOverall mention probabilities are computed as:\\n\\n\\\\[\\np([i,j]) = \\\\sigma (\\\\text{start}(i) + \\\\text{end}(j) + \\\\sum_{t=i}^{j} \\\\text{mention}(t))\\n\\\\]\\n\\nTop candidates are selected as mention candidates and propagate to the next step.\\n\\nA.2 Entity Disambiguation\\n\\nThe ED head receives mention spans in the text and finds the best matching entity in the KB. Following Wu et al. (2020), ED is based on dense retrieval. Description-based entity representations are computed as follows:\\n\\n\\\\[\\ne = \\\\text{BERT}([\\\\text{CLS}] t(e) [\\\\text{SEP}] d(e) [\\\\text{SEP}])\\n\\\\]\\n\\nFollowing Li et al. (2020), mention representations are constructed with one pass of the encoder and without mention boundary tokens by pooling mention tokens through a single feed-forward layer (FFL):\\n\\n\\\\[\\nm_{i,j} = \\\\text{FFL}(p_i \\\\ldots p_j)\\n\\\\]\\n\\nSimilarity score \\\\(s\\\\) between the mention candidate and an entity candidate \\\\(e \\\\in E\\\\) are computed:\\n\\n\\\\[\\ns(e, [i,j]) = e^* m_{i,j}\\n\\\\]\\n\\nA likelihood distribution over all entities, conditioned on the mention \\\\([i,j]\\\\) is computed:\\n\\n\\\\[\\np(e | [i,j]) = \\\\frac{\\\\exp(s(e, [i,j]))}{\\\\sum_{e' \\\\in E} \\\\exp(s(e', [i,j]))}\\n\\\\]\\n\\n\\\\(e^*\\\\), such that \\\\(e^* = \\\\arg\\\\max_{e} (p([i,j], e))\\\\), are passed as a candidate <mention span, entity> tuple to the rejection head.\\n\\nA.3 Rejection head\\n\\nMD and ED steps over-generate. R looks at an \\\\((e^*, [i,j])\\\\) pair holistically decides whether to accept it. Input features to R are the MD score \\\\(p([i,j])\\\\), the ED score \\\\(p(e^* | [i,j])\\\\), the mention representation \\\\(y_{i,j}\\\\), top-ranked candidate representation \\\\(x_{e^*}\\\\) as well as their difference and Hadamard product. The concatenation of these features is fed through a feed-forward network to output the final entity linking score \\\\(p([i,j], e^*)\\\\). All \\\\(p([i,j], e^*) > \\\\gamma\\\\) are accepted where \\\\(\\\\gamma\\\\) is a threshold set to 0.4.\\n\\nA.4 Training\\n\\nFollowing prior work (Sun et al., 2015b; Cao et al., 2018; Gillick et al., 2019; Onoe and Durrett, 2020), training is split into two stages. First, ED only is trained on a Wikipedia dataset. This dataset is constructed by extracting Wikipedia hyperlinks to labeled mention-entity pairs and consists of 17M training samples. Then, ED, MD and R are trained jointly on the downstream dataset (either Oscar or Wikipedia). Outputs from one component are fed as input to the next and losses are summed together.\\n\\nTo train the ED head, frozen entity representations are used. As entity embeddings do not change during training, entity embeddings can be indexed using quantization algorithms for a fast kNN search (using FAISS (Johnson et al., 2017) framework with HNSW index). A likelihood distribution over positive and mined hard negative entities for each...\"}"}
{"id": "emnlp-2022-main-593", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Hyper-parameters adaptation phase\\n\\nUsing OSCAR adapt-dev, we optimize mention score threshold $s_m$, greedy NN distance threshold $d_m$ and mention entity similarity threshold $d_e$.\\n\\nWe optimize $s_m$ in range 0.0 to 1.0 in steps of 0.1 for EU discovery recall. We optimize $d_m$ in range 0.5 to 1.0, in steps of 0.0001 for NMI. We optimize $d_e$ for EU discovery recall in range 50 to 250 in steps of 10. For results, see Table 5.\\n\\nWe report recall of 81% and precision of 6% for clusters referring to unknown entities. Recall of clusters referring to known entities is 88% with precision 96%. Clustering NMI is 0.92.\\n\\nWe show that by re-training L after indexing, L learns to circumvent EU: We identify known entities part of the training set that are in close proximity of unknown entities (confusable known entities).\\n\\nWe compare the average similarity between mentions and their respective linked entity when adding unknown entities before training vs. after training. Mean similarity when adding unknown entities before training is 93.28 for confusable known entities and 92.57 for other known entities. A t-test shows that this difference is significant (p-value of 0.0001 with $< 0.05$). As a reference, mean similarity when adding unknown entities post training is 92.65 irrespective of whether they are confusable or not.\\n\\nBesides end-to-end performance, we also report entity disambiguation performance with oracle mention detection in Table 6.\\n\\nWe report performance on Wikipedia $d_t^2$-test in Table 7. Due to a smaller $d_m$, end-to-end performance is lower. When filtering Wikipedia $d_t^2$-test for mentions of discovered entities, $L_t^2$-Cluster Oracle precision is 40.5 and $L_t^2$-Cluster recall is 15.3.\\n\\nWe ran all training distributed across 8 NVIDIA TESLA V100 GPUs, each with 32 GB of memory. The first training stage took 48h, the second one 12h.\\n\\nAdaptation phase is currently limited by expensive greedy NN clustering with quadratic time complexity but the type of clustering is interchangeable for more efficient ones. We chose this type of clustering as Logan IV et al. (2021) showed it performs decently for BLINK based mention encodings.\"}"}
{"id": "emnlp-2022-main-593", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"|          | Unknown Entities | Known Entities |          | Unknown Entities | Known Entities |\\n|----------|-----------------|----------------|----------|-----------------|----------------|\\n| Model    | R               | P              | NMI      | R               | P              | NMI      |\\n| Dt1      | 0.0             | 0.0            | 0.0      | 92.2            | 92.2           | 96.0     |\\n| Dt2      | 63.5            | 45.3           | 96.8     | 90.0            | 90.2           | 96.0     |\\n| Lt1      | -Descp          | 58.0           | 96.3     | 92.1            | 92.3           | 96.1     |\\n|          | -Mention        | 26.2           | 92.7     | 92.2            | 92.2           | 96.0     |\\n| EDIN     | (Lt1-Cluster)   | 27.9           | 93.4     | 92.2            | 92.2           | 96.2     |\\n\\nTable 6: Entity Disambiguation performance on OSCAR.\\n\\n|          | Unknown Entities | Known Entities |          | Unknown Entities | Known Entities |\\n|----------|-----------------|----------------|----------|-----------------|----------------|\\n| Model    | R               | P              | NMI      | R               | P              | NMI      |\\n| Dt1      | 0.0             | 0.0            | 0.0      | 70.5            | 75.8           | 95.4     |\\n| Dt2      | 33.6            | 25.0           | 98.3     | 70.6            | 75.4           | 95.3     |\\n| Lt1      | -Descp          | 33.9           | 98.0     | 71.2            | 74.4           | 95.3     |\\n|          | -Cluster-Oracle | 7.8            | 90.6     | 70.1            | 75.9           | 95.6     |\\n| EDIN     | (Lt1-Cluster)   | 1.8            | 93.4     | 71.1            | 74.1           | 95.3     |\\n\\nTable 7: End-to-end EL performance on Wikipedia.\\n\\nFigure 2: Dataset splits:\\nA schema illustrating the composition of Dt1 and Dt2. Note, that contrary to what this plot suggests, the number of samples per data split is equal for Dt1 and Dt2.\"}"}
