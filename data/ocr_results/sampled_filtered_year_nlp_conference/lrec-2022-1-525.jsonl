{"id": "lrec-2022-1-525", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SCAI-QReCC Shared Task on Conversational Question Answering\\n\\nSvitlana Vakulenko, Johannes Kiesel, Maik Fr\u00f6be\\n\\nAmazon, Spain, svvakul@amazon.com\\n\\nBauhaus-Universit\u00e4t Weimar, Germany, johannes.kiesel@uni-weimar.de\\n\\nMartin-Luther-Universit\u00e4t Halle-Wittenberg, Germany, maik.froebe@informatik.uni-halle.de\\n\\nAbstract\\n\\nSearch-Oriented Conversational AI (SCAI) is an established venue that regularly puts a spotlight upon the recent work advancing the field of conversational search. SCAI\u201921 was organised as an independent online event and featured a shared task on conversational question answering, on which this paper reports. The shared task featured three subtasks that correspond to three steps in conversational question answering: question rewriting, passage retrieval, and answer generation. This report discusses each subtask, but emphasizes the answer generation subtask as it attracted the most attention from the participants and we identified evaluation of answer correctness in the conversational settings as a major challenge and an open research gap. Alongside the automatic evaluation, we conducted two crowdsourcing experiments to collect annotations for answer plausibility and faithfulness. As a result of this shared task, the original conversational QA dataset used for evaluation was further extended with alternative correct answers produced by the participant systems.\\n\\nKeywords: Conversational Systems, Question Answering\\n\\n1. Introduction\\n\\nConversational Question Answering (QA) is a challenging task at the current research frontier (Qiu et al., 2021; Kim et al., 2021) important for developing conversational information retrieval (conversational search) systems (Anand et al., 2019). In conversational QA, a system is required to return a correct answer given a question and the previous conversation turns. Such questions are often ambiguous outside of the conversational context and require incorporating additional information contained in the previous conversation turns. Moreover, evaluating systems for conversational QA, especially for questions that require long generative answers drawn from multiple information sources, remains an open research problem in its own right (Voorhees, 2003; Krishna et al., 2021; Siblini et al., 2021; Li et al., 2021).\\n\\nThis paper provides an overview of the SCAI-QReCC 2021 shared task on conversational question answering. It reports on the extended conversational QA dataset employed in this task, participating systems and insights gained from their performance, and the challenges we faced when evaluating the submissions and our approach for dealing with those. Specifically, we designed and tested a set of guidelines to support evaluation of (conversational) QA models.\\n\\nThe shared task was built around the recently introduced QReCC dataset (Anantha et al., 2021). QReCC contains sequences of conversational questions paired with answers produced by human annotators. Such sequences imitate a dialogue session with follow-up questions asked by the user. The annotators had access to a web search engine and every answer is based on the content of a single web page, while several web pages may be used to answer different questions within the same sequence. These web pages were downloaded and chunked into passages. The task of the system is to retrieve information from this passage collection and produce the correct answer based on this information. Since QReCC contains only one answer provided by the human annotators, our goal was not only to evaluate the current state-of-the-art but also to collect alternative correct answers for this dataset. While the ground truth provides a single correct answer per question, in practice more than one answer can be considered correct.\\n\\nWe received 29 runs, including the submissions made by four participating teams and the results produced by our three baseline models. Each of these runs contains answers to all questions of the QReCC test set. To assess these runs, we employed a range of automated metrics and arranged two crowdsourcing tasks.\\n\\nHuman assessment remains crucial for QA evaluation since automated measures may only assess similarity to the ground truth but the answer may be correct even when it differs from the ground truth. Our goal was to find such answers and add them to the QReCC dataset. Since it is unfeasible to manually evaluate all 17K answers we collected, we came up with a set of techniques to elicit a smaller subset that was more likely to contain alternative correct answers.\\n\\nAnother major challenge we faced was in judging answer correctness. All teams participating in the shared task used generative models to produce the answers by conditioning on several previously retrieved passages. However, it was previously shown that such models tend to deviate from the information provided in the passages (Krishna et al., 2021). While these generated...\"}"}
{"id": "lrec-2022-1-525", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conversational Question: \u201cDo NPs or PAs make more?\u201d\\nRewritten Question: \u201cDo nurse practitioners or physician assistants make more?\u201d\\nHuman Answer: \u201cAccording to the Bureau of Labor and Statistics (BLS), the 2017 median pay for Nurse Practitioners is $110,930 per year, or $53.33 per hour. In comparison, 2017 median pay for Physician Assistants is $104,860 per year, or $50.41 per hour.\u201d\\n\\nGenerated Answer: \u201cThe average salary for a nurse practitioner is $65,000. The average salary for a physician\u2019s assistant is $30,000.\u201d\\n\\nExtracted Answer: \u201cThe BLS reports that the median annual wage for nurse practitioners was $109,820 as of May 2019, while the median annual wage for physician assistants reached $112,260 during the same month.\u201d\\n\\n2. Task Setup\\nThis section describes the setup of the 2021 edition of the SCAI-QReCC2021 shared task in more detail, including the three subtasks relevant for conversational QA, our baselines, and participating systems.\\n\\n2.1. Subtasks\\nFollowing the same setup as in QReCC, we decomposed the end-to-end conversational QA task into three subtasks that can be implemented and evaluated separately: (1) question rewriting \u2013 ability of the model to correctly interpret and reformulate the question into its unambiguous equivalent; (2) passage retrieval \u2013 ability of the model to locate information relevant for answering the question; and (3) answer generation \u2013 ability of the model to produce faithful and grammatically correct answers.\\n\\nWhile the conversational QA task can be approached end-to-end using a single model, such decomposition is beneficial for several reasons: (1) it allows to reuse the same passage retrieval and answer generation components already tuned for non-conversational QA (Vakulenko et al., 2021); (2) it allows to efficiently scale retrieval to large document collections using sparse representation approaches (Luan et al., 2021); (3) it enables a more thorough evaluation providing insights into the bottlenecks and opportunities to improve the end-to-end performance (Vakulenko et al., 2020).\\n\\n2.2. Baselines\\nWe introduced three baseline models to serve as reference points:\\n\\n- **Basic**: This baseline implements a naive approach for question answering: it submits the question as the answer. Though not intuitive at first glance, this baseline is surprisingly hard to beat as most QA metrics are based on token overlap between submitted and ground truth answer, and the ground truth answers naturally often share tokens with the respective questions.\\n\\n- **Simple**: This baseline implements low-effort approaches for each subtask. For question rewriting, it submits the question without modification. For passage retrieval, it employs Pyserini BM25 with $k_1 = 0.82$ and $b = 0.68$ as in the QReCC paper (Anantha et al., 2021). For question answering, it submits the sentence from the retrieved passages that contains the most of the stemmed noun phrases of the question. The baseline\u2019s source code is available on Github.\\n\\n- **GPT3**: This baseline uses the OpenAI GPT3 API (Brown et al., 2020) with default parameters (see Appendix) for question answering. The answers for 16,451 conversational questions of the test set were generated in 90 minutes for 33 USD. As the model prompt, all preceding questions and answers were prefixed with \u201cQ:\u201d and \u201cA:\u201d respectively and then concatenated.\\n\\n1 https://github.com/castorini/pyserini\\n2 https://github.com/scai-conf/SCAI-QReCC-21/tree/main/code/simple-baseline\"}"}
{"id": "lrec-2022-1-525", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3. Participating Systems\\n\\nWe encouraged participants to submit working software through the TIRA (Potthast et al., 2019) platform but also allowed for traditional run file submissions. Each participant received access to a dedicated virtual machine with full admin rights and access to the QReCC dataset and a pre-build Anserini (Yang et al., 2017) index to deploy their software submissions and improve upon the available Pyserini baseline. We deployed the Pyserini baseline as software submission in TIRA and made the code available to simplify adaptations and encourage reproducibility because software submissions in TIRA can be executed on new datasets in the future without adoption. Still, all participants submitted run files and no working software because the deadline of the SCAI-QReCC 2021 shared task was close to the deadline of the TREC 2021 CAsT track. Overall, four teams submitted results to the shared task:\\n\\n1. Rachael (Gon\u00e7alo Raposo and Coheur, 2022) implemented a three-stage pipeline that rewrites the question with T5 (Raffel et al., 2020) and summarizes the top-10 passages retrieved with BM25 for the rewritten question using PEGASUS (Zhang et al., 2020a) to generate the answer. The T5 model for query rewriting is pre-trained on the CANARD dataset (Elgohary et al., 2019) and uses the questions and answers of previous turns and the current question to make the current question context-independent. The rewritten query is used to retrieve the top-10 passages with BM25 implemented in Pyserini. Finally, the rewritten query and the top-10 passages are concatenated and fed to the abstractive summarizer PEGASUS fine-tuned on the official QReCC training set. The resulting summary is returned as the answer.\\n\\n2. Rali-QA used the human rewritten questions to retrieve passages with a pipeline identical to our simple baseline (BM25 with k_1 = 0.82 and b = 0.68) and a fine-tuned extractive BERT model for question answering on the top-k passages. The BERT model was fine-tuned for span extraction during a single epoch on the official QReCC training dataset, starting with the weights from a BERT model pre-trained on SQuAD v1.\\n\\n3. Torch uses a three-stage pipeline that rewrites the question with GPT2 (Radford et al., 2019), retrieves passages with BM25 and a BERT-based re-ranker, and generates the answer from the top-scored passage using T5 (Raffel et al., 2020). The question rewriting follows the idea of Yu et al. (Yu et al., 2020) and uses GPT2 fine-tuned on the official QReCC training set with the questions and answers of previous turns and the current question to rewrite the current question. The passage retrieval re-ranks the top-1000 results for the rewritten query of BM25 implemented in Anserini (Yang et al., 2017) (k_1 = 0.9 and b = 0.4) using the OpenMatch (Liu et al., 2021b) BERT re-ranker pre-trained on MS-MARCO. Initially, the answer generation was intended in two stages using two T5 models. The first stage was intended to use a T5 model to generate a dedicated answer for each of the top-10 passages from the BERT re-ranking by concatenating the passage to the rewritten question. The second stage was intended to use another T5 model that uses the ten generated answers as input to generate the final answer as output. Due to the length of the passages, the fine-tuning of the second T5 model failed. Hence, the final answer was generated using the top-passage concatenated to the rewritten question (leaving the second stage of answer generation for future work).\\n\\n4. Ultron uses a two-stage pipeline rewriting the question with the sequence-to-sequence model BART (Lewis et al., 2020a) and generating the answers using RAG (Lewis et al., 2020b). The BART model uses the current question and the queries of the previous turns as input to rewrite the current question. The BART model was fine-tuned for question rewriting using the official QReCC training set. Due to the large size of the collection, team ultron tested three different indexes for answer generation with RAG: (1) the Wikipedia index, (2) a filtered version of the QReCC passages using the top-10 passages for the questions of all turns of the conversation retrieved with BM25, and (3) a filtered version of the QReCC passages using the top-100 passages for the questions of all turns of the conversation retrieved with BM25.\"}"}
{"id": "lrec-2022-1-525", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is only performed and evaluated on the original dataset as the questions in the rewritten dataset are already the ground-truth rewrites.\\n\\n- **QR.** We report, ROUGE-1 (Lin, 2004) here, which is the unigram recall (on token level) between the automatically rewritten questions and the ground-truth one as recommended based on the experimental evaluation performed by (Anantha et al., 2021) (Section 6: Question Rewriting Metrics Validation).\\n\\n**Passage Retrieval**\\n\\nFrom the dataset paper we adopt the use of the mean reciprocal rank to evaluate the ranking of passages retrieved for each question (Anantha et al., 2021). Although the use of MRR faces some criticism (Fuhr, 2017; Zobel and Rashidi, 2020), it is still used for evaluations in the TREC 2021 Conversational Assistance and Deep Learning tracks. We employ it here for comparability with previous work.\\n\\n- **MRR.** This metric is equal to \\\\( \\\\frac{1}{r} \\\\), where \\\\( r \\\\) is the rank of the highest-ranked relevant passage. We employ the token overlap heuristic of Anantha et al. (2021) to determine relevance. Using token overlap is similar in spirit to several of the question answering metrics described below.\\n\\n**Question Answering**\\n\\nWe experiment with eight different metrics for comparing the generated answers with the ground-truth answers:\\n\\n- **EM.** The \u201cExact Match\u201d is 1 if the answer is identical to the ground-truth after lowercasing, stemming, punctuation, and stopword removal. An Exact Match of 0 corresponds to disjoint pairs of text.\\n\\n- **F1.** The F-Measure is the harmonic mean of precision and recall. These correspond here to the fraction of shared tokens between predicted and ground-truth answer among the tokens in the predicted or in the ground-truth answer, respectively.\\n\\n- **R1.** ROUGE-1: the same as QR for question rewriting.\\n\\n- **POSS.** The POSSCORE (Liu et al., 2021a) computes the cosine similarity of averaged word embeddings between predicted and ground-truth answers, but does so separately and weighted for tokens with specific part-of-speech tags and tokens with other tags. We use the default tag set: ADJ, ADV, VERB, PROPN, NOUN.\\n\\n- **SAS.** Semantic Answer Similarity (Risch et al., 2021) uses a cross-encoder neural network, where both predicted and ground-truth answer are first concatenated with a separator token in between and then fed into a language model for similarity prediction.\\n\\n- **BERT.** BERTScore (Zhang et al., 2020b) computes the token-wise F-Measure (see above) between predicted and ground-truth answers, but matches tokens based on the highest cosine similarity of the respective contextual BERT embeddings and uses this similarity instead of a binary exact match. Moreover, matches are weighed by the inverse document frequency of the matched token.\\n\\n- **BKPQA / RKPQA.** BERTScore-KPQA and ROUGE-L-KPQA (Lee et al., 2021) are modified version of BERTScore (see there) and ROUGE-L (Lin, 2004) that weight each token by a predicted importance of the answer token with respect to the question. This importance score is predicted using a fully connected neural network that is trained on extractive question answering datasets.\\n\\n### 3.2. Results\\n\\nThe upper part of Table 1 compares our baselines (Section 2.2) and the participating systems (Section 2.3) on the test split of the QReCC dataset. The bottom part shows the scores when directly using the human rewritten questions as input instead of the original ones.\\n\\n**Question Rewriting**\\n\\nThis subtask is evaluated on the original dataset only. The QR column shows the ROUGE-1 that the approaches reached. The simple baseline returns the question as-is and is outperformed by every run of the two teams that implemented their own rewriting approaches. Both teams achieved similar scores, with team Rachael having a slight advantage indicating that T5 might be a good starting point for further improving question rewriting.\\n\\n**Passage Retrieval**\\n\\nThe teams Rachael, Rali-QA, and Torch submitted results for the passage retrieval subtask, which we compared in terms of the mean reciprocal rank (MRR). Nearly all submitted runs improve upon the simple baseline, which uses the BM25-retrieval model with a standard parameter set. The best run, by team Rachael, reaches even a more than twice as high score. However, the run with the highest QR does not reach the highest MRR. Indeed, the Pearson correlation of a runs QR and MRR for team Rachael is -0.35, indicating a weak negative correlation. The scores for QR and MRR are similar across team Rachael\u2019s runs, though. In general, a considerably higher MRR is reached when using human rewritten questions, showing the necessity of the rewriting step.\\n\\nFigure 2 visualizes this performance increase. Surprisingly, the simple baseline performed best in this case. A possible explanation is that the not-rewritten questions it uses provide a much worse starting point for passage retrieval than the automatic rewritten questions of the participating teams.\\n\\n**Question Answering**\\n\\nAll four teams participated in the question answering subtask. For this task, the baselines are consistently beaten by at least one participant.\"}"}
{"id": "lrec-2022-1-525", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Evaluation results on the original dataset (top) and when using the human rewritten questions as input. Metrics are described in Section 3.1. A \\\"-\\\" denotes that no output was submitted for the respective task or the evaluation code failed (for BERT, BKPQA, and RKPQA).\\n\\nFigure 2: Highest scores reached per dataset variant for each metric. For all metrics except POSS a score of 1 corresponds to optimal performance. However, we find that the rankings of the different metrics often disagree, leaving otherwise an inconclusive image. As an extreme case, the runs by team Rachael that achieved the highest POSS-CORE (for original and human rewritten questions) actually got the lowest score of their runs for most other metrics. The only exceptions are the BERTScore and BERTScore-KPQA metric when using original questions. Despite these differences, however, the metrics generally agree on ranking the runs of team Rachael high up, indicating the potential of their approach. Moreover, as Figure 2 shows, using human rewritten questions instead of the original ones does also improve question answering performance\u2014for all metrics\u2014, though to a lesser extent than for passage retrieval (MRR). To enrich these automated results and provide more insight, we also employed a human evaluation procedure outlined in the next section.\\n\\n4. Human Evaluation\\nAn overview of our human evaluation of the QA performance is given in Figure 3 and consists of two phases. We start with the set of candidate answers $A_C$ for question $q$ that was obtained from the runs submitted by the participants and our baseline approaches. Then, we use the SAS score (Risch et al., 2021) that shows similarity to the ground truth answer $a$ from QReCC to filter...\"}"}
{"id": "lrec-2022-1-525", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Our human evaluation including assessment of the answer plausibility and answer faithfulness.\\n\\nOut the candidate answers that are more likely to be correct. The subset with the highest SAS scores is then used to crowdsource answer plausibility labels (see Section 5 for more details). The result of this phase is the subset of all plausible answers identified for question \\\\( q \\\\).\\n\\nThe next evaluation phase is designed to assess faithfulness of the plausible answers identified at the previous phase. For each plausible answer in \\\\( A_P \\\\), we collect a set of evidence spans \\\\( E_C \\\\) by matching each answer to the top-\\\\( k \\\\) passages submitted by the participants using a sliding window and a token-overlap heuristic. We keep the token-overlap threshold low to find semantic matches and then apply the SAS score to filter out the candidate spans. The resulting sets of spans extracted from the passages \\\\( E_C' \\\\) paired with the corresponding plausible answers \\\\( A_P' \\\\) is then used to crowdsource faithfulness labels (see Section 6 for more details).\\n\\n5. Answer Plausibility\\n\\nIn this section, we describe our answer sampling approach and annotation instructions. We conclude with summarising our results.\\n\\n5.1. Answer sampling\\n\\nWe obtained 16,736 answers, in total, from all the submitted runs including our baselines and participant systems. Our goal was to find alternative correct answers or answer variations that could help us to extend the ground-truth answers in QReCC. We also decided to focus on the questions on which the participating systems tend to disagree. To this end, we sampled at least four alternative answers per question submitted by the systems that are all distinct but at the same time semantically close to the ground truth answer using the SAS score (described in Section 3.1) with the threshold above 0.7. While the SAS score allows to find answer paraphrases beyond the lexical overlap matches, it also enables us to capture alternative answers that may contradict the ground truth answers (see the Generated Answer in Figure 1).\\n\\n5.2. Annotation task\\n\\nWe asked the crowd workers using Amazon Mechanical Turk (MTurk) to judge the answers with respect to the questions (see Figure 4). This annotation task was estimated to take about 30 seconds for a single QA pair, on average. The workers were reimbursed with $0.06 per sample they annotated.\\n\\nFigure 4: Task setup for answer plausibility annotation.\\n\\nWe started the crowdsourcing experiment with several smaller batches at first and performed the quality assurance of the results to select a pool of the trusted crowd workers who then completed the rest of the annotations. One of the authors did the quality assurance by manually examining the samples with disagreements. We collected two annotations per sample. The disagreements were manually resolved for the first batches. The rest of samples on which our pool of trust crowd workers disagreed were automatically discarded.\\n\\n5.3. Results\\n\\nAfter processing the annotations, we collected 1,863 answers judged as plausible, 108 malformed answers, and 107 implausible answers (see Table 2 for per run distribution). Thereby, we obtained 465 clusters with more than one plausible answers for the same question (avg: 4, max: 13 answers per cluster). In some cases those answers were paraphrases, shortened or more detailed versions of the same answer. In other cases, the answers are different, which may or may not be contradictory, such as pointing at different aspects in a definition (see the example provided in Figure 5).\\n\\n6. Answer Faithfulness\\n\\nThis section describes our answer grounding approach that produces short evidence spans enabling the crowd-workers to efficiently judge answer faithfulness.\\n\\n6.1. Evidence sampling\\n\\nEvaluating answer correctness with respect to the passage collection (faithfulness) is a hard task because the passages are very long for crowd workers to go through. Our approach to sampling evidence spans is based on the observation that the human answers often paraphrase the original paragraph text in QReCC but,\"}"}
{"id": "lrec-2022-1-525", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q: \\\"What is a physician's assistant?\\\"\\nA1: \\\"A physician's assistant (PA) is a medical assistant who works under the supervision of a physician and is licensed to practice medicine in the state in which the patient resides.\\\"\\nA2: \\\"A physician assistant is a person who has successfully completed an accredited education program for physician assistant, is licensed by the state and is practicing within the scope of that license.\\\"\\nA3: \\\"A physician's assistant is a person who assists a physician in the performance of his or her duties.\\\"\\nA4: \\\"A physician assistant is a medical professional who assists a doctor in the diagnosis and treatment of a patient.\\\"\\n\\nFigure 5: A sample cluster of plausible answers for the same question from the SCAI-QReCC results.\\n\\n---\\n\\nFirstly, we use the same span heuristic proposed in (Anantha et al., 2021) to select short text spans within the passages with the maximum token overlap. We lower the token overlap threshold considerably to allow for non-verbal semantic matches as well. The evidence spans were trimmed to contain full sentences for readability. Secondly, we use the SAS score to compare between the matched sentences and all the generated answers to select a pool of evidence for each question. Similarly, we also sample additional evidence using the ground-truth answers from QReCC produced by human annotators.\\n\\nAfterwards, we just merge all the evidence spans detected this way into a single paragraph. In this manner, we still attempt to evaluate the answers for cases when the collected spans may already contain sufficient evidence to judge the answer faithfulness even though they did not match any of the evidence spans directly.\\n\\n6.2. Annotation task\\nWe annotated two batches with 578 samples. Each sample contains a triple of a question, one of the plausible answers and the text, which is a concatenation of all the evidence spans obtained for this question (see Figure 6 for an example of one such sample). The workers were reimbursed $0.12 per sample since they had to read through the answers and the matched evidence spans as well. We followed the setup similar to the previous annotation task with two workers annotating the same sample. We also used the same pool of MTurk crowd workers that was selected during the previous annotation task. The annotation task distinguishes texts which are irrelevant or relevant to the question, and for relevant texts whether the answer can be deduced from the text (\u201ccontains information sufficient to judge the answer as correct\u201d). Only triples with a relevant text and deducable answer are seen as faithful.\\n\\nFigure 6: Task setup for answer grounding annotation with evidence sentences sampled from several retrieved passages by matching to the generated and ground-truth answers.\\n\\n6.3. Results\\nAfter removing the samples on which the workers disagreed, we obtained 386 answers that were judged as faithful given the matched evidence spans (i.e., the first option in Figure 6). Our results are summarised in Table 2. Most of the answers were judged as faithful given the evidence spans we extracted, which shows the effectiveness of our evidence extraction approach and fidelity of the evaluated models. The table also demonstrates that some of the models that produced many plausible answers, such as GPT3, have a lower proportion of answers judged as faithful than other models. Note that this may also indicate that the answers generated by these models are very different from the retrieved passages. In this case, our approach is not sufficient to detect relevant evidence spans. Therefore, we believe that a reasonable requirement for generative QA models in the future shared tasks should be to provide the short evidence spans alongside with the passage IDs that can be used for answer evaluation.\\n\\n7. Conclusion\\nResults of the SCAI-QReCC shared task identified main achievements as well as the major challenges when applying the state-of-the-art models to the task of open-domain QA. All of the submitted runs used a sparse index with BM25 for initial retrieval in combination with question rewriting for conversational QA. Due to the large collection size, none of the participant teams managed to scale dense passage retrieval that could allow to deploy an end-to-end conversational QA model. These results provide an important indicator of the technology maturity level for large-scale QA and conversational QA beyond Wikipedia-sized corpora.\\n\\nOverall, we proposed and tested in practice an evaluation procedure that allowed us to compare the model\"}"}
{"id": "lrec-2022-1-525", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Run                  | Original Score | Faithful | Plausible | Implausible | Malformed |\\n|---------------------|----------------|----------|-----------|-------------|-----------|\\n| GPT3 baseline       | 149            | 4        | 8         | 28          | 3         |\\n| Ultron rag-bm25      | 173            | 15       | 6         | 27          | 2         |\\n| Rachael 2021-09-04-10-39-42 | 183              | 5        | 4         |             |           |\\n| Rachael 2021-09-08-21-49-44 | 120              | 4        | 5         | 30          | 1         |\\n| Rachael 2021-09-08-07-07-57 | 120              |        |           |             |           |\\n| Rachael 2021-09-15-09-07-49 | 103             | 4        | 6         | 29          | 1         |\\n| Rachael 2021-09-06-09-21-43 | 158            | 4        | 3         | 26          |           |\\n| Ultron 2021-09-08-15-04-28 | 149             | 16       |           |             |           |\\n| Rachael 2021-09-15-19-36-31 | 132             | 2        | 2         | 24          |           |\\n| Rachael 2021-09-15-09-06-44 | 73              | 0        | 4         | 22          | 1         |\\n| Rachael 2021-09-08-07-09-57 | 75             | 2        | 4         | 16          | 1         |\\n| Rali-qa 2021-09-09-13-01-07 | 33             | 6        | 11        | 16          | 1         |\\n| Rachael 2021-09-08-15-40-34 | 41             | 6        | 2         | 14          | 3         |\\n| Torch usi T5 raw2   | 36             | 7        |           |             |           |\\n\\nTable 2: Human evaluation results of answer plausibility and faithfulness. The runs are ordered by the number of faithful answers. The highest values in each of the columns are highlighted in bold.\\n\\nPerformance and discover new plausible and faithful answers. We used it to extend the original conversational QA dataset used for evaluation with multiple correct answers per question. While it is impossible to elicit a complete list of all correct answers for a given question, especially for an open-ended non-factoid question, this dataset is designed to improve our understanding of answer variations and specific properties important for a good answer. Our dataset is made available under the public URL: https://doi.org/10.5281/zenodo.5749472\\n\\nOur evaluation results showed that the modern QA models are already able to produce fluent answers but we cannot always trust those answers to be correct. Ordinary users are unaware of such models' limitations and can be easily persuaded or misguided by plausible but unfaithful answers. There's a need to establish a way to produce high quality answers grounded in the external information sources that can be referenced by the QA model. While we showed an efficient approach to mine plausible and faithful answers, it is not possible to evaluate faithfulness if the generated answers are very different from the original text. To make further progress on the generative QA task, the models should be required to provide evidence alongside with their answers explicitly indicating the answer provenance. Such evidence should be also limited to relatively short spans of bounded length, such that they can be easily examined and assessed by human evaluators.\\n\\n8. Bibliographical References\\n\\nAnand, A., Cavedon, L., Joho, H., Sanderson, M., and Stein, B. (2019). Conversational search (Dagstuhl seminar 19461). Dagstuhl Reports, 9(11):34\u201383.\\n\\nAnantha, R., Vakulenko, S., Tu, Z., Longpre, S., Pulman, S., and Chappidi, S. (2021). Open-domain question answering goes conversational via question rewriting. In Kristina Toutanova, et al., editors, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT\u201921), pages 520\u2013534. Association for Computational Linguistics.\\n\\nBrown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. (2020). Language models are few-shot learners. In Hugo Larochelle, et al., editors, 33rd Annual Conference on Neural Information Processing Systems (NeurIPS\u201920).\\n\\nElgohary, A., Peskov, D., and Boyd-Graber, J. L. (2019). Can you unpack that? learning to rewrite questions-in-context. In Kentaro Inui, et al., editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 5917\u20135927.\"}"}
{"id": "lrec-2022-1-525", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fuhr, N. (2017). Some common mistakes in IR evaluation, and how they can be avoided. SIGIR Forum, 51(3):32\u201341.\\n\\nGon\u00e7alo Raposo, Rui Ribeiro, B. M. and Coheur, L. (2022). Question rewriting? Assessing its importance for conversational question answering. In Advances in Information Retrieval. 44th European Conference on IR Research (ECIR 2022), Lecture Notes in Computer Science, Berlin Heidelberg New York, March. Springer.\\n\\nKim, G., Kim, H., Park, J., and Kang, J. (2021). Learn to resolve conversational dependency: A consistency training framework for conversational question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6130\u20136141. Association for Computational Linguistics.\\n\\nKrishna, K., Roy, A., and Iyyer, M. (2021). Hurdles to progress in long-form question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2021, Online, June 6-11, 2021, pages 4940\u20134957. Association for Computational Linguistics.\\n\\nLee, H., Yoon, S., Dernoncourt, F., Kim, D. S., Bui, T., Shin, J., and Jung, K. (2021). KPQA: A metric for generative question answering using keyphrase weights. In Kristina Toutanova, et al., editors, Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT'21), pages 2105\u20132115. Association for Computational Linguistics.\\n\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020a). BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 7871\u20137880. Association for Computational Linguistics.\\n\\nLewis, P. S. H., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., K\u00fctter, H., Lewis, M., Yih, W., Rockt\u00e4schel, T., Riedel, S., and Kiela, D. (2020b). Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nLi, H., Gao, T., Goenka, M., and Chen, D. (2021). Ditch the gold standard: Re-evaluating conversational question answering. arXiv preprint arXiv:2112.08812.\\n\\nLin, C.-Y. (2004). ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain, July. Association for Computational Linguistics.\\n\\nLiu, Z., Zhou, K., Mao, J., and Wilson, M. L. (2021a). POSSCORE: A simple yet effective evaluation of conversational search with part of speech labelling. In Gianluca Demartini, et al., editors, 30th ACM International Conference on Information and Knowledge Management (CIKM'21), pages 1119\u20131129. ACM.\\n\\nLiu, Z., Zhang, K., Xiong, C., Liu, Z., and Sun, M. (2021b). Openmatch: An open source library for neu-ir research. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 2531\u20132535. ACM.\\n\\nLuan, Y., Eisenstein, J., Toutanova, K., and Collins, M. (2021). Sparse, dense, and attentional representations for text retrieval. Trans. Assoc. Comput. Linguistics, 9:329\u2013345.\\n\\nPotthast, M., Gollub, T., Wiegmann, M., and Stein, B. (2019). TIRA integrated research architecture. In Nicola Ferro et al., editors, Information Retrieval Evaluation in a Changing World - Lessons Learned from 20 Years of CLEF, volume 41 of The Information Retrieval Series, pages 123\u2013160. Springer.\\n\\nQiu, M., Huang, X., Chen, C., Ji, F., Qu, C., Wei, W., Huang, J., and Zhang, Y. (2021). Reinforced history backtracking for conversational question answering. In Thirty-Fifth AAAI Conference on Artificial Intelligence, AAAI 2021, Thirty-Third Conference on Innovative Applications of Artificial Intelligence, IAAI 2021, The Eleventh Symposium on Educational Advances in Artificial Intelligence, EAAI 2021, Virtual Event, February 2-9, 2021, pages 13718\u201313726. AAAI Press.\\n\\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. (2019). Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nRaffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. J. (2020). Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\\n\\nRisch, J., M\u00f6ller, T., Gutsch, J., and Pietsch, M. (2021). Semantic answer similarity for evaluating question answering models. In 3rd Workshop on Machine Reading for Question Answering, pages 149\u2013157.\\n\\nSiblini, W., Sayil, B., and Kessaci, Y. (2021). Towards a more robust evaluation for conversational question answering. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 2: Short Papers), Virtual Event, August 1-6, 2021, pages 3118\u20133122. Association for Computational Linguistics.\"}"}
{"id": "lrec-2022-1-525", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
