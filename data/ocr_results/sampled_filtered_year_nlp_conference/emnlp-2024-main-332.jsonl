{"id": "emnlp-2024-main-332", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Context-Aware Assistant Selection for Improved Inference Acceleration with Large Language Models\\n\\nJerry Huang*\\nMila - Quebec AI Institute\\nUniversit\u00e9 de Montr\u00e9al\\n\\nPrasanna Parthasarathi\\nHuawei Noah's Ark Lab\\n\\nMehdi Rezagholizadeh\\nHuawei Noah's Ark Lab\\n\\nSarath Chandar\\nMila - Quebec AI Institute\\nPolytechnique Montr\u00e9al\\n\\nCanada CIFAR AI Chair\\n\\nAbstract\\n\\nDespite their widespread adoption, large language models (LLMs) remain prohibitive to use under resource constraints, with their ever-growing sizes only increasing the barrier for use. One noted issue is the high latency associated with auto-regressive generation, rendering large LLMs use dependent on advanced computing infrastructure. Assisted decoding, where a smaller draft model guides a larger target model's generation, has helped alleviate this, but remains dependent on alignment between the two models. Thus if the draft model is insufficiently capable on some domain relative to the target model, performance can degrade. Alternatively, one can leverage multiple draft models to better cover the expertise of the target, but when multiple black-box draft models are available, selecting an assistant without details about its construction can be difficult. To better understand this decision-making problem, we observe it as a contextual bandit, where a policy must choose a draft model based on a context. We show that even without prior knowledge of the draft models, creating an offline dataset from only outputs of independent draft/target models and training a policy over the alignment of these outputs can accelerate performance on multiple domains provided the candidates are effective. Further results show this to hold on various settings with multiple assisted decoding candidates, highlighting its flexibility and the advantageous role that such decision making can play.\\n\\n1 Introduction\\n\\nWith the introduction of the Transformer (Vaswani et al., 2017) has emerged the era of large language models (Chowdhery et al., 2022; Touvron et al., 2023; OpenAI, 2024) and the development of LLMs capable of reasoning and acting in astonishingly human-like manner (Kaplan et al., 2020; Wei et al., 2023; Ouyang et al., 2022). However, the use of resource-intensive models and techniques remains a prerequisite and accordingly, methods have been developed and applied to alleviate concerns relating to the practical usability of these models (Dettmers et al., 2022; Dao, 2024). One major area that has observed consistent improvement over time is the auto-regressive decoding aspect of text generation, where each generation of a new token requires a complete inference pass through the model, which under-utilizes the property of attention and the ability of modern accelerators (e.g. GPUs, TPUs) to parallelize computations (de Jong et al., 2022; Kim et al., 2023a).\\n\\nA growing approach towards addressing this is speculative decoding (Xia et al., 2023; Leviathan et al., 2023). In speculative decoding, latency is reduced by minimizing the amount of high-latency sequential computations and replacing them with cheaper ones. Rather than sampling directly from the larger model, the sampling is approximated with samples from a smaller and cheaper model through accept-reject sampling. Specifically, a small draft model auto-regressively generates text which is then verified by a larger target model in parallel (Stern et al., 2018; Sun et al., 2021). Thus the large model does not need to generate text repeatedly but rather guides the small model by correcting outputs when it is truly incapable. This can reduce the number of calls to the large LLM, saving both time and memory. However, two models are required, along with some similarity in their generative abilities in order for this method to see significant speedups. While approaches exist to circumvent some of these needs (Yang et al., 2023; Zhang et al., 2023; Li et al., 2024; Cai et al., 2024; Hooper et al., 2024), these are often limited by the need for additional tuning (Liu et al., 2024b; Cai et al., 2024; Li et al., 2024), which is difficult in resource-constrained settings, or quality degradation in generations (Kim et al., 2023b). Because of\"}"}
{"id": "emnlp-2024-main-332", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Candidate Draft Models\\n\\nOline Dataset\\n\\nExpert Draft Model\\n\\nDraft Model\\n\\nTraining example\\n\\nExpert Output\\n\\nDraft Output\\n\\nDraft Output\\n\\nGreed Generation\\n\\nCandidate Draft Models\\n\\nDraft Model\\n\\nDraft Model\\n\\nTesting example\\n\\nExpert\\n\\nDraft Model\\n\\nAssisted Output\\n\\nPolicy\\n\\nPolicy\\n\\nSelect a draft candidate from the policy.\\n\\nUse query as policy input.\\n\\nTesting/Inference\\n\\nTraining\\n\\nScoring\\n\\nCost Penalties\\n\\nFigure 1: Overview of our methodology. We first train a policy using offline data collected from greedily decoded output from each model, which are scored to produce reward samples. At test time, the policy takes in a query $q'$ to select a draft candidate model, which is then used for assisted generation with the target model.\\n\\nthe evident size-cost tradeoff, this is very efficient if the draft model is well aligned to the target.\\n\\nHowever, while one can ensure that the final output follows the target distribution (Chen et al., 2023), selecting an inadequate draft model can lead to a lack of acceleration due to the significant number of rejections that will occur. While other methods allow for changes in the output distribution shift (Kim et al., 2023a; Zhou et al., 2024; Fu et al., 2024) to further speed-up inference, such types of shifts can be problematic in many high-risk scenarios. From this perspective, the presence of multiple draft models, each suited for different settings, can be helpful for inference acceleration without degradations in generation quality. By dynamically choosing a draft model, speedups can be achieved on multiple domains with marginal additional costs. However this requires learning how to choose the best draft option given a context, introducing a decision making problem which needs to be solved.\\n\\nSo how can this decision making process be learned? We start by observing this as a contextual bandits (Woodroofe, 1979; Auer, 2003), where the goal is to have a policy select a draft model based on a given query. This also requires rewards from which the policy can learn to estimate and compare the ideality of different actions that can be taken. To this end, we use an offline process to estimate the contextual alignment between different draft models and the target model on a set of training examples (Figure 1), enabling the construction of a dataset that defines the preference a target can have towards specific draft candidates. This enables us to train a policy that can take into account such preferences without knowing further details about the draft models. By deploying this policy at inference time it becomes possible to weigh these preferences, leading to speedups in generation with the target. We further show that this policy is useful when using self-speculative decoding, whereby the draft model is a subset of the target model parameters.\\n\\n2 Methodology\\n\\n2.1 Motivation\\n\\nAssume a large target model, $M$, incurs large end-to-end latencies that one wants to avoid. Speculative decoding aims to solve the latency issue by using a draft model to approximate the target model. However, as previously discussed, the draft model must be similar to the target model otherwise the\\n\"}"}
{"id": "emnlp-2024-main-332", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sampling distribution is too different and produce no speedups. Therefore, while draft models can help, they are only reliable when their knowledge distribution resembles that of the target. Accordingly, using only one draft model may not serve well in general if the target has multiple expertises. But by dynamically choosing between different draft models in any given scenario, then benefits from each draft model can be observed as long as the decision maker is competent and efficient.\\n\\n2.2 Problem Formulation\\n\\nWhen presented with a query \\\\( q \\\\), selecting a draft model among multiple unique candidates can lead to varying performance based on the chosen option. From a contextual bandits lens,\\\\( q \\\\) is a context for which there are \\\\( k \\\\) arms that each returns an independent reward \\\\( r \\\\). Each of arm corresponds to a different drafter whose reward is the time it takes to generate the output sequence through speculative decoding. Accordingly, each arm can produce a different reward for each \\\\( q \\\\). The objective then consists of learning a policy \\\\( \\\\pi(\\\\cdot|q) \\\\) which, for any given context \\\\( q \\\\), can select among the arm which can produce the greatest reward. From a speculative decoding scenario, the goal is to select the draft model whose abilities best align with the target for that given query, as this will minimize the number of times the target model must be invoked.\\n\\nRandomly choosing a draft model risks significant increases in latency, therefore learning to make the correct decision in a sample efficient manner is important. While the ideal reward is the real/observed speed-up, this can be expensive if the alignment with draft models is unknown. As such, a cheaper proxy may be necessary. However, two factors have a direct effect on the true reward: 1) the alignment between target and drafter and 2) the size of the drafter. This provides an alternative way to collect policy training data: use the draft models auto-regressively and compute alignment scores with the target outputs, then adjust these based on the size of the drafter. Next, we describe how we collect our data to train a policy offline.\\n\\n2.3 Offline Data Collection\\n\\nGiven a set of queries \\\\( Q = \\\\{q_i\\\\}_{i=1}^{n} \\\\), we produce outputs based on each \\\\( q_i \\\\) for the target model, \\\\( o_e \\\\), as well as each of the candidate drafters, \\\\( \\\\{o_j \\\\}_{j=1}^{k} \\\\).\\n\\nWe then use a similarity metric to compute scores for each candidate output \\\\( s_j = f(o_e, o_j) \\\\) (1) as a way to measure the alignment between target and candidates for \\\\( q_i \\\\). It is further possible to incorporate a score for the inference speed. For example, if we consider some relative measure of the inference speed for the specific drafter to be \\\\( c_j \\\\), then one can adjust the score as a weighted sum \\\\( s_j = \\\\alpha \\\\cdot f(o_e, o_j) + (1 - \\\\alpha) \\\\cdot c_j \\\\) (2) which takes into account both factors where \\\\( \\\\alpha \\\\in [0, 1] \\\\) weighs the two components.\\n\\n2.4 Decision Making\\n\\nWith the offline dataset, it becomes possible to train a policy \\\\( \\\\pi \\\\) which can independently act on a context by choosing a drafter to use with the target. We consider each \\\\( (q_i, j, s_j) \\\\) as state-action-reward tuples used to train \\\\( \\\\pi \\\\). Within the contextual bandits reformulation, each query-action pair \\\\( (q_t, a_t) \\\\in \\\\mathcal{Q} \\\\times \\\\mathcal{A} \\\\) is the drafter which produced an observed reward \\\\( r(q_t, a_t) \\\\). Here, we use the score \\\\( s_j \\\\) directly as the reward, as it acts as an estimate for the effectiveness of drafter \\\\( j \\\\) on the context. The policy is represented by a mapping \\\\( \\\\pi_{\\\\theta}(a|q) \\\\) from \\\\( \\\\mathcal{Q} \\\\times \\\\mathcal{A} \\\\) to \\\\( \\\\mathbb{R} \\\\) and we want to find parameters \\\\( \\\\theta^* \\\\) that maximize \\\\( J_{\\\\pi} = \\\\mathbb{E}_{\\\\mathcal{Q}, a \\\\sim \\\\pi_{\\\\theta}(\\\\cdot|q)} [r(q,a)] \\\\) where \\\\( \\\\mathcal{P}_{\\\\mathcal{Q}} \\\\) is the sampling distribution of the context. As the action space is discrete, integrating over the action space is equivalent to\\n\\n\\\\[\\n\\\\int a \\\\pi_{\\\\theta}(a|q) da = \\\\sum_{a \\\\in \\\\mathcal{A}(q)} \\\\pi_{\\\\theta}(a|q) = 1\\n\\\\]\\n\\nand the gradient with respect to the policy is\\n\\n\\\\[\\n\\\\nabla_{\\\\theta} J_{\\\\pi_{\\\\theta}} = E_{\\\\mathcal{Q}, a \\\\sim \\\\pi_{\\\\theta}(\\\\cdot|q)} [\\\\nabla \\\\log \\\\pi_{\\\\theta}(a|q) r(q,a)]\\n\\\\]\\n\\nwhich is equivalent to the REINFORCE (Williams, 2004) policy gradients method and we therefore use it to train our policy.\\n\\n3 Experimental Results\\n\\n3.1 Experimental Setup\\n\\nModels and Tasks. We select publicly available LLMs to use for our experiments. We conduct a number of experiments, which we motivate by...\"}"}
{"id": "emnlp-2024-main-332", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Results\\n\\n### Learning to choose the draft model.\\n\\nFor our first experiment, we use a T5 (Raffel et al., 2020) encoder-decoder models. As the target, we use an instruction-finetuned (Wei et al., 2022) Flan-T5-XXL (Chung et al., 2022) while our draft candidates are publicly available T5-Small models, one the base version and another fine-tuned on text summarization. We evaluate on translation (IWSLT2017 EN-DE (Cettolo et al., 2017)) and text summarization (XSUM (Narayan et al., 2018)).\\n\\n| Model                | BLEU | Decoding Speedup Accept (%) | ROUGE-L Decoding Speedup Accept (%) |\\n|----------------------|------|-----------------------------|-------------------------------------|\\n| T5-Small             | 18.26 | 1.00 \u00d7 (31.20 \u00b1 0.04 ms/token) | 35.93 1.00 \u00d7 (36.92 \u00b1 0.08 ms/token) |\\n| T5-Small-XSum        | 18.26 | 1.00 \u00d7 (31.06 \u00b1 0.06 ms/token) | 29.83 1.00 \u00d7 (37.06 \u00b1 0.06 ms/token) |\\n| Greedy Assisted Decoding | 18.26 | 1.10 \u00d7 (28.24 \u00b1 0.15 ms/token) | 41.68 35.93 0.97 \u00d7 (38.60 \u00b1 0.18 ms/token) |\\n| T5-Small             | 18.26 | 1.10 \u00d7 (37.61 \u00b1 0.19 ms/token) | 7.23 35.93 1.21 \u00d7 (30.71 \u00b1 0.15 ms/token) |\\n| T5-Small-XSum        | 18.26 | 0.83 \u00d7 (37.61 \u00b1 0.19 ms/token) | 29.10 0.99 \u00d7 (37.06 \u00b1 0.06 ms/token) |\\n| Speculative Decoding | 18.26 | 1.10 \u00d7 (28.14 \u00b1 0.17 ms/token) | 38.21 1.03 \u00d7 (37.53 \u00b1 0.14 ms/token) |\\n| Speculative Decoding + Decision Making | 18.76 | 1.09 \u00d7 (28.56 \u00b1 0.16 ms/token) | 37.72 1.09 \u00d7 (37.45 \u00b1 0.18 ms/token) |\\n| Dynamic              | 18.26 | 1.10 \u00d7 (29.20 \u00b1 0.17 ms/token) | 26.02 1.04 \u00d7 (31.33 \u00b1 0.16 ms/token) |\\n\\nTable 1 compares when draft models of the same size vary in their domain of expertise. While each model accelerates generation non-trivially within their knowledge domain (EN-DE for T5-Small and XSUM for T5-Small-XSum), they are largely unhelpful or detrimental when used outside their domain of expertise, as seen with the 1% slowdown from T5-Small on XSUM and a 17% decrease using T5-Small-XSum on EN-DE. In comparison, the policy ensures acceleration within both domains with negligible latency from decision making. This highlights some immediate benefits of policy use, namely that it can identify the correct draft model for a context without any explicit information regarding the draft candidates themselves. Rather, generating sampling outputs from each draft model and the target individually is sufficient to develop a general ability to differentiate between domains through the use of the computed rewards.\"}"}
{"id": "emnlp-2024-main-332", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Effect of varying the tradeoff between output alignment and draft model size (controlled through $\\\\alpha$). Each compares the use of Flan-T5-Small as a draft model (red horizontal line). As $\\\\alpha$ increases, the model increasingly uses the smallest draft model for decoding, demonstrating that the offline dataset is sufficient to learn how to balance the quality of the draft model's outputs and the cost of using it. All cases use speculative sampling/decoding.\\n\\nTable 2: Speeds of different draft models on XSum with a Flan-T5-XXL model expert (averaged over 5 seeds). Observed decoding speed varies as an effect of drafter size and alignment with the expert.\\n\\nBalancing quality and speed. It is also important that the draft model is sufficiently inexpensive to use relative to the target model. This motivates our second experiment, which is evaluated only on XSum, but compares draft candidates that vary in terms of size and target model alignment. Multiple draft models are compared: a Flan-T5-Small (80M parameters), the same T5-Small (60M) models mentioned above, and Flan-T5-Base (220M). Table 2 shows the speed-ups earned through speculative decoding using these different draft candidates and a Flan-T5-XXL target. Although larger draft models may be better aligned with the target compared to smaller options, using them incurs a latency that can end up being less efficient. Balancing alignment and efficiency is therefore an issue to consider when deciding between candidates. Figure 2 shows how offline training can help accomplish this. Setting $\\\\alpha$ to vary between the objectives defined in \u00a72.3, where we use fixed inference costs based on the size of the draft models, we observe how a dynamic policy can eventually adapt to the preferences set by the choice of $\\\\alpha$. For example, as $\\\\alpha$ approaches 1, the policy places increasing preference on the smallest draft model regardless of quality. Meanwhile $\\\\alpha \\\\rightarrow 0$ shows increasing preference towards the model that has greatest alignment with the target generations. This demonstrates the general flexibility that can come with using such a weighting scheme of different rewards, while demonstrating that even simpler proxies for the inference penalty are sufficient to properly balance the two.\\n\\nHow many examples need to be learned to differentiate? It is further necessary to consider...\"}"}
{"id": "emnlp-2024-main-332", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the number of examples that are needed for the decision maker to properly learn to differentiate between different examples. To this end, we investigate how quickly the policy can learn to use the annotated scores within the offline dataset to demonstrate a visible speed-up improvement. We re-use our models from the first experiment, but keep track of the decoding speed as the number of examples used to train our policy $\\\\pi^\\\\theta$ increases.\\n\\nAs we can observe in Figure 3, learning to select the correct model occurs rather quickly, as training for fewer than a total of 10000 examples is sufficient to attain a level of performance that is equivalent to training on the entire offline dataset, which consists of nearly 400 thousand examples. This result demonstrates the general efficiency of this method, as collecting and training the policy on outputs from a minimal amount of examples shows the ability to generalize quite strongly.\\n\\nAuto-regressive generation as an option. Scenarios exist where the draft models will not be useful, in which case using the target auto-regressively remains the most reasonable option.\\n\\nTo this end, we attempt to observe how providing this option to the decision maker can affect our previous experiments. We repeat the same experiment from Table 1 but allow our policy to learn to choose to generate auto-regressively. To avoid trivially perfect matching of outputs, we sample outputs from the target model and score against the greedy output. Due to the large size of the target compared to the drafters, we use $\\\\alpha = 0.5$ to balance the size and quality scores.\\n\\nTable 3: Decoding speeds under a dynamic decision making regime where auto-regressive generation is a decoding option, on IWSLT2017 EN-DE and XSUM.\\n\\n|                  | Greedy $\\\\pi^\\\\theta$ | Dynamic $\\\\pi^\\\\theta$ |\\n|------------------|----------------------|-----------------------|\\n| EN-DE            | 10.67 \u00b1 0.00 (22.03 \u00b1 0.16 ms/token) | 10.64 \u00b1 0.95 (23.54 \u00b1 0.19 ms/token) |\\n| XSum             | 10.69 \u00b1 0.78 (28.08 \u00b1 0.18 ms/token) | 10.59 \u00b1 0.74 (29.91 \u00b1 0.16 ms/token) |\\n\\nTable 4: Decoding speeds on GSM8K (test set) with a Flan-T5-XXL expert. Inference on the latter two tasks is negligibly different from Table 3.\\n\\nWe further verify whether the policy can ignore draft models when they are not useful. We experiment by including GSM8K to our tasks, which only the target is aligned. Since neither draft model can accelerate inference on this task, the policy should ideally avoid drafting for examples from this setting. Since GSM8K is significantly smaller than EN-DE and XSum, we reduce the number of examples to match all datasets in terms of size.\\n\\nTable 4 shows auto-regressive generation to (unsurprisingly) outperform assisted generation. However, using a policy shows comparable speed to auto-regressive generation, indicating that it learns to ignore the draft models due to the stark contrast in the greedy outputs from each model.\\n\\nGeneralization to Multi-Task Drafters. To demonstrate the applicability of this method to more general settings, in particular cases where the draft models may be competent at multiple tasks, we further apply our policy-based selection method to SpecBench (Xia et al., 2024) using a Vicuna-33B (Chiang et al., 2023) target with smaller draft models (Table 5). Given the size of SpecBench (480 examples, divided equally into 6 tasks), we use this exclusively as a test-set. To train our policy, we use the original task datasets from which SpecBench examples were extracted and sample even amounts of examples from each (2000). For MT-BENCH, there are only 80 total examples which are all included in the test set but which we sample with replacement to use for a training set. Accordingly, results on this task may be over-confident. Because Vicuna models are decoder-only Transformers, we adjust the sentence representation to be the final hidden representation of the input sequence. Our results show that our initial findings from a T5 architecture hold, suggesting that such a policy-based training method is both robust and generalizable to different settings.\\n\\nAblation with self-drafting. Despite the benefits of assisted decoding, drafting relies on the availability...\"}"}
{"id": "emnlp-2024-main-332", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method      | MT-BENCH | TRANS. SUM. | QA MATH | RAG AVG. |\\n|-------------|----------|-------------|---------|---------|\\n| Auto-regressive | 1.00\u00d7   | 1.00\u00d7      | 1.00\u00d7   | 1.00\u00d7   |\\n\\nVicuna-68m and Vicuna-160m (Fedus et al., 2021; Ong et al., 2024) have been explored as a way to leverage the multitude of LLMs that exist within the target. To explore the differences with Table 6 show that although intermediate layer or degrade generation quality (Cai et al., 2024).\\n\\nThis leads to a dilemma when deploying LLMs in a scenario for deciding when to become irreconcilable with resource constraints. The policy meanwhile acts as a router to the correct sub-networks, similar to a Mixture-of-Experts (MoE) (Shazeer et al., 2017) style paradigm. The policy methods are capable of recovering to a performance similar to the most capable model, highlighting that the proposed offline policy learning approach has potential for self-drafting as well.\\n\\n### Table 5: Acceleration on SpecBench using a single LLM\\n\\n| Method       | Layer 8 Drafting | Layer 32 Drafting | Layer 24 Drafting | Layer 16 Drafting |\\n|--------------|------------------|-------------------|-------------------|-------------------|\\n| Auto-regressive | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n| Vicuna-68m   | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| Vicuna-160m  | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| LLaMA-68m    | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| LLaMA-13B-Chat | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n| LLaMA-2-13B-Chat | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n\\nExpert with well. This demonstrates the use of a policy remains effective. This is not the case with TENCH, where the auto-regressive option is not available, we note that the policy methods are capable of recovering to a performance similar to the case where the auto-regressive option is not available, we note that the policy methods are capable of recovering to a performance similar to the best case intermediate drafter on LLaMA-68m.\\n\\n### Table 6: Results in a scenario for deciding when to use a single LLM\\n\\n| Method       | Intermediate Drafting + Policy + Auto-regressive Option | Auto-Regressive Generation |\\n|--------------|--------------------------------------------------------|-----------------------------|\\n| Auto-regressive | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n| Vicuna-68m   | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| Vicuna-160m  | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| LLaMA-68m    | 1.00\u00d7           | 1.00\u00d7             | 1.00\u00d7             | 1.00\u00d7             |\\n| LLaMA-13B-Chat | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n| LLaMA-2-13B-Chat | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 | 1.00\u00d7 |\\n\\nSuch models can be difficult to obtain, leading to number of potential draft options and necessitate pre-determined path flows during inference (Zhang et al., 2023). Meanwhile, methods that use additional end-to-end heads for parallel decoding may be well suited for the same set of tasks, meaning that routing to the most suitable model can be lower-quality responses. Similarly, not all models to smaller models can save costs but may result in responses but can be expensive, while routing queries to the most capable model leads to the highest-quality responses.\\n\\nIn particular with the presence of an auto-regressive decoding option, highlighting that the proposed offline policy training can minimize performance loss in a useful manner to fall back to the most effective model. For example, using a policy can minimize performance loss in a useful manner to fall back to the most effective model. For example, this setting, we conduct an additional ablation.\\n\\nOur work shares a great deal of similarity with RIVIA, conducted on each dataset independently, under this setup.\\n\\n### 4 Discussion\\n\\nThis demonstrates the use of a policy remains effective. This is not the case with TENCH, where the auto-regressive option is not available, we note that the policy methods are capable of recovering to a performance similar to the best case intermediate drafter on A.\\n\\n...\"}"}
{"id": "emnlp-2024-main-332", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in the wild, but have yet to be widely used within\\ndownstream settings such as speculative decoding.\\n\\nAdaptive Speculative Decoding.\\n\\nSpeculative decoding methods require the use of many pre-defined hyper-parameters which can significantly influence acceleration, with even minor changes having noticeable effects. Recent work has begun to explore how to decouple this process, such as by dynamically selecting the number of drafting tokens to generate at each decoding step (Wang et al., 2024; Liu et al., 2024a). Kavehzadeh et al. (2024) further discussed dynamically selecting a model per instance, however their method is limited to their specific setup due to needing to compute confidence scores after generation at early exits.\\n\\nWhile we do not introduce a new decoding algorithm, we make a first attempt to make the speculative decoding adaptive through the ability to switch between multiple draft models based on the input. However, more complex levels of adaptivity may be necessary as each decoding step may not be the same, necessitating perhaps a need to carefully adjust different hyperparameters through the process in order to maximize acceleration.\\n\\nDecision Making for Assisted Decoding.\\n\\nAs-assisted decoding can require making multiple decisions. One of these is determining an ideal number of draft tokens to decode at each step. Another relates to how to reject tokens, which commonly uses either greedy (Xia et al., 2023) or sampling-based token-matching heuristics (Leviathan et al., 2023). However, there are trade-offs when enforcing specific choices, which requires further investigation to better understand how to tune such techniques.\\n\\nThis work proposes adding an additional decision at the beginning of the decoding process, namely at the beginning of the process under the assumption that multiple drafting options exist. While we limit ourselves to make a more complete analysis within a more self-contained setting, various ways to have these methods co-exist within one larger pipeline are possible. However such work is left for future exploration due to the non-trivial nature of understanding how different choices and effect overall reported results in conjunction.\\n\\nMeasuring Alignment Between Outputs.\\n\\nWe observe that token-level similarity scores are effective for training the decision maker, which can be attributed to the fact that assisted decoding itself relies on matching the token-level distribution of outputs. As such, if the greedy-decoded output from a draft model highly resembles the target output, it follows that this will be represented by a higher degree of similarity between the probability distributions in the logit space, which can then lead to fewer rejections when sampling.\\n\\nHowever, such metrics have limitations (Deutsch et al., 2022) due to capturing primarily superficial elements of text, where marginal differences in distribution have large effects on the output text. Furthermore, different metrics may overfit specific tasks, necessitating the need for better measures of draft/target alignment, which can hopefully lead to better estimation of rewards for training improved policies, either by designing better metrics themselves or by learning to compare features at different levels of granularity (ex. target and draft logits against text outputs). Additionally, semantic meaning also can play an important role, as outputs with significant structure may still possess the same meaning, something that token-level similarity metrics will not adequately capture.\\n\\nSpeculative Decoding as Approximate Inference.\\n\\nSpeculative decoding can be analogized as a form of approximate inference where due to the intractability of performing inference with a model of interest, approximation methods are used to learn an estimate of the model. While training the draft model is equivalent to performing variational inference (i.e. approximating an intractable distribution with a surrogate), this can be expensive. Accordingly, training only a policy can be seen as weighing a set of fixed distributions to act as a better surrogate for the target model.\\n\\nSome works have further attempted to study speculative decoding from this angle. In particular, Zhou et al. (2024) explore such a process by building a draft model through the use of KL-divergence losses, effectively building a posterior distribution of the target model based on likelihood information from the draft output. Liu et al. (2024b) meanwhile explore the same technique as the distribution of examples changes, building a draft model that can adapt to changing user inputs. Such settings also could perhaps benefit from multiple draft models, where conditioning on the query can enable more effective adaptation of draft models to better generalize to unseen settings.\\n\\nHosting Multiple Draft Models.\\n\\nAn important aspect of this method relates to the need to host multiple draft models in conjunction with the expert.\"}"}
{"id": "emnlp-2024-main-332", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This can incur additional costs, in particular if the expert and selected drafter do not reside in the same device. While methods such as self-drafting avoid this issue and the possibility to create minimally-sized drafters generally alleviates the concern of excessive memory usage, one particular aspect of consideration remains hardware level optimizations which can best enable for the selected drafters to be loaded at maximal speed, avoiding additional latency that can result from the bandwidth constraints that relate to data transfer between devices.\\n\\n5 Conclusion\\n\\nThis work presents the first work at attempting to integrate assisted generation within a setting where multiple black-box draft candidates exist. When no a-priori knowledge of which draft candidate is best suited for assisting the decoding of a given example, the problem can be modeled as a contextual bandits problem, where the goal is to estimate the unknown reward from each drafting option. Our work demonstrates that offline RL presents an efficient method for learning to distinguish the available options and provide accelerated decoding across various examples within this setting, with a logical way to collect offline data from models for learning. Our results and ablations show that learning a policy with this approach can adapt to general preferences while accounting for more complex aspects of the decision making, highlighting its robustness. Furthermore, such a method is scalable and robust to the introduction of more draft models or the removal of draft models, presenting a viable alternative to settings where a uniquely superior draft model may be unavailable.\\n\\nNevertheless, areas of further development exist. For example, learning in an online fashion may render this method more broadly applicable. Alternatively, exploring how to dynamically choose drafters at every decoding step rather than per example, as well as combining this direction of work with that which attempts to adaptively choose the speculation length at every step, are feasible ways of combining our findings with concurrent work in the hopes of reaping the benefits of all methods.\\n\\n6 Limitations\\n\\nThis work has a few limitations which define the scope of future work.\\n\\nChoice of draft models and data domains\\n\\nResults may stem from the distinct boundaries that exist between domains/tasks. In settings where such boundaries are not well defined, outcomes may differ. However technical limitations and the absence of sufficient pre-trained models for comparison make this difficult to explore immediately.\\n\\nAdditional storage and memory\\n\\nThe usage of multiple models that draft independently requires additional memory, which can be more difficult to manage when there are explicit constraints on this front (self-drafting avoids this due to the use of a single model). Furthermore, collecting an offline dataset can be difficult in some specific scenarios where inference is burdensome, for example when input/output sequences are very long, or when many offline examples are required.\\n\\nSelf-Drafting\\n\\nWe work on a setting where we do not conduct any additional training of parameters that are explicitly linked to the language model itself, whether they are existing parameters or new parameters added as a result of the method. While there are ways in which our explored method can be applied to these as well, computational limitations make it difficult to rigorously conduct such studies at the moment and we leave it to future work for this reason.\\n\\n7 Ethics Statement\\n\\nThis paper discusses the concept of dynamically choosing between multiple black-box draft models for speculative decoding, proposing an offline reinforcement learning approach for adaptively selecting a good draft model for assistance. Our results are related to the decoding speed of models, which is unlikely to lead to ethical concerns or problematic interpretations of such results.\\n\\n8 Acknowledgements\\n\\nJerry Huang received financial support under the form of a National Science and Engineering Research Council (NSERC) Canada Graduate Scholarship, a Fonds de Recherche du Qu\u00e9bec Nature et technologies (FRQNT) Training Scholarship and a Bourse d\u2019Excellence Hydro-Qu\u00e9bec. Sarath Chandar is supported by a Canada CIFAR AI Chair, the Canada Research Chair in Lifelong Machine Learning and a NSERC Discovery Grant.\"}"}
{"id": "emnlp-2024-main-332", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nPeter Auer. 2003. Using confidence bounds for exploitation-exploration trade-offs. J. Mach. Learn. Res., 3(null):397\u2013422.\\n\\nTianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. 2024. Medusa: Simple llm inference acceleration framework with multiple decoding heads.\\n\\nMauro Cettolo, Marcello Federico, Luisa Bentivogli, Jan Niehues, Sebastian St\u00fcker, Katsuhito Sudoh, Koichiro Yoshino, and Christian Federmann. 2017. Overview of the IWSLT 2017 evaluation campaign. In Proceedings of the 14th International Conference on Spoken Language Translation, pages 2\u201314, Tokyo, Japan. International Workshop on Spoken Language Translation.\\n\\nCharlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. 2023. Accelerating large language model decoding with speculative sampling. Preprint, arXiv:2302.01318.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90% chatgpt quality.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways. Preprint, arXiv:2204.02311.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models. Preprint, arXiv:2210.11416.\\n\\nTri Dao. 2024. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations.\\n\\nMichiel de Jong, Yury Zemlyanskiy, Joshua Ainslie, Nicholas FitzGerald, Sumit K. Sanghai, Fei Sha, and William Cohen. 2022. Fido: Fusion-in-decoder optimized for stronger performance and faster inference. ArXiv, abs/2212.08153.\\n\\nTim Dettmers, Mike Lewis, Younes Belkada, and Luke Zettlemoyer. 2022. GPT3.int8(): 8-bit matrix multiplication for transformers at scale. In Advances in Neural Information Processing Systems.\\n\\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2022. On the limitations of reference-free evaluations of generated text. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10960\u201310977, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nWilliam Fedus, Barret Zoph, and Noam M. Shazeer. 2021. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23:120:1\u2013120:39.\\n\\nYichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. 2024. Break the sequential dependency of llm inference using lookahead decoding. Preprint, arXiv:2402.02057.\\n\\nColeman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Sophia Shao. 2024. Speed: Speculative pipelined execution for efficient decoding. Preprint, arXiv:2310.12072.\\n\\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. 2020. Scaling laws for neural language models. Preprint, arXiv:2001.08361.\\n\\nParsa Kavehzadeh, Mojtaba Valipour, Marzieh Tahaei, Ali Ghodsi, Boxing Chen, and Mehdi Rezagholizadeh. 2024. Sorted LLaMA: Unlocking the potential of intermediate layers of large language models for dynamic inference. In Findings of the Association for Computational Linguistics: EACL 2024, pages 2129\u20132145, St. Julian's, Malta. Association for Computational Linguistics.\\n\\nSehoon Kim, Coleman Hooper, Thanakul Wattanawong, Minwoo Kang, Ruohan Yan, Hasan Genc, Grace Dinh, Qijing Huang, Kurt Keutzer, Michael W. Mahoney, Sophia Shao, and Amir Gholami. 2023a. Full stack optimization of transformer inference. In Architecture and System Support for Transformer Models (ASSYST @ISCA 2023).\"}"}
{"id": "emnlp-2024-main-332", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. 2023b. Speculative decoding with big little decoder. In Thirty-seventh Conference on Neural Information Processing Systems.\\n\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR.\\n\\nYuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. 2024. Eagle: Speculative sampling requires rethinking feature uncertainty.\\n\\nJiahao Liu, Qifan Wang, Jingang Wang, and Xunliang Cai. 2024a. Speculative decoding via early-exiting for faster llm inference with thompson sampling control mechanism. Preprint, arXiv:2406.03853.\\n\\nXiaoxuan Liu, Lanxiang Hu, Peter Bailis, Ion Stoica, Zhijie Deng, Alvin Cheung, and Hao Zhang. 2024b. Online speculative decoding.\\n\\nIlya Loshchilov and Frank Hutter. 2019. Decoupled weight decay regularization. In International Conference on Learning Representations.\\n\\nShashi Narayan, Shay B. Cohen, and Mirella Lapata. 2018. Don't give me the details, just the summary! topic-aware convolutional neural networks for extreme summarization. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1797\u20131807, Brussels, Belgium. Association for Computational Linguistics.\\n\\nIsaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph E. Gonzalez, M Waleed Kadous, and Ion Stoica. 2024. Routellm: Learning to route llms with preference data. Preprint, arXiv:2406.18665.\\n\\nOpenAI. 2024. Gpt-4 technical report. Preprint, arXiv:2303.08774.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations.\\n\\nMitchell Stern, Noam M. Shazeer, and Jakob Uszkoreit. 2018. Blockwise parallel decoding for deep autoregressive models. In Neural Information Processing Systems.\\n\\nXin Sun, Tao Ge, Furu Wei, and Houfeng Wang. 2021. Instantaneous grammatical error correction with shallow aggressive decoding. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 5937\u20135947, Online. Association for Computational Linguistics.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models. Preprint, arXiv:2302.13971.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nSiqi Wang, Hailong Yang, Xuezhu Wang, Tongxuan Liu, Pengbo Wang, Xuning Liang, Kejie Ma, Tianyu Feng, Xin You, Yongjun Bao, Yi Liu, Zhongzhi Luan, and Depei Qian. 2024. Minions: Accelerating large language model inference with adaptive and collective speculative decoding. Preprint, arXiv:2402.15678.\\n\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903.\\n\\nRonald J. Williams. 2004. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229\u2013256.\\n\\nMichael Woodroofe. 1979. A one-armed bandit problem with a concomitant variable. Journal of the American Statistical Association, 74(368):799\u2013806.\\n\\nHeming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. 2023. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 3909\u20133925, Singapore. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-332", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Experimental Details\\n\\nA.1 Baselines\\nTo baseline and compare our architectural constraints against (Leviathan et al., 2023), we partially benchmark our experiments against theirs. These are presented in Table 7 and 8. We conduct this as we suspect a difference in both system architecture used for experiments as well as for implementation of models.\\n\\nWe observe that their results are generally show speedups that are consistently 2.5 to 3.0 times larger than ours, with minor deviations. We attribute this to the usage of different computational resources and potential implementation differences. Additionally, given the small amount of variation in the relative differences between the observed and reported speedups, we contend that these differences are not due to errors in implementation.\\n\\nA.2 Technical Details\\nAll experiments are conducted on a machine with a single NVIDIA A100 GPU with 8 CPU cores. We run all experiments using PyTorch and HuggingFace models.\\n\\nA.3 Hyperparameter Configurations\\nDetails about hyperparameters we use within our experiments are detailed here.\\n\\nA.4 Templates\\nFor each example, we use a specific prompt based on the dataset from which the data originates (see Table 10). We follow the templates provided originally by Raffel et al. (2020) and Chung et al. (2022).\\n\\nA.5 Cost Function\\nWhen considering multiple draft candidates, we use the following simple function for generating fixed costs for the different models. Suppose the candidates have parameters $P = \\\\{p_1, p_2, ..., p_k\\\\}$.\\n\\nThen the cost for the models are $c_i = 1 - e^{p_i} e^{p_j}$ where $j = \\\\text{argmax}_i p_i$.\\n\\nA.6 Accept Rate Computation\\nTo compute the accept rate of tokens, we define the number of generated tokens in a given draft as the total number of tokens generated by the draft model (this is equivalent to $\\\\gamma$). The number of accepted tokens in a given draft is the number of generated tokens that are validated as correct by the target model. When a token is rejected within a draft, all subsequent tokens are considered rejected as well. The accept rate is then the quotient of the total number of accepted tokens divided by the total number of generated tokens.\\n\\nA.7 Computing Wall-Clock Performance\\nTo compute the wall-clock time when using a policy, we include the amount of time used to infer on the policy. However, we do not include the time needed to generate the sentence representation. This is because upon generating the original sentence representation, the large model's KV cache can be updated to store these for the future verification passes, meaning that they do not need to be recomputed again in the future. As such, we treat this initial pass through as being part of the first verification pass.\\n\\nAdditionally, one could theoretically save on the policy inference by performing batched inference on many examples at once. However, this is not particularly applicable in practice, where different inputs arrive at different times. As such, we treat each example individually and include these times within the per-example speeds.\\n\\nB Different Sampling Hyperparameters\\nWe run our ablations based on our setup for our first experiment.\"}"}
{"id": "emnlp-2024-main-332", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 7: Reproduced translation results\\n\\n| Target Model | Draft Model | Temperature | Draft Tokens | Ours | Original | Relative Difference |\\n|--------------|-------------|-------------|--------------|------|----------|---------------------|\\n| T5-XXL       | None        | 0           | -            | 1.00 | -        | 1.00\u00d7(19.6 ms/token) |\\n| T5-XXL       | T5-Small    | 0           | 7            | 1.21 | 1.10     | 31.8 ms/token       |\\n| T5-XXL       | T5-Base     | 0           | 0            | 1.96 | 1.80     | 32.1 ms/token       |\\n| T5-XXL       | T5-Large    | 0           | 0            | 1.61 | 1.47     | 35.3 ms/token       |\\n| T5-XXL       | T5-Small    | 1           | 1            | 1.03 | 1.02     | 19.1 ms/token       |\\n| T5-XXL       | T5-Base     | 1           | 0            | 1.83 | 1.82     | 23.5 ms/token       |\\n| T5-XXL       | T5-Large    | 1           | 0            | 1.56 | 1.47     | 32.7 ms/token       |\\n\\n### Table 8: Reproduced summarization results\\n\\n| Target Model | Draft Model | Temperature | Draft Tokens | Ours | Original | Relative Difference |\\n|--------------|-------------|-------------|--------------|------|----------|---------------------|\\n| T5-XXL       | None        | 0           | -            | 0.99 | 0.99     | 32.1 ms/token       |\\n| T5-XXL       | T5-Small    | 0           | 0            | 0.87 | 0.87     | 36.4 ms/token       |\\n| T5-XXL       | T5-Base     | 0           | 0            | 0.59 | 0.59     | 53.8 ms/token       |\\n| T5-XXL       | T5-Large    | 1           | 0            | 0.86 | 0.86     | 33.0 ms/token       |\\n\\n### Table 9: Optimization Hyperparameters\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| Optimizer      | AdamW |\\n| Learning Rate  | 0.001 |\\n| Weight Decay   | 0.01  |\\n| $\\\\beta_1$      | 0.9   |\\n| $\\\\beta_2$      | 0.99  |\\n| $\\\\epsilon$     | 1e-8  |\\n\\n### Table 10: Prompts for the different tasks\\n\\n| Task | Prompt |\\n|------|--------|\\n| EN   | translate English to German: {input} |\\n| XS   | summarize: {input} |\\n| GSM8K| Q: {input} |\\n\\n### B.1 Effect of Number of Draft Tokens\\n\\nWe test our method with 5, 7 and 10 draft tokens in Table 11 and Table 12.\\n\\n### B.2 Effect of Temperature\\n\\nWe test our method with varying temperature values in Table 13 and Table 14.\\n\\nWe noted through ablations that increasing temperature past $T = 1$ resulted in a significant slowdown in the decoding speed as well as the quality of the sampled generations. As such, we only present results on values of $T \\\\leq 1$. \\n\\n5829\"}"}
{"id": "emnlp-2024-main-332", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 11: Varying the number of drafted tokens for assisted generation on IWSLT2017 EN-DE.\\n\\n| Model          | \u03b3 | Speedup 1.00\u00d7 (ms/token) |\\n|----------------|---|-------------------------|\\n| Auto-regressive | 5 | 31.06                   |\\n| T5-Small       | 7 | 28.93                   |\\n| T5-Small-XSUM  | 7 | 31.20                   |\\n| Greedy Policy  | 5 | 28.45                   |\\n| Dynamic Policy | 5 | 29.53                   |\\n\\nT = 1 for all cases.\\n\\n### Table 12: Varying the number of drafted tokens for assisted generation on XSUM.\\n\\n| Model          | \u03b3 | Speedup 1.00\u00d7 (ms/token) |\\n|----------------|---|-------------------------|\\n| Auto-regressive | 5 | 31.06                   |\\n| T5-Small       | 7 | 29.00                   |\\n| T5-Small-XSUM  | 7 | 31.20                   |\\n| Greedy Policy  | 5 | 27.83                   |\\n| Dynamic Policy | 5 | 29.02                   |\\n\\nT = 1 for all cases.\\n\\n### Table 13: Varying temperature for assisted generation on IWSLT2017 EN-DE.\\n\\n| Model          | \u03b3 | Speedup 1.00\u00d7 (ms/token) |\\n|----------------|---|-------------------------|\\n| Auto-regressive | 7 | 37.06                   |\\n| T5-Small       | 7 | 38.21                   |\\n| T5-Small-XSUM  | 7 | 31.31                   |\\n| Greedy Policy  | 7 | 27.83                   |\\n| Dynamic Policy | 7 | 29.02                   |\\n\\nT = 0.5, 0.9, 1.\\n\\n### Table 14: Varying temperature for assisted generation on XSUM.\\n\\n| Model          | \u03b3 | Speedup 1.00\u00d7 (ms/token) |\\n|----------------|---|-------------------------|\\n| Auto-regressive | 7 | 37.70                   |\\n| T5-Small       | 7 | 38.21                   |\\n| T5-Small-XSUM  | 7 | 31.31                   |\\n| Greedy Policy  | 7 | 27.83                   |\\n| Dynamic Policy | 7 | 29.02                   |\\n\\nT = 0.5, 0.9, 1.\\n\\n\u03b3 = 7 for all cases.\"}"}
