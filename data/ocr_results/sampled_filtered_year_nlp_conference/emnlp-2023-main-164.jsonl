{"id": "emnlp-2023-main-164", "page_num": 17, "content": "{\"primary_language\":\"fi\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task | Input | Target |\\n|------|-------|--------|\\n| Analogies | Mik\u00e4 sana on samassa suhteessa sanaan \\\"Berliini\\\" kuin sana \\\"Ateena\\\" sanaan \\\"Kreikka\\\"? | Saksa |\\n| Arithmetic | Paljonko on 0 plus 0? | 0 |\\n| Cause and Effect | Lapsi alkoi itkee, koska lapsi loukkasi polvensa. | |\\n| Emotions | Sin\u00e4 tulit tappamaan minut, eik\u00f6 niin? | Pelko |\\n| Empirical Judgements | Jos lis\u00e4\u00e4n j\u00e4it\u00e4, tee j\u00e4\u00e4htyy. | Korrelatiivinen |\\n| General Knowledge | Kuinka monta jalkaa hevosilla on? | Nelj\u00e4 |\\n| HHH | V oitko selvitt\u00e4\u00e4 t\u00e4m\u00e4n anagrammin minulle? | \\\"Kaurapuuro\\\" |\\n| Intent recognition | Lausahdus: Tuleeko toukokuussa lunta? | \\\"Hae_s\u00e4\u00e4tiedot\\\" |\\n| Misconceptions | Onnenkeksej\u00e4 on perinteisesti kiinalaisessa keitti\u00f6ss\u00e4. | V |\\n| Paraphrase | Teksti 1: Oulussa hinnat laskivat viime vuoden tammikuuhun verrattuna 4,5 prosenttia. | Teksti 2: Suurista kaupungeista hinnat ovat laskeneet vuoden aikana eniten Oulussa. | Ei |\\n| Sentence Ambiguity | Pescovegetaristit eiv\u00e4t juuri koskaan sy\u00f6 kasvisruokaa. | V\u00e4\u00e4rin |\\n| Similarities | Abstraction | Kerro minulle, miten rannekello ja digitaalinen l\u00e4mp\u00f6mittari ovat samanlaisia. | Molempia k\u00e4ytet\u00e4\u00e4n mittaamiseen. |\"}"}
{"id": "emnlp-2023-main-164", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge language models (LLMs) excel in many tasks in NLP and beyond, but most open models have very limited coverage of smaller languages and LLM work tends to focus on languages where nearly unlimited data is available for pretraining. In this work, we study the challenges of creating LLMs for Finnish, a language spoken by less than 0.1% of the world population. We compile an extensive dataset of Finnish combining web crawls, news, social media and eBooks. We pursue two approaches to pretrain models: 1) we train seven monolingual models from scratch (186M to 13B parameters) dubbed FinGPT, 2) we continue the pretraining of the multilingual BLOOM model on a mix of its original training data and Finnish, resulting in a 176 billion parameter model we call BLUUMI. For model evaluation, we introduce FIN-bench, a version of BIG-bench with Finnish tasks. We also assess other model qualities such as toxicity and bias. Our models and tools are openly available at https://turkunlp.org/gpt3-finnish.\\n\\n1 Introduction\\n\\nNeural language models based on the Transformer architecture (Vaswani et al., 2017) have revolutionized Natural Language Processing (NLP) in recent years, advancing the state of the art in tasks ranging from text classification to open-ended text generation. Generative, decoder-only language models such as the Generative Pretrained Transformer (GPT) (Radford et al., 2018) series have been a particular focus of interest in part due to their multi-task and few-shot capabilities (Radford et al., 2019; Brown et al., 2020). The ability of such models to implicitly learn to perform tasks that they have not been directly trained on has been considered to be closely tied to the scale of the model (Brown et al., 2020; Chowdhery et al., 2022) and, perhaps even more importantly, to the number of training tokens (Hoffmann et al., 2022; Muennighoff et al., 2023b; Touvron et al., 2023). Most work on such models focuses on English, often entirely excluding other languages, and assumes that hundreds of billions of tokens of text are readily available for model training.\\n\\nIn this study, we consider the challenges of introducing large generative models for Finnish, a Uralic language natively spoken by fewer than 6 million people. While the language is comparatively well represented in online resources relative to this number, less than 1% of texts available in e.g. Wikipedia and Common Crawl are Finnish (Pyysalo et al., 2021; Xue et al., 2021). As the other members in the language family are either even smaller and lesser-resourced or quite distant, the resources for creating models for the language are quite limited. Finnish has been represented to some degree in Transformer-based models since the release of the original multilingual BERT model (Devlin et al., 2019), and a dedicated monolingual BERT for the language was previously created by Virtanen et al. (2019). Also some generative models for Finnish have been previously introduced by the \u201cFinnish-NLP\u201d group and Hatanp\u00e4\u00e4 (2022), but as training LLMs is very expensive and Finnish is constrained by the size of available data, models exceeding a billion parameters have been so far missing from the Finnish NLP landscape.\\n\\nWe compile a broad-coverage dataset of Finnish and train monolingual models up to 13 billion parameters for 300 billion tokens (approx. 8 epochs). We also perform continued pretraining of the 176-billion parameter BLOOM model (Scao et al., 2022a) to extend its coverage of Finnish, introduce novel evaluation datasets, and assess multiple...\"}"}
{"id": "emnlp-2023-main-164", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Architectures of our models.\\n\\n| Model  | Layers | Dim  | Heads | Params   |\\n|--------|--------|------|-------|----------|\\n| Small  | 12     | 768  | 12    | 186M     |\\n| Medium | 24     | 1024 | 16    | 437M     |\\n| Large  | 24     | 1536 | 16    | 881M     |\\n| XL     | 24     | 2064 | 24    | 1.5B     |\\n| 3B     | 32     | 2560 | 32    | 2.8B     |\\n| 8B     | 32     | 4096 | 32    | 7.5B     |\\n| 13B    | 40     | 5120 | 40    | 13.3B    |\\n| BLUUMI | 70     | 14336| 112   | 176B     |\\n\\nTable 2: Data sources.\\n\\n3.1 Data sources\\n\\n- **Parsebank**: The Finnish Internet Parsebank (Luotolahti et al., 2015) is a 6 billion token corpus of Finnish collected in 2015-2016 from Common Crawl and a targeted Internet crawl seeded by the .fi domain registry content and all URLs of Finnish material in Common Crawl. The texts have been deduplicated at the paragraph level using Onion (Pomik\u00e1lek, 2011) and cleaned using the jusText library.\\n\\n- **mC4**: The multilingual colossal, cleaned version of Common Crawl's web crawl corpus (mC4) was introduced by Xue et al. (2021) for training the mT5 models. mC4 was derived from the 71 web scrapes (2013-2020) released by Common Crawl prior to the creation of the corpus. We use the Finnish subset of mC4 as identified by cld3, which contains 8 billion tokens across 19 million documents.\\n\\n- **CC-Fi**: To maximize coverage of Finnish text in Common Crawl resources, we applied a custom extraction process to all crawls from 2013-2022, emphasizing recall of Finnish. We extracted texts using Trafilatura (Barbaresi, 2021) and performed exact document-level deduplication using MurmurHash prior to the general preprocessing steps described below. This processing produced 55 million documents totaling 20 billion tokens.\\n\\n- **Fiwiki**: The Finnish portion of the Wikipedia free encyclopedia consists of approximately 180,000 openly licensed articles created by volunteer editors. For this work, we extracted text from the 20221120 dump of the Finnish Wikipedia using WikiExtractor (Attardi, 2015), producing a dataset of 110 million tokens.\\n\\n- **L\u00f6nnrot**: Projekti L\u00f6nnrot is a project digitizing out-of-copyright Finnish and Swedish literature. For this work, we used the 2574 Finnish works that were published by Projekti L\u00f6nnrot by the start of pretraining, which contain a total of 125 million tokens.\\n\\n- **Yle**: Archives of the national public broadcasting.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Data sources.\\n\\nThe complete Yle archives available at the start of our model pretraining, which consist of approximately 800,000 articles (220 million tokens) from 2011-2020, of which 0.3% are easy-to-read news. STT\\n\\nAs for Yle, archives of the Finnish News Agency (Suomen Tietotoimisto or STT) are provided for research through the Language Bank of Finland. The collection available at the start of this study spans publications from 1992-2018 and contains 2.8 million newswire articles which total approximately 300 million tokens.\\n\\nePub\\n\\nThe National Library of Finland maintains a collection of electronically published books in Finland. For the purposes of this project, the library granted access to its ePub collection of approximately 30,000 Finnish eBook contents. As these books remain copyrighted, it is not possible to redistribute texts from this dataset.\\n\\nLehdet\\n\\nThe Lehdet dataset is based on archived HTML material collected by the National Library of Finland and includes daily, weekly and monthly crawls of newspaper internet sites and also a yearly .fi-domain crawl covering years from 2015 to 2021. The total cleaned dataset consists of 85 billion characters from 60 million HTML documents. The dataset was provided by the National Library and can not be redistributed due to copyright.\\n\\nSuomi24\\n\\nArchives of the largest social networking site in Finland, Suomi24, are available for research via the Language Bank of Finland. For this study, we downloaded the complete archives available at the time, consisting of 95 million comments and 5 billion words from 2001-2020.\\n\\nReddit-Fi\\n\\nThe social site Reddit includes a few predominantly Finnish-language discussion forums. For this work, we downloaded Reddit archives and extracted text from posts to r/Suomi, the largest such forum. The dataset contains over 150,000 submissions and nearly 4 million comments (in total 150 million tokens) from 2009-2022.\\n\\nROOTS\\n\\nThe Responsible Open-science Open-collaboration Text Sources (ROOTS) dataset (Laur\u00e9n\u00e7on et al., 2022) consists of 1.6 terabytes of text data spanning 59 languages used for pretraining BLOOM (Scao et al., 2022a). While Finnish was not included as an official language, a contamination analysis found 0.03% of ROOTS to be Finnish (Muennighoff et al., 2022). We use ROOTS in the continued pretraining of the BLOOM model, but not for the monolingual Finnish models.\\n\\n3.2 Preprocessing\\n\\nWe next briefly describe the preprocessing steps performed for the source datasets. All processing scripts, parameters, and models are available along with detailed statistics at https://github.com/TurkuNLP/finngen-tools.\\n\\nDeduplication\\n\\nIn addition to the deduplication steps already performed for some of the datasets (see Section 3.1), we performed approximate N-gram overlap-based deduplication using Onion (Pomik\u00e1lek, 2011) separately for all datasets. We run Onion with default parameters, marking as duplicate any line of text (paragraph, title, etc.) where at least 50% of N-grams have appeared previously.\\n\\n8https://files.pushshift.io/reddit/\\n\\n9https://www.reddit.com/r/Suomi\"}"}
{"id": "emnlp-2023-main-164", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Preprocessed data statistics, weights, and ratios by source. The data is graphed in Appendix E.\\n\\nWe then trim duplicate lines from the beginning and end of each document. Finally, if at least 50% of the remaining lines in the document are duplicates, we discard the entire document.\\n\\nHeuristic filtering\\nTo filter out texts that are unlikely to be Finnish prose text, we apply a set of rule-based filters, extending on the heuristics introduced by Virtanen et al. (2019). In short, these filters remove texts that have e.g. an unusually high ratio of punctuation or digits to alphabetic characters, a high ratio of non-Finnish to Finnish alphabetic characters, a low type-token ratio, or a low average line length. This step removed only a small proportion of texts, with more than 95% of texts remaining in most resources.\\n\\nN-gram model filtering\\nTo further remove texts that have the surface characteristics of prose text but are unlikely to represent standard Finnish, we applied a perplexity filter using an N-gram model. We first trained a KenLM (Heafield, 2011) model on the set of known good Finnish texts prepared by Virtanen et al. (2019) for training their FinBERT model and then applied this model to documents, removing lines with perplexity > 100 000. This filter was not applied to sources estimated to be predominantly well-edited text (news, L\u00f6nnrot, and Wikipedia). For the three web crawl datasets, the filter removed 15-20% of text; for the social media datasets, this proportion was 2-5%.\\n\\nToxicity filtering\\nTo reduce the proportion of texts that contain e.g. obscenities or identity attacks, we applied the Finnish toxicity detection classifier introduced by Eskelinen et al. (2023). The classifier is a FinBERT model (Virtanen et al., 2019) fine-tuned on a machine-translated version of the Jigsaw Toxicity dataset.\\n\\nMasking personal data\\nWe applied a set of high-recall regular expressions and rule-based scripts to mask personal data such as email addresses and potential phone numbers. These scripts impacted approximately 0.2% of characters in total.\\n\\nTokenization\\nWe train a new monolingual Finnish tokenizer on a sample of the pretraining data using the tokenizers library. We follow the BLOOM recipe for the tokenizer, creating a byte-level BPE tokenizer without Unicode normalization and use the same regular expression-based pre-tokenization as in BLOOM. As Finnish is an agglutinative language with complex morphology and thus a high number of word forms, we chose to create a comparatively large vocabulary for a monolingual tokenizer of 131,072 tokens.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 5: Pretraining hyperparameters.\\n\\n| Model | Batch Size | Samples | Tokens | LR          |\\n|-------|------------|---------|--------|-------------|\\n| Small | 256        | 524288  |        | 6.0 \u00d7 10\u207b\u2074  |\\n| Medium| 256        | 524288  |        | 3.0 \u00d7 10\u207b\u2074  |\\n| Large | 256        | 524288  |        | 2.5 \u00d7 10\u207b\u2074  |\\n| XL    | 512        | 1048576 |        | 2.0 \u00d7 10\u207b\u2074  |\\n| 3B    | 512        | 1048576 |        | 1.6 \u00d7 10\u207b\u2074  |\\n| 8B    | 1024       | 2097152 |        | 1.2 \u00d7 10\u207b\u2074  |\\n| 13B   | 1024       | 2097152 |        | 1.0 \u00d7 10\u207b\u2074  |\\n\\n#### 3.4 Register analysis\\n\\nWe characterize the contents of the Web-based datasets (mC4, CC-Fi and Parsebank) by automatically analyzing their distribution of text registers (Biber, 1988). To this end, we apply a register identification model based on the FinCore corpus, trained using XLM-R (Conneau et al., 2020). The model and corpus were both presented by Skantsi and Laippala (2022). The register categories present text varieties with different characteristics and communicative objectives, such as narrative, interactive discussion and lyrical. Table 4 presents the proportions of the registers in the three datasets. We see a broadly similar register distribution across the datasets, with narrative clearly most frequent in all three and categories such as how-to, spoken and lyrical representing only small fractions of the total.\\n\\n#### 4 Pretraining\\n\\nThis work leverages the LUMI supercomputer, as of this writing the third-largest and seventh greenest in the world (Strohmaier et al., 2023). The LUMI data center allows power consumption to be fully supplied with hydroelectricity, and waste heat produced by LUMI is utilized by the city of Kajaani, providing up to 20% of the district heating.\\n\\nTraining was done on up to 192 nodes, each consisting of 4 AMD Instinct MI250X GPUs, a single 64-core AMD Trento CPU and 512GB of memory. Since the MI250X GPU is a multi-chip module with two Graphics Compute Dies (GCDs), each node can be considered to have 8 GPUs in total. In this perspective, the training utilized up to 1536 GPUs. The 64-core CPU is configured as 4 NUMA nodes linked to the GPUs. Because of a \\\"low noise\\\" mode used on the nodes, only 63 cores were available for training.\\n\\n![Loss curve](https://www.lumi-supercomputer.eu/)\\n\\nFigure 1: Validation losses with 5-point moving average smoothing.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We next present a few-shot evaluation dataset for Finnish and compare the capability of the models using this data. We additionally assess model alignment, bias, and toxicity in separate evaluations.\\n\\n5.1 FIN-bench dataset\\n\\nBIG-bench (Srivastava et al., 2022) is a collection of tasks created to assess various aspects of model capabilities. For this study, we created a similar Finnish evaluation dataset, FIN-bench, based on a BIG-bench subset augmented with newly introduced tasks. The tasks were primarily generated by machine translating the text of the equivalent BIG-bench tasks and subsequently correcting any translation errors as well as assuring that the questions remain culturally relevant to Finnish. Exceptions include the Arithmetic tasks (generated data) and new tasks (Paraphrase, Analogy, Emotions). The FIN-bench dataset contains 3919 examples in total, divided over the tasks described briefly below. Examples of the tasks can be found from Appendix G.\\n\\n- **Analogy**: Analogies of the type *Paris is to France as Helsinki is to ...* represent a well-established approach for evaluating language models. We created an analogy dataset using templates to reformulate analogy quadruples into natural language questions. We created 130 examples from the dataset of Venekoski and Vankka (2017) and the data of Mikolov et al. (2013) translated to Finnish.\\n\\n- **Arithmetic**: tests the degree to which a model has acquired an ability to perform basic one- to five-digit addition, subtraction, multiplication and division. The Finnish variant of the task was automatically generated by manually translating the templates in the scripts for the corresponding BIG-bench task and consists of 1923 examples in total.\\n\\n- **Cause and effect**: evaluates a model's ability to reason about the causality of two events. Each example states two events, the cause and the effect, and the model is asked to select the correct ordering. The task consists of 153 examples.\\n\\n- **Emotions**: evaluates the ability of a model to classify sentences according to the emotion that they express. The task is derived from the XED dataset (\u00d6hman et al., 2020) by selecting examples of at least five words that have exactly one emotion label and then manually filtering a random selection of these to identify 160 examples that a human annotator without reference to specific annotation instructions would be expected to label correctly.\\n\\n- **Empirical judgments**: measures how well a model can distinguish sentences that express a causal relation from ones that express a correlative relation. The task also contains neutral passages of text that mimic the structure of the sentences containing a correlative or causal relation, but do not contain either. There are 33 examples of each category in the task, i.e. 99 in total.\\n\\n- **General knowledge**: measures the ability of models to answer simple questions which can easily be answered by most people, such as \\\"How many legs does a horse have?\\\". The task is a translation of the 70 examples in the BIG-bench original for all but three questions regarding imperial unit conversion, which we replace with questions on metric units.\\n\\n- **Intent recognition**: tests the logical reasoning of models by measuring how well they can recognize the correct intent from an input. The task may be a good predictor of performance in task-oriented dialogue systems. It includes 693 translated examples originally from the dataset introduced by Coucke et al. (2018).\\n\\n- **Misconceptions**: assesses a model's ability to distinguish popular misconceptions from facts; models trained on increasingly bigger datasets of mixed-quality internet data may not discern between common assertions and ones that are true. Translations of this task were heavily filtered by our annotators due to being considered culturally too U.S.-centric. Approximately 40% of the original questions were removed from the dataset, resulting in a task with 134 examples.\\n\\n- **Paraphrase**: tests whether a model can distinguish full paraphrases from sentences that are merely similar. The task was created by selecting 100 positive and 100 negative examples from the Finnish Paraphrase Corpus (Kanerva et al., 2021), emphasizing cases that people can categorize without reference to the specifics of the corpus annotation guidelines.\\n\\n- **Sentence ambiguity**: evaluates to what degree a model can identify whether sentences with intentionally introduced ambiguous aspects state a true or false claim. The task consists of 60 examples translated from BIG-bench.\\n\\n- **Similarities abstraction**: measures a model's ability to identify human-like abstract associations between objects: for example, a dog and a parakeet are similar in that they are both pets. The data consists of 76 multiple-choice questions.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2 Few-shot results\\n\\nWe evaluate models on FIN-bench in zero- to three-shot settings and summarize results using mean accuracy across all tasks. For tasks that are organized into subtasks (Cause and effect and Arithmetic), we first average over the subtasks before taking the overall average. Primary evaluation results are visualized in Figure 2.\\n\\nWe find that our monolingual models at least match and in most instances outperform the results of previously released Finnish models of comparable sizes, lending support to the choices we have made for data selection and preprocessing as well as the model architecture and pretraining process. The best performance of the models released previously for Finnish, 38.5%, is achieved by the largest model introduced by Hatanp\u00e4\u00e4 (2022). Our best monolingual model outperforms this result by over 10% points and the BLUUMI model by over 20% points, representing a substantial advance in the state of the art in the capability of generative models trained for Finnish.\\n\\nAs expected, overall performance generally increases with the number of in-context examples (zero to three shots) as well as with model size, with some exceptions. First, some small models break the expected pattern, showing better zero-shot performance than one- to three-shot. This could be related to a tendency of less capable models to simply repeat patterns from preceding context, which can lead the models to copy whatever appears after \\\"Answer:\\\" (or equivalent) in the preceding few-shot examples. Second, we notice a consistent drop in performance between our 8B and 13B parameter models. This may be caused by overfitting due to an excessive number of parameters and training steps compared to a relatively small amount of (non-repeated) text, which can lead to decreasing performance (Muennighoff et al., 2023b). Based on these results, we estimate that the 8B parameter model may be our most capable monolingual model and, more generally, that approximately 10B parameters may represent a limit for effectively training monolingual models of this type for languages whose resources are broadly comparable to those available for Finnish.\\n\\nTo further evaluate the BLUUMI model, we compared its performance to that of the original BLOOM model on FIN-bench (Figure 3) and on English tasks from the EleutherAI evaluation...\"}"}
{"id": "emnlp-2023-main-164", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We find that BLUUMI performs notably better than BLOOM on FIN-bench tasks on all the few-shot evaluation tests, with a 12-18% point accuracy difference in favor of BLUUMI. On the English tasks, we find no significant difference in performance between the original BLOOM and BLUUMI (two-sided t-test). These results indicate that the continued pre-training has succeeded in substantially improving the Finnish capabilities of the model without compromising the existing English capabilities of the original model.\\n\\n5.3 Alignment\\n\\nWe assess model alignment using the BIG-bench HHH alignment task (Askell et al., 2021), which includes four categories: harmlessness, honesty, helpfulness, and other. In contrast to most other tasks in BIG-bench, both of the two choices in each example can be considered correct: for instance, when assessing harmlessness, it is undesirable for a model to provide instructions for violent acts, and refusing to help is considered the correct answer.\\n\\nWe create a Finnish version of the HHH alignment task through initial machine translation and manual correction, and evaluate models using the same process as for the other BIG-bench tasks. Results are shown in Figure 5. We find that all models perform poorly at these tasks, only exceeding the random baseline for the other category and measuring particularly low for helpfulness. While it is not surprising that base models that have not been specifically trained to follow instructions or operate in a dialogue context score low at this task, the results emphasize the need to align the models to assure that their output is helpful, harmless, and more factually accurate. We note that although there appear to be some correlations between model size and HHH performance, all differences remain within one standard deviation and are not significant.\\n\\n5.4 Bias\\n\\nLanguage models have an established tendency to repeat or amplify biases present in training data. As one example of bias, female/male gender stereotypes in models is a concern because their widespread use can result in further amplifying these biases (Bolukbasi et al., 2016). We assessed the occurrence of such bias using prompts with the structure \\\"The name of the [professional or occupation holder] was\\\" and categorized predicted names into male or female when the name had that association in 95% of cases in national statistics. The distribution predicted by the model was then compared to the distribution in the most recent published labor data records published by Statistics Finland in 2020.\\n\\nAs illustrated in Figure 6 and detailed in Appendix C, the model broadly reflects the actual labor distribution, indicating that...\"}"}
{"id": "emnlp-2023-main-164", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Gender bias of 13B model predictions on occupation holder vs statistics from the Statistics Finland. It has learned this bias from the pretraining data. We note that while this is just one example of a type of bias that our models (as well as most other present-day models) can learn in their pretraining, it demonstrates why such models should not be naively applied e.g. for hiring decisions (see also Limitations below).\\n\\n5.5 Toxicity\\n\\nTo test to what degree our models are prone to generating toxic content, we follow the unprompted generation approach of Gehman et al. (2020), prompting the models with only their end-of-sequence (EOS) token to signal the start of a new context. The unprompted generations were then classified for toxic content using the model introduced by Eskelinen et al. (2023) (see also Section 3.2) and a small sample manually assessed to assure labeling quality. The results of this evaluation are summarized in Figure 7. We find that our models more than halve the fraction of generated toxic content when compared to models from Hatanp\u00e4\u00e4 (2022), which were trained without filtering pretraining texts for toxicity. Our models nevertheless produce unprompted toxic generations approx. 2% of the time, reflecting remaining challenges in their alignment.\\n\\n6 Discussion and conclusions\\n\\nIn this study, we compiled an extensive dataset of Finnish and created in total eight new large language models: seven monolingual Finnish models ranging from 185 million to 13 billion parameters and a multilingual 176-billion parameter model, BLUUMI. We additionally introduced a new evaluation dataset, FIN-bench, and evaluated the models in few-shot settings as well as specifically assessed their alignment, bias and toxicity. We found that our models are substantially more capable than prior Finnish models and that continued pretraining has greatly improved the Finnish capability of BLUUMI without compromising its existing English capabilities. We also demonstrated limitations of the models in terms of their alignment, incorporation of bias, and remaining tendency to generate toxic content, which we aim to address in future work. We hope our models will serve as foundation models for Finnish that can be used in research and leveraged through instruction finetuning and other alignment methods (Ouyang et al., 2022) to create a range of capable tools for processing Finnish text.\\n\\nIn future work, we hope to continue our study of efficient and environmentally sustainable approaches for creating capable open foundation models for lesser-resourced languages.\\n\\nAcknowledgments\\n\\nThe authors wish to acknowledge CSC \u2013 IT Center for Science, Finland, for generous computational resources on the LUMI supercomputer. This project has received funding from the European Union's Horizon Europe research and innovation programme under Grant agreement No 101070350 and the Finnish Research Council, grant number 331297. The contents of this publication are the sole responsibility of its authors and do not necessarily reflect the opinion of the European Union.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThe models introduced in this work are trained predominantly on data sourced from the internet, and despite our efforts to remove potentially harmful texts from the pretraining data, they carry many of the well-established limitations of such models (Bender et al., 2021; Weidinger et al., 2021). In our evaluation, we have experimentally demonstrated specific limitations in terms of model alignment (Section 5.3), bias (Section 5.4), and toxicity (Section 5.5). While the introduced models notably improve over the capabilities of previously released models in a range of Finnish tasks, due to these and other limitations the models should primarily be considered resources for research and a potential foundation for tools and applications, but they should not be used as-is for user-facing applications or for any task with potential for high impact on people's rights or well-being, such as hiring decisions. Substantial further work is likely to be required to create versions of the models that can be assured to be well aligned, free of bias, and not prone to generating toxic output.\\n\\nOur work focuses on large models for a lesser-resourced language, and the amount of Finnish text available for model pretraining is a fundamental limitation of our work. Despite drawing on a broad range of sources, it was not possible to assemble enough text to avoid multiple epochs over the data to match the GPT-3 pretraining process, and the repetition of data may be reflected in reduced capability, especially for the largest monolingual model (Section 5.2). The challenges of collecting sufficient high-quality Finnish text for large model training also forced us to make a choice between data quality and quantity on the one hand and replicability on the other. We chose to partly train on texts provided by the National Library of Finland as part of a research collaboration. While these are some of the highest-quality texts in our dataset, they cannot be readily redistributed, and complete replication of our work is thus impossible without the involvement of the national library. While we regret this limitation, we note that lack of access to complete pretraining data is a negative aspect that our models share with many other present-day models. Future work may consider increasing the available data via augmentation techniques (Dhole et al., 2021) or mixing with data from a different modality such as code (Muennighoff et al., 2023b,a; Allal et al., 2023; Li et al., 2023).\\n\\nReferences\\n\\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, et al. 2023. Santacoder: don't reach for the stars! arXiv preprint arXiv:2301.03988.\\n\\nAmanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al. 2021. A general language assistant as a laboratory for alignment. arXiv preprint arXiv:2112.00861.\\n\\nGiusepppe Attardi. 2015. Wikiextractor. https://github.com/attardi/wikiextractor.\\n\\nAdrien Barbaresi. 2021. Trafilatura: A web scraping library and command-line tool for text discovery and extraction. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations, pages 122\u2013131.\\n\\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623.\\n\\nDouglas Biber. 1988. Variation across speech and writing. Cambridge University Press, Cambridge.\\n\\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco \u2013\"}"}
{"id": "emnlp-2023-main-164", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-164", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. arXiv preprint arXiv:2203.02155.\\n\\nJan Pomik\u00e1lek. 2011. Removing boilerplate and duplicate content from web corpora. Ph.D. thesis, Masaryk university, Faculty of informatics, Brno, Czech Republic.\\n\\nOfir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation. arXiv preprint arXiv:2108.12409.\\n\\nSampo Pyysalo, Jenna Kanerva, Antti Virtanen, and Filip Ginter. 2021. Wikibert models: Deep transfer learning for many languages. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pages 1\u201310.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. 2018. Improving language understanding by generative pre-training.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pages 3505\u20133506.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Elodie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Romain Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. 2022a. Bloom: A 176b-parameter open-access multilingual language model.\\n\\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Lucile Saulnier, Stas Bekman, M Saiful Bari, Stella Bideman, Hady Elsahar, Niklas Muennighoff, Jason Phang, et al. 2022b. What language model to train if you have one million gpu hours? arXiv preprint arXiv:2210.15424.\\n\\nMohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. 2019. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053.\\n\\nValtteri Skantsi and Veronika Laippala. 2022. Analyzing the unrestricted web: The finnish corpus of online registers. Nordic Journal of Linguistics.\\n\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2022. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. arXiv preprint arXiv:2206.04615.\\n\\nErich Strohmaier, Jack Dongarra, Horst Simon, Martin Meuer, and Hans Meuer. 2023. Top500 - the list. https://www.top500.org/.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30.\\n\\nViljami Venekoski and Jouko Vankka. 2017. Finnish resources for evaluating language model semantics. In Proceedings of the 21st Nordic Conference on Computational Linguistics, NoDaLiDa, 22-24 May 2017, Gothenburg, Sweden, 131, pages 231\u2013236. Link\u00f6ping University Electronic Press, Link\u00f6pings universitet.\\n\\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and Sampo Pyysalo. 2019. Multilingual is not enough: Bert for finnish. arXiv preprint arXiv:1912.07076.\\n\\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, et al. 2021. Ethical and social risks of harm from language models. arXiv preprint arXiv:2112.04359.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mt5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.\\n\\nZheng-Xin Yong, Hailey Schoelkopf, Niklas Muenighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, et al. 2022. BLOOM+ 1: Adding language support to bloom for zero-shot prompting. arXiv preprint arXiv:2212.09535.\"}"}
{"id": "emnlp-2023-main-164", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Timespan covered by Finnish datasets\\n\\nThe rough timespan covered by the Finnish datasets is summarized in the following figure, excluding the L\u00f6nnrot dataset (0.4% of the data), which covers out-of-copyright literature and mostly consists of books published before 1950. Due to the difficulty of assigning a publication date to web-based materials that may be continuously edited, for these resources we report the timespan of their retrieval.\\n\\n| Year | Dataset 1 | Dataset 2 | Dataset 3 | Dataset 4 | Dataset 5 | Dataset 6 | Dataset 7 | Dataset 8 | Dataset 9 | Dataset 10 | Dataset 11 | Dataset 12 | Dataset 13 | Dataset 14 |\\n|------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| 1990 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1991 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1992 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1993 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1994 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1995 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1996 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1997 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1998 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 1999 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2000 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2001 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2002 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2003 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2004 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2005 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2006 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2007 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2008 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2009 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2010 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2011 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2012 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2013 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2014 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2015 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2016 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2017 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2018 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2019 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2020 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2021 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n| 2022 |           |           |           |           |           |           |           |           |           |           |           |           |           |           |\\n\\nComparison of mC4-Fi and CC-Fi datasets\\n\\nThe mC4-Fi and CC-Fi datasets are both derived from Common Crawl data, but cover different sets of crawls and apply different selection criteria and text extraction and filtering pipelines. To assess the overlap of these two datasets after preprocessing, we first compared the sets of URLs in the metadata of the two datasets, finding that 65% of the mC4-Fi URLs are also found in CC-Fi, while only 29% of CC-Fi URLs are also in mC4-Fi, indicating substantial differences in which documents are included and suggesting that the processing to create the CC-Fi dataset was successful in increasing coverage of Finnish documents selected from Common Crawl resources compared to mC4-Fi.\\n\\nTo further assess textual overlap, we first sampled 100,000 random URLs found in both datasets. For each URL we created the set of 5-grams from the document texts in mC4-Fi and CC-Fi as well as their intersection. We found that 73% of 5-grams in mC4-Fi overlap with those of the corresponding document in CC-Fi, and 84% of CC-Fi 5-grams appeared also in the mC4-Fi document. This indicates that while the texts extracted from each matching document are highly similar in the two resources, they are not identical, and the redundancy of these resources is thus lower than suggested by simple URL overlap.\\n\\nFull gender bias results on 13B model\\n\\n| Occupation      | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (% | F (%) | Predicted | M (%"}
{"id": "emnlp-2023-main-164", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Occupation                        | Employment stats | Predicted | Predicted employment % |\\n|----------------------------------|------------------|-----------|-------------------------|\\n| software engineer                | 25,110           | 433       | 85.91%                  |\\n| kindergarten teacher              | 6,560            | 69        | 13.80%                  |\\n| software architect                | 15,220           | 291       | 89.26%                  |\\n| agriculture machinist             | 18,090           | 423       | 98.14%                  |\\n| accountant                        | 6,445            | 230       | 97.87%                  |\\n| teaching assistant                | 2,314            | 1         | 0.26%                   |\\n| carpenter                         | 15,870           | 228       | 95.40%                  |\\n| driver                            | 14,006           | 281       | 96.23%                  |\\n| building electrician              | 14,084           | 513       | 100.00%                 |\\n| plumber                           | 13,618           | 455       | 100.00%                 |\\n| senior physician                  | 5,505            | 204       | 90.67%                  |\\n| store manager                     | 4,661            | 371       | 85.68%                  |\\n| machinist                         | 11,868           | 217       | 92.74%                  |\\n| farmer                            | 10,331           | 295       | 84.53%                  |\\n| study advisor                     | 3,498            | 7         | 1.36%                   |\\n| hairdresser                       | 867              | 1         | 0.26%                   |\\n| mailman                           | 6,503            | 163       | 90.56%                  |\\n| coffee shop worker                | 1,927            | 51        | 25.00%                  |\\n| real estate agent                 | 6,496            | 114       | 46.91%                  |\\n| bus driver                        | 9,099            | 335       | 91.28%                  |\\n| guardsman                         | 7,496            | 160       | 91.43%                  |\\n| bank worker                       | 2,145            | 274       | 84.31%                  |\\n| electrician                       | 9,343            | 480       | 100.00%                 |\\n| physiotherapist                   | 2,008            | 73        | 29.55%                  |\\n| sales engineer                    | 6,422            | 434       | 93.13%                  |\\n| waiter                            | 2,191            | 52        | 42.98%                  |\\n| special education teacher         | 1,223            | 48        | 10.60%                  |\\n| careers adviser                   | 1,584            | 233       | 56.55%                  |\\n| storekeeper                       | 4,678            | 309       | 80.47%                  |\\n| physical education instructor     | 2,829            | 96        | 19.51%                  |\\n| office secretary                  | 230              | 150       | 30.18%                  |\\n| purchasing agent                  | 4,066            | 140       | 76.09%                  |\\n| physician                         | 2,882            | 251       | 84.80%                  |\\n\\n- 1.10.0-1.10.0\"}"}
{"id": "emnlp-2023-main-164", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Identity attack | Insult | Obscene | Severe toxicity | Threat | Toxicity |\\n|---------------|-----------------|-------|---------|-----------------|--------|----------|\\n| Hatanp\u00e4\u00e4/small | 0.149 %         | 1.471 % | 2.132 % | 0.070 %         | 0.026 % | 5.377 %  |\\n| Hatanp\u00e4\u00e4/xl   | 0.185 %         | 1.344 % | 2.055 % | 0.109 %         | 0.015 % | 5.241 %  |\\n| TurkuNLP/small | 0.039 %         | 0.208 % | 0.435 % | 0.004 %         | 0.008 % | 1.658 %  |\\n| TurkuNLP/medium| 0.048 %         | 0.248 % | 0.410 % | 0.002 %         | 0.011 % | 1.896 %  |\\n| TurkuNLP/large | 0.039 %         | 0.280 % | 0.490 % | 0.001 %         | 0.011 % | 1.981 %  |\\n| TurkuNLP/xl   | 0.061 %         | 0.272 % | 0.546 % | 0.002 %         | 0.011 % | 2.211 %  |\\n| TurkuNLP/3B   | 0.069 %         | 0.343 % | 0.618 % | 0.004 %         | 0.021 % | 2.290 %  |\\n| TurkuNLP/8B   | 0.058 %         | 0.304 % | 0.645 % | 0.012 %         | 0.021 % | 2.317 %  |\\n| TurkuNLP/13B  | 0.065 %         | 0.309 % | 0.637 % | 0.005 %         | 0.016 % | 2.374 %  |\"}"}
{"id": "emnlp-2023-main-164", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: 3-shot results of each FIN-bench task + HHH\"}"}
