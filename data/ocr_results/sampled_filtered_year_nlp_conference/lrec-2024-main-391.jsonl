{"id": "lrec-2024-main-391", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Drift in Clinical Outcome Prediction from Admission Notes\\nPaul Grundmann \u2217, Jens-Michalis Papaioannou \u2217, Tom Oberhauser \u2217, Thomas Steffek \u2217, Amy Siu \u2217, Wolfgang Nejdl \u2020, Alexander L\u00f6ser \u2217\\n\\n\u2217 Berliner Hochschule f\u00fcr Technik (BHT) - Luxemburger Stra\u00dfe 10, 13467 Berlin\\npgrundmann, toberhauser, mpapaioannou, tsteffek, asiu, aloeser@bht-berlin.de\\n\\n\u2020 Leibniz University Hannover - Welfengarten 1, 30167 Hannover\\nnejdl@L3S.de\\n\\nAbstract\\nClinical NLP research faces a scarcity of publicly available datasets due to privacy concerns. MIMIC-III marked a significant milestone, enabling substantial progress, and now, with MIMIC-IV, the dataset has expanded significantly, offering a broader scope. In this paper, we focus on the task of predicting clinical outcomes from clinical text. This is crucial in modern healthcare, aiding in preventive care, differential diagnosis, and capacity planning. We introduce a novel clinical outcome prediction dataset derived from MIMIC-IV. Furthermore, we provide initial insights into the performance of models trained on MIMIC-III when applied to our new dataset, with specific attention to potential data drift. We investigate challenges tied to evolving documentation standards and changing codes in the International Classification of Diseases (ICD) taxonomy, such as the transition from ICD-9 to ICD-10. We also explore variations in clinical text across different hospital wards. Our study aims to probe the robustness and generalization of clinical outcome prediction models, contributing to the ongoing advancement of clinical NLP in healthcare.\\n\\nKeywords: Corpus (Creation, Annotation, etc.), Document Classification, Text categorisation, Neural language representation models\\n\\n1. Introduction\\nIn the realm of clinical NLP research, publicly available datasets are a rarity, primarily due to privacy and ethical concerns. The pivotal release of MIMIC-III (Johnson et al., 2016) has been decisive for the reproducibility of research results and progress in the area of clinical NLP. A tremendous amount of influential research was published based on the dataset. To support such research efforts, MIMIC-IV (Johnson et al., 2020) was released, expanding the dataset to include patient data up to 2019. This release increased the number of unique patients by approximately six times, providing a richer and more extensive dataset. Recently, van Aken et al. (2021) introduced the task of predicting clinical outcome from clinical text, only incorporating information available at admission time. Predicting clinical outcome is essential in modern healthcare. It serves as a preventive tool, aiding doctors during the differential diagnosis process by identifying potential risks and symptoms, as well as assisting hospitals in proactive capacity planning. In this investigation, we present a clinical outcome prediction dataset derived from MIMIC-IV. Additionally, we offer initial observations regarding the efficacy of models trained on MIMIC-III when applied to our dataset, paying particular attention to data drift from the inclusion of more recent documents from MIMIC-IV.\\n\\nIn addition to understanding complex linguistic aspects like relationships, ambiguity, negations, abbreviations, and other language intricacies, clinical documentation of diagnoses and procedures presents extra challenges for these models when applied to real-world tasks. To determine the correct diagnosis and procedure codes that are associated with the patients\u2019 clinical note, professional coders need to manually assign codes, organized in a standardized taxonomic hierarchy like ICD-9, ICD-10 or ICD-11 (International Statistical Classification of Diseases and Related Health Problems (ICD)). The ICD system is widely established and is used for billing and clinical documentation purposes. Searle et al. (2020) highlight that clinical text, combined with the often stringent time constraints placed on clinical coders, adds to the likelihood of errors and inconsistencies in this process. Moreover, the ICD-Code taxonomy is updated regularly, which results in inconsistencies between each major revision. The goal of the regular updates is to capture an expanding range of health conditions, procedures, and therapies with increasing granularity. Major updates to the ICD system are not fully backwards compatible, which makes using clinical data across time challenging. We present the first analysis of different aspects of the generalization and robustness capabilities of state-of-the-art models trained on MIMIC-III data and evaluated on MIMIC-IV. Our evaluation focuses on the following aspects:\\n\\n\u2022 The implementation of the major revision of ICD-9 to ICD-10 that is partially present in the MIMIC-IV data\\n\\nhttps://www.who.int/standards/classifications/classification-of-diseases\"}"}
{"id": "lrec-2024-main-391", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The effect of changes in clinical documentation standards or guidelines\\n\\nThe different hospital wards in which the text is produced, i.e. the emergency department and the ICU.\\n\\nContribution\\n\\nWe summarize our contribution as follows:\\n\\n1. Generation of an outcome prediction dataset utilizing MIMIC-IV data\\n2. Application of techniques to establish correspondence between existing MIMIC-III documents within MIMIC-IV, facilitating comparability for models previously trained on the MIMIC-III outcome prediction task.\\n3. Rigorous selection of dataset splits designed to facilitate the reuse of the original MIMIC-III test dataset for outcome prediction purposes (van Aken et al., 2021).\\n4. Evaluation and comparison of existing state-of-the-art models for clinical outcome prediction with regard to data drift.\\n\\nRelated Work\\n\\nMIMIC-III\\n\\nThe freely available Medical Information Mart for Intensive Care v1.4 database, also known as MIMIC-III (Johnson et al., 2016), fueled medical computational science research since 2016 with thousands of citations and provided a valuable foundation to an abundance of publications. MIMIC-III contains de-identified electronic health record data including textual discharge summaries in English of the Beth Israel Deaconess Medical Center (BIDMC) in Massachusetts ranging from 2001 to 2012.\\n\\nMIMIC-IV\\n\\nThe MIMIC-IV (Johnson et al., 2020) dataset is the successor to MIMIC-III and contains MIMIC-III data from 2008 to 2012 as well as new data collected from 2013 to 2019. Furthermore, in addition to ICU data, it also contains data from the BIDMC emergency department. The incorporation of emergency department data significantly enhances the phenotypic diversity within MIMIC-IV, capturing a broader spectrum of patient profiles, including those not requiring critical care interventions.\\n\\nCoding Differences Between MIMIC-III and MIMIC-IV\\n\\nMIMIC-III uses the ICD-9 coding standard to encode diagnoses and procedures. MIMIC-IV relies on the newer ICD-10 standard for data collected between 2013 and 2019, as well as ICD-9 for data before 2013. While ICD-9 contains 3,878 procedure as well as 14,567 diagnosis codes, the much more fine-grained ICD-10 contains 71,920 procedure and 69,832 diagnosis codes. Researchers and practitioners who seek to evaluate tasks that rely on the medical coding information, like clinical outcome prediction, have to tackle issues that come with multiple coding standards, e.g. ambiguous label spaces. One approach is to perform independent evaluations on each respective label space, e.g. Bornet et al. (2023) use only the documents annotated with ICD-10 codes. However, depending on the type of code (diagnostic or procedures) MIMIC-IV contains up to 66.7% of ICD-9 coded data which forms the majority of the dataset (s. Section 3 for more details). To enable comparability of models on both coding standards present in MIMIC-IV, they need to be combined into a common label space. For ICD-10 as well as ICD-9, there are sophisticated mapping mechanisms provided by the Centers for Medicare and Medicaid Services as well as the Centers for Disease Control and Prevention called \\\"General Equivalency Mappings\\\" (GEMs).\\n\\nClinical Outcome Prediction Task\\n\\nvan Aken et al. (2021) proposed a task to predict a patient's outcome from a clinical note written at admission time. They proposed a dataset based on MIMIC-III with four tasks: 1. Diagnosis Prediction, 2. Procedure Prediction, 3. Length-of-Stay and 4. In-Hospital Mortality Prediction. With the help of trained medical professionals, they chose to extract a list of sections from the discharge summaries from MIMIC-III that are known at admission time. The resulting documents, called Admission Notes, only contain information that is likely to represent the knowledge about a patient's state at hospital admission. Thus, they can be used to evaluate the task of clinical outcome prediction. This differentiates the task of clinical outcome prediction from the task of ICD coding (Edin et al., 2023; Mullenbach et al., 2018), where the full document is used.\\n\\nClinical Outcome Prediction Approaches\\n\\nTo solve the outcome prediction task, several approaches have been proposed, such as the CORe (Clinical Outcome Representations) model (van Aken et al., 2021) that aims to learn clinical outcome representations of admission notes by continuing pre-training on discharge summaries. Besides the continued work of the authors, like using prototypical networks for more interpretable predictions (van Aken et al., 2022) or behavioral testing.\"}"}
{"id": "lrec-2024-main-391", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PRESENT ILLNESS: 58yo woman w/ hx of hypertension, AFib on coumadin\\npresented to ED with headache and chest pain. Husband reports states that patient\\nhas been complaining of headache for 1 days, chest pain for 3 days and has lost\\nconsciousness 2 days ago for a minute\\n\\nMEDICATION ON ADMISSION: The Preadmission Medication list is accurate and\\ncomplete. 1. Aspirin 120 mg PO DAILY, 2. Simvastatin 20 mg PO QPM\\n\\nPHYSICAL EXAM: Vitals: P: 82 R: 12 BP: 140/75 SaO2: 95%\\nCardiac: RRR\\natraumatic, normocephalic Pupils: 4-3mm. Abd: Soft, BS+ Extrem:\\nWarm and well-perfused.\\n\\nFAMILY HISTORY: non-contributary\\n\\nSOCIAL HISTORY: Lives together with husband\\n\\nFigure 1: Example of an admission note from MIMIC-IV .\\n\\nvan Aken et al. (2022), there have been many\\nworks on improving on the task. Winter et al. (2022)\\nenhance encoder models with external in-\\nformation from knowledge graphs by injecting it\\ninto redundant attention heads. Naik et al. (2022)\\naugment the classification process by adding re-\\ntrieved documents from PubMed. Grundmann\\net al. (2022) augment encoder models to use op-\\ntional previously known diagnosis codes to sup-\\nport the prediction. Taylor et al. (2023) employ a\\nprompt learning approach. Ji and Marttinen\\n(2021) use task-specific embeddings. Papaioannou et al.\\n(2022) enhance the encoders by applying cross-\\nlingual knowledge transfer. Also, the outcome pre-\\ndiction dataset proposed by van Aken et al.\\n(2021) was used for numerous other tasks besides clinical\\noutcome prediction, e.g. evaluation of trustworthi-\\nteness of synthetic data (Belgodere et al., 2023)\\nor analyzing the interpretability of classifiers (Naylor\\net al., 2021). To the best of our knowledge, the\\napproach of van Aken et al. (2022), based on pro-\\ntotypical networks, provides the best performance\\non the diagnosis prediction task.\\n\\n3. PREPARING THE DATASET\\nMIMIC-III consists of in total 53,423 patient stays\\nand 38,597 unique patients, resulting in an aver-\\nage of 1.38 stays per patient. It covers a time span\\nfrom 2001 until 2012. The MIMIC-IV (Johnson\\net al., 2020) dataset can be understood as the suc-\\ncessor to MIMIC-III and contains MIMIC-III data\\nfrom 2008 to 2012 as well as new data collected\\nfrom 2013 to 2019. Furthermore, in addition to ICU\\ndata (66,239 stays with 50,920 unique pa-\\ntients), it also contains data from the BIDMC emergency de-\\npartment (431,231 stays with 180,733 unique pa-\\ntients).\\n\\n3.1. EXTRACTION OF AdMISSION Notes\\nFollowing van Aken et al. (2021) we adapt the ex-\\ntraction methodology and extract the following sec-\\ntions from the discharge summaries in MIMIC-IV:\\nChief Complaint, Present Illness, Medical History,\\nAdmission Medications, Allergies, Physical exam,\\nFamily History and Social History. In contrast to\\nMIMIC-III, the section Social History is often not\\npresent in MIMIC-IV . We provide an example of an\\nadmission note, modified for anonymity reasons in\\nFigure 1.\\n\\n3.2. MATCHING\\nMIMIC-IV contains parts of MIMIC-III due to the\\noverlap during the years 2008-2012. To establish\\ncomparability with the outcome prediction dataset\\nby van Aken et al. (2021), it is necessary to ex-\\nclude any document from the MIMIC-III training\\ndataset split in our MIMIC-IV test dataset splits.\\nTherefore, we need to identify the respective docu-\\nments from MIMIC-III in MIMIC-IV . However, iden-\\ntifying the content of MIMIC-III in MIMIC-IV is not\\ntrivial. First, all the unique identifiers, e.g. of pa-\\ntients and admissions, have been re-generated so\\nthat no direct match is possible, and it is unknown\\nwhether the entire ICU subset from that timeframe\\nis part of MIMIC-III or not. Second, MIMIC-IV\\nchanged the de-identification process. Instead\\nof replacing HIPAA (Health Insurance Portability\\nand Accountability Act) defined data by random\\nidentifiers, all discriminating data is replaced with\\nthree underscores. Furthermore, MIMIC-IV only\\ncontains information about the year of a patients'\\nadmission instead of the day, week and season\\nidentifiers present in MIMIC-III. Patients are now\\ngrouped by a so-called anchor-year-group (e.g.\\n2011-2013) that indicates when the patients' first\\nadmission took place. MIMIC-IV contains four an-\\nchor year groups in total: 2008-2010, 2011-2013,\\n2014-2016 and 2017-2019. Because the data col-\\nlection for MIMIC-III stopped in 2012, there can-\\nnot be any MIMIC-III admission in the anchor-year-\\ngroups after 2013. In consequence, we focus\\non the groups 2008-2010 and 2011-2013 which\\nmust contain admissions from MIMIC-III. However,\\nthis data also includes all patients and admis-\\nsions from 2013, not present in MIMIC-III. Furth-\\nmore, Johnson et al. (2022) provide a dataset split\\ncalled \\\"MIMIC-III Clinical Database CareVue sub-\\ncset\\\" based on MIMIC-III that is guaranteed to be\\nnot part of MIMIC-IV and mostly consists of docu-\\nments from before 2008. It uses the same unique\\nidentifiers as MIMIC-III. By removing every docu-\\nment from MIMIC-III that is also part of CareVue,\\nwe can reduce the number of false positives result-\\ning from our matching approaches. The resulting\\ndifference contains 23,294 patients and 32,140 ad-\\nmission notes. We use the following two matching\\napproaches to identify those remaining MIMIC-III\\nadmission notes in MIMIC-IV:\"}"}
{"id": "lrec-2024-main-391", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Matching by Earliest Possible Year\\n\\nPatients are assigned an anchor-year-group according to their first admission. In addition, a patient receives an anchor year with added noise. Each admission of a patient has a discharge time feature that is based upon the anchor year. First, we identify patients in the first two anchor-year groups (2008-2010 and 2011-2013) and filter the admissions. We subtract the difference between the anchor year and the minimum of the anchor-year-group from the year of discharge of an admission. This enables us to filter out all admissions from before 2013. We refer to this matching approach in the following as \\\\textit{EP Y}.\\n\\nFeature-Based Matching by Length-Of-Stay, Diagnoses and Procedures\\n\\nTo increase the filtering precision, we match admissions that share the following features: Diagnoses, procedures and length-of-stay. We define the length-of-stay to be the difference between the discharge and admission time of a patient stay. Furthermore, we match by using the respective timestamp of the admission, but only use the hour, minutes and seconds as they are kept the same as in MIMIC-III. Year, month and day features are obscured in MIMIC-IV. We consider an admission to be part of MIMIC-III if all the mentioned features match. In the following, we refer to this matching approach as \\\\textit{F BM}.\\n\\nMatching Quality Estimation\\n\\nTo guarantee the precision of our chosen matching methods to identify matching admission notes in MIMIC-III and MIMIC-IV, we perform a range of validation experiments. In a quantitative evaluation, we choose random text sequences from the discharge notes of two matching admissions and evaluate whether we can find overlaps between the two. We find that feature-based matching of the admission does not contain false positive matches. Matching by earliest possible year \\\\textit{EP Y} overlaps mostly with the feature-based matching \\\\textit{F BM} but contains many false positives. Further, we perform a qualitative evaluation by analyzing random examples from matched admission notes. We find that the set \\\\textit{EP Y} \\\\textbackslash \\\\textit{F BM} contains different admission- and discharge timestamps for the matched admissions. We consider those admissions a false positive match. \\\\textit{F BM} \\\\textbackslash \\\\textit{EP Y} shows us that MIMIC-IV contains around 3,000 patients which are not in the ICU module but part of MIMIC-III. We also find 128 documents that are not matched via the \\\\textit{EP Y} matching. Finally, we find 60 examples in the set of MIMIC-III \\\\textbackslash MIMIC-III (\\\\textit{CareV ue} \\\\cap \\\\textit{EPY} \\\\cap \\\\textit{FBM}). Five examples of this set did not have discharge summaries, and 55 could not be matched. We conclude that the combination of our chosen matching methods provides sufficient precision that we can say that, up to our knowledge, there are no false positive matches resulting from our matching approaches. Using our matching we can identify 99.37\\\\% of the remaining subset in MIMIC-IV.\\n\\n3.3. Dataset Splits\\n\\nIn order to evaluate data drift, we create six different splits out of MIMIC-IV that we use for evaluation, presented in Table 1. \\\\textit{III} \\\\textbackslash \\\\textit{test} is the original test split of van Aken et al. (2021) which serves as a baseline. \\\\textit{IV} \\\\textbackslash \\\\textit{III} \\\\textbackslash \\\\textit{test} contains all the data from MIMIC-IV that we identified using our matching algorithm to be also present in the \\\\textit{III} \\\\textbackslash \\\\textit{test} dataset split. This split allows us to compare the effects of the new de-identification scheme used in MIMIC-IV.\\n\\n\\\\textit{IV} \\\\textit{HOSP} contains all the admissions from the emergency department / hospital module in MIMIC-IV. It excludes all ICU admissions. This split enables us to evaluate whether models are capable to generalize from seen ICU data to another clinical domain. Analogously, the emergency department split, \\\\textit{IV} \\\\textit{ICU} uses only the ICU data. This dataset also contains parts of the MIMIC-III training data that we use for training. Finally, \\\\textit{IV} \\\\textit{ICU} \\\\textbackslash \\\\textit{III} consists of all admissions that are not present in MIMIC-III. Therefore, it can be used to measure the data drift from 2001-2012 to the new data in MIMIC-IV from 2013-2019.\\n\\n3.4. General Equivalency Mappings\\n\\nShare of ICD-9 and ICD-10 Codes in MIMIC-IV\\n\\nMIMIC-IV contains data from 2008 until 2019. ICD-10 was introduced in 2012. In consequence, we find examples annotated with both ICD-9 and documents annotated with ICD-10 in MIMIC-IV. The dataset contains 58.2\\\\% ICD-9 diagnosis codes, and 66.7\\\\% ICD-9 procedures codes. The remaining codes are annotated in ICD-10 format. In order to enable comparable results, we establish a common label space. The available GEMs allow us to convert either from ICD-9 to ICD-10 or vice versa. As ICD-9 does not directly map to ICD-10 and ICD-10 is more fine-granular, the GEMs define four different match types: \\\"NO MAP\\\", \\\"IDENTICAL\\\", \\\"APPROXIMATE\\\" and \\\"COMBINATION\\\". \\\"NO MAP\\\" and \\\"IDENTICAL\\\" represent that there is no mapping between both codes, or that there exists a direct one-to-one mapping. \\\"COMBINATION\\\" often can be resolved automatically, as it means that a certain combination of codes lead to either a different combination of codes or a single code. The same applies for \\\"APPROXIMATE\\\" which means that often there is a one-to-one mapping that fails to be an identical match due to less specificity in the target system. However, \\\"COMBINATION\\\" and \\\"APPROXIMATE\\\" can also lead to non-automatically resolvable mappings that require additional processing.\"}"}
{"id": "lrec-2024-main-391", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Overview of the dataset statistics from MIMIC-III and different splits we choose to generate from the MIMIC-IV dataset.\\n\\nThis table shows the average number of diagnoses and procedures per admission and the average length of stay in days as well as the average mortality of an admitted patient.\\n\\n| Dataset | Diag./Adm. | Proc./Adm. | Length-of-Stay | Mortality |\\n|---------|------------|------------|----------------|-----------|\\n| III-\\\\(\\\\text{train}\\\\) | 11.17 | 4.16 | 10.14 days | 10.41% |\\n| III-\\\\(\\\\text{test}\\\\) | 11.27 | 4.10 | 10.00 days | 10.44% |\\n| IV-\\\\(\\\\text{III}\\\\) | 13.24 | 3.73 | 8.99 days | 9.07% |\\n| IV-HOSP | 10.68 | 2.17 | 4.60 days | 0.72% |\\n| IV-ICU | 14.73 | 3.23 | 3.12 days | 9.39% |\\n| IV-ICU \\\\(9\\\\) | 15.85 | 3.05 | 3.20 days | 9.60% |\\n\\n### Application of GEMs - Translation Direction\\n\\nAs mentioned in Section 2, the ICD-10 system is much more specific and contains almost five times as many diagnosis codes and 20 times as many procedure codes in comparison to ICD-9. In Figure 2 we show that it is harder to map from ICD-9 to ICD-10, especially regarding procedure codes. We find, that mapping from ICD-9 to ICD-10 leads to more cases of \\\"APROXIMATE ALTERNATIVES\\\", \\\"COMBINATION ALTERNATIVES\\\" and \\\"MULTIPLE SCENARIOS\\\" which are not automatically resolvable. This confirms our initial hypothesis that mapping to a more specialized system is harder than the other way around. Mapping from ICD-10 to ICD-9 results in fewer non-resolvable codes, but involves losing some of the specialized and detailed codes from the more recent ICD-10 system. This works both for the procedure codes as well as the diagnosis codes. Therefore, we decide to translate all codes in our experiments from ICD-10 to ICD-9 and evaluate only using the ICD-9 system.\\n\\n### Improving Translation by Code Grouping\\n\\nIn Figure 3 we show that 80.5% of the admissions contain at least one diagnosis code and 80.2% at least one procedure code that is not mappable to the respective other system. The ICD system is built hierarchically. This allows us to reduce the complexity of each code following the approach of van Aken et al. (2021). We group the ICD codes by using only their first three digits. In consequence, this increases the number of resolvable codes. This also leads to a denser label space, and the remaining three digits of each code still contain meaningful and helpful clinical information. In Figure 2, we illustrate the impact of grouping to four and three digits, denoted by \\\\([4]\\\\) and \\\\([3]\\\\). We find that reducing the precision to three digits leads to a large decrease of non-resolvable codes.\\n\\n### Investigation of Remaining Non-Resolvable Codes\\n\\nWe further investigate which codes still remain non-resolvable after code grouping. We observe that the occurrence of not resolvable...\"}"}
{"id": "lrec-2024-main-391", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Relative frequencies of MIMIC-IV admissions with non-resolvable codes. 81.2% contain at least one non-resolvable code in their diagnosis, 74% in their annotated procedures. ICD-9 in red, ICD-10 in blue codes follows a power law distribution. The top five most occurring not resolvable diagnosis codes are responsible for 62.14% of not resolvable cases. An example for this is the ICD-10 code F32.9 Major depressive affective disorder, single episode, unspecified, that can either be translated to 296.20 Major depressive affective disorder, single episode, unspecified or 311 Depressive disorder, not elsewhere classified. We find that almost all of the identified codes are part of the APPROXIMATE ALTERNATIVES category in the GEM mapping.\\n\\n4. Experiments\\n\\nWe evaluate the effect of data drift on models trained on MIMIC-III using the evaluation datasets defined in Subsection 3.3. We follow van Aken et al. (2021) and evaluate state-of-the-art models on the set of clinical outcome prediction tasks. The tasks have been identified from doctors as a meaningful evaluation in the clinical setting. The set of clinical outcome prediction tasks consists of the following tasks:\\n\\n- **Diagnosis and Procedures Prediction**\\n  The task is to predict the resulting diagnoses or procedures at discharge time, given a patient\u2019s admission note. We use the grouped first three digits of the annotated codes. This task is a multi-label classification task.\\n\\n- **Diagnosis+ and Procedure+**\\n  In addition to the procedure and diagnosis code prediction tasks, we add the four-digit codes and a bag of words representation of each code\u2019s name to the label set. This enables a more precise prediction, as the model can predict either the three digit, four-digit or the bag of words variant of a code. We call this task setting in the following section ICD+, analogue to the implementation of van Aken et al. (2021).\\n\\n- **Length-Of-Stay**\\n  The task is to predict the duration of a patient\u2019s stay, given his admission note. To ease the usage of classification models, the length-of-stay is grouped into four categories: less than 3 days, 3 to 7 days, 7 to 14 days and more than 14 days. These four groups were recommended by medical professionals.\\n\\n- **In-Hospital Mortality Prediction**\\n  The task is to predict whether a patient deceases during his hospital stay, given his admission note.\\n\\n4.1. Models\\n\\nWe evaluate the following models on the task, as they have shown good performance on the outcome prediction tasks on MIMIC-III data and because the models and training code are publicly available. We use the original CORe (van Aken et al., 2021), PubMedBERT (Gu et al., 2021) and the ProtoPatient (van Aken et al., 2022) model.\\n\\n- **CORe**\\n  The CORe model is a BioBERT-based model (Lee et al., 2020) that was further pretrained on admission notes from MIMIC-III and clinical cases from PubMed using a masked-language modeling task. In addition, it was trained on a task similar to next-sentence prediction, where the model has to predict whether two text sequences are part of the same patient note.\\n\\n- **PubMedBERT**\\n  For comparison, we evaluate our tasks on PubMedBERT (Gu et al., 2021), which was pretrained on 14 million abstracts from PubMed. Unlike other models like BioBERT (Lee et al., 2020), Gu et al. (2021) also pretrained the tokenizer, thereby enhancing the representation for domain-specific words by preserving them from being broken down into single word pieces. Hence, PubMedBERT generally exhibits superior performance compared to BioBERT. We opted against utilizing Clinical BERT (Alsentzer et al., 2019) due to its pre-training on text sourced from MIMIC-III, a factor that could potentially confer the model with an unjustified advantage.\\n\\n- **ProtoPatient**\\n  In addition to CORe, van Aken et al. (2022) propose a neural network architecture based on prototypical part networks (Chen et al., 2019) and Transformer-based language models. The ProtoPatient model uses label-wise attention to learn one prototype- and attention vector for each diagnosis. A patient\u2019s note is mapped to multiple prototype vectors. The combination of attention and prototype vectors enhances the interpretability of the classification. Each prototype vector linked to a diagnosis label highlights a short,\"}"}
{"id": "lrec-2024-main-391", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Experimental Setup\\n\\nWe fine-tune all models on the original MIMIC-III training split from van Aken et al. (2021) on the original ICD-9 labels. For all evaluation splits, we remove all admission notes where one or more ICD codes are not automatically resolvable using the code matching described in Section 3.4 and evaluate only on ICD-9 codes. For the CORe model and PubMedBERT we perform a hyperparameter optimization on the same of dataset splits defined by van Aken et al. (2021). We tune the learning rate from $5 \\\\times 10^{-6}$ to $1 \\\\times 10^{-4}$, warm up steps from 30 to 5000, gradient accumulation steps from 1 to 20, dropout from 0.1 to 0.3 and optimized for maximization of AUROC. For ProtoPatient, we used the hyperparameters proposed by the authors, as they led to reproducible scores from the paper. The ProtoPatient model enhances interpretability on especially many-label classification tasks because it performs the classification on a per-token basis. Therefore, we decided against the evaluation of this model on the mortality and length-of-stay prediction tasks. Furthermore, we only perform more detailed experiments using the CORe model, as it showed similar performance to PubMedBERT. For CORe and ProtoPatient we use the original implementations by van Aken et al. (2021, 2022).\\n\\n5. Results\\n\\nDiagnosis and Procedure Task Results\\n\\nTable 2 shows the performance of the chosen models, trained on MIMIC-III, on the respective split of MIMIC-IV and MIMIC-III. A lower relative performance compared to the MIMIC-III test split can be attributed to data drift. We observe that as expected the models do not generalize well from ICU training data to the provided emergency department data in IVHOSP. This applies to procedures as well as the diagnosis code prediction task. For the ICD+ variants, we observe a larger relative performance in comparison to their 3-digit counterparts for all models. PubMedBERT benefits from the added information on the ICD+ procedures task. CORe and PubMedBERT both show reduced performance on the diagnoses ICD+ task. We observe that PubMedBERT performs slightly worse than the CORe model but follows similar performance trends regarding the performance on the MIMIC-IV splits.\\n\\nPerformance Impact on ProtoPatient\\n\\nWe find that especially the ProtoPatient model does not generalize well on the MIMIC-IV diagnosis task in comparison to the CORe model. This applies to all subsets and results in an average of -8.62 p.p. (percentage points) of AUROC in comparison to MIMIC-III. From the scores we cannot account this problem to data drift alone as the performance is also worse on the IVIII-test split from MIMIC-IV. This means that the new de-identification scheme that has been applied on the MIMIC-IV data is partially responsible for the worse performance of the ProtoPatient model. We hypothesize that the model learns to focus more on very specific key-words and thus overfits on the de-identification schema from MIMIC-III. However, in contrast to the diagnosis task, we observe that the ProtoPatient model still performs well on the procedure code prediction task. Note, that IVIII-test splits for the diagnosis and procedure tasks do not fully contain all admission notes from the original III-test split due to our matching approach, neither any note from before 2008.\\n\\n5.1. Impact of GEMs\\n\\nTo assess the effect of using GEMs for translating ICD-10 annotated patient notes to ICD-9, we compare the performance on notes originally coded with ICD-9 and those annotated solely with ICD-10 codes. From the resulting scores in Table 3, we deduce that applying the GEMs has a negative effect on the prediction performance of ProtoPatient on the procedure predictions. CORe, on the other hand, seems to handle the introduced data drift from newer coding standards better. It remains to note that the absolute performance numbers are not comparable due to different label spaces and number of examples in each split. For the diagnosis code prediction, we notice almost no difference in prediction performance.\\n\\n5.2. Mortality Prediction - Data Drift on Emergency Department Data\\n\\nWe observe in Table 2 that the CORe model, trained on MIMIC-III at first sight, performs well on the mortality prediction task and also seems to generalize on emergency department data (IVHOSP). However, on closer inspection, using Precision, Recall and F1 we observe in Table 4 that the model fails to predict the minority class and only performs well on the majority class. As ICU patients are 5.5 times more likely to decease during their stay, the model favors the prediction of a patients' death, resulting in a low precision score.\\n\\n5.3. Length-of-stay\\n\\nApplying MIMIC-III trained models on the length-of-stay task on MIMIC-IV data, we observe that\"}"}
{"id": "lrec-2024-main-391", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 2:\\n\\nEvaluation results in macro averaged AUROC. We also present the performance difference compared to the performance on the original III test dataset. ProtoPatient does not generalize to MIMIC-IV data for diagnosis code prediction but manages to generalize well for procedure code prediction.\\n\\n| Model Task | AUROC (%) |\\n|------------|-----------|\\n| III ICU    | 77.07     |\\n| IV ICU     | 89.62     |\\n\\n### Table 3:\\n\\nComparison between the performance on the subset of admissions that use ICD-9 codes and the subset of admissions that use ICD-10 codes. For the ICD-10 subset, the codes are translated to ICD-9. Diagnosis prediction remains unaffected by the change of the coding system. Procedure code prediction performs worse for the CORe model on original ICD-9 codes and slightly worse for ICD-10 codes.\\n\\n| Model Task | AUROC (%) |\\n|------------|-----------|\\n| III ICU    | 79.38     |\\n| IV ICU     | 78.26     |\\n\\n### Table 4:\\n\\nRecall, Precision and F1 performance of the CORe model on the mortality prediction task. On closer inspection, the model fails to generalize from ICU data to emergency department hospital data. The performance is similar to the MIMIC-III performance for the ICU data regarding the measured macro AUROC in Table 2. However, the performance drops drastically on the emergency department data (IV HOSP). This behavior is expected because the length-of-stay of a patient in the MIMIC-III training dataset is around twice as long (s. Table 1), thus the training distribution drastically differs from the test data distribution. We also measure the F1 score for each class on the III, IV ICU, IV HOSP splits in Table 5. We observe that even though the IV HOSP split follows a different distribution with on average shorter stays than patients in MIMIC-III (III), the models perform surprisingly good at identifying shorter stays. For longer stays, however, the performance decreases drastically, with a trend that especially long stays are very unlikely to be predicted. Furthermore, it is noticeable that the average length-of-stay in the newer MIMIC-IV ICU admissions (IV ICU) is shorter than in MIMIC-III, which also explains the reduced performance on the ICU test split.\\n\\n### Table 5:\\n\\nClass-wise F1 of the CORe model on the length-of-stay task. The model does not seem to generalize on the emergency department data.\\n\\n| Class        | Recall | Precision | F1    |\\n|--------------|--------|-----------|-------|\\n| III ICU      | 93.61  | 37.36     | 66.87 |\\n| IV ICU       | 93.42  | 45.00     | 67.41 |\\n| III HOSP     | 99.49  | 6.67      | 54.57 |\\n\\n### Discussion and Findings\\n\\nGEMs Negligibly Impact Performance\\n\\nWe conclude from our observations in Section 5 that...\"}"}
{"id": "lrec-2024-main-391", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"applying GEMs to automatically map from ICD-10 to ICD-9 does not have a large negative effect on the models' prediction performance in the clinical outcome prediction tasks.\\n\\nData Drift Does Not Drastically Affect Diagnosis and Procedure Prediction Performance.\\n\\nIn contrast to Yang et al. (2022) we could not observe that our selected models suffer from drastically decreased performance due to data drift with the exception for the ProtoPatient model on the diagnosis classification task. According to Yang et al. (2022), the switch from ICD-9 to ICD-10 and changes in how microbiology samples are taken were the main reasons for performance drop. We suggest that compared to time-series data, it is easier to link clinical features with the text they are associated with in clinical notes. Similarly, the features linked to certain diagnosis codes, even with changes in coding standards over time, tend to stay more consistent. We hypothesize that the new de-identification scheme in MIMIC-IV has a bigger effect on how well the models perform in the evaluations compared to other changes. This indicates that Transformer-based language models trained on MIMIC-III are able to generalize well on MIMIC-IV for the diagnosis and procedure prediction tasks from the outcome prediction benchmark.\\n\\nDecreased Performance on Emergency Department Data (IV HOSP)\\n\\nAs expected, we find that models trained on MIMIC-III ICU data perform significantly worse on the new emergency department data from MIMIC-IV (IV HOSP). Especially for the length-of-stay and mortality prediction tasks, we see decreased performance. Surprisingly, the performance impact on the ICD code classification tasks is lower than expected for CORe and PubMedBERT, with -2.58 p.p. macro AUROC on average.\\n\\nProtoPatient Suffers More from Data Drift\\n\\nIn contrast to the pure Transformer-based language models CORe and PubMedBERT, we observe a drastically lower performance on the MIMIC-IV diagnosis prediction task for the ProtoPatient model. We find that the de-identification scheme has a large negative impact on the models' performance. This implies that the model hyper-focuses on specific keywords from MIMIC-III, lowering the performance on unseen and structurally slightly different data.\\n\\nConclusion\\n\\nIn this work, we present the new clinical outcome prediction dataset based on MIMIC-IV that is backwards compatible and thus comparable with MIMIC-III. We also use GEMs to map the label space to ICD-9. We show the effect of data drift on language models trained on outcome prediction tasks like diagnosis-, procedure-, length-of-stay and mortality prediction. We provide empirical evidence that Transformer-based language models trained on MIMIC-III are capable of generalizing to unseen admission notes in MIMIC-IV. Finally, this work should allow researchers to probe the performance of models fine-tuned on MIMIC-III data on MIMIC-IV without any effect of data poisoning in the test split. For future work, we suggest considering to refine the mapping of remaining complex not automatically resolvable ICD codes to enhance the datasets quality and to increase the dataset size. Furthermore, we encourage the work on applying language models on the outcome prediction task, especially interpretable models that can help doctors build an intuition for the models' prediction.\\n\\nEthical Considerations\\n\\nUsing transformer-based language models to predict patient outcomes from clinical text can be helpful for clinicians and medical professionals. However, it is important to recognize that these text-based models might introduce biases from their training and fine-tuning, thus they should not be blindly trusted. Moreover, the incorporation of billing codes, such as ICD-9 and ICD-10, within datasets like MIMIC-III and MIMIC-IV, inherently introduces biases. Therefore, these codes should be approached with careful consideration as an optimal label space. Of particular concern is the inclusion of both ICD-9 and ICD-10 codes in MIMIC-IV, which amplifies the risk of introducing further biases due to the necessity of mapping between the two systems. This work underscores the persistent challenge of noisy mappings between these systems, which can alter the clinical context and significance of certain annotated codes. It is crucial to note that predicting patient outcomes solely based on clinical text, especially in the absence of supplementary data, poses inherent challenges.\\n\\nAcknowledgements\\n\\nThis work was founded by the German Federal Ministry of Education and Research (BMBF) under grant agreements 01IS23013C (More-with-Less), as well as the grant agreement 01IS23015C (SCM) and the grant agreement 16SV8857 (KIP-SDM).\\n\\nBibliographical References\"}"}
{"id": "lrec-2024-main-391", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew McDermott. 2019. Publicly available clinical BERT embeddings. In Proceedings of the 2nd Clinical Natural Language Processing Workshop, pages 72\u201378, Minneapolis, Minnesota, USA. Association for Computational Linguistics.\\n\\nBrian Belgodere, Pierre Dognin, Adam Ivankay, Igor Melnyk, Youssef Mroueh, Aleksandra Mojsilovic, Jiri Navartil, Apoorva Nitsure, Inkit Padhi, Mattia Rigotti, et al. 2023. Auditing and generating synthetic data with controllable trust trade-offs. arXiv preprint arXiv:2304.10819.\\n\\nAlban Bornet, Dimitrios Proios, Anthony Yazdani, Fernando Jaume-Santero, Guy Haller, Edward Choi, and Douglas Teodoro. 2023. Comparing neural language models for medical concept representation and patient trajectory prediction. medRxiv, pages 2023\u201306.\\n\\nChaofan Chen, Oscar Li, Daniel Tao, Alina Barnett, Cynthia Rudin, and Jonathan K Su. 2019. This looks like that: deep learning for interpretable image recognition. Advances in neural information processing systems, 32.\\n\\nJoakim Edin, Alexander Junge, Jakob D Havtorn, Lasse Borgholt, Maria Maistro, Tuukka Ruotsalo, and Lars Maal\u00f8e. 2023. Automated medical coding on mimic-iii and mimic-iv: A critical review and replicability study. arXiv preprint arXiv:2304.10909.\\n\\nPaul Grundmann, Tom Oberhauser, Felix Gers, and Alexander L\u00f6ser. 2022. Attention networks for augmenting clinical text with support sets for diagnosis prediction. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4765\u20134775, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\n\\nYu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthcare, 3(1).\\n\\nShaoxiong Ji and Pekka Marttinen. 2021. Patient outcome and zero-shot diagnosis prediction with hypernetwork-guided multitask learning. arXiv preprint arXiv:2109.03062.\\n\\nAlistair Johnson, Tom Pollard, and Roger Mark. 2022. Mimic-iii clinical database carevue subset.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2020. Biobert: a pre-trained biomedical language representation model for biomedical text mining. Bioinform., 36(4):1234\u20131240.\\n\\nJames Mullenbach, Sarah Wiegreffe, Jon Duke, Jimeng Sun, and Jacob Eisenstein. 2018. Explainable prediction of medical codes from clinical text. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1101\u20131111, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nAakanksha Naik, Sravanthi Parasa, Sergey Feldman, Lucy Wang, and Tom Hope. 2022. Literature-augmented clinical outcome prediction. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 438\u2013453.\\n\\nMitchell Naylor, Christi French, Samantha Terker, and Uday Kamath. 2021. Quantifying explainability in nlp and analyzing algorithms for performance-explainability tradeoff. arXiv preprint arXiv:2107.05693.\\n\\nJens-Michalis Papaioannou, Paul Grundmann, Betty van Aken, Athanasios Samaras, Ilias Kyparissidis, George Giannakoulas, Felix Gers, and Alexander Loeser. 2022. Cross-lingual knowledge transfer for clinical phenotyping. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 900\u2013909, Marseille, France. European Language Resources Association.\\n\\nThomas Searle, Zina M. Ibrahim, and Richard J. B. Dobson. 2020. Experimental evaluation and development of a silver-standard for the MIMIC-III clinical coding dataset. In Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing, BioNLP 2020, Online, July 9, 2020, pages 76\u201385. Association for Computational Linguistics.\\n\\nNiall Taylor, Yi Zhang, Dan W Joyce, Ziming Gao, Andrey Kormilitzin, and Alejo Nevado-Holgado. 2023. Clinical prompt learning with frozen language models. IEEE Transactions on Neural Networks and Learning Systems.\"}"}
{"id": "lrec-2024-main-391", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"self-supervised knowledge integration. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 881\u2013893, Online. Association for Computational Linguistics.\\n\\nBetty van Aken, Jens-Michalis Papaioannou, Marcel Naik, Georgios Eleftheriadis, Wolfgang Nejdl, Felix Gers, and Alexander Loeser. 2022. This patient looks like that patient: Prototypical networks for interpretable diagnosis prediction from clinical text. In Proceedings of the 2nd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 12th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 172\u2013184, Online only. Association for Computational Linguistics.\\n\\nBenjamin Winter, Alexei Figueroa Rosero, Alexander L\u00f6ser, Felix Alexander Gers, and Amy Siu. 2022. KIMERA: Injecting domain knowledge into vacant transformer heads. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 363\u2013373, Marseille, France. European Language Resources Association.\\n\\nJanice Yang, Ludvig Karstens, Casey Ross, and Adam Yala. 2022. AI gone astray: Technical supplement. CoRR, abs/2203.16452.\\n\\nLanguage Resource References\\n\\nJohnson, Alistair and Bulgarelli, Lucas and Pollard, Tom and Horng, Steven and Celi, Leo Anthony and Mark, Roger. 2020. MIMIC-IV. [link].\\n\\nJohnson, Alistair EW and Pollard, Tom J and Shen, Lu and Lehman, Li-wei H and Feng, Mengling and Ghassemi, Mohammad and Moody, Benjamin and Szolovits, Peter and Anthony Celi, Leo and Mark, Roger G. 2016. MIMIC-III, a freely accessible critical care database. Nature Publishing Group. [link].\\n\\nvan Aken, Betty and Papaioannou, Jens-Michalis and Mayrdorfer, Manuel and Budde, Klemens and Gers, Felix and Loeser, Alexander. 2021. Clinical Outcome Prediction from Admission Notes using Self-Supervised Knowledge Integration. Association for Computational Linguistics. [link].\"}"}
