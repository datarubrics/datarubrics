{"id": "acl-2022-long-290", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThe rapid development of conversational assistants accelerates the study on conversational question answering (QA). However, the existing conversational QA systems usually answer users' questions with a single knowledge source, e.g., paragraphs or a knowledge graph, but overlook the important visual cues, let alone multiple knowledge sources of different modalities. In this paper, we hence define a novel research task, i.e., multimodal conversational question answering (MMCoQA), aiming to answer users' questions with multi-modal knowledge sources via multi-turn conversations. This new task brings a series of research challenges, including but not limited to priority, consistency, and complementarity of multimodal knowledge. To facilitate the data-driven approaches in this area, we construct the first multimodal conversational QA dataset, named MMConvQA. Questions are fully annotated with not only natural language answers but also the corresponding evidence and valuable decontextualized self-contained questions. Meanwhile, we introduce an end-to-end baseline model, which divides this complex research task into question understanding, multi-modal evidence retrieval, and answer extraction. Moreover, we report a set of benchmarking results, and the results indicate that there is ample room for improvement.\\n\\n1 Introduction\\nThe ever-increasing variety of information leads to the current information explosion. Question answering (QA) systems play an important role in alleviating information overload by providing users brief and accurate answers. Towards this end, a great many QA systems have been developed by utilizing external knowledge sources to obtain the correct answer, including knowledge-based QA (Deng et al., 2019), document-based QA (Wang et al., 2018), and community-based QA (Fang et al., 2016). Recently, as the rapid development of conversational assistants, there is growing interest in all matters conversational. Conversational QA, aiming to satisfy users' complex information needs via multi-turn conversations, attracts a lot of attention.\\n\\nThe existing conversational QA systems usually rely on a single knowledge source, e.g., paragraphs or a knowledge graph, and assume it contains enough evidence to extract answers to users' questions. However, these conversational QA systems are limited in real-world QA scenarios due to the following reasons. On the one hand, the important visual cues are overlooked in the existing conversational QA systems. As an old saying goes, \u201ca picture is worth a thousand words\u201d, namely a picture can often vividly express a lot of information. For example, as shown in Figure 1, the question \u201cWhich city features a green copper statue of a woman holding a torch?\u201d can be naturally answered by looking at the related picture.\\n\\nWhich city features a green copper statue of a woman holding a torch?\\nNew York City.\\n\\nWhat is the largest catholic church in it?\\nSt. Patrick\u2019s Cathedral.\\n\\nOn what dates was the Better with U Tour in the city?\\nMarch 9, 2012\\n\\nSt. Patrick\u2019s Cathedral is the largest Gothic Revival Catholic cathedral in North America.\\n\\nDate City\\nMarch 7, 2012 Nashville\\nMarch 9, 2012 New York\"}"}
{"id": "acl-2022-long-290", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On the other hand, the series of questions in a conversation may dynamically require multiple knowledge sources that encompass different modalities rather than only one constant knowledge source. As shown in Figure 1, three questions in the conversation involve images, passages, and structured tables respectively to extract the correct answers. In fact, although QA systems have been well studied thus far, conversational question answering with multiple knowledge sources of multi-modalities is still untapped. In this paper, we hence define this novel research task, i.e., multimodal conversational question answering (MMCoQA), aiming to answer users' questions with multimodal knowledge sources via multiturn conversations.\\n\\nMMCoQA is indeed non-trivial due to the following research challenges. 1) Priority of multi-modal knowledge. For a specific question, one modality may be more suitable for locating its corresponding answer than the others. For example, questions about numerical inquiries like date or statistics are better answered by utilizing tables. Different from the previous conversational QA tasks, the most appropriate modality that can be used to answer the current question is not given in MMCoQA. Given the conversation context, how to correctly determine the appropriate modality for the current question is a challenge. 2) Consistency of multimodal knowledge. Different modalities may provide consistent evidence to extract the correct answer for a question. For example, for the first question in Figure 1, the visual modality provides intuitional and direct information, while the related paragraph \u201cThe Statue of Liberty ..., off the coast of New York City. She holds a torch in her raised right hand ...\u201d also reveals a certain of cues to indicate the correct answer. How to utilize the consistency among different modalities to verify the answer is another challenge. 3) Complementarity of multimodal knowledge. Some questions may require evidences of different modalities to reason the final answer. For example, the question \u201cBilly Slater played for the NRL team in 2006 with a character holding what on the logo?\u201d must be answered based on both the table about Billy Slater\u2019s career and the image of his team logo. Therefore, to answer these questions, the system is required to have the ability of reasoning across multiple modalities. More importantly, the aforementioned three issues are not standalone but interweaved as conversation goes. Thus, MMCoQA is not the simple combination of multimodal QA and conversational QA but requires deep multimodal understanding and reasoning abilities across multi-turn conversations, which leaves ample room to study.\\n\\nTo advance the progress of building MMCoQA systems using data-driven approaches, we construct the MMConvQA dataset, the first dataset for MM-CoQA (see Table 1). Each question is fully annotated with not only the natural language answer but also the related evidence. Besides, the valuable decontextualized self-contained questions are also annotated for all questions. Hence, MMConvQA can be used to develop individual system modules for multimodal conversational search, conversational question rewrite, and multimodal QA systems. Accordingly, we introduce an end-to-end baseline model and provide a set of benchmarking results, which may facilitate a lot of exciting ongoing researches in the area.\\n\\nThe contributions of this work are threefold: \u2022 To the best of our knowledge, this is the first work towards the multimodal conversational question answering problem. We clearly define the research scope of this task and identify its potential research challenges. \u2022 We construct the first dataset, MMConvQA, for the multimodal conversational QA task. MMConvQA contains multiple supervised labels, including related evidence, answers, and decontextualized questions, which facilitates the data-driven approaches in this community. \u2022 We introduce an end-to-end model as the baseline and report a set of results. Experiment results indicate the significant room for future improvement. Besides, the data and codes of this work are released.\\n\\n2 Related Work\\nConversational QA is a relatively new topic in the QA community. Benefiting from the released dataset (Reddy et al., 2019; Choi et al., 2018), text-based conversational QA has been greatly developed. Researchers proposed to model and filter the conversation context via binary term classification (Voskarides et al., 2020) and question rewriting (Elgohary et al., 2019; Vakulenko et al., 2021; Yu et al., 2020). Recently, some efforts (Qu et al., 2020; Li et al., 2022; Anantha et al., 2021b)\\n\\n1https://github.com/liyongqi67/MMCoQA.\"}"}
{"id": "acl-2022-long-290", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of MMConvQA with datasets from related research tasks.\\n\\n| Task         | Dataset                        | Conversational | Modality | Retrieval | DQ     |\\n|--------------|--------------------------------|----------------|----------|-----------|--------|\\n| Conversational QA | QuAC (Choi et al., 2018)        |                |          |           |        |\\n|               | CoQA (Reddy et al., 2019)       |                |          |           |        |\\n|               | QReCC (Anantha et al., 2021a)   |                |          |           |        |\\n|               | CSQA (Saha et al., 2018b)       |                |          |           |        |\\n| Multimodal QA | MMQA (Talmor et al., 2021)      |                | Multi    |           |        |\\n|               | ManyMod QA (Hannan et al., 2020)|                | Multi    |           |        |\\n| Conversational Search | CAsT (Dalton et al., 2020)     |                |          |           |        |\\n|               | SaaC (Ren et al., 2021)         |                |          |           |        |\\n| Multimodal QA | MMConvQA (this work)            |                | Multi    |           |        |\\n\\nexpanded conversational QA to the open-domain setting, where the related passages must be retrieved rather than given directly. In addition to text-based conversational QA, knowledge-based conversational QA (Christmann et al., 2019) was also developed to answer conversational questions based on a knowledge base. Saha et al. (2018a) created a large-scale dataset and Shen et al. (2019) proposed a multi-task learning framework to resolve coreference in conversations and detect entities simultaneously. However, these existing methods only involved one knowledge source and the important visual cues were overlooked.\\n\\nOur work is also closely related to multimodal QA. Essentially, the task of VQA (Jing et al., 2020; Shah et al., 2019) is multimodal and involves images and textual questions. However, in this work, we are more interested in question answering with multimodal knowledge sources. In fact, QA with multiple mediums has been studied for a long time. For example, in the year of 2011, Nie et al. proposed to enrich textual question answering with image and video data. Besides, Textbook QA (Kembhavi et al., 2017) and TVQA (Lei et al., 2018) were also explored under specific scenes. Recently, Hannan et al. introduced the ManymodalQA challenge, where the questions are ambiguous and the modality is not easily determined based solely upon the question. Talmor et al. innovatively introduced the complex question scenario to multimodal QA, where a complex question requires several modalities to answer. In this work, we believe that conversational QA is a natural scenario for combining multimodal knowledge sources, where different modalities are dynamically required as the conversation moves on.\\n\\n## 3 Dataset Construction\\n\\nIn fact, there are few websites or applications where we can directly obtain a huge amount of questions that are answered with multimodal knowledge sources, let alone in the conversational form. Fortunately, we notice that the MMQA (Talmor et al., 2021) dataset contains a number of complex questions answered with multiple modalities of knowledge. Considering that an important intention of developing conversational QA systems is to gradually satisfy users' complex information needs via multi-turn conversations (Dalton et al., 2020), we intuitively propose to decompose these complex questions into conversational questions (Saha et al., 2018c). For example, as shown in Figure 2, the complex question \u201cThe player not wearing a helmet in 2019-20 Buffalo Sabres season free agents was on what team?\u201d can be better presented in a conversation \u201cQ1: Which player not wear a helmet in 2019-20 Buffalo Sabres season free agents\u201d and \u201cQ2: He was on what team in that season?\u201d. However, if we obtain conversational questions only by decomposing complex questions, the number of questions in a conversation is rather limited since a complex question can only be decomposed into two or three questions. Therefore, we automatically generate potential conversations as references for annotators to refine.\"}"}
{"id": "acl-2022-long-290", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Q1: Which player not wear a helmet in 19-20 Buffalo Sabres season free agents?\\n\\nIntermedia Answer: John Gilmour\\n\\nAnswer: New York Rangers\\n\\nQ2: John Gilmour was on what team in 19-20 Buffalo Sabres season?\\n\\nAnswer: New York Rangers\\n\\nQ3: What is the New York Rangers goal song called?\\n\\nAnswer: The \\\"Slapshot\\\"\"}"}
{"id": "acl-2022-long-290", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics for MMConvQA dataset. TextQ, TableQ, and ImageQ refer to the questions related to text, tables, and images, respectively.\\n\\n| Item                  | Value                  |\\n|-----------------------|------------------------|\\n| # Dialogs             | 1,179                  |\\n| # Questions           | 5,753                  |\\n| # Avg. Q in Dialogs   | 4.88                   |\\n| # Min. Q in Dialogs   | 3                      |\\n| # Max. Q in Dialogs   | 10                     |\\n| # Passages            | 218,285                |\\n| # Tables              | 10,042                 |\\n| # Images              | 57,058                 |\\n\\nKnowledge Collection\\n\\n- # TextQ: 2,624 (45.6%)\\n- # TableQ: 1,715 (29.8%)\\n- # ImageQ: 1,414 (24.6%)\\n\\n4 Dataset Analysis\\n\\nMMConvQA contains 1,179 conversations and 5,753 QA pairs. There are 4.88 QA pairs on average for each conversation, as summarized in Table 2. The multimodal knowledge collection consists of 218,285 passages, 10,042 tables, and 57,058 images. Each question is annotated with the related evidence (a table, an image or a passage in the knowledge collection), and a natural language answer. Besides, each question is also accompanied with a corresponding self-contained question.\\n\\nQuestion Analysis\\n\\nFigure 3 shows sunburst plots of question types in MMConvQA. We can see that most of the first words are similar to those questions in other conversational QA datasets (Choi et al., 2018; Reddy et al., 2019; Saha et al., 2018b). \u201cThe\u201d and \u201cIn\u201d are frequently used because they usually relate to the coherence of conversations, such as \u201cThe actor\u201d. There are also some special patterns in MMConvQA featuring multi-modalities. For example, \u201cWhat Color\u201d pattern is related to the visual modality and \u201cHow Many\u201d may refer to the tables. On average, each question contains 14.4 words, while this number in the MMQA dataset is 19.2. This illustrates that we well decompose the complex questions. The average number of words for gold questions is 15.5, which is slightly bigger than that of the conversational questions. This is because conversational questions embody the linguistic phenomena of dialogues, such as coreference and ellipsis, thus have less words than gold questions. It is worth mentioning that two different complex questions may produce the same single question. Therefore, there are some duplicated questions in a conversation.\\n\\nAnswer Analysis\\n\\nThe types of answers in MMConvQA are diverse. Most of answers are text spans of passages, cells of tables, and titles of images, whereas some answers do not exactly overlap with the evidence. For example, the answer to the question \u201cThe singer of \u2018Take Me As I Am\u2019 is shown wearing what item on her neck?\u201d is \u201cscarf\u201d, which needs to be detected based on a related image rather than the title of the image. Apart from single answers, 9.9% questions require a list of answers. For example, the answer to the question \u201cwho is the owner of cape town knight riders?\u201d is \u201cShah Rukh Khan and Juhi Chawla\u201d. On average, each answer contains 2.11 words.\\n\\nModality Analysis\\n\\nAs summarized in Table 2, there are 45.6% questions can be answered with textual passages. Besides, 29.8% and 24.6% questions must be answered based on images and tables, respectively. Among the 1,179 conversations in this dataset, 57.7% conversations involve two different modalities of knowledge and 24.4% conversations involve three modalities. This indicates that as conversations proceed, questions dynamically require different modalities of knowledge to answer. To better illustrate the conversation flow,\"}"}
{"id": "acl-2022-long-290", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Transitions of modalities as conversation goes. The x-axis indicates the turn number and the y-axis indicates the modalities of questions. The height of one modality reflects the number of questions in each conversation turn of that modality, and the width of the bonds is proportional to the frequency of transition among modalities.\\n\\nLinguistic Phenomena. To measure the quality of the conversational questions and analyze their linguistic phenomena, we sample 100 follow-up questions in the development set and annotate various phenomena. Our analysis shows that around 33% questions do not rely on coreference with the conversational history and are answerable on their own. Around 57% questions contain explicit coreference markers such as he, she, it. The remaining 10% do not have explicit coreference markers but refer to an entity or event implicitly. Another feature of open-retrieval conversational QA is the topic switch. Among the questions, 24% change the conversation topic (WikiEntity).\\n\\n5 MAE Model\\n\\nWe introduce a Multimodal Conversational QA system with Adaptive Extractors, MAE for short, as a baseline model. As Figure 5 illustrates, MAE divides the MMCoQA task into three steps: conversational question understanding, multimodal evidence retrieval, and adaptive answer extraction.\\n\\n5.1 Problem Formulation\\n\\nAssume that the current turn in a conversation is $k$ and the current question is $q_k$. The conversation context for the current question $q_k$ is denoted as $H_k = \\\\{q_1, a_1, ..., q_{k-1}, a_{k-1}\\\\}$. A multimodal knowledge collection that contains different modalities of items is given, denoted as $C = \\\\{C_p \\\\cup C_t \\\\cup C_i\\\\}$, where $C_p$, $C_t$, and $C_i$ are the sets of passages, tables, and images, respectively. The system is required to retrieve the related evidence from the knowledge collection $C$ and extract a natural language span $\\\\hat{a}_k$ to answer the question $q_k$.\\n\\n5.2 Question and Multimodal Knowledge Encoder\\n\\nTo understand the current question with the conversation context $H_k$, we apply the sliding window mechanism (Qu et al., 2020) to filter the previous questions. We feed the reformatted question $q'_k$ into the BERT network (Devlin et al., 2019) to obtain the question representation, which is formulated as,\\n\\n$$v_q = W_q F_q(q'_k),$$  \\n\\n(1)\\n\\nwhere $F_q$ is the BERT based question encoder, $W_q$ is the question projection matrix, and $v_q \\\\in \\\\mathbb{R}^{d_q}$.\\n\\nFor different modalities of items in $C$, we pass them to different knowledge encoders. For each passage $p_j$ in $C_p$, we obtain its representation $v_j$ as follows,\\n\\n$$v_j = W_p F_p(p_j),$$  \\n\\n(2)\\n\\nwhere $F_p$ is the BERT based passage encoder, $W_p$ is the passage projection matrix, $v_j \\\\in \\\\mathbb{R}^{d_p}$. Following the prior work (Herzig et al., 2020; Talmor et al., 2021), we linearize tables by rows as $t'_j$ to obtain their representations. The table's representation is computed via,\\n\\n$$v_j = W_t F_t(t'_j),$$  \\n\\n(3)\\n\\nwhere $F_t$ is the BERT based encoder and $v_j \\\\in \\\\mathbb{R}^{d_t}$.\\n\\nFor an image $i_j$ in $C_i$, its representation is obtained as,\\n\\n$$v_j = W_i F_i(i_j),$$  \\n\\n(4)\\n\\nwhere $F_i$ is a pretrained Resnet (He et al., 2016) network on ImageNet, and $v_j \\\\in \\\\mathbb{R}^{d_i}$. Noticed that $d_q$, $d_p$, $d_t$, $d_i$ have the same dimension.\\n\\n5.3 Evidence Retrieval\\n\\nTo facilitate large-scale retrieval, we apply the dense retriever mechanism inspired from open-domain QA (Karpukhin et al., 2020). Differently, ...\"}"}
{"id": "acl-2022-long-290", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On what dates was the Better with U Tour in New York City?\\n\\nMarch 9, 2012\\n\\n5.4 Adaptive Answer Extraction\\n\\nThe retrieved evidence list $I_r$ contains items of different modalities, and different modalities need different answer extractors. We hence first detect the most appropriate modality for the question. We regard the modality detection as a multi-class classification task where the network takes a question as input to predict the probabilities of three modalities. The classifier is formulated as,\\n\\n$$s_b = f(WcFc(q'k)),$$\\n\\n(5)\\n\\nwhere $f()$ denotes the softmax function, $F_c$ is the question encoder and $s_b \\\\in \\\\mathbb{R}^3$.\\n\\nTextExtractor. It is basically a machine reading comprehension model. Given the reformulated question and a passage in $P_r$ as input, TextExtractor predicts an answer span by computing two scores for each token in a passage in $P_r$ to be the start token and the end token, respectively.\\n\\nTableExtractor. Following the previous work (Herzig et al., 2020), we concatenate the question text to the linearized table sequence, and encode them using BERT. Two linear classifiers are then followed to compute the probability of the token being the start token and the end token of the answer span, respectively.\\n\\nImageExtractor. We collect the answers in the training set as the answer set for testing (Talmor et al., 2021). We extract the visual feature $v_i$ for an image with the ResNet, and append the question text with all the answers in the answer set as a text sequence. And then we input the text sequence into the BERT to obtain the representations for all tokens, which are then simply combined with the visual feature $v_i$. Similarly, two linear classifiers are then followed to compute the probability of the token in the text sequence being the start token and the end token.\\n\\nThe answer extraction score $s_c$ for a candidate answer predicted by the above three extractors is defined as the average of the probabilities of the start and the end token. For each candidate answer, we compute its final score as the sum of the retrieval score $s_a$, the modality score $s_b$, and the answer extraction score $s_c$. The training details are illustrated in Appendix B.\\n\\n6 Experiments\\n\\n6.1 Evaluation Protocols\\n\\nWe comprehensively evaluated the baseline models based on their performance in evidence retrieval and answer extraction. We adopted Recall and NDCG to evaluate the coverage and the rank position of the retrieval list. Following previous conversational QA tasks (Reddy et al., 2019), we reported macro-average F1 in the word level and Exact Match (EM) to estimate the performance of answer extraction.\\n\\n6.2 Baseline Models\\n\\nWe evaluated the open-retrieval conversational QA system ORConvQA and a multimodal QA model ManyModalQA on our MMCoQA dataset. And\"}"}
{"id": "acl-2022-long-290", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of various methods on the test set. ER denotes the related evidence needs to be retrieved, and EG means the related evidence is manually included. Recall and NDCG are computed for top-2000 retrieved items.\\n\\n| Methods          | Dev Recall | Dev NDCG | Dev F1 | Dev EM | Test Recall | Test NDCG | Test F1 | Test EM |\\n|------------------|------------|----------|--------|--------|-------------|----------|--------|--------|\\n| ER               | 14.11      | 1.91     | 3.02   | 1.20   | 19.05       | 2.34     | 1.87   | 1.06   |\\n| MAE w/o context  | 31.17      | 4.32     | 2.13   | 0.71   | 33.28       | 4.63     | 1.74   | 0.82   |\\n| Gold question    | 62.13      | 12.43    | 7.06   | 3.27   | 63.39       | 12.46    | 6.29   | 3.73   |\\n| Gold answer      | 39.93      | 5.94     | 3.49   | 2.24   | 42.54       | 6.54     | 3.58   | 2.88   |\\n| EG               | 100        | 11.97    | 26.83  | 19.79  | 100         | 11.96    | 28.33  | 22.03  |\\n| MAE w/o context  | 100        | 9.68     | 22.15  | 19.54  | 100         | 9.73     | 24.16  | 18.41  |\\n| Gold question    | 100        | 15.93    | 32.89  | 23.58  | 100         | 15.84    | 36.93  | 28.31  |\\n| Gold answer      | 100        | 11.20    | 30.18  | 21.51  | 100         | 11.78    | 32.29  | 24.92  |\\n\\n6.3 Results Analysis\\n\\nThe results are summarized in Table 3. By analyzing the results, we gained the following insights. (1) The existing open-retrieval conversational QA and multimodal QA methods cannot handle the MM-CoQA problem well, since they are either single-modal or single-turn. (2) The results of the MAE variants partly evaluate the quality of the MM-CoQA dataset. When the conversation context is removed, the performance drops, which verifies the dependency on the conversation context. (3) Using extra data benefits the model, which illustrates that the size of the dataset is kind of small and pretraining can alleviate this problem. (4) When we manually complemented the relevant evidence into the retrieval list, it outperforms the normal MAE model a lot. It seems that the evidence retrieval is a bottleneck for the current model because the relation among multimodal knowledge is complex as claimed before.\\n\\n6.3.1 Modality Analysis\\n\\nWe summarized the performance of MAE-EG on the three different modal questions in Figure 6(a). It can be seen that the performance on ImageQ is the worst. It may be because that our ImageExtractor is a little coarse and more fine-grained interactions are expected. Besides, we selected some items that associated with same entities and visualized their embeddings in Figure 6(b). It is observed that the images' embeddings are isolated, which illustrates that the visual and semantic meanings are not well-aligned. Some text's and tables' embeddings are partly syncretic, but it is still far away from an ideal common space where the embeddings of different modal items are evenly distributed according to their meanings.\\n\\n7 Conclusion\\n\\nWe define a novel and practical task, i.e., MM-CoQA, and identify its research challenges, including priority, consistency, and complementarity of multimodal knowledge. We construct the MM-CoQA dataset, containing multiple supervised labels to facilitate related researches in this community. We also report a set of results and analyze the current bottleneck.\"}"}
{"id": "acl-2022-long-290", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments\\nThe work described in this paper was supported by Research Grants Council of Hong Kong (PolyU/5210919, PolyU/15207821), National Natural Science Foundation of China (62076212) and PolyU internal grants (ZVQ0).\\n\\nReferences\\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021a. Open-domain question answering goes conversational via question rewriting. In Proceedings of the International Conference of the North American Chapter of the Association for Computational Linguistics, pages 520\u2013534.\\n\\nRaviteja Anantha, Svitlana Vakulenko, Zhucheng Tu, Shayne Longpre, Stephen Pulman, and Srinivas Chappidi. 2021b. Open-domain question answering goes conversational via question rewriting. In Proceedings of the International Conference of the North American Chapter of the Association for Computational Linguistics, pages 520\u2013534.\\n\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wen-tau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing, pages 2174\u20132184.\\n\\nPhilipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, and Gerhard Weikum. 2019. Look before you hop: Conversational question answering over knowledge graphs using judicious context expansion. In Proceedings of the International Conference on Information and Knowledge Management, pages 729\u2013738.\\n\\nJeffrey Dalton, Chenyan Xiong, Vaibhav Kumar, and Jamie Callan. 2020. Cast-19: A dataset for conversational information seeking. In Proceedings of the International Conference on Research and Development in Information Retrieval, pages 1985\u20131988.\\n\\nYang Deng, Yuexiang Xie, Yaliang Li, Min Yang, Nan Du, Wei Fan, Kai Lei, and Ying Shen. 2019. Multitask learning with multi-view attention for answer selection and knowledge base question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 6318\u20136325.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the International Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171\u20134186.\\n\\nAhmed Elgohary, Denis Peskov, and Jordan Boyd-Graber. 2019. Can you unpack that? learning to rewrite questions-in-context. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing, pages 5918\u20135924.\\n\\nHanyin Fang, Fei Wu, Zhou Zhao, Xinyu Duan, Yueting Zhuang, and Martin Ester. 2016. Community-based question answering via heterogeneous social network learning. In Proceedings of the AAAI Conference on Artificial Intelligence, 1.\\n\\nDarryl Hannan, Akshay Jain, Mohit Bansal, li, and li. 2020. Manymodalqa: Modality disambiguation and qa over diverse inputs. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):7879\u20137886.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the International conference on computer vision and pattern recognition, pages 770\u2013778.\\n\\nJonathan Herzig, Pawel Krzysztof Nowak, Thomas Mueller, Francesco Piccinno, and Julian Eisenschlos. 2020. Tapas: Weakly supervised table parsing via pre-training. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 4320\u20134333.\\n\\nChenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, and Qi Wu. 2020. Overcoming language priors in vqa via decomposed linguistic representations. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 11181\u201311188.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing, pages 6769\u20136781.\\n\\nAniruddha Kembhavi, Minjoon Seo, Dustin Schwenk, Jonghyun Choi, Ali Farhadi, and Hannaneh Hajishirzi. 2017. Are you smarter than a sixth grader? textbook question answering for multimodal machine comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern recognition, pages 4999\u20135007.\\n\\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. 2019. Latent retrieval for weakly supervised open domain question answering. In Proceedings of the Annual Meeting of the Association for Computational Linguistics, pages 6086\u20136096.\\n\\nJie Lei, Licheng Yu, Mohit Bansal, and Tamara Berg. 2018. Tvqa: Localized, compositional video question answering. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing, pages 1369\u20131379.\"}"}
{"id": "acl-2022-long-290", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yongqi Li, Wenjie Li, and Liqiang Nie. 2022. Dynamic graph reasoning for conversational open-domain question answering. ACM Transactions on Information Systems, 40(4):1\u201324.\\n\\nLiqiang Nie, Meng Wang, Zhengjun Zha, Guangda Li, and Tat-Seng Chua. 2011. Multimedia answering: enriching text qa with media information. In Proceedings of the International Conference on Research and Development in Information Retrieval, pages 695\u2013704.\\n\\nChen Qu, Liu Yang, Cen Chen, Minghui Qiu, W Bruce Croft, and Mohit Iyyer. 2020. Open-retrieval conversational question answering. In Proceedings of the International conference on research and development in Information Retrieval, pages 539\u2013548.\\n\\nSiva Reddy, Danqi Chen, Christopher D. Manning, li, and li. 2019. Coqa: A conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249\u2013266.\\n\\nPengjie Ren, Zhumin Chen, Zhaochun Ren, Evangelos Kanoulas, Christof Monz, and Maarten de Rijke. 2021. Conversations with search engines: Serp-based conversational response generation.\\n\\nAmrita Saha, Vardaan Pahuja, Mitesh Khapra, Karthik Sankaranarayanan, and Sarath Chandar. 2018a. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 705\u2013713.\\n\\nAmrita Saha, Vardaan Pahuja, Mitesh M. Khapra, Karthik Sankaranarayanan, and Sarath Chandar. 2018b. Complex sequential question answering: Towards learning to converse over linked question answer pairs with a knowledge graph. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 705\u2013713.\\n\\nSanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8876\u20138884.\\n\\nTao Shen, Xiubo Geng, Tao Qin, Daya Guo, Duyu Tang, Nan Duan, Guodong Long, and Daxin Jiang. 2019. Multi-task learning for conversational question answering over a large-scale knowledge base. In Proceedings of the International Conference on Empirical Methods in Natural Language Processing, pages 2442\u20132451.\\n\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hanhnane Hajishirzi, and Jonathan Berant. 2021. Multimodal qa: complex question answering over text, tables and images. In International Conference on Learning Representations.\\n\\nSvitlana Vakulenko, Shayne Longpre, Zhucheng Tu, and Raviteja Anantha. 2021. Question rewriting for conversational question answering. In Proceedings of the International Conference on Web Search and Data Mining, pages 355\u2013363.\\n\\nNikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke. 2020. Query resolution for conversational search with limited supervision. In Proceedings of the International Conference on Research and Development in Information Retrieval, pages 921\u2013930.\\n\\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang, Gerry Tesauro, Bowen Zhou, and Jing Jiang. 2018. R 3: Reinforced ranker-reader for open-domain question answering. In Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\nShi Yu, Jiahua Liu, Jingqin Yang, Chenyan Xiong, Paul Bennett, Jianfeng Gao, and Zhiyuan Liu. 2020. Few-shot generative conversational query rewriting. In Proceedings of the International conference on research and development in Information Retrieval, pages 1933\u20131936.\\n\\nA Annotation Guidelines\\n\\nA.1 Decompose complex questions\\n\\nFor a complex question, we give the question text and its question type to the annotators. We also provide the final and intermediate answers to a complex question. For example, as shown in Table 4, we give the question text of a complex question and its question type \u201cCompose(TableQ,ImageQ)\u201d. We have identified this complex question can be decomposed into two questions: the first one is a \u201cImageQ\u201d and its answer is \u201cJohn Gilmour\u201d; The second one is a \u201cTableQ\u201d and its answer is \u201cFrom New York Rangers\u201d. It is noticed that the question type of the original question is very helpful, because it indicates the logical flow of the complex question. The question types and their meanings are listed as follows:\\n\\n\u2022 TableQ: Return a question asked over tables.\\n\u2022 TextQ: Return a text corpus question.\\n\u2022 ImageQ: Return a question about a single image.\"}"}
{"id": "acl-2022-long-290", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: An annotation example for constructing the MMConvQA dataset. \u2663 denotes the provided information for annotators, and \u2660 denotes that the column needs to be filled in by annotators.\\n\\n**Potential Conversation**\\n\\n| Type | Answers | Decomposed Type | Decomposed Conversation |\\n|------|---------|----------------|-------------------------|\\n| \u2663 | \u2660 | | |\\n\\nThe player not wearing a helmet in 2019-20 Buffalo Sabres season free agents was on what team?\\n\\nCompose(TableQ, ImageQ)\\n\\nJohn Gilmour ImageQ From New York Rangers TableQ\\n\\nWhat is the New York Rangers goal song called?\\n\\nTextQ The \\\"Slapshot\\\" TextQ\\n\\nThe corresponding evidence for the above three questions \u2663:\\n\\nWhen the Rangers score a goal at Madison Square Garden the \\\"Slapshot\\\". (aka \\\"The New York Rangers Goal Song\\\") song is played following....\\n\\n\u2022 Compose(*;*): Take a single question containing a single WikiEntity as the first argument, and a single question that produces that WikiEntity as the output answer as its second argument. For example, Compose(\\\"Where was Barack Obama born?\\\", \\\"Who was the 44th president of the USA?\\\"). The function replaces the WikiEntity in the first-argument single question with the second-argument single question and returns the resulting a complex question \\\"Where was the 44th president of the USA born?\\\".\\n\\n\u2022 Intersect(*;*): Take two single questions that return lists of more than one WikiEntity, and returns their intersection as the answer. E.g. \\\"Who was born in Hawaii and is the parent of Sasha Obama?\\\".\\n\\n\u2022 Compare(*;*): Take two single questions and each returns one WikiEntity that can be linked to one cell in a table.\\n\\nWhen decomposing complex questions, annotators should follow these instructions:\\n\\n\u2022 Decomposed questions keep close to the original questions as possible.\\n\u2022 Decomposed questions should keep consistent with the given answers.\\n\u2022 Decomposed questions should be independent and can be answered without any conversation context.\\n\\nA.2 Refine conversational questions\\n\\nAfter the complex question decomposition step, we have obtained a sequence of single questions. Now we need to refine these questions into a natural conversation. Please follow these instructions:\\n\\n\u2022 The refined questions should depend on the conversation context as possible and are hard to answer without conversational context.\\n\u2022 Annotators can use some pronouns to replace the entities that occurred in previous questions or answers. Annotators can also use some elliptical sentence like \\\"When?\\\", \\\"How?\\\". Some synonyms are also encouraged.\\n\u2022 Keep the whole conversation smooth as possible. You can rearrange questions and delete some low-quality questions. You can also report to delete the whole conversation.\\n\\nB Training Details\\n\\nRecall that we encode all the items offline for efficient retrieval. Specifically, we follow the previous work (Qu et al., 2020) to pretrain the three encoders so that it can provide reasonably good retrieval results to the subsequent components for further processing. After offline encoding, a set of item representations are obtained.\\n\\nWe define the loss for the evidence retrieval as,\\n\\n$$L_{er} = - \\\\sum_{j=1}^{N_r} (y \\\\log(S_j a) + (1 - y) \\\\log(1 - S_j a)),$$\\n\\nwhere $S_j a$ is the retrieval score of an item in $I_r$ and $y$ denotes whether it is a positive item or not.\\n\\nThe modality detection loss $L_{md}$ is a typical cross-entropy loss used for training multi-class classification, while the answer extraction loss used for...\"}"}
{"id": "acl-2022-long-290", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"training the three extractors is as follows.\\n\\n\\\\[ L_{ae} = -\\\\sum_{k=1}^{N_{toX}} (y_1 \\\\log(S_k^s) + (1-y_1) \\\\log(1-(S_k^s))) - \\\\sum_{k=1}^{N_{toX}} (y_2 \\\\log(S_k^e) + (1-y_2) \\\\log(1-(S_k^e))) \\\\]\\n\\nwhere \\\\( y_1 \\\\) and \\\\( y_2 \\\\) indicate whether the token is the start token and the end token, respectively. The final loss is defined as the sum of the above losses.\\n\\n## Implement Details\\n\\nThe data, code, and parameters are uploaded in the supplementary material. Specifically, the \\\\( d_q, d_p, d_t, \\\\) and \\\\( d_i \\\\) are set to 128, and \\\\( N_r \\\\) is set to 10. In the pretraining phase, we set the batch size to 4 and use Adam optimizer with learning rate 0.0001 to train the knowledge encoders for 12 epochs. In the following training phases, the parameters of knowledge encoders are frozen. We set the batch size to 1 and use Adam optimizer with learning rate 0.0001 to train the question encoder and extractors. We select the parameters that perform best in the development set and evaluate the model in the test set. The experiments are conducted on a server with a 3090 GPU card and Ubuntu operating system.\\n\\nFor ORConvQA and ManyModalQA, we did not use extra data, like VQA data, to pretrain the models for a fair comparison. And since ManyModalQA does not contain a evidence retrieval component, we apply our evidence retrieval component to it.\\n\\nWithout conversation context: Remove the conversation context, and use the current question alone.\\n\\nGold question: Replace the reformulated question \\\\( q'_k \\\\) with the gold question \\\\( q^*_k \\\\).\\n\\nQR: Rewrite the current question based on the conversation context.\\n\\nPretrain: Use the ORQuAC data to pretrain the evidence retrieval component.\\n\\nGold answer: Append the previous gold answers to the reformulated question \\\\( q'_k \\\\).\\n\\nEvidence given: Manually complement the evidence that supports the answer into the retrieved item list \\\\( I_r \\\\) if it is not retrieved.\"}"}
