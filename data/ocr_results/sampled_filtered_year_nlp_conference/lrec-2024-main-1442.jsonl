{"id": "lrec-2024-main-1442", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Realistic Few-Shot Relation Extraction: A New Meta Dataset and Evaluation\\n\\nFahmida Alam, Md Asiful Islam, Robert Vacareanu, Mihai Surdeanu\\n\\nUniversity of Arizona, Tucson, USA\\n\\nTechnical University of Cluj-Napoca, Cluj-Napoca, Romania\\n\\n{fahmidaalam, asifulislam, rvacareanu, msurdeanu}@arizona.edu\\n\\nAbstract\\n\\nWe introduce a meta dataset for few-shot relation extraction, which includes two datasets derived from existing supervised relation extraction datasets \u2013 NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020) and WIKIDATA (Sorokin and Gurevych, 2017) \u2013 as well as a few-shot form of the TACRED dataset (Sabo et al., 2021).\\n\\nImportantly, all these few-shot datasets were generated under realistic assumptions such as: the test relations are different from any relations a model might have seen before, limited training data, and a preponderance of candidate relation mentions that do not correspond to any of the relations of interest. Using this large resource, we conduct a comprehensive evaluation of six recent few-shot relation extraction methods, and observe that no method comes out as a clear winner. Further, the overall performance on this task is low, indicating substantial need for future research. We release all versions of the data, i.e., both supervised and few-shot, for future research.\\n\\nKeywords: relation extraction, few-shot learning, evaluation\\n\\n1. Introduction\\n\\nInformation Extraction (IE) plays a pivotal role in Natural Language Processing (NLP). IE is fundamental to many NLP tasks such as question answering, event extraction, knowledge base population, etc. Relation Extraction (RE) is a sub-task of IE with the focus of identifying entities and their semantic relations in a given text, enabling the extraction of structured information from unstructured data. For instance, in the sentence \\\"John Doe was born in New York City\\\", Relation Extraction can transform this into a structured tuple such as \u2192 (John Doe, born in, New York City), indicating the inherent relation between the person, action, and location.\\n\\nMany supervised methods have been proposed to address the relation extraction task (Soares et al., 2019; Zhang et al., 2018; Wang et al., 2016; Miwa and Bansal, 2016, inter alia). However, a traditional supervised machine learning (ML) setup is not always realistic for RE due to the large amount of training data required. This setup is mostly incompatible with real-world RE scenarios such as pandemic response or intelligence, in which RE models must be developed and deployed quickly with minimal supervision.\\n\\nConsidering this task setup, a realistic choice for solving this problem is few-shot learning (FSL) and its RE equivalent, few-shot relation extraction (FSRE), in which (a) each relation class is associated with a very small number of examples (typically 1 or 5), and the relation classes in the testing partition are different from any relations a model might have seen before. While several FSRE datasets and methods have been proposed recently (see Related Work), this subfield of NLP is still poorly understood due to a lack of realistic datasets and rigorous evaluations. This observation has motivated this work, in which we introduce a meta dataset for the task as well as a meaningful evaluation of multiple FSRE methods on this data.\\n\\nThe key contributions of our work are:\\n\\n(a) We develop a meta dataset for FSRE, which includes three datasets: one based on NYT29 (Takanobu et al., 2019; Nayak and Ng, 2020), one based on WIKIDATA (Sorokin and Gurevych, 2017), and lastly the few-shot variant of TACRED proposed by (Sabo et al., 2021). All these datasets were converted into realistic few-shot variants using the procedure detailed in \u00a73.4. This procedure guarantees a setup that is aligned with real-world applications, e.g., the test relations are different from any relations available in a background dataset, limited training data, preponderance of candidate relation mentions that do not correspond to any of the relations of interest, etc.\\n\\n(b) We conduct a comprehensive evaluation of six recent FSRE methods using this meta dataset. Our evaluation reveals that none of the models emerged as a definitive winner. Furthermore, the overall performance of the best models was notably low, indicating the substantial need for future research. We release all versions of the data, i.e., both supervised and few-shot, for future research.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets will contribute as an invaluable re-\\nsource for this future research.\\n\\n2. Related Work\\n\\n2.1. Methods\\n\\nHistorically, relation extraction approaches can be \\ncategorized as either rule-based or relying on sta-\\ntistical models. In the past decade, the latter \\ncategory has been dominated by neural-based \\nmethods. More recently, hybrid directions have \\nemerged, aiming to combine the advantages of \\nboth. We delve deeper into each of these direc-\\ntions.\\n\\n2.1.1. Rule-based Methods\\n\\nPrior to the widespread adoption of statistical ma-\\nine learning, rule-based approaches enjoyed a \\nperiod of prominence. These methods typically in-\\nvolve the acquisition of rules representative of spe-\\ncific relations. For example, the rule \\n\\\\[ \\\\text{ne=PER} \\\\] + <nsubj born >nmod_in \\\\[ \\\\text{ne=LOC} \\\\] + \\ncaptures \\na syntactic pattern for the \\n\\\\text{born_in} relation, where \\nthe pattern matches if the underlying constraints \\nare satisfied: a named entity labeled as person \\nis connected to the word \\n\\\\text{born} with a \\nnominal \\nsubject \\ndependency, and the same word \\n\\\\text{born} is further connected to a named entity labeled as \\n\\\\text{location} with a \\nnominal modifier \\ndependency.\\n\\nFor example, this pattern will match the sentence: \\n\\\\text{John Doe was born in New Y ork City}.\\n\\nA match of \\nsuch rules is then interpreted as an indication that \\nthe two entities participate in the corresponding re-\\nlation.\\n\\nIn (Hearst, 1992), the authors propose a set of \\nhandwritten rules to extract words satisfying the \\nhyponymy relation. Subsequently, efforts were di-\\nrected toward automating the learning of such pat-\\nterns (Riloff, 1993, 1996; Riloff and Jones, 1999) \\nwith and without supervision. (Gupta and Manning, \\n2014) improves automatic pattern learning by al-\\nlowing soft matching in the form of predicting the \\nlabels on unlabeled entities.\\n\\nAnother prominent line of work for rule-based \\nmethods is that of casting the pattern learning \\nproblem as a graph-based problem and leverag-\\ning graph-based algorithms (Kozareva et al., \\n2008; Vacareanu et al., 2022a).\\n\\n2.1.2. Neural-based Methods\\n\\nThe adoption of neural-based methods has grown \\nsignificantly due to their high performance, making \\nthem the de facto approach for relation extraction \\ntasks today. Many underlying architectures were \\nproposed for relation extraction, such as ones \\nbased on CNNs (Zeng et al., 2014; Nguyen and \\nGrishman, 2015), RNNs (Zhang and Wang, 2015), \\nLSTMs (Zhang et al., 2017), or, more recently, \\nTransformers (Vaswani et al., 2017; Joshi et al., \\n2019). These approaches typically operate end-to-\\nend and are built on top of pre-trained embeddings, \\neither static (Mikolov et al., 2013; Pennington et al., \\n2014; Bojanowski et al., 2016) or contextual (Mc-\\ncann et al., 2017; Peters et al., 2018; Devlin et al., \\n2019).\\n\\nA more recent direction has been translating the \\nrelation extraction task into a different NLP task \\nto leverage more training data (Chen et al., \\n2022). For example, relation extraction can be cast as an \\nentailment problem (Sainz et al., 2021; Rahimi and \\nSurdeanu, 2023), or as summarization (Lu et al., \\n2022).\\n\\nA distinctive direction emerged in the last years, \\nattempting to combine the advantages of both rule-\\nbased systems and neural-based systems. For ex-\\nample, Vacareanu et al. (2022b) teaches a neural \\nnetwork to generate rules for RE. Other directions \\naiming to improve the explainability of the resulting \\nmodel include: (i) learning an explainability classi-\\nfier jointly with the RE model to ensure faithfulness \\nof explanations (Tang and Surdeanu, 2021, 2023), \\nor (ii) learning a neural \u201csoft\u201d (or semantic) matcher \\nto improve the rules\u2019 recall (Zhou et al., 2020).\\n\\n2.2. Datasets and Methods for Few-Shot \\nRelation Extraction\\n\\nA key contribution to the RE space is the creation \\nof datasets that support the development of new \\nRE approaches. A recent survey (Bassignana and \\nPlank, 2022) categorized popular relation classifi-\\ncation datasets based on their data sources into \\nthree main categories: (i) News and Web, (ii) Sci-\\nentific Publications, and (iii) Wikipedia, totaling 17 \\ndatasets. We refer the reader to this survey for \\nmore details.\\n\\nAn important and realistic setting for this task is \\nfew-shot relation extraction (FSRE), where only a \\nsmall number of training examples are available \\nfor each relation class to be learned. Notably, only \\nthree datasets are available in a few-shot format \\n(Bassignana and Plank, 2022): FewRel (Han et al., \\n2018), FewRel 2.0 (Gao et al., 2019), and few-shot \\nTACRED (Sabo et al., 2021).\\n\\nThe FewRel dataset, containing 70,000 sen-\\ntences covering 100 relations from Wikipedia, is \\ncreated by identifying relation mentions through \\ndistant supervision; noise is subsequently filtered \\nby crowd-workers (Han et al., 2018). Later on, \\nthe FewRel 2.0 dataset (Gao et al., 2019), an ex-\\ntension of the original FewRel dataset (Han et al., \\n2018), introduced a new test set in a distinct do-\\nmain and included the option of a NOT A (None of \\nthe Above) relation.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sabo et al. (2021) argues that FewRel provides an unrealistic benchmark due to its uniform relation distribution and the prevalence of proper nouns as entities. Although FewRel 2.0 tried to amend it using an updated episode sampling procedure, the evaluation setup is still notably unrealistic (Sabo et al., 2021). As a solution, Sabo et al. (2021) converted the supervised T ACRED dataset (Zhang et al., 2017) into a few-shot T ACRED variant by applying realistic episode sampling. Concretely, the episode in an FSRE evaluation should be selected in a way that follows all the criteria (a\u2013f) we mention in Section 3.4. To develop our other few-shot datasets, i.e., NYT29 and WIKIDA T A, we followed a similar strategy (see \u00a73.4 and 3.5).\\n\\nNevertheless, despite the unquestionable contribution of such datasets to the RE field, we observed a lack of consistency in the results observed in the various proposed evaluations. For example, some methods evaluated on FewRel at-tained an accuracy of 93.9%, surpassing human-level performance at 92.2% (Soares et al., 2019). While FewRel 2.0 yields lower results, i.e., the best method achieved 80.3% (Gao et al., 2019), they are still remarkably high, given the challenging nature of the task.\\n\\nFurther, (Sabo et al., 2021) evaluated their MNA V model (which was state-of-art at the time) on FewRel 2.0 and achieved an F1 score of approximately 78% for 5-way 1-shot and 80% for 5-way 5-shot, whereas the best results on T ACRED are much lower: the F1 score is 12.4% for 5-way 1-shot and 30.0% for 5-way 5-shot. These differences are caused by differences in how the datasets are constructed, which impacts consistent analyses of the proposed methods. To remedy this issue, we propose a meta dataset for few-shot RE that includes three datasets that are constructed using the same realistic procedure and capture multiple important phenomena. This allows us to rigorously evaluate multiple approaches for few-shot RE as shown in \u00a75.2.\\n\\n3. Dataset Construction Process\\n\\nWe detail next our first key contribution: the construction of a meta dataset for FSRE, which combines two new FSRE datasets and a third existing one.\\n\\n3.1. Data Sources\\n\\nWe leverage three existing supervised datasets for RE to serve as our starting point. These datasets cover a diverse set of domains: NYT29, WIKIDA T A, and T ACRED.\\n\\nNYT29: The NYT29 dataset originates from the New York Times corpus, which comprises a collection of more than 1.8 million articles authored and released by the New York Times between January 1, 1987, and June 19, 2007, with article metadata provided by the New York Times Newsroom (Sandhaus, 2008). This dataset was annotated with relations from Freebase using distant supervision by Riedel et al. (2010). Depending on how many relation classes are kept, this original dataset has multiple versions, e.g., \\\"NYT10,\\\" \\\"NYT11,\\\" and \\\"NYT29\\\" (Takanobu et al., 2019; Nayak and Ng, 2020). Our work relies on the latter version, which contains 29 distinct relations (e.g., /people/person/place_lived), and it covers a wide range of topics, news events, and perspectives.\\n\\nWIKIDA T A: The WIKIDA T A dataset is a subset of Wikipedia, wherein articles have been marked with Wikidata relations using distant supervision (Sorokin and Gurevych, 2017). This corpus encompasses two primary types of annotations: entities and relations. Entity annotations are derived from Wikipedia article links. Each link has been converted to a Wikidata identifier using the mappings from the Wikidata itself. Additional entities are recognized using a named entity recognizer and are linked to Wikidata.\\n\\nT ACRED: Unlike the previous two datasets, which were annotated using distant supervision, T ACRED was manually annotated for 42 relation classes from the T AC KBP challenge (Surdeanu and Heng, 2014) (e.g., per:schools_attended and org: members) plus no_relation. The dataset contains 106,264 RE examples, which were annotated over textual data from both newswire sources and the corpus employed in the annual T AC Knowledge Base Population (T AC KBP) challenges (Zhang et al., 2017). These examples are generated by merging human annotations obtained from the T AC KBP challenges and crowdsourcing. It is important to note that these datasets capture distinct phenomena that are important for RE: (1) NYT29 and WIKIDA T A were annotated using distant supervision, whereas T ACRED was manually annotated. It is known that distant supervision introduces label noise (Riedel et al., 2010). This is particularly important for the negative class, i.e., in the case of distant supervision, negative labels can be false negatives. That is, they should not be interpreted as \\\"no known relation label applies\\\" but rather as \\\"we have no information about this entity pair in the knowledge base.\\\" This impacts the sampling procedure discussed later in this section.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NYT29 allows multiple relations to exist between the same two entities in the same sentence. For example, in the sentence \\\"Mr. Mashal, speaking in Damascus, Syria, said \u2026\\\" and the entity pair \\\"Damascus\\\" and \\\"Syria\\\" is annotated with two relations: administrative_divisions and capital. Because of this, multi-label RE classifiers may have an advantage on NYT29.\\n\\nWIKIDATA allows for overlapping entities. For example, in the sentence \\\"\u2026featuring Lon Chaney and Andrew Lloyd Webber's 1986 musical.\\\" and the entity pair \\\"1986\\\" and \\\"1986 musical\\\" is annotated with first_performance. This is likely to confuse methods that rely on entity markers (Zhou and Chen, 2022).\\n\\n3.2. Linguistic Annotations\\nSince some of these datasets were not accompanied by linguistic annotations, we processed the texts in house to guarantee that the same linguistic information is available for all three datasets. For all linguistic annotations, we used the processors library.\\n\\nThis library uses LSTM-CRF (Lample et al., 2016) for case restoration, part-of-speech (POS) tagging, named entity recognition (NER), and the method of Vacareanu et al. (2020) for dependency parsing.\\n\\n3.2.1. NYT29\\nIn the original NYT29 dataset, the texts in the three partitions (train, dev, test) were initially presented in lowercase, which led to certain inaccuracies during linguistic annotation. To solve this problem, we first restored case using the LSTM-CRF in the processors library. On a small sample, we observed that this restoration is over 95% accurate. We then tokenized the text and applied POS tagging, NER, and dependency parsing. However, to determine the subject and object type for each relation mentioned, we used the provided gold entity labels in the original dataset (see Table 1).\\n\\nWe observed that a small number of sentences in the NYT29 dataset were not parsed into a dependency tree by the processors parser (i.e., the parser produced several subtrees that covered different sentence fragments). The main cause of this error was long and complex sentences. However, the number of sentences with such errors was small: 0.1% of the training sentences, 0.07% in dev, and 0.1% in the test. For simplicity, we removed these sentences from train and dev, and, in order to not modify the test partition, we manually corrected the parse trees for the sentences in the test.\\n\\nTable 1: An example from NYT29 with gold and predicted entity labels. We used the gold entity labels for this dataset.\\n\\n| Dataset   | Entity Labeling Scheme |\\n|-----------|------------------------|\\n| NYT29     | Gold labels            |\\n| WIKIDATA  | Predicted labels       |\\n| TACRED    | Predicted labels       |\\n\\nTable 2: Labeling scheme for entities participating in relations in the three datasets considered.\\n\\n3.2.2. WIKIDATA\\nFor WIKIDATA, we used the same NLP library for tokenization, POS tagging, NER, and dependency parsing. Case restoration was not needed for the WIKIDATA sentences. However, one important difference between NYT29 and WIKIDATA is that the labels for entities participating in relations in WIKIDATA are limited to just two: \\\"Lexical\\\" for named entities, and \\\"Date\\\" for dates. To increase the informativeness of entity labels, we adopted the labels predicted by the processors NER if they overlap with the span of the entity labels in WIKIDATA. If no predicted NE overlaps with a relation entity, we keep the default WIKIDATA entity label.\\n\\n3.2.3. TACRED\\nIn the TACRED dataset, essential NLP tasks, i.e., POS tagging, NER, and dependency parsing, were performed using Stanford CoreNLP (Manning et al., 2014) and included in the original dataset. To maintain compatibility with previous works, we keep the same linguistic annotations. Importantly, TACRED and our version of WIKIDATA use labels predicted by a NER for the entities participating in a relation, whereas NYT29 uses gold labels. Table 2 summarizes this information.\\n\\n3.3. Negative Class Label Standarization\\nThe concept of negative relations refers to instances where the relation between two entities either does not fit into any predefined categories, or\"}"}
{"id": "lrec-2024-main-1442", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"it may indicate that there is no relation between them at all. Note that negative labels are handled differently in the three datasets considered:\\n\\n1. NYT29 contains no annotations for the negative relation label. In this situation, we introduce negative examples using the supervised-to-few-shot transformation described in Section 3.4 and Algorithm 1.\\n\\n2. In contrast, TACRED and WIKIDA T A explicitly annotate some negative relations between entity pairs that co-occur in the same sentence (TACRED uses the no_relation label, while WIKIDA T A uses P0).\\n\\nThe above differences impact the few-shot version of these datasets (see \u00a73.4) and, thus, the performance of few-shot RE models. Lastly, we standardize the label for negative relations to NOTA across the three datasets.\\n\\nTo increase reproducibility, after all these preprocessing steps were applied, we formatted all three datasets using the same tabular format. The format is described in Appendix A. This is the same format the TACRED dataset used. We followed the exact format so that we could apply the transformation technique of converting the supervised dataset into the few-shot dataset described in (Sabo et al., 2021).\\n\\n3.4. Supervised to Few-Shot Transformation\\n\\nWe transform the supervised NYT29, TACRED, and WIKIDA T A datasets into FSRE datasets by applying a generalized form of the transformation method described in (Sabo et al., 2021). This process transforms a supervised dataset into a realistic FSRE dataset by following a series of constraints that are likely to occur in real-world applications:\\n\\n(a) The test (or \u201ctarget\u201d) relation classes are different from any of relations that might be available in a background dataset (\u201cbackground relations\u201d);\\n(b) The number of training examples $K$ for each target relation class is very small (typically 1 or 5);\\n(c) The distribution of relations is not uniform, i.e., some relations are rarer than others;\\n(d) Most candidate relation mentions do not correspond to a target relation;\\n(e) Many relation candidates seen in testing may not correspond also to a background relation.\\n\\nThus, a traditional supervised RE classifier that trains on the background data is not applicable;\\n\\n(f) Entities participating in relations may include named entities, as well as pronouns and common nouns.\\n\\nBefore we formalize the transformation process, we introduce some necessary notations:\\n\\n$C$ \u2013 A set of known relation classes in a dataset partition.\\n\\nNOTA \u2013 The relation class NOTA (None-of-the-above) is assigned to entity pairs whose corresponding relation class is not in the applicable $C$ set. Note that this is different from the no_relation label used in the supervised datasets. In the FSRE setting, NOTA includes both no_relation examples as well as all positive relation labels that are not used in the dataset partition at hand (Sabo et al., 2021).\\n\\n$D$ \u2013 A relation classification dataset such that $D$: \\\\{($x_i$, $c_i$)\\\\} $i=1$ $n$, where $\\\\forall c_i \\\\in C \\\\cup \\\\{NOTA\\\\}$.\\n\\n$x_i$ \u2013 Represents the $i$-th instance in a RE dataset $D$ such that $x_i = (e_1, e_2, s_i)$ where $e_1$ and $e_2$ represent a pair of entities in a sentence $s$, where the relation between this two entity is labeled $c_i$.\\n\\n$N$-Way $K$-Shot \u2013 We follow the $N$-way $K$-shot setup for FSRE, as proposed by (Vinyals et al., 2016; Snell et al., 2017). In an $N$-way $K$-shot setup, a classifier aims to discriminate between $N$ target relation classes using only a support set $K$ examples of each. Typically, $K$ is a very small number, e.g., 1 or 5.\\n\\nAlgorithm 1 describes the transformation process of a supervised RE dataset $D$ containing relation labels $C$ into a few-shot dataset $D_{FS}$, $C_{FS}$.\\n\\nThe two key steps of the transformation algorithm are as follows. First, we split the original dataset into three partitions (train/dev/test) such that they are pairwise disjoint with respect to the positive relations they contain (steps 1 and 2). For example, if the train partition contains the relation country of origin, this relation is not allowed to appear in dev and test. Second, for each partition, we convert all relation labels that are assigned to another partition to NOTA (steps 3 and 4). Table 3 shows an example of the transformation process for WIKIDA T A.\\n\\n3.5. Episode Sampling\\n\\nThe small number of examples per class in FSRE ($K$) may introduce statistical instability in the results observed. To address this, episodic learning repeats the training/evaluation of a given method over a large number of episodes that sample different support sentences for the given classes. More formally, for a $N$-way $K$-shot setup an episode $E$ consists of three items:\"}"}
{"id": "lrec-2024-main-1442", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\nTransformation of a supervised RE dataset to few-shot RE using the N-way K-shot setup\\n\\nInput: $D$, $C$\\nOutput: $D_{FS}$, $C_{FS}$\\n\\nStep 0: Replace no_relation with NOTA in $D$; remove no_relation from $C$, if present\\n\\nStep 1: Split $D$ in $D_{train}$, $D_{dev}$, and $D_{test}$\\n\\nStep 2: Choose a random split of $C$ as $C_{train}$, $C_{dev}$, and $C_{test}$ such that the following two conditions are true:\\n\\n(a) $C_{train}$, $C_{dev}$, and $C_{test}$ be pairwise disjoint\\n(b) $|C_{train}|$, $|C_{dev}|$, and $|C_{test}| \\\\geq N$ (for N-way K-shot)\\n\\nStep 3: for each $(x_i, c_i) \\\\in D_{train}$ do\\nif $c_i / \\\\in C_{train}$ then $c_i = \\\\text{NOTA}$\\nelse Retain the original label\\n\\nStep 4: Repeat Step 3 for $D_{dev}$ and $D_{test}$ using their corresponding $C_{dev}$, and $C_{test}$ label sets\\n\\nStep 5: $C_{train} = C_{train} \\\\cup \\\\{\\\\text{NOTA}\\\\}$\\n$C_{dev} = C_{dev} \\\\cup \\\\{\\\\text{NOTA}\\\\}$\\n$C_{test} = C_{test} \\\\cup \\\\{\\\\text{NOTA}\\\\}$\\n\\nStep 6: $C_{FS} = (C_{train}, C_{dev}, C_{test})$\\n$D_{FS} = (D_{train}, D_{dev}, D_{test})$\\nreturn $C_{FS}$, $D_{FS}$\\n\\n(a) Randomly chosen target relations: $C_{target} = \\\\{c_1, c_2, \\\\ldots, c_N\\\\}$ s.t. $c_1 \\\\ldots N / \\\\in \\\\{\\\\text{NOTA}\\\\}$\\n\\n(b) A randomly chosen support set of size $K$ for each of the $N$ relations:\\n$X_{supt} = \\\\{X_1, X_2, \\\\ldots, X_i, \\\\ldots, X_N\\\\}$\\n$X_i = \\\\{(x_1, c_i), (x_2, c_i), \\\\ldots, (x_j, c_i), \\\\ldots, (x_K, c_i)\\\\}$\\n\\n(c) A randomly chosen labeled example as a query $Q = (x_q, c_q)$ such that $(x_q, c_q) / \\\\in X_{supt}$.\\n\\nGiven an episode $E = (C_{target}, X_{supt}, Q)$, the goal of a Few-Shot learning classifier is to create a decision function to choose a label from $C_{target} \\\\cup \\\\{\\\\text{NOTA}\\\\}$ for the given query $Q$. We describe a general approach of N-Way K-Shot episode sampling procedure in the Algorithm 2, where $D_E$ and $C_E$ are input dataset and labels.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Dataset Statistics\\n\\nTable 4 summarizes key statistics for the three supervised datasets that serve as the starting point for FSRE. We chose three datasets with a significant variation in the number of relations. Table 4 shows that TACRED has 42 relations, NYT29 has 29 relations, and WIKIDA T A has 352 relations, which is much larger. Additionally, when we look at the NOTA instances, these three datasets differ enormously. For instance, in the supervised NYT29, there are no NOTA instances. In supervised TACRED, the number of NOTA instances is higher than the number of NOTA instances in the supervised WIKIDA T A. Table 5 summarizes how the number of relation instances and NOTA instances in three resulting FS datasets have been changed from supervised datasets. In Appendix B, we present further statistics and analysis demonstrating that our FSRE meta-dataset meets all the requirements of a realistic few-shot relation extraction dataset.\\n\\n| Dataset     | Train Size | Dev Size | Test Size | Relation Class | Relation Instances | NOTA Instances |\\n|-------------|------------|----------|-----------|----------------|--------------------|----------------|\\n| TACRED      | 68,124     | 22,631   | 15,509    | 42             | 21,773             | 84,491         |\\n| NYT29       | 78,885     | 5859     | 8759      | 29             | 93,503             | 0              |\\n| WIKIDA T A  | 775,919    | 251,802  | 739,408   | 352            | 1,299,085          | 468,044        |\\n\\nTable 5: Statistics of the Few-Shot TACRED, NYT29, and WIKIDA T A datasets.\\n\\n5. Experimental Results\\n\\n5.1. Experimental Setup\\n\\nWe applied the transformation techniques outlined in \u00a73.4 and \u00a73.5 on all datasets described in the previous section to produce their FSRE variants. We tested on all datasets in 5-way 1-shot and 5-way 5-shot scenarios. In both cases, we repeat the procedure with 5 different random seeds.\\n\\n5.2. Models\\n\\nWe evaluated the following baselines and models:\\n\\n- **Unsupervised Baseline** \u2013 This baseline model uses solely the entity types in both the query sentence and the support sentences for classification during inference (Vacareanu et al., 2022b). If there are support sentences with the same entity types as the query sentence, the model randomly chooses one and predicts its relation. In other cases, the baseline predicts NOTA.\\n\\n- **Sentence-Pair** \u2013 We implement a baseline similar to (Gao et al., 2019), which operates as follows: We pair each query sentence to each support sentence and feed the concatenated text to a sentence transformer (Reimers and Gurevych, 2019) to obtain a single score that quantifies the degree to which both sentences convey the same underlying relation. During training, we fine-tune the model to maximize the score between sentences with the same relation and minimize the score between sentences with different relation (or NOTA). During inference, we predict the relation associated with the highest score, provided it is above a threshold tuned on the development partition. Otherwise, we predict NOTA. We use a pre-trained model and show results with and without fine-tuning.\\n\\n- **MNAV** \u2013 Multiple NOTA Vectors (MNAV) is an extended version of the NAV method, which computes a score between the query vector, each support sentence vector, and, additionally, a learned vector for the NOTA class (Sabo et al., 2021). Instead of just one vector for NOTA, MNAV uses multiple vectors to account for the fact that NOTA is a \\\"catch all\\\" for all other relations. The number of NOTA vectors is tuned on the development set. In the classification process, the model selects the nearest vector to the query representation to establish the predicted relation label.\\n\\n- **OdinSynth** \u2013 OdinSynth is a transformer-based rule synthesis model that generates rules from the provided support sentences and then applies these rules to the query sentence (Vacareanu et al., 2022b). If none of the rules match, the model predicts NOTA. If there exists a match with one or more rules, the model predicts the relation through majority voting.\\n\\n- **Hard-Matching Rules** \u2013 Represent lexico-syntactic rules created over the shortest path connecting the two entities.\\n\\n- **Soft-Matching Rules** \u2013 This is a neuro-symbolic model (Vacareanu et al., 2024) that aims to increase the recall of rules by leveraging the high expressivity of neural networks. The method first...\"}"}
{"id": "lrec-2024-main-1442", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: The results for the 5-way 1-shot and 5-way 5-shot settings on the test partition of the FS TACRED dataset.\\n\\n| Model                  | P    | R    | F1   |\\n|------------------------|------|------|------|\\n| Unsupervised Baseline  | 5.70 | 91.02| 10.73|\\n| Sentence-Pair (not fine-tuned) | 3.90 | 5.21 | 4.45 |\\n| Sentence-Pair (fine-tuned) | 6.89 | 28.56| 11.10|\\n| MNA V                  | 15.11| 8.47 | 10.85|\\n| OdinSynth              | 23.48| 11.46| 15.40|\\n| Hard-matching Rules    | 51.35| 2.94 | 5.56 |\\n| Soft-matching Rules    | 33.46| 19.69| 24.78|\\n\\nTable 7: The results for the 5-way 1-shot and 5-way 5-shot settings on the test partition of the FS NYT dataset.\\n\\n| Model                  | P    | R    | F1   |\\n|------------------------|------|------|------|\\n| Unsupervised Baseline  | 2.52 | 29.99| 4.64 |\\n| Sentence-Pair (not fine-tuned) | 6.40 | 2.55 | 3.65 |\\n| Sentence-Pair (fine-tuned) | 6.65 | 7.99 | 7.26 |\\n| MNA V                  | 17.49| 6.76 | 9.74 |\\n| OdinSynth              | 12.99| 6.15 | 8.34 |\\n| Hard-matching Rules    | 6.38 | 0.38 | 0.72 |\\n| Soft-matching Rules    | 35.88| 2.73 | 5.06 |\\n\\nTable 8: The results for the 5-way 1-shot and 5-way 5-shot settings on the test partition of the FS WIKI-DATA dataset.\\n\\n| Model                  | P    | R    | F1   |\\n|------------------------|------|------|------|\\n| Unsupervised Baseline  | 2.52 | 29.99| 4.64 |\\n| Sentence-Pair (not fine-tuned) | 6.40 | 2.55 | 3.65 |\\n| Sentence-Pair (fine-tuned) | 6.65 | 7.99 | 7.26 |\\n| MNA V                  | 17.49| 6.76 | 9.74 |\\n| OdinSynth              | 12.99| 6.15 | 8.34 |\\n| Hard-matching Rules    | 6.38 | 0.38 | 0.72 |\\n| Soft-matching Rules    | 35.88| 2.73 | 5.06 |\\n\\nResults Analysis:\\n\\nTable 6, 7, 8 represent the result of different models on our resulting FSRE datasets. We draw the following conclusions:\\n\\nFirst, no single method emerges as the clear top performer across all scenarios. For instance, Soft-matching Rules achieves the highest performance on Few-Shot TACRED (Table 6), MNA V excels on Few-Shot WIKI-DATA (Table 8), and in the case of Few-Shot NYT, MNA V performs best for 1-shot, while Sentence-Pair leads for 5-shot (Table 7). The latter result is surprising, given the simplicity of this baseline.\\n\\nSecond, the performance varies drastically between the datasets for both 1-shot and 5-shot scenarios. For instance, in Few-Shot WIKI-DATA 5-way 1-shot, the top-performing method achieves...\"}"}
{"id": "lrec-2024-main-1442", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"an F1 score of 9.74, whereas in Few-Shot T A-CRED 5-way 1-shot, the best method reaches an F1 score of 24.78. This underscores the importance of employing multiple evaluation datasets to gain a realistic assessment of a model's performance. Furthermore, the overall performance across all datasets is low, which indicates a substantial need for future research in this domain.\\n\\nIn our evaluation of the six models, FS WIKI-DATA exhibited comparatively lower performance across all datasets. To understand the underlying reasons, we conducted a qualitative error analysis on FS WIKIDA T A, the details of which are provided in Appendix D.\\n\\n6. Conclusion\\n\\nIn this paper, we presented a meta dataset for few-shot relation extraction (FSRE), which comprises three FSRE datasets: two were derived from established supervised relation extraction datasets, while one is an existing FSRE dataset. All datasets were intricately derived to replicate real-world scenarios, ensuring a strong alignment with real-world contexts. Then, we assessed six relation extraction methods on this meta dataset and found that no single model consistently performs well across all scenarios. This suggests the need for future research in this domain.\\n\\nAs future work, we plan to leverage the resulting dataset to develop methods that demonstrate consistent and robust performance.\\n\\n7. Acknowledgements\\n\\nThis work was partially supported by the Defense Advanced Research Projects Agency (DARP A) under the Habitus and Automating Scientific Knowledge Extraction and Modeling (ASKEM) programs. Mihai Surdeanu declares a financial interest in lum.ai. This interest has been properly disclosed to the University of Arizona Institutional Review Committee and is managed in accordance with its conflict of interest policies.\\n\\n8. Bibliographical References\\n\\nEneko Agirre. 2022. Few-shot information extraction is here: Pre-train, prompt and entail. Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval.\\n\\nElisa Bassignana and Barbara Plank. 2022. What do you mean by relation extraction? a survey on datasets and study on scientific relation classification. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop, pages 67\u201383, Dublin, Ireland. Association for Computational Linguistics.\\n\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2016. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146.\\n\\nLihu Chen, Simon Razniewski, and Gerhard Weikum. 2023. Knowledge base completion for long-tail entities. arXiv preprint arXiv:2306.17472.\\n\\nMuhao Chen, Lifu Huang, Manling Li, Ben Zhou, Heng Ji, and Dan Roth. 2022. New frontiers of information extraction. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Tutorial Abstracts.\\n\\nAmir Cohen, Shachar Rosenman, and Yoav Goldberg. 2020. Relation extraction as two-way span-prediction. ArXiv, abs/2010.04829.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics.\\n\\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng Li, Maosong Sun, and Jie Zhou. 2019. Fewrel 2.0: Towards more challenging few-shot relation classification. In Conference on Empirical Methods in Natural Language Processing.\\n\\nKyle Gorman and Steven Bedrick. 2019. We need to talk about standard splits. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2786\u20132791, Florence, Italy. Association for Computational Linguistics.\\n\\nS. Gupta and Christopher D. Manning. 2014. Improved pattern learning for bootstrapped entity extraction. In CoNLL.\\n\\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yao, Zhiyuan Liu, and Maosong Sun. 2018. Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation. In Conference on Empirical Methods in Natural Language Processing.\\n\\nHany Hassan, Ahmed Hassan Awadallah, and Ossama Emam. 2006. Unsupervised information\"}"}
{"id": "lrec-2024-main-1442", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"extraction approach using graph mutual reinforcement. In *EMNLP*. Marti A. Hearst. 1992. \\n\\nAutomatic acquisition of hyponyms from large text corpora. In *COLING 1992 Volume 2: The 14th International Conference on Computational Linguistics*. Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva, Preslav Nakov , Diarmuid \u00d3 S\u00e9aghdha, Sebasti\u00e1n Pad\u00f3, Marco Pennacchiotti, Lorenza Romano, and Stan Szpakowicz. 2010. \\n\\nSemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals. In *Proceedings of the 5th International Workshop on Semantic Evaluation*, pages 33\u201338, Uppsala, Sweden. Association for Computational Linguistics. Mandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld, Luke Zettlemoyer , and Omer Levy . 2019. \\n\\nSpanbert: Improving pre-training by representing and predicting spans. *Transactions of the Association for Computational Linguistics*, 8:64\u201377. Yu Su Kai Zhang, Bernal Jim\u00e9nez Guti\u00e9rrez. 2023. \\n\\nAligning instruction tasks unlocks large language models as zero-shot relation extractors. In *Findings of ACL*. Zornitsa Kozareva, Ellen Riloff, and Eduard H. Hovy . 2008. Semantic class learning from the web with hyponym pattern linkage graphs. In *ACL*. Guillaume Lample, Miguel Ballesteros, Sandeep Subramanian, Kazuya Kawakami, and Chris Dyer . 2016. Neural architectures for named entity recognition. *arXiv preprint arXiv:1603.01360*. K. Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, and Muhao Chen. 2022. \\n\\nSummarization as indirect supervision for relation extraction. *ArXiv*, abs/2205.09837. Christopher D. Manning. 2015. Computational linguistics and deep learning. *Computational Linguistics*, 41:701\u2013707. Christopher D Manning, Mihai Surdeanu, John Bauer , Jenny Rose Finkel, Steven Bethard, and David McClosky . 2014. The stanford corenlp natural language processing toolkit. In *Proceedings of 52nd annual meeting of the association for computational linguistics: system demonstrations*, pages 55\u201360. Bryan McCann, James Bradbury , Caiming Xiong, and Richard Socher . 2017. \\n\\nLearned in translation: Contextualized word vectors. In *Neural Information Processing Systems*. Tomas Mikolov , Ilya Sutskever , Kai Chen, Geoffrey S. Corrado, and Jeffrey Dean. 2013. \\n\\nDistributed representations of words and phrases and their compositionality. In *Neural Information Processing Systems*. Makoto Miwa and Mohit Bansal. 2016. End-to-end relation extraction using lstms on sequences and tree structures. *arXiv preprint arXiv:1601.00770*. Tapas Nayak and Hwee Tou Ng. 2020. Effective modeling of encoder-decoder architecture for joint entity and relation extraction. In *Proceedings of the AAAI conference on artificial intelligence*, volume 34, pages 8528\u20138535. Thien Huu Nguyen and Ralph Grishman. 2015. \\n\\nRelation extraction: Perspective from convolutional neural networks. In *VS@HLT-NAACL*. Jeffrey Pennington, Richard Socher , and Christopher D. Manning. 2014. \\n\\nGlove: Global vectors for word representation. In *Conference on Empirical Methods in Natural Language Processing*. Matthew E. Peters, Mark Neumann, Mohit Iyyer , Matt Gardner , Christopher Clark, Kenton Lee, and Luke Zettlemoyer . 2018. \\n\\nDeep contextualized word representations. *ArXiv*, abs/1802.05365. Mahdi Rahimi and Mihai Surdeanu. 2023. \\n\\nImproving zero-shot relation classification via automatically-acquired entailment templates. In *Workshop on Representation Learning for NLP*. Nils Reimers and Iryna Gurevych. 2019. \\n\\nSentence-bert: Sentence embeddings using siamese bert-networks. In *Conference on Empirical Methods in Natural Language Processing*. Sebastian Riedel, Limin Yao, and Andrew McCallum. 2010. Modeling relations and their mentions without labeled text. In *Machine Learning and Knowledge Discovery in Databases: European Conference, ECML PKDD 2010, Barcelona, Spain, September 20-24, 2010, Proceedings, Part III* 21, pages 148\u2013163. Springer . Ellen Riloff. 1993. Automatically constructing a dictionary for information extraction tasks. In *AAAI*. Ellen Riloff. 1996. Automatically generating extraction patterns from untagged text. In *AAAI/IAAI*, Vol. 2. Ellen Riloff and R. Jones. 1999. Learning dictionaries for information extraction by multi-level bootstrapping. In *AAAI/IAAI*.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ellen Riloff and Janyce Wiebe. 2003. Learning extraction patterns for subjective expressions. In EMNLP.\\n\\nDan Roth and Wen-tau Yih. 2004. A linear programming formulation for global inference in natural language tasks. In Proceedings of the Eighth Conference on Computational Natural Language Learning (CoNLL-2004) at HL T-NAACL 2004, pages 1\u20138, Boston, Massachusetts, USA. Association for Computational Linguistics.\\n\\nOfer Sabo, Yanai Elazar, Yoav Goldberg, and Ido Dagan. 2021. Revisiting few-shot relation classification: Evaluation data and classification schemes. Transactions of the Association for Computational Linguistics, 9:691\u2013706.\\n\\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, Ander Barrena, and Eneko Agirre. 2021. Label verbalization and entailment for effective zero and few-shot relation extraction. ArXiv, abs/2109.03659.\\n\\nEvan Sandhaus. 2008. The new york times annotated corpus. Web Download. LDC2008T19.\\n\\nJake Snell, Kevin Swersky, and Richard Zemel. 2017. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30.\\n\\nLivio Baldini Soares, Nicholas FitzGerald, Jeffrey Ling, and Tom Kwiatkowski. 2019. Matching the blanks: Distributional similarity for relation learning. arXiv preprint arXiv:1906.03158.\\n\\nDaniil Sorokin and Iryna Gurevych. 2017. Context-aware representations for knowledge base relation extraction. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1784\u20131789.\\n\\nMihai Surdeanu and Ji Heng. 2014. Overview of the english slot filling track at the tac2014 knowledge base population evaluation. In Proceedings of the TAC-KBP 2014 Workshop.\\n\\nMihai Surdeanu and Christopher D. Manning. 2010. Ensemble models for dependency parsing: Cheap and good? In Proceedings of the North American Chapter of the Association for Computational Linguistics Conference (NAACL-2010), Los Angeles, CA.\\n\\nRyuichi Takanobu, Tianyang Zhang, Jiexi Liu, and Minlie Huang. 2019. A hierarchical framework for relation extraction with reinforcement learning. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 7072\u20137079.\\n\\nZheng Tang and Mihai Surdeanu. 2021. Interpretability rules: Jointly bootstrapping a neural relation extractor with an explanation decoder. Proceedings of the First Workshop on Trustworthy Natural Language Processing.\\n\\nZheng Tang and Mihai Surdeanu. 2023. Bootstrap-predicting neural relation and explanation classifiers. In Annual Meeting of the Association for Computational Linguistics.\\n\\nRobert Vacareanu, Fahmida Alam, Md Asiful Islam, Haris Riaz, and Mihai Surdeanu. 2024. Best of both worlds: A pliable and generalizable neuro-symbolic approach for relation classification. In Findings of the Association for Computational Linguistics: NAACL 2024. Association for Computational Linguistics.\\n\\nRobert Vacareanu, George C. G. Barbosa, Marco A. Valenzuela-Escarcega, and Mihai Surdeanu. 2020. Parsing as tagging. In Proceedings of the 12th International Conference on Language Resources and Evaluation (LREC).\\n\\nRobert Vacareanu, Dane Bell, and Mihai Surdeanu. 2022a. Patternrank: Jointly ranking patterns and extractions for relation extraction using graph-based algorithms. In PANDL.\\n\\nRobert Vacareanu, Marco A. Valenzuela-Escarcega, George Caique Gouveia Barbosa, Rebecca Sharp, Gustave Hahn-Powell, and Mihai Surdeanu. 2022b. From examples to rules: Neural guided rule synthesis for information extraction. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6180\u20136189, Marseille, France. European Language Resources Association.\\n\\nAshish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Neural Information Processing Systems.\\n\\nOriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. 2016. Matching networks for one shot learning. Advances in neural information processing systems, 29.\\n\\nLinlin Wang, Zhu Cao, Gerard De Melo, and Zhiyuan Liu. 2016. Relation classification via multi-level attention cnns. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1298\u20131307.\\n\\nJanyce Wiebe and Ellen Riloff. 2005. Creating subjective and objective sentence classifiers from unannotated texts. In CICLing.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9 summarizes the tabular format used to represent the three supervised datasets that are the starting point of the few-shot datasets generated in this work.\\n\\n| Field | Description |\\n|-------|-------------|\\n| id | Incremental unique ID for each example or sentence |\\n| docid | For dev set docid = \\\"dev\\\", for test set docid= \\\"test\\\", and for train set docid = \\\"train\\\" |\\n| relation | This field denotes the relation labels between the given entities |\\n| token | An instance of a sequence or word in the sentence |\\n| subj_start | Start index of the subject in a sentence |\\n| subj_end | End index of the subject in a sentence |\\n| obj_start | Start index of the object in a sentence |\\n| obj_end | End index of the object in a sentence |\\n| subj_type | Subject type (e.g., person name) in a sentence |\\n| obj_type | Object type (e.g., person name) in a sentence |\\n| stanford_pos | POS tag of the current token |\\n| stanford_ner | Named entity label of the current token |\\n| stanford_head | 1-based index of the dependency head of the current token |\\n| stanford_deprel | dependency relation of the current token to its head token |\\n\\nNote that the \\\"stanford\\\" prefix for the last three columns is maintained for compatibility with the TACRED format; in NYT29 and WIKIDAT, this information is generated using the processors library instead.\"}"}
{"id": "lrec-2024-main-1442", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Few-Shot TACRED top five relation distribution meta dataset fulfills all these six criteria. For instance, we split the original dataset into three partitions (train/dev/test) such that they are pairwise disjoint with respect to the positive relations they contain (as outlined in Steps 1 and 2 of Algorithm 1). This guarantees that the test relation classes in our dataset are distinct from any relations that might be present in a background dataset, thereby fulfilling constraint (a) of the realistic Few-Shot assumption. Moreover, we follow the 5-way 1-shot and 5-way 5-shot setup for episode sampling, which ensures that the number of training examples for each target relation class is very small (1 or 5), thus satisfying constraint (b). Figures 1, 2, 3 illustrate the non-uniformity of the relation classes and the predominance of NOTA class, indicative of satisfying realistic constraints (c), (d), and (e). Figures 4, 5, 6 indicate the presence of a variety of POS tags, with a notable percentage of proper nouns, common nouns, and pronouns, reflecting the diversity and realism of entity distributions, thus satisfying constraint (f).\\n\\nAppendix C\\n\\nZero-Shot LLM Baseline\\n\\nWe evaluated the Zero-Shot relation classification performance of the Large Language Model (LLM) using GPT-4. The experiment was conducted on a test set containing ten episodes, with each episode containing three test sentences. For each sentence, we prompted GPT-4 to identify a relation for a given entity pair using the prompting technique described by Kai Zhang (2023). The prompt includes the label verbalization technique to articulate the relations. We conducted the experiment NOT A /location/location/contains /people/person/nationality /people/person/place_lived /location/country/capital\\n\\nFigure 2: Few-Shot NYT29 top five relation distribution in both 5-way 1-shot and 5-way 5-shot configurations, where GPT-4 was tasked with classifying the relation for the given entity pair into one of the five target relations or indicating 'None of the Above' (NOTA) if none is applicable. Figure 7 illustrates an example of the prompt. The results of the experiment are presented in Tables 10, 11, and 12. In the tables, we included the performance scores of other models on the same test set to facilitate easier comparison. The results show that zero-shot LLM achieves low precision and high recall in FS TACRED (see Table 10) and FS NYT29 (see Table 11). The low precision is attributed to a high false positive rate, where\"}"}
{"id": "lrec-2024-main-1442", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given a sentence and two entities within the sentence, classify the relation between the two entities based on the provided sentence. All possible relations are listed below:\\n\\n- org:top_members/employees: Entity 1 has the high level member Entity 2\\n- per:schools_attended: Entity 1 studied in Entity 2\\n- org:founded_by: Entity 1 was founded by Entity 2\\n- per:origin: Entity 1 has the nationality Entity 2\\n- per:date_of_birth: Entity 1 has birthday on Entity 2\\n- NOTA: None of the above\\n\\nSentence: \\\"In an atmosphere of conflict and misunderstanding, the travel and tourism industry can be an incredibly powerful force for conciliation,\\\" said PATA president and chief executive officer Peter de Jong.\\n\\nEntity 1: PATA\\nEntity 2: Peter de Jong\\n\\nFigure 7: An example of prompt for Zero-Shot LLM baseline.\\n\\nGPT-4 often chose a positive relation from the target set instead of selecting NOTA when the correct relation was not among the target relations. However, when the correct relation is included in the target set, GPT-4 tends to identify it correctly, resulting in a high true positive rate. Although GPT-4 generally performs better than the other models, it is not always the best in every scenario. For example, in the FS NYT29 5-way 1-shot configuration, the MNA V model outperforms GPT-4, and in the FS WIKIDA T A 5-way 5-shot setup, the Unsupervised Baseline model performs better than GPT-4. This reinforces the conclusion drawn in section 5.3 that no single model consistently stands out as the best performer across all scenarios, underscoring the significant need for continued research in this field.\\n\\nSince a small test set was utilized in this experiment, further research is necessary to gain a deeper understanding of the Zero-Shot relation classification capabilities of Large Language Models.\\n\\nAppendix D\\nQualitative Error Analysis\\nIn the few-shot relation extraction (FSRE) setting, the performance of all six models we evaluated was comparably low when evaluated on WIKI-\"}"}
{"id": "lrec-2024-main-1442", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 10: The results for the 5-way 1-shot and 5-way 5-shot settings on a small test partition of the FSTACRED dataset.\\n\\n| Model                  | 5-way 1-shot | 5-way 5-shot |\\n|------------------------|--------------|--------------|\\n|                        | P  | R  | F1 | P  | R  | F1  |\\n| Unsupervised Baseline  | 8.33 | 33.33 | 13.33 | 11.76 | 66.67 | 20  |\\n| MNA V                  | 0  | 0  | 0  | 25 | 33.33 | 28.57 |\\n| Hard-matching Rules    | 0  | 0  | 0  | 0  | 0  | 0   |\\n| Soft-matching Rules    | 66.67 | 66.67 | 66.67 | 16.67 | 33.33 | 22.22 |\\n\\nTable 11: The results for the 5-way 1-shot and 5-way 5-shot settings on a small test partition of the FS NYT29 dataset.\\n\\n| Model                  | 5-way 1-shot | 5-way 5-shot |\\n|------------------------|--------------|--------------|\\n|                        | P  | R  | F1 | P  | R  | F1  |\\n| Unsupervised Baseline  | 18.18 | 50 | 26.67 | 12.5 | 37.50 | 18.75 |\\n| MNA V                  | 58.33 | 87.5 | 69.99 | 10.07 | 55.77 | 17.06 |\\n| Hard-matching Rules    | 0  | 0  | 0  | 0  | 0  | 0   |\\n| Soft-matching Rules    | 25  | 37.5 | 30  | 25  | 37.5 | 30   |\\n| Zero-Shot LLM (GPT 4)  | 26.08 | 75 | 38.71 | 21.74 | 62.5 | 32.26 |\\n\\nTable 12: The results for the 5-way 1-shot and 5-way 5-shot settings on a small test partition of the FS WikiData. This can be primarily attributed to the high prevalence of long-tail entities in WikiData. In (Chen et al., 2023), it is reported that approximately half of the entities in WikiData fall into the long-tail category. The challenges stemming from this prevalence of long-tail entities contribute significantly to the observed performance degradation. Firstly, the data scarcity inherent in long-tail entities exacerbates the already challenging few-shot learning scenario, where models are expected to generalize from limited examples. With fewer instances available for these long-tail entities, models struggle to capture the diverse range of relation patterns and semantic nuances associated with them. Additionally, the lack of contextual cues and varied semantic contexts surrounding these entities further compounds the difficulty of accurate relation extraction. As a result, the efficacy of models in the FSRE setting is hampered by the combination of data scarcity and the intricate nature of relations involving long-tail entities in WikiData.\"}"}
