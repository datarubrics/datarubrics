{"id": "emnlp-2023-main-506", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Superlim:\\nA Swedish Language Understanding Evaluation Benchmark\\n\\nAleksandrs Berdicevskis* Gerlof Bouma* Robin Kurtz\u2020 Felix Morger*\\nJoey \u00d6hman\u2021 Yvonne Adesam*\\nLars Borin* Dana Dann\u00e9lls*\\nMarkus Forsberg*\\nTim Isbister\u00a7 Anna Lindahl*\\nMartin Malmsten\u2020 Faton Rekathati\u2020 Magnus Sahlgren\u00a7\\nElena Volodina*\\nLove B\u00f6rjeson\u2020 Simon Hengchen|| Nina Tahmasebi\u00b6\\n\\nSpr\u00e5kbanken Text, University of Gothenburg\\nKBLab, National Library of Sweden\\nEmbark Studios\\nAI Sweden\\niguanodon.ai\\nUniversity of Gothenburg\\n\\nAbstract\\n\\nWe present Superlim, a multi-task NLP benchmark and analysis platform for evaluating Swedish language models, a counterpart to the English-language (Super)GLUE suite. We describe the dataset, the tasks, the leaderboard and report the baseline results yielded by a reference implementation. The tested models do not approach ceiling performance on any of the tasks, which suggests that Superlim is truly difficult, a desirable quality for a benchmark. We address methodological challenges, such as mitigating the Anglocentric bias when creating datasets for a less-resourced language; choosing the most appropriate measures; documenting the datasets and making the leaderboard convenient and transparent. We also highlight other potential usages of the dataset, such as, for instance, the evaluation of cross-lingual transfer learning.\\n\\n1 Introduction\\n\\nIn recent years, the move towards Transformer-based large pretrained language models has brought the need for comprehensive and standardized multi-task benchmarking in NLP into focus. Since these models, through their generic pretraining setup, can be fine-tuned on many different tasks, a test suite which covers a range of different NLP domains is required to measure overall performance. For this reason, several different benchmarks have been proposed. For English, the most notable suites arguably are the (Super)GLUE benchmark (Wang et al., 2018, 2019) for natural language understanding and GEM (Gehrmann et al., 2021) for natural language generation.\\n\\nIn this paper, we present Superlim, a multi-task benchmark for Swedish natural language understanding. It contains 14 different datasets which cover 15 tasks ranging from word understanding tasks, such as word analogy and word similarity, to text classification tasks, such as natural language inference and sentence acceptability. The release of this benchmark comes at a time when multiple pretrained language models have been created for Swedish, and larger models are under construction (ranging from Malmsten et al., 2020 to Ekgren et al., 2023). In this active research environment, a multi-task benchmark is crucial to gauge state-of-the-art performance and guide future development.\\n\\nFor Swedish this is especially important, since limitations (financial, computational and related to data availability) are more considerable than for English.\\n\\nCreating a benchmark for Swedish comes with its own unique set of practical challenges and methodological considerations. While there exist high-quality Swedish datasets that can be reused, not all of them have been created with the task of evaluating large language models in mind. Here, the relatively close linguistic relationship to English comes with possibilities of translating (or adapting) existing datasets. Such approaches do however potentially introduce linguistic and cultural Anglocentric bias into Swedish language benchmarking. Therefore, Superlim combines datasets of three kinds: a) preexisting ones, reformatted and revised, b) translated or adapted from English, and c) created from scratch specifically for Superlim.\\n\\nTo facilitate progress tracking and sharing of results, models and datasets, we create an online leaderboard for Superlim, which will serve as a platform for submitting and publishing results, and\"}"}
{"id": "emnlp-2023-main-506", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"release reference implementations with baseline results for a collection of existing Swedish language models.\\n\\nThe rest of the paper looks as follows: In Section 2 we briefly cover related work. Section 3 describes the tasks included in Superlim and the corresponding datasets, and Section 4 their standardized documentation. Section 5 motivates Superlim's main evaluation measure: Krippendorff's \u03b1. The online design of the leaderboard is the subject of Section 6. Sections 7 and 8 describe the reference implementations and the baseline results. Section 9 provides details on where to find Superlim and Section 10 concludes the paper.\\n\\n2 Related work\\n\\nThe already mentioned (Super)GLUE has inspired the creation of similar benchmarks for other languages: CLUE (Xu et al., 2020) and CLiMP (Xiang et al., 2021) for Chinese, Russian SuperGLUE (Shavrina et al., 2020), KLEJ (Rybak et al., 2020) and LEPISZCZE (Augustyniak et al., 2022) for Polish, ParsiNLU for Persian (Khashabi et al., 2021), IndoNLU for Indonesian (Wilie et al., 2020) and NorBench (Samuel et al., 2023) for Norwegian. Several suggestions to move to other evaluation approaches have been voiced (Ethayarajh and Jurafsky, 2020; Ribeiro et al., 2020; Kiela et al., 2021), but the GLUE-like benchmarks still remain the default approach.\\n\\nSuperlim is not the only benchmark for Swedish. Very recently, ScandEval has been released (Nielsen, 2023). This multilingual suite contains four tasks for Swedish. Some of the datasets used for the evaluation contain synthetic data (for instance, incorrect sentences created by scrambling word order in the correct sentences), which is not necessarily problematic, but presumably less valuable than natural data. Another benchmark for Swedish, OverLim (Kurtz, 2022), is entirely machine translated from English.\\n\\n3 Tasks and datasets\\n\\nWe have three types of tasks: text level (Section 3.1), word level (Section 3.2) and diagnostic (Section 3.3). Describing the creation of all datasets in detail is beyond the scope of this paper. Instead we focus on briefly describing the datasets, their tasks and any substantial changes that have been made to the original sources for the purposes of Superlim. For more detailed information about the datasets and their creation, we refer to the documentation that accompanies Superlim, itself described in Section 4. Table 1 lists all the tasks in Superlim.\\n\\nMost datasets are divided into predefined training, development and test splits. All test sets are of gold-standard quality: they have been manually annotated from scratch or manually corrected after automatic processing. Some of the training and development sets may contain non-corrected data, for instance automatically translated data. Our rationale here is that a model can be trained on anything, as long as it yields high, robust, unbiased performance. Evaluation, however, must always occur on gold data to serve its purpose. An important caveat, however, is that those test sets that were automatically translated and then manually corrected may suffer from so-called post-editese (Toral, 2019). In all cases, the editing was carefully done by native speakers with background in NLP and/or linguistics, who were aware of this potential problem. We hope that these factors have at least mitigated it.\\n\\nThe word-level datasets were mostly created with non-contextualized, type level approaches in mind (for instance, static word vectors like those obtained from word2vec). These therefore do not contain large training splits, but small training sets that can be used to calibrate otherwise unsupervised methods, or for few-shot training generative models. The diagnostic datasets only have test splits. Here, the training and development splits of SweNLI are to be used.\\n\\n3.1 Text-level tasks\\n\\n3.1.1 Absabank-Imm\\n\\nAbsabank-Imm is derived from the Swedish aspect-based sentiment (stance) analysis corpus described in Rouces et al. (2020). We selected the part annotated for author attitude regarding immigration in Sweden, quantified as real-valued scores between 1 (very negative) and 5 (very positive). These scores are the arithmetic mean of integer scores from individual annotators, but since most paragraphs were labelled by only one annotator, most average values are also integer.\\n\\nThe data for this task comes from different sources: editorials and opinion pieces from the national Swedish newspapers Svenska dagbladet and Aftonbladet, and forum posts from Flashback, a large Swedish discussion forum.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview of the tasks in Superlim. Task type is one of: binary labelling, ternary labelling, scoring (regression), answer selection (multiple choice, pairing) or text generation. A dataset counts as translated when the training and test items were translated (manually or automatically) from its English counterpart.\\n\\n3.1.2 Argumentation sentences\\nIn Argumentation sentences, the task is to categorize sentences into argument for, argument against, or unrelated to a given topic. The topics include abortion, death penalty, nuclear power, marijuana legalization, minimum wage and cloning. The data for this task was machine translated from the argument unit recognition and classification dataset described in Trautmann et al. (2020) and manually corrected by a native speaker of Swedish.\\n\\n3.1.3 DaLAJ-GED\\nFor DaLAJ-GED, sentences have to be classified as being grammatically correct or not. Incorrect datapoints were collected from the Swedish learner corpus SwELL (Volodina et al., 2019). Sentences containing errors from this material were manipulated to contain only one error per datapoint \u2013 a sentence with multiple mistakes can thus give rise to multiple datapoints. Correct datapoints were sampled from the same material and from the course book corpus COCTAILL (Volodina et al., 2014). The DaLAJ-GED materials contain additional annotation, such as the type and location of an error, its recommended correction, and information about the sentence source. This additional annotation is not currently used in the task itself, but we consider it valuable for error analysis, and it is a potential basis for future tasks using this material.\\n\\n3.1.4 SweParaphrase\\nThe SweParaphrase dataset consists of sentence pairs, and the task is to estimate their semantic similarity, that is, to what extent they express the same state of affairs on a scale from 0 (dissimilar) to 5 (similar). The gold standard annotation is an arithmetic mean of multiple judgements. The dataset is based on the Semantic Textual Similarity Benchmark (STS-B) (Cer et al., 2017), which was first automatically translated to Swedish using the Google Translate API (Isbister et al., 2021) and then manually corrected by a native speaker. The similarity values were taken from STS-B without any adjustment.\\n\\n3.1.5 SweDN\\nSweDN (Monsen and J\u00f6nsson, 2021) is a summarization task, the only sequence generation task included in Superlim. This dataset collects almost 40 thousand articles from the Swedish newspaper Dagens Nyheter. The lead paragraph in each article serves as its ground truth summary.\\n\\n3.1.6 SweFAQ\\nThe dataset for SweFAQ consists of answers to frequently asked questions taken from websites of nine Swedish authorities, such as the Social Insurance Agency and the Swedish Tax Agency. The dataset contains 976 question-answer pairs that fall into 100 different categories, for instance COVID-19.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"19 vaccination or parental benefits. Each datapoint in the dataset is a question and a list of all answers from the same category in a randomized order. The task is to select the matching answer, by giving its index. The length of the list of question-answer pairs varies across categories.\\n\\n3.1.7 SweNLI\\nSweNLI is a natural language inference task: for two sentences, a premise and a hypothesis, predict the relation between the two (neutral, contradiction or entailment). The SweNLI dataset is derived from two existing resources. The test split was constructed from a grammar-based automatic Swedish translation of the FraCaS test suite (Cooper et al., 1996; Ljungl\u00f6f and Siverbo, 2012). For Superlim, the Swedish FraCaS suite was extensively manually revised and culturally adapted to Swedish real-world facts.\\n\\nFor the training and development splits, a machine translated version of the Multi-genre Natural Language Inference (Williams et al., 2018) dataset is used. The original data was collected from ten different texts ranging from transcribed telephone calls to magazine articles. We translated the dataset to Swedish using the OPUS-MT machine translation framework (Tiedemann and Thottingal, 2020).\\n\\n3.1.8 SweWiC\\nSweWiC is modelled after the word-in-context task described in Pilehvar and Camacho-Collados (2019). A polysemous target word is provided in two contexts. The system's task is to determine whether the word is used with the same meaning or not.\\n\\nThe development and test splits use example sentences collected for the Swedish lexical resource SALDO (Borin et al., 2013), and sentences taken from the Eukalyptus corpus (Adesam et al., 2015), which has word-sense annotation based on SALDO. Training data was constructed from Swedish Wiktionary.\\n\\nSince the word-in-context task doesn\u2019t directly rely on the sense inventory itself, but only asks whether two instances use the same meaning or not, differences between Wiktionary and SALDO in terms of sense inventory are acceptable (see also Pilehvar and Camacho-Collados, 2019).\\n\\n3.1.9 SweWinograd\\nSweWinograd is a coreference resolution task, cast as a binary mention-pair classification problem. Each datapoint contains a fragment in which a pronoun and a possible antecedent are highlighted. The system has to predict whether they are coreferent or not. SweWinograd was created by manual translation of the Winograd schema challenge (Levesque et al., 2012), as included in SuperGLUE. Note that the Winograd schema challenge is also present in the original GLUE benchmark, but as an entailment task.\\n\\n3.2 Word-level tasks\\nFor each of the word-level tasks described below, we created small training splits by randomly selecting 10% of the combined data, meant to enable few shot learning or calibration of existing models.\\n\\n3.2.1 SuperSim relatedness and SuperSim similarity\\nSuperlim includes two tasks based on the SuperSim dataset (Hengchen and Tahmasebi, 2021). SuperSim contains word pairs annotated for semantic similarity and for semantic relatedness, both with scores between 0 (low degree) and 10 (high degree). Semantic similarity is the extent to which two concepts share semantic properties, semantic relatedness refers to a more general association between the concepts. For instance, cup and coffee are related, but not similar. SuperSim relatedness and SuperSim similarity concern scoring pairs for relatedness and similarity, respectively.\\n\\nSuperSim\u2019s word pairs were translated from SimLex-999 (Hill et al., 2015) and WordSim-353 (Finkelstein et al., 2002) and then labelled by five native speakers of Swedish. The ground truth is the arithmetic mean of the labels, but the individual labels are preserved, too.\\n\\n3.2.2 Swedish Analogy\\nThe Swedish Analogy dataset (Adewumi et al., 2022) consists of analogies of the form \\\"Stockholm is to Sverige ('Sweden') as Berlin is to Tyskland ('Germany').\\\" The dataset contains different categories of analogy, of semantic (real world facts, as above, or lexical semantic relations) or of morphological nature. The dataset is partly based on Mikolov et al. (2013) for English, but was enriched with additional categories and translated to Swedish using automatic tools and Wiktionary matches, after which it was proof-read by native speakers.\\n\\n1\\nSee https://sv.wiktionary.org/.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Swedish Analogy, analogies have to be completed by predicting the fourth element given the first three. The set of items from which the model may select an answer is not limited by the task, but only by the vocabulary of the model itself.\\n\\n3.2.3 SweSAT\\n\\nSweSAT was created in the context of Superlim, and collected from the synonyms part of the Swedish Scholastic Aptitude Test (H\u00f6gskoleprovet) for the years 2006\u20132021. The task is, given a word or an expression, to select the best synonym from a list of five candidates.\\n\\n3.3 Diagnostic tasks\\n\\nSuperlim comes with two diagnostic tasks, which are both cast as inference tasks. Participating systems should use the training materials supplied for SweNLI.\\n\\n3.3.1 SweDiagnostics\\n\\nSweDiagnostics is a manually translated version of the GLUE diagnostic dataset (Wang et al., 2018). The dataset is used for diagnosing a system\u2019s ability to handle specific linguistic phenomena. The diagnostic task is cast as an inference task, where the premise and hypothesis are identical but for a minimal manipulation, like the insertion of negation. The entailment relationship (entailment, neutral, or contradiction) relies on the linguistic phenomenon targeted. There are 33 fine-grained phenomena across four coarse-grained categories (lexical semantics, predicate-argument structure, logic and common sense). Translating the English dataset, we made sure that the targeted linguistic phenomena are present even in the resulting Swedish sentences and that the sentences are idiomatic Swedish.\\n\\nIn SuperGLUE, the same diagnostic dataset was used, but formulated as a binary classification problem. In Superlim, we have chosen to keep the original ternary version.\\n\\n3.4 SweWinogender\\n\\nSweWinogender is an inference task designed to diagnose gender bias in models. It is a reformulation of the pronoun resolution test set of Hansson et al. (2021), which, in turn, was manually translated from/inspired by Winogender (Rudinger et al., 2018). We mirror SuperGLUE in this choice, since it has an inference task reformulation of the English Winogender.\\n\\nItems in SweWinogender consist of a premise, a short text containing a pronoun that allows only one reasonable interpretation of this pronoun, and an hypothesis that spells out some interpretation of this pronoun. For correctly resolved pronouns, there is an entailment relation between the two sentences. Each premise-hypothesis pair occurs three times in the dataset, each time with a different pronoun: han \u2018he\u2019, hon \u2018she\u2019, or hen \u2018(singular) they\u2019. For a model that does not display any gender bias, its performance on the inference task should not correlate with the choice of pronoun.\\n\\n4 Documentation\\n\\nThe importance of documenting datasets cannot be overestimated. Still this step is often overlooked in the NLP field. Since some parts of Superlim are revised versions of pre-existing datasets, it is particularly important to be explicit about what changes were made. We therefore devised a documentation sheet: a template inspired by Gebru et al.\u2019s (2021) \u201cDatasheets\u201d and Pushkarna et al.\u2019s (2022) \u201cData cards\u201d. Our template, however, is more compact than the former and is specifically adapted to the Superlim context. Documentation sheets consist of six sections:\\n\\n1. Identifying information: basic information about the dataset and its creators;\\n2. Usage: why the dataset was created and how it can be used;\\n3. Data: the description of the contents (including basic statistics, format, data source, collection method, inter-annotator agreement etc.);\\n4. Ethics and caveats: ethical considerations, potential pitfalls, discouraged usage;\\n5. About documentation: information about the documentation and dataset versions;\\n6. Other information, including a bibliography.\\n\\nEach Superlim task is accompanied by a documentation sheet. The compactness of the template is intentional: it improves the chances that documentation will actually be written, maintained and read. An example can be found in Appendix A.\\n\\n5 Evaluation measures\\n\\nA benchmark containing a mix of task types will almost by necessity involve a wide range of evaluation methods and measures. The appropriate choices depend on the tasks themselves and which\"}"}
{"id": "emnlp-2023-main-506", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Good research practice tells us to evaluate models in those ways that teach us most about them. However, in a benchmark \u201ccompetition\u201d setting, we would like to summarize a model's performance with one final score, this leads to a problem. In GLUE, this is dealt with by providing an average over very different beasts like accuracy, Matthews correlation coefficient, f-score, and Spearman\u2019s correlation coefficient (that have different ranges and different interpretations). In the context of the Superlim benchmark, we explore an alternative: the use (where possible) of a single family of measures, Krippendorff\u2019s $\\\\alpha$ (Krippendorff, 2018).\\n\\nKrippendorff\u2019s $\\\\alpha$ is a measure of inter-annotator agreement, for instance used in work on linguistic annotation (Paun et al., 2022). We use $\\\\alpha$ as an evaluation measure by treating the system to be evaluated as one annotator, and the gold-standard creators as another. A perfect system always agrees with the gold-standard creators ($\\\\alpha = 1$), whereas a poor system shows no systematic agreement with them ($\\\\alpha \\\\approx 0$). Even lower scores ($-1 \\\\leq \\\\alpha < 0$) are signs of systematic mistakes.\\n\\nKrippendorff\u2019s $\\\\alpha$ is parameterized by a distance metric (Krippendorff, 2011) which allows us to use it on different types of task. For binary and ternary labelling tasks, we use nominal-$\\\\alpha$. In the binary case, nominal-$\\\\alpha$ linearly maps to macro-averaged f-score. For scoring tasks we use interval-$\\\\alpha$. Since it is a measure of agreement, interval-$\\\\alpha$ is stricter than correlation measures like Pearson\u2019s $r$, which do not require a system to get the sizes of predicted scores correct, only the shape of their distribution.\\n\\nFor a benchmark, we find this strictness a desirable property. The selection tasks do not allow for a direct application of $\\\\alpha$. However, we can calculate a derived measure, a pseudo-$\\\\alpha$, by pretending, after the fact, that the selection tasks are binary decision tasks and by calculating a nominal-$\\\\alpha$ for this recasted task. We present a more elaborate investigation of $\\\\alpha$ as an evaluation measure in a separate paper (Bouma, 2023).\\n\\nWe currently do not have an $\\\\alpha$-based measure for the summarization task SweDN, for which the benchmark uses ROUGE-1. SweWinogender is evaluated using nominal-$\\\\alpha$, but in addition also using gender parity, which measures how consistently the model answers across datapoints that only differ in the choice of pronoun.\\n\\nThe benchmarking results are collected at the Superlim leaderboard, where both development and test set results are made available. In addition to the evaluation outcomes on the different tasks, the leaderboard also lists information about a participating model\u2019s type and size (in number of parameters), and the size of the dataset used to pre-train the model.\\n\\nThe Superlim project strives for transparency and openness, not just with regards to our datasets but also to the contents of the leaderboard. As part of this, we link participating models\u2019 predictions on development and test sets as well as scripts and configuration details used to produce those results. This facilitates replication of results and confirmation of their validity. There is an additional point with this level of transparency: The gold labels of the Superlim test sets are not hidden. This makes evaluation easier, but it also makes cheating (intentional or accidental, see discussion in the limitations section) very easy. We hope that asking participants to be open about their methods can act as a countermeasure to this potential problem.\\n\\nThe design of the leaderboard is also guided by the idea that there is no single best model (Ethayarajh and Jurafsky, 2020). The choice of a model for some application sometimes need only depend on the performance on certain subsets of our evaluation suite and can even be restricted to only included models up to a certain size. We therefore allow users of the leaderboard to filter and sort out all the tasks and models that are irrelevant for their specific needs: users can include and exclude tasks, filter out model families, sort by size, overall performance, and task-specific performance.\\n\\nConsider the following use case: a developer wants to build a proofreading application using a transformer model, and needs their application to run as fast as possible on limited hardware. The most relevant task in Superlim for them is DaLAJ-GED, and the developer may wish to restrict themselves to models with less than 150M parameters. In the leaderboard, they can now easily select all base models, exclude all word-level tasks to declutter the table, and sort according to performance on DaLAJ-GED. That would bring on top the KBLab/bert-base-swedish-cased-new model, that otherwise performs worse than average. The developer further checks if the performance on the validation set matches the performance on the test set.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Language models used for the reference implementation\\n\\n| Model Type                  | Model Name                                      | Language |\\n|-----------------------------|-------------------------------------------------|----------|\\n| KB/bert-base-swedish-cased  | BERT base                                       | sv       |\\n| KBLab/bert-base-swedish-cased-new | BERT base                                       | sv       |\\n| KBLab/megatron-bert-base-swedish-cased-600k | BERT base                                       | sv       |\\n| KBLab/megatron-bert-large-swedish-cased-165k    | BERT large                                      | sv       |\\n| AI-Nordics/bert-large-swedish-cased          | BERT large                                      | sv       |\\n| xlm-roberta-base               | RoBERTa base                                    | multi    |\\n| xlm-roberta-large              | RoBERTa large                                   | multi    |\\n| AI-Sweden/gpt-sw3-. . .       | GPT 126M\u201340B                                    | sv, no, da, is, en |\\n\\nTable 3: Average standard deviation of performance across hyperparameter configurations\\n\\n| Model Name                                      | Average Standard Deviation |\\n|-------------------------------------------------|-----------------------------|\\n| AI-Nordics/bert-l-sw-c                           | 0.013                       |\\n| KBLab/mt-bert-l-sw-c-165k                         | 0.018                       |\\n| KB/bert-b-sw-c                                   | 0.018                       |\\n| KBLab/mt-bert-b-sw-c-600k                         | 0.034                       |\\n| NbAiLab/nb-bert-base                              | 0.056                       |\\n| xlm-roberta-base                                 | 0.085                       |\\n| KBLab/bert-b-sw-c-new                             | 0.122                       |\\n| xlm-roberta-large                                 | 0.324                       |\\n\\n7 Reference implementation\\n\\nTo create baselines for Superlim, we built a number of reference implementations using the Swedish language models listed in Table 2, all hosted at HuggingFace. In addition to these models, we also provide results for a set of non-neural supervised machine learning methods, including SVM, Decision Trees, and Random Forests. Random choice and majority label give lower bounds.\\n\\nWe used the following hyperparameters when fine tuning the language models, along with the default arguments in the HuggingFace Trainer class:\\n\\n- warm-up ratio 0.06\\n- weight decay 0.1 (0.0 for GPT models)\\n- number of training epochs 10\\n- fp16\\n\\nWe use early stopping with patience 5 for the number of epochs. Learning rate and batch size were tuned for all tasks and models, from the following search space:\\n\\n- learning rate \\\\[1e^{-5}, 2e^{-5}, 3e^{-5}, 4e^{-5}\\\\]\\n- batch size \\\\[16, 32\\\\]\\n\\nBecause of the size of the training set, we only considered the lowest and highest learning rate for SweNLI. The hyperparameter search gives us a performance score (Krippendorff\u2019s \\\\(\\\\alpha\\\\)) for each task, model and configuration of hyperparameters. The average standard deviation of these scores are summarized in Table 3, which shows xlm-roberta-large to be by far most sensitive to these settings.\\n\\nWe created reference implementations for all text-level tasks except for SweDN.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Reference model performance for the word-level tasks, reported as Krippendorff's $\\\\alpha$.\\n\\n| Model                     | SuperSim rel | SuperSim sim | Swedish Analogy | SweSAT         |\\n|---------------------------|--------------|--------------|-----------------|---------------|\\n| AI-Sweden/gpt-sw3-126m    | 0.400        | 0.340        |                 |               |\\n| AI-Sweden/gpt-sw3-1.3b    | 0.732        | 0.665        |                 |               |\\n| AI-Sweden/gpt-sw3-20b     | 0.824        | 0.820        |                 |               |\\n| AI-Sweden/gpt-sw3-40b     | 0.838        | 0.854        |                 |               |\\n| Random                    | -0.037       | -0.302       | 0.000           | 0.000         |\\n| Static word vectors       | 0.410        | -0.109       | 0.013           | 0.365         |\\n\\nTable 5: Reference model performance for the text-level tasks, sorted by average Krippendorff's $\\\\alpha$, and overall diagnostic score. See the main text for discussion of the SweWinogender results, here omitted for space reasons.\\n\\n| Model                     | 3-lab | 2-lab | 3-lab | 2-lab | 2-lab | 3-lab |\\n|---------------------------|-------|-------|-------|-------|-------|-------|\\n| KBLab/mt-bert-l-sw-c-165k | 0.508 |       | 0.628 |       | 0.753 |       |\\n| AI-Nordics/bert-l-sw-c    | 0.480 | 0.563 | 0.745 | 0.862 | 0.719 | 0.241 |\\n| KB/bert-b-sw-c            | 0.529 | 0.555 | 0.740 | 0.845 | 0.641 | 0.179 |\\n| xlm-roberta-large         | 0.516 | 0.584 | 0.738 | 0.882 | 0.584 | 0.205 |\\n| KBLab/mt-bert-b-sw-c-600k | 0.449 | 0.562 | 0.718 | 0.867 | 0.709 | 0.218 |\\n| NbAiLab/nb-bert-base      | 0.390 | 0.541 | 0.644 | 0.823 | 0.660 | 0.172 |\\n| KBLab/bert-b-sw-c-new     | 0.428 | 0.554 | 0.753 | 0.755 | 0.447 | 0.163 |\\n| xlm-roberta-base          | 0.366 | 0.497 | 0.701 | 0.813 | 0.473 | 0.186 |\\n| SVM                       | 0.286 | 0.354 | 0.518 | 0.239 | 0.038 | 0.000 |\\n| Decision Tree             | 0.117 | 0.156 | 0.269 | 0.200 | 0.040 | 0.192 |\\n| Random                    | 0.008 | 0.013 | 0.007 | -0.043 | -0.150 | -0.091 |\\n| Random Forest             | 0.005 | -0.272 | -0.312 | 0.143 | 0.032 | -0.411 |\\n| Majority label/Avg        | -0.052 | -0.272 | -0.340 | -0.001 | -0.310 | -0.434 |\\n\\nParticularly low scores are achieved for the inference task SweNLI and the pronoun resolution task SweWinograd. The bad performance for the former is connected to the low scores on both diagnostic tasks, which use the same training data and are of the same task type. Overall performance on SweDiagnostics maxes out at $\\\\alpha = 0.415$, a breakdown of SweDiagnostics results can be found in Appendix C.\\n\\nThere is no column for the SweWinogender diagnostic task in Table 5: all models receive perfect gender parity scores but very low $\\\\alpha$-s of around -0.3. This is the result of outputting the same label for basically all test items. As mentioned above, SweNLI and the diagnostic tasks are inference tasks that share the same, automatically translated training dataset. It is tempting to assume performance is affected by low quality of the training set. We note, however, that Argumentation sentences also is a ternary labelling task with automatically translated training data, and that the models fare much better there. It is likely that these are just hard tasks: also in Bowman and Dahl (2021) it is noted that GLUE's diagnostic set was challenging even for the best models, and SuperGLUE's co-reference task has the lowest baseline performance amongst the non-diagnostic tasks.\\n\\nThe models perform best on the paraphrase task SweParaphrase.\\n\\n9 Distribution\\n\\nAll datasets constituting Superlim are available under Creative Commons licenses (CC BY 4.0,\"}"}
{"id": "emnlp-2023-main-506", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CC BY-SA 4.0, respectively), and can be downloaded together with their documentation sheets from Spr\u00e5kbanken Text\u2019s website or accessed through HuggingFace. The Superlim leaderboard can be found at KBLab\u2019s website, and includes model predictions as well as detailed information about the participating models.\\n\\nConclusion\\n\\nWe have presented Superlim, a Swedish benchmark and analysis platform for natural language understanding in the style of GLUE and SuperGLUE. Superlim contains 13 regular tasks and 2 diagnostic tasks from a broad range of task types, including word-level tasks that are not commonly seen in GLUE-style benchmarks. All tasks come with gold-standard test data, with training and development data \u2013 albeit not always of gold standard quality \u2013 and with detailed, standardized documentation.\\n\\nWe have chosen to make Superlim\u2019s leaderboard flexible and transparent, which we hope may help promote the dissemination of knowledge about the capacities of natural understanding models for Swedish and knowledge about their construction. Finally, by having comparable tasks to the de facto standard SuperGLUE, Superlim not only contributes to the development of natural language understanding for Swedish, but also to research into transfer learning and multilingual models, and thus to a much wider part of the field of natural language processing.\\n\\nAuthor contributions\\n\\nThe authors are divided into three groups (Berdicevskis to \u00d6hman, Adesam to Volodina, and B\u00f6rjeson to Tahmasebi), depending on their contribution to the project and the paper. Within each group, the contributions are considered to be of equal weight, and the author names are listed alphabetically. The division of labour between the three partner institutions was as follows: dataset collection and/or construction \u2013 Spr\u00e5kbanken Text; reference implementation \u2013 AI Sweden; leaderboard development \u2013 KBLab.\\n\\nAcknowledgments\\n\\nThe Superlim project was supported by Sweden\u2019s Innovation Agency (Vinnova, grant nos 2020-02523 and 2021-04165) and by Nationella Spr\u00e5kbanken \u2013 jointly funded by the Swedish Research Council (2018\u20132024, grant no. 2017-00626) and 10 partner institutions. Joey \u00d6hman contributed to Superlim while at AI Sweden. The contributions of Simon Hengchen and Nina Tahmasebi have in part been funded by Towards Computational Lexical Semantic Change Detection (Swedish Research Council, 2019\u20132022, grant no. 2018-01184) and Change is Key! (Riksbankens Jubileumsfond, grant no. M21-0021). RISE Research Institutes of Sweden was involved in the project at its early stage.\\n\\nWe would like to thank everybody who contributed to the creation of datasets or in any other way helped us with the project, in particular Francisca Hoyer, Yousuf Ali Mohammed, Tosin Adewumi, Julius Monsen, Arne J\u00f6nsson, Peter Ljungl\u00f6f and Jacobo Rouces.\\n\\nLimitations\\n\\nUnlike some other benchmarks, Superlim has no hidden data, which may lead to data leaks. The models (pre)trained on the data crawled from the Internet may have seen the test data. While we require the submissions to be as transparent and thoroughly documented as possible, it is not always possible to control what exactly the very large training sets contain (and often it is not even known by those who trained the model). There is a risk that this problem will exacerbate as recent evidence have shown that larger language models (equal to or larger than 6B parameters), such as the GPT-J model (Wang and Komatsuzaki, 2021) and ChatGPT, are capable of memorizing data verbatim (Carlini et al., 2023).\\n\\nIn order to better understand how this affects Superlim, we encourage future work exploration of how much of the Superlim data has been memorized by large language models. We do not currently have a human baseline to put the performance of models on Superlim into perspective. Some of the training sets in the Superlim were automatically translated and have not been manually corrected. The translated test sets have all been...\"}"}
{"id": "emnlp-2023-main-506", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"thoroughly corrected, but, of course, they still may suffer to some extent from translationese (Geller-stam, 1986) or its exacerbated variant post-editese (Toral, 2019).\\n\\nIt can be argued that Superlim is too heterogeneous, containing, on the one hand more traditional natural language understanding tasks such as NLI, sentiment analysis, semantic similarity at the sentence level as well as word-level tasks, which are less often used to evaluate Transformer models, on the other (and one generation task in addition).\\n\\nReferences\\n\\nYvonne Adesam, Gerlof Bouma, and Richard Johansson. 2015. Defining the Eukalyptus forest \u2013 the Koala treebank of Swedish. In Proceedings of the 20th Nordic Conference of Computational Linguistics (NODALIDA 2015), pages 1\u20139, Vilnius, Lithuania. Link\u00f6ping University Electronic Press, Sweden.\\n\\nTosin Adewumi, Foteini Liwicki, and Marcus Liwicki. 2022. Exploring Swedish & English fastText embeddings. In Proceedings of the 8th International Workshop on Artificial Intelligence and Cognition, pages 201\u2013208.\\n\\nLukasz Augustyniak, Kamil Tagowski, Albert Sawczyn, Denis Janiak, Roman Bartusiak, Adrian Szymczak, Arkadiusz Janz, Piotr Szyma\u0144ski, Marcin W\u0105trosa, Miko\u0142aj Morzy, Tomasz Kajdanowicz, and Maciej Piasecki. 2022. This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish. In Advances in Neural Information Processing Systems, volume 35, pages 21805\u201321818. Curran Associates, Inc.\\n\\nLars Borin, Markus Forsberg, and Lennart L\u00f6nngren. 2013. SALDO: a touch of yin to WordNet's yang. Language resources and evaluation, 47(4):1191\u20131211.\\n\\nGerlof Bouma. 2023. Krippendorff's $\\\\alpha$ as an evaluation measure. [Unpublished manuscript] Spr\u00e5kbanken Text, Department of Swedish, Multilingualism, Language Technology, University of Gothenburg.\\n\\nSamuel R. Bowman and George Dahl. 2021. What will it take to fix benchmarking in natural language understanding? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4843\u20134855, Online. Association for Computational Linguistics.\\n\\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2023. Quantifying memorization across neural language models. In The Eleventh International Conference on Learning Representations.\\n\\nDaniel Cer, Mona Diab, Eneko Agirre, I\u00f1igo Lopez-Gazpio, and Lucia Specia. 2017. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 1\u201314, Vancouver, Canada. Association for Computational Linguistics.\\n\\nRobin Cooper, Dick Crouch, Jan Van Eijck, Chris Fox, Johan Van Genabith, Jan Jaspars, Hans Kamp, David Milward, Manfred Pinkal, Massimo Poesio, et al. 1996. Using the framework. Technical Report LRE 62-051 D-16, The FraCaS Consortium.\\n\\nAriel Ekgren, Amaru Cuba Gyllensten, Felix Stollenwerk, Joey \u00d6hman, Tim Isbister, Evangelia Gogoulou, Fredrik Carlsson, Alice Heiman, Judit Casademont, and Magnus Sahlgren. 2023. GPT-SW3: An autoregressive language model for the Nordic languages.\\n\\nKawin Ethayarajh and Dan Jurafsky. 2020. Utility is in the eye of the user: A critique of NLP leaderboards. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4846\u20134853, Online. Association for Computational Linguistics.\\n\\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2002. Placing search in context: the concept revisited. ACM Trans. Inf. Syst., 20(1):116\u2013131.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, Hal Daum\u00e9 III, and Kate Crawford. 2021. Datasheets for datasets. Commun. ACM, 64(12):86\u201392.\\n\\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Anuoluwapo Aremu, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna-Adriana Clinciu, Dipanjan Das, Kaustubh Dhole, Wanyu Du, Esin Durmus, Ond\u0159ej Du\u0161ek, Chris Chinenye Emezue, Varun Gangal, Cristina Garbacea, Tatsunori Hashimoto, Yufang Hou, Yacine Jernite, Harsh Jhamtani, Yangfeng Ji, Shailza Jolly, Mihir Kale, Dhruv Kumar, Faisal Ladhak, Aman Madaan, Mounica Maddela, Khyati Mahajan, Saad Mahamood, Bodhisattwa Prasad Majumder, Pedro Henrique Martins, Angelina McMillan-Major, Simon Mille, Emiel van Miltenburg, Moin Nadeem, Shashi Narayan, Vitaly Nikolaev, Andre Niyongabo Rubungo, Salomey Osei, Ankur Parikh, Laura Perez-Beltrachini, Niranjan Ramesh Rao, Vikas Raunak, Juan Diego Rodriguez, Sashank Santhanam, Jo\u00e3o Sedoc, Thibault Sellam, Samira Shaikh, Anastasia Shimostrina, Marco Antonio Sobrevilla Cabezudo, Hendrik Strobelt, Nishant Subramani, Wei Xu, Diyi Yang, Akhila Yerukola, and Jiawei Zhou. 2021. The GEM benchmark: Natural language generation, its evaluation and metrics. In Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 96\u2013120, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-506", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-506", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4762\u20134772, Barcelona, Spain (Online). International Committee on Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-506", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I. IDENTIFYING INFORMATION\\n\\nTitle* SweWinogender\\nSubtitle A Swedish diagnostic set for gender bias in natural language inference models\\nCreated by* Yvonne Adesam (yvonne.adesam@gu.se), Gerlof Bouma (gerlof.bouma@gu.se)\\nPublisher(s)* Spr\u00e5kbanken Text\\nLink(s) / permanent identifier(s)* https://spraakbanken.gu.se/en/resources/swewinogender\\nLicense(s)* CC BY 4.0\\n\\nAbstract* The SweWinogender test set is a diagnostic dataset to measure gender bias in coreference resolution/textual entailment. It is modeled after the English Winogender benchmark and is released with reference statistics on the distribution of men and women between occupations and the association between gender and occupation in modern corpus material.\\n\\nFunded by* Vinnova (dnr 2020-02523, dnr 2021-04165); Spr\u00e5kbanken Text\\n\\nCite as [1]\\n\\nRelated datasets Part of the Superlim collection (https://spraakbanken.gu.se/en/resources/superlim)\\nSee SweWinogender v1.0 for a formulation of this task as a pronoun resolution problem.\\n\\nBased upon/partially translated from Winogender Schemas [2]\\n\\nII. USAGE\\n\\nKey applications Diagnose gender bias in natural language inference systems\\n\\nIntended task(s)/usage(s) (1) Indirectly the pronoun interpretation task cast as a natural language inference problem: decide whether a discourse fragment containing a pronoun entails a sentence with the pronoun replaced by a candidate antecedent.\\n(2) Compare system predictions between pronoun types (masc/fem/gender-neutral)\\n(3) Compare system predictions with auxiliary statistics on gender and occupation\\n\\nRecommended evaluation measures\\n(1) Krippendorff's alpha on binary label\\n(2) \u201cGender parity\u201d: The proportion of triples of items differing only by the type of pronoun, that receives identical labels. See [3].\\n(3) Correlation (Spearman\u2019s rho); plotting/visual inspection. See [2].\\n\\nDataset function(s) Diagnostics\\n\\nRecommended split(s) Test data only\\n\\nIII. DATA\\n\\nPrimary data* Text\\nLanguage* Swedish\\nDataset in numbers* 624 test items from 104 templates. 312 positive cases ('entailment') and 312 negative cases ('neutral').\"}"}
{"id": "emnlp-2023-main-506", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The test items are constructed from short discourse templates that contain two participants: one referred to by occupation, and one either by a role description. Furthermore, the templates contain a pronominal reference to one of these participants. The templates are constructed such that the interpretation of the pronoun follows from (common sense) reasoning. Each template gives rise to 6 test items: 3 possibilities depending on whether the feminine (\\\"hon/henne/hennes\\\"), masculine (\\\"han/honom/hans\\\") or gender-neutral pronoun (\\\"hen/hens\\\") is used, 2 possibilities depending on whether the hypothesis is entailed or not. A natural language inference model that is not sensitive to gender biases should therefore answer the same way for a triple of test items that only differs in which pronoun they contain.\\n\\nThe test set is accompanied by an auxiliary dataset that contains two sets of statistics on the association between occupation and gender for the occupations mentioned in the test set. These statistics were extracted from a real-world database and from a corpus, respectively. The auxiliary data can be used to study gender-occupation biases in the system more directly.\\n\\nTest items: JSON Lines, with 1 test item per line. Test items are given as a pair of sentences (\u2018premise\u2019 and \u2018hypothesis\u2019) and a \u2018label\u2019 attribute that says whether the hypothesis is entailed by the premise (\u2018entailment\u2019) or not (\u2018neutral\u2019). The metadata (\u2018meta\u2019) contains identifying information about the sentence template that generated the test item, and a \u2018tuple-id\u2019 that can be used to calculate parity.\\n\\nAuxiliary data: TSV file with one occupation per row. Gives the following columns of information: 1) occupation; 2) % female practitioners according to SCB; (3)\u2013(5) % occurrences in female-associated contexts using small/medium/large collocate sets. See [1] for an explanation of the different corpus measures.\\n\\nData source(s) The test items are loose translations and/or inspired by the Winogender Schemes of [2]. The auxiliary data was collected by the first authors of [1], in the context of an MA course. The real-world statistics on gender and occupation were compiled on the basis of Statistics Sweden/SCB\u2019s open data (CC BY 4.0). Where occupations do not map 1-1 to SCB\u2019s categorization scheme, the supplied statistics are averages over several relevant categories. See [1] for details. The corpus-based statistics on gender-association of occupations were compiled from the Swedish Culturomics Gigaword Corpus [4].\\n\\nData collection method(s) See [1]\\n\\nData selection and filtering See [1]\\n\\nData preprocessing See [1]\\n\\nData labeling Test items contain gold-standard coreference data by design.\\n\\nAnnotator characteristics Test item compilation: 1 native speaker of Swedish with PhD in computational linguistics, 1 near-native speaker of Swedish with PhD in (corpus) linguistics.\\n\\nIV . ETHICS AND CAVEATS\\n\\nNature of the content* The test items are constructed from short discourse templates that contain two participants: one referred to by occupation, and one either by a role description. Furthermore, the templates contain a pronominal reference to one of these participants. The templates are constructed such that the interpretation of the pronoun follows from (common sense) reasoning. Each template gives rise to 6 test items: 3 possibilities depending on whether the feminine (\\\"hon/henne/hennes\\\"), masculine (\\\"han/honom/hans\\\") or gender-neutral pronoun (\\\"hen/hens\\\") is used, 2 possibilities depending on whether the hypothesis is entailed or not. A natural language inference model that is not sensitive to gender biases should therefore answer the same way for a triple of test items that only differs in which pronoun they contain.\\n\\nThe test set is accompanied by an auxiliary dataset that contains two sets of statistics on the association between occupation and gender for the occupations mentioned in the test set. These statistics were extracted from a real-world database and from a corpus, respectively. The auxiliary data can be used to study gender-occupation biases in the system more directly.\\n\\nTest items: JSON Lines, with 1 test item per line. Test items are given as a pair of sentences (\u2018premise\u2019 and \u2018hypothesis\u2019) and a \u2018label\u2019 attribute that says whether the hypothesis is entailed by the premise (\u2018entailment\u2019) or not (\u2018neutral\u2019). The metadata (\u2018meta\u2019) contains identifying information about the sentence template that generated the test item, and a \u2018tuple-id\u2019 that can be used to calculate parity.\\n\\nAuxiliary data: TSV file with one occupation per row. Gives the following columns of information: 1) occupation; 2) % female practitioners according to SCB; (3)\u2013(5) % occurrences in female-associated contexts using small/medium/large collocate sets. See [1] for an explanation of the different corpus measures.\\n\\nData source(s) The test items are loose translations and/or inspired by the Winogender Schemes of [2].\\n\\nThe auxiliary data was collected by the first authors of [1], in the context of an MA course. The real-world statistics on gender and occupation were compiled on the basis of Statistics Sweden/SCB\u2019s open data (CC BY 4.0). Where occupations do not map 1-1 to SCB\u2019s categorization scheme, the supplied statistics are averages over several relevant categories. See [1] for details. The corpus-based statistics on gender-association of occupations were compiled from the Swedish Culturomics Gigaword Corpus [4].\"}"}
{"id": "emnlp-2023-main-506", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical considerations The auxiliary data contains information about the distribution between women and men across occupations, and therefore contains data about subpopulations. The data does not contain reference to individuals \u2014 neither directly nor indirectly.\\n\\nThings to watch out for This is meant as a diagnostic, not as a target for training. The diagnostic only concerns occupation and gender, and this is only one of the many ways gender bias may be present in a coreference resolution model. In the words of [2]: \u201cas a diagnostic test of gender bias, we view the schemas as having high positive predictive value and low negative predictive value; that is, they may demonstrate the presence of gender bias in a system, but not prove its absence.\u201d\\n\\nAlthough the test items contain a threeway distinction in the pronouns used (han [masc], hon [fem], hen [neutral], the auxiliary data is restricted to a binary gender perspective. For task (3) above, it may however be interesting to compare system predictions for the gender-neutral pronoun (\u201chen\u201d) items with the auxiliary statistics to better understand how a system handles resolution of this pronoun.\\n\\nV. ABOUT DOCUMENTATION\\n\\nData last updated* 20230125 v2.0\\n\\nWhich changes have been made, compared to the previous version*\\n\\nReformulation as a natural language inference task.\\n\\nAccess to previous versions\\n\\nEarlier versions available from website.\\n\\nThis document created* 20210614; Gerlof Bouma (gerlof.bouma@gu.se)\\n\\nThis document last updated* 20230208; Gerlof Bouma (gerlof.bouma@gu.se)\\n\\nWhere to look for further details\\n\\n- Documentation template version* v1.1\\n\\nVI. OTHER\\n\\nRelated projects - References\\n\\n[1] Hansson, Mavromatakis, Adesam, Bouma and Dann\u00e9lls (2021): The Swedish Winogender Dataset. In Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa), pp452\u2013459. http://www.ep.liu.se/ecp/178/052/ecp2021178052.pdf\\n\\n[2] Rudinger, Naradowsky, Leonard and Van Durme (2018): Gender bias in coreference resolution. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pp8\u201314. https://doi.org/10.18653/v1/N18-2002\\n\\n[3] Wang, Pruksachatkun, Nangia, Singh, Michael, Hill, Levy and Bowman (2019): SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. In Advances in Neural Information Processing Systems 32. https://papers.nips.cc/paper/2019/file/4496bf24afe7fab6f046bf4923da8de6-Paper.pdf\"}"}
{"id": "emnlp-2023-main-506", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Figure 1: A screenshot of the website showing the leaderboard, filtered to show only large language models of the \u2018base\u2019 category, sorted by performance on the DaLAJ-GED task on the validation (dev) set, with datasets for which the results are not available excluded.\\n\\n| Model                        | LS   | PAS  | L    | K    | UQnt | MNeg | 2Neg | Coref | Restr | Down | Accuracy |\\n|------------------------------|------|------|------|------|------|------|------|-------|-------|------|-----------|\\n| xlm-roberta-large            | 0.415| 0.441| 0.434| 0.345| 0.290| 0.509| 0.648| 0.641 | 0.391 | -0.541| 0.693     |\\n| KBLab/mt-bert-l-sw-c-165k    | 0.393| 0.368| 0.430| 0.305| 0.314| 0.669| 0.434| 0.641 | 0.334 | 0.115 | 0.760     |\\n| KBLab/mt-bert-b-sw-c-600k    | 0.363| 0.303| 0.392| 0.306| 0.283| 0.669| 0.43 | 0.641 | 0.365 | 0.000 | 0.911     |\\n| KB/bert-b-sw-c              | 0.349| 0.319| 0.354| 0.282| 0.273| 0.582| 0.366| 0.433 | 0.349 | -0.114| 0.618     |\\n| AI-Nordics/bert-l-sw-c      | 0.347| 0.310| 0.348| 0.308| 0.239| 0.510| 0.646| 0.799 | 0.391 | -0.114| 0.697     |\\n| KBLab/bert-b-sw-c-new       | 0.338| 0.327| 0.387| 0.277| 0.214| 0.513| 0.505| 0.571 | 0.361 | -0.114| 0.619     |\\n| xlm-roberta-base            | 0.318| 0.307| 0.359| 0.245| 0.180| 0.500| 0.145| 0.107 | 0.310 | -0.116| 0.517     |\\n| NbAiLab/nb-bert-base        | 0.314| 0.305| 0.358| 0.270| 0.144| 0.582| 0.578| 0.339 | 0.262 | -0.057| 0.520     |\\n| Decision tree               | 0.037| 0.000| 0.041| 0.047| 0.028| 0.555| -0.247| -0.141 | 0.113 | 0.266 | -0.149   |\\n| SVM                         | 0.026| -0.006| 0.000| 0.047| 0.003| 0.319| -0.366| -0.429 | -0.032| 0.080 | -0.082   |\\n| Random forest               | 0.010| -0.034| -0.052| 0.006| 0.067| 0.347| -0.284| 0.328 | -0.099| -0.127| -0.169   |\\n| Random                      | 0.004| -0.005| 0.040| 0.013| 0.020| 0.341| 0.004| -0.086 | 0.048 | -0.089| -0.119   |\\n| Majority label/Avg          | -0.404| -0.378| -0.482| -0.376| -0.350| -0.300| -0.351| -0.626| -0.411| -0.600| -0.579   |\\n\\nTable 7: The results on the SweDiagnostics dataset. We report Krippendorf\u2019s $\\\\alpha$ on the coarse-grained categories Lexical Semantics (LS), Predicate-Argument Structure (PAS), Logic (L) as well as Knowledge (K). In addition, five fine-grained categories, Universal Quantifiers (UQnt), Morphological Negation (MNeg), Double Negation (2Neg), Anaphora/Coreference (Coref), Restrictivity (Restr) and Downward Monotone (Down) are selected. The table reports categories identical to the ones reported in Table 5 in the GLUE paper (Wang et al., 2018), but note the difference in measure, which was $R^3$ in the cited paper.\"}"}
