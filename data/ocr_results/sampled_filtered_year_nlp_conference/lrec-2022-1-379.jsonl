{"id": "lrec-2022-1-379", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generating Textual Explanations for Machine Learning Models: A Table-to-Text Task\\n\\nIsaac Ampomah, James Burton, Amir Enshaei, Noura Al Moubayed\\n\\n1 Durham University, 2 Newcastle University, 3 Caspian Learning Ltd\\n{isaac.k.ampomah, james.burton, noura.al-moubayed}@durham.ac.uk, amir.enshaei@newcastle.ac.uk\\n\\nAbstract\\nNumerical tables are widely employed to communicate or report the classification performance of machine learning (ML) models with respect to a set of evaluation metrics. For non-experts, domain knowledge is required to fully understand and interpret the information presented by numerical tables. This paper proposes a new natural language generation (NLG) task where neural models are trained to generate textual explanations, analytically describing the classification performance of ML models based on the metrics' scores reported in the tables. Presenting the generated texts along with the numerical tables will allow for a better understanding of the classification performance of ML models. We constructed a dataset comprising numerical tables paired with their corresponding textual explanations written by experts to facilitate this NLG task. Experiments on the dataset are conducted by fine-tuning pre-trained language models (T5 and BART) to generate analytical textual explanations conditioned on the information in the tables. Furthermore, we propose a neural module, Metrics Processing Unit (MPU), to improve the performance of the baselines in terms of correctly verbalising the information in the corresponding table. Evaluation and analysis conducted indicate, that exploring pre-trained models for data-to-text generation leads to better generalisation performance and can produce high-quality textual explanations.\\n\\nKeywords: Table-to-text generation, Structured data, Model performance summarisation\\n\\n1. Introduction\\nStructured data-to-text generation is a natural language generation (NLG) task aiming to generate text from a structured source of data, most commonly either graphs or tables (Wen et al., 2015; Liu et al., 2018; Strauss and Kipp, 2008). Earlier work on data-to-text generation predominantly used rule-based methods (Goldberg et al., 1994; Reiter and Dale, 1997; Strauss and Kipp, 2008). These methods generate natural language text by employing linguistic rules and heuristics to select and populate pre-defined templates. However, a typical NLG system requires different sets of rules to perform content determination, text planning, sentence planning and surface realisation modules (Goldberg et al., 1994; van der Lee et al., 2017). This makes traditional NLG models difficult to maintain and less generalised.\\n\\nRecently, leveraging deep neural methods for NLG has been shown to outperform existing rule-based methods (Wen et al., 2015; Liu et al., 2018; Puduppully et al., 2019; Parikh et al., 2020; Suadaa et al., 2021). These models are usually trained end-to-end without the need for pre-defined linguistic rules. In broader terms, transfer learning has been shown to produce close to state-of-the-art performance for downstream NLP tasks with a limited amount of the dataset by utilising large pre-trained language models (Devlin et al., 2019; Radford et al., 2019). Furthermore, (Peng et al., 2020; Chen et al., 2020c) argue that pre-trained language models (GPT-2 (Radford et al., 2019) and T5 (Raffel et al., 2020)) can indeed improve performance on structured data-to-text task.\\n\\nGiven the machine learning problem under consideration, the model achieved a high accuracy score of 90.73% with a corresponding high AUC score of 95.87%. Also, the precision score is 89.13% and the recall/sensitivity score is 90.32%. From the dataset distribution provided, we can conclude that only the precision score and sensitivity score are important to accurately assess the performance of the model on this ML task. The scores achieved across these metrics are very high which imply that prediction decisions for the majority of the test cases will be correct. The recall and precision score motivate a higher trust in output predictions.\\n\\nDataset Imbalanced (62% and 38%)\\nLabels C1 and C2\\n\\nMetrics\\nSensitivity | Precision | Accuracy | AUC\\n---|---|---|---\\n90.32 | 89.13 | 90.73 | 95.87\\n\\nFigure 1: A table summarising the classification performance of a classifier.\\n\\nDomain knowledge are required to make sense of the graphs and tables. Therefore, non-experts will find it more challenging to fully understand the implications of the model's scores across the metrics. In response, this work presents a study on training neural models to generate textual explanations that analytically describe the classification performance of machine learning models. The generated textual explanation is based on the evaluation metrics' scores achieved, along with information on the underlying dataset (class labels and dataset distribution across the classes) of an arbitrary classification problem. The neural models are trained on table-explanation pairs annotated by computer science experts. Due to the limited size of our dataset, experiments are conducted by fine-tuning the pre-trained language models T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). These pre-trained models treat all text-based language tasks as text-to-text generation; therefore, following (Moryossef et al., 2019; Chen et\"}"}
{"id": "lrec-2022-1-379", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"al., 2020b; Suadaa et al., 2021), the performance summary tables are linearised as flat strings. However, converting structured data to flat strings can result in the loss of important information and relations (Mager et al., 2020; Hoyle et al., 2021; Suadaa et al., 2021), therefore, exploring strategies to improve the encoding of the structured data can further improve the quality of the output generated texts. To this end, we propose a neural module, **Metrics Processing Unit (MPU)**, to improve the performance of the pre-trained language models in terms of producing textual explanations verbalising correctly the information in the corresponding table. The MPU is employed to learn a semantic representation by directly encoding the information about the metrics from the table. The encoder combines the MPU's output representation with the embedding of the linearised representation to generate the contextualised joint representation information passed to the decoder.\\n\\nThe contributions of this work are as follows:\\n\\n- Introducing a new dataset for generating analytical textual explanations describing the performance of classifiers on several machine learning tasks. The textual explanations are written by computer science experts and checked manually to ensure that they accurately reflect or verbalise the information in the corresponding performance table. To the best of our knowledge, this is the first of its kind to focus on explaining the performance of ML models.\\n\\n- Proposing a neural module, **Metrics Processing Unit (MPU)**, which improves the encoding of the information about the metrics ensuring that the outputs of the pre-trained models accurately verbalise the performance report summarised by the related table.\\n\\n- Experiments with state-of-the-art neural models demonstrating the opportunities and challenges for future research on this table-to-text generation task.\\n\\nThe remainder of the paper is organised as follows: Section 2 briefly reviews the related works and Section 3 introduces the model performance explanations dataset. Section 4 introduces the models trained on the proposed dataset, the experiments conducted are presented in Section 5, and the results are compared and discussed in Section 6. Finally, the conclusions are presented in Section 7, together with avenues for future work.\\n\\n### 2. Related Works\\n\\n#### 2.1. Natural Language Generation (NLG)\\n\\nGenerating text from structured data has been a persistent NLG task over the years (Kukich, 1983; Reiter and Dale, 1997; Goldberg et al., 1994). Earlier work on data-to-text generation predominately employed rule-based methods (Goldberg et al., 1994; McKeown, 1992; Reiter and Dale, 1997; Strauss and Kipp, 2008). These methods generate natural language text by employing linguistic rules and heuristics to select and populate pre-defined templates. A typical NLG system requires different sets of rules to perform the content determination and text planning, sentence planning and surface realisation modules (Goldberg et al., 1994; McKeown, 1992; van der Lee et al., 2017). This makes traditional NLG models difficult to maintain and less generalisable. Furthermore, the output texts lack flexibility and diversity given that the same set of templates are used for text generations.\\n\\nRecent NLG research studies are moving towards exploring deep neural applications to automatically generate texts from structured data without relying on hand-engineered features and rules. These applications are table-to-text (Liu et al., 2018; Parikh et al., 2020; Su et al., 2021; Suadaa et al., 2021), table-based question answering (Wang et al., 2018; Chen et al., 2020a; Chemmengath et al., 2021), and graph to text generation (Ribeiro et al., 2020; Koncel-Kedziorski et al., 2019). However, neural models, despite their appeal, are data-hungry models requiring a large amount of high quality data to achieve higher generation performance.\\n\\nThe findings of the research works (Su et al., 2021; Peng et al., 2020; Chen et al., 2020b) demonstrate that state-of-the-art pre-trained language models such as GPT-2 (Radford et al., 2019), T5 (Raffel et al., 2020) and BART (Lewis et al., 2020) can be exploited to improve the performance on text generation tasks with limited amount of dataset. These pre-trained language models are well suited for text-to-text; hence applications to data-to-text tasks such as table-to-text and graphs-to-text require converting the structured data into linearised strings (Moryossef et al., 2019; Chen et al., 2020b; Hoyle et al., 2021). Despite this solution, the findings of (Mager et al., 2020; Hoyle et al., 2021; Suadaa et al., 2021) argue that the linearisation process can result in the loss of important information and relations. Therefore, in this work we propose the **Metrics Processing Unit (MPU)**, which improves the encoding of the information about the metrics ensuring that the generated texts from the pre-trained models accurately verbalise the performance report summarised by the related table.\\n\\n#### 2.2. Dataset and Task Design\\n\\nExisting datasets for table-to-text generation task include ROBERTA (Chen and Mooney, 2008), RO-TOWIRE (Wiseman et al., 2017), E2E (Novikova et al., 2016; Novikova et al., 2017), KBGENSEN (Banik et al., 2013), WEATHERGOV (Liang et al., 2009), and WIKIBIO (Lebret et al., 2016). These datasets cover different topics and themes. For example, E2E focuses on...\"}"}
{"id": "lrec-2022-1-379", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the model performance narrations\\ndataset\\n\\ngenerates summaries of basketball sporting event and WIKIBIO focuses on producing biographies from a given Wikipedia bio-table. Numerical tables are usually employed to report the performance of trained ML models. Understanding these numerical tables requires background and domain knowledge; hence augmenting the numerical tables with analytical texts will enable non-experts to find it easy to make sense of the scores achieved by the model on the arbitrary ML task. To the best of our knowledge, there is no work conducted on generating textual explanations describing the performance of ML models trained on arbitrary prediction tasks. The dataset most similar to the one introduced in this paper is the NUMERICAL TABLE-TO-TEXT proposed by Suadaa et al. (2021). Unlike (Suadaa et al., 2021), our dataset is not curated from numerical tables of experimental results published in scientific research articles. We trained different ML models on 59 classification tasks, and the performance of each model was summarised in numerical tables. For each table, computer science experts were tasked to provide analytical statements describing the performance of the model on the corresponding classification task. Fig. 2 shows an example of a classification performance summary table and the corresponding textual explanation.\\n\\n3. Performance Explanations Dataset\\n\\nTo acquire the dataset for this study, different ML models were trained on 59 classification tasks across different application domains. Across each classification task, five different classifiers were trained. These classification models include random forest, support vector machines, logistic regression and K-nearest neighbour (KNN). For simplicity, only the common classification metrics (accuracy, precision, AUC, recall, specificity, F1-score, and F2-score) were considered. Ten computer science experts were hired to provide the analytical textual explanations. For each classifier, the expert is tasked to provide sentences summarising the performance based on the information in the corresponding table. To guide the annotators, they were instructed to respond (in English) to the following:\\n\\na. Provide a summary of the scores achieved by the model across the evaluation metrics.\\n\\nb. Discuss the overall performance of the model as shown by the values of the evaluation metrics. (Your answer should capture the implications of achieving such scores across the different metrics.)\\n\\nWhen tasked to summarise information in numerical tables, humans typically perform numerical reasoning where the values are rated as either high or low or moderate. These ratings are used as a guide to accurately present the implications of the values in numerical tables. Therefore, during the annotation, the experts were asked to rate the scores across each metric (taking into account the distribution of the dataset across the classes) on a 3-point scale (Low, Moderate, and High). These ratings are used to enrich the metrics' tables (for example, see Fig. 2), providing the NLG model with a form of numerical reasoning.\\n\\nWe collected 1010 annotations from experts; each submission was analysed manually by comparing it to the corresponding table. Out of 1010 submissions, 825 accurately captured the information presented in the table. We sampled 100 table-explanation pairs randomly as the test set and the remaining 725 table-explanation pairs are used for the training set. Ideally, we expect the NLG model to generate similar narratives for a numerical table, irrespective of the order of the metrics in the linearised input. However, our preliminary analysis suggested that neural models tend to be sensitive to the order of the metrics; that is, the models generated\"}"}
{"id": "lrec-2022-1-379", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given the machine learning problem under consideration, the model achieved a high accuracy score of 90.73% with a corresponding high AUC score of 95.87%. Also, the precision score is 89.13% and the recall/sensitivity score is 90.32%. From the dataset distribution provided, we can conclude that only the precision score and sensitivity score are important to accurately assess the performance of the model on this ML task. The scores achieved across these metrics are very high which imply that prediction decisions for the majority of the test cases will be correct. The recall and precision score motivate a higher trust in output predictions.\\n\\nNeural Generator\\n\\nFigure 3: An illustration of our table-to-text neural generator trained to produced the textual explanation based on the linearised table and metrics-ratings-values semantic representation, learned by the Metrics Processing Unit (MPU).\\n\\nDifferent narrations for the same table depending on the order of the metrics. Even though these narrations were fluent, they sometimes contained incorrect facts when compared to the related tables. Therefore, to make the models less dependent on the order of the metrics and focus more on verbalising the facts presented in the table, the training set is augmented with new table-explanation pairs from the permutations of the order of the metrics. This process increased the training set size from 725 to 4529 table-explanation pairs. Table 1 provides some statistics about the dataset. In Section 6, we compare the performance of the neural generators trained on the original dataset to those trained on the permuted dataset.\\n\\n4. Models\\n\\n4.1. Problem Definition\\n\\nIn this work, the input to the NLG models is a numerical table containing the evaluation metrics' scores of a classifier, the list of class labels and the distribution of the dataset across the labels. Specifically, the metrics table summarising the classifier's performance on a given machine learning problem is represented as \\\\( T = [D, C, S] \\\\), where\\n\\n\\\\( D \\\\in \\\\{ \\\\text{is balanced, is imbalanced} \\\\} \\\\)\\n\\nis a flag indicating if the dataset was balanced, \\\\( C = [C_1, C_2, \\\\ldots, C_k] \\\\) is the list of class labels, and \\\\( S = [(m_1, r_1, v_1), (m_2, r_2, v_2), \\\\ldots, (m_n, r_n, v_n)] \\\\) is the list of performance scores. The performance scores \\\\( S \\\\) consist of the metrics \\\\( [m_1, m_2, \\\\ldots, m_n] \\\\) their values \\\\( [v_1, v_2, \\\\ldots, v_n] \\\\) and the annotator's ratings \\\\( [r_1, r_2, \\\\ldots, r_n] \\\\) where \\\\( n \\\\) is the number of metrics. The goal is to generate an analytical textual explanation \\\\( Y = (y_1, y_2, \\\\ldots, y_b) \\\\) of length \\\\( b \\\\) based on the information presented in \\\\( T \\\\), where \\\\( y_t \\\\) is the \\\\( t \\\\)th target word. It is noteworthy that the generated text \\\\( Y \\\\) should be fluent and numerically supported by \\\\( T \\\\). An example of a table and the analytical textual explanation is shown in Fig. 2.\\n\\nThe neural generation model learns its parameter \\\\( \\\\theta \\\\) by maximising the likelihood function:\\n\\n\\\\[\\nP(Y | T; \\\\theta) = \\\\prod_{t=1}^{b} P(y_t | y_{<t}, T; \\\\theta)\\n\\\\]\\n\\nwhere \\\\( y_{<t} = y_1, \\\\ldots, y_{t-1} \\\\) is the partial target sequence generated.\\n\\n4.2. Baselines\\n\\nGiven the limited amount of data, experiments are conducted using pre-trained language models, T5 (Raffel et al., 2020) and BART (Lewis et al., 2020). Leveraging these pre-trained language models has been shown to produce high quality texts even when fine-tuned on limited amount of data (Peng et al., 2020; Su et al., 2021; Suadaa et al., 2021).\\n\\nT5 model\\n\\nT5 (Raffel et al., 2020) is a transformer-based model trained in a multitask fashion on a variety of unsupervised and supervised NLP tasks including summarisation, classification, and translation. This neural model treats all text-based language problems as a text-to-text generation task (Raffel et al., 2020).\"}"}
{"id": "lrec-2022-1-379", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Architecture of the Metrics Processing Unit (MPU) employed to learn to extract h scores from the list of performance scores S.\\n\\nBART model\\n\\nBART (Lewis et al., 2020) is a denoising auto-encoder for pre-training sequence to sequence models. This transformer-based architecture was trained to reconstruct the original text from corrupted input text. BART exploits the strengths of BERT (Devlin et al., 2019) and GPT (Radford et al., 2019). Specifically, it comprises a bidirectional encoder (similar to BERT) and a left-to-right decoder (similar to GPT).\\n\\n4.3. Table Representation\\n\\nBART and T5 were not trained on any table-to-text generation task and as such, all text-based language problems using these models need to be framed as text-to-text generation problems (Chen et al., 2020b; Suadaa et al., 2021; Hoyle et al., 2021). In this study, the input to the neural NLG models is a table summarizing the classification performance of a given classifier on an arbitrary ML task, as shown in Fig. 2.\\n\\nTherefore, in order to convert it to a text-to-text problem and following (Moryossef et al., 2019; Chen et al., 2020b; Suadaa et al., 2021), the input table is linearised into a flat string. In this paper, the linearisation is performed by concatenating metric information, class labels, and the dataset distribution based on the following template:\\n\\n\\\\[\\n\\\\begin{align*}\\n&METRICS INFO \\\\quad m_1 \\\\mid VALUE_r_1 \\\\mid v_1 \\\\quad \\\\& \\\\quad m_2 \\\\mid VALUE_r_2 \\\\mid v_2 \\\\quad \\\\& \\\\quad \\\\cdots \\\\quad \\\\& \\\\quad m_n \\\\mid VALUE_r_n \\\\mid v_n \\\\quad <| \\\\quad SECTION-SEP |> \\\\\\\\\\n&TASK ML \\\\mid DATASET_ATTRIBUTES \\\\mid D \\\\quad \\\\& \\\\quad TASK ML \\\\mid CLASS_LABELS \\\\mid C_1, C_2, \\\\cdots, C_k \\\\quad <| \\\\quad TABLE_2 |> \\\\quad TEXT |\\n\\\\end{align*}\\n\\\\]\\n\\nFig. 3 shows an example of the input data and the corresponding linearised representation. The linearised data is tokenised as \\\\( X = [x_1, x_2, \\\\cdots, x_p] \\\\) and converted into the tokens embedding \\\\( E_x = [e_x_1, e_x_2, \\\\cdots, e_x_p] \\\\) by the encoder's embedding unit. \\\\( p \\\\) is the number of tokens in the linearised data. The encoder outputs the contextual representation of the linearised table, \\\\( h \\\\in \\\\mathbb{R}^{p \\\\times d_{model}} \\\\), based on \\\\( E_x \\\\). The decoder iteratively attends to previously generated tokens \\\\( y_t \\\\) (via self-attention) and the encoder outputs \\\\( h \\\\) (via cross-attention) to predict the next target token, \\\\( y_{t+1} \\\\).\\n\\n4.4. Metrics Processing Unit\\n\\nOne drawback of using only the linearised data input is that it fails to capture the actual information and relationships represented by the structured data (Suadaa et al., 2021; Mager et al., 2020; Hoyle et al., 2021). On this task, we would like the models' decoders to be able to accurately verbalise the metrics information. The performance of the decoder is linked to that of the encoder; hence, exploiting strategies to improve the data representation ability of the encoder can further improve the quality of the output summaries. To this end, we propose augmenting the linearised input with semantic representations of the metrics information generated by a neural module: the Metrics Processing Unit (MPU).\\n\\nScores Embedding Unit\\n\\nGiven \\\\( S = [(m_1, r_1, v_1), (m_2, r_2, v_2), \\\\cdots, (m_n, r_n, v_n)] \\\\), this unit generates the subword token-based embeddings \\\\( \\\\hat{M} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}}, \\\\hat{R} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\) and \\\\( \\\\hat{V} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\), respectively, representing the metrics, rate and values. \\\\( z \\\\) is the maximum number of subword tokens and \\\\( d_{model} \\\\) is the dimension of hidden representation. To reduce the size of the final model, the embeddings are performed using the same embedding parameters employed by the Neural Generator's encoder and decoder subnetworks.\\n\\nHighlight Module\\n\\nThis module produces the semantic representations of the metric names, values and ratings from their corresponding subword tokens representation produced by the Scores Embedding Unit. Specifically, the inputs to this module are \\\\( \\\\hat{M} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}}, \\\\hat{R} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\) and \\\\( \\\\hat{V} \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\), the outputs of the Scores Embedding Unit. Three self-attention units (one for each input representation) are employed to transform \\\\( \\\\hat{M}, \\\\hat{V} \\\\) and \\\\( \\\\hat{R} \\\\), respectively into \\\\( h_m \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}}, h_v \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\), and \\\\( h_r \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}} \\\\).\\n\\nInteraction Module\\n\\nThis module generates the representation \\\\( h_s \\\\in \\\\mathbb{R}^{n \\\\times z \\\\times d_{model}}, \\\\) the metrics-values-ratings contextual information, from combination of outputs of the Highlight Module. As shown in Fig. 4, this module consists of two Combination (CM) units and one Aggregation unit (AGG). The output is computed as follows:\\n\\n\\\\[\\n\\\\begin{align*}\\nh_s &= AGG(\\\\hat{h}_m, \\\\hat{h}_v) + h_m + h_v + h_r \\\\\\\\\\n\\\\hat{h}_m &= CM(h_m, h_r), \\\\quad \\\\hat{h}_v = CM(h_v, h_r) \\\\\\\\\\n\\\\hat{h} &= CM(A, B)\\n\\\\end{align*}\\n\\\\]\\n\\n\\\\( \\\\hat{h} \\\\) is the result of the interaction module. The CM function is defined as:\\n\\n\\\\[\\nCM(A, B) = ReLU(W_c [A; B]) + B\\n\\\\]\\n\\nwhere \\\\( W_a, W_c \\\\in \\\\mathbb{R}^{2 \\\\cdot d_{model} \\\\times d_{model}} \\\\) are trainable model parameters employed to compute \\\\( h_s \\\\).\"}"}
{"id": "lrec-2022-1-379", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"notes concatenation operation. The generated $h_s = [h_{s1}, h_{s2}, \\\\cdots, h_{sn}]$ is concatenated with the embedding of the linearised input table ($E_x$), then processed via the different encoder layers to generate the contextualised joint source representations $h \\\\in \\\\mathbb{R}^{(n \\\\times z_p + p) \\\\times d_{model}}$.\\n\\n5. Experiments\\n\\n5.1. Model Settings, Training and Inference\\n\\nDifferent variants of the T5 and BART models are explored in this work: T5-small, T5-base, T5-large, BART-base, and BART-large. These models differ in terms of the number of parameters (model size). The fine-tuning of the models is performed using Adam optimizer (Kingma and Ba, 2014) with an initial learning rate equal to $3 \\\\times 10^{-4}$. For the T5 models, the learning rate is scheduled linearly during the course of training without any warmup schedule steps. However, our preliminary experiments conducted on BART models showed that warmup steps are required to achieve good generation performance. Therefore, 21% of the training steps are used as the linear warmup schedule steps. For simplicity, the T5-variant+MPU and BART-variant+MPU respectively, denote the T5 and BART model variants augmented with the auxiliary information from the MPU. During inference, the textual explanations are generated via beam search with a beam size of 8, length penalty $\\\\alpha = 8^6$, and repetition penalty of 1.5. Our models implementations are based on Huggingface Transformers (Wolf et al., 2019).\\n\\n5.2. Evaluation Metrics\\n\\nThe generation performance of the models is evaluated based on the quality of the generated textual explanations. The metrics employed to assess the quality are BLEU (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), BLEURT (Raffel et al., 2020) and PARENT (Dhingra et al., 2019). BLEU and METEOR compute the surface-level similarity between the generated texts and only the human (reference) texts. Dhingra et al. (2019) argued that BLEU, and METEOR sometimes penalise generated text for including additional information, correct according to the table but missing in the reference text. In response, they proposed PARENT, a data-to-text evaluation metric that takes into consideration the information present in the table. The BLEURT score is a semantic equivalence-based metric indicating the extent to which the generated text is fluent and conveys the meaning of reference text. For each metric mentioned above, the higher the score, the better the model.\\n\\nEach model is trained five times with different random seeds and we report the generation performance of the models based on the average of the scores for each evaluation metric.\\n\\n6. Results\\n\\nThis section presents the evaluation performance of the models on the NLG task under consideration. Table 2 shows the scores achieved by the models across the evaluation metrics: BLEU, METEOR, PARENT, and BLEURT. The different variants of the pre-trained models achieved varying scores. Among the T5 models, the T5-small achieved the worst performance, according to BLEU, and METEOR. However, it outperformed T5-base and T5-large in terms of PARENT and BLEURT scores. For large T5 models (T5-base and T5-large), there is a higher possibility that the generated texts will be identical to the reference texts; hence, the high scores for the word-overlap metrics: BLEU, and METEOR. The PARENT score suggests the T5-small is very good at matching both the reference text and the source table. Between BART-base and BART-large, the former outperforms the later judging based on the METEOR, PARENT and BLEURT scores. On the other hand, BART-large scored +0.74 BLEU over BART-base.\\n\\nEffect of Metric Processing Unit\\n\\nAccording to Table 2, augmenting the linearised representation with the semantic representations of the metrics information in the table further improves the generation performance of the underlying models. The T5-small bene...\"}"}
{"id": "lrec-2022-1-379", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Given the machine learning problem under consideration, the model achieved a high accuracy score of 90.73% with a corresponding high AUC score of 95.87%. Also, the precision score is 89.13% and the recall/sensitivity score is 90.32%. From the dataset distribution provided, we can conclude that only the precision score and sensitivity score are important to accurately assess the performance of the model on this ML task. The scores achieved across these metrics are very high which imply that prediction decisions for the majority of the test cases will be correct. The recall and precision score motivate a higher trust in output predictions.\\n\\nT5-small\\nThe performance of the classifier on this binary classification task as evaluated based on the precision, AUC, accuracy, and sensitivity achieved the scores 89.13%, 90.32%, 95.87%, and 90.73% respectively. These scores support the conclusion that this model will be highly effective at correctly classifying most test cases/samples with only a few misclassifications. Furthermore, the confidence in predictions related to the label C2 is very high.\\n\\nT5-base\\nOn this imbalanced classification task, the trained model scored 90.32% (recall), 90.73% (accuracy), and 95.87% (AUC). From the precision score, it is obvious that the model has a moderately high false-positive rate hence the confidence in predictions related to the C2 label is very high. However, based on the sensitivity and precision scores, we can see that some examples belonging to C1 are likely to be misclassified as C2. This is further supported by the high accuracy and AUC scores.\\n\\nT5-base+MPU\\nThe classifier scored close to perfect scores across all the metrics (i.e. Precision, AUC, Accuracy and Sensitivity). From the results table, we can see that it scored 90.73% (accuracy), 90.32% (sensitivity), and 95.87% (AUC). Surprisingly, these scores were achieved even though the dataset was imbalanced. With such high scores for precision and sensitivity, the model is shown to have a lower misclassification error rate. Overall, this model has relatively high classification performance, and hence will struggle to correctly identify the labels for only a few test cases belonging to the different classes.\\n\\nT5-large\\nEvaluating the performance of the model on this classification task produced the scores: 95.87% for AUC, 90.73% for accuracy, 89.13% precision, and 90.32% for sensitivity (recall). From the recall and precision scores, we can see that the classification algorithm is very effective at correctly classifying most unseen test cases or samples with only a small margin of error (the misclassification error rate is 9.27%). The very high accuracy coupled with the very low precision score demonstrates its capability to correctly identify the correct class labels for several test instances.\\n\\nT5-large+MPU\\nThe classifier scored close to perfect scores across all the metrics (i.e. Precision, AUC, Accuracy and Sensitivity). From the results table, we can see that it scored 89.13% (Precision), 90.32% (sensitivity), 95.87% (AUC), and 90.73%(Accuracy). Since the dataset was imbalanced, it would be wise to analyze prediction performance based on the balance between the recall and precision scores. The precision and sensitivity scores show how good the model is at correctly recognising the observations under the different classes (C1 and C2). In summary, the models are likely to have a lower misclassification error.\\n\\nBART-base\\nThe classification performance scores achieved by the model on this binary classification task are as follows: (1) AUC score of 95.87, (2) Accuracy equal to 90.73%, (3) Precision score equal 89.13%, and (4) Sensitivity (sometimes referred to as the recall) score is 90.32%. These scores across the different metrics suggest that this model is very effective and can accurately identify the true labels for several test cases/samples with a small margin of error (actually, the misclassification error is 9.27%)\\n\\nBART-base+MPU\\nThe scores achieved by the AI algorithm on this binary classification task are as follows (1) AUC score of 95.87%, (2) Accuracy equal to 90.73%, and (3) Precision score equal 89.13%. These scores across the different metrics suggest that this model is very effective and can accurately identify the true labels for the majority of the cases with a small margin of error. Furthermore, the precision and recall scores indicate that the likelihood of misclassifying C1 cases as C2 is very marginal (that is, it has a very low false-positive rate).\\n\\nBART-large\\nThe performance of the model on this binary classification task as evaluated based on the precision, AUC, accuracy, and sensitivity scored 89.13%, 95.87%, 90.73%, and 90.32% respectively, implying that it is a very effective model. These scores indicate that the likelihood of misclassifying test samples is very marginal. However, the scores were expected the dataset was perfectly balanced between the two class labels C1 and C2.\\n\\nBART-large+MPU\\nOn this imbalanced classification task, the trained model reached an AUC score of 95.87, an accuracy of 90.73, with a precision and sensitivity scores equal to 89.13 and 90.32, respectively. These results/scores are very impressive as one can conclude that this model is almost perfect with higher confidence in its prediction decisions. In summary, only a small number of test cases are likely to be misclassified as indicated by the accuracy, sensitivity, and precision.\"}"}
{"id": "lrec-2022-1-379", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Edited most among all the models. Specifically, there is +2.75, +3.94, +1.92, and +0.6 increase in the BLEU, METEOR, PARENT and BLEURT, respectively. Furthermore, T5-small+MPU is shown to have the best match to both the reference text and the source table according to PARENT. It outperforms the next best models BART-base+MPU and BART-large+MPU by +0.77 PARENT, and +0.78 PARENT, respectively. Furthermore, despite having a poor surface-level match to the reference texts according to BLEU and METEOR, it achieves the best BLEURT score meaning it\u2019s outputs are fluent and semantically equivalent to the reference texts.\\n\\nPermuted vs Original\\nTo analyse the impact of augmenting the dataset with the diverse representations of the tables by permuting the order of the metrics, we trained all the neural models on the original 725 training pairs. The generation performance of the neural models is summarised in Table 3. Compared to the results in Table 2, the models performed worse in general according to the BLEU and METEOR scores. For example, while BART-large scored 28.09 BLEU, and 35.55 METEOR when trained on the original dataset, it scored 45.57 BLEU and 46.41 METEOR when trained on the permuted dataset. Conversely, in some cases, the model trained on the original dataset outperformed the corresponding variant trained on the permuted dataset in terms of the PARENT and BLEURT scores. Judging based on the average performance across all the metrics, the augmentation of the dataset is shown to improve generalization performance given that the models will be exposed to diverse representations of the tables.\\n\\nQuality Analysis\\nTable 4 shows the performance of the models under consideration based on the table shown in Fig. 2. Sentences and phrases conveying correct information according to the related table are highlighted in green, while incorrect ones are marked in red. As shown, the generators are able to produce high-quality classification performance summaries capturing the information presented in the input structured data. However, there were a number of cases where the generators failed to accurately verbalise the content of the related performance metric table. The errors are mainly from the models trained without MPU and among these models, only BART-base produced a correct verbalisation of the input table. The summary from T5-small is mostly valid; however, the metrics (AUC, accuracy and sensitivity) and their corresponding scores are mentioned in the wrong order. The T5-base made an incorrect assessment of the \u201cprecision\u201d and \u201crecall\u201d scores when it concluded that the \u201cfalse-positive rate\u201d is moderately high. In the case of the T5-large, it stated that the precision is very low even though it was rated \u201cHIGH\u201d. BART-large produced a wrong statement about the distribution of the dataset between the classes, C1 and C2. Augmenting the linearised representations with the metrics-values-ratings, contextual information from MPU allow the T5 and BART models to generate accurate analytical textual explanations based on the related table.\\n\\n7. Conclusion\\nThis work presents a new NLG dataset for generating textual explanations describing the performance of classification models. Presenting the generated texts along with the numerical tables will allow for a better understanding of the classification performance of ML models. We trained baselines by fine-tuning state-of-the-art pre-trained models: T5 and BART. Experimental results show the feasibility of utilising these large pre-trained language models to generate fluent and accurate statements based on structured data. However, analyses suggest that neural models in some instances produce statements containing wrong information according to the input table. This weakness can be attributed to direct linearisation of the input tables. To address this problem, we introduced the Metric Processing Unit (MPU) which, when combined with the linearised input, produced the best performance across the different T5 and BART variants.\\n\\nIn the future, we plan on conducting in-depth human evaluations to assess the quality and usefulness of the generated texts. As stated in Section 3 only the most popular evaluation metrics were considered hence the NLG models will struggle to produce meaningful analytical texts for instances with less common metrics such as Cohen Kappa, Matthews Correlation Coefficient (MCC), and Brier score. Therefore, a possible avenue of future is expanding the dataset to include textual explanations on these metrics.\\n\\n8. Bibliographical References\\nBanerjee, S. and Lavie, A. (2005). Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\\nBanik, E., Gardent, C., and Kow, E. (2013). The kb-gen challenge. In Proceedings of the 14th European Workshop on Natural Language Generation, pages 94\u201397.\\nChemmengath, S., Kumar, V., Bharadwaj, S., Sen, J., Canim, M., Chakrabarti, S., Gliozzo, A., and Sankaranarayanan, K. (2021). Topic transferable table question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4159\u20134172.\\nChen, D. L. and Mooney, R. J. (2008). Learning to sportscast: a test of grounded language acquisition. In Proceedings of the 25th international conference on Machine learning, pages 128\u2013135.\\nChen, W., Chang, M.-W., Schlinger, E., Wang, W. Y., and Cohen, W. W. (2020a). Open question answering...\"}"}
{"id": "lrec-2022-1-379", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chen, W., Chen, J., Su, Y., Chen, Z., and Wang, W. Y. (2020b). Logical natural language generation from open-domain tables. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7929\u20137942.\\n\\nChen, Z., Eavani, H., Chen, W., Liu, Y., and Wang, W. Y. (2020c). Few-shot nlg with pre-trained language model. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 183\u2013190.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1).\\n\\nDhingra, B., Faruqui, M., Parikh, A., Chang, M.-W., Das, D., and Cohen, W. (2019). Handling divergent reference texts when evaluating table-to-text generation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4884\u20134895.\\n\\nGoldberg, E., Driedger, N., and Kittredge, R. I. (1994). Using natural-language processing to produce weather forecasts. IEEE Expert, 9(2):45\u201353.\\n\\nHoyle, A. M., Marasovi\u00b4c, A., and Smith, N. A. (2021). Promoting graph awareness in linearized graph-to-text generation. ArXiv, abs/2012.15793.\\n\\nKingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv:1412.6980.\\n\\nKoncel-Kedziorski, R., Bekal, D., Luan, Y., Lapata, M., and Hajishirzi, H. (2019). Text generation from knowledge graphs with graph transformers. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2284\u20132293.\\n\\nKukich, K. (1983). Design of a knowledge-based report generator. In 21st Annual Meeting of the Association for Computational Linguistics, pages 145\u2013150.\\n\\nLebret, R., Grangier, D., and Auli, M. (2016). Neural text generation from structured data with application to the biography domain. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1203\u20131213.\\n\\nLewis, M., Liu, Y., Goyal, N., Ghazvininejad, M., Mohamed, A., Levy, O., Stoyanov, V., and Zettlemoyer, L. (2020). Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880.\\n\\nLiang, P., Jordan, M. I., and Klein, D. (2009). Learning semantic correspondences with less supervision. In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP, pages 91\u201399.\"}"}
{"id": "lrec-2022-1-379", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a unified text-to-text transformer.\\n\\nReiter, E. and Dale, R. (1997). Building applied natural language generation systems. Natural Language Engineering, 3(1):57\u201387.\\n\\nRibeiro, L. F., Schmitt, M., Sch\u00fctze, H., and Gurevych, I. (2020). Investigating pretrained language models for graph-to-text generation. arXiv preprint arXiv:2007.08426.\\n\\nStrauss, M. and Kipp, M. (2008). Eric: a generic rule-based framework for an affective embodied commentary agent. In Proceedings of the 7th international joint conference on Autonomous agents and multiagent systems-Volume 1, pages 97\u2013104.\\n\\nSu, Y., Meng, Z., Baker, S., and Collier, N. (2021). Few-shot table-to-text generation with prototype memory. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 910\u2013917.\\n\\nSuadaa, L. H., Kamigaito, H., Funakoshi, K., Okumura, M., and Takamura, H. (2021). Towards table-to-text generation with numerical reasoning. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1451\u20131465.\\n\\nvan der Lee, C., Krahmer, E., and Wubben, S. (2017). Pass: A dutch data-to-text system for soccer, targeted towards specific audiences. In Proceedings of the 10th International Conference on Natural Language Generation, pages 95\u2013104.\\n\\nWang, H., Zhang, X., Ma, S., Sun, X., Wang, H., and Wang, M. (2018). A neural question answering model based on semi-structured tables. In Proceedings of the 27th International Conference on Computational Linguistics, pages 1941\u20131951.\\n\\nWen, T.-H., Gasic, M., Mrksic, N., Su, P.-h., Vandyke, D., and Young, S. J. (2015). Semantically conditioned lstm-based natural language generation for spoken dialogue systems. In EMNLP.\\n\\nWiseman, S., Shieber, S. M., and Rush, A. M. (2017). Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253\u20132263.\\n\\nWolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., et al. (2019). Huggingface's transformers: State-of-the-art natural language processing. arXiv preprint arXiv:1910.03771.\"}"}
