{"id": "lrec-2024-main-868", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Korean Bio-Medical Corpus (KBMC) for Medical Named Entity Recognition\\n\\nSungjoo Byun\\\\textsuperscript{1}, Jiseung Hong\\\\textsuperscript{2}, Sumin Park\\\\textsuperscript{1}, Dongjun Jang\\\\textsuperscript{1}, Jean Seo\\\\textsuperscript{1}, Minseok Kim\\\\textsuperscript{1}, Chaeyoung Oh\\\\textsuperscript{1}, Hyopil Shin\\\\textsuperscript{1}\\n\\n\\\\textsuperscript{1}Seoul National University\\n\\\\{byunsj, mam3b, qwer4107, seemdog, snumin44, nyong10, hpshin\\\\}@snu.ac.kr\\n\\n\\\\textsuperscript{2}KAIST\\njiseung.hong@kaist.ac.kr\\n\\nAbstract\\n\\nNamed Entity Recognition (NER) plays a pivotal role in medical Natural Language Processing (NLP). Yet, there has not been an open-source medical NER dataset specifically for the Korean language. To address this, we utilized ChatGPT to assist in constructing the KBMC (Korean Bio-Medical Corpus), which we are now presenting to the public. With the KBMC dataset, we noticed an impressive 20% increase in medical NER performance compared to models trained on general Korean NER datasets. This research underscores the significant benefits and importance of using specialized tools and datasets, like ChatGPT, to enhance language processing in specialized fields such as healthcare.\\n\\nKeywords: Medical NER, Korean NER dataset, Domain-specific, Data construction with LLM\\n\\n1. Introduction\\n\\nThe significance of domain-specific Named Entity Recognition (NER), especially in fields like law and medicine, calls for more in-depth research and investigation. The role of NER in medical NLP is as follows: Firstly, NER contributes to processing medical terminology. Medical NER enables language models to identify and process medical terminologies and jargon. Next, it facilitates information extraction from unstructured data. In fact, Pearson et al. (2021) have performed NER to remove or encode information from an unstructured medical dataset. Moreover, NER contributes to entity identification and the anonymization of sensitive patient-specific information (Catelli et al., 2021).\\n\\nHowever, it is problematic that medical NER datasets are insufficient. This problem becomes even more challenging as domain-specific NER tasks require extensive labeling, particularly for specific entity categories like Disease, Body, and Treatment. The difficulty is further amplified due to the necessity of expert-level knowledge in medical domains. The data scarcity issue worsens in relatively low-resource languages like Korean. The fact that there is no open-source medical NER dataset for Korean demonstrates the severity of the problem.\\n\\nIn order to resolve the data scarcity problem, we introduce KBMC (Korean Bio-Medical Corpus), the first open-source medical NER dataset for Korean. We utilize ChatGPT\\\\textsuperscript{1} for effective sentence creation. Subsequently, we annotate entities corresponding to disease name, body part, and treatment following the BIO format. To augment the dataset and to check the performance in general text as well, we concatenate the Naver dataset,\\\\textsuperscript{2} which is the Korean NER dataset with our KBMC in the experiment.\\n\\nIn our research, we evaluate the effectiveness and utility of KBMC by comparing the performance of multiple language models. These models either use a general NER dataset (solely the Naver NER dataset) or a domain-specific dataset (a combination of the Naver NER dataset and KBMC). The results demonstrate that our dataset significantly enhances the accurate recognition of medical entities by more than 20 percent.\\n\\nContributions of our research are as follows:\\n\\n\u2022 We describe and publicly release Korean Bio-Medical Named Entity Recognition Corpus (KBMC), the first open-source Korean medical NER dataset. This contributes to solving the data scarcity problem.\\n\\n\u2022 Our research aims to play crucial role in medical data processing. Medical NER would facilitate the sensitive data anonymization process and contribute to the reconstruction of medical data that lack standardized formats.\\n\\n2. Related Work\\n\\nMedical NER\\n\\nAs a part of the entity representation task, various studies, mainly in English, have explored the medical field. Traditional research has conducted bio-medical NER using Long short-term memory (LSTM) models (Liu et al., 2017; Lyu et al., 2017;...\"}"}
{"id": "lrec-2024-main-868", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cho and Lee, 2019). Peng et al. (2019) test Biomedical Language Understanding Evaluation (BLUE) benchmark, including NER with BERT and ELMo. Bio-BERT, a pre-trained language representation model for biomedical text mining, shows high performance in bio-NER (Lee et al., 2019). Also, various toolkits that facilitate clinical NER implemented using SpaCy (Eyre et al., 2021), Apache Spark (Kocaman and Talby, 2022), and Flair (Weber et al., 2021) have been introduced.\\n\\nMedical NER dataset\\n\\nMedical NER is crucial, as shown by numerous medical concept extraction challenges, such as those hosted by i2b2 (Uzuner et al., 2011) and n2c2 (Henry et al., 2019). To tackle the data scarcity in the medical field, SemClinBER, a Portuguese medical NER dataset, was introduced by Oliveira et al. (2022), and a Chinese NER dataset was developed by Cheng et al. (2021). Additionally, NCBI-disease (Dogan et al., 2014) and BC5CDR (Li et al., 2016) provide annotations for medical entities in PubMed abstracts. To further address limited data, strategies including data augmentation (Ding et al., 2020), few-shot approaches (Hofer et al., 2018; Yang and Katiyar, 2020; Wang et al., 2021), cross-lingual transfer learning (Chaudhary et al., 2018; Zhou et al., 2022), and web-based annotation tools (Tarcar et al., 2020) have been employed.\\n\\n3. KBMC: Korean Bio-Medical Corpus\\n\\n3.1. Data Construction\\n\\nWe use the ChatGPT API to create sentences that include medical terminology such as disease names, bodyparts, and treatments. Given the availability of comprehensive medical domain knowledge and the capabilities of the large language model, we augment the sentences that include medical terminology via responses from gpt-3.5-turbo. The prompts are designed as \u201cCreate a Korean sentence comprising more than 20 words that includes given medical terminology.\u201d All the sentences augmented by ChatGPT undergo thorough review and verification to mitigate the risk of hallucination issues. Medical terms are downloaded from the Korean Standard Terminology of Medicine (KOSTOM). It includes 8th revised terms of Korean Standard Classification of Diseases (KCD) and local terms used in the medical field. To facilitate the annotation process, we develop a pre-annotation algorithm that automatically assigns Named Entity tags as a preliminary step.\\n\\nGiven a set of collected sentences $W = \\\\{w_1, w_2, ..., w_N\\\\}$, we tokenize each sentence using Open-source Korean Text Processor (OKT) so that the input data can be expressed as $\\\\hat{W} = \\\\{x_1, x_2, ..., x_M\\\\}$, where $x_i$ indicates each token. Also, we establish a vocabulary list for three entity types $E = \\\\{\\\\text{Disease}, \\\\text{Body}, \\\\text{Treatment}\\\\}$. Then, for each entity type $e \\\\in E$, we detect a set of spans $S_e = \\\\{s_{jk} | s_{jk} = \\\\{x_j, ..., x_k\\\\} \\\\subset \\\\hat{W}\\\\}$ that matches the vocabulary in the list. Lastly, the algorithm automatically annotates the first token of the span $x_j$ with a B-tag, such as B-Disease, and annotates the rest of the tokens with an I-tag, such as I-Disease. In other words, $\\\\forall e \\\\in E, \\\\forall s_{jk} \\\\in S_e$, the pre-annotation of each token $x_i$ can be described as: $\\\\text{Annotate}(x_i) = (B-e, \\\\text{for } i = j, I-e, \\\\text{for } j < i \\\\leq k)$.\"}"}
{"id": "lrec-2024-main-868", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The distribution of Named Entity labels in two datasets: the original Naver NER dataset (left), and a combined version of the Naver NER dataset (partial) and KBMC (right). The original Naver dataset contains the label TRM, representing medical and IT-related terms. In the combined dataset, sentences that include TRM from the original dataset have been replaced with data from KBMC, aiming to achieve a more accurate classification of medical terms into refined categories.\\n\\nThe annotation process essentially adheres to the standards of the Korean Standard Terminology of Medicine (KOSTOM). However, if there is a disagreement between the annotator and the third annotator (reviewer) regarding the annotation results, the remaining annotators collectively review the mismatched terminology.\\n\\nFigure 1 summarizes the data construction process. Please refer to Appendix A for example sentences of KBMC.\\n\\n3.2. Annotation Result\\n\\nKBMC consists of 6,150 sentences, 153,971 tokens in total. Table 1 displays the label distribution of our dataset. The dataset includes 4,162 distinct disease names, 841 body parts, and 396 treatments.\\n\\nFigure 2 shows the results of the KBMC annotation. We utilize the OKT (Open-source Korean Text) tokenizer for constructing KBMC. While many Korean NER datasets employ word-level annotation, this approach can be problematic for Korean text. Specifically, word-level tokenization often fails to distinguish between nouns and associated postpositional particles, leading to imprecise annotations by attributing a single Named Entity tag to combined terms and particles. Given that Korean is an agglutinative language, tokenizing at the morpheme level is more precise. Thus, unlike conventional Korean NER datasets, we tokenize sentences into morphemes to ensure more accurate annotations.\\n\\n3.3. Data Application\\n\\nFor data augmentation and comparison of NER in general and domain-specific text, the Naver NER dataset is concatenated with KBMC. The Naver NER dataset is a general NER dataset, published by Naver and Changwon University. The Naver NER dataset comprises 90,000 sentences and includes 14 named entities, such as PER (Person), FLD (Field), NUM (Number), DAT (Date), and ORG (Organization). Specifically, the dataset includes annotated named entities labeled as TRM (TERM), which refer to medicine and IT-related terminology. To prevent any potential mismatches when concatenated with our KBMC, we exclude 12,426 sentences containing TRM from the Naver NER dataset. The concatenated version of the Naver NER dataset and KBMC includes 13 general Named Entities and 3 medical Named Entities, totaling 16 Named Entities. The integration of the datasets and their label distribution are demonstrated in Figure 3.\\n\\n4. KBMC\\n\\nIn this section, we compare the performance between the utilization of the Naver dataset, also known as a general NER dataset, and the application of KBMC. Next, to assess the applicability of KBMC, we conduct NER using MedSpaCy.\"}"}
{"id": "lrec-2024-main-868", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Models\\n\\nThe data variation experiment confirms and quantifies the impact of the training data. We evaluate our dataset on six different language models: KM-BERT (Kim et al., 2022), KR-BERT (Lee et al., 2020a), KoBERT, KR-ELECTRA, KoELECTRA v3, and BiLSTM-CRF (Huang et al., 2015). These are advanced Korean NLP models, each with unique architectures and approaches to understanding language. While KR-BERT, KoBERT, KR-ELECTRA, and KoELECTRA v3 leverage transformer-based architectures to achieve state-of-the-art performance on various NLP tasks, BiLSTM-CRF combines bidirectional long short-term memory units with a conditional random field layer, catering to tasks such as NER. Among these models, KM-BERT is a domain-specific language model that has been trained on the Korean medical corpus. Both the learning rate (ranging from 1e-5 to 5e-5) and the batch size (ranging from 32 to 128) are adjusted for optimal performance.\\n\\n### Results\\n\\n#### Medical NER using general dataset\\n\\nWe initially fine-tune six language models using the Naver dataset, which primarily contains general labels. For the experiments, the dataset is split into 90% for training and 10% for testing. All medical entities in this dataset are grouped under one label, TRM. However, this label is not solely for medical terms; it also includes IT-related entities. This generalization makes it difficult to accurately identify and differentiate medical terms since they are consolidated with IT terms under TRM. As a result, the identification of specific medical terminology becomes challenging. Additionally, the F1 score for medical NER using the Naver dataset is below average, as indicated in Table 2.\\n\\n| Model         | Avg.F1(General) | Medical NEs F1 of Medical NER |\\n|---------------|-----------------|-------------------------------|\\n| KM-BERT       | 87.08           | Disease: 98.04 (+22.69)       |\\n|               |                 | Body: 98.13 (+22.78)          |\\n|               |                 | Treatment: 98.53 (+23.18)     |\\n| KR-BERT       | 75.35(Kim et al., 2022) | Disease: 98.04 (+22.78)       |\\n|               |                 | Body: 98.32 (+23.06)          |\\n|               |                 | Treatment: 97.82 (+22.56)     |\\n| KoBERT        | 86.51           | Disease: 98.25 (+20.04)       |\\n|               |                 | Body: 98.22 (+20.01)          |\\n|               |                 | Treatment: 98.18 (+19.97)     |\\n| KR-ELECTRA    |                 |                               |\\n|               | 75.26(Lee et al., 2020b) | Disease: 98.21 (+21.96)       |\\n|               |                 | Body: 98.31 (+22.06)          |\\n|               |                 | Treatment: 98.53 (+22.28)     |\\n| KoELECTRA     |                 |                               |\\n|               | 88.00           | Disease: 98.05 (+21.47)       |\\n|               |                 | Body: 97.72 (+21.14)          |\\n|               |                 | Treatment: 96.56 (+19.98)     |\\n| BiLSTM-CRF    | 55.23           | Disease: 81.44 (+39.21)       |\\n|               |                 | Body: 81.44 (+39.21)          |\\n|               |                 | Treatment: 61.14 (+18.91)     |\\n\\nTable 3: Medical Named Entities and Performance: KBMC applied. The numbers in blue indicate the degree of improvement when compared to the experimental results in Table 2.\\n\\n| Avg.F1 | Precision | Recall |\\n|--------|-----------|--------|\\n| MedSpaCy | 95.69     | 97.02  |\\n\\nTable 4: Performance of MedSpaCy NER using KBMC Medical NER using KBMC\\n\\nWe introduce KBMC to address the shortcomings of the Naver dataset. By combining the Naver NER dataset (excluding sentences with TRM) with KBMC, we achieve a more balanced dataset. The average F1 score in Table 3 encompasses 13 general entities from the Naver dataset (TRM excluded) and 3 medical entities from KBMC. The dataset is divided into 90% for training and 10% for testing. To avoid data imbalance, we maintain consistent proportions of general and medical data in both training and testing phases. KBMC offers precise categorization, separating medical entities from IT-related ones and allowing for detailed classification. This specificity results in a performance increase, with the F1 scores for Disease, Body, and Treatment labels surpassing the TRM label by nearly 20 points. The consistent performance across different models demonstrates the quality and reliability of our dataset.\\n\\n### KBMC Applicability Assessment\\n\\nMedical NER using MedSpaCy\\n\\nIn order to test the utility of KBMC, we also test our dataset using MedSpaCy. Eyre et al. (2021) have released a library of tools for clinical NLP and text processing with SpaCy. We apply MedSpaCy on the Korean dataset by using ko_core_news_md.\"}"}
{"id": "lrec-2024-main-868", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As shown in Table 4, our KBMC dataset demonstrates remarkable performance on a clinical text processing toolkit in Python as well. While MedSpaCy may not be primed for general entity recognition, it excels in identifying medical terms, especially when enhanced with KBMC.\\n\\n5. Conclusion\\nIn our research, we introduce KBMC, the first open-source biomedical NER dataset tailored for the Korean language. KBMC provides a training ground for language models to detect and categorize medical Named Entities, addressing the issue of data scarcity in this domain.\\n\\nWe evaluate the utility of the KBMC dataset in two scenarios: one using only a pre-existing general NER dataset, and another incorporating the KBMC dataset. The inclusion of KBMC resulted in enhanced predictions for medical Named Entities and an elevated overall F1 score, which averages the F1 scores for both general and medical entities. With KBMC, models can recognize a broader spectrum of medical terms. Notably, when paired with MedSpaCy, a Python toolkit designed for clinical NLP, our dataset showcases impressive results.\\n\\nWe anticipate that our KBMC dataset will contribute substantially to ongoing research in the field of medical NLP.\\n\\nLimitations\\nThe primary challenge arises from the limited availability of Korean medical data, which makes it difficult to develop a comprehensive corpus. Due to this constraint, we were unable to manually create a labeled dataset for downstream tasks other than NER task. As a result, an important avenue for future research lies in the construction of a more expansive and diverse Korean medical corpus to facilitate the development of other downstream tasks, such as question-answering (QA). Moreover, while our intention was to compare different general NER datasets in terms of medical entity extraction, The Naver dataset was the only available Korean NER dataset that provided annotations for medical terminology. This kind of problem also occurred in terms of domain-specific models as well. KM-BERT was the only medical language model available for our testing. This limited access to resources restricted our capacity for a comprehensive comparison.\\n\\nEthics Statement\\nUsing our KBMC dataset enables precise identification of entity categories. When implemented in the medical sphere, our dataset and model can assist in de-identifying personal details of patients. In the realm of medical NLP, transferring and accessing data is challenging due to the presence of sensitive content. To address these privacy and data sensitivity issues, integrating medical NER into real-world medical institutions offers a safeguarded approach. Resolving these challenges sets the stage for a flourishing future in NLP research, spanning areas such as the medical and legal fields.\\n\\nRosario Catelli, Francesco Gargiulo, Valentina Casola, Giuseppe De Pietro, Hamido Fujita, and Massimo Esposito. 2021. A novel covid-19 data set and an effective deep learning approach for the de-identification of Italian medical records. IEEE Access, PP:1\u20131.\\n\\nAditi Chaudhary, Chunting Zhou, Lori Levin, Graham Neubig, David R. Mortensen, and Jaime G. Carbonell. 2018. Adapting word embeddings to new languages with morphological and phonological subword representations. Ming Cheng, Shufeng Xiong, Fei Li, Pan Liang, and Jianbo Gao. 2021. Multi-task learning for Chinese clinical named entity recognition with external knowledge. BMC Medical Informatics and Decision Making, 21.\\n\\nHyejin Cho and Hyunju Lee. 2019. Biomedical named entity recognition using deep neural networks with contextual information. BMC Bioinformatics, 20.\\n\\nBosheng Ding, Linlin Liu, Lidong Bing, Canasai Kruengkrai, Thien Hai Nguyen, Shafiq Joty, Luo Si, and Chunyan Miao. 2020. Daga: Data augmentation with a generation approach for low-resource tagging tasks. Rezarta Dogan, Robert Leaman, and Zhiyong Lu. 2014. NCBI disease corpus: A resource for disease name recognition and concept normalization. Journal of biomedical informatics, 47.\\n\\nHannah Eyre, Alec B Chapman, Kelly S Peterson, Jianlin Shi, Patrick R Alba, Makoto M Jones, Tamara L Box, Scott L DuVall, and Olga V Patterson. 2021. Launching into clinical space with medspacy: A new clinical text processing toolkit in Python. Sam Henry, Kevin Buchan, Michele Filannino, Amber Stubbs, and Ozlem Uzuner. 2019. 2018n2c2 shared task on adverse drug events and medication extraction in electronic health records. Journal of the American Medical Informatics Association, 27(1):3\u201312.\"}"}
{"id": "lrec-2024-main-868", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maximilian Hofer, Andrey Kormilitzin, Paul Goldberg, and Alejo J. Nevado-Holgado. 2018. Few-shot learning for named entity recognition in medical text. CoRR, abs/1811.05468.\\n\\nZhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional LSTM-CRF models for sequence tagging.\\n\\nYoojoong Kim, Jeong Lee, Moon Jang, Yun Yum, Seongtae Kim, Unsub Shin, Young-Min Kim, Hyung Joo, and Sanghoun Song. 2022. A pretrained BERT for Korean medical natural language processing. Scientific Reports, 12:13847.\\n\\nVeysel Kocaman and David Talby. 2022. Accurate clinical and biomedical named entity recognition at scale. Software Impacts, 13:100373.\\n\\nJinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang. 2019. BioBERT: a pretrained biomedical language representation model for biomedical text mining. Bioinformatics, 36(4):1234\u20131240.\\n\\nSangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, and Hyopil Shin. 2020a. KR-BERT: A small-scale Korean-specific language model.\\n\\nSangah Lee, Hansol Jang, Yunmee Baik, Suzi Park, and Hyopil Shin. 2020b. KR-BERT: A small-scale Korean-specific language model.\\n\\nSangah Lee and Hyopil Shin. 2022. KR-ELECTRA: a Korean-based ELECTRA model. https://github.com/snunlp/KR-ELECTRA.\\n\\nJiao Li, Yueping Sun, Robin Johnson, Daniela Sciaky, Chih-Hsuan Wei, Robert Leaman, Allan PETER Davis, Carolyn Mattingly, Thomas Wiegers, and Zhiyong Lu. 2016. Biocreative V CDR task corpus: a resource for chemical disease relation extraction. Database, 2016:baw068.\\n\\nZengjian Liu, Ming Yang, Xiaolong Wang, Qingcai Chen, Buzhou Tang, Zhe Wang, and Wang Qi. 2017. Entity recognition from clinical texts via recurrent neural network. BMC Medical Informatics and Decision Making, 17.\\n\\nChen Lyu, Bo Chen, Yafeng Ren, and Donghong Ji. 2017. Long short-term memory RNN for biomedical named entity recognition. BMC Bioinformatics, 18.\\n\\nLucas Emanuel Silva Oliveira, Ana Carolina PETERS, Adalniza Moura Puccada Silva, Caroline Pilatti Gebeluca, Yohan Bonescki Gumiel, Lilian Mie Mukai Cintho, Deborah Ribeiro Carvalho, Sadid Al Hasan, and Claudia Maria Cabral Moro. 2022. SemClinBr - a multi-institutional and multi-specialty semantically annotated corpus for Portuguese clinical NLP tasks. Journal of Biomedical Semantics, 13(1).\\n\\nCole Pearson, Naeem Seliya, and Rushit Dave. 2021. Named entity recognition in unstructured medical text documents.\\n\\nYifan Peng, Shankai Yan, and Zhiyong Lu. 2019. Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMO on ten benchmarking datasets. CoRR, abs/1906.05474.\\n\\nAmogh Kamat Tarcar, Aashis Tiwari, Vineet Naique Dhaimodker, Penjo Rebelo, Rahul Desai, and Dattaraj Rao. 2020. HealthcareNER models using language model pretraining.\\n\\n\u00d6zlem Uzuner, Brett R South, Shuying Shen, and Scott L DuVall. 2011. 2010 I2B2/va challenge on concepts, assertions, and relations in clinical text. Journal of the American Medical Informatics Association, 18(5):552\u2013556.\\n\\nYaqing Wang, Haoda Chu, Chao Zhang, and Jing Gao. 2021. Learning from language description: Low-shot named entity recognition via decomposed framework.\\n\\nLeon Weber, Mario S\u00e4nger, Jannes M\u00fcnchmeyer, Maryam Habibi, Ulf Leser, and Alan Akbik. 2021. HunFlair: an easy-to-use tool for state-of-the-art biomedical named entity recognition. Bioinformatics, 37(17):2792\u20132794.\\n\\nYi Yang and Arzoo Katiyar. 2020. Simple and effective few-shot named entity recognition with structured nearest neighbor learning.\\n\\nRan Zhou, Xin Li, Lidong Bing, Erik Cambria, Luo Si, and Chunyan Miao. 2022. Conner: Consistency training for cross-lingual named entity recognition.\"}"}
{"id": "lrec-2024-main-868", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A. KBMC sentences\\n\\nSystemic myasthenia is a condition in which the whole body loses strength, making daily life difficult, accompanied by muscle pain and a sense of lethargy.\\n\\nPancreatic cancer refers to a tumor (a lump of tumor) made up of cancer cells that form in the pancreas.\\n\\nSuch diseases lead to symptoms such as respiratory distress, coughing, asthma attacks, etc., caused by decreased lung function, greatly affecting daily life.\\n\\nBurkitt lymphoma is a malignant tumor that originates in the lymph nodes. Early detection and treatment are crucial, and various treatment methods, such as chemotherapy and radiation therapy, exist.\\n\\nTable 5: Examples of the KBMC dataset\"}"}
