{"id": "emnlp-2024-main-969", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ClimRetrieve: A Benchmarking Dataset for Information Retrieval from Corporate Climate Disclosures\\nTobias Schimanski1, Jingwei Ni2, Roberto Spacey3, Nicola Ranger3, Markus Leippold1, 4\\n\\n1 University of Zurich 2 ETH Zurich 3 University of Oxford 4 Swiss Finance Institute (SFI)\\ntobias.schimanski@df.uzh.ch\\n\\nAbstract\\nTo handle the vast amounts of qualitative data produced in corporate climate communication, stakeholders increasingly rely on Retrieval Augmented Generation (RAG) systems. However, a significant gap remains in evaluating domain-specific information retrieval \u2013 the basis for answer generation. To address this challenge, this work simulates the typical tasks of a sustainability analyst by examining 30 sustainability reports with 16 detailed climate-related questions. As a result, we obtain a dataset with over 8.5K unique question-source-answer pairs labeled by different levels of relevance. Furthermore, we develop a use case with the dataset to investigate the integration of expert knowledge into information retrieval with embeddings. Although we show that incorporating expert knowledge works, we also outline the critical limitations of embeddings in knowledge-intensive downstream domains like climate change communication.\\n\\n1 Introduction\\nMotivation. Climate change presents the most pressing challenge of our time. The underlying concepts and challenges generate a wealth of information with inherent complexity and interconnectedness. At the same time, most of the data on corporate climate disclosure is qualitative \u2013 hidden in textual statements (Weber and Baisch, 2023; Commission, 2024). Qualitative disclosures typically include narrative descriptions of climate-related risks, opportunities, strategies, and governance. These are crucial to understanding how a company perceives and manages climate-related issues and their potential impacts on business operations.\\n\\nAll the data and code for this project is available on https://github.com/tobischimanski/ClimRetrieve.\\n\\nWe thank the expert annotators Aysha Emmerson, Emily Hsu, and Capucine Le Meur for their work on this project.\\n\\nFor example, companies must describe the processes they use to identify, assess, and manage these risks and opportunities, as well as the roles of the board and management in these processes. For a recent solution approach, see Ni et al., 2024.\\n\\n![Figure 1: Overview of the core columns of ClimRetrieve.](image-url)\"}"}
{"id": "emnlp-2024-main-969", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"experiment to compare human expert annotations with various embedding search strategies. This investigation aims to understand how to integrate expert knowledge into the retrieval process.\\n\\nResults.\\n\\nWe find that SOTA embedding models (on which RAG systems heavily rely) usually fail to effectively reflect domain expertise. This shows that bringing expert knowledge into the retrieval process is a non-trivial task. Thus, we underline the importance of new approaches in information retrieval. This dataset can present a basis for improvement approaches.\\n\\nImplications.\\n\\nThe implications of our study are significant for both practice and research. Knowledge-intensive downstream domains like climate change are nuanced, and details matter. This paper can significantly help researchers evaluate new RAG systems (e.g., Ni et al., 2023) and corporate climate report analysts to obtain useful information for decision-making.\\n\\n2 Background\\n\\nRetrieval Augmented Generation (RAG).\\n\\nRAG has been widely adopted to mitigate hallucination and enhance application performance (Vaghefi et al., 2023; Ni et al., 2023; Colesanti Senni et al., 2024). RAG systems base their answers on external information integrated into the prompt rather than parametric knowledge learned during training (Lewis et al., 2020). This approach critically shifts the problem from learning the information during training to retrieving the right information and summarizing and arguing over the provided content. Many related projects explore how to evaluate the quality of LLM generation augmented with retrieval (Zhang et al., 2024; Saad-Falcon et al., 2024; Asai et al., 2023; Schimanski et al., 2024a). However, how to directly assess the information retrieval thoroughness and precision is still underexplored, especially for specific but important domains like corporate climate disclosure. The only work to date that tries to integrate domain-specific nuances explicitly is Ni et al., 2024.\\n\\nClimate Change NLP.\\n\\nPrior work, specifically before the popularisation of RAG, has mainly worked with BERT-based classifiers to address climate change questions. This ranges from the verification of environmental claims (Stammbach et al., 2023), the detection of climate change topics (Varini et al., 2021), the verification of facts (Diggelmann et al., 2021; Leippold et al., 2024), the detection of net zero and reduction targets (Schimanski et al., 2023) or more generally environmental, social and governance texts (Schimanski et al., 2024b). Although this provided valuable information on communication patterns, for example, in corporate reporting (Bingler et al., 2024; K\u00f6lbel et al., 2022), fine-granular, nuanced reasoning analyses were only enabled after the popularization of RAG (Ni et al., 2023; Colesanti Senni et al., 2024). Recently, Builian et al. (2023) developed a comprehensive evaluation framework based on science communication principles to assess the performance of LLMs in generating climate-related information.\\n\\n3 Data\\n\\nThis project constructs a dataset comprising authentic questions, sources, and answers to benchmark information retrieval in RAG systems in the use case of corporate climate disclosures. In this process, we simulate an analyst question-answering process based on documents. The dataset creation involves an iterative question definition and report span labeling process (see Figure 2). It starts with 16 Yes/No questions about climate change. The questions are inspired by the guidance of Bernhofen and Ranger (2023) and analyze companies' climate change adaptation. Thus, the question asks for details simulating an analyst's point of view on a company (see Appendix C). These questions are distributed among three expert annotators (see Appendix D). For each question, an annotator creates a definition and concepts of the information sought in the question. Then, both are discussed in the expert group. This step is crucial to understanding the question in detail (see Appendix B for details on the question definition and concepts).\\n\\nIn the next step, the expert annotators create the dataset using a specific sustainability report. Annotators search for relevant information in the report and annotate the sources from various perspectives. In this way, they replicate an analyst workflow in which the task is to read the document and search for relevant information to answer the question and rate its relevancy. Then, they answer the question based on the information. Ultimately, they create a dataset containing the following columns:\\n\\n1. **Document**: Report under investigation.\\n2. **Question**: Question under investigation.\\n3. **Relevant**: Full-sentence form question-relevant information.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. **Context**: Context of the question relevant information (extending the relevant information by a sentence before and afterward).\\n\\n5. **Page**: Page of the relevant information.\\n\\n6. **Source From**: Answers whether the relevant information is from text, table or graph.\\n\\n7. **Source Relevance Score**: Classifies from 1-3 how relevant the information is for answering the question (see Appendix E for details on the relevance classification).\\n\\n8. **Unsure Flag**: Flag whether it is unclear if this source is question-relevant.\\n\\n9. **Addressed directly**: Flag whether the relevant information addresses the question directly or indirectly.\\n\\n10. **Answer**: Answer to the question based on all retrieved relevant information.\\n\\nAfter each report, the expert annotators have the option to discuss the question definitions and concepts with the expert group and retrofit them to the dataset. This allows for an iterative refinement of the nuances of question understanding.\\n\\nThis process is repeated for 30 sustainability reports. As a result, we obtain a base dataset with 743 entries of relevant question-source-answer pairs (see Appendix F for details). Furthermore, we can create a report-level dataset since we know which parts of the report are relevant. In this dataset, we split the reports into paragraphs of equal length and mark relevant vs. nonrelevant parts with the question-source-answer pairs. This results in a dataset with 8,628 paragraphs labeled with the question's relevance. Since the questions are in semantic proximity, one paragraph can be relevant to multiple questions. For this reason, we ultimately create a dataset that contains unique report-paragraph-question pairs. For each question, the whole report is labeled. Thus, a report's paragraphs are repeated for each question to create an easy-to-assess dataset. In this way, we obtain a large report-level dataset with 43,445 entries (for details, see Appendix G).\\n\\n### Investigating Embedding Search\\n\\nWe construct a specific use case to demonstrate the report-level dataset's practical applicability. Given the scarcity of research on information retrieval specific to climate-related corporate disclosures, this use case study is concentrated on this particular area.\\n\\nWithin the framework of a basic RAG model, inquiries posed to the document are utilized to identify pertinent paragraphs. This information retrieval typically follows a two-step process. First, embedding models are used to create a vector representation of the questions and all the paragraphs in the report. Second, the question vector is compared to all paragraph vectors to obtain the top k most similar paragraphs. However, as previous research has shown, LLMs are prone to be confused when presented with wrong or contradictory sources (Cuconasu et al., 2024; Watson and Cho, 2024; Schimanski et al., 2024a), and the relevancy of the question to the sources plays a significant role (Niu et al., 2024). Thus, the retrieval process is central to creating the true output.\\n\\nAs previously outlined, climate change is a complex downstream domain with knowledge-intensive questions (see Section 2 and Appendix B). An expert labeler will likely consider additional concepts and definitions when searching for relevant information in reports. Thus, only using the question in the embedding search process might limit the results to semantically similar paragraphs to the question, not to all concepts embedded in the expert annotator's mind.\\n\\nTherefore, we construct an experiment that gradually replaces question in the top-k search process with longer and more expert-informed question explanations. To obtain question explanations, we use two setups. First, we use the definitions and concepts the labelers used during their annotation (see Appendix B for an example). Second, we make use of the capabilities of the closed-source LLM GPT-4. We proceed in two steps. In the first step, we ask the model to create...\"}"}
{"id": "emnlp-2024-main-969", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Results for the F1-score of the different embedding models and information retrieval approaches (question, definition, concepts, generic, inf_3, inf_all) aggregated across all top-k values (5, 10, 15). The best-performing information retrieval strategy (in bold) is the expert-informed explanations.\\n\\nWe analyze all setups individually as well as aggregate the scores over all embeddings or top-k values.\\n\\nOur first step is to compare the questions in the retrieval process with the definitions and concepts written by the annotators. As Table 1 indicates, replacing the question with these definitions rather decreases the performance (see Appendix J for more reinforcing results).\\n\\nThis trend changes when using example-informed question explanations. As Table 1 shows generally, and Figure 3 illustrates for just contemplating the results of text-embedding3-large, using these explanations can improve retrieval. The higher the top-k value, the more relevant sources are found in the retrieved ones. Also, the higher the top-k value, the less relevant sources are found relative to K (see Figure 3). Beyond these obvious insights, these results entail three major findings. First, using an example-informed, that is, an expert-informed explanation, improves the retrieval in contrast to using the definitions and concepts of the labelers. This probably originates from the fact that the example-inspired explanations offer greater detail tailored for the retrieval instead of capturing general concepts (see the appendix J for comparison). Second, the most promising strategy for optimizing the embedding search is using expert-informed definitions that exclude the question. This is an interesting finding, indicating that the concept behind the questions seems to be more targeted for search than the question itself (results hold on aggregated scores, see Appendix K). Third, in light of the challenges around source quality and...\"}"}
{"id": "emnlp-2024-main-969", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recall@K\\nPrecision@K\\n\\nFigure 3: Results for the different experimental setups\\n(Embeddings = \u201ctext-embedding-3-large\u201d).\\n\\nThere is a need to improve efficient information retrieval processes. Although embeddings and using definitions certainly present a good first pathway, improvement in the nuance of question-source relevance beyond a fixed top-k number could improve the ultimate results. All these insights are consistently confirmed when considering different analysis metrics, embeddings, and relevance thresholds (see Appendix L for these investigations).\\n\\n5 Conclusion\\n\\nIn this work, we develop a unique dataset that simulates an expert analyst workflow to evaluate RAG systems. We show its utility by analyzing the dominant embedding retrieval strategy with different search setups. We find that embeddings face major limitations in information retrieval for knowledge-intensive tasks. Therefore, this work sets the path for including and evaluating the improvement of expert-integrated information retrieval for RAG systems (see Ni et al., 2024 for a potential solution approach).\\n\\nLimitations\\n\\nAs with every work, our work has limitations. The first limitation comes from the expert workflow that we are using. Previous work has shown that experts face selection bias when annotating for information retrieval tasks (Thakur et al., 2021). This means that we certainly know that the source is relevant once labeled, but we do not know whether the source is irrelevant if not labeled. This likely means our results represent a lower bound rather than an absolute truth.\\n\\nSecond, as mentioned in creating the example-informed definitions, we intentionally allowed data leakage between the set to inspire the explanations and the test set. However, we argue that a real-world expert would act similarly when designing the explanations based on her previous experience.\\n\\nEthics Statement\\n\\nHuman Annotation: In this work, all human annotators are Graduate or Doctorate researchers who have good knowledge about scientific communication and entailment. They are officially hired and have full knowledge of the context and utility of the collected data. We adhered strictly to ethical guidelines, respecting the dignity, rights, safety, and well-being of all participants.\\n\\nData Privacy or Bias: There are no data privacy issues or biases against certain demographics with regard to the data collected from real-world applications and LLM generations. All artifacts we use are under a Creative Commons license. We also notice no ethical risks associated with this work.\\n\\nReproducibility Statement: To ensure full reproducibility, we will disclose all codes and data used in this project, as well as the LLM generations, GPT-4, and human annotations. For OpenAI models, we use \u201cgpt-4-0125-preview\u201d We always fix the temperature to 0 when using APIs.\\n\\nAcknowledgements\\n\\nThis paper has received funding from the Swiss National Science Foundation (SNSF) under the project \u2018How sustainable is sustainable finance? Impact evaluation and automated greenwashing detection\u2019 (Grant Agreement No. 100018_207800).\\n\\nReferences\\n\\nAkari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. 2023. Self-rag: Learning to retrieve, generate, and critique through self-reflection. Preprint, arXiv:2310.11511.\\n\\nMark Bernhofen and Nicola Ranger. 2023. Aligning finance with adaptation and resilience goals: Targets and metrics for financial institutions. Technical report, University of Oxford, UK Center for Greening Finance & Investment, Global Resilience Index Initiative.\\n\\nJulia Anna Bingler, Mathias Kraus, Markus Leippold, and Nicolas Webersinke. 2024. How cheap talk in climate disclosures relates to climate initiatives, corporate emissions, and reputation risk. Journal of Banking & Finance, 164:107191.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-969", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A\\n\\nComplexity of Knowledge-Intensive Questions\\n\\nKnowledge-intensive domains like climate change have knowledge-intensive questions. Consider, i.e., the following question: \\\"What are the company's emissions for the previous year?\\\" While emissions serve as a fundamental indicator of a company's environmental impact, the associated complexities are profound. Emissions can be stratified into various categories, including carbon dioxide (CO2), methane (CH4), among others. Moreover, it is increasingly critical to distinguish between direct, indirect, and supply chain emissions, both upstream and downstream (Scope 1-3). This example underscores the extensive complexity that must be integrated into the analysis of ostensibly straightforward questions.\\n\\nB Definitions and Concepts\\n\\nFor knowledge-intensive domains like climate change, it is of central importance to obtain the right question understanding. As demonstrated with the emission example in Appendix A, simple questions can unfold a large underlying mass of concepts. Generally, two differentiations are used in this work. When an expert reads a question, she might have two things in mind: definitions and concepts. On the one hand, definitions constitute the elucidation of the terminologies referenced within the query. For example, when inquiring about emissions, one might interpret them as the gases the company generates during its value-creation processes. This interpretation is inherently complex and varies significantly among experts and specific use cases.\\n\\nConversely, concepts pertain to the interconnected themes associated with the questions. We can distinguish between two types of concepts. First, core concepts are intrinsically linked to the query and exhibit substantial overlap with definitions. For example, in the question \\\"What are the emissions of the company in the last year?\\\", the term \\\"emissions\\\" constitutes a core concept. However, the phrase \\\"last year\\\" introduces potential ambiguity if not explicitly defined\u2014whether it refers to a reporting year or a calendar year. Second, lateral concepts represent broader, knowledge-graph-like connections. For instance, in the context of emissions, a lateral concept might encompass climate change. An expert's interpretation of the lateral concepts in the question \\\"What are the emissions of the company in the last year?\\\" could extend to inquiries regarding climate change mitigation.\\n\\nGiven these considerations, it is imperative to elucidate both definitions and concepts when seeking information and formulating responses. These concepts and definitions could manifest entirely differently depending from person to person. For this dataset, the important thing is that the question sources, answers, definitions, and concepts are consistent with itself. Table B.1 gives an example of a definition and concepts for the question \\\"Do the environmental/sustainability targets set by the company reference external climate change adaptation goals/targets?\\\".\\n\\nC Questions\\n\\nTable C.2 displays the questions the expert annotators answered for the reports. The focus lies on climate change adaptation and the resilience of companies. Thus, the questions are detailed and specific. The questions were created based on the guidance by Bernhofen and Ranger (2023). Furthermore, all questions are designed to be answerable with Yes or No and a free text explanation. This offers a nuanced level of detail in the potential analyses. In this project, we focus on the retrieved sources and not on the answers because retrieval is much less researched and the source dataset offers a richer amount of analysis potential.\\n\\nAll data and code is open-source under https://github.com/tobischimanski/ClimRetrieve.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do the environmental/sustainability targets set by the company reference external climate change adaptation goals/targets?\\n\\nExternal climate change adaptation goals or targets include national, regional or sectoral adaptation plans set either by government, industry bodies, standard setters, or international organisations such as the United Nations, the World Bank or others. The external targets must be provided.\\n\\n1. **Core**  **Reducing Greenhouse Gas Emissions**: Setting targets to decrease emissions of carbon dioxide (CO2), methane (CH4), and other greenhouse gases to mitigate climate change.\\n\\n2. **Core**  **Increasing Renewable Energy Usage**: Establishing goals to increase the percentage of energy generated from renewable sources such as solar, wind, hydroelectric, and geothermal power.\\n\\n3. **Latent**  **Conservation of Biodiversity**: Setting targets to preserve and protect natural habitats, endangered species, and ecosystems to maintain biodiversity.\\n\\n4. **Latent**  **Reducing Waste and Promoting Recycling**: Implementing measures to minimize waste generation, increase recycling rates, and promote a circular economy.\\n\\n5. **Latent**  **Water Management and Conservation**: Developing strategies to manage water resources more efficiently, such as investing in water-saving technologies, implementing rainwater harvesting systems, and improving water storage and distribution infrastructure to cope with changing precipitation patterns and droughts.\\n\\n6. **Core**  **Building Climate-Resilient Infrastructure**: Integrating climate resilience into infrastructure planning and design, including constructing buildings and roads that can withstand extreme weather events, improving drainage systems to manage flooding, and upgrading energy and transportation networks to reduce vulnerability to climate impacts.\\n\\n7. **Core**  **Enhancing Disaster Preparedness and Response**: Developing early warning systems, emergency response plans, and community resilience programs to prepare for and respond to natural disasters such as hurricanes, floods, wildfires, and heatwaves.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Does the company have a specific process in place to identify risks arising from climate change?\\n2. Does the company report the methodology used to identify the dependencies and impact of its business activities on the environment?\\n3. Does the company refer to any third party scenarios when identifying climate-related risks or opportunities (e.g. IPCC trajectories, NGFS scenarios, etc.)?\\n4. Does the company encourage downstream partners to carry out climate-related risk assessments?\\n5. Does the company report how adjustments to its business operations will allow it to adapt to climate change?\\n6. Does the company provide definitions for climate change adaptation?\\n7. Has the company identified any synergies between its climate change adaptation goals and other business goals?\\n8. Does the company report the climate change scenarios used to test the resilience of its business strategy?\\n9. Does the company seek to adjust its business model to better provide climate change adaptation products and services?\\n10. Does the company have any engagements with industry peers in relation to climate change?\\n11. Do the environmental/sustainability targets set by the company reference external climate change adaptation goals/targets?\\n12. Do the environmental/sustainability targets set by the company align with external climate change adaptation goals/targets?\\n13. Does the company report short-term actions taken or planned to reduce its waste generation?\\n14. Does the company report a plan to engage with downstream partners on water consumption or water pollution?\\n15. Does the company identify any impacts of its business activities on the environment?\\n16. Does the company have a strategy on waste management?\"}"}
{"id": "emnlp-2024-main-969", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D Expert Annotators and Expert Group\\n\\nThe three annotators involved in this study hold an undergraduate degree with a minor or major focus in the climate domain. All annotators have at least one year of professional experience in the field. During the process of labeling, all annotators are enrolled in a master's program with a focus in the sustainability or climate domain at the University of Oxford.\\n\\nThe expert group in this project is composed of the three expert annotators, two junior and one senior researcher in the domain. The expert group collectively defined questions, discussed definitions and concepts for the questions and was involved in the iterative refinement of the dataset.\\n\\nE Relevance Labels of the Dataset\\n\\nFor answering a question, texts of different relevance can be in a report. To reflect this fact, we introduce three relevance labels where 1 is partially relevant, 2 is relevant, and 3 is highly relevant. This means, there is a clear difference between 2 and 3 being certainly relevant and 1 where the labeler might be unsure about relevance or can only identify indirect relevance. However, this also means that experiments using the final dataset may want to reflect the fact that a paragraph with label 1 differs from those with labels 2 and 3.\\n\\nF Relevant Question-Source-Answer Pairs\\n\\nThe core result of the labeling process is 743 question-source-answer pairs with the 16 questions under consideration. For each question, sources are searched, labeled by relevance and the other categories (see Section 3), and finally answered. The questions are split amongst the annotators so that two annotators label 5 questions per report and one annotator labels 6 questions per report. As Table F.3 shows, there is a discrepancy in how many question-source-answer pairs per question exist in the dataset. The determining factor for this variance is the number of sources found per question. While more sources can be found for more general questions like \"\"Does the company have a specific process in place to identify risks arising from climate change?\"\" (66 sources found across the dataset), detailed questions like \"\"Does the company provide definitions for climate change adaptation?\"\" are less often answered through the reports (6 sources found across the dataset). Thus, the dataset also contains questions where no sources were found. After labeling, we arrive at a dataset containing majorly relevant question-source-answer. As Figure F.1 shows, the majority of the relevant question-source-answer pairs are indeed very relevant (relevance label 3). This speaks for the nature of the analyst workflow employed in this work where an analyst will likely search for the most relevant bits of information to answer the question.\\n\\nG Report-Level Dataset\\n\\nTo obtain a report-level dataset of relevant vs. non-relevant paragraphs, we use the LLamaIndex SentenceSplitter function. This function allows the splitting of a document around a fixed length but tries to ensure the full-sentence form of the paragraphs. We specify the paragraph length to be around 350 words, while we allow for an overlap in paragraphs of 50 words. The overlap should prevent the loss of context through random cut-offs. This results in obtaining a dataset with 8628 paragraphs from the 30 reports. Once we obtain the paragraphs, we use our dataset with relevant question-source-answer pairs to assign a label to the whole set of paragraphs. Since the annotated dataset contains relevant sentences, we deem a paragraph relevant once it contains one of the sentences of the relevant text parts. The retrieved paragraphs from the reports sometimes have minor differences from the ones in the dataset, e.g. different spacing or headlines are in-\\n\\n6See https://docs.llamaindex.ai/en/stable/api_reference/node_parsers/sentence_splitter/ for more details.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table F.3: Descriptive statistics of the question-source-answer pairs per question.\\n\\nWe use the difflab SequenceMatcher function to compare the similarity of sentences. We use a similarity threshold of 0.9 for matching. This is oriented on experimentation with examples. However, the majority of the samples are clearly matchable with this threshold. Figure G.2 shows the similarities between the most similar relevant text part from the question-source-answer pairs with the paragraphs from the report-level dataset. It becomes apparent that the paragraphs are either extremely similar to the sources (i.e., it\u2019s a match) or very dissimilar indicating that there is indeed no match found.\\n\\nSince we want to obtain a dataset where every paragraph obtains a relevance score toward a question, we have to repeat the matching for each question that was answered for the report. Thus, we obtain a dataset with 43,445 entries from the original 8,628. These paragraphs now can appear multiple times with multiple questions. In its essence, the final report-level dataset contains pairs of paragraphs with questions. For each question, a relevance label is given between 0 (no relevance) and 1-3 (labeled as relevant by annotators). If the paragraph is relevant, we also give the relevant text part with which it was matched.\\n\\nWe fail to match the entire 743 question-source-answer pairs with the report-level dataset. This originates from problems with the chunking of the reports (e.g., not every paragraph is parsed correctly), issues when matching (e.g., the string was formatted differently and the threshold was not low enough), or the fact the information is retrieved from graphs or tables where the string matching doesn\u2019t work either. Finally, the report-level dataset contains 595 paragraphs with question-relevant information. Some paragraphs are relevant for multiple questions. The number of relevant unique paragraphs is 446 (within the 8,628).\\n\\nInformation Retrieval Explanation\\nTo replace the questions in the information retrieval process with definitions, we create generic and example-inspired explanations. Generic explanations simply take the question and create an explanation with the embedded knowledge of GPT-4 (the gpt-4-0125-preview checkpoint is used for all generations). We differentiate between explanations with the question (see Prompt H.3) and without question (see Prompt H.4). This serves as a non-informed base case. To inform the question with actually relevant content, we make use of the already labeled relevant paragraphs and ask the model to abstract from these examples to create informed explanations. We again create an explanation with and without question (see Prompt H.5 and Prompt H.6). In the labeled dataset, the sources\u2019 relevance is differentiated from 1 (loosely relevant) to 3 (highly relevant). In order to ensure that only specific information informs the explanation creation process, we only consider sources of relevance 2 and higher as examples. We create explanations of different lengths (60 and 150 words) and with and without the questions. To illustrate these explanations, refer to Table H.4 with examples of length 60 without question and Table H.5 with examples of length 150 with the question.\\n\\nWhile the beginning of the query remains the same, the longer queries might have different shapes in...\"}"}
{"id": "emnlp-2024-main-969", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I Details on the Experimental Setup\\n\\nFollowing the Information Retrieval Explanation (see Appendix H), we also choose to set a relevance threshold for the base setup of our evaluation. For the base evaluation, the threshold is 2 or higher. Again, we argue that for the binary label at hand (relevant or not), the label of relevance 1 might be confusing since in its definition it is not entirely clear whether the source is really relevant. Thus, future investigations should focus on determining uncertainty around relevance labeling.\\n\\nFurthermore, in the base setup, we use a random baseline, BM25, DRAGON+, GTE-base, ColBERTv2, OpenAI's text-embedding-ada-002, text-embedding3-small, and text-embedding3-large to embed questions, definitions, and paragraphs. We aggregate the results over all embeddings or top-k values to compare scenarios. Furthermore, we use the text-embedding3-large to showcase single aspects.\\n\\nComparing Retrieval with Questions, Definitions and Concepts vs. Explanations\\n\\nTable J.6 shows the results of comparing the retrieval with text-embedding-3-large with questions, definitions, and concepts along all metrics and top k values. It becomes apparent that using the sole question for information retrieval is the best. This might raise the question of whether the definition and concepts are wrong. However, we argue that the definition and concepts work worse for two reasons. First, the definitions and concepts are an aid for the individual labeler to remain consistent with herself. This means the labeler might not explicitly state exact details in the definitions or concepts. The real labeling knowledge may remain with the expert. This is also highly interconnected with the second reason. Neither the definitions nor the concepts were optimized for the search with embeddings. The labeler has a high degree of freedom regarding how long the definitions or concepts are.\\n\\nIn contrast, the generic and expert-informed explanations are the result of a thought concept to optimize embedding search. As Tables H.4 and H.5 show, these explanations offer dense mentioning of targeted contents relating to the question. They have a higher level of specificity when compared to the example definition and concepts in Table J.6. We argue that this is also the reason why using an example-informed, that is, an expert-informed explanation, improves the retrieval in contrast to using the definitions and concepts of the labelers (see 3). This is also reinforced by comparing the generic definition with the informed explanations. Interestingly, a small nuance becomes apparent when comparing inf$^3$ and inf$^all$. There seems to be no significant jump in performance when letting the definition be inspired by three vs. all reports' relevant sources as examples. This indicates that (1) designing the definitions based on a limited sample is enough and (2) there might even be an overfitting in only orientating on examples. We argue the level of detail of the explanations can serve as a good basis for future definitions and concepts enabling an iterative expert-machine-integrated process. This could ultimately aim to provoke the human to be more precise and reflect with the machine.\\n\\nK Aggregated Results for Question vs. No Question\\n\\nAs Table K.7 shows, the most promising strategy remains expert-informed explanations that exclude the question across all settings. This observation is consistent with the single observation with text-embedding-3-large.\\n\\nL All Results with Metrics, Embeddings and Relevance Thresholds for text-embedding-3-large\\n\\nTo solidify the results of our experiments, we employ a set of different metrics. In this section, we show the results for the strongest embedding model, text-embedding-3-large. While the results in Figures L.8 and L.9 confirm the results in Figure 3, they add one dimension of nuance. The results indicate that a higher top k value is optimal because more annotated sources are found. However, it also comes with the downside of more irrelevant sources as well. These results again indicate that more nuanced relevant labels abstracting from fixed thresholds might be optimal.\\n\\nFurthermore, it is interesting to see how the results change when changing the underlying embedding model. Thus, we also change the embedding model from \\\"text-embedding-3-large\\\" to \\\"text-embedding-3-small\\\".\"}"}
{"id": "emnlp-2024-main-969", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Do the environmental/sustainability targets set by the company reference external climate change adaptation goals/targets?\\n\\nWe search for details on whether the company\u2019s sustainability objectives align with broader climate change adaptation benchmarks, such as those outlined by international agreements (e.g., Paris Agreement) or national adaptation plans. This includes examining if goals address enhancing resilience to climate impacts, integrating climate adaptation into business strategies, and contributing to global efforts to adapt to changing climate conditions.\\n\\nWe search for details on how a company\u2019s sustainability goals align with recognized external climate change frameworks or initiatives, such as the UN\u2019s early warning systems, the Science Based Targets initiative, the Paris Agreement, or the ISO Net Zero Guidelines. This includes commitments to renewable energy, emissions reduction, and investments in nature-based solutions, demonstrating alignment with global efforts to combat climate change and promote resilience.\\n\\nThe question \u201cDo the environmental/sustainability targets set by the company reference external climate change adaptation goals/targets?\u201d is asking for details on how a company\u2019s sustainability or environmental objectives align with broader, recognized climate change adaptation and resilience frameworks or initiatives. This includes looking for evidence that the company has set its environmental targets in response to or in alignment with international agreements (such as the Paris Agreement), initiatives by global organizations (like the UN or the Science Based Targets initiative), or standards and guidelines set by authoritative bodies (such as the International Organization for Standardization). The question seeks to identify whether the company is not only setting internal goals but also contributing to global efforts to combat climate change through adaptation and resilience. This could involve commitments to renewable energy, science-based targets for reducing greenhouse gas emissions, investments in nature-based solutions, or participation in global calls to action for climate resilience. The aim is to gauge the company\u2019s active engagement in the global climate adaptation agenda beyond its immediate operational boundaries.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The question is asking for details on...\\n\\nWe search for details on... Don't mention the question itself in the text.\\n\\nA comparison can be found here: https://platform.openai.com/docs/guides/embeddings/\"}"}
{"id": "emnlp-2024-main-969", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The question \"<QUESTION >\" is asking for details on ... Don't mention the question itself in the text.\"}"}
{"id": "emnlp-2024-main-969", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table J.6: Recall@K, Precision@K, F1-Score@K for the retrieval with question, definition, and concepts (Embeddings = \\\"text-embedding-3-large\\\").\\n\\n|                | Question 5 | Definition 5 | Concepts 5 |\\n|----------------|------------|--------------|------------|\\n| Setup          | 0.2263     | 0.1818       | 0.1960     |\\n| Top K Found    | 0.1503     | 0.1208       | 0.1302     |\\n| Rel. Sources   | 0.1806     | 0.1452       | 0.1565     |\\n| Rel. Retrieved Sources | 0.1524 | 0.1451 | 0.1542 |\\n| F1-Score       |            |              |            |\\n\\nTable K.7: Table R.2: Results for the F1-score of the different strategies for optimizing the embedding search across all topk values (5, 10, 15) and embedding models. The best-performing strategies for optimizing the embedding search (in bold) are using expert-informed explanations excluding the question.\\n\\n| Strategy       | Question 5 | Definition 5 | Concepts 5 |\\n|----------------|------------|--------------|------------|\\n| Question       | 0.132      | 0.132        | 0.132      |\\n| Definition     | 0.131      | 0.130        | 0.132      |\\n| Concepts       | 0.145      | 0.142        | 0.159      |\\n| Expert-Informed| 0.144      | 0.142        | 0.160      |\\n| Short_Q        | 0.144      | 0.142        | 0.159      |\\n| Long_Q         | 0.142      | 0.159        | 0.160      |\\n| Short_noQ      | 0.145      | 0.132        | 0.140      |\\n| Long_noQ       | 0.140      | 0.132        | 0.160      |\\n\\nFigure L.11: Precision@K for the different experimental setups (Embeddings = \\\"text-embedding-3-small\\\").\\n\\nFigure L.12: F1-score for the different experimental setups (Embeddings = \\\"text-embedding-3-small\\\").\\n\\nFigure L.13: Recall@K for the different experimental setups and a relevance threshold of 1 (Embeddings = \\\"text-embedding-3-large\\\").\\n\\nFigure L.14: Recall@K sources for the different experimental setups and a relevance threshold of 3 (Embeddings = \\\"text-embedding-3-large\\\").\"}"}
