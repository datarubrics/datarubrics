{"id": "lrec-2024-main-232", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Building a Japanese Document-Level Relation Extraction Dataset Assisted by Cross-Lingual Transfer\\n\\nYoumi Ma, An Wang, Naoaki Okazaki\\nTokyo Institute of Technology\\nTokyo, Japan\\n{youmi.ma@nlp, an.wang@nlp, okazaki@ctitech.ac.jp}\\n\\nAbstract\\nDocument-level Relation Extraction (DocRE) is the task of extracting all semantic relationships from a document. While studies have been conducted on English DocRE, limited attention has been given to DocRE in non-English languages. This work delves into effectively utilizing existing English resources to promote DocRE studies in non-English languages, with Japanese as the representative case. As an initial attempt, we construct a dataset by transferring an English dataset to Japanese. However, models trained on such a dataset suffer from low recalls. We investigate the error cases and attribute the failure to different surface structures and semantics of documents translated from English and those written by native speakers. We thus switch to explore if the transferred dataset can assist human annotation on Japanese documents. In our proposal, annotators edit relation predictions from a model trained on the transferred dataset. Quantitative analysis shows that relation recommendations suggested by the model help reduce approximately 50% of the human edit steps compared with the previous approach. Experiments quantify the performance of existing DocRE models on our collected dataset, portraying the challenges of Japanese and cross-lingual DocRE.\\n\\nKeywords:\\nInformation Extraction, Document-level Relation Extraction, Dataset Construction, Japanese\\n\\n1. Introduction\\nDocument-level Relation Extraction (DocRE) aims to identify all semantic relationships between entities in a document (Yao et al., 2019). The task promotes Relation Extraction (RE) to a more practical setting, where relations can reside between entity pairs document-wise, i.e., within and beyond the sentence boundary. DocRE is worth spotlighting as it not only inherits the significance of RE in benefiting knowledge graph completion and question answering but also showcases how models comprehend long text (Yu et al., 2017; Trisedya et al., 2019; Chen et al., 2023a). Even in the era of large language models (LLMs), the task deserves more attention as in-context learning of DocRE was considered not yet feasible (Wadhwa et al., 2023). DocRE research has been conducted mainly in English (Yao et al., 2019; Zhou et al., 2021; Tan et al., 2022b). This work aims to promote DocRE in other languages with the help of English resources. Specifically, we utilize existing resources of English DocRE to construct datasets and models for non-English DocRE. We chose Japanese as our target language for the following two reasons. Firstly, despite Japanese being a widely used language for web content, there is currently a notable absence of general-purpose Japanese DocRE resources. Our work thus contributes to the community by establishing the foundation for Japanese DocRE. Secondly, Japanese stands out as one of the most linguistically distant languages (ente!, rel!, ent\\\"\\\")(ente\\\", rel\\\", ent#)(ente#, rel#, ent$)(ente!, rel!, ent\\\") from English (Chiswick and Miller, 2004). The dissimilarity encompasses various aspects, including script models and word order. Therefore, our research setting is highly representative, and the insights we gain will hold value when acquiring resources for other languages.\\n\\nWe first explore if DocRE resources of high quality can be obtained with zero human effort. To this end, we automatically construct a Japanese DocRE dataset with cross-lingual transfer. Specifically, we translate Re-DocRED (Tan et al., 2022b), a popular English DocRE dataset of high quality, into Japanese with a machine translator. An automatically constructed dataset (hereafter referred...\"}"}
{"id": "lrec-2024-main-232", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset | Language | # Triples | # Docs | Avg. # Toks. | # Rels. | Evi. |\\n|---------|----------|-----------|-------|-------------|--------|------|\\n| DocRED  | en.      | 50,503    | 198   | 96          | Y      |      |\\n| Re-DocRED | en. | 120,664   | 4,053 | 198.4       | 96     | N    |\\n| HacRED  | zh.      | 56,798    | 7,731 | 122.6       | 26     | N    |\\n| HistRED | kr.      | 9,965     | 5,816 | 100.6       | 20     | Y    |\\n| JacRED  | ja.      | 42,241    | 2,000 | 260.1       | 35     | Y    |\\n\\nTable 1: Statistics of existing and proposed DocRE datasets. Column Evi. shows whether each dataset annotates evidence sentences or not. Statistics for DocRED are from the human-annotated subset.\\n\\nTo as Re-DocRED (ja) can thus be obtained without human annotators. The translation-based cross-lingual transfer has been successfully applied to other information extraction (IE) tasks, including named entity recognition and sentence-level relationship extraction (Chen et al., 2023b; Hennig et al., 2023). However, we observe that models trained on Re-DocRED ja suffer from low recalls when extracting relation triples from raw Japanese text. We investigate the error cases and attribute the failures to the discrepancies between documents in Re-DocRED ja and those composed by native speakers. The discrepancies include deviations of topics and wording. Our observation underscores the uniqueness and complexity of DocRE in comparison to other IE tasks.\\n\\nGiven that Re-DocRED ja is not suitable for immediate practical application, we explore if the dataset can assist human annotation. As in Figure 1, we adopt a semi-automatic, edit-based annotation scheme, where annotators edit machine recommendations by removing incorrect instances and supplementing missed instances (Yao et al., 2019; Cheng et al., 2021; Tan et al., 2022b). In contrast to previous works where only relation instances from an existing knowledge base are recommended (Yao et al., 2019; Cheng et al., 2021; Tan et al., 2022b), we recommend instances with a state-of-the-art DocRE model trained on Re-DocRED ja. The collected dataset is named as JacRED (Japanese Document-level Relation Extraction Dataset), with statistics shown in Table 1. We quantitatively analyze recommendations from the model trained on Re-DocRED ja and those from knowledge base queries and find the former reduces the human edit steps to half of the latter.\\n\\nWe employ JacRED as a benchmark for evaluation. Firstly, we evaluate the performance of existing models on Japanese DocRE. While models trained using the train set of JacRED perform fairly on the test set, the scores fall short of those achieved on Re-DocRED. The result indicates that JacRED introduces extra challenges in addition to Re-DocRED. Notably, we observe that in-context learning of LLMs yields poor performance on JacRED, in line with the findings of Wadhwa et al. (2023). Next, we quantify the performance gap between models trained on Re-DocRED ja and those trained on JacRED. The results further demonstrate that, although translation-based cross-lingual transfer appears effective for a range of IE tasks, it does not hold true for DocRE, especially for distant language pairs. Additionally, JacRED also enables the evaluation of cross-lingual DocRE. We assess the cross-lingual transferability of existing DocRE models between English and Japanese, from which we observe challenges due to the complexity of document semantics.\\n\\nOur dataset will be publicly available.\\n\\n### Dataset Construction\\n\\n**Task Definition.** For each document $D$ consisted of $n$ sentences $X_D = \\\\{x_1, x_2, \\\\ldots, x_n\\\\}$, entities within the document are given as $E_D = \\\\{e_1, e_2, \\\\ldots, e_k\\\\}$, where each entity $e_i \\\\in E_D$ is a collection of all its proper-noun mentions $e_i = \\\\{m_{i1}, m_{i2}, \\\\ldots, m_{il}\\\\}$. A DocRE model is expected to extract all relation triples within the document in the form of $(e_h, r, e_t)$, where $e_h$ is the head entity, $e_t$ is the tail entity, and $r$ is a relation label chosen from a predefined set. Additionally, we also expect the model to perform evidence retrieval, where evidence for each relation prediction is provided at the sentence level. In other words, for a predicted triple $(e_h, r, e_t)$, the model should be able to return the evidence sentences $V_{e_h, r, e_t} \\\\subseteq X_D$.\\n\\n**Approach.** We explore ways to construct language resources for Japanese DocRE using existing English resources. To do so, we start by automatically building a dataset with cross-lingual transfer (Section 2.1). The approach has been reported successful in other IE tasks (Chen et al., 2023b; Hennig et al., 2023); If the transferred dataset portrays the characteristics of Japanese DocRE well, there is no need to recruit human annotators. However, we observe that the DocRE models trained with such a dataset err on raw Japanese text. Nevertheless, the model yields predictions of fair quality. We thus adopt the model trained on the transferred dataset as an intermediate tool to assist human annotation (Section 2.2).\\n\\n1 The dataset is available at https://github.com/YoumiMa/JacRED\"}"}
{"id": "lrec-2024-main-232", "page_num": 3, "content": "{\"primary_language\":\"ja\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u30e2\u30ed\u30b4\u30ed\u5dde\u306f\u3001\u30bf\u30f3\u30b6\u30cb\u30a2\u306b\u3042\u308b31\u884c\u653f\u533a\u306e\u3072\u3068\u3064\u3002\u5dde\u90fd\u306f\u30e2\u30ed\u30b4\u30ed\u5e02\u3067\u3042\u308b\u30022012\u5e74\u56fd\u52e2\u8abf\u67fb\u306b\u3088\u308b\u3068\u3001\u540c\u5dde\u306e\u4eba\u53e3\u306f2,218,492\u4eba\u3067\u3001\u56fd\u52e2\u8abf\u67fb\u524d\u306e\u4e88\u6e2c\u50242,209,072\u4eba\u3092\u4e0a\u56de\u3063\u305f\u3002\"}"}
{"id": "lrec-2024-main-232", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semi-Automatic Construction\\n\\nHaving observed drawbacks of Re-DocRED ja, we postulate that human annotations are necessary to better depict Japanese DocRE. We thus involve human annotators in constructing a Japanese DocRE dataset, which we call JacRED. The annotation process consists of two phases: the entity mention annotation phase and the relation annotation phase. Both phases follow an edit-based scheme (Yao et al., 2019): Annotators only need to edit machine recommendations instead of listing all relation instances from scratch.\\n\\nThe quality of machine recommendation is crucial under the edit-based scheme: Poor recommendations require more edits, which will drastically increase the annotators' workload and affect the dataset's quality. The problem is recognized in DocRED as the false-negative issue, where too many relation instances are left out in the recommendations to be mended by human edits (Huang et al., 2022). We propose to mitigate this issue using Re-DocRED ja, utilizing a model trained on Re-DocRED ja to recommend relation instances.\\n\\nDocuments. JacRED is built on top of the Japanese edition of Wikipedia. We clean up the dump and extract the opening text of each page as the document, with only those longer than 256 characters kept in our annotation pool.\\n\\nAnnotators. Given the complexity of the task, we recruit native Japanese speakers with expertise in annotating language resources instead of crowdsourcing. The annotators first work individually on different data and then cross-check the worked annotations. The annotation tool is BRAT (Stenetorp et al., 2012) during both phases.\\n\\n2.2.1. Entity Mention Annotation\\n\\nThe purpose of the entity annotation phase is two-fold: (1) to obtain high-quality entity mention annotations for each document and (2) to filter out documents involved with few entities and relations.\\n\\nEntity Types. We adopt the definition of IREX (Information Retrieval and Extraction Exercise, Sekine and Isahara, 2000) with 8 types, whose scope is similar to that of DocRED. A list of entity types is provided in Table 7 of Appendix A.\\n\\nMachine Recommendations. We parse each document and obtain machine predictions of named entity mentions using KWJA (Ueda et al., 2023), a unified analyzer for Japanese.\\n\\nDocument Filtering. Another round of document filtering is performed based on the machine prediction to remove documents that are likely to contain few cross-sentence relations. To this end, we first link each mention to Wikidata entities (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014). If an edge with label $r$ connects a certain entity pair $(e_h, e_t)$ in the knowledge base, we treat $(e_h, r, e_t)$ as an extractable relation triple from the document, following the distant-supervision assumption (Mintz et al., 2009). Only documents with more than 4 cross-sentence relations are preserved in the annotation pool. We employ mGENRE (De Cao et al., 2022) for entity linking and KGTK (Ilievski et al., 2020) for connectivity check.\\n\\nHuman Edits. We randomly select 2,000 documents from the annotation pool for human annotation. Human annotators review recommendations in each document, correcting wrongly predicted entity mentions and supplementing missed ones.\\n\\n2.2.2. Relation Annotation\\n\\nRelations and coreferences are annotated based on entities. Our approach differs from existing works in that (1) we define a smaller relation label set that covers a sufficient number of relation instances, and (2) we provide machine recommendations with a model trained with Re-DocRED ja.\\n\\nCoreference Recommendations. For each entity $e_i$, we treat all its mentions $\\\\{m_{i1}, \\\\ldots, m_{il}\\\\}$ as coreferences of each other. As introduced in the task definition, we only consider proper nouns as mentions while excluding the pronouns. Mentions linked to the same Wikidata entity are recommended as coreferences.\\n\\nRelation Types. (Re-)DocRED's relation label set $R$ contains 96 relation types. However, it is hard for annotators to comprehend such a large label set, which will eventually affect the annotation quality. We thus reduce the relation label set based on the following principles: (1) All relation categories defined in ERE (Song et al., 2015) should be covered; (2) Explicitly-defined inverse relation pairs, e.g., has_part and part_of, are merged into one; (3) Relations frequently appearing in Re-DocRED are preserved as much as possible. This results in a label set $R'$ of 28 relations covering over 88% relation instances in Re-DocRED.\"}"}
{"id": "lrec-2024-main-232", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Helen Craig McCullough (1918 - 1998) was an American scholar of Japanese classics. She translated many Japanese classics into English, but is not as well known in Japan as Donald Keene and Edward G. Seidensticker. She was born in California. She was graduated from the University of California, Berkeley (political science major) in 1939. With the outbreak of the Pacific War, she entered the Naval Japanese Language School in Boulder, Colorado. After the war ended, she came to Japan and worked as an interpreter, returning to Berkeley in 1950 to earn her M.A. and Ph.D. After teaching at Stanford University, she returned to Berkeley in 1969 and became a professor in 1975. She visited Japan several times and received a medal of honor from the Japanese government, retiring in 1988. Her husband is William McCullough, also a scholar of Japanese literature.\"}"}
{"id": "lrec-2024-main-232", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison of (Re-)DocRED and JacRED. Values are average for each document.\\n\\n|                | DocRED | Re-DocRED | JacRED |\\n|----------------|--------|-----------|--------|\\n| Sentences      | 7.98   | 7.98      | 8.39   |\\n| Entities       | 19.51  | 19.45     | 17.87  |\\n| Relations      | 12.45  | 29.77     | 21.12  |\\n| Evidences      | 1.60   | 0.88      | 1.67   |\\n\\nOn one hand, documents in JacRED contain more relation instances than DocRED on average, implying that the false negative issue is mitigated in JacRED compared to DocRED. On the other hand, documents in JacRED contain fewer relation instances than in Re-DocRED. One possible reason is our re-definition of relation types, where symmetric relation types are merged into one as they represent the same knowledge.\\n\\nEvidence Annotation. Re-DocRED revises DocRED to alleviate the false negative issue by supplying missed relation instances. However, evidence sentences for those supplied instances are not included in Re-DocRED. In contrast, we collect human-annotated evidence sentences during the relation annotation phase. JacRED thus better portrays the correlation between relation and evidence sentences than Re-DocRED.\\n\\n3.2. Number of Human Edits\\n\\nWe quantify the distance between machine recommendations and human annotations of relation instances. To this end, we compare machine recommendations against final human annotations to see how many edits have been made. Specifically, we randomly sample 400 documents from JacRED and calculate the number of recommendations being deleted/substituted/supplied as in Table 3.\\n\\nHuman Annotations v.s. Machine Recommendations. We observe that more than 20% of machine recommendations (1,490 out of 6,500) were regarded as inappropriate, whose relation labels were deleted or substituted by human annotators. The human annotators also supplied another 2,740 relation instances, taking up more than 40% of the recommendations. We thus conclude that DocRE models trained on the automatically constructed dataset still lag behind human performance considerably, suggesting the importance of a manually collected dataset.\\n\\nCross-Lingual Transfer v.s. Knowledge Base Queries.\\n\\nWe further measure the distance of human annotations from relations recommended by querying Wikidata, a de-facto method used in previous works (Yao et al., 2019; Cheng et al., 2021). Compared with model predictions, Wikidata provides only half as many recommendations: To reach the human annotations, 50% (1,572 out of 3,200) of the recommendations need to be revised, with another 200% instances to be added. In total, it takes 7,805 steps to align Wikidata recommendations with the final annotation, while only 4,230 steps are needed when employing cross-lingual transfer. These statistics reveal the usefulness of Re-DocRED in reducing human efforts.\\n\\n4. Experiments\\n\\nPurposes. We employ JacRED as a benchmark to examine the capability of existing DocRE models. Our major concerns are: (1) How well can existing DocRE models perform Japanese DocRE? (2) How different can a DocRE model perform when trained on Re-DocRED and JacRED? Additionally, we evaluate the cross-lingual transferability of existing DocRE models with JacRED.\\n\\nSettings. We split JacRED into train/dev/test sets with 1400/300/300 documents. Models are trained and evaluated on a single Tesla V100 16GB GPU. For evaluation, we follow previous works to compute the micro-averaged F1 scores for relations and evidence sentences (Yao et al., 2019). Additionally, we compute Rel F1 Ign, a variant of F1, where relation instances seen in the training set are ignored during evaluation. Average scores of 5 runs initialized with different random seeds are reported throughout this paper.\\n\\n4.1. Models Trained on JacRED\\n\\nWe measure the performance of existing models when supervised by the training split of JacRED. Specifically, we train and evaluate 4 popular models on top of tohoku-nlp/bert-base-japanese-v2 available on Huggingface, with results summarized in Table 4. Among these models, DREEAM is the current state-of-the-art model on (Re-)DocRED for extracting both relations and evidence sentences. We also evaluate the performance of LLM with in-context learning. JacRED introduces extra challenges beyond those in Re-DocRED. In Table 4, all DocRE models score above 60 on Relation F1. Although acceptable, the performance of each model is worse than their equivalents trained on Re-DocRED, with a gap of 10 F1 points (cf. Table 6). The result suggests potential challenges in...\"}"}
{"id": "lrec-2024-main-232", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Recommendations # Deletions # Substitutions # Supplements\\n\\n## Cross-lingual Transfer\\n\\n|                | Number of relation instances automatically recommended and how they should be revised to reach the final human annotations. |\\n|----------------|-------------------------------------------------------------------------------------------------------------------------|\\n|                | Dev Set | Test Set | Rel F1 | Rel F1 | Ign Evi | F1 | Rel F1 | Rel F1 | Ign Evi | F1 |\\n| ATLOP (Zhou et al., 2021) | 66.53 | 65.21 | \u2013 | 68.04 | 66.80 | \u2013 |\\n| DocuNet (Zhang et al., 2021) | 66.67 | 65.37 | \u2013 | 67.66 | 66.47 | \u2013 |\\n| KD-DocRE (Tan et al., 2022a) | 67.12 | 65.70 | \u2013 | 68.29 | 66.99 | \u2013 |\\n| DREEAM (Ma et al., 2023) | 67.34 | 65.90 | 61.46 | 68.73 | 67.40 | 62.11 |\\n| gpt-3.5-turbo-instruct (Ouyang et al., 2022) | 13.46 | 12.84 | \u2013 | 13.17 | 12.90 | \u2013 |\\n| gpt-4 (OpenAI, 2023) | 24.17 | 23.63 | \u2013 | 27.45 | 26.99 | \u2013 |\\n\\n## Models Trained on Transferred Re-DocRED\\n\\nSection 2.1 has mentioned limitations in the dataset automatically constructed from cross-lingual transfer. Specifically, we showcased how DocRE models trained on such a dataset fail to extract relation triples from raw Japanese documents. Here, we quantify the performance gap between a model trained on the automatically constructed dataset (Re-DocRED ja) and the human-annotated dataset with machine assistance (JacRED). The test set of JacRED is adopted as the benchmark, with results shown in Table 5.\\n\\nTable 5: Precision (P), Recall (R), and F1 scores of DREEAM trained on different data, evaluated on the test set of JacRED. The number of documents in each set is shown in parentheses.\\n\\n|                | Relation Training Data | PR F1 |\\n|----------------|------------------------|-------|\\n| JacRED (1,400) | 64.76 | 73.29 | 68.73 |\\n| Re-DocRED ja (3,053) | 56.14 | 53.67 | 54.87 |\\n| Re-DocRED ja (1,400) | 55.52 | 51.77 | 53.56 |\\n\\nModels trained on Re-DocRED ja suffer from low recalls. From Table 5, we witness that DREEAM trained on Re-DocRED ja underperforms its equivalent trained on JacRED. Taking a closer look at the scores, we find the gap in recalls (73.29 vs. 53.67) is more significant than that in precisions (64.76 vs. 56.14). The result corresponds to our observation in Section 2.1 that models trained on the transferred dataset cannot identify some relation instances due to the limitation of texts translated from English.\\n\\nThe gap between models trained on Re-DocRED ja and JacRED is evident under the same setting. We further train DREEAM on Re-DocRED ja with 1,400 documents, aligned with the number of documents in JacRED. The F1 score drops from 54.87 to 53.56, lagging behind that of the model trained on JacRED with a gap of 15 F1 points. The results indicate that JacRED provides...\"}"}
{"id": "lrec-2024-main-232", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Cross-lingual performance on the test set of JacRED (ja.) and Re-DocRED (en.) of models with mBERT as the encoder.\\n\\nCrosslingual DocRE\\nJacRED also enables the evaluation of cross-lingual DocRE. Although DocRE datasets have been collected in Chinese (Cheng et al., 2021) and Korean (Yang et al., 2023), they lay in different domains than (Re-)DocRED. In contrast, JacRED is collected from Wikipedia following a pipeline similar to DocRED. The domain and label sets of JacRED and (Re-)DocRED thus match each other, enabling the evaluation of cross-lingual DocRE. Here, we take the first attempt to measure the cross-lingual transferability of existing models using Re-DocRED and JacRED.\\n\\nSpecifically, we train models on the training set in one language and evaluate them on the test set in another. The relation label set of Re-DocRED is projected onto JacRED using the same method as in Section 2.2. To ensure the multilingualism of trained models, we adopt multilingual BERT (mBERT, Devlin et al. (2019)) as the encoder. Evaluation results are shown in Table 6. Cross-lingual performance of existing models is limited. All models exhibited a decreased accuracy in the target language. Different from sentence-level tasks, DocRE requires not only an understanding of individual sentences but also inter-sentence semantics within the whole document, which improves the difficulty of building cross-lingual models. This may offer a potential explanation as to why translation-based cross-lingual transfer is ineffective for DocRE, despite its successful application in sentence-level RE and OpenIE (Kolluru et al., 2022; Hennig et al., 2023).\\n\\n5. Related Work\\n\\nDocRE corpora in English. The most well-known definition of DocRE was proposed by Yao et al. (2019), along with a dataset collected from English Wikipedia named DocRED. While two document-level relation extraction datasets, namely CDR (Li et al., 2016) and GDA (Wu et al., 2019), have been proposed ahead of DocRED, they were collected in the biomedical domain, thus unsuitable for developing general-purpose DocRE models. DocRED suffers from the false negative issue where a considerable amount of relation instances are absent from the ground-truth annotations (Huang et al., 2022; Xie et al., 2022; Tan et al., 2022b). Huang et al. (2022) randomly selected 96 documents from DocRED and relabeled them from scratch, while Tan et al. (2022b) revised the whole dataset as Re-DocRED with machine assistance. This work follows a machine-assisted annotation process as DocRED and Re-DocRED while paying extra attention to providing better machine recommendations with the model trained on a dataset transferred from Re-DocRED.\\n\\nDocRE corpora in other languages. Cheng et al. (2021) constructed HacRED from Chinese DBpedia to promote relation extraction from complex contexts. Yang et al. (2023) focused on Korean historical RE research and collected HistRED from a travel diary written between the 16th and 19th centuries. These datasets were collected independently from DocRED with distinct domains and label sets. Apart from these studies, Cheng et al. (2022) released a system for medical relation extraction on Japanese documents, while the dataset is not publicly available. In this work, we explore how existing resources can help construct DocRE resources in other languages. We share the insights that models trained under cross-lingual transfer techniques are not ready for practical use. However, they serve as good assistants for aiding human annotations.\\n\\nCross-lingual transfer for structured predictions.\\n\\nSeveral works have adopted translation-based cross-lingual transfer approaches to solve cross-lingual and multi-lingual structured prediction tasks (Faruqui and Kumar, 2015; Kolluru et al., 2022). More recently, Hennig et al. (2023) constructed MultiTACRED, a multilingual version of TACRED (Victor Zhong et al., 2018), using similar approaches as ours. They confirmed the dataset's quality to be high enough even without human modifications. Our work examines the approach's usefulness in the literature of DocRE and reports its shortcomings. Unlike other sentence-level IE tasks, DocRE involves understanding not only\"}"}
{"id": "lrec-2024-main-232", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work publishes JacRED, the first benchmark for general-purpose Japanese DocRE. In the process of building JacRED, we explore how to utilize existing English DocRE resources to construct resources for other languages, using Japanese as the representative. Starting from constructing a dataset by translation-based cross-lingual transfer, we have shown how and why such a dataset is not ready for practical use. Nevertheless, models trained on the dataset can replace existing approaches, i.e., knowledge base queries, to provide better recommendations for human annotation. Our insights can benefit the development of DocRE resources for other languages. Benchmarking with JacRED portrays the challenge of not only Japanese but also cross-lingual DocRE.\\n\\nIn the future, we plan to utilize models trained on JacRED to help downstream tasks such as question answering and reading comprehension.\\n\\n7. Ethics Statement\\nIn this work, we collected a dataset from Wikipedia, whose text content can be used under the terms of the CC-BY-SA. We thus presume that no copyright issues are involved in constructing and publishing our dataset.\\n\\nAutomatic Annotations.\\nFor the machine translator, we adopted DeepL API at the cost of 2,500 JPY (approx. 16$) per 1 million characters. For the LLM, we tested with the instructed version of GPT-3.5 provided by OpenAI at the cost of $0.0015 per 1 thousand tokens for input and $0.002 per 1 thousand tokens for output. For existing DocRE models, all resources we adopted are publicly available and free of charge.\\n\\nHuman Annotations.\\nBefore the annotation, we arranged meetings in advance to (1) explain the purpose of collecting the dataset and (2) adjust the workload. The annotators understand and agree that their work will be used to train neural networks. For both the entity and relation annotation phases, we explained the purpose of building the dataset and provided a detailed annotation guideline. During the annotation, we frequently discussed with the annotators how to handle irregular cases and adjust the guidelines when necessary. 7 annotators are involved in the entity mention annotation phase, and 6 annotators are involved in the relation annotation phase. Each annotator is paid 5,000 JPY (approx. 30$) per hour, which is higher than the standard salary in Japan.\\n\\n8. Acknowledgements\\nThis paper is based on results obtained from a project, JPNP18002, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).\\n\\n9. Bibliographical References\\nHaotian Chen, Bingsheng Chen, and Xiangdong Zhou. 2023a. Did the models understand documents? benchmarking models for language understanding in document-level relation extraction. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6418\u20136435, Toronto, Canada. Association for Computational Linguistics.\\n\\nYang Chen, Chao Jiang, Alan Ritter, and Wei Xu. 2023b. Frustratingly easy label projection for cross-lingual transfer. In Findings of the Association for Computational Linguistics: ACL 2023, pages 5775\u20135796, Toronto, Canada. Association for Computational Linguistics.\\n\\nFei Cheng, Shuntaro Yadag, Ribeka Tanaka, Eiji Aramaki, and Sadao Kurohashi. 2022. JaMIE: A pipeline Japanese medical information extraction system with novel relation annotation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3724\u20133731, Marseille, France. European Language Resources Association.\\n\\nQiao Cheng, Juntao Liu, Xiaoye Qu, Jin Zhao, Jiaqing Liang, Zhefeng Wang, Baoxing Huai, Nicholas Jing Yuan, and Yanghua Xiao. 2021. HacRED: A large-scale relation extraction dataset toward hard cases in practical applications. In Findings of the Association for Computational Linguistics: ACL -IJCNLP 2021, pages 2819\u20132831, Online. Association for Computational Linguistics.\\n\\nBarry Chiswick and Paul Miller. 2004. Linguistic distance: A quantitative measure of the distance between English and other languages. IZA Discussion Papers 1246, Institute of Labor Economics (IZA).\"}"}
{"id": "lrec-2024-main-232", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-232", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-232", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Joint Conference on Artificial Intelligence, IJCAI-21, pages 3999\u20134006. International Joint Conferences on Artificial Intelligence Organization.\\n\\nWenxuan Zhou, Kevin Huang, Tengyu Ma, and Jing Huang. 2021.\\n\\nDocument-level relation extraction with adaptive thresholding and localized context pooling. In Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\nLanguage Resource References\\n\\nVictor Zhong et al. 2018. TAC Relation Extraction Dataset. The Stanford NLP Group. Linguistic Data Consortium, The Stanford NLP Group resources, 1.0, ISLRN 927-859-759-915-2.\\n\\n(Re-)DocRED (6) JacRED (8)\\n\\nPERSON PERSON\\n\\nORGANIZATION ORGANIZATION\\n\\nLOCATION LOCATION\\n\\nTIME ARTIFACT\\n\\nNUM TIME\\n\\nMISC DATE\\n\\nPERCENT\\n\\nMONEY\\n\\nTable 7: Comparison of entity types of existing dataset and our proposed dataset. The total number of entity types is indicated in the parenthesis following each dataset.\\n\\nAppendix A: Entity Label Types\\n\\nIn this section, we list all entity types in JacRED as in Table 7, together with those defined in (Re-)DocRED.\\n\\nAppendix B: Relation Label Types\\n\\nIn this section, we list all relation types included in JacRED as in Table 8.\\n\\nAppendix C: Prompt for In-Context Learning\\n\\nWe showcase the prompt used for the in-context learning of LLM in Figure 5. In previous work where LLM are utilized for relation extraction (Wadhwa et al., 2023; Li et al., 2023), the prompt has been designed to return all relation triples within a document. However, it is hard to identify all relation triples across a document at once. Furthermore, most supervised approaches tackle DocRE by classifying relation types entity-pair wise (Zhou et al., 2021; Xie et al., 2022; Tan et al., 2022a; Ma et al., 2023). We thus design prompts to reduce the task complexity by querying one relation type for each API call. By using our prompt, GPT-3.5 yields better performance than reported in existing works.\\n\\n10 In early experiments, we evaluated the performance when querying: 1) one relation type; 2) all relation types of one entity pair; 3) one relation type of one entity pair during each API call, where 2) yields good performance at a low cost.\"}"}
{"id": "lrec-2024-main-232", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Relation types included in our proposed dataset. Column ID shows the Wikidata property ID linked to each relation type. The last category Others includes relation types undefined in ERE type.\\n\\nPerform Document-level Relation Extraction task. Given a context and an entity list, identify all entity pairs with relation type {located in the administrative territorial entity} in the context. Note that only a few entity pairs hold relations. Please return entity pairs as {head, tail} and make sure they follow the relation definition:\\n\\nlocated in the administrative territorial entity: {head} is located in the administrative territorial entity {tail}.\\n\\n###\\n\\nContext:\\n\u6771\u4eac\u30fb\u677f\u6a4b\u51fa\u2f9d\u3002\\n\\nEntity List:\\n\u6771\u4eac || \u677f\u6a4b\\n\\nExtracted Entity Pairs: {\u677f\u6a4b, \u6771\u4eac}\\n\\n###\\n\\nContext:\\n\u5357\u90fd\u516d\u5b97\uff08\u306a\u3093\u3068\u308d\u304f\u3057\u3085\u3046\u3001\u306a\u3093\u3068\u308a\u304f\u3057\u3085\u3046\uff09\u3068\u306f\u3001\u5948\u826f\u6642\u4ee3\u3001\u5e73\u57ce\u4eac\u3092\u4e2d\u2f3c\u306b\u6804\u3048\u305f\u2f47\u672c\u4ecf\u6559\u306e6\u3064\u306e\u5b97\u6d3e\u306e\u7dcf\u79f0\u3002\u4e09\u8ad6\u5b97\uff08\u3055\u3093\u308d\u3093\u3057\u3085\u3046\u3001\u4e2d\u8ad6\u30fb\u2f17\u2f06\u2fa8\u8ad6\u30fb\u767e\u8ad6\uff09-\u83ef\u53b3\u5b97\u3084\u771f\u2f94\u5b97\u306b\u5f71\u97ff\u3092\u4e0e\u3048\u305f\u6210\u5b9f\u5b97\uff08\u3058\u3087\u3046\u3058\u3064\u3057\u3085\u3046\u3001\u6210\u5b9f\u8ad6\uff09-\u4e09\u8ad6\u5b97\u306e\u4ed8\u5b97\uff08\u5bd3\u5b97\uff09\u6cd5\u76f8\u5b97\uff08\u307b\u3063\u305d\u3046\u3057\u3085\u3046\u3001\u552f\u8b58\u5036\u820e\u5b97\uff09-\u6cd5\u76f8\u5b97\u306e\u4ed8\u5b97\uff08\u5bd3\u5b97\uff09\u83ef\u53b3\u5b97\uff08\u3051\u3054\u3093\u3057\u3085\u3046\u3001\u83ef\u53b3\u7d4c\uff09\u5f8b\u5b97\uff08\u308a\u3063\u3057\u3085\u3046\u3001\u56db\u5206\u5f8b\uff09-\u771f\u2f94\u5f8b\u5b97\u7b49\u304c\u2f63\u307e\u308c\u305f\u306a\u304a\u3001\u5948\u826f\u6642\u4ee3\u5f53\u6642\u304b\u3089\u300c\u5357\u90fd\u516d\u5b97\u300d\u3068\u547c\u3070\u308c\u3066\u3044\u305f\u308f\u3051\u3067\u306f\u306a\u304f\u3001\u5e73\u5b89\u6642\u4ee3\u4ee5\u964d\u5e73\u5b89\u4eac\u3092\u4e2d\u2f3c\u306b\u6804\u3048\u305f\u300c\u5e73\u5b89\u2f06\u5b97\u300d\uff08\u5929\u53f0\u5b97\u30fb\u771f\u2f94\u5b97\uff09\u306b\u5bfe\u3059\u308b\u547c\u3073\u540d\u3067\u3042\u308b\u3002\\n\\nEntity List:\\n\u5948\u826f\u6642\u4ee3 || \u5e73\u5b89\u6642\u4ee3 || \u5e73\u57ce\u4eac || \u2f47\u672c || \u5e73\u5b89\u4eac || \u5e73\u5b89\\n\\nExtracted Entity Pairs: {\u5e73\u5b89\u4eac, \u2f47\u672c}\\n\\n###\\n\\nContext:\\n\u30a2\u30f3\u30bd\u30cb\u30fc\u4e16\u754c\u3092\u99c6\u3051\u308b\uff08\u30a2\u30f3\u30bd\u30cb\u30fc\u305b\u304b\u3044\u3092\u304b\u3051\u308b\uff09\u306f\u3001\u30a2\u30e1\u30ea\u30ab\u5408\u8846\u56fd\u306eCNN\u3067\u653e\u9001\u3055\u308c\u3066\u3044\u308b\u30c6\u30ec\u30d3\u756a\u7d44\u30022013\u5e744\u2f49\u304b\u3089\u653e\u9001\u3092\u958b\u59cb\u3057\u305f\u3002\u30a8\u30df\u30fc\u8cde\u30924\u56de\u53d7\u8cde\u3001\u307e\u305f\u3001\u811a\u672c\u8cde\u3001\u2fb3\u97ff\u8cde\u3001\u7de8\u96c6\u8cde\u3001\u64ae\u5f71\u8cde\u306b11\u56de\u30ce\u30df\u30cd\u30fc\u30c8\u3055\u308c\u3066\u3044\u308b\u3002\u307e\u305f2013\u5e74\u306b\u306f\u30a2\u30e1\u30ea\u30ab\u306e\u30c6\u30ec\u30d3\u30fb\u30e9\u30b8\u30aa\u30fb\u30a6\u30a7\u30d6\u30b5\u30a4\u30c8\u306e\u512a\u308c\u305f\u653e\u9001\u4f5c\u54c1\u306b\u8d08\u3089\u308c\u308b\u30d4\u30fc\u30dc\u30c7\u30a3\u8cde\u3092\u53d7\u8cde\u3057\u305f\u3002\u2f83\u3089\u6599\u7406\u2f08\u3067\u3042\u308a\u3001\u30ce\u30f3\u30d5\u30a3\u30af\u30b7\u30e7\u30f3\u300c\u30ad\u30c3\u30c1\u30f3\u30fb\u30b3\u30f3\u30d5\u30a3\u30c7\u30f3\u30b7\u30e3\u30eb\u300d\u306e\u8457\u8005\u3067\u3082\u3042\u308b\u30a2\u30f3\u30bd\u30cb\u30fc\u30fb\u30dc\u30fc\u30c7\u30a3\u30f3\u304c\u4e16\u754c\u306e\u6d25\u3005\u6d66\u3005\u3092\u65c5\u3057\u3001\u3042\u307e\u308a\u77e5\u3089\u308c\u3066\u3044\u306a\u3044\u5730\u57df\u306e\u666f\u89b3\u3001\u2fb5\u4fd7\u3001\u2fb7\u6750\u3001\u6599\u7406\u306a\u3069\u3092\u7d39\u4ecb\u3059\u308b\u3002\\n\\nEntity List:\\n\u30a2\u30e1\u30ea\u30ab\u5408\u8846\u56fd || \u30a2\u30f3\u30bd\u30cb\u30fc\u4e16\u754c\u3092\u99c6\u3051\u308b || CNN || 2013\u5e744\u2f49 || \u30a8\u30df\u30fc\u8cde || 2013\u5e74 || \u30d4\u30fc\u30dc\u30c7\u30a3\u8cde || \u30ad\u30c3\u30c1\u30f3\u30fb\u30b3\u30f3\u30d5\u30a3\u30c7\u30f3\u30b7\u30e3\u30eb || \u30a2\u30f3\u30bd\u30cb\u30fc\u30fb\u30dc\u30fc\u30c7\u30a3\u30f3\\n\\nExtracted Entity Pairs:\"}"}
