{"id": "emnlp-2023-main-132", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Ablation experiments in four groups: (a) number of demonstrations, (b) number of demonstrations for CoT, (c) decomposition types, (4) directness.\\n\\nTable 11: A comparison of CRT-QA and other Table QA datasets. CRT-QA is the first TableQA dataset that contains implicit questions, detailed annotations of human reasoning paths, and question decomposition types. \u2020: MultiHiertt only contains math expression as the reasoning path.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table Read the table below regarding \u201cyugoslavia national football team results\u201d\\n\\n| # | date     | city          | opponent     | results | type of game |\\n|---|----------|---------------|--------------|---------|--------------|\\n| 0 | april 18 | belgrade      | france       | 1:0     | 1966 wcq     |\\n| 1 | may 9    | belgrade      | england      | 1:1     | friendly     |\\n| 2 | june 16  | oslo, norway  | norway       | 0:3     | 1966 wcq     |\\n| 3 | september 4 | moscow, russia | ussr       | 0:0     | friendly     |\\n| 4 | september 19 | luxembourg | luxembourg   | 5:2     | 1966 wcq     |\\n| 5 | october 9 | paris, france | france      | 0:1     | 1966 wcq     |\\n| 6 | november 7 | belgrade | norway       | 1:1     | 1966 wcq     |\\n\\nQuestion: Did the Yugoslavia national football team play any games against teams outside of Europe in the table? Answer with only 'Yes' or 'No' that is most accurate and nothing else.\\n\\nAnswer: Yes\\n\\nQuestion: How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition?\\n\\nCode:\\n```\\nimport pandas as pd\\n\\n# get the overall average score per game of the competition\\noverall_avg = df['average'].mean()\\n\\n# filter the dataframe to only include players with a higher average score per game than the overall average\\nhigher_avg_df = df[df['average'] > overall_avg]\\n\\n# count the number of players with a higher average score per game than the overall average\\nnum_higher_avg_players = len(higher_avg_df)\\n\\n# print the result\\nprint(\"Number of players with a higher average score per game than the overall average: \", num_higher_avg_players)\\n```\"}"}
{"id": "emnlp-2023-main-132", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table Tittle: \\\"1982 all-ireland senior hurling championship\\\"\\n\\n| rank | player          | county | tally | total | matches | average |\\n|------|-----------------|--------|-------|-------|---------|---------|\\n| 0    | p\u00e1draig horan   | offaly | 5-17  | 32    | 4       | 8       |\\n| 1    | billy fitzpatrick| kilkenny| 2-24  | 30    | 4       | 7.5     |\\n| 2    | tony o'sullivan | cork   | 0-28  | 28    | 4       | 7       |\\n| 3    | p j molloy      | galway | 3-11  | 20    | 2       | 10      |\\n| 4    | christy heffernan| kilkenny| 3-9   | 18    | 4       | 4.5     |\\n| 5    | pat horgan      | cork   | 0-18  | 18    | 4       | 4.5     |\\n\\nQuestion: How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition?\\n\\nThought 1: I need to first get the overall average score per game of the competition and then count the number of players with a higher average score per game than the overall average.\\n\\nAction 1: overall_avg = df['average'].mean() print(\"The overall average score per game of the competition is \" + overall_avg)\\n\\nObservation 1: The overall average score per game of the competition is 6.916666666666667\\n\\nThought 2: Then I should filter the dataframe to only include players with a higher average score per game than the overall average which is 6.916666666666667 and count the length of the filtered dataframe.\\n\\nAction 2: higher_avg_df = df[df['average'] > overall_avg] num_higher_avg_players = len(higher_avg_df) print(\"Number of players with a higher average score per game than the overall average: \", num_higher_avg_players)\\n\\nObservation 2: Number of players with a higher average score per game than the overall average: 4\\n\\nThought 3: Number of players with a higher average score per game than the overall average is 4. So the answer is 4.\\n\\nAction 3: Finish. The answer is 4.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt Design for Answerability\\n\\nTask description In table-based QA tasks, not all questions need to be answered. You should determine whether to answer according to table and commonsense knowledge, that is to judge if the following question is answerable or unanswerable. The definitions of unanswerable questions are as follows.\\n\\n| Type of unanswerable | Definition |\\n|---------------------|------------|\\n| Out of scope        | Lacking essential information based on the given table. |\\n| Hallucination       | The assumption in the question is invalid based on the table. |\\n| Problematic         | The question itself contains logical error. |\\n| Subjective          | The answer varies due to different metrics, algorithms, and criteria. |\\n| Others              | Other types of questions that can not be labeled. |\\n\\n4-shot\\n\\nRead the table below regarding \u201c1982 all-Ireland senior hurling championship\u201d to judge if the following question is answerable or unanswerable.\\n\\n| rank | player     | county  | tally  | total | matches | average |\\n|------|------------|---------|--------|-------|---------|---------|\\n| 0    | p\u00e1draig horan | offaly | 5 - 17 | 32    | 4       | 8       |\\n| 1    | billy fitzpatrick | kilkenny | 2 - 24 | 30    | 4       | 7.5     |\\n| 2    | tony o\u2019 Sullivan | cork    | 0 - 28 | 28    | 4       | 7       |\\n| 3    | p j molloy | galway | 3 - 11 | 20    | 2       | 10      |\\n| 4    | christy heffernan | kilkenny | 3 - 9  | 18    | 4       | 4.5     |\\n| 5    | pat horgan | cork    | 0 - 18 | 18    | 4       | 4.5     |\\n\\nQuestion How many players in the 1982 all-Ireland senior hurling championship had a higher average score per game than the overall average score per game of the competition?\\nAnswer: answerable. (4.)\\n\\nRead the table below regarding \u201cg.d. estoril praia\u201d to judge if the following question is answerable or unanswerable.\\n\\n| season | competition | round | opponent   | home  | away |\\n|--------|-------------|-------|------------|-------|------|\\n| 2013 - 14 | uefa europa league | 3q     | hapoel ramat gan | 0 - 0 | 1 - 0 |\\n| 2013 - 14 | uefa europa league | play-off | pasching | 2 - 0 | 2 - 1 |\\n| 2013 - 14 | uefa europa league | group h | sevilla | 1 - 2 | - |\\n| 2013 - 14 | uefa europa league | group h | slovan liberec | - | 1 - 2 |\\n| 2013 - 14 | uefa europa league | group h | freiburg | - | 1 - 1 |\\n\\nQuestion Was there a correlation between GD Estoril Praia\u2019s performance in home games and away games during the 2013-14 UEFA Europa League competition?\\nAnswer: answerable. (No.)\\n\\nRead the table below regarding \u201c1941 in brazilian football\u201d to judge if the following question is answerable or unanswerable.\\n\\n| position | team     | points | played | drawn | lost | against | difference |\\n|----------|----------|--------|--------|-------|------|---------|------------|\\n| 1        | corinthians | 35     | 20     | 3     | 1    | 17      | 44         |\\n| 2        | s\u00e3o paulo | 31     | 20     | 5     | 2    | 32      | 23         |\\n| 3        | palestra it\u00e1lia - sp | 30     | 20     | 6     | 2    | 19      | 25         |\\n| 4        | portuguesa | 20     | 20     | 6     | 7    | 46      | - 3        |\\n| 5        | santos | 20     | 20     | 4     | 8    | 60      | - 1        |\\n| 6        | s\u00e3o paulo railway | 18     | 20     | 4     | 9    | 53      | - 5        |\\n| 7        | hespanha | 18     | 20     | 2     | 10   | 57      | - 9        |\\n| 8        | portuguesa santista | 15     | 20     | 7     | 9    | 43      | - 2        |\\n| 9        | ypiranga - sp | 14     | 20     | 4     | 11   | 52      | - 3        |\\n| 10       | juventus | 14     | 20     | 4     | 11   | 49      | - 17       |\\n| 11       | comercial - sp | 5      | 20     | 3     | 16   | 76      | - 52       |\\n\\nQuestion Are teams with higher points more likely to win the teams with lower points?\\nAnswer: unanswerable.\\n\\nRead the table below regarding \u201cneo geo online collection\u201d to judge if the following question is answerable or unanswerable.\\n\\n(Due to its excessive size, the table is disregarded as it cannot be displayed)\\n\\nQuestion Does the length of the Japanese titles differ significantly between pre-2005 and post-2005 releases?\\nAnswer: unanswerable.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sub type Definition and Template\\n\\nIndexing D: Mapping between the values in a specific column and the corresponding rows in a table\\nT: Find the row/columns of [ENTITY]\\n\\nFilter D: Retrieve data from a table based on specific conditions.\\nT: Filter the rows/columns based on the [ENTITY]\\n\\nGrouping D: Group data based on one or more columns/rows.\\nT: Filter the rows/columns based on the [ENTITY]\\n\\nSorting D: Order data in a specific way.\\nT: Sort the rows/columns based on [ENTITY]\\n\\nGrounding D: Determining whether a given statement logically follows from a set of premises or background knowledge.\\nT: Group the rows/columns based on [ENTITY]\\n\\nAuto-categorization D: Categorizing or classifying information into predefined categories or groups based on its content.\\nT: The term of [ENTITY1] in the question is mapped to [ENTITY2]\\n\\nTemporal Reasoning D: Make presumptions about humans' knowledge of times, durations, and time intervals.\\nT: Based on the [ENTITY1] time, the temporal indicator is [ENTITY2]\\n\\nGeographical/Spatial Reasoning D: Reasoning about Geographical/Spatial knowledge.\\nT: Conduct geographical/spatial reasoning on [ENTITY]\\n\\nAggregating D: Combining multiple values into a single value to summarize data and make it easier to understand.\\nT: Conduct the aggregating operation of [ENTITY1] on the value of [ENTITY2]\\n\\nArithmetic D: Basic mathematical operations.\\nT: Conduct the arithmetic operation of [ENTITY1] on the value of [ENTITY2]\\n\\nReasoning with Quantifiers D: The process of making logical and mathematical inferences from statements that contain quantifiers.\\nT: Conduct the reasoning with quantifiers of [ENTITY1] on the domain of discourse of [ENTITY2]\\n\\nTable 10: The definitions (D) and templates (T) of reasoning and operations types in our proposed taxonomy.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 22, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-132", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Our detailed data annotation interface.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CRT-QA: A Dataset of Complex Reasoning Question Answering over Tabular Data\\n\\nZhehao Zhang1\u2217, Xitao Li2\u2217, Yan Gao3, Jian-Guang Lou3\\n\\n1Darmouth College, 2Xi'an Jiaotong University 3Microsoft Research Asia\\n\\nzhehao.zhang.gr@dartmouth.edu, wuitenye@stu.xjtu.edu.cn\\n{yan.gao, jlou}@microsoft.com\\n\\nAbstract\\n\\nLarge language models (LLMs) show powerful reasoning abilities on various text-based tasks. However, their reasoning capability on structured data such as tables has not been systematically explored. In this work, we first establish a comprehensive taxonomy of reasoning and operation types for tabular data analysis. Then, we construct a complex reasoning QA dataset over tabular data, named CRT-QA (Complex Reasoning QA over Tabular data), with the following unique features: (1) it is the first Table QA dataset with multi-step operation and informal reasoning; (2) it contains fine-grained annotations on questions' directness, composition types of sub-questions, and human reasoning paths which can be used to conduct a thorough investigation on LLMs' reasoning ability; (3) it contains a collection of unanswerable and indeterminate questions that commonly arise in real-world situations.\\n\\nWe further introduce an efficient and effective tool-augmented method, named ARC (Auto-exemplar-guided Reasoning with Code), to use external tools such as Pandas to solve table reasoning tasks without handcrafted demonstrations. The experiment results show that CRT-QA presents a strong challenge for baseline methods and ARC achieves the best result. The dataset and code are available at https://github.com/zzh-SJTU/CRT-QA.\\n\\n1 Introduction\\n\\nLarge language models (LLMs) (Brown et al., 2020; Chowdhery et al., 2022; Chung et al., 2022; Touvron et al., 2023; OpenAI, 2023a,b) have recently shown emergent abilities, such as the capacity for \\\"reasoning\\\", when they are sufficient in size (Wei et al., 2022). A large number of works (Zhang et al., 2022a; Wei et al., 2023; Kojima et al., 2023; * \u2217 Work done during Zhehao and Xitao's internship at Microsoft Research Asia.\\n\\nYao et al., 2023a) focus on LLMs' reasoning abilities on text-based NLP tasks. However, the capability of LLMs on table reasoning tasks has not been systematically investigated (Chen, 2023a). Evaluating LLMs' reasoning ability over tabular data and improving their performance can produce a significant impact on efficient data analysis, decision-making, and so on in real-life applications.\\n\\nCurrent Table question answering (Table QA) datasets are primarily concerned with obtaining factoids to answer simple queries and lack in-depth analysis. Although recent works (Chen et al., 2021b) start to investigate multi-hop \\\"reasoning\\\" questions over tables, they do not have a clear definition of reasoning types and the \\\"reasoning\\\" they investigate (e.g., operations like filtering) does not align with current research on LLMs' reasoning ability. Besides, the current Table QA datasets only contain explicitly questions. However, in real-life scenarios, users frequently ask implicit even ambiguous questions over tables.\\n\\nTo fill these gaps and conduct an in-depth analysis...\"}"}
{"id": "emnlp-2023-main-132", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analysis of LLMs' reasoning abilities over tabular data, we first establish a fine-grained taxonomy of commonly-used reasoning and operations for table analysis. Different from previous works (Chen et al., 2021b), we separate the steps that can be easily executed using a single Pandas or SQL query from reasoning and categorize them as operations. Following recent studies on the reasoning capacity of LLMs (Wei et al., 2023), we focus on informal reasoning which utilizes intuition, experience, and common sense to deduce outcomes.\\n\\nThen, we construct CRT-QA dataset (Complex Reasoning QA over Tabular data) over Wikipedia tables. Answer-based evaluation proves inadequate for assessing LLMs' reasoning ability, as it does not fully capture the complexity of their cognitive processes. Nonetheless, devising a robust method for evaluating such reasoning capabilities remains a formidable challenge within the field. When dealing with complex table analysis queries, humans typically begin by reformulating the questions (possibly implicitly) into more explicit ones, followed by decomposing them into sub-questions, and ultimately conducting atomic reasoning. Inspired by this process, we propose fine-grained annotations on the directness of questions, composition types of sub-questions, and human reasoning paths. To explore the ambiguous questions mentioned earlier, we incorporate a subset of unanswerable and indeterminate queries. During question collection, we propose a human-in-the-loop question generation pipeline that utilizes LLM to generate questions necessitating complex, multi-step reasoning. Our proposed pipeline can efficiently produce high-quality queries while mitigating issues such as biases, insufficient complexity, and lack of diversity.\\n\\nWe evaluate LLMs (e.g., GPT-4) with different prompting methods on CRT-QA. Inspired by the finding that LLMs can often generate correct reasoning plans but fail on execution, we propose an efficient and effective method, named ARC (Auto-exemplar-guided Reasoning with Code), to alleviate such limitation. Instead of expensive human effort for code design, ARC first uses an instructional prompt to generate exemplar code on the dev set queries and serve as an in-context demonstration for test questions. After executing the generated code with an external Python interpreter, we then inject the output into the prompt and LLM generates the final answer by reflection. Experiment results demonstrate that CRT-QA poses a significant challenge for baseline methods, as the current most powerful model, GPT-4, achieves an accuracy of 56.32% through few-shot in-context learning. Our proposed ARC achieves the best result, outperforming various prompting and tool-use baselines.\\n\\n2 Related Works\\n2.1 TableQA Datasets\\nTable QA is the task of answering queries concerning tabular data. A large number of datasets have been proposed for this task. Datasets such as WTQ (Pasupat and Liang, 2015), WikiSQL (Zhong et al., 2017), SQA (Iyyer et al., 2017) and Spider (Yu et al., 2018) contain tables for QA or text-to-SQL tasks. Recently, numerous works construct datasets that require multi-hop reasoning on tables: OT-TQA (Chen et al., 2021a), HybridQA (Chen et al., 2021b), TabFact (Chen et al., 2020b), LogicNLG (Chen et al., 2020a), AIT-QA (Katsis et al., 2021), MultiModalQA (Talmor et al., 2021), FeTaQA (Nan et al., 2021). However, they are focused on iterated factoid retrieval (Ho et al., 2022) where the definition of reasoning does not align with the reasoning ability of LLMs. Datasets like FinQA (Chen et al., 2022b), TAT-QA (Zhu et al., 2021), MultiHiertt (Zhao et al., 2022) and TABMWP (Lu et al., 2023b) focus on numerical reasoning over tabular data. Yin et al., 2022 propose ARCADE, a benchmark of 1,082 code generation using the pandas for tabular data analysis. However, they do not introduce commonsense in the datasets and their labels are not natural languages.\\n\\n2.2 Language Models for Reasoning\\nLLMs' reasoning abilities numerous works (Fu et al., 2023b; Wang et al., 2023b; Zelikman et al., 2022; Creswell et al., 2022; Yao et al., 2023a) focus on increasing LLM's arithmetic (Lewkowycz et al., 2022; Chen et al., 2022a; Zhou et al., 2022; Taylor et al., 2022), commonsense (Liu et al., 2022; Madaan et al., 2022) and symbolic reasoning (Zhou et al., 2023). Notably, simply adding \u201cLet's think step by step\u201d before each answer or using chain-of-thought (CoT) (Wei et al., 2023) prompting which contains a number of intermediate steps can better elicit LLM's reasoning ability.\\n\\nLLM with tools External tools such as web browsers, search engines, Python interpreters, and models of other modalities have been incorporated to complete complex tasks (Nakano et al., 2023).\"}"}
{"id": "emnlp-2023-main-132", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Our proposed taxonomy for operation and reasoning types in Table QA, accompanied by examples and their proportion in CRT-QA. We emphasize keywords for their respective categories.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. What is the average points of the counties?\\n2. How many players has a higher average score per game than the overall average score?\\n\\n| Player | County | Ave Points |\\n|--------|--------|------------|\\n| p\u00e1draig | offaly | 8.0        |\\n| billy  | kikenny| 7.5        |\\n| christy| kikenny| 7.0        |\\n\\nYou are a data analyst and are good at coming up with insightful questions. Question requirements:\\n\\n1. ...\\n2. ...\\n\\n3.2 Dataset Collection\\n\\nWe select open-domain tables from the TabFact (Chen et al., 2020b) datasets, where the tables are from Wikipedia. Then, inspired by recent works on LLM's ability to aid human annotations (Bartolo et al., 2022; T\u00f6rnberg, 2023), we design a pipeline to efficiently generate multi-step complex reasoning questions by incorporating LLMs and human feedback. After obtaining the questions, we conduct fine-grained annotations on their directness, decomposition types, and human reasoning paths.\\n\\n3.2.1 Human-in-the-loop question generation using LLMs\\n\\nAs shown in Figure 2, the pipeline has two main steps: initially generating queries using LLMs, followed by human selection and feedback to enhance them in accordance with human preferences.\\n\\nInitial question generation\\n\\nInspired by the effectiveness of LLMs' role-playing capability (Park et al., 2023; Wang et al., 2023a; Fu et al., 2023a; Liu et al., 2023), we use LLM (i.e., ChatGPT) as the question generator, which largely reduces the cost of data annotations. Specifically, we design an instructional prompt containing question requirements to generate question candidates. However, there are three problems when we use such prompts for ChatGPT:\\n\\n(i). lack of complexity: Although we provide corresponding instructions on complexity, ChatGPT usually generates simple questions that do not contain multi-hop reasoning;\\n(ii). lack of diversity: When we ask ChatGPT to generate multiple questions, we find that many queries have similar formats. For example, the majority of them start with 'Is there';\\n(iii). unanswerable questions: ChatGPT may generate questions that cannot be answered only given the table. We collect some unanswerable and indeterminate questions and conduct an in-depth analysis in Section 6. The next paragraph described the approach we use to mitigate the above issues.\\n\\nHuman selection and feedback\\n\\nHuman feedback is essential for LLMs because it helps them align with human preferences and values. Inspired by recent works on model refinement (Ouyang et al., 2022; Huang et al., 2022; Shinn et al., 2023), we let human annotators select the questions that meet our requirements and then provide LLM with feedback to improve the quality of the questions.\\n\\nFor feedback design, we use several lexical features such as use math and more complex to resolve the problems mentioned above and reduce potential biases. Empirically, we find that ChatGPT can better improve their generated questions by providing them with specific lexical features than high-level instructions. Details on the feedback design can be found in Appendix A.1.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2.2 Fine-grained annotations\\n\\nAmong the reasoning datasets, most of them only contain label-related annotations without human reasoning paths or fine-grained reasoning types. However, we argue that only goal-oriented annotations are insufficient to analyze the reasoning ability of LLMs. To fill in this gap, after annotating the answer, we further annotate whether a question is implicit or explicit and how sub-questions are composed. We also annotate the main steps of table operations and reasoning. After that, we use a template-filling method to efficiently annotate human reasoning paths to solve the questions. The details on template design and the complete annotation interface can be found in Appendix E and F.\\n\\nDirectness\\n\\nInspired by StrategyQA (Geva et al., 2021), we first introduce implicit questions over tabular data. Following Geva et al., 2021, we use the following rule-of-thumb to determine whether a question is implicit or explicit: the question is explicit if it can be written using words from the question, their inflections, and function words, while implicit questions require new content words to describe the reasoning process.\\n\\nDecomposition types\\n\\nAs the queries in our dataset contain multi-step reasoning, we further annotate how these sub-questions are composed together. Following Min et al., 2019, we categorize the question decomposition into the following 3 types:\\n\\n- **Bridging**: needs to find the first-hop evidence in order to find the second-hop evidence;\\n- **Intersection**: requires finding an entity that meets two independent requirements;\\n- **Comparison**: requires comparing the property of two different entities.\\n\\nOur annotation can be used to analyze LLMs' question decomposition abilities.\\n\\nHuman reasoning path\\n\\nTo better evaluate LLMs' reasoning ability, we further annotate human reasoning paths for solving these queries. However, it is impractical for annotators to write their detailed reasoning paths due to the great volume of data. Hence, we design a template-filling paradigm to let annotators fill the objects of reasoning or operation. We first let annotators select the type of reasoning or operation for each step in order (selections are listed in Table 1). Then, for each step, they are asked to fill in a template. Examples of these 3 decomposition types are in Appendix B.\\n\\n| Property | Value |\\n|----------|-------|\\n| Unique Tables | 423 |\\n| Total Questions | 1000 |\\n| Answerable Questions | 744 |\\n| Unanswerable Questions | 256 |\\n| Question Length (Avg/Median) | 141.2/144.5 |\\n| Answer Length (Avg/Median) | 5.5/3.0 |\\n| Annotation Length (Avg/Median) | 54.3/45.0 |\\n| Rows per Table (Avg/Median) | 12.6/10.0 |\\n| Num of reasoning (Avg/Median) | 3.2/3.3 |\\n| Num of operation (Avg/Median) | 3.1/2.8 |\\n| Length of reasoning path (Avg/Median) | 2.9/3.0 |\\n\\nTable 2: Core Statistics of CRT-QA. Lengths are the number of characters. Both and the complexity (assessed by humans) demonstrate the significant challenge posed by our dataset.\\n\\nComparison to existing datasets\\n\\nCompared with previous datasets, CRT-QA dataset exhibits several distinctive features: (1) CRT-QA is the first Table QA dataset that contains both multi-step operations and informal reasoning. (2) CRT-QA is the first Table QA dataset that contains fine-grained annotations on questions' directness, composition types, and human reasoning paths. (3) CRT-QA comprehensively compare them.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: How many times did the New Jersey Devils win or lose by a one goal margin during the 2007-08 season?\\n\\nPython code:\\n```python\\n# Assuming the table data is in a pandas DataFrame\\n# df is a pandas DataFrame containing the game results\\n\\ntotal_one_goal_games = df[(df['New Jersey Devils'] < 0) | (df['New Jersey Devils'] > 0)].shape[0]\\n\\nprint(f\"There were {total_one_goal_games} games where the New Jersey Devils won or lost by a one goal margin during the 2007-08 season.\\n\\nAnswer:\\nThere were 20 games where the New Jersey Devils won or lost by a one goal margin during the 2007-08 season.\\n```\"}"}
{"id": "emnlp-2023-main-132", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Operation Types | Reasoning Types | Method          | Prompting w/ ChatGPT | Few-shot-CoT | Tool Use w/ GPT-3.5-turbo | Tool Use w/ GPT-4 |\\n|----------------|----------------|-----------------|----------------------|--------------|---------------------------|------------------|\\n| Overall        |                | GRO CAT TEM AGG ARI SPA QUA OTH |                     |              |                           |                  |\\n|                |                |                 | 47.20 50.00 46.34 37.15 46.34 44.44 53.09 36.39 37.68 42.11 | 62.22 52.41 |                           |                  |\\n| Zero-shot      |                | PAL             | 46.51 42.96 51.27 36.39 42.41 11.11 42.59 35.54 41.87 17.54 63.38 45.37 44.11 | 53.19 50.00 |                           |                  |\\n|                |                | ReAct           | 47.69 58.97 51.58 38.21 43.57 22.22 44.44 42.08 22.22 46.77 60.00 46.77 45.24 | 42.71 35.00 |                           |                  |\\n|                |                | ARC (Ours)      | 50.62 43.33 58.20 40.42 47.15 37.04 37.74 44.19 45.46 40.35 64.83 51.15 49.41 | 55.28 52.50 |                           |                  |\\n|                |                |                 | 46.21 34.92 39.74 36.84 39.74 11.11 48.15 34.92 39.74 36.84 60.23 55.16 46.21 | 53.19 50.00 |                           |                  |\\n|                |                | PROMPTING W/ GPT-3.5-TURBO | 46.99 51.28 56.47 35.27 50.46 11.11 48.15 34.92 39.74 36.84 60.23 55.16 46.21 | 43.83 45.00 |                           |                  |\\n|                |                | ARC (Ours)      | 55.28 52.50 64.21 51.77 66.67 60.71 46.56 46.56 52.12 16.67 | 61.20 48.72 |                           |                  |\\n|                |                |                 | 52.88 56.94 45.00 56.94 75.00 44.44 46.95 54.69 57.14 76.86 61.34 58.69 | 61.88 65.00 |                           |                  |\\n\\nTable 3: Evaluation results of various baselines and our method on our proposed CRT-QA:\\n\\n- **GRO**: Grounding\\n- **CAT**: Auto-categorization\\n- **TEM**: Temporal reasoning\\n- **AGG**: aggregating\\n- **ARI**: Arithmetic\\n- **SPA**: Spatial/Geographical reasoning\\n- **QUA**: Reasoning with quantifiers\\n- **OTH**: Other commonsense reasoning.\\n\\nThe mean p-values for the paired t-test between ARC and other top-performing baselines is 0.041, indicating significant differences. Among all the methods except Zero-shot and Zero-shot-CoT, ARC is the only method that requires no handcrafted exemplar.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CoT reasoning ability. (5). Among the 3 tool-use baselines, ReAct can not have comparative performances with the other two methods with GPT-3.5-turbo and GPT-4. By investigating the reasoning path, we find that ReAct often finishes without any answer. Alternatively, ReAct often conducts a substantial number of iterations, resulting in not only increased costs but also an extremely long reasoning pathway that becomes out of control.\\n\\nFrom fine-grained reasoning types shown in Table 3, we observe that all prompting-based methods are bad at aggregation and arithmetic compared with other reasoning types. Noticeably, our proposed ARC and PAL can greatly improve LLMs' ability on these two reasoning types. Besides, we observe that among all the reasoning types, LLMs perform the best in reasoning with quantifiers.\\n\\nDue to page constraints, a comprehensive ablation study on the number of exemplars, error analysis, and case study are in Appendix H, J, and I.\\n\\n6 Unanswerable and Indeterminate Question\\n\\nMost Table QA datasets are designed for answering the questions with golden labeling (Pasupat and Liang, 2015; Chen et al., 2020b, 2021b,a), but real users possibly ask questions that are inherently difficult to answer due to the complexity of the real world. Motivated by this, we incorporate a sub-set of unanswerable and indeterminate queries where some questions go beyond common external knowledge, while others are inherently problematic. We categorize these questions into four categories and conduct the answerability of LLMs based on them.\\n\\n| Type Definition                  | Percentage |\\n|----------------------------------|------------|\\n| Out of scope                     | 73.2%      |\\n| Hallucination                    | 10.4%      |\\n| Problematic                      | 4.8%       |\\n| Subjective                       | 6.4%       |\\n| Others                           | 5.2%       |\\n\\nTable 4: Category and ratio of indeterminate and unanswerable questions in CRT-QA dataset.\\n\\nAs shown in Table 4, out-of-scope, hallucination, and problematic questions are unanswerable. The main reason is the absence of essential information or logical flaws within the question itself. For example, there is an implicit assumption underlying the question \u201cIf the score increase by years, ...\u201d, but the table content cannot support the implied assumption. For indeterminate questions, annotators can yield different answers due to different metrics, algorithms, and criteria. These questions can be answered from both subjective and objective perspectives. A \u201cbest guess\u201d can be made using subjective reasoning, while these questions can also be objectively asked for user's further clarification in certain scenarios.\\n\\nAnswerability Study\\n\\nWe evaluate the language model's ability to determine whether to answer a question under three approaches. As a baseline approach, Random approach randomly predicts the responses under the prior probability distribution. Binary Classification presents a binary classification problem, wherein the model must output with either \u201cunanswerable\u201d or \u201canswerable\u201d. Question Answering approach produces the correct answer if the question is answerable and respond with \u201cunanswerable\u201d otherwise. We conduct these experiments on the whole dataset, including unanswerable/indeterminate and \u201cnormal\u201d questions.\\n\\n| Method                | Acc  | P   | R   | F1  |\\n|-----------------------|------|-----|-----|-----|\\n| Random                | 0.596| 0.731| 0.727| 0.729|\\n| Binary Classification | 0.680| 0.908| 0.637| 0.749|\\n| Question Answering    | 0.779| 0.928| 0.763| 0.838|\\n\\nTable 5: Results for identifying answerability. We report common metrics for binary classification, i.e., Acc (Accuracy), P (Precision), R (Recall) and F1 score.\\n\\nBased on the results presented in Table 5, Binary Classification shows improvements over Random, indicating its effectiveness in identifying questions\u2019 answerability. Question Answering proves to be the most effective approach for identifying answerability, probably because generating answers is easier than determining whether a question can be answered. It mimics some pre-training tasks like reading comprehension. This study benefits a broader understanding of how language models can tackle unanswerable and indeterminate questions and provides directions to enhance performance.\\n\\n7 Conclusion\\n\\nIn this work, to systematically evaluate LLMs' reasoning ability on tabular data, we first establish a comprehensive taxonomy on operation and reasoning types for table analysis. Then, we propose CRT-QA, a dataset of complex reasoning QA over\"}"}
{"id": "emnlp-2023-main-132", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tables. We propose ARC which effectively utilizes table analysis tools to solve table reasoning tasks without manually-annotated exemplars. Extensive experiments show CRT-QA poses a significant challenge for LLMs and our proposed ARC achieves the best EM scores. Besides the main experiments, we also conduct thorough ablation studies, error analyses, answerability study, and case study for further analysis.\\n\\nLimitations\\n(1) CRT-QA is a test-only dataset, which means no gradient updates are performed. While striving for problem complexity, we face challenges in balancing the quantity of our dataset. This is primarily due to the intricate nature of our annotation process, which demands more time for answer generation and fine-grained process labeling. (2) Similar to previous works discussed in Table 11, we only focus on single-table question answering. However, queries across multi-tables are also common in real-life table analysis scenarios. (3) We don\u2019t research the boundary of external knowledge. The appearance of unanswerable and indeterminate questions is associated with our data generation goal, which is to generate complex and diverse questions. Specifically, the indeterminate questions are contrasted with implicit questions, while indeterminate questions stand out beyond implicit questions. We leave this study as future work. (4) In our study, we utilize a combination of exact match and human evaluation as our evaluation metric. It is reasonable because, during the question generation process, we only select the questions that can be answered within several words without ambiguity. Although this is not comprehensive for free-form answer-generation tasks, alternative metrics such as F1, ROUGE-L, and BLEU-1 also possess inherent limitations. Evaluation of the free-form answer-generation task seems promising. Moreover, our fine-grained annotations provide a feasible path to help answer the question. Although the path is not unique. Currently, there is no effective method to evaluate the reasoning path. This aspect will be left for future research and development.\\n\\nReferences\\nSumit Asthana and Aaron Halfaker. 2018. With few eyes, all hoaxes are deep. Proc. ACM Hum.-Comput. Interact., 2(CSCW).\\nMax Bartolo, Tristan Thrush, Sebastian Riedel, Pontus Stenetorp, Robin Jia, and Douwe Kiela. 2022. Models in the loop: Aiding crowdworkers with generative annotation assistants.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\\nWenhu Chen. 2023a. Large language models are few(1)-shot table reasoners.\\nWenhu Chen. 2023b. Large language models are few(1)-shot table reasoners. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1120\u20131130, Dubrovnik, Croatia. Association for Computational Linguistics.\\nWenhu Chen, Ming-Wei Chang, Eva Schlinger, William Wang, and William W. Cohen. 2021a. Open question answering over tables and text.\\nWenhu Chen, Jianshu Chen, Yu Su, Zhiyu Chen, and William Yang Wang. 2020a. Logical natural language generation from open-domain tables.\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2022a. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks.\\nWenhu Chen, Hongmin Wang, Jianshu Chen, Yunkai Zhang, Hong Wang, Shiyang Li, Xiyou Zhou, and William Yang Wang. 2020b. Tabfact: A large-scale dataset for table-based fact verification.\\nWenhu Chen, Hanwen Zha, Zhiyu Chen, Wenhan Xiong, Hong Wang, and William Wang. 2021b. Hybridqa: A dataset of multi-hop question answering over tabular and textual data.\\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2022b. Finqa: A dataset of numerical reasoning over financial data.\\nZhoujun Cheng, Tianbao Xie, Peng Shi, Chengzu Li, Rahul Nadkarni, Yushi Hu, Caiming Xiong, Dragomir Radev, Mari Ostendorf, Luke Zettlemoyer, Noah A. Smith, and Tao Yu. 2023. Binding language models in symbolic languages.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek\"}"}
{"id": "emnlp-2023-main-132", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems.\\n\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. 2022. Selection-inference: Exploiting large language models for interpretable logical reasoning.\\n\\nYao Fu, Hao Peng, Tushar Khot, and Mirella Lapata. 2023a. Improving language model negotiation with self-play and in-context learning from AI feedback.\\n\\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark, and Tushar Khot. 2023b. Complexity-based prompting for multi-step reasoning.\\n\\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. Pal: Program-aided language models.\\n\\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle use a laptop? a question answering benchmark with implicit reasoning strategies.\\n\\nMatthew Ho, Aditya Sharma, Justin Chang, Michael Saxon, Sharon Levy, Yujie Lu, and William Yang Wang. 2022. Wikiwhy: Answering and explaining cause-and-effect questions.\\n\\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin Wu, Xuezhi Wang, Hongkun Yu, and Jiawei Han. 2022. Large language models can self-improve.\\n\\nMohit Iyyer, Wen-tau Yih, and Ming-Wei Chang. 2017. Search-based neural structured learning for sequential question answering. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1821\u20131831, Vancouver, Canada. Association for Computational Linguistics.\\n\\nYannis Katsis, Saneem Chemmengath, Vishwajeet Kumar, Samarth Bharadwaj, Mustafa Canim, Michael Glass, Alfio Gliozzo, Feifei Pan, Jaydeep Sen, Karthik Sankaranarayanan, and Soumen Chakrabarti. 2021. AIT-QA: Question answering dataset over complex tables in the airline industry.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large language models are zero-shot reasoners.\\n\\nAitor Lewkowycz, Anders Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. 2022. Solving quantitative reasoning problems with language models.\\n\\nJiacheng Liu, Skyler Hallinan, Ximing Lu, Pengfei He, Sean Welleck, Hannaneh Hajishirzi, and Yejin Choi. 2022. Rainier: Reinforced knowledge introspector for commonsense question answering.\\n\\nRuibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. 2023. Training socially aligned language models in simulated human society.\\n\\nPan Lu, Baolin Peng, Hao Cheng, Michel Galley, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, and Jianfeng Gao. 2023a. Chameleon: Plug-and-play compositional reasoning with large language models.\\n\\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023b. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.\\n\\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners.\\n\\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2019. Multi-hop reading comprehension through question decomposition and rescoring. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6097\u20136109, Florence, Italy. Association for Computational Linguistics.\\n\\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders,\"}"}
{"id": "emnlp-2023-main-132", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-132", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\n\\nPetter T\u00f6rnberg. 2023. Chatgpt-4 outperforms experts and crowd workers in annotating political twitter messages with zero-shot learning.\\n\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-...e agent with large language models.\\n\\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023b. Self-consistency improves chain of thought reasoning in language models.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. 2022. Emergent abilities of large language models.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. Hotpotqa: A dataset for diverse, explainable multi-hop question answering.\\n\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models.\\n\\nPengcheng Yin, Wen-Ding Li, Kefan Xiao, Abhishek Rao, Yeming Wen, Kensen Shi, Joshua Howland, Paige Bailey, Michele Catasta, Henryk Michalewski, Alex Polozov, and Charles Sutton. 2022. Natural language to code generation in interactive data science notebooks.\\n\\nTao Yu, Rui Zhang, Kai Yang, Michihiro Yasunaga, Dongxu Wang, Zifan Li, James Ma, Irene Li, Qingning Yao, Shanelle Roman, Zilin Zhang, and Dragomir Radev. 2018. Spider: A large-scale human-labeled dataset for complex and cross-domain semantic parsing and text-to-SQL task. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3911\u20133921, Brussels, Belgium. Association for Computational Linguistics.\\n\\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. 2022. Star: Bootstrapping reasoning with reasoning.\\n\\nZhexin Zhang, Jian Guan, Guowei Xu, Yixiang Tian, and Minlie Huang. 2022a. Automatic comment generation for Chinese student narrative essays. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 214\u2013223, Abu Dhabi, UAE. Association for Computational Linguistics.\\n\\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex Smola. 2022b. Automatic chain of thought prompting in large language models.\\n\\nYilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. MultiHiERT: Numerical reasoning over multi-hierarchical tabular and textual data.\\n\\nVictor Zhong, Caiming Xiong, and Richard Socher. 2017. Seq2sql: Generating structured queries from natural language using reinforcement learning.\\n\\nDenny Zhou, Nathanael Sch\u00e4rli, Le Hou, Jason Wei, Nathan Scales, Xuezhi Wang, Dale Schuurmans, Claire Cui, Olivier Bousquet, Quoc Le, and Ed Chi. 2023. Least-to-most prompting enables complex reasoning in large language models.\\n\\nFan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng, Shi Han, and Dongmei Zhang. 2022. Reflection of thought: Inversely eliciting numerical reasoning in language models via solving linear systems.\\n\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.\\n\\nCaleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. 2023. Can large language models transform computational social science?\\n\\n| Hyper-parameter Setting for LLMs |\\n|----------------------------------|\\n| **Table 6** |\\n| **Parameter** | **Value** |\\n| Temperature | 0.7 |\\n| max_len (CoT) | 1024 |\\n| max_len (Code) | 1024 |\\n| max_len (Few-shot/Zero-shot) | 16 |\\n| top_p | 1.0 |\\n| best_of | 1 |\\n\\n| Selection from dev set | Average EM score |\\n|-----------------------|------------------|\\n| Selection 1           | 49.41            |\\n| Selection 2           | 49.89            |\\n| Selection 3           | 48.96            |\\n\\n**Table 7:** Different dev set selections' performance for ARC.\\n\\nA.1 Human feedback for question generation\\n\\nThe following are examples of feedback for LLMs to generate desired questions.\\n\\n- Generate another 10 more complex questions.\\n- Generate another 10 questions with different question types.\\n- Generate another 10 more complex questions that require math to solve them.\"}"}
{"id": "emnlp-2023-main-132", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generate another 10 more complex questions that require common sense for column 1. Other choices may also improve the quality of LLMs' generated questions.\\n\\nA.2 Prompt for Baselines\\nAll prompt designs for the main experiment and experiment in Section 6 can be found in Table 8 and Table 9 respectively.\\n\\nB Question Decomposition Types\\nFollowing Min et al., 2019, we study the following three different types of question decomposition types:\\n\\n- **Bridging**: requires finding first-hop evidence before moving on to the second-hop evidence. Example question: \\\"What was the average number of years between a TV station's affiliation with the Canadian TV system and their eventual disaffiliation?\\\"\\n\\n- **Intersection**: requires finding an entity that meets two independent conditions. Example question: \\\"Are there any counties within the Mid-Indiana Football Conference that contain more than one school?\\\"\\n\\n- **Comparison**: requires comparing the features of two distinct entities. Example question: \\\"How often does Tim Lajcik win fights in the first round compared to subsequent rounds?\\\"\\n\\nC Data Topic Distribution\\nFollowing Parikh et al., 2020, we use Wikimedia Foundation's topic categorization model (Asthana and Halfaker, 2018) to visualize the topic distribution of our dataset. Figure 4 shows that our data are mostly related to sports, biography, regions, and media. Overall, CRT-QA dataset covers a fairly wide range of topic domains.\\n\\nD Question Type Distribution\\nFollowing (Yang et al., 2018), by taking the three neighboring tokens along with the central question word (CQW), we can determine the question types. A visual representation of the distribution is shown in Figure 5, which illustrates the syntactic diversity of questions in our proposed CRT-QA.\\n\\nE Data Annotation Interface\\nFigure 8 shows the detailed interface for data annotation.\\n\\nF Data Annotation Details\\nWe enroll 2 undergraduate students and 1 Ph.D. student majoring in computer science for data annotations. All of them have at least one year of data analysis experience.\\n\\nG Experiment Implementation Details\\nThe models we use for experiments are text-chat-davinci-003, GPT-3.5-turbo, and GPT-4 through Microsoft Azure API. For tool-use baselines, empirically, we find that the LLM-generated code may contain some syntax errors which make it impossible to run and generate output. For these cases, we let LLM re-generate code a maximum of five times. Once it\"}"}
{"id": "emnlp-2023-main-132", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Table**\\n\\n| Rank | Player      | County   | Tally | Total | Matches | Average |\\n|------|-------------|----------|-------|-------|---------|---------|\\n| 0    | P\u00e1draig Horan | Offaly   | 5-17  | 32    | 4       | 8       |\\n| 1    | Billy Fitzpatrick | Kilkenny | 2-24  | 30    | 4       | 7.5     |\\n| 2    | Tony O'Sullivan  | Cork     | 0-28  | 28    | 4       | 7       |\\n| 3    | P J Molloy     | Galway   | 3-11  | 20    | 2       | 10      |\\n| 4    | Christy Heffernan | Kilkenny | 3-9   | 18    | 4       | 4.5     |\\n| 5    | Pat Horgan    | Cork     | 0-18  | 18    | 4       | 4.5     |\\n\\n**Question:** How many players in the 1982 All-Ireland Senior Hurling Championship had a higher average score per game than the overall average score per game of the competition?\\n\\n**Answer:** 4\\n\\n---\\n\\n**Table**\\n\\n| Season | Competition          | Round | Opponent       | Home | Away |\\n|--------|----------------------|-------|----------------|------|------|\\n| 2013-14 | UEFA Europa League | 3Q    | Hapoel Ramat Gan | 0-0  | 1-0  |\\n| 2013-14 | UEFA Europa League | Play-off | Pasching | 2-0  | 2-1  |\\n| 2013-14 | UEFA Europa League | Group H | Seville | 1-2  | -    |\\n| 2013-14 | UEFA Europa League | Group H | Slovan Liberec | -    | 1-2  |\\n| 2013-14 | UEFA Europa League | Group H | Freiburg | -    | 1-1  |\\n\\n**Question:** Was there a correlation between GD Estoril Praia's performance in home games and away games during the 2013-14 UEFA Europa League competition?\\n\\n**Answer:** No\\n\\n---\\n\\n**Table**\\n\\n| Date | City       | Opponent       | Results | Type of Game |\\n|------|------------|----------------|---------|--------------|\\n| April 18 | Belgrade | France | 1:0 | 1966 WCQ |\\n| May 9   | Belgrade | England | 1:1 | Friendly |\\n| June 16 | Oslo, Norway | Norway | 0:3 | 1966 WCQ |\\n| September 4 | Moscow, Russia | USSR | 0:0 | Friendly |\\n| September 19 | Luxembourg | Luxembourg | 5:2 | 1966 WCQ |\\n| October 9 | Paris, France | France | 0:1 | 1966 WCQ |\\n| November 7 | Belgrade | Norway | 1:1 | 1966 WCQ |\\n\\n**Question:** Did the Yugoslavia national football team play any games against teams outside of Europe in the table?\\n\\n**Answer:** Yes\\n\\n---\\n\\n**Table**\\n\\n| Rank | Player      | County   | Tally | Total | Matches | Average |\\n|------|-------------|----------|-------|-------|---------|---------|\\n| 0    | P\u00e1draig Horan | Offaly   | 5-17  | 32    | 4       | 8       |\\n| 1    | Billy Fitzpatrick | Kilkenny | 2-24  | 30    | 4       | 7.5     |\\n| 2    | Tony O'Sullivan  | Cork     | 0-28  | 28    | 4       | 7       |\\n| 3    | P J Molloy     | Galway   | 3-11  | 20    | 2       | 10      |\\n| 4    | Christy Heffernan | Kilkenny | 3-9   | 18    | 4       | 4.5     |\\n| 5    | Pat Horgan    | Cork     | 0-18  | 18    | 4       | 4.5     |\"}"}
{"id": "emnlp-2023-main-132", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"generates runnable code, we execute it and get the output. If the LLM cannot generate runnable code five times, we keep the code in the prompt and set the output to \\\"None\\\". The hyperparameters we use can be found in Table 6.\\n\\nH Ablation Study\\n\\nFor our proposed ARC, we select 3 different examples from the dev set to conduct zero-shot code generation as exemplars for the test set. Table 7 shows that the performance difference among 3 different selections is within 1 EM score demonstrating the robustness of our proposal.\\n\\nWe also design four sets of contrast experiments for the ablation study as Figure 6 shows. We find the table reasoning ability differs from the models. GPT4 is best and turbo performs on par with ChatGPT. For the in-context learning, GPT4 benefits a lot from the increase in the number of demonstrations, but the increase is not significant for other models. We study the impact of up to 2-shot because structured tables consuming lots of tokens can easily break the input limitation.\\n\\nAs expected, different decomposition types vary in difficulty with bridging being the most challenging and comparison being the easiest. Besides, LLMs obtain similar performances on implicit and explicit questions in our Table QA dataset.\\n\\nI Case Study\\n\\nWe show how our method ACR uses external tools to solve table reasoning tasks in Figure 7. The comments in ACR show the reasoning sketch and guide the generation of code. Using external tools enhances numerical computation compared with plain text reasoning. In contrast, CoT fails even with the right reasoning path.\\n\\nJ Error Analyses\\n\\nTo analyze how the error was caused, we randomly choose 50 samples and go depth into error analysis based on the performance of ARC.\\n\\nWe find five types of errors: (1) Code generation error (20%). The code is not executable and the output is none or an illegal type. (2) Gross error of reasoning (32%). The reasoning path deviates from the requirements of the user query. (3) Condition missing error (18%). The code framework has no problem in general, but some subtle conditions or operations are missed. (4) Format error (26%). The model returns with an answer but cannot be judged by the metric. To ease the trouble of format error, we augment the EM with human evaluation. (5) Refuse to answer (4%). The answerable query is regarded as an unanswerable question. The model\u2019s completion contains some expressions that refused to answer like \\\"I am unable to write Python code for this question as the data does not provide information ...\\\". We find that the ARC method of the program enhances numerical processing while weakening the semantic recognition of the text. So the method makes mistakes for the match of strings such as \\\"n/a\\\" and \\\"n/a\\\".\"}"}
