{"id": "acl-2023-long-131", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Better Simultaneous Translation with Monotonic Knowledge Distillation\\n\\nShushu Wang 1, Jing Wu 2, Kai Fan 2, Wei Luo 2, Jun Xiao 1, Zhongqiang Huang 2\\n\\n1 Zhejiang University, 2 Alibaba DAMO Academy\\n\\n1{wangshushu0213, junx}@zju.edu.cn\\n2{wj334275, k.fan, muzhuo.lw, z.huang}@alibaba-inc.com\\n\\nAbstract\\n\\nSimultaneous machine translation (SiMT) presents a unique challenge as it requires generating target tokens before the source sentence is fully consumed. This can lead to the hallucination problem, where target tokens are generated without support from the source sentence. The prefix-to-prefix training data used to train SiMT models are not always parallel, due to divergent word order between the source and target languages, and can contribute to the problem. In this paper, we propose a novel approach that leverages traditional translation models as teachers and employs a two-stage beam search algorithm to generate monotonic yet accurate reference translations for sequence-level knowledge distillation. Experimental results demonstrate the significant improvements achieved by our approach over multiple strong SiMT baselines, leading to new state-of-the-art performance across various language pairs. Notably, when evaluated on a monotonic version of the WMT15 De \u2192 En test set, which includes references generated in a more monotonic style by professional translators, our approach achieves even more substantial improvement over the baselines. The source code and data are publicly available for further exploration.\\n\\n1 Introduction\\n\\nSimultaneous machine translation (SiMT) starts to translate with only a partial observation of the source sentence and can present unique challenges compared to full-sentence translation, particularly when employing offline NMT models. Prefix-to-prefix (P2P) methods such as the wait-k policy (Ma et al., 2019a) have been developed to narrow the gap between training and inference. However, these methods inherently rely on parallelism at the prefix level, which may not always be present in conventional parallel text.\\n\\nFigure 1: An example of a parallel sentence pair, with color-coded parallel clauses. The boxes highlight the prefixes selected based on a wait-3 approach.\\n\\nTrainset\\n\\n\\\\[\\n\\\\begin{array}{cccccc}\\n  k=1 & k=3 & k=5 & k=7 & k=9 \\\\\\\\\\n  \\\\text{WMT15 De \u2192 En} & 30.4 & 15.2 & 8.5 & 5.1 & 3.3 \\\\\\\\\\n  \\\\text{CWMT19 Zh \u2192 En} & 25.4 & 12 & 6.3 & 3.6 & 2.1 \\\\\\\\\\n  \\\\text{IWSLT15 En \u2192 Vi} & 17.3 & 5.2 & 1.9 & 0.8 & 0.4 \\\\\\\\\\n\\\\end{array}\\n\\\\]\\n\\nTable 1: Anticipation rates (AR%) of the original training sets, measuring the percentage of target tokens with a reordering distance \\\\( \\\\geq k \\\\) (see definition in Appendix B).\\n\\nThe parallel text utilized for training offline MT models exhibits a wide range of word reordering between the source and target languages, resulting in non-parallel prefix-to-prefix pairs, as depicted in Figure 1. Table 1 highlights the challenge faced by a wait-\\\\( k \\\\) model, which must predict a significant percentage of target tokens without access to the corresponding words in the source prefix across multiple parallel corpora. For example, when training a wait-3 model on the WMT15 De \u2192 En dataset, the model needs to anticipate 15.2% of the target tokens during training, exacerbating the hallucination problem during inference.\\n\\nAn alternative approach is to train SiMT models on simultaneous interpretation corpora. However, there are two primary issues. First, the available interpretation training data is scant. Second, due to the real-time nature of simultaneous interpretation, the data tends to be overly simplified, making it less ideal for SiMT models where preservation of information is important. On the other hand, traditional parallel data is abundant. If this data could.\"}"}
{"id": "acl-2023-long-131", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"be restructured to more closely follow the source word order, it would be more beneficial for SiMT models. This is the idea behind approaches such as (Chen et al., 2021). In line with this direction, we propose a two-stage beam search algorithm to reconstruct the training data, producing accurate yet monotonic translations. This restructured data is then utilized to train the SiMT model using knowledge distillation (KD) (Kim and Rush, 2016).\\n\\nSimilarly, traditional test sets are less ideal for evaluating SiMT models that produce translations in a more monotonic style. To address this, we constructed a new set of human references for the WMT15 De-En test set that more closely follows the source word order. This new reference can provide a more precise measurement of both translation quality and latency in a SiMT setting.\\n\\nOur primary contributions include:\\n\\n\u2022 We have developed a two-stage beam search algorithm to generate accurate monotonic training data. This algorithm is adjustable for different levels of monotonicity and is capable of leveraging both parallel and monolingual corpora.\\n\\n\u2022 We have curated new human references for the WMT15 De-En test set that is more suitable for evaluating SiMT models. We are pleased to offer these for public access.\\n\\n\u2022 Our empirical results demonstrate that our approach consistently outperforms strong SiMT baselines. We release both code and data to facilitate future research.\\n\\n2 Related Works\\n\\nSiMT Policy\\n\\nThere are two types of SiMT policies: fixed and adaptive. Fixed policies, such as wait-k in Ma et al. (2019a), first READ k source tokens and then alternately READ/WRITE one token. Elbayad et al. (2020) proposed an efficient multipath training for the wait-k policy to randomly sample k during training.\\n\\nAdaptive policies make READ/WRITE decisions dynamically. Gu et al. (2016) decides READ/WRITE actions via reinforcement learning. MILk (Arivazhagan et al., 2019) predicts a Bernoulli variable to determine READ/WRITE actions, which is further implemented into transformer architecture MMA (Ma et al., 2019b). Zheng et al. (2020) developed adaptive wait-k through heuristic ensemble of multiple wait-k models. Miao et al. (2021) proposed a generative framework to generate READ/WRITE decisions. Liu et al. (2021) applies Connectionist Temporal Classification (CTC) by treating the blank symbol as the wait action. Zhang and Feng (2022) develops a READ/WRITE policy by modeling the translation process as information transport and taking the received information as the evidence for READ/WRITE decisions.\\n\\nMonotonic SiMT\\n\\nAnother approach to SiMT is to focus on producing the target as monotonically as possible with the source. Chen et al. (2021) proposed test-time wait-k to produce pseudo-references which are non-anticipatory. Han et al. (2021) proposed a method of chunk-wise reordering to refine the target sentences in an offline corpus and build a monotonically aligned parallel corpus for SimulMT. Deng et al. (2022) proposed a novel monolingual sampling strategy for SiMT, considering both chunk length and monotonicity. Chang et al. (2022) decomposed the translation process into a monotonic translation step and a reordering step, which rearranged the hidden states to produce the order in the target language. Our method extends (Chang et al., 2022) to include a rescoring stage based on the full sentence to produce more accurate translations.\\n\\nKnowledge Distillation in NMT\\n\\nKnowledge distillation (KD) approaches (Hinton et al., 2015) aim to transfer knowledge from a teacher model to a student model. Kim and Rush (2016) first applied knowledge distillation to NMT using sequence-level KD. In terms of online NMT, Zhang et al. (2021b) proposed to use a conventional Transformer as the teacher of the incremental Transformer, and tried to embed future information in the model through knowledge distillation. Ren et al. (2020) proposed to transfer knowledge from the attention matrices of simultaneous NMT and ASR models to a simultaneous speech to text translation system.\\n\\n3 Background\\n\\nOffline NMT\\n\\nOffline NMT models typically employ an encoder-decoder framework. The encoder has access to the full source sentence $x$ and maps it into hidden representations. The decoder autoregressively generates each target token $y_t$ conditioned on $x$ and the previously generated tokens, as shown in Eq. (1):\"}"}
{"id": "acl-2023-long-131", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Simultaneous NMT only has access to part of the source sentence. Let \\\\( g(t) \\\\) be a monotonic non-decreasing function of \\\\( t \\\\) that denotes the number of source tokens processed by the encoder when generating the target word \\\\( y_t \\\\). SiMT uses the source prefix \\\\((x_1, x_2, ..., x_{g(t)})\\\\) to predict \\\\( y_t \\\\) as shown in Eq. (2):\\n\\n\\\\[\\np(y|x; \\\\theta) = |y| \\\\prod_{t=1}^{g(t)} p(y_t|x, y_{<t}; \\\\theta)\\n\\\\]  \\n\\nWe propose two approaches for creating monotonic pseudo-targets for source sentences in traditional parallel data. This new data is then used to train SiMT models through knowledge distillation (KD).\\n\\n### 4.1 Standard KD\\n\\nA simple approach is to use an offline NMT model as a teacher to translate each source sentence of the parallel training data into a pseudo-target through beam search, as shown in Algorithm 2 in Appendix A. The resulting (source, pseudo-target) data adheres more closely to the source word order, as machine-translated sentences tend to have fewer long-distance reorderings. This data is then used to train SiMT models through sequence-level knowledge distillation (KD) (Kim and Rush, 2016), with the training loss represented in Eq. (3):\\n\\n\\\\[\\nL_{\\\\text{seq-}kd} = -\\\\log p(\\\\hat{y}|x; \\\\theta)\\n\\\\]\\n\\nwhere \\\\( \\\\hat{y} \\\\) represents the target predicted by the teacher model. Note that this diverges from conventional sequence-level KD training, which also utilizes the training loss over the original references, as the long-distance reorderings in the original data could be detrimental to the SiMT model.\\n\\n### 4.2 Monotonic KD\\n\\nA key drawback of standard KD is that, although the resulting target translations are more monotonic, they still depend on full sentences, and the degree of monotonicity cannot be controlled. To overcome this limitation, we propose a two-stage beam search strategy to produce target translations in a way similar to real-time simultaneous translation, while also preserving the translation quality. As detailed in Algorithm 1 and depicted in Figure 2, our approach first translates pieces of the source incrementally, akin to a wait-\\\\( k \\\\) policy, and then rescores and selects the better partial hypotheses using a full-sentence offline model.\\n\\nIn Stage 1, the streaming source prefix is fed into the offline teacher model to generate the initial \\\\( b_1 \\\\) partial hypotheses at each beam search step following a wait-\\\\( k \\\\) policy. This stage simulates real-time simultaneous translation with incremental input, and ensures that the decoding is based on local information, thereby increasing monotonicity. By defining the desired latency \\\\( k \\\\), the monotonicity level of the partial hypotheses can be controlled. In Stage 2, we use the teacher model to rescore each of the \\\\( b_1 \\\\) partial hypotheses conditioned on the full source sentence and only keep the top \\\\( b_2 \\\\) (\\\\( b_2 < b_1 \\\\)) partial hypotheses for the next step in the two-stage beam search process. With this strategy, future information in the source sentence is utilized to improve the quality of top partial hypotheses, while also preserving the local word order dictated by the prefix source.\\n\\nNote that we can reverse the translation direction and construct more monotonic pseudo-source given the original target through backward translation. However, empirical results show that it is inferior than forward translation for SiMT (see Figure 13 in Appendix E), probably due to the discrepancy between pseudo-source and normal source text.\"}"}
{"id": "acl-2023-long-131", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: $k$-Anticipation Rates ($AR_k$) of the training data with original references and pseudo-targets generated by our KD methods.\\n\\nFigure 4: Evaluation of offline NMT models in offline (dashed) and simultaneous (solid) scenarios.\\n\\n5 Experiments\\n\\n5.1 SiMT Models\\nWe conduct experiments on three representative modeling approaches that have been used for simultaneous machine translation.\\n\\nOffline MT: a Transformer NMT model (Vaswani et al., 2017) trained on full sentences.\\n\\nMultipath Wait-$k$: a wait-$k$ policy model (El-bayad et al., 2020) trained by randomly sampling different $k$ values between batches during training.\\n\\nITST: an adaptive read/write policy model (Zhang and Feng, 2022) that formulates the translation process as an optimal information transport problem. To the best of our knowledge, ITST is currently the state of the art method for SiMT.\\n\\n5.2 Data\\nWe select three datasets of different language pairs that have been used before for investigations of SiMT models.\\n\\nWMT15 De \u2192 En (Callison-Burch et al., 2009) is a parallel corpus with 4.5M training pairs, which are tokenized and split using 32K BPE merge operations with a shared vocabulary for German and English. We use newstest2013 (3000 sentence pairs) as the development set and report results on newstest2015 (2169 sentence pairs).\\n\\nCWMT19 Zh \u2192 En contains 9.4M sentence pairs in the training set, which are tokenized and split using 32K BPE merge operations for both the source and the target languages. We use the validation set of 956 sentence pairs from BSTC (Zhang et al., 2021a) as the test set.\\n\\nIWSLT15 En \u2192 Vi (Luong and Manning, 2015) contains 133K training pairs. We use TED tst2012 as the validation set (1553 sentence pairs) and TED tst2013 as the test set (1268 sentence pairs). Following the settings in (Ma et al., 2020), we replace rare tokens (frequency < 5) by <unk>. The resulting vocabulary sizes are 17K and 7.7K for English and Vietnamese respectively.\\n\\nFigure 3 compares $AR$ curves at various $k$ values in both the original and the reconstructed training data with pseudo-targets. Our two KD methods\"}"}
{"id": "acl-2023-long-131", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Two-Stage Beam Search\\n\\nInput:\\n- $x$: source sentence\\n- $b_1$: max beam size before rescoring\\n- $b_2$: max beam size after rescoring\\n- $n_{\\\\text{max}}$: max hypothesis length\\n- $k$: fixed latency\\n- $l$: source length $|x|$\\n- $\\\\text{score}(\\cdot, \\\\cdot)$: scoring function\\n\\nOutput:\\n- Best monotonic translation at $k$\\n\\n1. beam format: $\\\\langle \\\\text{score}, \\\\text{hypothesis} \\\\rangle$\\n\\n2. $B_0, B \\\\leftarrow \\\\{ \\\\langle 0, \\\\text{BOS} \\\\rangle \\\\}$\\n\\n3. for $i \\\\in \\\\{1, \\\\cdots, n_{\\\\text{max}}\\\\}$\\n   \\n4. \\\\hspace{1em} $B_{\\\\text{before}}, B_{\\\\text{after}} \\\\leftarrow \\\\emptyset, \\\\emptyset$\\n\\n5. \\\\hspace{2em} for $\\\\langle s, y \\\\rangle \\\\in B_{i-1}$\\n   \\n6. \\\\hspace{3em} if $y$.last() = EOS then\\n5. \\\\hspace{3em} $B.\\\\text{add}(\\\\langle s, y \\\\rangle)$\\n6. \\\\hspace{3em} continue\\n\\n7. \\\\hspace{2em} $l = \\\\min(i + k - 1, |x|)$\\n\\n8. \\\\hspace{2em} for $y \\\\in V$\\n   \\n9. \\\\hspace{3em} // score by partial input\\n   \\n10. \\\\hspace{4em} $s \\\\leftarrow \\\\text{score}(x[:l], y \\\\cdot y)$\\n\\n11. \\\\hspace{4em} $B_{\\\\text{before}}.\\\\text{add}(\\\\langle s, y \\\\cdot y \\\\rangle)$\\n\\n12. \\\\hspace{2em} $B_{\\\\text{before}} \\\\leftarrow B_{\\\\text{before}}.\\\\text{top}(b_1)$\\n\\n13. \\\\hspace{2em} for $\\\\langle s, y \\\\rangle \\\\in B_{\\\\text{before}}$\\n   \\n14. \\\\hspace{3em} // score by oracle input\\n   \\n15. \\\\hspace{4em} $s \\\\leftarrow \\\\text{score}(x, y)$\\n\\n16. \\\\hspace{4em} $B_{\\\\text{after}}.\\\\text{add}(\\\\langle s, y \\\\rangle)$\\n\\n17. \\\\hspace{2em} $B_i \\\\leftarrow B_{\\\\text{after}} \\\\leftarrow B_{\\\\text{after}}.\\\\text{top}(b_2)$\\n\\n18. return $B.\\\\text{max}()$\\n\\n---\\n\\nThe results on full sentences (represented by dashed lines) are derived using greedy search. Note that student models trained on KD-produced data can surpass the teacher model in terms of offline BLEU scores. This can be attributed to the fact that the KD data was generated by the teacher model with a beam size of 5. Essentially, the student models are distilled from a teacher model equipped with beam search and thus can perform better than the same teacher model in greedy search.\"}"}
{"id": "acl-2023-long-131", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Evaluation of multipath wait-\\nk models in offline (dashed) and simultaneous (solid) scenarios.\\n\\nWe train wait-\\nk SiMT models, following (Elbayad et al., 2020), on the original\\ntraining data as well as the reconstructed training\\ndata with pseudo-target produced by the two KD\\nHowever, when both models utilize beam search, the student\\nmodels are likely to lag behind in performance compared to\\nthe teacher model.\\n\\nFigure 7: Evaluation of ITST models in offline (dashed) and simultaneous (solid) scenarios.\\n\\nFinally we train ITST models, following\\nZhang and Feng (2022), to see if our methods can\\nachieve similar improvements with advanced adap-\\ntive read/write models. The results are shown in\\nFigure 7. Similarly, we observe overall improve-\\nment in translation quality by training ITST models\\non the pseudo data. As illustrated in the example\\nin Figure 9, the decoding path of the mono-KD\\ntrained ITST model is closer to the diagonal and its\\ntranslation is more faithful and monotonic to the\\nsource input.\"}"}
{"id": "acl-2023-long-131", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and thus cannot accurately measure improvement in translation quality of more monotonic translations. To test this hypothesis, we took the first 500 pairs from the De$\\\\rightarrow$En test set and commissioned a new set of reference translations that are as monotonic as possible without sacrificing the translation quality. We re-evaluated our De$\\\\rightarrow$En models on this monotonic test set and the results are shown in Figure 8. Compared to the previous results on the original test set, the improvement from the monotonic KD method becomes more prominent, on par with the standard KD method or in many cases outperforming. Moreover, the overall improvement from the KD methods also becomes greater on this monotonic test set. Although the monotonic test set is only a subset of the original test set, the same conclusion holds when only comparing results on this subset (see performance of the multipath wait-$k$ method on the original subset in Figure 14 in Appendix E).\\n\\nFigure 10: Effect of monolingual data on multipath wait-$k$ models on WMT15 De$\\\\rightarrow$En.\\n\\n5.6 Scaling with Monolingual Data\\n\\nGiven that only source sentences are needed for an offline teacher model to produce pseudo-targets, we can expand the KD training data by generating pseudo-targets using monolingual data. We conducted experiments on WMT15 De$\\\\rightarrow$En and collected 1 and 4 times of additional pseudo parallel data using the monotonic KD method on German sentences selected from News Crawl articles, excluding sentences longer than 190 characters. The results with the multipath wait-$k$ model are presented in Figure 10. The improvements from more pseudo data suggest that the ability to use a monolingual source corpus is another advantage of our approach.\\n\\nIn Figure 11, we focus on WMT15 De$\\\\rightarrow$En and demonstrate how our approach can further advance the current state of the art in SiMT. We take ITST, the current SOTA in SiMT, as our modeling method, and compare with ITST and another recent SiMT method wait-info (Zhang et al., 2022a).\"}"}
{"id": "acl-2023-long-131", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Hypotheses | k=1 | k=3 | k=5 | k=7 | k=9 |\\n|------------|-----|-----|-----|-----|-----|\\n| Origin     | 3.1 | 1.8 | 1.3 | 1.1 | 1.0 |\\n| Mono KD    | 2.2 | 1.2 | 1.0 | 0.8 | 0.8 |\\n| KD         | 3.0 | 1.5 | 1.0 | 0.8 | 0.8 |\\n\\nTable 2: HR% of multipath wait-k models on WMT15 De $\\\\rightarrow$ En.\\n\\nFigure 11: Comparison with SOTAs on WMT15 De $\\\\rightarrow$ En.\\n\\nFair comparison, we rerun the original ITST and observe a minor performance dip under high latency conditions. The results show that the monotonic KD method combined with additional monolingual data can achieve new state of the art for SiMT.\\n\\n5.7 Effects on Hallucination\\n\\nHallucination, a known issue in machine translation models, presents significant challenges for real-time simultaneous translation. Hallucination Rate (HR%) (Chen et al., 2021) measures the percentage of words in the target output that are hallucinated (see full definition in Appendix C). We compare the HR% of multipath wait-k models trained on the original parallel data or the pseudo data constructed by the KD methods. As shown in Table 2, the monotonic KD method has the lowest HR% across different latency settings. Examples of hallucination in translation results can be found in Table 6 of Appendix E.\\n\\n6 Discussions\\n\\nThe first beam search stage of our monotonic KD method is equivalent to test-time wait-k inference described in (Chen et al., 2021). This stage, however, may fail to produce accurate rankings of partial hypotheses, given that it relies on offline models for translating partial inputs. The second stage, designed to incorporate full sentence information, is capable of more accurately scoring and ranking these partial hypotheses. We conducted an analysis on the WMT15 De $\\\\rightarrow$ En test set to compare the quality of translations produced by test-time wait-k (i.e., monotonic one-stage beam search) and our monotonic two-stage beam search. As shown in Table 3, the rescoring process in the second stage significantly improves translation quality.\\n\\nTable 4 shows the quality of pseudo-targets generated by standard KD, monotonic one-stage beam search, and monotonic two-stage beam search, measured in BLEU with respect to the original references. Across both De $\\\\rightarrow$ En and En $\\\\rightarrow$ Vi, the standard KD achieves the highest BLEU scores, closely followed by the monotonic KD method that uses two-stage beam search. The one-stage only beam search method results in the lowest translation quality among the three approaches, particularly on De $\\\\rightarrow$ En where the BLEU score is 4 points lower. Figure 12 illustrates the performance of multipath wait-k models trained on the respective training data. The two-stage method consistently outperforms the one-stage method on De $\\\\rightarrow$ En and is better in most latency settings on En $\\\\rightarrow$ Vi. It is notable that the one-stage method leads to substantially inferior SiMT models on De $\\\\rightarrow$ En due to the markedly lower quality of the pseudo-targets.\"}"}
{"id": "acl-2023-long-131", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: Comparison of different KD methods with multipath wait-k models. Experiments on three language pairs demonstrate that this method can consistently improve multiple SiMT models and achieve new state of the art performance for simultaneous translation.\\n\\nLimitations\\nOur monotonic KD approach requires searching for a hyper-parameter $k$ to strike a balance between monotonicity and translation quality for generating pseudo-targets. The current process requires substantial computational resources to determine the optimal value, which may be different depending on the dataset. More studies are needed to establish an efficient method.\\n\\nAcknowledgements\\nWe would like to thank all the anonymous reviewers for the insightful and helpful comments. This work was supported by Alibaba Research Intern Program, the National Key Research & Development Project of China (2021ZD0110700), the National Natural Science Foundation of China (U19B2043, 61976185), and the Fundamental Research Funds for the Central Universities (226-2022-00051). This work was done during the first author's internship at Alibaba DAMO Academy.\\n\\nReferences\\nNaveen Arivazhagan, Colin Cherry, Wolfgang Macherey, Chung-Cheng Chiu, Semih Yavuz, Ruoming Pang, Wei Li, and Colin Raffel. 2019. Monotonic infinite lookback attention for simultaneous machine translation. arXiv preprint arXiv:1906.05218.\\n\\nChris Callison-Burch, Philipp Koehn, Christof Monz, and Josh Schroeder. 2009. Findings of the 2009 Workshop on Statistical Machine Translation. In Proceedings of the Fourth Workshop on Statistical Machine Translation, pages 1\u201328, Athens, Greece. Association for Computational Linguistics.\\n\\nChih-Chiang Chang, Shun-Po Chuang, and Hung-yi Lee. 2022. Anticipation-free training for simultaneous machine translation. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 43\u201361.\\n\\nJunkun Chen, Renjie Zheng, Atsuhito Kita, Mingbo Ma, and Liang Huang. 2021. Improving simultaneous translation by incorporating pseudo-references with fewer reorderings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5857\u20135864.\\n\\nHexuan Deng, Liang Ding, Xuebo Liu, Meishan Zhang, Dacheng Tao, and Min Zhang. 2022. Improving simultaneous machine translation with monolingual data. arXiv preprint arXiv:2212.01188.\\n\\nChris Dyer, Victor Chahuneau, and Noah A Smith. 2013. A simple, fast, and effective reparameterization of IBM model 2. In Proceedings of the 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 644\u2013648.\\n\\nMaha Elbayad, Laurent Besacier, and Jakob Verbeek. 2020. Efficient wait-k models for simultaneous machine translation. arXiv preprint arXiv:2005.08595.\\n\\nJiatao Gu, Graham Neubig, Kyunghyun Cho, and Victor OK Li. 2016. Learning to translate in real-time with neural machine translation. arXiv preprint arXiv:1610.00388.\\n\\nHyojung Han, Seokchan Ahn, Yoonjung Choi, Insoo Chung, Sangha Kim, and Kyunghyun Cho. 2021. Monotonic simultaneous translation with chunk-wise reordering and refinement. arXiv preprint arXiv:2110.09646.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\\n\\nYoon Kim and Alexander M Rush. 2016. Sequence-level knowledge distillation. arXiv preprint arXiv:1606.07947.\\n\\nDan Liu, Mengge Du, Xiaoxi Li, Ya Li, and Enhong Chen. 2021. Cross attention augmented transducer networks for simultaneous translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 39\u201355.\\n\\nMinh-Thang Luong and Christopher D Manning. 2015. Stanford neural machine translation systems for spoken language domains. In Proceedings of the 12th International Workshop on Spoken Language Translation: Evaluation Campaign.\"}"}
{"id": "acl-2023-long-131", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mingbo Ma, Liang Huang, Hao Xiong, Renjie Zheng, Kaibo Liu, Baigong Zheng, Chuanqiang Zhang, Zhongjun He, Hairong Liu, Xing Li, et al. 2019a. Stacl: Simultaneous translation with implicit anticipation and controllable latency using prefix-to-prefix framework. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3025\u20133036.\\n\\nXutai Ma, Juan Pino, James Cross, Liezl Puzon, and Jiatao Gu. 2019b. Monotonic multihead attention. arXiv preprint arXiv:1909.12406.\\n\\nXutai Ma, Juan Miguel Pino, James Cross, Liezl Puzon, and Jiatao Gu. 2020. Monotonic multihead attention. In International Conference on Learning Representations.\\n\\nYishu Miao, Phil Blunsom, and Lucia Specia. 2021. A generative framework for simultaneous machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6697\u20136706.\\n\\nYi Ren, Jinglin Liu, Xu Tan, Chen Zhang, Tao Qin, Zhou Zhao, and Tie-Yan Liu. 2020. Simulspeech: End-to-end simultaneous speech to text translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3787\u20133796.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nRuiqing Zhang, Xiyang Wang, Chuanqiang Zhang, Zhongjun He, Hua Wu, Zhi Li, Haifeng Wang, Ying Chen, and Qinfei Li. 2021a. Bstc: A large-scale Chinese-English speech translation dataset. arXiv preprint arXiv:2104.03575.\\n\\nShaolei Zhang and Yang Feng. 2022. Information-transport-based policy for simultaneous translation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, Online and Abu Dhabi. Association for Computational Linguistics.\\n\\nShaolei Zhang, Yang Feng, and Liangyou Li. 2021b. Future-guided incremental transformer for simultaneous translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 14428\u201314436.\\n\\nShaolei Zhang, Shoutao Guo, and Yang Feng. 2022a. Wait-info policy: Balancing source and target at information level for simultaneous machine translation.\\n\\nShaolei Zhang, Shoutao Guo, and Yang Feng. 2022b. Wait-info policy: Balancing source and target at information level for simultaneous machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2022, Online and Abu Dhabi. Association for Computational Linguistics.\\n\\nBaigong Zheng, Kaibo Liu, Renjie Zheng, Mingbo Ma, Hairong Liu, and Liang Huang. 2020. Simultaneous translation policies: From fixed to adaptive. arXiv preprint arXiv:2004.13169.\"}"}
{"id": "acl-2023-long-131", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: Standard Beam Search\\n\\nInput: $x$: source sentence\\n$b$: max beam size\\n$n_{\\\\text{max}}$: max hypothesis length\\n$\\\\text{score}(\\cdot, \\cdot)$: scoring function\\n\\nOutput: Best hypothesis\\n\\n1. $B_0 \\\\leftarrow \\\\{<0, \\\\text{BOS}>\\\\}$\\n2. for $i \\\\in \\\\{1, \\\\cdots, n_{\\\\text{max}}\\\\}$ do\\n3.   $B \\\\leftarrow \\\\emptyset$\\n4.   for $<s, y> \\\\in B_{i-1}$ do\\n5.     if $y$.last() = $\\\\text{EOS}$ then\\n6.       $B$.add($<s, y>$)\\n7.       continue\\n8.     for $y \\\\in V$ do\\n9.       $s \\\\leftarrow \\\\text{score}(x, y \\\\circ y)$\\n10.      $B$.add($<s, y \\\\circ y>$)\\n11.     $B_{i} \\\\leftarrow B$.top($b$)\\n12.    return $B$.max()\\n\\nB Anticipation Rate of (Pseudo-)Refs\\nDuring the training of a simultaneous translation model, an anticipation happens when a target word is generated before the corresponding source word is encoded. To identify the anticipations, we need the word alignment between the parallel sentences.\\n\\nWe use fast-align in our experiments (Dyer et al., 2013) to get a word alignment $a$ between a source sentence $x$ and a target sentence $y$. It is a set of source-target word index pairs $(s, t)$ where the $s$th source word $x_s$ aligns with the $t$th target word $y_t$.\\n\\nFormally, a target word $y_t$ is $k$-anticipated ($A_k(t, a) = 1$) if it aligns to at least one source word $x_s$ where $s \\\\geq t + k$:\\n\\n$A_k(t, a) = 1 \\\\left[ \\\\{ (s, t) \\\\in a \\\\mid s \\\\geq t + k \\\\} \\\\neq \\\\emptyset \\\\right]$\\n\\nThe $k$-anticipation rate ($\\\\text{AR}_k$) of an $(x, y, a)$ triple is further defined under wait-$k$ policy:\\n\\n$\\\\text{AR}_k(x, y, a) = \\\\frac{1}{|y|} \\\\sum_{t=1}^{\\\\text{|y|}} A_k(t, a)$\\n\\nC Hallucination Rate of Hypotheses\\nHR is defined to quantify the number of hallucinations in decoding. A target word $\\\\hat{y}_t$ is a hallucination if it cannot be aligned to any source word.\\n\\nFormally, based on word alignment $a$, whether target word $\\\\hat{y}_t$ is a hallucination is:\\n\\n$H(t, a) = 1 \\\\left[ \\\\{ (s, t) \\\\in a \\\\} = \\\\emptyset \\\\right]$\\n\\nHallucination rate $\\\\text{HR}$ is further defined as:\\n\\n$\\\\text{HR}(x, \\\\hat{y}, a) = \\\\frac{1}{|\\\\hat{y}|} \\\\sum_{t=1}^{|\\\\hat{y}|} H(t, a)$\\n\\nD WMT15 De $\\\\rightarrow$ En Test Set Annotations\\nIn order to properly evaluate the quality of SiMT, we expect to remove the long-distance reorderings in the test set. So we ask the professional interpreters to rephrase the references in the test set of WMT15 De $\\\\rightarrow$ En into simultaneous style. We hired two profession interpreters and spent 888 US dollars in total to get the monotonic test set. The annotation guidelines we provided with them are as follows:\\n\\n\u2022 A monotonic translation should be faithful and fluent, following common practices in professional translation of sentences, without adding, deleting, or substituting meaningful information in the source sentence. The original professional translations are provided for reference only and annotators should feel free to start from scratch, or reuse the original translation and make necessary edits, in order to produce a monotonic translation that is faithful and fluent.\\n\\n\u2022 A monotonic translation should reduce long distance reordering between words and try to emulate the word order in the source language if possible, under the requirement of criterion 1.\\n\\n\u2022 While it can be difficult and time-consuming to come up with the best monotonic translation for a source sentence, we require reasonable effort to create a more monotonic translation that is quantitatively better than the original translation according to criterion 2, unless the original translation is already monotonic.\\n\\n\u2022 There may exist multiple monotonic translations for a source sentence with varying degrees of monotonicity. We require reasonable effort to create a more monotonic translation but it does not need to be the most monotonic translation. We welcome diversity in monotonic translation and would collect multiple versions of monotonic translations from different in-house and external professional translators.\"}"}
{"id": "acl-2023-long-131", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Additional Training Details and Experimental Results\\n\\n### Figure 13: Evaluation of back-translation on multipath wait-$k$ models on WMT15 De\u2192En. We re-generate monotonic source input by standard beam search and trained a multipath wait-$k$ model on it.\\n\\n### Figure 14: Evaluation of multipath wait-$k$ models on the original first 500 pairs test set of WMT15 De\u2192En.\\n\\n| Multipath Wait-$k$ De-En | origin | mono KD | mono KD +monol*1 | mono KD +monol*4 |\\n|-------------------------|--------|---------|------------------|------------------|\\n| AL BLEU                 |        |         |                  |                  |\\n| 3                       | 2.12   | 26.21   | 2.23             | 2.22             |\\n| 5                       | 4.09   | 28.53   | 4.41             | 4.49             |\\n| 7                       | 6.03   | 29.72   | 6.34             | 6.39             |\\n| 9                       | 7.90   | 30.69   | 8.19             | 8.23             |\\n| 11                      | 9.72   | 31.11   | 10.00            | 10.03            |\\n| 13                      | 11.42  | 31.41   | 11.72            | 11.77            |\\n| +\u221e                      | -       | 32.25   | -                | -                |\\n\\n| ITST De-En             | origin | delta   | mono KD | mono KD +monol*1 | mono KD +monol*4 |\\n|------------------------|--------|---------|---------|------------------|------------------|\\n| AL BLEU                |        |         |         |                  |                  |\\n| 0.2                    | 2.15   | 24.88   | 2.15   | 2.13             |\\n| 0.3                    | 2.69   | 28.25   | 2.45   | 2.33             |\\n| 0.4                    | 3.74   | 29.50   | 3.16   | 2.89             |\\n| 0.5                    | 5.28   | 30.54   | 4.34   | 3.85             |\\n| 0.6                    | 7.21   | 31.00   | 6.17   | 5.42             |\\n| 0.7                    | 9.50   | 31.22   | 8.59   | 7.80             |\\n| 0.8                    | 12.39  | 31.21   | 12.09  | 11.59            |\\n| +\u221e                     | -       | 32.25   | -       | -                |\\n\\nTable 5: Numerical Results in figure 10 and figure 11.\"}"}
{"id": "acl-2023-long-131", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The second feedback function is intervening in NLU results.\\n\\nWhat happened during this dialogue?\\n\\nI think from my perspective, from our perspective, it is about time.\\n\\nSo we are only permitted to use digital products without any gaming functions.\"}"}
{"id": "acl-2023-long-131", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|      | Material | Method | Evaluation   |\\n|------|----------|--------|--------------|\\n|      | mono     | KD     |              |\\n|      | delta    | AL BLEU|              |\\n| 0.2  | 2.15     | 24.88  |              |\\n|      | 2.18     | 30.00  |              |\\n|      | 1.71     | 12.11  |              |\\n|      | 2.53     | 27.36  |              |\\n| 0.3  | 2.69     | 28.25  |              |\\n|      | 2.74     | 32.34  |              |\\n|      | 2.21     | 13.45  |              |\\n|      | 3.68     | 29.50  |              |\\n| 0.4  | 3.74     | 29.50  |              |\\n|      | 3.79     | 33.42  |              |\\n|      | 2.90     | 14.79  |              |\\n|      | 5.49     | 29.83  |              |\\n| 0.5  | 5.28     | 30.54  |              |\\n|      | 5.39     | 33.75  |              |\\n|      | 3.83     | 15.71  |              |\\n|      | 7.12     | 30.12  |              |\\n| 0.6  | 7.21     | 31.00  |              |\\n|      | 7.48     | 33.93  |              |\\n|      | 4.97     | 16.21  |              |\\n|      | 9.02     | 30.16  |              |\\n| 0.7  | 9.50     | 31.22  |              |\\n|      | 9.85     | 33.84  |              |\\n|      | 6.35     | 16.87  |              |\\n|      |          |        |              |\\n| 0.8  | 12.39    | 31.21  |              |\\n|      | 13.05    | 33.81  |              |\\n|      | 7.90     | 16.95  |              |\\n|      |          |        |              |\\n\\nTable 7: Numerical Results in figure 6, figure 7 and figure 8.\"}"}
{"id": "acl-2023-long-131", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?  \\n\u25a1 A2. Did you discuss any potential risks of your work?  \\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?  \\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB \u25a1 Did you use or create scientific artifacts?  \\n\u25a1 B1. Did you cite the creators of artifacts you used?  \\n\u25a1 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?  \\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?  \\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect / anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?  \\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits, etc. for the data that you used / created? Even for commonly-used benchmark datasets, include the number of examples in train / validation / test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC \u25a1 Did you run computational experiments?  \\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-131", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
