{"id": "lrec-2024-main-1018", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MoZIP: A Multilingual Benchmark to Evaluate Large Language Models in Intellectual Property\\n\\nShiwen Ni, Minghuan Tan, Yuelin Bai, Fuqiang Niu, Min Yang, Bowen Zhang, Ruifeng Xu, Xiaojun Chen, Ye Li, Jianping Fan\\n\\n1 Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences\\n2 Shenzhen Technology University\\n3 Shenzhen University\\n4 Harbin Institute of Technology, Shenzhen\\n\\n{sw.ni, mh.tan, yl.bai, min.yang, ye.li, jp.fan}@siat.ac.cn, nfq729@gmail.com, zhang_bo_wen@foxmail.com, xuruifeng@hit.edu.cn, xjchen@szu.edu.cn\\n\\nAbstract\\n\\nLarge language models (LLMs) have demonstrated impressive performance in various natural language processing (NLP) tasks. However, there is limited understanding of how well LLMs perform in specific domains (e.g., the intellectual property (IP) domain). In this paper, we contribute a new benchmark, the first Multilingual-oriented Quiz on Intellectual Property (MoZIP), for the evaluation of LLMs in the IP domain. The MoZIP benchmark includes three challenging tasks: IP multiple-choice quiz (IPQuiz), IP question answering (IPQA), and patent matching (PatentMatch). In addition, we also develop a new IP-oriented multilingual large language model (called MoZi), which is a BLOOMZ-based model that has been supervised fine-tuned with multilingual IP-related text data. We evaluate our proposed MoZi model and four well-known LLMs (i.e., BLOOMZ, BELLE, ChatGLM and ChatGPT) on the MoZIP benchmark. Experimental results demonstrate that MoZi outperforms BLOOMZ, BELLE and ChatGLM by a noticeable margin, while it had lower scores compared with ChatGPT. Notably, the performance of current LLMs on the MoZIP benchmark has much room for improvement, and even the most powerful ChatGPT does not reach the passing level. Our source code, data, and models are available at https://github.com/AI-for-Science/MoZi.\\n\\nKeywords: Intellectual property, benchmark, large language model, multilingual\\n\\n1. Introduction\\n\\nWith the development of Large Language Models (LLMs) (Workshop et al., 2022; OpenAI, 2023; Zeng et al., 2023; Touvron et al., 2023), AI-assisted agents are showing increasing abilities in understanding natural language and manipulating well-formatted text drafted by humans. Recent studies show that LLMs supervised fine-tuned on domain-specific data achieve significant progress in a wide range of fields, such as Finance (Wu et al., 2023), Law (Liu et al., 2023; Huang et al., 2023a), Medicine (Zhang et al., 2023a; Wang et al., 2023) and Programming (Li et al., 2022). As large models have exhibited remarkable versatility in our daily lives, it is crucial to effectively evaluate their abilities in handling specific tasks and identify potential shortcomings. Recently, several benchmarks were proposed to evaluate the large models from different perspectives such as factual consistency (Laban et al., 2023), question answering in the medical field (Singhal et al., 2022), the fairness of recommendation (Zhang et al., 2023b), the programming ability (Chen et al., 2021), and comprehensive evaluation in general fields (Huang et al., 2023b).\\n\\nHowever, we notice that in the area of AI for science, the protection and inspiration for creativity are still overlooked by the community. Intellectual Property (IP) has been a widely-used term to address rights in encouraging innovation and creativity. Since 2000, the World Intellectual Property Organization (WIPO) has established World Intellectual Property Day to \u201craise awareness of how patents, copyright, trademarks and designs impact on daily life\u201d and \u201cto celebrate creativity, and the contribution made by creators and innovators to the development of economies and societies across the globe\u201d (WIPO, 2011). Despite the impressive capabilities of LLMs in natural language understanding and generation, it remains a challenge to explore to what extent LLMs can understand innovative ideas, cutting-edge creations, and their protections against infringements.\\n\\nAs far as we are concerned, the major challenges to developing LLMs in the IP domain are two-fold. First, due to the wide coverage of IP rights, there is still a lack of benchmarks evaluating how LLMs understand IP-related concepts and regulations. The major IP rights include Patents, Trademarks, Industrial Designs, Geographical Indications, Copyright and Trade Secrets. Unfortunately, existing benchmarks in QA or Laws do not focus on these directions. Second, despite the great demand for people from different professions, there are still obstacles to acquiring highly relevant information and protecting potential IPs using appropriate strategies.\"}"}
{"id": "lrec-2024-main-1018", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gies. According to WIPO PCT Yearly Review 2022, the top 50 PCT geographical clusters accounted for nearly 60% of total PCT filings. As a result, it is essential to develop an IP-oriented LLM and construct a benchmark for evaluating the performance of different LLMs.\\n\\nTo make fair comparisons of LLMs over IP knowledge, we propose a multilingual benchmark called MoZIP (Multilingual-oriented Quiz for Intellectual Property). The MoZIP benchmark consists of three datasets, IPQuiz, IPQA, and PatentMatch. Since IP rights are protected under IP systems, a prior requirement for the LLMs is domain-specific knowledge about terminologies and regulations of IP systems. To consider the above knowledge, we first construct the IPQuiz dataset, which consists of 2000 multilingual multiple-choice questions on IP knowledge. These questions are gathered from online IP knowledge tests from different countries and languages. We also pay special attention to frequently asked questions on websites of IP-related organizations and agencies. These questions contain important information that IP consumers care about. From these questions, we select 100 items to form the IPQA test set. Among all the highlighted rights of IP, patents encourage the development of innovations and new technologies in every field. However, there have been numerous new patents being submitted each day that searching or indexing similar patents has been a challenge for current systems. We collect patent documents covering different languages across the globe to help models learn how patents are drafted and innovations are described. Based on this, we further construct the PatentMatch dataset to test how models may differentiate patents based on the descriptions.\\n\\nIn this paper, we also develop our IP-oriented LLM MoZi. The model is fine-tuned from BLOOMZ-MT-7B through three stages. For the first stage, we use 24 million official patent documents to make the model aware of how patents are usually crafted. For the second stage, we conduct 3 million instruction fine-tuning using various instruction types across multiple fields. In the last stage, 58k IP instruction fine-tuning data constructed by ourselves is used to enable the model to learn about IP knowledge. We conducted experiments based on five LLMs of MoZi, ChatGPT, ChatGLM, BELLE, and BLOOMZ and evaluated each model on our benchmark MoZIP. Our experiments show that ChatGPT performs best overall, followed by the other 6-7b parameter number models in which MoZi performs best. Overall, there is still much room for improvement in the current LLMs' underperformance on MoZIP. The contributions of this paper to the community are summarized as follows:\\n\\n\u2022 This paper presents MoZIP, the first IP benchmark covering nine languages for evaluating the capabilities of large language models in the IP domain.\\n\u2022 In this work, we propose the first IP-oriented multilingual large language model MoZi, and experimental results on the MoZIP benchmark show that MoZi performs the best among models of the same parameter level.\\n\u2022 We conducted a comprehensive evaluation using five LLMs on the MoZIP benchmark, and the experimental results show the challenge of the MoZIP benchmark and illustrate the deficiencies of LLMs in the IP field at this stage.\\n\u2022 In order to contribute to the development of LLMs in intellectual property, we have made available source code, MoZIP benchmark, instruction fine-tuning data, and MoZi model.\\n\\n2. Primary data collection\\nTo facilitate the construction of an IP-oriented benchmark, we collect different categories of data from multiple resources, including IP acts, patents as well as frequently asked questions related to IP. These data can be used as instructions for fine-tuning to inject IP-related knowledge into LLMs.\\n\\nIPACT To protect IP rights, there has been plenty of legal information on intellectual property available online, including IP laws and regulations, WIPO-administered and IP related treaties, and leading judicial decisions on IP. This part of the data is basically objective knowledge. LLMs need to access such resources to acquire knowledge about mechanisms of IP systems.\\n\\nIPFAQ Frequently Asked Questions (FAQs) relating to IP usually contain basic knowledge about IP rights and important information that IP customers care most about but likely to misunderstand. We crawl FAQs from worldwide websites of IP organizations and agencies. Their answers are officially provided with guaranteed reliability and accuracy. The dataset therefore contains questions asked by people from different countries and answers given online.\\n\\nPatent A patent is an exclusive right granted for an invention, which is a product or a process that provides. (WIPO, 2011) Generally speaking, a patent needs to propose a new way of doing something, or offer a new technical solution to a problem. In order to obtain a patent, one must reveal technical details about their invention in a patent application that is made available to the public. We gather patent documents from a database of CNIPA. Figure 1 shows an example of a patent.\"}"}
{"id": "lrec-2024-main-1018", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"document. Each patent document contains different fields of data, like country of the patent, \\npatent type, public number, claims, International Patent Classification (IPC) information and description. \\nThe patents may be written in different languages regardless of the country of the patent. We use the \\nclaims field and the description field as language indicators. If the two fields are empty (about half of the Patent data has this issue), the patent is not used for language detection. We adopt a language \\ndetector from FastText (Joulin et al., 2016a, b) to categorize the patents into different language groups. \\n\\nThe right of Figure 1 shows the statistics for country and language distributions for all used patents.\\n\\n3. Benchmark\\n\\nThe MoZIP benchmark contains IPQuiz, IPQA and PatentMatch, see Table 1.\\n\\nIPQuiz\\n\\nWe construct the IPQuiz dataset to evaluate if models understand IP-related concepts and regulations. IPQuiz is a multiple-choice question answering dataset gathered from publicly accessible websites of various languages across the globe. For each question, the model needs to choose a response from a list of candidates, see Figure 2. The dataset is keeping increasing in size and coverage of languages. We will host the dataset on HuggingFace's datasets website. For the initial version of IPQuiz, it contains 2k questions with seven languages. Figure 1 shows the distribution of languages for the initial version of the dataset.\\n\\nIPQA\\n\\nWe reserve 100 questions from IPFAQ as test set to evaluate how LLMs understand IP-related questions. The IPQA contains questions in seven languages, and the 100 data items include 35 each in Chinese and English, and 6 each in Spanish, Japanese, German, French, and Russian. The specific IP-questions are shown in Figure 3. Their responses are further judged by human annotators to compare the quality.\\n\\nTable 1: Statistics for each dataset in MoZIP benchmark. IPQuiz-XL includes DE, ES, JP, KO and PT. IPQA-XL includes ES, JP, DE, FR, RU. M-C: Multiple-choice.\"}"}
{"id": "lrec-2024-main-1018", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Examples of questions in IPQuiz. The words in blue below non-English content are the corresponding English translations.\\n\\nDE: K\u00f6nnen Minderj\u00e4hrige Erfindungen zum Patent anmelden?\\n[Can minors file patent applications for inventions?]\\n\\nEN: How much does it cost to patent an invention?\\n\\nES: \u00bfEn qu\u00e9 consiste la exclusividad de la PI?\\n[What does IP exclusivity consist of?]\\n\\nFR: Quelle est la dur\u00e9e de la protection par brevet?\\n[How long does patent protection last?]\\n\\nJP: \u5927\u5b66\u3068\u4f01\u696d\u304c\u5171\u540c\u7814\u7a76\u3092\u884c\u3046\u5834\u5408\u306b\u77e5\u7684\u8ca1\u7523\u306e\u6271\u3044\u306f\u3069\u3046\u306a\u308a\u307e\u3059\u304b\u3002[How is intellectual property treated when a university and a company conduct joint research?]\\n\\nRU: \u042f\u0432\u043b\u044f\u044e\u0442\u0441\u044f \u043b\u0438 \u043f\u0430\u0442\u0435\u043d\u0442\u044b \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u044b\u043c \u0441\u0440\u0435\u0434\u0441\u0442\u0432\u043e\u043c \u043e\u0445\u0440\u0430\u043d\u044b \u0438\u0437\u043e\u0431\u0440\u0435\u0442\u0435\u043d\u0438\u0439?\\n[Are patents the only means of protecting inventions?]\\n\\nZH: \u83b7\u5f97\u4e86\u4e13\u5229\u8bc1\u4e66\uff0c\u5c31\u8868\u793a\u4f60\u5fc5\u7136\u4f1a\u62e5\u6709\u8be5\u4e13\u5229\u6743\u4e86\u5417?\\n[Does obtaining a patent certificate mean that you will necessarily own the patent?]\\n\\nWe construct PatentMatch to assess whether the model truly comprehends the inventions described in patent documents and accurately differentiates between different patents. We first create a parallel dataset of 250k patents granted by WIPO from 2010 to 2022 collected from Google Public Datasets, with both Chinese and English abstracts extracted from the original documents. These contents are written by the patent applicants, ensuring the accuracy of the descriptions.\\n\\nWe construct this sub-task following the steps below:\\n\\n\u2022 First, we leverage the Pyserini toolkit (Lin et al., 2021) to build a BM25 database. And we use OpenAI API (text-embedding-ada-002 model) to build a dense vector database based on Pinecone with cosine similarity metric.\\n\\n\u2022 Then, we select 500 patents across various technical domains based on 8 sections from the IPC classification system to construct 1000 multiple-choice questions (500 questions in each language). Next, we use the abstracts of these patents as queries to retrieve the top-k results from both the BM25 database and the dense vector database.\\n\\n\u2022 As our setting is to select the most relevant patent from the options, we follow the following steps to construct these choices. (1) To determine the accurate answer, we choose patents that belong to the same IPC subgroup as the patent mentioned in the question. We then make sure these patents have a low BM25 ranking but a high ranking in vector-based assessments. This setup allows us to assess whether the model can effectively recognize similar patents even when there are fewer words.\\n\\nGuides for embeddings: https://platform.openai.com/docs/guides/embeddings\\n\\nPinecone: https://www.pinecone.io/\"}"}
{"id": "lrec-2024-main-1018", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Please select the most similar patent number from A, B, C and D. Which number is?\\n\\nA large foldable four-ridge-eight-row peanut planter comprises a mounting frame (1) and a planter main frame (26), and also comprises a parallelogram-shaped hydraulic folding beam frame (2). The mounting frame (1) is fixedly disposed at a front end of the parallelogram-shaped hydraulic folding beam frame (2), and the planter main frame (26) is disposed behind the parallelogram-shaped hydraulic folding beam frame (2).\\n\\nThe planter has a compact structure, complete functions, high operation efficiency and good ground-contour following capability, and integrates multiple processes.\\n\\nDisclosed are a weeding machine and weeding method for seedlings in a paddy field. The weeding machine comprises a paddy field power chassis (1), a lifting hydraulic cylinder (2), a parallelogram suspension frame (3), a weeding machine frame (5), a transmission assembly and a weeding unit, wherein the paddy field power chassis vertically adjusts the working depth of the weeding machine by means of the parallelogram suspension frame and the lifting hydraulic cylinder; the transmission assembly transmits power to the weeding unit; the weeding unit is used for pulling inter-row weeds and feeding same into a weed chopping channel; ... According to the weeding machine, the inter-plant weeds are removed in a weeding-chopping-burying working process, which can reserve seedlings, and achieve a thorough weeding operation, a high working efficiency and a good weeding effect.\\n\\nDisclosed is a planter for planting crops, the planter comprising a seed box (1), a rotary cultivator A (2) and a rotary cultivator B (3), wherein the seed box (1), the rotary cultivaor A (2) and the rotary cultivaor B (3) are all mounted on a support beam (4); the rotary cultivator A (2) is located between the seed box (1) and the rotary cultivator B (3), and the rotary cultivator B (3) is located at the right of the rotary cultivator A (2); a lower end of the seed box (1) is connected to a seeding device (13); ... arranged in the seeding device (13) can allow seeds to be seeded evenly, ensuring that the crop seeds are seeded evenly in the land and allowing crops to grow more evenly, which is practical and suitable for extensive promotion and use.\\n\\nA method and device for controlling the seeding depth of a no-tillage seeder. The device is disposed on the no-tillage seeder. The device for controlling the seeding depth of the no-tillage seeder comprises: an overall control system (1); a detection system, comprising a furrowing depth detecting unit (21), a pressing force sensor (22), a compaction force sensor (23) and a data acquisition unit (24); and a regulating system, comprising a hydraulic controller (31), a hydraulic valve group (32), a pressing oil cylinder (33) and a compacting oil cylinder (34).\\n\\nA multifunction foldable bed comprising a folding-type bed (1). A center vertical board (12) is arranged at one side of the folding-type bed (1). End vertical boards (11), side upper boards (13), and side lower boards (14) are arranged symmetrically at two sides of the center vertical board (12). The center vertical board (12), the end vertical boards (11), the side upper boards (13), and the side lower boards (14) are assembled to form a parallelogram-shaped linked structure. End upper boards (15) and end lower boards (16) fitted with the end vertical boards (11) are arranged at ends of the folding-type bed (1). Multiple flexible casters (10) are arranged on the bottom face of the folding-type bed (1), while a wooden board (17) is arranged on the top face. Because the multifunction foldable bed is provided with the casters (10), the effects of folding, thereby coming into contact with the floor (F) over a large area, a significant stabilization effect is thus provided.\"}"}
{"id": "lrec-2024-main-1018", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Foundation Model\\n\\nPatents\\n\\nPre-training\\n\\nGeneral instructions\\n\\nIP-specific instructions\\n\\nIP-specific multi-turn conversation instructions\\n\\nFigure 5: Schematic of our proposed IP-oriented multilingual large language model MoZi.\\n\\nInstructions, and 5) GuanacoDataset Chinese, English, and Japanese general instructions.\\n\\n1\\n\\nGeneral Instruction Fine-tuning\\n\\n2\\n\\nIP-specific Instruction Fine-tuning\\n\\n3\\n\\nChatGPT\\n\\nFor the model after fine-tuning the generic instruction, we perform the second stage of IP-specific Instruction Fine-tuning upon it. The fine-tuned data in this phase consisted of a total of 58,874 items, including multilingual Q&A datasets we collected from the IPFAQ dataset, Chinese IP-related law articles in IPACT, and Chinese multi-turn conversation data in the IP domain generated by ChatGPT. Let's think of each law as an instruction, where the input is the name of the law and the output is the specifics of the law. ChatGPT generates a prompt for a multiple round conversation: \\\"The following is a conversation between a user and a patented AI assistant. The user and the patented AI assistant are having a conversation around this topic: [seed question]. The user utterance begins with human and the AI assistant utterance begins with assistant. The user asks relevant questions about the topic in question or about previous conversations. When they have no more questions, the user will stop the conversation. The AI assistant tries not to ask questions.\\\"\\n\\n5. Experiments\\n\\n5.1. Training Details\\n\\nOur MoZi-7b model was implemented in PyTorch using the transformer and deepspeed packages and with Bloomz-7b1-mt (Workshop et al., 2022) as the Foundation model. We used ZeRO-3 (Rajbhandari et al., 2020) to distribute the model over 8 \u00d7 A100 GPUs for training. During supervised fine-tuning, we set the learning rate, batch size, and maximum token length to 5e-6, 64, and 2048, respectively. Moreover, weight_decay is set to 0.0001 and num_warmup_steps is set to 100. The training epochs for the pre-training phase are 1 time, the general instruction fine-tuning phase is 2 times, and the IP instruction fine-tuning stage is 4 times.\\n\\n5.2. Baselines\\n\\nIn addition to evaluating the proposed MoZi-7b on the MoZIP benchmark, we evaluated the following baseline models:\\n\\n\u2022 ChatGPT 11\\n\\nA powerful large language model developed by OpenAI, which is based on the gpt-3.5-turbo model. We used the API provided by openai for the evaluation.\\n\\n\u2022 ChatGLM-6b (Du et al., 2022; Zeng et al., 2022)\\n\\nAn open source, Chinese-English bilingual large language model proposed by Tsinghua University, based on the general language model (GLM) architecture.\\n\\n\u2022 BELLE-7b (Ji et al., 2023a,b)\\n\\nIt is a Bloomz-7b1-mt fine-tuned model that combines 2 million Chinese data and 50,000 English data from the open source Stanford-Alpaca, which makes it have excellent Chinese comprehension and response generation capabilities.\\n\\n\u2022 BLOOMZ-7b (Workshop et al., 2022; Muenighoff et al., 2022)\\n\\nA LLM obtained by fine-tuning the BLOOM and mT5 pre-trained multilingual models on a cross-language task mixture (xP3) with the ability to generalize across languages to unseen tasks and languages.\\n\\nThe LLaMA family of large language models performs adequately only in English, and its multilingual capability is limited. Consequently, we opted\\n\\nhttps://huggingface.co/datasets/BNNT/mozi_IP_instructions\"}"}
{"id": "lrec-2024-main-1018", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset Model\\n\\n| Model           | IPQuiz-EN | IPQuiz-ZH | IPQuiz-XL | Average |\\n|-----------------|-----------|-----------|-----------|---------|\\n| MoZi-7b (ours)  | 41.5      | 39.2      | 37.4      | 39.4    |\\n| BLOOMZ-7b       | 29.3      | 29.1      | 29.4      | 29.3    |\\n| BELLE-7b        | 42.7      | 38.6      | 36.1      | 39.1    |\\n| ChatGPT         | 60.8      | 45.8      | 42.2      | 49.6    |\\n\\nTable 2: Performance of all used models on IPQuiz.\\n\\nFigure 6: The number of questions on IPQA that MoZi wins, ties or loses.\\n\\n5.3. Experimental Results\\n\\nIPQuiz consists of 2k multiple-choice questions about IP knowledge covering 7 languages. We construct this prompt (\\\"Please give the correct option for the following question\\\") to LLMs and collect generated responses from them. We used regularity and manual verification to determine which candidate was selected as the answer. Then we compute answer accuracy over the dataset. The experimental results are shown in Table 2, ChatGPT performs the best on IPQuiz-zh, IPQuiz-en and IPQuiz-xl because of its parameter level and the amount of training data. With the exception of ChatGPT, our MoZi-7b had the best average accuracy of 39.4% among models with approximately 7b number of parameters. Compared to the foundation model we used, BLOOMZ-7b, our MoZi-7b improved on average by 10.1% on IPQuiz tasks in various languages, which illustrates the significant effect of further pre-training and instruction fine-tuning. Overall, none of the scores exceeded 60%, except for ChatGPT, which reached 60.8% on IPQuiz-en. This shows that current LLMs almost always fail in performance on our proposed IPQuiz dataset. There is a lot of room for improving the performance of LLMs in the IP domain at this stage.\\n\\nIPQA\\n\\nWe provide the annotators with a question and two potential answers generated by different models. The annotators were then tasked with indicating which answer they thought was superior, or whether there was a significant difference between the two answers. If there is no distinction then label the tie. We assess inter-annotator agreement using tie-discounted accuracy. In this method, we award one point when both annotators agree on a label, half a point when either annotator (but not both) labels a tie, and no points when there is no agreement. We compared two different human annotators and obtained a consistent score of 81%. Despite the presence of some subjective elements in this task, the agreement among human annotators is reasonably good.\\n\\nThe results of the experiments on IPQA are shown in Figure 6, and are ChatGPT, MoZi-7b, ChatGLM-6b, BELLE-7b, and BLOOMZ-7b in descending order of performance results. Our MoZi-7b model defeated the original foundation model, BLOOMZ-7b, a total of 88 times, losing only 3 times. Also both BLOOM-based models, our MoZi-7b beat BELLE-7b a total of 67 times, losing only 5 times and tying 28 times. ChatGLM-6b has performed\"}"}
{"id": "lrec-2024-main-1018", "page_num": 8, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u4ec0\u4e48\u662f\u4e13\u5229\u7533\u8bf7\u4e2d\u7684\u4f18\u5148\u6743\uff1f\\n\\n\u4e13\u5229\u7533\u8bf7\u4e2d\u7684\u4f18\u5148\u6743\u662f\u6307\u7533\u8bf7\u4eba\u81ea\u53d1\u660e\u6216\u8005\u5b9e\u7528\u65b0\u578b\u5728\u5916\u56fd\u7b2c\u4e00\u6b21\u63d0\u51fa\u4e13\u5229\u7533\u8bf7\u4e4b\u65e5\u8d77\u5341\u4e8c\u4e2a\u6708\u5185\uff0c\u6216\u8005\u81ea\u5916\u89c2\u8bbe\u8ba1\u5728\u5916\u56fd\u7b2c\u4e00\u6b21\u63d0\u51fa\u4e13\u5229\u7533\u8bf7\u4e4b\u65e5\u8d77\u516d\u4e2a\u6708\u5185\uff0c\u53c8\u5728\u4e2d\u56fd\u5c31\u76f8\u540c\u4e3b\u9898\u63d0\u51fa\u4e13\u5229\u7533\u8bf7\u7684\uff0c\u4f9d\u7167\u8be5\u5916\u56fd\u540c\u4e2d\u56fd\u7b7e\u8ba2\u7684\u534f\u8bae\u6216\u8005\u5171\u540c\u53c2\u52a0\u7684\u56fd\u9645\u6761\u7ea6\uff0c\u6216\u8005\u4f9d\u7167\u76f8\u4e92\u627f\u8ba4\u4f18\u5148\u6743\u7684\u539f\u5219\uff0c\u53ef\u4ee5\u4eab\u6709\u4f18\u5148\u6743\u3002\\n\\n\u4f18\u5148\u6743\u7684\u610f\u601d\u5c31\u662f\uff0c\u4f60\u53ef\u4ee5\u5728\u5176\u4ed6\u4eba\u7684\u4e13\u5229\u4e4b\u524d\u63d0\u51fa\u4f60\u7684\u4e13\u5229\u3002\\n\\n\u5728\u4e13\u5229\u7533\u8bf7\u4e2d\uff0c\u7533\u8bf7\u4eba\u53ef\u4ee5\u9009\u62e9\u5c06\u7533\u8bf7\u63d0\u4ea4\u81f3\u591a\u4e2a\u56fd\u5bb6\u6216\u5730\u533a\uff0c\u8fd9\u4e9b\u4e0d\u540c\u7684\u63d0\u4ea4\u70b9\u79f0\u4e3a\u4f18\u5148\u6743\u3002\u901a\u8fc7\u4f18\u5148\u6743\uff0c\u7533\u8bf7\u4eba\u53ef\u4ee5\u83b7\u5f97\u5bf9\u540c\u4e00\u53d1\u660e\u5728\u4e0d\u540c\u56fd\u5bb6\u7684\u4fdd\u62a4\uff0c\u4ece\u800c\u5b9e\u73b0\u5168\u7403\u8303\u56f4\u5185\u7684\u4e13\u5229\u4fdd\u62a4\u3002\\n\\n\u4f18\u5148\u6743\u662f\u6307\u5728\u540c\u4e00\u56fd\u5bb6\u6216\u4e0d\u540c\u56fd\u5bb6\u7684\u4e0d\u540c\u4e13\u5229\u7533\u8bf7\u4e2d\uff0c\u5148\u524d\u7533\u8bf7\u7684\u7533\u8bf7\u4eba\u5bf9\u540e\u6765\u7533\u8bf7\u7684\u540c\u4e00\u53d1\u660e\u6216\u5b9e\u7528\u65b0\u578b\u7684\u6743\u5229\u4f18\u4e8e\u540e\u6765\u7533\u8bf7\u7684\u7533\u8bf7\u4eba\u3002\u4f18\u5148\u6743\u662f\u786e\u4fdd\u5236\u5ea6\uff0c\u65e8\u5728\u9f13\u52b1\u66f4\u65e9\u5730\u63d0\u4ea4\u4e13\u5229\u7533\u8bf7\uff0c\u4f7f\u53d1\u660e\u4eba\u66f4\u80fd\u591f\u53d7\u76ca\u4e8e\u5176\u53d1\u660e\uff0c\u5e76\u4fc3\u8fdb\u6280\u672f\u8fdb\u6b65\u3002\u4f18\u5148\u6743\u7684\u65f6\u95f4\u9650\u5236\u548c\u9002\u7528\u6761\u4ef6\u5728\u4e0d\u540c\u56fd\u5bb6\u548c\u56fd\u9645\u4e13\u5229\u7ec4\u7ec7\u7684\u6cd5\u89c4\u4e2d\u4f1a\u6709\u6240\u4e0d\u540c\u3002\"}"}
{"id": "lrec-2024-main-1018", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We introduce MoZIP, the inaugural multilingual benchmark for evaluating Language Models (LLMs) in the field of Intellectual Property (IP). MoZIP encompasses a wide range of real-world objective knowledge and various types of user questions. It comprises three distinct datasets: IPQuiz, IPQA, and PatentMatch, which collectively cover nine languages. As part of this research, we present MoZi, the first IP-oriented Multilingual Large Language Model. We evaluate five models, namely MoZi-7b, ChatGPT, ChatGLM-6b, BELLE-7b, and BLOOMZ-7b, using the MoZIP benchmark. The experimental findings highlight the challenging nature of the MoZIP benchmark and the current deficiency of IP-related knowledge among LLMs. To facilitate research within the recommendation community, we release the source code, benchmark datasets, instruction fine-tuning data, and the MoZi model. Our aim is for MoZIP to serve as a standardized benchmark for evaluating LLMs in the IP domain. Looking ahead, we plan to develop a more comprehensive dataset that includes a greater number of minor languages.\\n\\n7. Ethics Statement\\n\\nThe sources of our data are publicly available URLs on the Internet, URLs that can be accessed without registering for an account. And the data does not involve personal privacy. The data are collected legally. The sources of all our data are open and transparent. The purpose of the data use is to evaluate and improve the capabilities of large-scale language models in the field of intellectual property.\\n\\n8. Acknowledgements\\n\\nThis work is supported by National Key Research and Development Program of China (2022YFF0902100), Postdoctoral Fellowship Program of CPSF (GZC20232873), GuangDong Basic and Applied Basic Research Foundation (2023A1515110718 and 2023A1515110496), China Postdoctoral Science Foundation (2023M733654), National Natural Science Foundation of China (62376262), Shen- zhen Science and Technology Innovation Program (KQTD20190929172835662), Shenzhen Basic Research Foundation (JCYJ20210324115614039 and JCYJ20200109113441941).\\n\\n9. Bibliographical References\\n\\nAnthony McEnery and others. 2004. The EMILLE/CIIL Corpus. EMILLE (Enabling Minority Language Engineering) Project. distributed via ELRA: ELRA-Id W0037, ISLRN039-846-040-604-0.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evaluating large language models trained on code.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm: General language model pretraining with autoregressive blank infilling. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 320\u2013335.\\n\\nQuzhe Huang, Mingxu Tao, Zhenwei An, Chen Zhang, Cong Jiang, Zhibin Chen, Zirui Wu, and Yansong Feng. 2023a. Lawyer llama technical report. arXiv preprint arXiv:2305.15062.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, and Junxian He. 2023b. C-eval: A multi-level multi-discipline chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.\\n\\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Baochang Ma, and Xiangang Li. 2023a. Belle: Be everyone\u2019s large language model engine. https://github.com/LianjiaTech/BELLE.\\n\\nYunjie Ji, Yong Deng, Yan Gong, Yiping Peng, Qiang Niu, Lei Zhang, Baochang Ma, and Xiangang Li. 2023b. Exploring the impact of instruction data scaling on large language models:\"}"}
{"id": "lrec-2024-main-1018", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An empirical study on real-world use cases.\\n\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016a. FastText.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.\\n\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.\\n\\nKhalid Choukri and Niklas Paullson. 2004. The OrienTel Moroccan MCA (Modern Colloquial Arabic) database. Distributed via ELRA: ELRA-Id ELRA-S0183, ISLRN 613-578-868-832-2.\\n\\nPhilippe Laban, Wojciech Kry\u015bci\u0144ski, Divyansh Agarwal, Alexander R. Fabbri, Caiming Xiong, Shafiq Joty, and Chien-Sheng Wu. 2023. LLMs as factual reasoners: Insights from existing benchmarks and beyond.\\n\\nYujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, R\u00e9mi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien de Masson d'Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven Gowal, Alexey Cherepanov, James Molloy, Daniel J. Mankowitz, Esme Sutherland Robson, Pushmeet Kohli, Nando de Freitas, Koray Kavukcuoglu, and Oriol Vinyals. 2022. Competition-level code generation with alphacode. Science, 378(6624):1092\u20131097.\\n\\nJimmy Lin, Xueguang Ma, Sheng-Chieh Lin, Jheng-Hong Yang, Ronak Pradeep, and Rodrigo Nogueira. 2021. Pyserini: A Python toolkit for reproducible information retrieval research with sparse and dense representations. In Proceedings of the 44th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR 2021), pages 2356\u20132362.\\n\\nHongcheng Liu, Yusheng Liao, Yutong Meng, and Yuhao Wang. 2023. LawGPT: Chinese legal dialogue language model. https://github.com/LiuHC0428/LAW_GPT.\\n\\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, Xiangru Tang, Dragomir Radev, Alham Fikri Aji, Khalid Almubarak, Samuel Albanie, Zaid Alyafeai, Albert Webson, Edward Raff, and Colin Raffel. 2022. Crosslingual generalization through multitask finetuning.\\n\\nOpenAI. 2023. GPT-4 technical report.\\n\\nSamyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. 2020. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pages 1\u201316. IEEE.\\n\\nKaran Singhal, Shekoofeh Azizi, Tao Tu, S. Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, Perry Payne, Martin Seneviratne, Paul Gamble, Chris Kelly, Nathaneal Scharli, Aakanksha Chowdhery, Philip Mansfield, Blaise Aguera y Arcas, Dale Webster, Greg S. Corrado, Yossi Matias, Katherine Chou, Juraj Gottweis, Nenad Tomasev, Yun Liu, Alvin Rajkomar, Joelle Barral, Christopher Semturs, Alan Karthikesalingam, and Vivek Natarajan. 2022. Large language models encode clinical knowledge.\\n\\nSpeecon Consortium. 2011. Catalan Speecon database. SpeeCon. Speecon Project, distributed via ELRA: ELRA-Id S0327, Speecon resources, 1.0, ISLRN 935-211-147-357-5.\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. Llama: Open and efficient foundation language models.\\n\\nHaochun Wang, Chi Liu, Nuwa Xi, Zewen Qiang, Sendong Zhao, Bing Qin, and Ting Liu. 2023. Huatuo: Tuning llama model with Chinese medical knowledge.\\n\\nWIPO. 2011. Wipo website, world intellectual property day \u2013 26 April. [Online; Accessed 20 April 2011].\\n\\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\\n\\nShijie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhjan Kambadur, David Rosenberg, and Gideon Mann. 2023. BloombergGPT: A large language model for finance. arXiv preprint arXiv:2303.17564.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu,\"}"}
{"id": "lrec-2024-main-1018", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wendi Zheng, Xiao Xia, Weng Lam Tam, Zixuan Ma, Yufei Xue, Jidong Zhai, Wenguang Chen, Zhiyuan Liu, Peng Zhang, Yuxiao Dong, and Jie Tang. 2023. GLM-130b: An open bilingual pre-trained model. In The Eleventh International Conference on Learning Representations.\\n\\nAohan Zeng, Xiao Liu, Zhengxiao Du, Zihan Wang, Hanyu Lai, Ming Ding, Zhuoyi Yang, Yifan Xu, Wendi Zheng, Xiao Xia, et al. 2022. Glm-130b: An open bilingual pre-trained model. arXiv preprint arXiv:2210.02414.\\n\\nHongbo Zhang, Junying Chen, Feng Jiang, Fei Yu, Zhihong Chen, Jianquan Li, Guiming Chen, Xiangbo Wu, Zhiyi Zhang, Qingying Xiao, Xiang Wan, Benyou Wang, and Haizhou Li. 2023a. Huatuogpt, towards taming language models to be a doctor. arXiv preprint arXiv:2305.15075.\\n\\nJizhi Zhang, Keqin Bao, Yang Zhang, Wenjie Wang, Fuli Feng, and Xiangnan He. 2023b. Is chatgpt fair for recommendation? evaluating fairness in large language model recommendation.\"}"}
