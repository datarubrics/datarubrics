{"id": "acl-2023-long-507", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nContent Warning: This paper contains examples of homophobic and transphobic stereotypes.\\n\\nWe present WinoQueer: a benchmark specifically designed to measure whether large language models (LLMs) encode biases that are harmful to the LGBTQ+ community. The benchmark is community-sourced, via application of a novel method that generates a bias benchmark from a community survey. We apply our benchmark to several popular LLMs and find that off-the-shelf models generally do exhibit considerable anti-queer bias. Finally, we show that LLM bias against a marginalized community can be somewhat mitigated by finetuning on data written about or by members of that community, and that social media text written by community members is more effective than news text written about the community by non-members. Our method for community-in-the-loop benchmark development provides a blueprint for future researchers to develop community-driven, harms-grounded LLM benchmarks for other marginalized communities.\\n\\nNote: This version corrects a bug found in evaluation code after publication. General findings have not changed, but tables 5 and 6 and figure 1 have been corrected.\\n\\n1 Introduction\\n\\nRecently, there has been increased attention to fairness issues in natural language processing, especially concerning latent biases in large language models (LLMs). However, most of this work focuses on directly observable characteristics like race and (binary) gender. Additionally, these identities are often treated as discrete, mutually exclusive categories, and existing benchmarks are ill-equipped to study overlapping identities and intersectional biases. There is a significant lack of work on biases based on less observable characteristics, most notably LGBTQ+ identity (Tomasev et al., 2021). Another concern with recent bias work is that \u201cbias\u201d and \u201charm\u201d are often poorly defined, and many bias benchmarks are insufficiently grounded in real-world harms (Blodgett et al., 2020).\\n\\nThis work addresses the lack of suitable benchmarks for measuring anti-LGBTQ+ bias in large language models. We present a community-sourced benchmark dataset, WinoQueer, which is designed to detect the presence of stereotypes that have caused harm to specific subgroups of the LGBTQ+ community. This work represents a significant improvement over WinoQueer-v0, introduced in (Felkner et al., 2022). Our dataset was developed using a novel community-in-the-loop method for benchmark development. It is therefore grounded in real-world harms and informed by the expressed needs of the LGBTQ+ community. We present baseline WinoQueer results for a variety of popular LLMs, as well as demonstrating that anti-queer bias in all studied models can be partially mitigated by finetuning on a relevant corpus, as suggested by (Felkner et al., 2022).\\n\\nThe key contributions of this paper are:\\n\\n\u2022 the WinoQueer (WQ) dataset, a new community-sourced benchmark for anti-LGBTQ+ bias in LLMs.\\n\\n1 https://github.com/katyfelkner/winoqueer\"}"}
{"id": "acl-2023-long-507", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"baseline WinoQueer benchmark results on BERT, RoBERTa, ALBERT, BART, GPT2, OPT, and BLOOM models, demonstrating significant anti-queer bias across model types and sizes.\\n\\nversions of benchmarked models, that we debiased via finetuning on corpora about or by the LGBTQ+ community.\\n\\n2 Related Work\\n\\nAlthough the issue of gender biases in NLP has received increased attention recently (Costa-juss\u00e0, 2019), there is still a dearth of studies that scrutinize biases that negatively impact the LGBTQ+ community (Tomasev et al., 2021). Devinney et al. (2022) surveyed 176 papers regarding gender bias in NLP and found that most of these studies do not explicitly theorize gender and that almost none consider intersectionality or inclusivity (e.g., non-binary genders) in their model of gender. They also observed that many studies conflate \u201csocial\u201d and \u201clinguistic\u201d gender, thereby excluding transgender, nonbinary, and intersex people from the discourse.\\n\\nAs (Felkner et al., 2022) observed, there is a growing body of literature that examines anti-queer biases in large language models, but most of this work fails to consider the full complexity of LGBTQ+ identity and associated biases. Some works (e.g. Nangia et al., 2020) treat queerness as a single binary attribute, while others (e.g. Czarnowska et al., 2021) assume that all subgroups of the LGBTQ+ community are harmed by the same stereotypes. These benchmarks are unable to measure biases affecting specific LGBTQ+ identity groups, such as transmisogyny, biphobia, and lesbophobia.\\n\\nDespite such efforts, scholars have pointed out the lack of grounding in real-world harms in the majority of bias literature. For instance, Blodgett et al. (2020) conducted a critical review of 146 papers that analyze biases in NLP systems and found that many of those studies lacked normative reasoning on \u201cwhy\u201d and \u201cin what ways\u201d the biases they describe (i.e., system behaviors) are harmful \u201cto whom.\u201d The same authors argued that, in order to better address biases in NLP systems, research should incorporate the lived experiences of community members that are actually affected by them. There have been a few attempts to incorporate crowd-sourcing approaches to evaluate stereotypical biases in language models such as StereoSet (Nadeem et al., 2021), CrowS-Pairs (Nangia et al., 2020), or Gender Lexicon Dataset (Cryan et al., 2020). N\u00e9v\u00e9ol et al. (2022) used a recruited volunteers on a citizen science platform rather than using paid crowdworkers. However, these studies lack the perspective from specific communities, as both crowdworkers and volunteers were recruited from the general public. While not directly related to LGBTQ+ issues, Bird (2020) discussed the importance of decolonial and participatory methodology in research on NLP and marginalized communities. Recently, Smith et al. (2022) proposed a bias measurement dataset (HOLISTICIAS), which incorporates a participatory process by inviting experts or contributors who self-identify with particular demographic groups such as the disability community, racial groups, and the LGBTQ+ community. This dataset is not specifically focused on scrutinizing gender biases but rather takes a holistic approach, covering 13 different demographic axes (i.e., ability, age, body type, characteristics, cultural, gender/sex, sexual orientation, nationality, race/ethnicity, political, religion, socioeconomic). Nearly two dozen contributors were involved in creating HOLISTICIAS, but it is uncertain how many of them actually represent each demographic axis, including the queer community. This study fills the gap in the existing literature by introducing a benchmark dataset for homophobic and transphobic bias in LLMs that was developed via a large-scale community survey and is therefore grounded in real-world harms against actual queer and trans people.\\n\\n3 Methods\\n\\n3.1 Queer Community Survey\\n\\nWe conducted an online survey to gather community input on what specific biases and stereotypes have caused harm to LGBTQ+ individuals and should not be encoded in LLMs. Unlike previous studies which recruited crowdworkers from the general public (Nadeem et al., 2021; Nangia et al., 2020; Cryan et al., 2020), this study recruited survey respondents specifically from the marginalized community against whom we are interested in measuring LLM bias (in this case, the LGBTQ+ community). This human subjects study was reviewed and determined to be exempt by our IRB. These survey responses are used as the basis of template creation which will be further discussed in the next section.\"}"}
{"id": "acl-2023-long-507", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Survey Questions on Harmful Stereotypes and Biases\\n\\nWhat general anti-LGBTQ+ stereotypes or biases have harmed you?\\n\\nWhat stereotypes or biases about your gender identity have harmed you?\\n\\nWhat stereotypes or biases about your sexual/romantic orientation have harmed you?\\n\\nWhat stereotypes or biases about the intersection of your gender & sexual identities have harmed you?\\n\\nTable 1: Example questions from the community-driven survey.\\n\\nSurvey participants were recruited online through a variety of methods, including university mailing lists, Slack/Discord channels of LGBTQ+ communities and organizations, and social media (e.g., NLP Twitter, gay Twitter). Participants saw a general call for recruitment and were asked to self-identify if interested in participating. Participants who met the screening criteria (i.e. English-speaking adults who identify as LGBTQ+) were directed to the informed consent form. The form warned participants about the potentially triggering content of the survey and explicitly stated that the survey is optional and that participants are free to skip questions and/or quit the survey at any time. The consent form also explained that data would be collected anonymously and short excerpts used to create a publicly available benchmark dataset, but that entire responses and any identifying information would be kept confidential. Personally identifying information was redacted from responses.\\n\\nParticipants who consented to the research (n=295) answered survey questions on what bases or stereotypes about their gender and/or sexual/romantic orientation or about the LGBTQ+ community in general have personally caused them harm. Example survey questions are listed in Table 1. We used an intentionally broad definition of harm: \u201cemotional and psychological discomfort, as well as physical violence, discrimination, bullying and cyberbullying, adverse material or financial impacts, and loss of personal or professional opportunities.\u201d In addition, participants were asked to self-identify their gender and sexuality; the results of which are summarized in Table 2. There were also optional demographic questions about race/ethnicity, age range, and country of residence; respondent statistics are listed in Appendix A.\\n\\n3.2 WinoQueer Template Creation\\n\\nWe introduce the first \u201ccommunity-in-the-loop\u201d bias benchmark dataset, WinoQueer. It was modeled after the CrowS-Pairs (Nangia et al., 2020) paired sentence bias probing task. As far as the authors are aware, this dataset is the first to explore identity-specific anti-queer and anti-trans biases by incorporating input directly from the affected community. Each sentence in the WinoQueer benchmark is a 4-way Cartesian product of:\\n\\nTemplate sentences:\\n\\nTemplates are the general structure into which other elements are slotted. Our choice of templates was informed by Cao et al. (2022). An example template is:\\n\\nI don\u2019t like <name> because they are <identity>.\\n\\nNames/pronouns:\\n\\nFor names, we chose the 20 most common male and female names from the US census. We then chose 20 nonbinary and unisex names from Kirby Conrod\u2019s informal survey of nonbinary names for linguistics examples and Fivethirtyeight\u2019s list of common unisex names. For pronouns, we used he, she, and they.\\n\\nIdentity descriptors:\\n\\nStarting from the list of gender and sexuality descriptors in Czarnowska et al. (2021), we bucketed the terms into 9 high-level identity groups: LGBTQ, Queer, Transgender, Nonbinary, Bisexual, Pansexual, Lesbian, Asexual, and Gay. These identities are not mutually exclusive, and LGBTQ+ individuals can fit into one or several. We also selected the terms Cisgender, Cis, Heterosexual, and Straight for use in counterfactual sentences.\\n\\nPredicates:\\n\\nPredicates were extracted from free-text responses to the survey described in Section 3.1. After sorting results by identity categories, we read all responses and manually coded for the top ways people were discriminated against (i.e. gay people have family issues, trans people are predatory).\\n\\nWe then generated tuples for each combination of templates, names/pronouns, and predicates, subject to the following rules. All names and pronouns were combined with identity descriptors LGBTQ, Queer, Transgender, Bisexual, Asexual, and Pansexual. Nonbinary names and they/them pronouns were included in the set. All combinations of predicates were included.\\n\\nhttp://www.kirbyconrod.com\\n\\nhttps://fivethirtyeight.com/features/there-are-922-unisex-names-in-america-is-yours-one-of-them/\"}"}
{"id": "acl-2023-long-507", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Self-identified gender and sexuality of respondents. Results do not sum to 100 because respondents could select multiple answers.\\n\\n| Category                  | % Respondents |\\n|---------------------------|---------------|\\n| Gender                    |               |\\n| woman                     | 43.55         |\\n| man                       | 34.41         |\\n| nonbinary                 | 24.73         |\\n| transgender               | 20.43         |\\n| cisgender                 | 17.74         |\\n| gender non-conforming      | 13.44         |\\n| all other responses       | 18.83         |\\n| Sexuality                 |               |\\n| bisexual                  | 26.16         |\\n| queer                     | 21.19         |\\n| gay                       | 16.23         |\\n| pansexual                 | 11.26         |\\n| asexual                   | 9.93          |\\n| lesbian                   | 8.61          |\\n| all other responses       | 6.62          |\\n\\nAfter generating sentences from tuples, we paired each sentence with a counterfactual sentence that replaced its identity descriptor with a corresponding non-LGBTQ+ identity. For sentences containing sexuality descriptors Gay, Bisexual, Lesbian, Pansexual, and Asexual, each sentence was duplicated and paired with a counterfactual replacing the descriptor with \u201cstraight\u201d and another replacing the descriptor with \u201cheterosexual.\u201d Similarly, sentences containing gender identity descriptors Transgender and Nonbinary were paired with counterfactuals containing \u201ccisgender\u201d and \u201ccis.\u201d Sentences containing LGBTQ and Queer, which are broader terms encompassing both sexuality and gender, were paired with all four possible counterfactuals. Table 3 shows example sentence pairs from the dataset.\\n\\nOverall, the WinoQueer benchmark dataset contains 45540 sentence pairs covering 11 template sentences, 9 queer identity groups, 3 sets of pronouns, 60 common names, and 182 unique predicts. A unique strength of the WinoQueer dataset is that it is fully human-created and human-audited. We chose this approach for two reasons. First, Blodgett et al. (2020) have uncovered data quality issues with crowdsourced bias metrics; second, Bender et al. (2021) advocate for careful human auditing of datasets, especially bias benchmarks.\\n\\nA Note on Terminology\\nWe grouped names, pronouns, and identity descriptors in this way in order to capture gender-based stereotypes about LGBTQ+ individuals while still allowing for diversity of gender identity and expression. The \u201clesbian\u201d identity descriptor provides a natural way to explore both misogynistic and homophobic stereotypes about queer women. We decided that it was important for our benchmark to have similar capability to measure gender-based stereotypes about queer men. While the word \u201cgay\u201d can refer to people of any gender and many women do self-identify as gay, it was also the closest analogy to \u201clesbian\u201d for the purpose of measuring intersectional stereotypes about orientation and gender. Therefore, the WinoQueer benchmark uses \u201cgay\u201d to refer to gay men specifically and \u201cqueer\u201d as a more general umbrella term. We hope that this and other bias benchmarks will continue to evolve with language use in the LGBTQ+ community.\\n\\n3.3 Data Collection\\nTo debias models and improve their performance on the WQ benchmark, we finetuned them on two datasets: QueerNews, containing articles from US national news media addressing LGBTQ+ issues, and QueerTwitter, containing Tweets about LGBTQ+ topics. The time frame of both datasets is Jan 1, 2015 \u2013 Sep 30, 2022.\\n\\nWe collected QueerTwitter by using the Twitter Academic API to conduct a retroactive search of Tweets. Data was retrieved in accordance with Twitter\u2019s Terms of Service and personally identifying information was redacted from the final data set. For search terms, we used anti-trans bill numbers retrieved from the \u201cLegislative Tracker: Anti-Transgender Legislation\u201d website, which tracks proposed state and federal legislation that would limit the rights of trans people in the United States, as well as hashtags commonly used by those fighting anti-trans legislation. We iteratively analyzed co-occurring hashtags with regard to anti-trans bills to build a more comprehensive search.\"}"}
{"id": "acl-2023-long-507", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Example sentence pairs from WinoQueer benchmark.\\n\\nterm list. The resultant list included hashtags related to anti-trans bills (i.e., #transrightsarehumanrights, #transbill, #KilTheBill, #antitransbill, #DontSayGay, #DontSayGayBill) and those related to LGBTQ+ events (i.e., #transdayofvisibility, #lesbianvisibilityday, #bisexualawarenessweek, #stonewall, #stonewall50). We conducted a random sample of relevant Tweets for each day in the time frame. After filtering, our second search with co-occuring hashtags included yields a total of 4,339,205 tweets (4,122,244 sentences).\\n\\nQueerNews was collected using the open source platform Media Cloud.\\n\\nWe conducted a keyword search based on anti-trans bill numbers and search terms related to anti-trans bills (i.e., anti-trans bill, trans bill, anti-trans) and LGBTQ+ identity (i.e., lgbtq, lgbt, gay, lesbian, queer, trans, bisexual). For MediaCloud, we used more general search terms related to the LGBTQ+ community because Media Cloud yields fewer results compared to Twitter when using the same search terms. This resulted in a corpus of 118,894 news articles (4,108,194 sentences). New articles were retrieved abiding by Media Cloud's Terms of Use.\\n\\n3.4 Evaluation Metrics\\n\\nEvaluation on WQ follows the methodology of Nangia et al. (2020), which introduced a novel pseudo-log-likelihood metric for bias in masked language models. This metric can be reported from 0 to 1 or 0 to 100; for consistency, we always report scores out of 100. For a sentence $S(s_1, s_2, \\\\ldots, s_n)$, each token shared between the two templates (unmodified tokens, $U$) is masked one-at-a-time, while the modified tokens ($M$) are held constant, summing the probability of predicting the correct masked token for each possible position of the mask. Their scoring function is formulated\\n\\n$$\\\\text{score}(S) = 100 \\\\frac{1}{|U|} \\\\sum_{i=1}^{X} \\\\log P\\\\left(u_i \\\\in U | U \\\\setminus u_i, M, \\\\theta\\\\right)$$\\n\\nThis function is applied to pairs of more stereotypical (i.e. stating a known stereotype or bias about a marginalized group) and less stereotypical sentences (stating the same stereotype or bias about the majority group). The bias score is the percentage of examples for which the likelihood of the more stereotypical sentence is higher than the likelihood of the less stereotypical sentence. A perfect score is 50, i.e. the language model is equally likely to predict either version of the sentence. A score greater than 50 indicates that the LM is more likely to predict the stereotypical sentence, meaning the model encodes social stereotypes and is more likely to produce biased, offensive, or otherwise harmful outputs.\\n\\nThis metric is only applicable to masked language models. However, we generalize their metric by introducing an alternative scoring function for autoregressive language models:\\n\\n$$\\\\text{score}(S) = 100 \\\\frac{1}{|U|} \\\\sum_{i=1}^{X} \\\\log P\\\\left(u_i | s<u_i, \\\\theta\\\\right)$$\\n\\nwhere $s<u_i$ is all tokens (modified or unmodified) preceding $u_i$ in the sentence $S$. Intuitively, we ask the model to predict each unmodified token in order, given all previous tokens (modified or unmodified). For autoregressive models, the model's beginning of sequence token is prepended to all sentences during evaluation. While the numeric scores of individual sentences are not directly comparable between masked and autoregressive models, the bias score (percentage of cases where the model is more likely to predict more stereotypical sentences) is comparable across model types and scoring functions.\"}"}
{"id": "acl-2023-long-507", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.5 Model Debiasing Via Fine-tuning\\n\\nWe selected the following large pre-trained language model architectures for evaluation: BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT (Lan et al., 2020), BART (Lewis et al., 2020), GPT2 (Radford et al., 2019), OPT (Zhang et al., 2022), and BLOOM (Workshop, 2022). Details of model sizes and compute requirements for finetuning can be found in Table 4. All models were trained on 1 node with 2 GPUs, and the time reported is the total number of GPU hours. In addition to finetuning, we used about 218 GPU hours for evaluation and debugging. In total, this project used 2,256 GPU hours across NVIDIA P100, V100, and A40 GPUs.\\n\\nWe aimed to choose a diverse set of models representing the current state of the art in NLP research, at sizes that were feasible to finetune on our hardware. We produce two fine-tuned versions of each model: one fine-tuned on QueerNews, and one fine-tuned on QueerTwitter. For QueerNews, articles were sentence segmented using SpaCy (Montani et al., 2023) and each sentence was treated as a training datum. For QueerTwitter, each tweet was treated as a discrete training datum and was normalized using the tweet normalization script from Nguyen et al. (2020). In the interest of energy efficiency, we did not finetune models over 2B parameters. For these four models (OPT-2.7b, OPT-6.7b, BLOOM-3b, and BLOOM-7.1b), we report only WQ baseline results.\\n\\nMost models were fine-tuned on their original pre-training task: masked language modeling for BERT, RoBERTa, and ALBERT; causal language modeling for GPT2, OPT, and BLOOM. BART's pre-training objective involved shuffling the order of sentences, which is not feasible when most tweets only contain a single sentence. Thus, BART was finetuned on causal language modeling. Models were finetuned for one epoch each, with instantaneous batch size determined by GPU capacity, gradient accumulation over 10 steps, and all other hyperparameters at default settings, following Felkner et al. (2022). We evaluate the original off-the-shelf models, as well as our fine-tuned versions, on the WinoQueer benchmark.\\n\\n4 Results and Discussion\\n\\n4.1 Off-the-shelf WinoQueer Results\\n\\nTable 5 shows the WinoQueer bias scores of 20 tested models. These bias scores represent the percentage of cases where the model is more likely to output the stereotypical than the counterfactual sentence. A perfect score is 50, meaning the model is no more likely to output the offensive statement in reference to an LGBTQ+ person than the same offensive statement about a straight person. The average bias score across all models is 66.50, meaning the tested models will associate homophobic and transphobic stereotypes with queer people about twice as often than they associate those same toxic statements with straight people.\\n\\nAll 20 models show some evidence of anti-queer bias, ranging from slight (55.93, ALBERT-xxl-v2) to gravely concerning (79.83, BART-base). In general, the masked language models (BERT, RoBERTa, ALBERT) seem to show less anti-queer bias than the autoregressive models (GPT2, BLOOM, OPT), but this result is specific to the WQ test set and may or may not generalize to other bias metrics and model sets.\\n\\n6 BERT and RoBERTa models show significant but not insurmountable bias. We chose to include ALBERT in our analysis because we were curious whether the repetition of (potentially bias-inducing) model layers would increase bias scores, but this does not seem to be the case, as ALBERT models have slightly lower bias scores than BERT and RoBERTa. Among autoregressive models, GPT2 shows the least bias, followed by BLOOM, and then OPT. BART is excluded from all masked vs. autoregressive comparisons because it does not fit neatly into either category. It has a BERT-like encoder and GPT2-like decoder, and can be used for both mask-filling and generative tasks.\"}"}
{"id": "acl-2023-long-507", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Bias scores for tested models on the entire WinoQueer dataset and subsets of the dataset pertaining to specific subpopulations. A perfectly unbiased model scores 50. In each row, the highest bias score is bold and the lowest is italic. The last column is the average magnitude (absolute value) of the difference between the overall score and the 9 subpopulation scores for each model. Across models, it is clear that significant anti-queer bias is present and that bias severity varies widely across subgroups and between models. Column header abbreviations: WQ - WinoQueer overall bias score, Trans - transgender, NB - nonbinary, Bi - bisexual, Pan - pansexual, Les. - lesbian, Ace - asexual.\\n\\n**4.2 Finetuning for Debiasing Results**\\n\\nFinetuning results are reported in Table 5. In general, we find that finetuning on both QueerNews and QueerTwitter substantially reduces bias scores on the WQ benchmark. In fact, the finetuning is so effective that it sometimes drives the bias score below the ideal value of 50, which is dis-\"}"}
{"id": "acl-2023-long-507", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Results of finetuning on QueerNews and QueerTwitter. Finetuning is generally effective, with QueerTwitter being slightly more effective than QueerNews. Across 16 finetuned models, finetuning on QueerNews reduced WQ bias score by an average of 10.28 points, while finetuning on QueerTwitter reduced bias score by an average of 17.98 points.\\n\\nWhile this method of debiasing via finetuning is generally quite effective, its benefits are not equitably distributed among LGBTQ+ subcommunities. Fig. 1 shows the effectiveness of our finetuning (measured as the average over all models of the difference between finetuned WQ score and baseline WQ score) on the same nine subpopulations of the LGBTQ+ community. The finetuning is most effective for general stereotypes about the entire LGBTQ+ community. It is much less effective for smaller subcommunities, including nonbinary and asexual individuals. Twitter is more effective than news for most subpopulations, but news performs better for the queer and nonbinary groups. News data has a positive effect on the bias score against asexual individuals. However, the scores represented in the figure are means over all models, and the actual effects on individual models vary widely.\\n\\nIt is important to note that while evaluation is separated by identity, the finetuning data is not. These disparities could likely be reduced by labelling the finetuning data at a more granular level and then balancing the data on these labels.\\n\\n5 Conclusions\\n\\nThis paper presented WinoQueer, a new bias benchmark for measuring anti-queer and anti-trans bias in large language models. WinoQueer was developed via a large survey of LGBTQ+ individuals, meaning it is grounded in real-world harms and based on the experiences of actual queer people. We detail our method for participatory benchmark development, and we hope that this method will be extensible to developing community-in-the-loop benchmarks for LLM bias against other marginalized communities.\"}"}
{"id": "acl-2023-long-507", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Difference in WQ score between baseline and finetuned models, for both QueerNews and QueerTwitter finetuning data. Results are averaged across all 16 models we finetuned and separated by LGBTQ+ identity groups.\\n\\nthat WQ bias scores can be improved by finetuning LLMs on either news data about queer issues or Tweets written by queer people. Finetuning on QueerTwitter is generally more effective at reducing WQ bias score than finetuning on QueerNews, demonstrating that direct input from the affected community is a valuable resource for debiasing large models. The prevalence of high WQ bias scores across model architectures and sizes makes it clear that homophobia and transphobia are serious problems in LLMs, and that models and datasets should be audited for anti-queer biases as part of a comprehensive fairness audit. Additionally, the large variance in bias against specific subgroups of the LGBTQ+ community across tested models is a strong reminder that LLMs must be audited for potential biases using both intrinsic, model-level metrics like WQ and extrinsic, task-level metrics to ensure that their outputs are fair in the context where the model is deployed.\\n\\nOur results show that LLMs encode many biases and stereotypes that have caused irreparable harm to queer individuals. Models are liable to reproduce and even exacerbate these biases without careful human supervision at every step of the training pipeline, from pretraining data collection to downstream deployment. As queer people and allies, the authors know that homophobia and transphobia are ubiquitous in our lives, and we are keenly aware of the harms these biases cause. We hope that the WinoQueer benchmark will encourage allyship and solidarity among NLP researchers, allowing the NLP community to make our models less harmful and more beneficial to queer and trans individuals.\\n\\nLimitations\\n\\nCommunity Survey\\n\\nThe WinoQueer benchmark is necessarily an imperfect representation of the needs of the LGBTQ+ community, because our sample of survey participants does not represent the entire queer community. Crowdsourcing, or volunteer sampling, was used for recruiting survey participants in this study as it has its strength in situations where there is a limitation in availability or willingness to participate in research (e.g., recruiting hard-to-reach populations). However, this sampling method has a weakness in terms of generalizability due to selection bias and/or undercoverage bias. We limited our survey population to English-speakers, and the WinoQueer benchmark is entirely in English. We also limited our survey population to adults (18 and older) to avoid requiring parental involvement, so queer youth are not represented in our sample. Additionally, because we recruited participants online,\"}"}
{"id": "acl-2023-long-507", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"younger community members are overrepresented, and queer elders are underrepresented. Compared to the overall demographics of the US, Black, Hispanic/Latino, and Native American individuals are underrepresented in our survey population. Geographically, our respondents are mostly American, and the Global South is heavily underrepresented. These shortcomings are important opportunities for growth and improvement in future participatory research.\\n\\nFinetuning Data Collection\\n\\nIn an effort to balance the amount of linguistic data retrieved from Media Cloud and Twitter respectively, we had to use additional search terms for Media Cloud as it yielded significantly fewer results than Twitter when using the same search terms. Also, news articles from January to May 2022 are excluded from the news article dataset due to Media Cloud\u2019s backend API issues. Due to the size of our datasets and the inexact nature of sampling based on hashtags, it is likely that there are at least some irrelevant and spam Tweets in our sample.\\n\\nTemplate Creation\\n\\nOur generated sentences have several limitations and areas for improvement. First, our nine identity subgroups are necessarily broad and may not represent all identities in the queer community. The WinoQueer benchmark is limited to biases about gender and sexual orientation. It does not consider intersectional biases and the disparate effects of anti-LGBTQ+ bias on individuals with multiple marginalized identities. The names used in templates are taken from the US Census, so they are generally Western European names common among middle-aged white Americans. Non-European names are not well-represented in the benchmark. Additionally, the benchmark currently only includes he, she, and they personal pronouns; future versions should include a more diverse set of personal pronouns. Finally, sentences are generated from a small set of templates, so they do not represent every possible stereotyping, offensive, or harmful statement about LGBTQ+ individuals. A high WinoQueer bias score is an indicator that a model encodes homophobic and transphobic stereotypes, but a low bias score does not indicate that these stereotypes are absent.\\n\\nEvaluation and Finetuning\\n\\nWe used similar, but not identical, scoring functions to evaluate masked and autoregressive language models. It is possible that the metrics are not perfectly calibrated, and that one category of models may be evaluated more harshly than the other. Additionally, some of our finetuned models scored below the ideal bias score of 50. This means that they are more likely to apply homophobic and transphobic stereotypes to heterosexual and cisgender people than to LGBTQ+ people. Many of these stereotypes are toxic and offensive regardless of the target, but others do not carry the same weight when applied to cis and straight individuals. Currently, it is not well-defined what WQ scores under 50 mean, in theory or in practice. This definition will need to be developed in consultation with researchers, end users, and the LGBTQ+ community. This paper only includes results for a small fraction of available pretrained language models, and our results only represent comparatively small models. We present baseline results for models up to 7.1 billion parameters and finetuned results for models up to 1.5 billion parameters, but many of the models in use today have hundreds of billions of parameters. Finally, our results are limited to open-source models and do not include closed-source or proprietary models.\\n\\nAcknowledgements\\n\\nThis material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. 2236421. Any opinion, findings, and conclusions or recommendations expressed in this material are those of the authors(s) and do not necessarily reflect the views of the National Science Foundation. We also wish to thank Dr. Kristina Lerman and Dr. Fred Morstatter, who co-taught the Fairness in AI course where the authors met and this work was initially conceived. Finally, we would like to thank our three anonymous reviewers for their detailed and helpful suggestions.\\n\\nReferences\\n\\nEmily M. Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623, Virtual Event Canada. ACM.\"}"}
{"id": "acl-2023-long-507", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-507", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Demographics of Survey Respondents\\n\\n#### Gender Identity\\n\\n| Gender Identity | % Respondents |\\n|-----------------|--------------|\\n| woman           | 43.55        |\\n| man             | 34.41        |\\n| nonbinary       | 24.73        |\\n| transgender     | 20.43        |\\n| cisgender       | 17.74        |\\n| gender non-conforming | 13.44 |\\n| genderfluid     | 7.53         |\\n| agender         | 5.38         |\\n| questioning     | 4.30         |\\n| two-spirit      | 0.54         |\\n| other           | 3.23         |\\n| prefer not to say | 1.08   |\\n\\n**Table 7**: Self-identified gender of survey respondents. Results do not sum to 100 because respondents were allowed to select multiple options.\\n\\n#### Sexual Orientation\\n\\n| Sexual Orientation | % Respondents |\\n|--------------------|--------------|\\n| bisexual           | 26.16        |\\n| queer              | 21.19        |\\n| gay                | 16.23        |\\n| pansexual          | 11.26        |\\n| asexual            | 9.93         |\\n| lesbian            | 8.61         |\\n| straight           | 3.31         |\\n| other              | 2.32         |\\n| prefer not to say  | 0.99         |\\n\\n**Table 8**: Self-identified sexual orientation of survey respondents. Results do not sum to 100 because respondents were allowed to select multiple options.\\n\\n#### Race/Ethnicity\\n\\n| Race/Ethnicity | % Resp. |\\n|----------------|---------|\\n| White          | 46.93   |\\n| Asian          | 22.37   |\\n| Hispanic or Latino/a/x | 10.96 |\\n| Middle Eastern / N. African / Arab | 4.82 |\\n| Black or African American | 2.19 |\\n| American Indian or Alaska Native | 1.75 |\\n| Native Hawaiian or Pacific Islander | 0.88 |\\n| biracial or mixed race | 5.70 |\\n| other           | 3.07     |\\n| prefer not to say | 1.32    |\\n\\n**Table 9**: Self-identified race/ethnicity of survey respondents. 228 of 295 participants answer this question.\\n\\n#### Age Range\\n\\n| Age Range | % Respondents |\\n|-----------|--------------|\\n| 18\u201320     | 24.86        |\\n| 20\u201329     | 54.05        |\\n| 30\u201339     | 12.43        |\\n| 40\u201349     | 5.94         |\\n| 50\u201359     | 1.08         |\\n| 60\u201369     | 0.54         |\\n| 70+       | 0.00         |\\n| prefer not to answer | 1.08 |\\n\\n**Table 10**: Age ranges of survey respondents. Of 295 participants, 185 selected an age range.\"}"}
{"id": "acl-2023-long-507", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Country          | % Respondents |\\n|------------------|--------------|\\n| United States    | 76.14        |\\n| United Kingdom   | 6.82         |\\n| India            | 4.55         |\\n| Germany          | 2.27         |\\n| Spain            | 2.84         |\\n| Canada           | 1.14         |\\n| New Zealand      | 1.14         |\\n| Sweden           | 1.14         |\\n\\nTable 11: Country of residence of survey respondents. Of 295 participants, 194 selected a country of residence.\"}"}
