{"id": "lrec-2022-1-618", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialogue Collection for Recording the Process of Building Common Ground in a Collaborative Task\\n\\nKoh Mitsuda\\\\(^1\\\\), Ryuichiro Higashinaka\\\\(^1\\\\), Yuhei Oga\\\\(^2\\\\)*, Sen Yoshida\\\\(^1\\\\)\\n\\n\\\\(^1\\\\)NTT Human Informatics Laboratories, NTT Corporation, Japan\\n\\\\(^2\\\\)Graduate School of Systems and Information Engineering, University of Tsukuba, Japan\\n\\n\\\\{koh.mitsuda.td,ryuichiro.higashinaka.tp,sen.yoshida.tu\\\\}@hco.ntt.co.jp\\ns2020716@s.tsukuba.ac.jp\\n\\nAbstract\\nTo develop a dialogue system that can build common ground with users, the process of building common ground through dialogue needs to be clarified. However, the studies on the process of building common ground have not been well conducted; much work has focused on finding the relationship between a dialogue in which users perform a collaborative task and its task performance represented by the final result of the task. In this study, to clarify the process of building common ground, we propose a data collection method for automatically recording the process of building common ground through a dialogue by using the intermediate result of a task. We collected 984 dialogues, and as a result of investigating the process of building common ground, we found that the process can be classified into several typical patterns and that conveying each worker's understanding through affirmation of the counterpart's utterances especially contributes to building common ground. In addition, toward dialogue systems that can build common ground, we conducted an automatic estimation of the degree of built common ground and found that its degree can be estimated quite accurately.\\n\\nKeywords: dialogue systems, data collection, common ground, collaborative task\\n\\n1. Introduction\\nDialogue systems need to be able to build common ground with users accurately for them to successfully perform joint activities (Clark, 1996; Traum, 1994; Kopp and Kramer, 2021). To develop dialogue systems able to handle common ground, we believe the key is to reveal the process of building common ground.\\n\\nIn previous studies, the process of building common ground has been investigated by analyzing a dialogue between people who accomplish a collaborative task (Benotti and Blackburn, 2021; Chandu et al., 2021). These studies mainly focused on finding the relationship between a dialogue and the final result of the task (Anderson et al., 1991; Foster et al., 2008; He et al., 2017); however, the studies on the process of building common ground itself have not been well conducted. A few exceptions include the study by Udagawa and Aizawa (2020), who associated reference expressions with objects in a collaborative task to analyze the process of building common ground. Bara et al. (2021) recorded intermediate common ground by having each worker answer questions about his/her understanding of his/her partner's belief and behavior in real time during a collaborative task in a virtual space. These studies manually record common ground, which is costly.\\n\\nIn this paper, we look into the process of building common ground by automatically recording the intermediate result of the task as a proxy for the common ground being built. Recording the intermediate result of the task enables us to collect dialogues with common ground at low cost and investigate the information that the humans mutually believe at each step of dialogue.\\n\\nWe design a task called CommonLayout (Figure 1), in which two workers design the layout of objects into a common one through text chat. To quantify intermediate common ground through dialogue, we use the similarity of layouts created by two workers.\\n\\nAs a result of collecting such dialogues and investigating the process of building common ground, we found that the process can be classified into several typical clusters and that conveying each worker's understanding through affirmation of the counterpart's utterances especially contributes to building the common ground. In addition, toward the dialogue systems that can build common ground, we conducted an automatic estimation of the degree to which common ground is being built and found that its degree can be estimated quite accurately.\\n\\nThis paper makes three contributions:\\n\u2022 Propose a method to automatically and directly record the built common ground through dialogue. We can collect dialogues that show the process of building common ground through dialogue.\\n\u2022 Conduct an automatic estimation of the degree to which common ground is being built.\"}"}
{"id": "lrec-2022-1-618", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"building common ground without manual annotation.\\n\\n\u2022 Clarify the process of building common ground. We found that the process of building common ground can be divided into typical clusters and that positive evaluations and empathy particularly contribute to building common ground.\\n\\n\u2022 Conduct an experiment to automatically estimate the degree of common ground toward a dialogue system that can build common ground and show that the estimation is accurate.\\n\\nAlthough our task may seem artificial, as far as we know, this study is the first attempt to quantify the process of building common ground. Since the process of building common ground can appear in any form of dialogue that needs mutual understanding, we consider our work to be a valuable contribution to dialogue system research.\\n\\n2. Related Work\\n\\nIn previous studies, the process of building common ground has been investigated by analyzing human-human dialogue in which a collaborative task is performed (Benotti and Blackburn, 2021; Chandu et al., 2021). Common ground is considered to be successfully built when the task is successfully performed on the basis of the final result of the task. For example, the collaborative tasks for operating certain objects in maps (Anderson et al., 1991; Carletta et al., 1991; Bard et al., 2000; Denis and Striegnitz, 2012) and puzzles (Foster et al., 2008; Spanger et al., 2009; Tokunaga et al., 2012) have been investigated.\\n\\nRecently, in the task of finding or creating certain objects, various collaborative games have been proposed regarding graphics (Liu et al., 2012; de Vries et al., 2017; Kim et al., 2019; Udagawa and Aizawa, 2019), virtual environments (Polyak and Davier, 2017; Ilinykh et al., 2019; Hahn et al., 2020; Jayannavar et al., 2020), and texts (He et al., 2017; Lewis et al., 2017; Gero et al., 2020). Another major line of studies, albeit in human-system dialogue, aims at manipulating robots in real space by generating utterances that can be grounded to physical objects (Moratz and Tenbrink, 2006; Hough and Schlangen, 2017; Chai et al., 2017; Van Waveren et al., 2019). In these studies, the relationship between the dialogue and common ground is analyzed on the basis of the final result of the task. This is because these studies aim to clarify what dialogue phenomena occur in the dialogue where the interlocutors need to build common ground for completing a task successfully. However, in such a framework, the process of building common ground is not recorded and thus cannot be analyzed.\\n\\nOur research attempts to record the process of building common ground and is most closely related to the task of OneCommon (Udagawa and Aizawa, 2019; Udagawa and Aizawa, 2020). In OneCommon (Udagawa and Aizawa, 2019), a graphical plane is prepared including multiple dots in random positions, sizes, and colors. Two workers are given only a slightly different portion of the plane, and each worker selects one dot through dialogue. The task is successful if the selected dots are identical between the workers. Grounding reference expressions to the dots between workers is considered to reflect the process of building common ground, and such expressions are manually annotated (Udagawa and Aizawa, 2020). In MindCraft (Bara et al., 2021), two players of Minecraft are given the knowledge and skills to work together to create a specific object. Every 75 seconds while proceeding with the task, a player must answer a predetermined question about the common ground (e.g., \u201cWhat do you think your counterpart is building right now?\u201d). Our research is similar to these studies but differs in two ways. First, we automatically record the common ground, making data collection less costly. Second, we clarify the process of building common ground on the basis of quantified common ground using the similarity of layouts.\\n\\n3. Data Collection\\n\\nIn this section, after explaining the requirements of the task for recording the process of building common ground, we describe the task description and the process of data collection.\\n\\n3.1. Requirements\\n\\nFor designing the task to reveal the process of building common ground, we followed OneCommon (Udagawa and Aizawa, 2019). OneCommon was proposed on the basis of the requirements that necessitated continuous and partially-observable contexts for introducing the difficulty of building common ground. The continuous context means that a task should include not only categorical information but also continuous information (thus, it is difficult to use symbolic expressions to describe the information) such as graphics, which makes it necessary for speakers to exchange multiple utterances to build common ground. The partially-observable context means that only partial information is shared among the workers. In such a context, the workers need to build common ground by sharing their own information through exchanges of utterances.\\n\\nIn addition to the requirements asserted in OneCommon, we defined two other requirements for our task. First, the task should require workers to perform multiple operations to complete it. This is because we want to collect intermediate task results as common ground. Second, the intermediate results of the task should be quantifiable in terms of its progress to quantify common ground being built.\\n\\n3.2. Task Description\\n\\nFigure 1 illustrates the proposed task, CommonLayout, in which two workers lay out the same figure set into a\"}"}
{"id": "lrec-2022-1-618", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Worker A started\\nWorker B started\\nA: Nice to meet you\\nB: Nice to meet you too\\nB: What shall we make?\\nA: Do you have any preferences?\\nB: I don't have any idea\\nA: I'm thinking too\\nB: The circle in the center seems to be a nose\\nA: Then let's make a face.\\n\\nFigure 2: Interface for collecting dialogues in CommonLayout. This example shows the interface presented to one of the workers. Each worker can see the common chat interface and his/her layout interface but cannot see the partner's one. Dialogue shown was originally in Japanese and translated by authors.\\n\\nFigure 3: Figure sets Simple (upper) and Building (lower) used in CommonLayout\\n\\n| Figure set | Simple | Building |\\n|------------|--------|----------|\\n| No. of figures | 5      | 7        |\\n| Workers    | 283    | 287      |\\n| Worker pairs | 210    | 213      |\\n| Dialogues  | 245    | 244      |\\n| Utterances | 6,652  | 6,632    |\\n| Utts./dialogue | 27.1  | 27.1     |\\n| Letters/utts. | 14.5  | 14.3     |\\n| Moves      | 14,580 | 19,584   |\\n| Moves/dialogue | 59.5  | 81.3     |\\n\\nTable 1: Statistics of collected data\\n\\nIn the data collection, 287 workers familiar with PC operations participated. They were recruited through a recruitment agency and paid for their participation. The workers were allowed to quit the experiment at any time. Two workers who had never met were randomly put into pairs for a total of 213 pairs, and each pair conducted the task four times (five figures from Simple/Building and seven figures from Simple/Building in a randomized order). We instructed the workers to be creative in their layout and decide on the final layout with as much input from their partner as possible. The dialogues were conducted in Japanese.\\n\\nTable 2 shows a collected dialogue of CommonLayout. This example is the dialogue collected in the session shown in Figure 2. The idea of the final layout was agreed upon by U14, and the figures are placed after U15. In this dialogue, we can see the process of building common ground, such as the utterances from U7 to U14, in which the idea of the figure placement was decided through consultation, and U24 or U29, in which the workers confirmed their placement.\\n\\nDue to the difficulty of assigning workers, 24 of 213 pairs performed four sessions multiple times.\"}"}
{"id": "lrec-2022-1-618", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Example of collected dialogue in Common-Layout. ID and S corresponds to utterance ID and speaker. This dialogue is collected in the session shown in Figure 2. Utterances were translated from Japanese by the authors.\\n\\n| ID | Utterance |\\n|----|-----------|\\n| 1  | A Nice to meet you. |\\n| 2  | B Nice to meet you too. |\\n| 3  | B What shall we make? |\\n| 4  | A Do you have any preferences? |\\n| 5  | B I don't have any idea. |\\n| 6  | A I'm thinking too. |\\n| 7  | B The circle in the center seems to be a nose. |\\n| 8  | A Then, let's make a face. |\\n| 9  | A The circle looks like a nose to me. |\\n| 10 | B I agree with you. |\\n| 11 | B How about Pinocchio? |\\n| 12 | A That's good. |\\n| 13 | A Shall we make Pinocchio? |\\n| 14 | B Okay. |\\n| 15 | B It moves, doesn't it? |\\n| 16 | A I just tried to change the angle of the bottom triangle, but I can't seem to do it. |\\n| 17 | A Let's create a tilted face. |\\n| 18 | B That's good. |\\n| 19 | B Let's move the mouth a little to the left. |\\n| 20 | A What do you think? |\\n| 21 | A I've just moved it. |\\n| 22 | B That's good. |\\n| 23 | B Put the nose in the middle a little bit. |\\n| 24 | A What about the shape on the left? |\\n| 25 | B I tried to make a downward-pointing triangle and a square for the eyes. |\\n| 26 | B A trapezoid looks like a head forelock. |\\n| 27 | A I see. |\\n| 28 | A I've just moved it around. |\\n\\nTable 3: Frequency of final layout patterns\\n\\n| Pattern       | Simple | Total |\\n|---------------|--------|-------|\\n| Perfect       | 13%    | 26%   |\\n| Shifted       | 15%    | 5%    |\\n| Switched 1    | 0%     | 3%    |\\n| Switched 2    | 32%    | 45%   |\\n| Symmetric     | 3%     | 3%    |\\n| Resized       | 10%    | 5%    |\\n| Scattered     | 27%    | 13%   |\\n\\nWe found that final layout patterns can be categorized into seven patterns. The patterns, except for Perfect, had different positions of figures. In addition to Perfect, we decided to regard Shifted as success of the task. This is because we did not instruct the workers to put the figures into the same position in absolute coordinates.\\n\\nTable 3 shows the frequency of final layout patterns in all samples annotated by a worker different from the authors. The table shows that the success rate (Perfect or Shifted) was 25% (= 19% + 6%). Regarding the figure sets, the success rate was higher in Building than in Simple. Regarding the number of figures, the frequency of each pattern was similar between five and seven; thus, the difficulty of the task does not depend on the number of figures. Interestingly, there were many cases in which the users failed in the task, making the data a valuable resource for analyzing how common ground can/cannot be built within a dialogue.\"}"}
{"id": "lrec-2022-1-618", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Quantification of Common Ground\\n\\nOn the basis of the final layout patterns, we introduce a measure to quantify common ground. We assume that the task succeeds at pattern 1\u20132 (Perfect and Shifted), almost succeeds at pattern 3\u20136 (Switched 1, Switched 2, Symmetric, and Resized), and fails at pattern 7 (Scattered). We call these patterns Success, Middle, and Fail. We introduce layout distance, which becomes small in Success. This measure is the sum of the distances between two arbitrary figures in a layout pair, as shown in the following equation.\\n\\n\\\\[\\n\\\\text{distance}(L_A, L_B) = \\\\sum_{i,j \\\\in \\\\text{Figures}} \\\\| \\\\vec{a}_{i,j} - \\\\vec{b}_{i,j} \\\\|,\\n\\\\]\\n\\nwhere \\\\(L_A\\\\) and \\\\(L_B\\\\) are the layouts created by workers A and B, and \\\\(\\\\vec{a}_{i,j}\\\\) and \\\\(\\\\vec{b}_{i,j}\\\\) are the vectors (representing x and y coordinates) defined between figures \\\\(i\\\\) and \\\\(j\\\\) in \\\\(L_A\\\\) and \\\\(L_B\\\\). The smaller the Euclidean distance, the more common ground is considered to have been built. On the basis of the layout distance, we can quantify the degree of common ground in each time step within a dialogue.\\n\\nFigure 5 shows the degree of common ground in each time step on average. Note that an utterance is regarded as one time step, and moving the same figure multiple times in succession is also regarded as one time step. The layout distance at the end of the task (when the time step is 50) is larger in the order of Success, Middle, and Fail, confirming that the degree of common ground can be quantified as we defined it.\\n\\nTo reveal the typical process of building common ground, we used k-Shape, which is a time-series clustering method based on k-means (Paparrizos and Gravano, 2015). We applied k-Shape to all 987 dialogues (i.e., 987 trajectories of layout distances) in the collected data. Figure 6 shows the results of time-series clustering on the layout distance. We set the number of clusters to five because similar clusters appear if this number is increased, and fewer \\\\(k\\\\) led to clusters with inconspicuous traits. The x-axis shows time steps, and the y-axis shows the z-normalized layout distance. The ratios of the number of dialogues for the five clusters (Clusters 1 to 5) were 29%, 28%, 24%, 12%, and 7%, respectively.\\n\\nIn Cluster 1, the layout distance consistently decreased throughout the dialogue; thus, common ground seems to have been smoothly built. From Clusters 2 to 4, building common ground stagnated. In these clusters, the workers discussed the positions of partial figures until the middle of the dialogue. They then confirmed the idea of a final layout and moved the remaining figures to the same positions towards the end. Cluster 5 corresponds to Scattered in final layout patterns where common ground was not properly built.\\n\\nTable 4: Correspondence between task results and clusters (denoted as C1\u2013C5) in the process of building common ground shown in Figure 6.\\n\\n| Cluster | Success | Middle | Fail |\\n|---------|---------|--------|------|\\n| C1      | 28%     | 58%    | 14%  |\\n| C2      | 32%     | 57%    | 11%  |\\n| C3      | 27%     | 58%    | 15%  |\\n| C4      | 15%     | 51%    | 34%  |\\n| C5      | 10%     | 46%    | 44%  |\"}"}
{"id": "lrec-2022-1-618", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Top five statistically significant occurrences of linguistic expressions in dialogues divided by the results of the task. **\u201d and *\u2019 denotes $p < 0.01$ and $p < 0.05$ in statistical test.\\n\\n4.3. Process of Building Common Ground\\n\\nOn the basis of final layout patterns and layout distance, we look into linguistic phenomena in the dialogues where common ground is successfully built or not. These phenomena are important to create a dialogue system that can build common ground with users. We first divided dialogues into three types (Success, Middle, and Fail) and investigated the linguistic expressions and dialogue acts that frequently occur in Success, Middle, or Fail. In addition, we used a Hidden Markov Model (HMM) to model the transitions of dialogue acts in each kind of dialogue and then investigated which transitions are especially effective to build common ground.\\n\\nTable 5 shows the top five statistically significant occurrences of linguistic expressions in the dialogues divided by the results of the task (Success, Middle, and Fail). Each expression represents a predicate (translated by the authors). A Japanese morphological analyzer JTAG (Fuchi and Takagi, 1998) was used to extract the expressions. Fisher\u2019s exact probability test was used as the statistical test, and the expressions that appeared significantly more often in dialogues categorized in one of the three types were obtained by sorting them in ascending order by p-value. From this table, regarding Success, expressions that indicate positive evaluation such as \u201cGood\u201d and those that indicate graphic manipulation such as \u201cLet\u2019s do it\u201d and \u201cIt\u2019s done\u201d appear more frequently. As for Middle, instead of positive expressions, there are many expressions showing sympathy such as \u201cI agree\u201d and graphic manipulations such as \u201cI did it.\u201d In the case of Fail, the expressions regarding positive evaluations and graphic manipulations do not appear, and ambiguous expressions such as \u201cWhat do we do?\u201d and \u201cI\u2019m looking at it\u201d appeared more frequently. These results suggest that evaluation expressions or sympathetic expressions that convey one\u2019s understanding and behavior are important in building common ground successfully.\\n\\nTable 6 shows the top three statistically significant occurrences of dialogue acts in dialogues divided by the results of the task. We used the label set of dialogue acts proposed by Meguro et al. (2011). There are 33 types for dialogue acts (e.g., self-disclosure: disclosure of preferences and feelings, information: delivery of objective information, and sympathy: sympathetic utterances and praise). For the estimation, we used the support vector machine trained by (Higashinaka et al., 2014). As in the analysis for Table 5, Fisher\u2019s exact probability test was used for listing the dialogue acts. From this table, we can see that in Success, there are many expressions that affirm the counterpart, such as self-disclosure of positive preference, thanks, and empathy. Similar to Success, many expressions of empathy appeared in Middle. There are also many questions about the evaluation. In the case of Fail, there are many expressions that are different from those of Success and Middle, such as information and greetings. Expressions related to evaluation also appeared, but their contents were negative. These results suggest that, as in the case of Table 5, self-disclosure regarding positive evaluation and empathy that conveys one\u2019s understanding as well as information that conveys one\u2019s behavior is important for building common ground.\\n\\nThe analysis up to here has focused on linguistic expressions and dialogue acts; by focusing on the transitions of the dialogue act and layout distance, it may be possible to clarify what kind of dialogue particularly contributes to the process of building common ground.\\n\\nTo investigate the transition of the dialogue acts, we modeled it using a HMM, which is commonly used to learn the structure of data series with an unknown number of states (Rabiner and Juang, 1986). The HMM library, hmmlearn, was used for modeling. The learning method followed that of (Meguro et al., 2014). Specifically, we initialized the ergodic HMM so that only the dialogue acts of a specific speaker (Speaker A) were output from half of the states, and only the dialogue acts of the counterpart speaker (Speaker B) were output from the other half of the states. The number of states ranged from 1 to 10, and 10 HMMs were trained in each number of states, resulting in 100 HMMs. The optimal HMM was selected using the Minimum Description Length (MDL) criterion.\\n\\nFigures 7 and 8 show the HMMs constructed from the series of dialogue acts in Success and Fail. One of Middle is omitted because a similar HMM was constructed for Success. In each state, \u2018A\u2019 and \u2018B\u2019 represent the two speakers, and dialogue acts with observation probability are listed. \u2018p\u2019 on the edges represents the transition probability.\"}"}
{"id": "lrec-2022-1-618", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: HMM created from a series of dialogue acts in dialogues categorized in Success. \u2018A\u2019 and \u2018B\u2019 represent two speakers, and \u2018p\u2019 and \u2018d\u2019 on edge represent transition probability and average difference of layout distance in transition. Edge is thicker when transition probability is higher.\\n\\nFigure 8: HMM created from a series of dialogue acts in dialogues categorized in Fail.\\n\\n5. Estimation Presentments\\n\\nToward dialogue systems that can build common ground with users, we conducted an automatic estimation of the degree of built common ground. We address the problem of estimating the degree to which common ground is being built, represented by layout distance. The definition of the problem is that, at a certain time step \\\\( i \\\\) in the dialogue, given the dialogue context (the utterances from \\\\( U_1 \\\\) to \\\\( U_i \\\\)) and layout \\\\( L_{X,i} \\\\), where \\\\( X \\\\) is a speaker of \\\\( U_i \\\\), the layout distance at the time step \\\\( i \\\\) is estimated without the information about the layout of worker \\\\( X \\\\)\u2019s partner.\\n\\n5.1. Model and Data Preparation\\n\\nFigure 9 shows the model architecture for estimating the layout distance from the dialogue and one\u2019s own layout. We trained a model as in Soleymani et al. (2019), which takes into account both the dialogue and layout by using pre-trained models. We used Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2019) for the dialogue and Residual Network (ResNet) (He et al., 2016) for the layout as pre-trained models. The embeddings obtained from the pre-trained models were concatenated and converted into a vector with fully connected layers, finally producing a scalar value denoting the layout distance (i.e., the degree of common ground).\\n\\nThe collected 984 dialogues were divided into 8:1:1 (training, development, and test set). We split the data in a manner in which the dialogues by the same pair of workers were not included in different sets. The layout distance was normalized from zero to one using min-max normalization. The pre-trained models were a BERT-base model trained for Japanese from https://huggingface.co/cl-tohoku and ResNet-18 model from https://huggingface.co/cl-tohoku.\"}"}
{"id": "lrec-2022-1-618", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Layout corresponds to layout of speaker X when X utters U$i$. FC denotes fully connected layer.\\n\\nFigure 9: Model architecture for estimating layout distance from given dialogue and layout (Dialogue+Layout).\\n\\n5.2. Results\\n\\nTable 7 shows the MSEs between the estimated and true layout distance from the given dialogue and layout. The MSE for Dialogue+Layout was the lowest with significant differences (p < .05) in a Steel-Dwass multiple comparison test (Dwass, 1960), confirming the effectiveness of considering both dialogue and layout. The performances of Dialogue and Layout were lower than that of Dialogue+Layout. This may be because Dialogue+Layout determined which figures were grounded from the layout information with the content of the dialogue. Since the MSE for Dialogue+Layout was 0.0178, the error between the estimated layout distance and true value was about +/\u2013 0.134 on average, which confirms that the estimation is quite accurate. These results suggest that it will be possible to create a dialogue system that can perform CommonLayout with users while understanding the degree of built common ground.\\n\\n6. Conclusion\\n\\nTo reveal the process of building common ground in dialogue, we devised a task called CommonLayout, in which two workers collaboratively placed figures in a common layout. For automatically recording the process of building common ground through a dialogue, we utilized intermediate task results where commonality of layouts between the workers is regarded as common ground being built within a dialogue. We collected 984 dialogues where workers performed CommonLayout. By investigating the collected data, we found seven final layout patterns. On the basis of these final layout patterns, we introduced layout distance as quantification of common ground. Time-series clustering was applied to the transitions of the layout distance for revealing a typical process of building common ground. We found that the process of building common ground can be divided into five typical clusters. In addition, we analyzed the linguistic phenomena that lead to task accomplishment in terms of dialogue and layout distance. The results suggest that conveying one's understanding to others through positive evaluation and empathy is the most important factor in building common ground. We also found that the degree of common ground being built can be estimated to some extent.\\n\\nFuture work includes automatically estimating a partner's figure layout from dialogue and one's own layout since dialogue systems must be able to understand the belief of a partner for better task success. Eventually, we want to integrate such an estimator into a dialogue system so that it can successfully perform CommonLayout with users. For this purpose, we can refer to the dialogue model proposed by Fried et al. (2021), which successfully accomplished OneCommon with users. In addition, a method needs to be developed for quantifying common ground in other collaborative tasks, such as those requiring more elaborate and complex interactions, to verify the generalizability of the results.\"}"}
{"id": "lrec-2022-1-618", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAnderson, A. H., Bader, M., Bard, E. G., Boyle, E., Doherty, G., Garrod, S., Isard, S., Kowtko, J., McAlister, J., Miller, J., Sotillo, C., Thompson, H. S., and Weinert, R. (1991). The HCRC map task corpus. Computational Linguistics, 34(4):351\u2013366.\\n\\nBara, C.-P., CH-Wang, S., and Chai, J. (2021). MindCraft: Theory of mind modeling for situated dialogue in collaborative tasks. In Proc. of EMNLP, pages 1112\u20131125.\\n\\nBard, E. G., Anderson, A. H., Sotillo, C., Doherty-Sneddon, M. A. G., and Newlands, A. (2000). Controlling the intelligibility of referring expressions in dialogue. Journal of Memory and Language, 42:1\u201322.\\n\\nBenotti, L. and Blackburn, P. (2021). Grounding as a collaborative process. In Proc. of EACL, pages 515\u2013531.\\n\\nCarletta, J., Isard, A., Isard, S., Kowtko, J. C., Doherty-Sneddon, G., and Anderson, A. H. (1991). The reliability of dialogue structure coding scheme. Language and Speech, 23(1):13\u201332.\\n\\nChai, J. Y., Fang, R., Liu, C., and She, L. (2017). Collaborative language grounding toward situated human-robot dialogue. AI Magazine, 37(4):32\u201345.\\n\\nChandu, K. R., Bisk, Y., and Black, A. W. (2021). Grounding 'grounding' in NLP. In Findings of ACL-IJCNLP, pages 4283\u20134305.\\n\\nClark, H. H. (1996). Using language. Cambridge university press.\\n\\nde Vries, H., Strub, F., Chandar, S., Pietquin, O., Larochelle, H., and Courville, A. C. (2017). Guess-What?! visual object discovery through multi-modal dialogue. In Proc. of CVPR, pages 5503\u20135512.\\n\\nDenis, A. and Striegnitz, K. (2012). A collaborative puzzle game to study situated dialog. In Proc. of AAAI, pages 37\u201340.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL, pages 4171\u20134186.\\n\\nDwass, M. (1960). Some k-sample rank-order tests. Contributions to probability and statistics, pages 198\u2013202.\\n\\nFoster, M. E., Bard, E. G., Guhe, M., Hill, R. L., Oberlander, J., and Knoll, A. (2008). The roles of haptic-ostensive referring expressions in cooperative, task-based human-robot dialogue. In Proc. of HRI, pages 295\u2013302.\\n\\nFried, D., Chiu, J., and Klein, D. (2021). Reference-centric models for grounded collaborative dialogue. In Proc. of EMNLP, pages 2130\u20132147.\\n\\nFuchi, T. and Takagi, S. (1998). Japanese morphological analyzer using word co-occurrence -JTAG-. In Proc. of COLING, pages 409\u2013413.\\n\\nGero, K. I., Ashktorab, Z., Dugan, C., Pan, Q., Johnson, J., Geyer, W., Ruiz, M., Miller, S., Millen, D. R., Campbell, M., Kumaravel, S., and Zhang, W. (2020). Mental models of AI agents in a cooperative game setting. In Proc. of CHI, pages 1\u201312.\\n\\nHahn, M., Krantz, J., Batra, D., Parikh, D., Rehg, J. M., Lee, S., and Anderson, P. (2020). Where are you? localization from embodied dialog. In Proc. of EMNLP, pages 806\u2013822.\\n\\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition. In Proc. of CVPR, pages 770\u2013778.\\n\\nHe, H., Balakrishnan, A., Eric, M., and Liang, P. (2017). Learning symmetric collaborative dialogue agents with dynamic knowledge graph embeddings. In Proc. of ACL, pages 1766\u20131776.\\n\\nHigashinaka, R., Imamura, K., Meguro, T., Miyazaki, C., Kobayashi, N., Sugiyama, H., Hirano, T., Makino, T., and Matsuo, Y. (2014). Towards an open domain conversational system fully based on natural language processing. In Proc. of COLING, pages 928\u2013939.\\n\\nHough, J. and Schlangen, D. (2017). It's not what you do, it's how you do it: Grounding uncertainty for a simple robot. In Proc. of HRI, pages 1\u201310.\\n\\nIlinykh, N., Zarrie\u00df, S., and Schlangen, D. (2019). Meet Up! a corpus of joint activity dialogues in a visual environment. In Proc. of SEMDIAL, pages 1\u201310.\\n\\nJayannavar, P., Narayan-Chen, A., and Hockenmaier, J. (2020). Learning to execute instructions in a Minecraft dialogue. In Proc. of ACL, pages 2589\u20132602.\\n\\nKim, J.-H., Kitaev, N., Chen, X., Rohrbach, M., Zhang, B.-T., Tian, Y., Batra, D., and Parikh, D. (2019). CoDraw: Collaborative drawing as a testbed for grounded goal-driven communication. In Proc. of ACL, pages 6495\u20136513.\\n\\nKingma, D. P. and Ba, J. (2015). Adam: A method for stochastic optimization. In Proc. of ICLR, pages 1\u201315.\\n\\nKopp, S. and Kramer, N. (2021). Revisiting human-agent communication: The importance of joint co-construction and understanding mental states. Frontiers in Psychology, 12.\\n\\nLewis, M., Yarats, D., Dauphin, Y. N., Parikh, D., and Batra, D. (2017). Deal or no deal? end-to-end learning for negotiation dialogues. In Proc. of EMNLP, pages 2443\u20132453.\\n\\nLiu, C., Fang, R., and Chai, J. Y. (2012). Towards mediating shared perceptual basis in situated dialogue. In Proc. of SIGDIAL, pages 140\u2013149.\\n\\nMeguro, T., Higashinaka, R., Minami, Y., and Dohsaka, K. (2011). Evaluation of listening-oriented dialogue control rules based on the analysis of HMMs. In Proc. of Interspeech, pages 809\u2013812.\\n\\nMeguro, T., Minami, Y., Higashinaka, R., and Dohsaka, K. (2014). Learning to control listening-oriented dialogue using partially observable markov...\"}"}
{"id": "lrec-2022-1-618", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Spatial reference in linguistic human-robot interaction: Iterative, empirically supported development of a model of projective relations. \\n\\nPaparrizos, J. and Gravano, L. (2015). k-Shape: Efficient and accurate clustering of time series. In Proc. of SIGMOD, pages 1855\u20131870.\\n\\nPolyak, S. and Davier, A. V. (2017). Analyzing game-based collaborative problem solving with computational psychometrics. In Proc. of SIGKDD, pages 1\u201314.\\n\\nRabiner, L. and Juang, B. (1986). An introduction to hidden markov models. IEEE ASSP magazine, 3(1):4\u201316.\\n\\nSoleymani, M., Stefanov, K., Kang, S.-H., Ondras, J., and Gratch, J. (2019). Multimodal analysis and estimation of intimate self-disclosure. In Proc. of ICMI, pages 59\u201368.\\n\\nSpanger, P., Yasuhara, M., Iida, R., and Tokunaga, T. (2009). Using extra linguistic information for generating demonstrative pronouns in a situated collaboration task. In Proc. of preCogsci, pages 1\u20138.\\n\\nTokunaga, T., Iida, R., Terai, A., and Kuriyama, N. (2012). The REX corpora: A collection of multimodal corpora of referring expressions in collaborative problem solving dialogues. In Proc. of LREC, pages 422\u2013429.\\n\\nTraum, D. R. (1994). A computational theory of grounding in natural language conversation. Technical report, Rochester Univ NY Dept of Computer Science.\\n\\nUdagawa, T. and Aizawa, A. (2019). A natural language corpus of common grounding under continuous and partially-observable context. In Proc. of AAAI, pages 7120\u20137127.\\n\\nUdagawa, T. and Aizawa, A. (2020). An annotated corpus of reference resolution for interpreting common grounding. In Proc. of AAAI, pages 9081\u20139089.\\n\\nVan Waveren, S., Carter, E. J., and Leite, I. (2019). Take one for the team: The effects of error severity in collaborative tasks with social robots. In Proc. of IVA, pages 151\u2013158.\"}"}
