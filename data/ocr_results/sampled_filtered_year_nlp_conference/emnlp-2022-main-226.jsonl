{"id": "emnlp-2022-main-226", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"parison with future works if there is any kind of changes.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nExisting image captioning systems are dedicated to generating narrative captions for images, which are spatially detached from the image in presentation. However, texts can also be used as decorations on the image to highlight the key points and increase the attractiveness of images. In this work, we introduce a new task called captioning on image (CapOnImage), which aims to generate dense captions at different locations of the image based on contextual information. For this new task, we introduce a large-scale benchmark called CapOnImage2M, which contains 2.1 million product images, each with an average of 4.8 spatially localized captions. To fully exploit the surrounding visual context to generate the most suitable caption for each location, we propose a multi-modal pre-training model with multi-level pre-training tasks that progressively learn the correspondence between texts and image locations from easy to hard. To avoid generating redundant captions for nearby locations, we further enhance the location embedding with neighbor locations. Compared with other image captioning model variants, our model achieves the best results in both captioning accuracy and diversity aspects.\\n\\n1 Introduction\\nBuilding upon the advances in computer vision and natural language processing areas, the new research direction called vision-and-language has attracted more and more attentions, which pushes to tackle new problems that need to bridge the two areas to advance the concept comprehension and reasoning capabilities.\\n\\nThe image captioning task, as one of the most classic vision-and-language tasks, aims to generate natural language descriptions for images (Vinyals et al., 2015; Anderson et al., 2018; Zhang et al., 2021; Johnson et al., 2016). However, the captions and the images in this task are spatially detached in presentation, without any association between each other (Figure 1(a)). In fact, there are many scenarios where the image and text are tightly associated. For example, the product images on e-commercial website (Figure 1(b)) usually contain descriptive texts, explaining multiple perspectives of the product (e.g., product characteristics, selling points, etc), which makes the image more informative and attractive. On the social media platform, users usually upload daily pictures with descriptive texts as decorations (Figure 1(c)). Therefore, it is significant to explore captioning on the image, which requires to consider not only the visual description, but also the text description placement on the image. Besides, to generate the informative captions in these scenarios, additional textual knowledge is usually needed, such as the product information.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, in this work, we introduce a new task called CapOnImage, which aims to generate dense captions at different locations of the image based on contextual information. The CapOnImage task involves two steps, where the model needs to first predict a reasonable and aesthetic text layout (Arroyo et al., 2021; Gupta et al., 2020; Jyothi et al., 2019), and then generates a phrase or sentence for each text box. In this work, we simplify this task to generate captions for a provided list of text box locations, thereby removing the requirement for layout prediction. Therefore, the main focus of this work is to generate captions that are most suitable for the corresponding image locations.\\n\\nThe CapOnImage task involves two new challenges:\\n\\n(i) **Better understanding of context**: the captions at different locations can be greatly diverse. As shown in Figure 1(b), the texts around the product are descriptive captions describing the product features, while those at the bottom introduce the selling points. Therefore, the model needs to fully exploit the visual context around the text box to determine what caption is suitable to generate here. Since our task aims to generate captions on image, location context is vital for our task which is also validated on Table 2. Overall, compared with traditional caption task, the CapOnImage task needs better understanding of context.\\n\\n(ii) **Caption redundancy**: some texts can be suitable for adjacent locations. Therefore, if the model can only \u201csee\u201d the isolated text box without surrounding ones, it tends to generate the same caption for nearby text boxes because it suits all of them, thus causing the problem of caption redundancy.\\n\\nTo solve the aforementioned challenges, we propose a multi-modal pre-training and fine-tuning framework which contains multi-level pre-training tasks to effectively exploit multi-modal contextual information.\\n\\nFirst, to better exploit context, we design multi-level pre-training tasks to help the model \u201cfeel\u201d the context. It explicitly equip the model with the ability to distinguish which captions are appropriate for the current location and image while which are not. Besides, inspired by the learning progression of easy to complex biological vision systems, we further propose a progressive training strategy which learns multi-level pre-training tasks from easy to hard.\\n\\nSecond, to solve the problem of caption redundancy, we introduce a neighbor-enhanced location encoding module, which utilizes the surrounding text box locations as context, so that our model can \u201csee\u201d the adjacent context. We show the captioning diversity results with different ranges of adjacent text boxes involved in the location encoding module, and demonstrate the importance of such neighbor context.\\n\\nIn order to evaluate our model and benchmark progress in the CapOnImage task, we introduce the CapOnImage2M dataset. It contains 2.1 million product images crawled from an e-commercial website, and each image contains multiple spatially localized captions describing the associated product. We automatically acquire the text contents and their spatial locations from the image via OCR (Li et al., 2017; Liu et al., 2018), and finally collect 4.8 captions for each image on average. We also crawl the product title and attributes as additional context information for caption generation. With the empirical analysis on CapOnImage2M dataset, we show that the visual context, location information and the additional product information are beneficial for the caption generation, and our model can generate corresponding types of captions at different spatial locations (Figure 3). Furthermore, we demonstrate that our proposed neighbor-enhanced location encoding module and multi-level pre-training tasks significantly improve the captioning accuracy and diversity.\\n\\nThe main contributions of this work are as follows:\\n\\n\u2022 We introduce a new vision-and-language task called CapOnImage, which requires the model to tightly associate image and texts as a whole.\\n\\n\u2022 We analyze the challenges of CapOnImage task and propose a context enhanced model with progressive training strategy, which achieves the best result compared with other image captioning model variants.\\n\\n\u2022 We propose a large-scale multi-modal dataset called CapOnImage2M, with 50 categories images and localized captions to support the CapOnImage research.\\n\\n2 Related Work\\n\\nIn recent years, significant progress has been made in the image captioning task (Vinyals et al., 2015; Anderson et al., 2018; Huang et al., 2019; Pan et al., 2020; Zhang et al., 2021), which aims to describe the image content in one natural sentence. With\"}"}
{"id": "emnlp-2022-main-226", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the advances in visual understanding abilities, researchers are not satisfied with generating dull and less informative captions and extend the traditional image captioning task along two directions. The first direction is called dense captioning (Johnson et al., 2016; Melas-Kyriazi et al., 2018; Krishna et al., 2017a; Wang et al., 2021; Song et al., 2021), which targets to describe detailed visual content with a set of sentences. Johnson et al. (Johnson et al., 2016) propose a fully convolutional localization network to unify the object detection (Sermanet et al., 2014; Girshick et al., 2014; Ren et al., 2015) and image captioning in one framework to predict a set of descriptions across object regions. Krishna et al. (Krishna et al., 2017a) migrate it to the video, which aims to predict sequential event proposals and generate description for each clip. In these works, the dense captions deliver more details of visual content than traditional single sentence. The second direction is called text-aware image captioning (Biten et al., 2019; Sidorov et al., 2020; Yang et al., 2021), where the model generates captions not only according to the image, but also utilizes additional textual information as context. Besides, Sidorov et al. (Sidorov et al., 2020) and Gurari et al. (Gurari et al., 2020) propose to generate image captions with scene texts, which exploit OCR tokens as the textual context.\\n\\nAlthough impressive progresses have been made along the two directions, they still remain separate. The proposed CapOnImage task can be considered as a combination of the two directions, where the model needs to first predict the text layout (spatial locations on the image) and then generate caption for each location conditioned on both the image and textual information.\\n\\nThere are several key distinctions between our proposed CapOnImage task and dense captioning:\\n1) Dense image captioning aims to generate captions for subregions within an image, and there is no length limitation of captions. However, our task is to generate text and affix it to specific regions within an image, and the text length should be controlled according to the region size.\\n2) The visual content is the only input of dense captioning. While for our task, there are three inputs: visual content, additional textual information, and the specified location to affix. All of them will impact the generated text content.\\n3) The generated text is a plain description of the visual content for dense captioning. But for our task, the generated text is on the image, making it more informative, together with the visual content.\\n\\nIn this section, we introduce our proposed CapOnImage2M dataset, which is the benchmark for the CapOnImage task. We first present an overview of the dataset collection and statistics, and then compare it with other related image captioning datasets. A more detailed datasheet describing the motivation, composition, and recommended uses of our CapOnImage2M dataset following (Gebru et al., 2018) can be found in the Appendix A.\\n\\n### 3.1 Dataset Collection and Statistics\\n\\nThe CapOnImage2M dataset contains 2.1 million product images crawled from a Chinese e-commerce website, where each image contains both the product and descriptive captions describing the product features, efficacy, brand and so on. (detail information, e.g word cloud, can be found in Appendix A.) For each image, we employ an OCR toolkit to recognize the texts and their spatial locations on the image, and remove the noise with high perplexities by a pre-trained GPT.\\n\\n### 3.2 Comparison with Other Datasets\\n\\nIn Table 1, we compare our CapOnImage2M dataset with other image captioning datasets. The CapOnImage2M dataset is substantially larger in both the number of images and texts. Unlike VG (Krishna et al., 2017b), where the dense captions independently describe different regions of the image, the CapOnImage2M dataset contains dense captions that describe the same product from different aspects. In addition to the dense captions on the image, each image also comes with a product title and attributes with an average length of 2.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Model\\n\\nIn this section, we introduce our CapOnImage method based on the pre-training and fine-tuning framework as illustrated in Figure 2. First, we introduce the multi-modal representation of visual, location coordinates, and product information of the given input images. Then, a progressive training strategy with multi-level pre-training tasks is proposed to enhance the correspondence learning between textbox locations and captions for caption generation with a multi-layer transformer.\\n\\n4.1 Input Representation\\n\\nThe inputs of our model include three parts from different modalities: the visual image, the textbox location coordinate and the textual product information. We independently encode each modality input as a sequence of $d$-dimensional feature vectors as follows.\\n\\n**Image representation.** Given the image, we extract the grid features with standard ResNet-50 (He et al., 2016) backbone, which is further end-to-end fine-tuned with our model. We flatten the $k \\\\times k$ feature map into a sequence and add spatial position embedding similarly as DETR (Carion et al., 2020). Specifically, for the $i$-th grid whose horizontal and vertical indexes are $x_i$ and $y_i$, we add learnable spatial embedding and segment embedding which indicates the image modality to the appearance feature $\\\\hat{v}_i$ as follows:\\n\\n$$\\\\hat{v}_i = v_i + [\\\\text{Emb}_h(x_i); \\\\text{Emb}_v(y_i)] + \\\\text{SE}_v,$$\\n\\n(1)\\n\\nwhere $\\\\text{Emb}_h(\\\\cdot)$ and $\\\\text{Emb}_v(\\\\cdot)$ are horizontal and vertical embedding layers with the output dimension of $d_2$, $[; ]$ denotes concatenation and SE denotes segment embedding. Finally, we represent the image with a sequence of patch features $\\\\hat{V} = \\\\{\\\\hat{v}_1, \\\\cdots, \\\\hat{v}_{k \\\\times k}\\\\}$.\\n\\n**Neighbor-enhanced location representation.** We represent a text box location with 2D coordinates $\\\\{(x_{\\\\text{min}}, y_{\\\\text{min}}), (x_{\\\\text{max}}, y_{\\\\text{max}})\\\\}$, where $(x_{\\\\text{min}}, y_{\\\\text{min}})$ is the top left corner coordinate and $(x_{\\\\text{max}}, y_{\\\\text{max}})$ is the bottom right corner coordinate. We map the real value coordinates into the $k \\\\times k$ grid and represent them with the same spatial position embeddings as image:\\n\\n$$e_{\\\\text{cur}} = [\\\\text{Emb}_h(x_i); \\\\text{Emb}_v(y_i); \\\\text{Emb}_h(x_j); \\\\text{Emb}_v(y_j)],$$\\n\\n(2)\\n\\nwhere $\\\\{(x_i, y_i), (x_j, y_j)\\\\}$ is the corresponding grid index.\\n\\nFurthermore, to avoid the problem of caption redundancy for adjacent locations, we enhance the location representation with neighbor locations as context. We define the distance of two text boxes as the distance of their centers, the text boxes whose upper left corner are with smaller value of $x$-coordinate plus $y$-coordinate than the current one as the previous text boxes, and those larger than the current one as the next text boxes. Then, we employ the nearest previous textbox location and the nearest next textbox location as the neighbor context, and encode them similarly as $e_{\\\\text{cur}}$. After encoding, we concatenate them with the current location embedding and add a segment embedding indicating the location modality as follows:\\n\\n$$l = [W_1 \\\\cdot [e_{\\\\text{prev}}; e_{\\\\text{next}}]; W_2 \\\\cdot e_{\\\\text{cur}}] + \\\\text{SE}_l,$$\\n\\n(3)\\n\\nwhere $W_1 \\\\in \\\\mathbb{R}^{4 \\\\times d_2}$ and $W_2 \\\\in \\\\mathbb{R}^{2 \\\\times d_2}$ are learned matrices, $e_{\\\\text{prev}}$ and $e_{\\\\text{next}}$ are the neighbor location embeddings, SE is the segment embedding.\\n\\n**Product information representation.** To generate informative product descriptions, we also exploit product information as the textual context, which is the product title and attribute in this work. We concatenate them with a special $<SEP>$ token.\\n\\nGiven the product information $X = \\\\{x_1, \\\\cdots, x_K\\\\}$ with $K$ words, we embed these words via the same word embedding matrix as the target caption words, and add positional and segment embeddings as follows:\\n\\n$$w_{\\\\text{info}}_i = W_e \\\\cdot x_i + \\\\text{PE}_i + \\\\text{SE}_{x_i},$$\\n\\n(4)\\n\\nwhere $W_e$ is the word embedding matrix, PE denotes sequence positional embedding as in BERT (Devlin et al., 2019) and SE denotes segment embedding. Finally, we represent the product title with a sequence of $d$-dimensional feature vectors as $W_{\\\\text{info}} = \\\\{w_{\\\\text{info}}_k\\\\}_{k=1}^{K}$.\\n\\n4.2 Pre-training Tasks and Strategy\\n\\nAfter encoding each input modality into the common embedding space, we employ transformer layers on the multi-modal input to fuse the multi-modal information. To generate appropriate and diverse descriptions at different textbox locations,\"}"}
{"id": "emnlp-2022-main-226", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Illustration of our model with four input modalities: image patches, location coordinates, product information, and the predicted text tokens. Two pre-training tasks are employed to optimize the model with a progressive learning strategy from easy to difficult. \u201c=\u201d denotes parameter sharing and \u201c+\u201d denotes addition. We add English translation for product information for better understanding.\\n\\nwe pre-train the model with two pre-training tasks, including Caption Generation (CG) and Caption Matching (CM). We first pre-train the model with both CG and CM tasks, and then fine-tune it only with the CG task for the final caption generation.\\n\\nTask #1: Caption Generation (CG). We generate captions using the same multi-modal transformer layers as decoder following the prefix LM (Raffel et al., 2020; Dong et al., 2019). Each word prediction can attend to all the image features, neighbor-enhanced location and product information embeddings, as well as previous generated words. We adopt the auto-regressive training objective for the CG task, which can be expressed as follows:\\n\\n\\\\[\\nL_{CG} = -\\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\log p(y^*_t | y^*_{<t}, \\\\hat{V}, l, W_{info}; \\\\Theta),\\n\\\\]\\n\\nwhere \\\\(y^*_t\\\\) denotes the \\\\(t\\\\)-th word of ground-truth caption for the current textbox location, and \\\\(\\\\Theta\\\\) denotes all learnable parameters of the pre-training model. During the inference phase, we first encode the image, location and product information embeddings, and then feed a special start token [SOS] to predict the caption word by word.\\n\\nTask #2: Caption Matching (CM). To help the model learn which captions are appropriate for the current image and location while which are not, we further introduce another pre-training task called Caption Matching. It is similar to the ITM task commonly used in vision-and-language pre-training models (Chen et al., 2020; Li et al., 2020; Lu et al., 2019; Zhuge et al., 2021), which requires the model to predict if the image and caption are semantically aligned. A score \\\\(s\\\\) between 0 and 1 is predicted by the hidden output of the [SOS] token. The positive examples of this task are corresponding pairs in the dataset, while the negative examples can be diverse. In this work, we design three levels of negative example construction and progressively learn the task from easy to difficult.\\n\\nLevel-I: Image caption matching. The first negative level is to randomly replace the correct caption with descriptions of other images. Therefore, it is not consistent with the current image content. We expect the model can recognize such negative examples according to the visual image and product information, which are the easiest negative cases.\\n\\nLevel-II: Location caption matching. The second negative level is to replace the caption with those in other locations of the same image. It is more difficult than the Level-I because the negative caption exactly describes the current image but is not suitable for the current location. For example, the product efficacy descriptions may be inappropriate to appear on the left corner of the product image, while the product brand is more suitable. We expect the model can learn the relationship of texts and textbox locations according to the surrounding visual context.\\n\\nLevel-III: Neighbor-location caption matching. Since the captions in neighbor locations are the most confusing samples, we further introduce the third negative level, where we randomly replace the caption with those in neighbor locations, including the nearest previous location and the nearest next location defined in Section 4.1. It can be seen as a...\"}"}
{"id": "emnlp-2022-main-226", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: We report BLEU (B), METEOR (M), CIDEr and Diversity (D) scores for the captioning on image task on the CapOnImage2M dataset. Since the CapOnImage task is a newly proposed task in this work, we adapt conventional state-of-the-art image captioning models to this task by introducing text location and textual knowledge for comparison. We run our experiments 5 times under different random seed and report the average value.\\n\\n| Methods              | B@1   | B@4   | M     | CIDEr | D@1   | D@2   |\\n|----------------------|-------|-------|-------|-------|-------|-------|\\n| Up-down w/o TAtt     | 15.51 | 9.79  | 7.93  | 108.11| 48.73 | 41.22 |\\n| M2 w/o TAtt          | 24.32 | 20.96 | 14.58 | 200.32| 54.83 | 49.29 |\\n| RSTNet w/o TAtt      | 25.18 | 20.46 | 14.73 | 196.91| 56.17 | 50.85 |\\n| Up-down w/ TAtt      | 20.49 | 13.54 | 11.52 | 181.04| 65.24 | 56.27 |\\n| M2 w/ TAtt           | 34.18 | 24.31 | 18.14 | 273.35| 63.29 | 53.43 |\\n| RSTNet w/ TAtt       | 33.42 | 23.91 | 17.28 | 267.60| 64.12 | 54.53 |\\n| M4C w/o copying      | 36.46 | 27.08 | 20.19 | 296.73| 65.69 | 55.69 |\\n| M4C w/ copying       | 35.98 | 28.35 | 20.58 | 299.31| 65.98 | 55.03 |\\n| baseline             | 36.46 | 27.08 | 20.19 | 296.73| 65.69 | 55.69 |\\n| w/o locations        | 17.78 | 9.36  | 10.03 | 99.08 | 22.94 | 17.24 |\\n| no-image             | 20.81 | 13.88 | 11.53 | 133.26| 55.87 | 47.32 |\\n| context              | 37.69 | 27.98 | 21.91 | 313.64| 70.25 | 60.54 |\\n| full                 | 41.77 | 32.20 | 24.52 | 357.03| 74.05 | 63.20 |\\n| human                | -     | -     | -     | 90.13 | 75.53 |       |\\n\\nspecial case of Level-II, which limits the negative location to the neighboring locations and makes it more difficult to distinguish.\\n\\nProgressive training strategy. Since the three levels of CM task are from easy to difficult, inspired by the human learning procedure, we propose a progressive training strategy to dynamically adjust the proportion of each level. Specifically, we randomly replace captions with 60% probability to form negative samples and leave 40% unchanged as positive ones. The negative captions come from the three levels with \\\\( p_1 \\\\), \\\\( p_2 \\\\) and \\\\( p_3 \\\\) probabilities respectively, where \\\\( p_1 + p_2 + p_3 = 1 \\\\). We vary the probabilities over the course of training, according to the following formula:\\n\\n\\\\[\\np_1 = \\\\min(1, 2 \\\\cdot \\\\text{step}_\\\\text{num} - 0.2), \\\\tag{6}\\n\\\\]\\n\\n\\\\[\\np_3 = \\\\min(1, \\\\text{step}_\\\\text{num} \\\\cdot 0.5), \\\\tag{7}\\n\\\\]\\n\\n\\\\[\\np_2 = \\\\max(0, 1 - p_1 - p_3). \\\\tag{8}\\n\\\\]\\n\\nIt corresponds to rapidly decreasing the probability of Level-I from 1 at the beginning and then slowly decreasing to 0, while linearly increasing the probability of Level-III from a very small value to 1. As a result, the probability of Level-II will increase first, and then decrease. Overall, the training objective of the CM task can be expressed as follows:\\n\\n\\\\[\\nL_{CM} = -E(\\\\hat{V}, l, W_{\\\\text{info}}, Y) \\\\sim D \\\\left[ r \\\\log s + (1 - r) \\\\log (1 - s) \\\\right], \\\\tag{9}\\n\\\\]\\n\\nwhere \\\\( s \\\\) refers to the predicted matching score of a training sample and \\\\( r \\\\in [0, 1] \\\\) is the ground-truth label indicating whether it is a negative or positive sample.\\n\\n5 Experiments\\n\\nWe carry out experiments to evaluate the ability of models for captioning on image given a provided text layout on the CapOnImage2M dataset. We evaluate the caption generation qualities from multiple aspects, including the accuracy measurement against the references, and the diversity measurement within an image. Since the correct caption for each textbox location is not unique, we further evaluate the fitness of generated captions to the corresponding textbox locations with respect to the caption length and type. Besides, we also conduct human evaluations. More results can be found in supplementary materials.\\n\\n5.1 Experimental Setup\\n\\nEvaluation metrics. For the accuracy measurement, we evaluate the generated captions against the ground-truth with standard metrics used in the image captioning task, including BLEU (Papineni et al., 2002), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015). For the diversity measurement, we concatenate the dense captions within an image as a paragraph and compute the ratio of unique \\\\( n \\\\)-grams, called \\\\( \\\\text{Div}@n \\\\) (Shetty et al., 2017). For the fitness measurement, we show the relationship of generated caption length to the aspect ratio of text box, and...\"}"}
{"id": "emnlp-2022-main-226", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Captioning results with different pre-training tasks and strategies. *fixed* denotes training the multi-level CM task with a fixed proportion, while *progressive* denotes varying the proportion according to the degree of difficulty and training steps.\\n\\n| Row | Pretrain tasks | Pretrain strategy | Validation B@1 | Validation B@4 | Validation M | CIDEr | Test B@1 | Test B@4 | Test M | CIDEr |\\n|-----|----------------|-------------------|----------------|----------------|--------------|-------|-----------|-----------|-------|-------|\\n| 1   | -              | -                 | -              | -              | -            | -     | -         | -         | -     | -     |\\n| 2   | \u2713              | -                 | \u2713             | -              | \u2713            | \u2713     | \u2713         | \u2713         | \u2713     | \u2713     |\\n| 3   | \u2713              | \u2713                 | \u2713             | -              | \u2713            | \u2713     | \u2713         | \u2713         | \u2713     | \u2713     |\\n| 4   | \u2713              | \u2713                 | \u2713             | \u2713              | \u2713            | \u2713     | \u2713         | \u2713         | \u2713     | \u2713     |\\n| 5   | \u2713              | \u2713                 | \u2713             | \u2713              | \u2713            | \u2713     | \u2713         | \u2713         | \u2713     | \u2713     |\\n\\nThe type distribution of generated captions.\\n\\nImplementation details. We initialize the ResNet-50 backbone pre-trained on ImageNet, and fine-tune it with our model in an end-to-end manner. Our model has $L = 6$ transformer layers with the hidden dimension of $d = 1024$ and attention head $A = 8$. In the pre-training stage, we sample the batch of CG and CM tasks with a proportion of 3:1 for 200K steps. We adopt a warming-up strategy for the first 4K steps. For text processing, we tokenize Chinese captions into characters and build a vocabulary with 6263 tokens. We implement our method using pytorch (Paszke et al., 2019). We manually search hyper-parameters.\\n\\n5.2 Comparison with Baseline Models\\n\\nSince the CapOnImage task is a newly proposed task in this work, we adapt conventional state-of-the-art image captioning models (Up-down (Anderson et al., 2018), RSTNet (Zhang et al., 2021), M2 (Cornia et al., 2020), M4C-Captioner (Sidorov et al., 2020)) to this task as the baselines for comparison. The details of compared baseline methods are expanded on the supplementary materials.\\n\\nVariants of our model. We also compare with different variants of our model. Since all the words to be generated are already in the vocabulary, the copy mechanism bring no significant improvement (Table 2), so we remove it and use M4C-Captioner w/o copying as our baseline model. The no-info model adopts the same architecture as the baseline model except that the product information input is removed. Similarly, the no-image and the no-locations model share the same baseline model architecture but with the image and locations input removed respectively. Our context model is the baseline model enhanced with neighbor location contexts, which is still trained only with the CG task. The full model is our complete model with progressive pre-training by both CG and CM tasks on the same dataset.\\n\\nTable 2 reports the captioning on image results of different models on the CapOnImage2M validation and test sets. It is shown that the conventional image captioning model without any adaptation perform poorly on the CapOnImage task. This is because these models lack the textual context that can provide rich information for caption generation. Enhancing the Up-down, M2, and RSTNet with textual attentions on the additional product information, the captioning results are significantly improved. However, they are still inferior to the adapted text-aware image captioning model, which has a good ability of multi-modal fusion with the cross transformer encoder. Therefore, it stands for a strong baseline for our model. Compared with the baseline model, our context model enhances the text location embedding with neighbor location contexts, which brings significant improvements on both accuracy and diversity metrics. It demonstrates the importance of location relationship modeling especially for reducing caption redundancy. Although good results have been achieved, the model is only trained with caption generation objective against the ground-truth, which is not sufficient to help the model learn complex correspondence between texts and image locations. Therefore, when pre-training the context model with both CG and CM tasks in a progressive manner, our full model achieves the state-of-the-art results. Nevertheless, there is still a gap with the human annotations on the captioning diversity metrics.\\n\\nTo further explore the contribution of each input modalities, we also report the captioning results with some input removed (no-locations, no-info, and no-image). It shows that the location information is more important than the visual image and the textual information for the CapOnImage task.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, these three models are severely inferior to the baseline model with multi-modal input, which shows the necessity of multi-modal fusion for this new task.\\n\\n### 5.3 Ablative Analysis\\n\\n#### Parameters determination.\\n\\nIn Figure 4, we conduct ablation studies to investigate the suitable parameters for our model. We use our context model and operate our experiment on test set. The parameter that needs to be determined are number of layers of transformer, hidden dimension of transformer and grid feature size of resnet backbone. In Figure 4(a), we study the impact of these three parameters for caption performance (BLEU@4 and CIDEr). We choose 6 layer transformer with hidden dimension 1024 and $8 \\\\times 8$ grid size resnet for the intuition of Accuracy-Efficiency Trade-Offs.\\n\\n#### Pre-training tasks and strategy.\\n\\nIn Table 3, we ablate the proposed multi-level pre-training tasks and progressive training strategy. It shows that pre-training with only Level-I of the CM task (row 2) can significantly improve the non-pretrained model with only CG objective (row 1). It demonstrates the importance of multi-modal alignment to the CapOnImage task. Upgrading the CM task with more difficult negatives in Level-II helps the model better learn the relationship of captions and text locations and thus yields better results (row 3). Further incorporating negatives in Level-III brings additional gains (row 4), which confirms the importance of context information in CapOnImage task. However, since three levels of negative samples are built from easy to difficult, we seek to boost the learning process of CM task in an adaptive fashion: the ratio of pre-training tasks need to be adapted to the training status and vary in a progressive manner (as opposed to a fixed proportion of 30%:40%:30%). Therefore, we propose a progressive training strategy with the proportion of easy task decreased and hard task increased in the training process. It boosts the results stably (row 5).\\n\\n### 5.4 Caption length to textbox aspect ratio.\\n\\nGiven a textbox location, the generated caption should exactly fit in with it for visual aesthetic. The text length and font size are the influencing factors. Since the short side of the textbox determines the font size, the aspect ratio (long side length / short side length) can reflect the most suitable text length.\\n\\n### Table 4: Human evaluation of our full model vs. baseline model on the test set w.r.t. relevance, diversity and informativeness.\\n\\n|                  | Base wins (%) | Full wins (%) | Delta |\\n|------------------|---------------|---------------|-------|\\n| relevance        | 31.2          | 68.8          | +37.6 |\\n| diversity        | 34.5          | 65.5          | +31.0 |\\n| informativeness  | 32.8          | 67.2          | +34.4 |\\n\\nTherefore, we show the relationship of our generated caption length with aspect ratio of the corresponding textbox in the Figure 4(b). It shows that with the textbox aspect ratio increased, our model generates longer captions almost linearly, which demonstrates the controllability of the text box size to the length of the caption generated by our model.\\n\\n### 5.5 Caption type to textbox location.\\n\\nBesides the caption length, the types of captions in CapOnImage2M dataset are diverse at different image locations. To explore whether the type of our generated captions is suitable to the given locations, we visualize the caption type distribution on the image. Since the caption type annotations are not available, we automatically group the ground-truth captions into 4 categories by k-means based on their sentence-level BERT (Devlin et al., 2019) embeddings. We then display the same type of captions using the same color on an image. As shown in Figure 3(a), the captions with the same type are located together, which shows that the caption type is very related to its location. We assign our generated captions to the 4 clusters and visualize them in the same way in Figure 3(b). It looks very similar to the ground-truth type distribution map, which shows that our model effectively learns the relationship of text location and text type. The meaning of each color are illustrated in Figure 3(c).\\n\\n### 5.6 Human Evaluation\\n\\nIn addition to the objective evaluation, we also conduct human evaluation on 400 randomly sampled images from the test set. We render the generated dense captions from baseline model and our full model on the image via opencv. We instruct 5 workers to choose which one is better or they are not distinguishable based on relevance, diversity and informativeness respectively and do the majority voting. To avoid the prior bias, we anonymize the model names and shuffle the predictions randomly. Table 4 shows the human evaluation results. Our full model significantly outperforms baseline.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: (a) Distribution of the GT caption types. (b) Distribution of generated caption types. (c) Illustration of the four caption types in CapOnImage2M dataset via automatic clustering.\\n\\nFigure 4: (a1) Ablation of number of transformer layer. (a2) Ablation of the hidden dimension of transformer model. (a3) Ablation of grid size of resnet backbone. (b) The average captioning length of our model for the text box with different aspect ratios.\\n\\nThe baseline model especially on all three aspects, which demonstrates the effectiveness of the proposed neighbor-enhanced location embedding and multi-level progressive pre-training.\\n\\n5.7 Qualitative Results\\n\\nFigure 5 visualizes some results of our full model and baseline model. The baseline model is shown to generate repetitive captions due to the lack of global layout awareness. For example, for adjacent locations, the baseline model repeats the concept of \\\"mild\\\", while our model generates more informative caption of \\\"sensitive skin friendly\\\". Furthermore, our model is also shown to better exploit the visual context to generate more suitable captions. In the second example, our model generates the text \\\"suitable for large area makeup\\\" for the down-right region where a hand appears, while the baseline model fails to distinguish it with the up-right region and generates similar descriptions about the \\\"oblique slop brush\\\". More visualization results can be found in the supplementary material.\\n\\n6 Conclusion\\n\\nIn this work, we propose a new vision-and-language task called CapOnImage, which aims to generate dense captions at different locations on an image with visual and textual context. We propose a multi-modal pre-training and fine-tuning model with multi-level pre-training tasks from easy to difficult for the correspondence learning between image location and text, and enhance the current location embedding with neighboring locations to reduce captioning redundancy. Experimental results shown that our model can generate controllable length and type of captions at different image locations. In the future work, we will explore to generate dense captions with self-predicted text layout and combine the layout generation with caption generation in one joint framework to benefit from each other.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThe definition of our proposed task, i.e., generating text on image locations based on visual and textual context, can be found in many scenarios, such as billboard photos, posters, social platform images, etc. In this paper, we only report performance on our collected e-commercial dataset for the convenience of validating our key idea and our proposed task does not rely on any priors of specific inputs, so it can be expanded to a wide range of scenarios. In the future, we plan to collect more types of datasets, which can help us to apply our approach to more scenarios. Also, our dataset only contains caption annotations in Chinese.\\n\\nAcknowledgement\\n\\nThis work was supported by National Key R&D Program of China (No. 2020AAA0106900), the National Natural Science Foundation of China (No. U19B2037, No. 61876152), Shaanxi Provincial Key R&D Program (No. 2021KWZ-03), Natural Science Basic Research Program of Shaanxi (No. 2021JCW-03) and Alibaba Group through Alibaba Innovative Research Program. We thank Yuqing Song, Wei Suo, Mengyang Sun, and Peng Wu for their helpful discussion.\\n\\nReferences\\n\\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6077\u20136086.\\n\\nDiego Mart\u00edn Arroyo, Janis Postels, and Federico Tombari. 2021. Variational transformer networks for layout generation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 13642\u201313652.\\n\\nAli Furkan Biten, Llu\u00eds G\u00f3mez, Mar\u00e7al Rusi\u00f1ol, and Dimosthenis Karatzas. 2019. Good news, everyone! context driven entity-aware captioning for news images. In IEEE Conference on Computer Vision and Pattern Recognition, pages 12466\u201312475.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229.\\n\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. UNITER: universal image-text representation learning. In European Conference on Computer Vision, pages 104\u2013120.\\n\\nMarcella Cornia, Matteo Stefanini, Lorenzo Baraldi, and Rita Cucchiara. 2020. Meshed-memory transformer for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10578\u201310587.\\n\\nMichael J. Denkowski and Alon Lavie. 2014. Meteor universal: Language specific translation evaluation for any target language. In Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 376\u2013380.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: pre-training of deep bidirectional transformers for language understanding. In Conference of the North American Chapter of the Association for Computational Linguistics, pages 4171\u20134186.\\n\\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xiaodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, and Hsiao-Wuen Hon. 2019. Unified language model pre-training for natural language understanding and generation. In Annual Conference on Neural Information Processing Systems, pages 13042\u201313054.\\n\\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna M. Wallach, Hal Daum\u00e9 III, and Kate Crawford. 2018. Datasheets for datasets. CoRR, abs/1803.09010.\\n\\nRoss B. Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik. 2014. Rich feature hierarchies for accurate object detection and semantic segmentation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 580\u2013587.\\n\\nKamal Gupta, Alessandro Achille, Justin Lazarow, Larry Davis, Vijay Mahadevan, and Abhinav Shrivastava. 2020. Layout generation and completion with self-attention. CoRR, abs/2006.14615.\\n\\nDanna Gurari, Yinan Zhao, Meng Zhang, and Nilavra Bhattacharya. 2020. Captioning images taken by people who are blind. In European Conference on Computer Vision, pages 417\u2013434.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778.\\n\\nLun Huang, Wenmin Wang, Jie Chen, and Xiaoyong Wei. 2019. Attention on attention for image captioning. In IEEE International Conference on Computer Vision, pages 4633\u20134642.\\n\\nJustin Johnson, Andrej Karpathy, and Li Fei-Fei. 2016. Densecap: Fully convolutional localization networks for dense captioning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4565\u20134574.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Akash Abdu Jyothi, Thibaut Durand, Jiawei He, Leonid Sigal, and Greg Mori. 2019. Layoutvae: Stochastic scene layout generation from a label set. In IEEE International Conference on Computer Vision, pages 9894\u20139903.\\n\\nRanjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. 2017a. Dense-captioning events in videos. In IEEE International Conference on Computer Vision, pages 706\u2013715.\\n\\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A. Shamma, Michael S. Bernstein, and Li Fei-Fei. 2017b. Visual genome: Connecting language and vision using crowdsourced dense image annotations. Int. J. Comput. Vis., 123:32\u201373.\\n\\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and Daxin Jiang. 2020. Unicoder-vl: A universal encoder for vision and language by cross-modal pre-training. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, pages 11336\u201311344.\\n\\nHui Li, Peng Wang, and Chunhua Shen. 2017. Towards end-to-end text spotting with convolutional recurrent neural networks. In IEEE International Conference on Computer Vision, pages 5248\u20135256.\\n\\nXuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan. 2018. FOTS: fast oriented text spotting with a unified network. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5676\u20135685.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. 2019. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Annual Conference on Neural Information Processing Systems, pages 13\u201323.\\n\\nLuke Melas-Kyriazi, Alexander M. Rush, and George Han. 2018. Training for diversity in image paragraph captioning. In Conference on Empirical Methods in Natural Language Processing, pages 757\u2013761.\\n\\nYingwei Pan, Ting Yao, Yehao Li, and Tao Mei. 2020. X-linear attention networks for image captioning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 10968\u201310977.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67.\\n\\nShaoqing Ren, Kaiming He, Ross B. Girshick, and Jian Sun. 2015. Faster R-CNN: towards real-time object detection with region proposal networks. In Annual Conference on Neural Information Processing Systems, pages 91\u201399.\\n\\nPierre Sermanet, David Eigen, Xiang Zhang, Micha\u00ebl Mathieu, Rob Fergus, and Yann LeCun. 2014. Overfeat: Integrated recognition, localization and detection using convolutional networks. In International Conference on Learning Representations.\\n\\nRakshith Shetty, Marcus Rohrbach, Lisa Anne Hendricks, Mario Fritz, and Bernt Schiele. 2017. Speaking the same language: Matching machine to human captions by adversarial training. In IEEE International Conference on Computer Vision, pages 4155\u20134164.\\n\\nOleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. 2020. Textcaps: A dataset for image captioning with reading comprehension. In European Conference on Computer Vision, pages 742\u2013758.\\n\\nYuqing Song, Shizhe Chen, and Qin Jin. 2021. Towards diverse paragraph captioning for untrimmed videos. In IEEE Conference on Computer Vision and Pattern Recognition, pages 11245\u201311254.\\n\\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. 2015. Cider: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4566\u20134575.\\n\\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. 2015. Show and tell: A neural image caption generator. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3156\u20133164.\\n\\nTeng Wang, Ruimao Zhang, Zhichao Lu, Feng Zheng, Ran Cheng, and Ping Luo. 2021. End-to-end dense video captioning with parallel decoding. In IEEE International Conference on Computer Vision, pages 6847\u20136857.\\n\\nXuewen Yang, Heming Zhang, Di Jin, Yingru Liu, Chi-Hao Wu, Jianchao Tan, Dongliang Xie, Jue Wang, and Xin Wang. 2020. Fashion captioning: Towards generating accurate descriptions with semantic rewards. In European Conference on Computer Vision, pages 1\u201317.\\n\\nZhengyuan Yang, Yijuan Lu, Jianfeng Wang, Xi Yin, Dinei Flor\u00eancio, Lijuan Wang, Cha Zhang, Lei Zhang, and Jiebo Luo. 2021. TAP: text-aware pretraining for text-vqa and text-caption. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8751\u20138761.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xuying Zhang, Xiaoshuai Sun, Yunpeng Luo, Jiayi Ji, Yiyi Zhou, Yongjian Wu, Feiyue Huang, and Rongrong Ji. 2021. Rstnet: Captioning with adaptive attention on visual and non-visual words. In IEEE Conference on Computer Vision and Pattern Recognition, pages 15465\u201315474.\\n\\nMingchen Zhuge, Dehong Gao, Deng-Ping Fan, Linbo Jin, Ben Chen, Haoming Zhou, Minghui Qiu, and Ling Shao. 2021. Kaleido-bert: Vision-language pre-training on fashion domain. In IEEE Conference on Computer Vision and Pattern Recognition, pages 12647\u201312657.\\n\\nA Appendix\\n\\nIn the appendix, we first conduct further analysis of the choices of contextual locations and the diversity of our generated captions. Then claim the motivation and the challenge for the novel CapOnImage task. At last, we take hierarchical presentation of the CaptionOnImage2M from different perspectives.\\n\\nA.1 Choice of contextual locations.\\n\\nTable A5: Captioning results on the test set with different contextual locations. The number in () means how many locations used as the context to enhance the current location embedding.\\n\\n| Methods                      | B@1   | B@4   | CIDEr  | D@1   | D@2   |\\n|------------------------------|-------|-------|--------|-------|-------|\\n| w/o context (0)              | 35.73 | 26.24 | 287.61 | 69.31 | 59.01 |\\n| w/ two random (2)            | 33.98 | 24.79 | 283.73 | 67.43 | 57.82 |\\n| w/ top-1 nearest (2)         | 37.02 | 27.23 | 305.50 | 70.98 | 60.90 |\\n| w/ top-2 nearest (4)         | 37.88 | 27.10 | 307.61 | 70.12 | 60.34 |\\n\\nIn Table A5, we take a further study on the neighbor-enhanced location embedding module with different contextual locations. With the nearest neighbor (previous and next) locations used as the context as described in Section 4.1, our model significantly improves the accuracy and diversity metrics. To figure out where the benefit comes from, we compare with the model using the same amount of randomly selected locations as context. Experimental results show that the randomly selected locations cannot improve the results and may even bring noise, which demonstrates the effectiveness of our model in encoding neighboring layout information to generate more appropriate and diverse captions. When further expanding the contextual range from the top-1 nearest to the top-2 nearest (top-2 previous and top-2 next), the model achieves slightly better result on the accuracy metric. To balance the efficiency and quality, we finally use the top-1 nearest locations as the context in our model.\\n\\nA.2 Diversity from the textual input.\\n\\nIn Table A6, we calculate the Bleu score between the input product information and our generated captions. Results show that there is only a small percentage of copy text in our generated captions, demonstrating that our model is not just simply \u201ccopying text from the input product information\u201d, but generating diverse captions conditioned on the multi-modality input.\\n\\nTable A6: Bleu score between the input product information and our generated captions.\\n\\n| #    | Bleu1 | Bleu2 | Bleu3 | Bleu4 |\\n|------|-------|-------|-------|-------|\\n| Test | 0.032 | 0.021 | 0.013 | 0.007 |\\n\\nA.3 Motivation\\n\\nFor what purpose was the dataset created? The dataset was created to support the research on the captioning on image (CapOnImage) task, which aims to generate informative captions at different appropriate locations in the given image. CapOnImage is a valuable task for both vision-and-language research and industrial applications. We show the pipeline of our task on Figure A6.\\n\\nA.4 Composition\\n\\nWhat do the instances that comprise the dataset represent? Each instance in the CapOnImage2M dataset contains 50 categories product image, a sentence of product title, several product attributes, and multiple spatially localized captions with bounding box coordinates, describing the product from multiple aspects. We show the word cloud of product categories on Figure A7. Figure A8 shows some examples of the CapOnImage2M dataset.\\n\\nHow many instances are there in total? The dataset consists of 2.1M images and 10.07M texts in total. Each image contains an average of 4.8 spatially localized captions.\\n\\nDoes the dataset contain all possible instances or is it a sample (not necessarily random) of instances from a larger set? CapOnImage2M is a new independent dataset. The instances in CapOnImage2M dataset are crawled from an e-commercial website. New product images with texts will continue to emerge. Therefore, the current dataset cannot cover all possible instances.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What data does each instance consist of? Raw data (e.g., unprocessed text or images) or features?\\n\\nEach instance contains a product image with short side as 256 pixel in PNG format, the processed product title and captions. The original image with high resolution can be downloaded by the provided image URL.\\n\\nIs any information missing from individual instances?\\n\\nNo. Everything is included in the dataset.\\n\\nAre there recommended data splits (e.g., training, development/validation, testing)?\\n\\nYes. The dataset is split into 2.06M images for training, 20K for validation, and 20K for testing.\\n\\nAre there any errors, sources of noise, or redundancies in the dataset?\\n\\nWe have removed redundant or similar images whose captions are overlapped over a threshold. Therefore, the instance redundancy in the dataset is low. The captions are automatically recognized by an OCR model, therefore, there are noise in the captions of training set. To ensure the evaluation accuracy, we further manually clean the OCR errors for the validation and testing sets.\\n\\nIs the dataset self-contained, or does it link to or otherwise rely on external resources (e.g., websites, tweets, other datasets)?\\n\\nThe dataset is self-contained, except that the original high resolution images are linked to a public website. Nevertheless, the self-contained images are enough to reproduce the results, and the high resolution images are used for better visualization.\\n\\nDoes the dataset contain data that might be considered confidential (e.g., data that is protected by legal privilege or by doctor-patient confidentiality, data that includes the content of individuals' non-public communications)?\\n\\nNo. All data was collected from a publicly available e-commercial website.\\n\\nDoes the dataset contain data that, if viewed directly, might be offensive, insulting, threatening, or might otherwise cause anxiety?\\n\\nNo. The dataset only consists of product images.\\n\\nDoes the dataset relate to people?\\n\\nMost of the images only contain products, and only a few images contain public product spokesmen.\\n\\nDoes the dataset contain data that might be considered sensitive in any way (e.g., data that reveals racial or ethnic origins, sexual orientations, religious beliefs, political opinions or union memberships, or locations; financial or health data; biometric or genetic data; forms of government identification, such as social security numbers; criminal history)?\\n\\nNo. The dataset does not contain confidential information since all information was crawled from a public e-commercial website.\"}"}
{"id": "emnlp-2022-main-226", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Product Title: \u4e07\u5229\u8fbe\u70e7\u6c34\u58f6\u5927\u5bb9\u91cf\u5feb\u70e7\u58f6\u7172\u6c34\u5bbf\u820d\u5b66\u751f\u5c0f\u578b\u9632\u70eb\u4e0d\u9508\u94a2\u5bb6\u7528\u716e\u6c34\u58f6 (Valida Boiling Water Large Capacity Quick Boiling Students Anti-Scalding Stainless Steel Kettle)\\n\\nProduct Attributes:\\n- \u54c1\u724c: \u4e07\u5229\u8fbe (Brand: Valida)\\n- \u6750\u8d28: \u4e0d\u9508\u94a2 (material: stainless steel)\\n- \u9505\u76d6\u7c7b\u578b: \u4e0d\u9508\u94a2\u76d6 (Lid Type: stainless steel lid)\\n\\nDense Spatially Localized Captions:\\n- \u771f\u5927\u5bb9\u91cf (large capacity)\\n- \u81ea\u52a8\u6052\u6e29 (automatic constant temperature)\\n- \u54c1\u724c\u4fdd\u969c (brand guarantee)\\n- \u65b0\u5347\u7ea7\u52a0\u539a (upgrade thickening)\\n- \u5b98\u65b9\u6b63\u54c1 (official product)\\n\\nProduct Title: \u521b\u610f\u5973\u5b69\u793c\u54c1\u667a\u529b\u624b\u7ed8\u677f\u591a\u529f\u80fd\u7f8e\u672f\u7ed8\u753b\u677f\u7d20\u63cf\u4fbf\u643a\u5f0f\u753b\u677f\u513f\u7ae5 (Creativity Girls Gifts Children's Drawing Creative Card Stickers Portable)\\n\\nProduct Attributes:\\n- \u9002\u7528\u6027\u522b: \u4e2d\u6027 (Gender: Neutral)\\n- \u6750\u8d28: \u5408\u91d1 (Material: Alloy)\\n\\nDense Spatially Localized Captions:\\n- \u53cc\u9762 (double side)\\n- \u7ffb\u8f6c (rotate)\\n- \u6298\u53e0 (fold)\\n- \u5347\u964d (lift)\\n- \u8010\u810f (dirt resistant)\\n\\nProduct Title: \u5bb6\u7528\u5851\u6599\u6c34\u52fa\u52a0\u6df1\u957f\u67c4\u6c34\u74e2\u53ef\u6302\u900f\u660e\u5851\u6599\u6c34\u52fa\u6d74\u5ba4\u5b9d\u5b9d\u6c90\u6d74\u6d17\u5934\u6c34\u8200 (Plastic Water Scoop, Plastic Water Ladle Bath Ladle Dipper Shampoo Ladle Cup Household Accessories for Kitchen Bathroom)\\n\\nDense Spatially Localized Captions:\\n- \u5927\u5bb9\u91cf (large Capacity)\\n- \u5b9e\u7528\u578b\u6c34\u52fa (Practical Water Scoop)\\n- \u5065\u5eb7\u751f\u6d3b\u4ece\u6e05\u6d01\u5f00\u59cb (Healthy live starts with cleanliness)\\n- \u7528\u9014\u5e7f\u6cdb (Widely Usage)\\n\\nProduct Title: \u68ee\u68ee\u8ff7\u4f60\u78c1\u529b\u5237\u6c34\u65cf\u53cc\u9762\u6e05\u6d01\u5237\u9664\u82d4\u73bb\u7483\u60ac\u6d6e\u522e\u5200\u84dd\u8272\u5c0f\u5de7\u78c1\u529b\u5237 (SENSEN Magnetic Aquarium Fish Tank Brushes Floating Clean Glass Window Algae Scraper Cleaner Brush Easy To Use Fish Tank Clean Tools)\\n\\nProduct Attributes:\\n- \u54c1\u724c: \u68ee\u68ee (Brand: SENSEN)\\n\\nDense Spatially Localized Captions:\\n- \u4e0a\u6d6e\u8bbe\u8ba1 (float design)\\n- \u5c0f\u5de7\u65b9\u4fbf (tiny and easy to use)\\n- \u9002\u5408\u78c1\u529b (mini magnetic brush)\\n- \u6e05\u6d01\u53bb\u85fb\u597d\u5e2e\u624b (algae clean tools)\\n\\nProduct Title: \u6d17\u8863\u5237\u8f6f\u6bdb\u5bb6\u7528\u5237\u5b50\u6d17\u8863\u670d\u7684\u978b\u5b50\u6d17\u978b\u4e13\u7528\u786c\u6bdb\u6e05\u6d01\u591a\u529f\u80fd\u677f\u5237\u978b\u5237 (Multi-function Silicone Laundry Brush Soft Hair Cleaning Shoes Brush Underwear Brushes)\\n\\nDense Spatially Localized Captions:\\n- \u5f97\u5fc3\u5e94\u624b (handy)\\n- \u5237\u8863\u66f4\u7701\u529b (easy to brush clothes)\\n- \u4f18\u8d28\u5237\u6bdb (high quality brush hair)\\n- \u4e0d\u4f24\u8863\u7269 (no harm for clothes)\\n- \u5237\u6bdb\u7d27\u5bc6 (dense brush hair)\\n\\nProduct Title: \u5b9e\u6728\u683c\u6805\u677f\u5b9e\u6728\u683c\u6805\u7535\u89c6\u80cc\u666f\u5899\u677f\u7f51\u683c\u6805\u5b9e\u6728\u683c\u6805\u677f\u62a4\u5899\u677f (TV Background Wall Border Decorative Strips Stickers Skirting Waist Line Self-adhesive Skirting Wall Stickers Soft Lines)\\n\\nDense Spatially Localized Captions:\\n- \u591a\u79cd\u989c\u8272\u53ef\u9009 (multiple colors to choose)\\n- \u5b9e\u6728 (wood)\\n- \u514d\u6f06 (paint free)\\n- \u65b0\u897f\u5170\u677e\u5b9e\u6728\u5373\u88c5\u5373\u4f4f (Newzealand pine solid wood anytime ready to live)\\n- \u6e90\u5934\u5382\u5bb6\u53ef\u5b9a\u5236 (customizable)\\n\\nProduct Title: \u8425\u517b\u571f\u901a\u7528\u578b100\u65a4\u82b1\u571f\u5927\u530550\u65a4\u56ed\u571f100\u65a4\u9633\u53f0\u79cd\u83dc\u591a\u8089\u7528\u79cd\u690d\u571f (50kg Nutrient Soil Rich Fertilizer NPK For Plant Flower Succulent Garden Bonsai)\\n\\nDense Spatially Localized Captions:\\n- \u8150\u6b96\u8425\u517b\u571f (humic nutrient soil)\\n- \u690d\u7269\u901a\u7528\u578b (plant universal)\\n- \u517b\u5206\u6301\u7eed (nutrients last)\\n- \u4fdd\u80a5\u6301\u6c34 (keep fertilizer and water)\\n\\nProduct Title: \u7ea2\u6728\u9e21\u7fc5\u6728\u9ad8\u6863\u793c\u54c1\u5bb6\u7528\u7b77\u5b50\u5957\u88c5\u9632\u9709\u9632\u6ed1\u53ef\u523b\u5b57\u4e2d\u56fd\u98ce (Red Wood High-quality Household Alloy Chopstick Home Non-slip Tableware Reusable Food Sticks Traditional Chinese Style)\\n\\nDense Spatially Localized Captions:\\n- \u516d\u798f (the meaning of this stick)\\n- \u4e0a\u518d\u52a0\u4e00\u798f (plus additional luck)\\n- \u516d\u798f\u4e3a\u4e94\u798f\u7684\u57fa\u7840 (six six large smooth)\\n\\nProduct Title: \u5c45\u5bb6\u73b0\u4ee3\u7b80\u7ea6\u5b9e\u6728\u5e8a1.8\u7c73\u6728\u7c73\u53cc\u4eba\u5e8a\u767d\u6a61\u6728\u9ed1\u767d\u8272\u5e8a\u5367\u5ba4\u5bb6\u7528 (Solid wood bed double bed large bed solid wood master bedroom 1.8 m wedding bed)\\n\\nDense Spatially Localized Captions:\\n- \u7bb1\u6846\u7ed3\u6784 (box frame structure)\\n- \u53cc\u62bd\u5c49\u50a8\u7269 (double drawer storage)\\n- \u8d85\u5927\u5185\u5bb9\u91cf (extra large amount of content)\\n- \u65b9\u4fbf (convience)\\n- \u5b9e\u7528 (practical)\\n\\nProduct Attributes:\\n- \u98ce\u683c: \u6b27\u5f0f (Style: European)\\n- \u6750\u8d28: \u6a61\u80f6\u6728 (Material: Wood)\\n\\nProduct Title: \u9ad8\u6b3e\u5e8a\u5934\u67dc\u7f6e\u7269\u67b6\u7b80\u7ea6\u73b0\u4ee3\u5317\u6b27\u98ce\u5367\u5ba4\u6536\u7eb3\u5c0f\u578b\u5e8a\u8fb9\u50a8\u7269\u5c0f\u67dc (Bedside Cabinet Modern Minimalist Small Storage Cabinet Bedside Cabinet Storage European Style Cabinet Bedroom)\\n\\nDense Spatially Localized Captions:\\n- \u5229\u7528\u7a84\u7a7a\u95f4 (take advantage of narrow spaces)\\n- \u5e73\u65b9\u53d8\u7acb\u65b9 (square to cube)\\n- \u653e\u7f6e\u96f6\u4e71\u7269 (place messy)\\n- \u751f\u6d3b\u66f4\u65b9\u4fbf (convience life)\\n- 30\u5398\u7c73\u6df1\u5ea6\u8bbe\u8ba1 (30cm depth design)\\n- \u5bbd\u655e\u53f0\u9762\u5bb9\u7eb3\u66f4\u591a (width surface to accommodate more)\"}"}
{"id": "emnlp-2022-main-226", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-226", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.5 Collection Process\\nWhat mechanisms or procedures were used to collect the data? The raw images and product title sentences in this dataset were automatically crawled from Taobao website. The spatially localized captions are extracted from the image by an OCR model, and further manually cleaned for the validation and testing sets.\\n\\nA.6 Preprocessing\\nWas any preprocessing/cleaning/labeling of the data done? The following steps were taken to process the data: (1) Crawling raw images and product titles. We first crawl the raw product images and titles from the e-commercial website, and then resize the images with short side as 256. (2) Detecting texts on the image. For each image, we employ an OCR toolkit to automatically detect the texts on the image as well as their bounding box coordinates. (3) Removing redundant instances. We remove redundant instances whose images contain similar captions that exceed the overlap threshold. (4) Removing discount information. We remove redundant instances that have high correlation with discount information. (5) Cleaning instances. We remove the texts longer than 10 or shorter than 2 characters, and remove the instances with only one caption on the image. Then, we input the automatically recognized captions into a pre-trained GPT model, and remove the captions with high generation perplexities. (6) Manual labeling. We further manually clean the captions with OCR errors in the validation and testing sets for accurate evaluation.\\n\\nA.7 Uses\\nHas the dataset been used for any tasks already? No, the dataset is newly collected from scratch in this work to support the proposed new task. What (other) tasks could the dataset be used for? The dataset was created to support the CapOnImage task. In addition, since each product image is accompanied by a product title sentence, it can be directly used for the Fashion Captioning (Yang et al., 2020) task. It may support a wider range of vision-and-language tasks as well.\\n\\nA.8 Distribution\\nWe show the distribution of caption length and number of captions per image in Figure A10. Will the dataset be distributed to third parties outside of the entity (e.g., company, institution, organization) on behalf of which the dataset was created? Yes. The dataset will be released publicly. How will the dataset will be distributed (e.g., tarball on website, API, GitHub)? The dataset can be downloaded from Google Drive and Baidu Disk as a gzipped tar file. When will the dataset be distributed? The dataset will be released upon the publication of this work. Will the dataset be distributed under a copyright or other intellectual property (IP) license, and/or under applicable terms of use (ToU)? There will be no license. Users only need to fill in an agreement form regarding the dataset not to be used for commercial purposes and citation suggestions etc. Have any third parties imposed IP-based or other restrictions on the data associated with the instances? No. There are no fees or restrictions.\\n\\nA.9 Maintenance\\nWill the dataset be updated (e.g., to correct labeling errors, add new instances, delete instances)? Yes. The dataset will be updated for fair comparison to correct labeling errors, add new instances, delete instances).\"}"}
