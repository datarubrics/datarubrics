{"id": "emnlp-2022-main-343", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AEG: Argumentative Essay Generation via A Dual-Decoder Model with Content Planning\\n\\nJianzhu Bao1,4\u2217, Yasheng Wang2, Yitong Li2,3, Fei Mi2, Ruifeng Xu1,4,5\u2020\\n\\n1Harbin Institute of Technology, Shenzhen, China\\n2Huawei Noah's Ark Lab\\n3Huawei Technologies Co., Ltd.\\n4Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies\\n5Peng Cheng Laboratory, Shenzhen, China\\n\\njianzhubao@gmail.com, xuruifeng@hit.edu.cn\\n{wangyasheng, feimi2, liyitong3}@huawei.com\\n\\nAbstract\\nArgument generation is an important but challenging task in computational argumentation. Existing studies have mainly focused on generating individual short arguments, while research on generating long and coherent argumentative essays is still under-explored. In this paper, we propose a new task, Argumentative Essay Generation (AEG). Given a writing prompt, the goal of AEG is to automatically generate an argumentative essay with strong persuasiveness. We construct a large-scale dataset, ArgEssay, for this new task and establish a strong model based on a dual-decoder Transformer architecture. Our proposed model contains two decoders, a planning decoder (PD) and a writing decoder (WD), where PD is used to generate a sequence for essay content planning and WD incorporates the planning information to write an essay. Further, we pre-train this model on a large news dataset to enhance the plan-and-write paradigm. Automatic and human evaluation results show that our model can generate more coherent and persuasive essays with higher diversity and less repetition compared to several baselines.\\n\\n1 Introduction\\nAutomatic argument generation, literally the task of generating persuasive arguments on controversial issues (Toulmin, 2003; Zukerman et al., 2000), has received many research interests in recent years (Khatib et al., 2021; Schiller et al., 2021). Many works have involved different arguments generation such as the counter-arguments generation (Hua and Wang, 2018; Hua et al., 2019; Hidey and McKeown, 2019; Alshomary et al., 2021b) and the\\n\\n\u2217This work was done during the internship at Huawei Noah\u2019s Ark Lab.\\n\u2020Corresponding Author\\n\\n1 Code and data are available at https://github.com/HITSZ-HLT/AEG\\n\\nWriting Prompt:\\nOnline education is becoming more and more popular. Some people claim that e-learning has so many benefits that it will replace face-to-face education soon. Others say that traditional education is irreplaceable. Discuss both views and give your opinion.\\n\\nArgumentative Essay:\\nAcquiring knowledge virtually has become extremely popular in the present times. While many individuals believe that there are various advantages and might overtake traditional learning in the future, a sizeable group thinks that the traditional method cannot be replaced.\\n\\nI believe that the use of the classroom might reduce, but it cannot be replaced. This essay will discuss both views and substantiate my view in the course of the essay.\\n\\nTo commence with, virtual learning is widely implemented because it is convenient and cost-effective. It provides us with the opportunity to obtain an education without the hassle of travelling. Students, for instance, can attend classes at the comfort of their home, resulting in saving time that might have been spent on commuting in the past. Similarly, learning online can also be considered cost-efficient. Instead of spending an immense amount of college funds, we can attain the same level of qualifications at a cheaper price as it does not involve infrastructure.\\n\\nOn the contrary, traditional learning offers guided learning and hands-on experience. Classroom teaching practices assist students in obtaining better study skills, such as organising and gathering reliable information due to constant interaction with the teacher, resulting in improved academic achievement. In addition, it helps in gaining practical knowledge through sessions in laboratories that are not a part of digital practices. For example, pupils are provided with constant guidance during face to face teaching along with acquiring real-time experience.\\n\\nIn conclusion, although online classes might seem beneficial in terms of convenience together with being budget-friendly, classroom education provides a better learning and practical experience.\\n\\nTherefore, I think that face-to-face classes are not replaceable.\\n\\nTable 1: An example of our proposed Argumentative Essay Generation task. Given a writing prompt about a controversial topic, the task is to generate a well-organized argumentative essay with nice coherence and strong persuasiveness. The major claims express the topic, stance, and main idea of this essay.\\n\\ncontrolled arguments generation under certain top-ics or aspects (Gretz et al., 2020; Schiller et al., 2021; Alshomary et al., 2021a; Khatib et al., 2021). However, real-life scenarios like news editorials, competitive debating, and even television shows, are requiring more powerful ways of systematically organizing arguments in composing long-form essays or speeches that can fully express opinions and persuade the audiences. Previous studies predominantly focused on generating individual and rel-a\"}"}
{"id": "emnlp-2022-main-343", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tively short arguments, which can be weak when addressing these long-form argument generation tasks.\\n\\nIn this paper, we aim with the question of how to generate and compose a comprehensive and coherent argumentative essay, which can contain multiple arguments with different aspects. This is a challenging but fundamental task, requiring much more capability of understanding human intelligence towards general artificial intelligence to fully address this problem (Slonim et al., 2021). However, with superior development of pre-training methods (Devlin et al., 2019; Brown et al., 2020; Bommasani et al., 2021), generating coherent long-form documents is touchable with reasonable qualities (Guan et al., 2021; Yu et al., 2021). Therefore, to facilitate this line of research, we introduce a new document-level generation task, Argumentative Essay Generation (AEG), which focuses on generating long-form argumentative essays with strong persuasiveness given the writing prompt. An example of AEG is shown in Table 1. In this example, the given writing prompt specifies a topic about \\\"online education\\\". The expected argumentative essay first introduces the topic and states the stance (paragraph 1), then justifies its point through a series of arguments (paragraphs 2-3), and finally summarizes and echos the main idea (paragraph 4). We can see that AEG requires generating relevant claims and evidences of diverse aspects relevant to a given topic, and further appropriately incorporating them in a logical manner to compose an argumentative essay.\\n\\nIn order to make progress towards AEG, we construct a large-scale dataset, ArgEssay, containing 11k high-quality argumentative essays along with their corresponding writing prompts on a number of common controversial topics such as technological progress, educational methodology, environmental issues, etc. Our proposed dataset is built upon the writing task of several international standardized tests of English, such as IELTS and TOEFL, which also being studied in other tasks of automated essay scoring (Blanchard et al., 2013) and argument mining (Stab and Gurevych, 2017). Compared to previous argument generation datasets collected from social media, the essays in our dataset are more formal in wording and writing and therefore of higher quality, making our dataset a better choice for studying argument generation.\\n\\nTo tackle the proposed AEG task, we adopt the plan-and-write paradigm for generating diverse and content-rich argumentative essays, as content planning proves to be beneficial for long-form text generation (Fan et al., 2019; Hua and Wang, 2019). We establish encoder-decoder based Transformer models with dual-decoder, which contains a planning decoder (PD) for generating keywords or relational triplets as essay content planning and a writing decoder (WD) for composing an essay guided by the planning. Adopting this dual-decoder architecture can keep planning and writing process separate to avoid mutual interference. Automatic evaluation results show that our model outperforms several strong baselines in terms of diversity and repetition. Human evaluation results further demonstrate that the essays generated by our model maintain good coherence and strong persuasiveness. We also show that our model yields better plannings compared to baselines, and the content of the generated essays can be effectively controlled by the plannings. In addition, the performance of our model can be further improved after being pre-trained on a large news dataset.\\n\\nWe summarize our contributions as follows:\\n\u2022 We propose a new task of argumentative essay generation and create a large-scale and high-quality benchmark for this task.\\n\u2022 We establish a Transformer-based model with dual-decoder which generates argumentative essays in a plan-and-write manner, and further improve the model performance via pre-training.\\n\u2022 Using both automatic and human evaluations, we demonstrate that our proposed model can generate more coherent and persuasive argumentative essays with higher diversity and less repetition rate compared to several baselines.\\n\\n2 Related Work\\n2.1 Argumentative Essay Analysis\\nThe analysis of argumentative essays has been extensively studied in previous work since an early stage (Madnani et al., 2012; Beigman Klebanov and Flor, 2013). To comprehensively study the structure of argumentation in argumentative essays, Stab and Gurevych (2014, 2017) presented the Persuasive Essay dataset with the annotations of both argument components and argumentative relations. Based on this dataset, many subsequent researches are conducted to better parsing the argumentation...\"}"}
{"id": "emnlp-2022-main-343", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"structure in argumentative essays (Persing and Ng, 2016; Eger et al., 2017; Potash et al., 2017; Kuribayashi et al., 2019; Bao et al., 2021).\\n\\nThese studies above are closely related to our work, since the analysis of the structure and quality of argumentative essays can support AEG by providing structured argument knowledge.\\n\\n2.2 Argument Generation\\n\\nEarly work on argument generation involved a lot of hand-crafting features, such as constructing the argument knowledge base (Reed, 1999; Zukerman et al., 2000) or designing argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000).\\n\\nTo frame existing argumentative text into new arguments, some work employs the argument retrieval (Levy et al., 2018; Stab et al., 2018) based methods to generate arguments (Sato et al., 2015; Hua and Wang, 2018; Wachsmuth et al., 2018), while others synthesize arguments by reframing existing claims or evidences (Yanase et al., 2015; Bilu and Slonim, 2016; Baff et al., 2019).\\n\\nRecently, more attention has focused on end-to-end generation of arguments using neural models (Hua and Wang, 2018; Hidey and McKeown, 2019). Hua et al. (2019) presented a sequence-to-sequence framework enhanced by external knowledge for generating counter-arguments. Gretz et al. (2020) explored the use of a pipeline based on the pre-trained language model GPT-2 (Radford et al., 2019) to generate coherent claims. Schiller et al. (2021) developed a controllable argument generation model, which can control the topic, stance, and aspect of a generated argument. Alshomary et al. (2021a) proposed the belief-based claim generation task and leveraged conditional language models to generate arguments controlled by the prior beliefs of the audience. Khatib et al. (2021) proposed to control the generation of arguments with argumentation knowledge graphs.\\n\\nHowever, current argument generation research is limited to generating individual and relatively short arguments, without consideration given to the generation of long and coherent argumentative essays containing multiple aspects of arguments.\\n\\n2.3 Long-form text generation\\n\\nOur work is also closely related to long-form text generation research, such as story generation (Fan et al., 2018; Yao et al., 2019; Guan et al., 2020; Xu et al., 2020), data-to-text generation (Puduppully et al., 2019; Hua et al., 2021; Hua and Wang, 2020; Dong et al., 2021), paragraph generation (Hua and Wang, 2019; Yu et al., 2021), and essay generation (Feng et al., 2018; Yang et al., 2019; Qiao et al., 2020; Liu et al., 2021).\\n\\nMost of studies focus generating narrative texts or description texts, while we concentrate on generating argumentative essays, with more emphasis on the argumentativeness.\\n\\n3 Dataset Creation\\n\\nOur dataset is collected from Essay Forum, an online community established by professional writers and editors to help users write, edit, and revise their essays. Specifically, we selected the essays and prompts of high-quality in the writing feedback section of Essay Forum, where users post their essays for revision suggestions in preparation for standardized English test like IELTS or TOEFL.\\n\\nIn addition, the essays in the writing feedback section have also been used in the researches on argument mining (Stab and Gurevych, 2014, 2017).\\n\\nFirst, we collect all the post in the writing feedback section of Essay Forum. Then, to obtain the prompt-essay pairs and ensure the text quality, we conduct several pre-processing steps including:\\n\\n- Separating the essay and the prompt in each post. For posts where the author does not mark the prompt in bold or italics, we filter them out and then process them manually;\\n- Filtering prompt-essay pairs with non-argumentative essays (like narrative essays, character description essays, and graphical analysis essays, etc.) by manually summarized rules (see Appendix B.1 for details.);\\n- Cleaning irrelevant text like special characters, user names, and expressions of thanks or greetings through rule-based deletion and manual processing (see Appendix B.2 for details.);\\n- Only keeping prompt-essay pairs whose essay contains less than 500 tokens (tokenized by the Stanford CoreNLP toolkit (Manning et al., 2014)) and 4 or 5 paragraphs. The reason for this procedure is that, in the writing feedback section of Essay Forum, essays that do not satisfy these aforementioned attributes are likely\\n\\n2https://essayforum.com\\n3An example post in the Essay Forum can be found in Appendix A.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 2: Comparison of our dataset with existing argument generation datasets.\\n\\n| Dataset                      | Avg. Tokens | Avg. Sents |\\n|------------------------------|-------------|------------|\\n| (Hua and Wang, 2018)         | 161.10      | 7.70       |\\n| (Hua et al., 2019)           | 66.00       | 2.95       |\\n| (Khatib et al., 2021)        | 81.89       | 3.85       |\\n| ArgEssay (Ours)              | 327.35      | 14.41      |\\n\\nIndicates the average number of tokens/sentences in the target generation text.\\n\\n---\\n\\nIt is worth noting that the Essay Forum administrator will review and remove any posts that are considered to be libelous, racist, or otherwise inappropriate. Thus, the ethic of our dataset can be assured. Further, we also manually check the dataset to avoid ethical issues.\\n\\nAs for the data split, we want to minimize the overlap between the train set and the validation/test set in terms of prompts, otherwise it would be difficult to test the model's generalization ability on new prompts. Thus, we first extract keywords from the prompts based on TF-IDF and measure the similarity of any two prompts as the Jaccard similarity between their keywords set. Then, when splitting the data, for any prompt in the validation/test set, we ensure that the similarity between it and each prompt in the train set does not exceed a threshold $\\\\epsilon$. After several rounds of manual verification, we set $\\\\epsilon = 0.65$, as we observe that this threshold can reasonably separate the prompts with more than 70% of the validation/test prompts having a similarity of less than 0.30 to any training prompt.\\n\\nThe final dataset consists of 11,282 prompt-essay pairs in English, in which 9,277/1,002 pairs are used for training/validation/testing, respectively. We compare our proposed dataset with existing argument generation datasets in Table 2.\\n\\nOur ArgEssay contains longer target text with richer content, which makes it more challenging. Also, most existing datasets are constructed from social media, while the essays in our dataset are written for the standardized English tests, which are more formal in terms of wording and structuring.\\n\\n### 4 Methods\\n\\nOur proposed AEG task can be formulated as follows: given a writing prompt $X = [x_1, x_2, ..., x_m]$, a relevant argumentative essay $Y_e = [y_1, y_2, ..., y_n]$ should be generated.\\n\\nIn order to generate diverse and content-rich essays, we propose a Transformer-based dual-decoder model with a plan-and-write strategy. In detail, our model first predicts a planning sequence $Y_p$, then it generates the argumentative essay $Y_e$ under the guidance of the planning sequence through the planning attention. The planning strategy is commonly used in long-form text generation studies. Here, instead of using a standalone model for predicting the planning (Fan et al., 2019; Xu et al., 2020), we utilize a dual-decoder architecture to enable end-to-end training for generating the planning and the essay.\\n\\nIn the following, we will first introduce the method of constructing planning sequence $Y_p$ for training and then describe our model in detail.\\n\\n#### 4.1 Construction of Planning\\n\\nFor flexibility, we do not strictly restrict the form of the planning, as long as it is natural language text. In this paper, we investigate two kinds of planning using automatic methods, a keyword-based planning and a relation-based planning.\\n\\n- **1) Keyword-based (KW) planning:**\\n  \\n  We use TF-IDF (Salton and McGill, 1984) score to determine important words as keywords. We calculate the TF-IDF based on the corpus and then select words with the top scores to construct the keyword-based planning $Y_p = k_1#1|k_2#2|...|k_l#l$, where $k_i$ is the $i$-th keyword, \"#\" and \"#\" are special tokens, and keywords are separated by \"|\".\\n\\n- **2) Relation-based (Rel) planning:**\\n  \\n  Similarly, for the relation-based planning, we firstly apply an off-the-shelf OpenIE (Angeli et al., 2015) to extract all the relational triplets in each essay and then randomly sample $l$ triplets to construct the relation-based planning $Y_p = s_1#r_1#o_1#1|...|s_l#r_l#o_l#l$, where $s_i$, $r_i$ and $o_i$ are subject, relation and object of the $i$-th triplet.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Dual-decoder Model\\n\\nFor essay generation task, we adopt the encoder-decoder architecture with a pre-trained BART backbone (Lewis et al., 2020) and extend it to a dual-decoder architecture. Figure 1 illustrates the overall architecture of the proposed dual-decoder. Overall, the proposed model consists of a shared encoder to encode the input writing prompt, a planning decoder to generate a planning sequence, and a writing decoder to write the argumentative essay.\\n\\n**Shared Encoder**\\n\\nWe use the same encoder of BART as the shared encoder in our model, whose output will be utilized by both decoders. Specifically, we feed $X$ into the encoder:\\n\\n$$H_e = \\\\text{Encoder}(X)$$\\n\\nwhere $H_e \\\\in \\\\mathbb{R}^{m \\\\times d}$, and $d$ is the hidden dimension.\\n\\n**Planning Decoder (PD)**\\n\\nBased on the input prompt sequence, the planning decoder serves to predict the planning that contains important information of the essay. The generated planning can help plan the perspectives or aspects to be discussed in the essay before the formal writing, as well as enrich the wording and improve the diversity of the generated essay. Adopting the planning decoder allows to keep planning and writing process separate, with two decoders being responsible for each. The reason behind this design is that the distribution of the planning text and the essay text are significantly different, forcing one same decoder to handle both processes can decrease the performance.\\n\\nOur planning decoder is based on the decoder of BART, whose decoding target text is $Y_p$:\\n\\n$$h_{pd}^t = \\\\text{PD}(H_e, Y_p^{<t})$$\\n\\n$$\\\\hat{Y}_p^t = \\\\text{Softmax}(W_{pd}h_{pd}^t + b_{pd})$$\\n\\nwhere $h_{pd}^t \\\\in \\\\mathbb{R}^d$ is the hidden representation of the $t$-th token in the generated logits $\\\\hat{Y}_p^t$; $W_{pd}$ and $b_{pd}$ are learnable parameters.\\n\\nEach Transformer layer of the BART decoder contains three sub-layers, i.e., a self multi-head attention layer, a cross multi-head attention layer and a feed-forward layer. For the self multi-head attention sub-layer of the $j$-th Transformer layer, we denote the keys and values matrix as $K(i)_{pd}$ and $V(i)_{pd} \\\\in \\\\mathbb{R}^{l \\\\times d}$, which will be used to guide the writing decoder subsequently.\\n\\n**Writing Decoder (WD)**\\n\\nThe writing decoder can incorporate the generated planning and the input writing prompt to write an essay:\\n\\n$$h_{wd}^t = \\\\text{WD}(H_e, K_{pd}, V_{pd}, Y_e^{<t})$$\\n\\n$$\\\\hat{Y}_e^t = \\\\text{Softmax}(W_{wd}h_{wd}^t + b_{wd})$$\\n\\nwhere $h_{wd}^t \\\\in \\\\mathbb{R}^d$ is the hidden representation of the $t$-th token in $Y_e$; $K_{pd}$ and $V_{pd}$ are the keys and values of all the Transformer layers of PD; $W_{wd}$ and $b_{wd}$ are learnable parameters.\\n\\nHere, we introduce a planning attention (PA) module that enables PD to guide WD. For each Transformer layer of the WD, we modify the self multi-head attention sub-layer to enable WD to attend all the tokens in the planning generated by PD when decoding each token of an essay. Specifically, when calculating the self multi-head attention in the $i$-th Transformer layer of WD, we use $Q(i)_{wd}$, $K(i)_{wd}$ and $V(i)_{wd}$ as the query, key and value:\\n\\n$$Q(i)_{wd} = Q(i)_{wd}'$$\\n\\n$$K(i)_{wd} = [K(i)_{pd} \\\\oplus K(i)_{wd}']$$\\n\\n$$V(i)_{wd} = [V(i)_{pd} \\\\oplus V(i)_{wd}']$$\\n\\nwhere $Q(i)_{wd}'$, $K(i)_{wd}'$, $V(i)_{wd}' \\\\in \\\\mathbb{R}^{n \\\\times d}$ is the original query, key, value matrix of the BART decoder Transformer layer, and $\\\\oplus$ denotes the matrix concatenation operation in the first dimension.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training & Inference. During training, we use\\nthe negative log-likelihood loss:\\n\\n\\\\[ L = L_p + L_w \\\\]\\n\\n\\\\[ L_p = -\\\\sum_{t=1}^{n} \\\\log P(Y_p|Y_{p<t},X) \\\\]\\n\\n\\\\[ L_w = -\\\\sum_{t=1}^{n} \\\\log P(Y_e|Y_{e<t},Y_p,X) \\\\]\\n\\nwhere \\\\( L_p \\\\) and \\\\( L_w \\\\) are the loss functions for optimizing planning and writing, respectively.\\n\\nDuring inference, we first generate the planning sequence and then write the essay, both of which are performed in an autoregressive manner.\\n\\n4.3 Pre-training\\n\\nTo better adapt the model to the plan-and-write paradigm, we explore first pre-training our model on a large news dataset, then fine-tuning it on our ArgEssay dataset. In detail, we employ CNN-DailyMail (Hermann et al., 2015) as the pre-training data, which is a large-scale news dataset commonly used for summarization. We treat the highlights as the prompts and the associated news articles as the essays. Regarding the planning sequences, the keywords/triplets are extracted from the news articles in the same way as described in Section 4.1.\\n\\n5 Experimental Setups\\n\\n5.1 Comparison Models\\n\\nWe build the following baselines for comparison. \\n\\n- **BART**\\n  BART (Lewis et al., 2020) is a strong sequence-to-sequence baseline model for natural language generation, which is pre-trained on several denoising tasks. We fine-tune the pre-trained BART model on our proposed ArgEssay dataset without using any planning information.\\n\\n- **BART-KW**\\n  Following approaches of incorporating knowledge information with the arguments in previous work (Schiller et al., 2021), we conduct a BART-KW method by concatenating each planning before the essay as the overall target for prediction. That is BART-KW first predicts the keyword planning and then generates the essay. BART-KW is also fine-tuned from BART-base.\\n\\n- **DD-KW**\\n  For our dual-decoder (DD) models, we denote the dual-decoder model with keyword-based planning as DD-KW. Note that DD-KW is not pre-trained by news data but we use BART-base as the start point. Also, based on DD-KW, we implement the following two models for further comparisons:\\n  - **DD-KW w/o planning-att**\\n    We make an ablation of planning attention module, that is we replace the planning attention for DD-KW with the normal attention, to investigate the effectiveness of using planning to explicitly guide essay generation. Note that this model differs from BART in that the planning can influence essay generation through the encoder during training.\\n  - **DD-KW w. pre-training**\\n    We apply the news pre-training on DD-KW (see Section 4.3).\\n\\n- **BART-Rel** and **DD-Rel**\\n  Similar for the methods using relation-based planning, we implement four models: BART-Rel, DD-Rel, DD-Rel w/o planning-att and DD-Rel w. pre-training.\\n\\n5.2 Implementation Details\\n\\nFor all models, we use the pre-trained BART-Base as the base model. Following previous work (Gretz et al., 2020; Xu et al., 2020; Khatib et al., 2021), for decoding at inference, we used a top-k sampling scheme with \\\\( k = 40 \\\\) and a temperature of 0.7. Our model is implemented in PyTorch (Paszke et al., 2019) and is trained on a NVIDIA Tesla V100 GPU. We restrict the generated text to be longer than 200 tokens. The AdamW optimizer (Kingma and Ba, 2015) is employed for parameter optimization with an initial learning rate of \\\\( 3\\\\times10^{-5} \\\\).\\n\\n5.3 Evaluation Metrics\\n\\n**Automatic Evaluation.**\\n\\nWe employ the following metrics for automatic evaluation. \\n\\n1. **Distinct** measures the diversity of generated essays by computing the ratio of the distinct n-grams to all the generated n-grams (Li et al., 2016).\\n2. **Novelty** measures the difference between the generated essays and the training data. Specifically, following Yang et al. (2019) and Zhao et al. (2020), for each generated essay, we calculate its Jaccard similarity coefficient based on n-grams with every essay in the training set and choose the highest similarity as the novelty score.\\n3. **Repetition** measures the redundancy of the generated essay by computing the percentage of generated essays that contain at least one repeated n-gram (Shao et al., 2019).\\n4. **BLEU** (Papineni et al., 2002) computes the n-gram overlap between the generated texts and the reference texts. If the readability or fluency of the\"}"}
{"id": "emnlp-2022-main-343", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Automatic evaluation results [%].\\n\\n| Models     | Dist-3 | Dist-4 | Nov-1 (\u21d3) | Nov-2 (\u21d3) | Rep-3 (\u21d3) | Rep-4 (\u21d3) | BLEU-4 |\\n|------------|--------|--------|------------|------------|------------|------------|--------|\\n| BART       | 46.68  | 70.43  | 26.73      | 9.45       | 19.04      | 3.09       | 6.85   |\\n| BART-KW    | 48.95  | 72.18  | 26.67      | 9.31       | 17.24      | 2.89       | 6.74   |\\n| DD-KW      | 50.07  | 72.72  | 26.31      | 9.29       | 16.87      | 2.55       | 6.81   |\\n| w/o planning-att | 47.13 | 70.76  | 26.78      | 9.43       | 18.74      | 2.51       | 6.79   |\\n| w. pre-training | 51.35 | 73.71  | 26.26      | 9.21       | 16.75      | 2.39       | 6.94   |\\n| BART-Rel   | 47.45  | 71.39  | 27.41      | 9.48       | 21.14      | 3.29       | 6.72   |\\n| DD-Rel     | 49.10  | 72.55  | 26.99      | 9.34       | 19.24      | 2.67       | 6.83   |\\n| w/o planning-att | 47.16 | 70.63  | 26.78      | 9.46       | 19.34      | 3.09       | 6.93   |\\n| w. pre-training | 51.11 | 73.57  | 26.75      | 9.20       | 19.18      | 2.39       | 6.84   |\\n\\nThe best score is in bold. \u2020 indicates the second best result.\\n\\nTable 4: Human evaluation results.\\n\\n| Models Rel. Coh. Cont. |\\n|-----------------------|\\n| BART                  |\\n| 3.27                  |\\n| 2.83                  |\\n| 3.09                  |\\n| BART-KW               |\\n| 3.31                  |\\n| 2.71                  |\\n| 3.31                  |\\n| DD-KW                 |\\n| 3.60                  |\\n| 2.83                  |\\n| 3.42                  |\\n| w. pre-training       |\\n| 3.63                  |\\n| 3.05                  |\\n| 3.49                  |\\n| BART-Rel              |\\n| 3.27                  |\\n| 2.78                  |\\n| 3.29                  |\\n| DD-Rel                |\\n| 3.59                  |\\n| 2.82                  |\\n| 3.36                  |\\n| w. pre-training       |\\n| 3.60                  |\\n| 3.06                  |\\n| 3.43                  |\\n\\nRel., Coh. and Cont. indicate relevance, coherence and content richness, respectively.\\n\\nThe generated essay is poor, its BLEU score will be extremely low. Hence, we provide the BLEU score as a reference to assess the essay\u2019s quality.\\n\\nHere, distinct and novelty are used for assessing diversity, while repetition and BLEU are used for assessing quality.\\n\\nHuman Evaluation.\\n\\nFor a more comprehensive analysis, we conduct human evaluations that contain three aspects. (1) Relevance evaluates whether the entire content of the generated essay is semantically relevant to the given writing prompt, which is a basic requirement for a qualified argumentative essay. (2) Coherence indicates whether the generated essay is logically consistent and reasonable in terms of semantic and causal dependencies in the context, which is closely related to the persuasiveness of an argumentative essay. (3) Content Richness measures the amount of distinct relevant aspects covered in the generated essay, which is a significant characteristic of argumentative essays.\\n\\nAll three aspects are expected to be scored from 1 (worst) to 5 (best). We randomly sampled 50 writing prompts from the test set. Each annotation item contains the input writing prompt and the generated essays of different models. We assign 3 annotators for each item who are not aware of which model the generated essays come from.\\n\\n6 Results and Analysis\\n\\n6.1 Automatic Evaluation\\n\\nTable 3 shows the automatic evaluation results. Compared to BART, our proposed DD-KW and DD-Rel achieve significantly better distinct scores and moderately better repetition and novelty scores. BART-KW and BART-Rel are worse in distinct, repetition, and novelty than DD-KW and DD-Rel, showing the effectiveness of the dual-decoder architecture. Also, removing the planning attention (w/o planning-att) decreases the distinct and repetition scores. Regarding the BLEU scores, DD-KW and DD-Rel perform similar to BART, indicating that the dual-decoder architecture does not degrade the readability and fluency of the generated essays. In addition, incorporating pre-training into our dual-decoder models can further boost the performance, showing that pre-training can enhance this plan-and-write generation paradigm. The average length of the essays generated by each model is around 290-300.\\n\\nOverall, with the support of the dual-decoder architecture and the pre-training strategy, our model can generate more diverse and less repetitive essays at the same time maintaining good readability and fluency.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Planning quality evaluation [\\\\%].\\n\\n| Model          | Recall | Keyword Repetition | Invalidity | Planning Relevance |\\n|----------------|--------|--------------------|------------|--------------------|\\n| BART-KW        | 18.06  | 6.45               | -          | 77.40              |\\n| DD-KW          | 19.41  | 1.80               | -          | 82.00              |\\n| w. pre-training| 23.95  | 1.01               | -          | 84.80              |\\n| BART-Rel       | 14.81  | -                  | 1.76       | 72.20              |\\n| DD-Rel         | 15.05  |                    | 0.85       | 76.60              |\\n| w. pre-training| 15.43  |                    | 0.40       | 78.40              |\\n\\n6.2 Human Evaluation\\n\\nThe results of human evaluation are presented in Table 4. The average Fleiss' kappa is 0.42. Regarding relevance, BART, BART-KW, and BART-Rel perform poorly because of the topic drift problem, that is, the generated essay is barely relevant to the given topic (see case study in Appendix C for details). Compared to BART, all other models with planning achieve better content richness score, since the generated planning can provide more diverse aspects information and guide the models to write essays containing more examples or perspectives. Also, the pre-training strategy can bring significant improvement to coherence.\\n\\n6.3 Planning Quality\\n\\nWe measure the quality of the generated plannings from the following aspects: (1) Recall: evaluates how many keywords/triplets in the oracle planning sequence are predicted. (2) Keyword Repetition: (only for keyword-based planning) measures how many keywords in the generated planning sequence are repeated at least once. (3) Invalidity: (only for relation-based planning) measures how many generated triplets is invalid, i.e., not in the form described in Section 4.1. (4) Planning Relevance: evaluates whether each predicted keyword/triplets is relevant to the prompt, and is obtained by manually analysis of 50 randomly selected samples.\\n\\nAs shown in Table 5, simply using a single decoder to generate the planning and the essay together (BART-KW and BART-Rel) causes the problem of high keyword repetition or high invalidity rate. In contrast, employing an individual planning decoder (DD-KW and DD-Rel) not only improves both the recall and the planning relevance, but also alleviates the keyword repetition or invalidity problem. Moreover, we can also observe that the planning quality can further be refined by pre-training our dual-decoder models.\\n\\nTable 6: Controllability evaluation [\\\\%].\\n\\n| Model          | Appearance | Appropriateness |\\n|----------------|------------|-----------------|\\n| BART-KW        | 63.72      | 66.80           |\\n| DD-KW          | 66.63      | 71.60           |\\n| w/o planning-att | 43.66    | 47.60           |\\n| w. pre-training| 72.58      | 73.20           |\\n| BART-Rel       | 43.43      | 43.40           |\\n| DD-Rel         | 51.31      | 52.40           |\\n| w/o planning-att | 19.01    | 37.20           |\\n| w. pre-training| 52.99      | 57.40           |\\n\\nFigure 2: Impact of the length of planning.\\n\\n6.4 Controllability Evaluation\\n\\nTo evaluate how well the generated essay can be controlled by the planning, we measure whether each keyword/triplet appear in the generated essay (Appearance). Also, we manually check 50 generated samples and determine whether the information enclosed by each keyword/triplet is appropriately used (Appropriateness). As shown in Table 6, BART-KW and BART-Rel achieve low appearance and appropriateness, while our dual-decoder models (DD-KW and DD-Rel) give significantly better results. With pre-training, around 73.20%/57.40% keywords/triplets are appropriately adopted by the writing decoder, showing a high controllability. Besides, removing the planning attention module (w/o planning-att) decreases both appearance and appropriateness dramatically.\\n\\n6.5 Impact of the Planning Length\\n\\nOn top of the models with keyword-based planning, we further investigate the impact of the planning length $l$ on the diversity (Dist-4) and accuracy (BLEU-4). As shown in Figure 2, for all models, as the planning length grows, the diversity increases, but the accuracy decreases. By manual review, we find that the readability of essays becomes extremely poor (low fluency and high repetition) when BLEU-4 is less than about 6.3. Thus, selecting a proper planning length is crucial for generating essays that are both diverse and readable.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nevertheless, our pre-trained dual-decoder model (DD-KW w. pre-training) can not only achieve better diversity with an appropriate planning length, but also ensure better readability than baselines even under extreme conditions.\\n\\n7 Conclusion\\nIn this paper, we propose a challenging new task, AEG, to generate long-form and coherent argumentative essays. To tackle this task, we present a large-scale dataset and further devise a dual-decoder architecture based on the basis of BART, which can generate a planning and a planning-guided essay in an end-to-end fashion. The experimental results demonstrate the superiority of our model. For future work, we plan to draw on external knowledge to generate more diverse and informative argumentative essays.\\n\\nLimitations\\nFirst, as discussed in Appendix C, there is still an undeniable gap between generated essays and human written essays in terms of logical coherence. In our method, we do not design mechanisms to ensure factual and causal logicality of the generated essays, which remains a great challenge. Hence, future work could consider improving the logical coherence of the generated essays by using external knowledge or causal inference techniques.\\n\\nSecond, although our dual-decoder architecture enables content planning and generates better essays, it also introduces some new parameters and computations. Future work could thus investigate more efficient methods with fewer model parameters.\\n\\nEthics Statement\\nOur dataset is collected from publicly available sources without any personal identity characteristics. When crawling data from the online platform \u201cessayforum.com\u201d, we carefully read and follow the privacy policy, terms of use of this platform. According to the agreement of this platform, any content in it can be accessed and used with an indication of the source.\\n\\nSince the administrators of the online platform we use will review and remove any posts that are considered to be libelous, racist, or otherwise inappropriate, the ethic of our dataset can be assured. We also manually double-check each sample in our dataset to confirm that no ethical issues exists.\\n\\nAcknowledgments\\nThis work was partially supported by the National Natural Science Foundation of China 62006062 and 62176076, Shenzhen Foundational Research Funding JCYJ20200109113441941, JCYJ20210324115614039, the Major Key Project of PCL2021A06, Guangdong Provincial Key Laboratory of Novel Security Intelligence Technologies 2022B1212010005.\\n\\nReferences\\nMilad Alshomary, Wei-Fan Chen, Timon Gurcke, and Henning Wachsmuth. 2021a. Belief-based generation of argumentative claims. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, EACL 2021, Online, April 19 - 23, 2021, pages 224\u2013233. Association for Computational Linguistics.\\n\\nMilad Alshomary, Shahbaz Syed, Arkajit Dhar, Martin Potthast, and Henning Wachsmuth. 2021b. Counter-argument generation by attacking weak premises. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1816\u20131827, Online. Association for Computational Linguistics.\\n\\nGabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL 2015, July 26-31, 2015, Beijing, China, Volume 1: Long Papers, pages 344\u2013354. The Association for Computer Linguistics.\\n\\nRoxanne El Baff, Henning Wachsmuth, Khalid Al Khatib, Manfred Stede, and Benno Stein. 2019. Computational argumentation synthesis as a language modeling task. In Proceedings of the 12th International Conference on Natural Language Generation, INLG 2019, Tokyo, Japan, October 29 - November 1, 2019, pages 54\u201364. Association for Computational Linguistics.\\n\\nJianzhu Bao, Chuang Fan, Jipeng Wu, Yixue Dang, Jiachen Du, and Ruifeng Xu. 2021. A neural transition-based model for argumentation mining. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6, 2021, pages 6354\u20136364. Association for Computational Linguistics.\\n\\nBeata Beigman Klebanov and Michael Flor. 2013. Argumentation-relevant metaphors in test-taker essays. In Proceedings of the First Workshop on Metaphor in NLP, pages 11\u201320, Atlanta, Georgia. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-343", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-343", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-343", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christian Stab, Johannes Daxenberger, Chris Stahlhut, Tristan Miller, Benjamin Schiller, Christopher Tauchmann, Steffen Eger, and Iryna Gurevych. 2018. Argumentext: Searching for arguments in heterogeneous sources. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics, NAACL-HLT 2018, New Orleans, Louisiana, USA, June 2-4, 2018, Demonstrations, pages 21\u201325. Association for Computational Linguistics.\\n\\nChristian Stab and Iryna Gurevych. 2014. Annotating argument components and relations in persuasive essays. In COLING 2014, 25th International Conference on Computational Linguistics, Proceedings of the Conference: Technical Papers, August 23-29, 2014, Dublin, Ireland, pages 1501\u20131510. ACL.\\n\\nChristian Stab and Iryna Gurevych. 2017. Parsing argumentation structures in persuasive essays. Comput. Linguistics, 43(3):619\u2013659.\\n\\nStephen E. Toulmin. 2003. The Uses of Argument, 2 edition. Cambridge University Press.\\n\\nHenning Wachsmuth, Shahbaz Syed, and Benno Stein. 2018. Retrieval of the best counterargument without prior topic knowledge. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 1: Long Papers, pages 241\u2013251. Association for Computational Linguistics.\\n\\nPeng Xu, Mostofa Patwary, Mohammad Shoeybi, Raul Puri, Pascale Fung, Anima Anandkumar, and Bryan Catanzaro. 2020. MEGATRON-CNTRL: control-lable story generation with external knowledge using large-scale language models. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 2831\u20132845. Association for Computational Linguistics.\\n\\nToshihiko Yanase, Toshinori Miyoshi, Kohsuke Yanai, Misa Sato, Makoto Iwayama, Yoshiki Niwa, Paul Reisert, and Kentaro Inui. 2015. Learning sentence ordering for opinion generation of debate. In Proceedings of the 2nd Workshop on Argumentation Mining, ArgMining@HLT-NAACL 2015, June 4, 2015, Denver, Colorado, USA, pages 94\u2013103. The Association for Computational Linguistics.\\n\\nPengcheng Yang, Lei Li, Fuli Luo, Tianyu Liu, and Xu Sun. 2019. Enhancing topic-to-essay generation with external commonsense knowledge. In Proceedings of the 57th Conference of the Association for Computational Linguistics, ACL 2019, Florence, Italy, July 28- August 2, 2019, Volume 1: Long Papers, pages 2002\u20132012. Association for Computational Linguistics.\\n\\nLili Yao, Nanyun Peng, Ralph M. Weischedel, Kevin Knight, Dongyan Zhao, and Rui Yan. 2019. Plan-and-write: Towards better automatic storytelling. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 7378\u20137385. AAAI Press.\\n\\nWenhao Yu, Chenguang Zhu, Tong Zhao, Zhichun Guo, and Meng Jiang. 2021. Sentence-permuted paragraph generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event / Punta Cana, Dominican Republic, 7-11 November, 2021, pages 5051\u20135062. Association for Computational Linguistics.\\n\\nLiang Zhao, Jingjing Xu, Junyang Lin, Yichang Zhang, Hongxia Yang, and Xu Sun. 2020. Graph-based multi-hop reasoning for long text generation. CoRR, abs/2009.13282.\\n\\nIngrid Zukerman, Richard McConachy, and Sarah George. 2000. Using argumentation strategies in automated argument generation. In INLG 2000 - Proceedings of the First International Natural Language Generation Conference, June 12-16, 2000, Mitzpe Ramon, Israel, pages 55\u201362. The Association for Computer Linguistics.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA An Example Post from Essay Forum\\n\\nAn example post from the writing feedback section of the Essay Forum platform is shown in Figure 3.\\n\\nB Rules For Data Pre-process\\n\\nB.1 Filtering Details\\n\\n\u2022 Removing prompt-essay pairs which are from IELTS writing task 1 by checking for the presence of keywords like \\\"bar\\\", \\\"chart\\\", \\\"diagram\\\" and \\\"task 1\\\" in the prompts, since essays in these samples are graphical analysis essays without argumentativeness.\\n\\n\u2022 Removing prompt-essay pairs about narrative, character description or letter by checking the keywords such as \\\"describe\\\", \\\"describing\\\", \\\"letter\\\", \\\"narrative\\\", \\\"summary\\\", etc.\\n\\nB.2 Data Cleaning Details\\n\\n\u2022 Deleting special characters like \\\"=\\\", \\\"*\\\", \\\"+\\\", etc.\\n\\n\u2022 Selecting out prompt-essay pairs that contain irrelevant text expressing gratitude, asking for help, greeting, or self-introduction by keywords like \\\"please\\\", \\\"pls\\\", \\\"grammar\\\", \\\"hello\\\", \\\"feedback\\\", \\\"grammar\\\", \\\"comment\\\", \\\"my name\\\", \\\"my essay\\\", \\\"thank\\\", \\\"appreciated\\\", etc. Then manually checking and deleting these irrelevant text.\\n\\nC Case Study\\n\\nTable 7 demonstrates several sample outputs from different models for the writing prompt about \\\"multinational companies\\\". We only show a snippet of each essay, which is taken from a similar location in the context.\\n\\nWe can see that BART and BART-KW show different degrees of topic drift problem, i.e., the generated text is less relevant to the given topic of \\\"multinational corporations\\\". In contrast, the models with dual decoders avoid this problem by better generating and utilizing the essay content planning. Regarding the planning generation, BART-KW suffers from generating planning with repeated keywords. Also, as can be seen, the pre-trained dual-decoder models can better leverage the planning to guide the essay generation. For example, the generated essay of (DD-KW w. pre-training) can coherently embody the keywords \\\"employees\\\", \\\"coexist\\\", \\\"corporations\\\", etc. By comparison, the DD-KW without pre-training performs slightly worse on this point. However, it is undeniable that there is still a large gap between the performance of our models and that of humans, which we plan to investigate in future work. For example, the human generated essay in Table 7 is significantly better than the all the model generated essays in terms of logical coherence.\"}"}
{"id": "emnlp-2022-main-343", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Writing Prompt: Nowadays, most large companies operate multinationally. To what extent those companies should have responsibility towards local communities in which they operate?\\n\\nEssay: More importantly, global operations ought to respect the national customs where they set up. In other words, the firms should discover insights into the native cultures and deter themselves from offending indigenous people or what they respect. For example, when running business in Thailand, the organizations cannot advertise their products by using pictures of royal Kings as Thai citizens extremely respect royal family. Therefore, international operations should develop carefully and stay away from some taboos, which offend the inhabitants.\\n\\nSecondly, the public of the city can know about the local communities. They should be aware of the situations which they are dealing with. To elaborate, the local governments should make a better understanding of the cultures of the districts and they should also pay attention to the surrounding communities. Furthermore, they should be a part of the local companies' culture.\\n\\nAdditionally, the governments should also take a responsibility for the local companies. As the workers are being recruited more and more, the profits from these organizations will increase. For example, the factories can operate in different areas, in order for the workers to get a better life. Moreover, the workers will be more motivated to work more and have more opportunities to earn more income.\\n\\nOn the other hand, multinational companies have a large number of employees. Many of them spend most of their time in a different place and do not have the time to engage with the other employees. Moreover, The employees are boundlessly affected by the cultures, traditions, customs, and traditions. Therefore it is necessary for them to take a team effort to build a good relationship with the others and make sure that they have a good collaboration.\\n\\nOn the other hand, the main responsibility of the large multinational corporations is to coexist with the communities in the local areas, and they should share the costs and amenities of their business. In other words, they should be aware of the needs of their employees and the benefits of their job. Furthermore, they have to offer equal opportunities to all of their staffs. For instance, in Bangladesh, the government has made it mandatory for the companies to co-operate with the local community in order to facilitate the overall development of their businesses.\\n\\nFurthermore, it also can be argued if the local government has more responsibility for the well-being of their citizens. In some cases, they could not provide the basic needs for their citizens, such as food and clothing. If a company only has a little staff, they have to make sure their local residents have healthy food and a stable living environment.\\n\\nI think that a large company would have some responsibilities in the areas where they operate. Many multinational companies operate multilaterally, so they should have a responsibility towards the areas in which they operate in. In other words, they should be accountable for the way they operate from one part of the world to another. If these companies were able to build a business with their employees, they would have the ability to control the way that they operate, which would create a big impact on their business.\\n\\nThese multinationally large firms should have their own efforts regarding the local community. It is believed that their efforts can improve the situation of the community and people in their local areas. For instance, it could be better to invest in infrastructure that could improve the lives of the residents. Moreover, it allows them to start their efforts with different initiatives that would help them to increase their efficiency.\"}"}
