{"id": "acl-2024-long-849", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MultiPICo: Multilingual Perspectivist Irony Corpus\\nSilvia Casola\u22c6, Simona Frenda\u22c6\u2299, Soda Marem Lo\u22c6, Erhan Sezerer\u22c4, Antonio Uva\u22c4, Valerio Basile\u22c6, Cristina Bosco\u22c6, Alessandro Pedrani\u22c4, Chiara Rubagotti\u22c4, Viviana Patti\u22c6, Davide Bernardi\u22c4\\n\\nAbstract\\nRecently, several scholars have contributed to the growth of a new theoretical framework in NLP called perspectivism. This approach aims to leverage data annotated by different individuals to model diverse perspectives that affect their opinions on subjective phenomena such as irony. In this context, we propose MultiPICo, a multilingual perspectivist corpus of ironic short conversations in different languages and linguistic varieties extracted from Twitter and Reddit. The corpus includes sociodemographic information about its annotators. Our analysis of the annotated corpus shows how different demographic cohorts may significantly disagree on their annotation of irony and how certain cultural factors influence the perception of the phenomenon and the agreement on the annotation. Moreover, we show how disaggregated annotations and rich annotator metadata can be exploited to benchmark the ability of large language models to recognize irony, their positionality with respect to sociodemographic groups, and the efficacy of perspective-taking prompting for irony detection in multiple languages.\\n\\n1 Introduction\\nThe pervasiveness of AI-based technologies has renewed the interest in making Artificial Intelligence more inclusive and attentive to users' needs. AI-based technology mirrors the quality and problems of data that feed it (Dignum, 2023). Therefore, the way corpora are created and the design biases that are \u2014 consciously or unconsciously \u2014 included in datasets and models are of central importance. Perspectivist corpora allow turning bias from an undesirable criticality into a measurable trait by reporting the perception of multiple individuals with different social and cultural traits on pragmatic phenomena (e.g., hate speech or irony).\\n\\nIn this work, we present MultiPICo (Multilingual Perspectivist Irony Corpus), a multilingual corpus of short conversations (Post-Reply) extracted from Twitter and Reddit and annotated as ironic or not ironic by crowdsourcing workers with different social backgrounds. MultiPICo covers 9 languages and 25 varieties, ranging from high to low resources. Specifically, we include 5 varieties of English (Australian, British, Indian, Irish and US English), 5 varieties of Spanish (Argentinean, Castilian, Colombian, Mexican, and US Spanish), 5 varieties of Arabic (Egyptian, Iraqi, Moroccan, Saudi Arabian, and Yemen), 2 varieties of French (Canadian and French), 3 varieties of German (Austrian, German, and Swiss), 2 varieties of Portuguese (Brazilian and Portuguese), and Italian, Dutch, and Hindi. Selected annotators' nationality is related to these linguistic varieties, the annotators' gender is balanced, and a rich set of other sociodemographic information (age, ethnicity, student and employment status) is provided. Starting from these metadata, we analyze the perception of irony in different sociodemographic groups and explore which demographic dimensions primarily explain the annotation. Furthermore, we also show how to exploit this multifaceted corpus as a benchmark for AI-based technologies, including Large Language Models (LLMs). These models are becoming pervasive, but the lack of information on their training data and methods makes their intrinsic bias hard to measure.\\n\\nIn short, our contributions are the following:\\n\u2022 We present MultiPICo, a disaggregated multilingual dataset annotated with the presence of irony in social media short conversations (Section 3).\\n\u2022 We analyze the impact of sociodemographic\\n\\n1 MultiPICo is available at https://huggingface.co/datasets/Multilingual-Perspectivist-NLU/MultiPICo with a CC-BY 4.0 license.\"}"}
{"id": "acl-2024-long-849", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\nThe NLP community is increasingly questioning how annotators\u2019 background can influence their choices and perceptions of pragmatic linguistic phenomena, such as irony or hate speech.\\n\\nBender and Friedman (2018) proposed a set of best practices for NLP technicians to avoid ethical issues such as exclusion and misrepresentation of users. Their recommendations include reporting annotators\u2019 demographic data, as they can impact how annotators use and interpret language.\\n\\nNumerous works have highlighted human annotation is subjective (Aroyo and Welty, 2015), especially for semantic and pragmatic phenomena \u2014 as the point of view might differ in relation to one\u2019s social background, beliefs, and demographics. An increasing number of works emphasize abandoning a single ground truth and preserving the disagreement among annotators (Leonardelli et al., 2021; Uma et al., 2021a; Leonardelli et al., 2023). Dealing with human label variation has consequences on the whole Machine Learning pipeline (Plank, 2022), from data to modeling (Mostafazadeh Davani et al., 2022) and evaluation (Uma et al., 2021c; Basile et al., 2021).\\n\\nThe perspectivist approach aims at leveraging disagreement and modeling annotators\u2019 perspectives (Basile et al., 2021). To do so, scholars have exploited individual annotators\u2019 decisions (Mostafazadeh Davani et al., 2022) or grouped their annotations based on their beliefs (Akhtar et al., 2019) or demographic traits (Frenda et al., 2023b). To work in this direction, disaggregated corpora with demographic information become of fundamental importance (Cabitza et al., 2023). In fact, aggregated data tend to reflect a minority of perspectives, under-representing others (Prabhakaran et al., 2021; Frenda et al., 2023b).\\n\\nDisaggregated datasets have become more widespread in recent years, as listed in the Perspectivist Data Manifesto\\n\\nSome of these corpora account for annotators\u2019 demographic. For example, Sachdeva et al. (2022) built a perspectivist corpus for hate speech detection, asking annotators to self-identify their race, gender, sexuality, religion, education, and income. Almanea and Poesio (2022) collected a dataset about misogyny and sexism in Arabic and had it annotated by three Muslim annotators (2 female, 1 male), who self-identified their religious belief as liberal, moderate, and conservative. They also provided annotations by a larger cohort on a small set of their dataset. The work showed a correlation between annotators\u2019 beliefs and their perception of misogyny and sexism in the texts.\\n\\nSimilarly, Akhtar et al. (2021) developed a dataset for hate speech detection involving migrants as victims of offensive texts and demonstrating that members of the same group tend to agree more. Moreover, they implemented perspective-aware models by creating a gold standard for each group, training two separate models, and finally ensembled by majority vote. Perspective-based ensembling methods have also been explored by Casola et al. (2023) for irony and hate speech detection in English. In their work, perspectives are extracted using sociodemographic information of annotators and mined from annotations.\\n\\nThe use of demographic data when working on highly subjective language phenomena has shown improvement in NLP research. Sap et al. (2022) investigated how annotators with different identities and beliefs perceive toxic content considering the characteristics of texts along three dimensions: racial prejudices, African American English vernacular and dialects, and vulgar words. The authors demonstrated that identities and beliefs impact annotators\u2019 judgments. Sociodemographic information has also been used to build a disagreement predictor by Wan et al. (2023), who tried to quantify how controversial an utterance is. They show that adding demographic information to the input improves results and returns insights on which instances need a more diverse group of annotators.\\n\\nFinally, Santy et al. (2023) developed a framework to identify datasets and model design biases, highlighting their positionality. Results show available datasets strongly align with the young WEIRD population (Western, Educated, Industrialized, Rich, 2https://pdai.info/ 3www.github.com/mainlp/ awesome-human-label-variation).\"}"}
{"id": "acl-2024-long-849", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Democratic).\\n\\nFew disaggregated datasets for irony detection exist. Simpson et al. (2019) released a corpus about humor detection in English, used as a benchmark in the first edition of the Learning With Disagreement shared task (Uma et al., 2021b). No annotators' metadata, however, are included. Frenda et al. (2023b) proposed a dataset for irony detection and investigated the influence of the annotators' characteristics on their perception (Frenda et al., 2023a); the dataset, however, contains English text only.\\n\\nTo the best of our knowledge, no multilingual disaggregated dataset exists. We fill this gap by proposing a perspectivist dataset for irony in 9 languages and a total of 25 high and low-resourced varieties. We also provide demographic information on all the annotators involved.\\n\\n3 MultiPICo\\n\\nIn this section, we describe MultiPICo, a corpus of 18,778 short conversations collected from Reddit (8,956) and Twitter (9,822) in 9 languages, and a total of 25 varieties. Data has been collected reproducing the structure of short conversations. Annotators were asked to read a set of Post and Reply pairs and answer whether the text of the reply was ironic or not, given the context.\\n\\nWe collected Reddit comments (first comments in the vast majority of cases and second-level comments in a few cases) with their direct replies and conversation-starting messages. For Twitter, the Post could be both a conversation starter or a direct reply to it. Reddit data were retrieved using the Pushshift repository from January 2020 to June 2021. To collect data in several linguistic varieties, we picked 26 subreddits, as reported in Table 10.\\n\\nThus, we inferred the linguistic variety of subreddit users from the subreddit. We filtered out pairs having at least one deleted or removed comment and further analyzed the target languages using the Python library for language identification LangID.\\n\\nWe collected Twitter data via Twitter Stream API, using the geolocation service and excluding quotes and retweets. Then, we retrieved the full conversation and retained tweets that directly replied to the starting ones. We tried to collect the same number of Post-Reply pairs from the two selected sources. However, there was not enough data on Reddit for some linguistic varieties. This was the case of US-American Spanish, Swiss-German, Iraqi, and Yemen Arabic (see Table 10). The data collection resulted in 18,778 instances, together with their metadata, consisting of Post-Reply original IDs, subreddits, and geolocation information.\\n\\nThe human annotation of the collected data was performed on the crowdsourcing platform Prolific, through an integrated custom-built annotation interface designed to collect a diverse and balanced set of annotators. The interface mimicked a message conversation, having the Post as context and asking whether the Reply was Ironic or Not ironic. We prevented an instance from being annotated more than a predefined number of times, with a mean of 5.02 annotations per instance (see Table 1).\\n\\nFigure 1: Screenshot of the annotation interface.\\n\\nAnnotators were selected based on three factors: i) their completion rate had to be at least 99%; ii) they had to be native speakers of the considered language; iii) the set of annotators needed to be balanced both across genders (230 Female, 274 Male, 1 Prefer not to say, 1 Null) and nationality (we hired around 24 annotators for all linguistic varieties, except for the English dataset with around 15). Considering both annotators' mother tongue and nationality, we inferred that annotators from a specific country speak the corresponding linguistic variety. Because of the lack of annotators, we were not able to preserve balance for the Arabic subset.\\n\\nThe quality of the annotation has been further assured using attention check questions in the form of \\\"Please answer X to this question.\\\" Annotators had 1% probability of receiving these special questions. We set a threshold of 50% correct answers; only 13 annotators failed the test, and their annotations were discarded.\\n\\n6 https://www.prolific.com/\\n7 The instructions and an example of annotation is presented in Appendix A.1.\\n8 This annotator did not share the information.\\n9 The Arabic portion of MultiPICo has 46 self-assessed males, 21 self-assessed females, and 1 annotator who prefers not to reveal their gender. Since we did not find enough annotators from the 5 considered countries, we removed this filter, collecting annotations from all mother-tongue annotators.\"}"}
{"id": "acl-2024-long-849", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language    | #Annotators | #Annotations | Label rate | #Texts | Sources | Annotation mean |\\n|-------------|-------------|--------------|------------|--------|---------|----------------|\\n|             |             |              |            |        |         | %not %iro #Reddit #Twitter |\\n| Arabic      | 68          | 10,609       | 68 32      | 2,181  | 949 1,232 | 4.86 |\\n| Dutch       | 25          | 4,991        | 73 27      | 1,000  | 500 500    | 4.99 |\\n| English     | 74          | 14,171       | 69 31      | 2,999  | 1,499 1,500 | 4.73 |\\n| French      | 50          | 8,770        | 70 30      | 1,760  | 1,000 760   | 4.98 |\\n| German      | 70          | 12,510       | 68 32      | 2,375  | 1,042 1,333 | 5.27 |\\n| Hindi       | 24          | 4,711        | 65 35      | 786    | 286 500    | 5.99 |\\n| Italian     | 24          | 4,790        | 69 31      | 1,000  | 500 500    | 4.79 |\\n| Portuguese  | 49          | 9,754        | 62 38      | 1,994  | 997 997    | 4.89 |\\n| Spanish     | 122         | 24,036       | 67 33      | 4,683  | 2,183 2,500 | 5.13 |\\n| Total       | 506         | 94,342       | 68 32      | 18,778 | 8,956 9,822 | 5.02 |\\n\\nTable 1: Number of annotators, annotations, texts per source, and annotation means for each language.\\n\\nwere excluded from the final corpus, leading to a total of 506 annotators. Together with the annotators' gender and nationality, we collected other demographic information, specifically: Age Group (13 Baby Boomer, 66 Gen-X, 260 Gen-Y, 162 Gen-Z, 5 Null), Ethnicity (315 White, 64 Mixed, 44 Asian, 13 Black, 66 Other, 4 Null), Student status (260 No, 165 Yes, 81 Null), Employment status (178 Full-time, 74 Part-time, 74 Unemployed and job seeking, 24 Not in paid work, 11 Due to start a new job within the next month, 36 Other, 109 Null).\\n\\n4 Annotators' polarization\\n\\nIn this section, we present an exploratory analysis of MultiPICo. We employ primarily the polarization index (P-index) proposed by Akhtar et al. (2019), which measures the instance-based polarization for k groups. For each instance i, the P-index is defined as:\\n\\n$$P(i) = \\\\frac{1}{k} \\\\sum_{1 \\\\leq w \\\\leq k} a(G_w)(1 - a(G_w))$$\\n\\nwhere k is the number of different groups of annotators, a(G_w) indicates the internal agreement for group G_w, and a(G) indicates the overall agreement on the instance. We compute the instance-level agreement as:\\n\\n$$a(G) = 1 - \\\\chi^2(G) / |M|$$\\n\\nwhere \\\\(\\\\chi^2(G)\\\\) is the chi-square statistics and M is the complete set of annotators. A high P-index (close to 1) indicates that the annotation of an instance is highly divergent (or polarized) across groups, while each group is internally consistent.\\n\\nDetails per language are in Appendix A.2.\\n\\n11 The values of Ethnicity for French and Dutch are uncertain because of the unbalance amount of annotators for each trait of this dimension (see Table 8 in Appendix A.2).\\n\\nFor instance, looking at table 3, Example #1 is annotated as ironic regardless of the annotators' sociodemographic traits, and its P-index is 0; Example #2 has a P-index of 1 because it is annotated as ironic by annotators who self-identified as male and not ironic by the one belonging to the other group (female).\\n\\nWe applied the P-index for two analyses to understand the relevance of sociodemographic information in the interpretation of irony and to investigate its cultural basis:\\n\\n1. analysis of dimensions: we compared the P-indices obtained by dividing the population based on gender, generation, nationality, ethnicity, student, and employment status (Section 4.1);\\n2. analysis of cultural basis: we examined the possible connection between the linguistic variety of the instances and the nationality of annotators, observing the mean of P-index scores of the instances belonging to the same linguistic variety when annotated as ironic by workers from the same country (Section 4.2).\\n\\nTo evaluate the significance of the obtained P-index scores, we computed random P-index values for each instance using random partitions with the same number of annotators for dimension and trait (see Table 8 in Appendix A). Thus, we could compare the mean of real P-index scores per group of annotators (henceforth 'real P-index') and the mean of their corresponding random P-index values (henceforth simply 'random P-index'). Finally, specifically for analyses 1 and 2, we also calculated the difference in percentage (\u2206) between real and random P-index.\"}"}
{"id": "acl-2024-long-849", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Analysis of dimensions\\n\\nAs the first exploratory analysis of MultiPICo, we examined the general level of polarization of all the dimensions available in this dataset: gender, generation, nationality, ethnicity, student, and employment status. In this case, annotators have been separated into various groups according to their trait (for instance, Boomers), and the P-index is computed among all the groups of the same demographic dimension (thus, e.g., Boomers, GenX, GenY, and GenZ for generation).\\n\\nTable 2 shows that all the differences between real and random P-index are positive (see column of % \u2206), except in very few cases (i.e., two cases in Dutch and one in Italian), likely due to the unbalance of information available about annotators. Moreover, for specific dimensions, such as generation and student status, % \u2206 is higher than 10% for the majority of languages. This observation suggests that age and student status are the most polarizing dimensions when detecting irony, i.e., people from the same age group tend to agree on the annotation while often disagreeing with those in another age range.\\n\\n12 In this case k depends on the number of traits per dimension (i.e., k = 2 in gender, k = 4 in generation, etc.) (Akhtar et al., 2019).\\n\\n4.2 Analysis of cultural basis\\n\\nIn this second analysis, to examine if irony is affected by the cultural background, we investigated the connection between linguistic varieties and nationality, observing the P-index of instances annotated as ironic by the majority of contributors coming from the corresponding nationality. In this case, we used the P-index in a binary fashion, i.e., one-vs-all. For each nationality, we divided the annotators into two groups, i.e., those with that nationality vs. those with other nationalities, and computed the P-index with k = 2 accordingly.\\n\\nIn Table 4, the agreement in the interpretation of irony appears to be country-based for specific varieties. A clear example is in German, where Austrian and German texts report the highest P-index when the texts are considered ironic by annotators coming from Austria and Germany respectively. The case of English is also interesting: the identification of irony in Indian texts polarizes specifically when the annotators come from India. Similar results are also observed, for instance, between American and British English and the annotators from their corresponding countries. The same tendency is visible in Spanish, where higher values of the P-index are reported when Mexican annotators detect irony in Mexican texts.\"}"}
{"id": "acl-2024-long-849", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Connection between the nationality of annotators and the language varieties per language through the P-index.\\n\\nThe situation in French and Portuguese is less crisp, showing specific varieties (French from France and Brazilian, respectively) that induce polarization regardless of the provenience of annotators. Looking at French texts, we noticed that most of the Post-Reply pairs annotated as ironic also by Canadians are humorous:\\n\\n(3) [Post] Direction Amsterdam?\\n[Reply] @user Salon de l'herbe?\\n\\nIn general, results in Table 4 suggest that the interpretation of irony is similar in speakers of the same language. However, for specific varieties, the sensibility of annotators is affected by their cultural background.\\n\\n5 MultiPICo as a benchmarking tool\\n\\nGiven its fine-grained metadata, both at the annotation and the annotator level, MultiPICo allows for detailed performance evaluation in a perspectivist setting. In this section, we will first analyze LLMs zero-shot performance for irony detection (Section 5.1) and then show some of the evaluation possibilities in exploring the LLMs' positionality (Section B.3) and the effectiveness of socio-demographic prompting (Section 5.3).\\n\\n5.1 Baseline performance\\n\\nThis section analyzes the irony detection performance of multilingual Large Language Models on the aggregated dataset. For all languages, we prompt the models to classify examples as ironic or not ironic. All prompts are written by native speakers of the target language. An example of a prompt for English is in Appendix B.1.\\n\\nTable 5 reports the f1-score for the positive class for all languages. Open models tend to perform lower than ChatGPT and are often close to random when considering irony. All models tend to output some non-standard labels, i.e., different than irony or not irony as requested in the prompt; however, the labels are semantically meaningful and can be mapped to the correct class in a semi-automatic way. In contrast, mT0 tends to prepend the prompt to the labels and produces many meaningless labels; we map the output to the intended label when possible and consider meaningless outputs incorrect by default. Results show that irony detection is still an open problem in a zero-shot scenario, particularly in non-mainstream languages.\"}"}
{"id": "acl-2024-long-849", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2 Positionality\\n\\nSanty et al. (2023) have exploited annotators' sociodemographic metadata to investigate design bias and the intrinsic positionality of datasets and models. Given the rich set of metadata available in MultiPICo, we can explore LLMs' positionality in the perception of irony for multiple languages. Following Santy et al. (2023), we grouped annotators according to their demographic traits and computed the mean label for annotators having a certain trait only. We use an LLM to label the instance in a zero-shot approach, using the base prompt in Appendix B.1. For all instances, we take the label produced by the model and compute their Pearson's correlation coefficient with the group-specific mean labels.\\n\\nWe find that, for all languages, ChatGPT tends to align with the perspective of young annotators (see Table 6). This is consistent with OpenAI's report on InstructGPT (Ouyang et al., 2022), where the annotators are described as \u00abquite young (75% less than 35 years old)\u00bb. Other dimensions \u2014 other than student status \u2014 show a weaker signal (see Table 11). Characterizing models' positionality is particularly important with closed-source systems, for which the design choices, training data, and algorithms are unknown and can only be characterized by their observed behaviors (Gallegos et al., 2023; Li et al., 2023; Kotek et al., 2023).\\n\\n5.3 Perspective-taking prompting\\n\\nTo explicitly model the differences in human views toward given phenomena, recent work has explored ad-hoc prompting, impersonating a specific user (Deshpande et al., 2023; Cheng et al., 2023) or exploiting relevant opinions previously expressed by the user (Hwang et al., 2023). In this context, Beck et al. (2023) have shown that sociodemographic prompting lacks robustness and propose to use it to identify ambiguous instances. However, its effect is hard to evaluate.\\n\\nMultiPICo provides a novel benchmark to evaluate sociodemographic prompting for irony detection in multiple languages. To showcase this, we construct perspective-based datasets per language. For each sociodemographic trait, we consider labels from annotators with that trait only and aggregate them using majority voting. Then, we divide each dataset into a training (80%) and a test (20%) set. We perform the following experiments:\\n\\n- No additional information (base): we prompt the model with a Post-Reply pair as discussed in Section 5.1.\\n- Trait-based perspective-taking (trait): we ask the model to impersonate a person from a specific socio-demographic group by prepending the information, e.g., \\\"You are a self-identified male/female/. . . /Indian/. . . \\\".\\n- Data-driven perspective-taking (data): we extract the most representative examples for each trait. We split the annotators into two groups: one composed of people with the trait and its complement. Among instances with a high agreement in the target group (> .60), we then rank the instances by their P-index computed according to this binary split and select the top 3 ironic and not ironic instances according to the target group annotation. We prompt the model in a few-shot setting, providing the extracted examples and the related labels. Note that this approach is completely data-driven and can be exploited to perform perspective-taking prompting even when the nature of the target group is difficult to describe.\\n\\nAn example of all prompts is in Appendix B.1. We use PolyLM for all experiments. Table 7 shows a widely variable performance across languages. For English, base results show the best performance in many cases, particularly for gender and generation. For Spanish and German, perspective-taking prompting works well, with the few-shot approach consistently superior to the prompt-based one. The data-driven approach also obtains good results for the Dutch language, where the fixed prompts do not improve over the baseline. For French, the few-shot approach is the only one that works; otherwise, the model never predicts irony. Finally, results for Portuguese and Italian are mixed, while the model\\n\\n...\"}"}
{"id": "acl-2024-long-849", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Sex       | Female | Male |\\n|-----------|--------|------|\\n|           | 0.408  | 0.395|\\n|           | 0.352  | 0.315|\\n|           | 0.380  | 0.377|\\n|           | 0.495  | 0.426|\\n|           | 0.513  | 0.416|\\n|           | 0.368  | 0.401|\\n|           | 0.439  | 0.407|\\n|           | 0.364  | 0.451|\\n\\n| Generation | Boomer | GenX | GenY | GenZ | Old | Young | Student | Employed | Nationality |\\n|------------|--------|------|------|------|-----|-------|---------|----------|-------------|\\n|            | 0.519  | 0.368| 0.441| 0.439| 0.378| 0.410 | 0.387   | 0.400    | 0.459       |\\n|            | 0.333  | 0.276| 0.381| 0.359| 0.354| 0.435 | 0.386   | 0.407    | 0.456       |\\n|            | 0.469  | 0.462| 0.443| 0.472| 0.519| 0.448 | 0.408   | 0.449    | 0.435       |\\n|            | 0.552  | 0.381| 0.447| 0.483| 0.517| 0.458 | 0.449   | 0.458    | 0.460       |\\n|            | \u2013      | \u2013    | \u2013    | \u2013    | \u2013   | \u2013     | \u2013       | \u2013        | \u2013           |\\n|            | \u2013      | \u2013    | \u2013    | \u2013    | \u2013   | \u2013     | \u2013       | \u2013        | \u2013           |\\n|            | 0.429  | 0.424| 0.404| 0.393| 0.453| 0.386 | 0.413   | 0.417    | 0.473       |\\n|            | 0.512  | 0.427| 0.369| 0.382| 0.432| 0.379 | 0.404   | 0.432    | 0.478       |\\n|            | 0.520  | 0.527| 0.453| 0.483| 0.519| 0.432 | 0.412   | 0.432    | 0.514       |\\n|            | 0.558  | 0.527| 0.445| 0.460| 0.517| 0.458 | 0.455   | 0.458    | 0.510       |\\n\\n| Nationality | Australia | Argentina | Canada | Austria | India | Colombia | France | Germany | Ireland | Mexico | Switzerland | UK | Spain | US | PT | IT | NL | AR |\\n|-------------|-----------|-----------|--------|---------|-------|----------|--------|---------|---------|--------|------------|----|-------|----|----|----|----|----|\\n|             | 0.459     | 0.456     | 0.293  | 0.474   | 0.426 | 0.433    | 0.473  | 0.424   | 0.457   | 0.478  |\\n|             | 0.435     | 0.452     | 0.405  | 0.453   | 0.460 | 0.445    | 0.337  | 0.376   | 0.404   | 0.419  |\\n|             | 0.410     | 0.429     | 0.374  | 0.562   | 0.538 | 0.556    | 0.342  | 0.392   | 0.419   | 0.419  |\\n|             | 0.466     | 0.454     | 0.396  | 0.455   | 0.455 | 0.452    | 0.466  | 0.454   | 0.396   | 0.452  |\\n|             | 0.400     | 0.417     | 0.401  | 0.521   | 0.520 | 0.521    | 0.400  | 0.417   | 0.401   | 0.521  |\\n\\nTable 7: PolyLM's f1 results for the positive class when prompting without sociodemographic information (base), with trait-based perspectives (trait) and with data-driven perspective examples (data) for all languages. The best performance is underlined. Data for some sociodemographic groups might be missing due to the lack of or the very low number of annotations.\"}"}
{"id": "acl-2024-long-849", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"does not seem to have a notion of irony for the Arabic language.\\n\\n6 Conclusions and Future Work\\nWe have presented MultiPICo, a multilingual perspectivist corpus for irony. The corpus consists of Post-Reply pairs from social media platforms, and it is released with disaggregated annotations and annotators' metadata.\\n\\nOur analysis of the corpus shows a high variability in the perception of irony across sociodemographic groups, confirming the importance of considering the annotators' characteristics and background for irony analysis. Some demographic dimensions like age and student status particularly induce a strong polarization of annotations. In some languages, such as German, English, and Spanish, we notice a connection between annotators' nationality and their sensitivity to irony across their linguistic varieties. It suggests that annotators with a background that is in line with a specific language variety tend to have higher agreement.\\n\\nFurthermore, we showcase the utility of MultiPICo as a benchmark for perspectivist modeling of irony through experiments of perspective-taking prompting with LLMs. In particular, we compare prompts without sociodemographic information, with trait-based perspectives, and with data-driven perspective examples, showing that it is possible to evaluate models in a perspectivist setting. Moreover, MultiPICo allowed us to perform a positionality analysis of ChatGPT, discovering that it tends to be aligned with the youngest annotators.\\n\\nIn the future, we plan to expand the analysis of linguistic patterns that characterize irony across languages and language varieties, looking also at the different types of irony (i.e., situational and verbal irony) involved in the Post-Reply pairs. Furthermore, we plan to mine meaningful clusters of annotators based on their annotation rather than on their demographic traits and to analyze whether a meaningful relationship exists between these groups and those based on sociodemographic information.\\n\\n7 Limitations\\nWhile our proposed resource is the first multilingual corpus of irony distributed with disaggregated annotations, we acknowledge that the choice of languages is still limited and somewhat arbitrary. The sociodemographic information about the annotators is also partial, bound to what was available from the crowdsourcing platform, and following a discretization of human personal traits that could be perceived as forces (e.g., representing self-identified gender as a single binary label).\\n\\nSimilarly to Sachdeva et al. (2022); Sap et al. (2022); Forbes et al. (2020), we noticed the ethnocity of annotators was unbalanced. We point out this issue to highlight the limitation of paid crowdsourcing (Santy et al., 2023). Furthermore, as shown by Orlikowski et al. (2023), annotators' sociodemographics do not always align with the most relevant grouping of annotators according to the language phenomenon under study. However, approaches based on mining perspectives (Lo and Basile, 2023), as opposed to strictly categorizing annotators, may alleviate this issue.\\n\\nIn the vast majority (\u223c90%) of cases, we downloaded the conversation-starting messages and their direct replies to capture the full conversational context. In a few cases, the downloaded reply was not direct but rather a second-level reply (a reply to a direct reply); thus, some conversational context might be missing.\\n\\n7.1 Ethical considerations\\nOur work stresses the necessity to consider and include the subjectivity of the annotators in NLP applications, encouraging reflection on the different perspectives encoded in annotated datasets to minimize the amplification of biases. The proposed corpus, moreover, can be used as a starting point also for investigating and evaluating LLMs across a multilingual spectrum, to make them apt to final users.\\n\\nFor building the proposed resource, we adopted measures to protect the privacy of annotators, and our data handling protocols are designed to safeguard personal information (like anonymization of users' mentions). Although our attention during the collection of data was focused on ironic content spread online, we acknowledge that some of the material to annotate could contain racist, sexist, stereotypical, violent, or generally disturbing content.\\n\\nRegarding the annotation process, we aimed to pay annotators fairly, estimating an average rate of 9\u00a3 per hour. Moreover, we tried to balance annotators through their nationality and self-identified gender. However, we are aware that considering gender in a binary form is limited. We plan to adopt a more inclusive approach toward non-binary annotators in future work.\"}"}
{"id": "acl-2024-long-849", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some sociodemographic information had not been managed by us before the annotation process, and, when examining them in this work, we noticed a substantial unbalance for some dimensions, like the self-identified ethnicities. This pattern suggests the need to interact differently with annotators or social communities if we want a diversity of annotators and perspectives in terms of social background.\\n\\nAcknowledgements\\n\\nThis work was funded by the 'Multilingual Perspective-Aware NLU' project in partnership with Amazon Alexa. The work of Cristina Bosco is also supported by Compagnia di San Paolo - Bando ex-post 2020 - SteroetypHate. The work of Valerio Basile is also supported by Compagnia di San Paolo - Bando ex-post 2020 - \\\"Toxic Language Understanding in Online Communication - BREAKhateDOWN\\\".\\n\\nWe thank all the people involved in providing and checking the prompt translations.\\n\\nReferences\\n\\nSohail Akhtar, Valerio Basile, and Viviana Patti. 2019. A new measure of polarization in the annotation of hate speech. In AI*IA 2019 \u2013 Advances in Artificial Intelligence, pages 588\u2013603, Cham. Springer International Publishing.\\n\\nSohail Akhtar, Valerio Basile, and Viviana Patti. 2021. Whose opinions matter? Perspective-aware models to identify opinions of hate speech victims in abusive language detection. arXiv preprint arXiv:2106.15896.\\n\\nDina Almanea and Massimo Poesio. 2022. ArMIS - the Arabic Misogyny and Sexism Corpus with Annotator Subjective Disagreements. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 2282\u20132291, Marseille, France. European Language Resources Association.\\n\\nLora Aroyo and Chris Welty. 2015. Truth is a lie: Crowd truth and the seven myths of human annotation. AI Magazine, 36(1):15\u201324.\\n\\nValerio Basile, Michael Fell, Tommaso Fornaciari, Dirk Hovy, Silviu Paun, Barbara Plank, Massimo Poesio, Alexandra Uma, et al. 2021. We need to consider disagreement in evaluation. In Proceedings of the 1st workshop on benchmarking: past, present and future, pages 15\u201321. Association for Computational Linguistics.\\n\\nTilman Beck, Hendrik Schuff, Anne Lauscher, and Iryna Gurevych. 2023. How (not) to use sociodemographic information for subjective NLP tasks. CoRR.\\n\\nEmily M Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604.\\n\\nFederico Cabitza, Andrea Campagner, and Valerio Basile. 2023. Toward a perspectivist turn in ground truthing for predictive computing. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 6860\u20136868.\\n\\nSilvia Casola, Soda Lo, Valerio Basile, Simona Frenda, Alessandra Cignarella, Viviana Patti, and Cristina Bosco. 2023. Confidence-based ensembling of perspective-aware models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3496\u20133507, Singapore. Association for Computational Linguistics.\\n\\nMyra Cheng, Esin Durmus, and Dan Jurafsky. 2023. Marked personas: Using natural language prompts to measure stereotypes in language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1504\u20131532, Toronto, Canada. Association for Computational Linguistics.\\n\\nAmeet Deshpande, Vishvak Murahari, Tanmay Rajpurohit, Ashwin Kalyan, and Karthik Narasimhan. 2023. Toxicity in chatgpt: Analyzing persona-assigned language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1236\u20131270.\\n\\nVirginia Dignum. 2023. Responsible Artificial Intelligence: Recommendations and Lessons Learned. In Responsible AI in Africa: Challenges and Opportunities, pages 195\u2013214. Springer International Publishing Cham.\\n\\nMaxwell Forbes, Jena D. Hwang, Vered Shwartz, Maarten Sap, and Yejin Choi. 2020. Social chemistry 101: Learning to reason about social and moral norms. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 653\u2013670, Online. Association for Computational Linguistics.\\n\\nSimona Frenda, Soda Marem Lo, Silvia Casola, Bianca Scarlini, Cristina Marco, Valerio Basile, and Davide Bernardi. 2023a. Does anyone see the irony here? Analysis of perspective-aware model predictions in irony detection. In ECAI 2023 Workshop on Perspectivist Approaches to NLP.\\n\\nSimona Frenda, Alessandro Pedrani, Valerio Basile, Soda Marem Lo, Alessandra Teresa Cignarella, Raffaella Panizzon, Cristina Marco, Bianca Scarlini, Viviana Patti, Cristina Bosco, and Davide Bernardi. 2023b. EPIC: Multi-perspective annotation of a corpus of irony. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 13844\u201313857, Toronto, Canada. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-849", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Isabel O. Gallegos, Ryan A. Rossi, Joe Barrow, Md Mehrab Tanjim, Sungchul Kim, Franck Dernoncourt, Tong Yu, Ruiyi Zhang, and Nesreen K. Ahmed. 2023. Bias and fairness in large language models: A survey.\\n\\nEunJeong Hwang, Bodhisattwa Majumder, and Niket Tandon. 2023. Aligning language models to user opinions. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5906\u20135919, Singapore. Association for Computational Linguistics.\\n\\nHadas Kotek, Rikker Dockum, and David Sun. 2023. Gender bias and stereotypes in large language models. In Proceedings of The ACM Collective Intelligence Conference, CI '23, page 12\u201324, New York, NY, USA. Association for Computing Machinery.\\n\\nElisa Leonardelli, Stefano Menini, Alessio Palmero Aprosio, Marco Guerini, and Sara Tonelli. 2021. Agreeing to disagree: Annotating offensive language datasets with annotators\u2019 disagreement. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, page 10528\u201310539.\\n\\nElisa Leonardelli, Alexandra Uma, Gavin Abercrombie, Dina Almanea, Valerio Basile, Tommaso Fornaciari, Barbara Plank, Verena Rieser, and Massimo Poesio. 2023. Semeval-2023 task 11: Learning with disagreements (lewidi). In Proceedings of the 17th International Workshop on Semantic Evaluation (SemEval-2023), page 2304\u20132318.\\n\\nYingji Li, Mengnan Du, Rui Song, Xin Wang, and Ying Wang. 2023. A survey on fairness in large language models.\\n\\nSoda Marem Lo and Valerio Basile. 2023. Hierarchical clustering of label-based annotator representations for mining perspectives. In Proceedings of the 2nd Workshop on Perspectivist Approaches to NLP co-located with 26th European Conference on Artificial Intelligence (ECAI 2023), Krak\u00f3w, Poland, September 30th, 2023, volume 3494 of CEUR Workshop Proceedings. CEUR-WS.org.\\n\\nAida Mostafazadeh Davani, Mark D\u00edaz, and Vinodkumar Prabhakaran. 2022. Dealing with disagreements: Looking beyond the majority vote in subjective annotations. Transactions of the Association for Computational Linguistics, 10:92\u2013110.\\n\\nMatthias Orlikowski, Paul R\u00f6ttger, Philipp Cimiano, and Dirk Hovy. 2023. The ecological fallacy in annotation: Modeling human label variation goes beyond sociodemographics. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 1017\u20131029, Toronto, Canada. Association for Computational Linguistics.\\n\\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback.\\n\\nBarbara Plank. 2022. The \u201cproblem\u201d of human label variation: On ground truth in data, modeling and evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10671\u201310682.\\n\\nVinodkumar Prabhakaran, Aida Mostafazadeh Davani, and Mark Diaz. 2021. On releasing annotator-level labels and information in datasets. In Proceedings of the Joint 15th Linguistic Annotation Workshop (LAW) and 3rd Designing Meaning Representations (DMR) Workshop, page 133\u2013138.\\n\\nPratik Sachdeva, Renata Barreto, Geoff Bacon, Alexander Sahn, Claudia von Vacano, and Chris Kennedy. 2022. The measuring hate speech corpus: Leveraging rasch measurement theory for data perspectivism. In Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022, pages 83\u201394, Marseille, France. European Language Resources Association.\\n\\nSebastin Santy, Jenny Liang, Ronan Le Bras, Katharina Reinecke, and Maarten Sap. 2023. NLPositionality: Characterizing design biases of datasets and models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 9080\u20139102, Toronto, Canada. Association for Computational Linguistics.\\n\\nMaarten Sap, Swabha Swayamdipta, Laura Vianna, Xuhui Zhou, Yejin Choi, and Noah A. Smith. 2022. Annotators with attitudes: How annotator beliefs and identities bias toxic language detection. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5884\u20135906, Seattle, United States. Association for Computational Linguistics.\\n\\nTeven Le Scao, Angela Fan, Christopher Akiki, Elie Pavlick, Suzana Ili \u00b4c, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Luccioni, Fran\u00e7ois Yvon, Matthias Gall\u00e9, Jonathan Tow, Alexander M. Rush, Stella Biderman, Albert Webson, Pawan Sasanka Ammanamanchi, Thomas Wang, Beno\u00eet Sagot, Niklas Muennighoff, Albert Villanova del Moral, Olatunji Ruwase, Rachel Bawden, Stas Bekman, Angelina McMillan-Major, Iz Beltagy, Huu Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Lauren\u00e7on, Yacine Jernite, Julien Launay, Margaret Mitchell, Colin Raffel, Aaron Gokaslan, Adi Simhi, Aitor Soroa, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris Emezue, Christopher Klamm, Colin Leong, Daniel van Strien, David Ifeoluwa Adelani, and et al. 2023. Bloom: A 176b-parameter open-access multilingual language model. CoRR, abs/2211.05100.\"}"}
{"id": "acl-2024-long-849", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instructions provided to the annotators before starting the annotation task:\\n\\nIs it ironic?\\n\\nIn this study, we ask the participants to read a message and a reply, and judge if the reply is ironic. Irony is a figurative language device that conveys the opposite of literal meaning, profiling intentionally a secondary or extended meaning. For instance:\\n\\nmessage:\\nIf ur homeless u probably wouldn't have a phone.\\nreply:\\nYes, and all your belongings would be in a handkerchief tied at the end of a stick. \u2013> irony: yes\\n\\nmessage:\\nIf ur homeless u probably wouldn't have a phone.\\nreply:\\nYes, you're right. \u2013> irony: no\\n\\nThis annotation consists of 200 small conversations of tweets and Reddit's posts and will take more or less 2 hours. There are no requirements for taking part in this study, simply annotate the presence of irony as you perceive it. Thank you for your interest in this research!\\n\\nExample of task:\\n\\nMessage\\nThis man is so completely focused towards engineering riots.\\nReply\\nGotta stick to your strengths\\nIs the reply ironic?\\nIronic or Not ironic\\n\\nAppendix A\\n\\nA.1 Annotation process\\n\\nAppendix B\\n\\nB.1 Prompt example\\n\\nIn this section, we report the prompts written by native speakers and used for our experiments. Instruction: You are provided in input (Input) a pair of sentences (Post, Reply) extracted from social media conversations. Your task is to determine if the Reply is ironic in the context of the Post. Please provide in output (Output) a single label \u201cirony\u201d or \u201cnot irony.\u201d\\n\\nInput:\\n- Post: Because the last generation was Gen. Z and there's no where to go past that, so we gotta start from the beginning.\\n- Reply: but we should have just named the first generation \\\"Alpha\\\" instead of doing it now\\nOutput:\"}"}
{"id": "acl-2024-long-849", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 8: Sociodemographic information about annotators per language.\\n\\n| Languages          | English | Spanish | Italian | French | Dutch | German | Hindi | Arabic | Portuguese |\\n|--------------------|---------|---------|---------|--------|-------|--------|-------|--------|------------|\\n| Demographics       |         |         |         |        |       |        |       |        |            |\\n| **Gender**         |         |         |         |        |       |        |       |        |            |\\n| Male               | 37%     | 28%     | 22%     | 35%    | 39%   | 39%    | 37%   | 16%    | 32%        |\\n| Female             | 63%     | 72%     | 78%     | 65%    | 61%   | 61%    | 63%   | 84%    | 68%        |\\n| **Age group**      |         |         |         |        |       |        |       |        |            |\\n| GenX               | 22%     | 31%     | 26%     | 21%    | 24%   | 24%    | 26%   | 21%    | 24%        |\\n| GenY               | 17%     | 24%     | 15%     | 15%    | 16%   | 16%    | 15%   | 14%    | 16%        |\\n| GenZ               | 66%     | 45%     | 23%     | 69%    | 69%   | 69%    | 65%   | 86%    | 86%        |\\n| **Ethnicity**      |         |         |         |        |       |        |       |        |            |\\n| White              | 47%     | 46%     | 23%     | 40%    | 22%   | 66%    | 20%   | 37%    | 20%        |\\n| Black              | 3%      | 3%      | 5%      | 1%     | 2%    | 1%     | 2%    | 1%     | 2%         |\\n| Mixed              | 3%      | 3%      | 1%      | 3%     | 2%    | 3%     | 1%    | 10%    | 10%        |\\n| Other              | 3%      | 2%      | 8%      | 1%     | 1%    | 1%     | 8%    | 31%    | 31%        |\\n| **Student status** |         |         |         |        |       |        |       |        |            |\\n| Yes                | 13%     | 39%     | 14%     | 16%    | 7%    | 14%    | 8%    | 29%    | 30%        |\\n| No                 | 46%     | 60%     | 9%      | 30%    | 16%   | 39%    | 14%   | 25%    | 16%        |\\n| **Employment status** |     |         |         |        |       |        |       |        |            |\\n| Full-time          | 25%     | 41%     | 9%      | 24%    | 10%   | 24%    | 10%   | 20%    | 15%        |\\n| Unemployed         | 11%     | 24%     | 7%      | 5%     | 4%    | 3%     | 1%    | 11%    | 8%         |\\n| Part-time          | 11%     | 17%     | 5%      | 5%     | 3%    | 10%    | 4%    | 13%    | 6%         |\\n| Not in paid work   | 4%      | 4%      | 1%      | 5%     | 4%    | 5%     | 1%    | 1%     | 1%         |\\n| Due to start       | \u2013       | 3%      | 1%      | 1%     | 2%    | 2%     | 2%    | 2%     | 2%         |\\n| **Other**          | 1%      | 6%      | 6%      | 3%     | 1%    | 5%     | 1%    | 14%    | 14%        |\\n\\nTable 9: Characteristics of each demographic cohort\\n\\n| Characteristics                      | Subreddits | Total |\\n|--------------------------------------|------------|-------|\\n| **Gender**                           |            |       |\\n| Female                               |            |       |\\n| Male                                 |            |       |\\n| **Age group**                        |            |       |\\n| Boomers                              |            |       |\\n| GenX                                 |            |       |\\n| GenY                                 |            |       |\\n| GenZ                                 |            |       |\\n| Old                                  |            |       |\\n| Young                                |            |       |\\n| **Student status**                   |            |       |\\n| Yes                                  |            |       |\\n| No                                   |            |       |\\n| **Employment status**                |            |       |\\n| Yes                                  |            |       |\\n| No                                   |            |       |\\n\\nTable 10: Varieties for each language, number of annotated text per language variety, subreddits.\\n\\n| Languages       | Varieties | Subreddits | Total |\\n|-----------------|-----------|------------|-------|\\n| EN               | Australian| r/australia| 2,787 |\\n| ES               | Argentinean| r/argentina| 4,806 |\\n| ES               | Colombian| r/Colombia| 4,805 |\\n| ES               | Spanish | r/spain | 4,804 |\\n| ES               | Mexican | r/mexico | 4,812 |\\n| ES               | US English| r/AskReddit| 4,843 |\\n| ES               | Brazilian| r/brasil | 4,881 |\\n| ES               | Portuguese| r/portugal| 4,873 |\\n| ES               | Italian | r/italy | 4,790 |\\n| ES               | Canadian| r/Quebec | 3,788 |\\n| ES               | Netherlands| r/nederlands| 4,991 |\\n| ES               | Austrian | r/Austria | 4,607 |\\n| ES               | Swiss    | r/schwiiz | 3,233 |\\n| ES               | German   | r/de | 4,670 |\\n| ES               | Hindi    | r/Hindi | 4,711 |\\n| ES               | Egyptian| r/Egypt | 2,456 |\\n| ES               | Iraqi    | r/Iraq | 2,640 |\\n| ES               | Moroccan| r/Morocco | 2,630 |\\n| ES               | Saudi Arabian| r/saudiarabia| 2,455 |\\n| ES               | Yemen    | r/Yemen | 428 |\\n\\nInstructions for perspective-taking prompts:\\n\\n**Prompt-based perspective-taking prompt for English, for the British perspective.**\\n\\n**Instruction:** You are British. You are provided with input (Input) a pair of social media conversations. Your task is to determine if the Reply is ironic in the context of the Post. Please provide in output (Output) a single label \u201cirony\u201d or \u201cnot irony\u201d.\\n\\n**Example 1:**\\n\\n- **Input:**\\n  - Post: I went there about 10 years ago. Costs about \u00a320 to go in and it's just a greenhouse.\\n  - Reply: You go to look at the plants not the greenhouse. It's like saying a restaurant is just a bunch of tables.\\n- **Output:** irony\\n\\n**Example 2:**\\n\\n- **Input:**\\n  - Post: casually joins you. Every time I log out of my bank they're there. And ad with them in is on repeat in my local branch. How I'm a Zelebrity is still going after all these years is beyond me. And please, God, why do people pretend they can't tell them apart?\\n  - Reply: I think I'd have to switch banks\\n- **Output:** irony\\n\\n**Example 3:**\\n\\n- **Input:**\\n  - Post: I think I'd have to switch banks\\n- **Output:** irony\"}"}
{"id": "acl-2024-long-849", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11: ChatGPT positionality.\\n\\n|     | AR   | NL   | EN   | DE   | HI   | IT   | ES   | US   | MX   | AR   | CO   |\\n|-----|------|------|------|------|------|------|------|------|------|------|------|\\n|     | .345 | .045 | .237 | .162 | -    | -    | .208 | .207 | .282 | .235 | .317 |\\n| Generation Age Gender Nationality Working Studying | .362 | .136 | .207 | .277 | .267 | .359 | .181 | .267 | .282 | .317 | .235 |\\n| Boomer GenX GenY GenZ Young Old Male Female Y N Y N | .384 | .149 | .240 | .267 | .168 | .259 | .282 | .235 | .282 | .317 | .229 |\\n|     | .271 | .172 | .234 | .324 | .252 | .415 | .317 | .190 | .267 | .317 | .226 |\\n|     | .398 | .045 | .272 | .260 | .263 | .380 | .317 | .190 | .267 | .317 | .265 |\\n|     | .364 | .144 | .219 | .291 | .166 | .350 | .277 | .190 | .267 | .317 | .255 |\\n|     | .372 | .131 | .259 | .293 | .258 | .372 | .277 | .190 | .267 | .317 | .269 |\\n|     | .384 | .182 | .223 | .308 | .137 | .389 | .267 | .190 | .267 | .317 | .256 |\\n|     | .333 | .124 | .249 | .283 | .292 | .206 | .261 | .190 | .242 | .365 | .229 |\\n|     | .326 | .129 | .218 | .320 | .248 | .298 | .229 | .190 | .242 | .374 | .226 |\\n|     | .384 | .178 | .215 | .320 | .297 | .297 | .229 | .190 | .242 | .374 | .256 |\\n|     | .378 | .196 | .196 | .235 | .237 | .239 | .229 | .190 | .242 | .374 | .256 |\\n|     | .333 | .196 | .196 | .235 | .237 | .239 | .229 | .190 | .242 | .374 | .256 |\\n\\nExample 4:\\nInput:\\n- Post: We just outraised Greg Abbott \u2014 again.\\n- Reply: Money won't get you elected\\nOutput: not irony\\n\\nExample 5:\\nInput:\\n- Post: When you're young, work to learn don't work to earn. You should prioritise study over work. Go full time uni and part time work.\\n- Reply: >work to learn don't work to earn\\nOutput: not irony\\n\\nExample to label:\\n- Post: Because the last generation was Gen. Z and there's no where to go past that, so we gotta start from the beginning.\\n- Reply: but we should have just named the first generation \\\"Alpha\\\" instead of doing it now\\nYour output: Data-driven perspective-taking prompt for En-\\nglish, for the British perspective.\\n\\nB.2 Computational resources\\nBaseline performance (Sections 5 and B.3)\\nChatGPT (gpt-3.5-turbo) has a cost of $0.001/1000 tokens. We run PolyLM-13B on 5 v100 GPUs with 16GB VRAM; the experiment took approximately 7 hours. We ran the BLOOM zero-shot experiments on two NVIDIA-A40 GPUs; experiments were completed in around 200 hours. The long processing time for BLOOM is likely because the model tends to generate unnecessarily long outputs (e.g., repeating the prompt) rather than the actual label.\\n\\nPerspective-taking prompts\\nWe run PolyLM-13B on 5 v100 GPUs with 16GB VRAM; the experiments took approximately 200 hours in total.\\n\\nB.3 ChatGPT positionality\\nTable 11 reports the results for ChatGPT's positionality.\"}"}
