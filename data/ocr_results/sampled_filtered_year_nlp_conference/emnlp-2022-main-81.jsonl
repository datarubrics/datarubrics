{"id": "emnlp-2022-main-81", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAnswering complex questions that require making latent decisions is a challenging task, especially when limited supervision is available. Recent works leverage the capabilities of large language models (LMs) to perform complex question answering in a few-shot setting by demonstrating how to output intermediate rationalizations while solving the complex question in a single pass. We introduce \u201cSuccessive Prompting\u201d, where we iteratively break down a complex task into a simple task, solve it, and then repeat the process until we get the final solution. Successive prompting decouples the supervision for decomposing complex questions from the supervision for answering simple questions, allowing us to (1) have multiple opportunities to query in-context examples at each reasoning step (2) learn question decomposition separately from question answering, including using synthetic data, and (3) use bespoke (fine-tuned) components for reasoning steps where a large LM does not perform well. The intermediate supervision is typically manually written, which can be expensive to collect. We introduce a way to generate a synthetic dataset which can be used to bootstrap a model\u2019s ability to decompose and answer intermediate questions. Our best model (with successive prompting) achieves an improvement of \\\\( \\\\sim 5\\\\% \\\\) absolute F1 on a few-shot version of the DROP dataset when compared with a state-of-the-art model with the same supervision.\\n\\n1 Introduction\\n\\nCompositional reading comprehension datasets like HotpotQA (Yang et al., 2018) and DROP (Dua et al., 2019) have inspired a range of model architectures that learn to answer complex questions with weak supervision from the final answer. One recent direction is to leverage large language models (LMs) to solve compositional tasks with very few examples by generating latent reasoning steps before answering the question (Wei et al., 2022; Nye et al., 2021; Karpas et al., 2022). Given a complex question, this approach first finds nearest-neighbor training examples from a dataset of (question, reasoning, answer) triples and then concatenates them to create an input for the LM. A large LM is then prompted with this input to generate the intermediate reasoning steps needed, while answering the complex question in a single pass.\\n\\nWhile promising, this approach discards many of the benefits of prior approaches to this task (Khot et al., 2021; Karpas et al., 2022) by coupling the supervision for question decomposition to the supervision for performing the intermediate steps. Moreover, its non-modular nature does not allow using alternate symbolic reasoning engines in cases where they perform better than LMs. Additionally, the model gets exposed to only a single set of in-context examples, selected based on their proximity to the complex question, which may not contain\\n\\n---\\n\\nFigure 1: Example decomposition used by Successive Prompting\u2019s question decomposition and question answering stage on a DROP example. The model iterates between predicting a simple question to ask and answering the simple question.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"optimal supervision for the intermediate steps that need to be taken. We propose \\\"Successive Prompting\\\", where we iteratively decompose the complex question into the next simple question to answer, answer it, and then repeat until the complex question is answered (Figure 1). Each of these steps is performed with separate a query to the LM. Since the decomposition and answering steps are performed separately, we can decouple the supervision of each step, providing two primary benefits. First, when performing in-context learning, we get multiple opportunities to select different in-context examples, which can be tailored to the particular decomposition or answering step being performed, instead of selecting a single set of examples based only on the complex question. Second, when fine-tuning (with or without in-context examples (Chen et al., 2022)), we can provide training examples for each step independently, so the model only has to learn to perform one step at a time.\\n\\nThis decoupling additionally allows us to judiciously inject synthetic data into the learning process, e.g., to help the model answer a particular kind of simple question that it could not previously answer, or a new reasoning composition it did not know how to decompose. Because the steps are separate, we can isolate model failures and develop synthetic approaches to fill in the gaps. It also allows us to replace the LM with other, purpose-built components to perform symbolic reasoning when appropriate (Khot et al., 2021; Segal et al., 2020; Jin et al., 2021).\\n\\nWe demonstrate the utility of successive prompting using a few-shot variant of the DROP dataset (Dua et al., 2019), selecting 300 examples for training (either fine-tuning or in-context example selection). These 300 examples are manually annotated with simple QA pairs as decompositions. We find that performance of all models is quite low in this few-shot setting, so we develop a synthetic data generator that produces complex questions with their decompositions from semi-structured Wikipedia tables (Yoran et al., 2021). This synthetic data provides not just complex question supervision, but also supervision for the intermediate steps. We augment this data with the 300 (complex) training examples and their decompositions from DROP. In this few-shot setting, our best performing successive prompting model shows a \u223c5% improvement in F1 when compared to state-of-the-art model on DROP. The code and data are available at https://github.com/dDua/succesive_prompting.\\n\\n2 Decomposing Complex Questions\\nThe goal of compositional question answering is to answer a complex question $q$ in the context of a passage $p$ (together denoted as $x$) by reasoning through latent sequential decisions $z = z_1, z_2, ..., z_s$ to reach the final answer, $y$. Many models have been proposed to accomplish this with varying amounts of supervision and interpretability. In prompting methods like Chain-of-Thought (CoT, Wei et al., 2022) the latent steps are supervised, interpretable sentences; in other models these latent steps might be a program (Gupta et al., 2020; Chen et al., 2020) or even just the (unsupervised) hidden states in the model (Segal et al., 2020; Andor et al., 2019).\\n\\nWe focus on models that take in-context examples and produce a discrete, language-encoded $z$, with CoT being the primary exemplar. We write the general form for CoT, given an input $x$, a language model encoder $L$ and $N$ in-context examples obtained from querying an index $I$\u2014each containing a triplet of passage with complex question ($x_n$, $y_n$, $z_n$)\u2014as follows:\\n\\n$$y, z \\\\leftarrow L(x, \\\\{(x_n, y_n, z_n) | n \\\\in [1, N]\\\\})$$\\n\\n2.1 Successive prompting\\nIn successive prompting, we represent each latent step as a pair of simple question and answer, $z_k = (q_k, a_k)$ (see Figure 1 for example QA pairs) unlike CoT which represents each latent step as a declarative sentence. Moreover, CoT queries the index $I$ for in-context examples and prompts the language model $L$ for generating output only once. However, in successive prompting, we separate $z$ into multiple question and answering steps, which gives us many opportunities to prompt $L$, with potentially different in-context examples that are more tailored to the simple question at each step. It also enables us to re-encode the context given the intermediate state $z_k$, which can be useful in certain questions that need long chain referencing (e.g., the sort-count example in Figure 3). We can write a general form for successive prompting as follows:\\n\\n$$q_1 \\\\leftarrow L(x, \\\\{(x_n, q_n^1) | n \\\\in [1, N]\\\\})$$\\n\\n$$a_1 \\\\leftarrow L(p, q_1, \\\\{(p_m^*, q_m^*, a_m^*) | m \\\\in [1, M]\\\\})$$\"}"}
{"id": "emnlp-2022-main-81", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: A demonstration of successive prompting with in-context learning. The selected examples for supervision and complex question to be answered pre-pended with the context paragraph (omitted to simplify illustration) are encoded by the model to generate question and answer at QD and QA stage respectively. During fine-tuning, only training supervision is used in an i.i.d manner for learning QD and QA models.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of how in-context learning is executed step-by-step in each stage until QD outputs the special phrase \u201cThere are no more questions left to ask\u201d, along with a final answer.\\n\\nSuccessive prompting allows the QA stage to access simple questions derived from complex questions that would not have been retrieved by Chain-of-Thought prompting because on the surface they are not similar to the held-out complex question, even though they share similar sub-questions.\\n\\nModel Fine-tuning\\n\\nFor model fine-tuning, we use T5 (Raffel et al., 2020) based sequence-to-sequence models. Such models are typically trained with control codes in a multi-task setting (Ma et al., 2021; Rajagopal et al., 2022) to switch between QD and QA tasks with shared model parameters. We adapt and extend the control codes introduced by text modular networks (TMNs, Khot et al., 2021) for training with our synthetic data. TMNs are limited in terms of the operations they can handle as they do not go beyond first-order reasoning. We use synthetically generated data, which allows us to deal with higher-order reasoning questions in DROP. Because we are fine-tuning the model, we can use special tokens to denote question decomposition and other separators, instead of the natural language prompts shown in Figure 2, though the content is the same. The specific tokens used for each step are listed in Appendix A.\\n\\nSpecialized Modules\\n\\nSuccessive prompting also allows us to use specialized sub-modules for solving different QA tasks because we no longer perform QD and QA in an end-to-end manner. Solving arithmetic operations like counting, difference, sorting, etc., can be challenging for language models. As a result, we follow Khot et al. (2021) and construct a simple mathematical sub-module for QA which parses the generated simple question for symbolic operation type and its arguments and then executes them in a deterministic way. If the generated simple question cannot be parsed as a mathematical operation, we apply the language model to solve it.\\n\\n3 Synthetic Dataset\\n\\nAny method that prompts LMs to produce intermediate reasoning steps to answer complex questions needs some amount of supervision for those reasoning steps. This kind of annotation can be expensive to collect and often requires expert knowledge.\\n\\nTo mitigate these data issues, we propose a way to synthetically generate complex questions and their decompositions using semi-structured data which is easy to parse. We show that we can bootstrap model learning with this out-of-domain, synthetically generated data so it can adapt better when fine-tuned with limited in-domain supervision.\\n\\nGeneration Process:\\n\\nInspired by Yoran et al. (2021), we use semi-structured data from tables in English Wikipedia which are available in plenty. We employ curated templates to convert the rows in the tables into paragraphs. We use single column headers to create first-order simple questions and a combination of columns for higher-order complex questions. We synthesize data for 10 simple operations: COUNT, TOP(k), BOTTOM(k), FILTER, SUM, COMPARISON, DIFFERENCE, NEGATION, GATHER, and INTERSECTION. We generate higher order combinations of first-order operations, wherever possible. Figure 3 shows examples of higher order combinations of the atomic operation COUNT with a few other simple operations using Table 1 as context. The complete list of all decompositions is provided in Appendix A. Depending on the model, we use either symbolic or natural language version of the arithmetic operations. If we are using an LM to perform arithmetic operations, we output natural language; if we are using a separate symbolic reasoning engine, we output symbolic operations. We generate approximately 141K total complex questions which result in 525K examples for QD and 257K examples for QA. See Appendix A for more dataset statistics.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reasoning Complex Question and Decomposition (Question [Natural Language or Symbolic], Answer)\\n\\nCount How many opponents were there?\\n\\n- Q: What are all the opponents?\\n  Ans: Walsall; Liverpool; Sheffield Wed.; Oxford United; Portsmouth\\n\\n- Q: count(Walsall; Portsmouth; Sheffield Wed.; Oxford United; Portsmouth) Ans: 5\\n\\n- Q: How many items are in the list: Walsall, Liverpool, Sheffield Wed. and Oxford United, Portsmouth?\\n\\nHigher order decompositions\\n\\nSort-Count Which venue had the most number of opponents?\\n\\n- Q: What are all the venues? Ans: A; H\\n\\n- Q: What are opponents when venue was A? Ans: Walsall; Oxford United; Portsmouth\\n\\n- Q: count(Walsall; Oxford United; Portsmouth) Ans: 3\\n\\n- Q: What are opponents when venue was H? Ans: Liverpool; Sheffield Wed.\\n\\n- Q: count(Liverpool; Sheffield Wed.) Ans: 2\\n\\n- Q: top(1, 2; 3) Ans: 3\\n\\n- Q: What is the largest value in: 2 and 3?\\n\\nComparison-Count Which round had more venues: SF 1st Leg or QFR?\\n\\n- Q: What are the rounds when venue was A? Ans: R2 1st Left; SF 2nd Leg; QFR\\n\\n- Q: count(R2 1st Left; SF 2nd Leg; QFR) Ans: 3\\n\\n- Q: What are the rounds when venue was H? Ans: QFR; SF 1st Leg\\n\\n- Q: count(QFR; SF 1st Leg) Ans: 2\\n\\n- if_then(1 > 2; SF 1st Leg; QFR) Ans: QFR\\n\\n- Q: If 1 > 2 then answer is SF 1st Leg else it is QFR\\n\\nFigure 3: Examples of COUNT operation and some of its higher order combinations, with natural language and symbolic decompositions of the complex question. Underneath the first instance of a symbolic operation we show its corresponding natural language version. See Table 1 for the original table used to generate context and questions.\\n\\n4 Experiments and Results\\n\\nThe DROP dataset contains a variety of reasoning compositions which are not uniformly distributed. In order to get a fair representation of DROP examples, we first embed the examples using a sentence embedding method trained on the QQP dataset (Reimers and Gurevych, 2019). We then use cosine similarity to get the top-50 nearest neighbor questions for each training example. The connection graph between each training question to its neighbors is then used to obtain 300 questions that cover the majority of the training data, via the vertex cover algorithm. We manually annotate these 300 examples with decomposed QA pairs in the same format as our synthetic data (Figure 3). For synthetic examples, since we know the reasoning types, we uniformly sample example demonstration from each reasoning type.\\n\\n4.1 In-context Learning\\n\\nSetup\\n\\nWe use faiss\\\\(^1\\\\) index with the QQP-based sentence embedding (Reimers and Gurevych, 2019) for indexing all the questions. We use GPT-J (6B)\\\\(^2\\\\) which is the largest freely available model we could use with prompts containing 6 in-context examples.\\n\\nResults\\n\\nIn Table 2, we compare performance of language models without any prompting (Standard), with chain-of-thought prompting (CoT) and successive prompting. We observe that successive prompting performs better than CoT by 3.5% when only synthetic data is available, and 4.3% better with synthetic data and 300 annotations from DROP. The best successive prompting version on the dev set (Synthetic + DROP) has a test set performance of 30.6% F1. We also perform an ablation where the symbolic calculator is replaced by language model and observe that the performance drops by 1.5% F1. This further shows that modular approach is better over a single model that tries to solve all the tasks.\\n\\n\\\\(^1\\\\)https://ai.facebook.com/tools/faiss/\\n\\n\\\\(^2\\\\)https://github.com/EleutherAI\"}"}
{"id": "emnlp-2022-main-81", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Model Fine-tuning\\n\\nSetup\\n\\nWe employ a shared question decomposition (QD) and answering model (QA) based on T5-large version of UnifiedQA (Khashabi et al., 2020), trained in a multi-task manner. We use the format described in Appendix A for prompting UnifiedQA. For symbolic questions, we use a simple calculator that parses the operator and arguments in the generated question and executes the discrete operator on the detected arguments.\\n\\nTo deter the model from learning incorrect steps, we use contrastive estimation (Smith and Eisner, 2005). In particular, we first train the model for two epochs with cross-entropy loss while generating the output sequence (simple question or answer). Then we continue training by adding an auxiliary loss term which increases the likelihood of the intermediate sub-question that would produce a correct sub-answer at the cost of one that does not (Dua et al., 2021). We sample up to 3 negative samples at each step. We use HuggingFace transformers to train our models, with a learning rate of 5e-5 and maximum input length of 768.\\n\\nDue to variance in the types of context tables present in Wikipedia, the synthetic dataset distribution is not uniform across different reasoning types. To have a balanced representation of questions pertaining to different reasoning types, we employ dynamic sampling (Gottumukkala et al., 2020), where at the beginning of each epoch we select 80,000 instances from across all reasoning types in proportion to the drop in their current performance with respect to previous epoch on held-out synthetic data. For the first epoch we sample in proportion to original the size of each reasoning type. During inference, we use beam search with size 5 to generate decompositions, switching between QD and QA stages until QD reaches end of decomposition (\u201cEOQ\u201d) or maximum number of steps which we set as 10.\\n\\nBaseline models\\n\\nWe compare against a number of different baselines, both symbolic and non-symbolic. As non-symbolic baselines, we use UnifiedQA (Khashabi et al., 2020), which is pre-trained on a number of existing question answering datasets, and PReasM (Yoran et al., 2021), which is pre-trained on synthetically generated compositional QA pairs. We also include a baseline with symbolic components, TASE (Segal et al., 2020).\\n\\nResults\\n\\nIn Table 3, we use the DROP dev set to compare the performance of different symbolic and non-symbolic models in three settings: (1) using no training data from DROP (0-shot), (2) using only question-answer supervision from the 300 DROP examples, and (3) using both question-answer supervision and the decompositions for the 300 DROP examples. In each of these settings, we can train the model with or without the synthetic data that we generated.\\n\\nWe observe that our out-of-domain synthetic data universally improves model performance, and the improvement is most pronounced in TASE, nearing a 20% absolute improvement. Without synthetic data, PReasM is the best performing baseline, but TASE overtakes PReasM when synthetic data is available. Additionally, and unsurprisingly, increasing the amount of supervision from 0-shot to complex QA pairs to decompositions universally improves model performance.\\n\\nFinally, our method, which is a fine-tuned successive prompting model combined with a symbolic reasoning engine, achieves the best performance, giving an improvement of 5.4 F1 over the state-of-the-art model with similar supervision, i.e. TASE + Synthetic w/decomp. We follow the standard practice of using test set for only our final best performing model (SP w/decomp). We observe that our best model with a test set performance of 50.2 F1 is better than the state-of-the-art model with similar supervision (45.1 F1) by 5.1% F1.\\n\\nOverall, methods that learn to decompose complex questions into simple QA pairs adapt well to complex questions in new domain with little (SP w/decomp: 51.3 F1) to no in-domain supervision for decomposition (SP 0-shot: 49.8). If we have limited complex QA supervision (without any decompositions), un-interpretable symbolic models result in the best performance (TASE + Synthetic w/o decomp: 44.1). This is because of two reasons. First, such models can capture domain specific answer priors which may result in decent held-out performance (Dua et al., 2020; Agrawal et al., 2018).\"}"}
{"id": "emnlp-2022-main-81", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: F1 Performance of various model architectures on DROP dev-set pre-trained on synthetic data and further fine-tuned with 300 DROP examples.\\n\\n| Model          | 0-shot w/ o decomp | + Synthetic | + Synthetic |\\n|---------------|-------------------|-------------|-------------|\\n| Non-symbolic  | 24.5              | 26.7        | 27.2        |\\n| UnifiedQA     |                   |             |             |\\n| PReasM        | 24.9              | 34.6        | 37.5        |\\n| Symbolic      |                   |             |             |\\n| TASE          | -26.1             | 27.6        |             |\\n| Succ. Prompting |                 |             |             |\\n| 49.8          |                   |             |             |\\n\\nTable 4: F1 with QD and QA modules from in-context learning and fine-tuning with only DROP annotations.\\n\\n| Module        | 0-shot w/ o decomp | + Synthetic | + Synthetic |\\n|---------------|-------------------|-------------|-------------|\\n| QA            |                   |             |             |\\n| QD: In-context| 30.8              |             |             |\\n| QD: Fine-tuning|                 |             |             |\\n| QA            |                   |             |             |\\n\\nSecond, depending on the context, sometimes it may not be straightforward to decompose the complex questions into QA pairs.\\n\\n4.3 In-context vs Fine-tuning\\n\\nTo understand the gap in performance between successive prompting with in-context learning and fine-tuning, we perform ablations across in-context and fine-tuned version of QD and QA modules. We observe that in-context learning is unable to do well on answering simple questions that result in a list of answers\u2014which is especially important for DROP as symbolic aggregations are generally applied on a list of answers. On using a fine-tuned QA model we see an improvement of $\\\\sim 10\\\\%$ in F1 with an in-context QD model. Moreover, since the final answer performance is dependent on how well the QA model performs, using a better QD model (fine-tuned) does not help the overall performance much unless the QA model can handle the decompositions produced by the QD model.\\n\\n4.4 Qualitative Examples\\n\\nTo evaluate the correctness of decomposed QA pairs, we manually analyze a subset of predictions on the dev set with in-context (DROP-only) learning and model fine tuning (few shot). We do this by randomly sampling 50 correct predictions to determine how often the incorrect decompositions result in correct answer. We observe that QD stage has an accuracy of 88% for in-context and 96% for fine-tuned model. The incorrect decompositions are mainly because the decomposed question is identical to the original question. For instance, \\\"Who made the longest field goal?\\\" can sometimes be answered correctly without decomposing the question if the passage contains a single field goal mention.\\n\\nWe also sample 50 incorrect predictions to ascertain the reason for incorrect predictions in both in-context and fine-tune setup. We observe that the final predictions are incorrect due to three main categories of errors: incorrect QA model prediction, incorrect next question prediction (QD) and out-of-scope reasoning type. The QA model outputs incorrect answers to simple question 40% and 22% of the times for in-context and fine-tuned respectively. The second class of errors, due to incorrect decomposition, occur 30% of the times for both in-context and fine-tuned. The final class of errors, due to compositional questions that are not covered by synthetically generated annotations, occur 28% (in-context) and 46% (fine-tune) of the times.\\n\\nIn Figure 4, we show a few examples of correct and incorrect predictions and point out the strengths and weaknesses of successive prompting. The main strength of successive prompting is that, by breaking down the question, we are able to get improved supervision for QA. As a result, it is able to correctly identify the goals kicked in the first half while answering the question \\\"How many field goals did both teams kick in the first half?\\\", unlike CoT that returns goals for the entire game.\\n\\nOne of the limitations of in-context learning, when compared with fine-tuning (irrespective of the type of prompting), is that examples are chosen based on the question alone, overlooking the context. For instance, DROP has questions like \\\"How many people were not Germans, in terms of percentage?\\\" where we first need to answer \\\"How many people were Germans, in terms of percentage?\\\" and then perform a negation operation (i.e., subtract from 100). The word \\\"not\\\" influences the example lookup to choose decomposition that involves a negation even when the question being answered requires a different operation.\\n\\nA limitation of successive prompting is that it is sometimes challenging to decompose a question, especially when it involves implicit reasoning from the passage. For instance, for \\\"Which port did the Korean immigrants leave first Chemulpo or Veracruz?\\\", it is difficult to explicitly define a comparison style decomposition from the sentence,\"}"}
{"id": "emnlp-2022-main-81", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who threw the longest touchdown pass?\\nThe touchdown passes are 22-yard and eight-yard. Out of 22 and eight-yard, the longest touchdown pass is 22. Peyton Manning threw the 22-yard touchdown pass. The answer is Peyton Manning.\\n\\nIn which two quarters did Stephen Gostkowski only kick one field goal?\\nThe first quarter is the only quarter when Stephen Gostkowski kicked only one field goal. The count becomes 1. The second quarter is the only quarter when Stephen Gostkowski only kicked two field goals. The answer is first quarter.\\n\\nHow many field goals did both teams kick in the first half?\\nThe field goals in the game are 20, 24, 53, 30 and 40. The count becomes 7. The answer is 7.\\n\\nHow many of the personnel were not civil servants?\\nThe personnel are 1,513. Since 1,513 is less than 3,183. The answer is not civil servants.\\n\\nWhich port did the Korean immigrants leave first, Chemulpo or Veracruz?\\nIn 1905, Korean immigrants departed from the port of Chemulpo. After which they took a train to Veracruz. In the Veracruz port, another boat was taken to the port of Progreso. The Korean immigrants left Chemulpo. The answer is Chemulpo.\\n\\nFigure 4: Generated decompositions depicting strength and weaknesses of Successive Prompting.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After which they took a train to Veracruz.\\n\\n5 Related Work\\n\\nPrompting methods\\n\\nPrompting was introduced as a way to test the reasoning capabilities of large language models (Brown et al., 2020). In follow-up works (Schick, 2022; Chowdhery et al., 2022; Marasovi\u0107 et al., 2021) prompting techniques have been used as a mechanism to supervise the model decision with few demonstrations as a conditioning context to guide its predictions on an unseen example. Works like Chain-of-Thought reasoning (Wei et al., 2022; Zelikman et al., 2022) especially focus on compositional questions where they provide a chain of reasoning as demonstrations. In concurrent work, Least-to-Most prompting (Zhou et al., 2022) takes a similar view as ours to break down the problem into sub-problems. However, in Successive Prompting the question decomposition and answering stages are interleaved, unlike Least-to-Most where the problem is first reduced into sub-problem and then executed in a sequence. In our method, the next question prediction has access to previously answered sub-questions, which is useful in questions that need long chain referencing. Other contemporaneous works (Press et al., 2022; Khot et al., 2022) use very large language models (more than twice the size we used) and show better few-shot generalization. Works like Perez et al. (2021) have shown the importance of having the right in-context examples for downstream performance leading to works that learn to retrieve relevant in-context examples (Rubin et al., 2021).\\n\\nNon-symbolic methods\\n\\nMost non-symbolic methods are sequence-to-sequence models trained on a large amount of question answering data (Khashabi et al., 2020; Yoran et al., 2021).\\n\\nSymbolic methods\\n\\nNeural module networks like approaches parse complex questions into a pre-specified grammar and learn neural components to handle symbolic mathematical operations (Gupta et al., 2020; Chen et al., 2020; Nye et al., 2021) which are recursively executed. State-of-the-art models on DROP, however, use a combination of BERT-based contextual models along with a calculator that performs discrete operations (Andor et al., 2019; Segal et al., 2020; Hu et al., 2019).\\n\\nWorks like Text Modular networks (Khot et al., 2021) and MRKL (Karpas et al., 2022) are closest to our work. However, they are limited in the terms of types of simple questions they can answer (single-span only) and the complexity of reasoning they can do (single-order only). TMNs, additionally, use a classifier that scores the generated chains module and filters out incorrect question decompositions, while we use contrastive estimation to learn a better question decomposer and as a result do not need a chain scorer.\\n\\n6 Conclusion\\n\\nWe present a way to successively decompose complex questions into simple QA pairs, which allows for modular QD and QA systems that can be trained and queried independently. When performing in-context learning, we showed that successive prompting yields an improvement of 4.6 F1 over chain-of-thought prompting. When replacing just the in-context QA module with a fine-tuned one, which is adept at handling list type questions, we further improve the overall performance by 9.5 F1. We believe that modular systems that decompose and delegate tasks to the most appropriate model, whether that is a large LM or a tailored component, are more effective at solving complex tasks than trying to have a large LM solve the entire task on its own. Successive prompting shows one way this decomposition and delegation can be done.\\n\\nAcknowledgements\\n\\nWe would like to thank Anthony Chen, Catarina Belem and the anonymous reviewers for the discussions and feedback. This material is based upon work sponsored in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office Of Naval Research, in part by funding by AI2 and NSF IIS-1817183. We would also like to thank Hasso Plattner Institute (HPI) for supporting the first author through UCI-HPI fellowship. The views in this work are of authors and not the sponsors.\\n\\nLimitations\\n\\nWe propose a way to decompose complex questions into interpretable simple QA pairs as latent steps that get successively asked and answered by large pretrained models. The notion of performing complex tasks by iteratively finding and then filling information needs is very general, but we have only shown the applicability of one specific version of this idea in one specific setting. There are many...\"}"}
{"id": "emnlp-2022-main-81", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"potential challenges in applying successive prompting more broadly. The biggest is that it requires at least some decomposition data, which may be hard or even impossible to obtain. Some complex questions are not easily decomposed, and some domains can be very challenging to write synthetic data generators for. We were able to generate synthetic data that covered most of the reasoning types in DROP, but other kinds of complex questions would not be covered by our generator (e.g., questions that require commonsense or causal reasoning).\\n\\nThere is also significant difficulty in choosing a level of granularity for decomposition. If a large pretrained model can directly answer a question as complex as \u201cWhat was Barth's second field goal?\u201d, we should let the model answer the question instead of trying to decompose it further. The right granularity for the decomposition thus depends on the capabilities of the underlying model, and those capabilities are rapidly changing as newer and larger pretrained models are released. There is the possibility that newer model iterations will not need any decomposition to answer the complex questions covered by our synthetic data generator, making that generator obsolete. However, it seems unlikely that pretrained models will be able to handle all complex scenarios in the near future, so the ideas of successive prompting and generating synthetic data to bridge reasoning gaps should still be applicable even when our particular application of them becomes obsolete.\\n\\nThis method also increases the computational requirements for answering complex questions, as instead of making one query to a large LM, successive prompting makes many queries to answer a single question.\\n\\nEthics Statement\\nThis work focuses on improving complex question answering with limited data. It uses existing training data and conventional methods of testing model performance. This work does not deal with any social impacts or biases in natural language processing systems.\\n\\nReferences\\nAishwarya Agrawal, Dhruv Batra, Devi Parikh, and Aniruddha Kembhavi. 2018. Don\u2019t just assume; look and answer: Overcoming priors for visual question answering. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 4971\u20134980. IEEE Computer Society.\\n\\nDaniel Andor, Luheng He, Kenton Lee, and Emily Pitler. 2019. Giving BERT a calculator: Finding operations and arguments with reading comprehension. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5947\u20135952, Hong Kong, China. Association for Computational Linguistics.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nXinyun Chen, Chen Liang, Adams Wei Yu, Denny Zhou, Dawn Song, and Quoc V. Le. 2020. Neural symbolic reader: Scalable integration of distributed and symbolic representations for reading comprehension. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nYanda Chen, Ruiqi Zhong, Sheng Zha, George Karypis, and He He. 2022. Meta-learning via language model in-context tuning. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 719\u2013730, Dublin, Ireland. Association for Computational Linguistics.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.\\n\\nDheeru Dua, Pradeep Dasigi, Sameer Singh, and Matt Gardner. 2021. Learning with instance bundles for reading comprehension. ArXiv preprint, abs/2104.08735.\\n\\nDheeru Dua, Sameer Singh, and Matt Gardner. 2020. Benefits of intermediate annotations in reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5627\u20135634, Online. Association for Computational Linguistics.\\n\\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. 2019.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 2368\u20132378, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nAnanth Gottumukkala, Dheeru Dua, Sameer Singh, and Matt Gardner. 2020. Dynamic sampling strategies for multi-task reading comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 920\u2013924, Online. Association for Computational Linguistics.\\n\\nNitish Gupta, Kevin Lin, Dan Roth, Sameer Singh, and Matt Gardner. 2020. Neural module networks for reasoning over text. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, and Dongsheng Li. 2019. A multi-type multi-span network for reading comprehension that requires discrete reasoning. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1596\u20131606, Hong Kong, China. Association for Computational Linguistics.\\n\\nYue Jin, Tianqing Zheng, Chao Gao, and Guoqiang Xu. 2021. Mtmsn: Multi-task and multi-modal sequence network for facial action unit and expression recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3597\u20133602.\\n\\nEhud Karpas, Omri Abend, Yonatan Belinkov, Barak Lenz, Opher Lieber, Nir Ratner, Yoav Shoham, Hofit Bata, Yoav Levine, Kevin Leyton-Brown, et al. 2022. Mrkl systems: A modular, neuro-symbolic architecture that combines large language models, external knowledge sources and discrete reasoning. ArXiv preprint, abs/2205.00445.\\n\\nDaniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, Peter Clark, and Hannaneh Hajishirzi. 2020. UNIFIEDQA: Crossing format boundaries with a single QA system. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1896\u20131907, Online. Association for Computational Linguistics.\\n\\nTushar Khot, Daniel Khashabi, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2021. Text modular networks: Learning to decompose tasks in the language of existing models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1264\u20131279, Online. Association for Computational Linguistics.\\n\\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao Fu, Kyle Richardson, Peter Clark, and Ashish Sabharwal. 2022. Decomposed prompting: A modular approach for solving complex tasks. ArXiv preprint, abs/2210.02406.\\n\\nXiaofei Ma, Cicero Nogueira dos Santos, and Andrew O. Arnold. 2021. Contrastive fine-tuning improves robustness for neural rankers. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 570\u2013582, Online. Association for Computational Linguistics.\\n\\nAna Marasovi\u0107, Iz Beltagy, Doug Downey, and Matthew E Peters. 2021. Few-shot self-rationalization with natural language prompts. ArXiv preprint, abs/2111.08284.\\n\\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari, Henryk Michalewski, Jacob Austin, David Bieber, David Dohan, Aitor Lewkowycz, Maarten Bosma, David Luan, et al. 2021. Show your work: Scratchpads for intermediate computation with language models. ArXiv preprint, abs/2112.00114.\\n\\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021. True few-shot learning with language models. Advances in Neural Information Processing Systems, 34.\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A Smith, and Mike Lewis. 2022. Measuring and narrowing the compositionality gap in language models. ArXiv preprint, abs/2210.03350.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367.\\n\\nDheeraj Rajagopal, Siamak Shakeri, Cicero Nogueira dos Santos, Eduard Hovy, and Chung-Ching Chang. 2022. Counterfactual data augmentation improves factuality of abstractive summarization. ArXiv preprint, abs/2205.12416.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3982\u20133992, Hong Kong, China. Association for Computational Linguistics.\\n\\nOhad Rubin, Jonathan Herzig, and Jonathan Berant. 2021. Learning to retrieve prompts for in-context learning. ArXiv preprint, abs/2112.08633.\\n\\nTimo Schick. 2022. Few-shot learning with language models: Learning from instructions and contexts. Ph.D. thesis, lmu.\\n\\nElad Segal, Avia Efrat, Mor Shoham, Amir Globerson, and Jonathan Berant. 2020. A simple and effective model for answering multi-span questions. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), Hong Kong, China.\"}"}
{"id": "emnlp-2022-main-81", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Control codes for Model Fine-tuning\\n\\nTo generate a simple question given complex question and previously generated latent steps, we append the input with control code \\\"QS:\\\"\\n\\n{:x_n} QI: q_n 1 A: a_n 1 \u00b7\u00b7\u00b7 QI: q_n k-1 A: a_n k-1 QS:\\n\\nTo answer the simple question we prompt the model again with only the simple question this time and control code \\\"A:\\\" to generate answer,\\n\\n{p_n} QS: q_n k A:\\n\\nWe alternate between the two stages till we reach the end of decomposition marker \\\"EOQ\\\"\\n\\n{:x_n} QI: q_n 1 A: a_n 1 \u00b7\u00b7\u00b7 QI: q_n s A: a_n s QS: EOQ\\n\\nA.2 Synthetic Dataset Statistics\\n\\n| Reasoning          | Number of Examples |\\n|--------------------|--------------------|\\n| Filter             | 9634               |\\n| Count              | 11019              |\\n| Comparison         | 12239              |\\n| Difference         | 15433              |\\n| Negation           | 2020               |\\n| Intersection       | 9089               |\\n| Sum                | 18754              |\\n| Sort               | 10663              |\\n| Sort-Filter        | 5827               |\\n| Difference-Sort    | 13872              |\\n| Sum-Sort           | 3212               |\\n| Count-Filter       | 7382               |\\n| Gather-Count       | 1506               |\\n| Sum-Count          | 4138               |\\n| Difference-Count   | 4216               |\\n| Sort-Count         | 6328               |\\n| Comparison-Count   | 5998               |\\n| Total              | 141,330            |\\n\\nTable 5: Synthetic dataset statistics\"}"}
{"id": "emnlp-2022-main-81", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Round Date Opponent Venue Attendance\\n\\nR2 1st Leg 26 September 1990 Walsall A 5,666\\nR2 2nd Leg 10 October 1990 Portsmouth H 10,037\\nQFR 23 October 1990 Liverpool H 18,246\\nSF 1st Leg 24 February 1991 Sheffield Wednesday H 14,074\\nSF 2nd Leg 27 February 1991 Oxford United A 34,669\\nQFR 23 January 1991 Walsall A 33,861\\n\\nFilter What are the opponents when date was later than 21 January 1991 and attendance was less than 20000?\\n\\n\u2022 Q: What are the opponents when date was later than 21 January 1991? A: Sheffield Wednesday; Oxford United; Walsall\\n\\n\u2022 Q: Out of Sheffield Wednesday, Oxford United and Walsall, which opponents have attendance less than 20000? A: Sheffield Wednesday\\n\\nCount How many opponents were there?\\n\\n\u2022 Q: What are all the opponents? A: Walsall; Portsmouth; Liverpool; Sheffield Wednesday; Oxford United\\n\\n\u2022 Q: count(Walsall; Portsmouth; Liverpool; Sheffield Wednesday; Oxford United) A: 5\\n\\nComparison What round had a higher attendance: SF 2nd Leg or QFR?\\n\\n\u2022 Q: What was the attendance when round was SF 2nd Leg? A: 34,669\\n\u2022 Q: What was the attendance when round was QFR? A: 33,861\\n\\n\u2022 if_then(34,669 > 33,861; SF 2nd Leg; QFR) A: SF 2nd Leg\\n\\nDifference What is the difference between attendances when the opponent was Oxford United and Portsmouth?\\n\\n\u2022 Q: What was the attendance when opponent was Oxford United? A: 34,669\\n\u2022 Q: What was the attendance when opponent was Portsmouth? A: 10,037\\n\\n\u2022 diff(34,669; 10,037) A: 24,632\\n\\n\u2022 Q: What is the difference between 34,669 and 24,632?\\n\\nSum What were the total attendances when opponents were Walsall and Oxford United?\\n\\n\u2022 Q: What was the attendance when opponent was Walsall? A: 5,666; 33,861\\n\u2022 Q: What was the attendance when opponent was Oxford United? A: 34,669\\n\\n\u2022 sum(5,666; 33,861; 34,669) A: 74,196\\n\\nSort Which opponent had the second highest attendance?\\n\\n\u2022 Q: What are all the attendances? A: 5,666; 10,037; 14,074; 34,669; 33,861\\n\\n\u2022 Q: top(2, 5,666;10,037;14,074;34,669;33,861 A: 33,861\\n\\n\u2022 What was the opponent when attendance was 33,861? A: Walsall\"}"}
{"id": "emnlp-2022-main-81", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which opponent had the third lowest attendance after 1 January 1991?\\n\\n- Q: What are all the attendances after 1 January 1991? A: 14,074; 34,669; 33,861\\n- Q: bottom(3, 14,074; 34,669; 33,861) A: 14,074\\n\\nWhat was the opponent when attendance was 14,074 A: Sheffiled Wednesday\\n\\nWhat is the difference between the second highest attendance and lowest attendance?\\n\\n- Q: What are all the attendances? A: 5,666; 14,074; 18,246; 14,074; 34,669; 33,861\\n- Q: top(2, 5,666; 14,074; 18,246; 14,074; 34,669; 33,861) A: 33,861\\n- Q: bottom(1, 5,666; 14,074; 18,246; 14,074; 34,669; 33,861) A: 5,666\\n- Q: What is the smallest value in: 33,861 and 5,666? A: 28,195\\n\\nWhat was the total of highest attendance and third lowest attendance?\\n\\n- Q: What are all the attendances? A: 5,666; 14,074; 18,246; 14,074; 34,669; 33,861\\n- Q: top(1, 5,666; 14,074; 18,246; 14,074; 34,669; 33,861) A: 34,669\\n- Q: bottom(3, 5,666; 14,074; 18,246; 14,074; 34,669; 33,861) A: 18,246\\n- Q: sum(33,861; 5,666) A: 52,915\\n\\nHow many rounds had venue as A and attendance greater than 30000?\\n\\n- Q: What rounds had venue as A? A: R2 1st Left; SF 2nd Leg; QFR\\n- Q: Out of rounds R2 1st Left, SF 2nd Leg and QFR, which had attendance greater than 30000? A: SF 2nd Leg; QFR\\n- Q: count(SF 2nd Leg; QFR) A: 2\\n\\nHow many opponents were there for each of venue: A and H?\\n\\n- Q: What are the opponents when venue was A? A: Walsall; Oxford United\\n- Q: count(Walsall; Oxford United) A: 2\\n- Q: What are the opponents when venue was H? A: Portsmouth; Liverpool; Sheffield Wednesday United\\n- Q: count(Portsmouth; Liverpool; Sheffield Wednesday United) A: 3\\n- Q: gather(2; 3) A: 2 and 3\"}"}
{"id": "emnlp-2022-main-81", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reasoning Complex Question Decomposition (Question, Answer)\\n\\nAdditional Higher-order combinations with COUNT\\n\\nSum-Count\\n\\nWhat are the total number of opponents when venue were A and H?\\n\\n\u2022 Q: What are the opponents when venue was A?\\n  A: Walsall; Oxford United\\n\\n\u2022 Q: count(Walsall; Oxford United) A: 2\\n\\n\u2022 Q: What are the opponents when venue was H?\\n  A: Portsmouth; Liverpool; Sheffield Wednesday\\n\\n\u2022 Q: count(Portsmouth; Liverpool; Sheffield Wednesday) A: 3\\n\\n\u2022 Q: sum(2;3) A: 5\\n\\nDifference-Count\\n\\nWhat is the difference between number of rounds when venue was A and H?\\n\\n\u2022 Q: What are the venues when round was SF 1st Leg? Ans: H\\n\\n\u2022 Q: count(A) A: 1\\n\\n\u2022 What are the venues when round was QFR? A: H; A\\n\\n\u2022 Q: count(H;A) A: 2\\n\\n\u2022 diff(3; 2) Ans: 1\\n\\n(d) Higher order decompositions\\n\\nFigure 5: Examples of decompositions with wikitables\"}"}
