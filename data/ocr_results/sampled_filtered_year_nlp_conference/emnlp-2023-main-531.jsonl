{"id": "emnlp-2023-main-531", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This is not a Dataset: A Large Negation Benchmark to Challenge Large Language Models\\n\\nIker Garc\u00eda-Ferrero1, Bego\u00f1a Altuna1, Javier \u00c1lvez2, Itziar Gonzalez-D\u00edos1, German Rigau1\\n\\n1HiTZ Center - Ixa, University of the Basque Country UPV/EHU\\n2LoRea Group, University of the Basque Country UPV/EHU\\n\\n{iker.garciaf, begona.altuna, javier.alvez}@ehu.eus\\n{itziar.gonzalezd, german.rigau}@ehu.eus\\n\\nAbstract\\n\\nAlthough large language models (LLMs) have apparently acquired a certain level of grammatical knowledge and the ability to make generalizations, they fail to interpret negation, a crucial step in Natural Language Processing. We try to clarify the reasons for the sub-optimal performance of LLMs understanding negation. We introduce a large semi-automatically generated dataset of circa 400,000 descriptive sentences about commonsense knowledge that can be true or false in which negation is present in about 2/3 of the corpus in different forms. We have used our dataset with the largest available open LLMs in a zero-shot approach to grasp their generalization and inference capability and we have also fine-tuned some of the models to assess whether the understanding of negation can be trained. Our findings show that, while LLMs are proficient at classifying affirmative sentences, they struggle with negative sentences and lack a deep understanding of negation, often relying on superficial cues. Although fine-tuning the models on negative sentences improves their performance, the lack of generalization in handling negation is persistent, highlighting the ongoing challenges of LLMs regarding negation understanding and generalization. The dataset and code are publicly available: https://github.com/hitz-zentroa/This-is-not-a-Dataset\\n\\n1 Introduction\\n\\nLarge Language Models (LLMs) currently offer state of the art performance in many Natural Language Processing (NLP) tasks. Apparently, they have acquired the ability to capture syntactic (Batra, 2020) and semantic (Furrer et al., 2021) abstractions. However, recent experiments (Kassner and Sch\u00fctze, 2020; Hossain et al., 2020; Truong et al., 2022) have proven that LLMs fail at interpreting contexts in which understanding negation is required.\\n\\nBills are commonly part of birds. \u2705\\n\\nBills are never part of human bodies. \u2705\\n\\nBills are never part of birds. \u274c\\n\\nBills are commonly part of human bodies. \u274c\\n\\nFigure 1: Affirmative and negative sentences in the dataset.\\n\\nThe presence of negation in a sentence reverts the polarity of the proposition it represents, and thus affects its truth and factuality values. See how the adverb \u201cnever\u201d changes the truth value of the sentences in Figure 1. As a consequence, understanding negation correctly is crucial for all NLP tasks. Moreover, understanding negation should help LLMs to grasp how things happen in reality, boosting NLP tasks that involve commonsense, causality, entailment and world knowledge.\\n\\nThe reasons for the lower capabilities of LLMs dealing with negation remain largely unclear, although some point out at the under-representation of negation in corpora (Hossain et al., 2022). In this work, we present a corpus in which negation is present in around two thirds of the sentences in different forms. Taking advantage of the relations in WordNet (Fellbaum, 1998), we have generated a set of patterns to create descriptive sentences that work as truth and falsity tests which are then used together with a list of prompts to measure the sentence understanding of the different LLMs.\\n\\nThe dataset has been used in a series of experiments to test its quality and coherence. First, we assess the quality of the sentences by human annotation...\"}"}
{"id": "emnlp-2023-main-531", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tators. Then, to grasp its capacity of generalization and inference, we have used our dataset to test different configurations of LLMs available in a zero-shot approach. We have also fine-tuned some of these models to assess whether the understanding of negation can be learnt. Our initial hypothesis is that if the dataset is coherently and robustly built we will be able to learn how LLMs deal with negation.\\n\\nThe contributions of this paper are: i) We introduce the largest negation probing dataset. This dataset includes affirmative and negative sentences with and without distractors, incorporating multiple types of relations and negations. ii) We evaluate a comprehensive set of open LLMs using our dataset in both zero-shot and fine-tuning scenarios. iii) Our findings demonstrate that current LLMs, whether in zero-shot settings or after fine-tuning with examples from our dataset, possess a profound understanding of the truthfulness of affirmative sentences. However, when confronted with negation, these models heavily rely on superficial cues instead of effectively generalizing negation.\\n\\n2 Background\\n2.1 Related Works\\nNegation is a core operator in logic and in the structuring of the information in text and it has long been studied for its relevance in natural language understanding. In the last two decades, works on the analysis and processing of negation have multiplied. In the pre-generative-model era, most works centered on negation detection (Chapman et al., 2001; Vilares et al., 2015) and profiling (Morante and Daelemans, 2012), so the extracted negation information could be used in downstream tasks.\\n\\nWith the booming of deep-learning architectures that were based on abstract neural representations of texts, the paradigm shifted and negation was processed as the rest of the elements appearing in text. It was soon noticed that systems struggled to correctly process the information when negation was involved. Such is the case of negation in machine translation (Bentivogli et al., 2016; Tang et al., 2021), information extraction (Grivas et al., 2020) and sentiment analysis (Barnes et al., 2021) among others.\\n\\nIt has not been long since the scholar community started to analyse the reasons for the lack of capability of correctly processing negation. For example, Jumelet and Hupkes (2018) analysed the negation licensing strategies to measure neural language model ability to correctly process them. Chen et al. (2023) assess the ability of LLMs to handle negative commonsense knowledge. Since most available information exists in a positive and affirmative form, LLMs fail at dealing with world knowledge when it is presented in a negative form. They propose a two-task assessment in which LLMs need to i) answer yes or no to world knowledge questions and ii) generate commonsense compelling sentences from related keywords. Some recent research has been directed to building knowledge bases in which negative commonsense is stored (Arnaout et al., 2022), in order to be reused for commonsense reasoning.\\n\\n2.2 Negation in English\\nNegation in language is the representation of the logical operation in which a the truth value of a proposition is inverted. It is commonly expressed by a restricted list of negative adverbs (e.g. no, never), pronouns, determiners or prefixes, that appear in different contexts in the sentence. Pullum et al. (2002) offer a four axe classification of negation types from which we will focus on these:\\n\\n- **Verbal vs. non-verbal**: the verbal negation marker is associated with the verb and directly affects it, while the non-verbal couples with objects and adjuncts.\\n- **Analytic vs. synthetic**: analytic negation is represented by markers that only convey negation. Synthetic negation markers, instead, may have additional syntactic function (e.g. nothing and none might also be subjects or objects).\\n- **Clausal vs. sub-clausal**: clausal negation negates the whole clause that includes it, and sub-clausal negation only affects a part of the clause.\\n\\nIn Table 1 we present the different types of negations considered in our work.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Negation\\n\\nType Example\\n\\nVerbal Agreement is not an appropriate synonym of disagreement in any context.\\n\\nNon-verbal In context lectures may be part of courses.\\n\\nAnalytic No theft is a small replica of a person.\\n\\nSynthetic Mirror is never an appropriate hyponym of reduction.\\n\\nClausal Kissing is not commonly done by engineers.\\n\\nSub-clausal Bricks are made of clay in no context.\\n\\nTable 1: Examples of the types of negation.\\n\\n| Pattern      | Relation | Sentence Count |\\n|--------------|----------|----------------|\\n| #01 Synonymy | 21       | 2,996          |\\n| #02 Antonymy | 21       | 58             |\\n| #03 Synonymy | 24       | 14             |\\n| #04 Antonymy | 24       | 58             |\\n| #05 Hypernymy| 24       | 634            |\\n| #06 Part    | 16       | 199            |\\n| #07 Substance| 15       | 21             |\\n| #08 Member  | 17       | 11             |\\n| #09 Agent   | 2        | 60             |\\n| #10 Instrument| 7       | 9              |\\n| #11 Result  | 27       | 40             |\\n\\nTotal 381,300\\n\\nTable 2: Distribution of sentences by pattern.\\n\\nThe sentences in our dataset are obtained by means of patterns. Each of the 11 patterns (#01\u2013#11) is designed for a particular relation and includes several different templates of two types: affirmative templates, which are free of negation; negative templates, which include one of the types of negations described in Table 1. Using triples on the corresponding relation, these templates are used to create sentences by instantiation. Since each synset may include more than one word form in Core WordNet and the proposed templates include optional and alternative parts, we obtain several sentences from each couple of template and triple. The controlled application of the different templates enables us to determine the truth-value of the resulting sentences. In Appendix D, we describe each pattern in detail.\\n\\nMore specifically, we focus on the WordNet relations synonymy, hypernymy, antonymy, meronymy (part, member and substance) and the semantic roles agent, instrument and result provided by Morphosemantic Links (Fellbaum et al., 2009). Among the nouns and verbs compiled in WordNet, we concentrate exclusively on the ones provided by Core WordNet (Boyd-Graber et al., 2006), which is a list of the most frequently used word senses that includes 3,299 nouns and 1,000 verbs. In this way, we discard words that are less commonly used. Furthermore, we exclude the triples on synonymy and hyponymy that relate Basic Level Concepts (BLCs) (Izquierdo et al., 2007), which may result too general, and we use the mapping from WordNet to EuroWordNet Top Ontology (TCO) to ignore the triples on the member meronymy relation and the agent semantic role where the noun synsets are not referring to animals or persons.\\n\\nSince WordNet and the considered related resources only provide true knowledge\u2014that is, all the triples and mappings describe real relations and connections\u2014we automatically obtain false knowledge from WordNet triples using distractors, which are randomly selected words that replace the word senses of a synset. That is, given a WordNet triple that relates two synsets, from Core WordNet we select a distractor to replace the word senses of one of the synsets and obtain a distracting triple. Apart from BLCs, for the selection of suitable distractors we consider the lexicographer files provided by WordNet, which are 45 syntactic category and logical groupings of word senses, and WordNet Domains (Bentivogli et al., 2004), which consist of a hierarchy of 164 labels that characterize knowledge areas and to which each synset is connected. In Appendix C, we provide more details about the selection of distractors.\\n\\nNext, we illustrate the process of constructing our dataset. In Pattern #06, we have included the following positive and negative templates that state semantic correspondences between parts and wholes on the basis of triples of the form \u27e8part, noun\u2081, noun\u2082\u27e9:\\n\\n- \u27e8noun\u2081+(e)s\u27e9 [are commonly | may be] part of \u27e8noun\u2082+(e)s\u27e9.\\n- \u27e8noun\u2081+(e)s\u27e9 are never part of \u27e8noun\u2082+(e)s\u27e9.\\n\\nThe positive template yields true sentences when instantiated with true knowledge (i.e. WordNet triples), while we get false sentences using distracting triples. On the contrary, the negative one yields sentences with the opposite truth-value. For example, given the WordNet triple 1 Synonymy triples are obtained by reflexivity. 2 In Patterns #01 and #02, distractors are synsets when using glosses. 3 The expressions enclosed in square brackets are alternative.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Figure 1, we present two sets of four sentences. Each set is composed of two true and two false sentences. The first set exemplifies the prediction process using the positive template, while the second set illustrates the application of the negative template. The figures in Table 2 provide a summary of the dataset's characteristics, with columns for patterns, WordNet relations, and the number of applied templates, triples, and sentences. Pattern #01\u2013#04 incorporate both false positive sentences and true negative sentences derived from synonymy and antonymy WordNet triples via dual template application. For antonymy, the truth-value of the resulting sentences is determined by the template's instantiation, regardless of whether the triples are from WordNet or distractors. When using the same two templates for the negative instantiation, sentences in the second row of Figure 1 are labeled respectively as False and True. Table 3 evaluates the dataset's quality through subjective human assessment. Sentences were evaluated for truth, grammar, comprehensibility, and plausibility by two English native speakers on a random sample of 220 sentences. The human testers' answers were compared with the predictions generated from WordNet relations, with 90% of the predictions matching. The dataset's sentences are generally comprehensible to humans, though some issues with uncountable nouns (1) and lexical selection (2) were identified. However, low plausibility might be advantageous for experiments, as using infrequent sentences can mitigate the reliance on lexical co-occurrence models.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Experimental Setup\\n\\nIn this section, we define the evaluation protocol we use to measure the performance of Language Learning Models (LLMs) on our dataset.\\n\\n4.1 Models\\n\\nWe evaluate a diverse set of LLMs, ranging in size from 7 billion parameters up to 65 billion parameters. Our evaluation includes Foundation Models, along with versions that have undergone additional instruction-tuning and/or have been fine-tuned for conversation. We do not consider closed models where the data used for pretraining or even the model architecture is unknown, as drawing meaningful conclusions for such systems is not possible.\\n\\nWe evaluate the following models: the 12 billion parameter T5 (Raffel et al., 2020) encoder-decoder language model, as well as FLAN-T5 (Chung et al., 2022), an enhanced version of T5 that has been fine-tuned in a mixture of tasks; LLaMA (Touvron et al., 2023) decoder-only language models with parameter sizes ranging from 7 billion to 65 billion; LLaMA models that have been fine-tuned for specific tasks, including Vicuna v1.1 (Chiang et al., 2023), which has undergone additional fine-tuning as a chat-assistant, and WizardLM (Xu et al., 2023), which has been fine-tuned for following instructions; Pythia (Biderman et al., 2023) decoder-only 12 billion parameter model; the instruction-tuning model Dolly (Conover et al., 2023); and finally we evaluate Falcon (Almazrouei et al., 2023) 7 and 40 billion parameter models which are decoder-only models including the instruction-following fine-tuned versions. We also evaluate other open LLMs; the full model list can be found in Appendix A.\\n\\n4.2 Task Formulation\\n\\nWe evaluate each sentence in the dataset individually as a binary task in which the model must generate either True or False tokens. Following Scheurer et al. (2023), given the prompt \\\\( p_t \\\\) we compute the answer \\\\( A \\\\) as follows:\\n\\n\\\\[\\nA = \\\\begin{cases} \\n\\\\text{True} & \\\\text{if } p(\\\\text{True} | p_t) > p(\\\\text{False} | p_t) \\\\\\\\\\n\\\\text{False} & \\\\text{otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nWe use the following prompt as input for the models:\\n\\nIs the following statement True or False?\\n\\n{sentence}.\\n\\nWe found that models that have undergone a fine-tuning for conversation tend to generate an explanation instead of answering True or False. We use a slightly modified prompt that improves the results:\\n\\nIs the following statement True or False?\\n\\nAnswer only True or False. {sentence}.\\n\\nModels that have been fine-tuned as dialogue systems utilize different prompts to represent a conversation, such as using markers like \u201c<bot>\u201d and \u201c<human>\u201d, or custom system initial prompts. In order to accommodate these models, we format the input according to the recommendations provided by the authors. Implementation details of fine-tuning and inference are available in Appendix B.\\n\\n4.3 Metrics\\n\\nIn our dataset, we utilize two primary metrics for evaluating LLMs:\\n\\n- **Accuracy**: This metric is computed using the formula\\n\\n  \\\\[\\n  \\\\text{acc} = \\\\frac{TP + TN}{TP + TN + FP + FN},\\n  \\\\]\\n\\n  We evaluate the overall accuracy at the sentence level for all the sentences in our dataset. Additionally, we analyze the overall accuracy of different sentence types: Accuracy in Affirmative sentences, Negative sentences, Affirmative sentences with a distractor and Negative sentences that include a distractor.\\n\\n- **Coherence**: This metric aims to decouple the real-world and commonsense knowledge of the model from the understanding of negative sentences. We compute two coherence scores: one for the sentences without distractors (\u201cBills are commonly part of birds\u201d and \u201cBills are never part of birds\u201d) and another for the sentences with distractors (\u201cBills are commonly part of human bodies.\u201d and \u201cBills are never part of human bodies.\u201d). Answers are deemed coherent if the affirmative and negative sentences have opposite labels, regardless of whether the answer is correct or incorrect. However, if the model predicts the same label for both the affirmative and negative sentences, we consider the answer incoherent. To illustrate this metric, consider the sentence pair \u201cBills are commonly part of birds\u201d and \u201cBills are never part of birds\u201d. Both the answers \u201cTrue/False\u201d and \u201cFalse/True\u201d are considered coherent, whereas \u201cTrue/True\u201d and \u201cFalse/False\u201d are incoherent.\\n\\nMoreover, we calculate the overall coherence:\"}"}
{"id": "emnlp-2023-main-531", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Zero-shot performance of various LLMs in our dataset. The best results are highlighted in bold, and scores that surpass the Random baseline accuracy are underlined.\\n\\nThis happens when all the statements with and without distractors are coherent and correctly or all incorrectly classified. Referring to the example in Figure 1, we would deem the set of statements as overall coherent if the sentences with and without distractors are coherent and all the answers are either correct or all of them are incorrect. In the case of antonymy relations (Patterns #02 and #04), both the distractor-carrying and non distractor-carrying sentences carry the same label, so we evaluate the overall coherence accordingly. It is important to note that, for the sake of simplicity, the example only contains two sentences, but coherence is actually determined at the triple level. Triples can comprise between 2 to 27 templates. So, for a triple to be deemed coherent, responses to all the templates within it must be coherent. Therefore, this is a very challenging metric.\\n\\nBy examining coherence in these contexts, we gain insights into the models' ability to understand the negation, even if the models do not have the real-world knowledge to correctly label the sentences.\\n\\n5 Do LLMs understand negation?\\n\\nIn this section, we assess the performance of the LLMs in section 4.1 in our dataset. The evaluation is conducted in a zero-shot setting, meaning that we evaluate the models without any fine-tuning. The results of this evaluation are presented in Table 4. Foundation models, which are trained on large amounts of unlabeled data, demonstrate an All True behavior. They accurately label as True the majority of affirmative sentences and negative sentences with a distractor, which are True with the exception of the Antonymy patterns, that form approximately 5% of the total sentences. However, these models struggle to classify negative sentences and affirmative sentences with opposite labels. Their performance in these falls significantly below the random baseline exhibiting a total lack of coherence by the models.\\n\\nModels that have undergone dialogue or instruction tuning, particularly Vicuna and Flan-T5, demonstrate higher accuracy, instead. These models achieve a very high accuracy in sentences without a distractor. Specifically, Flan-T5 shows coherent answers for 46% of the triples. It is to be noted that this is a challenging metric, as a triple may be used to build up to 27 templates, and all of them must be coherent for the triple to be considered coherent.\\n\\nTable 5: Accuracy of Flan-T5-xxl and Vicuna13B in the Synonymy and Antonymy patterns. We evaluate the models in affirmative and negative sentences without distractors. Scores that surpass the Random baseline are indicated with underline.\\n\\nBy examining coherence in these contexts, we gain insights into the models' ability to understand the negation, even if the models do not have the real-world knowledge to correctly label the sentences.\\n\\n5 Do LLMs understand negation?\\n\\nIn this section, we assess the performance of the LLMs in section 4.1 in our dataset. The evaluation is conducted in a zero-shot setting, meaning that we evaluate the models without any fine-tuning. The results of this evaluation are presented in Table 4. Foundation models, which are trained on large amounts of unlabeled data, demonstrate an All True behavior. They accurately label as True the majority of affirmative sentences and negative sentences with a distractor, which are True with the exception of the Antonymy patterns, that form approximately 5% of the total sentences. However, these models struggle to classify negative sentences and affirmative sentences with opposite labels. Their performance in these falls significantly below the random baseline exhibiting a total lack of coherence by the models.\\n\\nModels that have undergone dialogue or instruction tuning, particularly Vicuna and Flan-T5, demonstrate higher accuracy, instead. These models achieve a very high accuracy in sentences without a distractor. Specifically, Flan-T5 shows coherent answers for 46% of the triples. It is to be noted that this is a challenging metric, as a triple may be used to build up to 27 templates, and all of them must be coherent for the triple to be considered coherent.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Performance of Vicuna13B and Flan-T5-xxl after fine-tuning in our dataset. The best results are highlighted in bold, and scores that surpass the Random baseline accuracy are underlined.\\n\\nHowever, these models fail to correctly label negative sentences with a distractor. We further analyze the performance of Flan-T5 and Vicuna in negative sentences, focusing on the Synonymy and Antonymy patterns. In Pattern #01 and #02, as well as Pattern #03 and #04, the templates are opposite to each other, as explained in Subsection 3.1. Table 5 presents the performance of Flan-T5 and Vicuna in handling these patterns. Interestingly, both models achieve good results in negative sentences from the Synonymy patterns (labeled as False) but struggle with the negative sentences from the Antonymy patterns (labeled as True). This, along with their poor performance in negative sentences with a distractor (which are expected to be True, but models predict the label False), confirms that the models are heavily biased to always predict the label False in the presence of negation, regardless of the actual meaning of the sentence. This behavior suggests that the models lack a deep understanding of negation, and that they tend to rely on superficial cues rather than comprehending the true meaning conveyed by the negative sentences.\\n\\nDespite the poor performance of the models in negative sentences, it is important to note that they demonstrate the ability to correctly label affirmative sentences, both with and without distractors. This demonstrates that the models have a deep understanding of truth and falsehood. Models' struggles primarily result from the presence of negation rather than a lack of comprehension or real-world knowledge.\\n\\nExposure to negation does not solve the problem. Understanding whether LLMs would understand negation if a sufficient number of negative sentences were present in the pretraining corpora is crucial for improving their reasoning capabilities and addressing the limitations associated with negative knowledge. However, due to the lack of sufficiently large datasets containing negative knowledge, this hypothesis has not been extensively explored. In contrast, our dataset is substantial enough to be split into training, development, and test sets. To investigate whether LLMs can learn to reason over negative knowledge given enough negated data, we split the dataset at the triple level, ensuring that all sentences within a triple are assigned to the same split to ensure no data contamination. Our training dataset consists of 268,505 sentences from 2,876 triples, the development dataset includes 2,514 sentences from 244 triples, and the test dataset contains 90,281 sentences from 980 triples.\\n\\nWe fine-tune Flan-T5 and Vicuna on our dataset; the results are listed in Table 6. The impact of fine-tuning is remarkable, as it completely transforms the models' performance compared to their zero-shot counterparts. Both Flan-T5 and Vicuna exhibit higher accuracy than human annotators and achieve a notably high level of coherence. However, are the models truly learning about negation, or are they just exploiting patterns in the data? We conduct experiments to assess this.\\n\\nFirst, we train Vicuna, the best performing model, using varying amounts and types of negative knowledge. We conduct separate fine-tuning experiments using all the affirmative sentences and all the negative sentences from the dataset, resulting in two distinct models. The results of this training are presented in Table 7. Training the model exclusively with affirmative sentences yields a high accuracy in the affirmative test sentences, but it labels incorrectly nearly all the negative sentences. Conversely, when trained solely with negative sentences, the model deals successfully with the negative sentences but struggles with the affirmative sentences. Despite being exposed to extensive real-world knowledge from WordNet, the models exhibit a significant failure in comprehending negation. They consistently overlook the presence of it.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 7: Accuracy of Vicuna13B after fine-tuning with different types and amount of negative knowledge. The best results are highlighted in bold, and scores that surpass the Random baseline accuracy are indicated with underline.\\n\\n- **Affirmations**: The model performs best when fine-tuned with affirmative sentences alone, achieving high accuracy across all negation types.\\n- **Affirmations + Verbal**: Combining affirmative sentences with verbal negations slightly decreases performance.\\n- **Affirmations + Non-Verbal**: Non-verbal negations lead to a decrease in accuracy compared to affirmative sentences alone.\\n- **Affirmations + Analytic**: Analytic negations also reduce accuracy, particularly for synthetic and clausal subclausal negations.\\n- **Affirmations + synthetic and clausal subclausal negations**: Struggle to accurately classify non-verbal, analytic, and sub-clausal sentences.\\n\\nWe also fine-tune models using various combinations of affirmative sentences and different types of negations. We observe that models trained with synthetic and clausal negations struggle to accurately classify non-verbal, analytic, and sub-clausal sentences. This suggests that while the models show proficiency in understanding and reasoning with certain types of negations, they face challenges in comprehending and correctly responding to other forms of negations that they have not seen in the fine-tuning step.\\n\\n**Figure 2**: Evaluation of Vicuna13B accuracy when trained on one pattern (rows) and evaluated on the others (columns).\\n\\nWe also fine-tune Vicuna13B with each of the 11 patterns in our dataset independently, and we evaluate its performance on the other patterns. Figure 2 shows the overall accuracy scores. The results reveal that training the model with one pattern does not facilitate any successful generalization across all other patterns. Notably, as discussed in Section 3.1, the labels for affirmative and negative sentences from the Antonymy patterns are opposite to those from the remaining patterns. The failure of models trained in other patterns to label the Antonymy patterns, as well as the failure of models trained in the Antonymy patterns to label other patterns, suggest that the models are relying on repetitive data structures that are not transferable to different patterns, rather than truly understanding the concept of negation. While exposure to negation may contribute to achieving favorable results within a specific dataset, it does not lead to a generalization on negation by the models. Negation continues to pose a significant challenge in the field of Natural Language Processing and remains an unsolved problem, requiring further research and development.\\n\\n**7 Conclusion**\\n\\nCurrent LLMs are typically trained using next to-ken or mask token prediction objectives, which have proven effective for various NLP tasks. However, it remains an open issue understanding how certain a model models negation. Negation tokens, which intermittently appear in sentences, hold little predictive importance for other tokens in the sentence. As a result, there is limited negation signal during language modeling training. Previous research has touched upon this issue but was limited by small manually generated datasets. In contrast, our study introduces the largest dataset to date comprising negative sentences. This comprehensive dataset includes affirmative and negative sentences with and without distractors, incorporating multiple types of relations and negations, which help to encode the underlying mechanisms for negation understanding. Through our analysis, we reveal that current LLMs, both in zero-shot settings and when fine-tuned with examples from our dataset, exhibit a profound understanding of\"}"}
{"id": "emnlp-2023-main-531", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the truthfulness of affirmative sentences. However, when it comes to negation, these models heavily rely on superficial cues instead of generalizing negation and these superficial cues are not transferable across different negative sentences.\\n\\nNegation remains a persistent and unsolved challenge in the field of NLP, demanding further research to develop systems capable of effectively handling it. Our dataset holds the potential to significantly contribute towards achieving this objective.\\n\\nIn our future work, we plan to explore advanced reasoning paradigms, such as Chain-of-Thought, with the aim of enhancing model performance on our dataset. However, dealing properly with negation may also require novel neural architectures.\\n\\nLimitations\\n\\nThe dataset contains a limited number of low-quality sentences, which are discussed in Section 3.2. Through manual evaluation, we find that over 96% of the sentences are considered understandable and grammatically correct by at least one human annotator and their prediction of whether the sentence is true or false matches the label in the dataset. Hence, the presence of low-quality sentences does not have a significant impact on the evaluation results. On the other side, a majority of sentences in the dataset are not plausible and unlikely to be spoken by English speakers. This feature provides a benefit by ensuring that the sentences are improbable to be found in the LLM training corpus, thereby it prevents models from relying solely on memorization to generate accurate responses.\\n\\nAll experiments were conducted by querying the models for the probability of True and False tokens. We did not explore more complex reasoning prompts, such as Chain of Thought. However, as explained in Section 7, we believe that models should be able to comprehend negation and provide accurate answers across diverse settings. Complex reasoning paradigms may not always be feasible in real-world applications, specially when models are used by non-NLP professionals.\\n\\nFinally, the performance of models in our dataset is not solely determined by their capability to understand negation. Factors such as performance in question answering and prompting tasks, as well as their understanding of real-world knowledge, play a crucial role. However, models like Vicuna13B and Flan-T5-xxl showcase remarkable proficiency in correctly responding to affirmative sentences, indicating that their struggles primarily arise from the presence of negation. Additionally, we introduce a coherence metric that considers whether the model changes its prediction in the presence of negation, rather than solely focusing on the accuracy of the model's answer to the question.\\n\\nEthics Statement\\n\\nThe dataset has been created through the English WordNet relations, so it reflects most of the \u201cwestern\u201d knowledge and might fall short in including concepts of non-English speaking communities. The generated triples from WordNet may include offensive or biased sentences. This can be caused by inherited biases from WordNet, or it can be caused unintentionally during the random sampling of synsets.\\n\\nAcknowledgements\\n\\nWe would like to thank Jeremy Barnes and Aritz Farwell for willingly offering themselves to conduct the dataset quality assessment experiments. Bego\u00f1a Altuna is supported by the Basque Government postdoctoral grant POS 2022 2 0024. Iker Garc\u00eda-Ferrero is supported by a doctoral grant from the Basque Government (PRE_2022_2_0208). This work has also been partially supported by HiTZ center and the Basque Government (Research group funding IT-1805-22). We also acknowledge the funding from the following projects: (i) Antidote project funded by (PCI2020-120717-2) MCIN/AEI/10.13039/501100011033 and by \u201cERDF A way of making Europe\u201d (ii) MOTION (PID2020-112581GB-C22) supported by the Ministry of Science and Innovation of the Spanish Government (iii) The Basque Project LoRea (UPV/EHU GIU21/044). (iv) DeepKnowledge (PID2021-127777OB-C21) and ERDF A way of making Europe (v) DeepR3 (TED2021-130295B-C31) and European Union NextGeneration EU/PRTR.\\n\\nReferences\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-531", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Apart from the models presented in Section 5, as anticipated, we have also tested the performance in the task of the following models: Koala\\\\textsuperscript{4}, which is a LLaMA model that has been fine-tuned for dialogue; Open-Assistant (oasst-sft-1-pythia-12b)\\\\textsuperscript{5}, which is a Pythia model fine-tuned on human generated assistant conversations; and INCITE\\\\textsuperscript{6} 7 billion foundation model along with the two models that have been further fine-tuned by the authors in the instruction-tuning paradigm and chat conversation. Table 8 shows the extended evaluation results.\\n\\n### Efficient inference and training\\n\\nTo facilitate inference of the models on a single GPU, we employed 8-bit quantization (Dettmers et al., 2022) for all of them. We conducted preliminary experiments with Vicuna 13 billion parameter model. Results are shown in Table 9. While the running cost of the models significantly decreases, we observed only minimal performance degradation in the quantified versions.\\n\\nFor the training process, we utilized Low-Rank Adaptation (LoRA) (Hu et al., 2022). This approach involves freezing the weights of the pre-trained model.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model name                  | Type                  | Coherence | Accuracy |\\n|----------------------------|-----------------------|-----------|----------|\\n| All W/o Distractor W/ Distractor |                       |           |          |\\n| All W/o Distractor W/ Distractor Affirmation Negation |                       |           |          |\\n| Random                      |                       | 0.5       | 0.9      |\\n| LLaMA7B Foundation          |                       | 0.0       | 0.2      |\\n| LLaMA13B Foundation         |                       | 0.0       | 0.2      |\\n| LLaMA30B Foundation         |                       | 0.1       | 0.3      |\\n| LLaMA65B Foundation         |                       | 0.0       | 0.0      |\\n| Vicuna7B Dialogue           |                       | 0.0       | 0.2      |\\n| Vicuna13B Dialogue          |                       | 0.2       | 8.8      |\\n| Koala7B Dialogue            |                       | 0.0       | 0.0      |\\n| Koala13B Dialogue           |                       | 0.0       | 0.7      |\\n| WizardLM7B Instruction      |                       | 0.0       | 0.2      |\\n| WizardLM13B Instruction     |                       | 0.2       | 0.2      |\\n| WizardLM30B Instruction     |                       | 0.0       | 6.0      |\\n| WizardLM7B-uncensored       |                       |           |          |\\n| WizardLM13B-uncensored      |                       |           |          |\\n| WizardLM30B-uncensored      |                       |           |          |\\n| Pythia12B Foundation        |                       | 0.0       | 0.1      |\\n| oasst-pythia12B DIalogue    |                       |           |          |\\n| Dolly12B Instruction        |                       |           |          |\\n| T5-xxl Foundation           |                       |           |          |\\n| Flan-T5-xxl Instruction     |                       |           |          |\\n| Falcon7b Foundation         |                       |           |          |\\n| Falcon7b-instruct           |                       |           |          |\\n| Falcon40b Foundation        |                       |           |          |\\n| Falcon40b-instruct          |                       |           |          |\\n| INCITE7B-Base Foundation    |                       |           |          |\\n| INCITE7B-Instruct           |                       |           |          |\\n| INCITE7B-Chat Dialogue      |                       |           |          |\\n\\nTable 8: Zero-shot performance of various LLMs in our dataset. The best results are highlighted in bold, and scores that surpass the Random baseline accuracy are indicated with underline.\\n\\n| Model name                  | Type                  | Coherence | Accuracy |\\n|----------------------------|-----------------------|-----------|----------|\\n| All W/o Distractor W/ Distractor |                       |           |          |\\n| All W/o Distractor W/ Distractor Affirmation Negation |                       |           |          |\\n| Vicuna13B 8-Bits            |                       | 0.2       | 8.8      |\\n| Vicuna13B Float16           |                       | 0.4       | 10.1     |\\n\\nTable 9: Zero-shot performance of Vicuna using 8-Bits quantification and the orifinal float16 weights. The best results are highlighted in bold, and scores that surpass the Random baseline are indicated with underline.\\n\\nTrained model and introducing trainable rank decomposition matrices into each layer. The frozen model weights are quantized into 8 bits, while the LoRA trainable weights remain in 16 bits (Dettmers et al., 2023). By adopting this efficient training paradigm, we were able to train LLMs with up to 13 billion parameters on a single GPU within a reasonable timeframe.\\n\\nWe perform all our experiments using a single NVIDIA A100 GPU with 80GB memory. The machine used has two AMD EPYC 7513 32-Core Processors and 1024GB of RAM.\\n\\nC Dataset construction: selection of distractors\\n\\nThe automatic creation of false knowledge (distracting triples) on the basis of WordNet triples requires the use of distractors. In general, distractors are randomly selected words that replace the word senses of a synset in a given triple, although the whole synset is replaced when using glosses in templates (Patterns #01 and #02). For each WordNet triple, we use a single distracting triple except\\n\\n| Pattern | W/o Distractor | W/ Distractor | Affirmation | Negation | Affirmation | Negation |\\n|---------|----------------|---------------|-------------|----------|-------------|----------|\\n| #01     | True           | False         | False       | True     |             |          |\\n| #02     | False          | True          | False       | True     |             |          |\\n| #03     | True           | False         | False       | True     |             |          |\\n| #04     | False          | True          | False       | True     |             |          |\\n| #05     | True           | False         | False       | True     |             |          |\\n| #06     | True           | False         | False       | True     |             |          |\\n| #07     | True           | False         | False       | True     |             |          |\\n| #08     | True           | False         | False       | True     |             |          |\\n| #09     | True           | False         | False       | True     |             |          |\\n| #10     | True           | False         | False       | True     |             |          |\\n| #11     | True           | False         | False       | True     |             |          |\\n\\nTable 10: Truth-value of resulting sentences by pattern.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for Patterns #02, #03 and #04, where we use two\\ndistracting triples obtained by using one distractor\\nper synset. Apart from BLCs, for the selection of\\nsuitable distractors we consider the\\nlexicographer\\nfiles\\nprovided by WordNet, which are 45 syntactic\\ncategory and logical groupings of word senses, and\\nWordNet\\nDomains\\n, which consist of a hierarchy of\\n164 labels that characterize knowledge areas and to\\nwhich each synset is connected. More concretely:\\n\\n\u2022 Words (synsets in the case of Pattern #01 and\\n  #02 when using glosses) belonging to some\\n  BLCs cannot be distractors to ensure that se-\\n  lected words (synsets) are not too general.\\n\u2022 The combined lexicographer file and Word-\\n  Net Domain annotation of any word sense of\\n  the given synset and of any synset where the\\ndistractor occurs (of the synset in the case of\\n  Pattern #01 and #02 when using glosses) have\\nto be different.\\n\\nIn general, these restrictions ensure that the result-\\nating false triples do not encode true knowledge. The\\nprobability of choosing a synset as distractor is di-\\nrectly proportional to the logarithm of its frequency.\\nFor example, wood can be used as distractor\\nof expenditure because wood belongs to the\\nlexicographer files noun.substance (nouns denot-\\ning cognitive processes and contents), noun.group\\n(nouns denoting groupings of people or ob-\\njects) and noun.artifact (nouns denoting man-\\nmade objects) while expenditure belongs to\\nnoun.possession (nouns denoting possession and\\ntransfer of possession) and noun.act (nouns de-\\nnoting acts or actions). Therefore, we get\\nthe distracting triple \u27e8ant, wood, income\u27e9 from\\n\u27e8ant, expenditure, income\u27e9. On the contrary,\\nthe word registration cannot be used as distractor\\nof expenditure as both words belong to the lexico-\\ngraphic file noun.act and the synsets expenditure\\nand registration belong to the economy domain.\\n\\nD Dataset Description\\nIn Table 10, we provide the truth-value of sentences\\nthat results by instantiating affirmative and negative\\ntemplates using WordNet and distracting triples ac-\\ncording to the pattern. In the following subsections,\\nwe describe each pattern and provide some exam-\\nples that are used in Figures 3\u20138 to illustrate the\\ninstantiation of a sample of the templates. In all the\\ntemplates described in Figures 3\u20138, alternative and\\noptional expressions are enclosed respectively in\\nsquare brackets and parentheses.\\n\\nD.1 Pattern #01: synonymy (gloss)\\nThis pattern includes 21 templates stating semantic\\n equivalence correspondences between a word and\\nthe gloss of a synset to which the word belongs.\\nSince WordNet does not provide triples for syn-\\nonymy relating two synsets, we get triples relating\\neach synset to itself by reflexivity. For each re-\\nsulting triple, templates are also instantiated using\\none distracting triple that is obtained by replacing\\nthe third component of each triple with a distractor\\n(synset).\\n\\nIn Figure 3, we introduce a positive and a nega-\\ntive template and illustrate their instantiation using\\nthe synset flight with gloss \\\"a scheduled trip by\\nplane between designated airports\\\" and the distrac-\\ntor troop (\\\"a group of soldiers\\\").\\n\\nD.2 Pattern #02: antonymy (gloss)\\nThis pattern includes 21 templates stating semantic\\nequivalence correspondences between a word and\\nthe gloss of an antonym synset, where the word and\\nthe gloss are respectively taken from the second\\nand third component of triples. Furthermore, for\\neach WordNet antonymy triple templates are also\\ninstantiated using two distracting triples that are\\nrespectively obtained by replacing the second and\\nthird component with distractors (for the third one,\\nthe distractor is a synset).\\n\\nIn Figure 3, we introduce a positive and a nega-\\ntive template and illustrate their instantiation using\\nthe antonym synsets brother and sister (\\\"a fe-\\nmale person who has the same parents as another\\nperson\\\") and the distractors stream and fiction\\n(\\\"a literary work based on the imagination and not\\nnecessarily on fact\\\").\\n\\nD.3 Pattern #03: synonymy\\nThis pattern includes 24 templates stating seman-\\ntic equivalence correspondences between words.\\nSince WordNet does not provide triples for syn-\\nonymy relating two synsets, we get triples relating\\neach synset to itself by reflexivity. For each re-\\nsulting triple, templates are also instantiated using\\ntwo distracting triples that are respectively obtained\\nby replacing the second and third component with\\ndistractors.\\n\\nIn Figure 4, we introduce a positive and a nega-\\ntive template and illustrate their instantiation using\"}"}
{"id": "emnlp-2023-main-531", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the synonym words path and route and the distractors engine and identity.\\n\\nD.4 Pattern #04: antonymy\\nThis pattern includes 24 templates stating semantic equivalence correspondences between words. For each WordNet antonymy triple, templates are also instantiated using two distracting triples that are respectively obtained by replacing the second and third component with distractors.\\n\\nIn Figure 5, we introduce a positive and a negative template and illustrate their instantiation using the antonym synsets expenditure\\\\textsuperscript{1\\\\textsubscript{n}} and income\\\\textsuperscript{1\\\\textsubscript{n}} and the distractors wood and year.\\n\\nD.5 Pattern #05: hypernymy\\nThis pattern includes 24 templates stating semantic subsumption correspondences between words. For each WordNet hypernymy triple, templates are also instantiated using one distracting triple that is obtained by replacing the hyponym with a distractor.\\n\\nIn Figure 5, we introduce a positive and a negative template and illustrate their instantiation using the synset auction\\\\textsuperscript{1\\\\textsubscript{n}}, which is hyponym of sale\\\\textsuperscript{2\\\\textsubscript{n}}, and the distractor breakdown.\\n\\nD.6 Pattern #06: meronymy (part)\\nThis pattern includes 16 templates stating semantic correspondences between parts and wholes. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the whole with a distractor.\\n\\nIn Figure 6, we introduce a positive and a negative template and illustrate their instantiation using the synset week\\\\textsuperscript{3\\\\textsubscript{n}}, which is related by part with month\\\\textsuperscript{1\\\\textsubscript{n}}, and the distractor fence.\\n\\nD.7 Pattern #07: meronymy (substance)\\nThis pattern includes 15 templates stating semantic correspondences between substances and things. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the whole with a distractor.\\n\\nIn Figure 6, we introduce a positive and a negative template and illustrate their instantiation using the synset sand\\\\textsuperscript{1\\\\textsubscript{n}}, which is related by substance with beach\\\\textsuperscript{1\\\\textsubscript{n}}, and the distractor decade.\\n\\nD.8 Pattern #08: meronymy (member)\\nThis pattern includes 17 templates stating semantic correspondences between members and groups. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the group with a distractor.\\n\\nIn Figure 7, we introduce a positive and a negative template and illustrate their instantiation using the synset voter\\\\textsuperscript{1\\\\textsubscript{n}}, which is related by member with electorate\\\\textsuperscript{1\\\\textsubscript{n}}, and the distractor sport.\\n\\nD.9 Pattern #09: semantic role (agent)\\nThis pattern includes 2 templates stating semantic correspondences between agents and events. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the agent with a distractor.\\n\\nIn Figure 7, we introduce a positive and a negative template and illustrate their instantiation using the synset rule\\\\textsuperscript{1\\\\textsubscript{n}}, which is related by agent with governor\\\\textsuperscript{1\\\\textsubscript{n}}, and the distractor hole.\\n\\nD.10 Pattern #10: semantic role (instrument)\\nThis pattern includes 7 templates stating semantic correspondences between instruments and events. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the event with a distractor.\\n\\nIn Figure 8, we introduce a positive and a negative template and illustrate their instantiation using the synset telephone\\\\textsuperscript{1\\\\textsubscript{n}}, which is related by instrument with call\\\\textsuperscript{3\\\\textsubscript{v}}, and the distractor lay.\\n\\nD.11 Pattern #11: semantic role (result)\\nThis pattern includes 27 templates stating semantic correspondences between results and events. For each WordNet triple, templates are also instantiated using one distracting triple that is obtained by replacing the event with a distractor.\\n\\nIn Figure 8, we introduce a positive and a negative template and illustrate their instantiation using the synset response\\\\textsuperscript{1\\\\textsubscript{n}}, which is related by result with answer\\\\textsuperscript{1\\\\textsubscript{v}}, and the distractor dress.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #01: synonymy (gloss)\\n\\nAffirmative template:\\nA/An \u27e8word\u27e9 is (commonly) \u27e8gloss\u27e9.\\n\\nSentences:\\nA flight is commonly a scheduled trip by plane between designated airports. True\\nA flight is a scheduled trip by plane between designated airports. True\\nA flight is commonly a group of soldiers. False\\nA flight is a group of soldiers. False\\n\\nNegative template (verbal, analytic and clausal):\\nA/An \u27e8word\u27e9 is not \u27e8gloss\u27e9.\\n\\nSentences:\\nA flight is not a group of soldiers. True\\nA flight is not a scheduled trip by plane between designated airports. False\\n\\nPattern #02: antonymy (gloss)\\n\\nAffirmative template:\\n\u27e8word\u27e9 (commonly) [ stands for | refers to ] \u27e8gloss\u27e9.\\n\\nSentences:\\nBrother commonly stands for a female person who has the same parents as another person. False\\nBrother commonly refers to a female person who has the same parents as another person. False\\nBrother stands for a female person who has the same parents as another person. False\\nBrother refers to a female person who has the same parents as another person. False\\nStream commonly stands for a female person who has the same parents as another person. False\\nStream commonly refers to a female person who has the same parents as another person. False\\nStream stands for a female person who has the same parents as another person. False\\nStream refers to a female person who has the same parents as another person. False\\nBrother commonly stands for a literary work based on the imagination and not necessarily on fact. False\\nBrother commonly refers to a literary work based on the imagination and not necessarily on fact. False\\nBrother stands for a literary work based on the imagination and not necessarily on fact. False\\nBrother refers to a literary work based on the imagination and not necessarily on fact. False\\n\\nNegative template (synthetic and subclausal):\\nA/An \u27e8word\u27e9 is never \u27e8gloss\u27e9.\\n\\nSentences:\\nA brother is never a female person who has the same parents as another person. True\\nA stream is never a female person who has the same parents as another person. True\\nA brother is never a literary work based on the imagination and not necessarily on fact. True\"}"}
{"id": "emnlp-2023-main-531", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #03: synonymy\\n\\nAffirmative template:\\n\u27e8noun1+(e)s\u27e9 and \u27e8noun2+(e)s\u27e9 [are | may be] always different.\\n\\nSentences:\\n- Path and route are always different. False\\n- Path and route may be always different. False\\n- Engine and route are always different. True\\n- Engine and route may be always different. True\\n- Path and identity are always different. True\\n- Path and identity may be always different. True\\n\\nNegative template (verbal, analytic and subclausal):\\n\u27e8noun1+(e)s\u27e9 and \u27e8noun2+(e)s\u27e9 [are not | may not be] synonyms in any context.\\n\\nSentences:\\n- Path and route are not synonyms in any context. False\\n- Path and route may not be synonyms in any context. False\\n- Engine and route are not synonyms in any context. True\\n- Engine and route may not be synonyms in any context. True\\n- Path and identity are not synonyms in any context. True\\n- Path and identity may not be synonyms in any context. True\\n\\nFigure 4: Description of Pattern #03.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #04: antonymy\\n\\nAffirmative template:\\n\u27e8noun1+(e)s\u27e9 and \u27e8noun2+(e)s\u27e9 [are | may be] synonyms (in certain contexts).\\n\\nSentences:\\n- Expenditure and income are synonyms in certain contexts. False\\n- Expenditure and income may be synonyms in certain contexts. False\\n- Expenditure and income are synonyms. False\\n- Expenditure and income may be synonyms. False\\n- Expenditure and year are synonyms in certain contexts. False\\n- Expenditure and year may be synonyms in certain contexts. False\\n- Expenditure and year are synonyms. False\\n- Expenditure and year may be synonyms. False\\n- Wood and income are synonyms in certain contexts. False\\n- Wood and income may be synonyms in certain contexts. False\\n- Wood and income are synonyms. False\\n- Wood and income may be synonyms. False\\n\\nNegative template (analytic and subclausal):\\n\u27e8noun1+(e)s\u27e9 and \u27e8noun2+(e)s\u27e9 [are | may be] the same thing in no context.\\n\\nSentences:\\n- Expenditure and income are the same thing in no context. True\\n- Expenditure and income may be the same thing in no context. True\\n- Expenditure and year are the same thing in no context. True\\n- Expenditure and year may be the same thing in no context. True\\n- Wood and income are the same thing in no context. True\\n- Wood and income may be the same thing in no context. True\\n\\nFigure 5: Description of Patterns #04 and #05.\\n\\nPattern #05: hypernymy\\n\\nAffirmative template:\\nA/An \u27e8hyponym\u27e9 [is | may be] a/an \u27e8hypernym\u27e9 in certain contexts.\\n\\nSentences:\\n- An auction is a sale in certain contexts. True\\n- An auction may be a sale in certain contexts. True\\n- A breakdown is a sale in certain contexts. False\\n- A breakdown may be a sale in certain contexts. False\\n\\nNegative template (synthetic and subclausal):\\nA/An \u27e8hyponym\u27e9 is never a/an \u27e8hypernym\u27e9.\\n\\nSentences:\\n- An auction is never a sale. False\\n- A breakdown is never a sale. True\\n\\nFigure 5: Description of Patterns #04 and #05.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #06: meronymy (part)\\n\\nAffirmative template:\\nA/An \u27e8part\u27e9 \\\\[ is commonly | may be \\\\] part of a/an \u27e8whole\u27e9.\\n\\nSentences:\\n- A week is commonly part of a month. True\\n- A week may be part of a month. True\\n- A week is commonly part of a fence. False\\n- A week may be part of a fence. False\\n\\nNegative template (synthetic and subclausal):\\nA/An \u27e8part\u27e9 is never part of a/an \u27e8whole\u27e9.\\n\\nSentences:\\n- A week is never part of a month. False\\n- A week is never part of a fence. True\\n\\nFigure 6: Description of Patterns #06 and #07.\\n\\nPattern #07: meronymy (substance)\\n\\nAffirmative template:\\n\u27e8thing+ (e)s\u27e9 \\\\[ are commonly | may be \\\\] made of \u27e8substance\u27e9.\\n\\nSentences:\\n- Beaches are commonly made of sand. True\\n- Beaches may be made of sand. True\\n- Decades are commonly made of sand. False\\n- Decades may be made of sand. False\\n\\nNegative template (Analytic and subclausal):\\nIn no context \u27e8thing+ (e)s\u27e9 \\\\[ are |may be \\\\] made of \u27e8substance\u27e9.\\n\\nSentences:\\n- In no context beaches are made of sand. False\\n- In no context decades are made of sand. True\"}"}
{"id": "emnlp-2023-main-531", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #08: meronymy (member)\\nAffirmative template:\\n\u27e8member + (e)s\u27e9 [are | may be] members of \u27e8group + (e)s\u27e9.\\n\\nSentences:\\nVoters are members of electorates. True\\nVoters may be members of electorates. True\\nVoters are members of sports. False\\nVoters may be members of sports. False\\n\\nNegative template (verbal, analytic and clausal):\\n\u27e8member + (e)s\u27e9 [are not | may not be] members of \u27e8group + (e)s\u27e9 in any context.\\n\\nSentences:\\nVoters are not members of electorates in any context. False\\nVoters may not be members of electorates in any context. False\\nVoters are not members of sports in any context. True\\nVoters may not be members of sports in any context. True\\n\\nPattern #09: semantic role (agent)\\nAffirmative template:\\n\u27e8event + ing\u27e9 is commonly done by \u27e8agent + (e)s\u27e9.\\n\\nSentences:\\nRuling is commonly done by governors. True\\nRuling is commonly done by holes. False\\n\\nNegative template (verbal, analytic and clausal):\\n\u27e8event + ing\u27e9 is not commonly done by \u27e8agent + (e)s\u27e9.\\n\\nSentences:\\nRuling is not commonly done by governors. False\\nRuling is not commonly done by holes. True\\n\\nFigure 7: Description of Patterns #08 and #09.\"}"}
{"id": "emnlp-2023-main-531", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pattern #10: semantic role (instrument)\\nAffirmative template:\\nA/An \u27e8instrument\u27e9 \\[ is commonly | may be \\] \\[ used | needed \\] for \u27e8event + ing\u27e9.\\nSentences:\\n- A telephone is commonly used for calling. True\\n- A telephone is commonly needed for calling. True\\n- A telephone may be used for calling. True\\n- A telephone may be needed for calling. True\\n- A telephone is commonly used for laying. False\\n- A telephone is commonly needed for laying. False\\n- A telephone may be used for laying. False\\n- A telephone may be needed for laying. False\\n\\nNegative template (Synthetic and subclausal):\\nA/An \u27e8instrument\u27e9 should never be \\[ used | needed \\] for \u27e8event + ing\u27e9.\\nSentences:\\n- A telephone should never be used for calling. False\\n- A telephone should never be used for laying. True\\n\\nPattern #11: semantic role (result)\\nAffirmative template:\\n\u27e8event + ing\u27e9 \\[ commonly leads | may lead \\] to a/an \u27e8result\u27e9.\\nSentences:\\n- Answering commonly leads to a response. True\\n- Answering may lead to a response. True\\n- Dressing commonly leads to a response. False\\n- Dressing may lead to a response. False\\n\\nNegative template (analytic and subclausal):\\n\u27e8event + ing\u27e9 \\[ leads | may lead \\] to a/an \u27e8result\u27e9 in no context.\\nSentences:\\n- Answering leads to a response in no context. False\\n- Answering may lead to a response in no context. False\\n- Answering leads to a response in no context. True\\n- Answering may lead to a response in no context. True\\n\\nFigure 8: Description of Patterns #10 and #11.\"}"}
