{"id": "acl-2023-long-731", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ExplainMeetSum: A Dataset for Explainable Meeting Summarization\\nAligned with Human Intent\\nHyun Kim\u2217 and Minsoo Cho\u2217\\nSuperintelligence Creative Research Lab.,\\nElectronics and Telecommunications Research Institute (ETRI), Republic of Korea\\n{h.kim, mscho}@etri.re.kr\\nSeung-Hoon Na\u2020\\nComputer Science and Engineering,\\nJeonbuk National University,\\nRepublic of Korea\\nnash@jbnu.ac.kr\\n\\nAbstract\\nTo enhance the explainability of meeting summarization, we construct a new dataset called \\\"ExplainMeetSum,\\\" an augmented version of QMSum, by newly annotating evidence sentences that faithfully \\\"explain\\\" a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracle. We further present an explainability-aware task, named \\\"Explainable Evidence Extraction\\\" (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics.\\n\\n1 Introduction\\nMeeting summarization typically is a form of long document summarization, because the input is usually given as a long conversational sequence from multi-party dialogues. Among various approaches for long document summarization, the extract-then-generate method is one of the promising methods; it first automatically selects \\\"salient\\\" contents which are relevant to a specific summarization and employs them to guide the generation of a summary (Chen and Bansal, 2018; Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021; Zhang et al., 2021; Mao et al., 2022), thereby inducing the manner of dealing with both efficiency (in processing a long input) and effectiveness (in locating accurately informative relevant contents).\\n\\n* These authors contributed equally to this work.\\n\u2020 Corresponding author\\n\\n1 Our code and dataset are available at https://github.com/hkim-etri/ExplainMeetSum\\n\\nFigure 1: An illustrated example of our annotation process in ExplainMeetSum where evidence sentences are manually aligned for every single summary sentence and categorized into two types \u2013 Central Evidence Sentence (CES) and Peripheral Evidence Sentence (PES), described in Section 3.\\n\\nHowever, the extract-then-generate method typically selects salient content in a distantly supervised or an end-to-end manner using only a final summary as a supervision signal, thereby likely being far from those in the chain-of-thought (or compression) required for the human summarization process. Thus, the resulting salient contents do not satisfactorily and convincingly \\\"explain\\\" or \\\"support\\\" a generated summary, and cause it to lack explainability.\\n\\nAiming to achieve a high degree of explainability in meeting summarization, this paper proposes a new dataset called ExplainMeetSum, an augmented version of QMSum, by manually and explicitly annotating evidence sentences that faithfully \\\"explain\\\" and \\\"support\\\" each summary sentence. Figure 1 illustrates an example of the annotation of evidence sentences. Based extensively on ExplainMeetSum, we propose Multi-DYLE, a generalized version of DYLE that enables multiple extractors, and present a novel explainability-aware benchmark task, called Explainable Evidence Extraction (E3), as follows.\\n\\nExplainMeetSum: A Dataset for Explainable Meeting Summarization\\nAligned with Human Intent\\nHyun Kim\u2217 and Minsoo Cho\u2217\\nSuperintelligence Creative Research Lab.,\\nElectronics and Telecommunications Research Institute (ETRI), Republic of Korea\\n{h.kim, mscho}@etri.re.kr\\nSeung-Hoon Na\u2020\\nComputer Science and Engineering,\\nJeonbuk National University,\\nRepublic of Korea\\nnash@jbnu.ac.kr\\n\\nAbstract\\nTo enhance the explainability of meeting summarization, we construct a new dataset called \\\"ExplainMeetSum,\\\" an augmented version of QMSum, by newly annotating evidence sentences that faithfully \\\"explain\\\" a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracle. We further present an explainability-aware task, named \\\"Explainable Evidence Extraction\\\" (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics.\\n\\n1 Introduction\\nMeeting summarization typically is a form of long document summarization, because the input is usually given as a long conversational sequence from multi-party dialogues. Among various approaches for long document summarization, the extract-then-generate method is one of the promising methods; it first automatically selects \\\"salient\\\" contents which are relevant to a specific summarization and employs them to guide the generation of a summary (Chen and Bansal, 2018; Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021; Zhang et al., 2021; Mao et al., 2022), thereby inducing the manner of dealing with both efficiency (in processing a long input) and effectiveness (in locating accurately informative relevant contents).\\n\\n* These authors contributed equally to this work.\\n\u2020 Corresponding author\\n\\n1 Our code and dataset are available at https://github.com/hkim-etri/ExplainMeetSum\\n\\nFigure 1: An illustrated example of our annotation process in ExplainMeetSum where evidence sentences are manually aligned for every single summary sentence and categorized into two types \u2013 Central Evidence Sentence (CES) and Peripheral Evidence Sentence (PES), described in Section 3.\\n\\nHowever, the extract-then-generate method typically selects salient content in a distantly supervised or an end-to-end manner using only a final summary as a supervision signal, thereby likely being far from those in the chain-of-thought (or compression) required for the human summarization process. Thus, the resulting salient contents do not satisfactorily and convincingly \\\"explain\\\" or \\\"support\\\" a generated summary, and cause it to lack explainability.\\n\\nAiming to achieve a high degree of explainability in meeting summarization, this paper proposes a new dataset called ExplainMeetSum, an augmented version of QMSum, by manually and explicitly annotating evidence sentences that faithfully \\\"explain\\\" and \\\"support\\\" each summary sentence. Figure 1 illustrates an example of the annotation of evidence sentences. Based extensively on ExplainMeetSum, we propose Multi-DYLE, a generalized version of DYLE that enables multiple extractors, and present a novel explainability-aware benchmark task, called Explainable Evidence Extraction (E3), as follows.\\n\\nExplainMeetSum: A Dataset for Explainable Meeting Summarization\\nAligned with Human Intent\\nHyun Kim\u2217 and Minsoo Cho\u2217\\nSuperintelligence Creative Research Lab.,\\nElectronics and Telecommunications Research Institute (ETRI), Republic of Korea\\n{h.kim, mscho}@etri.re.kr\\nSeung-Hoon Na\u2020\\nComputer Science and Engineering,\\nJeonbuk National University,\\nRepublic of Korea\\nnash@jbnu.ac.kr\\n\\nAbstract\\nTo enhance the explainability of meeting summarization, we construct a new dataset called \\\"ExplainMeetSum,\\\" an augmented version of QMSum, by newly annotating evidence sentences that faithfully \\\"explain\\\" a summary. Using ExplainMeetSum, we propose a novel multiple extractor guided summarization, namely Multi-DYLE, which extensively generalizes DYLE to enable using a supervised extractor based on human-aligned extractive oracle. We further present an explainability-aware task, named \\\"Explainable Evidence Extraction\\\" (E3), which aims to automatically detect all evidence sentences that support a given summary. Experimental results on the QMSum dataset show that the proposed Multi-DYLE outperforms DYLE with gains of up to 3.13 in the ROUGE-1 score. We further present the initial results on the E3 task, under the settings using separate and joint evaluation metrics.\\n\\n1 Introduction\\nMeeting summarization typically is a form of long document summarization, because the input is usually given as a long conversational sequence from multi-party dialogues. Among various approaches for long document summarization, the extract-then-generate method is one of the promising methods; it first automatically selects \\\"salient\\\" contents which are relevant to a specific summarization and employs them to guide the generation of a summary (Chen and Bansal, 2018; Zhang et al., 2019; Lebanoff et al., 2019; Xu and Durrett, 2019; Bajaj et al., 2021; Zhang et al., 2021; Mao et al., 2022), thereby inducing the manner of dealing with both efficiency (in processing a long input) and effectiveness (in locating accurately informative relevant contents).\\n\\n* These authors contributed equally to this work.\\n\u2020 Corresponding author\\n\\n1 Our code and dataset are available at https://github.com/hkim-etri/ExplainMeetSum\\n\\nFigure 1: An illustrated example of our annotation process in ExplainMeetSum where evidence sentences are manually aligned for every single summary sentence and categorized into two types \u2013 Central Evidence Sentence (CES) and Peripheral Evidence Sentence (PES), described in Section 3.\\n\\nHowever, the extract-then-generate method typically selects salient content in a distantly supervised or an end-to-end manner using only a final summary as a supervision signal, thereby likely being far from those in the chain-of-thought (or compression) required for the human summarization process. Thus, the resulting salient contents do not satisfactorily and convincingly \\\"explain\\\" or \\\"support\\\" a generated summary, and cause it to lack explainability.\\n\\nAiming to achieve a high degree of explainability in meeting summarization, this paper proposes a new dataset called ExplainMeetSum, an augmented version of QMSum, by manually and explicitly annotating evidence sentences that faithfully \\\"explain\\\" and \\\"support\\\" each summary sentence. Figure 1 illustrates an example of the annotation of evidence sentences. Based extensively on ExplainMeetSum, we propose Multi-DYLE, a generalized version of DYLE that enables multiple extractors, and present a novel explainability-aware benchmark task, called Explainable Evidence Extraction (E3), as follows.\"}"}
{"id": "acl-2023-long-731", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Multiple Extractors Guided Dynamic Latent Extraction for Abstractive Summarization (Multi-DYLE) straightforwardly extends DYLE (Mao et al., 2022) by newly employing a supervised extractor trained on the evidence sentences in ExplainMeetSum in addition to the original DYLE's extractor. The underlying assumption is that, being explicitly trained using \u201cexplainable\u201d evidence sentences, the extract-then-summarize method undertakes more likely \u201chuman-aligned\u201d salient sentences to guide the summary generation process, potentially leading to an improvement in the quality of summaries; this effect is to some extent similar to the chain-of-thought prompting (Wei et al., 2022) that explicitly supervises the human\u2019s reasoning steps for the decoder in the language models.\\n\\n2. Explainable Evidence Extraction (E3) is an explainability-aware task that aims to automatically detect all evidence sentences to explain and support a summary for meeting summarization. Thus, E3 is the task defined under the summarize-then-explain setting, where a generated summary is first provided and its explainable evidence sentences are extracted. By newly employing the evidence-based supervised extractor, the experimental results on the QM-Sum dataset show that the proposed Multi-DYLE outperforms DYLE with an increase of 3.13 in the ROUGE-1 score. We further evaluate the baseline transformer-based models for the E3 task and present the initial experiment results under separate and joint evaluation settings that unify the meeting summarization and E3. To our best of knowledge, our work is the first to explore the explainability of meeting summarization by providing manually annotated datasets of explainable evidence sentences.\\n\\nOur contributions are summarized as follows: 1) we newly introduce the ExplainMeetSum dataset as a valuable resource to enhance explainability in meeting summarization. 2) We propose Multi-DYLE, which enables the merging of multiple extractors in DYLE and achieves non-trivial improvements over DYLE. 3) We propose E3 using ExplainMeetSum as a new explainability-aware benchmark task, establishing the goal of extracting human-aligned explainable evidence sentences for a generated summary.\\n\\n2.1 Meeting Summarization\\nAmong the various approaches for meeting summarization such as divide-and-conquer (Grail et al., 2021; Zhang et al., 2022) and hierarchical method (Zhu et al., 2020), the extract-then-summarize (or locate-and-summarize) methods have been widely adopted owing to their effective two-stage manner of handling long inputs (Chen and Bansal, 2018; Lebanoff et al., 2019; Xu and Durrett, 2019; Zhang et al., 2019; Bajaj et al., 2021; Zhang et al., 2021; Mao et al., 2022).\\n\\nIn particular, DYLE presented a joint training approach (Mao et al., 2022) to strengthen the interaction between the extractor and generator in a bidirectional manner by proposing a consistency loss that forces the extractor distribution over a set of snippets to closely match their importance degrees assigned by the generator\u2019s view.\\n\\nSome studies have designed dynamic interactions between speakers during a dialogue. Qi et al. (2021) used pre-training methods based on a hierarchical encoder-decoder structure to model the semantic information between participants. Feng et al. (2020) proposed the graph modeling strategy to encode discourse relations in a conversation.\\n\\n2.2 Evaluation for Extractive Summarization\\nGiven the known limitations of using ROUGE due to its simplified n-gram matching style (Schluter, 2017), some studies have focused on evaluation in the setting of extractive summarization (Ma et al., 2021; Akter et al., 2022), pursuing automatic methods without requiring human annotation. DSMRC-S (Ma et al., 2021) transformed the summarization problem into a machine reading comprehension task, and Akter et al. (2022) proposed a semantic-aware nCG (normalized cumulative gain)-based evaluation metric that uses automatically generated semantic-aware ground truths.\\n\\nUnlike the existing \u201cautomatic\u201d approaches for extractive summarization, we newly present \u201cmanually\u201d annotated ground truths and explicitly define E3 in meeting summarization, being different from the extractive summarization task.\\n\\nFurthermore, evidence sentences manually extracted in our work are different from summarization content unit (SCU) (Nenkova and Passonneau, 2004; Louis and Nenkova, 2009). SCUs are obtained from multiple summaries by humans, not from an original document, whereas CES and PES.\"}"}
{"id": "acl-2023-long-731", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of ExplainMeetSum. The first two rows present the total number of transcripts and queries. The third row is the average number of sentences in summaries. The fourth row is the ratio of sentences between two types of evidence where the number of CES is larger than that of PES in all sets. The last row indicates the average numbers of CES and PES per summary sentence.\\n\\n3 ExplainMeetSum Dataset\\n\\n3.1 Annotation of Explainable Evidence\\n\\nWe conducted the annotation on top of the QM-Sum (Zhong et al., 2021), which is one of the largest datasets for meeting summarization containing \u201cquery-summary\u201d pairs on the meeting transcripts from AMI (Carletta et al., 2006), ICSI (Janin et al., 2003), and parliamentary committee meetings. For each summary sentence, annotators were required to select aligned evidence sentences by dividing them into two types \u2013 CES and PES \u2013 according to their degrees of relevance to the query-summary pair, informally defined as follows:\\n\\nCentral Evidence Sentence (CES) is an evidence sentence with key information that is exactly or semantically matched with \u201ccentral\u201d parts in the summary sentence or closely related examples. An example of a CES is as follows:\\n\\n(Gold Summary) The team members will work on their individual work.\\n\\n(CES) Project Manager: And uh you are going to work on your individual works.\\n\\nPeripheral Evidence Sentence (PES) is an evidence sentence that is relevant but less important than a CES, usually containing auxiliary information or examples that require a step of reasoning to match the given summary sentence. An example of a PES is as follows:\\n\\n(Gold summary) The remote will have buttons for channel changing, volume settings, numerals, and power on/off.\\n\\n(PES) Project Manager: but first maybe what is what are the usual function of a standard remote control?\\n\\nTo clearly classify evidence types, annotators were guided to choose a type of matching characteristic of a candidate evidence sentence to a summary sentence, and to determine CES for the cases of exact, semantic, and supportive matching types, and PES for illustrative, introductory, and connective matching types.\\n\\n3.2 Data Collection and Statistics\\n\\nTable 1 lists the statistics for the ExplainMeetSum dataset. The \u201cGeneral\u201d and \u201cSpecific\u201d subcolumns correspond to two types of queries in QMSum, respectively. \u201cGeneral(long)\u201d subcolumn refers to the summaries of AMI and ICSI.\\n\\nAppendix A.2 presents samples of ExplainMeetSum with an full annotation example. Appendices A.1 and A.3 present details and quality control methods in the annotation process, respectively.\\n\\n4 Multi-DYLE\\n\\nFigure 2 shows the overall architecture of the Multi-DYLE model. The key novelty of Multi-DYLE is the employment of heterogeneous extractors with separate sets of extractive oracles, M oracle losses, and a consistency loss under the generalized extractive-generator framework. This section presents the details of Multi-DYLE, including a brief description of DYLE (Mao et al., 2022).\"}"}
{"id": "acl-2023-long-731", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: An overall architecture of the proposed Multi-DYLE for the case of $M = 2$: i) Multi-DYLE consists of $M$ extractors, i.e., $E = \\\\{E(1), E(2)\\\\}$, which compute relevance scores for each sentence $x_i \\\\in X$; ii) For the $j$-th extractor, we select the top $K$ snippets with the highest relevance scores, denoted as $X(j)_K$ (i.e. Eq. (1)); iii) The resulting $M$ list of top-$K$ sentences are then merged to obtain $X_K$ as a final set, but allowing the \\\"duplicated\\\" snippets (i.e. Eq. (2)); iv) The merged set is used to guide the summary generation process at the decoding time steps (i.e., Eq. (3)); v) The dynamic weights computed by the generator over all decoding time steps are reflected back as a supervised signal to train each $j$-th extractor, thus leading to the consistency loss $L_{consist}$ (i.e. Eq. (4)); vi) Multi-DYLE is trained with multi-task learning using the combined losses (i.e., Eq. (6))\u2013generation loss $L_{gen}$, oracle losses $L(j)_{oracle}$ (i.e., Eq. (5)), and consistency loss $L_{consist}$.\\n\\n4.1 Multiple Extractors Guided Generator\\n\\nFollowing the notation of DYLE (Mao et al., 2022), suppose that a query $q$ is given, and $X = (x_1, \\\\ldots, x_L)$ is a sequence of $L$ snippets. Unless otherwise mentioned, a snippet indicates a single dialogue sentence of a speaker in a meeting transcript. In contrast to DYLE that uses a \\\"single\\\" extractor, we have $M$ multiple extractors, denoted as $E = \\\\{E(1), \\\\ldots, E(M)\\\\}$ which computes relevance score $s(j)_i = E(j)(q, x_i)$ for the $i$-th utterance sentence $x_i$. For the $j$-th extractor, we select the top $K$ snippets $X(j)_K$ based on their relevance scores, as follows:\\n\\n$$X(j)_K = \\\\text{top}_K\\\\{ (x_i, E(j)(q, x_i)) \\\\}_{i=1}^n$$\\n\\nwhere $\\\\text{top}_K(S)$ is the operator that chooses a list of the top $K$ keys by sorting $S = \\\\{(a_1, a_2), \\\\ldots, (a_n, a_n)\\\\}$, a set of $n$ key-value pairs (i.e., 2-tuples) after sorting $S$ in descending order according to their values.\\n\\nThe core part of Multi-DYLE is the merging stage, which combines the $M$ lists of the top-$K$ extracted sentences $\\\\{X(j)_K\\\\}_{j=1}^M$ as follows:\\n\\n$$X_K = \\\\text{merge}\\\\{X(1)_K, \\\\ldots, X(M)_K\\\\}$$\\n\\nOur merging enables the duplicate sentences in a single list, and the same sentence is treated differently. For example, for $K = 2$ and $M = 2$,\\n\\n$$\\\\text{merge}\\\\{(x_1, x_2), (x_2, x_3)\\\\} = \\\\{x_1, x_2(1), x_2(2), x_3\\\\}$$\\n\\nwhere $x_2(1)$ and $x_2(2)$ are considered differently, despite being identical.\\n\\nThe generator produces a summary by referring to $X_K$ as a set of retrieved content by computing the generation probabilities $P(y|q, X_K)$, similar to an extended version of the RAG-token model of Lewis et al. (2020), as follows:\\n\\n$$P(y|q, X_K) = \\\\prod_{t=1}^T \\\\sum_{x \\\\in X_K} P(x|q, X_K, y_1:t-1) P(y_t|q, x, y_1:t-1)$$\\n\\nwhere $y_1:t-1$ is the previously generated sequence at the $t$-th decoding time step, $P(x|q, X_K, y_1:t-1)$ is the dynamic weight of the snippet $x$, and $P(y_t|q, x, y_1:t-1)$ is the generation probability when $x$ is used as the additional encoded context.\\n\\nSimilar to DYLE, Multi-DYLE uses the average of the dynamic weights of a sentence $x \\\\in X_K$ across $T$ time steps as a supervised signal to train...\"}"}
{"id": "acl-2023-long-731", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"extractors, thereby introducing consistency loss, as follows:\\n\\n$$L_{\\\\text{consist}} = \\\\sum_{t=1}^{T} \\\\sum_{y=1}^{T} P(\\\\cdot|q,X,K,y) \\\\cdot \\\\| \\\\text{softmax}(E(q,x_i)) - x_i \\\\in X_K) \\\\right)$$\\n\\nwhere\\n\\n$$E(q,x_i) = E(j)(q,x_i)$$\\n\\nwhen\\n\\n$$x_i \\\\in X_j$$\\n\\nfor the top sentences selected by the $j$-th extractor, i.e., $x_i \\\\in X_j$.  \\n\\n4.2 Multiple Extractive Oracles\\n\\nTo provide basic supervised signals for multiple extractors, we employ $M$ separate extractive oracles, $\\{X_{jo}\\\\}_{j=1}^M$, thus introducing $M$ oracle losses, defined as follows:\\n\\n$$L_{\\\\text{oracle}}(j) = -\\\\sum_{x_i \\\\in X_{jo}} \\\\frac{1}{\\\\sum_{L_{i=1}}^{L} e^{E(j)(q,x_i)}}$$\\n\\nIn our setting, we deploy two different sets of extractive oracles for $X_{jo}$: ROUGE-based extractive oracles, as in DYLE, and our CES-based extractive oracles, which we clearly specify in Section 6.1.1.\\n\\n4.3 Generalized Training Objective\\n\\nThe final training objective is based on the $M$ oracle losses and the consistency loss as follows:\\n\\n$$L = \\\\lambda_g L_{\\\\text{gen}} + \\\\lambda_o M \\\\sum_{j=1}^{M} L_{\\\\text{oracle}}(j) + \\\\lambda_c L_{\\\\text{consist}}$$\\n\\nwhere\\n\\n$$L_{\\\\text{gen}}$$\\n\\nis the generation loss using NLL defined in DYLE (Mao et al., 2022), and\\n\\n$$\\\\lambda_g, \\\\lambda_o, \\\\lambda_c$$\\n\\nare hyperparameters, which are fixed to 1 in this study.\\n\\nMulti-DYLE degenerates to DYLE when $M = 1$ using $X_{(1)}$ as a set of ROUGE-based extractive oracles.\\n\\n5 Explainable Evidence Extraction (E3)\\n\\nIn this section, we introduce the details of E3, which identifies all CESs and PESs for a given summary, and baseline E3 models.\\n\\n5.1 Task Definition\\n\\nDifferent from the summarization task in Section 4, we now have a summary $S$, given as a sequence of $N$ summary sentences $S = (s_1, \\\\ldots, s_N)$ where $S$ is either a gold summary or automatically generated one. Given the meeting transcript $X = (x_1, \\\\ldots, x_L)$, let $Y_k \\\\subseteq X$ be a ground-truth set of CESs and PESs for the $k$-th summary sentence $s_k \\\\in S$, obtained in ExplainMeetSum. E3 is thus defined as the task of automatically identifying $Y_k$ for a given $s_k \\\\in S$.\\n\\n5.2 Model\\n\\nAs our baseline E3 model, often referred to as the evidence extractor (EE), we employ the extractor module in the DYLE (Mao et al., 2022) model, but using a given summary sentence as an additional input for the encoder. Formally, the EE's input is a concatenated sequence of the $k$-th summary sentence $s_k$, query $q$, and meeting transcript $X$, presented as $(s_k, q, X)$. EE then produces relevance scores for the $i$-th sentence $x_i \\\\in X$.\\n\\nBecause the meeting transcript is often too long to be contained within the maximum length limit, we split the transcript into a list of \u201cchunks\u201d with the fixed size of tokens, and separately encode all the chunks. The relevance score of the $i$-th sentence $x_i$ is obtained from the chunk-level representation which $x_i$ belongs to.\\n\\nFor training, the cross-entropy loss is adopted to maximize the classification probability of gold evidence sentences in the CES and PES. For the inference time, we further apply a filtering step to the classification probabilities, using threshold-based and top-K selection methods, as discussed in Section 6.2.\\n\\n6 Experiments\\n\\nIn our experiment, we first compare the summarization performance of Multi-DYLE, introduced in Section 4, with that of DYLE and its simple variants to check whether the use of multiple extractors lead to performance improvement. We further present the performance of our baseline EE described in Section 5 under the settings of separate and joint tasks in Sections 6.2 and 6.3, respectively. An illustration and examples of joint tasks are described in Appendix C, and the implementation details for the Multi-DYLE and EE models are presented in Appendix D.\"}"}
{"id": "acl-2023-long-731", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6.1 Meeting Summarization\\n\\n6.1.1 Main Results\\n\\nTable 2 presents the comparison results of Multi-DYLE (i.e. using ExplainMeetSum) and DYLE for QMSum.\\n\\nAs aforementioned in Section 4, Multi-DYLE uses sentence-level snippets whereas the original version of DYLE uses turn-level snippets. To clarify the different setups for using extractive oracles, with the abuse of notation, \\\\(X_{ROG}^o\\\\), \\\\(X_{CES}^o\\\\), and \\\\(X_{PES}^o\\\\) refer to the sets of ROUGE-based, CES-based, and PES-based extractive oracles (in ExplainMeetSum), respectively. The various types of Multi-DYLE are defined as follows:\\n\\n- **Multi-DYLE** \\\\((X_{\\\\alpha}^o)\\\\): the run using a single extractor \\\\((M = 1)\\\\) based on \\\\(X_{(1)}^o = X_{\\\\alpha}^o\\\\)\\n\\nSome variants of DYLE using \\\\(X_{ROG}^o\\\\) are denoted as follows:\\n\\n- **DYLE** \\\\((X_{ROG}^o)\\\\): the variant of DYLE using the fine-tuned DYLE model at turn-level settings but applying it to sentence-level snippets at inference time.\\n\\n- **Multi-DYLE** \\\\((X_{ROG}^o, X_{\\\\beta}^o)\\\\): the variant of DYLE in which both fine-tuning and testing are conducted under our setting of sentence-level snippets, unlike DYLE\\\\((X_{ROG}^o)\\\\).\\n\\nInterestingly, by performing inference only at sentence-level utterances without any fine-tuning, DYLE\\\\((X_{ROG}^o)\\\\) achieves a ROUGE-1 of 35.41, with an increase of approximately 1 in ROUGE-1 over the original turn-level DYLE. Multi-DYLE\\\\((X_{ROG}^o)\\\\) further increases the performance by fully fine-tuning DYLE in the sentence-level setting. The results consistently show that sentence-level snippets are more effective than turn-level ones.\\n\\nUsing the CES-based extractive oracles \\\\(X_{CES}^o\\\\), it is noticeable that Multi-DYLE\\\\((X_{CES}^o)\\\\) further improves the performance of Multi-DYLE\\\\((X_{ROG}^o)\\\\), resulting in an increase of about 0.7 in ROUGE-1.\\n\\n### Table 2: Meeting summarization results on test sets of QMSum, comparing Multi-DYLE and DYLE with other previous works, under ROUGE scores as evaluation metrics.\\n\\n| Model | ROUGE-\\n|-------|------------------|\\n| ROUGE-\\n| R-1   | R-2   | R-L   |\\n| **<Baselines>** | | | |\\n| BART-LS (Xiong et al., 2022) | 37.9 | 12.1 | 33.1 |\\n| SecEec-W (Vig et al., 2022) | 37.80 | 13.43 | 33.38 |\\n| DYLE (Mao et al., 2022) | 34.42 | 9.71 | 30.10 |\\n| **<Ours - Sentence level>** | | | |\\n| DYLE \\\\((X_{ROG}^o)\\\\) with turn-level finetuning | 35.41 | 10.74 | 31.00 |\\n| Multi-DYLE \\\\((X_{ROG}^o)\\\\) | 35.93 | 11.24 | 31.26 |\\n| Multi-DYLE \\\\((X_{CES}^o)\\\\) | 36.63 | 11.81 | 31.82 |\\n| Multi-DYLE \\\\((X_{ROG}^o, X_{CES}^o)\\\\) | 37.55 | | |\\n\\nBy merging the two types of extractors, Multi-DYLE\\\\((X_{ROG}^o, X_{CES}^o)\\\\) leads to non-trivial improvements over runs with a single extractor (i.e., Multi-DYLE\\\\((X_{ROG}^o)\\\\) or Multi-DYLE\\\\((X_{CES}^o)\\\\)), finally achieving 37.55 of ROUGE-1.\\n\\nOverall, the results confirm that the use of human annotated evidence sentences improves performance under the same framework, even without changing the model's architecture.\\n\\n6.1.2 Ablation Study\\n\\nTable 3 presents the ablation study of Multi-DYLE with different setups of extractive oracles by varying the number of extracted sentences \\\\(K\\\\). In addition to the runs in Table 2, we consider the union of two sets of extractive oracles \u2013 \\\\(X_{ROG}^o \\\\cup X_{CES}^o\\\\) and \\\\(X_{CES}^o \\\\cup X_{PES}^o\\\\).\\n\\nIn Table 3, the last two columns named \\\"ROG-Oracle\\\" and \\\"CES-Oracle\\\" refer to the extraction performances of \\\\(\\\\{X_{(j)}^K\\\\}_{M=1}^M\\\\), which are selected by \\\\(M\\\\) extractors (i.e., using Eq. (1) and their merged results \\\\(X^K\\\\) that is using Eq. (2)) under the precision, recall, and F1 metrics when using \\\\(X_{ROG}^o\\\\) and \\\\(X_{CES}^o\\\\) as ground-truth sets, respectively. Here, the subcolumns named \\\"Ext1,\\\" \\\"Ext2,\\\" and \\\"Merged\\\" indicate the extraction results of \\\\(X_{(1)}^K\\\\), \\\\(X_{(2)}^K\\\\), and \\\\(X^K\\\\), respectively.\\n\\nIt is clearly shown that the ROUGE scores tend to be proportional to the F1 score of the extraction when either \\\\(X_{ROG}^o\\\\) or \\\\(X_{CES}^o\\\\) is the ground-truth set. Particularly, the ROUGE scores are slightly more proportional to F1 when \\\\(X_{ROG}^o\\\\) is a ground-truth set, compared to the case that uses \\\\(X_{CES}^o\\\\) as the gold standard. When \\\\(M = 1\\\\), in the \\\"ROG-Oracle\\\" subcolumn, an interesting result is that Multi-DYLE\\\\((X_{ROG}^o \\\\cup X_{CES}^o)\\\\) achieves the best F1 result and ROUGE score, meaning...\"}"}
{"id": "acl-2023-long-731", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-DYLE (# Top-K ROUGE)\u2191 ROG-Oracle (P/R/F1) CES-Oracle (P/R/F1)\\nExt1 Ext2 Merged (R-1/R-2/R-L) Ext1 Ext2 Merged Ext1 Ext2 Merged...\\n\\nMulti-DYLE (XROG o)a\u20dd30 - 30\\n35.93/11.24/31.26\\n8.22/40.89/12.81\\n-\\n8.22/40.89/12.81\\n8.00/37.96/12.55\\n-\\n8.00/37.96/12.55\\n\\nMulti-DYLE (XCES o\u222aXPES o)30 - 30\\n36.49/11.56/31.67\\n8.35/44.95/13.43\\n-\\n8.35/44.95/13.43\\n11.55/51.46/17.82\\n-\\n11.55/51.46/17.82\\n\\nMulti-DYLE (XCES o)b\u20dd30 - 30\\n36.63/11.81/31.82\\n8.16/45.10/13.21\\n-\\n8.16/45.10/13.21\\n11.77/52.28/18.13\\n-\\n11.77/52.28/18.13\\n\\nMulti-DYLE (XROG o\u222aXCES o)30 - 30\\n36.93/12.18/32.49\\n9.25/47.56/14.65\\n-\\n9.25/47.56/14.65\\n10.89/49.33/16.87\\n-\\n10.89/49.33/16.87\\n\\nMulti-DYLE (XROG o,XCES o\u222aXPES o)15 15 30\\n37.10/12.19/32.56\\n11.55/31.13/15.46\\n10.58/30.86/14.81\\n9.84/42.28/15.11\\n11.53/28.54/15.33\\n16.16/38.10/21.10\\n12.03/44.23/17.98\\n\\nMulti-DYLE (XROG o,XCES o)c\u20dd15 15 30\\n37.55/12.43/32.76\\n12.55/31.92/16.63\\n10.49/30.33/14.55\\n10.23/41.82/15.53\\n12.05/29.84/16.01\\n16.92/39.43/21.93\\n12.66/45.97/18.79\\n\\nTable 3: Meeting summarization results of Multi-DYLE (X\u03b1 o) when M = 1 and Multi-DYLE (X\u03b1 o,X\u03b2 o) when M = 2 on test sets of QMSum, varying different settings of extractive oracles X\u03b1 o and X\u03b2 o under ROUGE scores as summarization metric (4th column, named ROUGE), and P/R/F1 scores as the extraction performance when XROG o (5th column, named ROG-Oracle) and XCES o (6th column, named CES-Oracle) are used as gold sets.\\n\\nThat when XCES o is used for an additional training set of an extractor, it has a positive impact on extracting XROG o. Although Multi-DYLE(XCES o) shows weak performance in correctly extracting XROG o, it shows better ROUGE scores than Multi-DYLE(XROG o).\\n\\nWhen M = 2, Multi-DYLE(XROG o,XCES o) slightly improves the performance of Multi-DYLE(XROG o,XCES o\u222aXPES o), indirectly indicating that the additional use of XPES o for training an extractor does not lead to further improvement in summarization.\\n\\nOverall, the results confirm that a strong correlation exists between ROUGE and F1 scores, enabling us to reasonably predict whether the model improves, based on the F1 scores of the extractors. The extractor trained only on XROG o does not exhibit the best performance in extracting XROG o, whereas the additional use of XCES o is complementary in identifying XROG o.\\n\\n6.2 Explainable Evidence Extraction\\n\\nTable 4 compares the results of our baseline EE model for E3, described in Section 5.2. Here, we use the gold setting in which a gold summary is assumed to be provided for EE.\\n\\nWhen extracting evidence sentences for each summary sentence, we have three types of filtering methods based on the classification probabilities for all candidate sentences in the meeting transcript X = (xi)Li=1:\\n\\n\u2022 Threshold-based method (i.e., thr-\u03b8): selects a sentence as an evidence sentence when its classification probability is larger than the threshold \u03b8.\\n\u2022 Top-R method (i.e., top-R): selects R sentences with the highest classification probabilities.\\n\u2022 Hybrid method (i.e., thr-\u03b8 & top-R): first applies thr-\u03b8 and then conditionally performs top-R when no sentence with thr-\u03b8 is selected.\\n\\nthr-1.0 & top-R is equivalent to top-R.\\n\\nAs shown in Table 4, in terms of the sentence-level E3 metric (in the upper part), the threshold-based methods (i.e., thr-\u03b8) show higher F1 scores than the top-R methods (i.e., top-R).\\n\\nDespite its superiority, thr-\u03b8 often suffers from its low recall; in our preliminary analysis, we observed the cases where no sentence was selected, given a threshold. Given that top-R is relatively strong in terms of the recall metric, the hybrid method (i.e., thr-\u03b8 & top-R) further increases performances, leading to achieve the best F1 score of 52.91. Similar results are observed in the summary-level metric in the lower part of Table 4; the hybrid method shows the best performance, outperforming both individual methods of top-R and thr-\u03b8, although the results are not fully presented.\\n\\nIn Table 4, as mentioned in its caption, the \\\"sentence\\\"-level P/R/F1 scores for E3 are computed based on the gold sets are defined per summary \\\"sentence,\\\" resulting in the two-stage macro-averaged score, i.e., the sentence-level scores are first averaged per query and then further macro-averaged across queries. The \\\"summary\\\"-level P/R/F1 scores for E3 use the gold sets defined per \\\"summary,\\\" resulting in the single-stage macro-averaged score, i.e., the summary-level scores are macro-averaged across queries.\"}"}
{"id": "acl-2023-long-731", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: E3 results of the baseline EE model on test sets of the gold evidences in ExplainMeetSum, varying the filtering options among the threshold-based selection ($thr-\\\\theta$), the top-$R$ selection ($top-R$), and its hybrid method ($thr-\\\\theta \\\\& top-R$). 1) Upper: Sentence-level P/R/F1 scores where ground-truth CESs and PESs are defined per gold summary \\\"sentence.\\\" 2) Lower: Summary-level P/R/F1 scores, where ground-truth CESs and PESs are defined per gold \\\"summary.\\\"\\n\\nIn Table 4, the subcolumns in \\\"# Evidence Extraction\\\" named 'Mean' and 'STD' indicate the mean and standard deviation of the number of the extracted evidence sentences, respectively, and 'diff' refers to the absolute difference between the mean numbers of extracted evidence sentences and gold ones. It is clearly seen that 'diff' strongly correlates with the F1 scores of the E3 extractor. Overall, the results show that the hybrid method produces the best F1 scores and these performances are correlated with how closely the number of extracted sentences is distributed to that of gold sentences.\\n\\n6.3 Joint Evaluation of Summarization and E3\\n\\nIn this section, we evaluate a pipelined model consisting of Multi-DYLE and the baseline EE model. To jointly evaluate a generated summary and its aligned evidence sentences in a single metric, we adopt ROUGE scores by merely viewing the addressed task as a unified sequence \\\"generation\\\" task. To be more specific, we first obtain a unified sequence from a summary and its aligned evidence sentences. With abuse of notation, suppose that $S = (s_1, \\\\cdots, s_N)$ is a given summary and $X = (X_e)_{i=1}^N$ is a list of $N$ evidence sentence collections where $X_e = (x_{i1}, \\\\cdots, x_{im})$ is a sequence of $m_e$ evidence sentences aligned to explain the $i$-th summary sentence $s_i \\\\in S$. The conversion process $\\\\psi(S, X)$ is defined as follows:\\n\\n$$\\\\psi(S, X) = \\\\bigoplus_{i=1}^N (s_i \\\\oplus x_{i1} \\\\oplus \\\\cdots \\\\oplus x_{im} \\\\oplus \\\"\\\\n\\\")$$\\n\\nwhere $\\\\oplus$ is the concatenation operator and \\\"\\\\n\\\" indicates a newline character, making $\\\\psi(S, X)$ comprise of $N$ sentences. Under this conversion process $\\\\psi$, we compute ROUGE scores by matching an output sequence resulting from the application of Multi-DYLE and the baseline EE model with its corresponding ground-truth sequence. To distinguish ROUGE scores in meeting summarization, we use Joint ROUGE scores to indicate the unified ROUGE scores evaluated in the joint setting.\\n\\nTable 5 presents comparison results of the pipelined system of Multi-DYLE and the baseline EE model with the hybrid selection method $thr-0.9 \\\\& top-5$, varying sets of extractive oracles, evaluated by Joint ROUGE, as well as the evaluation metrics for meeting summarization and E3. It should be noted that the evaluation of E3 in Table 5 is the (indirectly) joint setting based on automatically generated summaries by Multi-DYLE, unlike the gold setting in Table 4.\\n\\nThe results show that Multi-DYLE($X_{ROG} o, X_{CES} o$) with dual extractors again achieves the best performance under the joint settings involving E3, exhibiting increases of approximately 2 in the Joint ROUGE scores and of approximately 4 in the F1 score of E3, compared to those of...\"}"}
{"id": "acl-2023-long-731", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Example of ExplainMeetSum of comparing the summary and evidence sentences between the predicted results and gold ones for the query in QMSum (i.e., the transcript name: IS1006c, the query id: specific-6); G-Sent-n/P-Sent-n refer to n-th gold/predicted summary sentence. More examples of the summary sentences with the extracted evidence sentences for the same query are presented in Tables 8 and 13.\\n\\nMulti-DYLE(XROGo,XCESo). The performance gain obtained through both Multi-DYLE(XROGo,XCESo) and the EE model tends to be further enlarged compared to the gain of ROUGE only by Multi-DYLE(XROGo,XCESo) for meeting summarization.\\n\\nOverall, Multi-DYLE(XROGo,XCESo) shows the best performance on all evaluation metrics in meeting summarization and E3, achieving noticeable improvements particularly at joint settings involving E3.\\n\\n6.4 Case Study\\nAs an illustrated example for the query \\\"Summarize the discussion about trend watching and appearance design\\\" in QMSum, Figure 3 presents some of the predicted summary sentences using Multi-DYLE(XROGo,XCESo) (i.e., P-Sent-n), and their evidence sentences extracted using the hybrid EE model with thr0.9&top5, in comparison with the gold summary sentences (i.e., G-Sent-n) and their human-aligned CESs. As shown in Figure 3, the generated summary sentences, P-Sent-1\u223c2 are semantically matched well with the gold summary ones, (i.e., G-Sent-1\u223c2), by including the common keywords such as \\\"marketing,\\\" \\\"three aspects,\\\" \\\"first,\\\" and \\\"fancy.\\\" Importantly, all CESs (i.e., 403-4 and 403-5) for the gold summary are correctly extracted in the predicted evidence sentences for P-Sent-1\u223c2, confirming that P-Sent-1\u223c2 is a high quality summary which is supported by the human-aligned CESs.\\n\\n7 Conclusion\\nIn this paper, we presented a novel ExplainMeetSum, as an explainability-enhanced QMSum by providing complete manual annotation of two types of evidence sentences, CES and PES, as explanations to faithfully support or explain each sentence in gold summaries. Equipped with ExplainMeetSum, we proposed Multi-DYLE as a generalized DYLE to enable the addition of an explainable extractor based on CES-based extractive oracles. We further defined a novel task, E3, which aims to extract explainable evidence sentences when a summary sentence is given. The experimental results obtained on QMSum using ExplainMeetSum showed that the proposed Multi-DYLE based on an additional extractor towards a human-aligned explanation outperformed DYLE and led to improvements in the joint evaluation settings involving E3. In future work, we would like to invent a joint learning framework for meeting summarization and E3, extensively employing human-supervised explainable signals from ExplainMeetSum, towards better explainable meeting summarization. Furthermore, developing a novel joint evaluation metric for meeting summarization and E3 to overcome the limitations of the ROUGE-driven scores would be worthwhile.\"}"}
{"id": "acl-2023-long-731", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThis paper presents ExplainMeetSum to enhance the explainability of meeting summarization and provides Multi-DYLE as a generalized version of DYLE by employing multiple extractors. One limitation of our work is the restricted exploration of using ExplainMeetSum for meeting summarization. Although we propose the use of multiple extractors, it can go beyond DYLE\u2019s extractive-generator framework, thereby extending and generalizing other extract-then-generate methods, such as Dou et al. (2021). In addition, we currently use single and dual extractors (i.e., $M = 1$ or $M = 2$) for Multi-DYLE. However, other advanced settings using more extractors ($M > 2$) were not examined in this experiment.\\n\\nAnother limitation is the current joint evaluation metrics, such as precision, recall, F1 scores, and Joint ROUGE scores, adopted as initial trials of evaluating the \u201csummarize-then-explain\u201d joint setting. In particular, Joint ROUGE scores inherit the limitations of the original ROUGE scores. An important remaining issue is to design a more stable and agreeable joint evaluation metric that can be used as a standard evaluation metric for the joint setup of the summarize-then-explain task.\\n\\nFurthermore, our current applications using ExplainMeetSum are limited to meeting summarization and E3. However, ExplainMeetSum can be used as a suitable benchmark dataset to compare various interpretable and explainable models on summarization results. Given the emerging importance of interpretable and explainable models, arguably valuable work is to extensively examine the usefulness of ExplainMeetSum in more interpretable-related tasks by exploring and evaluating interpretable models, such as Ribeiro et al. (2016); Lundberg and Lee (2017); Sundararajan et al. (2017); Sanyal and Ren (2021); Saha et al. (2022).\\n\\nSome parts of the annotations in ExplainMeetSum were not fully utilized in this work. We also annotated the evidence sentences for General(long) types of queries in AMI and ICSI, however, our work on meeting summarization used only QMSum as a benchmark dataset. Thus, it would be valuable to obtain additional results using ExplainMeetSum for meeting summarization in AMI and ICSI to examine whether the use of ExplainMeetSum leads to improvements in other types of datasets.\"}"}
{"id": "acl-2023-long-731", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-731", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset Construction\\n\\nA.1 Annotation Process\\n\\nExplainMeetSum was built on top of the QMSum dataset. Here, we choose QMSum because its dataset is one of the most widely used datasets and its further annotated dataset, ExplainMeetSum, is also invaluable, which is likely to be of wide research interest. For annotation, we recruited four annotators and a coordinator based on their proficiency in English and prior experience with English dataset annotations. To create ExplainMeetSum, they were required to select and annotate evidence sentences from transcripts that are considered as aligned to \u201cexplain\u201d well each gold sentence summary in the approximately 2,000 queries distributed among 232 transcripts; once selecting an evidence sentence, an annotator labeled its main type, CES and PES, as well as matching characteristics (i.e., subtypes of CES and PES).\\n\\nDespite QMSum being relatively limited in its size, the annotation work is time-consuming because the annotation tasks are non-trivial and involve extensive revisions, based on multiple feedbacks and comments provided by the coordinator. Considering the large amount of annotation work, we set aside more than six months for this task based on the two-level feedback pipeline on which the annotators interacted with both a coordinator and an expert. The coordinator thoroughly checked the annotation process and interacted with the annotators, while the expert acted as a meta-reviewer who periodically inspected random samples in-depth, provided feedback, and updated the guidelines. The total cost of the annotator\u2019s work was $40,000 which equates to an estimation of $20 per query.\\n\\nA.2 Examples in ExplainMeetSum\\n\\nTable 7 shows examples in ExplainMeetSum collected across various queries in QMSum. Being categorized with main types and matching characteristics (i.e., subtypes), CESs and PESs aligned for each of the gold summary sentences indicated by \u2019[G-Sent]\u2019 are presented in the column \u2019Evidence Sentence.\u2019 Matching characteristics are classified into six labels: CES has exact, semantic, and supportive subtypes, whereas PES has illustrative, introductory, and connective subtypes, defined as follows:\"}"}
{"id": "acl-2023-long-731", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 CES/exact: A subtype of CES whose key-words are \\\"exactly\\\" matched with the contents in the target sentence of the summary.\\n\\n\u2022 CES/semantic: A subtype of CES whose key-words \\\"semantically\\\" match the contents in the target sentence of the summary.\\n\\n\u2022 CES/supportive: A subtype of CES that includes information or examples that directly support or entail abstractive expressions in the target sentence of the summary.\\n\\n\u2022 PES/illustrative: A subtype of PES that includes relevant information or examples, such as enumerating further detailed information in addition to CES, on which a reasoning step is required to match the target sentence of the summary.\\n\\n\u2022 PES/introductory: A subtype of PES, which includes relevant information that initiates the subject of the extracted evidences; it usually appears before CES.\\n\\n\u2022 PES/connective: A subtype of PES that includes relevant information used to build the connectivity between the extracted evidences; it is usually based on conjunctions that connect two subsequent CESs.\\n\\nAs a full annotation example, Table 8 presents CESs and PESs for the query \\\"Summarize the discussion about trend watching and appearance design\\\" in QMSum (i.e., the transcript name: IS1006c, the query id: specific-6), where the 'Type/subtype' column shows the main type of evidence sentences (i.e., CES or PES) and their matching characteristic, the 'Evidence Sentences' column presents the aligned CESs or PESs in the meeting transcript, and the 'Turn-sent Index' column shows the turn-based sentence identifier that is defined as a pair of turn and sentence-level indexes.\\n\\nA.3 Quality Control Methods\\nTo access the quality of the annotation, we established a two-level feedback-driven annotation process that was supervised and monitored by a coordinator and an expert (as a meta-reviewer) as follows:\\n\\n\u2022 Two-level coordinator-expert feedback process: 1) a coordinator periodically checks the annotation results such that they are continuously revised to sufficiently fulfill the high-level of quality, and frequently communicates by an expert. 2) an expert regularly inspects random samples of the annotation, provides feedback to the coordinator, and updates the guideline.\\n\\nWe further applied a series of test suites to check the annotation quality semi-automatically, as follows:\\n\\n\u2022 Comparing labeling statistics across annotators: We periodically compare labeling statistics from annotators, and reexamine annotation results when some statistics are significantly different from the others.\\n\\n\u2022 Computing neural similarities between aligned evidence sentences and summary ones: We compare similarities using the SBERT model between candidate sentences and a summary sentence, under the assumption that the exact, semantic, supportive CES, PES, and others have the highest degree of similarity in that order. For the SBERT model, we used the sentence transformer model from Hugging Face's 'all-MiniLM-L6-v2' model to compute similarities between candidate sentences and a summary sentence.\\n\\nTable 9 presents examples of how initial tagging errors are revised correctly via the test-suite based on neural similarities between evidence and summary sentences. As in the table, given evidence sentences initially labeled by annotators, the coordinator is further provided with the SBERT-based similarities between evidence sentences and the gold summary sentences. When their similarities are abnormally large or small compared to the average similarity values of their subtypes, the coordinator re-examines the abnormal cases and revises them correctly. In the first row, for example, an annotator initially labeled the type \\\"CES/supportive\\\" for the given evidence sentence. However, its\"}"}
{"id": "acl-2023-long-731", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SBERT-based similarity with the gold one is 72.20, which is \\\"abnormally\\\" high considering the average of CES/supportive-typed cases. Once this abnormal neural similarity was detected and alarmed, a coordinator carefully looked into the problematic evidence sentence, identified that its subtype was wrongly labeled, and finally, correctly revised the label to \\\"CES/semantic.\\\" Similar revisions were made in other two rows.\\n\\nLabeling matching characteristics: In our work, CES and PES are the main types of evidence sentences only required for meeting summarization and E3 tasks, whereas matching characteristics were not necessary for these tasks. Importantly, our intention on labeling matching characteristics (i.e., subtypes of CES and PES) is to provide an additional quality control suite, thus making annotator carefully examine sentences in more depth to reduce errors in annotating CES and PES.\\n\\nB Extractive Oracles: Turn-level and Sentence-level\\n\\nIn Table 2, we evaluated DYLE based on two types of extractive oracles \u2014 turn-level and sentence-level ones \u2013 and showed that DYLE equipped with sentence-level extractive oracles exhibited improvements over the case using turn-level oracles. To clearly demonstrate the difference between these types of extractive oracles, Tables 10 and 11 show the turn-level and sentence-level ROUGE-based extractive oracles obtained for the same query in Table 8, where bold-faced index refers to human-annotated aligned CESs. It is clearly seen that the resulting sentences are different in two types of oracles; while some turns, including simple ones that consist of a single sentence, appear in both turn-level and sentence-level oracles (i.e., 392-nd, 405-th, 427-th, and 438-th turns), most turns are not shared across them. In particular, even if a turn is shared between two oracles, a small number of sentences tend to commonly appear, as in the 405-th turn and its sentences.\\n\\nC Summarization and E3 Tasks\\n\\nFigure 4 presents the overall framework of the proposed architecture that performs meeting summarization and E3 tasks. In the upper part, the Multi-DYLE in Section 4 is deployed to perform the summarization task, where the Multi-DYLE of $M=1$ can be replaced with other variants of Multi-DYLE ($X^\\\\alpha_0$, $X^\\\\beta_0$). In the lower part, the EE model in Sections 5 and 6.2 is employed to address the E3 task. Although Multi-DYLE includes its evidence extractor, we used a separate EE model to address the E3 task. During inference, given\"}"}
{"id": "acl-2023-long-731", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a test query and meeting script, Multi-DYLE first generates a summary, and the EE model extracts evidence sentences for each generated summary sentence by computing their relevance scores and applying filtering methods, as described in Sections 5 and 6.2.\\n\\nAs illustrated examples, Tables 12 and 13 present the extracted evidence sentences and their extraction performances when using the Multi-DYLE's extractors and the EE model, given the query in Table 8. Note that CESs are used for gold evidence sentences in Table 12, whereas a union of CESs and PESs is used for gold ones in Table 13, and thus these scores are not fairly comparable.\\n\\nGiven the differences in the evaluation settings, the extraction score in Table 12 is considerably higher than that in Table 13.\\n\\nIn Table 12, the ROG-based and CES-based extractors refer to those induced from Multi-DYLE\\\\(^{(X_{ROG}o, X_{CES}o)}\\\\). In Table 13, the row 'Generated Summary' presents a generated summary, and the row 'Extracted Explainable Evidence' describes the evidence sentences extracted by the EE model for each sentence, P-Sent\\\\(_n\\\\), in the generated summary.\\n\\nThe performances in Table 13 are per-query scores of the meeting summarization and E3 task, which is computed specifically for the query in Table 8; 'Summarization performance' indicates the per-query ROUGE score under the standard summarization metric, and 'E3 performance' refers to the per-query extraction (\\\"summary-level\\\") and Joint ROUGE scores as the joint evaluation metric, as in Section 6.3 and Table 5.\\n\\nD Implementation Details\\n\\nThe proposed framework is implemented by extending the base DYLE model. To train Multi-DYLE, we explored different models for parameter initialization and used RoBERTa-base and DYLE(generator) to initialize the extractor and generator modules, respectively, as they show reliable performance in Table 6. To train the EE model, we fine-tuned the RoBERTa-base model with an Adam optimizer, learning rate of 5e-5, batch size of 8, and a gradient accumulation step of 8. We also utilized the ROUGE package to evaluate the performance of summarization and the NLTK library to preprocess the dataset.\\n\\nWe used an RTX 6000 NIVIDA GPU with a Multi-DYLE (Initialization Models) quadro architecture, which allowed us to run all models using the Pytorch library. To ensure the reliability of our results, we performed five distinct experimental runs, each with a different random seed, and stored the checkpoints with the maximum evaluation score.\"}"}
{"id": "acl-2023-long-731", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Central Evidence Sentence (CES)\\n\\n(1) exact\\n\\n[G-Sent] They also decided to start with basic functions and then move on to the more advanced feature.\\n\\nCES Marketing: Well, should we start with just the core, the basic functions that we need. And then we can move on to the more advanced features.\\n\\n(2) semantic\\n\\n[G-Sent] The project manager briefed the team on some new requirements and initiated a discussion in which the team discussed and decided on various features to include in the remote they will produce.\\n\\nCES Project Manager: So um I have to inform you I receive an email from the management board today and they have new requirements for the remote control. Um\\n\\n(3) supportive\\n\\n[G-Sent] The project manager opened the meeting and then the marketing expert discussed user requirements.\\n\\nCES Marketing: Okay, so basically I'm gonna present some findings of a study we conducted uh into what users want in this remote control.\\n\\nPeripheral Evidence Sentence (PES)\\n\\n(1) illustrative\\n\\n[G-Sent] The project manager briefed the team on some new requirements and initiated a discussion in which the team discussed and decided on various features to include in the remote they will produce.\\n\\nCES Project Manager: So um I have to inform you I receive an email from the management board today and they have new requirements for the remote control. Um\\n\\nPES Project Manager: first um, they say that's uh about something about teletext. Um the second thing is uh they suggest that we should use the remote control only for TV, not for DVD and other devices.\\n\\nPES Project Manager: the third one is uh about the image of the company.\\n\\n(2) introductory\\n\\n[G-Sent] The remote will have buttons for channel changing, volume settings, numerals, and power on/off.\\n\\nCES Project Manager: but first maybe what is what are the usual function of a standard remote control?\\n\\nCES Marketing: Okay, well, I mean the obvious one is changing channels.\\n\\n(3) connective\\n\\n[G-Sent] Whether using radio waves will interfere with other technology a user owns.\\n\\nCES Marketing: Do you think radio waves um will interfere with other appliances in the home?\\n\\nPES User Interface: Uh, I don't think so, because uh we can make the wave in a specific frequency.\\n\\nPES User Interface: so they can be in a range which is not interfering with the other devices inside the home.\"}"}
{"id": "acl-2023-long-731", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G-Sent A\\n\\ncurriculum reform was to carry out throughout Wales.\\n\\nEvidence Sentence\\nMeilyr Rowlands: So, we need to ensure that those qualifications are reformed as a result of the reform of the curriculum, and, of course, Qualifications Wales is carrying out that work currently.\\n\\nG-Sent\\n\\nWhen it comes to continuing mental health service during the lockdown, Vaughan Gething insisted that it was of great necessity to carry out a mental health recovery plan that with such a system, government can ensure the children could enjoy a healthy mental state during the school lockdown.\\n\\nEvidence Sentence\\nVaughan Gething: AM: So, children's mental health was a central concern and remains so for both myself and the education Minister.\\n\\nG-Sent\\n\\nWhen discussing the governmental issue of dealing with systematic racism, Justin Trudeau mentioned that actually there had been serious systematic racism in most national institutions for the past two years, so he called for a revolution in those organizations to welcome equal cooperation with the black colleagues and indigenous communities.\\n\\nEvidence Sentence\\nMr. Jagmeet Singh: Is the Prime Minister committed to a full-scale overhaul of the RCMP to root out systemic racism?\"}"}
{"id": "acl-2023-long-731", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marketing: it has to be fancy.\\n\\nIndustrial Designer: we want to follow general trend.\\n\\nMarketing: first maybe just a small recap on how do we watch trends...\\n\\nMarketing: It's fancy.\\n\\nMarketing: The first one, which seems to be the most important one, is that it has to be fancy, it has to have a fancy look and feel.\\n\\nIndustrial Designer: it's not particular to the remote control.\\n\\nMarketing: When we were talking about... more general trend\\n\\nIndustrial Designer: Or it's...\\n\\nIndustrial Designer: we will try to explore these two options\\n\\nMarketing: The the young people want to be different from their friends.\\n\\nMarketing: and uh so I'm just asking them what are the current trends according to them when they go in the stores and when they ask uh their uh friends\\n\\nMarketing: but uh basically there are uh in in the market of of remote controls there are three aspects that we should very pay much attention to.\\n\\nMarketing: It has this distinctive look and feel and look\\n\\nMarketing: currently the the trends that we see in l in l big cities like Paris and Milan, well, it seems that... uh way of of look or feel\\n\\nMarketing: We have to I think we have to have the look of fruit and vegetables.\\n\\nUser Interface: But you know if you want to be different you just take your remote control with you all the time.\\n\\nMarketing: And finally of course it has to be useful as a remote control\\n\\nMarketing: The first one, which seems to be the most important one, is that it has to be fancy, it has to have a fancy look and feel.\\n\\nMarketing: The young people want to be different from their friends.\\n\\nUser Interface: But you know if you want to be different you just take your remote control with you all the time.\\n\\nMarketing: But uh basically there are uh in in the market of of remote controls there are three aspects that we should very pay much attention to.\\n\\nMarketing: but uh basically there are uh in in the market of of remote controls there are three aspects that we should very pay much attention to.\\n\\nMarketing: and uh so I'm just asking them what are the current trends according to them when they go in the stores and when they ask uh their uh friends\\n\\nMarketing: but uh basically there are uh in in the market of of remote controls there are three aspects that we should very pay much attention to.\\n\\nMarketing: It has this distinctive look and feel and look\\n\\nMarketing: currently the the trends that we see in l in l big cities like Paris and Milan, well, it seems that... uh way of of look or feel\\n\\nMarketing: We have to I think we have to have the look of fruit and vegetables.\"}"}
{"id": "acl-2023-long-731", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A1. Did you describe the limitations of your work?\\nYes, it is discussed in Limitations Section.\\n\\nA2. Did you discuss any potential risks of your work?\\nNo, there are no potential risks of our work, as our work made additional annotation in the widely-used QMSum dataset and addressed the meeting summarization and evidence extraction tasks.\\n\\nA3. Do the abstract and introduction summarize the paper\u2019s main claims?\\nYes, the paper\u2019s main claim is stated in Abstract and Section 1.\\n\\nA4. Have you used AI writing assistants when working on this paper?\\nNo, we have not used AI writing assistants.\\n\\nB Did you use or create scientific artifacts?\\nYes, the artifacts for the dataset are discussed in Section 3, with its URL in Abstract, and the model part is described in Sections 4 and 5.\\n\\nB1. Did you cite the creators of artifacts you used?\\nYes, we have cited the creators of artifacts in Section 1.\\n\\nB2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nNo, we have cited the artifacts, but did not explicitly discuss the license or terms for use as it is understood in the NLP field to share or utilize related artifacts. For our dataset, we will specify the license name in the URL, when available.\\n\\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nYes, we have repeatedly discussed and specified the intended use of existing artifacts for our artifacts in Sections 1, 2, 3 and 4. The details of the datasets we used are presented in Section 3.\\n\\nB4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nNo, we do not reveal any new contents in our artifacts, because our annotation was made on the contents in the QMSum dataset. Thus, there is no new critical information in our dataset, such as individual people and names.\\n\\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nYes, we have provided basic information about the data in Section 3.\\n\\nB6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nYes, we have provided basic information about the existing and created data in Section 3.\"}"}
{"id": "acl-2023-long-731", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Did you run computational experiments?\\n\\nYes, we have described results of various computational experiments in Section 6.\\n\\nDid you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nYes, we have provided implementation details in Appendix D.\\n\\nDid you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nYes, the experimental setup for the best-performing model, including the used hyperparameters for the Multi-DYLE and Evidence Extraction (EE) model, are presented in Tables 2-4 in Section 6. The additional details on the setup are provided in Appendix D.\\n\\nDid you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nYes, we have provided the basic statistics such as the number of runs tried in Appendix D.\\n\\nIf you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nYes, we have provided it in Appendix D.\\n\\nDid you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nYes, we used human annotators to build our dataset, as in Section 3 and Appendix A.1.\\n\\nDid you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nYes, but instead of providing separate instructions for annotators, we established the feedback-based annotation protocol, leading by a coordinator and an expert, which are frequently communicated with the annotators, as in Appendix A.1 and A.3.\\n\\nDid you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nYes, we have provided the detailed annotation process in Appendix A.1.\\n\\nDid you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNo, the annotators we recruited only performed the labeling work on the QMSum. No new text which require agreement was produced during the process.\\n\\nWas the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNo, the necessary verification has already been performed at the source of the data. We directly use the original QMSum dataset without making any changes to it.\\n\\nDid you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNo, the necessary verification has already been reported at the source of the data.\"}"}
