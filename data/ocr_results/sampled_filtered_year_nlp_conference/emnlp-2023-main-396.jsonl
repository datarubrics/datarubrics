{"id": "emnlp-2023-main-396", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Advancements in Arabic Grammatical Error Detection and Correction: An Empirical Investigation\\n\\nBashar Alhafni, Go Inoue, Christian Khairallah, Nizar Habash\\n\\nComputational Approaches to Modeling Language Lab\\nNew York University Abu Dhabi\\n\u2020\\nMohamed bin Zayed University of Artificial Intelligence\\n{alhafni,christian.khairallah,nizar.habash}@nyu.edu\\ngo.inoue@mbzuai.ac.ae\\n\\nAbstract\\n\\nGrammatical error correction (GEC) is a well-explored problem in English with many existing models and datasets. However, research on GEC in morphologically rich languages has been limited due to challenges such as data scarcity and language complexity. In this paper, we present the first results on Arabic GEC using two newly developed Transformer-based pretrained sequence-to-sequence models. We also define the task of multi-class Arabic grammatical error detection (GED) and present the first results on multi-class Arabic GED. We show that using GED information as an auxiliary input in GEC models improves GEC performance across three datasets spanning different genres. Moreover, we also investigate the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared task datasets and establish a strong benchmark on a recently created dataset. We make our code, data, and pretrained models publicly available.\\n\\n1 Introduction\\n\\nEnglish grammatical error correction (GEC) has witnessed significant progress in recent years due to increased research efforts and the organization of several shared tasks (Ng et al., 2013, 2014; Bryant et al., 2019). Most state-of-the-art (SOTA) GEC systems borrow modeling ideas from neural machine translation (MT) to translate from erroneous to corrected texts. In contrast, grammatical error detection (GED), which focuses on locating and identifying errors in text, is usually treated as a sequence labeling task. Both tasks have evident pedagogical benefits to native (L1) and foreign (L2) language teachers and students. Also, modeling GED information explicitly within GEC systems yields better results in English (Yuan et al., 2021).\\n\\nWhen it comes to morphologically rich languages, GEC and GED have not received as much attention, largely due to the lack of datasets and standardized error type annotations. Specifically for Arabic, the focus on GEC started with the QALB-2014 (Mohit et al., 2014) and QALB-2015 (Rozovskaya et al., 2015) shared tasks; however, recent sequence-to-sequence (Seq2Seq) modeling advances have not been explored much in Arabic GEC. Moreover, multi-class Arabic GED has not been investigated due to the lack of error type information in Arabic GEC datasets. In this paper, we try to address these challenges. Our main contributions are as follows:\\n\\n1. We are the first to benchmark newly developed pretrained Seq2Seq models on Arabic GEC.\\n2. We tackle the task of Arabic GED by introducing word-level GED labels for existing Arabic GEC datasets, and present the first results on multi-class Arabic GED.\\n3. We systematically show that using GED information in GEC models improves performance across GEC datasets in different domains.\\n4. We leverage contextual morphological preprocessing in improving GEC performance.\\n5. We achieve SOTA results on two (L1 and L2) previously published Arabic GEC datasets. We also establish a strong benchmark on a recently created L1 Arabic GEC dataset.\\n\\n2 Related Work\\n\\nGEC Approaches\\n\\nEarly efforts focused on building feature-based machine learning (ML) classifiers to fix common error types (Chodorow et al., 2007; Tetreault and Chodorow, 2008; Dahlmeier and Ng, 2011; Kochmar et al., 2012; Rozovskaya and Roth, 2013; Farra et al., 2014). Such models required feature engineering and lacked the ability to correct all error types simultaneously.\\n\\nReformulating GEC as a monolingual MT task alleviated these issues, first with statistical MT approaches.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"proaches (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014, 2016) and then neural MT approaches (Yuan and Briscoe, 2016; Xie et al., 2016; Junczys-Dowmunt et al., 2018; Watson et al., 2018), with Transformer-based models being the most dominant (Yuan et al., 2019; Zhao et al., 2019; Grundkiewicz et al., 2019; Katsumata and Komachi, 2020; Yuan and Bryant, 2021).\\n\\nMore recently, edit-based models have been proposed to solve GEC (Awasthi et al., 2019; Malmi et al., 2019; Stahlberg and Kumar, 2020; Mallinson et al., 2020; Omelianchuk et al., 2020; Straka et al., 2021; Mallinson et al., 2022; Mesham et al., 2023). While Seq2Seq models generate corrections to erroneous input, edit-based models generate a sequence of corrective edit operations. Edit-based models add explainability to GEC and improve inference time efficiency. However, they generally require human engineering to define the size and scope of the edit operations (Bryant et al., 2023).\\n\\nGED Approaches\\nRei and Yannakoudakis (2016) presented the first GED results using a neural approach framing GED as a binary (correct/incorrect) sequence tagging problem. Others used pretrained language models (PLMs) such as BERT (Devlin et al., 2019), ELECTRA (Clark et al., 2020), and XLNeT (Yang et al., 2019) to improve binary GED (Bell et al., 2019; Kaneko and Komachi, 2019; Yuan et al., 2021; Rothe et al., 2021). Zhao et al. (2019) and Yuan et al. (2019) demonstrated that combining GED and GEC yields improved results: they used multi-task learning to add token-level and sentence-level GED as auxiliary tasks when training for GEC. Similarly, Yuan et al. (2021) showed that binary and multi-class GED improves GEC.\\n\\nArabic GEC and GED\\nThe Qatar Arabic Language Bank (QALB) project (Zaghouani et al., 2014, 2015) organized the first Arabic GEC shared tasks: QALB-2014 (L1) (Mohit et al., 2014) and QALB-2015 (L1 and L2) (Rozovskaya et al., 2015). Recently, Habash and Palfreyman (2022) created the ZAEBUC corpus, a new L1 Arabic GEC corpus of essays written by university students. We report on all of these sets.\\n\\nArabic GEC modeling efforts ranged from feature-based ML classifiers to statistical MT models (Rozovskaya et al., 2014; Bougares and Bouamor, 2015; Nawar, 2015). Watson et al. (2018) introduced the first character-level Seq2Seq model and achieved SOTA results on the L1 Arabic GEC data used in the QALB-2014 and 2015 shared tasks. Recently, vanilla Transformers (Vaswani et al., 2017) were explored for synthetic data generation to improve L1 Arabic GEC and were tested on the L1 data of the QALB-2014 and 2015 shared tasks (Solyman et al., 2021, 2022, 2023). To the best of our knowledge, the last QALB-2015 L2 reported results were presented in the shared task itself. We compare our systems against the best previously developed models whenever feasible.\\n\\nA number of researchers reported on Arabic binary GED. Habash and Roth (2011) used feature-engineered SVM classifiers to detect Arabic handwriting recognition errors. Alkhatib et al. (2020) and Madi and Al-Khalifa (2020) used LSTM-based classifiers. None of them used any of the publicly available GEC datasets mentioned above to train and test their systems. In our work, we explore multi-class GED by obtaining error type annotations from ARETA (Belkebir and Habash, 2021), an automatic error type annotation tool for MSA. To our knowledge, we are the first to report on Arabic multi-class GED. We report on publicly available data to enable future comparisons.\\n\\n3 Background\\n3.1 Arabic Linguistic Facts\\nModern Standard Arabic (MSA) is the official form of Arabic primarily used in education and media across the Arab world. MSA coexists in a diglossic (Ferguson, 1959) relationship with local Arabic dialects that are used for daily interactions. When native speakers write in MSA, there is frequent code-mixing with the dialects in terms of phonological, morphological, and lexical choices (Habash et al., 2008). In this paper, we focus on MSA GEC. While its orthography is standardized, written Arabic suffers many orthographic inconsistencies even...\"}"}
{"id": "emnlp-2023-main-396", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in professionally written news articles (Buckwalter, 2004; Habash et al., 2012). For example, hamzated Alifs (\\\\textbackslash{char0d} \\\\textbackslash{char40} \u02c7A \\\\textbackslash{char0d}) are commonly confused with the un-hamzated letter (\\\\textbackslash{char40} A), and the word-final letters \\\\textbackslash{charf8} y and \\\\textbackslash{charf8} \u00fd are often used interchangeably. These errors affect 11% of all words (4.5 errors per sentence) in the Penn Arabic Treebank (Habash, 2010).\\n\\nAdditionally, the use of punctuation in Arabic is very inconsistent, and omitting punctuation marks is very frequent (Awad, 2013; Zaghouani and Awad, 2016). Punctuation errors constitute \\\\(\\\\sim 40\\\\%\\\\) of errors in the QALB-2014 GEC shared task. This is ten times higher than punctuation errors found in the English data used in the CoNLL-2013 GEC shared task (Ng et al., 2013). Arabic has a large vocabulary size resulting from its rich morphology, which inflects for gender, number, person, case, state, mood, voice, and aspect, and cliticizes numerous particles and pronouns. Arabic's diglossia, orthographic inconsistencies, and morphological richness pose major challenges to GEC models.\\n\\n3.2 Arabic GEC Data\\n\\nWe report on three publicly available Arabic GEC datasets. The first two come from the QALB-2014 (Mohit et al., 2014) and QALB-2015 (Rozovskaya et al., 2015) shared tasks. The third is the newly created ZAEBUC dataset (Habash and Palfreyman, 2022). None of them were manually annotated for specific error types. Table 1 presents a summary of the dataset statistics. Detailed dataset statistics are presented in Appendix B.\\n\\nQALB-2014 consists of native/L1 user comments from the Aljazeera news website, whereas QALB-2015 consists of essays written by Arabic L2 learners with various levels of proficiency. Both datasets have publicly available training (Train), development (Dev), and test (Test) splits. The ZAEBUC dataset comprises essays written by native Arabic speakers, which were manually corrected and annotated for writing proficiency using the Common European Framework of Reference (CEFR) (Council of Europe, 2001). Since the ZAEBUC dataset did not have standard splits, we randomly split it into Train (70%), Dev (15%), and Test (15%), while keeping a balanced distribution of CEFR levels.\\n\\nThe three sets vary in a number of dimensions: domain, level, number of words, percentage of erroneous words, and types of errors. Appendix C presents automatic error type distributions over the training portions of the three datasets. Orthographic errors are more common in the L1 datasets (QALB-2014 and ZAEBUC) compared to the L2 dataset (QALB-2015). In contrast, morphological, syntactic, and semantic errors are more common in QALB-2015. Punctuation errors are more common in QALB-2014 and QALB-2015 compared with ZAEBUC.\\n\\n3.3 Metrics for GEC and GED\\n\\nGEC systems are most commonly evaluated using reference-based metrics such as the MaxMatch (M\\\\textsuperscript{2}) scorer (Dahlmeier and Ng, 2012), ERRANT (Bryant et al., 2017), and GLEU (Napoles et al., 2015), among other reference-based and reference-less metrics (Felice and Briscoe, 2015; Napoles et al., 2016; Asano et al., 2017; Choshen et al., 2020; Maeda et al., 2022). In this work, we use the M\\\\textsuperscript{2} scorer because it is language agnostic and was the main evaluation metric used in previous work on Arabic GEC. The M\\\\textsuperscript{2} scorer compares hypothesis edits made by a GEC system against annotated reference edits and calculates the precision (P), recall (R), and F\\\\textsuperscript{0.5}. In terms of GED, we follow previous work (Bell et al., 2019; Kaneko and Komachi, 2019; Yuan et al., 2021) and use macro precision (P), recall (R), and F\\\\textsuperscript{0.5} for evaluation. We also report accuracy.\\n\\n4 Arabic Grammatical Error Detection\\n\\nMost of the work on GED has focused on English (\u00a72), where error type annotations are provided manually (Yannakoudakis et al., 2011; Dahlmeier et al., 2013) or obtained automatically using an error type annotation tool such as ERRANT (Bryant et al., 2017). However, when it comes to morphologically rich languages such as Arabic, GED remains a challenge. This is largely due to the lack of manually annotated data and standardized error type frameworks. In this work, we treat GED as a multi-class sequence labeling task. We present a method to automatically obtain error type annotations by extracting edits from parallel erroneous and corrected sentences and then passing them to an Arabic error type annotation tool. To the best of our knowledge, this is the first work that explores multi-class GED in Arabic.\\n\\n4.1 Edit Extraction\\n\\nBefore automatically labeling each erroneous sentence token, we need to align the erroneous and\"}"}
{"id": "emnlp-2023-main-396", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An example showing the differences between the alignments of the M\u00b2 scorer, a standard Levenshtein distance, ARETA, and our proposed algorithm. The edit operations are keep (K), replace (R), insert (I), delete (D), merge (M), and split (S). Dotted lines between the erroneous and corrected sentences represent gold alignment. The last three rows present different granularities of ARETA error types based on our alignment. The sentence in the figure can be translated as \\\"Social media must be used wisely, as it has both negative and positive effects corrected sentence pairs to locate the positions of all edits so as to map errors to corrections. This step is usually referred to as edit extraction in GEC literature (Bryant et al., 2017).\\n\\nWe first obtain character-level alignment between the erroneous and corrected sentence pair by computing the weighted Levenshtein edit distance (Levenshtein, 1966) for each pair of tokens in the two sentences. The output of this alignment is a sequence of token-level edit operations representing the minimum number of insertions, deletions, and replacements needed to transform one token into another. Each of these operations involves one token at most belonging to either sentence. However, some errors may involve more than one single edit operation. To capture multi-token edits, we extend the alignment to cover merges and splits by implementing an iterative algorithm that greedily merges or splits adjacent tokens such that the overall cumulative edit distance is minimized.\\n\\n4.2 Error Type Annotation\\n\\nNext, we pass the extracted edits to an automatic annotation tool to label them with specific error types. We use ARETA, an automatic error type annotation tool for MSA (Belkebir and Habash, 2021). Internally, ARETA is built using a combination of rule-based components and an Arabic morphological analyzer (Taji et al., 2018; Obeid et al., 2020). It uses the error taxonomy of the Arabic Learner Corpus (ALC) (Alfaifi and Atwell, 2012; Alfaifi, 2013) which defines seven error classes covering orthography (O), morphology (M), syntax (X), semantics (S), punctuation (P), merges, and splits. The error classes are further differentiated into 32 error tags that can be assigned individually or in combination.\\n\\nARETA comes with its own alignment algorithm that extracts edits, however, it does not handle many-to-one and many-to-many edit operations (Belkebir and Habash, 2021). We replace ARETA's internal alignment algorithm with ours to increase the coverage of error typing. Using our edit extraction algorithm with ARETA enables us to automatically annotate single-token and multi-token edits with various error types. Appendix C presents the error types obtained from ARETA by using our alignment over the three GEC datasets we use.\\n\\nTo demonstrate the effectiveness of our alignment algorithm, we compare our algorithm to the alignments generated by the M\u00b2 scorer, a standard Levenshtein edit distance, and ARETA. Table 2 presents the evaluation results of the alignment algorithms against the manual gold alignments of the datasets.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"QALB-2014 and QALB-2015 Dev sets in terms of precision (P), recall (R), and alignment error rate (AER) (Mihalcea and Pedersen, 2003; Och and Ney, 2003). Results show that our alignment algorithm is superior across all metrics.\\n\\nFigure 1 presents an example of the different alignments generated by the algorithms we evaluated. The $M_2$ scorer's alignment over-clusters multiple edits into a single edit (words 6\u201313). This is not ideal, particularly because the $M_2$ scorer does not count partial matches during the evaluation, which leads to underestimating the models' performances (Felice and Briscoe, 2015). A standard Levenshtein alignment does not handle merges correctly, e.g., words 8 and 9 in the erroneous sentence are aligned to words 9 and 10 in the corrected version. Among the drawbacks of ARETA's alignment is that it does not handle merges, e.g., erroneous words 8 and 9 are aligned with corrected words 9 and 10, respectively.\\n\\n5 Arabic Grammatical Error Correction\\n\\nRecently developed GEC models rely on Transformer-based architectures, from standard Seq2Seq models to edit-based systems built on top of Transformer encoders. Given Arabic's morphological richness and the relatively small size of available data, we explore different GEC models, from morphological analyzers and rule-based systems to pretrained Seq2Seq models. Primarily, we are interested in exploring modeling approaches to address the following two questions:\\n\\n\u2022 RQ1: Does morphological preprocessing improve GEC in Arabic?\\n\u2022 RQ2: Does modeling GED explicitly improve GEC in Arabic?\\n\\nMorphological Disambiguation (Morph)\\n\\nWe use the current SOTA MSA morphological analyzer and disambiguator from CAMeL Tools (Inoue et al., 2022; Obeid et al., 2020). Given an input sentence, the analyzer generates a set of potential analyses for each word and the disambiguator selects the optimal analysis in context. The analyses include minimal spelling corrections for common errors, diacritizations, POS tags, and lemmas. We use the dediacritized spellings as the corrections.\\n\\nMaximum Likelihood Estimation (MLE)\\n\\nWe exploit our alignment algorithm to build a simple lookup model to map erroneous words to their corrections. We implement this model as a bigram maximum likelihood estimator over the training data:\\n\\n$$P(c_i | w_i, w_i-1, e_i);$$\\n\\nwhere $w_i$ and $w_{i-1}$ are the erroneous word (or phrases in case of a merge error) and its bigram context, $e_i$ is the error type of $w_i$, and $c_i$ is the correction of $w_i$. During inference, we pick the correction that maximizes the MLE probability. If the bigram context ($w_i$ and $w_{i-1}$) was not observed during training, we backoff to a unigram. If the erroneous input word was not observed in training, we pass it to the output.\\n\\nChatGPT\\n\\nGiven the rising interest in using large language models (LLMs) for a variety of NLP tasks, we benchmark ChatGPT (GPT-3.5) on the task of Arabic GEC. We follow the setup presented by Fang et al. (2023) on English GEC. To the best of our knowledge, we are the first to present ChatGPT results on Arabic GEC. The experimental setup along with the used prompts are presented in Appendix A.\\n\\nSeq2Seq with GED Models\\n\\nWe experiment with two newly developed pretrained Arabic Transformer-based Seq2Seq models: AraBART (Kamal Eddine et al., 2022) (pretrained on 24GB of MSA data mostly in the news domain), and AraT5 (Nagoudi et al., 2022) (pretrained on 256GB of both MSA and Twitter data).\\n\\nWe extend the Seq2Seq models we use to incorporate token-level GED information during training and inference. Specifically, we feed predicted GED tags as auxiliary input to the Seq2Seq models. We add an embedding layer to the encoders of AraBART and AraT5 right after their corresponding token embedding layers, allowing us to learn representations for the auxiliary GED input. The GED embeddings have the same dimensions as the positional and token embeddings, so all three embeddings can be summed before they are passed to the multi-head attention layers in the encoders. Our approach is similar to what was done by Yuan et al. (2021), but it is much simpler as it reduces the model's size and complexity by not introducing an additional encoder to process GED input. Since the training data we use is relatively small, not drastically increasing the size of AraBART and AraT5 becomes important not to hinder training.\\n\\n6 Experiments\\n\\n6.1 Arabic Grammatical Error Detection\\n\\nWe build word-level GED classifiers using Transformer-based PLMs. From the many avail-\"}"}
{"id": "emnlp-2023-main-396", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: GED results on the Dev and Test sets in terms of macro precision, recall, F\\\\(_0.5\\\\), and accuracy.\\n\\nWe take advantage of the modularity of the ARETA error tags to conduct multi-class GED experiments, reducing the 43 error tags to their corresponding 13 main error categories as well as to a binary space (correct/incorrect). The statistics of the error tags we model across all datasets are in Appendix D. Figure 1 shows an example of error types at different granularity levels. Table 3 presents the GED granularity results. Unsurprisingly, all numbers go up when we model fewer error types. However, modeling more error types does not significantly worsen the performance in terms of error detection accuracy. It seems that all systems are capable of detecting comparable numbers of errors despite the number of classes, but the verbose systems struggle with detecting the specific class labels.\\n\\n6.2 Arabic Grammatical Error Correction\\n\\nWe explore different variants of the above-mentioned Seq2Seq models. For each model, we study the effects of applying morphological preprocessing (+Morph), providing GED tags as auxiliary input (+GED), or both (+Morph+GED). Applying morphological preprocessing simply means correcting the erroneous input using the morphological disambiguator before training and inference. To increase the robustness of the models that take GED tags as auxiliary input, we use predicted (not gold) GED tags when we train the GEC systems. For each dataset, we run its respective GED model on the same training data it was trained on and we pick the predictions of the worst checkpoint. During inference, we resolve merge and delete errors before feeding erroneous sentences to the model. This experimental setup yields the best performance across all GEC models.\\n\\nTo ensure fair comparison to previous work on Arabic GEC, we follow the same constraints that were introduced in the QALB-2014 and QALB-2015 shared tasks: systems tested on QALB-2014 are only allowed to use the QALB-2014 training data, whereas systems tested on QALB-2015 are allowed to use the QALB-2014 and QALB-2015 training data. For ZAEBUC, we train our systems on the combinations of the three training datasets. We report our results in terms of precision (P), recall (R), F\\\\(_1\\\\), and F\\\\(_{0.5}\\\\). F\\\\(_1\\\\) was the official metric used in the QALB-2014 and QALB-2015 shared tasks. However, we follow the most recent work on GEC and use F\\\\(_{0.5}\\\\) (weighing precision twice as much as recall) as our main evaluation metric.\\n\\nWe use Hugging Face\u2019s Transformers (Wolf et al., 2019) to build our GED and GEC models. The hyperparameters we used are detailed in Appendix A.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4: GEC results on the Dev sets of QALB-2014, QALB-2015, and ZAEBUC. B&B (2015) and W+ (2018) refer to Bougares and Bouamor (2015) and Watson et al. (2018), respectively. The best results are in bold.\\n\\n| System                | QALB-2014 | QALB-2015 | ZAEBUC | Avg. |\\n|-----------------------|-----------|-----------|--------|------|\\n|                       | P | R | F | P | R | F | P | R | F | P | R | F | P | R | F |\\n| B&B (2015)            | 56.7 | 34.8 | 43.1 | 50.4 |     |     |     |     |     |     |     |     |     |     |     |\\n| W+ (2018)             |     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n| Morph                 | 76.4 | 30.4 | 43.5 | 58.7 | 56.2 | 9.4 | 16.2 | 28.2 | 78.0 | 36.9 | 50.1 | 63.8 | 50.2 |\\n| MLE                   | 89.2 | 41.3 | 56.5 | 72.4 | 73.7 | 20.1 | 31.6 | 48.0 | 90.1 | 55.6 | 68.8 | 80.1 | 66.9 |\\n| +Morph                | 88.5 | 44.9 | 59.6 | 74.1 | 68.3 | 22.0 | 33.2 | 48.0 | 89.1 | 61.8 | 73.0 | 81.9 | 68.0 |\\n| ChatGPT               | 67.7 | 60.6 | 63.9 | 66.1 | 54.9 | 36.9 | 44.1 | 50.0 | 68.1 | 52.1 | 59.1 | 64.2 | 60.1 |\\n| AraT5                 | 82.5 | 66.3 | 73.5 | 78.6 | 69.3 | 39.4 | 50.2 | 60.2 | 84.1 | 67.4 | 74.8 | 80.1 | 73.0 |\\n| +Morph                | 83.1 | 65.8 | 73.4 | 78.9 | 69.7 | 40.6 | 51.3 | 60.9 | 85.0 | 71.3 | 77.5 | 81.8 | 73.9 |\\n| +GED                  | 43.1 | 82.6 | 67.1 | 74.1 | 79.0 | 69.5 | 41.9 | 52.3 | 61.4 | 85.7 | 66.7 | 75.0 | 81.0 |\\n| +Morph +GED           | 43.1 | 83.1 | 67.9 | 74.7 | 79.6 | 68.4 | 41.5 | 51.7 | 60.6 | 85.2 | 71.2 | 77.6 | 82.0 |\\n| AraBART               | 83.2 | 64.9 | 72.9 | 78.7 | 68.6 | 42.6 | 52.6 | 61.2 | 87.3 | 70.6 | 78.1 | 83.4 | 74.4 |\\n| +Morph                | 82.4 | 67.2 | 74.0 | 78.8 | 68.5 | 44.3 | 53.8 | 61.7 | 87.2 | 71.6 | 78.7 | 83.6 | 74.7 |\\n| +GED                  | 43.1 | 83.3 | 65.9 | 73.6 | 79.1 | 68.2 | 45.3 | 54.4 | 61.9 | 87.2 | 72.9 | 79.4 | 83.9 |\\n| +Morph +GED           | 43.1 | 83.4 | 66.3 | 73.9 | 79.3 | 68.2 | 46.6 | 55.4 | 62.4 | 87.3 | 73.6 | 79.9 | 84.2 |\\n|                     |     |     |     |     |     |     |     |     |     |     |     |     |     |     |\\n\\n### Table 5: GED granularity results when used within the best GEC system (AraBART+Morph+GED) on the Dev sets of QALB-2014, QALB-2015, and ZAEBUC. The best results are in bold.\\n\\n| System                | QALB-2014 | QALB-2015 | ZAEBUC |\\n|-----------------------|-----------|-----------|--------|\\n|                       | P | R | F | P | R | F | P | R | F |\\n| 7 Results             |     |     |     |     |     |     |     |     |     |\\n\\n### Baselines\\n\\nThe Morph system which did not use any training data constitutes a solid baseline for mostly addressing the noise in Arabic spelling. The MLE system claims the highest precision of all compared systems, but it suffers from low recall as expected. ChatGPT has the highest recall among the baselines, but with lower precision. A sample of 100 ChatGPT mismatches reveals that 37% are due to mostly acceptable punctuation choices and 25% are valid paraphrases or re-orderings; however, 38% are grammatically or lexically incorrect.\\n\\n### Seq2Seq Models\\n\\nAraT5 and AraBART outperform previous work on QALB-2014 and QALB-2015, with AraBART being the better model on average.\\n\\nDoes morphological preprocessing improve Arabic GEC?\\n\\nAcross all models (MLE, AraT5, and AraBART), training and testing on morphologically preprocessed text improves the performance, except for MLE+Morph on QALB-2015 where there is no change in $F_{0.5}$.\\n\\nDoes GED help Arabic GEC?\\n\\nWe start off by using the most fine-grained GED model (43-Class) to exploit the full effect of the ARETA GED tags and to guide our choice between AraBART and AraT5. Using GED as an auxiliary input in both AraT5 and AraBART improves the results across all three Dev sets, with AraBART+GED demonstrating superior performance compared to the other models, on average. Applying morphological preprocessing as well as using GED as an auxiliary input yields the best performance across the three Dev sets, except for QALB-2015 in the case of AraT5+Morph+GED. Overall, AraBART+Morph+GED is the best performer on average in terms of $F_{0.5}$. The improvements using GED with GEC systems are mostly due to recall. An error comparison between AraBART and the AraBART+Morph+GED model (Appendix E) shows improved performance on the majority of the error types.\\n\\nTo study the effect of GED granularity on GEC, we train two additional AraBART+Morph+GED models with 13-Class and 2-Class GED tags. The results in Table 5 show that 13-Class GED was best in QALB-2014 and ZAEBUC, whereas 43-Class GED tags. The results in Table 5 show that 13-Class GED was best in QALB-2014 and ZAEBUC, whereas 43-Class...\"}"}
{"id": "emnlp-2023-main-396", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: GED granularity results when used within GEC on the Test sets of QALB-2014, QALB-2015, and ZAEBUC. B&B (2015), W+ (2018), and S+ (2022) refer to Bougares and Bouamor (2015), Watson et al. (2018), and Solyman et al. (2022), respectively. The best results are in bold.\\n\\n|          | QALB-2014 | QALB-2015-L1 | QALB-2015-L2 | ZAEBUC | Avg. |\\n|----------|-----------|--------------|--------------|--------|------|\\n| AraBART  | 89.5      | 77.3         | 83.0         | 86.8   | 90.1 |\\n|          | 90.1      | 81.4         | 85.5         | 88.2   | 90.1 |\\n|          | 71.8      | 40.7         | 52.0         | 62.3   | 89.5 |\\n|          | 89.5      | 76.9         | 82.7         | 86.6   | 81.0 |\\n| +Morph   | 88.4      | 78.9         | 83.4         | 86.3   | 89.9 |\\n|          | 83.1      | 86.4         | 88.5         | 70.2   | 83.1 |\\n|          | 84.7      | 53.5         | 62.3         | 89.5   | 76.9 |\\n|          | 88.4      | 76.3         | 81.9         | 85.7   | 81.9 |\\n| +GED     | 43.0      | 89.7         | 78.9         | 84.0   | 87.3 |\\n|          | 89.8      | 81.8         | 85.6         | 88.1   | 89.8 |\\n|          | 70.7      | 43.6         | 53.9         | 62.9   | 89.2 |\\n|          | 89.2      | 77.0         | 82.7         | 86.5   | 81.2 |\\n| +Morph+GED| 43.0    | 88.8         | 80.1         | 84.2   | 86.9 |\\n|          | 90.0      | 83.8         | 86.8         | 90.0   | 90.0 |\\n|          | 86.8      | 86.8         | 69.0         | 43.6   | 88.7 |\\n| +GED     | 13.0      | 89.8         | 78.9         | 84.0   | 87.3 |\\n|          | 89.8      | 82.2         | 85.8         | 88.2   | 89.8 |\\n|          | 71.0      | 42.8         | 53.4         | 62.7   | 89.9 |\\n| +Morph+GED| 13.0    | 88.6         | 80.0         | 84.1   | 86.7 |\\n|          | 83.1      | 84.1         | 68.9         | 43.5   | 88.9 |\\n|          | 78.4      | 43.6         | 53.4         | 61.8   | 88.7 |\\n| +GED     | 2.0       | 89.3         | 77.6         | 83.0   | 86.7 |\\n|          | 89.4      | 81.8         | 85.5         | 87.8   | 89.4 |\\n|          | 70.6      | 42.4         | 53.0         | 62.3   | 89.0 |\\n| +Morph+GED| 2.0    | 87.8         | 79.8         | 83.6   | 86.1 |\\n|          | 89.9      | 83.0         | 86.3         | 89.9   | 89.9 |\\n|          | 69.5      | 43.5         | 53.5         | 62.1   | 89.2 |\\n|          | 77.7      | 42.8         | 53.4         | 62.7   | 89.9 |\\n| +Morph+GED| 2.0    | 88.6         | 80.0         | 84.1   | 86.7 |\\n|          | 88.9      | 77.8         | 83.4         | 87.2   | 83.4 |\\n|          | 81.4      | 42.8         | 53.4         | 62.7   | 89.9 |\\n| +GED     | 2.0       | 89.3         | 77.6         | 83.0   | 86.7 |\\n|          | 89.4      | 81.8         | 85.5         | 87.8   | 89.4 |\\n|          | 70.6      | 42.4         | 53.0         | 62.3   | 89.0 |\\n| +Morph+GED| 2.0    | 87.8         | 79.8         | 83.6   | 86.1 |\\n|          | 89.9      | 83.0         | 86.3         | 89.9   | 89.9 |\\n|          | 69.5      | 43.5         | 53.5         | 62.1   | 89.2 |\\n| +GED     | 2.0       | 89.3         | 77.6         | 83.0   | 86.7 |\\n|          | 89.4      | 81.8         | 85.5         | 87.8   | 89.4 |\\n|          | 70.6      | 42.4         | 53.0         | 62.3   | 89.0 |\\n| +Morph+GED| 2.0    | 87.8         | 79.8         | 83.6   | 86.1 |\\n|          | 88.9      | 77.8         | 83.4         | 87.2   | 83.4 |\\n|          | 81.4      | 42.8         | 53.4         | 62.7   | 89.9 |\\n\\nTable 7: No punctuation GED granularity results when used within GEC on the Test sets of QALB-2014, QALB-2015, and ZAEBUC. The best results are in bold.\\n\\nGED was best in QALB-2015 in terms of $F_{0.5}$. However, in terms of precision and recall, GED models with different granularity behave differently across the three Dev sets. On average, using any GED granularity improves over AraBART, with 13-Class GED yielding the best results, although it is only 0.1 higher than 43-Class GED in terms of $F_{0.5}$. For completeness, we further estimate an oracle upper bound by using gold GED tags with different granularity. The results (in Table 5) show that using GED with different granularity improves the results considerably. This indicates that GED is providing the GEC system with additional information; however, the main bottleneck is the GED prediction reliability as opposed to GED granularity. Improving GED predictions will most likely lead to better GEC results.\\n\\nTest Results\\n\\nSince the best-performing models on the three Dev sets benefit from different GED granularity when used with AraBART+Morph, we present the results on the Test sets using all different GED granularity models. The results of using AraBART and its variants on the Test sets are presented in Table 6. On QALB-2014, using Morph, GED, or both improves the results over AraBART, except for 2-Class GED. AraBART+43-Class GED is the best performer (0.3 increase in $F_{0.5}$, although not statistically significant). It is worth noting that AraBART+Morph achieves the highest recall on QALB-2014 (2.7 increase over AraBART and statistically significant at $p < 0.05$).\\n\\nFor QALB-2015-L1, using GED by itself across all granularity did not improve over AraBART, but when combined with Morph, the 43-Class GED model yields the best performance in $F_{0.5}$ (0.6 increase statistically significant at $p < 0.05$). When it comes to QALB-2015-L2, Morph does not help, but using GED alone improves the results over AraBART, with 43-Class and 13-Class GED being the best (0.4 increase). Lastly, in ZAEBUC, Morph does not help, but using 13-Class GED by itself improves over AraBART (0.4 increase). Overall, all the improvements we observe are attributed to recall, which is consistent with the Dev results.\\n\\nFollowing the QALB-2015 shared task (Rozovskaya et al., 2015) reporting of no-punctuation\\n\\nStatistical significance was done using a two-sided approximate randomization test.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"results due to observed inconsistencies in the references (Mohit et al., 2014), we present results on the Test sets without punctuation errors in Table 7. The results are consistent with those with punctuation, indicating that GED and morphological preprocessing yield improvements compared to using AraBART by itself across all Test sets. The score increase among all reported metrics when removing punctuation, specifically in the L1 data, indicates that punctuation presents a challenge for GEC models and needs further investigation both in terms of data creation and modeling approaches.\\n\\nAnalyzing the Test Results\\nTable 8 presents the average absolute changes in precision and recall over the Test sets when introducing Morph, GED, or both. Adding Morph alone or GED alone improves recall (up to 0.8 in the case of Morph) and slightly hurts precision. When using both Morph and GED, we observe significant improvements in recall with an average of 1.5 but with higher drops of precision with an average of \u22120.7.\\n\\nConclusion and Future Work\\nWe presented the first results on Arabic GEC using Transformer-based pretrained Seq2Seq models. We also presented the first results on multi-class Arabic GED. We showed that using GED information as an auxiliary input in GEC models improves GEC performance across three datasets. Further, we investigated the use of contextual morphological preprocessing in aiding GEC systems. Our models achieve SOTA results on two Arabic GEC shared tasks datasets and establish a strong benchmark on a recently created dataset.\\n\\nIn future work, we plan to explore other GED and GEC modeling approaches, including the use of syntactic models (Li et al., 2022; Zhang et al., 2022). We plan to work more on insertions, punctuation, and infrequent error combinations. We also plan to work on GEC for Arabic dialects, i.e., the conventional orthography of dialectal Arabic normalization (Habash et al., 2018; Eskander et al., 2013; Eryani et al., 2020).\\n\\nLimitations\\nAlthough using GED information as an auxiliary input improves GEC performance, our GED systems are limited as they can only predict error types for up to 512 subwords since they are built by fine-tuning CAMeLBERT. We also acknowledge the limitation of excluding insertion errors when modeling GED. Furthermore, our GEC systems could benefit from employing a copying mechanism (Zhao et al., 2019; Yuan et al., 2019), particularly because of the limited training data available in Arabic GEC. Moreover, the dataset sizes of QALB-2015-L2 and ZAEBUC are too small to allow us to test for statistical significance.\\n\\nAcknowledgements\\nWe thank Ted Briscoe for helpful discussions and constructive feedback. We acknowledge the support of the High Performance Computing Center at New York University Abu Dhabi. Finally, we wish to thank the anonymous reviewers at EMNLP 2023 for their feedback.\\n\\nReferences\\nAhmed Abdelali, Sabit Hassan, Hamdy Mubarak, Kareem Darwish, and Younes Samih. 2021. Pre-training bert on arabic tweets: Practical considerations.\\nMuhammad Abdul-Mageed, AbdelRahim Elmadany, and El Moatez Billah Nagoudi. 2021. ARBERT & MARBERT: Deep bidirectional transformers for Arabic. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 7088\u20137105, Online. Association for Computational Linguistics.\\nAbdullah Alfaifi and Eric Atwell. 2012. Arabic learner corpora (alc): a taxonomy of coding errors. In The 8th International Computing Conference in Arabic.\\nAbdullah Alfaifi, Eric Atwell, and Ghazi Abuhakema. 2013. Error annotation of the Arabic learner corpus.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In Language Processing and Knowledge in the Web, pages 14\u201322. Springer.\\n\\nManar Alkhatib, Azza Abdel Monem, and Khaled Shaalan. 2020. Deep learning for Arabic error detection and correction. ACM Trans. Asian Low-Resour. Lang. Inf. Process., 19(5).\\n\\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020. AraBERT: Transformer-based model for Arabic language understanding. In Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, pages 9\u201315, Marseille, France. European Language Resource Association.\\n\\nHiroki Asano, Tomoya Mizumoto, and Kentaro Inui. 2017. Reference-based metrics can be replaced with reference-less metrics in evaluating grammatical error correction systems. In Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 343\u2013348, Taipei, Taiwan. Asian Federation of Natural Language Processing.\\n\\nDana Awad. 2013. La ponctuation en Arabe: histoire et r\u00e8gles. etude constrative avec le fran\u00e7ais et l'anglais.\\n\\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal, Sabyasachi Ghosh, and Vihari Piratla. 2019. Parallel iterative edit models for local sequence translation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4260\u20134270, Hong Kong, China. Association for Computational Linguistics.\\n\\nRiadh Belkebir and Nizar Habash. 2021. Automatic error type annotation for Arabic. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 596\u2013606, Online. Association for Computational Linguistics.\\n\\nSamuel Bell, Helen Yannakoudakis, and Marek Rei. 2019. Context is key: Grammatical error detection with contextual word representations. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103\u2013115, Florence, Italy. Association for Computational Linguistics.\\n\\nFethi Bougares and Houda Bouamor. 2015. UMUM@QALB-2015 shared task: Character and word level SMT pipeline for automatic error correction of Arabic text. In Proceedings of the Second Workshop on Arabic Natural Language Processing, pages 166\u2013172, Beijing, China. Association for Computational Linguistics.\\n\\nChristopher Bryant, Mariano Felice, \u00d8istein E. Andersen, and Ted Briscoe. 2019. The BEA-2019 shared task on grammatical error correction. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 52\u201375, Florence, Italy. Association for Computational Linguistics.\\n\\nChristopher Bryant, Mariano Felice, and Ted Briscoe. 2017. Automatic annotation and evaluation of error types for grammatical error correction. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 793\u2013805, Vancouver, Canada. Association for Computational Linguistics.\\n\\nChristopher Bryant, Zheng Yuan, Muhammad Reza Qorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe. 2023. Grammatical error correction: A survey of the state of the art.\\n\\nTim Buckwalter. 2004. Issues in Arabic orthography and morphology analysis. In Proceedings of the Workshop on Computational Approaches to Arabic Script-based Languages, pages 31\u201334, Geneva, Switzerland. COLING.\\n\\nMartin Chodorow, Joel Tetreault, and Na-Rae Han. 2007. Detection of grammatical errors involving prepositions. In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions, pages 25\u201330, Prague, Czech Republic. Association for Computational Linguistics.\\n\\nLeshem Choshen, Dmitry Nikolaev, Yevgeni Berzak, and Omri Abend. 2020. Classifying syntactic errors in learner language. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 97\u2013107, Online. Association for Computational Linguistics.\\n\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations.\\n\\nCouncil of Europe. 2001. Common European framework of reference for languages: learning, teaching, assessment. Cambridge University Press.\\n\\nDaniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages 915\u2013923, Portland, Oregon, USA. Association for Computational Linguistics.\\n\\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better evaluation for grammatical error correction. In Proceedings of the 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 568\u2013572, Montr\u00e9al, Canada. Association for Computational Linguistics.\\n\\nDaniel Dahlmeier, Hwee Tou Ng, and Siew Mei Wu. 2013. Building a large annotated corpus of learner English: The NUS corpus of learner English. In Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 22\u201331, Atlanta, Georgia. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for\"}"}
{"id": "emnlp-2023-main-396", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-396", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marcin Junczys-Dowmunt, Roman Grundkiewicz, Shubha Guha, and Kenneth Heafield. 2018. Approaching neural grammatical error correction as a low-resource machine translation task. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 595\u2013606, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nMoussa Kamal Eddine, Nadi Tomeh, Nizar Habash, Joseph Le Roux, and Michalis Vazirgiannis. 2022. AraBART: a pretrained Arabic sequence-to-sequence model for abstractive summarization. In Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pages 31\u201342, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.\\n\\nMasahiro Kaneko and Mamoru Komachi. 2019. Multi-head multi-layer attention to deep language representations for grammatical error detection.\\n\\nSatoru Katsumata and Mamoru Komachi. 2020. Stronger baselines for grammatical error correction using a pretrained encoder-decoder model. In Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing, pages 827\u2013832, Suzhou, China. Association for Computational Linguistics.\\n\\nEkaterina Kochmar, \u00d8istein Andersen, and Ted Briscoe. 2012. HOO 2012 error recognition and correction shared task: Cambridge University submission report. In Proceedings of the Seventh Workshop on Building Educational Applications Using NLP, pages 242\u2013250, Montr\u00e9al, Canada. Association for Computational Linguistics.\\n\\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020. An empirical study of pre-trained transformers for Arabic information extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4727\u20134734, Online. Association for Computational Linguistics.\\n\\nV. I. Levenshtein. 1966. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10:707.\\n\\nZuchao Li, Kevin Parnow, and Hai Zhao. 2022. Incorporating rich syntax information in grammatical error correction. Information Processing & Management, 59(3):102891.\\n\\nNora Madi and Hend Al-Khalifa. 2020. Error detection for Arabic text using neural sequence labeling. Applied Sciences, 10(15).\\n\\nKoki Maeda, Masahiro Kaneko, and Naoaki Okazaki. 2022. IMPARA: Impact-based metric for GEC using parallel data. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3578\u20133588, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\n\\nJonathan Mallinson, Jakub Adamek, Eric Malmi, and Aliaksei Severyn. 2022. EdiT5: Semi-autoregressive text editing with t5 warm-start. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 2126\u20132138, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nJonathan Mallinson, Aliaksei Severyn, Eric Malmi, and Guillermo Garrido. 2020. FELIX: Flexible text editing through tagging and insertion. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 1244\u20131255, Online. Association for Computational Linguistics.\\n\\nEric Malmi, Sebastian Krause, Sascha Rothe, Daniil Mirylenka, and Aliaksei Severyn. 2019. Encode, tag, realize: High-precision text editing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5054\u20135065, Hong Kong, China. Association for Computational Linguistics.\\n\\nStuart Mesham, Christopher Bryant, Marek Rei, and Zheng Yuan. 2023. An extended sequence tagging vocabulary for grammatical error correction.\\n\\nRada Mihalcea and Ted Pedersen. 2003. An evaluation exercise for word alignment. In Proceedings of the HLT-NAACL 2003 Workshop on Building and Using Parallel Texts: Data Driven Machine Translation and Beyond, pages 1\u201310.\\n\\nBehrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi Zaghouani, and Ossama Obeid. 2014. The first QALB shared task on automatic text correction for Arabic. In Proceedings of the EMNLP 2014 Workshop on Arabic Natural Language Processing (ANLP), pages 39\u201347, Doha, Qatar. Association for Computational Linguistics.\\n\\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and Muhammad Abdul-Mageed. 2022. AraT5: Text-to-text transformers for Arabic language generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 628\u2013647, Dublin, Ireland. Association for Computational Linguistics.\\n\\nCourtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground truth for grammatical error correction metrics. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 588\u2013593, Beijing, China. Association for Computational Linguistics.\\n\\nCourtney Napoles, Keisuke Sakaguchi, and Joel Tetreault. 2016. There's no comparison: Reference-less evaluation metrics in grammatical error correction. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2109\u20132115, Austin, Texas. Association for Computational Linguistics.\\n\\nMichael Nawar. 2015. CUFE@QALB-2015 shared task: Arabic error correction system. In Proceedings of the Second Workshop on Arabic Natural Language Processing (ANLP), pages 39\u201347, Doha, Qatar. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-396", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-396", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Detailed Experimental Setup\\n\\nGrammatical Error Detection\\n\\nOur GED models were fine-tuned for 10 epochs using a learning rate of $5 \\\\times 10^{-5}$, a batch size of 32, and a seed of 42. At the end of the fine-tuning, we pick the best checkpoint based on the performance on the Dev sets.\\n\\nGrammatical Error Correction\\n\\nWhen using AraBART, we fine-tune the models for 10 epochs by using a learning rate of $5 \\\\times 10^{-5}$, a batch size of 32, a maximum sequence length of 1024, and a seed of 42. For AraT5, we fine-tune the models for 30 epochs by using a learning rate of $1 \\\\times 10^{-4}$ and the rest of the hyperparameters are the same as the ones used in AraBART. During inference, we use beam search with a beam width of 5 for all models. At the end of the fine-tuning, we pick the best checkpoint based on the performance on the Dev sets by using the $M_2$ scorer. The $M_2$ scorer suffers from extreme running times in cases where the generated outputs differ significantly from the input. To mitigate this bottleneck, we extend the $M_2$ scorer by introducing a time limit for each sentence during evaluation. If the evaluation of a single generated sentence surpasses this limit, we pass the input sentence to the output without modifications. We use this extended version of the $M_2$ scorer when reporting our results on the Dev sets. When reporting our results on the Test sets, we use the $M_2$ scorer release that is provided by the QALB shared task. We make our extended version of the $M_2$ scorer publicly available.\\n\\nChatGPT\\n\\nWe start with prompting ChatGPT with a 3-shot prompt. Our exact prompt is the following:\\n\\n\\\"Please identify and correct any spelling and grammar mistakes in the following sentence indicated by <input> INPUT </input> tag. You need to comprehend the sentence as a whole before gradually identifying and correcting any errors while keeping the original sentence structure unchanged as much as possible. Afterward, output the corrected version directly without any explanations. Here are some in-context examples:\\n\\n(1), <input> SRC-1 </input>: <output> TGT-1 </output>.\\n(2), <input> SRC-2 </input>: <output> TGT-2 </output>.\\n(3), <input> SRC-3 </input>: <output> TGT-3 </output>.\\nPlease feel free to refer to these examples. Remember to format your corrected output results with the tag <output> Your Corrected Version </output>. Please start: <input> INPUT </input>\\\"\"}"}
{"id": "emnlp-2023-main-396", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset   | Split   | Lines  | Words   | Err. % | Level     | Domain   |\\n|-----------|---------|--------|---------|--------|-----------|----------|\\n| QALB-2014 | Train-L1| 19,411 | 1,021,165 | 30%    | Native    | Comments |\\n|           | Dev-L1  | 1,017  | 53,737  | 31%    | Native    | Comments |\\n|           | Test-L1 | 968    | 51,285  | 32%    | Native    | Comments |\\n|           | Test-L2 | 920    | 48,547  | 27%    | Native    | Comments |\\n|           | Train-L2| 310    | 43,353  | 30%    | L2        | Essays   |\\n|           | Dev-L2  | 154    | 24,742  | 29%    | L2        | Essays   |\\n|           | Test-L1 | 158    | 22,808  | 29%    | L2        | Essays   |\\n| ZAEBUC    | Train-L1| 150    | 25,127  | 24%    | Native    | Essays   |\\n|           | Dev-L1  | 33     | 5,276   | 25%    | Native    | Essays   |\\n|           | Test-L1 | 31     | 5,118   | 26%    | Native    | Essays   |\\n\\nTable 9: Corpus statistics of Arabic GEC datasets.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Tag       | Error Description                  | Example | QALB-2014 | QALB-2015 | ZAEBUC |\\n|-----------|------------------------------------|---------|-----------|-----------|--------|\\n| Orthography (O) |                                      |         |           |           |        |\\n| OA        | Alif, Ya & Alif-Maqsura             | \ufecb\ufee0\ufef2 \u2190 \ufecb\ufee0\ufef0 | 7,627     | 3%        | 290    |\\n| OC        | Char Order                          | \ufe97\ufe92\ufeae\ufbfe\ufee8\ufe8e \u2190 \ufe97\ufeae\ufe91\ufbff\ufee8\ufe8e | 466       | 0%        | 45     |\\n| OD        | Additional Char                     | \ufbfe\ufecc\ufeaa\u0648\u0645 \u2190 \ufbfe\ufeaa\u0648\u0645 | 4,086     | 1%        | 283    |\\n| OG        | Lengthening short vowels            | \ufee7\ufed8\ufbff\ufee4\ufeee \u2190 \ufee7\ufed8\ufbff\ufee2 | 0         | 0%        | 0      |\\n| OH        | Hamza errors                        | \u0627\ufedb\ufe9c\ufeae \u2190 \u0623\ufedb\ufe9c\ufeae | 90,579    | 30%       | 1,076  |\\n| OM        | Missing char(s)                     | \ufed3\ufe8e\ufedf\ufbff\ufee6 \u2190 \ufeb3\ufe8e\ufe8b\ufee0\ufbff\ufee6 | 4,062     | 1%        | 361    |\\n| ON        | Nun & Tanwin Confusion              | \ufed3\ufeee\u0628\u064c \u2190 \ufe9b\ufeee\u0628\u064c | 0         | 0%        | 0      |\\n| OR        | Char Replacement                    | \ufee3\ufebc\ufee0\ufee8\ufe8e \u2190 \u0648\ufebb\ufee0\ufee8\ufe8e | 8,350     | 3%        | 762    |\\n| OS        | Shortening long vowels              | \u0623\u0648\ufed7\ufe8e\u062a \u2190 \u0623\u0648\ufed7\ufe96 | 0         | 0%        | 0      |\\n| OT        | Ha/Ta/Ta-Marbuta Confusion          | \ufee3\ufeb8\ufe8e\u0631\ufb90\ufbab \u2190 \ufee3\ufeb8\ufe8e\u0631\ufb90\ufe94 | 14,688    | 5%        | 54     |\\n| OW        | Confusion in Alif Fariqa            | \u0648\ufedb\ufe8e\ufee7\ufeee \u2190 \u0648\ufedb\ufe8e\ufee7\ufeee\u0627 | 1,885     | 1%        | 32     |\\n| OO        | Other orthographic errors            | -        | 1,632     | 1%        | 38     |\\n| Morphology (M) |                                      |         |           |           |        |\\n| MI        | Word inflection                     | \ufecb\ufe8e\u0631\u0641 \u2190 \ufecb\ufeae\u0648\u0641 | 1,360     | 0%        | 400    |\\n| MT        | Verb tense                          | \ufedb\ufeae\ufe98\ufee8\ufef2 \u2190 \u0623\ufed3\ufeae\ufea3\ufe98\ufee8\ufef2 | 76        | 0%        | 136    |\\n| MO        | Other morphological errors           | -        | 15        | 0%        | 7      |\\n| Syntax (X) |                                      |         |           |           |        |\\n| XC        | Case                                | \u0631\u0627\ufe8b\ufecc\ufe8e \u2190 \u0631\u0627\ufe8b\ufeca | 5,980     | 2%        | 279    |\\n| XF        | Definiteness                        | \ufeb3\ufee6 \u2190 \ufeb3\ufee6 | 852       | 0%        | 835    |\\n| XG        | Gender                              | \u0627\ufedf\ufed0\ufeae\ufe91\ufbff\ufe94 \u2190 \u0627\ufedf\ufed0\ufeae\ufe91\ufef2 | 809       | 0%        | 317    |\\n| XM        | Missing word                        | Null \u2190 \ufecb\ufee0\ufef0 | 1,375     | 0%        | 763    |\\n| XN        | Number                              | \ufed3\ufedc\ufe8e\u0631\u064a \u2190 \ufed3\ufedc\ufeae\ufe97\ufef2 | 1,107     | 0%        | 210    |\\n| XT        | Unnecessary word                    | Null \u2190 \ufecb\ufee0\ufef0 | 1,047     | 0%        | 418    |\\n| XO        | Other syntactic errors               | -        | 3,270     | 1%        | 122    |\\n| Semantics (S) |                                      |         |           |           |        |\\n| SF        | Conjunction error                   | \ufed3\ufeb4\ufe92\ufea4\ufe8e\u0646 \u2190 \ufe97\ufeb4\ufe92\ufea2\u0627\u0646 | 96        | 0%        | 46     |\\n| SW        | Word selection error                 | \ufecb\ufee6 \u2190 \ufecb\ufee6 | 4,711     | 2%        | 865    |\\n| SO        | Other semantic errors                | -        | 380       | 0%        | 114    |\\n| Punctuation (P) |                                      |         |           |           |        |\\n| PC        | Punctuation confusion               | \ufed7\ufe8e\u0644 \u2190 \ufed7\ufe8e\u0644 | 11,361    | 4%        | 854    |\\n| PM        | Missing punctuation                 | \u0627\ufedf\ufecc\ufec8\ufbff\ufee2 \u2190 \u0627\ufedf\ufecc\ufec8\ufbff\ufee2\u060c | 97,271    | 32%       | 2,915  |\\n| PT        | Unnecessary punctuation              | \ufecb\ufe8e\u0645\u060c \u2190 \ufecb\ufe8e\u0645 | 5,553     | 2%        | 213    |\\n| PO        | Other errors in punctuation         | -        | 0         | 0%        | 0      |\\n| Merge (MG) | Words are merged                    | \ufefb \ufbfe\ufee0\ufeb0\u0645 \u2190 \ufefb \ufbfe\ufee0\ufeb0\u0645 | 15,063    | 5%        | 377    |\\n| Split (SP) | Words are split                     | \u0648 \ufed7\ufe8e\u0644 \u2190 \u0648 \ufed7\ufe8e\u0644 | 7,828     | 3%        | 80     |\\n| Unknown (UNK) | Unknown Errors                     | \u0627\ufedf\ufec8\ufe8e\ufedf\ufee4\ufeee\u0646 \u2190 \u0627\ufedf\ufeac\ufbfe\ufee6 \ufe97\ufee0\ufee4\ufeee\u0627 | 2,053     | 1%        | 303    |\\n\\nTable 10: The statistics of the error types in the Train sets of QALB-2014, QALB-2015, and ZAEBUC. The error types are based on the extended ALC (Alfaifi et al., 2013) taxonomy as used by Belkebir and Habash (2021).\"}"}
{"id": "emnlp-2023-main-396", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|        | 2-Class | 13-Class | 43-Class |\\n|--------|---------|----------|----------|\\n|        | Train   | Dev      | Test     |\\n|        |         |          |          |\\n| E      | 6,442   | 346      | 540      |\\n| Delete | 6,442   | 346      | 540      |\\n|        |         |          |          |\\n| Merge-B| 15,063  | 797      | 795      |\\n|       | 15,063  | 797      | 795      |\\n|        |         |          |          |\\n| Merge-I| 15,296  | 812      | 807      |\\n|        | 15,296  | 812      | 807      |\\n|        |         |          |          |\\n| M      |         |          |          |\\n|        |         |          |          |\\n| MI     | 1,360   | 69       | 59       |\\n|        | 1,360   | 69       | 59       |\\n|        |         |          |          |\\n| MT     | 76      | 4        | 136      |\\n|        | 76      | 4        | 136      |\\n|        |         |          |          |\\n| M+O    | 243     | 17       | 15       |\\n|        | 243     | 17       | 15       |\\n|        |         |          |          |\\n| O      | 3,255   | 166      | 164      |\\n|        | 3,255   | 166      | 164      |\\n|        |         |          |          |\\n| OA     | 7,627   | 313      | 252      |\\n|        | 7,627   | 313      | 252      |\\n|        |         |          |          |\\n| OC     | 466     | 27       | 19       |\\n|        | 466     | 27       | 19       |\\n|        |         |          |          |\\n| OD     | 4,086   | 207      | 204      |\\n|        | 4,086   | 207      | 204      |\\n|        |         |          |          |\\n| OH     | 90,579  | 4,785    | 4,632    |\\n|        | 90,579  | 4,785    | 4,632    |\\n|        |         |          |          |\\n| OM     | 4,062   | 228      | 217      |\\n|        | 4,062   | 228      | 217      |\\n|        |         |          |          |\\n| OR     | 8,358   | 425      | 446      |\\n|        | 8,358   | 425      | 446      |\\n|        |         |          |          |\\n| OT     | 14,688  | 758      | 623      |\\n|        | 14,688  | 758      | 623      |\\n|        |         |          |          |\\n| OW     | 1,885   | 149      | 107      |\\n|        | 1,885   | 149      | 107      |\\n|        |         |          |          |\\n| OA+OH  | 480     | 19       | 12       |\\n|        | 480     | 19       | 12       |\\n|        |         |          |          |\\n| OA+OR  | 215     | 8        | 6        |\\n|        | 215     | 8        | 6        |\\n|        |         |          |          |\\n| OD+OG  | 573     | 32       | 32       |\\n|        | 573     | 32       | 32       |\\n|        |         |          |          |\\n| OD+OH  | 317     | 11       | 17       |\\n|        | 317     | 11       | 17       |\\n|        |         |          |          |\\n| OD+OM  | 104     | 4        | 5        |\\n|        | 104     | 4        | 5        |\\n|        |         |          |          |\\n| OD+OR  | 675     | 33       | 26       |\\n|        | 675     | 33       | 26       |\\n|        |         |          |          |\\n| OH+OM  | 2,339   | 134      | 123      |\\n|        | 2,339   | 134      | 123      |\\n|        |         |          |          |\\n| OH+OT  | 1,468   | 56       | 65       |\\n|        | 1,468   | 56       | 65       |\\n|        |         |          |          |\\n| OM+OR  | 382     | 15       | 19       |\\n|        | 382     | 15       | 19       |\\n|        |         |          |          |\\n| OR+OT  | 193     | 10       | 7        |\\n|        | 193     | 10       | 7        |\\n|        |         |          |          |\\n| O+X    |         |          |          |\\n|        |         |          |          |\\n| OH+XC  | 323     | 24       | 18       |\\n|        | 323     | 24       | 18       |\\n|        |         |          |          |\\n| P      | 11,379  | 598      | 687      |\\n|        | 11,379  | 598      | 687      |\\n|        |         |          |          |\\n| S      | 536     | 41       | 19       |\\n|        | 536     | 41       | 19       |\\n|        |         |          |          |\\n| SF     | 96      | 5        | 46       |\\n|        | 96      | 5        | 46       |\\n|        |         |          |          |\\n| SW     | 4,804   | 201      | 229      |\\n|        | 4,804   | 201      | 229      |\\n|        |         |          |          |\\n| X      | 3,668   | 216      | 182      |\\n|        | 3,668   | 216      | 182      |\\n|        |         |          |          |\\n| XC     | 5,980   | 373      | 369      |\\n|        | 5,980   | 373      | 369      |\\n|        |         |          |          |\\n| XC+XG  | 296     | 23       | 40       |\\n|        | 296     | 23       | 40       |\\n|        |         |          |          |\\n| XC+XN  | 500     | 18       | 41       |\\n|        | 500     | 18       | 41       |\\n|        |         |          |          |\\n| XF     | 852     | 63       | 25       |\\n|        | 852     | 63       | 25       |\\n|        |         |          |          |\\n| XG     | 809     | 38       | 30       |\\n|        | 809     | 38       | 30       |\\n|        |         |          |          |\\n| XM     | 225     | 15       | 6        |\\n|        | 225     | 15       | 6        |\\n|        |         |          |          |\\n| XN     | 1,107   | 47       | 41       |\\n|        | 1,107   | 47       | 41       |\\n|        |         |          |          |\\n| XT     | 155     | 16       | 9        |\\n|        | 155     | 16       | 9        |\\n|        |         |          |          |\\n| UNK    | 6,835   | 331      | 300      |\\n|        | 6,835   | 331      | 300      |\\n|        |         |          |          |\\n| C      | 795,510 | 41,875   | 39,690   |\\n|        | 795,510 | 41,875   | 39,690   |\\n|        |         |          |          |\\n|        | 1,021,165 | 53,737      | 51,285    |\\n|        | 1,021,165 | 53,737      | 51,285    |\\n\\nTable 11: The statistics of the different GED granularity error types we model across the three datasets. The description of the labels in the 13-Class and 43-Class categories are in Appendix C. For the 2-Class labels, E refers to erroneous words and C refers to correct words.\"}"}
{"id": "emnlp-2023-main-396", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 12: Specific error type performance of AraBART and our best system (AraBART+Morph+GED) on average on the Dev sets of QALB-2014, QALB-2015, and ZAEBUC. Results are reported in terms of F0.5.\\n\\n| Error Type | QALB-2014 | QALB-2015 | ZAEBUC |\\n|------------|-----------|-----------|--------|\\n| Delete     | 40.1      | 40.8      | 45.3   |\\n| Merge-B    | 91.2      | 93.0      | 82.4   |\\n| Merge-I    | 91.0      | 93.0      | 81.7   |\\n| M          | 24.8      | 27.6      | 37.0   |\\n| M+O        | 54.8      | 37.7      | 17.2   |\\n| O          | 94.0      | 94.3      | 80.3   |\\n| O+X        | 67.7      | 73.9      | 0.0    |\\n| P          | 76.4      | 77.4      | 64.5   |\\n| S          | 43.3      | 44.5      | 33.8   |\\n| X          | 58.5      | 61.1      | 59.6   |\\n| Split      | 87.6      | 87.1      | 78.0   |\\n| UNK        | 50.2      | 57.2      | 37.9   |\\n| C          | 96.2      | 96.8      | 89.9   |\\n\\nThe best results are in bold.\"}"}
