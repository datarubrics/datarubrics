{"id": "emnlp-2024-main-1216", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 21814\u201321828\\nNovember 12-16, 2024 \u00a92024 Association for Computational Linguistics\\n\\nDo LLMs Plan Like Human Writers? Comparing Journalist Coverage of Press Releases with LLMs\\nAlexander Spangher1, Nanyun Peng1, Sebastian Gehrmann3, Mark Dredze3\\n1University of Southern California, Information Sciences Institute\\n2University of California, Los Angeles,\\n3Bloomberg\\nspangher@usc.edu, mdredze@bloomberg.net\\n\\nAbstract\\nJournalists engage in multiple steps in the news writing process that depend on human creativity, like exploring different \\\"angles\\\" (i.e. the specific perspectives a reporter takes). These can potentially be aided by large language models (LLMs). By affecting planning decisions, such interventions can have an outsize impact on creative output. We advocate a careful approach to evaluating these interventions to ensure alignment with human values. In a case study of journalistic coverage of press releases, we assemble a large dataset of 250k press releases and 650k articles covering them. We develop methods to identify news articles that challenge and contextualize press releases. Finally, we evaluate suggestions made by LLMs for these articles and compare these with decisions made by human journalists. Our findings are three-fold: (1) Human-written news articles that challenge and contextualize press releases more take more creative angles and use more informational sources. (2) LLMs align better with humans when recommending angles, compared with informational sources. (3) Both the angles and sources LLMs suggest are significantly less creative than humans.\\n\\n1 Introduction\\nIn-depth news coverage goes beyond summarizing a story: it confirms or refutes narratives, offers viewpoints, and contextualizes events to expand readers' understanding (Hamilton, 2016). This process requires time and resources (Schudson, 1989). In an era where journalists are inundated with complex topics to cover and resources are dwindling (Angelucci and Cag\u00e9, 2019), approaches to facilitate such coverage are needed (Cohen et al., 2011).\\n\\n1 Including notable press releases \u2013 OpenAI's GPT2 announcement, Meta's Cambridge Analytica Scandal, etc.\\n2 For more details about our dataset and code release, see: https://github.com/alex2awesome/press-releases-emnlp.\\n\\nFigure 1: Two steps that precede writing news articles based on press releases are: formulating an angle (i.e., a specific focus), and selecting sources (i.e., a person or document contributing information). We compare planning steps made by human journalists (left), to those made by LLMs under various prompts designed to stimulate creative aid (right). We find that LLM plans are significantly less creative and diverse. We call for deeper alignment with fundamental human decision-making before creative-aid tools are widely deployed.\\n\\nLLMs have been proposed as tools to facilitate creative planning in journalism. Petridis et al. (2023), for instance, explored how well LLMs could recommend unique angles to cover press releases. While LLMs have been found to contribute positively, important questions remain. How often do LLM planning decisions align with human values? How can we adjust such decision-making to ensure better alignment?\\n\\nIn this work, we lay the groundwork for more broadly developing AI approaches for aiding creative tasks, ensuring they align with human values, and outlining a path to improvement. With a broad, novel dataset, we compare the planning decisions LLMs would make to the decisions humans have made in the past. As such, our work represents a generalizable benchmark in creative planning tasks and can serve as a template for creative planning evaluation going forward.\\n\\n3 Most prior work in this vein has limited generalizability due to small sample sizes \u2013 e.g., Petridis et al. (2023) tested two articles with 12 participants.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We start by assembling a corpus of press releases and news articles covering them, and identify articles that have effectively covered these releases. According to Maat and de Jong (2013), effective coverage substantially challenges and contextualizes press releases. To measure this, we quantify how much the article entails and contradicts a press release. For intuition on why we measure both, complete entailment would simply indicate a vanilla summary (Laban et al., 2022) while complete contradiction could indicate off-topic. We find, via extensive manual evaluation, that a mixture of both indicates effective coverage (.81 F1).\\n\\nNext, we ask what planning decisions characterize effective coverage. On a dataset of 6,000 human-written news articles and press release pairs, we find strong positive correlations between the overall criticality of a news article's coverage, and: (1) the creativity of the news article's angle ($r = .29$) and (2) the number of sources used in the article ($r = .5$). With this in hand, we turn to using our dataset to evaluate how LLMs might facilitate these two planning steps.\\n\\nFirst, we explore an LLM's ability to recommend \u201cangles\u201d, or story directions, building off Petridis et al. (2023). Next, we compare the kinds of sources suggested by an LLM with the sources human journalists used to cover these articles. Overall, we have two core findings: (1) We find that LLMs perform well at recommending angles that humans ultimately took (63.6 F1-score), but perform poorly at recommending kinds of sources (27.9 F1-score). (2) However, the level of creativity for both angles and sources is low. In sum, we make the following contributions:\\n\\n\u2022 We study how journalists make coverage decisions. We build a dataset of 650k articles covering 250k press releases across 10 years.\\n\u2022 To find examples of effective press release coverage, we define the task of contrastive summarization, and develop an approach based on Laban et al. (2022). We find that effective coverage takes more creative angles ($r = .29$) and uses more informational sources ($r = .5$) than average coverage patterns.\\n\u2022 We use these examples to study angle and source recommendations made by LLMs. We find, through extensive manual evaluation, that model plans lack creativity compared with human suggestions and do an especially poor job recommending types of sources. However, LLMs align better when recommending angles, suggesting some degree of capacity to reason about narratives. Taken together, these indicate that substantial work is needed during the planning stages of creative acts in order to align LLMs with the creativity of human work. However, our results, especially angle formulation, suggest that narrative planning exists in LLMs, and future work improving our approach might yield significant progress.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Theranos will close our clinical labs, impacting approximately 340 employees. We are profoundly grateful to these teammates...\\n\\nFew tears shed for E. Holmes as Theranos bleeds jobs. Theranos shot to fame in 2014. Then came an investigation from WSJ...\\n\\nThere is a false allegation that Tesla terminated employees in response to a new union campaign. These are the facts behind the event: Tesla conducts performance review cycles every six months... Underperforming employees are let go.\\n\\nEmployees said [they're] tracked down to the key stroke. \u201cIf you even go to the bathroom, you won\u2019t hit your time goal...\u201d\\n\\nAfter hours on Thursday, Tesla called [retaliation] allegations false, saying [workers] had been terminated due to poor performance.\\n\\nWe found reducing the earnings gap for Black women will create 1.2-1.7M U.S. jobs and increase GDP by $300-450B.\\n\\nStudies have found Black women\u2019s contributions to the U.S. economy as consumers, entrepreneurs, and employees play a key factor...\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"28% of our dataset are covered by more than one news article, for a total of 509,820 articles. This presents a rich corpus of multiply-covered stories: while in the present work, we do not utilize this direction, it opens the door for future work analyzing different possible coverage decisions.\\n\\n3 Press Release Coverage as Contrastive Summarization\\n\\nWe seek to identify when a news article effectively covers a press release, as defined by (Maat and de Jong, 2013). Identifying effective coverage is not trivial: many articles uncritically summarize press releases or use them peripherally in larger narratives. We examine pairs of news articles and press releases, answering the following two questions: (1) Is this news article substantially about this press release? (2) Does this news article challenge the information in the press release? While many articles discuss press releases, most of them simply repeat information from the release without offering insights. After examining hundreds of examples, we devise a novel framework, contrastive summarization, to describe \\\"effective coverage\\\". A piece of text is a contrastive summary if it not only conveys the information in a source document, but contextualizes and challenges it.\\n\\nCan we automatically detect when a piece of text is a contrastive summary? To do so, we represent each press release and news article as sequences of sentences, \\\\( \\\\vec{P} = p_1, \\\\ldots, p_n \\\\), \\\\( \\\\vec{N} = n_1, \\\\ldots, n_m \\\\), respectively. We establish the following two criteria:\\n\\n1. Criteria #1: \\\\( \\\\vec{N} \\\\) contextualizes \\\\( \\\\vec{P} \\\\) if:\\n   \\\\[\\n   \\\\sum_{j=1}^{n} P(\\\\text{references} | \\\\vec{N}, p_j) > \\\\lambda_1.\\n   \\\\]\\n\\n2. Criteria #2: \\\\( \\\\vec{N} \\\\) challenges \\\\( \\\\vec{P} \\\\) if:\\n   \\\\[\\n   \\\\sum_{j=1}^{n} P(\\\\text{contradicts} | \\\\vec{N}, p_j) > \\\\lambda_2.\\n   \\\\]\\n\\nWe define \\\"references\\\" (or \\\"contradicts\\\") as 1 if any sentence in \\\\( \\\\vec{N} \\\\) references (or contradicts) \\\\( p_j \\\\), 0 otherwise. Viewed in an NLI framework (Dagan et al., 2005), \\\"contradicts\\\" is as defined in NLI, and \\\"references\\\" = \\\\(\\\"entails\\\" \\\\lor \\\"contradicts\\\"\\\\). We expect this approach can get us close to our goal of discovering press releases that are substantially covered and challenged by news articles. A press release is substantially covered if enough of its information is factually consistent or contradicted by the news article. It's substantially challenged if enough of its sentences are contradicted by the news article. Laban et al. (2022) found that aggregating sentence-level NLI relations to the document-level improved factual consistency estimation. We take a nearly identical approach to the one shown in their work.\\n\\nFirst, we calculate sentence-level NLI relations, \\\\( P(y | p_i, n_j) \\\\), between all \\\\( \\\\vec{P} \\\\times \\\\vec{N} \\\\) sentence pairs. Then, we average the top-\\\\( k \\\\) inner relations for each \\\\( p_i \\\\), generating a \\\\( p_i \\\\)-level score. Finally, we average the top-\\\\( k \\\\) outer \\\\( p_i \\\\)-level scores. \\\\( k \\\\) inner is the number of times each press release sentence should be referenced before it is \\\"covered\\\", and \\\\( k \\\\) outer is the number of sentences that need to be \\\"covered\\\" to consider the entire press release to be substantially covered. Using NLI to identify press release/news article coverage pairs provides a computationally cheap and scalable method.\\n\\n3.1 Detecting Contrastive Summaries\\n\\nTo train a model to detect when a news article contrastively summarizes a press release, we annotate 1,100 pairs of articles and press releases with the two questions posed at the beginning of this section. Our annotations are done by two PhD students, where the first annotated all documents and the second doubly-annotated 50 articles, from which an agreement \\\\( \\\\kappa > 0.8 \\\\) is calculated. We divide these documents into an 80/10/10% train/val/test split. We test the variations: We test resolving coreferences in each document, (+coref). Coreference resolution can generate sharper predictions by incorporating more context into a sentence (Spangher et al., 2023). We also try three different classifiers: Logistic Regression (LogReg), a multilevel perceptron with \\\\( l \\\\) levels (MLP), and a binned-MLP (Hist), introduced in Laban et al. (2022).\\n\\nTable 2 shows how well we can detect contrastive summarization in press release-article pairs. We find that Hist +coref performed best, with 73.0 F1. Laban et al. (2022) noted that the histogram approach likely reduces the effect of outlier NLI scores. See Appendix B for more experiments.\\n\\nFollowing this, we apply Hist +coref to our entire PressRelease corpus, obtaining Doc-Level NLI scores for all pairs of articles and press releases in PressRelease. In the next section, we describe three primary insights we gain from analyzing these scores. Each insight sheds more light into how journalists cover press releases.\\n\\nThe only difference being that we also consider the contradiction relation, whereas they only consider entailment.\\n\\n11 Using LingMess (Otmazgin et al., 2022)\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: F1-scores for our classifiers, based on document-level NLI scores, to capture factual consistency in news covering press releases. We manually label press releases and news articles for whether they cover and challenge the press release. +coref resolution increases performance. (See Appendix B for more details and experiments.)\\n\\n|          | LogReg/MLP/Hist | +coref |\\n|----------|-----------------|--------|\\n| Q1: Does article cover press release? | 72.1 / 72.9 / 79.0 | 74.6 / 75.2 / 80.5 |\\n| Q2: Does article challenge press release? | 60.3 / 62.9 / 69.4 | 61.2 / 62.4 / 73.0 |\\n\\nTable 3: Correlation between doc-level NLI labels and the # sources in the article. Sources extracted via Spangher et al. (2023)'s source-attribution pipeline.\\n\\n|          | Correlation |\\n|----------|-------------|\\n| Contradiction | 0.50        |\\n| Entailment   | 0.29        |\\n| Neutral      | -0.50       |\\n\\nTable 4: Correlation between doc-level NLI labels and the creativity of planning steps journalists took (see Section 5.2 for more information about creativity measurement).\\n\\n|          | Correlation |\\n|----------|-------------|\\n| Contradiction | 0.29        |\\n| Entailment   | 0.27        |\\n| Neutral      | -0.07       |\\n\\nTable 5: Correlation between the level of contradiction between a news article and press release and the types of sources used in the news article. Types defined by Spangher et al. (2023).\\n\\n| Source Type | Correlation |\\n|-------------|-------------|\\n| Person-derived Quotes | 0.38        |\\n| Published Work/Press Report | 0.30        |\\n| Email/Social Media Post | 0.25        |\\n| Statement/Public Speech | 0.25        |\\n| Proposal/Order/Law | 0.25        |\\n| Court Proceeding | 0.18        |\\n\\nInsight #1: Effective news coverage incorporates both contextualization and challenging statements. Our first insight is that NLI-based classifiers can be useful for the task of identifying effective coverage. This is not entirely obvious: NLI classification is noisy (Nie et al., 2020) and contradiction relations might exist not only in directly opposing statements, but in ones that are orthogonal or slightly off-topic (Arakelyan et al., 2024). However, our strong results on a large annotated dataset\u2014our annotators were instructed to determine whether a news article effectively covers a press release\u2014indicate that this method is effective. Our performance results, between 70-80 F1-score, are within range of Laban et al. (2022) (66.4-89.5 F1 across 6 benchmarks), who first used NLI to evaluate vanilla summaries. That a similar methodology can work for both tasks emphasizes the relatedness of the two: identifying effective coverage is a version of identifying a summary. Thus, we call our task contrastive summarization, to describe the task of condensing and challenging information in a document.\\n\\nInsight #2: Articles that contradict and entail press releases (1) take more creative angles and (2) use more sources. We first noticed that articles with more creative angles contradict and entail press releases more, as shown in Table 4. In order to further explore these kinds of articles, we analyze the sources they used. Spangher et al. (2023) developed methods to identify informational sources mentioned in news articles. We utilize this work to identify sources in our corpus: as shown in Table 1, examples of sources we identify include a \\\"union\\\", an \\\"employee\\\" or a \\\"study\\\". We find that most news articles in our corpus use between 2 to 7 different sources, corresponding to Spangher et al. (2023)'s findings. Next, we correlate the number of sources in an article to the degree to which it contradicts or entails a press release. Interestingly, news articles that contradict press releases more also use more sources. Table 3 shows a strong correlation with creativity. Our methods for measuring creativity is defined further in Section 5.2.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"correlation of $r = .5$ between document-level contradiction and # sources. Articles in the top quartile of contradiction scores (i.e., $> .78$) using a median of 9 sources, while articles in the bottom quartile use 3.\\n\\nInsight #3: News articles that contradict press releases more use more resource-intensive sources.\\n\\nOf the kinds of sources used in news articles, the majority are either Quotes, 40%, (i.e., information derived directly from people the reporter spoke to), or Press Reports, 23% (i.e., information from other news articles). We obtain these labels by scoring our documents using models trained and described by Spangher et al. (2024a). As shown in Table 5, the use of Quotes, or person-derived information, is correlated more with Contradictory articles. Quotes are typically more resource-intensive to obtain than information derived from other news articles. A reporter usually obtains quotes through personal conversations with sources (Houston and Horvit, 2020); this is a longer process than simply deriving information from other news articles (Bruni and Comacchio, 2023). Additionally, in terms of the distribution of sources used in each article, Court Proceedings and Proposal/Order/Laws are overrepresented in Contradictory articles: they are 124% and 112% more likely to be used than in the average article. In general, these kinds of sources require journalistic expertise to assess and integrate (Machill et al., 2007), and might offer more interesting angles.\\n\\nTake-away: Taken together, our three insights suggest that any approach to assisting journalists in covering press releases must have an emphasis on (1) suggesting directions for contrastive summaries and (2) incorporating numerous sources. We take these insights forward into the next section, where we assess the abilities of LLMs to assist journalists.\\n\\n5 LLM-Based Document Planning\\n\\nBased on the insights in the previous section, we now study how LLMs might assist journalists. Specifically, we ask: How well can an LLM (1) provide a starting-point, or an \u201cangle\u201d, for a contrastive summary and (2) How well can an LLM suggest useful kinds of sources to utilize?\\n\\nPetridis et al. (2023) explored how LLMs can aid press release coverage. The authors used GPT-3.5 to identify potential controversies, identify areas to investigate, and ideate potential negative outcomes. They showed that LLMs serve as useful creative tools for journalists, reducing the cognitive load of consuming press releases. While promising, their sample was small: they tested 2 press releases and collected feedback from 12 journalists.\\n\\nWith our dataset, PressReleases, we are able to conduct a more comprehensive experiment to benchmark LLMs planning abilities. In this section, we identify 300 critical news articles and the press releases they cover. We compare plans generated by LLMs with the plans pursued by human journalists: such an approach, along with recent work (Tian et al., 2024), is part of an emerging template for comparing LLM creativity with human creativity and studying how LLMs might be used in human-in-the-loop creative pipelines.\\n\\n5.1 Experimental Design\\n\\nWe sample 300 press releases and articles scoring in the top 10% of contrastive summarization scores (identified by Hist. + coref in the previous section). We manually verify each to be a true example of effective coverage. By implication, these are press releases that contained ample material for human journalists to criticize. We use these to explore the critical directions LLMs will take.\\n\\nFigure 2 shows our overall process. In the first step, (1) LLM as a planner, we give an LLM the press release, mimicking an environment where the LLM is a creative aide. We prompt an LLM to \u201cde-spin\u201d the press release, or identify where it portrays the described events in an overly positive light, and suggest potential directions and sources to pursue.\\n\\nOur angle prompt builds off Petridis et al. (2023), however, our source prompt is novel, given the importance attributed to sources in Section 3. Next, (2) Human as a planner, we use another LLM to assess what the human actually did in their reporting. Finally, (3) Comparing, we assess how the LLM plans are similar or different from the human plans.\\n\\n5.2 Models and Evaluations\\n\\nWe consider two pre-trained closed models (GPT3.5 and GPT4__) and two high-performing open-source models (Mixtral (Jiang et al., 2024) and gpt-4-0125-preview, gpt-3.5-turbo-0125, as of February 9th, 2024.)\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Probing LLM\u2019s Planning Abilities: To assess how well LLMs might assist in the planning stages of article-writing, we attempt to compare the plans suggested by an LLM with the steps human journalists actually took during reporting. We infer these steps from the final article. In (1) \u201cGenerating an LLM plan\u201d, the LLM is asked to suggest angles and sources to pursue. In (2) \u201cAssessing the human\u2019s steps\u201d, we infer the steps the human took while writing the article by analyzing completed articles using LLMs. Finally, in (3) \u201cComparing\u201d, we compare how much of the LLM\u2019s plan aligns with the steps taken by the human.\\n\\n| Angle Source          | Prec | Recall | F1  | Prec | Recall | F1  |\\n|-----------------------|------|--------|-----|------|--------|-----|\\n| **Zero-shot**         |      |        |     |      |        |     |\\n| mixtral-8x7b          | 35.1 | 24.5   | 28.1| 15.7 | 16.3   | 14.7|\\n| command-r-35b         | 57.2 | 61.4   | 57.0| 28.5 | 26.2   | 25.1|\\n| gpt3.5                | 56.3 | 54.0   | 52.7| 23.8 | 15.5   | 17.8|\\n| gpt4                  | 53.6 | 63.4   | 56.3| 23.2 | 21.5   | 21.2|\\n| **Few-shot**          |      |        |     |      |        |     |\\n| mixtral-8x7b          | 40.8 | 28.9   | 31.8| 17.3 | 13.3   | 13.7|\\n| command-r-35b         | 55.7 | 60.0   | 56.1| 21.2 | 21.7   | 20.1|\\n| gpt3.5                | 53.3 | 51.0   | 48.7| 20.8 | 15.1   | 14.8|\\n| gpt4                  | 51.6 | 59.3   | 53.4| 19.5 | 17.9   | 17.8|\\n| **Fine-tuned gpt3.5** | 67.6 | 62.7   | 63.6| 31.9 | 27.5   | 27.9|\\n\\nTable 6: The plans and suggestions made by LLMs for covering press releases generally do not align with human journalists. Precision (Prec.) is the number of items from the plan that the journalist actually pursued (averaged per press release). Average Recall (Recall) is the number of items from the human-written article also suggested by the plan (averaged across news article). Angle is suggestions for directions to pursue, (Petridis et al., 2023), and is a combination of all points identified in parts #1 and #2 of Figure 5. Source is suggestions for sources to speak with, in general terms (e.g. \u201ca manager at the plant\u201d, \u201can industry expert\u201d).\\n\\nWe conduct experiments in 3 different settings: Zero-shot, where the LLM is given the press release and definitions for \u201cangle\u201d and \u201csource\u201d, and asked to generate plans. Few-shot, where the LLM is given 6 examples of press release summaries and the human-written plans. Finally, we fine-tune GPT3.5 on a training set composed of press releases paired with human plans. We give full prompts for all LLM queries run in this paper in the Appendix.\\n\\n---\\n\\n16 We use summaries to inform our few-shot examples because full press releases are too long for the context window.\\n17 We manually write the summaries and the plans.\\n18 Using OpenAI\u2019s fine-tuning API: https://platform.openai.com/docs/guides/fine-tuning\\n\\nEvaluation 1: Precision/Recall of LLM Plans\\nWe first analyze plans made by humans: we extract sources used in human-written news articles with models trained by Spangher et al. (2023). Then, we give GPT4, our strongest LLM, the press release and human-written news article and ask GPT4 to infer the angle that the author took. We manually validate a sample of 50 such angles and do not find any examples we disagree with. Finally, we use GPT4 to check how the sources and the angle proposed by the LLMs match the steps taken by the journalist. From this, we calculate Precision/Recall per document, which we average across the corpus.\\n\\nEvaluation 2: Creativity of the Plans\\nWe recruit two journalists as annotators to measure the...\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"creativity of the plans pursued both by the LLMs and the article authors. We develop a 5-point scale, inspired by Nylund (2013), who studied the journalistic ideation processes. They found that journalists engaged in processes of new-material ingestion, brainstorming in meetings to assess coverage trends, and individual ideation/investigation. In our scale, scores of 1-2 capture \u201cingestion\u201d, or a simplistic engagement and surface-level rebuttals of the press release; scores of 3-4 capture \u201ctrend analysis\u201d, or bigger-picture rebuttals; scores of 5 capture novel directions.\\n\\nResults\\nTable 6 shows the results of our matching experiment. We find that LLMs struggle to match the approaches taken by human journalists, but LLMs are better at suggesting angles than source ideas. Few-shot demonstrations do not seem to improve performance, in fact, we observe either neutral or declining performance. Fine-tuning, on the other hand, substantially improves the performance of GPT3.5, improving to 63.6 average recall for Angle suggestions and 27.9 average recall for Source suggestions, a 10-point increase in both categories.\\n\\nWe manually annotate 60 samples from the LLM matching to see if we concur with its annotations. We find an accuracy rate of 77%, or $\\\\kappa = 0.54$. The cases of disagreement we found were either when the LLMs plans were too vague, or contained multiple different suggestions: we usually marked these \u201cno\u201d while the LLM marked them \u201cyes\u201d.\\n\\nWe observe slight different results for creativity. As shown in Figure 6, creativity is overall lower for all categories of LLM: zero-shot, few-shot, and fine-tuning. However, in contrast to the prior experiment, we find that the differences between human/LLM creativity are relatively similar for source plans and angles. Further, when we observe the creativity of just the human plans that were retrieved by GPT3.5-fine-tuned, shown in Figure 7, we observe a similar pattern: the human plans matched to GPT3.5\u2019s plans are, overall, less creative than those that were not matched. We discuss the implications of these findings next.\\n\\nDiscussion\\nWe assessed how LLMs can help journalists plan and write news articles. We constructed a large corpus of news articles covering press releases to identify existing journalistic practices and evaluate how LLMs could support those processes. We found that LLM suggestions performed quite poorly compared with the reporting steps actually taken by humans, both in terms of alignment as well as creativity. Does this suggest that LLMs are poor planners in practice? Our benchmark provides a useful check for this question, but we do not believe our experiments here are conclusive. Instead, we view our approach as a first step: we compare basic prompt engineering with human actions that are observed from final-draft writing. Clearly, the final drafts written by humans result from multi-step, iterative reporting, accumulated experience, and real-world knowledge. While LLMs are not able to match many of these plans, they may nevertheless be helpful when paired with journalists.\\n\\nUsing human-decision making as a basis of comparison for LLMs is standard, even in creative, open-ended tasks: e.g. story-planning (Mostafazadeh et al., 2016), computational journalism (Spangher et al., 2024b, 2023, 2022) and others (Tian et al., 2023a). If this problem were unlearnable (e.g. there were simply too many an...\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gles to take, or so much prior knowledge needed to form any kind of plan), then we would not see any improvement after fine-tuning. Crucially, the 10-point improvement we observe from fine-tuning is evidence that there are learnable patterns. Existing research into journalism pedagogy, which implies that observation of other journalists' standard practice is as important as gaining subject-matter expertise and conducting on-the-ground work (Ryfe, 2023), should further support the hypothesis that planning is learnable.\\n\\nHowever, the low scores after fine-tuning imply the need for more fundamental work. Our current approach is naive: we expect LLMs to produce human-level plans with simple prompting and no references, besides the press release. There are two major directions for advancement in this task: (1) creativity-enhancing techniques: The creativity gap we observed between humans and LLMs reflect similar findings in other recent research related to creativity in AI (Harel-Canada et al.; Tian et al., 2023b; Gilhooly, 2023; Zhao et al., 2024). Chain-of-thought style prompts that explicitly include creative planning steps (Tian et al., 2024; Wei et al., 2022), or multi-LLM approaches (Zhao et al., 2024) could improve creativity. (2) retrieval-oriented grounding: we observe that many of failures in LLM plans are rooted in LLMs lack of awareness of prior events, even high-profile events that were within its training window (e.g. it interpreted many Theranos press releases without any awareness of the company's travails (Rogal, 2020)). Retrieval-augmented generation (Lewis et al., 2020) and tool-based approaches (Schick et al., 2023) might yield improvement.\\n\\nAs LLMs are increasingly used for planning-oriented creative tasks (Tian et al., 2024), careful analysis is required. Our goal in this work was to outline a novel task requiring planning and affirm a basic to perform this analysis. We believe that our use of LLMs in article planning represents an emerging and as-yet-underexplored application of LLMs to tasks upstream of the final writing output. In these cases, the decisions made by the LLM might one day have the ability to impact even more fundamental steps: which sources to talk to, which angles to take, and which details to highlight. Professional journalists ground their approach to these decisions in institutional values: fairness, reducing sourcing bias, and confirming details. Without carefully comparing the steps that LLMs make with humans, we risk disregarding these values.\\n\\nRelated Work\\nOur work is inspired by the task outlined in AngleKindling (Petridis et al., 2023), which introduced LLM-assistants for press release coverage as a useful writing tool and utilized LLMs to summarize press releases and suggest angles. Our work fits into a larger literature utilizing LLMs as writing assistants (Yeh et al., 2024; Quere et al., 2024; Mirowski et al., 2023). We take a data-driven approach toward identifying journalists' needs through corpus and benchmark construction. Whether LLMs can serve as effective planners in creative acts is currently an unresolved debate (Kambhampati et al., 2024; Chakrabarty et al., 2023). However, the two-step process of planning then creating has been explored extensively (Yao et al., 2019; Alhussain and Azmi, 2021; Rashkin et al., 2020). Our work aims to build in this direction by constructing an evaluation set.\\n\\nWe see broad parallels between the notion of a plan, which is an unobserved generative process preceding the generation of observable text, and earlier generations of discrete latent variable modeling (Bamman et al., 2013, 2014; Blei et al., 2003). Work like (Spangher et al., 2024a) seeks to extend concepts and framing in this work into a more modern era by selecting the best plan from multiple plans. We believe that various approaches are converging to a novel approach to LLM and human interaction, and we hope that our work serves as a good addition and a useful benchmark.\\n\\nConclusion\\nWe have built a corpus to study professional human planning decisions by identifying well-reported news articles covering press releases. These are articles use a variety sources, engage in criticism, and challenge the source material (Maat and de Jong, 2013). We assessed how LLMs could suggest plans for covering source documents for these articles. Our goal is to ground LLM planning in the observation of human dynamics, opening the door to aligning future developments to journalistic practice. Our approach captures more broadly the objectives of human journalists across many different organizations, across decades of coverage. Our benchmark compares the plans an LLM makes to approaches taken by journalists who were covering press releases in real-life settings, and establishes a new direction for exploring how LLMs can support the journalistic process.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"10 Ethical Considerations\\n\\n10.1 Privacy\\nWe believe that there are no adverse privacy implications in this dataset. The dataset comprises news articles and press releases that were already published in the public domain with the expectation of widespread distribution. We did not engage in any concerted effort to assess whether information within the dataset was libelous, slanderous, or otherwise unprotected speech. We instructed annotators to be aware that this was a possibility and to report to us if they saw anything, but we did not receive any reports. We discuss this more below.\\n\\n10.2 Limitations and Risks\\nThe primary theoretical limitation in our work is that we did not include a robust non-Western language source. This work should be viewed with that important caveat. We cannot assume a priori that all cultures necessarily follow this approach to breaking news. Indeed, all of the theoretical works that we cite in justifying our directions also focus on English-language newspapers. So, we do not have a good basis for generalizing any of our claims about LLM planning outside of the U.S.\\n\\nAnother limitation is our core assumption that human planning is the gold-standard. We tried address this limitation by also considering creativity as a secondary evaluation of plans. But there are other ways to assess a plan in creative endeavors, including factuality, robustness, or efficiency. We did not consider any of these metrics. Thus, our evaluations might be overly harsh towards LLMs and fail to evaluate some of the ways their plans might be different but equal to human plans.\\n\\nOur dataset has some risks. Because we include instances of major corporate malfeasance, like Enron or Theanos, we might be including news coverage that is particularly angled, opinionated, or extreme. These may not represent the core beat needs of typical business reporting. We tried to address this by evaluating over a large dataset. In line with this, another possible risk is that some of the information contained in our dataset contains unprotected speech: libel, slander, etc. Instances of First Amendment lawsuits where the plaintiff was successful in challenging content are rare in the United States. We are not as familiar with the guidelines of protected speech in other countries.\\n\\n10.3 Computational Resources\\nThe experiments in our paper require computational resources. Our models run on a single 30GB NVIDIA V100 GPU or on one A40 GPU, along with storage and CPU capabilities provided by our campus. While our experiments do not need to leverage model or data parallelism, we still recognize that not all researchers have access to this resource level.\\n\\nWe use Huggingface models for our predictive tasks, and we will release the code of all the custom architectures that we construct. Our models do not exceed 300 million parameters.\\n\\n10.4 Annotators\\nWe recruited annotators our academic network. All the annotators consented to annotate as part of the experiment, and were paid $1 per task, above the highest minimum wage in the U.S. Both were based in large U.S. cities. One annotator identified as white, and one as Asian. Both identified as male. This data collection process is covered under a university IRB. We do not publish personal details about the annotations, and their annotations were given with consent and full awareness that they would be published in full.\\n\\nReferences\\nArwa I Alhussain and Aqil M Azmi. 2021. Automatic story generation: A survey of approaches. ACM Computing Surveys (CSUR), 54(5):1\u201338.\\n\\nCharles Angelucci and Julia Cag\u00e9. 2019. Newspapers in times of low advertising revenues. American Economic Journal: Microeconomics, 11(3):319\u2013364.\\n\\nErik Arakelyan, Zhaoqi Liu, and Isabelle Augenstein. 2024. Semantic sensitivities and inconsistent predictions: Measuring the fragility of NLI models. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 432\u2013444.\\n\\nDavid Bamman, Brendan O\u2019Connor, and Noah A Smith. 2013. Learning latent personas of film characters. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 352\u2013361.\\n\\nDavid Bamman, Ted Underwood, and Noah A Smith. 2014. A Bayesian mixed effects model of literary character. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 370\u2013379.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1216", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-1216", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Our approach for identifying news articles that cover and challenge press releases. Inspired by La-ban et al. (2022), we obtain doc-level NLI labels from sentence-level NLI relations, $p(y|p_i,n_j)$, by (1) averaging, for each $p_i$, the top $k$ inner $(p_i,n_j)$ predictions, and then (2) averaging across the top $k$ outer $p_i$-level scores. Coverage is satisfied if enough sentence-pairs do not have neutral relations. Challenging is satisfied if enough sentence-pairs have contradiction relations.\\n\\nA Additional Dataset Processing\\n\\nWe clean each news article and press release's text in the following ways. Of the retrievals, 80% are HTML, 10% are XML, 5% are DOCX and 2% are PDFs. We exclude XML, as these are usually news feeds. For HTML documents, we strip all tags except <a> tags, which we use to determine link position in the document. We exclude links that are referenced in the bottom 50% of the document, as these are also usually feeds. We parse text from DOCX using docx-parser. We parse PDF documents using the pdf2image Python library. This leaves us with full text for 500,000 documents.\\n\\nWe remove short sentences and non-article sentences (e.g. \u201cSign up for... here!\u201d) by running a news article sentence classifier which identifies non-article sentences with high accuracy (Spangher et al., 2021). Additionally, we exclude press release and article pairs that are published chronologically far apart (>1 month difference). Such timescales tend to occur when the press release is used as a archival reference in the news article, not as a main topic of coverage. We find that existing parsing libraries do not reliably extract dates from articles and press releases, so we query Wayback Machine to find the earliest collection-timestamps of the documents. A manual analysis of 50 articles confirms that this approach is reliable.\\n\\nB Doc-Level NLI Experimental Details\\n\\nWe define Document-Level NLI as an aggregation over all pairwise Sentence-Level NLI relations. Figure 5 shows our process: first, we calculate sentence-level NLI relations, $p(y|p_i,n_j)$, between all $\\\\vec{P} \\\\times \\\\vec{N}$ sentence pairs. Then, we average the top-$k$ inner relations for each $p_i$, generating a $p_i$-level scores.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Description of the 5-point creativity scale that we used to evaluate press releases. Based on Nylund (2013), our scale captures different levels of creative ideation: direct engagement with the press release (1-2), contextual/trend-level rebuttals (3-4), substantial and novel investigatory directions.\\n\\nTrial F1 Score\\n\\n|                  | outer | inner |\\n|------------------|-------|-------|\\n|                 | k     | k     |\\n| Con. Ent. Neut.  | 72.1  | 70    |\\n|                 | 72.9  | 72    |\\n|                 | 79.0  | 71    |\\n|                 | 20    | 5     |\\n|                 | 22    | 20    |\\n| Q1: Does the news article cover the press release? | LogReg/MLP/Hist | +coref |\\n|                 | 74.6  | 74    |\\n|                 | 75.2  | 76    |\\n|                 | 80.5  | 68    |\\n|                 | 40    | 45    |\\n|                 | 78    | 74    |\\n|                 | 90    | 95    |\\n|                 | 7     | 5     |\\n|                 | 33    | 10    |\\n|                 | 34    | 20    |\\n| Q2: If so, does the news article challenge information in the press release? | LogReg/MLP/Hist | +coref |\\n|                 | 60.3  | 45    |\\n|                 | 62.9  | 74    |\\n|                 | 69.4  | 95    |\\n|                 | 40    | 5     |\\n|                 | 78    | 10    |\\n|                 | 90    | 30    |\\n\\nTable 8: Ability of sentence-level NLI-relational metrics to capture effective coverage. We show F1-scores on a set of 100 pairs of press releases and news articles manually labeled.\\n\\nk<sub>outer</sub> and k<sub>inner</sub> columns are hyperparameter settings: k<sub>inner</sub> shows how many news article sentences must contradict/entail a sentence in the press release. k<sub>outer</sub> shows how many sentences in the press release should be considered in the overall doc-level calculation. Coref resolution increases performance of doc-level NLI and enables lower k<sub>inner</sub>, k<sub>outer</sub>, indicating more precision.\\n\\nFinally, we average the top-k<sub>outer</sub>-level scores. Document-Level NLI following is:\\n\\n$$\\\\text{NLI-Doc}(y|\\\\vec{P}, \\\\vec{N}) = \\\\frac{1}{k_{outer}} \\\\sum_{i=1}^{s(1)} \\\\ldots s(k_{outer}) \\\\left[ \\\\frac{1}{k_{inner}} \\\\sum_{j=1}^{s(1)} \\\\ldots s(k_{inner}) p(y|p_i,n_j) \\\\right]$$\\n\\nWhere s(1) ... s(n) is a list of indices sorted according to the value of the inner equation. If y \u2208 {entail, contradict}, we sort descending, if y = neutral we sort ascending. Intuitively, this approach gets us close to our goal of discovering press releases that are substantially covered by news articles: a press release is substantially covered if enough of its sentences' information is used or challenged by the news article.\\n\\nk<sub>inner</sub> (k<sub>inner</sub>) sets a level for which each press release sentence should be referenced before it is determined to have been \u201ccovered\u201d, and k<sub>outer</sub> (k<sub>outer</sub>) sets a level for how many of these sentences are enough to consider the entire press release to be substantially covered. With Figure 5 an example: (p\u2081, n\u2081) strongly entail each other while (p\u2082, n\u2082), (p\u2082, n\u2083) contradict. All other pairs (e.g. (p\u2081, n\u2083)) are neutral.\\n\\nAt k<sub>inner</sub> = 2, p\u2081 would get an entailment score of \u223c5.5, while p\u2082 would get a contradiction score of \u223c915. All other {entail, contradict} scores would be low while neutral would be high. At k<sub>outer</sub> = 2, the documents would have an entailment score of \u223c25, a contradiction score of \u223c5, and a neutral score of \u223c3.\\n\\nFigure 8 shows the best settings of the hyperparameters, k<sub>inner</sub> and k<sub>outer</sub> are within expectation. After resolving coreferences, we find 5-10 news article sentences contradict or entail a press release sentence before it is meaningfully addressed. On the other hand, much more sentence pairs must be neutral before the sentence is considered neutral. Overall, we find that resolving coreferences before performing sentence-level NLI improves performance: it both increases the overall f1-score, and it narrows the k<sub>inner</sub>, k<sub>outer</sub> thresholds, indicating overall precision increases.\"}"}
{"id": "emnlp-2024-main-1216", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ADUHELM, a treatment for Alzheimer's disease, has been granted accelerated approval based on its ability to reduce amyloid beta plaques in the brain, marking a significant advancement in Alzheimer's treatment, with continued approval contingent on further verification of clinical benefits.\\n\\nThe news piece might focus on the need for another trial to confirm the drug's clinical benefit, indicating that the drug's approval could be seen as provisional or not fully justified by existing evidence. Exploring the concerns raised by healthcare providers and experts about the accelerated approval process and the need for more substantial evidence of clinical benefit from confirmatory trials post-approval.\\n\\nSource: Gilead Sciences' Chairman and CEO, Daniel O'Day, announced that the company is rapidly advancing clinical trials for remdesivir as a potential COVID-19 treatment, emphasizing a commitment to safety, efficacy, and accessibility, while also expanding compassionate use to meet urgent patient needs. Medical professionals and bioethicists might comment on the ethical considerations and challenges of drug distribution during a pandemic. Potential sources to speak to include healthcare professionals involved in the clinical trials of remdesivir, as well as bioethicists who can provide insights into the ethical considerations surrounding the drug's distribution and use.\\n\\nAngle: Elon Musk is considering taking Tesla private at $420 per share, a move aimed at benefiting shareholders and enhancing Tesla's mission, with funding discussions ongoing, including significant interest from the Saudi Arabian sovereign wealth fund. The news article might carefully examine Elon Musk's claims in the press release about having secured funding to take Tesla private. Potential controversies to investigate include the timing and handling of Musk's announcement, particularly the claim of 'funding secured' and its impact on Tesla's stock price and investor perceptions.\\n\\nSource: Theranos refutes allegations in a Wall Street Journal article by highlighting its commitment to accuracy and reliability through FDA clearances, partnerships, and industry-leading transparency, while criticizing the Journal's reliance on uninformed and biased sources. Former Theranos employees and their families provide insider perspectives on the company's operations and challenges. Speaking to current and former employees of Theranos to get a more balanced perspective on the company's operations and technology.\"}"}
