{"id": "lrec-2024-main-1428", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Post-Interaction survey\\n\\nThe survey given to users after interacting with their assigned style of chatbot can be seen below. Users were asked to fill out a usability questionnaire (Finstad, 2010) and the trust and reliability subscales from the trust in automation questionnaire (K\u00f6rber, 2018).\"}"}
{"id": "lrec-2024-main-1428", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards a Zero-Data, Controllable, Adaptive Dialog System\\n\\nDirk V\u00e4th, Lindsey Vanderlyn, Ngoc Thang Vu\\nUniversity of Stuttgart\\nStuttgart, Germany\\n{vaethdk|vanderly|thangvu}@ims.uni-stuttgart.de\\n\\nAbstract\\nConversational Tree Search (V\u00e4th et al., 2023) is a recent approach to controllable dialog systems, where domain experts shape the behavior of a Reinforcement Learning agent through a dialog tree. The agent learns to efficiently navigate this tree, while adapting to information needs, e.g., domain familiarity, of different users. However, the need for additional training data hinders deployment in new domains. To address this, we explore approaches to generate this data directly from dialog trees. We improve the original approach, and show that agents trained on synthetic data can achieve comparable dialog success to models trained on human data, both when using a commercial Large Language Model for generation, or when using a smaller open-source model, running on a single GPU. We further demonstrate the scalability of our approach by collecting and testing on two new datasets: ONBOARD, a new domain helping foreign residents moving to a new city, and the medical domain DIAGNOSE, a subset of Wikipedia articles related to scalp and head symptoms. Finally, we perform human testing, where no statistically significant differences were found in either objective or subjective measures between models trained on human and generated data.\\n\\nKeywords: Conversational Systems/Dialogue/Chatbots, Corpus, Usability, User Satisfaction\\n\\n1. Introduction\\nWhile the breakthroughs of modern Large Language Models (LLMs) have made the creation of new dialog systems much easier, controlling their generated output remains an open challenge. This makes LLMs especially unsuitable for sensitive domains, e.g., legal or medical domains, where users must be able to implicitly trust the system's output. In such domains, dialog designers usually have the choice between implementing an FAQ-retrieval system or a hand-crafted dialog system.\\n\\nFAQ systems directly match user queries to question/answer pairs curated by domain experts, allowing close control of outputted texts (Wu et al., 2005). However, as they are single-turn systems and cannot ask clarifying questions, they are only able provide general answers, rather than personalized content for a specific user and their situation. Including information for multiple cases in one answer would make them unapproachably long, while adding FAQs for each case, would make retrieval challenging. Retrieval accuracy itself is an open challenge (Thakur et al., 2021), creating a trade-off: Either providing a single, possibly incorrect answer to a user's question, or providing multiple answers and shifting the burden of selecting the correct one to the user, which might be challenging for users unfamiliar with the domain.\\n\\nDialog systems, in contrast, allow for turn-based interactions, which can provide shorter, personalized answers, as well as support users new to a domain without enough experience to formulate precise questions. However, such systems either suffer from longer interactions (for handcrafted systems), or require large amounts of training data (Raghu et al., 2021) and lack transparency and controllability (Gao et al., 2018) (in the case of machine learning approaches), making them less suitable for low-resource settings (Zhang et al., 2020) or sensitive domains (Cohen, 2020).\\n\\nV\u00e4th et al. (2023) address this problem by proposing a new type of hybrid dialog task bridging these two interaction styles, called Conversational Tree Search (CTS). In this task, dialog experts first define a dialog tree. An agent then learns to either walk the user through each node in the tree, or to skip over parts not required to answer a user's more specific question. In this way, the agent is able to adapt its behavior to the user's preferred interaction style, supporting both specific and vague user queries, without sacrificing the controllability required in sensitive domains.\\n\\nHowever, CTS still requires that dialog designers collect a corpus of real-user utterances, which poses a barrier to scaling this approach to new domains, especially for large and complicated domains. The goal of this paper is to remove this barrier by exploring how CTS can scale to new domains through the use of synthetically generated training data.\\n\\nConcretely, we seek to answer the following research questions:\\n\\n\u2022 (RQ1) How can we effectively generate data for a zero data approach to training CTS agents?\\n\\n\u2013 (RQ1.1) How can we analyze the quality of generated data?\"}"}
{"id": "lrec-2024-main-1428", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2013 (RQ1.2) How do agents trained on generated data perform in simulation, compared to agents trained on human data?\\n\\n\u2013 (RQ1.3) How well do the data generation techniques transfer to new domains?\\n\\n\u2022 (RQ2) How does a CTS agent trained on generated data perform with real users compared to an agent trained on human data?\\n\\nTo address these questions, we investigate how LLMs can be leveraged to automatically generate training data for new domains, while at the same time maintaining the controllability aspect of the CTS task. We compare the quality of different data generation schemes by evaluating the performance of Reinforcement Learning (RL) agents trained on the synthetic data. Then, we test scalability of our approach to new domains in simulation using multiple generative LLMs. Finally, we perform user testing to verify the transferability to real-world cases. All code and data is publicly available.\\n\\nOur main contributions are: 1) Creating two new datasets, ONBOARD and DIAGNOSE. 2) Improving the training procedure for the CTS agent, increasing absolute dialog success by more than 18%. 3) Introducing a new prompting method for generating diverse data, and demonstrating that automatic diversity and answerability metrics can provide insights for downstream dialog performance. 4) Demonstrating that our generation techniques scale to new domains, where agents trained on synthetic data show comparable (no statistically significant difference) or better dialog success than agents trained on human data. 5) Showing that success of agents in simulation translates to successful interactions with real users, with no statistically significant differences.\\n\\n2. Related Work\\n\\n2.1. Task-oriented Dialog Systems\\n\\nWhile open-domain dialog systems allow users to freely talk about any topic without a concrete goal, task-oriented dialog systems focus on helping a user reach a specific goal. Many task-oriented dialog systems use a slot-filling approach, where the dialog system tries to fill values for a selection of slots, e.g., cuisine type, that are necessary to reach that goal from the user (Bobrow et al., 1977). While slot filling approaches can allow hand-crafted dialog policies to follow pre-defined dialog flows (Lucas, 2000), or can help efficiently narrowing down searches across e.g. database rows, such as finding restaurants or getting trip recommendations (Louvan and Magnini, 2020), they are usually unable to perform semantic searches over the dialog domain and in cases of learned systems, unable to follow a dialog-designer controlled flow.\\n\\n2.2. Adaptive Dialog Systems\\n\\nResearch into adaptive dialog systems aims to better align dialog system output with user expectations. Much research in this area uses generative models to adapt linguistic style, e.g., adjusting utterances depending on users' emotional states (Ma et al., 2020) or personalities (Yang et al., 2018; Firdaus et al., 2023). However, generative models are by their nature difficult to control (Du\u0161ek and Kasner, 2020). Some approaches even adapt the complexity of language (Janarthanam and Lemon, 2014). In order to adapt underlying system behavior, however, additional cues have usually been required, e.g. social cues like laughter (Ritschel and Andr\u00e9, 2018), or explicit fine-tuning by the user (Chen and Pu, 2012; Narducci et al., 2018). However, eliciting such social cues is difficult for text-based systems and asking for explicit feedback places extra burden on the user.\\n\\n2.3. Controllable Dialog Systems\\n\\nIn sensitive domains, it is crucial subject-experts maintain control of dialog flow to ensure correctness of system outputs. However, purely hand-crafted systems struggle to handle the breadth of possible user inputs. To this end, several hybrid approaches have been investigated. Early approaches involved hand-crafting the set of actions allowed at a given dialog turn (Williams, 2008). More recent approaches expand on this idea for neural systems (Williams et al., 2017; Liang and Yang, 2018; Razumovskaia and Eskenazi, 2019), where the action space can be constrained using masks, e.g., by automatically converting expert designed dialog trees into hybrid code networks (Shukla et al., 2020). While such approaches help control dialog agent behavior, they do not provide a mechanism for skipping portions of a dialog irrelevant to a user, which leads to longer interactions that can be frustrating for users with more domain familiarity.\\n\\n2.4. Data Generation and Augmentation\\n\\nCommon data augmentation approaches include lexical substitution (Wei and Zou, 2019), where tokens are inserted, deleted or substituted with semantically similar replacements, as well as back-translation (Sennrich et al., 2016) where data is...\"}"}
{"id": "lrec-2024-main-1428", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"16435\\n\\nWhile such approaches can help to expand an existing dataset, they still require seed data, which may not exist for new domains.\\n\\nTo address this, research in, e.g., the field of low-resource Question Answering (QA) has started exploring the role of LLMs in data generation (Puri et al., 2020; Chen et al., 2023). Given a text, LLMs can be prompted to generate questions about it, e.g., by asking the model to generate a question for which a given named entity is the answer (Li et al., 2023).\\n\\nHowever, LLMs are black-box algorithms and suffer from hallucination (Azaria and Mitchell, 2023; Peng et al., 2023; Manakul et al., 2023). As such, it is difficult to guarantee that the generated questions are logical, natural, or answerable by the original text. Moreover, commonly used automatic evaluation metrics for text generation do not necessarily correlate with human judgment (Nema and Khapra, 2018). In light of this, we explore different generation strategies and techniques for analyzing the artificial data quality, rather than trusting a single metric.\\n\\nA recent approach in the dialog community trains a model for generating synthetic dialog acts and user utterances for flowchart-grounded troubleshooting dialogs (Zhan et al., 2023). While this method also relies on the domain representation in form of a structured graph, our generation approach does not require any model training, nor any training data besides the domain graph itself. Additionally, CTS is not limited to the specific task format of trouble-shooting dialogs.\\n\\n2.5. Conversational Tree Search\\n\\nThe goal of CTS, as outlined by V\u00e4th et al. (2023), is to train an RL agent to traverse a dialog tree, guiding a user to the answer for a given question. By using fixed system outputs (which can be personalized via a template mechanism), and by preventing skipping between branches of the dialog tree, the CTS task allows subject-experts to maintain controllability.\\n\\nAt the same time, the trained agent can adapt its behavior to different interaction styles, based on the users' utterances. CTS proposes two sub-tasks: guided mode and free mode, representing the extreme cases of information seeking scenarios, as well as the interpolation between. Guided mode supports users unable to formulate their information need as a specific question, by guiding them step-by-step through each node in the dialog graph (e.g., new users not familiar with a domain). In contrast, free mode aims to support users with a specific question by learning to skip over as many nodes as possible, while still clarifying the information need enough to deliver an appropriate and personalized answer.\\n\\nFigure 1 shows three example dialogs for the same user goal, and how a CTS agent would adapt to each scenario, deciding to output or skip nodes as needed.\\n\\nTraining is performed against a simulated user, which represents the RL environment. For each simulated dialog, a random goal node is drawn which the simulated user is trying to reach, by asking questions or responding to system requests.\\n\\n3. Datasets\\n\\nTo investigate the scalability of our data generation techniques, we examine the performance of the CTS agent on three new datasets, and compare to the original REIMBURSE dataset from V\u00e4th et al. (2023). In contrast to the REIMBURSE dataset, the goal of all new datasets is to serve as a zero-data test-bed for testing training and testing models on data generated directly from the nodes themselves. While we do provide a test and a train set, like that in REIMBURSE, the goal of this is to allow for the training of reference models to act as a benchmark for models trained entirely on generated data.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset  | Split | #Nodes | Tree Depth | Max. Node Degree | #User Questions | Avg. User Questions | #Answer Paraphrases | Avg. Answer Paraphrases |\\n|---------|-------|--------|------------|------------------|----------------|---------------------|---------------------|------------------------|\\n| REIMBURSE | Train | 123    | 3.2        | 14               | 279            | 3.5                 | 246                 | 3.4                    |\\n|         | Test  | 173    | 2.2        | 162              | 2.2            |                     |                     |                        |\\n| REIMBURSE-En | Train | 123    | 3.2        | 14               | 279            | 3.5                 | 246                 | 3.4                    |\\n|         | Test  | 173    | 2.2        | 162              | 2.2            |                     |                     |                        |\\n| DIAGNOSE | Train | 98     | 2.9        | 6                | 219            | 2.9                 | 298                 | 3.0                    |\\n|         | Test  | 150    | 2.0        | 298              | 3.0            |                     |                     |                        |\\n| ONBOARD  | Train | 88     | 2.4        | 9                | 141            | 2.4                 | 175                 | 3.1                    |\\n|         | Test  | 117    | 2.0        | 152              | 2.7            |                     |                     |                        |\\n\\nTable 1: Overview of original REIMBURSE, translated REIMBURSE-En, and newly created DIAGNOSE and ONBOARD datasets (numbers rounded to one decimal).\\n\\nCTS task. It is a challenging real-world dataset in the travel reimbursement domain, created with domain experts. Along with the dialog tree, questions and answer paraphrases were collected from real user interactions. These questions and answer-paraphrases have been split into a train and test set which can each be used by the provided user simulator to generate an arbitrary number of simulated dialogs. A breakdown of the dataset statistics can be found in Table 1.\\n\\nAlthough we do not train any new models on this dataset, we use it as a benchmark to compare the performance of our agents to.\\n\\n3.2. REIMBURSE-En\\nIn order to make the CTS task more accessible to a wider audience, we choose to translate the REIMBURSE dataset to English. Additionally, this opens up more options for language models and resources, which might not have been available for the original German data. This dataset represents a direct translation of the REIMBURSE dataset, sharing all of the same characteristics, in order to allow for comparisons to the findings of the original CTS paper. The translation was performed manually by a bilingual domain-expert in order to obtain a faithful and factually correct English equivalent. Dataset statistics are shown in Table 1.\\n\\n3.3. DIAGNOSE\\nThe DIAGNOSE dataset was created for the medical domain. It was designed to help users identify different medical conditions based on symptoms, as well as to find out more about treatment options and risk factors. The dataset is based on a small subset of Wikipedia articles about conditions related to scalp and head symptoms. DIAGNOSE was designed to be comparatively easy. Even though the node texts contain a large amount of domain-specific vocabulary, the dialog tree has a lower maximum node degree and a shallower tree depth than REIMBURSE-En. Additionally, the dialog graph for this domain does not contain any variable- or logic nodes. A breakdown of dataset properties can be found in Table 1.\\n\\nAn example node and associated questions can be seen below:\\n\\nNODE TEXT: Anemia symptoms include fatigue, pale skin and gums, blue color in the whites of the eyes, brittle nails, irritability, dizziness, sore tongue, shortness of breath, unusual food cravings, and headache.\\n\\nQUESTION 1: What are symptoms of anemia?\\nQUESTION 2: How do I know if I have anemia?\\nQUESTION 3: Is a sore tongue a common symptom of anemia?\\n\\n3.4. ONBOARD\\nThe ONBOARD dataset provides users with information about moving to a new city in a foreign country, and the legal and financial steps they will need to undertake, i.e., setting up bank accounts, acquiring health insurance, applying for required visas or residence permits, etc. This domain presents an additional challenge as it contains code-switching for topics related to legal issues, in order to provide users with official names for documents, concepts, and institutions. Similar to the REIMBURSE dataset, the dialog tree for ONBOARD contains multiple variable nodes and several logic nodes. A breakdown of the dataset statistics can be found in Table 1.\\n\\nAn example of a dialog node and test questions is given below:\\n\\nNODE TEXT: The registration office will provide you with a confirmation of your registration [Meldebest\u00e4tigung], which you will need for opening a bank account and for obtaining a residence permit (if applicable).\\n\\nQUESTION 1: Where do I get confirmation that I've registered my address?\\nQUESTION 2: What do I need the confirmation of registering my address for?\"}"}
{"id": "lrec-2024-main-1428", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dialog Agent Implementation\\n\\nFor our RL dialog agent, we follow the architecture and training process outlined in (V\u00e4th et al., 2023) with the following changes:\\n\\n1) We swap the original language model for an MPNET (Song et al., 2020) based Sentence-Transformer (Reimers and Gurevych, 2019), as the new datasets we introduce are in English, and it reports the highest average performance of pre-trained Sentence-Transformers for English.\\n\\n2) In contrast to free mode, rewards for guided mode only considered whether the agent moved to the correct next node, rather than checking that a global goal was reached by the end of the dialog. After analyzing conversations between CTS agent and user simulator obtained by the original implementation, we believe it is more realistic that, even in guided mode, users would have a consistent question they wanted answered. Therefore, we now draw global goals for guided mode users (a node anywhere in the graph) instead of choosing one of the immediate neighboring nodes as the next goal each turn. We then assign a large reward to reaching the global goal. At the same time, we keep a small positive reward for skipping to the correct follow-up node along the sampled trajectory, as a sequence of locally correct decisions (reaching a correct immediate neighbor) implies global correctness (reaching the correct goal node). These changes result in a harsher evaluation metric for dialog success, since e.g. in a 5-step dialog, following a correct trajectory, but missing the final goal in the last turn, will now result in a failed dialog (0% success) instead of a partially successful dialog (80% success), which we consider to be more realistic.\\n\\n3) Finally, the original CTS agent was trained jointly on navigating the graph and on predicting the appropriate interaction style (intent). Here, we scale the loss of the interaction style prediction objective down to 0.1 to emphasize learning Q-values as the main task: $L_{\\\\text{ddqn}} + 0.1L_{\\\\text{intent}}$. We found this had no significant impact on the interaction style prediction F1 score.\\n\\n4) We tune several other hyperparameters, increasing the batch size from 128 to 256, and the training steps from $1.5e6$ to $2e6$.\\n\\nAll hyperparameters for training the dialog agent are listed in Appendix A.\\n\\n5. Data Generation Methods\\n\\nAs the user simulator from V\u00e4th et al. (2023) requires both, initial user questions and per-node user responses, we explore methods for generating both of these types of utterances. We test these generation methods with a small LLM, and with a large commercial one, both of which can process separate system and user input directives.\\n\\n5.1. Question Generation\\n\\nMethod 1\\n\\nThe first method, Gen$V_1$, is a naive prompt instructing an LLM to generate diverse, FAQ-style questions about a given dialog node's text via the system directive. The amount of questions to generate and the node context are then given via user input (see Table 2).\\n\\nMethod 2\\n\\nFor Gen$V_2$, we use the same user input, but change the system directive to explicitly generate shorter questions (Table 2).\\n\\nMethod 3\\n\\nFor the last method, Gen$V_3$, we were inspired by Li et al. (2023) and Chen et al. (2023), who use Named Entity Recognition (NER) to steer question generation. However, these approaches only generate cloze questions, where the named entity is the answer, severely limiting the diversity of generated questions (Puri et al., 2020). Therefore, we develop a novel mixed method to increase question diversity. We first generate 3 questions about the whole node text using the Method 2, to get a basic coverage of the node. Then, we perform NER and explicitly prompt the LLM to generate three questions about each entity \u2013instead of forcing the entities to only be the answer\u2013 using a second set of prompts (see Table 2). If the total number of generated questions is lower than 10, we generate the difference using Method 2.\\n\\n5.2. Response Generation\\n\\nTo generate responses, we extract all nodes requiring user input from the dialog graph. Then, we instruct the LLMs to generate 5 paraphrases for each possible answer prototype, in the context of the full node text (Table 3; A). Additionally, to mimic different user interaction styles, we instruct the LLMs to generate 5 paraphrases of the responses using only keywords (Table 3; B).\\n\\n6. Experimental Setup\\n\\n6.1. RQ 1.1: Analysis of Generated Data\\n\\nWe generate data using the methods described in sections 5.1 and 5.2. We use two different LLMs: ChatGPT (gpt-3.5-turbo, via API) and a LLAMA-based (Touvron et al., 2023), instruction fine-tuned and quantized model that fits onto a\\n\\n2 https://platform.openai.com/docs/models/gpt-3-5\\n3 https://huggingface.co/TheBloke/upstage-llama-30b-instruct-2048-GPTQ\"}"}
{"id": "lrec-2024-main-1428", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are a truthful assistant, generating diverse FAQ-style questions given some facts. The generated questions should be answerable using the given fact only, without additional knowledge. The questions should also be human-like. Try to vary the amount of information between questions. Present the results in a numbered list.\\n\\nYou are generating semantically similar paraphrases for a given response to some question. The generated response paraphrases should be human-like and short, using frequently used words and phrases only. Present the results in a numbered list.\\n\\nTo calculate question similarity, we use the Sentence Transformer model from section 4. Answer confidence scores are calculated with a QA model pretrained on the SQUAD2.0 dataset (Rajpurkar et al., 2018), using a generated question and associated node text that is supposed to contain the answer as inputs. Finally, we measure diversity using Self-BLEU (Zhu et al., 2018) scores.\\n\\n6.2. RQ 1.2: Human Data vs. Synthetic Data\\nFor automatic evaluation, we use the updated CTS user simulator (section 4) with 500 randomly chosen dialog goals on the REIMBURSE-En test split. We evaluate not only the combined success rate (average between guided and free mode success), but also present a metric representing the user's perceived dialog length, which counts only the nodes shown to the user.\\n\\n6.3. RQ 1.3: Method Generalizability\\nTo evaluate how well our data generation method generalizes to new domains, we perform additional evaluation in simulation, analogous to (section 6.2), using the test splits of the new datasets ONBOARD and DIAGNOSE.\\n\\n6.4. Human Evaluation (RQ 2)\\nTo understand how performance of an agent trained on generated data translates to real-world users, we recruit 44 participants from the crowdsourcing platform Prolific to take part in human evaluation. Participants were native English speakers with varying experience with business travel (self-rating between 2 and 5 on a 5 point Likert-scale). They were compensated at the platform recommended rate of 9\u00a3/hour. The experiment took roughly 20 minutes.\\n\\nStudy Design\\nWe asked each participant to interact with either a CTS agent trained on real data or one trained on generated data in the REIMBURSE domain. Apart from demographic information, we ask for previous experience with dialog systems and with business travel. During the experiment, participants were asked to complete three conversations with their assigned dialog system. Each conversation, they were randomly assigned a new goal, covering one of three expected interaction styles: 1) \u201copen\u201d goals representing a general/vague information need, 2) \u201ceasy\u201d goals representing a concrete information need, and 3) \u201chard\u201d goals representing a concrete information need requiring personalized information to correctly answer. Personalized information refers to the user\u2019s specific circumstances, e.g. trip duration or funding organization, which can change the dialog flow. Between each dialog, users were\"}"}
{"id": "lrec-2024-main-1428", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"asked to rate their subjective perception of dialog length and how well their question was answered. After the interaction, they were asked to rate the usability of the dialog agent, how much they trusted it, and its reliability. For more details see Appendix B.\\n\\n6.4.1 Evaluation Metrics\\n\\nThe perceived dialog length was measured on a 5-point scale from 1 (much too short) to 5 (much too long). Perceived success was measured on a 4-point scale, where users were asked to rate how well their question had been answered from 1 (not at all) to 4 (completely). Additionally, the objective dialog length and success condition were logged for each dialog. Usability of the dialog agent was measured using the Universal Measure of User Experience scale developed by Finstad (2010). User trust was measured using the reliability and trust subscales from K\u00f6rber (2018).\\n\\n7. Results & Discussion\\n\\nBefore testing performance of agents trained on generated data, we first verify our changes to the CTS agent. As the hyperparameters for the original agent were tuned on the German dataset, for fairness, we report the original CTS agent\u2019s performance on both English and German (Table 5).\\n\\nOur changes to the CTS agent improve the combined success rate by over 10\\\\% compared to the original agent on the German REIMBURSE dataset and 18\\\\% for the English REIMBURSE-En. It should be noted that the actual improvement over the German agent is likely larger, as the success metric reported for German comes from (V\u00e4th et al., 2023), rather than the new and harsher metric we use for English (section 4).\\n\\n7.1 RQ 1: Transitioning to a Zero Data Approach\\n\\nRQ 1.1 Analyzing the quality of generated data\\n\\nLooking at the question lengths between human data and data generated by GENV1 (Figure 2), we observe that the generated questions seem to be longer than human questions. When manually inspecting the generated questions, we also find them to be much less natural than those from the human data.\\n\\nWe amend the original prompt used, creating GENV2, to explicitly ask for short outputs (subsection 5.1) in an effort to align the syntax of the generations better with the human data. This change to the prompt shifts the distribution of question lengths more towards the human training distribution, and qualitatively yields more natural utterances. However, it still does not ensure that the artificial data is semantically similar to human data.\\n\\nTo investigate how semantically similar the generated questions are to human data, we calculate the pair-wise similarities between all human and generated questions for each node from the dialog graph, and then average the similarities across all nodes (Figure 3). Here, we see that the GENV2 data is still quite distinct from the human data. When manually inspecting the generations, we find that generated questions tend to focus only on one part of the node text, making them lack diversity and omit topics real users might ask about. To address this, we develop the novel two-step GENV3 prompt, steering the model to explicitly ask about all named entities in a node (subsection 5.1). We see that doing so significantly (p < 1e-11) increases the similarity of the generated (avg. 0.52) to the human training data than GENV2 (avg. 0.47), as measured with a standard t-test.\\n\\nWe also look at the diversity of the generated questions. The self-BLEU scores (Table 4) show that the GENV3 data are the most diverse. This metric can be used to analyze the quality of the generated data even in the absence of human comparison data.\\n\\n|                | n-1 | n-2 | n-3 | n-4 | n-5 |\\n|----------------|-----|-----|-----|-----|-----|\\n| Human          | 0.78| 0.68| 0.60| 0.54| 0.49|\\n| GENV1          | 0.95| 0.92| 0.87| 0.83| 0.80|\\n| GENV2          | 0.95| 0.90| 0.85| 0.80| 0.76|\\n| GENV3          | 0.85| 0.78| 0.71| 0.66| 0.62|\\n\\nTable 4: Self-BLEU scores for different n-gram sizes on human and generated data.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5: Simulation results on REIMBURSE(-En) test splits of original CTS agent (German), our improved agent (English), and our CTS agent trained on generated data only (English).\\n\\n|                  | Dialog Mode | Prediction F1 | Dialog Mode | Prediction Consistency |\\n|------------------|-------------|---------------|-------------|------------------------|\\n| Original human (GER) |             |               |             |                        |\\n| Original human (EN) |             |               |             |                        |\\n| Ours human (EN)   |             |               |             |                        |\\n| Ours V1 (LLAMA)   |             |               |             |                        |\\n| Ours V2 (LLAMA)   |             |               |             |                        |\\n| Ours V3 (LLAMA)   |             |               |             |                        |\\n| Ours V1 (ChatGPT) |             |               |             |                        |\\n| Ours V2 (ChatGPT) |             |               |             |                        |\\n| Ours V3 (ChatGPT) |             |               |             |                        |\\n\\nIn conjunction with diversity, we estimate the average \u201canswerability\u201d via QA confidence scores of the generated questions, given the node text as answer. Here, we also see that the improvements from Gen V3 and Gen V2 together also significantly (p < 0.0003) increase the average answerability, from an average of 0.36 with naive prompt to 0.42 with Gen V3, according to a t-test.\\n\\nWhen looking at downstream performance (Table 5), we see that improvements in these metrics also lead to higher dialog success, suggesting they can be used as an indicator of generation quality.\\n\\nRQ1.2: Human Data vs. Synthetic Data\\nTo investigate whether synthetic data can be a viable alternative to human data, we compare agent performance in simulation. From Table 5, we see that the best performing agent trained on artificial data (Gen V3: 69.44% success) performs comparably to the best performing agent trained on human data (CTS ours: 73.86% success). Using a standard t-test, we find no statistically significant difference.\\n\\nRQ1.3: Generalizing to new domains\\nTo test of the scalability of our generation methods, we analyze model performance on two new domains. As each of these has their own challenges (section 3), we compare each model trained on generated data to a baseline trained on human data. When looking at Table 6, the agent trained on data generated by LLAMA is again nearly able to match the performance of the model trained on human data for the DIAGNOSE dataset, while the model trained on data generated by ChatGPT surpasses it. On the other hand, the ONBOARD dataset may present a more challenging domain, due in part to the code-switching present in the dialog nodes. Despite this, the model trained on data from ChatGPT nearly reaches the performance of models trained on human data.\\n\\nBased on this, we find that the generation techniques do appear to scale to new domains, as t-tests show no statistically significant differences between the best synthetically trained agents and the agents trained on real data in any domain.\\n\\n7.2. RQ 2: Human Evaluation\\n7.2.1. Generated vs. Real Data\\nAfter performing human evaluation, we find that there are no statistically significant differences (using a standard t-test) between either subjective or objective measures of success or dialog length (Table 7). Additionally, we find no difference in the reported trust, reliability, or usability scores between either group. This suggests that there is no human-observable loss in performance when using generated data compared to real data, either in terms of objective metrics or subjective metrics.\\n\\n7.2.2. Human Evaluation vs. Simulator\\nFinally, to validate our updated user simulator, we additionally compare the objective performance metrics from the human evaluation (Table 7) to those obtained in simulation (Table 5). We find that the success rates between the simulated and human dialogs are very comparable (73.86% and 77.59% respectively for the model trained on human data, and 69.44% and 72.73% for the model trained on generated data). We perform statistical analysis using Welch's t-test to account for the difference in sample size, and find no significant difference, regardless of the source of training data.\\n\\nBased on this, we conclude that results from simulation translate well to real human interaction, suggesting the simulator can be a good proxy for real user evaluation. We therefore expect the results reported in (Table 6) will translate to similar performance with real users.\\n\\n8. Conclusion\\nIn this paper, we present two new and publicly available datasets, ONBOARD, providing help for moving to a new city in a foreign country, and DIAGNOSE, a medical domain. The datasets each consist of a dialog tree and human-collected text inputs. We apply a harsher, more realistic evaluation metric and improve on the agent training method...\"}"}
{"id": "lrec-2024-main-1428", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Performance of CTS agents trained on human and generated data on the new domains DIAGNOSE and ONBOARD in simulation.\\n\\n| Training Data | # Turns | Success | Perceived Length | Answer Satisfaction |\\n|---------------|---------|---------|------------------|--------------------|\\n| Human         | 6.14    | 77.59   | 2.88             | 2.93               |\\n| V3 (LLAMA)    | 5.27    | 72.73   | 2.65             | 2.73               |\\n| V3 (ChatGPT)  | 5.65    | 85.12   | 2.46             | 2.78               |\\n\\nTable 7: Average objective and subjective performance metrics of a CTS agent trained on human data vs. generated data.\\n\\nGiven a dialog tree, we explore several zero-data prompting-based methods for generating user utterance data to train a CTS agent, developing a novel two-stage prompting approach to increase question diversity. Through this process, we find that automatic scores for diversity and answerability can be indicative of downstream dialog task performance.\\n\\nFurthermore, we show that there is no statistically significant difference in objective metrics between agents trained on human data or on generated data in the REIMBURSE-En domain. We verify this both through simulation and through testing with real users. User evaluation further reveals no statistically significant differences on subjective metrics (trust, reliability, usability, subjective length, or subjective dialog success) either. This suggests that we can effectively generate training data from a dialog tree, such that CTS agents can be trained in zero data settings with negligible performance loss. We also find that the size of the tested LLMs does not result in significant differences in task performance.\\n\\nTo evaluate how well our techniques scale to new domains, we further tested agent performance on both new datasets we introduced. For ONBOARD, we again find that performance of agents trained on generated data is comparable to that of agents trained on human data. For DIAGNOSE, performance can even exceed that of the agent trained on human data. This suggests that our methods scale well to new domains.\\n\\n9. Ethical Considerations\\n\\nTo ensure that users could give informed consent, we provided a detailed description of the task and research objectives both on the crowdsourcing platform and once they had accepted the task. In respect of participant privacy, we specifically did not collect personally identifying data from any users. To this end, we store all logs and survey responses using an anonymous hash generated based on a given username, rather than with the username itself. In this way, users could log in again if they needed to take a break in the middle of the interaction, but we had no way of directly linking any recorded results to, e.g., users' Prolific account identifiers. To ensure that participants were fairly compensated, we followed best practices recommended by the crowdsourcing platform paying users at 9\u00a3/hr. We additionally used our pilot study to verify that our estimated time was below the median time we selected when advertising the task.\\n\\n10. Limitations\\n\\nWhile we try to cover many different real-world use cases with the presented domains, we cannot account for the challenges of all possible future domains. Additionally, although our work removes the necessity to collect training data, creating a dialog tree is still required (which may be large for complex domains). Finally, replicating the exact data generated and analyzed in this paper depends on the specific versions of the LLMs used.\\n\\n11. Bibliographical References\\n\\nAmos Azaria and Tom Mitchell. 2023. The internal state of an llm knows when its lying. arXiv preprint arXiv:2304.13734.\\n\\nDaniel G. Bobrow, Ronald M. Kaplan, Martin Kay, Donald A. Norman, Henry Thompson, and Terry Winograd. 1977. Gus, a frame-driven dialog system. Artificial Intelligence.\\n\\nLi Chen and Pearl Pu. 2012. Critiquing-based recommenders: survey and emerging trends. User Modeling and User-Adapted Interaction, 22(1):125\u2013150.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiusi Chen, Yu Zhang, Jinliang Deng, Jyun-Yu Jiang, and Wei Wang. 2023. Gotta: generative few-shot question answering by prompt-based cloze data augmentation. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM), pages 909\u2013917. SIAM.\\n\\nPhilip R. Cohen. 2020. Back to the future for dialogue research. In The Thirty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pages 13514\u201313519. AAAI Press.\\n\\nOnd\u0159ej Du\u0161ek and Zden\u011bk Kasner. 2020. Evaluating semantic accuracy of data-to-text generation with natural language inference. In Proceedings of the 13th International Conference on Natural Language Generation, pages 131\u2013137, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKraig Finstad. 2010. The usability metric for user experience. Interacting with Computers, 22(5):323\u2013327.\\n\\nMauajama Firdaus, Arunav Shandilya, Asif Ekbal, and Pushpak Bhattacharyya. 2023. Being polite: Modeling politeness variation in a personalized dialog agent. IEEE Transactions on Computational Social Systems, 10(4):1455\u20131464.\\n\\nJianfeng Gao, Michel Galley, and Lihong Li. 2018. Neural approaches to conversational AI. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR \u201918, page 1371\u20131374, New York, NY, USA. Association for Computing Machinery.\\n\\nSrinivasan Janarthanam and Oliver Lemon. 2014. Adaptive generation in dialogue systems using dynamic user modeling. Computational Linguistics, 40(4):883\u2013920.\\n\\nMoritz K\u00f6rber. 2018. Theoretical considerations and development of a questionnaire to measure trust in automation. In Congress of the International Ergonomics Association, pages 13\u201330. Springer.\\n\\nJunlong Li, Zhuosheng Zhang, and Hai Zhao. 2023. Self-prompting large language models for zero-shot open-domain QA.\\n\\nWeiri Liang and Meng Yang. 2018. Hierarchical hybrid code networks for task-oriented dialogue. In Intelligent Computing Theories and Application - 14th International Conference, ICIC 2018, Wuhan, China, August 15-18, 2018, Proceedings, Part II, volume 10955 of Lecture Notes in Computer Science, pages 194\u2013204. Springer.\\n\\nSamuel Louvan and Bernardo Magnini. 2020. Recent neural methods on slot filling and intent classification for task-oriented dialogue systems: A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 480\u2013496, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nBruce Lucas. 2000. Voicexml for web-based distributed conversational applications. Communications of the ACM, 43(9):53\u201357.\\n\\nYukun Ma, Khanh Linh Nguyen, Frank Z. Xing, and Erik Cambria. 2020. A survey on empathetic dialogue systems. Information Fusion, 64:50\u201370.\\n\\nPotsawee Manakul, Adian Liusie, and Mark JF Gales. 2023. Selfcheckgpt: Zero-resource black-box hallucination detection for generative large language models. arXiv preprint arXiv:2303.08896.\\n\\nFedelucio Narducci, Marco de Gemmis, Pasquale Lops, and Giovanni Semeraro. 2018. Improving the user experience with a conversational recommender system. In International Conference of the Italian Association for Artificial Intelligence, pages 528\u2013538. Springer.\\n\\nPreksha Nema and Mitesh M. Khapra. 2018. Towards a better metric for evaluating question generation systems. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3950\u20133959, Brussels, Belgium. Association for Computational Linguistics.\\n\\nBaolin Peng, Michel Galley, Pengcheng He, Hao Cheng, Yujia Xie, Yu Hu, Qiuyuan Huang, Lars Liden, Zhou Yu, Weizhu Chen, et al. 2023. Check your facts and try again: Improving large language models with external knowledge and automated feedback. arXiv preprint arXiv:2302.12813.\\n\\nRaul Puri, Ryan Spring, Mohammad Shoeybi, Mostofa Patwary, and Bryan Catanzaro. 2020. Training question answering models from synthetic data. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5811\u20135826, Online. Association for Computational Linguistics.\\n\\nPeng Qi, Yuhao Zhang, Yuhui Zhang, Jason Bolton, and Christopher D. Manning. 2020. Stanza: A Python natural language processing\"}"}
{"id": "lrec-2024-main-1428", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"toolkit for many human languages. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations. Dinesh Raghu, Shantanu Agarwal, Sachindra Joshi, and Mausam. 2021.\\n\\nEnd-to-end learning of flowchart grounded task-oriented dialogs. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4348\u20134366, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\\n\\nKnow what you don't know: Unanswerable questions for SQuAD. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784\u2013789, Melbourne, Australia. Association for Computational Linguistics. Evgeniia Razumovskaia and Maxine Eskenazi. 2019. Incorporating rules into end-to-end dialog systems. In Proc. 3rd NeurIPS Workshop on Conversational AI, Vancouver, Canada, pages 1\u201311. Nils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019, pages 3980\u20133990. Association for Computational Linguistics.\\n\\nHannes Ritschel and Elisabeth Andr\u00e9. 2018. Shaping a social robot's humor with natural language generation and socially-aware reinforcement learning. In Proceedings of the workshop on NLG for human\u2013robot interaction, pages 12\u201316. Rico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 86\u201396, Berlin, Germany. Association for Computational Linguistics. Swadheen Shukla, Lars Liden, Shahin Shayan-deh, Eslam Kamal, Jinchao Li, Matt Mazzola, Thomas Park, Baolin Peng, and Jianfeng Gao. 2020. Conversation Learner - a machine teaching tool for building dialog managers for task-oriented dialog systems. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 343\u2013349, Online. Association for Computational Linguistics.\\n\\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-Yan Liu. 2020. Mpnet: Masked and permuted pre-training for language understanding. Advances in Neural Information Processing Systems, 33:16857\u201316867. Nandan Thakur, Nils Reimers, Andreas R\u00fcckl\u00e9, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: A heterogenous benchmark for zero-shot evaluation of information retrieval models. In 35th Conference on Neural Information Processing Systems (NeurIPS 2021) Track on Datasets and Benchmarks. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971. Dirk V\u00e4th, Lindsey Vanderlyn, and Ngoc Thang Vu. 2023. Conversational tree search: A new hybrid dialog task. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1264\u20131280, Dubrovnik, Croatia. Association for Computational Linguistics. Jason Wei and Kai Zou. 2019. EDA: Easy data augmentation techniques for boosting performance on text classification tasks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6382\u20136388, Hong Kong, China. Association for Computational Linguistics.\\n\\nJason D Williams. 2008. Integrating expert knowledge into pomdp optimization for spoken dialog systems. In Proceedings of the AAAI-08 Workshop on Advancements in POMDP Solvers, volume 2, page 25. Jason D. Williams, Kavosh Asadi, and Geoffrey Zweig. 2017. Hybrid code networks: practical and efficient end-to-end dialog control with supervised and reinforcement learning. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 665\u2013677, Vancouver, Canada. Association for Computational Linguistics. Chung-Hsien Wu, Jui-Feng Ye, and Ming-Jun Chen. 2005. Domain-specific faq retrieval.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"using independent aspects.\\n\\nACM Transactions on Asian Language Information Processing, 4(1):1\u201317.\\n\\nMin Yang, Qiang Qu, Kai Lei, Jia Zhu, Zhou Zhao, Xiaojun Chen, and Joshua Z. Huang. 2018.\\n\\nInvestigating Deep Reinforcement Learning Techniques in Personalized Dialogue Generation, pages 630\u2013638.\\n\\nHaolan Zhan, Sameen Maruf, Lizhen Qu, Yufei Wang, Ingrid Zukerman, and Gholamreza Hafifari. 2023.\\n\\nTurning flowchart into dialog: Augmenting flowchart-grounded troubleshooting dialogs via synthetic data generation. In Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association, pages 88\u201399, Melbourne, Australia. Association for Computational Linguistics.\\n\\nZheng Zhang, Ryuichi Takanobu, Qi Zhu, Min-Lie Huang, and Xiaoyan Zhu. 2020.\\n\\nRecent advances and challenges in task-oriented dialog systems. Science China Technological Sciences, 63(10):2011\u20132027.\\n\\nYaoming Zhu, Sidi Lu, Lei Zheng, Jiaxian Guo, Weinan Zhang, Jun Wang, and Yong Yu. 2018.\\n\\nTexygen: A benchmarking platform for text generation models. In The 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, SIGIR '18, page 1097\u20131100, New York, NY, USA. Association for Computing Machinery.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Parameter                                      | Value       |\\n|-----------------------------------------------|-------------|\\n| Optimizer                                     | Adam        |\\n| Learning Rate                                 | $1 \\\\times 10^{-4}$ |\\n| $\\\\lambda$                                     | 0.1         |\\n| Maximum Training Dialog Turns                 | 2           |\\n| Max. Gradient Norm                            | 1.0         |\\n| Batch Size                                    | 256         |\\n| $\\\\gamma$                                      | 0.99        |\\n| Exploration fraction of Training Turns        | 0.99        |\\n| Exploration Scheme                            | $\\\\epsilon$-greedy |\\n| $\\\\epsilon_{start}$                            | 0.6         |\\n| $\\\\epsilon_{end}$                              | 0.0         |\\n| Training frequency (w.r.t. dialog turns)      | 3           |\\n| Training start (w.r.t. dialog turns)          | 1280        |\\n| DDQN Target Network update frequency (w.r.t. training steps) | 15 |\\n| Q-Value clipping                              | 10.0        |\\n| Munchausen $\\\\tau$                             | 0.03        |\\n| Munchausen $\\\\alpha$                           | 0.9         |\\n| Munchausen Clipping                           | -1.0        |\\n| Evaluation frequency (w.r.t. dialog turns)    | 10000       |\\n| Evaluation dialogs                            | 500         |\\n\\nTable 8: Hyperparameters for training the Reinforcement Learning agents.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. User Study\\n\\nB.1. Data Agreement\\n\\nBefore beginning the experiment, users were provided with a data agreement. Although we did not collect any personally identifying data, we wanted to make sure that users were aware of what they would be asked to do, the purpose of the research, what data we would collect and how the data would be processed.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"During the interaction, users were provided with the following interface, on the right side they had an information goal for which they should find an answer. On the left side, they had a window with their conversation with the chatbot. Once they felt they had found an answer to their question, they could click on the button underneath the goal to move on to the next dialog.\"}"}
{"id": "lrec-2024-main-1428", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B.3. Interaction Surveys\\n\\nB.3.1. Pre-Interaction Survey\\nThe survey given to users before the interaction can be seen below. Here they were asked general questions about their demographics, previous experience with the domain and chatbots.\\n\\nB.3.2. Post-Dialog Survey\\nAfter each interaction, users were asked to rate their perception of the dialog length on a five-point Likert scale and their perception of how well their question was answered on a four-point Likert scale.\"}"}
