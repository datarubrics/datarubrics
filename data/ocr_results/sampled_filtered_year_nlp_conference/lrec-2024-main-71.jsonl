{"id": "lrec-2024-main-71", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Matter of Perspective: Building a Multi-Perspective Annotated Dataset for the Study of Literary Quality\\n\\nYuri Bizzoni\u2217, Pascale Feldkamp\u2217, Ida Marie S. Lassen\u2217, Mads Rosendahl Thomsen\u2020, Kristoffer L. Nielbo\u2217\u2020\\n\\nComparative Literature \u2013 School of Communication and Culture, Aarhus University\\nCenter for Humanities Computing, Aarhus University\\nyuri.bizzoni@cc.au.dk\\n\\nAbstract\\nStudies on literary quality have constantly stimulated the interest of critics, both in theoretical and empirical fields. To examine the perceived quality of literary works, some approaches have focused on data annotated through crowd-sourcing platforms, and others relied on available expert annotated data. In this work, we contribute to the debate by presenting a dataset collecting quality judgments on 9,000 19th and 20th century English-language literary novels by 3,150 predominantly Anglophone authors. We incorporate expert opinions and crowd-sourced annotations to allow comparative analyses between different literary quality evaluations. We also provide several textual metrics chosen for their potential connection with literary reception and engagement. While a large part of the texts is subjected to copyright, we release quality and reception measures together with stylometric and sentiment data for each of the 9,000 novels to promote future research and comparison.\\n\\nKeywords: Literary quality, stylometry, digital humanities, literary analysis, sentiment analysis, readability\\n\\n1. Introduction\\n\\nThe advent of computational methods is changing how we analyze and understand literature. From the quantitative assessment of linguistic patterns to the deep, qualitative insights into thematic elements, computational literary studies are redefining the landscape of literary criticism and research. The operationalization of complex concepts in computational linguistics and digital humanities comes with the possibility of deepening our understanding of literary narrative and writing but also involves the difficulty of relying on quantifiable elements. However, comprehensive, well-structured, and curated datasets are indispensable to leverage the full potential of quantitative methods.\\n\\nIn this work, we present a new dataset designed to further the analysis of one of the most complex and controversial concepts of literary theory: quality\u2014with a focus, in this case, on the possible relation between textual features and perceived quality at a statistical level. While the study and discussion of literary quality are thousands of years old, extensive datasets to approach the problem from a quantitative and statistical perspective are not abundant.\\n\\nWe present a dataset designed to explore the theme of \\\"quality\\\" in computational literary studies, offering a rich array of textual and metadata features and a diverse collection of \\\"quality\\\" or reception proxies. It comprises various literary works spanning multiple genres, periods, and cultural contexts, although mainly confined to the Anglo-Saxon world. It can also be a robust foundation for related research objectives, such as sentiment analysis, stylistic evolution, and literary thematic categorization.\\n\\nThe paper is structured as follows. Section 2 provides an overview of the state of the art on literary quality, especially in the more recent context of computational studies. Section 3 offers an overview of the dataset, including its size and origin. Section 4 presents the various metrics for assessing the perceived quality of literary novels that we have collected. In continuation, Section 5 presents the textual metrics we calculated for each novel, and Section 6 briefly explains the metadata fields accompanying the dataset. Section 7 discusses the limitations of our dataset, its advantages, its intended uses and proposes directions for future enhancements.\\n\\n2. Related works\\n\\nWhile the ability to process and analyze large quantities of texts through complex statistical experiments has recently made new ways of study\\n\\n2 We make both intrinsic and extrinsic features for all novels publicly available at: https://github.com/centre-for-humanities-computing/chicago_corpus\\n\\n3 While a large part of the corpus is subject to copyright (so that full texts cannot be released), full text of the pre-1924 novels can be found here: https://artflsrv04.uchicago.edu/philologic4.7/chicago_novel_corpus_pre1923_12-20/\"}"}
{"id": "lrec-2024-main-71", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing literary appreciation possible, the question of how to define literary quality is probably as old as literature. Modeling perceived literary quality or reader appreciation poses a challenge to research on at least two dimensions: the number of features one could explore and the number of potential \\\"judges\\\" one could interrogate. Even if there may be a large consensus on the quality of a particular text, the underlying reasons are usually elusive and not necessarily rooted in the text itself. Setting aside possible biases underlying literary judgments for a moment, text-oriented schools of thought such as Van Peer (2008) have tended to look at the intrinsic textual features of literary works to explore their effectiveness. Still, that alone is a complex endeavor. Through the centuries, there have been many rules and recommendations to write better, supposedly applicable across genres and to both high and low-brow literature. Sherman (1893), for example, proposed that simplicity \u2013 i.e. shorter sentences, closer to the way we speak \u2013 should be a marker of a \\\"better\\\" literary style. Measuring textual simplicity has often been done via readability indices (gauging, generally, sentence and word length), which have also more recently been valued as creative writing and publishing aids \u2013 implemented in editing tools such as the Hemingway or Marlowe applications. Still, the importance of the \\\"readability\\\" of a literary text in the context of reader appreciation is essentially controversial (Martin, 1996; Garthwaite, 2014). Considering the complexity and internal heterogeneity of what we call \\\"literature\\\". Naturally, features beyond sentence and word length impact the reading experience. Still, studies seeking to predict literary success or perceived literary quality follow the intuitive idea that readers perceive a difference between \\\"difficult\\\" and \\\"easy\\\" reads and tend to approximate some form of stylistic complexity by using textual features related to readability indices, such as sentence-length, vocabulary richness, or redundancy (Brottrager et al., 2022; van Cranenburgh and Bod, 2017; Crosbie et al., 2013; Koolen et al., 2020; Maharjan et al., 2017; Algee-Hewitt et al., 2016). On this intuition, more general \\\"simplicity laws\\\" have been developed by critics and writers alike \u2013 for example, Ernest Hemingway's recommendation of a \\\"direct and personal\\\" style in \\\"simple and vigorous\\\" words (Hemingway, 1999). King (2010) offers very concrete advice in On Writing, where he advocates, among other things, more \\\"readable\\\" texts (shorter words and sentences) and fewer adverbs. Strunk et al.'s influential book The Elements of Style advocates very concrete advice, such as using the active voice and putting statements in the positive form, together with vaguer rules such as omitting needless words. Conversely, others have promoted what has been termed \\\"purple prose\\\" (a notion derived from Horace's Ars Poetica), characterized as challenging, \\\"rich, succulent and full of novelty\\\" (West, 1985). Indeed, reader preferences regarding the \\\"difficulty\\\" of prose, at least in terms of readability formulas, appear to be audience-specific (Bizzoni et al., 2023a). Studies that seek to model reader appreciation or canonicity have generally looked at stylistic features, ranging from the most basic measures of difficulty or complexity, such as sentence length (Maharjan et al., 2017; Mohseni et al., 2022), to more experimental measurements like the compressibility of a text file using standard file compressors (Koolen et al., 2020). Beyond the stylistic level, some work has been done on more underlying narrative features of literary texts, especially with the use of sentiment analysis, even if questions persist on how to measure narratological components or, in the case of sentiment analysis, how to operationalize an affective narratology (Rebora, 2023). Studies have sought to measure the shapes of a text's sentiment arc or to approximate narrative complexity (Maharjan et al., 2018; Reagan et al., 2016; Bizzoni et al., 2022b), on the intuition that readers tend to appreciate certain shapes or a certain balance in the complexity of a narrative flow or arc. Studies have emphasized the potential of sentiment analysis (Alm, 2008; Mohammad, 2018), at the word (Mohammad, 2018), sentence (M\u00e4ntyl\u00e4 et al., 2018) and paragraph (Li et al., 2019) level, to uncover meaningful mechanisms in the reading experience (Drobot, 2013; Cambria et al., 2017; Kim and Klinger, 2018; Brooke et al., 2015; Jockers, 2017), usually by drawing scores from human annotations (Mohammad and Turney, 2013) or induced lexica (Islam et al., 2020). While most studies have focused on the shapes of sentiment arcs, Hu et al. (2021) have modeled their persistence, coherence, and predictability by looking at the arcs' entropy or by using fractal analysis (Mandelbrot and Ness, 1968; Mandelbrot, 1982, 1997; Beran, 1994; Eke et al., 2002; Kuznetsov et al., 2013). Finally, some studies have explored changes in reader preferences as a historical development. For example, a taste for stylistically \\\"easier\\\" books may be an effect of changes in reader demographics with the emergence of mass readership (Klancher, 1983). A more general basic level of literacy across society strata may have led to a consumer demand for more accessible books, and an increasing market-logic may have pressed editors to pre-\"}"}
{"id": "lrec-2024-main-71", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. More straightforward literary style (Winter and O'Neill, 2022). Similarly, readers might have become younger, for example, with the Young Adult fiction boom in the 1960s (Bach, 2022). Lower reading speed and hermeneutic difficulty may have come to be viewed as a vice rather than a virtue (Steiner, 1978), so authors and publishers have favored more direct prose. Such conjectures of changes in reader demographics do not exclude the existence of a many-tiered literary audience, where an increasing number of readers demand more straightforward texts and different \u201chigh culture\u201d readerships favor challenging works. A perspectivist approach to \u201cliterary quality\u201d, considering many \u201cjudges\u201d or audiences, allows insight both into developments in reader demographics and into the multi-faceted phenomenon of literary preference.\\n\\n2.1. Works using the resource\\n\\nSome works have already used the presented resource to explore trends related to contemporary English-language literature and the question of literary quality. Some studies have applied sophisticated measures to gauge shapes and approximate complexity at the narrative level of the books in the corpus, relating these sentiment dynamics to reader appreciation.\\n\\nBizzoni et al. (2023b) modeled the persistence, coherence, and predictability of arcs through the Hurst coefficient and Approximate Entropy (ApEn) to measure global and local complexity, using them to train classifiers able to gauge the reception and perceived quality of unseen texts. Such measures appear to be applicable for distinguishing between types of literature (e.g., prize-winning novels vs. bestsellers) (Bizzoni et al., 2024). The resource has proved valuable to train and test classifiers that try to gauge the reception and perceived quality of unseen novels (Bizzoni et al., 2023b). Moreover, the corpus has been used to explore the relation between different types of reader valuation, as well as the relation of different such proximities to textual characteristics. For example, it has been used to find that features of style vary across \u201ctypes\u201d of literature: award-winning works are less readable, while more readable books appear to be rated more often on GoodReads (Bizzoni et al., 2023a). Similarly, it has also been the basis for a recent study finding that prestigious literature appears to elicit higher LLM-based perplexity than popular literature (Wu et al., 2024). Finally, beyond the relation between textual features and reader appreciation, the corpus has also been used for tracking stylistic change diachronically (Feldkamp et al., 2023).\\n\\n3. Corpus\\n\\nThe corpus of texts from which we constructed our dataset was assembled by Hoyt Long and Richard Jean So; it encompasses 9088 novels published in the United States between 1880 and 2000 and was compiled based on the worldwide number of libraries holding each title, favoring works with a higher number of library holdings for their selection. Because of this selection criteria, the corpus comprises much high-quality fiction from authors who have received prestigious distinctions, such as the Nobel Prize, the National Book Award (including Don DeLillo, Joyce Carol Oates, and Philip Roth), as well as important works of genre-fiction (i.a., Tolkien or Philip K. Dick). Still, library holdings appear to reflect high distinction and mass popularity, as acquisition reflects the average library user's demand and preferences. As such, the corpus also comprises influential novels from mainstream literature (i.a., Agatha Christie), with notable contributions on the broad spectrum of so-called \u201cgenre literature\u201d, from Mystery to Science Fiction (Long and Roland, 2016).\\n\\nThe corpus has a geographical bias, comprising primarily Anglophone authors (with few exceptions). This bias inevitably situates any analysis of it within the context of a US and \u201cAnglocentric\u201d literary field. Books in the corpus vary in length, from 341 words (Beatrix Potter's The Story of Miss Moppet) to 714,744 words (Ben A. Williams' House Divided), though only 255 books \u2013 2.9% of the corpus \u2013 are shorter than 35,000 words \u2013 the length of titles like Orwell's Animal Farm or Hemingway's The Old Man and the Sea. The total word count of the corpus is 1,060,549,793 words.\\n\\nWe divide the measures that we provide in our datasets into two categories: quality metrics and textual metrics.\\n\\n---\\n\\n6 Based on the WorldCat catalog.\\n\\n7 Previous quantitative literary analyses have employed this corpus, (Underwood et al., 2018; Cheng, 2020; Bizzoni et al., 2022a).\"}"}
{"id": "lrec-2024-main-71", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quality Metrics\\n\\nThe quality metrics are arguably the rarer of the two categories in literary datasets and the most complicated from a conceptual standpoint. Understanding and quantifying \\\"literary quality\\\" is a complex endeavor (Bizzoni et al., 2022a), often subject to subjective evaluations. However, in the context of this study, we have operationalized this concept by considering a range of metrics commonly used in the academic and public discourse.\\n\\nThe quality metrics that we have collected belong to two main types: crowd-based, representing the result of many unfiltered readers, and, on the other hand, expert-based, drawn from prestigious proximities curated by experts, often institutionally affiliated. It should be noted that this distinction is heuristic above all else, as various metrics, such as translation counts, are both subject to expert choice and the taste judgements of a larger reader-ship.\\n\\n4.1. Crowd-based Metrics\\n\\nThe main crowd-based metrics that we collected are:\\n\\n1. GoodReads' Rating Count: This metric approximates a book's popularity among a general audience. It is the total number of ratings a book has received on GoodReads.\\n\\n2. GoodReads' Average Rating: Unlike the rating count, the average rating measures how well GoodReads users received the book on a scale of 1 to 5.\\n\\n3. Audible Rating Count and Average Rating: Rating count and average rating for the titles represented in Audible.\\n\\n4. GoodReads Lists: Users can collectively create and populate lists. Lists like Best Books of the 20th Century constitute a crowd-based representation of the concept of high-quality (and often canonical) literature.\\n\\n5. WorldCat Holdings: This metric indicates the number of libraries worldwide holding a particular book, which can indirectly indicate the book's quality and importance.\\n\\n6. Wikipedia Author-page Rank: Using Wikipedia page-views, the number of times visits to an author's page on Wikipedia is also sometimes used as a proxy for canonicity or literary success (Hube et al., 2017).\\n\\nIn Hube et al.'s (and our) variation of page-rank (a Google algorithm) hubs or author-pages on Wikipedia that have the highest number of other pages referencing them have a higher rank, so that more referenced authors rank higher. The Wikipedia page rank thus also measures authors' presence in the popular and cultural sphere, if we consider that Wikipedia-pages may be created and edited by various types of users. It should be noted that ranks refer to authors, so that books by the same author will have the same rank, independently from differences in prestige or popularity between individual titles.\\n\\n7. Translation Count: The Index Translationum database collects all translations published in ca. 150 UNESCO member states.\"}"}
{"id": "lrec-2024-main-71", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A ward Titles\\n\\nNational book award 108\\nPulitzer prize 53\\nNobel prize* 85\\nScifi awards 163\\nHugo award\\nNebula award\\nPhilip K. Dick award\\n(Pope, 2019) J.W. Campbell award\\nPrometheus award\\nLocus sci-fi award\\nFantasy awards 40\\nWorld fantasy award\\nLocus fantasy award\\nBritish fantasy award\\nMythopoeic award\\nHorror awards 19\\nBram Stoker award\\nLocus horror award\\nRomantic awards* 54\\nRita awards*\\nRNa awards*\\n\\nTable 2:\\nNumber of longlisted titles for general fiction and genre-fiction awards, and the specific awards collected. Proxies marked * are author-based: For these, we included all titles extant in the corpus by the author mentioned, either due to the scarcity of awards in the genre or the nature of the award, e.g., the Nobel prize given to authors, rather than to individual titles. All other awards are title-based.\\n\\nCompiled from local bibliographical institutions or national libraries, cataloguing more than 2 million works. Note that the database was created in 1979 and stopped compiling in 2009. As such, the resource lists translations of a particular period, and not the most translated works of all time. The proxy should be interpreted with that in mind. Translation counts not a clear-cut crowd-based metric, as various factors (beyond popular demand) may influence which works are translated.\\n\\n4.2. Expert-based Metrics\\n\\n1. Awards and Prizes: Winning or being nominated for a prestigious literary award is a significant indicator of literary quality, so prizes can also serve as an expert-based quality metric. We collected long-listed titles (winners and finalists) for both prestigious literary awards: The Nobel Prize in Literature, the Pulitzer Prize, the National Book Award; as well as various genre-based awards (for the full list of awards, see table 2).\\n\\n2. Inclusion in Anthologies: Being included in respected anthologies or literary collections, such as the Norton Anthology (Pope, 2019), is another expert-based quality metric and can be seen as a proxy for canonization. For the present study, we marked all titles in our corpus written by authors mentioned in these two series, where the anthology of English Literature is the most widespread.\\n\\n3. College Syllabi: How often an author is assigned on college syllabi can serve as a complementary metric of canonization. We used the resource OpenSyllabus, which has collected 18.7 million college syllabi in an attempt to map the college curriculum. From their data, we count all titles in our corpus by authors who appear as authors of one of the top 1,000 titles assigned in English Literature college syllabi.\\n\\n4. Classics Series: Various large publishing houses, like Vintage or Penguin, have a type of classics series, while others, like Everyman\u2019s library, are entirely devoted to publishing \u201cthe classics.\u201d As Penguin is arguably one of the biggest publishers of anglophone literature (Alter et al., 2022), we collected their classics series, both individual titles and all titles extant in our corpus by authors included in the series.\\n\\nSome metrics, like GoodReads\u2019 rating count, are continuous, while others, like the Nobel Prize, are binary. This distinction allows for different statistical analyses and comparisons, enabling researchers to approach the question of literary quality from multiple angles. Crowd-based and expert-based metrics are only sometimes in agreement. For example, a high GoodReads rating count does not necessarily correlate with expert recognition (Bizzoni et al., 2023a), suggesting the multi-faceted nature of literary quality. See Tables 3 and 4 for a complete list of the metrics we include in the dataset.\\n\\n5. Textual Metrics: For each title in the collection, we provide several textual metrics.\\n\\n5.1. Readability: Readability formulae, like Flesch Ease, have used aspects such as sentence length, word lengths, and syllable count to measure linguistic complexity (Dale and Chall, 1948). Despite a multitude of formulae (Dubay, 2004), a handful of \u2018classic\u2019...\"}"}
{"id": "lrec-2024-main-71", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Number of titles in discontinuous quality proxies in our corpus.\\n\\n| Proxy            | Count | Mean   | Std     |\\n|------------------|-------|--------|---------|\\n| Translations     | 5082  | 11.77  | 21.47   |\\n| PageRank*        | 3558  | 0.15   | 0.24    |\\n| Audible Rat.Avg. | 629   | 4.17   | 0.50    |\\n| Audible Rat.Count| 629   | 796.92 | 3020.15 |\\n| GR Rat.Avg.      | 8989  | 3.77   | 0.36    |\\n| GR Rat.Count     | 8989  | 14368.39 | 121551.55 |\\n\\nTable 3: Continuous quality proxies. Proxies marked * are author-based. Note that the Wikipedia author page-rank has been multiplied with 100,000 for interpretability.\\n\\n| Proxy               | Count |\\n|---------------------|-------|\\n| Norton English*     | 62    |\\n| Norton American*    | 339   |\\n| OpenSyllabus*       | 477   |\\n| Penguin Classics Series | 77   |\\n| Penguin Classics Series* | 335 |\\n| Publishers Weekly Bestsellers | 139 |\\n| Goodreads Classics* | 62    |\\n| Goodreads Best 20th Century* | 44  |\\n| Nobel               | 85    |\\n| Pulitzer            | 54    |\\n| NBA                 | 108   |\\n| SciFi Awards        | 163   |\\n| Fantasy Awards      | 40    |\\n| Horror Awards       | 19    |\\n| Romantic Awards*    | 54    |\\n\\nTable 4: Discontinuous quality proxies, all literary awards (general and genre-oriented) appear below the line. Note that proxies marked * are author-based.\\n\\nReadability measures that go back to the 1970s remain widely used (Stajner et al., 2012). To avoid relying on one single interpretation of the readability concept, we offer five popular and interpretable formulas for the corpus, all calculated through the textstat package. These have been shown to be strongly correlated (Bizzoni et al., 2023a). They include the Flesch Reading Ease and Flesch-Kincaid Grade Level, both based on average sentence length (ASL) and syllable count per word; the SMOG Readability Formula that uses ASL and polysyllable count (McLaughlin, 1969); the Automated Readability Index, employing ASL and word length; and the New Dale\u2013Chall Readability Formula, which uses ASL and a \u2018difficult words\u2019 percentage (PDW), which represents the percentage of words unfamiliar to fourth graders (Chall and Dale, 1995; Dale and Chall, 1948).\\n\\nStylistic Metrics\\n\\nThe stylistic metrics that we provide are:\\n\\n1. Lexical Diversity or Type-Token Ratio: Measures the ratio of unique words to the total number of words in a text. Higher lexical diversity often suggests a richer vocabulary. A standard index of lexical richness, not used in readability metrics but normally considered indicative of a text\u2019s complexity and inner diversity (Torruella and Capsada, 2013).\\n\\n2. Average Sentence and Word Length: Average character-based sentence and word length. They both provide, in different ways, a simple yet effective measure of complexity. For example, Kerouac\u2019s *The Subterraneans*, a classic example of the \u201cspontaneous\u201d and vernacular prose of Beat Literature (Whaley, 2009), has the longest average sentence length.\\n\\n3. Compressibility: Measures how much a text is compressible through a standard compression algorithm. This measure becomes essentially a sign of redundancy and formulaic language.\"}"}
{"id": "lrec-2024-main-71", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the more a text tends to repeat sequences ad verbatim, the more compressible it will be (Benedetto et al., 2002; van Cranenburgh and Bod, 2017).\\n\\nUnigram and bigram entropy: entropy based on unigrams or bigram pairs, based on the code and study of Algee-Hewitt et al. (2016) of literary texts. Entropy refers to how much variation or randomness there is in terms of either words or word pairs (bigrams) in a given text. A lower entropy would indicate that words or bigrams recur more often, while a higher entropy would indicate a more significant variation in the vocabulary or the bigrams used. Unigram (word) entropy is, in this sense, similar to vocabulary richness measures.\\n\\nSentiment Analysis\\nAt an arguably deeper level, we computed the sentence-based sentiment arcs of the novels, using the nltk's implementation of VADER (Hutto and Gilbert, 2014), arguably one of the most widespread dictionary-based methods. We provide the full version of the arcs and their coarser-grain representation in twenty segments. The detrended sentiment arc based on our VADER scores of Hemingway's The Old Man and the Sea can be seen in Fig. 4, compared to a human baseline. Note that the Pearson and Spearman correlations of scores by the two human annotators for this work were robust (0.652, 0.624) but not perfect, reflecting the complexity of the task of assigning valences, as disagreements are considerable also among human annotators. In this light, the relatively straightforward rule-based system VADER appears to perform reasonably (Fig. 4), and has also been shown to have a high consistency across domains (Ribeiro et al., 2016).\\n\\nSentiment analysis in our study goes beyond merely categorizing the sentiment or overall valence of the text. We employ several statistical measures to provide a multi-faceted view of sentiment across the document. These measures include:\\n\\n- **Mean Sentiment**: This is the average sentiment score across all the sentences in the document. It provides an overall sense of the valence of the text.\\n\\n- **Standard Deviation of Sentiment (Std Sentiment)**: This metric captures the variability in sentiment across the document. A higher standard deviation implies a broader range of emotions expressed.\\n\\n- **End Sentiment**: Refers to the sentiment score of the concluding part of the document (the last 5% of the sentences). It provides insights into the sentiment with which the document concludes.\\n\\n- **Beginning Sentiment**: This is the sentiment score for the introductory part of the document (the first 5% of book). It sets the emotional stage for the reader.\\n\\n- **Difference Ending to Mean**: This is the difference between the end (5%) and the sentiment of the rest of the book. It indicates whether the document ends on a more positive or negative note compared to its overall valence.\\n\\n- **Hurst Exponent**: Used to detect long-term memory in time series data, the Hurst exponent in our context measures the persistence of sentiment over the document. Values near 0.5 suggest a random walk, while values far from 0.5 indicate trending or mean-reverting sentiment.\\n\\n- **Approximate Entropy**: This measures the complexity of the sentiment time series. A lower value indicates more regularity in the sentiment, while a higher value suggests a lower predictability.\\n\\nEach of these metrics serves a specific purpose, and when considered collectively, they provide a comprehensive understanding of the text's sentimental landscape. It is worth noting that these metrics are sensitive to the granularity of text segments (sentence vs. paragraph) and the sentiment lexicon used. Sentiment analysis (relying on word values and rules) provides a rare point of observation for novels, as it stands at the interface between their style and narrative structure. On one hand, the sentiment arcs of the novels represent the fluctuations of the narrative as developed through the novel. On the other hand, it detects the way in which the development is portrayed, rather than any judgment that the reader could give on the narrative, allowing us to detect the stylistic and rhetoric features of the text that we would otherwise easily override.\"}"}
{"id": "lrec-2024-main-71", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Detrended arcs and manually annotated valences of The Old Man and the Sea based on VADER valences and mean of human annotators (n=2).\\n\\n| Measure               | Mean  | Std.  |\\n|-----------------------|-------|-------|\\n| Wordcount             | 118584.71 | 64746.05 |\\n| Sentence Length       | 86.56 | 29.44 |\\n| Wordlength            | 3.67  | 0.18  |\\n| MSTTR-100             | 0.69  | 0.02  |\\n| Bzip                  | 2.92  | 0.14  |\\n| Bigram Entropy        | 14.63 | 0.55  |\\n| Word Entropy          | 9.69  | 0.30  |\\n| Flesch Ease           | 82.70 | 6.48  |\\n| Flesch Grade          | 5.19  | 1.74  |\\n| Smog                  | 8.20  | 1.05  |\\n| ARI                   | 6.91  | 2.06  |\\n| Dale Chall New        | 5.10  | 0.33  |\\n| Mean Sent.            | 0.03  | 0.04  |\\n| Std Sent.             | 0.35  | 0.04  |\\n| End Sent.             | 0.03  | 0.07  |\\n| Beginning Sent.       | 0.04  | 0.05  |\\n| Diff. Ending/Rest     | 0.01  | 0.05  |\\n| Hurst Exponent        | 0.61  | 0.04  |\\n| Approximate Entropy   | 1.75  | 0.15  |\\n\\nTable 5: Textual measures. From the bottom down: \u201csurface-level\u201d stylometrics, readability formul\u00e6, and measures associated with the novels\u2019 sentiment arcs.\\n\\n6. Metadata\\n\\nBeyond the textual and quality metrics, we provide metadata. The metadata accompanying our dataset is an essential framework for contextualizing its content. The fields collected for each book in the dataset include:\\n\\n- **Author Name**: The name of the individual or collective responsible for creating the work. It can be helpful in studies focusing on authorship patterns or historical context.\\n- **Title**: The book\u2019s title, which is instrumental for identification and categorical analysis.\\n- **Publication Date and Decade**: We provide the exact publication date and the decade to which the book belongs. These temporal markers assist in longitudinal studies and trend analysis.\\n- **Publishing Location**: This is the geographical location where the book was published, which can be valuable for regional studies and geopolitical analysis.\\n- **BookID**: A unique identifier assigned to each book in the dataset. It facilitates easy referencing and data manipulation.\\n- **Author\u2019s Gender**: Identifies the gender of the author(s). The distinction is currently binary.\\n- **Genre Tags**: Genre tags for one or more genres, manually added by a literary scholar. This addition could aid thematic categorization and genre-specific analyses. This information is available only for a subset of 1000 titles in the dataset. This limitation is due to the scope and difficulty of genre annotation.\\n\\nThe metadata fields offer a multi-dimensional lens to understand, segment, and analyze the dataset.\\n\\n7. Conclusion and Future Works\\n\\nWe have presented a large new dataset designed to study \u201cliterary quality\u201d as a compound of several different perspectives. Including crowd-based and expert-based assessments, the dataset allows for several combinations of textual and quality features and the study of continuous and discrete representations of \u201cliterary quality\u201d. To the best of...\"}"}
{"id": "lrec-2024-main-71", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"our knowledge, this is the largest extant dataset with multiple-perspective literary quality annotations containing extensive textual features. Naturally, we intend this dataset for scholars and critics to explore the complex interplays between textual and reception metrics. As it is, the dataset can be used to explore simple correlations between different textual metrics (e.g., the correlation between the mean sentiment and the end-sentiment of novels) and between different quality metrics alone (e.g., GoodReads' rating counts correlate more with audible rating counts than with the WorldCat's numbers). It is also essential to consider that binary quality metrics, such as the presence of a novel or an author in a given anthology, are not mutually exclusive. Some titles appear, for example, both in the Norton Anthology and in the Penguin Classics Series. This can allow the dataset users to obtain a non-binary metric, scoring higher the texts that appear in more than one proxy and creating a nuanced version of canonicity. However, the main goal of the dataset is to facilitate the study of the link between textual features and the perceived quality or reader appreciation of a literary text, but a subset of quality proxies can also be used to investigate \u201ccanonicity\u201d of literary texts. While we aim to provide a comprehensive range of quality metrics, we acknowledge that no metric can fully capture the nuanced and subjective nature of literary quality. Future work may incorporate additional metrics such as citations in academic work, or social media mentions, as well as a much more comprehensive range of textual features, such as syntactic and semantic profiles of the novels.\\n\\nBibliographical References\\n\\nMark Algee-Hewitt, Sarah Allison, Marissa Gemma, Franco Moretti, Ryan Heuser, and Hannah Walser. 2016. Canon/Archive. Large-scale Dynamics in the Literary Field. Pamphlets of the Stanford Literary Lab.\\n\\nEbba Cecilia Ovesdotter Alm. 2008. Affect in text and speech. Phd thesis, University of Illinois at Urbana-Champaign.\\n\\nAlexandra Alter, Elizabeth A. Harris, and David McCabe. 2022. Will the biggest publisher in the United States get even bigger? The New York Times.\\n\\nJacqueline Bach. 2022. Young Adult Boom. In Patrick O\u2019Donnell, Stephen J. Burn, and Lesley Larkin, editors, The Encyclopedia of Contemporary American Fiction 1980\u20132020, 1 edition, pages 1\u201310. Wiley.\\n\\nDario Benedetto, Emanuele Caglioti, and Vittorio Loreto. 2002. Language Trees and Zipping. Physical Review Letters, 88(4):1\u20135.\\n\\nJan Beran. 1994. Statistics for Long-Memory Processes, 1 edition. Chapman and Hall/CRC, New York.\\n\\nYuri Bizzoni, Pascale Feldkamp, Ida Marie Lassen, Mia Jacobsen, Mads Rosendahl Thomsen, and Kristoffer Nielbo. 2024. Good books are complex matters: Gauging complexity profiles across diverse categories of perceived literary quality.\\n\\nYuri Bizzoni, Ida Marie Lassen, Telma Peura, Mads Rosendahl Thomsen, and Kristoffer Nielbo. 2022a. Predicting Literary Quality How Perspectivist Should We Be? In Proceedings of the 1st Workshop on Perspectivist Approaches to NLP @LREC2022, pages 20\u201325, Marseille, France. European Language Resources Association.\\n\\nYuri Bizzoni, Pascale Moreira, Nicole Dwenger, Ida Lassen, Mads Thomsen, and Kristoffer Nielbo. 2023a. Good reads and easy novels: Readability and literary quality in a corpus of US-published fiction. In Proceedings of the 24th Nordic Conference on Computational Linguistics (NoDaLiDa), pages 42\u201351, T\u00f3rshavn, Faroe Islands. University of Tartu Library.\\n\\nYuri Bizzoni, Pascale Moreira, Mads Rosendahl Thomsen, and Kristoffer Nielbo. 2023b. Sentimental matters - predicting literary quality by sentiment analysis and stylometric features. In Proceedings of the 13th Workshop on Computational Approaches to Subjectivity, Sentiment, & Social Media Analysis, pages 11\u201318, Toronto, Canada. Association for Computational Linguistics.\\n\\nYuri Bizzoni, Telma Peura, Kristoffer Nielbo, and Mads Thomsen. 2022b. Fractal sentiments and fairy tales-fractal scaling of narrative arcs as predictor of the perceived quality of Andersen's fairy tales. Journal of Data Mining & Digital Humanities, NLP4DH.\\n\\nYuri Bizzoni, Telma Peura, Kristoffer Nielbo, and Mads Thomsen. 2022c. Fractality of sentiment arcs for literary quality assessment: The case of nobel laureates. In Proceedings of the 2nd International Workshop on Natural Language Processing for Digital Humanities, pages 31\u201341, Taipei, Taiwan. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-71", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"digital humanities research in the project guten-berg corpus. In Proceedings of the Fourth Workshop on Computational Linguistics for Literature, pages 42\u201347. Judith Brottrager, Annina Stahl, Arda Arslan, Ulrik Brandes, and Thomas Weitin. 2022.\\n\\nModeling and predicting literary reception. Journal of Computational Literary Studies, 1(1):1\u201327. Erik Cambria, Dipankar Das, Sivaji Bandyopadhyay, and Antonio Feraco. 2017.\\n\\nAffective computing and sentiment analysis. In A practical guide to sentiment analysis, pages 1\u201310. Springer.\\n\\nJeanne S. Chall and Edgar Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books.\\n\\nJonathan Cheng. 2020. Fleshing out models of gender in English-language novels (1850\u20132000). Journal of Cultural Analytics, 5(1):11652.\\n\\nTess Crosbie, Tim French, and Marc Conrad. 2013. Towards a model for replicating aesthetic literary appreciation. In Proceedings of the Fifth Workshop on Semantic Web Information Management, SWIM '13, pages 1\u20134, New York, NY, USA. Association for Computing Machinery.\\n\\nEdgar Dale and Jeanne S. Chall. 1948. A formula for predicting readability. Educational Research Bulletin, 27(1):11\u201328.\\n\\nIrina-Ana Drobot. 2013. Affective narratology. the emotional structure of stories. Philologica Jassyensia, 9(2):338.\\n\\nWilliam Dubay. 2004. The Principles of Readability. Impact Information.\\n\\nA. Eke, P. Herman, L. Kocsis, and L. R. Kozak. 2002. Fractal characterization of complexity in temporal physiological signals. Physiological Measurement, 23(1):R1.\\n\\nPascale Feldkamp, Yuri Bizzoni, Ida Marie S. Lassen, Mads Rosendahl Thomsen, and Kristoffer Nielbo. 2023. Readability and complexity: Diachronic evolution of literary language across 9000 novels. In Proceedings of the Joint 3rd International Conference on Natural Language Processing for Digital Humanities and 8th International Workshop on Computational Linguistics for Uralic Languages, pages 235\u2013247, Tokyo, Japan. Association for Computational Linguistics.\\n\\nCraig L. Garthwaite. 2014. Demand spillovers, combative advertising, and celebrity endorsements. American Economic Journal: Applied Economics, 6(2):76\u2013104.\\n\\nErnest Hemingway. 1999. On Writing. Touchstone, New York.\\n\\nQiyue Hu, Bin Liu, Mads Rosendahl Thomsen, Jianbo Gao, and Kristoffer L Nielbo. 2021. Dynamic evolution of sentiments in never let me go: Insights from multifractal theory and its implications for literary analysis. Digital Scholarship in the Humanities, 36(2):322\u2013332.\\n\\nChristoph Hube, Frank Fischer, Robert J\u00e4schke, Gerhard Lauer, and Mads Rosendahl Thomsen. 2017. World literature according to Wikipedia: Introduction to a DBpedia-based framework.\\n\\nClayton Hutto and Eric Gilbert. 2014. Vader: A parsimonious rule-based model for sentiment analysis of social media text. In Proceedings of the international AAAI conference on web and social media, volume 8, pages 216\u2013225.\\n\\nSM Mazharul Islam, Xin Dong, and Gerard de Melo. 2020. Domain-specific sentiment lexicons induced from labeled documents. In Proceedings of the 28th International Conference on Computational Linguistics, pages 6576\u20136587, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nJianbo Gao, H. Sultan, Jing Hu, and Wen-Wen Tung. 2010. Denoising Nonlinear Time Series by Adaptive Filtering and Wavelet Shrinkage: A Comparison. IEEE Signal Processing Letters, 17(3):237\u2013240.\\n\\nMatthew Jockers. 2017. Syuzhet: Extracts sentiment and sentiment-derived plot arcs from text (version 1.0.1).\\n\\nEvgeny Kim and Roman Klinger. 2018. A survey on sentiment and emotion analysis for computational literary studies. arXiv preprint arXiv:1808.03137.\\n\\nStephen King. 2010. On Writing: A Memoir of the Craft, anniversary edition. Scribner, New York.\\n\\nIrwin S. Kirsch, United States, Educational Testing Service, and National Center for Education Statistics, editors. 1993. Adult literacy in America: a first look at the results of the National Adult Literacy Survey, 2nd ed edition. Office of Educational Research and Improvement, U.S. Dept. of Education, Washington, D.C.\\n\\nJon P. Klancher. 1983. From \u201ccrowd\u201d to \u201caudience\u201d: The making of an English mass readership in the nineteenth century. ELH, 50(1):155\u2013173.\\n\\nCorina Koolen, Karina van Dalen-Oskam, Andreas van Cranenburgh, and Erica Nagelhout. 2020. Literary quality in the eye of the Dutch...\"}"}
{"id": "lrec-2024-main-71", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-71", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
