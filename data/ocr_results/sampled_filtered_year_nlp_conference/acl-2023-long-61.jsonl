{"id": "acl-2023-long-61", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scaling Multilingual Corpora and Language Models to 500 Languages\\n\\nAyyoob Imani\u22171,2, Peiqin Lin\u22171,2, Amir Hossein Kargaran1,2, Silvia Severini1,2, Masoud Jalili Sabet1, Nora Kassner1,2, Chunlan Ma1,2, Helmut Schmid1, Andr\u00e9 F. T. Martins3,4,5, Fran\u00e7ois Yvon6 and Hinrich Sch\u00fctze1,2\\n\\n1CIS, LMU Munich, Germany\\n2Munich Center for Machine Learning (MCML), Germany\\n3Instituto Superior T\u00e9cnico (Lisbon ELLIS Unit)\\n4Instituto de Telecomunica\u00e7\u00f5es\\n5Unbabel\\n6Sorbonne Universit\u00e9, CNRS, ISIR, France\\n\\n{ayyoob, linpq, amir, silvia}@cis.lmu.de\\n\\nAbstract\\n\\nThe NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., making them better for about 100 languages. We instead scale LLMs horizontally: we create, through continued pretraining, Glot500-m, an LLM that covers 511 predominantly low-resource languages. An important part of this effort is to collect and clean Glot500-c, a corpus that covers these 511 languages and allows us to train Glot500-m. We evaluate Glot500-m on five diverse tasks across these languages. We observe large improvements for both high-resource and low-resource languages compared to an XLM-R baseline. Our analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model. Our work addresses an important goal of NLP research: we should not limit NLP to a small fraction of the world\u2019s languages and instead strive to support as many languages as possible to bring the benefits of NLP technology to all languages and cultures. Code, data and models are available at https://github.com/cisnlp/Glot500.\\n\\n1 Introduction\\n\\nThe NLP community has mainly focused on scaling Large Language Models (LLMs) vertically, i.e., deepening their understanding of high-resource languages by scaling up parameters and training data. While this approach has revolutionized NLP, the achievements are largely limited to high-resource languages. Examples of \u201cvertical\u201d LLMs are GPT3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022) and Bloom (BigScience et al., 2022). In this paper, we create Glot500-m, a model that instead focuses on scaling multilingual LLMs horizontally, i.e., scaling to a large number of languages the great majority of which is low-resource. As LLMs are essential for progress in NLP, lack of LLMs supporting low-resource languages is a serious impediment to bringing NLP to all of the world\u2019s languages and cultures. Our goal is to address this need with the creation of Glot500-m.\\n\\nExisting multilingual LLMs support only about 100 (Conneau et al., 2020) out of the 7000 languages of the world. These supported languages are the ones for which large amounts of training data are available through projects such as Oscar (Su\u00e1rez et al., 2019) and the Wikipedia dumps. Following Siddhant et al. (2022), we refer to the 100 languages covered by XLM-R (Conneau et al., 2020) as head languages and to the remaining languages as tail languages. This terminology is motivated by the skewed distribution of available data per language: for the best-resourced languages there are huge corpora available, but for the long tail of languages, only small corpora exist. This is a key problem we address: the availability of data for tail languages is limited compared to head languages. As a result, tail languages have often been ignored by language technologies (Joshi et al., 2020).\\n\\nAlthough there exists some work on machine translation for a large number of tail languages (Costa-juss\u00e0 et al., 2022; Bapna et al., 2022), existing LLMs for tail languages are limited to a relatively small number of languages (Wang et al., 2019; Alabi et al., 2022; Wang et al., 2022). In this paper, we address this gap. Our work has three parts. (i) Corpus collection. We collect Glot2000-c, a corpus covering thousands of tail languages. (ii) Model training. Using Glot500-c, a subset of Glot2000-c, we train Glot500-m, an LLM covering 511 languages. (iii) Validation. We conduct an extensive evaluation of the quality of Glot500-m\u2019s...\"}"}
{"id": "acl-2023-long-61", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"representations of tail languages on a diverse suite of tasks.\\n\\nIn more detail, corpus collection considers three major sources: websites that are known to publish content in specific languages, corpora with classified multilingual content and datasets published in specific tail languages. The resulting dataset Glot2000-c comprises 700GB in 2266 languages collected from \\\\( \\\\approx 150 \\\\) sources. After cleaning and deduplication, we create the subset Glot500-c, consisting of 511 languages and 534 language-scripts (where we define a language-script as a combination of ISO 639-3 and script) to train Glot500-m.\\n\\nOur criterion for including a language-script in Glot500-c is that it includes more than 30,000 sentences.\\n\\nModel training. To train Glot500-m, we employ vocabulary extension and continued pretraining. XLM-R\u2019s vocabulary is extended with new tokens trained on Glot500-c. We then perform continued pretraining of XLM-R with the MLM objective (Devlin et al., 2019).\\n\\nValidation. We comprehensively evaluate Glot500-m on a diverse suite of natural language understanding, sequence labeling and multilingual tasks for hundreds of languages. The results demonstrate that Glot500-m performs better than XLM-R-B (XLM-R-base) for tail languages by a large margin while performing comparably (or better) for head languages.\\n\\nPrevious work on multilinguality has been hindered by the lack of LLMs supporting a large number of languages. This limitation has led to studies being conducted in settings dissimilar from real-world scenarios. For example, Dufter and Sch\u00fctze (2020) use synthetic language data. And the curse of multilinguality has been primarily studied for a set of high-resource languages (Conneau et al., 2020). By creating Glot500-m, we can investigate these issues in a more realistic setting. We make code, data and trained models available to foster research by the community on how to include hundreds of languages that are currently ill-served by NLP technology.\\n\\nContributions. (i) We train the multilingual model Glot500-m on a 600GB corpus, covering more than 500 diverse languages, and make it publicly available at https://github.com/cisnlp/Glot500. (ii) We collect and clean Glot500-c, a corpus that covers these diverse languages and allows us to train Glot500-m, and will make as much of it publicly available as possible. (iii) We evaluate Glot500-m on pseudoperplexity and on five diverse tasks across these languages. We observe large improvements for low-resource languages compared to an XLM-R baseline. (iv) Our extensive analysis shows that no single factor explains the quality of multilingual LLM representations. Rather, a combination of factors determines quality including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model. (v) Our work addresses an important goal of NLP research: we should not limit NLP to a relatively small number of high-resource languages and instead strive to support as many languages as possible to bring the benefits of NLP to all languages and cultures.\\n\\n2 Related Work\\nTraining multilingual LLMs using the masked language modeling (MLM) objective is effective to achieve cross-lingual representations (Devlin et al., 2019; Conneau et al., 2020). These models can be further improved by incorporating techniques such as discriminative pre-training (Chiet al., 2022) and the use of parallel data (Yanget al., 2020; Chiet al., 2021). However, this primarily benefits a limited set of languages with large corpora.\\n\\nRecent research has attempted to extend existing LLMs to languages with limited resources. Wang et al. (2019) propose vocabulary extension; Ebrahimi and Kann (2021) investigate adaptation methods, including MLM and Translation Language Model (TLM) objectives and adapters; Alabi et al. (2022) adapt XLM-R to 17 African languages; Wang et al. (2022) expand language models to low-resource languages using bilingual lexicons. Alternatively, parameter-efficient fine-tuning adapts pre-trained models to new languages by training a small set of weights effectively (Zhao et al., 2020; Pfeiffer et al., 2021; Anselletal., 2022).\\n\\nPfeiffer et al. (2022) address the \u201ccurse of multilinguality\u201d by sharing a part of the model among all languages and having separate modules for each language. We show that the common perception that multilinguality increases as we add more languages, until from some point, it starts decreasing, is naive. The amount of available data per language and the similarity between languages also play important roles (\u00a76.8).\\n\\nAnother approach trains LLMs from scratch for a limited number of tail languages; e.g., AfriBERTa\\n\\n3 Glot500-c\\n\\nThe Glot500-c dataset is a collection of multilingual text from the internet, websites, and corpora. It was created to provide a comprehensive and diverse set of languages for training multilingual language models. The dataset comprises 700GB of text from 2266 languages, all collected from approximately 150 sources. After cleaning and deduplication, the resulting dataset Glot500-c consists of 511 languages and 534 language-scripts, where a language-script is a combination of an ISO 639-3 code and a script.\\n\\nThe Glot500-c dataset was created to support research on multilingual language models, particularly for languages with limited resources. The dataset is available publicly at https://github.com/cisnlp/Glot500, and the creators have made an effort to ensure that as much of it as possible is made publicly available.\\n\\n4 Model Training\\n\\nTo train the Glot500-m model, vocabulary extension and continued pretraining were employed. XLM-R\u2019s vocabulary was extended with new tokens trained on the Glot500-c dataset. Then, continued pretraining of XLM-R was performed using the MLM objective (Devlin et al., 2019).\\n\\n5 Validation\\n\\nThe Glot500-m model was comprehensively evaluated on a diverse suite of natural language understanding, sequence labeling, and multilingual tasks for hundreds of languages. The results showed that Glot500-m performs better than XLM-R-B (XLM-R-base) for tail languages by a large margin while performing comparably (or better) for head languages. This demonstrates the effectiveness of the Glot500-c dataset in supporting high-quality multilingual language models.\\n\\n6 Related Work\\n\\nTraining multilingual language models using the MLM objective has been shown to be effective in achieving cross-lingual representations (Devlin et al., 2019; Conneau et al., 2020). However, this approach primarily benefits languages with large corpora.\\n\\nRecent research has focused on extending existing models to languages with limited resources. Techniques such as vocabulary extension (Wang et al., 2019), adaptation methods (Ebrahimi and Kann, 2021), and parameter-efficient fine-tuning (Zhao et al., 2020; Pfeiffer et al., 2021; Anselletal., 2022) have been explored. These methods aim to improve model performance for languages with limited data.\\n\\nAn important challenge in multilingual language modeling is the \u201ccurse of multilinguality,\u201d which refers to the notion that adding more languages to a model may not necessarily improve performance. Pfeiffer et al. (2022) addressed this issue by sharing a part of the model among all languages and having separate modules for each language. They showed that the common perception of a linear increase in model quality with the addition of more languages is naive. Factors such as the amount of available data per language and the similarity between languages also play crucial roles in determining model performance (\u00a76.8).\\n\\n7 Conclusion\\n\\nThe Glot500-c dataset and the Glot500-m model provide valuable resources for research in multilingual language processing. They enable more comprehensive and fair evaluations of model performance across a diverse range of languages, addressing the limitations of previous studies. The availability of these resources fosters innovation and collaboration in the field, contributing to the development of more inclusive and effective language technologies.\"}"}
{"id": "acl-2023-long-61", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oguejiet al., 2021a and IndicNLPSuite (Kakwani et al., 2020) are LLMs for 11 African languages and 11 Indic languages. In concurrent work, Adebara et al. (2022) train a multilingual model for 517 African languages on a 42 GB corpus, but without making the model available and with an evaluation on a smaller number of languages than ours.\\n\\nClosely related to our work on corpus creation, Bapna et al. (2022) and Costa-Juss\u00e0 et al. (2022) also create NLP resources for a large number of tail languages. They train a language identifier model and extract textual data for tail languages from large-scale web crawls. This approach is effective, but it requires significant computational resources and native speakers for all tail languages. This is hard to do outside of large corporations. Bapna et al. (2022) have not made their data available. Costa-Juss\u00e0 et al. (2022) have only released a portion of their data in around 200 languages.\\n\\nA key benefit of \u201chorizontally\u201d scaled multilingual LLMs is transfer from high- to low-resource languages. Our evaluations suggest that Glot500-m excels at this, but this is not the main focus of our paper. There is a large body of work on crosslingual transfer: (Artetxe and Schwenk, 2019; Imani-Goghar et al., 2022; Lauscher et al., 2020; Conneau et al., 2020; Turc et al., 2021; Fan et al., 2021; Severini et al., 2022; Choenni and Shutova, 2022; Wang et al., 2023), inter alia.\\n\\n3 Glot2000-c\\n3.1 Data Collection\\nOne of the major challenges in developing NLP technologies for tail languages is the scarcity of high-quality training data. In this work, we propose a lightweight methodology that is easily replicable for academic labs. We identify tail language data previously published by researchers, publishers, and translators and then crawl or download them. By crawling a few websites and compiling data from around 150 different datasets, we amass more than 700GB of text in 2266 languages. We will refer to these sources of data as data sources. Our data covers many domains, including religious texts, news articles, and scientific papers. Some of the data sources are high-quality, verified by native speakers, translators, and linguists. Others are less reliable such as web crawls and Wikipedia dumps. It is therefore necessary to clean the data. For a list of data sources, see \u00a7C.\\n\\n3.2 Language-Scripts\\nSome languages are written in multiple scripts; e.g., Tajik is written in both Cyrillic and Arabic scripts. Some data sources indicate the script, but others either do not or provide mixed text in multiple scripts. We detect the script for each sentence and treat each language-script as a separate entity.\\n\\n3.3 Ngram LMs and Language Divergence\\nWe train a 3-gram character-level language model $M_i$ for each language-script $L_i$, using KenLM (Heafield, 2011). We refer to the perplexity calculated for the corpus of language $L_i$ using language model $M_j$ as $PP(M_j, L_i)$. Similar to Gamallo et al. (2017), we define a perplexity-based divergence measure of languages $L_i$ and $L_j$ as:\\n\\n$$D_{L_i, L_j} = \\\\max(PP(M_j, L_i), PP(M_i, L_j))$$\\n\\nWe use $D$ to filter out noisy data in \u00a73.4 and study the effect of similar languages in LLM training in \u00a76.7 and \u00a76.8. For more details, see \u00a7A.\\n\\n3.4 Data Cleaning\\nTo remove noise, we use chunk-level and corpus-level filters.\\n\\nWhile some sources are sentence-split, others provide multiple sentences (e.g., a paragraph) as one chunk. Chunk-level filters process each chunk of text from a data source as a unit, without sentence-splitting. Some chunk-level filters are based on the notion of word: we use white space tokenization when possible and otherwise resort to sentencePiece (Kudo and Richardson, 2018) trained by Costa-Juss\u00e0 et al. (2022).\\n\\nAs chunk-level filters, we employ the sentence-level filters SF1\u2013SF5 from BigScience ROOTS (Lauren\u00e7on et al., 2022).\\n\\nSF1 Character repetition. If the ratio of repeated characters is too high, it is likely that the sentence has not enough textual content.\\n\\nSF2 Word repetition. A high ratio of repeated words indicates non-useful repetitive content.\\n\\nSF3 Special characters. Sentences with a high ratio of special characters are likely to be crawling artifacts or computer code.\\n\\nSF4 Insufficient number of words. Since training language models requires enough context, very small chunks of text are not useful.\\n\\nSF5 Deduplication. If two sentences are identical after eliminating punctuation and white space, one is removed.\"}"}
{"id": "acl-2023-long-61", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the rest of the paper, we refer to a chunk as a sentence'. A sentence' can consist of a short segment, a complete sentence or a chunk (i.e., several sentences).\\n\\nCorpus-level filters detect if the corpus of a language-script is noisy; e.g., the corpus is in another language or consists of non-meaningful content such as tabular data. We employ filters CF1 and CF2.\\n\\n**CF1** In case of mismatch between language and script, the corpus is removed; e.g., Chinese written in Arabic is unlikely to be Chinese.\\n\\n**CF2** Perplexity mismatch. For each language-script L1, we find its closest language-script L2: the language-script with the lowest perplexity divergence (\u00a73.3). If L1 and L2 are not in the same typological family, we check L1/L2 manually and take appropriate actions such as removing the corpus (e.g., if it is actually English) or correcting the ISO code assigned to the corpus.\\n\\n### 3.5 Training Data: Glot500-c\\n\\nAmong the 2000+ language-scripts that we collected data for, after cleaning, most have too little data for pretraining LLMs. It is difficult to quantify the minimum amount needed for pretraining. Therefore, we pick a relatively high \\\"safe\\\" threshold, 30,000 sentences', for inclusion of language-scripts in model training. This allows us to train the model effectively and cover many low-resource languages. Table 1 gives Glot500-c statistics. See \u00a7B for a list of language-scripts. We train Glot500-m on Glot500-c; note that while Glot500-c focuses on tail languages, it contains some data in head languages which we include in Glot500-m training to prevent catastrophic forgetting.\\n\\nWe divide the corpus for each language into train/dev/test, reserving 1000 sentences' each for dev and test and using the rest for train. We pick 1000 parallel verses if we have a Bible translation and add 500 each to test and dev. These parallel verses convey identical meanings and facilitate crosslingual evaluation. We pretrain the model using only the training data.\\n\\n### Table 2: Model sizes\\n\\n| Model       | Model Size | Vocab Size | Transformer Size |\\n|-------------|------------|------------|-----------------|\\n| Glot500-m   | 395M       | 401K       | 86M             |\\n| XLM-R-B     | 278M       | 250K       | 86M             |\\n| XLM-R-L     | 560M       | 250K       | 303M            |\\n\\nGlot500-m and XLM-R-B have the same transformer size, but Glot500-m has a larger vocabulary, resulting in an overall larger model.\\n\\n4 Glot500-m\\n\\n#### 4.1 Vocabulary Extension\\n\\nTo extend XLM-R's vocabulary, we use SentencePiece (Kudo and Richardson, 2018) with a unigram language model (Kudo, 2018) to train a tokenizer with a vocabulary size of 250K on Glot500-c. We sample data from different language-scripts according to a multinomial distribution, with $\\\\alpha = 0.3$. The amount we sample for head languages is the same as tail languages with the lowest amount; this favors tail languages \u2013 head languages are already well learned by XLM-R. We merge the obtained tokens with XLM-R's vocabulary. About 100K new tokens were in fact old tokens, i.e., already part of XLM-R's vocabulary. We take the probabilities of the (genuinely) new tokens directly from SentencePiece. After adding the 151K new tokens to XLM-R's vocabulary (which has size 250K), the vocabulary size of Glot500-m is 401K.\\n\\nWe could also calculate probabilities of existing and new tokens over a mixture of original XLM-R training corpus and Glot500-c (Chung et al., 2020). For head languages, the percentage of changed tokens using the new tokenizer compared to the original tokenizer ranges from 0.2% to 50%. However, we found no relationship between percentage of changed tokens and change in performance on downstream tasks. Thus, there was little effect of tokenization in our experiments.\\n\\n#### 4.2 Continued Pretraining\\n\\nWe create Glot500-m by continued pretraining of XLM-R-B with the MLM objective. The optimizer used is Adam with betas (0.9, 0.999). Initial learning rate: $5e^{-5}$. Each training step contains a batch of 384 training samples randomly picked from all language-scripts. The sampling strategy across language-scripts is the same as for vocabulary.\"}"}
{"id": "acl-2023-long-61", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation tasks and measures.\\n\\n| head | tail | measure (%) |\\n|------|------|-------------|\\n| Sentence Retrieval Tatoeba | 70 | 28 |\\n| Sentence Retrieval Bible | 94 | 275 |\\n| Text Classification | 90 | 264 |\\n| NER | 89 | 75 |\\n| POS | 63 | 28 |\\n| Roundtrip Alignment | 85 | 288 |\\n\\nWe save checkpoints every 10K steps and select the checkpoint with the best average performance on downstream tasks by early stopping. Table 2 lists the sizes of XLM-R-B, XLM-R-L and Glot500-m. Except for a larger vocabulary (\u00a74.1), Glot500-m has the same size as XLM-R-B.\\n\\nWe train Glot500-m on a server with eight NVIDIA RTX A6000 GPUs for two weeks. Similar to XLM-R, we concatenate sentences of a language-script and feed them as a stream to the tokenizer. The resulting output is then divided into chunks of 512 tokens and fed to the model.\\n\\nExperimental Setup\\n\\nFor most tail languages, there are no manually labelled evaluation data. Therefore, we adopt a mixed evaluation strategy: based partly on human labels, partly on evaluation methods that are applicable to many languages without requiring gold data. Table 3 lists all our evaluation tasks.\\n\\nPerplexity\\n\\nFollowing Salazar et al. (2020), we calculate pseudoperplexity (PPPL) over the held-out test set. PPPL is based on masking tokens one-by-one (not left to right). Salazar et al. (2020) give evidence that PPPL is a better measure of linguistic acceptability compared to standard left-to-right perplexity.\\n\\nRoundtrip Alignment\\n\\nFor assessing the quality of multilingual representations for a broad range of tail languages without human gold data, we adopt roundtrip evaluation (Dufter et al., 2018). We first word-align sentences in a parallel corpus based on the multilingual representations of an LLM. We then start from a word $w$ in a sentence in language-script L1, follow the alignment links to its translations in language-script L2, then the alignment links from L2 to L3 and so on, until in the end we follow alignment links back to L1. If this \u201croundtrip\u201d gets us back to $w$, then it indicates that the LLM has similar representations for the meaning of $w$ in language-scripts L1, L2, L3, etc. In other words, the cross-lingual quality of representations is high. Vice versa, failure to get back to $w$ is a sign of poor multilingual representations.\\n\\nWe use SimAlign (Jalili Sabet et al., 2020) and align on the sub-word level on the Bible part of test, based on the representations of the LLM computed by transformer layer 8 as suggested in the original paper. We use intersection symmetrization: each word in a sentence is aligned to at most one word in the other sentence.\\n\\nAs evaluation measure we compute the percentage of roundtrips that were successes, i.e., the roundtrip starts at $w$ in L1 and returns back to $w$.\\n\\nFor each language-script in test, we randomly select three language-scripts as intermediate points L2, L3, L4. Since the intermediate points influence the results, we run the experiment five times with different intermediate points and report the average. All models are evaluated with the same five sets of three intermediate language-scripts.\\n\\nSequence Labeling\\n\\nWe consider two sequence labeling tasks: Named Entity Recognition (NER) and Part-Of-Speech (POS) tagging. We use the WikiANN dataset (Pan et al., 2017) for NER and version v2.11 of Universal Dependencies (UD) (de Marneffe et al., 2021) for POS. Since training data does not exist for some languages, we finetune on English (with early stopping based on dev) and evaluate zero-shot transfer on all languages covered by WikiANN/UD. We set the learning rate to 2e-5 with Adam.\\n\\nSentence Retrieval\\n\\nFollowing (Hu et al., 2020), we use up to 1000 English-aligned sentences from Tatoeba (Artetxe and Schwenk, 2019) to evaluate SentRetr (sentence retrieval). We also use 500 English-aligned sentences from the Bible part of test. We find nearest neighbors using cosine similarity based on the average word embeddings in layer $l = 8$ \u2013 following Jalili Sabet et al. (2020) \u2013 and compute top10 accuracy. For fair comparison and because the architectures are the same, we do not optimize the hyperparameter $l$ for Glot500-m and XLM-R-B.\\n\\nText Classification\\n\\nWe evaluate on Taxi1500 (Ma et al., 2023). It provides gold data for text classification with six classes in a large number of language-scripts of which Glot500-m supports 354. We finetune on English (with early stopping on dev) and evaluate zero-shot on test of the target language-script. Learning rate: 2e-5, batch size:...\"}"}
{"id": "acl-2023-long-61", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we discuss aggregate results. For detailed results, see \u00a7D and \u00a7E.\\n\\n6.1 Results\\n\\nTable 4 gives results. Glot500-m outperforms XLM-R-B on all tasks for both head and tail language-scripts, except for POS on head. That Glot500-m outperforms XLM-R-B is expected for tail language-scripts (i.e., those not covered by XLM-R). For these language-scripts the improvement margin is large. Outperformance may seem counterintuitive for head language-scripts (those covered by XLM-R) since Glot500-m has the same number of (non-embedding) parameters as XLM-R-B. Since the number of covered languages has greatly increased, leaving less capacity per language, we might expect underperformance. There are a few possible explanations. First, XLM-R may be undertrained, and the inclusion of more head language training data may improve their representations. Second, having more languages may improve multilinguality by allowing languages to synergize and enhance each other's representations and cross-lingual transfer. Third, there are languages similar to head languages among the tail languages, which in turn aids head languages.\\n\\nThe gap between Glot500-m and the baselines for tail language-scripts in sequence labeling is smaller. These tasks do not require as deep an understanding of language and thus transfer from head to tail language-scripts is easier through shared tokens.\\n\\nGlot500-m also outperforms XLM-R-L for tail language-scripts (all tasks) and head language-scripts (3 tasks). This suggests that scaling up size is not the only way for improvements. We can also improve the quality of multilingual LLM representations by increasing the number of languages.\\n\\n6.2 Language Coverage\\n\\nTable 5 compares Glot500-m vs. XLM-R-B on pseudoperplexity. For fair comparison we use word-level normalization. For 69 head language-scripts, Glot500-m underperforms XLM-R-B. This is expected as Glot500-m's training data is small for these language-scripts. Glot500-m outperforms XLM-R-B for 420 tail language-scripts.\\n\\nThere are eight tail language-scripts for which Glot500-m performs worse than XLM-R-B. Five are tail languages with a similar head language where the two share a macro-language: est/Standard Estonian (est/Estonian), sqi/Gheg Albanian (sqi/Albanian), nor/Norwegian Bokmal (nor/Norwegian), srp/Serbo-Croatian (srp/Serbian), lav/Standard Latvian (lav/Latvian). Since XLM-R-B's pretraining corpus is large for the five head languages, its performance is good for the closest tail languages.\\n\\nThe other three languages all have a unique script: sat/Santali (Ol Chiki script), div/Dhivehi (Thaana script), iku/Inuktitut (Inuktitut syllabics). For these languages, XLM-R-B's tokenizer returns many UNK tokens since it is not trained on these scripts, resulting in an unreasonably optimistic estimate of pseudoperplexity by our implementation. Glot500-m's token-level normalized pseudoperplexity ranges from 1.95 for lhu/Lahu to 94.4 for tok/Toki Pona. The average is 13.5, the median 10.6. We analyze the five language-scripts with the highest pseudoperplexity: tok_Latn, luo_Latn, acm_Arab, ach_Latn, and teo_Latn.\\n\\nTok/Pona is a constructed language. According to Wikipedia: \u201cEssentially identical concepts can be described by different words as the choice relies on the speaker's perception and experience.\u201d This property can result in higher variability and higher perplexity.\\n\\nacm/Mesopotamian Arabic contains a large number of tweets in raw form. This may result in difficult-to-predict tokens in test.\\n\\nluo/Luo, ach/Acoli and teo/Teso are related Nilotic languages spoken in Kenya, Tanzania, Uganda and South Sudan. Their high perplexity results from the high variability of their vocabulary and grammatical structure.\"}"}
{"id": "acl-2023-long-61", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Evaluation of XLM-R base and large (XLM-R-B and XLM-R-L) and Glot500-mon pseudoperplexity and six multilingual tasks across 5 seeds. Each number is an average over head, tail and all language-scripts. See \u00a7D, \u00a7E for results per task and language-script. Glot500-m outperforms XLM-R-B in all tasks for head (except for POS) and tail language-scripts and XLM-R-L for tail language-scripts. Best result per row/column group in bold.\\n\\n| Task                | Head | Tail | All  |\\n|---------------------|------|------|------|\\n| Pseudoperplexity    | 304.2| 168.6| 12.2 |\\n| Sentence Retrieval  | 32.6 | 59.8 | 56.6 |\\n| Bible               | 7.4  | 43.2 | 19.3 |\\n| Text Classification | 13.7 | 46.6 | 23.3 |\\n| NER                 | 47.5 | 60.7 | 55.3 |\\n| POS                 | 41.7 | 62.3 | 65.8 |\\n| Roundtrip Alignment | 2.6  | 4.5  | 2.8  |\\n\\nTable 5: Pseudoperplexity Glot500-m vs XLM-R-B. Glot500-m\u2019s worse performance on head can be attributed to smaller training corpora and the relative difficulty of learning five times more languages with the same number of (non-embedding) parameters. Glot500-m performs better on almost all tail language-scripts. \u00a76.2 discusses the eight exceptions.\\n\\n6.3 Training Progression\\n\\nTo analyze the training process, we evaluate Glot500-m on sequence labeling and SentRetr at 10,000-step intervals. Figure 1 shows that performance improves rapidly at the onset of training, but then the rate of improvement slows down. This trend is particularly pronounced for tail languages in SentRetr. In comparison, sequence labeling is relatively straightforward, with the baseline (XLM-R-B, epoch 0) achieving high performance by correctly transferring prevalent classes such as verb and noun through shared vocabulary, resulting in a smaller improvement of Glot500-m vs. XLM-R-B.\\n\\nFor SentRetr, we observe larger improvements for the Bible than for Tatoeba. This is likely due to the higher proportion of religious data in Glot500-c, compared to XLM-R\u2019s training data (i.e., CC100). The average performance on downstream tasks peaks at 480K steps. We have taken a snapshot of Glot500-m at this stage and released it.\\n\\n6.4 Analysis across Language-Scripts\\n\\nTo analyze the effect of language-scripts, we select five tail language-scripts each with the largest and smallest gain when comparing Glot500-m vs. XLM-R-B for SentRetr and sequence labeling. Table 6 shows that Glot500-m improves languages with scripts not covered by XLM-R (e.g., div/Dhivehi, Thaana script, see \u00a76.2) by a large margin since XLM-R simply regards the uncovered scripts as unknown tokens and cannot compute meaningful representations for the input. The large amount of data we collected in Glot500-c also contributes to the improvement for tail languages, e.g., for tat_Cyrl (Tatar) in SentRetr Tatoeba and mlt_Latn (Maltese) in POS. See \u00a76.7 for a detailed analysis of the effect of corpus size.\\n\\nOn the other hand, Glot500-m achieves just comparable or even worse results for some language-scripts. We see at least three explanations. (i) As discussed in \u00a76.2, some tail languages (e.g., nob/Norwegian Bokmal) are close to a head language (e.g., nor/Norwegian), so Glot500-m has no advantage over XLM-R-B. (ii) A language is at the low end of our corpus size range (i.e., 30,000 sentences). Example: xav_Latn, Xav\u00e1nte. (iii) Some languages are completely distinct from all other languages in Glot500-c, thus without support from any similar language. An example is mau_Latn, Huautla Mazatec. Glot500-m has a much harder tail...\"}"}
{"id": "acl-2023-long-61", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Results for five tail languages with the largest (high end) and smallest (low end) gain Glot500-m vs. XLM-R-B for four tasks. Glot500-m's gain over XLM-R-B is large at the high end and small or slightly negative at the low end. L = Latin, C = Cyrillic, H = Hani, A = Armenian, T = Thaana.\\n\\n| language-script | XLM-R-B | Glot500-m | gain   |\\n|-----------------|---------|-----------|--------|\\n| uig_Arab head   |         | 45.8      | 56.2   | 10.4   |\\n| uig_Latn tail   |         | 9.8       | 62.8   | 53.0   |\\n| hin_Deva head   |         | 67.0      | 76.6   | 9.6    |\\n| hin_Latn tail   |         | 13.6      | 43.2   | 29.6   |\\n| uzb_Latn head   |         | 54.8      | 67.6   | 12.8   |\\n| uzb_Cyrl tail   |         | 6.2       | 78.8   | 72.6   |\\n| kaa_Cyrl tail   |         | 17.6      | 73.8   | 56.2   |\\n| kaa_Latn tail   |         | 9.2       | 43.4   | 34.2   |\\n| kmr_Cyrl tail   |         | 4.0       | 42.4   | 38.4   |\\n| kmr_Latn tail   |         | 35.8      | 63.0   | 27.2   |\\n| tuk_Cyrl tail   |         | 13.6      | 65.0   | 51.4   |\\n| tuk_Latn tail   |         | 9.6       | 66.2   | 56.6   |\\n\\nTable 7: Sentence Retrieval Bible performance of Glot500-m and XLM-R-B for six languages with two scripts: Uighur (uig), Hindi (hin), Uzbek (uzb), Kara-Kalpak (kaa), Northern Kurdish (kmr), Turkmen (tuk). Glot500-m clearly outperforms XLM-R-B with large differences for tail language-scripts.\\n\\nUnsurprisingly, XLM-R performs much better for a language-script it was pretrained on (\\\"head\\\") than on one that it was not (\\\"tail\\\"). We can improve the performance of a language, even surpassing the language-script covered by XLM-R, if we collect enough data for its script not covered by XLM-R. For languages with two scripts not covered by XLM-R, the performance is better for the script for which we collect a larger corpus. For example, kaa_Cyrl (Kara-Kalpak) has about three times as much data as kaa_Latn. This explains why kaa_Cyrl outperforms kaa_Latn by 30%.\\n\\nDufter and Sch\u00fctze (2020) found that, after training a multilingual model with two scripts for English (natural English and \\\"fake English\\\"), the model performed well at zero-shot transfer if the capacity of the model was of the right size (i.e., not too small, not too large). Our experiments with real data show the complexity of the issue: even if there is a \\\"right\\\" size for an LLM that supports both full acquisition of languages and multilingual transfer, this size is difficult to determine and it may be different for different language pairs in a large horizontally scaled model like Glot500-m.\\n\\n6.6 Analysis across Language Families\\n\\nTable 8 compares SentRetr performance Glot500-m vs. XLM-R-B for seven language families that have ten or more language-scripts in Glot500-c. We assign languages to families based on Glottolog.\\n\\nGenerally, XLM-R has better performance the more language-scripts from a language family are represented in its training data; e.g., performance is better for indo1319 and worse for maya1287. The results suggest that Glot500-m's improvement over\\n\\n4http://glottolog.org/glottolog/family\"}"}
{"id": "acl-2023-long-61", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Average Sentence Retrieval Bible performance of Glot500-m and XLM-R-B for seven language families. The difference in coverage of a family by Glot500-m vs. XLM-R-B is partially predictive of the performance difference.\\n\\n| Family | Glot500-m | XLM-R-B | Gain |\\n|--------|-----------|---------|------|\\n| indo1319 | 91 | 50 | 41.5 |\\n| atla1278 | 69 | 2 | 5.5 |\\n| aust1307 | 53 | 6 | 13.7 |\\n| turk1311 | 22 | 7 | 20.1 |\\n| sino1245 | 22 | 2 | 7.6 |\\n| maya1287 | 15 | 0 | 3.8 |\\n| afro1255 | 12 | 5 | 13.0 |\\n\\nTable 9: Performance on Sentence Retrieval Bible of continued pretraining on just one language-script (Glot+1) vs. on Glot500-c (Glot500-m). Glot500-m underperforms on the top three and outperforms on the bottom three. Our explanation is that the second group is supported by closely related languages in Glot500-c; e.g., for Southern Quechua (quh), Glot500-m also covers closely related Cuzco Quechua (quz). For the first group this is not the case; e.g., the Wa language (wbm) has no close relative in Glot500-c.\\n\\n6.8 Support through Related Languages\\n\\nBuilding on \u00a76.7, there is another way we can investigate the positive effect of closely related languages on performance: We can compare performance (again on SentRetr Bible) of continued pretraining on just one language (we refer to this model as Glot+1) vs. on all 511 languages represented in Glot500-c (i.e., Glot500-m). Table 9 presents results for six language-scripts selected from various language families and suggests that some languages do not receive support from related languages (top three). In that case, Glot+1 can fully concentrate on learning the isolated language and does better than Glot500-c. Other languages (bottom three) do receive support from related languages. For example, Southern Quechua (quh) seem to receive support in Glot500-m from closely related Cuzco Quechua (quz), resulting in Glot500-m outperforming Glot+1.\\n\\n7 Conclusion and Future Work\\n\\nWe collect and data-clean Glot500-c, a large corpus of hundreds of usually neglected tail (i.e., long-tail) languages and create Glot500-m, an LLM that is trained on Glot500-c and covers these languages. We evaluate Glot500-m on six tasks that allow us to evaluate almost all languages. We observe large improvements for both head and tail languages compared to XLM-R. Our analysis shows that no single factor fully explains the quality of the representation of a language in a multilingual model. Rather, a combination of factors is important, including corpus size, script, \u201chelp\u201d from related languages and the total capacity of the model.\\n\\nThis work is the first to create a language model on a dataset of several hundreds of gigabytes and to make it publicly available for such a large and diverse number of low-resource languages. In future research, we would like to train larger models to further investigate the effect of model size, distill highly multilingual models for resource-efficient deployment, explore alternatives to continued pretraining and use models for more tail language downstream tasks.\\n\\nLimitations\\n\\n(1) We did not perform any comprehensive hyperparameter search, which would have further consolidated our results. This decision was made due to the high cost of training multiple models. (2) Compared to current very large models, Glot500-m...\"}"}
{"id": "acl-2023-long-61", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is comparatively small. (3) Although we have tried to minimize the amount of noise in our data, some noise is still present.\\n\\nEthics Statement\\nThere are two issues worth mentioning in regards to this project. First, it was not feasible for us to thoroughly examine the content of the data for all languages, thus we cannot confirm the absence of discrimination based on factors such as race or sexuality. The data was solely utilized as a textual corpus, and the content should not be interpreted as an endorsement by our team. If the model is subsequently utilized for generation, it is possible that the training data may be reflected in the generated output. However, addressing potential biases within the data is an area for future research. Second, it is important to note that while the data sources utilized in this study do not explicitly prohibit the reuse of data for research purposes, some sources do have copyright statements indicating that such use is permissible while others do not. Additionally, certain sources prohibit the redistribution of data. As such, data from these sources is omitted from the published version of Glot2000-c.\\n\\nAcknowledgements\\nWe would like to thank Renhao Pei, Yihong Liu, Verena Blaschke, and the anonymous reviewers. This work was funded by the European Research Council (grants #740516 and #758969) and EU's Horizon Europe Research and Innovation Actions (UTTER, contract 101070631).\\n\\nReferences\\nSolomon Teferra Abate, Michael Melese, Martha Yi- firu Tachbelie, Million Mes}h}es}ha, Solomon Ati- nanf}u, Wondwossen Mulugeta, Yaregal Assabie, Hafte Abera, Binyam Ephrem, Tewodros Abebe, Wondim- agegn}hue Tsegaye, Amanuel Lemma, Tsegaye An- dargie, and Seifedin Shifaw. 2018. Parallel corpora for bi-lingual English-Ethiopian languages statistical machine translation. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3102\u20133111, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\nAhmed Abdelali, Hamdy Mubarak, Younes Samih, Sabit Hassan, and Kareem Darwish. 2021. QADI: Arabic dialect identification in the wild. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 1\u201310, Kyiv, Ukraine (Virtual). Association for Computational Linguistics.\\nKathrein Abu Kwaik, Motaz Saad, Stergios Chatzikyri- akidis, and Simon Dobnik. 2018. Shami: A corpus of Levantine Arabic dialects. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\nIfe Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Alcoba Inciarte. 2022. SERENGETI: Massively multilingual language models for Africa. arXiv preprint arXiv:2212.10785.\\nDavid Adelani, Jesujoba Alabi, Angela Fan, Julia Kreutzer, Xiaoyu Shen, Machel Reid, Dana Ruiter, Dietrich Klakow, Peter Nabende, Ernie Chang, Tajudeen Gwadabe, Freshia Sackey, Bonaventure F. P. Dossou, Chris Emezue, Colin Leong, Michael Beukman, Shamsuddeen Muhammad, Guyo Jarso, Oreen Yousuf, Andre Niyongabo Rubungo, Gilles Hacheme, Eric Peter Wairagala, Muhammad Umair Nasir, Benjamin Ajibade, Tunde Ajayi, Yvonne Gitau, Jade Abbott, Mohamed Ahmed, Millicent Ochieng, An- uoluwapo Aremu, Perez Ogayo, Jonathan Mukiibi, Fatoumata Ouoba Kabore, Godson Kalipe, Derguene Mbaye, Allahsera Auguste Tapo, Victoire Memdjokam Koagne, Edwin Munkoh-Buabeng, Valen- cia Wagner, Idris Abdulmumin, Ayodele Awokoya, Happy Buzaaba, Blessing Sibanda, Andiswa Bukula, and Sam Manthalu. 2022. A few thousand translations go a long way! leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3053\u20133070, Seattle, United States. Association for Computational Linguistics.\\nDavid Adelani, Dana Ruiter, Jesujoba Alabi, Damilola Adebonojo, Adesina Ayeni, Ayo- dele Esther Awokoya, and Cristina Espa\u00f1a-Bonet. 2021. The effect of domain and diacritics in Yoruba\u2013English neural machine translation. In Proceedings of Machine Translation Summit XVIII: Research Track, pages 61\u201375, Virtual. Association for Machine Translation in the Americas.\\nRodrigo Agerri, Xavier G\u00f3mez Guinovart, German Rigau, and Miguel Anxo Solla Portela. 2018. Developing new linguistic resources and tools for the Galician language. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\nJesujoba O. Alabi, David Ifeoluwa Adelani, Marius Mosbach, and Dietrich Klakow. 2022. Adapting pre-trained language models to African languages via multilingual adaptive fine-tuning. In Proceedings of the 29th International Conference on Computational Linguistics, pages 4336\u20134349, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\"}"}
{"id": "acl-2023-long-61", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Israa Alsarsour, Esraa Mohamed, Reem Suwaileh, and Tamer Elsayed. 2018. DART: A large dataset of dialectal Arabic tweets. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nAntonios Anastasopoulos, Alessandro Cattelan, Zi-Yi Dou, Marcello Federico, Christian Federmann, Dmitriy Genzel, Francisco Guzm\u00e1n, Junjie Hu, Macduff Hughes, Philipp Koehn, Rosie Lazar, Will Lewis, Graham Neubig, Mengmeng Niu, Alp \u00d6ktem, Eric Paquin, Grace Tang, and Sylwia Tur. 2020. TICO-19: the translation initiative for COVID-19. In Proceedings of the 1st Workshop on NLP for COVID-19 (Part 2) at EMNLP 2020, Online. Association for Computational Linguistics.\\n\\nAlan Ansell, Edoardo Ponti, Anna Korhonen, and Ivan Vuli\u0107. 2022. Composable sparse fine-tuning for cross-lingual transfer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1778\u20131796, Dublin, Ireland. Association for Computational Linguistics.\\n\\nMikel Artetxe and Holger Schwenk. 2019. Massively multilingual sentence embeddings for zero-shot cross-lingual transfer and beyond. Transactions of the Association for Computational Linguistics, 7:597\u2013610.\\n\\nNiyati Bafna. 2022. Empirical models for an indic language continuum.\\n\\nMarta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ram\u00edrez-S\u00e1nchez, Elsa Sarr\u00edas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555\u20134567, Online. Association for Computational Linguistics.\\n\\nMarta Ba\u00f1\u00f3n, Miquel Espl\u00e0-Gomis, Mikel L. Forcada, Cristian Garc\u00eda-Romero, Taja Kuzman, Nikola Ljubesic, Rik van Noord, Leopoldo Pla Sempere, Gema Ram\u00edrez-S\u00e1nchez, Peter Rupnik, V\u00edt Suchomel, Antonio Toral, Tobias van der Werff, and Jaume Zaragoza. 2022. Macocu: Massive collection and curation of monolingual and bilingual data: focus on under-resourced languages. In Proceedings of the 23rd Annual Conference of the European Association for Machine Translation, EAMT 2022, Ghent, Belgium, June 1-3, 2022, pages 301\u2013302. European Association for Machine Translation.\\n\\nAnkur Bapna, Isaac Caswell, Julia Kreutzer, Orhan Firat, Daan van Esch, Aditya Siddhant, Mengmeng Niu, Pallavi Baljekar, Xavier Garcia, Wolfgang Macherey, et al. 2022. Building machine translation systems for the next thousand languages. arXiv preprint arXiv:2205.03983.\"}"}
{"id": "acl-2023-long-61", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Platen, Pierre Cornette, Pierre Fran\u00e7ois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramanian, Aur\u00e9lie N\u00e9v\u00e9ol, Charles Lovering, Dan Garrette, Deepak Tunuguntla, Ehud Reiter, Ekaterina Taktscheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Jordan Clive, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, Shani Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zden\u011bk Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ana Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Ajibade, Bharat Saxena, Carlos Mu\u00f1oz Ferrandis, Danish Contractor, David Lansky, Davis David, Douwe Kiela, Duong A. Nguyen, Edward Tan, Emi Baylor, Ezinwanne Ozoani, Fatima Mirza, Frankline Ononiwu, Habib Rezanejad, Hessie Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jesse Passmore, Josh Seltzer, Julio Boris Sanz, Livia Dutra, Mairon Samagaio, Maraim Elbadri, Margot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, Muhammed Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nour Fahmy, Olanrewaju Samuel, Ran An, Rasmus Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas Wang, Sourav Roy, Sylvain Viguier, Thanh Le, Tobi Oyebade, Trieu Le, Yoyo Yang, Zach Nguyen, Abhinav Ramesh Kashyap, Alfredo Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Singh, Benjamin Beilharz, Bo Wang, Caio Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Cl\u00e9mentine Fourrier, Daniel Le\u00f3n Peri\u00f1\u00e1n, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Imane Bello, Ishani Dash, Jihyun Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthik Rangasai Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahnde Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria AC Castillo, Marianna Nezhurina, Mario S\u00e4nger, Matthias Samwald, Michael Cullan, Michael Weinberg, Michiel De Wolf, Mina Mihalj-cic, Minna Liu, Moritz Freidank, Myungsun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patrick Haller, Ramya Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Bharati, Tanmay Laud, Th\u00e9o Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yash Shailesh Bajaj, Yash Venkatraman, Yifan Xu, Yingxin Xu, Yu Xu, Zhe Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. BLOOM: a 176b-parameter open-access multilingual language model.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nJos\u00e9 Camacho-Collados, Claudio Delli Bovi, Alessandro Raganato, and Roberto Navigli. 2016. A large-scale multilingual disambiguation of glosses. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 1701\u20131708, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nZewen Chi, Li Dong, Furu Wei, Nan Yang, Saksham Singhal, Wenhui Wang, Xia Song, Xian-Ling Mao, Heyan Huang, and Ming Zhou. 2021. InfoXLM: An information-theoretic framework for cross-lingual language model pre-training. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3576\u20133588, Online. Association for Computational Linguistics.\\n\\nZewen Chi, Shaohan Huang, Li Dong, Shuming Ma, Bo Zheng, Saksham Singhal, Payal Bajaj, Xia Song, Xian-Ling Mao, Heyan Huang, and Furu Wei. 2022. XLM-E: Cross-lingual language model pre-training via ELECTRA. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6170\u20136182, Dublin, Ireland. Association for Computational Linguistics.\\n\\nRochelle Choenni and Ekaterina Shutova. 2022. Investigating language relationships in multilingual sentence encoder through the lens of linguistic typology. Computational Linguistics, 48(3):635\u2013672.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and Jason Riesa. 2020. Improving multilingual models with language-clustered vocabularies. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4536\u20134546, Online. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning with procedurally generated text.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\"}"}
{"id": "acl-2023-long-61", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-61", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland. Association for Computational Linguistics.\\n\\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham Neubig, Orhan Firat, and Melvin Johnson. 2020. XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 4411\u20134421. PMLR.\\n\\nAyyoob Imani Googhari, Silvia Severini, Masoud Jalili Sabet, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2022. Graph-based multilingual label propagation for low-resource part-of-speech tagging. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1577\u20131589, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nMasoud Jalili Sabet, Philipp Dufter, Fran\u00e7ois Yvon, and Hinrich Sch\u00fctze. 2020. SimAlign: High quality word alignments without parallel training data using static and contextualized embeddings. In Findings of the Association for Computational Linguistics: EMNLP2020, pages 1627\u20131643, Online. Association for Computational Linguistics.\\n\\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. 2020. The state and fate of linguistic diversity and inclusion in the NLP world. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6282\u20136293, Online. Association for Computational Linguistics.\\n\\nDivyanshu Kakwani, Anoop Kunchukuttan, Satish Golla, Gokul N.C., Avik Bhattacharyya, Mitesh M. Khapra, and Pratyush Kumar. 2020. IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP2020, pages 4948\u20134961, Online. Association for Computational Linguistics.\\n\\nFajri Koto and Ikhwan Koto. 2020. Towards computational linguistics in Minangkabau language: Studies on sentiment analysis and machine translation. In Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation, pages 138\u2013148, Hanoi, Vietnam. Association for Computational Linguistics.\\n\\nJulia Kreutzer, Isaac Caswell, Lisa Wang, Ahsan Wahab, Daan van Esch, Nasanbayar Ulzii-Orshikh, Allahsera Tapo, Nishant Subramani, Artem Sokolov, Clayton Sikasote, Monang Setyawan, Supheakmungkol Sarin, Sokhar Samb, Beno\u00eet Sagot, Clara Rivera, Annette Rios, Isabel Papadimitriou, Salomey Osei, Pedro Ortiz Suarez, Iroro Orife, Kelechi Ogueji, Andr\u00e9 Niyongabo Rubungo, Toan Q. Nguyen, Matthias M\u00fcller, Andr\u00e9 M\u00fcller, Shamsuddeen Hassan Muhammad, Nanda Muhammad, Ayanda Mnyakeni, Jamshidbek Mirzakhalov, Tapiwanashe Matangira, Colin Leong, Nze Lawson, Sneha Kudugunta, Yacine Jernite, Mathias Jenny, Orhan Firat, Bonaventure F. P. Dossou, Sakhile Dlamini, Nisansa de Silva, Sakine \u00c7abuk Ball\u0131, Stella Biderman, Alessia Battisti, Ahmed Baruwa, ankur Bapna, Pallavi Baljekar, Israel Abebe Azime, Ayodele Awokoya, Duygu Ataman, Orevaoghene Ahia, Oghenefego Ahia, Sweta Agrawal, and Mofetoluwa Adeyemi. 2022. Quality at a glance: An audit of web-crawled multilingual datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372.\\n\\nTaku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375, Melbourne, Australia. Association for Computational Linguistics.\\n\\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAnoop Kunchukuttan, Pratik Mehta, and Pushpak Bhattacharyya. 2018. The IIT Bombay English-Hindi parallel corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nHugo Lauren\u00e7on, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanovadel Moral, Teven Le Scao, Leandro Von Werra, Chenghao Mou, Eduuardo Gonz\u00e1lez Ponferrada, Huu Nguyen, et al. 2022. The BigScience ROOTS Corpus: A 1.6 TB Composite Multilingual Dataset. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\\n\\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli\u0107, and Goran Glava\u0161. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual Transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483\u20134499, Online. Association for Computational Linguistics.\\n\\nColin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, and Daniel Whitenack. 2022. Bloom library: Multimodal datasets in 300+ languages for a variety of downstream tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7\u201311, 2022, pages 8608\u20138621. Association for Computational Linguistics.\"}"}
{"id": "acl-2023-long-61", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-61", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-61", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|-----------------|---------|---------|-----------|\\n| swc_Latn        | 39.2    | 22.5    | 13.2      |\\n| top_Latn        | 589.2   | 89.6    | 23.5      |\\n| hin_Latn        |         |         |           |\\n| deu_Latn        | 4.4     | 3.6     | 10.2      |\\n| bin_Latn        | 278.1   | 169.8   | 13.3      |\\n| eng_Latn        | 5.7     | 4.0     | 7.5       |\\n| caq_Latn        | 185.9   | 129     | 21.6      |\\n| chw_Latn        | 778.9   | 645.8   | 33.9      |\\n| hus_Latn        | 134.6   | 68.2    | 5.3       |\\n| ceb_Latn        | 63.1    | 53.1    | 2.1       |\\n| hyw_Cyrl        | 268.5   | 233.5   | 6.3       |\\n| urh_Latn        | 236.8   | 211.5   | 11.4      |\\n| nia_Latn        | 280.3   | 85.5    | 7.5       |\\n| kor_Hang        | 7.2     | 2.6     | 11        |\\n| mkd_Cyrl        | 4.3     | 3.1     | 6.2       |\\n| urd_Arab        | 8.3     | 5.3     | 8.7       |\\n| btx_Latn        | 463     | 163.1   | 19.3      |\\n| wbm_Latn        | 58.9    | 47.3    | 13.6      |\\n| niu_Latn        | 600.1   | 437.5   | 10.1      |\\n| srn_Latn        | 609.3   | 137.2   | 12.6      |\\n| kwn_Latn        | 1053.6  | 753.2   | 32.0      |\\n| mrw_Latn        | 320.8   | 174.9   | 7.6       |\\n| llb_Latn        | 555.6   | 589.8   | 41.1      |\\n| guc_Latn        | 432.6   | 117.8   | 9.4       |\\n| bul_Cyrl        | 3.9     | 3.6     | 6.8       |\\n| cbk_Latn        | 129.5   | 60.4    | 11.6      |\\n| quc_Latn        | 270.7   | 83.9    | 5.6       |\\n| pau_Latn        | 333.7   | 147.3   | 7.2       |\\n| bcl_Latn        | 270     | 60.1    | 12.5      |\\n| nds_Latn        | 112.5   | 161.1   | 7.4       |\\n| tha_Thai        | 10.8    | 2.9     | 14.6      |\\n| csy_Latn        | 198.3   | 152.5   | 21.7      |\\n| ind_Latn        | 8.5     | 5.4     | 17.1      |\\n| ilo_Latn        | 786.7   | 184.4   | 13.8      |\\n| ctd_Latn        | 249.2   | 166.1   | 11.6      |\\n| nde_Latn        | 56.7    | 21.5    | 12.1      |\\n| kss_Latn        | 90.4    | 13.2    | 11.2      |\\n| zai_Latn        | 719.4   | 212.5   | 10.4      |\\n| mrw_Latn        | 320.8   | 174.9   | 7.6       |\\n| guw_Latn        | 267.7   | 65.5    | 6.9       |\\n| kab_Latn        | 744.5   | 203.5   | 24.3      |\\n| por_Latn        | 5.1     | 3.9     | 9.3       |\\n| kdr_Cyrl        | 175.7   | 94.4    | 9.1       |\\n| gom_Deva        | 82.8    | 48.4    | 9.0       |\\n| jpn_Jpan        | 7.9     | 3.9     | 10        |\\n| dln_Latn        | 238.8   | 207.8   | 7.5       |\\n| ukr_Cyrl        | 3.1     | 2.9     | 5.9       |\\n| spa_Latn        | 4.6     | 3.5     | 7.8       |\\n| war_Latn        | 200.9   | 110.7   | 2.3       |\\n| ast_Latn        | 27.5    | 18.6    | 4.8       |\\n| knv_Latn        | 129     | 78.3    | 5.8       |\\n| tca_Latn        | 70.4    | 49      | 6.0       |\\n| lvs_Latn        | 4.8     | 2.7     | 5.7       |\\n| agw_Latn        | 150.1   | 73.4    | 16.3      |\\n| iku_Cans        | 2.2     | 1.9     | 5.8       |\\n| rmn_Cyrl        | 624.3   | 513.1   | 8.7       |\\n| ige_Latn        | 181.1   | 105.2   | 11.9      |\\n| bjn_Latn        | 41.3    | 17.6    | 11.4      |\\n| kir_Cyrl        | 7.7     | 2.9     | 11.9      |\\n| dua_Latn        | 232.8   | 152.2   | 19.1      |\\n| ngu_Latn        | 918     | 110.9   | 13.4      |\\n| pfl_Latn        | 152     | 101.3   | 11.3      |\\n| ogo_Latn        | 131.3   | 129.7   | 31.1      |\\n| kmr_Latn        | 68      | 4.6     | 10.6      |\\n| bqc_Latn        | 102.7   | 71.1    | 26.5      |\\n| bas_Latn        | 410.4   | 437.7   | 16.7      |\\n| tgl_Latn        | 7.9     | 4.4     | 8.9       |\\n| yid_Hebr        | 7.6     | 4.8     | 5.1       |\\n| bpy_Beng        | 20      | 21.4    | 2.9       |\\n| eus_Latn        | 10.7    | 6.2     | 37.3      |\\n| fil_Latn        | 9.2     | 2.3     | 9.9       |\\n| lfn_Latn        | 60.4    | 51      | 6.9       |\\n| hra_Latn        | 212.1   | 177.7   | 54.3      |\\n| nap_Latn        | 81.7    | 39.6    | 10.5      |\\n| ton_Latn        | 116     | 65.2    | 2.8       |\\n| lue_Latn        | 839.2   | 627.4   | 19.8      |\\n| heb_Hebr        | 6.7     | 4.9     | 13.5      |\\n| lim_Latn        | 66.8    | 43.5    | 11.4      |\\n| pol_Latn        | 4.5     | 2.7     | 10.6      |\\n| sba_Latn        | 75.7    | 81.8    | 6.0       |\\n| lav_Latn        | 4.2     | 2.2     | 6.6       |\\n|ifa_Latn         | 371.9   | 266.1   | 6.0       |\\n| bih_Deva        | 27.6    | 16.1    | 5.0       |\\n| lat_Latn        | 15.3    | 3.7     | 24.5      |\\n| ami_Latn        | 1070.7  | 710.2   | 29.2      |\\n| gym_Latn        | 509.6   | 66.3    | 17.0      |\\n| div_Thaa        | 1.6     | 1.5     | 3.5       |\\n| gil_Latn        | 763.5   | 161.3   | 15.7      |\\n| ish_Latn        | 144.9   | 134     | 11.6      |\\n| min_Latn        | 105     | 39.7    | 3.9       |\\n| djk_Latn        | 360.4   | 93.4    | 13.4      |\\n| zea_Latn        | 69.6    | 27.5    | 8.7       |\\n| ctu_Latn        | 177.4   | 37.9    | 4.5       |\\n| new_Deva        | 36.1    | 29.8    | 4.5       |\\n| aln_Latn        | 3.9     | 2.3     | 12.7      |\\n| bang_Latn       | 74.5    | 23.7    | 46.8      |\\n| gcr_Latn        | 352.9   | 314.7   | 7.5       |\\n| dhv_Latn        | 509     | 435.8   | 11.8      |\\n| wol_Latn        | 236.4   | 158.3   | 32.0      |\\n| kal_Latn        | 377.2   | 370.9   | 8.3       |\\n| lua_Latn        | 706     | 784.5   | 21.7      |\\n| alt_Cyrl        | 140.7   | 50.9    | 9.3       |\\n| dan_Latn        | 6       | 3.6     | 13.1      |\\n| rmy_Cyrl        | 488.1   | 389.3   | 9.3       |\\n| kri_Latn        | 87.6    | 35.8    | 8.6       |\\n| tah_Latn        | 363     | 330.9   | 4.8       |\\n| zpa_Latn        | 476.1   | 550.1   | 13.6      |\\n| kom_Cyrl        | 93.4    | 57      | 4.9       |\\n| kik_Latn        | 205.8   | 55.5    | 12.1      |\\n| gom_Latn        | 405.7   | 282.9   | 27.9      |\\n| sah_Cyrl        | 99.9    | 91.1    | 4.5       |\\n| vmw_Latn        | 828.8   | 434.8   | 17.8      |\\n| dtp_Latn        | 166.4   | 78.7    | 5.5       |\\n| mzh_Latn        | 132.8   | 133.4   | 9.6       |\\n| eml_Latn        | 283.4   | 144.9   | 6.6       |\\n| fra_Latn        | 4.1     | 2.8     | 6.9       |\\n| sna_Latn        | 316.6   | 331.1   | 16.4      |\\n| sco_Latn        | 28.1    | 15.5    | 9.8       |\\n| cat_Latn        | 4.1     | 2.2     | 7.3       |\\n| bzj_Latn        | 264.7   | 75.8    | 10.9      |\\n| kac_Latn        | 189.9   | 76.3    | 17.9      |\\n| xmf_Geor        | 71.2    | 72.3    | 3.8       |\\n| nld_Latn        | 5.7     | 4.5     | 12        |\\n| ttj_Latn        | 865.2   | 509.5   | 15.5      |\\n| ixl_Latn        | 53      | 29.6    | 4.2       |\\n| gug_Latn        | 626.9   | 141.6   | 8.4       |\\n| lun_Latn        | 720.1   | 565.6   | 31.9      |\\n| ckb_Arab        | 72.2    | 80.6    | 6.0       |\\n| yue_Hani        | 17.8    | 10.6    | 10.8      |\\n| sot_Latn        | 269.1   | 122.4   | 8.1       |\\n| ahk_Latn        | 44.8    | 9.1     | 2.1       |\\n| fry_Latn        | 16.1    | 15.4    | 17.2      |\\n| mau_Latn        | 199.7   | 13.6    | 8.4       |\\n| sag_Latn        | 491.4   | 68.7    | 11.1      |\\n| jbo_Latn        | 132.3   | 187.1   | 9.0       |\\n| yan_Latn        | 134.4   | 108.4   | 31.4      |\\n| qug_Latn        | 505     | 135.2   | 13.7      |\\n| iba_Latn        | 529.3   | 87      | 16.6      |\\n| id"}
{"id": "acl-2023-long-61", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language  | xLM-R_B  | xLM-R_L  | Glot500-m |\\n|-----------|---------|---------|-----------|\\n| bts_Latn  | 205.7   | 204.5   | 8.8       |\\n| tsn_Latn  | 264.7   | 137.8   | 12.5      |\\n| orm_Latn  | 23.4    | 8.6     |           |\\n| gla_Latn  | 11.5    | 12.7    | 7.2       |\\n| pon_Latn  | 928.4   | 181.9   | 19.2      |\\n| luo_Latn  | 699.4   | 258.5   | 85.1      |\\n| kat_Latn  | 36.4    | 24.8    | 18.3      |\\n| nmf_Latn  | 297.6   | 310.6   | 44.9      |\\n| pcm_Latn  | 38.3    | 169.6   | 3.6       |\\n| uig_Latn  | 188.8   | 173.9   | 15.2      |\\n| ajg_Latn  | 147.1   | 149.5   | 22.6      |\\n| nnb_Latn  | 364.1   | 95      | 28.6      |\\n| kat_Geor  | 6       | 3.9     | 6.4       |\\n| tir_Ethi  | 28.3    | 15.7    | 4.4       |\\n| kaz_Cyrl  |         |         |           |\\n| mlg_Latn  | 10.9    | 4.4     | 7.6       |\\n| bhw_Latn  | 411.2   | 126.2   | 21.6      |\\n| dzo_Tibt  | 8.5     | 3.3     | 5.7       |\\n| mhr_Cyrl  | 122.9   | 168.4   | 5.8       |\\n| sun_Latn  | 23.6    |         |           |\\n| tuk_Latn  | 456.7   | 197.8   | 5.8       |\\n| swe_Latn  | 4.8     |         |           |\\n| vec_Latn  | 40.6    | 21.1    | 9.2       |\\n| vls_Latn  | 97.7    | 39.6    | 9.7       |\\n| scn_Latn  | 117     | 64.9    | 7.8       |\\n| ayr_Latn  | 261.1   | 237.6   | 27.7      |\\n| hyw_Armn  | 15.8    | 9.1     | 4.3       |\\n| udm_Cyrl  | 356.7   | 224.9   | 6.7       |\\n| oke_Latn  | 209.2   | 220.1   | 13.0      |\\n| que_Latn  | 447.9   | 536.1   | 11.9      |\\n| ifb_Latn  | 246.3   | 177.9   | 5.1       |\\n| kur_Latn  | 14.2    |         |           |\\n| snd_Arab  | 13.2    |         |           |\\n| naq_Latn  | 136.8   | 60.2    | 15.7      |\\n| mgh_Latn  | 680     | 272.8   | 23.7      |\\n| giz_Latn  | 81.9    | 82.9    | 37.7      |\\n| zlm_Latn  | 5.6     |         |           |\\n| tgk_Cyrl  | 181.3   | 153     | 4.5       |\\n| ita_Latn  | 4.5     |         |           |\\n| hrx_Latn  | 478.1   | 679.1   | 14.9      |\\n| sop_Latn  | 607.5   | 228.2   | 29.5      |\\n| qub_Latn  | 283.2   | 312.7   | 9.4       |\\n| lzh_Hani  | 70      | 58      | 21.8      |\\n| mos_Latn  | 272.6   | 118.3   | 13.2      |\\n| nav_Latn  | 228.5   | 126.5   | 5.2       |\\n| pap_Latn  | 674.4   | 149.3   | 18.1      |\\n| rap_Latn  | 36.1    | 31.1    | 2.8       |\\n| kqn_Latn  | 825.9   | 686.6   | 17.5      |\\n| cfm_Latn  | 235.1   | 155     | 14.0      |\\n| prk_Latn  | 69.4    | 45.9    | 7.1       |\\n| toh_Latn  | 758.3   | 216.6   | 19.6      |\\n| chv_Cyrl  | 122.5   | 73.8    | 5.4       |\\n| uzb_Cyrl  | 236.2   | 138.4   | 4.9       |\\n| mah_Latn  | 314.7   | 81.8    | 17.3      |\\n| tdt_Latn  | 641.9   | 78.6    | 9.7       |\\n| tog_Latn  | 821.1   | 777.7   | 13.4      |\\n| wes_Latn  | 144.6   | 103.9   | 14.3      |\\n| pan_Guru  | 4.4     |         |           |\\n| mal_Mlym  | 5       |         |           |\\n| nob_Latn  | 6.8     |         |           |\\n| pms_Latn  | 83.6    | 46.2    | 3.6       |\\n| nky_Latn  | 1182.6  | 914.2   | 16.5      |\\n| ext_Latn  | 68.3    | 38.2    | 8.1       |\\n| roh_Latn  | 243.5   | 170     | 7.0       |\\n| quy_Latn  | 949.7   | 320.2   | 14.5      |\\n| lam_Latn  | 233.7   | 160.8   | 21.6      |\\n| prs_Arab  | 6.8     |         |           |\\n| mwm_Latn  | 44.8    | 53.1    | 7.1       |\\n| tuk_Cyrl  | 277.4   | 86.3    | 6.7       |\\n| mcn_Latn  | 120.7   | 129.7   | 43.6      |\\n| kpg_Latn  | 165.9   | 122.6   | 15.1      |\\n| srm_Latn  | 257.5   | 74.5    | 12.3      |\\n| hau_Arab  | 5.3     |         |           |\\n| gsw_Latn  | 288.2   | 181.2   | 22.3      |\\n| gle_Latn  | 10.5    |         |           |\\n| ksd_Latn  | 150     | 154.9   | 7.7       |\\n| fat_Latn  | 192.3   | 149     | 17.6      |\\n| cab_Latn  | 1216.7  | 155.6   | 15.4      |\\n| zsm_Latn  | 12.2    |         |           |\\n| ldi_Latn  | 394.8   | 107.1   | 38.2      |\\n| mps_Latn  | 75.2    | 55.2    | 17.4      |\\n| hui_Latn  | 209.9   | 177     | 10.0      |\\n| kos_Latn  | 470.7   | 485.7   | 27.0      |\\n| pnb_Arab  | 51.8    | 30.8    | 7.1       |\\n| cym_Latn  | 8.2     |         |           |\\n| acr_Latn  | 155.7   | 90.7    | 5.8       |\\n| swa_Latn  | 11.4    |         |           |\\n| mri_Latn  | 63      | 59.5    | 8.7       |\\n| hnj_Latn  | 88.3    | 92.5    | 11.3      |\\n| bak_Latn  | 347.1   | 211     | 7.5       |\\n| frr_Latn  | 117.6   | 101     | 9.5       |\\n| haw_Latn  | 63.5    | 66.7    | 7.4       |\\n| zho_Hani  | 20.7    |         |           |\\n| mck_Latn  | 369.3   | 164.8   | 24.7      |\\n| tpi_Latn  | 891.8   | 67.8    | 8.8       |\\n| nno_Latn  |         | 9.9     | 12.7      |\\n| pes_Arab  | 5.5     |         |           |\\n| mco_Latn  |         |         |           |\\n| lit_Latn  | 4.4     |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| acr_Latn  | 155.7   |         |           |\\n| swa_Latn  | 11.4    |         |           |\\n| mri_Latn  | 63      | 59.5    | 8.7       |\\n| hnj_Latn  | 88.3    | 92.5    | 11.3      |\\n| bak_Latn  | 347.1   | 211     | 7.5       |\\n| frr_Latn  | 117.6   | 101     | 9.5       |\\n| haw_Latn  | 63.5    | 66.7    | 7.4       |\\n| zho_Hani  | 20.7    |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| tpi_Latn  | 891.8   | 67.8    | 8.8       |\\n| nno_Latn  |         | 9.9     | 12.7      |\\n| pes_Arab  | 5.5     |         |           |\\n| mco_Latn  |         |         |           |\\n| lit_Latn  | 4.4     |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| acr_Latn  | 155.7   |         |           |\\n| swa_Latn  | 11.4    |         |           |\\n| mri_Latn  | 63      | 59.5    | 8.7       |\\n| hnj_Latn  | 88.3    | 92.5    | 11.3      |\\n| bak_Latn  | 347.1   | 211     | 7.5       |\\n| frr_Latn  | 117.6   | 101     | 9.5       |\\n| haw_Latn  | 63.5    | 66.7    | 7.4       |\\n| zho_Hani  | 20.7    |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| tpi_Latn  | 891.8   | 67.8    | 8.8       |\\n| nno_Latn  |         | 9.9     | 12.7      |\\n| pes_Arab  | 5.5     |         |           |\\n| mco_Latn  |         |         |           |\\n| lit_Latn  | 4.4     |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| acr_Latn  | 155.7   |         |           |\\n| swa_Latn  | 11.4    |         |           |\\n| mri_Latn  | 63      | 59.5    | 8.7       |\\n| hnj_Latn  | 88.3    | 92.5    | 11.3      |\\n| bak_Latn  | 347.1   | 211     | 7.5       |\\n| frr_Latn  | 117.6   | 101     | 9.5       |\\n| haw_Latn  | 63.5    | 66.7    | 7.4       |\\n| zho_Hani  | 20.7    |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| tpi_Latn  | 891.8   | 67.8    | 8.8       |\\n| nno_Latn  |         | 9.9     | 12.7      |\\n| pes_Arab  | 5.5     |         |           |\\n| mco_Latn  |         |         |           |\\n| lit_Latn  | 4.4     |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| acr_Latn  | 155.7   |         |           |\\n| swa_Latn  | 11.4    |         |           |\\n| mri_Latn  | 63      | 59.5    | 8.7       |\\n| hnj_Latn  | 88.3    | 92.5    | 11.3      |\\n| bak_Latn  | 347.1   | 211     | 7.5       |\\n| frr_Latn  | 117.6   | 101     | 9.5       |\\n| haw_Latn  | 63.5    | 66.7    | 7.4       |\\n| zho_Hani  | 20.7    |         |           |\\n| mck_Latn  | 369.3   |         |           |\\n| tpi_Latn  | 891.8   | 6"}
{"id": "acl-2023-long-61", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACL 2023 Responsible NLP Checklist\\n\\nA For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\nSince our work deals with millions of sentences in hundreds of languages, it was impossible for us to check the content. We leave it as a future work.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC \u25a1 Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-61", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nFor continued pretraining, it is a single run due to computational resource limitation. For downstream task evaluation, it is multiple runs across 5 seeds.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nLeft blank.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nNo response.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNo response.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNo response.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNo response.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNo response.\"}"}
{"id": "acl-2023-long-61", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\( S = c_{1}, c_{2}, \\\\cdots, c_{T} \\\\)\\n\\nPerplexity (PP) of \\\\( S \\\\) given an n-gram character level language model \\\\( M \\\\) is computed as follows:\\n\\n\\\\[\\nPP(S, M) = \\\\frac{T}{\\\\sqrt{\\\\prod_{t=1}^{T} P(c_{t} | c_{t-1})}}\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\nP(c_{t} | c_{t-1}) = \\\\frac{C(c_{t-1}c_{t})}{C(c_{t-1})}\\n\\\\]\\n\\nGiven the definition of perplexity, we can determine how well a trained language model on language \\\\( L_{1} \\\\) predicts the test text of language \\\\( L_{2} \\\\) and vice-versa.\\n\\nThe divergence between two languages is computed with the maximum of the perplexity values in both directions. Two reasons lead to the use of max:\\n\\n1. A symmetrical divergence is required.\\n2. Languages differ in their complexity, so one direction of computing perplexity may result in a much lower perplexity than another. Thus, comparing perplexity results becomes difficult.\\n\\nAs an example, the Kuanua language (ksd_Latn) has short words and a simple structure, which results in 3-gram models getting lower perplexity on its text compared to other languages. The lower the perplexity the smaller the divergence between languages.\\n\\nThe divergence (\\\\( D \\\\)) between language \\\\( L_{i} \\\\) and \\\\( L_{j} \\\\) with trained language models of \\\\( M_{L_{i}} \\\\) and test texts of \\\\( S_{L_{j}} \\\\), where \\\\( L_{j} \\\\) is the corresponding language, computed as follows:\\n\\n\\\\[\\nD_{L_{i}, L_{j}} = \\\\max (PP(S_{L_{i}}, M_{L_{j}}), PP(S_{L_{j}}, M_{L_{i}}))\\n\\\\]\\n\\n**Runs and Data.**\\n\\nThe data used to train and test the character level n-gram models is the same data used for the training and testing of the Glot500-m. The training of the models was limited to 100,000 sentences per language-script. We use KenLM library (Heafield, 2011) to build n-gram models. This library uses an interpolated modified Kneser-Ney smoothing for estimating the unseen n-grams.\\n\\nOur evaluation has been performed over 7 n-gram models (\\\\( 3 \\\\leq n \\\\leq 9 \\\\)).\\n\\n**Baseline and Evaluation.**\\n\\nLanguage family trees were used as a baseline for evaluating the divergence measures of the proposed approach. We obtained language family tree data from Ethnologue online version (Eberhard et al., 2022). For each language, the family tree follows the general order from largest typological language family group to smallest. There is only one family tree for each language in the baseline data. Nodes in the family tree represent typological language family groups. Each node only has one parent, so if a node is common in the family tree of two languages, its parent is also common. We evaluate our perplexity method on the following binary classification task: Do the majority of a language \\\\( L_{z} \\\\)'s \\\\( k \\\\) nearest neighbors belong to the same typological language family group as \\\\( L_{z} \\\\)?\\n\\nAssuming languages \\\\( L_{i} \\\\) and \\\\( L_{j} \\\\), with the following family trees:\\n\\n- \\\\( T_{L_{i}}: 1 \\\\rightarrow 2 \\\\rightarrow 3 \\\\rightarrow 4 \\\\rightarrow 5 \\\\rightarrow 6 \\\\)\\n- \\\\( T_{L_{j}}: 1 \\\\rightarrow 2 \\\\rightarrow 7 \\\\rightarrow 8 \\\\)\\n\\nThese 2 languages belong to the same typological family group with family tree levels of \\\\( l \\\\in \\\\{1, 2\\\\} \\\\), but not with family tree levels of \\\\( l = 3 \\\\) and higher.\\n\\n**Result.**\\n\\nWhen it comes to language families, the majority of studies only refer to the largest typological language family group (level \\\\( l = 1 \\\\)). Here, we also assess our methodology for other levels. The results of classification accuracy for 3-gram model, \\\\( k \\\\in \\\\{1, 3, 7, 13, 21\\\\} \\\\) and \\\\( l \\\\in \\\\{1, 2, 3, \\\\text{max}\\\\} \\\\) are shown in Table 10. In cases where the maximum level of a tree is less than the \\\\( l \\\\) parameter, the maximum level for that language is used. Languages without a family or no other family member in our data are excluded. We only report the 3-gram model results as it gets the best results in most configurations among other n-gram models. With increasing \\\\( l \\\\), the accuracy decreases, since more languages fall outside the same typological family. As \\\\( k \\\\) increases, the accuracy decreases, because languages with faraway neighbors are being included but the number of languages in the language typological group family will remain the same. There are times when languages have a lot of loan words from other languages because of geological proximity or historical reasons (e.g., colonization), which make them similar to the languages they borrowed words from in our method. However, they are different when it comes to their typological families and our method fails in these cases. Aymara (Macrolanguage: aym_Latn) and Quechua (Macrolanguage: que_Latn), for example, had a great deal of contact and influence on each other, but they do not belong to the same typological group. As well, some of the typological families are not that large, which makes our results worse when \\\\( k \\\\) increases. This is...\"}"}
{"id": "acl-2023-long-61", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the case, for instance, of the Tarascan typological\\nfamily which only has two members.\\n\\nTable 10: Detecting the typological relatedness of lan-\\nguage with n-gram divergence: (Eq. 3);\\n\\n| \ud835\udc59      | \ud835\udc58        | accuracy (%) |\\n|--------|----------|--------------|\\n| 3-gram | 1        | 84.45        |\\n| 3-gram | 3        | 75.47        |\\n| 3-gram | 7        | 69.08        |\\n| 3-gram | 13       | 62.75        |\\n| 3-gram | 21       | 55.33        |\\n| 3-gram | 2         | 79.75        |\\n| 3-gram | 3         | 67.63        |\\n| 3-gram | 7         | 59.49        |\\n| 3-gram | 13        | 51.36        |\\n| 3-gram | 21        | 42.68        |\\n| 3-gram | 3         | 75.05        |\\n| 3-gram | 3         | 60.22        |\\n| 3-gram | 7         | 49.55        |\\n| 3-gram | 13        | 38.34        |\\n| 3-gram | 21        | 29.84        |\\n| 3-gram | max       | 59.31        |\\n| 3-gram | max       | 36.89        |\\n| 3-gram | max       | 18.81        |\\n| 3-gram | max       | 6.87         |\\n| 3-gram | max       | 2.89         |\\n\\nB Languages\\n\\nThe list of languages used to train Glot500-m with\\nthe amount of available data for each language is\\navailable in Tables 11, 12 and 13.\\n\\nOn Macrolanguages\\n\\nThe presence of language\\ncodes that are supersets of other language codes\\nwithin datasets is not uncommon (Kreutzer et al.,\\n2022). This issue becomes more prevalent in ex-\\ntensive collections. Within the ISO 639-3 standard,\\nthese languages are referred to as macrolanguages.\\n\\nWhen confronted with macrolanguages, if it is not\\nfeasible to ascertain the specific individual language\\ncontained within a dataset, the macrolanguage code\\nis retained. Consequently, it is possible that in\\nGlot2000-c and Glot500-c both the corpora for the\\nmacrolanguage and its individual languages have\\nbeen included.\\n\\nC List of data sources\\n\\nThe datasets and repositories used in this project\\ninvolve: AI4Bharat, AIFORTHAI-LotusCorpus,\\nAdd (El-Haj et al., 2018), AfriBERTa (Ogueji\\net al., 2021b), AfroMAFT (Adelani et al., 2022;\\nXue et al., 2021), Anuvaad,\\nAraBench (Sajjad\\net al., 2020), AUTSHUMATO,\\nBloom (Leong\\net al., 2022), CC100 (Conneau et al., 2020;\\nWenzek et al., 2020a), CCNet (Wenzek et al.,\\n2020b), CMU_Haitian_Creole,\\nCORP.NCHLT,\\nClarin,\\nDART (Alsarsour et al., 2018), Earth-\\nlings (Dunn, 2020), FFR,\\nFlores200 (Costa-juss\u00e0\\net al., 2022), GiossaMedia (G\u00f3ngora et al., 2022,\\n2021), Glosses (Camacho-Collados et al., 2016),\\nHabibi (El-Haj, 2020), HinDialect (Bafna, 2022),\\nHornMT,\\nIITB (Kunchukuttan et al., 2018), IndicNLP (Nakazawa et al., 2021), Indiccorp (Kak-\\nwaniet al., 2020), isiZulu,\\nJParaCrawl (Morishita\\net al., 2020), KinyaSMT,\\nLeipzigData (Goldhahn\\net al., 2012), Lindat,\\nLingala_Song_Lyrics,\\nLyrics,\\nMC4 (Raffel et al., 2020), MTData\\n(Gowda et al., 2021), MaCoCu (Ba\u00f1\u00f3n et al.,\\n2022), Makerere MT Corpus,\\nMasakhane com-\\nmunity,\\nMburisano_Covid,\\nMenyo20K (Ade-\\nlani et al., 2021), Minangkabau corpora (Koto\\nand Koto, 2020), MoT (Palen-Michel et al.,\\n2022), NLLB_seed (Costa-juss\u00e0 et al., 2022),\\nNart/abkhaz,\\nOPUS (Tiedemann, 2012), OS-\\nCAR (Su\u00e1rez et al., 2019), ParaCrawl (Ba\u00f1\u00f3n\\net al., 2020), Parallel Corpora for Ethiopian Lan-\\n5\"}"}
{"id": "acl-2023-long-61", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | Sent | Family Head |\\n|----------------|------|-------------|\\n| hbs_Latn       | 63411156 | indo1319   |\\n| vec_Latn       | 514240  | indo1319   |\\n| swh_Latn       | 95776   | atla1278   |\\n| mal_Mlym       | 48098273| drav1251   |\\n| jpn_Jpan       | 510722  | japo1237   |\\n| alt_Cyrl       | 95148   | turk1311   |\\n| aze_Latn       | 46300705| yes        |\\n| lus_Latn       | 509250  | sino1245   |\\n| rmn_Grek       | 94533   | indo1319   |\\n| guj_Gujr       | 45738685| indo1319   |\\n| crs_Latn       | 508755  | indo1319   |\\n| miq_Latn       | 94343   | misu1242   |\\n| ben_Beng       | 43514870| indo1319   |\\n| kqn_Latn       | 507913  | atla1278   |\\n| kaa_Cyrl       | 88815   | turk1311   |\\n| kan_Knda       | 41836495| drav1251   |\\n| snd_Arab       | 488730  | indo1319   |\\n| grn_Latn       | 87568   |              |\\n| mlt_Latn       | 40654838| afro1255   |\\n| yue_Hani       | 484700  | sino1245   |\\n| lhu_Latn       | 87255   | sino1245   |\\n| fra_Latn       | 39197581| indo1319   |\\n| tiv_Latn       | 483064  | atla1278   |\\n| lzh_Hani       | 86035   | sino1245   |\\n| ajp_Arab       | 83297   | afro1255   |\\n| cmn_Hani       | 80745   | sino1245   |\\n| fil_Latn       | 33493255| aust1307   |\\n| hin_Latn       | 466175  | indo1319   |\\n| gcf_Latn       | 80737   | indo1319   |\\n| nob_Latn       | 32869205| indo1319   |\\n| iku_Cans       | 465011   |              |\\n| rmn_Cyrl       | 79925   | indo1319   |\\n| kjh_Cyrl       | 79262   | turk1311   |\\n| deu_Latn       | 31015993| indo1319   |\\n| tdt_Latn       | 459818  | aust1307   |\\n| tur_Latn       | 29184662| turk1311   |\\n| mgh_Latn       | 78117   | atla1278   |\\n| pan_Guru       | 29052537| indo1319   |\\n| mfe_Latn       | 447435  | indo1319   |\\n| xmv_Latn       | 77896   | aust1307   |\\n| mar_Deva       | 28748897| indo1319   |\\n| swc_Latn       | 446378  | atla1278   |\\n| ige_Latn       | 77114   | atla1278   |\\n| por_Latn       | 27824391| indo1319   |\\n| mon_Latn       | 437950  | mong1349    |\\n| rmy_Latn       | 76991   | indo1319   |\\n| nld_Latn       | 25061426| indo1319   |\\n| ara_Arab       | 24524122| yes        |\\n| kik_Latn       | 437228  | atla1278   |\\n| bak_Latn       | 76809   | turk1311   |\\n| zho_Hani       | 24143786| yes        |\\n| cnh_Latn       | 436667  | sino1245   |\\n| gur_Latn       | 76151   | atla1278   |\\n| ita_Latn       | 23539857| indo1319   |\\n| idu_Latn       | 75106   | atla1278   |\\n| ind_Latn       | 23018106| aust1307   |\\n| yom_Latn       | 74818   | atla1278   |\\n| ell_Grek       | 22033282| indo1319   |\\n| tdx_Latn       | 74430   | aust1307   |\\n| bul_Cyrl       | 21823004| indo1319   |\\n| mzn_Arab       | 73719   | indo1319   |\\n| swe_Latn       | 20725883| indo1319   |\\n| cfm_Latn       | 70227   | sino1245   |\\n| ces_Latn       | 20376340| indo1319   |\\n| zpa_Latn       | 69237   | otom1299   |\\n| isl_Latn       | 19547941| indo1319   |\\n| kbd_Cyrl       | 67914   | abkh1242    |\\n| lvs_Latn       | 422952  | indo1319   |\\n| srp_Latn       | 18371769| indo1319   |\\n| qub_Latn       | 64973   | quec1387    |\\n| lao_Laoo       | 66966   | taik1256    |\\n| heb_Hebr       | 18128962| afro1255    |\\n| gug_Latn       | 392227  | tupi1275    |\\n| bsb_Latn       | 63634   | aust1307    |\\n| fas_Arab       | 18277593| yes        |\\n| bar_Latn       | 387070  | indo1319    |\\n| abn_Latn       | 61830   | atla1278    |\\n| heb_Hebr       | 18128962| afro1255    |\\n| ldz_Latn       | 61827   | atla1278    |\\n| srp_Latn       | 18371769| indo1319    |\\n| ort_Latn       | 61570   | ayma1253    |\\n| bba_Latn       | 61123   | atla1278    |\\n| aln_Latn       | 60989   | indo1319    |\\n| ceh_Latn       | 18149215| aust1307    |\\n| ayh_Latn       | 61140   | indo1319    |\\n| gom_Deva       | 61140   | indo1319    |\\n| gur_Latn       | 61827   | atla1278    |\\n| bcs_Latn       | 61827   | atla1278    |\\n| yra_Latn       | 61570   | ayma1253    |\\n| gwl_Latn       | 61140   | indo1319    |\\n| ale_Latn       | 61140   | indo1319    |\\n| leh_Latn       | 59944   | atla1278    |\\n| mkd_Cyrl       | 14717004| indo1319    |\\n| ban_Latn       | 59805   | aust1307    |\\n| slv_Latn       | 15719210| indo1319    |\\n| vie_Latn       | 15697827| aust1305    |\\n| leh_Latn       | 59944   | atla1278    |\\n| slk_Latn       | 14633631| indo1319    |\\n| ace_Latn       | 59333   | aust1307    |\\n| aln_Latn       | 60989   | indo1319    |\\n| bry_Latn       | 18128962| afro1255    |\\n| est_Latn       | 13600579| yes        |\\n| giv_Latn       | 17852274| indo1319    |\\n| gom_Deva       | 61140   | indo1319    |\\n| las_Latn       | 15719210| indo1319    |\\n| abn_Latn       | 61830   | atla1278    |\\n| goh_Latn       | 14576191| indo1319    |\\n| pes_Arab       | 57511   | indo1319    |\\n| est_Latn       | 13600579| yes        |\\n| wro_Latn       | 14633631| indo1319    |\\n| kro_Latn       | 14576191| indo1319    |\\n| abk_Cyrl       | 321578  | abkh1242    |\\n| ary_Arab       | 56933   | afro1255    |\\n| est_Latn       | 13600579| yes        |\\n| gwi_Latn       | 12775959| yes        |\\n| fra_Latn       | 12479626| indo1319    |\\n| glv_Latn       | 55641   | indo1319    |\\n| gur_Latn       | 12775959| yes        |\\n| fat_Latn       | 55609   | atla1278    |\\n| glv_Latn       | 55641   | indo1319    |\\n| lav_Latn       | 12143980| indo1319    |\\n| bci_Latn       | 384059  | atla1278    |\\n| sty_Latn       | 55641   | indo1319    |\\n| bna_Latn       | 12143980| indo1319    |\\n| sal_Latn       | 384059  | atla1278    |\\n| csk_Latn       | 55641   | indo1319    |\\n| mar_Knda       | 14576191| indo1319    |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959| yes        |\\n| gur_Latn       | 12775959"}
{"id": "acl-2023-long-61", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | Sent | Family Head |\\n|----------------|------|-------------|\\n| ast_Latn       | 4683554 | indo1319    |\\n| guc_Latn       | 249044   | araw1281    |\\n| hrx_Latn       | 45716    | indo1319    |\\n| mon_Cyrl       | 4616960  | mong1349     |\\n| mam_Latn       | 248348   | maya1287    |\\n| quh_Latn       | 45566    | quec1387    |\\n| hbs_Cyrl       | 4598073  | indo1319    |\\n| nia_Latn       | 247406   | aust1307    |\\n| hyw_Cyrl       | 45379    | indo1319    |\\n| hau_Latn       | 4368483  | afro1255    |\\n| nyn_Latn       | 241992   | atla1278    |\\n| rue_Cyrl       | 45369    | indo1319    |\\n| sna_Latn       | 4019596  | atla1278    |\\n| cab_Latn       | 240101   | araw1281    |\\n| eml_Latn       | 44630    | indo1319    |\\n| msa_Latn       | 3929084  | yes         |\\n| top_Latn       | 239232   | toto1251    |\\n| acm_Arab       | 44505    | afro1255    |\\n| som_Latn       | 3916769  | afro1255    |\\n| mco_Latn       | 231209   | mixe1284    |\\n| ach_Latn       | 43974    | nilo1247    |\\n| mlg_Latn       | 3715802  | yes         |\\n| tzh_Latn       | 230706   | maya1287    |\\n| vep_Latn       | 43076    | ural1272    |\\n| zul_Latn       | 3580113  | atla1278    |\\n| pms_Latn       | 227748   | indo1319    |\\n| npi_Deva       | 43072    | indo1319    |\\n| arz_Arab       | 3488224  | afro1255    |\\n| wuu_Hani       | 224088   | sino1245    |\\n| tok_Latn       | 42820    | arti1236    |\\n| nya_Latn       | 3409030  | atla1278    |\\n| plt_Latn       | 220413   | aust1307    |\\n| quy_Latn       | 42467    | indo1319    |\\n| yid_Hebr       | 220214   | indo1319    |\\n| ada_Latn       | 219427   | atla1278    |\\n| myv_Cyrl       | 42147    | ural1272    |\\n| uzb_Latn       | 3223485  | turk1311    |\\n| iba_Latn       | 213615   | aust1307    |\\n| sot_Latn       | 3205510  | atla1278    |\\n| kek_Latn       | 209932   | maya1287    |\\n| tat_Latn       | 41873    | aust1307    |\\n| uzb_Cyrl       | 3029947  | turk1311    |\\n| koo_Latn       | 209375   | atla1278    |\\n| lfn_Latn       | 41632    | arti1236    |\\n| cos_Latn       | 3015055  | indo1319    |\\n| sop_Latn       | 206501   | atla1278    |\\n| cgg_Latn       | 41196    | atla1278    |\\n| als_Latn       | 2954874  | indo1319    |\\n| kac_Latn       | 205542   | sino1245    |\\n| ful_Latn       | 41188    | atla1278    |\\n| amh_Ethi       | 2862985  | afro1255    |\\n| qvi_Latn       | 205447   | quec1387    |\\n| gor_Latn       | 41174    | aust1307    |\\n| war_Latn       | 2584810  | aust1307    |\\n| cak_Latn       | 204472   | maya1287    |\\n| ile_Latn       | 40984    | arti1236    |\\n| kbp_Latn       | 202877   | atla1278    |\\n| div_Thaa       | 2418687  | indo1319    |\\n| ctu_Latn       | 201662   | maya1287    |\\n| kri_Latn       | 201087   | indo1319    |\\n| fao_Latn       | 2365271  | indo1319    |\\n| mau_Latn       | 199134   | otom1299    |\\n| crh_Cyrl       | 39985    | turk1311    |\\n| uez_Cyrl       | 2293672  | turk1311    |\\n| scn_Latn       | 199068   | indo1319    |\\n| ksh_Latn       | 38130    | indo1319    |\\n| mri_Latn       | 2046850  | aust1307    |\\n| ncj_Latn       | 192962   | utoa1244    |\\n| hmn_Latn       | 1903898  |             |\\n| pau_Latn       | 190529   | aust1307    |\\n| crh_Latn       | 39896    | turk1311    |\\n| tyv_Cyrl       | 198649   | turk1311    |\\n| enm_Latn       | 39809    | indo1319    |\\n| bak_Cyrl       | 2264196  | turk1311    |\\n| ina_Latn       | 197315   | arti1236    |\\n| sat_Olck       | 39614    | aust1305    |\\n| ilo_Latn       | 2106531  | aust1307    |\\n| btx_Latn       | 193701   | aust1307    |\\n| mad_Latn       | 38993    | aust1307    |\\n| tso_Latn       | 2100708  | atla1278    |\\n| nch_Latn       | 193129   | utoa1244    |\\n| cac_Latn       | 38812    | maya1287    |\\n| mri_Latn       | 2046850  | aust1307    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1521612  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Latn       | 1449128  | indo1319    |\\n| quc_Latn       | 181559   | maya1287    |\\n| bts_Latn       | 36216    | aust1307    |\\n| lin_Latn       | 1408460  | atla1278    |\\n| yao_Latn       | 179965   | atla1278    |\\n| ajg_Latn       | 35631    | atla1278    |\\n| twi_Latn       | 1400979  | atla1278    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1543820  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Latn       | 1449128  | indo1319    |\\n| quc_Latn       | 181559   | maya1287    |\\n| bts_Latn       | 36216    | aust1307    |\\n| lin_Latn       | 1408460  | atla1278    |\\n| yao_Latn       | 179965   | atla1278    |\\n| ajg_Latn       | 35631    | atla1278    |\\n| twi_Latn       | 1400979  | atla1278    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1521612  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Latn       | 1449128  | indo1319    |\\n| quc_Latn       | 181559   | maya1287    |\\n| bts_Latn       | 36216    | aust1307    |\\n| lin_Latn       | 1408460  | atla1278    |\\n| yao_Latn       | 179965   | atla1278    |\\n| ajg_Latn       | 35631    | atla1278    |\\n| twi_Latn       | 1400979  | atla1278    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1521612  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Latn       | 1449128  | indo1319    |\\n| quc_Latn       | 181559   | maya1287    |\\n| bts_Latn       | 36216    | aust1307    |\\n| lin_Latn       | 1408460  | atla1278    |\\n| yao_Latn       | 179965   | atla1278    |\\n| ajg_Latn       | 35631    | atla1278    |\\n| twi_Latn       | 1400979  | atla1278    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1521612  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Latn       | 1449128  | indo1319    |\\n| quc_Latn       | 181559   | maya1287    |\\n| bts_Latn       | 36216    | aust1307    |\\n| lin_Latn       | 1408460  | atla1278    |\\n| yao_Latn       | 179965   | atla1278    |\\n| ajg_Latn       | 35631    | atla1278    |\\n| twi_Latn       | 1400979  | atla1278    |\\n| kss_Latn       | 185868   | atla1278    |\\n| bqc_Latn       | 36881    | mand1469    |\\n| kin_Latn       | 1521612  | atla1278    |\\n| afb_Arab       | 183694   | afro1255    |\\n| bim_Latn       | 36835    | atla1278    |\\n| hye_Armn       | 1463123  | indo1319    |\\n| mdy_Ethi       | 36370    | gong1255    |\\n| oci_Lat"}
{"id": "acl-2023-long-61", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | Sent | Family Head |\\n|----------------|------|-------------|\\n| pnb_Arab       | 899895 | indo1319    |\\n| sme_Latn       | 146803 | ural1272    |\\n| nfm_Latn       | 31997  | sino1245    |\\n| rar_Latn       | 894515 | aust1307    |\\n| gom_Latn       | 143937 | indo1319    |\\n| caq_Latn       | 31903  | aust1305    |\\n| fij_Latn       | 887134 | aust1307    |\\n| bum_Latn       | 141673 | atla1278    |\\n| rop_Latn       | 31889  | indo1319    |\\n| wls_Latn       | 882167 | aust1307    |\\n| mgr_Latn       | 138953 | atla1278    |\\n| tca_Latn       | 31852  | ticu1244    |\\n| ckb_Arab       | 874441 | indo1319    |\\n| ahk_Latn       | 135068 | sino1245    |\\n| yan_Latn       | 31775  | misu1242    |\\n| ven_Latn       | 860249 | atla1278    |\\n| kur_Arab       | 134160 | indo1319    |\\n| xav_Latn       | 31765  | nucl1710    |\\n| zsm_Latn       | 859947 | aust1307    |\\n| bas_Latn       | 133436 | atla1278    |\\n| chv_Cyrl       | 859863 | turk1311    |\\n| bin_Latn       | 133256 | atla1278    |\\n| cuk_Latn       | 31612  | chib1249    |\\n| tsz_Latn       | 133251 | tara1323    |\\n| hne_Deva       | 31465  | indo1319    |\\n| diq_Latn       | 128908 | indo1319    |\\n| wbm_Latn       | 31394  | aust1305    |\\n| guw_Latn       | 767918 | atla1278    |\\n| srd_Latn       | 127064 |               |\\n| zlm_Latn       | 31345  | aust1307    |\\n| bre_Latn       | 748954 | indo1319    |\\n| tcf_Latn       | 126050 | otom1299    |\\n| tui_Latn       | 31161  | atla1278    |\\n| toi_Latn       | 745385 | atla1278    |\\n| bzj_Latn       | 124958 | indo1319    |\\n| ifb_Latn       | 30980  | aust1307    |\\n| pus_Arab       | 731992 | indo1319    |\\n| udm_Cyrl       | 121705 | ural1272    |\\n| izz_Latn       | 30894  | atla1278    |\\n| che_Cyrl       | 728201 | nakh1245    |\\n| cce_Latn       | 120636 | atla1278    |\\n| rug_Latn       | 30857  | aust1307    |\\n| pis_Latn       | 714783 | indo1319    |\\n| meu_Latn       | 120273 | aust1307    |\\n| kon_Latn       | 685194 |               |\\n| chw_Latn       | 119751 | atla1278    |\\n| pxm_Latn       | 30698  | book1242    |\\n| hyw_Armn       | 679819 | indo1319    |\\n| ibg_Latn       | 118733 | aust1307    |\\n| mzh_Latn       | 30517  | mata1289    |\\n| kjb_Latn       | 111814 | atla1278    |\\n| con_Latn       | 683517 | indo1319    |\\n| bhw_Latn       | 117381 | aust1307    |\\n| ifa_Latn       | 30621  | aust1307    |\\n| nan_Latn       | 656389 | sino1245    |\\n| dln_Latn       | 30620  | sino1245    |\\n| lub_Latn       | 654390 | atla1278    |\\n| nyy_Latn       | 115914 | atla1278    |\\n| ext_Latn       | 30605  | indo1319    |\\n| lim_Latn       | 652078 | indo1319    |\\n| szl_Latn       | 112496 | indo1319    |\\n| ksd_Latn       | 30550  | aust1307    |\\n| tuk_Latn       | 649411 | turk1311    |\\n| ish_Latn       | 111814 | atla1278    |\\n| mwm_Latn       | 30432  | cent2225    |\\n| nse_Latn       | 105189 | atla1278    |\\n| krc_Cyrl       | 30353  | turk1311    |\\n| llb_Latn       | 30480  | atla1278    |\\n| hsa_Latn       | 104802 | indo1319    |\\n| alz_Latn       | 104392 | nilo1247    |\\n| pls_Latn       | 30136  | otom1299    |\\n| tll_Latn       | 586530 | atla1278    |\\n| apa_Arab       | 102392 | afro1255    |\\n| rap_Latn       | 30102  | aust1307    |\\n| ekk_Latn       | 582595 | ural1272    |\\n| fur_Latn       | 30052  | indo1319    |\\n| lug_Latn       | 566948 | atla1278    |\\n| mhr_Cyrl       | 100474 | ural1272    |\\n| ka_Latn        | 30031  | turk1311    |\\n| niu_Latn       | 566715 | aust1307    |\\n| djk_Latn       | 99234  | indo1319    |\\n| prs_Arab       | 26823  | indo1319    |\\n| wes_Latn       | 98492  | indo1319    |\\n| san_Latn       | 25742  | indo1319    |\\n| mah_Latn       | 534614 | aust1307    |\\n| gkn_Latn       | 97041  | atla1278    |\\n| som_Arab       | 14199  | afro1255    |\\n| tvl_Latn       | 521556 | aust1307    |\\n| grc_Grek       | 96986  | indo1319    |\\n| hbo_Hebr       | 96484  | afro1255    |\\n| hau_Arab       | 9593   | afro1255    |\\n\\nTable 13: List of languages used to train Glot500-m (Part III).\"}"}
{"id": "acl-2023-long-61", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languages (Abate et al., 2018), Phontron (Neubig, 2011), QADI (Abdelali et al., 2021), Quechua-IIC (Zevallos et al., 2022), SLI_GalWeb.1.0 (Agerri et al., 2018), Shami (Abu Kwaik et al., 2018), Stanford NLP, StatMT, TICO (Anastasopoulos et al., 2020), TIL (Mirzakhalov et al., 2021), Tatoeba, TeDDi (Moran et al., 2022), Tilde (Rozis and Skadins, 2017), W2C (Majli\u0161, 2011), WAT (Nakazawa et al., 2022), WikiMatrix (Schwenk et al., 2021), Wikipedia, Workshop on NER for South and South East Asian Languages (Singh, 2008), XLSum (Hasan et al., 2021).\\n\\nD Results for Each Task and Language\\n\\nWe report the detailed results for all tasks and languages in Table 14 (Sentence Retrieval Tatoeba), 15, 16 (Sentence Retrieval Bible), 17 (NER), and 18 (POS), 19, 20 (Text Classification), 21, 22 (Round Trip Alignment).\\n\\nE Perplexity Results for all Languages\\n\\nPerplexity number for all languages is presented in Table 23, Table 24, and Table 25.\"}"}
{"id": "acl-2023-long-61", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|-----------------|---------|---------|-----------|\\n| afrikani_Latn    | 71.9    | 76.5    | 81.1      |\\n| hebrew_Hebr     | 76.3    | 84.1    | 76.0      |\\n| pangusuan_Latn  | 4.8     | 5.6     | 11.0      |\\n| amharic_Ethi     | 35.1    | 37.5    | 44.6      |\\n| hindi_Deva      | 73.8    | 88.8    | 85.6      |\\n| persian_Arab    | 83.3    | 86.6    | 87.6      |\\n| arabic_Arab     | 59.2    | 66.8    | 64.2      |\\n| croatian_Latn   | 79.6    | 85.6    | 89.8      |\\n| pomeranian_Latn | 16.6    | 12.6    | 54.5      |\\n| arabic_Arab     | 32.5    | 47.8    | 63.5      |\\n| hrvorska_Latn   | 21.5    | 23.0    | 53.6      |\\n| polski_Latn     | 82.6    |         |           |\\n| asturian_Latn   | 59.8    | 59.8    | 87.4      |\\n| hunyadi_Latn    | 76.1    | 81.8    | 69.2      |\\n| portuguese_Latn | 91.0    | 92.1    | 90.1      |\\n| azersu_Latn     | 62.6    | 78.3    | 79.9      |\\n| hyeristani_Armn | 64.6    | 40.0    |           |\\n| romanian_Latn   | 86.0    |         |           |\\n| serbian_Latn    | 78.5    | 82.2    | 92.4      |\\n| bulgarian_Cyrl  | 84.4    |         |           |\\n| bulgarian_Latn  | 84.4    | 88.3    | 86.7      |\\n| islanti_Latn    | 78.7    | 84.5    | 84.0      |\\n| albanian_Latn   | 72.2    | 81.4    | 84.7      |\\n| catalan_Latn    | 72.8    | 73.9    | 78.7      |\\n| italiano_Latn    | 81.3    | 84.7    | 86.4      |\\n| srpski_Latn     | 78.1    | 85.0    | 90.0      |\\n| cyfrl_Belarusian | 70.0    | 80.5    | 19.9      |\\n| indonesian_Latn | 84.3    |         |           |\\n| espanol_Latn    | 85.5    |         |           |\\n| bulgarovski_Latn| 84.4    |         |           |\\n| breton_Latn     | 10.3    | 10.9    | 19.9      |\\n| japansu_Jpan    | 74.4    | 80.8    | 72.6      |\\n| svenska_Latn    | 90.4    |         |           |\\n| kabyle_Latn     | 3.7     | 3.0     | 16.4      |\\n| swahili_Latn    | 30.3    | 34.6    | 44.1      |\\n| cesky_Latn      | 71.1    |         |           |\\n| khmer_Khmr      | 41.1    | 45.0    | 52.5      |\\n| telugu_Telu     | 58.5    | 50.4    | 67.9      |\\n| cymru_Latn      | 45.7    | 45.7    | 55.7      |\\n| korase_Hang     | 73.4    | 84.3    | 78.0      |\\n| cebuano_Latn    | 15.2    | 15.0    | 41.3      |\\n| kabyle_Latn     | 3.7     | 3.0     | 16.4      |\\n| kannada_Taml    | 46.9    | 42.3    | 66.4      |\\n| chagossian_Latn | 33.2    | 36.0    | 49.4      |\\n| deutsz_Han     | 95.9    | 94.7    | 95.0      |\\n| latviski_Latn   | 33.6    | 48.0    | 42.8      |\\n| tukmen_Latn     | 16.3    | 14.8    | 63.5      |\\n| dtpnto_Latn     | 5.6     | 4.7     | 21.1      |\\n| lettviski_Latn  | 32.5    | 35.9    | 59.3      |\\n| turc_Latn       | 77.9    |         |           |\\n| ellada_Grek     | 76.2    | 84.1    | 80.2      |\\n| litviski_Latn   | 73.4    | 76.8    | 65.6      |\\n| turkmen_Cyrl    | 10.3    | 10.3    | 70.3      |\\n| bangalay_Deva   | 63.5    | 81.2    | 77.9      |\\n| turkmen_Cyrl    | 10.3    | 10.3    | 70.3      |\\n| maungiska_Mlym  | 80.1    |         |           |\\n| urdu_Arab       | 54.4    | 34.3    | 80.9      |\\n| euskara_Latn    | 45.9    |         |           |\\n| marathi_Deva    | 63.5    | 81.2    | 77.9      |\\n| uzbay_Cyrl      | 25.2    | 32.2    | 64.5      |\\n| faroese_Latn    | 45.0    | 42.7    | 82.4      |\\n| makedoniska_Cyrl| 70.5    |         |           |\\n| telugui_Telu    | 58.5    |         |           |\\n| gaelg_Latn      | 21.0    | 21.2    | 41.9      |\\n| nocnorski_Latn  | 70.7    | 77.8    | 87.8      |\\n| menglian_Hani   | 56.1    | 47.4    | 79.7      |\\n| frisian_Latn    | 60.1    | 62.4    | 75.1      |\\n| mordanca_Latn   | 28.8    | 29.0    | 77.1      |\\n| xhosa_Latn      | 28.9    | 31.7    | 56.3      |\\n| glagolitic_Latn | 21.0    | 21.2    | 41.9      |\\n| nederlands_Latn | 90.3    |         |           |\\n| sioux_Latn      | 0.3     | 0.2     | 0.1       |\\n| estonian_Latn   | 63.9    | 68.6    | 69.1      |\\n| warasavi_Latn   | 8.0     | 6.5     | 26.2      |\\n| francuska_Latn  | 85.7    |         |           |\\n| moldavianska_Cyrl| 70.5  |         |           |\\n| wu_han_Hani     | 56.1    | 47.4    | 79.7      |\\n| friulian_Latn   | 60.1    | 62.4    | 75.1      |\\n| ndebesa_Latn    | 28.8    | 29.0    | 77.1      |\\n| xhosa_Latn      | 28.9    | 31.7    | 56.3      |\\n| gallic_Latn     | 36.8    | 31.6    | 69.2      |\\n| oldcanarian_Latn| 22.9    | 23.2    | 46.9      |\\n| normandais_Latn | 28.8    | 29.0    | 77.1      |\\n| yiddish_Hebr    | 37.3    | 51.8    | 74.4      |\\n| gauls_Latn      | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| yue_Hani        | 50.3    | 42.3    | 76.3      |\\n| glagolica_Latn  | 21.0    | 21.2    | 41.9      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| vosges_Fr       | 32.0    | 36.9    | 50.8      |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| oldercanarian_Latn| 22.9 | 23.2 | 46.9 |\\n| normandais_Latn | 28.8    | 29.0    | 77.1      |\\n| yue_Hani        | 50.3    | 42.3    | 76.3      |\\n| oldcanarian_Latn| 22.9    | 23.2    | 46.9      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\\n| gaelic_Latn     | 32.0    | 36.9    | 50.8      |\\n| norwegianska_Latn| 70.7 | 77.8 | 87.8 |\"}"}
{"id": "acl-2023-long-61", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|----------|--------|---------|---------|-----------|\\n| ace      | Latn   | 4.4     | 4.6     | 53.4      |\\n| iba      | Latn   | 14.4    | 13.6    | 66.0      |\\n| pan      | Guru   | 43.2    |         | 59.4      |\\n| ach      | Latn   | 4.4     | 3.2     | 40.0      |\\n| ibo      | Latn   | 5.0     | 3.0     | 30.4      |\\n| pap      | Latn   | 12.4    | 9.2     | 72.4      |\\n| acr      | Latn   | 2.6     | 3.4     | 25.4      |\\n| ifa      | Latn   | 4.4     | 4.4     | 39.2      |\\n| ibo      | Latn   | 5.0     | 3.0     | 30.4      |\\n| pap      | Latn   | 12.4    | 9.2     | 72.4      |\\n| akr      | Latn   | 3.0     | 2.6     | 3.2       |\\n| ilo      | Latn   | 6.2     | 3.6     | 55.0      |\\n| pes      | Arab   | 69.4    | 72.2    | 80.8      |\\n| aka      | Latn   | 5.0     | 4.2     | 57.0      |\\n| ind      | Latn   | 82.6    | 80.4    | 72.2      |\\n| pis      | Latn   | 6.4     | 5.0     | 57.2      |\\n| aln      | Latn   | 67.8    | 72.4    | 67.6      |\\n| isl      | Latn   | 62.6    | 73.6    | 66.0      |\\n| pls      | Latn   | 5.0     | 4.0     | 34.4      |\\n| als      | Latn   | 51.4    | 48.0    | 55.8      |\\n| ita      | Latn   | 75.4    | 73.6    | 70.0      |\\n| plt      | Latn   | 26.6    | 28.0    | 59.8      |\\n| alt      | Cyrl   | 12.6    | 9.0     | 50.8      |\\n| ium      | Latn   | 3.2     | 3.0     | 24.8      |\\n| poh      | Latn   | 3.4     | 2.4     | 15.2      |\\n| alz      | Latn   | 4.6     | 3.8     | 34.6      |\\n| ixl      | Latn   | 4.0     | 3.0     | 18.4      |\\n| pol      | Latn   | 79.2    |         | 79.8      |\\n| amh      | Ethi   | 35.4    | 43.2    | 52.8      |\\n| izz      | Latn   | 2.8     | 2.8     | 25.6      |\\n| pon      | Latn   | 5.6     | 4.4     | 21.6      |\\n| aoj      | Latn   | 5.0     | 3.0     | 20.4      |\\n| jam      | Latn   | 6.6     | 4.4     | 67.8      |\\n| por      | Latn   | 81.6    |         | 81.6      |\\n| ary      | Arab   | 2.8     | 4.0     | 15.2      |\\n| arz      | Arab   | 5.4     | 4.8     | 24.8      |\\n| kaa      | Latn   | 9.2     | 9.8     | 43.4      |\\n| qub      | Latn   | 4.6     | 3.6     | 43.4      |\\n| asm      | Beng   | 26.2    | 40.6    | 66.6      |\\n| kaf      | Latn   | 3.4     | 2.4     | 20.6      |\\n| quc      | Latn   | 3.6     | 2.8     | 24.8      |\\n| ayr      | Latn   | 4.8     | 4.8     | 52.8      |\\n| kac      | Latn   | 3.6     | 3.2     | 26.4      |\\n| qug      | Latn   | 4.8     | 3.6     | 50.8      |\\n| ayr      | Latn   | 4.8     | 4.8     | 52.8      |\\n| kac      | Latn   | 3.6     | 3.2     | 26.4      |\\n| quh      | Latn   | 4.6     | 4.4     | 56.2      |\\n| aze      | Latn   | 71.0    | 78.6    | 73.0      |\\n| kan      | Knda   | 51.2    |         | 67.6      |\\n| kzm      | Knda   | 52.4    |         | 57.2      |\\n| kac      | Latn   | 3.6     | 3.2     | 26.4      |\\n| quz      | Latn   | 4.8     | 4.2     | 68.0      |\\n| ban      | Latn   | 9.0     | 9.8     | 33.0      |\\n| kbp      | Latn   | 2.6     | 2.6     | 36.0      |\\n| qvi      | Latn   | 4.4     | 3.4     | 46.8      |\\n| bar      | Latn   | 13.4    | 12.8    | 40.8      |\\n| kek      | Latn   | 5.0     | 3.4     | 26.4      |\\n| rap      | Latn   | 3.2     | 3.2     | 25.6      |\\n| bba      | Latn   | 3.8     | 3.4     | 36.8      |\\n| khm      | Khmr   | 28.4    | 42.6    | 47.6      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkn      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kia      | Latn   | 4.0     | 5.6     | 33.2      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kik      | Latn   | 3.2     | 2.8     | 53.4      |\\n| kkk      | Latn   | 5.0     | 5.0     | 59.4      |\\n| kia      | Latn   | 4.0     | 5"}
{"id": "acl-2023-long-61", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|-----------------|---------|---------|-----------|\\n| dtp_Latn        | 5.4     | 4.2     | 24.2      |\\n| mdy_Ethi        | 2.8     | 2.4     | 31.6      |\\n| tih_Latn        | 5.2     | 4.4     | 51.6      |\\n| dyu_Latn        | 4.2     | 2.4     | 50.2      |\\n| meu_Latn        | 5.6     | 4.4     | 52.0      |\\n| tir_Ethi        | 7.4     | 6.2     | 43.4      |\\n| dzo_Tibt        | 2.2     | 2.0     | 36.4      |\\n| mfe_Latn        | 9.0     | 6.8     | 78.6      |\\n| tlh_Latn        | 7.8     | 6.4     | 72.4      |\\n| eff_Latn        | 4.4     | 4.2     | 54.0      |\\n| mgh_Latn        | 5.2     | 3.4     | 23.6      |\\n| tob_Latn        | 2.2     | 3.0     | 16.8      |\\n| ell_Grek        |         |         | 48.6      |\\n| mgr_Latn        | 4.0     | 4.4     | 57.6      |\\n| toh_Latn        | 4.0     | 4.0     | 47.2      |\\n| enm_Latn        | 39.8    | 39.2    | 66.0      |\\n| mhr_Cyrl        | 6.6     | 5.4     | 48.0      |\\n| toi_Latn        | 4.2     | 4.4     | 47.4      |\\n| epo_Latn        |         |         | 56.2      |\\n| min_Latn        | 9.4     | 6.2     | 29.0      |\\n| toj_Latn        | 4.2     | 4.0     | 15.6      |\\n| est_Latn        | 72.0    |         | 56.4      |\\n| miq_Latn        | 4.4     | 4.4     | 47.4      |\\n| mlg_Latn        | 29.0    | 28.4    | 66.0      |\\n| tpi_Latn        | 5.8     | 4.4     | 58.0      |\\n| fao_Latn        | 24.0    | 28.4    | 73.4      |\\n| mlt_Latn        | 5.8     | 5.2     | 50.4      |\\n| tpm_Latn        | 3.6     | 3.0     | 39.6      |\\n| fas_Arab        | 78.2    | 80.4    | 89.2      |\\n| mos_Latn        | 4.2     | 3.6     | 42.8      |\\n| tsn_Latn        | 5.4     | 3.6     | 41.8      |\\n| fij_Latn        | 3.8     | 3.0     | 36.4      |\\n| mps_Latn        | 3.2     | 3.2     | 21.6      |\\n| mri_Latn        | 4.2     | 3.8     | 48.4      |\\n| msk_Latn        | 76.6    | 72.6    | 74.8      |\\n| msw_Latn        | 3.4     | 3.6     | 8.0       |\\n| mjd_Latn        | 3.6     | 3.0     | 36.4      |\\n| mra_Latn        | 4.0     | 4.4     | 47.4      |\\n| msh_Latn        | 4.2     | 4.4     | 47.4      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrd_Latn        | 4.6     | 4.4     | 47.4      |\\n| msh_Latn        | 3.2     | 3.2     | 13.6      |\\n| mrs_Latn        | 3.2     | 3.2     | 13.6      |\\n| msw_Latn        | 3.2     | 3.2     | 1"}
{"id": "acl-2023-long-61", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|----------------|---------|---------|-----------|\\n| ace_Latn       | 33.4    | 38.9    | 44.2      |\\n| heb_Hebr       | 51.5    | 56.5    | 49.0      |\\n| ori_Orya       | 31.4    | 27.6    | 31.0      |\\n| afr_Latn       | 75.6    | 78.3    | 76.7      |\\n| hin_Deva       | 67.0    | 71.1    | 69.4      |\\n| oss_Cyrl       | 33.7    | 39.2    | 52.1      |\\n| als_Latn       | 60.7    | 61.4    | 80.0      |\\n| hrv_Latn       | 77.2    | 78.9    | 77.3      |\\n| pan_Guru       | 50.0    | 50.5    | 48.1      |\\n| amh_Ethi       | 42.2    | 40.9    | 45.4      |\\n| hsb_Latn       | 64.0    | 69.0    | 71.2      |\\n| pms_Latn       | 71.2    | 74.9    | 75.9      |\\n| ara_Arab       | 44.7    | 48.7    | 56.1      |\\n| hun_Latn       | 76.2    | 79.8    | 75.9      |\\n| pnb_Arab       | 57.0    | 64.6    | 65.8      |\\n| arg_Latn       | 73.6    | 74.6    | 77.2      |\\n| hye_Armn       | 50.8    | 61.7    | 54.8      |\\n| pol_Latn       | 77.5    | 81.2    | 78.1      |\\n| arz_Arab       | 48.3    | 52.5    | 57.4      |\\n| ibo_Latn       | 40.8    | 42.8    | 58.6      |\\n| por_Latn       | 77.8    | 81.2    | 78.6      |\\n| asm_Beng       | 53.2    | 64.4    | 64.2      |\\n| ido_Latn       | 61.6    | 78.6    | 77.8      |\\n| pus_Arab       | 37.4    | 39.9    | 41.4      |\\n| ast_Latn       | 78.1    | 82.8    | 84.5      |\\n| ilo_Latn       | 55.3    | 65.3    | 77.1      |\\n| que_Latn       | 59.1    | 55.2    | 66.8      |\\n| aym_Latn       | 40.8    | 38.7    | 47.1      |\\n| ina_Latn       | 54.7    | 63.4    | 58.0      |\\n| roh_Latn       | 52.6    | 55.7    | 60.3      |\\n| aze_Latn       | 62.4    | 69.2    | 66.1      |\\n| ind_Latn       | 49.0    | 54.1    | 56.6      |\\n| ron_Latn       | 74.8    | 79.9    | 74.2      |\\n| bak_Cyrl       | 35.1    | 49.3    | 59.4      |\\n| isl_Latn       | 69.1    | 77.2    | 72.1      |\\n| rus_Cyrl       | 63.8    | 70.0    | 67.6      |\\n| bar_Latn       | 55.2    | 58.6    | 68.4      |\\n| ita_Latn       | 77.3    | 81.2    | 78.7      |\\n| sah_Cyrl       | 47.3    | 49.7    | 74.2      |\\n| bel_Cyrl       | 74.2    | 78.7    | 74.3      |\\n| jav_Latn       | 58.4    | 61.2    | 55.8      |\\n| san_Deva       | 36.9    | 37.3    | 35.8      |\\n| jbo_Latn       | 18.0    | 26.3    | 27.8      |\\n| scn_Latn       | 49.9    | 54.8    | 65.8      |\\n| bih_Deva       | 50.7    | 57.1    | 58.7      |\\n| jpn_Jpan       | 19.7    | 20.6    | 17.2      |\\n| sco_Latn       | 80.9    | 81.8    | 85.6      |\\n| bod_Tibt       | 2.5     | 3.0     | 31.6      |\\n| kan_Knda       | 56.9    | 60.8    | 58.4      |\\n| sgs_Latn       | 42.5    | 47.4    | 62.7      |\\n| bos_Latn       | 74.0    | 74.3    | 74.2      |\\n| kat_Geor       | 65.5    | 69.5    | 68.3      |\\n| sin_Sinh       | 52.2    | 57.0    | 57.8      |\\n| brc_Latn       | 59.1    | 63.9    | 63.3      |\\n| kaz_Cyrl       | 43.7    | 52.7    | 50.0      |\\n| slk_Latn       | 75.0    | 81.7    | 78.5      |\\n| khm_Khmr       | 43.3    | 46.2    | 40.6      |\\n| slv_Latn       | 79.4    | 82.2    | 80.1      |\\n| khq_Cyrl       | 44.2    | 46.9    | 46.7      |\\n| som_Latn       | 55.8    | 55.5    | 58.2      |\\n| ceb_Latn       | 55.1    | 57.8    | 53.8      |\\n| kor_Hang       | 49.1    | 58.5    | 50.9      |\\n| spa_Latn       | 72.8    | 73.3    | 72.8      |\\n| ces_Latn       | 77.6    | 80.8    | 78.3      |\\n| ksh_Latn       | 41.3    | 48.3    | 58.7      |\\n| sqi_Latn       | 74.0    | 74.4    | 76.6      |\\n| che_Cyrl       | 15.4    | 24.6    | 60.9      |\\n| kur_Latn       | 58.8    | 65.0    | 69.6      |\\n| srp_Cyrl       | 59.7    | 71.4    | 66.4      |\\n| chv_Cyrl       | 52.9    | 51.6    | 75.9      |\\n| lat_Latn       | 70.7    | 79.2    | 73.8      |\\n| sun_Latn       | 42.0    | 49.7    | 57.7      |\\n| ckm_Arab       | 33.1    | 42.6    | 75.5      |\\n| lav_Latn       | 73.4    | 77.1    | 74.0      |\\n| swa_Latn       | 65.6    | 69.0    | 69.6      |\\n| cos_Latn       | 54.3    | 56.4    | 56.0      |\\n| lij_Latn       | 36.9    | 41.6    | 46.6      |\\n| swe_Latn       | 71.8    | 75.9    | 69.7      |\\n| crh_Latn       | 44.3    | 52.4    | 54.7      |\\n| lim_Latn       | 59.9    | 64.7    | 71.8      |\\n| szl_Latn       | 58.2    | 56.7    | 67.6      |\\n| csb_Latn       | 55.1    | 54.2    | 61.2      |\\n| lin_Latn       | 37.4    | 41.3    | 54.0      |\\n| tam_Taml       | 55.0    | 57.9    | 55.2      |\\n| cym_Latn       | 57.9    | 60.1    | 59.7      |\\n| lit_Latn       | 73.4    | 77.0    | 73.5      |\\n| tat_Cyrl       | 40.7    | 47.7    | 68.0      |\\n| lmo_Latn       | 68.8    | 68.4    | 71.3      |\\n| tel_Telu       | 47.4    | 52.5    | 46.0      |\\n| ltz_Latn       | 47.4    | 55.8    | 69.1      |\\n| tgk_Cyrl       | 24.7    | 38.3    | 68.5      |\\n| div_Thaa       | 0.0     | 0.0     | 51.1      |\\n| mal_Mlym       | 61.0    | 63.3    | 61.3      |\\n| tha_Thai       | 4.2     | 1.6     | 3.2       |\\n| ell_Grek       | 73.7    | 78.6    | 72.8      |\\n| mar_Deva       | 60.2    | 63.4    | 60.7      |\\n| tuk_Latn       | 45.6    | 50.7    | 59.7      |\\n| eml_Latn       | 32.9    | 36.1    | 40.8      |\\n| mhr_Cyrl       | 44.3    | 48.3    | 63.1      |\\n| tur_Latn       | 74.9    | 79.3    | 76.1      |\\n| eng_Latn       | 82.7    | 84.5    | 83.3      |\\n| min_Latn       | 42.9    | 46.2    | 41.8      |\\n| uig_Arab       | 44.0    | 50.9    | 48.0      |\\n| epo_Latn       | 63.8    | 71.8    | 68.0      |\\n| mkd_Cyrl       | 74.5    | 80.4    | 73.3      |\\n| ukr_Cyrl       | 75.2    | 76.3    | 74.2      |\\n| est_Latn       | 72.2    | 78.5    | 73.5      |\\n| mlg_Latn       | 54.9    | 54.3    | 57.9      |\\n| urd_Arab       | 51.2    | 57.8    | 74.5      |\\n| msa_Latn       | 62.3    | 70.4    | 65.8      |\\n| mwl_Latn       | 42.6    | 47.5    | 45.3      |\\n| vep_Latn       | 59.8    | 59.3    | 71.3      |\\n| fas_Arab       | 44.6    | 58.0    | 51.2      |\\n| mta_Latn       | 62.3    | 70.4    | 65.8      |\\n| vls_Latn       | 68.1    | 73.6    | 73.7      |\\n| fin_Latn       | 75.5    | 79.1    | 75.2      |\\n| mzn_Arab       | 36.4    | 43.1    | 44.9      |\\n| mri_Latn       | 14.2    | 18.3    | 53.5      |\\n| vep_Latn       | 59.8    | 59.3    | 71.3      |\\n| frr_Latn       | 45.4    | 46.8    | 54.8      |\\n| mzo_Latn       | 72.4    | 74.3    | 66.9      |\\n| vec_Latn       | 59.0    | 63.3    | 66.4      |\\n| fao_Latn       | 61.1    | 70.8    | 72.4      |\\n| mri_Latn       | 14.2    | 18.3    | 53.5      |\\n| war_Latn       | 61.9    | 61.4    | 66.1      |\\n| mzi_Latn       | 72.4    | 74.3    | 66.9      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\n| nan_Latn       | 46.2    | 51.4    | 82.1      |\\"}
{"id": "acl-2023-long-61", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B 1 | XLM-R-L 2 | Glot500-m 3 |\\n|----------------|-----------|-----------|-------------|\\n| afr_Latn       | 88.7      | 89.3      | 87.5        |\\n| hbo_Hebr       | 38.9      | 45.7      | 54.2        |\\n| pol_Latn       | 84.7      | 85.4      | 82.4        |\\n| ajp_Arab       | 62.9      | 67.3      | 69.7        |\\n| heb_Hebr       | 68.0      | 69.2      | 67.2        |\\n| por_Latn       | 88.6      | 89.8      | 88.2        |\\n| aln_Latn       | 53.5      | 60.4      | 52.3        |\\n| hin_Deva       | 71.3      | 75.3      | 70.3        |\\n| quc_Latn       | 28.9      | 29.3      | 62.4        |\\n| amh_Ethi       | 64.5      | 66.2      | 66.1        |\\n| hrv_Latn       | 85.9      | 86.2      | 85.5        |\\n| ron_Latn       | 83.9      | 85.7      | 80.6        |\\n| ara_Arab       | 68.5      | 69.7      | 65.4        |\\n| hsb_Latn       | 71.5      | 74.4      | 83.6        |\\n| rus_Cyrl       | 89.1      | 89.7      | 88.7        |\\n| bam_Latn       | 25.4      | 23.5      | 40.8        |\\n| hun_Latn       | 82.6      | 82.7      | 81.2        |\\n| sah_Cyrl       | 20.3      | 22.8      | 76.8        |\\n| bel_Cyrl       | 86.2      | 86.2      | 86.0        |\\n| hye_Armn       | 85.2      | 86.5      | 84.0        |\\n| san_Deva       | 18.3      | 28.6      | 26.1        |\\n| ben_Beng       | 82.8      | 83.8      | 83.8        |\\n| hyw_Armn       | 78.5      | 82.5      | 80.4        |\\n| sin_Sinh       | 57.7      | 60.1      | 54.7        |\\n| bre_Latn       | 61.6      | 66.6      | 60.7        |\\n| ind_Latn       | 83.5      | 84.1      | 82.7        |\\n| slk_Latn       | 85.6      | 85.8      | 84.4        |\\n| bul_Cyrl       | 89.1      | 88.9      | 88.1        |\\n| isl_Latn       | 84.2      | 85.1      | 82.8        |\\n| slv_Latn       | 78.5      | 79.1      | 75.9        |\\n| cat_Latn       | 86.7      | 87.9      | 86.3        |\\n| ita_Latn       | 88.3      | 89.6      | 87.3        |\\n| sme_Latn       | 29.8      | 31.5      | 73.7        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| ell_Grek       | 87.3      | 87.0      | 85.4        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 86.1      | 86.6      | 85.3        |\\n| dan_Latn       | 90.7      | 91.0      | 90.2        |\\n| kmr_Latn       | 73.1      | 78.2      | 75.5        |\\n| swe_Latn       | 93.5      | 93.7      | 92.1        |\\n| eng_Latn       | 96.3      | 96.5      | 96.0        |\\n| lav_Latn       | 86.0      | 86.3      | 83.5        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| lat_Latn       | 75.0      | 80.3      | 72.4        |\\n| tat_Cyrl       | 45.0      | 48.8      | 70.1        |\\n| kor_Hang       | 53.7      | 53.4      | 53.1        |\\n| tam_Taml       | 76.1      | 76.9      | 75.0        |\\n| spa_Latn       | 88.5      | 89.0      | 88.0        |\\n| ces_Latn       | 85.0      | 85.4      | 84.4        |\\n| jpn_Jpan       | 17.3      | 32.2      | 31.7        |\\n| sqi_Latn       | 81.4      | 82.9      | 77.9        |\\n| cym_Latn       | 65.5      | 67.0      | 64.4        |\\n| kaz_Cyrl       | 77.3      | 79.1      | 75.9        |\\n| srp_Latn       | 8"}
{"id": "acl-2023-long-61", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|----------------|---------|---------|-----------|\\n| ace_Latn       | 15 25   |         |           |\\n| iba_Latn       | 30 35   |         |           |\\n| ote_Latn       | 6 5     |         |           |\\n| ach_Latn       | 9 8     |         |           |\\n| ibo_Latn       | 8 6     |         |           |\\n| pag_Latn       | 22 21   |         |           |\\n| acr_Latn       | 10 8    |         |           |\\n| ifa_Latn       | 12 12   |         |           |\\n| pam_Latn       | 20 18   |         |           |\\n| afr_Latn       | 54 64   |         |           |\\n| ifb_Latn       | 14 11   |         |           |\\n| pan_Guru       | 53 65   |         |           |\\n| agw_Latn       | 11 13   |         |           |\\n| ikk_Latn       | 11 7    |         |           |\\n| iko_Latn       | 15 13   |         |           |\\n| ilo_Latn       | 15 13   |         |           |\\n| inl_Latn       | 7 5     |         |           |\\n| ilu_Latn       | 6 7     |         |           |\\n| ilt_Latn       | 22 30   |         |           |\\n| mhd_Latn       | 11 13   |         |           |\\n| mts_Latn       | 14 17   |         |           |\\n| crh_Cyrl       | 22 31   |         |           |\\n| crs_Latn       | 14 17   |         |           |\\n| lzh_Hani       | 46 55 55|         |           |\\n| csy_Latn       | 9 7     |         |           |\\n| mam_Latn       | 10 6    |         |           |\\n| mar_Deva       | 55 63   |         |           |\\n| man_Latn       | 10 6    |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |         |           |\\n| dzo_Tibt       | 6 5     |        "}
{"id": "acl-2023-long-61", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language | Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|----------|--------|---------|---------|-----------|\\n| enm_Latn | 46 56  | 65 53   | 48      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n| est_Latn | 62 53  | 53 41   | 42      |\\n| evi_Latn | 10 9   | 50 48   | 40      |\\n\\nTable 20: F1 of XLM-R-B, XLM-R-L, and Glot500-m on Text Classification (Part II).\"}"}
{"id": "acl-2023-long-61", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 21: Accuracy of XLM-R-B, XLM-R-L, and Glot500-m on Round Trip Alignment (Part I).\\n\\n| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|-----------------|---------|---------|-----------|\\n| 5.10            |         |         |           |\\n| knv_Latn        | 1.27    | 1.53    |           |\\n| mad_Latn        | 2.65    | 3.29    |           |\\n| izz_Latn        | 1.65    | 2.06    |           |\\n| kan_Latn        | 1.62    |         |           |\\n| pls_Latn        | 2.14    | 2.57    |           |\\n| tgk_Cyrl        | 2.63    | 3.29    |           |\\n| sba_Latn        | 1.88    | 2.24    |           |\\n| hye_Latn        | 2.34    |         |           |\\n| rar_Latn        | 1.83    | 2.22    |           |\\n| ixl_Latn        | 1.62    | 1.94    |           |\\n| sin_Sinh        | 2.55    |         |           |\\n| rap_Latn        | 1.31    | 1.61    |           |\\n| lug_Latn        | 2.84    | 3.50    |           |\\n| slv_Latn        | 3.11    | 4.32    |           |\\n| kaa_Latn        | 2.34    | 2.96    |           |\\n| pdt_Latn        | 2.41    | 3.33    |           |\\n| quw_Latn        | 2.89    | 3.50    |           |\\n| kua_Latn        | 4.25    | 4.92    |           |\\n| leh_Latn        | 2.73    | 3.66    |           |\\n| quc_Latn        | 1.87    | 2.45    |           |\\n| tir_Ethi        | 1.90    | 1.93    |           |\\n| ltz_Latn        | 3.73    | 3.99    |           |\\n| pau_Latn        | 2.67    | 3.09    |           |\\n| lhu_Latn        | 1.43    |         |           |\\n| ksd_Latn        | 2.82    | 3.28    |           |\\n| mah_Latn        | 2.95    | 3.59    |           |\\n| rmy_Latn        | 2.85    | 3.68    |           |\\n| ikk_Latn        | 1.75    | 2.29    |           |\\n| tam_Taml        | 3.09    | 3.77    |           |\\n| quh_Latn        | 2.91    | 3.46    |           |\\n| slk_Latn        | 4.65    | 5.06    |           |\\n| jam_Latn        | 2.77    | 3.06    |           |\\n| rus_Cyrl        | 4.20    | 5.05    |           |\\n| qub_Latn        | 2.48    | 2.97    |           |\\n| ssw_Latn        | 3.27    | 4.02    |           |\\n| kpg_Latn        | 2.80    | 3.12    |           |\\n| spa_Latn        | 3.71    | 4.21    |           |\\n| quy_Latn        | 2.69    | 3.15    |           |\\n| lav_Latn        | 3.35    | 4.56    |           |\\n| toh_Latn        | 2.17    | 2.90    |           |\\n| tih_Latn        | 2.21    | 2.89    |           |\\n| kik_Latn        | 2.28    | 2.73    |           |\\n| teo_Latn        | 3.37    | 4.18    |           |\\n| kbp_Latn        | 1.47    | 1.65    |           |\\n| poh_Latn        | 0.92    | 1.10    |           |\\n| kaz_Cyrl        | 3.82    | 4.56    |           |\\n| ifb_Latn        | 2.22    | 2.58    |           |\\n| iba_Latn        | 2.77    | 3.85    |           |\\n| som_Latn        | 3.15    | 3.40    |           |\\n| krc_Cyrl        | 2.85    | 3.66    |           |\\n| tdt_Latn        | 3.20    | 3.48    |           |\\n| sxn_Latn        | 2.08    | 2.54    |           |\\n| prs_Arab        | 3.54    | 4.28    |           |\\n| hye_Armn        | 2.32    | 3.25    |           |\\n| kor_Hang        | 2.76    | 3.99    |           |\\n| kan_Knda        | 2.58    | 3.18    |           |\\n| swh_Latn        | 4.05    | 4.99    |           |\\n| pcm_Latn        | 3.81    | 4.44    |           |\\n| luo_Latn        | 3.34    | 4.09    |           |\\n| mar_Deva        | 3.87    | 5.13    |           |\\n| kmr_Cyrl        | 2.31    | 2.76    |           |\\n| lit_Latn        | 4.69    | 5.66    |           |\\n| pol_Latn        | 3.94    |         |           |\\n| pes_Arab        | 2.66    | 3.91    |           |\\n| snd_Arab        | 3.12    | 3.92    |           |\\n| srm_Latn        | 1.75    | 1.96    |           |\\n| kiu_Latn        | 2.67    | 3.26    |           |\\n| kir_Cyrl        | 4.54    | 4.35    |           |\\n| kat_Geor        | 4.06    | 4.99    |           |\\n| prk_Latn        | 2.10    | 2.70    |           |\\n| sop_Latn        | 2.80    | 3.55    |           |\\n| tat_Cyrl        | 2.13    | 2.62    |           |\\n| khm_Khmr        | 1.57    | 1.70    |           |\\n| lam_Latn        | 2.41    | 3.09    |           |\\n| lao_Laoo        | 2.61    | 3.21    |           |\\n| qvi_Latn        | 2.82    | 3.42    |           |\\n| tam_Latn        | 2.59    |         |           |\\n| kjh_Cyrl        | 3.13    | 3.81    |           |\\n| mau_Latn        | 1.60    |         |           |\\n| tob_Latn        | 1.42    | 1.84    |           |\\n| quz_Latn        | 3.33    | 3.89    |           |\\n| tel_Telu        | 2.87    | 3.78    |           |\\n| kjb_Latn        | 2.42    | 3.03    |           |\\n| run_Latn        | 3.33    | 3.98    |           |\\n| ksw_Mymr        | 0.95    | 1.46    |           |\\n| kri_Latn        | 1.90    | 2.52    |           |\\n| plt_Latn        | 3.74    | 3.99    |           |\\n| ilo_Latn        | 3.06    | 3.87    |           |\\n| mco_Latn        | 1.42    | 1.63    |           |\\n| ldi_Latn        | 3.41    | 3.94    |           |\\n| isl_Latn        | 4.40    | 5.22    |           |\\n| kmm_Latn        | 2.52    | 3.30    |           |\\n| pon_Latn        | 3.53    | 4.51    |           |\\n| jav_Latn        | 3.10    | 3.67    |           |\\n| seh_Latn        | 3.44    | 4.20    |           |\\n| kss_Latn        | 0.99    | 1.09    |           |\\n| mcn_Latn        | 3.74    | 4.42    |           |\\n| ifa_Latn        | 1.81    | 2.40    |           |\\n| mam_Latn        | 1.84    | 2.20    |           |\\n| san_Latn        | 1.54    | 2.23    |           |\\n| tca_Latn        | 1.29    | 1.56    |           |\\n| srp_Cyrl        | 6.48    | 6.50    |           |\\n| tgl_Latn        | 3.22    | 3.35    |           |\\n| sqi_Latn        | 4.07    | 5.07    |           |\\n| kal_Latn        | 3.00    | 3.90    |           |\\n| loz_Latn        | 3.35    | 3.91    |           |\\n| sna_Latn        | 2.89    | 3.39    |           |\\n| mbb_Latn        | 2.25    | 2.56    |           |\\n| por_Latn        | 3.61    | 4.35    |           |\\n| tha_Thai        | 1.50    | 2.72    |           |\\n| ron_Latn        | 3.33    | 4.00    |           |\\n| mck_Latn        | 3.34    | 4.06    |           |\\n| pam_Latn        | 2.85    | 3.52    |           |\\n| lin_Latn        | 1.78    | 2.73    |           |\\n| rug_Latn        | 2.56    | 2.95    |           |\\n| sus_Deva        | 1.68    | 1.66    |           |\\n| kab_Latn        | 2.51    | 3.08    |           |\\n| lus_Latn        | 2.43    | 2.99    |           |\\n| sun_Latn        | 2.98    | 3.69    |           |\\n| lat_Latn        | 4.65    | 5.51    |           |\\n| sag_Latn        | 2.92    | 3.52    |           |\\n| meu_Latn        | 3.26    | 3.79    |           |\\n| kac_Latn        | 1.66    | 2.17    |           |\\n| ita_Latn        | 3.55    | 4.02    |           |\\n| sme_Latn        | 2.70    | 3.35    |           |\\n| kmr_Latn        | 3.75    | 4.19    |           |\\n| mal_Latn        | 2.67    |         |           |\\n| kia_Latn        | 2.92    | 3.27    |           |\\n| lzh_Hani        | 3.21    |         |           |\\n| jpn_Jpan        | 3.62    |         |           |\\n| ium_Latn        | 2.00    | 2.27    |           |\\n| pan_Guru        | 2.11    | 2.73    |           |\\n| sot_Latn        | 3.49    | 4.31    |           |\\n| rop_Latn        | 1.60    | 2.08    |           |\\n| srn_Latn        | 3.40    | 3.86    |           |\\n| mdy_Ethi        | 1.36    | 1.26    |           |\\n| kek_Latn        | 1.91    | 2.45    |           |\\n| tbz_Latn        | 1.62    | 2.03    |           |\\n| pap_Latn        | 3.12    | 3.85    |           |\\n| kaa_Cyrl        | 2.99    | 3.91    |           |\\n| san_Deva        | 2.48    | 2.20    |           |\\n| pxm_Latn        | 1.76    | 2.15    |           |\\n| mal_Mlym        | 3.19    | 4.13    |           |\\n| swe_Latn        | 4.77    | 4.76    |           |\\n| ibo_Latn        | 2.05    | 2.43    |           |\\n| qug_Latn        | 2.44    | 2.99    |           |\\n| smo_Latn        | 2.26    | 2.72    |           |\\n| ind_Latn        | 4.06    | 5.00    |           |\\n| mai_Deva        | 1.79    | 2.02    |           |\\n| sah_Cyrl        | 2.31    | 3.01    |           |\\n| kor_Latn        | 0.92    |         |           |\\n| ltc_Hani        | 3.09    |         |           |\\n| ach_Latn        | 3.13    | 4.02    |           |\\n| amk_Latn        | 1.47    | 1.65    |           |\\n| xca_Latn        | 2.11    | 2.57    |           |\\n| qrl_Latn        | 1.62    | 1.94    |           |\\n| tam_Latn        | 2.59    |         |           |\\n| kia_Latn        | 2.92    | 3.27    |           |\\n| qwz_Latn        | 3.32    | 4.15    |           |\\n| jv_Latn         | 2.88    | 3.51    |           |\\n| xwa_Latn        | 2.29    | 2.99    |           |\\n| kow_Latn        | 1.88    | 2.49    |           |\\n| xca_Latn        | 2.11    | 2.57    |           |\\n| qwh_Latn        | 2.38    | 3.65    |           |\\n| xtr_Latn        | 2.43    | 3.11    |           |\\n| xcb_Latn       "}
{"id": "acl-2023-long-61", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|-----------------|---------|---------|-----------|\\n| 76x628          |         |         |           |\\n| 86x617          |         |         |           |\\n| 86x593          |         |         |           |\\n| 85x578          |         |         |           |\\n| 84x562          |         |         |           |\\n| 86x554          |         |         |           |\\n| 85x546          |         |         |           |\\n| 86x507          |         |         |           |\\n| 86x499          |         |         |           |\\n| 85x491          |         |         |           |\\n| 86x483          |         |         |           |\\n| 85x475          |         |         |           |\\n| 86x468          |         |         |           |\\n| 85x460          |         |         |           |\\n| 86x452          |         |         |           |\\n| 85x444          |         |         |           |\\n| 86x436          |         |         |           |\\n| 85x428          |         |         |           |\\n| 86x420          |         |         |           |\\n| 85x413          |         |         |           |\\n| 86x405          |         |         |           |\\n| 85x401          |         |         |           |\\n| 86x393          |         |         |           |\\n| 85x385          |         |         |           |\\n| 86x378          |         |         |           |\\n| 85x370          |         |         |           |\\n| 86x362          |         |         |           |\\n| 85x354          |         |         |           |\\n| 86x346          |         |         |           |\\n| 85x338          |         |         |           |\\n| 86x330          |         |         |           |\\n| 85x322          |         |         |           |\\n| 86x314          |         |         |           |\\n| 85x306          |         |         |           |\\n| 86x298          |         |         |           |\\n| 85x290          |         |         |           |\\n| 86x282          |         |         |           |\\n| 85x274          |         |         |           |\\n| 86x266          |         |         |           |\\n| 85x258          |         |         |           |\\n| 86x250          |         |         |           |\\n| 85x242          |         |         |           |\\n| 86x234          |         |         |           |\\n| 85x226          |         |         |           |\\n| 86x218          |         |         |           |\\n| 85x210          |         |         |           |\\n| 86x202          |         |         |           |\\n| 85x194          |         |         |           |\\n| 86x186          |         |         |           |\\n| 85x178          |         |         |           |\\n| 86x170          |         |         |           |\\n| 85x162          |         |         |           |\\n| 86x154          |         |         |           |\\n| 85x146          |         |         |           |\\n| 86x138          |         |         |           |\\n| 85x130          |         |         |           |\\n| 86x122          |         |         |           |\\n| 85x114          |         |         |           |\\n| 86x106          |         |         |           |\\n| 85x098          |         |         |           |\\n| 86x090          |         |         |           |\\n| 85x082          |         |         |           |\\n| 86x074          |         |         |           |\\n| 85x066          |         |         |           |\\n| 86x058          |         |         |           |\\n| 85x050          |         |         |           |\\n| 86x042          |         |         |           |\\n| 85x034          |         |         |           |\\n| 86x026          |         |         |           |\\n| 85x018          |         |         |           |\\n| 86x010          |         |         |           |\\n| 85x002          |         |         |           |\\n| 86x004          |         |         |           |\\n| 85x006          |         |         |           |\\n| 86x008          |         |         |           |\\n| 85x010          |         |         |           |\\n| 86x012          |         |         |           |\\n| 85x014          |         |         |           |\\n| 86x016          |         |         |           |\\n| 85x018          |         |         |           |\\n| 86x020          |         |         |           |\\n| 85x022          |         |         |           |\\n| 86x024          |         |         |           |\\n| 85x026          |         |         |           |\\n| 86x028          |         |         |           |\\n| 85x030          |         |         |           |\\n| 86x032          |         |         |           |\\n| 85x034          |         |         |           |\\n| 86x036          |         |         |           |\\n| 85x038          |         |         |           |\\n| 86x040          |         |         |           |\"}"}
{"id": "acl-2023-long-61", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Language-Script | XLM-R-B | XLM-R-L | Glot500-m |\\n|----------------|---------|---------|-----------|\\n| srd_Latn       | 87.2    | 66.6    | 5.4       |\\n| aka_Latn       | 86.7    | 74.1    | 14.2      |\\n| dyu_Latn       | 68.5    | 27.4    | 10.2      |\\n| ben_Beng       | 5.2     | 3.7     | 7.2       |\\n| mon_Latn       | 288     | 282.4   | 33.7      |\\n| nyy_Latn       | 628.5   | 198.3   | 18.0      |\\n| ajp_Arab       | 74.6    | 34.0    | 44.8      |\\n| gor_Latn       | 89.8    | 140.7   | 8.8       |\\n| tzh_Latn       | 320.3   | 82.8    | 4.7       |\\n| tdx_Latn       | 688.4   | 716.4   | 16.0      |\\n| kjb_Latn       | 110.8   | 81.1    | 16.2      |\\n| hne_Deva       | 80.1    | 60.3    | 9.1       |\\n| tpm_Latn       | 99.9    | 90.2    | 17.9      |\\n| lhu_Latn       | 44.7    | 12.3    | 2.0       |\\n| bel_Cyrl       | 3.4     | 2.5     | 2.5       |\\n| grc_Grek       | 10.1    | 10.4    | 3.4       |\\n| bos_Latn       | 6.1     | 3.4     | 7.9       |\\n| szl_Latn       | 46.4    | 30.2    | 3.1       |\\n| sxn_Latn       | 469.2   | 148.3   | 14.5      |\\n| lmo_Latn       | 48.4    | 25.9    | 6.1       |\\n| ksh_Latn       | 340.3   | 227.6   | 19.9      |\\n| cos_Latn       | 52.1    | 22.8    | 13.3      |\\n| mwn_Latn       | 697.8   | 543.8   | 30.7      |\\n| aym_Latn       | 1084.6  | 727.8   | 14.5      |\\n| ada_Latn       | 100     | 78.5    | 9.5       |\\n| sid_Latn       | 1003.6  | 782.3   | 34.5      |\\n| aoj_Latn       | 95.1    | 53.7    | 7.4       |\\n| pxm_Latn       | 101.3   | 120.7   | 2.7       |\\n| jam_Latn       | 213.3   | 195.2   | 15.8      |\\n| est_Latn       | 7.7     | 4.0     | 22.1      |\\n| xho_Latn       | 32.5    | 9.4     | 16.7      |\\n| bre_Latn       | 12.9    | 3.7     | 12.3      |\\n| kaa_Cyrl       | 72.9    | 29.2    | 8.8       |\\n| rop_Latn       | 150.7   | 93.4    | 8.4       |\\n| yua_Latn       | 246.8   | 55.1    | 4.6       |\\n| bim_Latn       | 142.2   | 97.3    | 11.3      |\\n| 'alz_Latn      | 511.9   | 145.6   | 47.7      |\\n| hrv_Latn       | 7.4     | 4.9     | 9.7       |\\n| tsc_Latn       | 726.3   | 501.1   | 17.0      |\\n| kwy_Latn       | 598.8   | 514.4   | 30.5      |\\n| jav_Latn       | 20.2    | 4.4     | 22        |\\n| mai_Deva       | 42.9    | 48.8    | 6.0       |\\n| ekk_Latn       | 7       | 3.8     | 7.9       |\\n| lao_Laoo       | 4.2     | 4.4     | 3.8       |\\n| tyv_Cyrl       | 104.1   | 104.4   | 7.3       |\\n| umb_Latn       | 920     | 838.8   | 17.4      |\\n| aze_Latn       | 5.6     | 3.6     | 5.4       |\\n| afb_Arab       | 68.7    | 44.4    | 55.9      |\\n| tam_Taml       | 7.2     | 2.3     | 3.8       |\\n| mya_Mymr       | 6.9     | 4.4     | 9.8       |\\n| twi_Latn       | 178.9   | 66.7    | 17.9      |\\n| toi_Latn       | 988.7   | 246.5   | 20.9      |\\n| ssw_Latn       | 345.7   | 108.4   | 20.2      |\\n| sme_Latn       | 293     | 368.2   | 6.5       |\\n| kon_Latn       | 463.7   | 418.9   | 16.3      |\\n| lus_Latn       | 493.5   | 131.2   | 16.4      |\\n| yom_Latn       | 468     | 240.7   | 43.1      |\\n| che_Cyrl       | 266.4   | 127.6   | 5.7       |\\n| krc_Cyrl       | 120.1   | 63.2    | 9.3       |\\n| tob_Latn       | 115     | 78.8    | 7.2       |\\n| gaa_Latn       | 109.3   | 33.3    | 13.5      |\\n| hbo_Hebr       | 6.3     | 3.6     | 5.6       |\\n| mxv_Latn       | 69.8    | 29.7    | 5.0       |\\n| tzo_Latn       | 246.5   | 54.3    | 7.0       |\\n| mgr_Latn       | 737.8   | 254.2   | 33.0      |\\n| ron_Latn       | 4.4     | 2.9     | 10.4      |\\n| ile_Latn       | 67.9    | 40.1    | 5.7       |\\n| cuk_Latn       | 211.5   | 72.1    | 32.0      |\\n| ara_Arab       | 10.1    | 6.3     | 18.8      |\\n| cce_Latn       | 468.3   | 123.5   | 22.5      |\\n| ces_Latn       | 4.4     | 3.1     | 11.6      |\\n| mar_Deva       | 7.5     | 4.6     | 11.2      |\\n| uzn_Cyrl       | 402.4   | 138.7   | 5.2       |\\n| rmy_Latn       | 288.2   | 349.8   | 25.0      |\\n| nba_Latn       | 638.8   | 675.1   | 14.6      |\\n| ibg_Latn       | 897.3   | 807.3   | 21.8      |\\n| mny_Latn       | 568.9   | 492.5   | 38.7      |\\n| hat_Latn       | 228     | 113.3   | 14.0      |\\n| glv_Latn       | 240.2   | 182.3   | 9.4       |\\n| run_Latn       | 817.5   | 218.5   | 16.9      |\\n| fij_Latn       | 377.3   | 96      | 12.8      |\\n| diq_Latn       | 256.6   | 120.5   | 13.4      |\\n| rus_Cyrl       | 3.3     | 2.3     | 4.5       |\\n| kbp_Latn       | 34.6    | 24.5    | 7.1       |\\n| poh_Latn       | 62.8    | 68.9    | 3.8       |\\n| hbs_Latn       | 4.5     | 2.6     | 6         |\\n| mlt_Latn       | 223     | 162.2   | 10.3      |\\n| oss_Cyrl       | 121.8   | 58.7    | 5.1       |\\n| lug_Latn       | 489     | 197.5   | 13.1      |\\n| kjh_Cyrl       | 209.8   | 88.8    | 16.4      |\\n| san_Deva       | 20.5    | 12.4    | 15.5      |\\n| ndo_Latn       | 892.3   | 178.1   | 21.1      |\\n| ote_Latn       | 127.8   | 71.2    | 8.0       |\\n| hif_Latn       | 21.6    | 46.7    | 13.5      |\\n| rar_Latn       | 458.1   | 50.2    | 12.1      |\\n| her_Latn       | 776     | 707.3   | 31.6      |\\n| tll_Latn       | 244.6   | 161     | 24.3      |\\n| ell_Grek       | 3.4     | 2.6     | 5.9       |\\n| efi_Latn       | 256.8   | 47      | 11.5      |\\n| crs_Latn       | 782.2   | 146.5   | 7.4       |\\n| tvl_Latn       | 634.1   | 378.5   | 7.1       |\\n| idu_Latn       | 117.7   | 90.9    | 12.0      |\\n| rng_Latn       | 656.6   | 606.8   | 11.7      |\\n| toj_Latn       | 287.1   | 113.6   | 9.6       |\\n| hye_Armn       |         |         | 3.6       |\\n| cjk_Latn       | 530.8   | 419.6   | 24.0      |\\n| ikk_Latn       | 67.8    | 49.5    | 8.6       |\\n| gcf_Latn       | 450.8   | 292.4   | 5.5       |\\n| seh_Latn       | 917.8   | 230     | 11.2      |\\n| ory_Orya       | 6.1     | 2.8     | 6.3       |\\n| pus_Arab       | 12.9    | 7.5     | 12.7      |\\n| nor_Latn       | 5       | 2.8     | 8.5       |\\n| sgs_Latn       | 119.2   | 124.7   | 10.5      |\\n| hau_Latn       | 14.5    | 7.1     | 17.2      |\\n| enm_Latn       | 43.1    | 31.0    | 36.6      |\\n| mbb_Latn       | 177.1   | 138     | 4.2       |\\n| uzb_Latn       | 5.6     | 3.6     | 5.8       |\\n| arz_Arab       | 17.5    | 1.5     | 6.8       |\\n| som_Arab       | 7.2     | 3.1     | 9.3       |\\n| bem_Latn       | 706.9   | 219.9   | 27.1      |\\n| hsb_Latn       | 109.6   | 103.6   | 5.2       |\\n| vep_Latn       | 218.1   | 111.5   | 6.1       |\\n| gkp_Latn       | 33.1    | 30.2    | 12.7      |\\n| ary_Arab       | 32.7    | 4.6     | 26        |\\n| guj_Gujr       | 6.2     | 3.6     | 6.5       |\\n| hmo_Latn       | 509.3   | 77.7    | 10.9      |\\n| azj_Latn       | 5.3     | 3.3     | 5.1       |\\n| tbz_Latn       | 39.2    | 40.4    | 8.4       |\\n| quw_Latn       | 177.8   | 157.7   | 26.1      |\\n| cac_Latn       | 51.4    | 39.3    | 7.0       |\\n| ven_Latn       | 268.3   | 62      | 9.4       |\\n| pag_Latn       | 923.5   | 232.4   | 25.8      |\\n| npi_Deva       | 8.6     | 4.9     | 7.3       |\\n| crh_Latn       | 151     | 70.9    | 6.5       |\\n| ber_Latn       | 639.1   | 981.4   | 21.3      |\\n| lin_Latn       | 377.3   | 96.6    | 15.3      |\\n| xmv_Latn       | 593.2   | 491.4   | 19.4      |\\n| chk_Latn       | 766.9   | 151.6   | "}
