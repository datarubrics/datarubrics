{"id": "lrec-2024-main-1460", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tree-Instruct: A Preliminary Study of the Intrinsic Relationship between Complexity and Alignment\\n\\nYingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu, Minghao Li, Fei Huang, Nevin L. Zhang, Yongbin Li\\nThe Hong Kong University of Science and Technology, Alibaba Group\\n{yzhaocx,lzhang}@connect.ust.hk\\n{yubowen.ybw,binyuan,yifei,liminghao,f.huang,shuide}@alibaba-inc.com\\n\\nAbstract\\n\\nTraining large language models (LLMs) with open-domain instruction data has yielded remarkable success in aligning to end tasks and human preferences. Extensive research has highlighted the importance of the quality and diversity of instruction data. However, the impact of data complexity, as a crucial metric, remains relatively unexplored from three aspects: (1) where the sustainability of performance improvements with increasing complexity is uncertain; (2) whether the improvement brought by complexity merely comes from introducing more training tokens; and (3) where the potential benefits of incorporating instructions from easy to difficult are not yet fully understood. In this paper, we propose Tree-Instruct to systematically enhance the instruction complexity in a controllable manner. By adding a specified number of nodes to instructions' semantic trees, this approach not only yields new instruction data from the modified tree but also allows us to control the difficulty level of modified instructions. Our preliminary experiments reveal the following insights: (1) Increasing complexity consistently leads to sustained performance improvements of LLMs. (2) Under the same token budget, a few complex instructions outperform diverse yet simple instructions. (3) Curriculum instruction tuning might not yield the anticipated results; focusing on increasing complexity appears to be the key. Code and data are released: https://github.com/xiuzbl/Tree-Instruct.\\n\\nKeywords: controllable instruction evolution, instruction complexity, large language models\\n\\n1. Introduction\\n\\nThe latest generation of large language models (LLMs) has attracted significant attention due to their immense potential in language technologies (OpenAI, 2022; Touvron et al., 2023; Wei et al., 2023; Li et al., 2023). To enhance interactive user requests and chat interfaces, these models undergo instruction-tuning using supervised input-output pairs (Iyer et al., 2022; Jang et al., 2023; Chung et al., 2022). This process enables the model to comprehend the required style and format for effective user interaction, showcasing the knowledge and capabilities gained during pretraining (Ouyang et al., 2022).\\n\\nConsequently, the efficacy of instruction data significantly influences LLMs' abilities, shaping users' perceptions of their capabilities (Wang et al., 2023g; K\u00f6pf et al., 2023; Chiang et al., 2023). Recently, LIMA has demonstrated that with just 1000 carefully curated prompts and responses, an LLM can achieve remarkably strong performance (Zhou et al., 2023). This suggests that the scaling laws of instruction tuning are not solely dependent on data quantity but rather influenced by prompt diversity and quality. However, one critical and less-explored aspect of evaluating instruction data is complexity. There are at least three unanswered questions related to complexity: (1) The scaling law of instruction complexity: Intuitively, more complex instruction data might elicit more potential capabilities in LLMs to address intricate problems (Luo et al., 2023; Mukherjee et al., 2023). WizardLM (Xu et al., 2023) introduce in-depth and in-bread the volving method to write prompts into more complex and diverse versions, resulting in a 12.4% increase in LLMs' win rate with the same amount of data. Yet, whether WizardLM's performance improvement is due to complexity or merely derived from diversity remains uncertain. Moreover, the ongoing enhancements in complexity are yet to be explored. (2) The relationship between complexity-induced performance improvement and token quantity: Enhancing instance complexity inevitably increases the number of tokens per instance (Dai et al., 2021). While WizardLM exhibits performance improvements with the same instance quantity, it increases the number of tokens per instance. This raises the question of whether complexity-induced improvement in LLMs results from increased training tokens. As known, enlarging LLMs' pretraining token counts can lead to better performance (Muennighoff et al., 2023; Tay et al., 2022). (3) The effectiveness of complexity-based curriculum instruction learning: Curriculum learning is a strategy in machine learning that starts with easy instances and gradually introduces harder ones (Bengio et al., 2009). Its effectiveness has been demonstrated in various NLP tasks like machine translation (Zhou et al., 2020), dialogue systems (Zhu et al., 2021), and question answering.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, its potential efficacy in instruction tuning is still under-explored. However, to answer the aforementioned questions, the key hurdle lies in finding a controlled way to increase the complexity of instruction data without introducing unwanted factors such as diversity. WizardLM (Xu et al., 2023) employs an in-depth evolving prompt like \\\"Your objective is to rewrite a given prompt into a more complex version to make ChatGPT and GPT4 a bit harder to handle.\\\" to complicate the existing instructions. Unfortunately, although intended to enhance complexity, this approach might inadvertently introduce diversity by diverging from the initial instruction objectives. This issue becomes particularly severe when repeatedly employing in-depth evolving to achieve varying levels of complexity. We study and analyze the instructions before and after in-depth evolving in Sec. 4.1. As illustrated in Fig. 2, the iteratively evolved instructions append additional objectives that deviate from the original instructions, showcasing a greater diversity.\\n\\nTo address this concern, we propose Tree-Instruct, which involves prompting LLMs to add a specific number of new nodes to the semantic tree of an existing instruction, as opposed to manipulating the text sequence directly, as done in Self-Instruct (Wang et al., 2022a) or WizardLM (Xu et al., 2023). We use the number of added nodes to represent the introduced level of complexity. The advantage of this approach lies in the fact that semantic tree nodes lack any sequential order (Shiv and Quirk, 2019). By enforcing LLMs to operate on the semantic tree, this process becomes analogous to inserting new words into the middle of the original instructions. This compels the models to complicate while adhering to the structural constraints of the initial instruction rather than merely appending new instructions. It can significantly mitigate the issue of straying from the primary theme of the initial instruction. We leverage GPT-4 to assess the consistency of evolved instructions with original ones, and the results verify that Tree-Instruct improves WizardLM's consistency score from 0.56 to 0.69. Fig. 1 highlights how the number of added nodes raises the complexity level of the samples.\\n\\nWith the help of Tree-Instruct, we have obtained the following preliminary experimental conclusions:\\n\\n1. As the complexity of the instruction data increases, the benefits of instruction tuning continue to grow: Following LIMA (Zhou et al., 2023), we attempt instruction tuning using 1,000 samples from Alpaca-GPT-4 as a base. We add 3, 6, and 10 nodes to the semantic tree of each sample, resulting in performance gains of 13%, 20%, and 26%, respectively, across eight sub-skills such as commonsense, writing, and coding, showing consistent improvements. Furthermore, this scaling law can be extended to more complex instruction data. For instance, when fine-tuning around 6,000 conversations filtered from ShareGPT via Open-Chat (Wang et al., 2023a) (showing excellent performance in the open-source LLMs), we observe that by increasing the complexity through Tree-Instruct to around 1,100 users' instructions, the winning rate increases from 84.56% to 86.19% for AlpacaEval benchmark.\\n\\n2. The increase in complexity partly comes from additional tokens, but a few complex instructions outperform diverse yet simple instructions, under the same token budget. We find that as the complexity increases, the number of tokens also increases. Adding ten nodes in the semantic tree increases the average token length of instructions from 186 to 607. Hence, to make a fair comparison, we increase the number of original instructions from 1,000 to 4,000 to match the total token quantity of our tree-instructed samples. Under this setting, the performance gain from adding ten nodes still achieves more than 15%. This indicates that the improvement due to complexity is partly attributed to the increased tokens, but increasing the complexity of samples is equivalent to the diversity achieved by four times the token count of simple samples. Moreover, with an equal number of instruction tokens, the evolution of Tree-Instruct yields a 2% higher win rate than those from three iterations of in-depth evolution of WizardLM, demonstrating the superior efficacy of Tree-Instruct in enhancing complexity.\\n\\n3. Curriculum instruction tuning may not be effective; increasing complexity is all you need: We implement curriculum learning by progressively training the LLM on increasing levels of difficulty. Initially, we train it on easy-level data with an addition of three nodes, followed by medium-level data with six nodes, and ultimately on hard data with ten nodes added. We observe that, when given the same number of training steps, curriculum learning does outperform training with a mixed difficulty of samples but still falls short compared to training solely on the added ten-node instruction data. This indicates that, as sample complexity increases, the significance of simpler samples diminishes significantly, suggesting that repeating training with complex samples may be sufficient.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The scaling law of instruction complexity. We experiment with enhancing the complexity of semantic trees for 1,000 Alpaca instructions by adding extra 3, 6, and 10 nodes. We then evaluate models fine-tuned on instruction data of varying complexities against text-davinci003 in terms of win rate on AlpacaEval (Left). Additionally, we examine win rates on different subsets of AlpacaEval (Right). In the left figure, we indicate above the stars the average token count for instructions of different complexity levels. We also use WizardLM's in-depth deepening as the baseline.\\n\\nare not without their limitations. These limitations encompass potential misunderstandings of human instructions, the propensity to generate biased content, and the sporadic generation of hallucinated information. Consequently, bringing LLMs in line with human expectations has become a central focal point within the research community (Bai et al., 2022; Song et al., 2023).\\n\\nTo attain this alignment, researchers need to amass high-quality instructional data that authentically mirrors human needs and expectations. A rational starting point for data collection involves the adaptation of existing NLP benchmarks into natural language instructions, like T0 (Sanh et al., 2021), PromptSource (Bach et al., 2022), SupernaturalInstruction (Wang et al., 2022b), UnnaturalInstructions (Honovich et al., 2022) and FLAN (Wei et al., 2021; Longpre et al., 2023) are spearheading this strategy. These benchmarks encompass a wide range of NLP tasks, spanning dialogue, reasoning, and coding, all unified under the realm of language instructions. T\u00dcLU(Wang et al., 2023e) showcases that instructions from NLP tasks significantly bolster the reasoning prowess of aligned LLMs, where the diversity of tasks plays a pivotal role in shaping the capabilities of LLMs.\\n\\nNevertheless, a notable trend in NLP datasets is their propensity to emphasize particular skills, consequently yielding instructions that possess a somewhat confined scope. This constraint has the potential to impede their capacity to meet the intricate requirements of real-world applications. In order to tackle these challenges, one possible approach is to formulate instructions via purposeful human annotations. An exemplary precursor to such a corpus is OpenAssistant (K\u00f6pf et al., 2023), which comprises over 10k dialogues involving the participation of 13k annotators from around the world. Another remarkable venture into harnessing human-generated instructions through crowdsourcing is ShareGPT. This platform encourages users to contribute and exchange their engaging conversations with ChatGPT and GPT4. While human annotation ensures both quality and diversity, it becomes challenging to ensure the quantity and complexity of instructional data due to the highly expensive annotation process (Chen et al., 2023c), and the distribution of difficulty levels in human-created instructions tends to skew towards being either easy (Luo et al., 2023). To address this issue, Self-Instruct (Wang et al., 2022a) leverages ChatGPT's in-context learning capability to generate a large volume of instructions from a predefined set of human-annotated instructions spanning diverse topics and task types. Building upon this foundation, LIMA (Zhou et al., 2023) and Alpagasus (Chen et al., 2023a) separately validate the significant impact of data diversity and quality on instructional effectiveness. The selection of thousands of high-quality and diverse instructional examples proves more advantageous in achieving better results compared to using the entire dataset. Further increasing the number of instructions could potentially induce a semantic shift in the LLMs (Al-Shikh et al., 2023). Up to this point, three key metrics within the instructional data\u2014diversity, quality, and quantity\u2014have been elucidated for their impact.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While WizardLM (Xu et al., 2023) demonstrates that evolving both the complexity and diversity of instructions can lead to performance enhancement, it does not deeply investigate the individual importance of complexity. This paper introduces a method, Tree-Instruct, which enhances instructional complexity while simultaneously constraining thematic consistency to mitigate variations in diversity. Our experiments preliminarily establish a scaling law regarding complexity, show that the improvement resulting from increased complexity isn't solely due to the introduction of more training tokens and illustrate that LLMs only require complex samples for instruction tuning, rather than simple samples serving as foundational padding for curriculum learning.\\n\\n3. Tree-Instruct\\n\\nEnhancing the complexity of natural language text seems like a straightforward task for proficient LLMs. For instance, WizardLM utilizes a simple text prompt to complexify instructions as mentioned in Sec. 1. However, due to the extensive pre-training of LLMs on massive corpora, where models predict the next token based on the preceding context, we've noticed that LLMs can often exploit the given instruction by simply continuing the text beyond the initial prompt to artificially amplify complexity. While adding continuation constraints can enhance the complexity of instructions, it simultaneously leads them away from the core thematic focus. This divergence expands the topic and domain, fostering diversity that hinders our ability to solely assess the impact of increased instruction complexity. We leverage GPT-4 to automatically score the consistency (range in $0 \\\\sim 1$) of the instructions before and after implementing in-depth deepening following WizardLM. We found that it only gets a 0.56 alignment score. Furthermore, upon iteratively enhancing the instruction's complexity, the guidance might become ineffective, losing its original essence. For instance, it might cease to present a question, rendering it arduous for the LLM to generate a suitable response. This phenomenon matches with observations made by WizardLM, which prompts them to introduce the Elimination Evolving procedure.\\n\\nTo address this issue, we first consider what determines the complexity of natural language text. In linguistics and education, there is a lack of precise scientific consensus on determining the complexity of the text. No single source can precisely summarize a text's complexity. Currently, a widely accepted perspective suggests that qualitative measures of text complexity require an informed judgment of text difficulty based on various factors. The standards use factors like purpose, levels of meaning, structure, language conventions, clarity, and knowledge demands to measure text difficulty. Among these, text structure is a more measurable indicator, as we can convert text sequences into tree structures using mature dependency or semantic tree parsers (Solovyev et al., 2019). Tree structures, prevalent in natural language representations, offer structural insights reflecting human text comprehension (Hancke et al., 2012). Furthermore, we can gauge text complexity accurately by measuring the width and depth of trees, as a deeper and wider grammar tree signifies more intricate sentence structures (Chevalier et al., 2007; Wang et al., 2013).\\n\\nInspired by the concept of tree complexity, we propose Tree-Instruct, wherein LLMs directly add a specific number of nodes to the semantic tree of an instruction. This increases the tree's width and depth, thereby enhancing text structure complexity. In detail, Tree-Instruct encompasses three steps:\\n\\n1. **Step 1: Tree Construction** involves semantic parsing, where a structured representation is created from a natural language sentence. This process yields a tree structure for an instruction. For instance, given the instruction \\\"Implementing effective strategies to curb environmental pollutants in the atmosphere\\\", we derive an original tree structure Tree-1 as shown in the first tree of Fig. 2.\\n\\n2. **Step 2: Nodes Expansion** operates on the acquired tree structure, expanding it in depth or width by adding new nodes, thus influencing the new tree's complexity. We only add meaningful nodes representing nouns or verbs, since words like adjectives or prepositions contribute little to tree complexity. The second tree in Fig. 2 illustrates Tree-2 after adding ten nodes.\\n\\n3. **Step 3: Tree Sentenceization** aims to make LLMs revert the complex new tree structure (Tree-2) back to fluent natural language instruction by introducing connected words. Additionally, we present all three steps into a single prompt, guiding LLMs to implement our requirements step by step without external semantic parsing tools (see Block 3, where \\\"your_added_number\\\" indicates the desired number of nodes we aim to add to the tree.) Especially we directly control the complexity by adjusting \\\"your_added_number\\\". Visually, with more nodes added, the tree and the instruction become more complex. This gradual increase results in a tree with 3, 6, or 10 additional nodes, progressively increasing the complexity of instructions, as shown in Fig. 2. We also observe that adding nodes to the semantic tree constructs a framework for the original instruction. This approach prevents significant\\n\\n---\\n\\n[https://www.generationready.com/wp-content/uploads/2021/04/Beginners-Guide-to-Text-Complexity.pdf](https://www.generationready.com/wp-content/uploads/2021/04/Beginners-Guide-to-Text-Complexity.pdf)\"}"}
{"id": "lrec-2024-main-1460", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implementing effective strategies to curb environmental pollutants in the atmosphere at different altitudes by reducing emissions from industrial sources like factories and vehicles. Additionally, monitor these emissions using specialized equipment and stringently enforce regulations to ensure industries adhere to best practices and environmental standards.\\n\\nThis approach is crucial for ensuring sustainable socio-economic progress while actively safeguarding and nurturing our planet's delicate ecological balance.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are an instruction rewriter. You need to rewrite a given user instruction following Procedures step by step. You MUST ONLY return the NEW instruction you rewrite.\\n\\nProcedure:\\n\\nstep-1: Parse the old \\\"instruction\\\" to a TREE-1 through Semantic Parsing in the natural language processing field.\\nstep-2: EXPAND the above NEW TREE-1 from depth or width by adding \\\"your_added_number\\\" meaningful NEW Nodes as nouns or verbs to form a NEW TREE-2. The new nodes should be constructed with detailed and pertinent information.\\nstep-3: Generate a totally NEW \\\"instruction\\\" based on the expanded NEW TREE-2.\\n\\nOld instruction: \\\"your_instruction\\\"\\n\\nNew instruction:\\n\\ncess, thereby increasing the complexity of each instruction within Alpaca-1K. In order to analyze the scaling law, we introduce three levels of complexity by augmenting the instructions by adding 3, 6, and 10 additional nodes to the semantic tree of original instructions, respectively. This allows us to observe the impact of these varying complexities on the outcomes. For the modified instructions, we employed gpt-4 once again to generate corresponding responses. To validate our findings, we replicate the results by applying the in-depth evolving with deepening prompt provided by WizardLM to the same Alpaca-1K instructions.\\n\\nTo demonstrate the scalability of our discoveries to larger datasets, we also conduct experiments on the extensive OpenChat dataset, which comprises 6,206 conversations between humans and GPT4, filtered from ShareGPT (Wanget al., 2023a). We employ the pre-trained LLaMA2 (Touvron et al., 2023) model as the initialization, fine-tuning it on instruction-tuning datasets generated through different methods. Each GPU processes batches of size 2 (for OpenChat evolved data, the batch size is set to 8), and the maximum sequence length was set to 4096. For optimization, we adopt the AdamW (Loshchilov and Hutter, 2017) optimizer with a learning rate of 1e-4 and a weight decay of 0.1, following the practices established by OpenChat. Using Deepspeed ZeRO-2 stage and 8 A100 GPUs, we train LLaMA2 for 10 epochs on Alpaca-1K and 5 epochs on OpenChat-6K data. During inference, a temperature of 0.7 and a top-p value of 0.9 are employed to evaluate all the methods under comparison.\\n\\nEvaluation Benchmarks\\n\\nWe mainly evaluate competing methods on two benchmarks: AlpacaEval and MT-Bench (Zhenget al., 2023). AlpacaEval is an LLM-based automatic evaluation, comprising 805 diverse samples, each showcasing various abilities. In AlpacaEval, responses from different LLM methods are then compared to those from text-davinci003 by GPT-4 auto-annotator. MT-Bench consists of 80 high-quality multi-turn questions to test multi-turn conversation and instruction-following ability, covering 8 common categories.\\n\\n4.1. Tree-Instruct is Better for Instruction Complexity Evolution\\n\\nWe start by investigating whether operating on a tree, as opposed to a sequence, better aligns with the intended objectives of the original instruction. Recent studies have introduced the LLMs-as-evaluator paradigm, leveraging LLMs to assess candidate samples, which closely approximates human evaluative agreement (Chen et al., 2023b; Fu et al., 2023; Ji et al., 2023; Zhang et al., 2023). Consequently, we employ gpt-4 to gauge which approach exhibits greater consistency with the initial instructions. As depicted in Figure 3, the result indicates that employing Tree-Instruct, which entails adding instructions with 6 additional nodes, achieves a higher degree of alignment with the original instructions in 63% of cases, compared to WizardLM\u2019s in-depth deepening that undergoes modifications and generates instructions with similar token quantities to Tree-6-nodes. This observation serves as evidence that the presence of a tree structure constraint enables LLMs to more effectively modify instructions within the framework of the original guidance, rather than diverging and incorporating unrelated content. A case study in Fig. 2 also indicates that WizardLM might produce phrases deviated from the original instruction during iterative evolution.\\n\\nFigure 3: GPT-4\u2019s comparison for the consistency preservation between Tree-6-Nodes and WizardLM\u2019s in-depth deepening with respect to the original instructions.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Analysis of the relationship between instruction complexity and LLM's ability. The numbers represent the win rates vs. text-davinci003 on various subsets of AlpacaEval and MT-Bench scores.\\n\\n| Method          | Win Rate | Total Token Size |\\n|-----------------|----------|------------------|\\n| Alpaca-1K       | 46.02    | 186,464          |\\n| Alpaca-4K       | 55.85    | 757,042          |\\n| WizardLM        | 64.20    | 556,981          |\\n| Tree-3-Nodes    | 59.53    | 385,760          |\\n| Tree-6-Nodes    | 66.38    | 546,731          |\\n| Tree-10-Nodes   | 72.66    | 608,556          |\\n\\nTree-Instruct exhibits a winrate improvement of 2.2 points over WizardLM's in-depth deepening. We attribute this enhancement to Tree-Instruct's adeptness at closely tailoring instructions to the central topic, thereby introducing complexity without deviation.\\n\\nIn contrast, in-depth evolving might deviate from the original theme and introduce irrelevant content, resulting in instructions of inadequate difficulty. Such instructions could potentially hinder LLMs from generating appropriate responses, rendering them less effective in the generation process.\\n\\nFigure 4: Evaluation of models trained on Alpaca-1K and evolved by adding various nodes vs. text-davinci003 on categories of the Vicuna test set.\\n\\n4.2. More Complexity, Better Capability\\n\\nAfter demonstrating the effectiveness of Tree-Instruct in enhancing sample complexity, we present a scaling law pertaining to complexity, as depicted in Fig. 1 and Table 1. As the number of nodes gradually increases from Tree-3-Nodes to Tree-6-Nodes and further to Tree-10-Nodes, the model's win rate on the AlpacaEval and scores on MT-Bench benchmarks exhibit a remarkable upward trend. This scaling law underscores the significance of complexity within instruction data.\\n\\nAdditionally, we carry out a meticulous evaluation for each skill/category within the Vicuna test sets. These sets are divided into distinct skill sets/categories, allowing for an intricate analysis of the proficiency attained through instruction tuning. Notably, Tree-10-Nodes outperforms Tree-6-Nodes across a majority of categories, encompassing Counterfactual, Roleplay, Knowledge, Generic, and more. Similar trends are evident when comparing Tree-6-Nodes with the original instructions, indicating that augmenting the complexity of instruction data leads to a comprehensive enhancement in the capabilities of the LLM.\\n\\nFinally, given that our experimentation is based on 1,000 instances, we extend our investigation to\"}"}
{"id": "lrec-2024-main-1460", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3. Less but Complex is Better Than More but Simple\\n\\nWhile we have demonstrated that increasing the complexity of instruction data can enhance the capabilities of LLMs, a new question arises: Is this improvement merely due to the introduction of more training tokens as complexity increases? Our analysis indicates that the average length of the original Alpaca data, combining both input and output, is 186 tokens. Upon incorporating an additional 10 nodes, this count escalates to 607 tokens \u2013 equivalent to a 3.26-fold increase in training data. With this question in mind, we introduce a new baseline: Alpaca-4K, trained with 4,000 samples (additionally sampled 3,000 instances from the original Alpaca data). As shown in Table 2, the total token count of Alpaca-4K surpasses that of Tree-10-Nodes by 24%. Despite this, with the same training steps, a significant 16.8% performance gap remains. Compared to Alpaca-1K, there is indeed a 9.8% improvement. This suggests that introducing more instruction tokens does enhance model performance. Nonetheless, the effectiveness of diverse yet simple instructions still falls short compared to a smaller quantity of more complex directives.\\n\\n4.4. Curriculum Learning May Be Not Effective for Instruction Tuning\\n\\nNow, armed with three sets of data featuring increasing difficulty levels and aligned themes, we can delve into an unanswered question in instruction tuning: Is it necessary to train LLM progressively from easy to hard? As depicted in Table 4, we embark on a trial, initially training on Tree-3-Nodes data, followed by Tree-6-Nodes, and finally Tree-10-Nodes. Each segment constitutes one-third of the total training steps. We also devise two baselines: one involving the combined training of all three difficulty levels and another wherein difficult samples are trained prior to the easy ones.\\n\\nExperimental results reveal that, compared to mixed-difficulty training and training samples from hard to easy, an easy-to-hard curriculum learning approach truly enhances model performance. However, the performance gain from curriculum learning still slightly underperforms exclusively training on Tree-10-Nodes, the hardest dataset we construct. This outcome slightly contrasts with previous observations of curriculum learning. We attribute this variance to the fact that modern LLMs possess parameter counts several times larger than those of earlier models like BERT (Devlin et al., 2019) or T5 (Raffel et al., 2020). With this substantial parameter increase, LLMs are now capable of directly learning from challenging samples, diminishing the need for foundational exposure to simpler samples. The more exposure to challenging samples, the more the model's capabilities are ignited.\\n\\n5. Conclusion\\n\\nIn this study, we have undertaken a preliminary exploration of the intrinsic relationship between instruction complexity and the ability of large language models to follow human instructions. We propose a controllable method for complicating instructions: Tree-Instruct. We conduct extensive experiments to explore the unknown. The results...\"}"}
{"id": "lrec-2024-main-1460", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reveal the following insights: (1) As the complexity of the instruction data increases, the benefits of instruction tuning continue to amplify. (2) The rise in complexity is partly attributed to additional tokens, yet a few intricate instructions outperform a large number of simpler instructions, all within the same token limit. (3) A curriculum-based instruction tuning, progressing from easier to harder, might not yield the desired effectiveness; embracing increased complexity proves essential. We anticipate that this exploration will supplement existing knowledge regarding the aspects of quality, quantity, diversity, and complexity of instruction data. This contribution aims to assist future researchers in constructing superior instruction data.\\n\\n6. Limitations\\nWe have conducted extensive experiments to study three unexplored aspects targeting the complexity of instruction-turning data: (1) the scaling law, (2) the impact of additional training tokens, and (3) curriculum learning. Kindly notice that, in Tree-Instruct, responses adapted to original instructions are no longer capable of fulfilling the needs of complex instructions, which means that users need to regenerate responses. Moreover, due to constraints in time and resources, such as computing resources or accessing GPT-4, there are still some unexplored questions: 1. Will instruction evolution ever reach a point of convergence? 2. How can we adapt Tree-Instruct for complex tasks, such as mathematics or coding, that require intricate reasoning? 3. Are larger language models still sensitive to the complexity of instruction-tuning data? We plan to delve into these areas in our subsequent research.\\n\\n7. Ethical Statement\\nThis paper studies the complexity of instruction-tuning data for large language models and proposes a novel method Tree-Instruct, to control the complexity evolution. All the datasets and models involved in this paper are publicly available. The prompts we design are also harmless and unbiased, leading to healthy and objective training data. There are no direct ethical concerns in our study.\\n\\n8. Bibliographical References\\nWaseem AlShikh, Manhal Daaboul, Kirk Goddard, Brock Imel, Kiran Kamble, Parikshith Kulkarni, and Melisa Russak. 2023. Becoming self-instruct: introducing early stopping criteria for minimal instruct tuning. arXiv preprint arXiv:2307.03692.\\n\\nAnthony McEnery and others. 2004. The EMILLE/CIIL Corpus. EMILLE (Enabling Minority Language Engineering) Project. distributed via ELRA: ELRA-Id W0037, ISLRN 039-846-040-604-0.\\n\\nStephen H Bach, Victor Sanh, Zheng-Xin Yong, Albert Webson, Colin Raffel, Nihal V Nayak, Abheesht Sharma, Taewoon Kim, M Saiful Bari, Thibault Fevry, et al. 2022. Promptsource: An integrated development environment and repository for natural language prompts. arXiv preprint arXiv:2202.01279.\\n\\nYuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862.\\n\\nAnthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2018. Identifying and controlling important neurons in neural machine translation. arXiv preprint arXiv:1811.01157.\\n\\nDavid Bau, Jun-Yan Zhu, Hendrik Strobelt, Agata Lapedriza, Bolei Zhou, and Antonio Torralba. 2020. Understanding the role of individual units in a deep neural network. Proceedings of the National Academy of Sciences, 117(48):30071\u201330078.\\n\\nYoshua Bengio, J\u00e9r\u00f4me Louradour, Ronan Collobert, and Jason Weston. 2009. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 41\u201348.\\n\\nAdithya Bhaskar, Alex Fabbri, and Greg Durrett. 2023. Prompted opinion summarization with gpt-3.5. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9282\u20139300.\\n\\nBSI. 1973a. Natural Fibre Twines, 3rd edition. British Standards Institution, London. BS 2570.\\n\\nBSI. 1973b. Natural fibre twines. BS 2570, British Standards Institution, London. 3rd. edn.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Castor and L. E. Pollux. 1992. The use of user modeling to guide inference and learning. *Applied Intelligence*, 2(1):37\u201353.\\n\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. *arXiv preprint arXiv:2307.03109*.\\n\\nLichang Chen, Shiyang Li, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Jay Srinivasan, Tianyi Zhou, Heng Huang, et al. 2023a. Alpagasus: Training a better alpaca with fewer data. *arXiv preprint arXiv:2307.08701*.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. *arXiv preprint arXiv:2107.03374*.\\n\\nYi Chen, Rui Wang, Haiyun Jiang, Shuming Shi, and Ruifeng Xu. 2023b. Exploring the use of large language models for reference-free text quality evaluation: A preliminary empirical study.\\n\\nZhihong Chen, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, et al. 2023c. Phoenix: Democratizing chatgpt across languages. *arXiv preprint arXiv:2304.10453*.\\n\\nJ.L. Chercheur. 1994. *Case-Based Reasoning*, 2nd edition. Morgan Kaufmann Publishers, San Mateo, CA.\\n\\nFanny Chevalier, David Auber, and Alexandru Telea. 2007. Structural analysis and visualization of C++ code evolution using syntax trees. In *Ninth international workshop on Principles of software evolution: in conjunction with the 6th ESEC/FSE joint meeting*, pages 90\u201397.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna.lmsys.org (accessed 14 April 2023).\\n\\nN. Chomsky. 1973. Conditions on transformations. In *A festschrift for Morris Halle*, New York. Holt, Rinehart & Winston.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. *arXiv preprint arXiv:2210.11416*.\\n\\nYinpei Dai, Hangyu Li, Yongbin Li, Jian Sun, Fei Huang, Luo Si, and Xiaodan Zhu. 2021. Preview, attend and review: Schema-aware curriculum learning for multi-domain dialog state tracking. *arXiv preprint arXiv:2106.00291*.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. GLM: General language model pretraining with autoregressive blank infilling. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 320\u2013335.\\n\\nUmberto Eco. 1990. *The Limits of Interpretation*. Indian University Press.\\n\\nJinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore: Evaluate as you desire. *arXiv preprint arXiv:2302.04166*.\\n\\nValentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia Schmid. 2020. Multi-modal transformer for video retrieval. In *Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part IV* 16, pages 214\u2013229. Springer.\\n\\nJulia Hancke, Sowmya Vajjala, and Detmar Meurers. 2012. Readability classification for German using lexical, syntactic, and morphological features. In *Proceedings of COLING 2012*, pages 1063\u20131080.\\n\\nYaru Hao, Li Dong, Furu Wei, and Ke Xu. 2019. Visualizing and understanding the effectiveness of bert. *arXiv preprint arXiv:1908.05620*.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 770\u2013778.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multi-task language understanding. *arXiv preprint arXiv:2009.03300*.\\n\\nPaul Gerhard Hoel. 1971a. *Elementary Statistics*, 3rd edition. Wiley series in probability and mathematical statistics. Wiley, New York, Chichester. ISBN 0 471 40300.\\n\\nPaul Gerhard Hoel. 1971b. *Elementary Statistics*, 3rd edition, Wiley series in probability and mathematical statistics, pages 19\u201333. Wiley, New York, Chichester. ISBN 0 471 40300.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. 2022. Unnatural instructions: Tuning language models with (almost) no human labor. arXiv preprint arXiv:2212.09689.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, et al. 2023. C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. arXiv preprint arXiv:2305.08322.\\n\\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru, Todor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster, Tianlu Wang, Qing Liu, Punit Singh Koura, et al. 2022. Opt-iml: Scaling language model instruction meta learning through the lens of generalization. arXiv preprint arXiv:2212.12017.\\n\\nSiddhartha Jain, Xiaofei Ma, Anoop Deoras, and Bing Xiang. 2023. Self-consistency for open-ended generations. arXiv preprint arXiv:2307.06857.\\n\\nJoel Jang, Seungone Kim, Seonghyeon Ye, Doyoung Kim, Lajanugen Logeswaran, Moontae Lee, Kyungjae Lee, and Minjoon Seo. 2023. Exploring the benefits of training expert language models over instruction tuning. arXiv preprint arXiv:2302.03202.\\n\\nOtto Jespersen. 1922. Language: Its Nature, Development, and Origin. Allen and Unwin.\\n\\nYunjie Ji, Yan Gong, Yiping Peng, Chao Ni, Peiyan Sun, Dongyu Pan, Baochang Ma, and Xiangang Li. 2023. Exploring chatgpt's ability to rank content: A preliminary study on consistency with human preferences.\\n\\nAndrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078.\\n\\nKhalid Choukri and Niklas Paullson. 2004. The OrienTel Moroccan MCA (Modern Colloquial Arabic) database. distributed via ELRA: ELRA-Id ELRA-S0183, ISLRN 613-578-868-832-2.\\n\\nTom Kocmi and Christian Federmann. 2023. Large language models are state-of-the-art evaluators of translation quality. arXiv preprint arXiv:2302.14520.\\n\\nAndreas K\u00f6pf, Yannic Kilcher, Dimitri von R\u00fctte, Sotiris Anagnostidis, Zhi-Rui Tam, Keith Stevens, Abdullah Barhoum, Nguyen Minh Duc, Oliver Stanley, Rich\u00e1rd Nagyfi, et al. 2023. Openassistant conversations\u2013democratizing large language model alignment. arXiv preprint arXiv:2304.07327.\\n\\nHyungtae Lee and Heesung Kwon. 2017. Going deeper with contextual cnn for hyperspectral image classification. IEEE Transactions on Image Processing, 26(10):4843\u20134855.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nMinghao Li, Feifan Song, Bowen Yu, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. Api-bank: A benchmark for tool-augmented llms. arXiv preprint arXiv:2304.08244.\\n\\nChin-Yew Lin. 2004a. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381. Association for Computational Linguistics.\\n\\nChin-Yew Lin. 2004b. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nSong Liu, Haoqi Fan, Shengsheng Qian, Yiru Chen, Wenkui Ding, and Zhongyuan Wang. 2021. Hit: Hierarchical transformer with momentum contrast for video-text retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11915\u201311925.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpte-val: Nlgevaluationusinggpt-4withbetterhumanalignment. arXiv preprint arXiv:2303.16634.\\n\\nShayne Longpre, Le Hou, Tu Vu, Albert Webson, Hyung Won Chung, Yi Tay, Denny Zhou, Quoc V Le, Barret Zoph, Jason Wei, et al. 2023. The flan collection: Designing data and methods for effective instruction tuning. arXiv preprint arXiv:2301.13688.\\n\\nIlya Loshchilov and Frank Hutter. 2017. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101.\\n\\nZiyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xiubo Geng, Wenxiang Hu, Chongyang Tao, Jing Ma, Qingwei Lin, and Daxin Jiang. 2023. Wizardcoder: Empowering code large language models with evol-instruct. arXiv preprint arXiv:2306.08568.\\n\\nAravindh Mahendran and Andrea Vedaldi. 2015. Understanding deep image representations by...\"}"}
{"id": "lrec-2024-main-1460", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"inverting them. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5188\u20135196.\\n\\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noumane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n\\nSubhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, and Ahmed Awadallah. 2023. Orca: Progressive learning from complex explanation traces of gpt-4. arXiv preprint arXiv:2306.02707.\\n\\nOpenAI. 2022. Introducing chatgpt.\\n\\nR OpenAI. 2023. Gpt-4 technical report. arXiv, pages 2303\u201308774.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:27730\u201327744.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002a. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002b. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nMandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Joao Henriques, and Andrea Vedaldi. 2020. Support-set bottlenecks for video-text representation learning. arXiv preprint arXiv:2010.02824.\\n\\nBaolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277.\\n\\nYifan Qiao, Chenyan Xiong, Zhenghao Liu, and Zhiyuan Liu. 2019. Understanding the behaviors of bert in ranking. arXiv preprint arXiv:1904.07531.\\n\\nAlec Radford, Rafal Jozefowicz, and Ilya Sutskever. 2017. Learning to generate reviews and discovering sentiment. arXiv preprint arXiv:1704.01444.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer.\\n\\nRoventini, Adriana and Marinelli, Rita and Bertagna, Francesca. 2016. ItalWordNet v.2. ILC-CNR for CLARIN-IT repository hosted at Institute for Computational Linguistics \\\"A. Zampolli\\\", National Research Council, in Pisa, ISLRN 532-206-426-067-2. PID http://hdl.handle.net/20.500.11752/ILC-62. Note: You don't really need both an ISLRN and another PID, but it can't hurt.\\n\\nMrinmaya Sachan and Eric Xing. 2016. Easy questions first? a case study on curriculum learning for question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 453\u2013463.\\n\\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2021. Multitask prompted training enables zero-shot task generalization. arXiv preprint arXiv:2110.08207.\\n\\nWilliam Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. 2022. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802.\\n\\nVighnesh Shiv and Chris Quirk. 2019. Novel positional encodings to enable tree-based transformers. Advances in neural information processing systems, 32.\\n\\nCharles Joseph Singer, E. J. Holmyard, and A. R. Hall, editors. 1954\u201358. A history of technology. Oxford University Press, London. 5 vol.\\n\\nValery Solovyev, Marina Solnyshkina, Vladimir Ivanov, and Ivan Rygaev. 2019. Computing syntactic parameters for automated text complexity assessment. In CEUR Workshop Proceedings, volume 2475, pages 62\u201371.\\n\\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. 2023. Preference ranking optimization for human alignment. arXiv preprint arXiv:2306.17492.\\n\\nAlessandro Sordoni, Xingdi Yuan, Marc-Alexandre C\u00f4t\u00e9, Matheus Pereira, Adam Trischler, Ziang Xiao, Arian Hosseini, Friederike Niedtner, and Nicolas Le Roux. 2023. Deep language networks: Joint prompt training of stacked llms using variational inference. arXiv preprint arXiv:2306.12509.\"}"}
{"id": "lrec-2024-main-1460", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-1460", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263\u201327277. Curran Associates, Inc.\\n\\nZheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, Songfang Huang, and Fei Huang. 2023. Rhhf: Rank responses to align language models with human feedback without tears. arXiv preprint arXiv:2304.05302.\\n\\nMatthew D Zeiler and Rob Fergus. 2014. Visualizing and understanding convolutional networks. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part I, pages 818\u2013833. Springer.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nXinghua Zhang, Bowen Yu, Haiyang Yu, Yangyu Lv, Tingwen Liu, Fei Huang, Hongbo Xu, and Yongbin Li. 2023. Wider and deeper llm networks are fairer llm evaluators. arXiv preprint arXiv:2308.01862.\\n\\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023a. A survey of large language models. arXiv preprint arXiv:2303.18223.\\n\\nWei Zhao, Maxime Peyrard, Fei Liu, Yang Gao, Christian M Meyer, and Steffen Eger. 2019. Moverscore: Text generation evaluating with contextualized embeddings and earth mover distance. arXiv preprint arXiv:1909.02622.\\n\\nYingxiu Zhao, Bowen Yu, Haiyang Yu, Bowen Li, Chao Wang, Fei Huang, Yongbin Li, and Nevin L Zhang. 2023b. Causal document-grounded dialogue pre-training. arXiv preprint arXiv:2305.10927.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. arXiv preprint arXiv:2306.05685.\\n\\nBolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. 2014. Object detectors emerge in deep scene cnns. arXiv preprint arXiv:1412.6856.\\n\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023. Lima: Less is more for alignment. arXiv preprint arXiv:2305.11206.\\n\\nYikai Zhou, Baosong Yang, Derek FWong, Yu Wan, and Lidia S Chao. 2020. Uncertainty-aware curriculum learning for neural machine translation. In Proceedings of the 58th Annual Meeting of the association for computational linguistics, pages 6934\u20136944.\\n\\nQingqing Zhu, Xiuying Chen, Pengfei Wu, JunFei Liu, and Dongyan Zhao. 2021. Combining curriculum learning and knowledge distillation for dialogue generation. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 1284\u20131295.\"}"}
