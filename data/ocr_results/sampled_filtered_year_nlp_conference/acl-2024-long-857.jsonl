{"id": "acl-2024-long-857", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDespite the significant advances made in Semantic Role Labeling (SRL), much work in this field has been carried out with a focus on verbal predicates, with the research on nominal SRL lagging behind. In many contexts, however, nominal predicates are often as informative as verbal ones, thus calling for proper treatment. In this paper we aim to fill this gap and make nominal SRL a first-class citizen. We introduce a novel approach in order to create the first large-scale, high-quality inventory of nominal predicates and organize them into semantically-coherent frames. Although it is automatically created, NounAtlas \u2013 our nominal frame inventory \u2013 is subsequently fully validated. We then put forward a technique for generating silver training data for nominal SRL and show that a state-of-the-art SRL model can achieve good performance. Interestingly, thanks to our design choices, which enable seamless integration of our predicate inventory with its verbal counterpart, i.e., VerbAtlas, we can mix verbal and nominal data and perform robust SRL on both types of predicate.\\n\\n1 Introduction\\n\\nIn the ever-evolving field of Natural Language Processing (NLP), the pursuit of Natural Language Understanding (NLU) has garnered increasing attention over the past two decades. Among the open challenges in this domain, Semantic Role Labeling (SRL) is still far from being solved. Pioneered by Gildea and Jurafsky (2002), SRL is the task of identifying the semantic relations between predicates and their arguments (i.e. \u201cWho did What to Whom, When, Where, and How?\u201d). SRL has already proven to be a valuable asset, contributing to diverse tasks, such as question answering (Shen and Lapata, 2007), textual inference (Bastianelli et al., 2013), neural machine translation (Song et al., 2019), visual semantic role labeling (Chen et al., 2021; Sadhu et al., 2021), text summarization (Trandab\u02d8at, 2011) and storytelling (Peng et al., 2022).\\n\\nOne common oversimplification regarding SRL is the assumption that most predicates are verbs. Indeed, research has focused heavily on verbs as the driving force of meaning in sentences, also propelled by the development of verb-centric resources (Palmer et al., 2005; Schuler, 2005; Di Fabio et al., 2019). In many contexts, however, nominal predicates are at least as important as their verbal counterparts. This phenomenon is particularly evident in media content, including newspaper headlines, blog titles, short text messages, etc. Consider for instance the sentence \u201cAstounding discovery in the laboratory yesterday night: Dr. Jones unveils groundbreaking research findings!\u201d. In this example, the noun \u201cdiscovery\u201d serves as the PREDICATE, indicating an action, with its arguments being \u201cAstounding\u201d (ATTRIBUTE of the discovery), \u201cin the laboratory\u201d (LOCATION), \u201cyesterday night\u201d (time), and \u201cDr. Jones\u201d (AGENT).\\n\\nSuch nominal predicates need to be specifically addressed in order for downstream applications to take full advantage of SRL.\\n\\nWhile some resources have been provided that include nominal framesets, such as NomBank (Meyers et al., 2004) and FrameNet (Baker et al., 1998), their coverage is limited and they provide either predicate-specific roles (NomBank) or over-specific or incomplete frame scenarios (FrameNet). Our work aims to fill this gap and equip Semantic Role Labeling with both a comprehensive inventory of nominal predicates (paired with verbal ones) and annotated datasets to perform the task.\\n\\nIn summary our contributions are the following:\\n\\n\u2022 We introduce NounAtlas, the first large-scale inventory of predicates for nominal SRL;\\n\u2022 To facilitate the design of SRL approaches leveraging our resource, we create a large-scale dataset annotated with nominal predicates;\"}"}
{"id": "acl-2024-long-857", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We conduct an extensive evaluation of several baselines trained on our dataset. Our experiments pave the way for the development of a state-of-the-art unified system for both nominal and verbal SRL.\\n\\nWe release our code and resource at https://github.com/SapienzaNLP/nounatlas under a CC license.\\n\\n2 Related Work\\n\\n2.1 Nominal Resources\\n\\nExisting resources for SRL are predominantly centered on verbs. The earliest examples of such resources include PropBank (Palmer et al., 2005) and VerbNet (Schuler, 2005). More recently, Di Fabio et al. (2019) introduced VerbAtlas, a hand-crafted lexical-semantic resource that provides a wide-coverage verbal predicate inventory offering coarse semantically-coherent frames and informative semantic role labels. The frames are created by systematically clustering all verbal synsets in WordNet (Miller, 1995) using FrameNet-style (Baker et al., 1998) scenarios, while adopting VerbNet's human-interpretable and cross-frame role labels. Based on the frame semantic theory of Fillmore (2006), FrameNet was the first resource to be used for SRL (Gildea and Jurafsky, 2002), and one that includes nominal predicates in its frames. However, FrameNet does not grant full coverage, proving challenging to scale for out-of-domain SRL (Hartmann et al., 2017). Furthermore, although it is explicit and thus human-readable, FrameNet's roles are frame-specific, limiting their generalizability across frames with similar roles.\\n\\nConcerning nominal predicate resources, the state of the art is currently represented by PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004). NomBank is a project that aims to annotate a subset of noun predicates that take arguments. It utilizes a framework similar to that of the first version of PropBank, but for annotating noun predicates rather than verbal ones. Differences arise from variations in noun and verb argument structures, as well as disparities in how nouns and verbs are handled within the Penn Treebank (Marcus et al., 1993). These differences in predicate treatment pose challenges in developing an SRL system for both verbal and nominal predicates due to the overhead of treating them in distinct ways, as seen in CoNLL-2009 (Haji\u02c7c et al., 2009). With NomBank not having been augmented or updated for a long time, nominal predicates began to be included in PropBank, starting from version 2 (Babko-Malaya et al., 2004), where they are referred to as event variables. However, PropBank is verb-oriented: all nominal units have been integrated as aliases of previously-annotated verbal rolesets, which means they are not as thoroughly annotated as their verbal counterparts. Both NomBank and PropBank are primarily limited by their use of enumerated arguments (Arg0, Arg1, etc.), with the absence of an explicit classification of the semantics of predicate roles, making the annotation hard to interpret and overloading the meaning of an argument label. For instance, the first arguments of \u201cdrinking\u201d and \u201cfeeling\u201d are both labeled as Arg0, despite representing distinct semantic roles (A_AGENT and E_EXPERIENCER, respectively).\\n\\nAlso, as of today, NomBank and PropBank suffer from limited coverage of nominal predicates, including 5577 and 4295 nouns, respectively. Moreover, NomBank is built upon PropBank, with 1308 frames and 861 rolesets being shared between the two resources, and a total of 8815 distinct rolesets.\\n\\n2.2 Nominal Semantic Role Labeling\\n\\nAs Orlando et al. (2023) highlighted in their experiments, nominal Semantic Role Labeling is still far from being solved. Here, a clear indication to focus on enriching nominal resources is the fact that the best SRL neural models (Shi and Lin, 2019; Conia et al., 2021a; Conia and Navigli, 2022; Conia et al., 2022), trained exclusively on verbal predicates, struggle to generalize to unseen nominal ones. Research on non-verbal predicates remains significantly underdeveloped, with existing efforts centered mainly around transferring knowledge from verbal predicates to their nominal counterparts in an unsupervised manner (Klein et al., 2020; Zhao and Titov, 2020), rather than creating resources that enable SRL to generalize easily between verbal and nominal predicates. The limited attention given to non-verbal predicates may be attributed to the prevailing design of current SRL datasets, which utilizes predominantly verbal predicates (Daza and Frank, 2020; Tripodi et al., 2021; Jindal et al., 2022) annotated based on the aforementioned predicate inventories.\"}"}
{"id": "acl-2024-long-857", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With our work we aim to fill the gap in nominal SRL. First, we focus on constructing NounAtlas, an inventory of over 10,000 nominal predicates, curated by means of leveraging and inheriting knowledge and semantic frames from VerbAtlas. This addresses the above-mentioned coverage issues and provides integration between nominal and verbal predicates, which can belong to the same frames. Second, we employ the resulting inventory to produce, for the first time, a large dataset for wide-coverage nominal SRL.\\n\\n3 NounAtlas\\n\\nBefore delving into the process of constructing NounAtlas, our large-scale nominal predicate inventory, we highlight a few features in VerbAtlas that enable this process.\\n\\nVerbAtlas\\n\\nAs mentioned in Section 2, VerbAtlas uses verbal synsets from WordNet as verbal predicate proxies, organizing them into semantically-coherent frames. This includes more than 400 synset clusters, each assigned a human-readable label, such as EAT-BITE and SPEAK. A key advantage of VerbAtlas over its alternatives is its common prototypical argument structure and cross-frame set of semantic roles \u00e0 la VerbNet: for instance, the frame SPEAK features the roles of AGENT and THEME. Importantly, being univocally associated with exactly one frame, each synset is associated with the corresponding frame semantics and predicate argument structure, which can be exploited for SRL. For instance, in Figure 1 the verbal synsets EMBARRASS.V.01 and PRIDE.V.01 are included in the CAUSE-MENTAL-STATE frame.\\n\\n3.1 Creating a Large-Scale Nominal Predicate Inventory\\n\\nOur first objective is to produce NounAtlas, a large-scale inventory of nominal predicates. To achieve this goal, we leverage the VerbAtlas frames to group WordNet nominal synsets that express analogous semantics in nominal form. For instance, we utilize the EAT-BITE verbal frame, which contains synsets such as EAT.V.01, BITE.V.01, and DEVOUR.V.04, to cluster nominal synsets like EATING.N.01, BITE.N.08 and CHOMPING.N.01. We now detail the procedure for creating our nominal predicate inventory.\\n\\n3.1.1 Event-characterizing synset selection\\n\\nFirst, we define a nominal predicate as a concept which conveys an event, action or situation, allowing a clear identification of roles such as AGENT, PATIENT, THEME, etc. In our work we aim to identify nominal synsets with such properties (which we call event-characterizing synsets) as proxies for nominal predicates. As an example, we include WORK.N.01 (an activity directed toward making or doing something) as an event-characterizing synset, whereas we exclude non-event synsets, such as WORKER.N.01 (a person who works at a specific occupation) or TABLE.N.02 (a piece of furniture). To do so, we sort the full list of nominal synsets in WordNet 3.0 by their number of hyponyms (i.e., descendants in the nominal taxonomy) and select the top synsets \\\\( s_j \\\\) in the list which are deemed to characterize events. This results in the identification of nine general event-characterizing synsets which are then reduced to four due to the removal of those subsumed by other synsets in the set: EVENT.N.01, EVENT.N.02, PROGRESS.N.02 and PROCESS.N.06. We then select as our final set of event-characterizing synsets all the 10,086 nominal synsets that are descendants, i.e., direct or indirect hyponyms, of our set of four general event-characterizing synsets, including the latter.\\n\\n3.1.2 WordNet-based synset-to-frame mapping\\n\\nWith the goal of clustering nominal synsets into frames, we bootstrap the synset-to-frame mapping process by exploiting the WordNet derivationally-related form (DRF) relations, which establish connections between senses of nominal synsets and their corresponding verbal counterparts. For instance, the nominal synset EATING.N.01 (the act of consuming food) is mapped to the verbal synset EAT.V.01 (take in solid food) through a DRF relation. Since the latter synset is contained in the EAT-BITE frame in VerbAtlas, we identify EATING.N.01 as a potential candidate for this frame. However, multiple cases need to be considered at this stage for a given nominal synset \\\\( s \\\\):\\n\\n- \\\\( s \\\\) is linked to a single verbal synset \\\\( v \\\\). This allows a straightforward inclusion in the corresponding VerbAtlas frame (Unambiguous link), i.e. \\\\( \\\\text{frame}(s) := \\\\text{frame}(v) \\\\), where \\\\( \\\\text{frame} \\\\) returns the VerbAtlas frame of the input synset.\"}"}
{"id": "acl-2024-long-857", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Visual representation of the different types of link between nominal and verbal synsets (and the corresponding VerbAtlas frames). Orange arrows represent the derivationally related forms (DRFs) connections in WordNet, whereas blue arrows represent the WordNet hyponymy relations. Best seen in colors.\\n\\n- There are multiple verbal synsets \\\\( V \\\\) linked to \\\\( s \\\\). If all the verbal synsets in \\\\( V \\\\) belong to the same frame, the frame mapping is again straightforward, i.e. \\\\( \\\\text{frame} (s) := \\\\text{frame} (v) \\\\) for any \\\\( v \\\\in V \\\\). Otherwise, we manually map the nominal synset \\\\( s \\\\) to the most suitable frame among those containing verbal synsets (Manually-curated links).\\n\\n- \\\\( s \\\\) is not associated with any verbal synset through DRFs (Non-existing links).\\n\\nWe provide a visual representation of each type of link in Figure 1. In this example, since \\\\textit{EMBARRASSMENT}.N.03 is connected to \\\\textit{EMBAR-RASS}.V.01 through an Unambiguous link, we can include it within the \\\\textit{CAUSE-MENTAL-STATE} frame. Conversely, \\\\textit{CONGRATULATION}.N.02 requires a manual annotation, since its DRF links lead to synsets belonging to multiple frames (i.e. \\\\textit{APPROVE-PRAISE} and \\\\textit{CAUSE-MENTAL-STATE}).\\n\\nFinally, \\\\textit{ALIENATION}.N.04 is an example of a Non-existing link, since it is not connected via DRF to any verbal synset from VerbAtlas.\\n\\nResults and Statistics\\n\\nWe report the statistics concerning the WordNet-based mapping of nominal synsets to VerbAtlas frames in Table 1. In Figure 2 we show the expectedly skewed distribution of verbal and nominal synsets within each frame, with some frames comprising over 100 synsets (e.g. \\\\textit{SPEAK} and \\\\textit{HIT}), and others being limited up to 3 (e.g. \\\\textit{RESERVE} and \\\\textit{FORGET}). Interestingly, the figure shows that frames containing more verbal synsets tend to also include a higher number of nominal synsets.\\n\\nTo evaluate the quality of the Unambiguous links, we manually classified a random subset comprising 100 instances without using the DRF information and we found that 82% of the manual classifications were in agreement with the automatic ones, with another 17% upon inspection found to be equally valid. These results demonstrate that the WordNet DRF relations can be leveraged effectively to classify a substantial portion, i.e. 31%, of the overall event-characterizing nominal synsets.\"}"}
{"id": "acl-2024-long-857", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then moved on to the Manually-curated links.\\n\\nTo estimate the complexity of this task, we computed the inter-annotator agreement on a randomly-sampled subset of 100 annotations, with a resulting Cohen's $\\\\kappa$ of 0.57 (moderate agreement).\\n\\nThe results on both types of link can be attributed to the frequent scenario where the proposed VerbAtlas frames can provide overlapping semantics, leading to multiple valid options for a given nominal synset. For example, the nominal synset \\\\textit{EX}\\\\textsc{-}\\\\textit{AMINATION}.N.01 was mapped to the \\\\textsc{Analyze} frame by one annotator and to \\\\textsc{Verify} by another.\\n\\n### 3.1.3 Ranking frames for unlinked synsets\\n\\nTo address the challenge of Non-existing links, and thereby increase the coverage of our mapping, we design an automatic approach aimed at mapping nominal synsets to the best suited verbal frame.\\n\\nFirst, we gather all the nominal synsets involved in Unambiguous and Manually-curated links together with all the verbal synsets from VerbAtlas. Then, we create a dataset organized into pairs of (nominal synset, verbal synset) definitions as follows: given a nominal synset $s$ and its mapped frame $f$, we pair the definition of $s$ with the definition of the verbal synset $v \\\\in f$ which is linked to $s$ through a DRF relation. We label this pair as positive. Then, we augment our dataset of positive pairs by additionally associating the definition of $s$ with the ones of $K$ randomly-selected verbal synsets also belonging to $f$. Finally, we obtain $K + 1$ negative pairs by repeating the same process, but this time selecting as many random frames $f_i$ such that $f_i \\\\neq f \\\\forall i \\\\in \\\\{1, \\\\ldots, K + 1\\\\}$, to sample the verbal synsets from. Each definition is expanded with the corresponding synset's lemmas, which are prepended in a comma-separated format. For instance, we generate a positive pair by associating the nominal synset \\\\textit{APPOINTMENT}.N.01 (\\\"the act of putting a person into a non-elective position\\\"), with the verbal synset \\\\textit{DELEGATE}.V.02 (\\\"give an assignment to a person to a post, or assign a task to a person\\\") both linked to the frame ASSIGN-\\\\textsc{-}\\\\textsc{SMT}-TO-\\\\textsc{SMN}. We then obtain a negative pair by associating the same nominal synset with a verbal synset from a different frame e.g. with \\\\textit{WASH}.V.09 (\\\"remove by the application of water or other liquid and soap or some other cleaning agent\\\"), belonging to the frame WASH\\\\_\\\\textsc{CLEAN}.\\n\\n**Ranking approach.** We use the resulting dataset in a binary classification task, with a positive label indicating that the two synsets in the pair belong to the same frame and a negative label indicating otherwise. We train a Cross-Encoder model (Reimers and Gurevych, 2019) on our dataset. Then, at inference time, given a nominal synset $s$, we link it to a VerbAtlas frame through the following strategy:\\n\\n1. From each VerbAtlas frame, we randomly select a maximum number of available verbal synset definitions, up to 10, and pair them with the definition of $s$;\\n2. We utilize our model to score each pair;\\n3. We derive the final score for a frame as the average score of its 10 pairs;\\n4. We rank frames according to their scores, arranged from highest to lowest. A higher score indicates a greater likelihood of the nominal synset being associated with the respective frame.\\n\\nUsing this approach, for each nominal synset with Non-existing links we are able to construct a similarity ranking of all the VerbAtlas frames.\\n\\n**Experimental setup.** We used mpnet-v2 as our Encoder model, and we finetuned it on our dataset. We added a linear binary classification head on top. Binary cross entropy was employed as loss, while we leveraged Adam as optimizer, with an initial learning rate of $3 \\\\times 10^{-5}$. All the experiments in our work were conducted on a computing system equipped with an NVIDIA RTX 3090. We implemented the Cross-Encoder in PyTorch and PyTorch Lightning, using the Transformers library (Wolf et al., 2020). We selected the hyperparameter values according to the best F1 score on the validation split of our dataset.\\n\\n**Training and evaluation datasets.** With the aim of assessing the quality of our ranking approach, we created a gold standard evaluation dataset composed of pairs of definitions of (nominal, verbal) synsets. To create the training and validation splits, we selected nominal synsets involved in Unambiguous links. Specifically, for each frame, we partitioned its synsets into 80% for training and 20% for validation. To build the test set and ensure its reliability, we used the definitions of all the nominal synsets with a Manually-curated link. We 2huggingface.co/sentence-transformers/all-mpnet-base-v2. Number of parameters: 109M.\"}"}
{"id": "acl-2024-long-857", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paired each nominal synset definition in the train, validation and test sets with a maximum of $2 \\\\cdot K$ definitions of verbal synsets, generating an equal number of $K$ positive and negative pairs. Recall from the first part of this section that, given a nominal synset $s$, half of the definition pairs are positive pairs built using definitions of verbal synsets sampled from the same frame that $s$ belongs to, i.e. frame $(s)$, while the other half are built using definitions of verbal synsets sampled from random frames, each different from frame $(s)$.\\n\\nWe set $K$ to 30, 3, and 3 for our training, validation, and test splits, respectively. As a result, we obtained 132,484 pairs for the training set, 36,484 for the validation set, and 65,46 for the test set.\\n\\nResults and discussion.\\n\\nWe evaluate the performance of our system for the binary classification task with the accuracy and F1 metrics, and for the ranking task using the Top-k accuracy scores. We report the results of our evaluation in Table 2.\\n\\nAs can be seen, our model achieves high accuracy and F1 on binary classification. Moreover, the Top-k accuracy achieved highlights the ability of our system to provide the correct frame among the top-5 ranked. This outcome paves the way for an effective manual annotation process, aimed at selecting one frame among the top-5 ranked by our system, given a nominal synset (cf. next section).\\n\\n3.1.4 Manual mapping of unlinked synsets\\n\\nWe can now proceed to mapping unlinked synsets to frames: given a nominal synset $s$, we tasked an expert annotator to select the most suitable frame from among the top-5 provided in the similarity ranking of $s$. In special cases where the annotator was not able to choose a suitable candidate from the provided frames, they had the option to consult VerbAtlas, and select a more appropriate frame. The annotator was specifically recruited for this task. We paid them in accordance with the standard salary of their geographical location. They took an average of 3 minutes to annotate one item. The result of our entire semi-automatic mapping process was a large-scale nominal predicate inventory for SRL, which we have called NounAtlas.\\n\\nWe note that, by construction, the VerbAtlas and NounAtlas frames share the same labels, making it possible to bring together nominal and verbal synsets into a unified nominal and verbal resource.\\n\\nResults.\\n\\nThe annotator successfully linked 5,876 out of the 5,904 synsets with non-existing links (99.6%). They selected 5,207 frames (88.2% of the total) among the top-5 ranked by our system, with 2,912 (49.3%) being the top-1. The remaining synsets were classified by directly accessing the full VerbAtlas frame inventory.\\n\\n3.1.5 Qualitative Comparison\\n\\nTo highlight the inherent advantages of using NounAtlas, consider the annotations reported in Figure 3 of a sample sentence annotated with different lexical-semantic nominal inventories. As mentioned in Section 2, PropBank's underspecified role semantics can result in models that struggle to generalize to out-of-domain predicates. FrameNet's roles, instead, are explicit, but frame-specific, resulting in a very large set of different role labels: in the example above, the roles \u201cIngestor\u201d and \u201cIngestible\u201d are specific only for the frame \u201cIngestion\u201d. These problems are solved when using NounAtlas, which provides just a few, explicit and cross-frame semantic roles over a very large set of nominal predicates, enabling a more extensive coverage.\\n\\n4 Creating a Nominal SRL Dataset\\n\\nAs a result of the work outlined in Section 3.1, we obtained an inventory of nominal predicates organized into frames. However, we still have to build a large dataset for nominal Semantic Role Labeling, i.e. annotated with our nominal predicates. In what follows, we achieve this goal by (i) collecting WordNet verbal sense occurrences (Section 4.1), (ii) transforming each verbal predicate annotation into a nominal one, adjusting the sense...\"}"}
{"id": "acl-2024-long-857", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance of the Cross-Encoder in the binary classification task and the top-k accuracies ($k = 1, \\\\ldots, 5$).\\n\\n4.1 Initial Sentence Corpus\\nThe initial step involves collecting sentences featuring sense-annotated verbal predicates. For this purpose, we use SemCor (Miller et al., 1993) \u2013 the largest corpus manually annotated with WordNet senses \u2013 to extract those sentences which contain at least one verb annotated with a synset $v$ such that i) $v$ is directly linked via DRF to a nominal synset $s$ and, ii) $v$ and $s$ are linked through an Unambiguous link.\\n\\n4.2 Predicate Nominalization\\nThe next step consists in transforming each verbal predicate occurrence in our collected sentences into nominal form. To do so, first, we use a Large Language Model (LLM) prompted to nominalize a given verbal predicate annotated with synset $v$ using any of the synonyms in the corresponding nominal synset $s = \\\\{n_1, \\\\ldots, n_m\\\\}$, i.e. its deverbal forms. We use the following prompt:\\n\\n*Change the sentence by nominalizing the verb \\\"w\\\\text{1}\\\". Use exactly one of these deverbal nouns: \\\"n_1, \\\\ldots, n_m\\\". Indicate the chosen deverbal noun with **: \\\"w_1\\\\ldots w_i\\\\ldots w_n\\\"* where $w_1\\\\ldots w_n$ is the sentence, and $w_i$ is an inflected form of a lemma in synset $v$. The prompt tasks the LLM with indicating the chosen deverbal noun within the sentence (we report some examples of instantiated prompts in Appendix B). To enhance the model's understanding of the task, we provide it with additional inputs as system-level instructions, and a list of manually-crafted nominalization examples for few-shot prompting, to enable in-context learning and coherent output formatting (see Appendix B). The few-shot examples are manually crafted from selected representative verbal samples of SemCor, in order to provide the LLM with a variety of possible nominalization forms and avoid a repetitive structure of the nominalized sentences. Note that, in the case of $N$ verbal predicates occurring in the same sentence, we create $N$ different prompts, each revolving on a different predicate to nominalize.\\n\\nSecond, each LLM-generated sentence is then automatically validated through the following heuristics to ensure reliability, automatically checking that: (i) a deverbal noun is clearly indicated in the output using the formatting requested in the prompt and outlined in the few-shot examples; (ii) the sentence and the chosen deverbal noun are POS-tagged to make sure that the chosen deverbal noun is indeed a noun; (iii) the lemmatized form $n_j$ of the chosen deverbal noun is one of the candidates provided, i.e. $n_j \\\\in s$. If all checks are successfully passed, the sentence is considered valid and we tag the nominal predicate occurrence in the generated sentence with frame($s$). We then proceed to the role propagation step.\\n\\nDuring the predicate nominalization, different LLMs and prompt combinations were tested, including ChatGPT models (OpenAI, 2024) and the Gemini-Pro model (Google, 2024) (see Appendix A for a comprehensive list and individual results). In order to finalize the prompt, we evaluated the results quantitatively using the aforementioned validation pipeline, but also qualitatively by manual inspection, over a representative subset of samples, using criteria such as grammatical correctness, adherence to the original sentence, and correct usage of the nominal predicate. These tests highlighted the need to craft a prompt that guides the LLM toward the desired result, and this was the reason for our decision to include the list of admissible deverbal nouns, few-shot examples, and clear, standardized output formatting.\\n\\n4.3 Verbal-to-Nominal Role Propagation\\nThe last step is to label the roles in each sentence. Recall that, by construction, frame($s$) = frame($v$), i.e. the frame associated with the nominal predicate $n_j$ is the same as that of its verbal counterpart (cf. Section 3.1.2).\\n\\nTo label the roles of the target nominal predicate, we carry out the following steps using both the original sentence $\\\\sigma_v$ used as input for the LLM,\"}"}
{"id": "acl-2024-long-857", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Precision, Recall and F1 results of the SRL baselines on our nominal dataset. We report the performance of our baselines trained on our nominal dataset (N), on OntoNotes (V), and on the combination of the two (V+N).\\n\\nFirst, $\\\\sigma_v$ is processed using a state-of-the-art SRL system, i.e., InVeRo-XL (Conia et al., 2021b), obtaining automatically-annotated span-based roles for the target verbal predicate frame ($v$).\\n\\nSecond, we use a bi-encoder architecture to encode $\\\\sigma_v$ and $\\\\sigma_n$, and compute the following steps for each role $r$ in $\\\\sigma_v$:\\n\\n1. We compute the span embedding $E_r$ of role $r$ in $\\\\sigma_v$ by mean pooling and L2 normalization of the embeddings of all the tokens in the span.\\n\\n2. We slide a window of the same size as the role span over $\\\\sigma_n$ by one word until the end of the sentence. For each resulting span $sp$, we compute the span embedding $E_{sp}$ as above.\\n\\n3. We select the span $sp$ in $\\\\sigma_n$ that maximizes the cosine similarity between $E_r$ and $E_{sp}$.\\n\\n4. We propagate the role annotation to the selected span in $\\\\sigma_n$.\\n\\nThe result of this process is a large automatically annotated SRL dataset, with nominal predicates identified and tagged using VerbAtlas/NounAtlas frames, and predicate arguments tagged with the corresponding roles.\\n\\n4.4 Results\\n\\nStatistics. Among its sentences, SemCor contains 36,295 verbal predicate occurrences, each of which is used to prompt the LLM to generate a corresponding nominal version. Of these, 29,221 passed the validation step, resulting in the same number of valid occurrences of nominal predicates. These latter are then passed into the role propagation step, obtaining 28,055 sentences annotated with nominal frames and their roles. The drop in the number of sentences in the role propagation step occurs because we consider as good sentences only the ones that have all the roles from the verbal sentence also present in the nominal counterpart. If even a single role is missing, then the sentence is discarded. The distribution of the latter sentences across frames is reported in Figure 4. As expected, a few frames have a considerable number of sentences, with these numbers decreasing exponentially. However, a reasonable number of sentences is also maintained in the long tail (e.g., 32 sentences at the median rank).\\n\\nDataset quality assessment. We set aside 500 sentences and tasked a human expert to annotate those sentences according to the following guidelines: for each sentence the expert was asked to (i) correct the sentence if it was found to be ungrammatical or nonsensical, (ii) double-check the frame assigned to the nominal predicate and correct it if needed, and (iii) correct each argument span and (iv) the corresponding role label. The annotator was specifically recruited for this task. We paid them in accordance with the standard salary of their geographical location.\"}"}
{"id": "acl-2024-long-857", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Precision, Recall and F1 results of the SRL baselines on OntoNotes. We report the performance of our baselines trained on OntoNotes (V), and on the combination of our nominal dataset with OntoNotes (V+N).\\n\\nAn example of a manual annotation carried out by the expert is:\\n\\nOriginal sentence: [The girl] crawled out into the renewing warmth of the sunshine [hugging] FIT [her shoulders].\\n\\nGenerated sentence with nominal predicate: [The girl] crawled out into the renewing warmth of the sunshine, performing an [embrace] FIT of [her shoulders].\\n\\nCorrected sentence: [The girl] crawled out into the renewing warmth of the sunshine, performing an [embrace] FIT of [her shoulders].\\n\\nIn this example, the automatic pipeline made a mistake in capturing the span of the THEME, neglecting to include the preposition \\\"of\\\".\\n\\nTo determine the quality of the automatically annotated nominal SRL dataset, we computed the percentage of i) grammatical sentences (92.11%), ii) unchanged frame assignments (95.56%), iii) unchanged role spans (77.11%), iv) unchanged role labels (95.27%). These results show the robustness of our approach.\\n\\n4.5 Baselines for Nominal SRL\\n\\nWe used our automatically annotated dataset to train and test a state-of-the-art SRL model in order to provide a baseline for the nominal task. We removed from our dataset the 500 sentences annotated for dataset quality assessment, and used them as our test set. We then split our silver-quality dataset into 90% training and 10% validation.\\n\\nWe selected the state-of-the-art SRL model introduced by Conia and Navigli (2020) as the backbone model for our baselines, closely following the training procedure outlined by the authors. We fine-tuned this model on three different datasets: (a) OntoNotes 5.0 (Weischedel et al., 2010), which serves as the standard dataset for verbal SRL, (b) our silver dataset for nominal SRL, (c) the concatenation of the two datasets. Because OntoNotes sentences are annotated with PropBank predicates, we use the PropBank-to-VerbAtlas mappings made available by Conia and Navigli (2020) to convert the annotations.\\n\\nResults. In Table 3, we report the performance of the aforementioned baselines on our manually-curated test set. We note that a state-of-the-art SRL approach, trained on verbal data (V), only moderately generalizes to the nominal setting, thereby confirming the findings of Orlando et al. (2023). Conversely, training the same model on our nominal SRL dataset (N) leads to a significant improvement in performance (21% on average). Interestingly, the baseline trained on the union of verbal and nominal data (V+N) achieves state-of-the-art performance on both OntoNotes (Table 4) and our gold nominal test set (Table 3). This outcome paves the way for the development of a competitive unified system performing both nominal and verbal SRL.\\n\\n5 Conclusions\\n\\nWith our work, we fill the gap in nominal SRL by introducing NounAtlas, a large-scale predicate inventory for this task. NounAtlas is constructed by linking nominal predicates from WordNet to VerbAtlas frames through a semi-automatic approach. We additionally create a large dataset for nominal SRL with the help of LLMs, annotated through label propagation with nominal predicates and semantic roles from NounAtlas. Finally, we benchmark multiple baselines consisting of state-of-the-art SRL approaches, trained and evaluated on our dataset. Our experiments demonstrate that a unified SRL approach, trained on both nominal and verbal data, achieves state-of-the-art performance in both settings. We release our code and resource at https://github.com/SapienzaNLP/nounatlas. https://github.com/SapienzaNLP/multi-srl\"}"}
{"id": "acl-2024-long-857", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\nWhile this study provides a valuable starting point for nominal SRL, additional steps could be taken to further enhance its impact. First, our approach utilizes WordNet synsets as predicates, which makes multilingual support feasible through BabelNet (Navigli and Ponzetto, 2010). Indeed each WordNet synset corresponds to a BabelNet synset, which provides translations into tens of other languages. However, lexical coverage varies depending on language (see https://babelnet.org/statistics). Generating a multilingual nominal SRL dataset is more challenging, as one should either use available multilingual corpora annotated with BabelNet, with varying coverage (Pasini et al., 2021), or translate SemCor into other languages and propagate annotations. We leave this to future work.\\n\\nAdditionally, our study focused on nominal arguments, neglecting the exploration of adjectival and adverbial SRL. Furthermore, although the inventory underwent a final step of manual annotation, the training set remains automatically generated. A manual annotation of the entire dataset is expected to bring improved performance on the task. Addressing these limitations in future studies would further advance SRL into a full-fledged NLU task, offering deeper insights into the structured analysis of text.\"}"}
{"id": "acl-2024-long-857", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Angel Daza and Anette Frank. 2020. X-SRL: A parallel cross-lingual semantic role labeling dataset. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 3904\u20133914, Online. Association for Computational Linguistics.\\n\\nAndrea Di Fabio, Simone Conia, and Roberto Navigli. 2019. VerbAtlas: a novel large-scale verbal semantic resource and its application to semantic role labeling. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 627\u2013637, Hong Kong, China. Association for Computational Linguistics.\\n\\nCharles Fillmore. 2006. Frame semantics and the nature of language. Annals of the New York Academy of Sciences, 280:20 \u2013 32.\\n\\nDaniel Gildea and Daniel Jurafsky. 2002. Automatic Labeling of Semantic Roles. Computational Linguistics, 28(3):245\u2013288.\\n\\nGoogle. 2024. Gemini: A family of highly capable multimodal models.\\n\\nJan Haji\u010d, Massimiliano Ciaramita, Richard Johanson, Daisuke Kawahara, Maria Ant\u00f2nia Mart\u00ed, Llu\u00eds M\u00e0rquez, Adam Meyers, Joakim Nivre, Sebastian Pad\u00f3, Jan \u0160t\u011bp\u00e1nek, Pavel Stra\u0148\u00e1k, Mihai Surdeanu, Nianwen Xue, and Yi Zhang. 2009. The CoNLL-2009 shared task: Syntactic and semantic dependencies in multiple languages. In Proceedings of the Thirteenth Conference on Computational Natural Language Learning (CoNLL 2009): Shared Task, pages 1\u201318, Boulder, Colorado. Association for Computational Linguistics.\\n\\nSilvana Hartmann, Ilia Kuznetsov, Teresa Martin, and Iryna Gurevych. 2017. Out-of-domain FrameNet semantic role labeling. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 471\u2013482, Valencia, Spain. Association for Computational Linguistics.\\n\\nIshan Jindal, Alexandre Rademaker, Micha\u0142 Ulewicz, Ha Linh, Huyen Nguyen, Khoi-Nguyen Tran, Huaiyu Zhu, and Yunyao Li. 2022. Universal Proposition Bank 2.0. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1700\u20131711, Marseille, France. European Language Resources Association.\\n\\nAyal Klein, Jonathan Mamou, Valentina Pyatkin, Daniela Stepanov, Hangfeng He, Dan Roth, Luke Zettlemoyer, and Ido Dagan. 2020. QANom: Question-answer driven SRL for nominalizations. In Proceedings of the 28th International Conference on Computational Linguistics, pages 3069\u20133083, Barcelona, Spain (Online). International Committee on Computational Linguistics.\\n\\nMitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. 1993. Building a large annotated corpus of English: The Penn Treebank. Computational Linguistics, 19(2):313\u2013330.\\n\\nAdam Meyers, Ruth Reeves, Catherine Macleod, Rachel Szekely, Veronika Zielinska, Brian Young, and Ralph Grishman. 2004. The NomBank project: An interim report. In Proceedings of the Workshop Frontiers in Corpus Annotation at HLT-NAACL 2004, pages 24\u201331, Boston, Massachusetts, USA. Association for Computational Linguistics.\\n\\nGeorge A. Miller. 1995. Wordnet: a lexical database for english. Commun. ACM, 38(11):39\u201341.\\n\\nGeorge A. Miller, Claudia Leacock, Randee Tengi, and Ross T. Bunker. 1993. A semantic concordance. In Human Language Technology: Proceedings of a Workshop Held at Plainsboro, New Jersey, March 21-24, 1993.\\n\\nRoberto Navigli and Simone Paolo Ponzetto. 2010. BeliefNet: Building a very large multilingual semantic network. In Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 216\u2013225, Uppsala, Sweden. Association for Computational Linguistics.\\n\\nOpenAI. 2024. Gpt-4 technical report.\\n\\nRiccardo Orlando, Simone Conia, and Roberto Navigli. 2023. Exploring non-verbal predicates in semantic role labeling: Challenges and opportunities. In Findings of the Association for Computational Linguistics: ACL 2023, pages 12378\u201312388, Toronto, Canada. Association for Computational Linguistics.\\n\\nMartha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An Annotated Corpus of Semantic Roles. Computational Linguistics, 31(1):71\u2013106.\\n\\nTommaso Pasini, Alessandro Raganato, and Roberto Navigli. 2021. Xl-wsd: An extra-large and cross-lingual evaluation framework for word sense disambiguation. In AAAI Conference on Artificial Intelligence.\\n\\nXiangyu Peng, Kaige Xie, Amal Alabdulkarim, Harshith Kayam, Samihan Dani, and Mark O. Riedl. 2022. Guiding neural story generation with reader models.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\\n\\nArka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, and Aniruddha Kembhavi. 2021. Visual semantic role labeling for video understanding. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5585\u20135596.\"}"}
{"id": "acl-2024-long-857", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We tested several LLM and prompt combinations for the task of nominalizing a verbal predicate. The tested LLMs are: OpenAI's ChatGPT models (gpt-3.5-turbo-1106, gpt-3.5-turbo-instruct, text-davinci-003) and Google's Gemini-Pro model. We evaluated them both quantitatively, using the validation pipeline described in Section 4.2, and qualitatively by manual inspection.\\n\\nOur initial prompt strategy left ample freedom for the LLM to choose the deverbal noun, since it had only the verbal predicate to convert as input. This approach provided sentences in which the nominal predicate was not easily identifiable, preventing its linkage to an existing frame. Also, the model struggled in adhering to the desired format. Adding explicit instructions to keep the formatting consistent helped, but did not completely solve this issue. The strategy of adding examples was more effective for the purpose of keeping the format consistent. To facilitate the identification of the resulting deverbal noun, a request to mark it in the output was added to the prompt. This solved any issue concerning the parsing of the output. At this stage, one problem still persisted: in some cases, the deverbal nouns produced by the LLM were impossible to link to an existing frame. To solve this issue, we included in the prompt a list of candidate deverbal which were directly linkable to a frame to choose from. Following this process, the resulting sentences were: i) adherent to the required format, ii) indicating and using a correct nominal predicate, i.e. a predicate linked via DRF to the verbal one, and iii) as similar as possible to the original one while preserving grammar and logic adequacy. As far as the selection of the LLM was concerned, we observed that gpt-3.5-instruct followed the instructions provided but it would often generate grammatically incorrect sentences when forced to use a specific deverbal noun. Instead, gpt-3.5-turbo and Gemini-pro produced the most natural sounding sentences. Gemini-pro was finally chosen based on a manual inspection of the results.\\n\\nThe validation results on a subset of 150 SemCor sentences converted using the selected prompt are shown in Table 5.\"}"}
{"id": "acl-2024-long-857", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Atlanta's recent primary election produced \\\"no evidence\\\" that any irregularities took place.\\n\\nChange the sentence by nominalizing the verb \\\"charged\\\" indicated by **. Use exactly one of these deverbal nouns: \\\"charge\\\", \\\"mission\\\", \\\"commission\\\", \\\"direction\\\". Indicate the chosen deverbal noun with **:\\n\\nThe September October term jury had been **charged** by Fulton Superior Court Judge Durwood Pye to investigate reports of possible \\\"irregularities\\\" in the hard-fought primary which was won by Mayor-nominate Ivan Allen Jr.\\n\\nC Guidelines for the human evaluation\\nIn this Section, we report the instructions provided to the annotator about the task of classifying the set of nominal synsets with non-existing links (cf. Section 3.1.2) into frames.\\n\\nTable 5: Number of samples passing validation across LLMs\\n\\n| LLM Model                      | Valid Samples | Validation Rate |\\n|--------------------------------|---------------|-----------------|\\n| GPT-3.5-turbo-1106             | 132/150       | 88%             |\\n| GPT-3.5-turbo-instruct         | 130/150       | 86.6%           |\\n| text-davinci-003               | 102/150       | 68%             |\\n| Gemini-Pro                     | 136/150       | 90.6%           |\\n\\nWe provided two Excel files.\\n\u2022 The first file:\\n  \u2022 Contains the nominal synsets requiring classification.\\n  \u2022 For each synset, presents five frames automatically selected by a machine learning model.\\n  \u2022 Annotators must choose the single most fitting frame from these five options for each synset.\\n\u2022 The second file:\\n  \u2022 Lists all 425 available frames within VerbAtlas along with their respective definitions.\\n  \u2022 For each frame, lists all the included verbal synsets and their respective definitions.\\n  \u2022 Serves as a reference for exploring frames beyond the five presented in the first file for specific synsets.\\n\\nThe classification process has to follow these steps for each synset:\\n1. Review the synset itself (column A), its lemmas (column B), and its definition (column C).\\n2. Examine the five proposed frames (column B) with their semantic definitions (column C). Additionally, consider the definitions explaining how synsets within the frame relate to its specific roles. For example, the frame \\\"LOSE\\\" might have the definition \\\"An agent loses a theme to a recipient\\\".\\n\\nWhile selecting the appropriate frame, possible cases arise:\\n1. Suitable frame in the 5 proposed:\\n   Place the \\\"x\\\" in the corresponding row.\\n2. No suitable frame among the 5 proposed:\\n   \u2022 Consult the second file to find an appropriate frame among the 425 options.\"}"}
{"id": "acl-2024-long-857", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Replace the name of a proposed frame with your chosen frame from the second file (keep the original definition).\\n\\n\u2022 Place an \u201cx\u201d in the corresponding row.\\n\\n3. Synset belongs to multiple frames: Place an \u201cx\u201d in each relevant frame row (Minimize this occurrence; use it only for true ambiguity where a single choice is impossible. Add a note in column D explaining the ambiguity).\\n\\n4. No existing frame seems appropriate: Don\u2019t mark any frames.\\n\\n5. Synset is not a nominal predicate: Don\u2019t mark any frames. This is uncommon; attempt classification even for broader event concepts.\"}"}
