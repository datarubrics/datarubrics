{"id": "acl-2022-long-300", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scheduled Multi-task Learning for Neural Chat Translation\\n\\nYunlong Liang1\u2217, Fandong Meng2, Jinan Xu1\u2020, Yufeng Chen1 and Jie Zhou2\\n\\n1Beijing Key Lab of Traffic Data Analysis and Mining, Beijing Jiaotong University, Beijing, China\\n2Pattern Recognition Center, WeChat AI, Tencent Inc, China\\n{yunlongliang,jaxu,chenyf}@bjtu.edu.cn\\n{fandongmeng,withtomzhou}@tencent.com\\n\\nAbstract\\nNeural Chat Translation (NCT) aims to translate conversational text into different languages. Existing methods mainly focus on modeling the bilingual dialogue characteristics (e.g., coherence) to improve chat translation via multi-task learning on small-scale chat translation data. Although the NCT models have achieved impressive success, it is still far from satisfactory due to insufficient chat translation data and simple joint training manners. To address the above issues, we propose a scheduled multi-task learning framework for NCT. Specifically, we devise a three-stage training framework to incorporate the large-scale in-domain chat translation data into training by adding a second pre-training stage between the original pre-training and fine-tuning stages. Furthermore, we investigate where and how to schedule the dialogue-related auxiliary tasks in multiple training stages to effectively enhance the main chat translation task. Extensive experiments on four language directions (English \u2194 Chinese and English \u2194 German) verify the effectiveness and superiority of the proposed approach. Additionally, we will make the large-scale in-domain paired bilingual dialogue dataset publicly available for the research community.\\n\\n1 Introduction\\nA cross-lingual conversation involves speakers in different languages (e.g., one speaking in Chinese and another in English), where a chat translator can be applied to help them communicate in their native languages. The chat translator bilaterally converts the language of bilingual conversational text, e.g., from Chinese to English and vice versa (Wang et al., 2016a; Farajian et al., 2020; Liang et al., 2021a, 2022).\\n\\n\u2217Work was done when Yunlong was interning at Pattern Recognition Center, WeChat AI, Tencent Inc, China.\\n\u2020Jinan Xu is the corresponding author.\\n\\nThe code and in-domain data are publicly available at: https://github.com/XL2248/SML\\n\\nFigure 1: The overall three-stage training framework.\\n\\nGenerally, since the bilingual dialogue corpus is scarce, researchers (Bao et al., 2020; Wang et al., 2020; Liang et al., 2021a,d) resort to making use of the large-scale general-domain data through the pre-training-then-fine-tuning paradigm as done in many context-aware neural machine translation models (Tiedemann and Scherrer, 2017; Maruf and Haffari, 2018; Miculicich et al., 2018; Tu et al., 2018; Voita et al., 2018, 2019a,b; Yang et al., 2019; Wang et al., 2019; Maruf et al., 2019; Ma et al., 2020, etc), having made significant progress. However, conventional pre-training on large-scale general-domain data usually learns general language patterns, which is also aimless for capturing the useful dialogue context to chat translation, and fine-tuning usually suffers from insufficient supervised data (about 10k bilingual dialogues). Some studies (Gu et al., 2020; Gururangan et al., 2020; Liu et al., 2021; Moghe et al., 2020; Wang et al., 2020; Ruder, 2021) have shown that learning domain-specific patterns by additional pre-training is beneficial to the models. To this end, we firstly construct the large-scale in-domain chat translation data. And to build the data, for English \u2194 Chinese (En \u2194 Zh), we crawl two consecutive English and Chinese movie subtitles (not aligned). For English \u2194 German (En \u2194 De), we download two consecutive English and German movie subtitles (not aligned). Then, we use several advanced technologies to align En \u2194 Zh and En \u2194 De subtitles. Finally, we obtain the paired bilingual dialogue dataset. Please refer to \u00a7 3.1 for details.\"}"}
{"id": "acl-2022-long-300", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"incorporate it for learning domain-specific patterns, we then propose a three-stage training framework via adding a second pre-training stage between general pre-training and fine-tuning, as shown in Fig. 1. To further improve the chat translation performance through modeling dialogue characteristics (e.g., coherence), inspired by previous studies (Phang et al., 2020; Liang et al., 2021d; Prucksachatkun et al., 2020), we incorporate several dialogue-related auxiliary tasks to our three-stage training framework. Unfortunately, we find that simply introducing all auxiliary tasks in the conventional multi-task learning manner does not obtain significant cumulative benefits as we expect. It indicates that the simple joint training manner may limit the potential of these auxiliary tasks, which inspires us to investigate where and how to make these auxiliary tasks work better for the main NCT task.\\n\\nTo address the above issues, we present a Scheduled Multi-task Learning framework (SML) for NCT, as shown in Fig. 1. Firstly, we propose a three-stage training framework to introduce our constructed in-domain chat translation data for learning domain-specific patterns. Secondly, to make the most of auxiliary tasks for the main NCT task, we analyze in which stage these auxiliary tasks work well and find that they are different strokes for different folks. Therefore, to fully exert their advantages for enhancing the main NCT task, we design a gradient-based strategy to dynamically schedule them at each training step in the last two training stages, which can be seen as a fine-grained joint training manner. In this way, the NCT model is effectively enhanced to capture both domain-specific patterns and dialogue-related characteristics (e.g., coherence) in conversation, which thus can generate better translation results.\\n\\nWe validate our SML framework on two datasets: BMELD (Liang et al., 2021a) (En \u2194 Zh) and BContrasT (Farajian et al., 2020) (En \u2194 De). Experimental results show that our model gains consistent improvements on four translation tasks in terms of both BLEU (Papineni et al., 2002) and TER (Snover et al., 2006) scores, demonstrating its effectiveness and generalizability. Human evaluation further suggests that our model can produce more coherent and fluent translations compared to the previous related methods.\\n\\nOur contributions are summarized as follows:\\n\u2022 We propose a scheduled multi-task learning framework with three training stages, where a gradient-based scheduling strategy is designed to fully exert the auxiliary tasks' advantages for the main NCT task, for higher translation quality.\\n\u2022 Extensive experiments on four chat translation tasks show that our model achieves new state-of-the-art performance and outperforms the existing NCT models by a significant margin.\\n\u2022 We contribute two large-scale in-domain paired bilingual dialogue corpora (28M for En \u2194 Zh and 18M for En \u2194 De) to the research community.\\n\\n2 Background: Conventional Multi-task Learning for NCT\\nWe introduce the conventional multi-task learning framework (Liang et al., 2021d) for NCT, which includes four parts: problem formalization (\u00a7 2.1), the NCT model (\u00a7 2.2), existing three auxiliary tasks (\u00a7 2.3), and training objective (\u00a7 2.4).\\n\\n2.1 Problem Formalization\\nIn a bilingual conversation, we assume the two speakers have alternately given utterances in different languages for $u$ turns, resulting in $X_1, X_2, X_3, ..., X_u$ and $Y_1, Y_2, Y_3, ..., Y_u$ on the source and target sides, respectively. Among these utterances, $X_1, X_3, X_5, ..., X_u$ are originally spoken and $Y_1, Y_3, Y_5, ..., Y_u$ are the corresponding translations in the target language. Similarly, $Y_2, Y_4, Y_6, ..., Y_{u\u22121}$ are originally spoken and $X_2, X_4, X_6, ..., X_{u\u22121}$ are the translated utterances in the source language. According to languages, we define the dialogue history context of $X_u$ on the source side as $C_{X_u} = \\\\{X_1, X_2, X_3, ..., X_{u\u22121}\\\\}$ and that of $Y_u$ on the target side as $C_{Y_u} = \\\\{Y_1, Y_2, Y_3, ..., Y_{u\u22121}\\\\}$.\\n\\nThe goal of an NCT model is to translate $X_u$ to $Y_u$ with dialogue history context $C_{X_u}$ and $C_{Y_u}$.\\n\\n2.2 The NCT Model\\nThe NCT model (Ma et al., 2020; Liang et al., 2021d) utilizes the standard transformer (Vaswani et al., 2017) architecture with an encoder and a decoder $^4$. For each of $\\\\{C_{X_u}, C_{Y_u}\\\\}$, we add the special token '[CLS]' tag at the head of it and use another token '[SEP]' to delimit its included utterances, as in Devlin et al. (2019).\\n\\n$^4$ Here, we just describe some adaptions to the NCT model, and please refer to Vaswani et al. (2017) for more details.\"}"}
{"id": "acl-2022-long-300", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the encoder, it takes $[C; X; u]$ as input, where $;$ denotes the concatenation. The input embedding consists of word embedding $WE$, position embedding $PE$, and turn embedding $TE$:\\n\\n$$B(x_i) = WE(x_i) + PE(x_i) + TE(x_i),$$\\n\\nwhere $WE \\\\in \\\\mathbb{R}^{|V| \\\\times d}$ and $TE \\\\in \\\\mathbb{R}^{|T| \\\\times d}$.\\n\\nWhen computation in the encoder, words in $CXu$ can only be attended by those in $Xu$ at the first encoder layer while $CXu$ is masked at the other layers, which is the same implementation as in Ma et al. (2020).\\n\\nIn the decoder, at each decoding time step $t$, the top-layer ($L$-th) decoder hidden state $h_{Ld,t}$ is fed into a softmax layer to predict the probability distribution of the next target token:\\n\\n$$p(Y_{u,t} | Y_{u,<t}, Xu, CXu) = \\\\text{Softmax}(W_o h_{Ld,t} + b_o),$$\\n\\nwhere $Y_{u,<t}$ denotes the preceding tokens before the $t$-th time step in the utterance $Y_u$, $W_o \\\\in \\\\mathbb{R}^{|V| \\\\times d}$ and $b_o \\\\in \\\\mathbb{R}^{|V|}$ are trainable parameters.\\n\\nFinally, the training loss is defined as follows:\\n\\n$$L_{NCT} = -\\\\frac{1}{X_t=1} \\\\sum \\\\log(p(Y_{u,t} | Y_{u,<t}, Xu, CXu)).$$\\n\\n### 2.3 Existing Auxiliary Tasks\\n\\nTo generate coherent translation, Liang et al. (2021d) present Monolingual Response Generation (MRG), Cross-lingual Response Generation (XRG), and Next Utterance Discrimination (NUD) tasks during the NCT model training.\\n\\n#### MRG.\\n\\nGiven the dialogue context $C_Yu$ in the target language, it forces the NCT model to generate the corresponding utterance $Y_u$ coherent to $C_Yu$. Particularly, the encoder of the NCT model is used to encode $C_Yu$, and the NCT decoder predicts $Y_u$. The training objective of this task is formulated as:\\n\\n$$L_{MRG} = -\\\\frac{1}{X_t=1} \\\\sum \\\\log(p(Y_{u,t} | C_Yu, Y_{u,<t})),\\n\\n\\\\quad p(Y_{u,t} | C_Yu, Y_{u,<t}) = \\\\text{Softmax}(W_m h_{Ld,t} + b_m),$$\\n\\nwhere $h_{Ld,t}$ is the $L$-th decoder hidden state at the $t$-th decoding step, $W_m$ and $b_m$ are trainable parameters.\\n\\n#### XRG.\\n\\nSimilar to MRG, the NCT model is also jointly trained to generate the corresponding utterance $Y_u$ which is coherent to the given dialogue context $CXu$ in the source language:\\n\\n$$L_{XRG} = -\\\\frac{1}{X_t=1} \\\\sum \\\\log(p(Y_{u,t} | CXu, Y_{u,<t})),\\n\\n\\\\quad p(Y_{u,t} | CXu, Y_{u,<t}) = \\\\text{Softmax}(W_c h_{Ld,t} + b_c),$$\\n\\nwhere $W_c$ and $b_c$ are trainable parameters.\\n\\n#### NUD.\\n\\nThe NUD task aims to distinguish whether the translated text is coherent to be the next utterance of the given dialogue history context. Specifically, the positive and negative samples are firstly constructed: (1) the positive sample ($C_Yu, Y_u+$) with the label $\\\\ell = 1$ consists of the target utterance $Y_u$ and its dialogue history context $C_Yu$; (2) the negative sample ($C_Yu, Y_u-$) with the label $\\\\ell = 0$ consists of the identical $C_Yu$ and a randomly selected utterance $Y_u-$ from the preceding context of $Y_u$. Formally, the training objective of NUD is defined as follows:\\n\\n$$L_{NUD} = -\\\\log(p(\\\\ell = 1 | C_Yu, Y_u+)) - \\\\log(p(\\\\ell = 0 | C_Yu, Y_u-)),$\\n\\n\\\\quad p(\\\\ell = 1 | C_Yu, Y_u) = \\\\text{Softmax}(W_n [HYu; HCYu]),$$\\n\\nwhere $HYu$ and $HCYu$ denote the representations of the target utterance $Y_u$ and $C_Yu$, respectively. Concretely, $HYu$ is calculated as $\\\\frac{1}{|Y_u|} \\\\sum_{t=1}^{P} h_{Le,t}$ while $HCYu$ is defined as the encoder hidden state $h_{Le,0}$ of the prepended special token '[CLS]' of $C_Yu$. $W_n$ is the trainable parameter of the NUD classifier and the bias term is omitted for simplicity.\\n\\n### 2.4 Training Objective\\n\\nWith the main chat translation task and three auxiliary tasks, the total training objective of the conventional multi-task learning is formulated as:\\n\\n$$L = L_{NCT} + \\\\alpha (L_{MRG} + L_{XRG} + L_{NUD}),$$\\n\\nwhere $\\\\alpha$ is the balancing factor between $L_{NCT}$ and other auxiliary objectives.\\n\\n### 3 Scheduled Multi-task Learning for NCT\\n\\nIn this section, we introduce the proposed Scheduled Multi-task Learning (SML) framework, including three stages: general pre-training, in-domain pre-training, and in-domain fine-tuning, as shown in Fig. 1. Specifically, we firstly describe the process of in-domain pre-training (\u00a7 3.1) and then present some findings of conventional multi-task learning (\u00a7 3.2), which inspire us to investigate the scheduled multi-task learning (\u00a7 3.3). Finally, we...\"}"}
{"id": "acl-2022-long-300", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"elaborate on the process of training and inference (\u00a7 3.4).\\n\\n3.1 In-domain Pre-training\\n\\nFor the second in-domain pre-training, we firstly crawl the in-domain consecutive movie subtitles used in the first pre-training stage. According to the finding that multi-task learning can enhance the NCT model (Liang et al., 2021d), we attain two in-domain paired bilingual dialogue datasets, the statistics of which are shown in Tab. 1. To construct the paired bilingual dialogue data, we firstly build an in-domain paired bilingual dialogue data corpus, we continue to pre-train the NCT model in the last two training processes (different training stages, under transformer learning experiments, aiming to achieve a better merial task, i.e. XRG, NUD, and XNUD), we investigate in which cross-lingual NUD (XNUD), given different strokes for different folks, the intuition that more dialogue-related tasks may be helpful, we investigate how to use these auxiliary tasks well. We find that each auxiliary task performs well on the second in-domain stage while XRG and XNUD tasks perform relatively poorly in the fine-tuning stage. Further, we observe that all auxiliary tasks in a conventional multi-task learning manner do not obtain significant cumulative benefits. That is, the auxiliary tasks are different in the conventional multi-task learning manner do not obtain significant cumulative benefits. Compared to the NUD task, the different point lies in the cross-lingual dialogue context history, i.e. translation from different languages. Similar to the NUD task described in \u00a7 2.3, the XNUD aims to distinguish whether the translated text is coherent to be the next utterance in the cross-lingual dialogue context history, i.e. translation from different languages. Formally, the training objective of XNUD is defined as follows:\\n\\n$$\\\\ell_u = \\\\log(p(Y_u|X_u)) - \\\\log(p(Y_u|\\\\overline{X_u}))$$\\n\\nwhere $\\\\overline{X_u}$ denotes the representation of a positive sample ($Y_u$), $\\\\overline{Y_u}$ denotes the representation of a negative sample ($\\\\overline{Y_u}$), $\\\\overline{X_u}$ is the trainable parameter of the XNUD classifier which is calculated as same as $W$. Note that, in the last two in-domain stages, we use the following is what we find from Fig. 2:\\n\\n- Each auxiliary task can always bring improvement compared with the NCT model w/o all auxiliary tasks in a conventional multi-task learning manner.\\n\\nTable 1: Statistics of our constructed chat translation dataset, the statistics of which are shown in Tab. 1. Based on the constructed in-domain bilingual dataset, the statistics of which are shown in Tab. 1.\\n\\n| Dataset | # Sentences | # Dialogues | # Utterances |\\n|---------|-------------|-------------|-------------|\\n| En \u2194 Zh | 28,214,769  | 28,238,877  | 22,244,006  |\\n| De      | 18,041,125  | 18,048,573  | 45,541,367  |\\n\\nhttps://opus.nlpl.eu/OpenSubtitles.php\\n\\nhttps://github.com/facebookresearch/LASER\\n\\nWMT sentence pairs used in the first pre-training stage.\"}"}
{"id": "acl-2022-long-300", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 By contrast, XRG and XNUD tasks perform relatively poorly in the final fine-tuning stage than MRG and NUD tasks;\\n\u2022 Some tasks used only in one stage (e.g., XRG and XNUD in the second stage) perform better than being used in both stages, revealing that different auxiliary tasks may prefer different stages to exert their advantages; one best setting seems that all tasks are used in the second stage while only MRG and NUD tasks are used in the final fine-tuning stage.\\n\\nUsing all auxiliary tasks in a conventional multi-task learning manner does not obtain significant cumulative benefits. Given the above findings, we wonder whether there exists a strategy to dynamically schedule them to exert their potential for the main NCT task.\\n\\n3.3 Scheduled Multi-task Learning\\n\\nInspired by Yu et al. (2020), we design a gradient-based scheduled multi-task learning algorithm to dynamically schedule all auxiliary tasks at each training step, as shown in Algorithm 1. Specifically, at each training step (line 1), for each task we firstly compute its gradient to model parameters \\\\( \\\\theta \\\\) (lines 2\u20134, and we denote the gradient of the main NCT task as \\\\( g_{nct} \\\\)). Then, we obtain the projection of the gradient \\\\( g_k \\\\) of each auxiliary task \\\\( k \\\\) onto \\\\( g_{nct} \\\\) (line 5), as shown in Fig. 3. Finally, we utilize the sum of \\\\( g_{nct} \\\\) and all projection (i.e., the blue arrows part, as shown in Fig. 3) of auxiliary tasks to update model parameters.\\n\\nThe core ideas behind the gradient-based SML algorithm are: (1) when the cosine similarity between \\\\( g_k \\\\) and \\\\( g_{nct} \\\\) is positive, i.e., the gradient projection \\\\( g'_k \\\\) is in the same gradient descent direction with the main NCT task, i.e., Fig. 3 (a), which could help the NCT model achieve optimal solution; (2) when the cosine similarity between \\\\( g_k \\\\) and \\\\( g_{nct} \\\\) is negative, i.e., Fig. 3 (b), which can avoid the model being optimized too fast and overfitted. Therefore, we also keep the inverse gradient to prevent the NCT model from overfitting as a regularizer. In this way, such auxiliary task joins in training at each step with the NCT task when its gradient projection is in line with \\\\( g_{nct} \\\\), which acted as a fine-grained joint training manner.\\n\\n3.4 Training and Inference\\n\\nOur training process includes three stages: the first pre-training stage on the general-domain sentence pairs (\\\\( X, Y \\\\)):\\n\\n\\\\[\\nL_{Sent-NMT} = - |Y| \\\\sum_{t=1}^{X} \\\\log(p(y_t|X, y_{<t}))\\n\\\\]\\n\\nthe second in-domain pre-training stage, and the final in-domain fine-tuning stage on the chat translation data:\\n\\n\\\\[\\nJ = L_{NCT} + \\\\alpha \\\\sum_{k \\\\in T} L_k,\\n\\\\]\\n\\nwhere \\\\( T \\\\) is the auxiliary tasks set and we keep the balancing hyper-parameter \\\\( \\\\alpha \\\\). Although the form of \\\\( L_k \\\\) is the same with Eq. 2, the gradient that participates in updating model parameters is different where it depends on the gradient descent direction of the NCT task in Eq. 4.\\n\\nAt inference, all auxiliary tasks are not involved and only the NCT model after scheduled multi-task fine-tuning is applied to chat translation.\\n\\n4 Experiments\\n\\n4.1 Datasets and Metrics\\n\\nDatasets. The training of our SML framework consists of three stages: (1) pre-train the model on a large-scale sentence-level NMT corpus (WMT209);\"}"}
{"id": "acl-2022-long-300", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Models | En\u2192Zh | Zh\u2192En | En\u2192De | De\u2192En |\\n|--------|-------|-------|-------|-------|\\n| BLEU\u2191  | TER\u2193  | BLEU\u2191 | TER\u2193  | BLEU\u2191 | TER\u2193  | BLEU\u2191 | TER\u2193  |\\n| Base Trans. w/o FT | 21.40 | 72.4 | 18.52 | 59.1 | 40.02 | 42.5 | 48.38 | 33.4 |\\n| Trans. | 25.22 | 62.8 | 21.59 | 56.7 | 58.43 | 26.7 | 59.57 | 26.2 |\\n| Dia-Trans. | 24.96 | 63.7 | 20.49 | 60.1 | 58.33 | 26.8 | 59.09 | 26.2 |\\n| Gate-Trans. | 25.34 | 62.5 | 21.03 | 56.9 | 58.48 | 26.6 | 59.53 | 26.1 |\\n| NCT | 24.76 | 63.4 | 20.61 | 59.8 | 58.15 | 27.1 | 59.46 | 25.7 |\\n| CPCC | 27.55 | 60.1 | 22.50 | 55.7 | 60.13 | 25.4 | 61.05 | 24.8 |\\n| CSA-NCT | 27.77 | 60.0 | 22.36 | 55.9 | 59.50 | 25.7 | 60.65 | 25.4 |\\n| SML (Ours) | 32.25 | 55.1 | 26.42 | 51.4 | 60.65 | 25.3 | 61.78 | 24.6 |\\n\\n| Big Trans. w/o FT | 22.81 | 69.6 | 19.58 | 57.7 | 40.53 | 42.2 | 49.90 | 33.3 |\\n| Trans. | 26.95 | 60.7 | 22.15 | 56.1 | 59.01 | 26.0 | 59.98 | 25.9 |\\n| Dia-Trans. | 26.72 | 62.4 | 21.09 | 58.1 | 58.68 | 26.8 | 59.63 | 26.0 |\\n| Gate-Trans. | 27.13 | 60.3 | 22.26 | 55.8 | 58.94 | 26.2 | 60.08 | 25.5 |\\n| NCT | 26.45 | 62.6 | 21.38 | 57.7 | 58.61 | 26.5 | 59.98 | 25.4 |\\n| CPCC | 28.98 | 59.0 | 22.98 | 54.6 | 60.23 | 25.6 | 61.45 | 24.8 |\\n| CSA-NCT | 28.86 | 58.7 | 23.69 | 54.7 | 60.64 | 25.3 | 61.21 | 24.9 |\\n| SML (Ours) | 32.87 | 54.4 | 27.58 | 50.6 | 61.16 | 25.0 | 62.17 | 24.4 |\\n\\n**Table 2:** Test results on BMELD (En\u2192Zh) and BConTrasT (En\u2192De) in terms of BLEU (%) and TER (%). The best and second best results are bold and underlined, respectively. \\\"\u2020\\\" and \\\"\u2020\u2020\\\" indicate that statistically significant better than the best result of all contrast NMT models with t-test $p < 0.05$ and $p < 0.01$ hereinafter, respectively. The results of contrast models are from Liang et al. (2021a,d). Strictly speaking, it is unfair to directly compare with them since we use additional data. Therefore, we conduct further experiments in Tab. 3 for fair comparison.\\n\\n**4.2 Implementation Details**\\nIn this paper, we adopt the settings of standard Transformer-Base and Transformer-Big in Vaswani et al. (2017). Generally, we utilize the settings in Liang et al. (2021d) for fair comparison. For more details, please refer to Appendix B. We investigate the effect of the XNUD task in \u00a7 5.4, where the new XNUD performs well based on existing auxiliary tasks.\\n\\n**4.3 Comparison Models**\\n**Sentence-level NMT Systems.**\\nTrans. w/o FT and Trans. (Vaswani et al., 2017): both are the de-facto transformer-based NMT models, and the difference is that the \\\"Trans.\\\" model is fine-tuned on the chat translation data after being pre-trained on sentence-level NMT corpus.\\n\\n**Context-aware NMT Systems.**\\nDia-Trans. (Maruf et al., 2018): A Transformer-based model where an additional encoder is used to introduce the mixed-language dialogue history, re-implemented by Liang et al. (2021a).\\n\\nGate-Trans. (Zhang et al., 2018) and NCT (Ma et al., 2020): Both are document-level NMT Transformer models where they introduce the dialogue history by a gate and by sharing the first encoder layer, respectively.\\n\\nCPCC (Liang et al., 2021a): A variational model that focuses on incorporating dialogue characteristics into a translator for better performance.\\n\\nCSA-NCT (Liang et al., 2021d): A multi-task learning model that uses several auxiliary tasks to help generate dialogue-related translations.\\n\\n---\\n\\n10 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a+version.1.4.13\"}"}
{"id": "acl-2022-long-300", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results on test sets of BMELD in terms of BLEU (%) and TER (%), where \u201cTwo-stage w/o data\u201d means the pre-training-then-fine-tuning paradigm and the in-domain data not being used, and \u201cThree-stage w/ data\u201d means the proposed three-stage method and this group uses the in-domain data. The \u201cM-NCT\u201d denotes the multi-task learning model jointly trained with four auxiliary tasks in a conventional manner. All models apply the same two/three-stage training strategy with our SML model for fair comparison except the \u201cTrans. w/o FT\u201d model, respectively.\\n\\n### 4.4 Main Results\\n\\nIn Tab. 2, we report the main results on En\u2192Zh and En\u2192De under Base and Big settings. In Tab. 3, we present additional results on En\u2192Zh.\\n\\n#### Results on En\u2192Zh.\\n\\nUnder the Base setting, our model significantly outperforms the sentence-level/context-aware baselines by a large margin (e.g., the previous best \u201cCSA-NCT\u201d), 4.58\u2191 on En\u2192Zh and 4.06\u2191 on Zh\u2192En, showing the effectiveness of the large-scale in-domain data and our scheduled multi-task learning. In terms of TER, SML also performs best on the two directions, 5.0\u2193 and 4.3\u2193 than \u201cCPCC\u201d (the lower the better), respectively. Under the Big setting, our model consistently surpasses all existing systems once again.\\n\\n#### Results on En\u2192De.\\n\\nOn both En\u2192De and De\u2192En, our model presents notable improvements over all comparison models by up to 2.50\u2191 and 2.69\u2191 BLEU gains under the Base setting, and by 2.55\u2191 and 2.53\u2191 BLEU gains under the Big setting, respectively. These results demonstrate the superiority of our three-stage training framework and also show the generalizability of our model across different language pairs. Since the baselines of En\u2192De are very strong, the results of En\u2192De are not so significant than En\u2192Zh.\\n\\n### Where to Use?\\n\\n| Models | En\u2192Zh BLEU\u2191 | TER\u2193 | Zh\u2192En BLEU\u2191 | TER\u2193 |\\n|--------|-------------|------|-------------|------|\\n| Two-stage (Not Use) | 29.49 | 55.8 | 24.15 | 53.3 |\\n| Two-stage (1\u20dd) | 31.17 | 53.2 | 26.14 | 51.4 |\\n| Two-stage (2\u20dd) | 29.87 | 53.7 | 27.47 | 50.5 |\\n| Three-stage (2\u20dd) | **33.45\u2020\u2020** | **51.1\u2020\u2020** | **29.47\u2020\u2020** | **49.3\u2020\u2020** |\\n\\nTable 4: Results on validation sets of where to use the large-scale in-domain data under the Base setting. The rows 0\u223c2 use the pre-training-then-fine-tuning (i.e., two-stage) paradigm while row 3 is the proposed three-stage method. For a fair comparison, the final fine-tuning stage of rows 0\u223c3 is all trained in the conventional multi-task training manner and the only difference is the usage of the in-domain data. Specifically, row 0 denotes without using the in-domain data. Row 1 denotes that we incorporate the in-domain data into the first pre-training stage (1\u20dd). Row 2 denotes that we introduce the in-domain data into the fine-tuning stage (2\u20dd). Row 3 denotes that we add a second pre-training stage to introduce the in-domain data.\\n\\n### Additional Results\\n\\nTab. 2 presents our overall model performance, though, strictly speaking, it is unfair to directly compare our approaches with previous ones. Therefore, we conduct additional experiments in Tab. 3 under two settings: (i) using the original pre-training-then-fine-tuning framework without introducing the large-scale in-domain data (i.e., \u201cTwo-stage w/o data\u201d group); (ii) using the proposed three-stage method with the large-scale in-domain data (i.e., \u201cThree-stage w/ data\u201d group). And we conclude that (1) the same model (e.g., SML) can be significantly enhanced by the second in-domain pre-training stage, demonstrating the effectiveness of the second pre-training on the in-domain data; (2) our SML model always exceeds the conventional multi-task learning model \u201cM-NCT\u201d in both settings, indicating the superiority of the scheduled multi-task learning strategy.\\n\\n### Analysis\\n\\n#### 5.1 Ablation Study\\n\\nWe conduct ablation studies in Tab. 4 and Tab. 5 to answer the following two questions.\\n\\nQ1: why a three-stage training framework?\\nQ2: why the scheduled multi-task learning strategy?\\n\\nTo answer Q1, in Tab. 4, we firstly investigate the effect of the large-scale in-domain chat translation data and further explore where to use it. Firstly, the results of rows 1\u223c3 substantially outperform those in row 0, proving the availability of incorporating the in-domain data. Secondly, the results of row 3...\"}"}
{"id": "acl-2022-long-300", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results on validation sets of the three-stage training framework in different multi-task training manners, under the Base setting. Row 1 denotes that the auxiliary tasks are randomly added in a conventional training manner at each training step. Row 2 denotes that we add the auxiliary tasks according to their performance in different stages, i.e., we add all tasks in the second stage while only considering MRG and NUD in the fine-tuning stage according to prior trial results in Fig. 2. Row 4 denotes that we remove the inverse gradient projection of auxiliary tasks (i.e., Fig. 3 (b)). Row 3 significantly surpasses rows 1 \u223c 2, indicating that the in-domain data used in the proposed second stage of our three-stage training framework is very successful rather than used in the stage of pre-training-then-fine-tuning paradigm. That is, the experiments show the effectiveness and necessity of our three-stage training framework.\\n\\nTo answer Q2, we investigate multiple multi-task learning strategies in Tab. 5. Firstly, the results of row 3 are notably higher than those of rows 0 \u223c 2 in both language directions, obtaining significant cumulative benefits of auxiliary tasks than rows 0 \u223c 2, demonstrating the validity of the proposed SML strategy. Secondly, the results of row 3 vs row 4 show that the inverse gradient projection of auxiliary tasks also has a positive impact on the model performance, which may prevent the model from overfitting, working as a regularizer. All experiments show the superiority of our scheduled multi-task learning strategy.\\n\\n5.2 Human Evaluation\\n\\nInspired by Bao et al. (2020) and Liang et al. (2021a), we use two criteria for human evaluation to judge whether the translation is:\\n\\n1. semantically coherent with the dialogue history?\\n2. fluent and grammatically correct?\\n\\nFirstly, we randomly sample 200 conversations from the test set of BMELD in En\u2192Zh. Then, we use 6 models in Tab. 6 to generate translated utterances of these sampled conversations. Finally, we assign the translated utterances and their corresponding dialogue history utterances in the target language to three postgraduate human annotators, and then ask them to make evaluations (0/1 score) according to the above two criteria, and average the scores as the final result.\\n\\nTable 6: Results of human evaluation (En\u2192Zh). All models use the three-stage training framework to introduce the in-domain data.\\n\\n| Models              | (En\u2192Zh) Coherence | Fluency |\\n|---------------------|-------------------|---------|\\n| Trans. w/o FT       | 0.585             | 0.630   |\\n| Trans.              | 0.620             | 0.655   |\\n| NCT                 | 0.635             | 0.665   |\\n| CSA-NCT             | 0.650             | 0.680   |\\n| M-NCT               | 0.665             | 0.695   |\\n| SML (Ours)          | **0.690\u2020**        | **0.735\u2020** |\\n\\nTable 7: Results (%) of dialogue coherence in terms of sentence similarity on validation set of BMELD in En\u2192Zh direction. The \u201c#-th Pr.\u201d denotes the #-th preceding utterance to the current one. \u201c\u2020\u2020\u201d indicates the improvement over the best result of all other comparison models is statistically significant (p < 0.01). All models use the three-stage training framework to introduce the in-domain data.\\n\\n| Models              | 1-th Pr. | 2-th Pr. | 3-th Pr. |\\n|---------------------|----------|----------|----------|\\n| Trans. w/o FT       | 58.11    | 55.15    | 52.15    |\\n| Trans.              | 58.77    | 56.10    | 52.71    |\\n| NCT                 | 59.19    | 56.43    | 52.89    |\\n| CSA-NCT             | 59.45    | 56.74    | 53.02    |\\n| M-NCT               | 59.57    | 56.79    | 53.18    |\\n| SML (Ours)          | **60.48\u2020\u2020** | 57.88\u2020\u2020 | 53.95\u2020\u2020 |\\n| Human Reference     | 61.03    | 59.24    | 54.19    |\\n\\nHuman Reference: 61.03 59.24 54.19\\n\\nTable 7 shows that our model generates more coherent and fluent translations when compared with other models (significance test, p < 0.05), which shows the superiority of our model. The inter-annotator agreements calculated by the Fleiss\u2019 kappa (Fleiss and Cohen, 1973) are 0.558 and 0.583 for coherence and fluency, respectively. It indicates \u201cModerate Agreement\u201d for both criteria.\\n\\n5.3 Dialogue Coherence\\n\\nWe measure dialogue coherence as sentence similarity following Lapata and Barzilay (2005); Xiong et al. (2019); Liang et al. (2021a):\\n\\n$$\\\\text{coh}(s_1, s_2) = \\\\cos(f(s_1), f(s_2)),$$\\n\\nwhere $\\\\cos$ denotes cosine similarity and $f(s_i) = \\\\frac{1}{|s_i|} \\\\sum_{w \\\\in s_i} (w)$ and $w$ is the vector for word $w$, and\\n\\n$$s_i,$$\"}"}
{"id": "acl-2022-long-300", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: The results on validation sets after adding the XNUD task on three auxiliary tasks, i.e., MRG, XRG and NUD (Liang et al., 2021d), which are trained in conventional manner (without incorporating in-domain data).\\n\\nEffect of the Auxiliary Task: XNUD\\n\\nWe investigate the effect of the XNUD task. As shown in Tab. 8, the \u201cM-NCT\u201d denotes the multi-task learning model jointly trained with four auxiliary tasks in conventional manner. After removing the XNUD task, the performance drops to some extent, indicating that the new XNUD task achieves further performance improvement based on three existing auxiliary tasks (Liang et al., 2021d). Then, based on the strong \u201cM-NCT\u201d model, we further investigate where and how to make the most of them for the main NCT task.\\n\\nRelated Work\\n\\nNeural Chat Translation. The goal of NCT is to train a dialogue-aware translation model using the bilingual dialogue history, which is different from document-level/sentence-level machine translation (Maruf et al., 2019; Ma et al., 2020; Yan et al., 2020; Meng and Zhang, 2019; Zhang et al., 2019).\\n\\nPrevious work can be roughly divided into two categories. One (Wang et al., 2016b; Maruf et al., 2018; Zhang and Zhou, 2019; Rikters et al., 2020) mainly pays attention to automatically constructing the bilingual corpus since no publicly available human-annotated data (Farajian et al., 2020). The other (Wang et al., 2021; Liang et al., 2021a,d) aims to incorporate the bilingual dialogue characteristics into the NCT model via multi-task learning. Different from the above studies, we focus on introducing the in-domain chat translation data to learn domain-specific patterns and scheduling the auxiliary tasks to exert their potential for high translation quality.\\n\\nMulti-task Learning.\\n\\nConventional multi-task learning (MTL) (Caruana, 1997), which trains the model on multiple related tasks to promote the representation learning and generalization performance, has been successfully used in many NLP tasks (Collobert and Weston, 2008; Ruder, 2017; Deng et al., 2013; Liang et al., 2021c,b). In the NCT, conventional MTL has been explored to inject the dialogue characteristics into models with dialogue-related tasks such as response generation (Liang et al., 2021a,d). In this work, we instead focus on how to schedule the auxiliary tasks at training to make the most of them for better translations.\\n\\nConclusion\\n\\nThis paper proposes a scheduled multi-task learning framework armed with an additional in-domain pre-training stage and a gradient-based scheduled multi-task learning strategy. Experiments on En\u2192Zh and En\u2192De demonstrate that our framework significantly improves translation quality on both BLEU and TER metrics, showing its effectiveness and generalizability. Human evaluation further verifies that our model yields better translations in terms of coherence and fluency. Furthermore, we contribute two large-scale in-domain paired bilingual dialogue datasets to the research community.\\n\\nAcknowledgements\\n\\nThe research work described in this paper has been supported by the National Key R&D Program of China (2020AAA0108001) and the National Natural Science Foundation of China (No. 61976015, 61976016, 61876198 and 61370130). Liang is supported by 2021 Tencent Rhino-Bird Research Elite Training Program. The authors would like to thank the anonymous reviewers for their valuable comments and suggestions to improve this paper.\\n\\nReferences\\n\\nCalvin Bao, Yow-Ting Shiue, Chujun Song, Jie Li, and Marine Carpuat. 2020. The university of maryland\u2019s submissions to the wmt20 chat translation task: Searching for more data to adapt discourse-aware\"}"}
{"id": "acl-2022-long-300", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-300", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of ACL, pages 311\u2013318.\\n\\nJason Phang, Iacer Calixto, Phu Mon Htut, Yada Pruksachatkun, Haokun Liu, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. English intermediate-task training improves zero-shot cross-lingual transfer too. In Proceedings of AACL, pages 557\u2013575.\\n\\nSoujanya Poria, Devamanyu Hazarika, Navonil Majumder, Gautam Naik, Erik Cambria, and Rada Mihalcea. 2019. MELD: A multimodal multi-party dataset for emotion recognition in conversations. In Proceedings of ACL, pages 527\u2013536.\\n\\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In Proceedings of WMT, pages 186\u2013191.\\n\\nYada Pruksachatkun, Jason Phang, Haokun Liu, Phu Mon Htut, Xiaoyi Zhang, Richard Yuanzhe Pang, Clara Vania, Katharina Kann, and Samuel R. Bowman. 2020. Intermediate-task transfer learning with pretrained language models: When and why does it work? In Proceedings of ACL, pages 5231\u20135247.\\n\\nMat\u0131ss Rikters, Ryokan Ri, Tong Li, and Toshiaki Nakazawa. 2020. Document-aligned Japanese-English conversation parallel corpus. In Proceedings of MT, pages 639\u2013645, Online.\\n\\nSebastian Ruder. 2017. An overview of multi-task learning in deep neural networks. CoRR, abs/1706.05098.\\n\\nSebastian Ruder. 2021. Recent Advances in Language Model Fine-tuning. http://ruder.io/recent-advances-lm-fine-tuning.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of ACL, pages 1715\u20131725.\\n\\nMatthew Snover, Bonnie Dorr, Richard Schwartz, Linnea Micciulla, and John Makhoul. 2006. A study of translation edit rate with targeted human annotation. In Proceedings of AMTA.\\n\\nZhixing Tan, Jiacheng Zhang, Xuancheng Huang, Gang Chen, Shuo Wang, Maosong Sun, Huanbo Luan, and Yang Liu. 2020. THUMT: An open-source toolkit for neural machine translation. In Proceedings of AMTA, pages 116\u2013122.\\n\\nBrian Thompson and Philipp Koehn. 2019. Vecalign: Improved sentence alignment in linear time and space. In Proceedings of EMNLP, pages 1342\u20131348.\\n\\nJ\u00f6rg Tiedemann and Yves Scherrer. 2017. Neural machine translation with extended context. In Proceedings of the DiscoMT, pages 82\u201392.\\n\\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang. 2018. Learning to remember translation history with a continuous cache. TACL, pages 407\u2013420.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proceedings of NIPS, pages 5998\u20136008.\\n\\nElena Voita, Rico Sennrich, and Ivan Titov. 2019a. Context-aware monolingual repair for neural machine translation. In Proceedings of EMNLP-IJCNLP, pages 877\u2013886.\\n\\nElena Voita, Rico Sennrich, and Ivan Titov. 2019b. When a good translation is wrong in context: Context-aware machine translation improves on deixis, ellipsis, and lexical cohesion. In Proceedings of ACL, pages 1198\u20131212.\\n\\nElena Voita, Pavel Serdyukov, Rico Sennrich, and Ivan Titov. 2018. Context-aware neural machine translation learns anaphora resolution. In Proceedings of ACL, pages 1264\u20131274.\\n\\nLongyue Wang, Zhaopeng Tu, Xing Wang, Li Ding, Liang Ding, and Shuming Shi. 2020. Tencent ai lab machine translation systems for WMT20 chat translation task. In Proceedings of WMT, pages 481\u2013489.\\n\\nLongyue Wang, Zhaopeng Tu, Xing Wang, and Shuming Shi. 2019. One model to learn both: Zero pronoun prediction and translation. In Proceedings of EMNLP-IJCNLP, pages 921\u2013930.\\n\\nLongyue Wang, Xiaojun Zhang, Zhaopeng Tu, Andy Way, and Qun Liu. 2016a. Automatic construction of discourse corpora for dialogue translation. In Proceedings of the LREC, pages 2748\u20132754.\\n\\nLongyue Wang, Xiaojun Zhang, Zhaopeng Tu, Andy Way, and Qun Liu. 2016b. Automatic construction of discourse corpora for dialogue translation. In Proceedings of LREC, pages 2748\u20132754.\\n\\nTao Wang, Chengqi Zhao, Mingxuan Wang, Lei Li, and Deyi Xiong. 2021. Autocorrect in the process of translation \u2014 multi-task learning improves dialogue machine translation. In Proceedings of NAACL: Human Language Technologies: Industry Papers, pages 105\u2013112.\\n\\nHao Xiong, Zhongjun He, Hua Wu, and Haifeng Wang. 2019. Modeling coherence for discourse neural machine translation. Proceedings of AAAI, pages 7338\u20137345.\\n\\nJianhao Yan, Fandong Meng, and Jie Zhou. 2020. Multi-unit transformers for neural machine translation. In Proceedings of EMNLP, pages 1047\u20131059, Online.\\n\\nZhengxin Yang, Jinchao Zhang, Fandong Meng, Shuhao Gu, Yang Feng, and Jie Zhou. 2019. Enhancing context modeling with a query-guided capsule network for document-level translation. In Proceedings of EMNLP-IJCNLP, pages 1527\u20131537.\\n\\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. 2020. Gradient surgery for multi-task learning. In Proceedings of NIPS, volume 33, pages 5824\u20135836.\"}"}
{"id": "acl-2022-long-300", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets\\n\\nAs mentioned in \u00a7 4.1, our experiments involve the WMT20 dataset for general-domain pre-training, the newly constructed in-domain chat translation data for the second pre-training (please refer to \u00a7 3.1), and two target chat translation corpora, BMELD (Liang et al., 2021a) and BConTrasT (Farajian et al., 2020). The statistics about the splits of training, validation, and test sets of BMELD (En $\\\\rightarrow$ Zh) and BConTrasT (En $\\\\rightarrow$ De) are shown in Tab. 9.\\n\\n### WMT20.\\nFollowing Liang et al. (2021a,d), for En $\\\\rightarrow$ Zh, we combine News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT Corpus, and WikiMatrix. For En $\\\\rightarrow$ De, we combine six corpora including Euporal, ParaCrawl, CommonCrawl, TildeRapid, NewsCommentary, and WikiMatrix. First, we filter out duplicate sentence pairs and remove those whose length exceeds 80. To pre-process the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, unicode conversion, punctuation normalization, and tokenization (Wang et al., 2020). After filtering, we apply BPE (Sennrich et al., 2016) with 32K merge operations to obtain subwords. Finally, we obtain 22,244,006 sentence pairs for En $\\\\rightarrow$ Zh and 45,541,367 sentence pairs for En $\\\\rightarrow$ De, respectively.\\n\\n### BMELD.\\nThe dataset is a recently released English $\\\\rightarrow$ Chinese bilingual dialogue dataset, provided by Liang et al. (2021a). Based on the dialogue dataset in the MELD (originally in English) (Poria et al., 2019), they firstly crawled the corresponding Chinese translations from https://www.zimutiantang.com/ and then manually post-edited them according to the dialogue history by native Chinese speakers who are post-graduate students majoring in English. Finally, following Farajian et al. (2020), they assume 50% speakers as Chinese speakers to keep data balance for Zh $\\\\rightarrow$ En translations and build the bilingual MELD (BMELD). For the Chinese, we follow them to segment the sentence using Stanford CoreNLP toolkit.\\n\\n### BConTrasT.\\nThe dataset is first provided by WMT 2020 Chat Translation Task (Farajian et al., 2020), which is translated from English into German and is based on the monolingual Taskmaster-1 corpus (Byrne et al., 2019). The conversations (originally in English) were first automatically translated into German and then manually post-edited by Unbabel editors who are native German speakers. Having the conversations in both languages allows us to simulate bilingual conversations in which one speaker (customer), speaks in German and the other speaker (agent), responds in English.\\n\\n### Implementation Details\\nFor all experiments, we follow the settings of Vaswani et al. (2017), namely Transformer-Base and Transformer-Big. In Transformer-Base, we use 512 as hidden size ($i.e.$, $d$), 2048 as filter size and 8 heads in multihead attention. In Transformer-Big, we use 1024 as hidden size, 4096 as filter size, and 16 heads in multihead attention. All our Transformer models contain $L_e = 6$ encoder layers and $L_d = 6$ decoder layers and all models are trained using THUMT (Tan et al., 2020) framework. For fair comparison, we set the training step for the first pre-training stage and the second pre-training stage totally to 200,000 (100,000 for each stage), and\\n\\n---\\n\\n14https://stanfordnlp.github.io/CoreNLP/index.html\\n15https://github.com/Unbabel/BConTrasT\\n16www.unbabel.com\"}"}
{"id": "acl-2022-long-300", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"set the step of fine-tuning stage 5,000. As for the balancing factor $\\\\alpha$ in Eq. 4, we follow (Liang et al., 2021d) to decay $\\\\alpha$ from 1 to 0 over training steps (we set them to 100,000 and 5,000 for the last two training stages, respectively). The batch size for each GPU is set to 4096 tokens. All experiments in three stages are conducted utilizing 8 NVIDIA Tesla V100 GPUs, which gives us about $8 \\\\times 4096$ tokens per update for all experiments. All models are optimized using Adam (Kingma and Ba, 2014) with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.998$, and learning rate is set to 1.0 for all experiments. Label smoothing is set to 0.1. We use dropout of 0.1/0.3 for Base and Big setting, respectively.\\n\\n$|T|$ is set to 10. When building the shared vocabulary $|V|$, we keep such word if its frequency is larger than 100. The criterion for selecting hyper-parameters is the BLEU score on validation sets for both tasks. During inference, the beam size is set to 4, and the length penalty is 0.6 among all experiments.\\n\\nIn the case of blind testing or online use (assumed dealing with En $\\\\rightarrow$ De), since translations of target utterances (i.e., English) will not be given, an inverse De $\\\\rightarrow$ En model is simultaneously trained and used to back-translate target utterances (Bao et al., 2020), which is similar for other translation directions.\\n\\n### Case Study\\n\\nIn this section, we present two illustrative cases in Fig. 4 to give some observations among the comparison models and ours.\\n\\nFor the case Fig. 4 (1), we find that most comparison models just translate the phrase \u201c30 seconds away\u201d literally as \u201c30 \u79d2\u4e4b\u5916 (30 mi\u02c7ao zh\u00afiw\u00e0i)\u201d, which is very strange and is not in line with Chinese language habits. By contrast, the \u201cM-NCT\u201d and \u201cSML\u201d models, through three-stage training, capture such translation pattern and generate an appropriate Chinese phrase \u201c\u65b9\u5706\u6570\u91cc (f\u00afangy\u00faan sh\u00f9l\u02c7i)\u201d. The reason behind this is that the large-scale in-domain dialogue bilingual corpus contains many cases of free translation, which is common in daily conversations translation. This suggests that the in-domain pre-training is indispensable for a successful chat translator.\\n\\nFor the case Fig. 4 (2), we find that all comparison models fail to translate the word \u201cgames\u201d, where they translate it as \u201c\u6e38\u620f (y\u00f3ux\u00ec)\u201d. The reason may be that they cannot fully understand the dialogue context even though some models (e.g., \u201cCSA-NCT\u201d and \u201cM-NCT\u201d) also jointly trained with the dialogue-related auxiliary tasks. By contrast, the \u201cSML\u201d model, enhanced by multi-stage scheduled multi-task learning, obtains accurate results.\\n\\nIn summary, the two cases show that our SML model enhanced by the in-domain data and scheduled multi-task learning yields satisfactory translations, demonstrating its effectiveness and superiority.\"}"}
{"id": "acl-2022-long-300", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: The illustrative cases of bilingual conversation translation.\"}"}
