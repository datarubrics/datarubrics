{"id": "acl-2023-long-845", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When slurs or standard group labels are substituted with dogwhistles, sentences are rated as significantly less toxic.\\n\\nWhere does GPT-3 perform well?\\n\\nGPT-3 readily identifies Islamophobic dogwhistles (e.g. radical Islam, Barack Hussein Obama), many anti-Semitic conspiracy theories (e.g. Khazars, Kalergi Plan, Israel Lobby), some racist dogwhistles whose covert meanings are more widely discussed among the out-group (e.g. inner-city, ghetto, thug, war on drugs, black-on-black crime), some alt-right memes (e.g. Pepe the Frog), and conservative political rhetoric (e.g. balance the budget, judicial activism, job creators).\\n\\nA.3 Details for toxicity detection\\n\\nTable A.7 shows a sample of the 237 hateful sentence templates used from HateCheck (R\u00f6ttger et al., 2021). We used the subset of HateCheck templates labeled as hateful and containing a placeholder for an identity term in either adjectival, singular nominal, or plural nominal form. We filled in these placeholders with either a standard group label, dogwhistle, or slur for three personae: anti-Semitic, racist, and transphobic. Specific terms used can be found in Table A.8.\\n\\nFor all personae and toxicity measures provided by Perspective API, dogwhistles are rated as less toxic than slurs and standard group labels (Figure A.4). Interestingly, except for the N-word, Perspective seems to not recognize the extreme offensiveness of most slurs, and thus toxicity scores for hateful sentences containing slurs are similar to or lower than scores for the same hateful sentences containing standard group labels.\"}"}
{"id": "acl-2023-long-845", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nYes, we used ChatGPT to rephrase a single sentence from the abstract to repeat the same point at the end of the introduction.\\n\\nB Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\nCongressional records data (Section 3), Glossary sources (Section 2), GPT-3 (Section 4), and Hate speech templates + models (Section 5)\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nOur glossary and taxonomy will be open and available to the public (Section 2)\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nYes, in Sections 2, 5, and 8\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nData released is anonymized with no identifying information\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nSection 2 and Section 8\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nSection 2\\n\\nC Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nNot applicable. We did not train our own model, and just used GPT-3 with the OpenAI API.\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-845", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nYes, section 4 and Appendix A.1 and A.2\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nYes, Sections 3-5, with additional statistics in A.1-A.3 in the appendix\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nYes, Section 4 and Appendix A.1-A.2\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nLeft blank.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nNo response.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nNo response.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nNo response.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nNo response.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNo response.\"}"}
{"id": "acl-2023-long-845", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Warning: content in this paper may be upsetting or offensive to some readers.\\n\\nDogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second one, often hateful or provocative, to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, in the sentence \\\"we need to end the cosmopolitan experiment,\\\" the word \\\"cosmopolitan\\\" likely means \\\"worldly\\\" to many, but secretly means \\\"Jewish\\\" to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians' speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3's performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks of such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources for future research in modeling dogwhistles and mitigating their online harms.\\n\\n1 Introduction\\n\\nThe cosmopolitan elite look down on the common affections that once bound this nation together: things like place and national feeling and religious faith. . . The cosmopolitan agenda has driven both Left and Right. . . It's time we ended the cosmopolitan experiment and recovered the promise of the republic.\\n\\n\u2013Josh Hawley (R-MO), 2019\\n\\nCosmopolitan and inner city are examples of dogwhistles, expressions that \\\"send one message to an out-group and a second (often taboo, controversial, or inflammatory) message to an in-group\\\" (Henderson and McCready, 2018). Many listeners would believe that Hawley is simply criticizing well-traveled or worldly people, but others recognize it as an attack on the Jewish people. Similarly, many assume that Ryan is discussing issues within a geographic location, but others hear a pernicious stereotype of Black men as lazy. Crucially, Hawley and Ryan can avoid alienating the out-group by maintaining plausible deniability: they never explicitly say \\\"Jewish\\\" or \\\"Black\\\", so they can reject accusations of racism (Haney-L\u00f3pez, 2014).\\n\\nBecause dogwhistles can bolster support for par...\"}"}
{"id": "acl-2023-long-845", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ticular policies or politicians among the in-group while avoiding social or political backlash from the out-group, they are a powerful mechanism of political influence (Mendelberg, 2001; Goodin and Saward, 2005). For example, racist dogwhistles such as states' rights and law and order were part of the post-Civil Rights Republican Southern Strategy to appeal to white Southerners, a historically Democratic bloc (Haney-L\u00f3pez, 2014). Despite polarization and technology that enables message targeting to different audiences, dogwhistles are still widely used by politicians (Haney-L\u00f3pez, 2014; Tilley et al., 2020) and civilians in online conversations (Bhat and Klein, 2020; \u00c5kerlund, 2021).\\n\\nBeyond political science, research on dogwhistles is urgent and essential for NLP, but they remain a challenge to study. Dogwhistles are actively and intentionally deployed to evade automated content moderation, especially hate speech detection systems (Magu et al., 2017). They may also have harmful unseen impacts in other NLP systems by infiltering data used for pretraining language models. However, researchers face many difficulties. First, unless they are a part of the in-group, researchers may be completely unaware of a dogwhistle's existence. Second, dogwhistles' meanings cannot be determined by form alone, unlike most overt hateful or toxic language. Rather, their interpretation relies on complex interplay of different factors (context, personae, content, audience identities, etc.; Khoo, 2017; Henderson and McCready, 2018, 2019; Lee and Kosse, 2020), as illustrated in Figure 1. Third, since their power is derived from the differences between in-group and out-group interpretations, dogwhistles continuously evolve in order to avoid being noticed by the out-group.\\n\\nWe establish foundations for large-scale computational study of dogwhistles by developing theory, providing resources, and empirically analyzing dogwhistles in several NLP systems. Prior work largely focuses on underlying mechanisms or political effects of dogwhistle communication (Albertson, 2015; Henderson and McCready, 2018) and typically considers a very small number of dogwhistles (often just one). To aid larger-scale efforts, we first create a new taxonomy that highlights both the systematicity and wide variation in kinds of dogwhistles (\u00a72.1). This taxonomy characterizes dogwhistles based on their covert meanings, style and register, and the personae signaled by their users. We then compile a glossary of 340 dogwhistles, each of which is labeled with our taxonomy, rich contextual information, explanations, and real-world examples with source links (\u00a72.2-2.3). As this glossary is the first of its kind, we highlight its value with a case study of racial dogwhistles in historical U.S. Congressional Speeches (\u00a73).\\n\\nWe then apply our taxonomy and glossary to investigate how dogwhistles interact with existing NLP systems (\u00a74). Specifically, we evaluate the ability of large language models (i.e. GPT-3) to retrieve potential dogwhistles and identify their covert meanings. We find that GPT-3 has a limited capacity to recognize dogwhistles, and performance varies widely based on taxonomic features and prompt constructions; for example, GPT-3 is much worse at recognizing transphobic dogwhistles than racist ones. Finally, we show that hateful messages with standard group labels (e.g. Jewish) replaced with dogwhistles (e.g. cosmopolitan) are consistently rated as far less toxic by a commercially deployed toxicity detection system (Perspective API), and such vulnerabilities can exacerbate online harms against marginalized groups (\u00a75).\\n\\nThis work highlights the significance of dogwhistles for NLP and computational social science, and offers resources for further research in recognizing dogwhistles and reducing their harmful impacts. Our glossary, code, results, GPT-3 outputs, and a form for adding new dogwhistles to our glossary are all available at: https://dogwhistles.allen.ai.\"}"}
{"id": "acl-2023-long-845", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dogwhistles serve as target group labels in three ways. Many are stereotype-based, whose interpretations rely on pre-existing associations between the dogwhistle and target group; we separate these into stereotype-based target group labels, which directly name the target group (e.g. cosmopolitan), while stereotype-based descriptors are less direct but still refer to the target group (e.g. inner-city). Others have an arbitrary or phonetic relationship to the group label; these are commonly used to evade content moderation, such as \\\"Operation Google\\\" terms invented by white supremacists on 4chan to replace various slurs (Magu et al., 2017; Bhat and Klein, 2020). The final subcategory, Bogeyman, includes names of people or institutions taken to represent the target group (e.g. George Soros \u2194 Jewish, or Willie Horton \u2194 Black).\\n\\nPersona refers to the in-group identity signalled by the dogwhistle. Figure 2 lists some personae, but this is an open class with many potential in-groups. There is considerable overlap in membership of listed in-groups (e.g. white supremacists are often antisemitic), so we label persona based directly on explanations from sources referenced in our glossary (as described in 2.2). Drawing upon third-wave sociolinguistics, personae are not static labels or stereotypes; rather, people actively construct and communicate personae through linguistic resources, such as dogwhistles (Eckert, 2008).\"}"}
{"id": "acl-2023-long-845", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Many anti-transgender people [claim that] women's \\\"sex-based rights\\\" are somehow being threatened, removed, ... which sounds wholesome but is a deniable and slippery code-word for a whole raft of unpleasant bigotry.\\n\\nSource: Medium post by David Allsopp\\n\\nWhen so-called leftists like @lloyd_rm demand that we give up our hard-won sex-based rights, they align themselves squarely with men's rights activists. To both groups, female trauma is white noise, an irrelevance, or else exaggerated or invented.\\n\\nContext: Tweet by J.K. Rowling on June 28, 2020\\n\\nTable 1: Example glossary entry for the transphobic dogwhistle\\n\\nAntisemitic, transphobic, and racist (mostly anti-Black but sometimes generally against people of color) dogwhistles are the most common, with over 70 entries for each persona. The glossary includes dogwhistles with other personae, such as homophobic, anti-Latinx, Islamophobic, anti-vax, and religious. See Table A.1 in the Appendix for glossary statistics across register, type, and persona.\\n\\nBecause dogwhistles continuously evolve, we intend for this resource to be a living glossary and invite the public to submit new entries or examples.\\n\\n3 Case study: racial dogwhistles in historical U.S. Congressional speeches\\n\\nWe showcase the usefulness of our glossary, with a diachronic case study of racial dogwhistles in politicians' speeches from the U.S. Congressional Record (Gentzkow et al., 2019; Card et al., 2022) to analyze the frequency of speeches containing racist dogwhistles from 1920-2020. For this case study, we simply identify glossary terms based on regular expressions and do not distinguish between covert and literal meanings of the same expressions. We also measure how ideologies of speakers using dogwhistles changed over time using DW-NOMINATE (Poole and Rosenthal, 1985), a scaling procedure that places politicians on a two dimensional map based on roll call voting records, such that ideologically similar politicians are located near each other (Carroll et al., 2009; Lewis et al., 2023). We consider the first dimension of DW-NOMINATE, which corresponds to a liberal-conservative axis.\\n\\nAs shown in Figure 3, dogwhistle use began to increase during the Civil Rights Era, following the 1954 Brown vs. Board of Education Supreme Court decision mandating racial integration of public schools. This aligns with qualitative accounts of the Republican Southern Strategy: because explicit racism was no longer acceptable, politicians turned to dogwhistles to make the same appeals implicitly, particularly aiming to gain the support of white voters in the Southern United States (Mendelberg, 2001). Their frequency continued to increase from the 1970s through the 1990s, paralleling Haney-L\u00f3pez (2014)'s account of dogwhistles during the Nixon, Reagan, Bush Sr., and Clinton presidencies. Since the 1990s, the frequency of racial dogwhistles has fluctuated but remained high. Like Haney-L\u00f3pez (2014), we qualitatively observe that the dogwhistles invoked post-9/11 have shifted towards being more Islamophobic and anti-Latinx rather than exclusively anti-Black. We caution that this case study and Figure 3 do not make novel claims; rather, our goal is to show that even a naive application of our glossary illustrates qualitatively well-established historical patterns in U.S. politics.\\n\\nFigure 4 shows how the average ideologies of speakers who use particular dogwhistles (property rights, thug, welfare reform, hardworking Americans, and Willie Horton) have shifted over time, and reveals interesting insights into the evolution and lifecycle of dogwhistles. Most racial dogwhistles in the U.S. Congressional Speeches have become increasingly associated with more conservative speakers over time. However, the inflection point when speaker ideologies shift varies across dogwhistles, suggesting that they emerged as dogwhistles at different points. For example, property rights became increasingly associated with more conservative speakers since the 1960s, while the average ideology of speakers using welfare reform...\"}"}
{"id": "acl-2023-long-845", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3: Frequency of speeches containing racial dogwhistles in the U.S. Congressional Record (as a fraction of total speeches) over time. The dotted red vertical lines represent noteworthy years. Use of racial dogwhistles began to increase during the Civil Rights Movement and their frequency continued to rise until the 1990s. Since the 1990s, the frequency of speeches containing dogwhistles has fluctuated but remained at overall high levels compared to earlier years.\\n\\nWillie Horton presents an interesting example. In his 1988 presidential campaign, George Bush ran a television advertisement featuring Willie Horton, a Black man convicted of rape and murder while on prison furlough (Mendelberg, 2001). The ad was so powerful among white voters that it propelled Bush to victory, but shortly afterwards was explicitly called out as racist (Haney-L\u00f3pez, 2014). We see this pattern in Figure 4: in 1988, Willie Horton was associated with extremely conservative speakers, but quickly became more liberal, and Willie Horton no longer functioned as a successful dogwhistle.\\n\\n4 Recognition of dogwhistles in GPT-3\\n\\nWe conduct two experiments to assess if a large language model, GPT-3 (Brown et al., 2020), can recognize dogwhistles. First, we interrogate whether GPT-3 is able to identify covert meanings of dogwhistles from our glossary, an ability that would be instrumental in understanding the breadth of online bigotry. Second, we measure GPT-3's ability to surface dogwhistles, motivated by the fact that dogwhistles are often intentionally obscured from researchers which makes it impossible to ensure that a manual search is complete or comprehensive. Since GPT-3 is trained on large portions of internet data, it may be able to reveal a more comprehensive and diverse set of dogwhistles than manual efforts. Finally, we present a small manual analysis of in-context dogwhistle recognition using our glossary examples and GPT-4 as a proof of concept.\\n\\nFigure 4: Average ideology score (DW-NOMINATE first dimension) for speakers who used selected dogwhistles over time: welfare reform (top left), thug (top right), property rights (bottom left), and Willie Horton (bottom right). Higher values indicate that the dogwhistle's speakers were more conservative, while lower values indicate that the dogwhistle's speakers were more liberal. For visualization, trends are Lowess-smoothed.\"}"}
{"id": "acl-2023-long-845", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Proportion of dogwhistles with covert meanings identified by GPT-3 when prompted with a dogwhistle definition, a secret cue, both, or neither. Confidence intervals are shown in brackets.\\n\\n| Condition            | Proportion of Covert Meanings Identified |\\n|----------------------|-----------------------------------------|\\n| No Secret Cue        | 0.085 [\u00b1.025]                            |\\n| No Definition         | 0.296 [\u00b1.021]                            |\\n| With Definition       | 0.437 [\u00b1.010]                            |\\n| With Secret Cue       | 0.543 [\u00b1.010]                            |\\n\\nResults\\n\\nThe first author manually evaluated whether or not each of the 5,760 GPT-3 generations contains the covert meaning for each dogwhistle. 80.3% of dogwhistles had their covert meanings identified in at least one generation. Overall, 56.0% generations contained the correct covert meaning for dogwhistles that are part of the formal/offline register, but just 29.4% for dogwhistles in the informal/online register. We refer readers to Appendix A.2 (Figure A.2) for more details about register-based variation and examples of dogwhistles for which GPT-3 performed particularly well or poorly.\\n\\nThe specific prompt form strongly impacts GPT-3's ability to generate covert meanings (Table 2). Without a definition or secret cue, covert meanings are identified in just 8.5% of generations. Including both a definition and secret cue improves GPT-3's performance over 5-fold, with dogwhistles' covert meanings identified in 54.3% of generations.\\n\\nWe observe wide variation in GPT-3's ability to identify covert meanings across personae. Among the most represented personae in our glossary (at least 100 generations for each), GPT-3 has the lowest recognition of transphobic dogwhistles, the highest recognition of homophobic and Islamophobic dogwhistles, with antisemitic, white supremacist, and racist dogwhistles in the middle (Appendix Table A.3). There is also variation in performance by dogwhistle type and the specific definition provided; we refer the reader to Appendix A.2 and Figure A.3 for more details.\\n\\n4.2 Surfacing dogwhistles\\n\\nIn addition to evaluating if GPT-3 can identify dogwhistles' covert meanings, we assess GPT-3's ability to surface dogwhistles in text generation.\\n\\nExperimental setup\\n\\nWe construct a series of prompts that begin with one of five definitions of dogwhistles from prior work (Table A.2). The definition is followed by a question or request for examples (see Appendix A.1 for more prompting details). In the following example, the definition is marked in blue, the request in purple, and GPT-3's response is highlighted in yellow.\\n\\n**A dogwhistle is the use of coded or suggestive language in political messaging to garner support from a particular group without provoking opposition. What are examples of dogwhistles?**\\n\\n1. \\\"Law and order\\\"\\n2. \\\"The silent majority\\\"\\n3. \\\"Family values\\\"\\n4. \\\"Welfare queens\\\"\\n5. \\\"Illegal aliens\\\"\\n\\nEvaluation\\n\\nWe use our glossary as a proxy to measure precision and recall of GPT-3's ability to surface dogwhistles because an exhaustive ground-truth set of dogwhistles does not exist. We calculate recall as the proportion of dogwhistles in our glossary that were also surfaced at least once by GPT-3. For precision, the authors manually inspect candidates appearing in at least 4% of GPT-3 text generations for generic, white supremacist, racist, antisemitic, Islamophobic, and transphobic prompt types. Because our glossary is not exhaustive, this method yields conservative estimates (see Appendix A.1 for more evaluation details).\\n\\nPrecision Results\\n\\nWe find that GPT-3 does have the ability to surface dogwhistles when prompted to do so, but caution that such results are imperfect and require manual verification. The most common errors involve explicit mentions of groups in stereotypes or conspiracy theories (\\\"Jews are behind the 9/11 attacks\\\") or phrases that may accompany dogwhistles but are not dogwhistles themselves (\\\"I'm not racist but...)\\n\\nPrecision in dogwhistle surfacing varies across prompt types; while the average precision over all six prompt types is 66.8%, scores range from just 50% for transphobic dogwhistle prompts to 91.3% for generic prompts (Figure A.1).\\n\\nRecall Results\\n\\nGPT-3 surfaced 153 of 340 dogwhistles in our glossary (45%). We observe significant differences by register: GPT-3 surfaced 69.4% of formal/offline dogwhistles but just 12.9% of informal/online dogwhistles. Despite its ability to generate emojis and other symbols, GPT-3 did not surface any symbols or emojis from our glossary except for the antisemitic triple parentheses \\\"((()))\\\".\\n\\nFigure 5 shows GPT-3 surfacing recall results by both register and in-group personae. We show results for the five most-frequent personae represented in our glossary. Recall of dogwhistles in the...\"}"}
{"id": "acl-2023-long-845", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Recall of GPT-3 dogwhistle surfacing separated by persona and register. Across all personae, GPT-3 surfaces under 20% of dogwhistles in the informal/online register. Performance is much higher for the formal/offline register but varies across personae, ranging from 44.8% (transphobic) to 100% (Islamophobic).\\n\\nInformal/online register is low across the board. For the formal/offline register, recall is considerably higher although it varies widely across personae.\\n\\nAs with precision, GPT-3 has the lowest performance for transphobic dogwhistles, surfacing just 44.8% of formal/offline transphobic dogwhistles. For formal/offline antisemitic dogwhistles, recall is considerably higher but far from perfect at 71.7%. GPT-3 has 80.3% and 83.3% recall of racist and white supremacist dogwhistles, respectively, and full 100% recall of Islamophobic dogwhistles.\\n\\n4.3 Identifying dogwhistles in context\\n\\nOur experiments show that LLMs have some ability to identify covert meanings of a given dogwhistle specified in the prompt (Section 4.1) and generate dogwhistles (Section 4.2). We now consider a natural follow-up question: can LLMs correctly identify dogwhistles in real-world texts?\\n\\nFully addressing this question would require a larger dataset including a variety of linguistic contexts per dogwhistle, documents containing no dogwhistles, and documents with dogwhistle expressions that do not carry the dogwhistle meaning. We leave such a rigorous analysis for future work.\\n\\nFor now, we present a small manual analysis with ChatGPT/GPT-4 (OpenAI, 2023), using several real-world examples from our glossary as a proof of concept for in-context dogwhistle recognition. We prompt GPT-4 with the instructions below.\\n\\nAppendix Table A.5 shows the exact glossary example and GPT-4 output text for the dogwhistles cosmopolitan (antisemitic), inner-city (racist), #IStandWithJKRowling (transphobic), and did you see Kyle? (white supremacist).\\n\\nA dogwhistle is the use of coded or suggestive language in political messaging to garner support from a particular group without provoking opposition. Identify the dogwhistle in the following message: [GLOSSARY EXAMPLE].\\n\\nUsing the examples presented in Section 1, GPT-4 correctly identifies the dogwhistles cosmopolitan (elite) and inner city, and generates a correct explanation for their covert meanings. However, the model does not work as well for the other examples. For the example containing #IStandWithJKRowling, GPT-4 correctly identifies that text covertly signals transphobia through support of JK Rowling, but does not select this hashtag as the dogwhistle. On the other hand, GPT-4 correctly identifies the dogwhistle in a tweet from JK Rowling, and correctly relates this symbol to the women's suffrage movement, but does not capture the appropriation of this symbol to covertly communicate transphobia. Finally, GPT-4 misses both the dogwhistle and the precise covert meaning for did you see Kyle? (\u201csee Kyle\u201d sounds similar to the Nazi slogan \u201cSieg Heil\u201d); while the model still ultimately identifies covert white supremacy, it generates a false explanation connecting the glossary example to this persona.\\n\\n5 Dogwhistles and toxicity detection\\n\\nBeyond evaluating language models' ability to recognize dogwhistles, we seek to understand how dogwhistles affect the decisions that NLP systems make, and how this has downstream implications for content moderation and online safety. We begin to address this with a study of how dogwhistles are handled by a widely-deployed toxic language detection system, Google/Jigsaw's Perspective API.\\n\\nPerspective API scores a text between 0 and 1 for a range of attributes (e.g. toxicity, identity attack, profanity), representing the estimated probability that a reader would perceive the text to contain that attribute. Perspective API's models are multilingual BERT-based models distilled into single-language convolutional neural networks for faster inference, and are trained on annotated data from online forums. We refer readers to the Perspective API Model Cards for more details.\\n\\n4 https://perspectiveapi.com/\\n5 https://developers.perspectiveapi.com/s/about-the-api-model-cards\"}"}
{"id": "acl-2023-long-845", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Experimental setup\\nWe consider 237 hateful sentence templates from HateCheck (R\u00f6ttger et al., 2021), a test suite for bias in hate speech detection, that contain placeholders for identity terms (group referents) in either adjectival, singular nominal, or plural nominal forms. We fill these templates with a standard group label, a slur, or a dogwhistle in the corresponding grammatical form requested by the template. For this experiment, we consider racist (mostly anti-Black), antisemitic, and transphobic terms, as these personae are the most common in our glossary (see Tables A.7 and A.8 for a sample of sentence templates and group label terms, respectively). We feed our resulting 7,665 sentences to Perspective API to get scores for toxicity, severe toxicity, and identity attack.\\n\\nResults\\nHateful sentences are rated as less toxic, less severely toxic, and less identity-attacking when dogwhistles are used instead of standard group labels or slurs (Figure 6). This pattern holds for all three personae (Appendix Figure A.4).\\n\\nInterestingly, mean toxicity scores for slurs are lower than for standard group labels, especially for antisemitic slurs. We observe relatively wide variation in Perspective API's ratings depending on the specific choice of slur. For example, sentences containing the N-word are almost always rated as more toxic than the same sentences containing Black or Black people. Lower toxicity ratings for other slurs, such as the highly derogatory antisemitic K-word, may be because, similar to dogwhistles, Perspective API does not recognize that these terms refer to identity groups. However, deeper analysis of slurs is outside the scope of the current work.\\n\\nDiscussion & Conclusion\\nWe lay the groundwork for NLP and computational social science research on dogwhistles by developing a new taxonomy and glossary with rich contextual information and examples. We demonstrate our glossary's utility in a case study of historical U.S. Congressional speeches, where our quantitative analysis aligns closely with historical accounts. We further use our glossary to show that GPT-3 has some, but limited, ability to retrieve dogwhistles and recognize their covert meanings. Finally, we verify that dogwhistles readily evade Perspective API's toxicity detection. We now turn to several implications of this work, highlighting potential future directions across disciplines.\\n\\nDogwhistles and toxic language\\nDogwhistles are closely related to other forms of subtle biases studied in NLP, such as implicit hate speech and symbols (Magu et al., 2017; Magu and Luo, 2018; ElSherief et al., 2018, 2021; Qian et al., 2019; Caselli et al., 2020; Menini et al., 2021; Arviv et al., 2021; Botelho et al., 2021; Wiegand et al., 2021a,b; Hartvigsen et al., 2022), microaggressions (Breitfeller et al., 2019), dehumanization (Mendelson et al., 2020), propaganda (Da San Martino et al., 2020), condescension (P\u00e9rez-Almendros et al., 2020), and stereotypes (Nangia et al., 2020; Sap et al., 2020; Nadeem et al., 2021).\\n\\nHowever, dogwhistles are distinct from toxic language in several important ways. First, although often implicitly abusive, they are not exclusively hateful; for example, wonder-working power covertly signals the speaker's Evangelical Christian identity (Albertson, 2015). Second, dogwhistles are characterized by dual meanings, wherein different sub-audiences interpret the exact same message differently (Henderson and McCready, 2018). Third, dogwhistles' true meanings are intentionally hidden from the out-group (Saul, 2018). Nevertheless, because dogwhistles are often deployed specifically to avoid hate speech detection and other content moderation tools, NLP researchers should consider how dogwhistles highlight a vulnerability in existing language technologies, which ultimately puts people's safety and well-being at risk.\"}"}
{"id": "acl-2023-long-845", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"marginalized groups. This finding is not surprising, as prior work shows that toxicity detection often fails on subtle language (Han and Tsvetkov, 2020; Hartvigsen et al., 2022), but underscores the need for toxicity and hate speech detection models to be able to flag hateful dogwhistles. One potential approach to improve such models could be to train them to recognize dogwhistles in naturally-occurring in-group contexts (starting with modeling contextual factors; Zhou et al., 2023). More broadly, content moderation pipelines should take context into account and consider mechanisms to identify when a dogwhistle has potentially negative consequences. Beyond toxicity detection, future work ought to consider the impact of dogwhistles in a broader range of NLP tasks, such as bias mitigation or story generation.\\n\\nHow do LLMs know about dogwhistles? Our findings regarding GPT-3\u2019s ability to surface and identify dogwhistles\u2019 covert meanings are probably driven by the contents of the training data. GPT-3\u2019s training data likely includes right-wing extremist content, as has been shown with its predecessor GPT-2 (Gehman et al., 2020), which may result in high performance for dogwhistles from these in-groups. Or perhaps the model is simply memorizing articles or social media posts that explicitly call out certain expressions as dogwhistles. Future work could evaluate if large language models can learn dogwhistles\u2019 covert meanings from in-context usage alone by experimentally controlling for whether or not these terms are explicitly exposed as dogwhistles in the training data.\\n\\nMoreover, we find that GPT-3\u2019s performance varies widely across target groups. Transphobic dogwhistles are notably difficult for GPT-3 to surface and identify. Perhaps this is because the model is trained on fewer data from transphobic communities compared to other in-groups considered in this work. Furthermore, transphobic dogwhistles may be less frequent in the training data because many have emerged relatively recently. Another reason may be formatting: transphobic dogwhistles are often emoji-based and appear in social media screen names and profile bios rather than in posts themselves. We hope that future work will investigate the links between language models\u2019 knowledge of dogwhistles and training data.\\n\\nPotential of LLMs for dogwhistle research Beyond the risks presented by current NLP technologies, we wish to highlight the potential benefits of using NLP to advance dogwhistle research. Even though LLMs\u2019 performance is likely due to vast training data, and even then, their outputs require manual verification, our experiments with GPT-3 demonstrate that LLMs have some ability to surface dogwhistles and explain their covert meanings. This is particularly valuable as dogwhistles are intentionally hidden from out-group members, and out-group researchers may have no other way to access this information. There is thus a unique opportunity for LLMs to assist dogwhistle research, and political content analysis more broadly.\\n\\nBridging large-scale analysis and mathematical models Our work builds foundations for large-scale computational analysis of dogwhistles in real-world political discourse. We diverge from prior quantitative dogwhistle research, which focuses on mathematically modeling the process underlying dogwhistle communication using probabilistic, game-theoretic, deep learning, and network-based approaches on simulation data (Smaldino et al., 2018; D\u00e9nigot and Burnett, 2020; Henderson and McCready, 2020; Breitholtz and Cooper, 2021; Smaldino and Turner, 2021; Xu et al., 2021; Hertzberg et al., 2022; van der Does et al., 2022). We are optimistic about future research synthesizing these two strands of work to address many of the challenges presented by dogwhistles. For example, future work could use our resources along with these mathematical models to develop systems that can automatically detect dogwhistle usages, emergence of new dogwhistles, or decline of older terms as dogwhistles due to out-group awareness.\\n\\nImplications for social science research Understanding dogwhistles at scale has vast implications across disciplines, so we develop resources useful for both NLP and social science researchers. We provide the most comprehensive-to-date glossary of dogwhistles and demonstrate through our case study how this resource can be used to analyze political speeches and other corpora, such as social media posts and newspaper articles. Dogwhistles have mostly been studied using primarily qualitative methods (Moshin, 2018; \u00c5kerlund, 2021) and experiments (Albertson, 2015; Wetts and Willer, 2019; Thompson and Busby, 2021), and we hope that by facilitating quantitative content analysis, our resources can add to dogwhistle researchers\u2019 methodological repertoires.\"}"}
{"id": "acl-2023-long-845", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7 Limitations\\n\\nThis work represents an initial push to bring dog-whistles to the forefront of NLP and computational social science research, and as such, has many limitations. Our glossary is the most comprehensive resource to date (to the best of our knowledge) but aims to document a moving target, as dogwhistles continuously emerge or fall out of use due to out-group awareness. We aim to make this resource a \\\"living glossary\\\" and encourage others to submit new entries or examples. We further encourage future research to develop models to automatically detect the emergence of new dogwhistles.\\n\\nAnother major limitation in this work is that we identify as out-group members for nearly all dog-whistles in the glossary and have an adversarial relationship with many of the communities studied (e.g. white supremacists). Although our work would ideally be validated by members of the in-groups, they have very little incentive to share this information, as that would damage the dogwhistle's utility as a tool for covert in-group communication. This work, like most prior work, is limited in that we operationalize dogwhistles as a static binary; we assume each term either does or does not have a dogwhistle interpretation and is categorically included or excluded from our glossary and analyses. In reality, dogwhistles are far more complicated constructs. For example, Lee and Kosse (2020) characterize dogwhistles along two dimensions: the size of their in-group and the degree to which their usage is conventionalized. Other axes of variation may include the level of out-group awareness, and the social and political risks of backlash to the communicator if the dogwhistle interpretation is exposed. It is even possible that audience members who hear a dogwhistle further recirculate it even if they themselves do not recognize the covert meaning (Saul, 2018). We hope future work will consider multifaceted and continuous measures of \\\"dogwhistleness\\\" that account for such nuances.\\n\\nFinally, the current work is limited in the scope of dogwhistles considered: they are all in English with the vast majority coming from the U.S. political and cultural contexts. However, dogwhistles are prominent across cultures (Pal et al., 2018; \u00c5kerlund, 2021) and we hope that future work will consider other languages and cultures, especially involving researchers who have high awareness of or expertise in non-U.S political environments.\\n\\n8 Ethical Implications\\n\\nWe caution readers about several potential ethical risks of this work. First is the risk of readers misusing or misunderstanding our glossary. We emphasize that dogwhistles are extremely context-dependent, and most terms in the glossary have benign literal meanings that may be more common than the covert dogwhistle meanings. For example, many entities from the financial sector have been used as antisemitic dogwhistles (e.g. Federal Reserve, bankers) but their primary usage has no antisemitic connotations.\\n\\nRelatedly, some glossary entries include terms that originate from the target group but were appropriated by the dogwhistles' in-group. Examples include the appropriation of goy (a Yiddish word for non-Jewish people) as an antisemitic in-group signal, and baby mama (originally from African American English) as a racist dogwhistle. As with hate speech detection (Sap et al., 2019), there is a risk of social bias in dogwhistle detection.\\n\\nAs we have discussed throughout this work, dog-whistle researchers face a challenge with no exhaustive ground truth and an unknown search space. We anticipate our glossary being a helpful resource for this reason, but because we also lack such exhaustive ground truth, there are bound to be biases in the representation of dogwhistles in our glossary. The current version of the glossary may exclude groups and thus lead to worse performance in dogwhistle detection, toxic language detection, and other downstream NLP tasks.\\n\\nOur glossary also includes real-world examples of how each dogwhistle is used. This presents a privacy risk, which we mitigate by prioritizing examples from public figures or examples from anonymous social media accounts whenever possible. We do not release personal information of any speaker who is not a well-known public figure.\\n\\nFinally, we do not pursue any computational modeling or prediction of dogwhistle usages in this work, but see it as a natural direction for future work. However, we caution researchers to consider dual-use issues in doing so. Many people use coded language in order to avoid censorship from authoritarian regimes (Yang, 2016) and marginalized groups may also use coded language for their own safety (Queen, 2007). When building computational models, we urge researchers to mitigate this dual-use risk as much as possible.\"}"}
{"id": "acl-2023-long-845", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We thank Ceren Budak, Yulia Tsvetkov, and audiences at Text as Data 2022 (TADA) and New Ways of Analyzing Variation 50 (NW AV) for their helpful feedback on an earlier version of this work. We also thank the anonymous reviewers for their comments and suggestions. J.M. gratefully acknowledges support from the Google PhD Fellowship.\\n\\nReferences\\n\\nMathilda \u00c5kerlund. 2021. Dog whistling far-right code words: the case of \u2018culture enricher\u2019 on the swedish web. Information, Communication & Society, pages 1\u201318.\\n\\nBethany L Albertson. 2015. Dog-whistle politics: Multivocal communication and religious appeals. Political Behavior, 37(1):3\u201326.\\n\\nEyal Arviv, Simo Hanouna, and Oren Tsur. 2021. It\u2019s a thin line between love and hate: Using the echo in modeling dynamics of racist online communities. Proceedings of the International AAAI Conference on Web and Social Media, 15(1):61\u201370.\\n\\nDavid A Bateman and John Lapinski. 2016. Ideal points and american political development: Beyond dw-nominate. Studies in American Political Development, 30(2):147\u2013171.\\n\\nPrashanth Bhat and Ofra Klein. 2020. Covert hate speech: White nationalists and dog whistle communication on twitter. In Twitter, the public sphere, and the chaos of online deliberation, pages 151\u2013172. Springer.\\n\\nAustin Botelho, Scott Hale, and Bertie Vidgen. 2021. Deciphering implicit hate: Evaluating automated detection algorithms for multimodal hate. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1896\u20131907.\\n\\nLuke Breitfeller, Emily Ahn, David Jurgens, and Yulia Tsvetkov. 2019. Finding microaggressions in the wild: A case for locating elusive phenomena in social media posts. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 1664\u20131674.\\n\\nEllen Breitholtz and Robin Cooper. 2021. Dogwhistles as inferences in interaction. In Proceedings of the Reasoning and Interaction Conference (ReInAct 2021), pages 40\u201346.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nEmily Burack. 2020. A list of antisemitic dogwhistles used by donald trump. Hey Alma.\\n\\nJustin Caffier. 2017. Get to know the memes of the alt-right and never miss a dog-whistle again. Vice.\\n\\nDallas Card, Serina Chang, Chris Becker, Julia Mendelsohn, Rob Vogt, Leah Boustan, Ran Abramitzky, and Dan Jurafsky. 2022. Computational analysis of 140 years of us political speeches reveals more positive but increasingly polarized framing of immigration. Proceedings of the National Academy of Sciences, 119(31):e2120510119.\\n\\nRoyce Carroll, Jeffrey B Lewis, James Lo, Keith T Poole, and Howard Rosenthal. 2009. Measuring bias and uncertainty in dw-nominate ideal point estimates via the parametric bootstrap. Political analysis, 17(3):261\u2013275.\\n\\nTommaso Caselli, Valerio Basile, Jelena Mitrovic, Inga Kartoziya, and Michael Granitzer. 2020. I feel offfended, don't be abusive! implicit/explicit messages in offensive and abusive language. In Proceedings of the 12th language resources and evaluation conference, pages 6193\u20136202.\\n\\nGiovanni Da San Martino, Alberto Barr\u00f3n-Cede\u00f1o, Henning Wachsmuth, Rostislav Petrov, and Preslav Nakov. 2020. SemEval-2020 task 11: Detection of propaganda techniques in news articles. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 1377\u20131414, Barcelona (online). International Committee for Computational Linguistics.\\n\\nQuentin D\u00e9nigot and Heather Burnett. 2020. Dogwhistles as identity-based interpretative variation. In Proceedings of the Probability and Meaning Conference (PaM 2020).\\n\\nPenelope Eckert. 2008. Variation and the indexical field 1. Journal of sociolinguistics, 12(4):453\u2013476.\\n\\nMai ElSherief, Vivek Kulkarni, Dana Nguyen, William Yang Wang, and Elizabeth Belding. 2018. Hate lingo: A target-based linguistic analysis of hate speech in social media. In Proceedings of the International AAAI Conference on Web and Social Media, volume 12.\\n\\nMai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. 2021. Latent hatred: A benchmark for understanding implicit hate speech. arXiv preprint arXiv:2109.05322.\\n\\nSamuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A Smith. 2020. Realtoxicityprompts: Evaluating neural toxic degeneration in language models. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3356\u20133369.\\n\\nMatthew Gentzkow, Jesse M Shapiro, and Matt Taddy. 2019. Measuring group differences in high-dimensional choices: method and application to congressional speech. Econometrica, 87(4):1307\u20131340.\"}"}
{"id": "acl-2023-long-845", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-845", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maarten Sap, Dallas Card, Saadia Gabriel, Yejin Choi, and Noah A Smith. 2019. The risk of racial bias in hate speech detection. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1668\u20131678.\\n\\nMaarten Sap, Saadia Gabriel, Lianhui Qin, Dan Jurafsky, Noah A Smith, and Yejin Choi. 2020. Social bias frames: Reasoning about social and power implications of language. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5477\u20135490.\\n\\nJennifer Saul. 2018. Dogwhistles, political manipulation, and philosophy of language. In Daniel Fogal, Daniel W. Harris, and Matt Moss, editors, New work on speech acts, volume 360, page 84. Oxford University Press Oxford.\\n\\nPaul E Smaldino, Thomas J Flamson, and Richard McElreath. 2018. The evolution of covert signaling. Scientific reports, 8(1):1\u201310.\\n\\nPaul E Smaldino and Matthew A Turner. 2021. Covert signaling is an adaptive communication strategy in diverse populations. Psychological review.\\n\\nAndrew Ifedapo Thompson and Ethan C Busby. 2021. Defending the dog whistle: The role of justifications in racial messaging. Political Behavior, pages 1\u201322.\\n\\nBrian P Tilley et al. 2020. \u201cI am the law and order candidate\u201d: A content analysis of Donald Trump\u2019s race-baiting dog whistles in the 2016 presidential campaign. Psychology, 11(12):1941.\\n\\nTamara van der Does, Mirta Galesic, Zackary Okun Dunivin, and Paul E Smaldino. 2022. Strategic identity signaling in heterogeneous networks. Proceedings of the National Academy of Sciences, 119(10):e2117898119.\\n\\nRachel Wetts and Robb Willer. 2019. Who is called by the dog whistle? Experimental evidence that racial resentment and political ideology condition responses to racially encoded messages. Socius, 5:2378023119866268.\\n\\nMichael Wiegand, Maja Geulig, and Josef Ruppenhofer. 2021a. Implicitly abusive comparisons\u2013a new dataset and linguistic analysis. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 358\u2013368.\\n\\nMichael Wiegand, Josef Ruppenhofer, and Elisabeth Eder. 2021b. Implicitly abusive language\u2013what does it actually look like and why are we not getting there? In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 576\u2013587. Association for Computational Linguistics.\\n\\nCanwen Xu, Wangchunshu Zhou, Tao Ge, Ke Xu, Julian McAuley, and Furu Wei. 2021. Blow the dog whistle: A Chinese dataset for cant understanding with commonsense and world knowledge. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2139\u20132145.\\n\\nFan Yang. 2016. Rethinking China\u2019s Internet Censorship: The practice of recoding and the politics of visibility. New Media & Society, 18(7):1364\u20131381.\\n\\nXuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, and Maarten Sap. 2023. Cobra frames: Contextual reasoning about effects and harms of offensive statements. In Findings of ACL.\"}"}
{"id": "acl-2023-long-845", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A\\n\\nTable A.1: Distribution of glossary entries across all registers, types, and personae.\\n\\n| Category                  | Count |\\n|---------------------------|-------|\\n| formal/offline            | 193   |\\n| informal/online           | 147   |\\n| stereotype-based target group label | 64   |\\n| concept (policy)          | 41    |\\n| concept (values)          | 37    |\\n| persona signal (symbol)   | 35    |\\n| stereotype-based descriptor | 34  |\\n| persona signal (self-referential) | 32 |\\n| concept (other)           | 29    |\\n| arbitrary target group label | 23  |\\n| persona signal (shared culture) | 18  |\\n| humor/mockery/sarcasm     | 11    |\\n| representative (Bogeyman) | 10    |\\n| phonetic-based target group label | 4  |\\n| persona signal (in-group label) | 2  |\\n| racist                    | 76    |\\n| transphobic               | 73    |\\n| antisemitic               | 73    |\\n| white supremacist          | 48    |\\n| Islamophobic              | 16    |\\n| conservative              | 8     |\\n| anti-liberal              | 7     |\\n| anti-Latino               | 6     |\\n| homophobic                | 6     |\\n| anti-vax                  | 5     |\\n| religious                 | 4     |\\n| climate change denier     | 4     |\\n| anti-Asian                | 3     |\\n| anti-LGBTQ                | 3     |\\n| liberal                   | 3     |\\n| xenophobic                | 2     |\\n| anti-GMO                  | 2     |\\n| misogynistic              | 1     |\\n\\nWe create 51 total request formulations that ask for generic examples of dogwhistles (n=17), dogwhistles that target specific social groups (n=25), and dogwhistles that are used by certain personae/in-groups (n=9). For each prompt, we also consider three spelling variations of \u201cdogwhistle\u201d: dogwhistle, dog-whistle, and dog whistle. Exact prompt text can be found in our project repository.\\n\\nTo encourage GPT-3 to generate a list, we conclude all prompts with a newline token followed by \u201c1.\u201d All prompts were provided to a GPT-3 Instruct model (text-davinci-002) with de-generative hyperparameters except for max_tokens=256, temperature=0.7, and num_outputs=5 (5 generations per prompt). The resulting texts are strings that take the form of an enumerated list. To aggregate and compare surfaced dogwhistles across each text completion, we post-process by: splitting by newline characters, removing enumeration and other punctuation, converting all outputs to lowercase, lemmatizing each surfaced term with SpaCy, and removing definite articles that precede generated dogwhistles. We then aggregate over all generations to determine how often each dogwhistle is surfaced for each in-group.\\n\\nIn calculating precision of dogwhistle surfacing, we mark each of the 154 candidate terms as true positives if they appear in the glossary. Some surfaced dogwhistles were marked as \u201ccorrect\u201d if they were closely related to a dogwhistle entry in our glossary, even if the exact term did not appear. Examples include national security, identity politics, the swamp, tax relief, and patriot. However, this is still a conservative estimate because our glossary is not exhaustive. GPT-3 surfaces a number of terms that potentially have dogwhistle usages but were not covered by our glossary, and thus not included in our precision estimates. Examples of these terms include names of Muslim political organizations (Hezbollah, Hamas, Muslim Brotherhood) and Second Amendment rights.\\n\\nFigure A.1 shows variation in precision of dogwhistle surfacing across prompt types (in-groups and generic prompting).\\n\\nA.2 Details for identifying covert meaning\\n\\nVariation across registers\\n\\nWe identify variation in GPT-3\u2019s ability to identify dogwhistles\u2019 covert meanings based on prompt features, dog-whistle surfacing, and persona cued in prompt.\"}"}
{"id": "acl-2023-long-845", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Albertson (2015) A dogwhistle is an expression that has different meanings to different audiences.\\n\\nHenderson and McCready (2018) A dogwhistle is a term that sends one message to an outgroup while at the same time sending a second (often taboo, controversial, or inflammatory) message to an ingroup.\\n\\nBhat and Klein (2020) A dogwhistle is a word or phrase that means one thing to the public at large, but that carry an additional, implicit meaning only recognized by a specific subset of the audience.\\n\\nMerriam-Webster A dogwhistle is a coded message communicated through words or phrases commonly understood by a particular group of people, but not by others.\\n\\nWikipedia A dogwhistle is the use of coded or suggestive language in political messaging to garner support from a particular group without provoking opposition.\\n\\nTable A.2: Definitions of dogwhistles and their sources used for prompting GPT-3.\\n\\nBelow are links for the Merriam-Webster and Wikipedia sources:\\n\\nhttps://www.merriam-webster.com/words-at-play/dog-whistle-political-meaning\\nhttps://en.wikipedia.org/wiki/Dog_whistle_(politics)\\n\\nFigure A.2: Percent of GPT-3 generations that capture dogwhistles' covert meanings, separated by register and if the prompt includes a definition or secret cue.\\n\\nGPT-3's performance varies widely across dogwhistle types in our taxonomy (\u00a72.1; Fig. 2). GPT-3 has the lowest performance for humor-based and arbitrary target group label dogwhistles, and the highest performance for representatives (Bogeymen), phonetic-based target group labels, and policies (Table A.4).\\n\\nTable A.3: Proportion of dogwhistles with covert meanings identified by GPT-3 across personae with at least 50 GPT-3 generations. GPT-3 has the lowest recognition of transphobic dogwhistles and the highest recognition of homophobic and Islamophobic dogwhistles.\\n\\nTable A.4: Proportion of dogwhistles with covert meanings identified by GPT-3 by the dogwhistle type.\"}"}
{"id": "acl-2023-long-845", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The cosmopolitan elite look down on the common affections that once bound this nation together: things like place and national feeling and religious faith. The cosmopolitan agenda has driven both Left and Right. It's time we ended the cosmopolitan experiment and recovered the promise of the republic.\\n\\nThe dogwhistle in this message is the term \\\"cosmopolitan elite,\\\" which is often used to target a specific group of people, such as urban, educated, and often liberal individuals, without explicitly naming them. It can also be interpreted as an anti-Semitic or anti-immigrant message, as \\\"cosmopolitan\\\" has historically been used to criticize Jewish people and immigrants for their perceived lack of loyalty to their country.\\n\\nWe have got this tailspin of culture, in our inner cities in particular, of men not working and just generations of men not even thinking about working or learning the value and the culture or work.\\n\\nThe dogwhistle in this message is the reference to \\\"inner cities\\\" and \\\"generations of men not working,\\\" which can be interpreted as coded language targeting African Americans or other minority groups living in urban areas, perpetuating stereotypes about laziness and unemployment.\\n\\n#IStandWithJKRowling because she wrote a series of books that made readers out of millions of children and adolescents. And she has every right to express her opinion on cultural matters. And because she's right about them.\\n\\nThe dogwhistle in this message is \\\"she's right about them,\\\" which indirectly supports J.K. Rowling's controversial opinions on gender and transgender issues without explicitly mentioning them.\\n\\nAnd together, we will win\\n\\nThe dogwhistle in this message is the use of the colors, which are associated with the women's suffrage movement. This suggests that the message is aimed at garnering support from women or feminists without explicitly stating it.\\n\\nDid you see Kyle? He's about this tall..\\n\\nThe dogwhistle in this message is \\\"Did you see Kyle? He's about this tall..\\\" which could be a reference to the white supremacist symbol of the \\\"K\\\" hand sign, where the fingers form the letters \\\"W\\\" and \\\"P\\\" for \\\"white power.\\\" The name \\\"Kyle\\\" is sometimes associated with white supremacist groups.\\n\\nTable A.5: Manual proof-of-concept analysis for using GPT-4 to identify dogwhistles in-context. The columns on the right indicate whether GPT-4 correctly identifies the dogwhistle term and its covert meaning, respectively.\\n\\n| Definition Source | GPT-3 Identified Covert Meaning (frac. of generations) |\\n|------------------|-------------------------------------------------------|\\n| None | 0.0 |\\n| Merriam-Webster | 0.1 |\\n| Albertson (2015) | 0.2 |\\n| Bhat & Klein (2020) | 0.3 |\\n| Henderson & McCready (2018) | 0.4 |\\n| Wikipedia | 0.5 |\\n\\nFigure A.3: Proportion of GPT-3 generations that correctly identify dogwhistles' covert meanings across prompted dogwhistle definitions and secret cues.\\n\\nTable A.6: Proportion of GPT-3 generations that correctly identify dogwhistles' covert meanings for each dogwhistle definition provided in prompting.\\n\\nVariation across dogwhistle definitions\\n\\nOnly 19.1% of GPT-3 generations include the correct covert meaning when prompted with no dogwhistle definition. Prompting GPT-3 with any of the five dogwhistle definitions greatly improved performance over no definition provided, but the extent varied, with the Merriam-Webster definition yielding the lowest improvement (43.8%) and Wikipedia yielding the highest (54.3%) (Table A.6). The boost in performance by adding the secret cue depends on the specific definition used; the secret cue has a bigger effect when using the Merriam-Webster and Albertson (2015) definitions (Figure A.3).\\n\\nWhere does GPT-3 perform poorly?\\n\\nMost unrecognized dogwhistles are part of the informal register, especially symbols (e.g. the transphobic spiderweb or cherry emojis). Other unrecognized dogwhistles include \\\"Operation Google\\\" terms (e.g. Skype, Yahoo), more recent terms (e.g. Let's Go Brandon), and several antisemitic and transphobic dogwhistles whose covert meanings are especially context-dependent (e.g. adult human female, XX, (Wikipedia) early life, fellow white people).\\n\\nUnrecognized formal dogwhistles tend to be extremely subtle and nuanced (e.g. Dred Scott as a conservative anti-abortion dogwhistle) or are highly-conventionalized phrases that may be far more commonly used without the covert implicature (e.g. the antisemitic dogwhistle poisoning the well).\"}"}
