{"id": "lrec-2024-main-835", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"JMultiWOZ: A Large-Scale Japanese Multi-Domain Task-Oriented Dialogue Dataset\\n\\nAtsumoto Ohashi*, Ryu Hirai*, Shinya Iizuka, Ryuichiro Higashinaka\\nGraduate School of Informatics, Nagoya University, Japan\\n{ohashi.atsumoto.c0, hirai.ryu.k6, iizuka.shinya.a8}@s.mail.nagoya-u.ac.jp\\nhigashinaka@i.nagoya-u.ac.jp\\n\\nAbstract\\nDialogue datasets are crucial for deep learning-based task-oriented dialogue system research. While numerous English language multi-domain task-oriented dialogue datasets have been developed and contributed to significant advancements in task-oriented dialogue systems, such a dataset does not exist in Japanese, and research in this area is limited compared to that in English. In this study, towards the advancement of research and development of task-oriented dialogue systems in Japanese, we constructed JMultiWOZ, the first Japanese language large-scale multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2.2. In addition, through evaluation experiments of interactive dialogues with the models and human participants, we identified limitations in the task completion capabilities of LLMs in Japanese.\\n\\nKeywords: Multi-domain task-oriented dialogue, Dialogue state tracking, Response generation\\n\\n1. Introduction\\nMethods based on deep learning have been actively introduced in the research of task-oriented dialogue systems (Gao et al., 2018; Zhang et al., 2020b), which have greatly improved their performance on task completion (Zhang et al., 2020a; Hosseini-Asl et al., 2020; He et al., 2022). Task-oriented dialogue datasets are essential for developing these neural models, and a number of single-domain task-oriented dialogue datasets in English have been developed previously (Henderson et al., 2014; Wen et al., 2017; Eric et al., 2017; Shah et al., 2018).\\n\\nMultiWOZ is a large-scale dialogue corpus that was developed to address more complex multi-domain dialogues (Budzianowski et al., 2018). It includes dialogues spanning seven domains, namely, tourist attractions, hotels, restaurants, taxis, trains, police stations, and hospitals, and led to the development of subsequent dialogue models. Following the introduction of MultiWOZ, numerous large-scale dialogue datasets have been constructed (Rastogi et al., 2020; Mosig et al., 2020; Chen et al., 2021), and task-oriented dialogue models using these as benchmarks have been actively researched. Cross-WOZ (Zhu et al., 2020) and other large-scale multi-domain dialogue datasets have been introduced for Chinese as well (Quan et al., 2020; Dai et al., 2022), promoting research on Chinese task-oriented dialogue systems.\\n\\nHowever, due to the high cost of constructing task-oriented dialogue corpora, there are fewer datasets in Japanese compared to English. In this study, we constructed JMultiWOZ, the first Japanese language multi-domain task-oriented dialogue dataset. Using JMultiWOZ, we evaluated the dialogue state tracking and response generation capabilities of the state-of-the-art methods on the existing major English benchmark dataset MultiWOZ2.2 and the latest large language model (LLM)-based methods. Our evaluation results demonstrated that JMultiWOZ provides a benchmark that is on par with MultiWOZ2.2. In addition, through evaluation experiments of interactive dialogues with the models and human participants, we identified limitations in the task completion capabilities of LLMs in Japanese.\\n\\nFigure 1: An example of dialogue across two domains: restaurants and taxis. The gray and green message bubbles represent the utterances of the user and the wizard, respectively. The red and blue boxes indicate the annotation of the dialogue state and the database results, respectively. The bubble for each utterance contains both the original Japanese utterance and its English translation by the authors.\"}"}
{"id": "lrec-2024-main-835", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"multi-domain task-oriented dialogue corpora in other languages compared to English and Chinese (Hung et al., 2022). In this study, we focus on Japanese, in which the research and development of task-oriented dialogue models based on deep learning have been limited thus far.\\n\\nToward the research and development of task-oriented dialogue systems in Japanese, we have constructed the Japanese Multi-Domain Wizard of Oz (JMultiWOZ), the first Japanese multi-domain task-oriented dialogue dataset. JMultiWOZ contains a total of 4,246 conversations spanning six travel-related domains (tourist attractions, accommodation, restaurants, shopping facilities, taxis, and weather). An example of a dialogue is shown in Figure 1. JMultiWOZ provides the dialogue state at each turn and the database of each domain for implementing and benchmarking task-oriented dialogue models.\\n\\nIn this paper, we outline the procedure for constructing the JMultiWOZ dataset and introduce its statistics. We evaluated the dataset on the two main tasks of task-oriented dialogue enabled by JMultiWOZ, i.e., dialogue state tracking (DST) and response generation (RG), using the state-of-the-art (SOTA) methods (Bang et al., 2023) and the latest LLM-based methods (Hude\u010dek and Dusek, 2023). For further validation, the end-to-end dialogue capability of these dialogue models was evaluated by interactions with human participants.\\n\\nThe contributions of this study are threefold:\\n\\n\u2022 We constructed JMultiWOZ, the first large-scale Japanese multi-domain task-oriented dialogue dataset.\\n\\n\u2022 We evaluated the dataset on DST and RG tasks using existing SOTA models and the latest LLMs, and demonstrated that JMultiWOZ can provide a Japanese benchmark of complexity comparable to that of the major English dataset MultiWOZ2.2.\\n\\n\u2022 We conducted a human evaluation experiment, which showed that, even with the latest LLMs, there remain challenges concerning the capabilities of task-oriented dialogue in Japanese.\\n\\n2. Related Work\\n\\n2.1. Task-Oriented Dialogue Corpora\\n\\nMany task-oriented dialogue corpora have been created in English thus far. Previously, single-domain dialogues, where only one domain appears in a dialogue, were predominant, such as WOZ2.0 (Wen et al., 2017), Frames (El Asri et al., 2017), and KVRET (Eric et al., 2017). All of these dialogues were conducted using the Wizard of Oz (WOZ) method (Kelley, 1984), with a human user and another person acting as the system (i.e., the wizard). There are also human-to-machine (Henderson et al., 2014) and machine-to-machine dialogue corpora simulated between machines (Shah et al., 2018). Multi-domain dialogue corpora, in which multiple domains appear in a single dialogue, are being increasingly constructed to address more complex requirements. MultiWOZ (Budzianowski et al., 2018) is a representative example, being a large-scale corpus with over 10,000 dialogues covering seven travel-related domains. Other existing large-scale multi-domain dialogue corpora include Schema-Guided Dialogue (Rastogi et al., 2020), STAR (Mosig et al., 2020), and ABCD (Chen et al., 2021).\\n\\nThere are also several large-scale multi-domain dialogue corpora in Chinese. CrossWOZ (Zhu et al., 2020) is the first multi-domain dialogue corpus in Chinese and contains about 6,000 travel-related dialogues. The RiSAWOZ corpus (Quan et al., 2020) introduced more domains and dialogues, and subsequently, CGoDial (Dai et al., 2022) was devised as an extension of other dialogue corpora including RiSAWOZ. In addition, BiTOD (Lin et al., 2021b) was constructed to develop bilingual multi-domain dialogue models in both English and Chinese.\\n\\nConstructing task-oriented dialogue corpora is generally costly, and there are few large-scale multi-domain datasets outside of English and Chinese (Hung et al., 2022). SCUD (Hayashibe, 2022) is a Japanese single-domain task-oriented dialogue corpus related to accommodation search but it only contains 210 dialogues.\\n\\n2.2. Translation-based Corpora\\n\\nGiven the high cost of constructing task-oriented dialogue corpora, efforts are being made to construct dialogue corpora in other languages by translating readily available English corpora. For instance, GlobalWOZ (Ding et al., 2022) is a dialogue corpus that expanded MultiWOZ into 17 languages through machine translation. Among the 17 languages, high quality has been achieved in three languages (Chinese, Spanish, and Indonesian) through post-editing by professional translators for some dialogues in the test set. However, the quality is not guaranteed for other languages, including Japanese. Other corpora constructed from machine translation and manual post-editing of MultiWOZ include AllWOZ (Zuo et al., 2021) and MultiWOZ (Hung et al., 2022), neither of which includes Japanese.\"}"}
{"id": "lrec-2024-main-835", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: List of slots defined in the ontology for each domain\\n\\n| Domain         | Slots                                                                 |\\n|----------------|----------------------------------------------------------------------|\\n| restaurant     | city, name, genre, area, pricerange, station, wifi, parking, opentime, phone, address, accessetime, closed, priceinfo, reference, people, day, time |\\n| accommodation  | city, name, genre, area, pricerange, station, wifi, parking, withrestaurant, phone, address, accesstime, checkintime, checkouttime, priceinfo, reference, people, day, stay |\\n| attraction     | city, name, genre, area, station, wifi, parking, opentime, phone, address, accesstime, closed, adultfee, childfee, priceinfo |\\n| shopping       | city, name, genre, area, station, parking, opentime, phone, address, accesstime, closed |\\n| taxi           | city, name, cashless, jumbo, phone, reference, day, time, departurepoint, arrivalpoint |\\n| weather        | city, area, day, weather, mintemperature, maxtemperature |\\n\\nProblems due to poor translations have been reported in translation-based dialogue corpora (e.g., 'translationese,' lack of cultural adaptation) (Majewska et al., 2023), which may prevent the models' practical performance from being evaluated accurately (Hu et al., 2023). Therefore, this study aims to construct a realistic dataset in the Japanese context by collecting dialogues from scratch.\\n\\n3. Data Collection\\n\\nJMultiWOZ is a corpus containing dialogues of travelers planning a trip to one of nine cities in Japan (Sapporo, Sendai, Tokyo, Yokohama, Nagoya, Kyoto, Osaka, Fukuoka, and Naha) while collecting tourist information. The six domains include tourist attractions, accommodations, restaurants, shopping facilities, taxis, and weather. Using the WOZ method, each dialogue was conducted by two human interlocutors, one as a traveler (user) and the other as an information provider (wizard).\\n\\nThe following sections describe the five steps of constructing this corpus: (1) ontology definition, (2) construction of the backend database that the wizard uses to obtain travel information, (3) design and creation of user goals, (4) dialogue collection, and (5) annotation of the full dialogue state.\\n\\n3.1. Definition of Ontology\\n\\nIn a task-oriented dialogue, the ontology represents the structure of the backend database. Specifically, it defines attributes such as name, address, and phone number for each entity in the database. An entity is a unit of record in the database, such as a specific tourist attraction or restaurant, and its attributes are called slots. In this study, the ontology of each domain was defined with reference to existing studies (Budzianowski et al., 2018; Zhu et al., 2020) while considering the characteristics of Japanese culture so as not to be unnatural. For instance, dialogues related to the police and hospital domains that exist in MultiWOZ are rarely encountered in Japanese dialogue travel centers. In view of this, we eliminated police and hospital domains, and instead introduced more culturally appropriate domains such as shopping and weather. The ontology for all domains is shown in Table 1.\\n\\n3.2. Construction of Backend Database\\n\\nBased on the ontology, a backend database that the wizard uses to retrieve entities and travel information during the dialogue was constructed for each domain. To enhance the realism of the dialogue using real entities, lists of facilities publicly available from governments and municipalities of various cities were used to construct databases for tourist spots, accommodations, restaurants, shopping facilities, and taxis. From this list, only facilities that have publicly accessible websites were selected to be included in the database, and information for all slots of each entity was manually obtained from the website. For the taxi domain, the unit of entities was set as taxi companies. The final number of entities contained in the database for each domain were as follows: 447 for tourist spots, 884 for accommodations, 952 for restaurants, 445 for shopping facilities, and 167 for taxis. For the construction of the weather domain database, the unit of entity was set as the date, and 365 days' worth of weather information was artificially created for each city.\\n\\n3.3. Design of User Goal\\n\\nA user goal is the objective the user aims to achieve through the dialogue with the wizard, and one goal is set for each dialogue. An example of a user goal is shown in Table 2. Each goal covers one or more domains, and each domain in the goal consists of one or more informable slots, which are the search criteria for the entity the user is looking for, such as the desired budget or destination, and one or more requestable slots, which are the attribute information the user requires for the goal.\"}"}
{"id": "lrec-2024-main-835", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Web UI of wizard for dialogue collection: (A): A search form for an entity from the backend database, (B): An interface displaying detailed information about the selected entity and a reservation form for the entity, (C): An interface for chatting with the user.\\n\\n| Domain Slot       | Value          |\\n|-------------------|----------------|\\n| attraction info.  | city = Nagoya  |\\n|                   | parking = yes(free) |\\n|                   | wifi = yes(free) |\\n| reqt. area, station, closed | |\\n| accommodation info. | pricerange = expensive |\\n|                   | wifi = yes(free) |\\n|                   | reqt. accesstime, genre |\\n\\nTable 2: Example of a user goal. \u201cinfo.\u201d and \u201creqt.\u201d in the \u201cSlot\u201d column indicate informable and requestable slots, respectively.\\n\\nThe composition and diversity of user goals are directly linked to the naturalness and diversity of the dialogues in the corpus. To introduce diversity in dialogue length and complexity, 1-3 domains are randomly selected for each user goal. Next, slots to be included in each domain are randomly chosen. From the ontology defined in Section 3.1, 2-7 slots, including informable, requestable, and possibly bookable slots, are selected for each domain. The informable slot \u201ccity\u201d, which indicates the user\u2019s tourist destination city, is shared among the domains within a user goal. To enhance the realism of the dialogue, following (Budzianowski et al., 2018), some goals were set with tasks to change the value of an informable slot and/or book slot amid the dialogue (for instance, change the originally communicated reservation condition of 5 p.m. to 6 p.m.). Ahead of subsequent dialogue collection, a total of 5,000 unique user goals were created.\\n\\n3.4. Dialogue Collection\\n\\nDialogues were collected using a backend database and randomly generated dialogue goals. Based on the dialogue collection platform of (Zhu et al., 2020), the dialogue web UIs for the user and wizard were implemented. The web UI used by the wizard is depicted in Figure 2. Crowd workers for both the user and wizard roles were recruited via Lancers, a major Japanese crowdsourcing service. Only those who consented to the publication of data obtained from the dialogue collection participated.\\n\\nThe workers read the instructions in the dialogue manual, watched a demonstration video that explained how to operate the web UI, and learned the workflow before participating in the dialogues. To ensure diverse user utterances, each user could participate in a maximum of 100 dialogues, and they could engage with the same wizard for a maximum of 20 dialogues. Meanwhile, because it is preferable for the wizard to behave consistently, there was no limit to the number of dialogues, and the same wizard was allowed to engage in conversations repeatedly. In the end, 65 users and 18 operators participated in the dialogue collection.\\n\\n2 https://github.com/thu-coai/CrossWOZ\\n3 https://www.lancers.jp\"}"}
{"id": "lrec-2024-main-835", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. You are planning a trip to Nagoya.\\n2. Find a tourist attraction to visit for one day. Choose a place where the parking is free. The place should also have free Wi-Fi.\\n3. Once you find a desired tourist spot, ask about its area, nearest station, and closed days.\\n4. Find a place to stay for the night. The budget is on the higher side. The place should also offer free Wi-Fi.\\n5. Once you find a suitable accommodation, ask about the time it takes from the nearest station and the type of accommodation.\\n\\nTable 3: Explanation of the user goal shown in Table 2. The blue text indicates informable slot values, while the green text indicates requestable slot values.\\n\\nThe tasks of the user and wizard, and the quality control for the wizards are explained in detail in the following paragraphs.\\n\\nUser's Task\\nThe user aims to properly convey the informable slots in the user goal to the wizard, obtain information about the requestable slots from the wizard, and in some cases, reserve the entity with the conditions of booking slots. Instead of a user goal formatted as in Table 2, users comprehend the goal by reading template sentences that describe each slot. Template sentences that explain the user goal in Table 2 are shown in Table 3.\\n\\nWizard's Task\\nThe wizard searches for entities in the backend database that match the constraints conveyed by the user and provides the user with information about the found entities. The wizard's web UI (Figure 2) has interfaces for (A) searching for entities based on the user's criteria and (B) checking detailed information or making reservations for the found entities in addition to (C) the panel for chatting with the user. The database search query (DB query) input by the wizard in each turn is recorded as part of the dialogue state (details can be found in Section 3.5).\\n\\nQuality Control of Wizard\\nSome studies (Eric et al., 2020; Zanget al., 2020) have reported issues such as inconsistencies in the slot value notation within the dialogue state (e.g., the value \u201c18:00\u201d for the time slot appears in several ways: \u201c1800\u201d, \u201c0600pm\u201d, and \u201c6 PM\u201d) in existing corpora. Such inconsistencies can confuse or underestimate the capabilities of the dialogue model, making it impossible to provide an appropriate benchmark. Therefore, input values were selected from dropdown menus to prevent wizards from manually entering DB queries. Additionally, to enhance the quality of the wizard, 3-5 practice dialogues were conducted.\\n\\nTable 4: Statistics of JMultiWOZ\\n\\n|                | Dialogues | Turns | Tokens | Vocab. |\\n|----------------|-----------|-------|--------|--------|\\n| Total Train    | 4,246     | 61,186| 1,102,658| 11,121 |\\n| Dev            | 3,654     | 52,405| 943,653| 10,309 |\\n| Test           | 300       | 4,346 | 78,683 | 2,854  |\\n| Test           | 300       | 4,435 | 80,322 | 2,971  |\\n\\nTable 5: Examples of a DB query obtained during dialogue collection and a dialogue state obtained from the additional annotation beforehand. The workers received feedback from the authors on errors. Only workers who no longer had issues after repeated feedback participated in the actual dialogues.\\n\\nThrough the above procedure, a total of 4,508 dialogues were collected. Dialogues that would be considered noise in the dataset were then revised or excluded through the following two procedures:\\n1. At the end of each dialogue, workers completed questionnaires to report any issues that occurred during the dialogue. We manually checked dialogues with reported issues and removed those containing major errors, such as when workers misinterpreted the user goal.\\n2. We deleted dialogues in which the DB query at the end of the dialogue did not match the informable slots in the user goal.\\n\\nThe resulting corpus after this modification contained a total of 4,246 dialogues. Here, the dev and test sets consist of 300 dialogues each, randomly selected from the 4,246 dialogues, and the remaining 3,646 dialogues are used as the train set (refer to Table 4 for the statistics of each set).\\n\\n3.5. Annotation of Full Dialogue State\\nThe dialogue state is the information about the conditions of the entity that the user seeks, known up to each turn and recorded as a set of slot-value pairs. The DB query entered by the wizard at each turn can be used as a part of the dialogue state annotation. However, the DB query does not contain any non-explicitly communicated values. For\"}"}
{"id": "lrec-2024-main-835", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Number of slot-value pairs annotated in JMultiWOZ\\n\\n| Language | MultiWOZ | CrossWOZ | JMultiWOZ |\\n|----------|----------|----------|-----------|\\n| **Domains** | 7 | 5 | 6 |\\n| **Slots** | 57 | 72 | 79 |\\n| **Dialogues** | 8,438 | 5,012 | 3,646 |\\n| **Turns** | 115,424 | 84,692 | 52,405 |\\n| **Avg. domains** | 1.8 | 3.24 | 2.02 |\\n| **Avg. turns** | 13.7 | 16.9 | 14.4 |\\n\\nTable 7: Comparison of multi-domain task-oriented dialogue datasets on the train set; \u201cSynchronous\u201d indicates whether one user and one wizard actually conducted the dialogue synchronously during the dialogue collection.\\n\\nexample, as shown by Table 5, if the user accepts the entity name proposed by the system (e.g., wizard: \u201cHow about JRINNSapporo?\u201d, and user: \u201cOK, what are the prices like?\u201d), the entity name is not searched by the wizard; hence it is not recorded automatically.\\n\\nTo build the complete dialogue state, we recruited additional crowd workers to annotate this non-explicit value. After reading the manual, each worker annotated the values using a dedicated UI. As in the case of dialogue collection, the values were input by selection in order to suppress the perturbation of the slot value notations. To ensure the dialogue quality, each worker annotated ten dialogues for training and received feedback from the authors on errors. After repeated feedback, only those workers who no longer had issues participated in the annotation process.\\n\\nIn the end, six workers shared the annotation tasks for a total of 30,593 wizard turns.\\n\\nTable 6 shows the statistics of the number of slots in the dialogue states. A total of 58,745 slots (about 37.8% of the slots recorded in the DB query) were added to the dialogue states. Here, 59 randomly selected dialogues were annotated by the authors, and the match rate between these annotations and those annotated by the workers was 94.1%, indicating that the annotations were sufficiently high quality.\\n\\nOf the total 61,186 turns in the dataset (see Table 4), 30,593 were wizard turns, and the dialogue states were annotated for them.\\n\\nFigure 3: The distribution of dialogue lengths, divided into dialogues containing only one domain (single-domain) and dialogues containing two or more domains (multi-domain).\\n\\n3.6. Statistics\\n\\nTable 7 shows the statistics of JMultiWOZ compared with the major multi-domain task-oriented dialogue datasets in English and Chinese, MultiWOZ and CrossWOZ, respectively. Given that the number of domains, the number of slots, the average number of domains, and the average number of turns are roughly equivalent, the complexity of dialogues in JMultiWOZ can be considered to be on par with the existing datasets. Figure 3 illustrates the distribution of the number of dialogue turns when all 4,246 dialogues are divided into dialogues containing only one domain (single-domain) and dialogues containing multiple domains (multi-domain). A wide variety of length and complexity can be seen in both types of dialogues.\\n\\n4. Benchmark\\n\\nJMultiWOZ provides benchmarks for two common tasks in task-oriented dialogues, dialogue state tracking (DST) and response generation (RG). DST is the task of estimating the dialogue state at each turn, while RG is the task of generating the next system response based on the dialogue context at each turn. To demonstrate that JMultiWOZ can provide benchmarks equivalent to those in existing English dialogue corpora, we evaluate the aforementioned two tasks in JMultiWOZ using the SOTA methods from MultiWOZ2.2 (Zang et al., 2020) and the latest LLM-based methods. Note that MultiWOZ2.2 is a version in which various annotation errors in the original MultiWOZ have been corrected.\\n\\n4.1. Baseline Models\\n\\nThe most recent task-oriented dialogue models can be generally categorized into two approaches: (1) fine-tuning with supervised learning on medium-sized pretrained language models (Supervised Fine-Tuning; SFT) (Lin et al., 2021a; Su et al., 2022), and (2) generating responses in a zero-/few-shot manner.\"}"}
{"id": "lrec-2024-main-835", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) T5 Pipeline (Bang et al., 2023). First, the dialogue state is predicted from the given dialogue context, and the result is added to the input to generate the final response. Both are performed on the same model.\\n\\n(b) LLM pipeline (Hude\u010dek and Dusek, 2023) in zero-shot setting. First, the current active domain is estimated from the dialogue context. Next, the dialogue state is tracked, and the response is generated for that domain using a prompt focused on that domain.\\n\\nFigure 4: Task-oriented dialogue model pipeline for DST and RG\\n\\nNote that the model-predicted dialogue states were used as input for RG, instead of the ground-truth dialogue states. This is in line with the settings used in previous studies (Bang et al., 2023; Hude\u010dek and Dusek, 2023), allowing for comparison with the scores reported in those studies.\\n\\nSupervised fine-tuning\\nFor SFT, we use the T5 (Raffel et al., 2020)-based model TOA-TOD (Bang et al., 2023), which is the SOTA model in MultiWOZ2.2. TOA-TOD uses T5-base pretrained on a large number of task-oriented dialogue corpora as its backbone model, but such corpora do not exist in Japanese. Therefore, for the evaluation of JMultiWOZ, we simply use T5-base/large pretrained on Japanese data as backbone models. Figure 4a depicts the overview of response generation using T5. First, the dialogue state is estimated from the dialogue context (i.e., DST). Then, the database search results, obtained by using the dialogue state as a DB query, are combined with the context and dialogue state to generate the final response (i.e., RG). During fine-tuning, the mapping of ground truth inputs and outputs at each step is learned using maximum likelihood estimation (MLE). Because the same model is used to perform both DST and RG, a prefix indicating the task is added to the input. Specific examples of the input-output sequences used in the training of T5 can be found in Section A in the appendix.\\n\\nIn the training of T5-base and T5-large, the batch size was set to 32, and they were trained for five epochs. The AdamW (Loshchilov and Hutter, 2019) optimizer was used, and the learning rate was initially set to 5e-5 and then linearly decayed according to the number of steps. For evaluation, the checkpoint at the final step was used, and for inference in DST and RG, greedy search was adopted in both cases.\\n\\nLLM zero-/few-shot\\nWe use the zero-/few-shot response generation pipeline using an LLM introduced by Hude\u010dek and Dusek (2023). Figure 4b shows the 3-step flow of zero-shot response generation. First, the LLM estimates the current active domain from the context. Then, focusing on that domain, it estimates the dialogue state. Based on the state, it searches the DB and uses all of the results to generate the final response. In the few-shot setting, two examples retrieved from the train set are mixed into each prompt in the pipeline. These examples are retrieved from the train set based on the similarity between the dialogue context embeddings of the retrieved examples and the current context embeddings. The Japanese sentence-transformers were used to create embedding vectors using two consecutive turns of utterances as the dialogue context.\\n\\nWe used APIs of OpenAI GPT-3.5 (gpt-3.5-turbo) and GPT-4 (gpt-4) for the LLM-based zero-/few-shot settings. GPT-3.5 is the highest performing model for DST and RG on MultiWOZ, as reported by Hude\u010dek and Dusek (2023).\"}"}
{"id": "lrec-2024-main-835", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Automatic evaluation results of the baseline model. Dagger \u2020 indicates that the score of T5-base in MultiWOZ2.2 is cited from the evaluation results of TOATOD reported in (Bang et al., 2023). Double-dagger \u2021 indicates that the score of GPT-3.5 in MultiWOZ2.2 is cited from the reported value in (Hude\u010dek and Dusek, 2023).\\n\\n4 is the latest model provided by OpenAI's API and is known for its high capabilities in Japanese. The Japanese prompts were created based on the prompts in (Hude\u010dek and Dusek, 2023). See Section B in the Appendix for examples of the prompts.\\n\\n4.2. Evaluation Metrics\\nTo evaluate DST, we used the most standard metric, joint goal accuracy (JGA). JGA is a measure of whether the estimated dialogue state perfectly matches the ground truth dialogue state at each turn, resulting in a binary outcome (0/1). Additionally, we also utilized Slot-F1 to evaluate for each turn the match rate between the set of slot values in the estimated dialogue state and the set of slot values in the ground truth state, in terms of F1 score. To evaluate RG, we used the BLEU score, which indicates the similarity between the generated response and the ground truth response.\\n\\n4.3. Results\\nTable 8 shows a comparison between the evaluation results reported in previous studies on MultiWOZ2.2 and those of JMultiWOZ. On JMultiWOZ, the SFT method, namely T5-base/large, generally had the highest performance for both DST and RG, and there seemed to be certain limitations to the performance of LLMs. This trend is consistent with the results of MultiWOZ2.2.\\n\\nFor the DST metrics, Slot-F1 was higher in JMultiWOZ. As described in Section 3.4, in the construction of JMultiWOZ, we chose the selection-based input for the DB search query by the wizard to reduce variations in dialogue state reported as issues in MultiWOZ. This enabled the dialogue models to predict exact values without confusion, resulting in higher slot-F1.\\n\\nFor JGA, the primary metric for DST, the difference between MultiWOZ2.2 and JMultiWOZ was minimal. This suggests that the complexity and annotation accuracy of JMultiWOZ are comparable to that of MultiWOZ2.2, demonstrating its capacity to provide benchmarks equivalent to the existing datasets. Notably, there is a difference of about 5% in JGA for T5-base across both corpora. This gap can be attributed to the fact that in MultiWOZ2.2, besides supervised learning, T5-base was boosted via reinforcement learning with the reward functions where JGA was incorporated.\\n\\nFor the RG metric, BLEU, JMultiWOZ yielded significantly higher scores compared to MultiWOZ2.2. This is likely because the wizard's utterances in JMultiWOZ are more consistent than in MultiWOZ2.2. During the dialogue collection for JMultiWOZ, the dialogues were conducted synchronously, ensuring that the wizards did not switch in the middle of each dialogue. Moreover, we provided ample training for the wizards. Through such quality control measures, we believe that consistent system responses could be attained.\\n\\n5. Human Evaluation\\nThe model's actual task completion capability in real dialogues must be assessed by actual people when evaluating task-oriented dialogue models as well as by automatic evaluation (Iizuka et al., 2023). In this section, we evaluate the end-to-end dialogue capabilities of the four dialogue models acquired in Section 4, namely, T5-base, T5-large, GPT-3.5, and GPT-4, by having them engage in conversations with crowd workers. Note that both GPT-3.5 and GPT-4 used a few-shot setting.\\n\\n5.1. Settings\\nThe setting of this evaluation experiment followed that of Iizuka et al. (2023), where they evaluated the end-to-end dialogue performance of the TOATOD model.\"}"}
{"id": "lrec-2024-main-835", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Human evaluation results. \u201cN\u201d indicates the number of participants who interacted with each model. \u201cUnd.\u201d, \u201cApp.\u201d, and \u201cSat.\u201d indicate the worker\u2019s subjective evaluation of the system\u2019s understanding, appropriateness of the system response, and satisfaction with the dialogue, respectively. Dagger \u2020 indicates that the score in MultiWOZ2.2 is cited from the reported value in (Iizuka et al., 2023). TOD (Bang et al., 2023) and Hude\u010dek and Dusek (2023)\u2019s LLM pipeline models built based on the MultiWOZ2.2 by using crowdsourcing.\\n\\nSpecifically, each worker was first given dialogue instructions and a user goal for each dialogue session. These user goals were randomly sampled from the test set. Then, they engaged in a dialogue with one of the four models. Each dialogue was set to a maximum of 20 turns (one turn consists of one user utterance and one system response), and workers judged whether they achieved the user goal within 20 turns. After the dialogue, the workers evaluated (1) the system\u2019s language understanding ability, (2) the appropriateness of the system\u2019s responses, and (3) their overall satisfaction with the dialogue, each on a 5-point scale. The workers were restricted to converse with only one system.\\n\\n5.2. Results\\n\\nTable 9 presents the evaluation results on JMultiWOZ, as well as the results reported by Iizuka et al. (2023) on MultiWOZ2.2. For the models trained with SFT, namely T5-base/large, there was no significant difference in Success, compared to T5-base (TOD) on MultiWOZ2.2. These results show that using JMultiWOZ allows for the development and evaluation of Japanese end-to-end dialogue models with performance comparable to that of MultiWOZ2.2.\\n\\nThe performance of the LLM models, i.e., GPT-3.5 and GPT-4, significantly declined compared to that of MultiWOZ2.2. This may be because even the latest LLMs, such as GPT-4, are not capable of handling dynamically changing dialogue contexts in Japanese. Specifically, since the model\u2019s predicted dialogue states and system responses are accumulated in the dialogue history, errors propagate gradually, making it difficult to maintain multi-turn conversations. GPT-4\u2019s Japanese language ability is not as high as that for English (OpenAI, 2023), and this difference in ability is likely reflected in the performance of task-oriented dialogue systems. This limitation of dialogue skills in LLMs in non-English languages should be addressed with multilingual resources in the future, and we anticipate that JMultiWOZ can contribute to such studies.\\n\\n6. Conclusion\\n\\nWe have presented JMultiWOZ, the first large-scale Japanese multi-domain task-oriented dialogue dataset. This corpus contains 4,246 WOZ dialogues spanning six travel-related domains and provides benchmarks for dialogue state tracking and response generation. Using JMultiWOZ, we evaluated existing SOTA methods with the T5-based models and the latest LLM-based methods, namely GPT-3.5/4, and demonstrated that JMultiWOZ performs comparably to the major benchmark dataset in English, MultiWOZ. Furthermore, we confirmed that the capabilities of GPT-3.5/4 in particular are limited in Japanese.\\n\\nFor future work, we would like to conduct comprehensive evaluation using more diverse models, including other LLMs with high multilingual abilities (Team et al., 2023). We hope that the development of JMultiWOZ will lead to further research on Japanese dialogue systems, including the improvement of LLMs\u2019 task-oriented dialogue capabilities in Japanese and the development of a multilingual task-oriented dialogue model.\\n\\n7. Limitations\\n\\nIn this study, to collect native dialogues specific to Japan, we collected online conversations through Japanese crowd workers. However, in prior research, multilingual corpora were built through machine translations of MultiWOZ, and GlobalWOZ (Ding et al., 2022) includes Japanese dialogue data, albeit being entirely machine-translated. Therefore, in the future, it will be necessary to investigate the advantages of JMultiWOZ, which we collected from scratch, compared to GlobalWOZ. Additionally, it may be possible to further improve performance by using GlobalWOZ for pre-training and then fine-tuning with our JMultiWOZ; this will need to be validated in the future.\\n\\nJMultiWOZ provides benchmarks for two main tasks, DST and RG, through dialogue state anno-\"}"}
{"id": "lrec-2024-main-835", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, it does not yet have dialogue act (DA) annotations (Budzianowski et al., 2018; Eric et al., 2020). This means that it does not provide a benchmark for another major task in task-oriented dialogue, policy optimization. Therefore, to further enhance its utility, DA annotations should be added in the future.\\n\\n8. Ethical Considerations\\n\\nOur ethical considerations span across data sources, dialogue collection, human evaluation, and the implications of using pretrained language models (PLMs).\\n\\nBackend Database\\n\\nWe exclusively employed information sources free from copyright constraints. Our list of entities was primarily derived from websites operated by the government or municipalities. Specific information for each entity was solely extracted from their respective official websites, ensuring authenticity and credibility. We consciously abstained from using tourism sites, or any other source potentially encumbered by copyright issues.\\n\\nDialogue Collection and Human Evaluation\\n\\nPrior to our data collection and evaluation experiments, ethical approval was obtained from the affiliated organization. In the data collection and evaluations, we engaged only crowd workers who explicitly consented to abstain from (1) unsolicited disclosure of personal information during dialogues. Additionally, these workers agreed to (2) relinquish copyright claims over data and artifacts produced during the dialogue collection phase and (3) publication of collected dialogue data. The data to be released will not contain any personally identifiable information of the workers, such as their names.\\n\\nDialogue Modeling with PLMs\\n\\nThe pre-training data of the PLMs uses a vast amount of textual data from Internet information. Therefore, our dialogue models based on these PLMs may produce potentially harmful or discriminatory responses.\\n\\n9. Acknowledgments\\n\\nThis work was supported by JST Moonshot R&D Grant number JPMJMS2011. We used the computational resources of the supercomputer \u201cFlow\u201d at the Information Technology Center, Nagoya University.\\n\\n10. Bibliographical References\\n\\nNamo Bang, Jeehyun Lee, and Myoung-Wan Koo. 2023. Task-Optimized Adapters for an End-to-End Task-Oriented Dialogue System. In Findings of the Association for Computational Linguistics: ACL 2023, pages 7355\u20137369.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bohsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u0107. 2018. MultiWOZ - A Large-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016\u20135026.\\n\\nDerek Chen, Howard Chen, Yi Yang, Alexander Lin, and Zhou Yu. 2021. Action-Based Conversations Dataset: A Corpus for Building More In-Depth Task-Oriented Dialogue Systems. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3002\u20133017.\\n\\nYinpei Dai, Wanwei He, Bowen Li, Yuchuan Wu, Zheng Cao, Zhongqi An, Jian Sun, and Yongbin Li. 2022. CGoDial: A large-scale benchmark for Chinese goal-oriented dialog evaluation. In Proceedings of the 2022 Conference on Empirical MethodsinNaturalLanguageProcessing, pages 4097\u20134111.\\n\\nBosheng Ding, Junjie Hu, Lidong Bing, Mahani Aljunied, Shafiq Joty, Luo Si, and Chunyan Miao. 2022. GlobalWoZ: Globalizing MultiWoZ to develop multilingual task-oriented dialogues systems. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics, pages 1639\u20131657.\\n\\nLayla El Asri, Hannes Schulz, Shikhar Sharma, Jeremie Zumer, Justin Harris, Emery Fine, Rahul Mehrotra, and Kaheer Suleman. 2017. Frames: a corpus for adding memory to goal-oriented dialogue systems. In Proceedings of the 18th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 207\u2013219.\\n\\nMihail Eric, Rahul Goel, Shachi Paul, Abhishek Sethi, Sanchit Agarwal, Shuyang Gao, Adarsh Kumar, Anuj Goyal, Peter Ku, and Dilek Hakkani-T\u00fcr. 2020. MultiWOZ 2.1: A consolidated multi-domain dialogue dataset with state corrections and state tracking baselines. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 422\u2013428.\\n\\nMihail Eric, Lakshmi Krishnan, Francois Charette, and Christopher D. Manning. 2017. Key-value...\"}"}
{"id": "lrec-2024-main-835", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"retrieval networks for task-oriented dialogue. In Proceedings of the 18th Annual Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 37\u201349. Jianfeng Gao, Michel Galley, and Lihong Li. 2018.\\n\\nNeural Approaches to Conversational AI. In Proceedings of the 41st International ACM SIGIR Conference on Research & Development in Information Retrieval, pages 1371\u20131374. Yuta Hayashibe. 2022.\\n\\nSelf-Contained Utterance Description Corpus for Japanese Dialog. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1249\u20131255. Wanwei He, Yinpei Dai, Yinhe Zheng, Yuchuan Wu, Zheng Cao, Dermot Liu, Peng Jiang, Min Yang, Fei Huang, Luo Si, Jian Sun, and Yongbin Li. 2022.\\n\\nGALAXY: A Generative Pre-trained Model for Task-Oriented Dialog with Semi-supervised Learning and Explicit Policy Injection. In Proceedings of the AAAI Conference on Artificial Intelligence, 10, pages 10749\u201310757. Matthew Henderson, Blaise Thomson, and Jason D. Williams. 2014.\\n\\nThe Second Dialog State Tracking Challenge. In Proceedings of the 15th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 263\u2013272. Ehsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020.\\n\\nA simple language model for task-oriented dialogue. In Proceedings of Advances in Neural Information Processing Systems, pages 20179\u201320191. Songbo Hu, Han Zhou, Mete Hergul, Milan Gritta, Guchun Zhang, Ignacio Iacobacci, Ivan Vuli\u0107, and Anna Korhonen. 2023.\\n\\nMulti 3 WOZ: A multilingual, multi-domain, multi-parallel dataset for training and evaluating culturally adapted task-oriented dialog systems. Transactions of the Association for Computational Linguistics, pages 1396\u20131415. Vojt\u011bch Hude\u010dek and Ondrej Dusek. 2023.\\n\\nAre Large Language Models All You Need for Task-Oriented Dialogue? In Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 216\u2013228. Chia-Chien Hung, Anne Lauscher, Ivan Vuli\u0107, Simone Ponzetto, and Goran Glava\u0161. 2022.\\n\\nMulti2WOZ: A robust multilingual dataset and conversational pretraining for task-oriented dialog. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3687\u20133703. Shinya Iizuka, Shota Mochizuka, Atsumoto Ohashi, Sanae Yamashita, Ao Guo, and Ryuichiro Higashinaka. 2023.\\n\\nClarifying the Dialogue-Level Performance of GPT-3.5 and GPT-4 in Task-Oriented and Non-Task-Oriented Dialogue Systems. In Proceedings of AAAI Fall Symposium on Artificial Intelligence for Human-Robot Interaction, pages 182\u2013186. J. F. Kelley. 1984.\\n\\nAn iterative design methodology for user-friendly natural language office information applications. ACM Transactions on Information Systems, 2:26\u201341. Zhaojiang Lin, Bing Liu, Seungwhan Moon, Paul Crook, Zhenpeng Zhou, Zhiguang Wang, Zhou Yu, Andrea Madotto, Eunjoon Cho, and Rajen Subba. 2021a.\\n\\nLeveraging Slot Descriptions for Zero-Shot Cross-Domain Dialogue State Tracking. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5640\u20135648. Zhaojiang Lin, Andrea Madotto, Genta Indra Winata, Peng Xu, Feijun Jiang, Yuxiang Hu, Chen Shi, and Pascale Fung. 2021b.\\n\\nBiTOD: A bilingual multi-domain dataset for task-oriented dialogue modeling. arXiv preprint arXiv:2106.02787. Ilya Loshchilov and Frank Hutter. 2019.\\n\\nDecoupled weight decay regularization. In International Conference on Learning Representations. Olga Majewska, Evgeniia Razumovskaia, Edoardo M. Ponti, Ivan Vuli\u0107, and Anna Korhonen. 2023.\\n\\nCross-Lingual Dialogue Dataset Creation via Outline-Based Generation. Transactions of the Association for Computational Linguistics, pages 139\u2013156. Johannes EM Mosig, Shikib Mehri, and Thomas Kober. 2020.\\n\\nSTAR: A Schema-Guided Dialog Dataset for Transfer Learning. arXiv preprint arXiv:2010.11853. OpenAI. 2023.\\n\\nGPT-4 Technical Report. ArXiv, abs/2303.08774. Jun Quan, Shian Zhang, Qian Cao, Zizhong Li, and Deyi Xiong. 2020.\\n\\nRiSAWOZ: A large-scale multi-domain Wizard-of-Oz dataset with rich semantic annotations for task-oriented dialogue modeling. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 930\u2013940. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\"}"}
{"id": "lrec-2024-main-835", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. *Journal of Machine Learning Research*, pages 1\u201367.\\n\\nGon\u00e7alo Raposo, Luisa Coheur, and Bruno Martins. 2023. Prompting, retrieval, training: An exploration of different approaches for task-oriented dialogue generation. In *Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue*, pages 400\u2013412.\\n\\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara, Raghav Gupta, and Pranav Khaitan. 2020. Towards Scalable Multi-Domain Conversational Agents: The Schema-Guided Dialogue Dataset. In *Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pages 8689\u20138696.\\n\\nPararth Shah, Dilek Hakkani-T\u00fcr, Gokhan T\u00fcr, Abhinav Rastogi, Ankur Bapna, Neha Nayak, and Larry Heck. 2018. Building a Conversational Agent Overnight with Dialogue Self-Play. *arXiv preprint arXiv:1801.04871*.\\n\\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task pre-training for plug-and-play task-oriented dialogue system. In *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics*, pages 4661\u20134676.\\n\\nGemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. 2023. Gemini: a family of highly capable multimodal models. *arXiv preprint arXiv:2312.11805*.\\n\\nTsung-Hsien Wen, David Vandyke, Nikola Mrk\u0161i\u0107, Milica Ga\u0161i\u0107, Lina M. Rojas-Barahona, Pei-Hao Su, Stefan Ultes, and Steve Young. 2017. A Network-based End-to-End Trainable Task-oriented Dialogue System. In *Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics*, pages 438\u2013449.\\n\\nXiaoxue Zang, Abhinav Rastogi, Srinivas Sunkara, Raghav Gupta, Jianguo Zhang, and Jindong Chen. 2020. MultiWOZ 2.2: A dialogue dataset with additional annotation corrections and state tracking baselines. In *Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI*, pages 109\u2013117.\\n\\nYichi Zhang, Zhijian Ou, and Zhou Yu. 2020a. Task-Oriented Dialog Systems That Consider Multiple Appropriate Responses under the Same Context. In *Proceedings of the AAAI Conference on Artificial Intelligence*, pages 9604\u20139611.\\n\\nZheng Zhang, Ryuichi Takanobu, Qi Zhu, Minlie Huang, and XiaoYan Zhu. 2020b. Recent advances and challenges in task-oriented dialog systems. *ScienceChina Technological Sciences*, pages 1\u201317.\\n\\nQi Zhu, Kaili Huang, Zheng Zhang, Xiaoyan Zhu, and Minlie Huang. 2020. CrossWOZ: A large-scale Chinese cross-domain task-oriented dialogue dataset. *Transactions of the Association for Computational Linguistics*, 8:281\u2013295.\\n\\nLei Zuo, Kun Qian, Bowen Yang, and Zhou Yu. 2021. AllWOZ: Towards Multilingual Task-Oriented Dialog Systems for All. *arXiv preprint arXiv:2112.08333*. \"}"}
{"id": "lrec-2024-main-835", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: An example of input/output data used for training the T5 on the DST task. At the beginning of the input sequence, a prefix indicating the DST task is attached. Additionally, each element of the input sequence is prefixed with indicators denoting the speaker.\\n\\nFigure 6: An example of input/output data used for training the T5 on the RG task. At the beginning of the input sequence, a prefix indicating the RG task is attached. Additionally, each element of the input sequence is prefixed with indicators denoting the speaker, belief states, and the results of database searches.\"}"}
{"id": "lrec-2024-main-835", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"B. Prompt Examples for LLMs\\n\\nWe created the Japanese prompts for JMultiWOZ based on the prompts for MultiWOZ used by Hude\u010dek and Dusek (2023). Figures 7 and 8 show examples of prompts used for LLMs in a zero-shot setting.\\n\\n\u5bfe\u8a71\u6587\u8108\u304b\u3089\u5224\u65ad\u3067\u304d\u308b,\u30ec\u30b9\u30c8\u30e9\u30f3\u306b\u95a2\u3059\u308b\u30b9\u30ed\u30c3\u30c8\u3068\u5024\u30da\u30a2(\u4fe1\u5ff5\u72b6\u614b)\u3092\u62bd\u51fa\u3057\u3066\u304f\u3060\u3055\u3044.\\n\\n\u4fe1\u5ff5\u72b6\u614b\u306f\u534a\u89d2\u30b9\u30da\u30fc\u30b9\u3068\u30ab\u30f3\u30de\u3092\u4f7f\u3044,\\n`\u30b9\u30ed\u30c3\u30c81 \u50241, \u30b9\u30ed\u30c3\u30c82 \u50242` ... \u306e\u3044\u305a\u308c\u304b.\\n\\n\u62bd\u51fa\u3059\u308b\u3079\u304d\u30b9\u30ed\u30c3\u30c8\u306f\u4ee5\u4e0b\u306e\u901a\u308a\u3067\u3059:\\n- \\\"name\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u540d\u524d\\n- \\\"genre\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u30b8\u30e3\u30f3\u30eb\\n- \\\"area\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u30a8\u30ea\u30a2.\\n  - \\\"ward\\\" \u3084 \\\"city\\\" \u306a\u3069\u306e\u5730\u533a\u540d.\\n- \\\"pricerange\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u4fa1\u683c\u5e2f.\\n  - \\\"affordable\\\" / \\\"moderate\\\" / \\\"expensive\\\" \u306e\u3044\u305a\u308c\u304b.\\n- \\\"station\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u6700\u5bc4\u308a\u99c5\\n- \\\"wifi\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306eWi-Fi\u306e\u6709\u7121.\\n  - \\\"free\\\" / \\\"paid\\\" / \\\"not available\\\" \u306e\u3044\u305a\u308c\u304b.\\n- \\\"parking\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u99d0\u8eca\u5834\u306e\u6709\u7121.\\n  - \\\"free\\\" / \\\"paid\\\" / \\\"not available\\\" \u306e\u3044\u305a\u308c\u304b.\\n- \\\"people\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u4e88\u7d04\u4eba\u6570\\n- \\\"day\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u4e88\u7d04\u65e5\\n- \\\"time\\\" \u30ec\u30b9\u30c8\u30e9\u30f3\u306e\u4e88\u7d04\u6642\u9593\\n\\n\u4e0a\u8a18\u4ee5\u5916\u306e\u60c5\u5831\u306f\u62bd\u51fa\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044.\\n\u307e\u305f,\u6587\u8108\u4e2d\u3067\u8a00\u53ca\u3055\u308c\u306a\u304b\u3063\u305f\u30b9\u30ed\u30c3\u30c8\u306e\u5024\u3082\u62bd\u51fa\u3057\u306a\u3044\u3067\u304f\u3060\u3055\u3044.\\n\\nFigure 7: Prompt examples used for the DST task in the restaurant domain for the LLMs\\n\\n\u3042\u306a\u305f\u306f\u9867\u5ba2\u306e\u8981\u671b\u306b\u6cbf\u3063\u305f\u30ec\u30b9\u30c8\u30e9\u30f3\u3092\u63a2\u3057\u51fa\u3057,\u4e88\u7d04\u3092\u3059\u308b\u30a2\u30b7\u30b9\u30bf\u30f3\u30c8\u3067\u3059.\\n\u30c7\u30fc\u30bf\u30d9\u30fc\u30b9\u3092\u7528\u3044,\u30a8\u30ea\u30a2,\u30b8\u30e3\u30f3\u30eb,\u4fa1\u683c\u5e2f\u7b49\u304b\u3089\u30ec\u30b9\u30c8\u30e9\u30f3\u3092\u691c\u7d22\u30fb\u4e88\u7d04\u3059\u308b\u3053\u3068\u304c\u3067\u304d\u307e\u3059.\\n\u30ec\u30b9\u30c8\u30e9\u30f3\u3092\u898b\u3064\u304b\u3063\u305f\u3089,\u305d\u306e\u540d\u524d,\u4f4f\u6240,\u96fb\u8a71\u756a\u53f7,\u305d\u306e\u4ed6\u5fc5\u8981\u306a\u60c5\u5831\u306a\u3069,\u9867\u5ba2\u304b\u3089\u5c0b\u306d\u3089\u308c\u305f\u60c5\u5831\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044.\\n\u4e88\u7d04\u304c\u6210\u529f\u3057\u305f\u3089,\u305d\u306e\u4e88\u7d04\u756a\u53f7(ref)\u3092\u63d0\u4f9b\u3057\u3066\u304f\u3060\u3055\u3044.\\n\\nFigure 8: Prompt examples used for the RG task in the restaurant domain for the LLMs\"}"}
