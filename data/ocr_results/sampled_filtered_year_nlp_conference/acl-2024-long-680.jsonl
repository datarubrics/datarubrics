{"id": "acl-2024-long-680", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To Distill or Not to Distill?\\nOn the Robustness of Robust Knowledge Distillation\\nAbdul Waheed\u03be Karima Kadaoui\u03be Muhammad Abdul-Mageed\u03b3,\u03bb\\nMBZUAI\\nThe University of British Columbia\\nInvertible AI\\n{abdul.waheed,karima.kadaoui}@mbzuai.ac.ae muhammad.mageed@ubc.ca\\n\\nAbstract\\nArabic is known to present unique challenges for Automatic Speech Recognition (ASR). On one hand, its rich linguistic diversity and wide range of dialects complicate the development of robust, inclusive models. On the other, current multi-lingual ASR models are compute-intensive and lack proper comprehensive evaluations. In light of these challenges, we distill knowledge from large teacher models into smaller student variants that are more efficient. We also introduce a novel human-annotated dataset covering five under-represented Arabic dialects for evaluation. We further evaluate both our models and existing SoTA multilingual models on both standard available benchmarks and our new dialectal data. Our best-distilled model's overall performance (45.0%) surpasses that of a SoTA model twice its size (SeamlessM4T-large-v2, WER=47.0%) and its teacher model (Whisper-large-v2, WER=55.1%), and its average performance on our new dialectal data (56.9% WER) outperforms all other models. To gain more insight into the poor performance of these models on dialectal data, we conduct an error analysis and report the main types of errors the different models tend to make. The GitHub repository for the project is available at https://github.com/UBC-NLP/distill-whisper-ar.\\n\\n1 Introduction\\nThere have been significant advancements in multi-lingual automatic speech recognition (ASR) in both training methodologies and architectures. Models such as OpenAI's Whisper (Radford et al., 2023) and Meta's SeamlessM4T (Communication et al., 2023) can transcribe speech from languages in the order of the hundreds, albeit with varying degrees of accuracy. Especially for low-resource languages, these models do not perform well (Radford et al., 2023; Williams et al., 2023; Talafha et al., 2023a). Arabic, for example, poses significant challenges to these multilingual models and hence is the object of the current work.\\n\\nArabic can be classified into three broad categories, namely: Classical Arabic (CA), used in early literature and religious texts; Modern Standard Arabic (MSA), the 'high' variety used in official documents and in the media; and Dialectal Arabic (DA), the collection of 'low' varieties used in day-to-day conversations (Bouamor et al., 2014). DA can vary extensively at the regional level (e.g. Gulf vs Maghrebi), country level (e.g. Egyptian vs Sudanese), and sub-country (e.g. Hourani or Northern Jordanian Dialect vs Urban or Madani dialect) (Habash, 2022; Abdul-Mageed et al., 2020; Shon et al., 2020; Abdul-Mageed et al., 2018). Due to the significant differences in lexicon, phonetics, and even grammar between these varieties, ASR systems trained on MSA alone cannot be reliably leveraged off-the-shelf for all Arabic speech. Developing effective models for DA can prove especially difficult, given the lack of standardized orthography, the scarceness of labeled data for many dialects, inconsistent use of diacritics, and use of code-switching (Ali et al., 2021).\\n\\nAlthough most multilingual and multimodal systems (e.g., (Radford et al., 2023; Barrault et al., 2023; Communication et al., 2023)) cover Arabic, their evaluation predominantly involves benchmarks established for MSA, such as FLEURS (Conneau et al., 2022), Common Voice (CV) (Ardila et al., 2020), and the Arabic Speech Corpus (ASC) (Halabi et al., 2016). Since Arabic exhibits substantial linguistic diversity, encompassing various varieties and dialects, evaluations conducted solely on MSA are inherently limited. Existing works aiming to address this gap, e.g., (Talafha et al., 2023b), lack thorough evaluation and do not cover current state-of-the-art (SoTA) models. To address this, we conduct a comprehensive evaluation of all recently developed models on a linguistic...\"}"}
{"id": "acl-2024-long-680", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Beyond the challenge of inadequate evaluation, the deployment of massive multilingual multi-modal systems such as SeamlessM4T (Commu-nication et al., 2023) and Whisper (Radford et al., 2023) is hampered by the considerable computational resources they require during both training and inference. These efficiency issues pose a significant accessibility barrier, discriminating against populations with limited resources. To alleviate this concern, we employ a framework for knowledge distillation (Gandhi et al., 2023) from large models such as Whisper (Radford et al., 2023) into relatively compact models for Arabic speech recognition. We show that our distilled models are not only compute-efficient but their performance is on par or better compared to larger counterparts.\\n\\nIn summary, the gaps in existing work include (1) the insufficient knowledge about the utility of recent multilingual speech model models on Arabic, including dialects, (2) the discrepancy in representing some Arabic dialects in existing dialectal benchmarks, and (3) the inefficiency of these models due to their large sizes which demands significant compute resources at both training and inference time. We address these limitations through a number of contributions, as follows:\\n\\n\u2022 We evaluate major multilingual speech models on a wide variety of standard benchmarks representing Arabic to identify their zero-shot performance.\\n\u2022 To evaluate the models under diverse varieties, we introduce a never-seen in-house labeled ASR dataset covering five under-represented Arabic dialects.\\n\u2022 We distill knowledge from large ASR models into relatively small, and hence more efficient, (student) models with minimal-to-no performance drops compared to the bigger (teacher) counterparts.\\n\\nThe rest of the paper is organized as follows: Section 2 is a review of related works. In Section 3, we introduce knowledge distillation and outline our related methods and training strategies. In Section 4, we provide details about our experiments, and in Section 5 introduce and discuss our results. Section 6 delivers a thorough error analysis based on the model predictions on our new dialectal data. We conclude the work in Section 7. Finally, we outline our limitations and ethical considerations in Sections 8 and 9, respectively.\"}"}
{"id": "acl-2024-long-680", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Knowledge distillation is a method used to transfer knowledge from large models to smaller ones, thereby reducing both memory and compute requirements (Hinton et al., 2015; Sanh et al., 2019; Gou et al., 2021; Lopes et al., 2017a; Kim and Rush, 2016). This technique has been effectively applied in various domains. For example, in computer vision applications, knowledge distillation results in compact and efficient models (Kaleem et al., 2024; Koohpayegani et al., 2020). Similarly, in diffusion models (Luo, 2023) and large language models (Xu et al., 2024), knowledge distillation produces small, efficient, and task-specific models. Yang et al. (2023) distill knowledge from multiple foundation models into small and dedicated speech recognition models. Ni et al. (2023) proposes cross-modality knowledge distillation from large language models into speech models.\\n\\nKnowledge distillation in speech. Ferraz et al. (2024) distill knowledge from a large Whisper model into small multilingual models but limit their evaluation to standard benchmarks in eight languages (Arabic not included in the set). Shao et al. (2023) apply a novel distillation approach to Whisper, reducing its size by 80-90% while also improving its performance. Chang et al. (2021) propose a layer-wise distillation approach that reduces the size of a Hubert model by 75% while increasing its processing speed by 73%, retaining most of the original model's performance across multiple tasks. In addition to that, researchers have introduced methods for model compression, such as data-free knowledge distillation and teacher-student (TS) learning for domain adaptation (Lopes et al., 2017b; Manohar et al., 2018). These approaches involve training student models to mimic teacher models using various strategies, including Gaussian noise generation and sequence-level Kullback-Leibler (KL) divergence (Kullback and Leibler, 1951).\\n\\nAmong different knowledge distillation approaches such as the ones highlighted above, the standard student-teacher distillation is a task- and modality-independent framework that is simple yet effective. Gandhi et al. (2023) use this framework to distill Whisper into small monolingual models for English using large-scale pseudo-labels. However, their work is limited to high-resource language. We take inspiration from (Gandhi et al., 2023) and distill Whisper into small models for Arabic and perform a thorough evaluation. One difference between our work and that of (Gandhi et al., 2023) is that while there is limited information about the out-of-distribution datasets of (Gandhi et al., 2023)'s work and whether they are part of the teacher's training data, we employ new dialectal speech data never seen by the model.\\n\\n### Knowledge Distillation\\n\\nKnowledge distillation is a method of transferring knowledge from a large model (teacher) to a relatively small model (student). The student model is trained to mimic the behavior of the teacher model both at the dense representation level and the sequence level (Hinton et al., 2015; Sanh et al., 2020; Kim and Rush, 2016). Following Gandhi et al. (2023), who distill a Whisper model for English ASR, we first generate large-scale pseudo-labels from the teacher model and apply a threshold to filter the output. We then train the student model with high-quality filtered pseudo-labels as ground truth, which can be expressed as:\\n\\n$$L_{PL} = -\\\\sum_{i=1}^{N'} \\\\log \\\\left( y_{\\\\hat{y}}^{-1}, H \\\\right)$$\\n\\nThe student model is also trained to minimize the discrepancy between the probability distributions over tokens of the student and teacher models, based on KL divergence:\\n\\n$$L_{KL} = \\\\sum_{i=1}^{N} KL(Q_i, P_i)$$\\n\\nObjective: We take the weighted sum of (1) and (2) to get the final objective, which can be written as:\\n\\n$$L_{KD} = \\\\alpha_{KL} L_{KL} + \\\\alpha_{PL} L_{PL}$$\\n\\nWe use the same values for $\\\\alpha_{KL} (0.8)$ and $\\\\alpha_{PL} (1.0)$ as Gandhi et al. (2023). Details about our teacher and student models, along with training data, can be found in Table 2.\\n\\n### Experiments\\n\\n#### 4.1 Datasets\\n\\nCommon Voice. CV (Ardila et al., 2020) is a widely used multilingual benchmark for speech recognition. In our experiments, we use the test and validation splits of four different CV versions (6.1, 9.0, 11.0, 15.0) which have been widely used.\"}"}
{"id": "acl-2024-long-680", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Utterance and word count statistics across the different dialects from our in-house dataset.\\n\\n|   | Dia. | Utt. | Words | Words/Utt. | Hours |\\n|---|------|------|-------|------------|-------|\\n| ALG | 815  | 8,900| 10.92 | 0.97       |\\n| JOR | 2,671| 28,291| 10.59 | 3.27       |\\n| PAL | 1,097| 15,152| 13.81 | 1.67       |\\n| UAE | 3,701| 41,345| 11.17 | 4.42       |\\n| YEM | 2,283| 27,605| 12.09 | 2.94       |\\n| **Total** | **10567** | **121293** | **11.48** | **13.29** |\\n\\nAvg.\\n\\n|   |   |   |   |    |\\n|---|---|---|---|----|\\n| ALG | 2113.4 | 24258.6 | 11.72 | 2.65 |\\n\\nDia.: Dialect. #: Number of.\\nAvg.: Average.\\nUtt: Utterance\\n\\nIn response to the notable scarcity of publicly available dialectal data, we manually curate a dataset representing five underrepresented Arabic dialects, namely Algerian (ALG), Jordanian (JOR), Palestinian (PAL), Emirati (UAE), and Yemeni (YEM), spanning four dialectal regions (North African, Levantine, Gulf, and Yemeni). We task native speakers of each dialect to annotate segments from local TV series sourced from YouTube. Our dataset comprises a total of 10,567 utterances and 121,293 words (2,133 utterances and 24,258 words per dialect, on average) amounting to over 13 total hours. Individual statistics for each dialect can be found in Table 1.\\n\\nWe evaluate a wide range of multilingual speech recognition models on different varieties of Arabic from the aforementioned datasets, including standard and accented MSA, and various Arabic dialects. We also distill small dedicated models from larger Whisper models. We categorize these systems as follows:\\n\\n4.2.1 Supervised Baselines\\nWe evaluate two openly available supervised baselines along with a Whisper model that we fine-tune on Arabic ASR in a supervised setting. The first two models are Wav2Vec2-XLS-R (Conneau et al., 2020; Babu et al., 2021a), trained on CV8.0 which has significant overlap with other versions of the CV dataset, and HuBERT (Hsu et al., 2021), trained on MGB-3 (Ali et al., 2017) and the Egyptian Arabic Conversational Speech Corpus (5.5 hours). The third model is whisper-large-v2, which we fine-tune on CV11.0 and MGB-2. We evaluate all three models on the datasets listed in Section 4.1. This includes the in-distribution test and dev splits of MGB-2 and CV11.0.\\n\\n4.2.2 Zero-Shot Models\\nLarge multilingual speech models are acclaimed for transcending language and task barriers. In particular, these models are usually claimed to demonstrate proficiency in a variety of speech tasks on English in the zero-shot setting. However, it is crucial to conduct thorough evaluations of these models on other languages and dialects and under diverse conditions. Hence, our objective is to assess a wide array of zero-shot models on a wide range of Arabic speech recognition datasets to assess their robustness and generalization capability beyond English. We focus on a number of recently introduced models that have gained popularity in the community as well as existing commercial systems, as we explain next.\\n\\nWhisper. Whisper (Radford et al., 2023) is a multilingual speech model capable of speech recognition and translation across languages including Arabic. We evaluate four variants of Whisper, namely small (W-S), medium (W-M), large-v2 (W-L-v2), and large-v3 (W-L-v3). We use all the default parameters for decoding with a maximum sequence length of 225 tokens.\\n\\n1Our models are \u2018dedicated\u2019 in the sense that they are solely focused on Arabic and only handle ASR.\"}"}
{"id": "acl-2024-long-680", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SeamlessM4T. Multimodal multilingual speech models are also capable of generating high-quality transcripts across languages (Communication et al., 2023). However, they lack a comprehensive evaluation in languages besides English. We address this by evaluating three available variants of SeamlessM4T (medium (SM4T-M), large-v1 (SM4T-L-v1) and large-v2 (SM4T-v2)) for Arabic ASR in a zero-shot setting. We use all the default parameters provided in the model\u2019s inference pipeline.\\n\\nCommercial Systems.\\n\\nWe broaden our evaluation beyond publicly accessible ASR models, incorporating proprietary platforms, with a focus on Amazon\u2019s ASR system. Due to cost considerations, our evaluation is exclusively centered on the Amazon Transcribe service on our in-house data.\\n\\n4.2.3 Distilled Models\\n\\nAs described in Section 3, we distill whisper-large-v2 into seven different student models (see Table 2). We provide more details about the teacher and student models and distillation data here.\\n\\nTeacher and Student Models.\\n\\nWe use a whisper-large-v2 checkpoint for pseudo-labeling and the same model as the teacher during training. We train four variants of the student model in different configurations in terms of the number of layers being removed. Following Gandhi et al. (2023), we initialize the student models with maximally spaced layers in the encoder and decoder block of the teacher model. We provide more details about our distilled models in Table 2.\\n\\n| Model   | EL  | DL  | Data  |\\n|---------|-----|-----|-------|\\n| W-L-v2  | 32  | 32  | N/A   |\\n| DW-8-8  | 8   | 8   | 100K  |\\n| DW-16-16| 16  | 16  | 100K  |\\n| DW-32-16| 32  | 16  | 100K  |\\n| DW-16-32| 16  | 32  | 100K  |\\n| DW-16-16++| 16 | 16  | 500K  |\\n| DW-32-16++| 32| 16  | 500K  |\\n| DW-16-16-1M| 32| 16  | 1M    |\\n\\nTable 2: The student models are initialized from maximally spaced layers of the teacher model. The size of data is stated as the number of segments. All distilled models are trained for ten epochs.\\n\\nW-L-v2: Whisper-large-v2.\\n\\n#:\\n\\nNumber of.\\n\\nDW: Distill-Whisper.\\n\\nEL: Encoder Layers.\\n\\nDL: Decoder Layers.\\n\\nTraining Data.\\n\\nWe randomly sample 100K and 500K segments from a mixture of MGB2 (Ali et al., 2016), MGB3 (Ali et al., 2017), FLEURS (Conneau et al., 2022), CommonVoice 15.0 (Ardila et al., 2020), QASR (Mubarak et al., 2021), Arabic Speech Corpus (Halabi et al., 2016), and Massive Arabic Speech Corpus (MASC) (Al-Fetyani et al., 2021). This amounts to roughly 100 and 500 hours of pseudo labeled speech data, respectively. We explicitly include only the train split of each dataset.\\n\\n4.3 Experimental Setup\\n\\nWe conduct all of our training and evaluation experiments on 8xA100/4xA100 (40G) GPU nodes. For the evaluation, we use the default decoding parameters used in the corresponding models unless otherwise specified. We use 225 as the maximum sequence length throughout our experiments and report Word Error Rate (WER) and Character Error Rate (CER) as our evaluation metrics. For distillation, we use a value of 80% for the WER threshold \\\\( \\\\lambda \\\\) to filter-out low-quality transcription from pseudo-labels for the results reported in Table 3. We also experiment with different threshold values and discuss the findings in Section 5. Although our threshold for main results seems too high, Gandhi et al. (2023) find that going from a threshold of 80 to five yields a marginal improvement of one point in terms of average WER across different in-distribution and out-of-distribution evaluation sets. In addition, we believe that a high threshold value also helps approximate the performance where we do not have labeled data to conduct the filtering process, especially when labeled data is scarce. Due to computing limitations, we do not conduct any training hyperparameter search and directly apply the configuration used in Gandhi et al. (2023). For the distillation process, we report our key parameters in Table 5 (Appendix B).\\n\\nText Preprocessing.\\n\\nIn everyday writing, Arabic is characterized by inconsistencies in diacritics use and letter variations (e.g. /char0d/char40 vs /char40). This linguistic variability poses a challenge for ASR evaluation, as transcriptions that are phonetically accurate and intelligible to a native speaker might still be marked as errors due to strict lexical mismatches. To address this, we follow Talafha et al. (2023b); Chowdhury et al. (2021) to standardize and normalize the text. Specifically, we (1) remove any special characters and diacritics, (2) remove all Latin characters since we are not concerned about code-switching, (3) transliterate all Arabic digits (i.e. /char31, /char32, /char33) to\"}"}
{"id": "acl-2024-long-680", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Results and Discussion\\n\\nWe evaluate all models on four versions of CV (6.1, 9.0, 11.0, 15.0), MGB-2, MGB-3, MGB-5, FLEURS, and our five novel dialectal sets. We report WER and CER scores on the orthographic and normalized predictions (as per Section 4.3) in Table 3 for test splits and in Table 6 (Appendix C) for dev splits. CV15.0 results are included in Table 3 and other versions can be found in Appendix C.\\n\\nCommercial Systems and Supervised Models.\\n\\nThe supervised finetuned (SFT) baselines are trained on MGB-2, MGB-3, and the CV datasets. Other evaluation sets thus represent out-of-distribution data. As a result, we see that supervised HuBERT (15.4) and Whisper (25.2) outperform all other models on in-distribution data MGB-2 and MGB-3, respectively. However, these baselines often perform poorly on all other evaluation sets that are not in their training data. On our private in-house data, the supervised models usually produce more incorrect words than the number of words in the corresponding reference. We find varying levels of transcription difficulty for these models when evaluated on distinct dialects and linguistic varieties.\\n\\nThe Amazon transcribe system performs well on our in-house data compared to the supervised baselines. It gives 45.5% WER on JOR which is not too far from the best WER of 41.5% by SeamlessM4T-large-v2. We find that it struggles with ALG, which goes along the trend noticed with all models.\\n\\nZero-Shot Models.\\n\\nWe find that both Whisper and SeamlessM4T models perform quite well on CV and FLEURS in zero-shot setting. More specifically, the best Whisper model shows WER scores of 15.8% and 11.3% on CV and FLEURS, respectively, while the best SeamlessM4T achieves 9.7% and 7.6% WER. MMS shows the lowest performance of the zero-shot models and the second lowest across all model types with an 82.5% average WER. Meanwhile, a consistent challenge across all models is observed with MGB-5, followed by MGB-3. These last two datasets involve dialects; namely, EGY and MOR dialects respectively. The transition from MSA to dialects thus marks a significant drop in performance, indicating the models\u2019 difficulties in adapting to dialectal variations. This pattern becomes even more apparent when looking at the results of our in-house data. All models particularly struggle with the ALG and YEM dialects, whereas JOR and PAL are less challenging to transcribe. This underscores the distinct issues that dialectal diversity poses to current ASR systems. The best-performing model overall on both the existing datasets and our new data is SeamlessM4T-large-v2, showing a significant improvement in performance compared to its previous version. Although the size is the same between the two systems, Barrault et al. (2023) attributes the higher performance to its novel UnitY2 architecture. We also find that both the SeamlessM4T and Whisper family models consistently improve as we increase in size, except for SeamlessM4T-medium (48.1% WER) which outperforms SeamlessM4T-large-v1 (51.1% WER) model on average.\\n\\nDistilled Models.\\n\\nWe distill a wide range of models of varying sizes from Whisper-large-v2 by reducing the number of encoder and decoder blocks. Our smallest distilled model, which has eight encoder and decoder blocks (resulting in approximately a 75% reduction in parameters from the teacher model), outperforms Whisper-medium with a WER of 64.8% compared to 65.4%, while being half the size (Table 3). When comparing our distilled models with smaller Whisper variants, we find that DW-16-16 outperforms Whisper-medium by over 12 points. However, both these models are similar in size. As expected, we observe that increasing the number of layers in the distilled model enhances its performance. Consequently, our best-performing distilled model, DW-32-16++ (WER 45.0%), surpasses all other models, including Whisper-large-v3 (WER 49.5%) and SeamlessM4T-v2 (WER 47.0%), despite being half of its size (see Table 3). To sum up, our best-performing distilled models yield the best results in terms of WER on four out of ten evaluated datasets and are on par with an overall best model in terms of average WER while being half in size. However, when looking at the average performance across in-house data only, it outperforms all other systems with a 56.9% WER, whereas the best zero-shot model (Seamlessm4T-large-v2) has 61.74% WER and teacher model...\"}"}
{"id": "acl-2024-long-680", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Size  | CV15.0 | MGB2 | MGB3 | MGB5 | Fleurs | In-house Data | Avg. | ALG | JOR | PAL | UAE | YEM |\\n|------------|--------|------|------|------|--------|--------------|------|-----|-----|-----|-----|-----|\\n| Normalized + No Diacritics |        |      |      |      |        |              |      |     |     |     |     |     |\\n| Amazon     | -/-    | -/-  | -/-  | -/-  | -/-    | -/-          | 88.0 | 71.6| 59.2| 29.1| 63.4| 32.2| 71.8| 45.0| 71.1| 44.3| 77.4| 47.7| 71.8| 45.0| 71.8| 45.0|\\n| XLS-R      | 0.96   | 89.7 | 39.4 | 97.6 | 53.1   | 99.1         | 55.2 | 18.9| 49.6| 17.3| 25.2| 9.5  | 34.9 | 10.9| 96.8| 44.3| 65.2| 23.3| 73.8| 27.9| 83.0| 36.7| 90.5| 38.8| 66.7| 27.3| 72.2| 32.5| 83.0| 38.5| 50.4| 18.2| 61.0| 23.3|\\n| HuBERT     | 0.31   | 55.2 | 18.9 | 49.6 | 17.3   | 25.2         | 92.4 | 45.5| 34.9| 10.9| 96.8 | 44.3 | 65.2 | 23.3| 73.8 | 27.9| 83.0 | 36.7| 90.5 | 38.8| 66.7 | 27.3| 72.2 | 32.5| 83.0 | 38.5| 50.4 | 18.2 | 61.0 | 23.3|\\n| W-FT       | 1.5    | 35.8 | 21.9 | 15.3 | 8.1    | 19.4         | 101.4| 62.3| 9.8 | 3.4 | 115.5| 69.6 | 67.8 | 37.2| 69.6 | 35.4| 105.9| 69.1 | 107.1| 64.8 | 68.9 | 29.5 | 59.9 | 21.7| 73.8 | 27.9| 83.0| 36.7| 90.5| 38.8| 66.7| 27.3|\\n| MMS-all    | 1.0    | 106.4| 80.9 | 39.3 | 13.4   | 75.3         | 89.7 | 45.9| 23.8| 6.3 | 100.2 | 78.0 | 89.8 | 55.4| 99.9 | 75.1| 100.1| 78.1 | 100.2| 76.6 | 82.5 | 54.4 | 12609|\\n| SM4T-M     | 1.2    | 16.3 | 5.7  | 19.5 | 9.0    | 17.3         | 83.8 | 46.6| 8.7 | 3.6 | 81.1 | 39.7 | 46.3 | 15.9| 55.2 | 20.1| 59.8 | 24.7 | 68.9 | 29.5 | 48.1 | 21.7 |\\n| SM4T-L-v1  | 2.3    |      |      |      |        |              |      |     |     |     |      |      |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |\\n| SM4T-L-v2  | 2.3    |      |      |      |        |              |      |     |     |     |      |      |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |      |     |\\n| W-S        | 0.24   | 40.3 | 16.4 | 46.8 | 24.7   | 81.4         | 226.5| 164.8| 28.2| 8.7 | 130.7 | 84.7 | 68.6 | 32.9| 73.8 | 36.3| 97.8 | 59.7 | 107.1| 66.7 | 47.0 | 22.6 | 1.5 | 15.8 | 5.2 | 15.9 | 7.6 | 83.0 | 38.5 | 50.4 | 18.2 |\\n| DW-8-8     | 0.44   | 32.7 | 12.3 | 39.6 | 17.8   | 64.9         | 89.7 | 53.0| 29.8| 11.4| 91.4 | 48.2 | 66.2 | 29.0| 73.2 | 33.0| 78.0 | 38.4 | 82.9 | 41.5 | 69.7 | 30.7 | 47.0 | 22.6 | 18.8 | 6.6 | 83.0 | 38.5| 50.4 | 18.2 | 61.0 | 23.3 |\\n| DW-32-16   | 1.12   | 18.8 | 5.9  | 21.1 | 8.9    | 43.8         | 78.9 | 40.4| 14.2| 4.8 | 79.5 | 33.4 | 44.4 | 14.7| 55.0 | 19.5| 58.1 | 22.8 | 68.5 | 28.1 | 48.2 | 20.0 |\\n| DW-16-32   | 1.22   | 21.5 | 7.3  | 25.0 | 10.7   | 49.1         | 83.0 | 47.5| 18.4| 6.0 | 84.3 | 44.0 | 49.8 | 18.0| 60.3 | 25.4| 64.4 | 29.0 | 73.8 | 36.8 | 53.0 | 25.1 |\\n| DW-16-16++ | 0.80   | 19.2 | 6.2  | 23.0 | 10.2   | 47.2         | 79.0 | 42.6| 15.0| 5.2 | 79.0 | 39.0 | 46.7 | 17.2| 56.4 | 21.6| 60.4 | 26.8 | 69.1 | 31.5 | 49.5 | 22.5 |\\n| DW-32-16++ | 1.12   | 17.1 | 5.5  | 19.7 | 8.8    | 40.7         | 76.6 | 40.6| 11.1| 3.1 | 74.6 | 33.3 | 41.6 | 13.4| 51.4 | 17.2| 53.5 | 21.1| 65.6 | 32.8 | 49.5 | 22.5 |\\n| Dataset    | Ortho  | Norm | Norm + ND |\\n| CV         | 47.1/18.9 | 39.4/17.0 | 19.4/6.8 |\\n| MGB3       | 52.4/28.2 | 46.6/23.9 | 43.5/21.9 |\\n| MGB5       | 85.2/52.2 | 83.6/49.5 | 83.0/49.1 |\\n| Fleurs     | 20.3/5.9  | 17.4/5.0  | 11.6/3.7  |\\n\\nTable 3: WER/CER scores after normalization and removing diacritics as well as on orthographic transcription. Average is the mean score across all the evaluation sets. All distilled models are trained with a filtering threshold of 80. We report the score on the test split of each dataset. Abbreviations. W - Whisper, FT - Finetuned, M - Medium, L - Large, S - Small, D - Distil. (Whisper-large-v2) 67.6%, showing substantial improvement on unseen dialects. We report the average across benchmark and in-house data in Appendix C Table 8. Our results underscore the distilled models' inherent efficiency and generalization to unseen dialects, possibly resulting from the mixture of data. This may imply that the process retains the critical linguistic and acoustic features necessary for high-quality ASR in a linguistically diverse setting.\\n\\nOrthographic, Normalized, and Non-Diacritized Evaluation. To better understand the effect of normalization and diacritics removal, we calculate WER/CER on orthographic, normalized, and normalized+non-diacritics (ND) transcriptions of Whisper-large-v2. We report the results in Table 4. With normalization, the WER goes down on CV from 47.1% to 39.4%. Similarly, we see a near 50% drop in WER on FLEURS which suggests that the model is much more prone to miss diacritics than missing entire words. However, in the case of MGB-3 and MGB-5, we do not notice any significant changes after pre-processing, which again shows the poor generalization capability of Whisper on unseen and linguistically diverse data.\\n\\n| Dataset Ortho | Norm | Norm + ND |\\n|---------------|------|-----------|\\n| CV            | 47.1 | 39.4      |\\n| MGB3          | 52.4 | 46.6      |\\n| MGB5          | 85.2 | 83.6      |\\n| Fleurs        | 20.3 | 17.4      |\\n\\nTable 4: WER/CER scores on orthogonal, normalized, and without diacritics outputs produced by Whisper-large-v2. Abbreviations: Norm - Normalized, ND - No Diacritics.\\n\\nEffect of WER Threshold. The knowledge distillation framework that we follow (Gandhi et al., 2023) involves pseudo-labels filtered by WER. While we initially use a WER threshold of 80 in Table 3, we experiment with different values to find an optimal threshold value that yields better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required, subsequently resulting in better results across different evaluation sets while reducing the amount of data required"}
{"id": "acl-2024-long-680", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"faster training. The summary of our results is illustrated in Figure 1 and the detailed results can be found in Table 9 in Appendix C. From our experiments with the DW-16-16 and DW-32-16 models (trained on 100K segments), we find that discarding examples where the WER is above 80% (amounting to about 28% of the total examples) results in the best overall performance across different evaluation setups closely followed by the 20% WER threshold. Both these models significantly outperform the base teacher model whisper-large-v2 and the whisper-medium model, which is comparable in size. That being said, reducing the threshold from 20% to 10% worsens the models' performance. However, training the models without applying any filtering still outperforms the zero-shot baselines. Based on these results, we conclude that there exists a trade-off between the quality and quantity of the distillation data. This implies that we can distill small and compute-efficient language-specific speech recognition models without training on any labeled speech data while being on par or better than the base models.\\n\\n![Figure 1: Average WER on five MSA benchmarks and five dialects from our in-house data with different filtering thresholds. The dotted flat line represents the Whisper-large-v2 (teacher) in the zero-shot setting.]\\n\\n**Abbreviations:**\\n- **Bench** - Benchmark.\\n- **IH** - In-house.\\n- **ZS** - Zero-shot.\\n\\n---\\n\\n**Data Scaling.**\\nWe train all of our models on 100K speech segments (\u2248100 hours) sampled from the mixture of over 3M segments (\u22484000 hours) described in Section 4.1. We increase the data size from 100K to 500K and then up to 1M segments to study the effect of the quantity of the data. With the filtering threshold set to 20%, DW-16-16 trained on 500K segments outperforms the zero-shot teacher baseline on the MSA benchmark (36.7% WER compared to 42.0%) and is significantly better on the in-house data. This trend remains consistent after scaling the data to 1M segments: the model achieves 35.0% and 60.0% WER on the MSA benchmark and in-house data, respectively, compared to 42.0% and 68.0% from the zero-shot baseline, despite being half its size.\\n\\n![Figure 2: Average WER from DW-16-16 model trained with different amounts of data. The dotted line represents the Whisper-large-v2 zero-shot baseline.]\\n\\n**Abbreviations:**\\n- **Bench** - Benchmark.\\n- **IH** - In-house.\\n- **ZS** - Zero-shot.\\n\\n---\\n\\n**6 Error Analysis**\\nTo gain a better understanding of the results, we conduct an error analysis on our in-house data by randomly sampling 20 sentences per dialect from each models' outputs, with the aim of identifying the specific types of errors present. We then categorize the errors into the following types:\\n\\n- **MSA Translation**: The transcription is semantically accurate but employs words in MSA that differ from the dialectal words spoken in the utterance.\\n- **Hallucination**: The transcription is found to be both semantically and acoustically distant from the utterance.\\n- **Deterioration**: The transcription is either gibberish (random characters) or involves an excessive repetition of the same word or expression.\\n- **Incomplete Transcription**: Parts of the utterance are omitted and do not appear in the transcription.\\n- **Empty Transcription**: The model fails to generate a prediction at all.\\n- **Dialectal Inaccuracies**: The prediction and ground truth mismatch is of dialectal nature. Instances such as unrecognized dialectal words, first names, cultural expressions, pronunciations (e.g. Emirati Arabic subbing \u064f\u0629\u064f/charf8/char0a for \u064f\u0629\u064f/char68/char2e) and alternate orthographies fall in this category.\\n\\nAn example for each of these categories can be found in Table 10 in Appendix D.\"}"}
{"id": "acl-2024-long-680", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Upon inspecting the initial results, we decided to further analyze the performance discrepancies among the ASR models by looking at the most problematic transcriptions. Due to the tedious aspect of this exercise, we limit this part of the analysis to five models: the best supervised baseline (HuBERT), the best Whisper (W-L-v3), the best SeamlessM4T (SM4Tv2), the best distilled (DW-32-16), and finally Whisper-medium (W-M) (given its closeness in size to DW-32-16). We set a threshold of 75% CER and look at all the transcriptions with a higher error rate. It is noteworthy that a transcription could embody multiple error categories simultaneously, such as being both incomplete and translated to MSA.\\n\\nOur results show that the supervised baselines struggle the most, with W-M amounting to 635 highly erroneous transcriptions with hallucination (closely followed by deterioration) being the category with the most instances. Our D-W-32-16 model has the least issues with 108 cases, most of which are simple inaccuracies. This indicates that this model produces the most coherent outputs. In other words, this model is more likely to make predictions that maintain relevance, are logically consistent and closely aligned with the input speech.\\n\\nThe fine-tuned model, HuBERT, makes considerably fewer errors than the supervised baselines but still struggles with a lot of deterioration. This category, however, looks different on HuBERT cases than it does on the Whisper and Seamless M4T models: instead of repeating words or characters, it outputs seemingly random sequences of characters occasionally including a single square bracket. These characters are strung together in word-sized sequences and can include /char10/chare8/char41/char9/chara6/char41/charae/charc6/char40/char09/charaf/char40/charf1/charbb/char51/char10/char1e/char15/char83/char40.\\n\\nIt also tends to eliminate spaces between correctly predicted words or fusing two or more half-words together. While empty transcriptions seems to be the category with the least appearances across all models, HuBERT shows a notable increase in these compared to the other systems. That being said, all models except for HuBERT show MSA Translation errors, with the least observed in the distilled model and the most committed by W-L-v3. We theorize that these could be due to the models being trained on data that include Arabic shows or movies spoken in dialect but mapped with MSA subtitles. Among the hallucinations of the Whisper models, we also notice a commonly occurring transcription: /char10/chare8/char41/char09/chara6/char41/charae/charc6/char40/char09/charaf/char40/charf1/charbb/char51/char10/char1e/char15/char83/char40 (Eng. subscribe to the channel), which we believe could be resulting from training models on videos from platforms like YouTube. These videos can include captions that contain these sentences in interludes when the audio contains no speech (noise or background music). At the dialect level, YEM and UAE are the most problematic across all models, surprisingly exceeding the ALG dialect given its higher error rate overall. PAL and Jordanian are the least challenging, which goes in line with the systems' overall performance on them. The exact statistics are provided in Table 11 in Appendix D.\\n\\nConclusion\\n\\nWe present a comprehensive evaluation of multilingual ASR systems on a wide range of Arabic varieties and dialects to assess the robustness and generalization capability of these systems to linguistic variations. We then distill small dedicated models for Arabic ASR from large multilingual speech models (Whisper). We evaluate our distilled models on ten diverse datasets and find that despite being 25-50% smaller, they outperform the base model and are on par with state-of-the-art models twice their size. We also find our distilled models to be the most robust to linguistic diversity. We further conduct a comprehensive error analysis to investigate the nature of the errors these models make. We find that speech models with language model decoding are more prone to hallucination compared to other models. Our work reveals an inherent limitation of these models to generalize beyond their training data. In the future, we intend to expand this work to low-resource and unseen languages.\\n\\nLimitations\\n\\nIn this study, we distill small Whisper models from relatively large ones via pseudo-labeling. While our distilled models are compute efficient and maintain a performance similar to or better than the base teacher model, we believe that our work has several limitations which we outline below.\"}"}
{"id": "acl-2024-long-680", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"collected and curated by native speakers and never seen before by any models. However, our varieties do not cover all Arabic-speaking regions. We aim to address this in future work by covering more varieties and dialects.\\n\\nEfficiency. Our distilled models are 25\u201375% compute efficient while maintaining the same performance as big models. However, the training process demands substantial computational resources. Our rough approximation indicates an expenditure of more than 3000 A100 (80G) GPU hours in our experiments, equivalent to over 500 kg of CO2 emissions of which zero percent is directly offset. To offer perspective, this carbon output aligns with what a typical internal combustion engine emits during a distance of about 2,000 kilometers. Our estimations rely on the Machine Learning Impact calculator presented in (Lacoste et al., 2019).\\n\\nDistillation Training Data. We distilled four variants of student models using 100K and 500K segments of which approximately 25% are filtered. We see improvement going from 100K ($\\\\approx 100$ hours) to 500K ($\\\\approx 500$ hours) segments. As (Gandhi et al., 2023) shows going over 1000 hours results in a better model, we aim to study how distillation can be done under a low resource setting which is why we do not scale the data. Additionally, we also keep the WER threshold high (80) so that we remain close to a setting where no labeled data is available (even for filtering). It would be interesting, however, to see how distilled models may perform on unfiltered data in low-resource setting.\\n\\nNature of Speech Data. Despite putting together a never-seen dataset of under-represented Arabic dialects, we realize that sourcing our data from television series renders its nature distant from speech spoken in the wild. This type of content tends to be more \u201ctheatrical\u201d and involves different elements such as background music and laughing tracks that do not accurately reflect regular conversational Arabic. Consequently, this could fail to accurately portray the performance of these models on real speech.\\n\\n9 Ethics Statement\\n\\nData Collection and Release. Given that we collect our data from TV series available on YouTube, we ensure that our use of this data aligns with the principles of fair use, given its application to a non-commercial academic setting. Each annotator of the data was made fully aware of the research objectives of the study and the intended use of their annotations.\\n\\nIntended Use. We believe our work will embolden further research on distilling small and efficient models from large and powerful foundation models especially applied to medium and low-resource languages. Our results show that small distilled models can yield on-par performance on even better results compared to large teacher models. Therefore, our work can raise the interest among the researchers who work on developing efficient machine learning systems under low resource settings however crucial to a wide range of population.\\n\\nPotential Misuse and Bias. Our distilled models can efficiently generate high-quality transcripts for multiple Arabic dialects and have the potential to be misused. Since there exists little-to-no clarity on the nature of the training data of the teacher model, our distilled models can produce potentially harmful and biased content that they can inherit from the teacher model. In addition to that, in our human evaluation, we find that these are susceptible to generating examples from the training data which raises the threat of information leakage. Therefore, we recommend against our distilled models being used without a careful prior consideration of potential misuse and bias.\\n\\nAcknowledgments\\n\\nWe acknowledge support from Canada Research Chairs (CRC), the Natural Sciences and Engineering Research Council of Canada (NSERC; RGPIN-2018-04267), the Social Sciences and Humanities Research Council of Canada (SSHRC; 895-2020-1004; 895-2021-1008), Canadian Foundation for Innovation (CFI; 37771), Digital Research Alliance of Canada, and UBC Advanced Research Computing-Sockeye.\\n\\nReferences\\n\\nMuhammad Abdul-Mageed, Hassan Alhuzali, and Mohamed Elaraby. 2018. You tweet what you speak: A city-level dataset of Arabic dialects. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\\n\\nMuhammad Abdul-Mageed, AbdelRahim A. Elmadany, and El Moatez Billah Nagoudi. 2021. ARBERT &\"}"}
{"id": "acl-2024-long-680", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MARBERT: deep bidirectional transformers for Arabic.\\n\\nMuhammad Abdul-Mageed, Chiyu Zhang, AbdelRahim Elmadany, and Lyle Ungar. 2020. Toward micro-dialect identification in diaglossic and code-switched environments. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5855\u20135876, Online. Association for Computational Linguistics.\\n\\nMohammad Al-Fetyani, Muhammad Al-Barham, Gheith Abandah, Adham Alsharkawi, and Maha Dawas. 2021. Masc: Massive Arabic speech corpus.\\n\\nAhmed Ali, Peter Bell, James Glass, Yacine Messaoui, Hamdy Mubarak, Steve Renals, and Yifan Zhang. 2016. The mgb-2 challenge: Arabic multi-dialect broadcast media recognition. In 2016 IEEE Spoken Language Technology Workshop (SLT), pages 279\u2013284. IEEE.\\n\\nAhmed Ali, Shammur Chowdhury, Amir Hussein, and Yasser Hifny. 2021. Arabic code-switching speech recognition using monolingual data.\\n\\nAhmed Ali, Suwon Shon, Younes Samih, Hamdy Mubarak, Ahmed Abdelali, James Glass, Steve Renals, and Khalid Choukri. 2019b. The mgb-5 challenge: Recognition and dialect identification of dialectal Arabic speech. In 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1026\u20131033.\\n\\nAhmed Ali, Stephan Vogel, and Steve Renals. 2017. Speech recognition challenge in the wild: Arabic mgb-3.\\n\\nRosana Ardila, Megan Branson, Kelly Davis, Michael Henretty, Michael Kohler, Josh Meyer, Reuben Morais, Lindsay Saunders, Francis M. Tyers, and Gregor Weber. 2020. Common voice: A massively-multilingual speech corpus.\\n\\nArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2021a. Xls-r: Self-supervised cross-lingual speech representation learning at scale.\\n\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020a. wav2vec 2.0: A framework for self-supervised learning of speech representations. Advances in neural information processing systems, 33:12449\u201312460.\\n\\nLo\u00efc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. 2023. Seamless: Multilingual expressive and streaming speech translation. ArXiv preprint arXiv:2312.05187.\\n\\nKaushal Santosh Bhogale, Sai Sundaresan, Abhigyan Raman, Tahir Javed, Mitesh M Khapra, and Pratyush Kumar. 2023. Vistaar: Diverse benchmarks and training sets for Indian language ASR. ArXiv preprint arXiv:2305.15386.\\n\\nHouda Bouamor, Nizar Habash, and Kemal Oflazer. 2014. A multidialectal parallel corpus of Arabic. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 1240\u20131245, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nWilliam Chan, Navdeep Jaitly, Quoc V Le, and Oriol Vinyals. 2015. Listen, attend and spell. ArXiv preprint arXiv:1508.01211.\\n\\nHeng-Jui Chang, Shu-Wen Yang, and Hung-yi Lee. 2021. Distilhubert: Speech representation learning by layer-wise distillation of hidden-unit BERT. CoRR, abs/2110.01900.\\n\\nVamsikrishna Chemudupati, Marzieh S. Tahaei, Heitor R. Guimar\u00e3es, Arthur Pimentel, Anderson R. Avila, Mehdi Rezagholizadeh, Boxing Chen, and Tiago H. Falk. 2023. On the transferability of whisper-based representations for \u201cin-the-wild\u201d cross-task downstream speech applications. ArXiv, abs/2305.14546.\\n\\nChung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. 2022. Self-supervised learning with random-projection quantizer for speech recognition. In International Conference on Machine Learning, pages 3915\u20133924. PMLR.\\n\\nShammur Absar Chowdhury, Amir Hussein, Ahmed Abdelali, and Ahmed Ali. 2021. Towards one model to rule all: Multilingual strategy for dialectal code-switching Arabic ASR.\"}"}
{"id": "acl-2024-long-680", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Howes, Bernie Huang, Min-Jae Hwang, Hirofumi Inaguma, Somya Jain, Elahe Kalbassi, Amanda Kallet, Ilia Kulikov, Janice Lam, Shang-Wen Li, Xutai Ma, Ruslan Mavlyutov, Benjamin Peloquin, M.L. Ramadanan, Abinesh Ramakrishnan, Anna Sun, Ke M. Tran, Tuan Tran, Igor Tufanov, Vish Vogeti, Carleigh Wood, Yilin Yang, Bo Yu, Pierre Yves Andrews, Can Balioglu, Marta Ruiz Costa-juss\u00e0, Onur \u00c7elebi, Maha Elbayad, Cynthia Gao, Francisco Guzm'an, Justine T. Kao, Ann Lee, Alexandre Mourachko, Juan Miguel Pino, Sravya Popuri, Christophe Ropers, Safiyyah Saleem, Holger Schwenk, Paden Tomasello, Changhan Wang, Jeff Wang, and Skyler Wang. 2023. Seamlessness4mt: Massively multilingual&multimodal machine translation.\\n\\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. 2020. Unsupervised cross-lingual representation learning for speech recognition. arXiv preprint arXiv:2006.13979.\\n\\nAlexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2022. Fleurs: Few-shot learning evaluation of universal representations of speech.\\n\\nKunal Dhawan, Dima Rekesh, and Boris Ginsburg. 2023. Towards training bilingual and code-switched speech recognition models from monolingual data sources. arXiv preprint arXiv:2306.08753.\\n\\nThomas Palmeira Ferraz, Marcely Zanon Boito, Caroline Brun, and Vassilina Nikoulina. 2024. Multilingual distilwhisper: Efficient distillation of multi-task speech models via language-specific experts.\\n\\nElias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. 2022. Gptq: Accurate post-training quantization for generative pre-trained transformers. arXiv preprint arXiv:2210.17323.\\n\\nSanchit Gandhi. 2024. Speculative decoding for 2x faster whisper inference. Hugging Face Blog.\\n\\nSanchit Gandhi, Patrick von Platen, and Alexander M. Rush. 2023. Distil-whisper: Robust knowledge distillation via large-scale pseudo labelling.\\n\\nJianping Gou, Baosheng Yu, Stephen J. Maybank, and Dacheng Tao. 2021. Knowledge distillation: A survey. International Journal of Computer Vision, 129(6):1789\u20131819.\\n\\nAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd international conference on Machine learning, pages 369\u2013376.\\n\\nNizar Y Habash. 2022. Introduction to Arabic natural language processing. Springer Nature.\\n\\nNawar Halabi et al. 2016. Arabic speech corpus. Oxford Text Archive Core Collection.\\n\\nGeoffrey Hinton, Oriol Vinyals, and Jeff Dean. 2015. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.\\n\\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. Hubert: Self-supervised speech representation learning by masked prediction of hidden units.\\n\\nRenren Jin, Jiangcun Du, Wuwei Huang, Wei Liu, Jian Luan, Bin Wang, and Deyi Xiong. 2024. A comprehensive evaluation of quantization strategies for large language models. arXiv preprint arXiv:2402.16775.\\n\\nSheikh Musa Kaleem, Tufail Rouf, Gousia Habib, Brejesh Lall, et al. 2024. A comprehensive review of knowledge distillation in computer vision. arXiv preprint arXiv:2404.00936.\\n\\nYoon Kim and Alexander M. Rush. 2016. Sequence-level knowledge distillation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1317\u20131327, Austin, Texas. Association for Computational Linguistics.\\n\\nSoroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. 2020. Compress: Self-supervised learning by compressing representations. CoRR, abs/2010.14713.\\n\\nAjinkya Kulkarni, Atharva Kulkarni, Sara Abedalmonem Mohammad Shatnawi, and Hanan Aldarmaki. 2023. Clartts: An open-source classical arabic text-to-speech corpus. arXiv preprint arXiv:2303.00069.\\n\\nSolomon Kullback and R. A. Leibler. 1951. On information and sufficiency. Annals of Mathematical Statistics, 22:79\u201386.\\n\\nAlexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. 2019. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700.\\n\\nYaniv Leviathan, Matan Kalman, and Yossi Matias. 2023. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pages 19274\u201319286. PMLR.\\n\\nJi Lin, Jiaming Tang, Haotian Tang, Shang Yang, Xingyu Dang, and Song Han. 2023. Awq: Activation-aware weight quantization for llm compression and acceleration. arXiv preprint arXiv:2306.00978.\\n\\nRaphael Gontijo Lopes, Stefano Fenu, and Thad Starner. 2017a. Data-free knowledge distillation for deep neural networks.\\n\\nRaphael Gontijo Lopes, Stefano Fenu, and Thad Starner. 2017b. Data-free knowledge distillation for deep neural networks. CoRR, abs/1710.07535.\"}"}
{"id": "acl-2024-long-680", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weijian Luo. 2023. A comprehensive survey on knowledge distillation of diffusion models. ArXiv, abs/2304.04262.\\n\\nVimal Manohar, Pegah Ghahremani, Daniel Povey, and Sanjeev Khudanpur. 2018. A teacher-student learning approach for unsupervised domain adaptation of sequence-trained ASR models. In 2018 IEEE Spoken Language Technology Workshop (SLT), pages 250\u2013257.\\n\\nHamdy Mubarak, Amir Hussein, Shammur Absar Chowdhury, and Ahmed Ali. 2021. Qasr: Qcri al-jazeera speech resource \u2013 a large scale annotated Arabic speech corpus.\\n\\nJinjie Ni, Yukun Ma, Wen Wang, Qian Chen, Dianwen Ng, Han Lei, Trung Hieu Nguyen, Chong Zhang, Bin Ma, and Erik Cambria. 2023. Adaptive knowledge distillation between text and speech pre-trained models.\\n\\nOpenNMT. Ctranslate2: Fast inference engine for transformer models.\\n\\nJing Pan, Tao Lei, Kwangyoun Kim, Kyu J. Han, and Shinji Watanabe. 2022. Sru++: Pioneering fast recurrence with attention for speech recognition. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 7872\u20137876.\\n\\nJuan M Perero-Codosero, Fernando M Espinoza-Cuadros, and Luis A Hern\u00e1ndez-G\u00f3mez. 2022. A comparison of hybrid and end-to-end ASR systems for the iberspeech-rtve 2020 speech-to-text transcription challenge. Applied Sciences, 12(2):903.\\n\\nRohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, Ralf Schl\u00fcter, and Shinji Watanabe. 2024. End-to-end speech recognition: A survey. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 32:325\u2013351.\\n\\nRiefkyanov Pratama and Agit Amrullah. 2024. Analysis of whisper automatic speech recognition performance on low resource language. Jurnal Pilar Nusa Mandiri, 20:1\u20138.\\n\\nVineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, Alexei Baevski, Yossi Adi, Xiaohui Zhang, Wei-Ning Hsu, Alexis Conneau, and Michael Auli. 2023a. Scaling speech technology to 1,000+ languages.\\n\\nVineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. 2023b. Scaling speech technology to 1,000+ languages. arXiv preprint arXiv:2305.13516.\\n\\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492\u201328518. PMLR.\\n\\nDima Rekesh, Samuel Kriman, Somshubra Majumdar, Vahid Noroozi, He Juang, Oleksii Hrinchuk, Ankur Kumar, and Boris Ginsburg. 2023. Fast conformer with linearly scalable attention for efficient speech recognition. 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 1\u20138.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2019. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter. arXiv preprint arXiv:1910.01108.\\n\\nVictor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. 2020. Distilbert, a distilled version of BERT: smaller, faster, cheaper and lighter.\\n\\nHang Shao, Wei Wang, Bei Liu, Xun Gong, Haoyu Wang, and Yanmin Qian. 2023. Whisper-kdq: A lightweight whisper via guided knowledge distillation and quantization for efficient ASR.\\n\\nSuwon Shon, Ahmed M. Ali, Younes Samih, Hamdy Mubarak, and James R. Glass. 2020. Adi17: A fine-grained Arabic dialect identification dataset. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8244\u20138248.\\n\\nSYSTRAN. faster-whisper: Faster whisper transcription with CTranslate2.\\n\\nBashar Talafha, Abdul Waheed, and Muhammad Abdul-Mageed. 2023a. N-shot benchmarking of whisper on diverse Arabic speech recognition. arXiv preprint arXiv:2306.02902.\\n\\nBashar Talafha, Abdul Waheed, and Muhammad Abdul-Mageed. 2023b. N-shot benchmarking of whisper on diverse Arabic speech recognition.\\n\\nHawau Olamide Toyin, Amirbek Djanibekov, Ajinkya Kulkarni, and Hanan Aldarmaki. 2023. Artst: Arabic text and speech transformer. arXiv preprint arXiv:2310.16621.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nAbdul Waheed, Bashar Talafha, Peter Sullivan, Abdel-Rahim Elmadany, and Muhammad Abdul-Mageed. 2023. VoxArabica: A robust dialect-aware Arabic speech recognition system. In Proceedings of ArabicNLP 2023, pages 441\u2013449, Singapore (Hybrid). Association for Computational Linguistics.\\n\\nAiden Williams, Andrea Demarco, and Claudia Borg. 2023. The applicability of Wav2Vec2 and Whisper for low-resource Maltese ASR. In Proc. 2nd Annual Meeting of the ELRA/ISCA SIG on Under-resourced Languages (SIGUL 2023), pages 39\u201343.\"}"}
{"id": "acl-2024-long-680", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Related Work\\n\\nWhile early ASR systems were primarily hybrid (Perero-Codosero et al., 2022), often in the form of combinations of Hidden Markov Models (HMMs) and either Gaussian Mixture Models (GMMs) or Deep Neural Networks (DNNs), the desire for simpler architectures led to a shift towards End-to-End (E2E) models (Prabhavalkar et al., 2024). This was made possible in part thanks to the availability of extensive labeled datasets and increased computational power. Transformers (Vaswani et al., 2017) have come to light as the dominant architecture in modern ASR systems (Pan et al., 2022), owing to their attention mechanism's ability to model long-range dependencies all while being scalable and efficient.\\n\\nOpenAI's Whisper (Radford et al., 2023), a weakly supervised encoder-decoder Transformer, was trained on an extensive 630K hours of multilingual data, 739 of which are in Arabic. Whisper supports multilingual ASR, Automatic Speech Translation (AST) to English, and Language Identification (LID). Massively Multilingual Speech (MMS) (Pratap et al., 2023a), a system for multilingual ASR, speech synthesis (TTS) and LID build by Meta, is the result of pre-training wav2vec 2.0 (Baevski et al., 2020b) models (300M and 1B parameter versions) on 419K hours from 6 different corpora, spanning 1406 languages. For the ASR task, they fine-tune the pre-trained 1B model on 44.7K hours of labeled data in 1107 languages using Connectionist Temporal Classification (CTC) (Graves et al., 2006). Meta also developed SeamlesM4T v2 (Barrault et al., 2023; Communication et al., 2023), a collection of models featuring the new w2v-BERT 2.0 speech encoder pre-trained on 4.5M unlabeled data hours and fine-tuned on automatically aligned pairs. It supports 100 languages and its Arabic training data includes 119K hours of raw audio and 822 hours of labeled data. Another system that performs ASR and AST is Google's Universal Speech Model (USM) (Zhang et al., 2023b), a 2B parameter model employing a Conformer encoder. It was pre-trained using BEST-RQ (Chiu et al., 2022) on 12M unlabeled hours covering 300 distinct languages. Supervised ASR training was then used on the Conformer features using either CTC or Listen, Attend and Spell (LAS) (Chan et al., 2015) transducers using 90K hours of labeled data across 70 languages. XLS-R (Babu et al., 2021b) is yet another wav2vec 2.0-based model used for ASR, AST and speech classification tasks (LID and Speaker ID). It comes in variants of 0.3B, 1B and 2B parameters, trained on 436K hours (95 are in Arabic) that include 128 languages. ArTST (Toyin et al., 2023) is a SpeechT5 model focused on MSA and fine-tuned on the MGB3 dataset for ASR and on ASC (Halabi et al., 2016) and ClArTTS (Kulkarni et al., 2023) for TTS.\"}"}
{"id": "acl-2024-long-680", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Training parameters. We use all the default training parameters provided in Huggingface Seq2SeqTrainingArguments unless otherwise specific in this table.\\n\\n| Parameter            | Value                  |\\n|----------------------|------------------------|\\n| warmup_steps         | 50                     |\\n| learning_rate        | 0.0001                 |\\n| lr_scheduler_type    | constant _with_warmup  |\\n| batch_size           | 128                    |\\n| max_label_length     | 225                    |\\n| gradient_accumulation_steps | 1          |\\n| dtype bfloat         | 16                     |\\n\\nTable 6: WER/CER on validation split of each dataset. Our in-house data only includes a single split reported in Table 3. Abbreviations. W - Whisper, FT - Finetuned, M - Medium, L - Large, S - Small, D - Distil.\"}"}
{"id": "acl-2024-long-680", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | Test Val. | Test Val. | Test Val. | Test Val. | Test Val. | Test Val. |\\n|------------------------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| XLS-R                  | 92.2/47.4 | 92.9/47.1 | 92.8/46.9 | 88.0/37.7 | 89.9/39.6 | 89.8/39.5 |\\n| HuBERT                 | 78.9/33.1 | 76.6/31.1 | 76.5/31.0 | 52.0/17.8 | 54.7/18.7 | 54.8/18.8 |\\n| W-FT                   | 74.9/36.7 | 69.8/33.5 | 69.5/33.3 | 32.8/21.8 | 34.9/21.1 | 35.0/21.1 |\\n| MMS-all                | 106.1/82.4| 106.0/82.6| 105.9/82.5| 106.8/80.2| 106.5/80.9| 106.4/80.9|\\n| SM4T-M                 | 40.8/17.4 | 42.1/18.2 | 42.1/18.1 | 13.2/4.9  | 16.2/5.7  | 16.2/5.7  |\\n| SM4T-L-v1              | 43.3/19.2 | 44.2/19.2 | 44.0/19.0 | 15.8/6.4  | 19.6/7.4  | 19.6/7.3  |\\n| SM4T-L-v2              | 34.2/13.5 | 37.5/15.8 | 37.4/15.7 | 8.4/2.8   | 11.1/3.5  | 11.1/3.5  |\\n| W-S                    | 73.9/35.4 | 68.7/31.7 | 68.9/31.9 | 44.0/19.2 | 40.3/16.3 | 40.3/16.4 |\\n| W-M                    | 59.1/26.3 | 55.4/24.4 | 55.5/24.6 | 25.8/11.9 | 29.5/13.0 | 29.8/13.4 |\\n| W-L-v2                 | 51.4/21.9 | 47.9/20.3 | 47.7/20.2 | 16.2/7.0  | 19.8/8.2  | 19.9/8.2  |\\n| W-L-v3                 | 49.2/19.8 | 43.7/17.3 | 43.6/17.1 | 12.8/4.4  | 15.5/5.1  | 15.6/5.2  |\\n| DW-8-8                 | 58.2/24.8 | 55.4/23.5 | 55.2/23.3 | 28.5/10.4 | 32.5/12.2 | 32.6/12.2 |\\n| DW-16-16               | 52.6/21.3 | 48.5/19.3 | 48.3/19.1 | 18.5/6.1  | 21.9/7.2  | 22.1/7.2  |\\n| DW-32-16               | 50.5/20.2 | 46.2/18.0 | 46.0/17.9 | 15.2/4.8  | 18.5/5.8  | 18.7/5.8  |\\n| DW-16-32               | 52.0/21.1 | 47.9/19.2 | 47.7/19.0 | 17.6/5.9  | 21.2/7.2  | 21.3/7.3  |\\n| DW-16-16++             | 51.2/20.6 | 46.6/18.4 | 46.5/18.2 | 15.8/5.2  | 19.0/6.2  | 19.1/6.2  |\\n| DW-32-16++             | 49.8/19.9 | 45.2/17.7 | 45.0/17.5 | 13.7/4.4  | 16.9/5.4  | 17.0/5.5  |\\n\\nTable 7: Test and validation split results for other common voice versions. Abbreviations. W - Whisper, FT - Finetuned, M - Medium, L - Large, S - Small, D - Distil.\"}"}
{"id": "acl-2024-long-680", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model     | Overall Avg. | Avg. Benchmark | Avg. In-House |\\n|-----------|--------------|----------------|--------------|\\n| Amazon    | 61.0/41.8    | -/-            | -/-          |\\n| XLS-R     | 97.7/58.4    | 96.1/53.2      | 99.4/63.5    |\\n| HuBERT    | 66.7/27.3    | 51.5/20.4      | 81.9/34.2    |\\n| W-FT      | 67.7/39.9    | 42.2/24.5      | 93.2/55.2    |\\n| MMS-all   | 82.5/54.4    | 66.9/36.2      | 98.0/72.6    |\\n| SM4T-M    | 48.1/21.7    | 33.9/17.3      | 62.3/26.0    |\\n| SM4T-L-v1 | 51.7/24.7    | 37.4/19.5      | 66.0/29.9    |\\n| SM4T-L-v2 | 47.0/22.6    | 32.3/17.7      | 61.7/27.6    |\\n| W-S       | 80.8/45.7    | 66.0/35.3      | 95.6/56.1    |\\n| W-M       | 65.4/38.5    | 54.3/32.9      | 76.4/44.1    |\\n| W-L-v2    | 55.1/32.3    | 42.0/25.7      | 68.2/38.9    |\\n| W-L-v3    | 49.5/25.4    | 31.4/15.6      | 67.7/35.2    |\\n| DW-8-8    | 64.8/32.1    | 51.3/26.2      | 78.3/38.0    |\\n| DW-16-16  | 53.2/23.2    | 40.0/18.6      | 66.3/27.9    |\\n| DW-32-16  | 48.2/20.0    | 35.4/16.3      | 61.1/23.7    |\\n| DW-16-32  | 53.0/25.1    | 39.4/19.6      | 66.5/30.6    |\\n| DW-16-16++| 49.5/22.5    | 36.7/17.8      | 62.3/27.2    |\\n| DW-32-16++| 45.0/19.2    | 33.0/15.7      | 56.9/22.7    |\\n\\nTable 8: Average WER/CER scores on the benchmark, in-house, and overall data. Avg.: Average.\"}"}
{"id": "acl-2024-long-680", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Filtering Threshold ($\\\\lambda$) | 10 (82.8) | 20 (74.7) | 40 (54.5) | 80 (28.0) | None (0.0) |\\n|-------------------------------|----------|----------|----------|----------|-----------|\\n| Model                         | Dataset   | Split     | Orth. N+ND | Orth. N+ND | Orth. N+ND |\\n\\nTable 9: Results for different threshold ($\\\\lambda$) values distilling from 100K segments. The value in the bracket along\\n\\nTable 10: Examples for the different error categories observed during error analysis. Dia.: Dialect. Trans.: Translation. Unr.: Unrecognized. Pron.: Pronunciation. Alt.: Alternative.\"}"}
{"id": "acl-2024-long-680", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Error Type   | Algeria | Jordan | Palestine | UAE | Yemen |\\n|---------------|--------------|---------|--------|-----------|-----|-------|\\n| SM4T-L-v2     | Total err. count | 86      | 24     | 20        | 106 | 124   |\\n|               | Hallucination (%) | 20.9   | 37.5   | 55.0      | 29.3 | 18.6  |\\n|               | Deterioration (%) | 26.7   | 33.3   | 20.0      | 26.4 | 28.2  |\\n|               | Empty (%) | 1.2     | 0.0    | 0.0       | 0.0  | 0.0   |\\n|               | Incomplete (%) | 8.1     | 4.2    | 5.0       | 1.89 | 0.8   |\\n|               | MSA translation (%) | 31.4   | 8.3    | 5.0       | 2.8  | 0.8   |\\n|               | Dia. inaccuracies (%) | 19.8   | 16.7   | 20.0      | 41.5 | 51.6  |\\n| W-L-v3        | Total err. count | 87      | 27     | 16        | 104 | 111   |\\n|               | Hallucination (%) | 32.2   | 18.5   | 25.0      | 28.9 | 21.6  |\\n|               | Deterioration (%) | 31.0   | 33.3   | 43.8      | 36.5 | 34.2  |\\n|               | Empty (%) | 0.0     | 0.0    | 0.0       | 0.0  | 0.0   |\\n|               | Incomplete (%) | 1.2     | 7.4    | 0.0       | 5.8  | 6.3   |\\n|               | MSA translation (%) | 23.0   | 33.3   | 12.5      | 10.6 | 8.1   |\\n|               | Dia. inaccuracies (%) | 18.4   | 11.1   | 18.8      | 19.2 | 29.7  |\\n| W-M           | Total err. count | 90      | 59     | 20        | 253 | 213   |\\n|               | Hallucination (%) | 35.6   | 28.8   | 40.0      | 35.2 | 33.3  |\\n|               | Deterioration (%) | 31.1   | 22.0   | 35.0      | 37.2 | 26.3  |\\n|               | Empty (%) | 0.0     | 0.0    | 0.0       | 0.0  | 0.0   |\\n|               | Incomplete (%) | 13.3    | 33.9   | 10.0      | 17.8 | 26.3  |\\n|               | MSA translation (%) | 14.4   | 15.3   | 10.0      | 6.7  | 4.2   |\\n|               | Dia. inaccuracies (%) | 6.7    | 5.1    | 5.0       | 6.3  | 13.2  |\\n| HuBERT        | Total err. count | 28      | 4      | 9         | 75  | 61    |\\n|               | Hallucination (%) | 14.3   | 25.0   | 22.2      | 22.7 | 37.7  |\\n|               | Deterioration (%) | 57.1   | 50.0   | 55.6      | 68.0 | 60.7  |\\n|               | Empty (%) | 10.7    | 0.0    | 11.1      | 5.3  | 16.4  |\\n|               | Incomplete (%) | 3.6     | 0.0    | 0.0       | 1.4  | 4.9   |\\n|               | MSA translation (%) | 0.0    | 0.0    | 0.0       | 0.0  | 0.0   |\\n|               | Dia. inaccuracies (%) | 14.3   | 25.0   | 11.1      | 2.7  | 3.3   |\\n| DW-32-16 (Ours) | Total err. count | 12      | 3      | 4         | 39  | 50    |\\n|               | Hallucination (%) | 50.0   | 66.7   | 50.0      | 20.5 | 12.0  |\\n|               | Deterioration (%) | 25.0   | 0.0    | 25.0      | 23.1 | 20.0  |\\n|               | Empty (%) | 0.0     | 0.0    | 0.0       | 0.0  | 2.0   |\\n|               | Incomplete (%) | 8.3     | 0.0    | 25.0      | 2.6  | 4.0   |\\n|               | MSA translation (%) | 8.3    | 33.3   | 0.0       | 2.6  | 4.0   |\\n|               | Dia. inaccuracies (%) | 8.3   | 0.0    | 0.0       | 51.3 | 58.0  |\\n\\nTable 11: Error analysis statistics of different systems evaluated on our in-house data. Err.: Error. Dia.: Dialectal.\"}"}
