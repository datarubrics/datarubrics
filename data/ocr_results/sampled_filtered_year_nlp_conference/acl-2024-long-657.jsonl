{"id": "acl-2024-long-657", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Joint Coreference-Aware Approach to Document-Level Target Sentiment Analysis\\nHongjie Cai, Heqing Ma, Jianfei Yu, Rui Xia\\n\\nSchool of Computer Science and Engineering, Nanjing University of Science and Technology, China\\n{hjcai, hqma, jfyu, rxia}@njust.edu.cn\\n\\nAbstract\\nMost existing work on aspect-based sentiment analysis (ABSA) focuses on the sentence level, while research at the document level has not received enough attention. Compared to sentence-level ABSA, the document-level ABSA is not only more practical but also requires holistic document-level understanding capabilities such as coreference resolution. To investigate the impact of coreference information on document-level ABSA, we conduct a three-stage research for the document-level target sentiment analysis (DTSA) task: 1) exploring the effectiveness of coreference information for the DTSA task; 2) reducing the reliance on manually annotated coreference information; 3) alleviating the evaluation bias caused by missing the coreference information of opinion targets. Specifically, we first manually annotate the coreferential opinion targets and propose a multi-task learning framework to model the DTSA task and the coreference resolution task jointly. Then we annotate the coreference information with ChatGPT for joint training. Finally, to address the issue of missing coreference targets, we modify the metric from strict matching to a loose matching method based on the clusters of targets. The experimental results demonstrate our framework's effectiveness and reflect the feasibility of using ChatGPT-annotated coreferential entities and the applicability of the modified metric. Our source code is publicly released at https://github.com/NUSTM/DTSA-Coref.\\n\\n1 Introduction\\nAspect-based sentiment analysis (ABSA) aims to extract opinion targets from review texts and the sentiment towards each opinion target (Hu and Liu, 2004). For example, in the sentence \\\"The food is delicious.\\\", the opinion target is \\\"food\\\" and its sentiment is positive.\\n\\nEarly work (Hu and Liu, 2004; Jiang et al., 2011) employed rules or machine learning methods to solve the task on their annotated datasets. After the introduction of ABSA evaluation tasks and benchmark datasets in (Pontiki et al., 2014, 2015, 2016), subsequent work deepened the research on ABSA, exploring different model paradigms in effects on ABSA (Li et al., 2018; Wu et al., 2020; Zhang et al., 2021a; Gou et al., 2023), also by improving the task definition, providing rich and diverse ABSA datasets to extract opinion targets and their sentiments more accurately (Jiang et al., 2019; Cai et al., 2021; Barnes et al., 2022). Despite these advancements, most existing work focuses on the sentence level, overlooking the fact that product reviews are typically presented in the form of documents, which is not only more practical but also enables the utilization of valuable document-level information for ABSA tasks. Recently, (Luo et al., 2022) introduced a task of document-level target sentiment analysis (DTSA), hierarchically depicting the opinion target. As shown in Figure 1, the review mentions three opinion targets: \\\"work shoes\\\", the \\\"leather\\\" of the work shoes (represented as \\\"work shoes\\\"), and \\\"toe area\\\" (represented as \\\"toe area\\\").\"}"}
{"id": "acl-2024-long-657", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shoes\u2013leather\u201d), and the \u201ctoe area\u201d of the work shoes (represented as \u201cwork shoes\u2013toe area\u201d). It expresses a positive sentiment towards \u201cleather\u201d, a negative sentiment towards \u201ctoe area\u201d, and a mixed sentiment towards \u201cwork shoes\u201d.\\n\\nA typical phenomenon in the DTSA task is the coreference of opinion targets, especially across multiple sentences. As the example shown in Figure 1, the \u201cwork shoes\u201d have two coreferential mentions \u201cthey\u201d and \u201cThey\u201d, and the three aspect terms \u201cwork shoes\u201d, \u201cthey\u201d, and \u201cThey\u201d refer to the same opinion target. Combining context understanding of these pronouns\u2019 referent may help differentiate the sentiment of different opinion targets and accurately identify the hierarchical relation among opinion targets. Although such coreference phenomenon is prevalent in review texts and can affect the accuracy of opinion target extraction in the ABSA research, very few studies have investigated the coreference problem (Mullick et al., 2023; R\u00f8nningstad et al., 2023), especially in the DTSA task, which is relatively complex and has a higher incidence of coreference in review texts. Currently, there is no relevant research addressing coreference problems in this task.\\n\\nTo explore the impact of coreference resolution on the DTSA task, the following three questions should be addressed: First, in the end-to-end DTSA task, does the coreference information contribute to the accurate identification of hierarchical relations among opinion targets and their sentiments? Second, coreference annotation requires substantial human effort and time. Is it possible to mine coreference information without manual efforts? Third, the same opinion target may have different forms of textual expression, yet existing ABSA datasets often select only one expression as the ground truth label. This approach may treat coreferent expressions of the same target as incorrect during evaluation. How can we mitigate the evaluation errors caused by the omission of coreference annotations?\\n\\nTo address the first question, we annotate the coreferential targets of the opinion targets, proposing a coreference-aware joint model to investigate whether the coreference information can improve the model\u2019s ability to identify hierarchical opinion targets and their sentiments. The model employs a dual-path architecture, utilizing multi-task learning to jointly handle the DTSA task and coreference resolution task. The left path is designed to identify opinion targets, their sentiments, and the hierarchical relations between them. The right path focuses on identifying the opinion targets and their coreferential relations. By sharing parameters and aligning tokens between the two paths, the model can learn to leverage coreference information and enhance its performance on the DTSA task. In response to the second question, we leverage ChatGPT to annotate the coreference information and verify the efficacy of the coreference annotation in the joint model. To answer the third question, we modify the metric to employ a cluster matching method that takes into account coreferent opinion targets. The experimental results demonstrate the effectiveness of introducing coreference information through a dual-path model. Additionally, the coreference annotations provided by ChatGPT can achieve comparable DTSA performance to those of human annotations. Finally, the cluster-level evaluation metric is also proven to be more reasonable in assessing the effectiveness of opinion target extraction in consideration of the coreference problem.\\n\\n2 Related Work\\n\\nIn recent years, ABSA has been extensively studied by researchers, and works are primarily focused on the sentence level, emphasizing either single-element extraction or multi-element extraction.\\n\\nSentence-level ABSA.\\n\\nEarly sentence-level ABSA (Dong et al., 2014) often constructed datasets sourced from Twitter. Following the introduction of the ABSA task through SemEval evaluations by (Pontiki et al., 2014, 2015, 2016), subsequent research proposed numerous methods for sentence-level ABSA (Ma et al., 2017; Xu et al., 2019; Tang et al., 2020). Additionally, a portion of the work further focused on the end-to-end ABSA task, mainly aiming to jointly extract the aspects and their sentiments within sentences. (Li et al., 2019; Luo et al., 2019; He et al., 2019) have introduced various solutions based on encoder-only models for this task. With the development of generative language models, (Zhang et al., 2021a,b) subsequently proposed solutions based on BART or T5.\\n\\nDocument-level ABSA.\\n\\nA few ABSA work was conducted at the document level. (Hu and Liu, 2004; Ding et al., 2008) annotated document-level ABSA datasets across multiple domains and researched multiple aspect extractions on these datasets. (Chen et al., 2020b) investigated the co...\"}"}
{"id": "acl-2024-long-657", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"consistency of the same aspect\u2019s sentiments across documents as well as the correlation between sentiments across different aspects. (Luo et al., 2022) introduced the DTSA task and proposed a framework based on BART to solve this task. Furthermore, (Song et al., 2023) introduced an encoder-based Sequence-to-Structure framework to explicitly model the hierarchical relations between entities. Our work, taking DTSA as a starting point, investigates the impact of coreference on DTSA.\\n\\nCoreference Resolution.\\n\\nCoreference resolution aims to identify all expressions that refer to the same entity from the text. Since (Lee et al., 2017) first introduced a deep learning method for end-to-end coreference resolution, coreference resolution has been increasingly integrated into related downstream tasks. (Hu and Liu, 2004) marked entities requiring pronoun resolution in ABSA datasets. (Ding and Liu, 2010) introduced a coreference classification task for objects and entities in comparative reviews. Building on this, (Chen et al., 2020a) proposed a method for automatic mining and utilizing domain-specific knowledge to address coreferences of entities. Moreover, (Mullick et al., 2023; R\u00f8nningstad et al., 2023) investigated whether the coreference resolution of entity is beneficial for the ABSC task.\\n\\n3 Problem Definition\\n\\nIn traditional sentence-level ABSA tasks, given a review sentence $s$, the goal is to identify the opinion targets (fine-grained entities or their aspects, collectively referred to as entities) mentioned in the sentence and their corresponding sentiment $\\\\{..., (a_i, p_i), ...\\}$, where $a_i$ represents the extracted $i$-th entity, typically presented as a continuous text span in the sentence, and $p_i$ denotes the sentiment towards $a_i$, such as positive, negative, or neutral.\\n\\nIn the DTSA task, the input extends from a single sentence to a document $d = [s_1, ..., s_n]$ consisting of $n$ sentences. The output aims to identify all hierarchical entities and their corresponding sentiments $\\\\{..., (t_i, p_i), ...\\}$. Unlike the flat entity $a_i$ in sentence-level ABSA, $t_i$ represents a multi-level entity composed of multiple flat entities. We will use hierarchical entities to represent multi-level opinion targets in the following. It can be seen that extracting hierarchical entities in the DTSA task is more challenging than sentence-level ABSA tasks, particularly when the metric requires an exact match with the ground truth. Considering the entity coreference in documents, we also modify the metric from the exact entity-level matching to a cluster-level matching.\\n\\n4 Methodology\\n\\nTo investigate whether the information of entity coreference can aid in identifying hierarchical entities and their sentiments, we first annotate the coreferential entities for each entity $a_i$, forming a coreference cluster $C_i = \\\\{a_{i0}, ..., a_{ik}\\\\}$ (\u00a74.1). Next, we incorporate the coreference information into the DTSA task through multi-task learning, leveraging both human-annotated and machine-annotated coreference information to improve the model\u2019s ability to recognize coreference (\u00a74.2).\\n\\n4.1 Entity Coreference Annotation and Analysis\\n\\nThe coreference of entities is manifested as multiple entities at different positions referring to the same entity. To annotate entity coreference, we need to label these entities at different positions and cluster them into a coreferential entity cluster. We obtain the coreference information of entities through both human annotation and ChatGPT annotation (refer readers to Appendix A for annotation details), respectively.\\n\\nStatistics and Analysis\\n\\nAs shown in Table 1, in each domain, the number of documents with human-annotated coreference is higher than those annotated by ChatGPT. The proportion of human-annotated coreference documents ranges from 44% to 59% of the total documents, while ChatGPT-annotated coreference documents account for 35% to 48%. However, for the documents annotated with coreference, the average number of entities per\"}"}
{"id": "acl-2024-long-657", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I wear between a 9.5 and 10 but ordered the 10, they fit but the toe area is a bit long. They are kinda narrow but will stretch being good leather. Overall it is an okay purchase for work shoes.\"}"}
{"id": "acl-2024-long-657", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"p(y|hm) = Sigmoid(Wm\\\\ne hm\\\\ne + bm\\\\ne ),\\n\\nWhen \\\\( p(ys|hm) > 0.5 \\\\), it indicates that \\\\( x_i \\\\) is a candidate start token. Similarly, when \\\\( p(ys|hm) > 0.5 \\\\), it indicates that \\\\( x_i \\\\) is a candidate end token.\\n\\nAfter performing the binary classification for all tokens in the text, we obtain the candidate start token set\\n\\\\[ S_{\\\\text{start}} = \\\\{\\\\ldots, st_i, \\\\ldots\\\\} \\\\]\\nand the candidate end token set\\n\\\\[ S_{\\\\text{end}} = \\\\{\\\\ldots, ed_i, \\\\ldots\\\\} \\\\], where the superscript \\\\( m \\\\) denotes the main task.\\n\\nAfter obtaining \\\\( S_{\\\\text{start}} \\\\) and \\\\( S_{\\\\text{end}} \\\\), we take the Cartesian product of the two sets to obtain the candidate entity set with paired start and end tokens\\n\\\\[ \\\\tilde{A}_m = \\\\{\\\\ldots, \\\\tilde{a}_i, \\\\ldots\\\\} \\\\], and apply the following heuristic rules to obtain the final entity set \\\\( A_m = \\\\{\\\\ldots, a_i, \\\\ldots\\\\} \\\\):\\n1) the index of the start token should be smaller than that of the end token;\\n2) the index distance is not greater than \\\\( t \\\\);\\n3) the sum of the probabilities of the start and end tokens should exceed the threshold \\\\( \\\\theta \\\\);\\n4) there are at most \\\\( K \\\\) candidate entities.\\n\\nWe define the loss of entity extraction as:\\n\\\\[\\n\\\\text{loss}_{ye} = -B \\\\left( \\\\frac{1}{n} \\\\sum_{i=1}^{n} (\\\\hat{y}_s^i \\\\log(p(ys^i)) + \\\\hat{y}_e^i \\\\log(p(ye^i))) \\\\right),\\n\\\\]\\nwhere \\\\( B \\\\) is the batch size, \\\\( n \\\\) is the number of tokens, \\\\( \\\\hat{y}_s \\\\) and \\\\( \\\\hat{y}_e \\\\) represent the gold labels for the start and end entity classification tasks, respectively.\\n\\nEntity Sentiment Classification\\nAfter obtaining the entity set \\\\( A_m \\\\), we use the entity-context cross-attention module to obtain the context-aware entity representation \\\\( h_{ma_i} \\\\), and then feed \\\\( h_{ma_i} \\\\) into a softmax layer to obtain the sentiment towards \\\\( a_i \\\\):\\n\\\\[\\np(ya_i|h_{ma_i}) = \\\\text{Softmax}(W_{ma}h_{ma_i} + b_{ma_i}),\\n\\\\]\\nwhere \\\\( ya_i \\\\in \\\\{\\\\text{positive, negative, mixed}\\\\} \\\\). In this way, we can obtain the set of entities and their corresponding sentiments\\n\\\\[ A_P = \\\\{\\\\ldots, (a_i, p_i), \\\\ldots\\\\} \\\\].\\n\\nThe loss for entity sentiment classification is:\\n\\\\[\\n\\\\text{loss}_{ym} = -B \\\\left( \\\\frac{1}{|A_m|} \\\\sum_{i=1}^{|A_m|} (\\\\hat{y}_a^i \\\\log(p(ya^i))) \\\\right),\\n\\\\]\\n\\nEntity Hierarchical Classification\\nSince the hierarchical entity \\\\( t_i \\\\) is composed of multiple entities \\\\( a \\\\), to obtain all hierarchical entities, we first pair each \\\\( a \\\\) in the entity set \\\\( A_m \\\\) to obtain a set of candidate edges \\\\( U \\\\), and then perform a three-class classification on each candidate edge \\\\( u_{ij} = (a_i, a_j) \\\\):\\n\\\\[\\np(y_{u_{ij}}|a_i, a_j) = \\\\text{Sigmoid}(W_{u_{ij}}[h_{ma_i}:h_{ma_j}] + b_{u_{ij}}),\\n\\\\]\\nwhere \\\\( y_{u_{ij}} \\\\in \\\\{0, 1, 2\\\\} \\\\), denoting no relation, hierarchical relation, and coreference relation between entities, respectively. Next, we connect the heads and tails of the candidate edges, remove cycles, and obtain the hierarchical entity set\\n\\\\[ \\\\{\\\\ldots, t_i, \\\\ldots\\\\} \\\\].\\n\\nThe loss for entity hierarchical classification is:\\n\\\\[\\n\\\\text{loss}_{my} = -B \\\\left( \\\\frac{1}{|U|} \\\\sum_{i=1}^{|U|} (\\\\hat{y}_{u_i} \\\\log(p(y_{u_i}))) \\\\right).\\n\\\\]\\n\\n4.2.2 Coreference Resolution Modeling\\nWe employ another coreference-RoBERTa model to encode coreference information and incorporate it into the DTSA task. It shares model parameters with the DTSA-RoBERTa model. Considering that both coreference and hierarchical relations are partial order among entities, we use the same classifier to model these two types of relations. Similarly, we obtain a candidate entity set\\n\\\\[ A_c = \\\\{\\\\ldots, a_c_i, \\\\ldots\\\\} \\\\] and their representations \\\\( \\\\{\\\\ldots, h_{ca_i}, \\\\ldots\\\\} \\\\) for the coreference resolution task.\\n\\nThen, we pair these entities to obtain a set of candidate coreferent entity pairs \\\\( C \\\\), and perform a three-class classification for each candidate entity pair \\\\( c_{ij} = (a_c_i, a_c_j) \\\\) to determine whether they refer to the same entity:\\n\\\\[\\np(y_{c_{ij}}|a_c_i, a_c_j) = \\\\text{Sigmoid}(W_{u_{c_{ij}}}[h_{ca_i}:h_{ca_j}] + b_{u_{c_{ij}}}),\\n\\\\]\\nwhere \\\\( W_{u} \\\\) is used to identify both hierarchical and coreference relations. After obtaining the information on whether each candidate entity pair refers to the same entity, we cluster these entity pairs to form the final coreferential clusters. The loss of coreference resolution is:\\n\\\\[\\n\\\\text{loss}_{cy} = -B \\\\left( \\\\frac{1}{|C|} \\\\sum_{i=1}^{|C|} (\\\\hat{y}_{c_i} \\\\log(p(y_{c_i}))) \\\\right).\\n\\\\]\\n\\nToken-Level Coreference Constraint\\nTo incorporate coreference information into the left-path entities while disregarding the noises brought by coreferential entities, we design the token-level coreference constraint (TC) module. By adding token-level constraints on whether the left-path entities and the right-path entities refer to the same entity, we align the representations of the left-path entities with their coreferent entities, thereby enhancing the left-path entities' ability to perceive the contexts of their coreferent entity. Specifically, we utilize the representations of the left-path entities \\\\( H_{mL} \\\\) and the representations of the right-path\"}"}
{"id": "acl-2024-long-657", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To obtain a token-level score matrix:\\n\\\\[ p(y_{tc \\\\mid i,j} | H_{m \\\\mid L}, H_{c \\\\mid L}) = \\\\text{Sigmoid}(W_{tc}((H_{m \\\\mid L})^\\\\top H_{c \\\\mid L}) + b_{tc}), \\\\]\\n\\nand the token-level coreference loss is:\\n\\\\[ \\\\text{loss}_{tc} = -\\\\frac{1}{B} \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{n} (\\\\hat{y}_{tc \\\\mid i,j} \\\\log(p(y_{tc \\\\mid i,j}))). \\\\]\\n\\nModel Training\\nOur model is based on a multi-task learning framework and consists of three modules: the main DTSA task, and two auxiliary tasks, coreference resolution, and the TC task. The loss is defined as the weighted sum of the individual task losses:\\n\\\\[ \\\\text{loss} = \\\\text{loss}_{m} + \\\\text{loss}_{c} + \\\\alpha \\\\text{loss}_{tc}, \\\\]\\n\\nwhere \\\\( \\\\text{loss}_{m} \\\\), \\\\( \\\\text{loss}_{c} \\\\), and \\\\( \\\\text{loss}_{tc} \\\\) are the losses for the DTSA, the coreference resolution, and the TC tasks, respectively, and \\\\( \\\\alpha \\\\) is the hyperparameter.\\n\\n5 Experiments\\n5.1 Datasets and Experimental Settings\\nDataset and Metrics\\nThe DTSA dataset (Luo et al., 2022) encompasses reviews from four e-commerce domains: Books, Clothing, Hotels, and Restaurants. Each domain contains approximately 1000 annotated documents, with the average number of sentences annotated per document ranging from 3 to 8. Table 2 provides detailed statistics on the dataset division across these domains, distributed in a 7:1:2 ratio for training, validation, and testing, respectively. In evaluation, a target-sentiment pair is viewed as correct if and only if the entities of the target, the sentiment, and their combination are the same as those in the gold target-sentiment pair. We calculate the Precision, Recall, and use \\\\( F_1 \\\\) score (Luo et al., 2022) as the evaluation metric for the DTSA task. In addition, we design a new cluster-based metric allowing for a more accurate assessment of opinion target extraction and report its results in \u00a75.6.\\n\\nParameter Setting\\nWe initialize the dual-path RoBERTa models with RoBERTa-base parameters. The maximum input length for RoBERTa is set to 512, with the maximum number of entities extracted from the entity set capped at 60. Additionally, the parameters for constraining the entity length \\\\( t \\\\) and the entity selection confidence \\\\( \\\\theta \\\\) are set to 8 and -0.1, respectively. The optimization of model parameters for both paths is conducted using the AdamW optimizer, with a learning rate of 3e-5. The training epochs and dropout rate are set to 30 and 0.1, respectively. During training, model parameters are saved at the point of best performance on the validation set, and the results on the test set are averaged over five random seeds.\\n\\n5.2 Baseline Systems\\nTo verify the effectiveness of the coreference information, we adopt the following competitive models as baseline systems, including ChatGPT, three generative models, and a non-generative model.\\n\\n\u2022 GPT-3.5-Turbo: Experiments are conducted on GPT-3.5-Turbo (Ouyang et al., 2022) with OpenAI's API. Specifically, prompt templates are designed for the DTSA task, and the model's performance is evaluated under a five-shot setting. For specific prompt design, please refer to Table 6.\\n\\n\u2022 Seq2Seq: (Luo et al., 2022) uses a generative model to model the DTSA task in an end-to-end manner.\\n\\n\u2022 BART/T5-Extraction: The extraction-based generative framework proposed by (Zhang et al., 2021b) for sentence-level ABSA. In this framework, (Song et al., 2023) use BART and T5 as backbones, separating hierarchical entities and sentiments with special symbols, and sequentially outputting the final results.\\n\\n\u2022 BART/T5-Paraphrase: The paraphrase-based generative framework proposed by (Zhang et al., 2021a) for sentence-level ABSA. In this framework, (Song et al., 2023) use BART and T5 as backbones, serializing the output sequence of the DTSA task into a natural language sentence.\\n\\n\u2022 Seq2Struct*: The Encoder-only model proposed by (Song et al., 2023), to reflect the improvement fairly brought by coreference resolution, we replace the backbone of this model with 1https://openai.com/chatgpt.\"}"}
{"id": "acl-2024-long-657", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Methods\\n\\n|              | Book  | Clothing | Restaurant | Hotel  | Average |\\n|--------------|-------|----------|------------|--------|---------|\\n| GPT-3.5-Turbo| 16.75 | 20.10    | 14.32      | 20.00  | 17.79   |\\n| Seq2Seq      | 34.76 | 49.40    | 19.08      | 34.17  | 34.35   |\\n| BART-Extraction| 33.83 | 55.42    | 33.05      | 58.90  | 45.30   |\\n| BART-Paraphrase| 32.90 | 55.18    | 33.21      | 59.71  | 45.25   |\\n| T5-Extraction | 32.66 | 52.49    | 32.85      | 57.92  | 43.98   |\\n| T5-Paraphrase | 32.64 | 53.47    | 33.36      | 57.95  | 44.36   |\\n| Seq2Struct\u2217  | 35.55 | 57.00    | 38.06      | 54.24  | 46.21   |\\n| Ours         | 37.20 | 58.64    | 38.29      | 54.46  | 47.15   |\\n\\nTable 3: Main results of the DTSA task for our approach and the baseline systems. The best results are in bold.\\n\\n#### 5.3 Main Results\\n\\nThe result for the DTSA task is reported in Table 3. There are three noteworthy observations:\\n\\n1. **Firstly**, the average result of the baseline system Seq2Struct\u2217 reaches 46.21% after fine-tuning, whereas the result of GPT-3.5-Turbo under a five-shot setting is 17.79%, which is significantly lower than our method. We speculate that the main reasons include two aspects: On one hand, DTSA is a relatively new task with limited related data available, and large models may not have undergone instruction tuning on this type of data. On the other hand, as (Zhang et al., 2023) has pointed out, large models still have significant shortcomings in extracting fine-grained and structured sentiment information.\\n\\n2. **Secondly**, the adapted Seq2Struct\u2217 model performs better than other generative baseline models. The average results across the four domains are 0.91 points higher than the best-performing BART-Extraction model. Additionally, in three out of the four domains, this model outperforms the best generative model by 2 points to 5 points, highlighting the effectiveness and adaptability of the Seq2Struct structure. However, the best-performing generative model in the Hotel domain outperforms the results of Seq2Struct\u2217 by 5.47 points. One possible reason is that the generative model has a relatively weaker ability to identify entity sentiments compared to Seq2Struct\u2217, and most entity sentiments in the Hotel domain are positive, which reduces the difficulty of entity sentiment identification and leads to better results than Seq2Struct\u2217.\\n\\n3. **Thirdly**, our model with coreference resolution has further improvements based on Seq2Struct\u2217. The average results across the four domains have improved by 0.94 points. Specifically, there are improvements of 1.65 points and 1.64 points in the Book and Clothing domains, respectively. The improvements in the Restaurant and Hotel domains are relatively smaller, at 0.23 points and 0.22 points, respectively. This may be because some entities in the right-path coreference entities are not mentioned in the left path. While introducing coreference information, these entities also affect the accuracy of left-path entity extraction to some extent, resulting in negative effects on the DTSA task. However, these negative effects are mitigated in the Book and Clothing domains by designing the two-path model structure and parameter sharing.\\n\\n#### 5.4 Evaluation on Test Sets with and without Coreference Annotation\\n\\nTo observe whether coreference information can improve the model's performance on the coreference test set, we divide the test set into two parts: a coreference annotated set and a non-coreference annotated set. We compare and evaluate the results of our model and Seq2Struct\u2217 on these two sets. As shown in Table 4, our model achieves an average improvement of 1.05 points and 0.83 points on the coreference test set and non-coreference test set, respectively.\\n\\n|              | Coref. | Non-Coref. |\\n|--------------|--------|------------|\\n| Books        | 30.19  | 40.71      |\\n| Clothing     | 51.22  |            |\\n| Restaurant   | 42.30  | 64.43      |\\n| Hotel        | 36.74  | 39.93      |\\n| Average      | 40.11  | 52.77      |\\n\\nTable 4: Results on the dataset with and without coreference annotation. The \u201cCoref.\u201d indicates labels with coreference annotations, while the \u201cNon-Coref.\u201d indicates labels without coreference annotations.\"}"}
{"id": "acl-2024-long-657", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Specifically, compared to Seq2Struct\u2217, our model experiences a decrease of 0.93 points on the coreference test set in the Hotel domain. One possible reason is the incorrect extraction of additional coreferential entities, which affects the model's overall performance on the coreference set. Our model also shows improvement on the non-coreference set, possibly because the examples with coreference annotations are more challenging, enhancing the model's ability to recognize coreference while improving its natural language understanding abilities.\\n\\n5.5 Results with Coreference Annotation Using ChatGPT\\n\\nTo alleviate the high labor and time costs of coreference annotation, we employ ChatGPT to annotate the coreferential entities. The average results of the model with ChatGPT-annotated coreference information in four domains are 47.01%, which is 0.8 points higher than that of Seq2Struct\u2217. Specifically, as shown in Figure 3, similar to the model with human-annotated coreference information, the model performs better than Seq2Struct\u2217 in the Book and Clothing domains, with improvements of 1 point and 1.9 points, respectively. However, in the Restaurant and Hotel domains, the performance is similar, with fluctuations of 0.1 points to 0.3 points. It can be observed that the introduction of entity coreference does not yield significant improvements in the Restaurant and Hotel domains, indicating the need to reduce the impact of other noise (such as additional entities) introduced during the coreference annotation process.\\n\\n5.6 Evaluation with the Coreference-Aware Metric\\n\\nThe opinion targets in the previous ABSA datasets were annotated on separate entities or aspect terms. Due to coreference issues, multiple entities may refer to the same opinion target. For example, in \\\"There are so many different places to choose from in Boston but Paddy Os is by far the best! The bar is spacious with good music,...\\\", the \\\"bar\\\" and \\\"Paddy Os\\\" refer to the same opinion target. Therefore, when the model correctly predicts either the \\\"bar\\\" or \\\"Paddy Os\\\", the previous entity-based evaluation metric would consider them as two different opinion targets, leading to inaccuracies in the assessment results. Therefore, based on the original exact matching metric, we consider the coreferential entities and propose the revised coreference cluster-based metric. For the DTSA task, the original Precision and Recall are calculated as follows:\\n\\n\\\\[\\nP = \\\\frac{\\\\#\\\\text{correct}}{\\\\#\\\\text{pred}}, \\\\quad R = \\\\frac{\\\\#\\\\text{correct}}{\\\\#\\\\text{gold}},\\n\\\\]\\n\\nwhere \\\\(\\\\#\\\\text{correct}_i\\\\) is the number of predicted \\\\((t_i, p_i)\\\\) pairs that are the same as the gold \\\\((\\\\hat{t}_i, \\\\hat{p}_i)\\\\). Since \\\\(t_i\\\\) should be an entity of its coreferential cluster, the new metric calculates the similarity between the predicted entity's coreferential cluster and the gold entity's coreferential cluster for \\\\(\\\\#\\\\text{correct}_i\\\\). Given \\\\(\\\\hat{t}_i = \\\\{\\\\hat{a}_1, \\\\ldots, \\\\hat{a}_k\\\\}\\\\), we first require the number of entities in \\\\(t_i\\\\) to be equal to \\\\(k\\\\), and calculate the matching score \\\\(\\\\text{score}(a_j)\\\\) between the coreferential cluster \\\\(\\\\text{cluster}(a_j)\\\\) of each layer \\\\(a_j\\\\) and the coreferential cluster \\\\(\\\\text{cluster}(\\\\hat{a}_j)\\\\) of \\\\(\\\\hat{a}_j\\\\). We then take \\\\(\\\\text{score}(t_i) = \\\\prod_{j=1}^{k} \\\\text{score}(a_j)\\\\times \\\\mathbb{I}(p_i = \\\\hat{p}_i)\\\\), and calculate the current predicted matching score based on \\\\(\\\\text{score}(t_i)\\\\):\\n\\n\\\\[\\n\\\\#\\\\text{correct} = |N| \\\\sum_{i=1}^{\\\\#\\\\text{pred}} (k \\\\prod_{j=1}^{k} \\\\text{score}(a_j)) \\\\times \\\\mathbb{I}(p_i = \\\\hat{p}_i),\\n\\\\]\\n\\nwhere \\\\(|N|\\\\) is the number of predicted \\\\((t_i, p_i)\\\\) pairs in the test set with matching scores greater than 0. \\\\(\\\\text{F}_1(\\\\text{cluster}(a_j), \\\\text{cluster}(\\\\hat{a}_j))\\\\) represents the F1 score calculated by treating the coreferential cluster of \\\\(\\\\hat{a}_j\\\\) as the gold label.\"}"}
{"id": "acl-2024-long-657", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|    | Seq2Struct\u2217@Old Metric | Seq2Struct\u2217@New Metric | Ours@Old Metric | Ours@New Metric |\\n|----|-----------------------|------------------------|----------------|-----------------|\\n| Book | 35.55                | 37.15                  | 37.20          | 38.92           |\\n| Clothing | 57.00                | 59.24                  | 58.64          | 60.19           |\\n| Restaurant | 38.06                | 39.74                  | 38.29          | 40.44           |\\n| Hotel  | 54.24                | 56.06                  | 54.46          | 56.23           |\\n| Average | 46.21                | 48.05                  | 47.15          | 48.94           |\\n\\nTable 5: Comparison results of cluster-based metric and entity-based metric on Seq2Struct\u2217 and our method.\\n\\nTo mitigate the bias caused by missing coref-erential entities during the evaluation process, we assess the impact of the revised metric. As shown in Table 5, the new metric can mitigate the effect of missing coreference targets on Seq2Struct\u2217 and our method. Specifically, the new metric results in an improvement ranging from 1.55 points to 2.15 points across different domains, with an average improvement of 1.79 points across all four domains on our method. The improvement brought by the new metric mainly stems from the inclusion of entities in coreference clusters that were not accounted for in the gold labels. These entities are ignored in entity-based metrics but can lead to evaluation errors when they appear in predicted entities. By considering cluster-level matching, this issue can be alleviated.\\n\\n6 Conclusion\\n\\nMost research on ABSA focuses on the sentence level. However, DTSA remains an underexplored area. A significant difference between DTSA and sentence-level ABSA is that DTSA requires richer coreference information, necessitating models to possess stronger contextual understanding capabilities. To explore the impact of coreference information on the DTSA task, we annotate coreferential entities with human and ChatGPT and design a multi-task learning framework to verify the positive role of coreference information in DTSA. Additionally, we revise the metrics from exact entity-level matching to a more lenient cluster-level matching to mitigate the bias caused by missing coreferential entities.\\n\\nLimitations\\n\\nThis paper aims to verify the effectiveness of coreference information on document-level target sentiment analysis, although ChatGPT-annotated coreference information is more efficient and labor-saving compared to manual annotation, it still suffers from the problem of being influenced by prompts. In addition, the revised coreference metrics require manual annotation of coreference information on the test set, which to some extent limits the use of the new evaluation metrics.\\n\\nAcknowledgments\\n\\nThe authors would like to thank the anonymous reviewers for their valuable comments. This work was supported by the Natural Science Foundation of China (No. 62076133 and 62006117).\\n\\nReferences\\n\\nJeremy Barnes, Laura Oberlaender, Enrica Troiano, Andrey Kutuzov, Jan Buchmann, Rodrigo Agerri, Lilja \u00d8vrelid, and Erik Velldal. 2022. Semeval 2022 task 10: structured sentiment analysis. In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pages 1280\u20131295.\\n\\nHongjie Cai, Rui Xia, and Jianfei Yu. 2021. Aspect-category-opinion-sentiment quadruple extraction with implicit aspects and opinions. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (ACL-IJCNLP), pages 340\u2013350.\\n\\nJiahua Chen, Shuai Wang, Sahisnu Mazumder, and Bing Liu. 2020a. A knowledge-driven approach to classifying object and attribute coreferences in opinion mining. arXiv preprint arXiv:2010.05357.\\n\\nXiao Chen, Changlong Sun, Jingjing Wang, Shoushan Li, Luo Si, Min Zhang, and Guodong Zhou. 2020b. Aspect sentiment classification with document-level sentiment preference modeling. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 3667\u20133677.\\n\\nXiaowen Ding and B. Liu. 2010. Resolving object and attribute coreference in opinion mining. In International Conference on Computational Linguistics.\\n\\nXiaowen Ding, Bing Liu, and Philip S Yu. 2008. A holistic lexicon-based approach to opinion mining. In Proceedings of the 2008 international conference on web search and data mining, pages 231\u2013240.\"}"}
{"id": "acl-2024-long-657", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Li Dong, Furu Wei, Chuanqi Tan, Duyu Tang, Ming Zhou, and Ke Xu. 2014. Adaptive recursive neural network for target-dependent twitter sentiment classification. In Proceedings of the 52nd annual meeting of the association for computational linguistics (volume 2: Short papers), pages 49\u201354.\\n\\nZhibin Gou, Qingyan Guo, and Yujiu Yang. 2023. MVP: Multi-view prompting improves aspect sentiment tuple prediction. arXiv preprint arXiv:2305.12627.\\n\\nRuidan He, Wee Sun Lee, Hwee Tou Ng, and Daniel Dahlmeier. 2019. An interactive multi-task learning network for end-to-end aspect-based sentiment analysis. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 504\u2013515.\\n\\nMinghao Hu, Yuxing Peng, Zhen Huang, Dongsheng Li, and Yiwei Lv. 2019. Open-domain targeted sentiment analysis via span-based extraction and classification. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics (ACL), pages 537\u2013546.\\n\\nMinqing Hu and Bing Liu. 2004. Mining and summarizing customer reviews. In Proceedings of the tenth ACM SIGKDD international conference on Knowledge discovery and data mining, pages 168\u2013177.\\n\\nLong Jiang, Mo Yu, Ming Zhou, Xiaohua Liu, and Tiejun Zhao. 2011. Target-dependent twitter sentiment classification. In Proceedings of the 49th annual Meeting of the association for computational linguistics (ACL), pages 151\u2013160.\\n\\nQingnan Jiang, Lei Chen, Ruifeng Xu, Xiang Ao, and Min Yang. 2019. A challenge dataset and effective models for aspect-based sentiment analysis. In Proceedings of the 2019 conference on empirical methods in natural language processing and the 9th international joint conference on natural language processing (EMNLP-IJCNLP), pages 6280\u20136285.\\n\\nJan-Christoph Klie, Michael Bugert, Beto Boullosa, Richard Eckart de Castilho, and Iryna Gurevych. 2018. The inception platform: Machine-assisted and knowledge-oriented interactive annotation. In Proceedings of the 27th international conference on computational linguistics: system demonstrations, pages 5\u20139.\\n\\nKenton Lee, Luheng He, Mike Lewis, and Luke Zettlemoyer. 2017. End-to-end neural coreference resolution. arXiv preprint arXiv:1707.07045.\\n\\nXin Li, Lidong Bing, Wai Lam, and Bei Shi. 2018. Transformation networks for target-oriented sentiment classification. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 946\u2013956.\\n\\nXin Li, Lidong Bing, Piji Li, and Wai Lam. 2019. A unified model for opinion target extraction and target sentiment prediction. In Proceedings of the 33rd AAAI Conference on Artificial Intelligence (AAAI), pages 6714\u20136721.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nHuaishao Luo, Tianrui Li, Bing Liu, and Junbo Zhang. 2019. Doer: Dual cross-shared rnn for aspect term-polarity co-extraction. arXiv preprint arXiv:1906.01794.\\n\\nYun Luo, Hongjie Cai, Linyi Yang, Yanxia Qin, Rui Xia, and Yue Zhang. 2022. Challenges for open-domain targeted sentiment analysis. arXiv preprint arXiv:2204.06893.\\n\\nDehong Ma, Sujian Li, Xiaodong Zhang, and Houfeng Wang. 2017. Interactive attention networks for aspect-level sentiment classification. arXiv preprint arXiv:1709.00893.\\n\\nDhruv Mullick, Bilal Ghanem, and Alona Fyshe. 2023. Better handling coreference resolution in aspect level sentiment classification by fine-tuning language models. arXiv preprint arXiv:2307.05646.\\n\\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems (NeurIPS), pages 27730\u201327744.\\n\\nMaria Pontiki, Dimitrios Galanis, Haris Papageorgiou, Ion Androutsopoulos, Suresh Manandhar, Mohammad Al-Smadi, Mahmoud Al-Ayyoub, Yanyan Zhao, Bing Qin, Orph\u00e9e De Clercq, et al. 2016. SemEval-2016 task 5: Aspect based sentiment analysis. In International workshop on semantic evaluation, pages 19\u201330.\\n\\nMaria Pontiki, Dimitrios Galanis, Harris Papageorgiou, Suresh Manandhar, and Ion Androutsopoulos. 2015. SemEval-2015 task 12: Aspect based sentiment analysis. In Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015), pages 486\u2013495.\\n\\nMaria Pontiki, Dimitris Galanis, John Pavlopoulos, Haris Papageorgiou, Ion Androutsopoulos, and Suresh Manandhar. 2014. SemEval-2014 task 4: Aspect based sentiment analysis. In Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 27\u201335, Dublin, Ireland. Association for Computational Linguistics.\\n\\nEgil R\u00f8nningstad, Erik Velldal, and Lilja \u00d8vrelid. 2023. Entity-level sentiment analysis (elsa): An exploratory task survey. arXiv preprint arXiv:2304.14241.\\n\\nNan Song, Hongjie Cai, Rui Xia, Jianfei Yu, Zhen Wu, and Xinyu Dai. 2023. A sequence-to-structure approach to document-level targeted sentiment analysis. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 7687\u20137698.\"}"}
{"id": "acl-2024-long-657", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hao Tang, Donghong Ji, Chenliang Li, and Qiji Zhou. 2020. Dependency graph enhanced dual-transformer structure for aspect-based sentiment classification. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics (ACL), pages 6578\u20136588.\\n\\nYucheng Wang, Bowen Yu, Yueyang Zhang, Tingwen Liu, Hongsong Zhu, and Limin Sun. 2020. Tplinker: Single-stage joint extraction of entities and relations through token pair linking. In Proceedings of the 28th International Conference on Computational Linguistics, pages 1572\u20131582.\\n\\nZhen Wu, Chengcan Ying, Fei Zhao, Zhifang Fan, Xinyu Dai, and Rui Xia. 2020. Grid tagging scheme for end-to-end fine-grained opinion extraction. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings, pages 2576\u20132585.\\n\\nHu Xu, Bing Liu, Lei Shu, and S Yu Philip. 2019. Bert post-training for review reading comprehension and aspect-based sentiment analysis. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL), pages 2324\u20132335.\\n\\nHang Yan, Yu Sun, Xiaonan Li, and Xipeng Qiu. 2022. An embarrassingly easy but strong baseline for nested named entity recognition. arXiv preprint arXiv:2208.04534.\\n\\nWenxuan Zhang, Yang Deng, Xin Li, Yifei Yuan, Lidong Bing, and Wai Lam. 2021a. Aspect sentiment quad prediction as paraphrase generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9209\u20139219.\\n\\nWenxuan Zhang, Yue Deng, Bing Liu, Sinno Jialin Pan, and Lidong Bing. 2023. Sentiment analysis in the era of large language models: A reality check. arXiv preprint arXiv:2305.15005.\\n\\nWenxuan Zhang, Xin Li, Yang Deng, Lidong Bing, and Wai Lam. 2021b. Towards generative aspect-based sentiment analysis. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 504\u2013510.\"}"}
{"id": "acl-2024-long-657", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prompt for coreference cluster annotation\\nYou are an expert in Coreference Resolution of Information Extraction. Given the document, the opinion targets, and their sentiment polarities, you MUST extract ONLY the coreference clusters of the given opinion targets in JSON format. The items in the coreference clusters MUST be the coreferential items of the opinion target. Let's think step by step.\\n\\n<Document>\\nMark and Roman were amazing hosts. Check-in and check-out procedures were simple. The Studio is very close to Hardvard ave station (green line). I recommend this place.\\n\\n<Opinion targets and their sentiment polarities>\\n{ \\\"place\\\": \\\"Positive\\\", \\\"Check-in\\\": \\\"Positive\\\", \\\"Roman\\\": \\\"Positive\\\", \\\"Mark\\\": \\\"Positive\\\", \\\"Check-out\\\": \\\"Positive\\\" }\\n\\n<Coreference clusters of the opinion targets>\\n{ \\\"place\\\": [\\\"place\\\", \\\"Studio\\\"], \\\"Check-in\\\": [\\\"Check-in\\\"], \\\"Roman\\\": [\\\"Roman\\\"], \\\"Mark\\\": [\\\"Mark\\\"], \\\"check-out\\\": [\\\"check-out\\\"] }\\n\\nPrompt for target-sentiment pair annotation\\nYou are an expert in Aspect-Based Sentiment Analysis. Given the document, you should ONLY extract the list of target-sentiment pairs. A target-sentiment pair is defined as 'target 1\u2013...\u2013target n##sentiment', where 'target 1\u2013...\u2013target n' is a multi-level opinion target with n denoting the number of its levels, and 'sentiment' in {Positive, Negative, Mixed}. Let's think step by step.\\n\\n<Document>\\nLovely shoes, and the customer support was wonderful.\\n\\n<Target-sentiment pairs>\\n[shoes##Positive, shoes\u2013customer support##Positive]\\n\\nTable 6: Example prompts for coreference cluster and target-sentiment pairs annotation using ChatGPT.\\n\\nA Entity Coreference Annotation\\nCoreference Annotation by Humans\\nWe utilize the Inception platform (Klie et al., 2018) to annotate coreferential entities. Specifically, for each entity, we first annotate the entities that are coreferential with it and connect these two entities with an undirected edge to represent the coreference relation. In the post-processing stage, we cluster the connected entities into coreferential clusters. These clusters are used for subsequent multi-task training and cluster-based metrics.\\n\\nCoreference Annotation by ChatGPT\\nTo reduce the manual labor and time required for coreference annotation, we employ ChatGPT 2 to annotate the coreferential clusters. First, we construct demonstration examples through five-shot prompting for each domain to extract coreferential entities in JSON format. We tried different task descriptions and output formats to design various prompts. After manually observing and comparing those annotation results from ChatGPT, the final prompt format we selected is shown in Table 6. Each example consists of <Document> and <Opinion targets and their sentiment polarities>. ChatGPT must output <Coreference clusters of the opinion targets> based on the given instruction and the five-shot annotated examples.\\n\\nTo eliminate the noise generated by ChatGPT during the entity extraction process, we remove the articles and adjective possessive pronouns of the entities and merge them as the final version of coreferential clusters. Additionally, we conduct a manual check of 20% of ChatGPT's annotated data (100 documents in each of the five domains), and the assessment yields a coreference annotation accuracy of 92%.\\n\\n12160\"}"}
