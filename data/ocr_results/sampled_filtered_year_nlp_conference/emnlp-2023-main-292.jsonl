{"id": "emnlp-2023-main-292", "page_num": 17, "content": "{\"primary_language\":\"ko\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Your task is to classify any spelling or grammar errors within a sentence. Always answer in Korean.\\nThe definition and examples are as follows:\\n\\n- \\n\\nExample:\\n\\n\uc131\uacf5\uc758 \uae38\uc744 \uc5f4\uc5b4\uc918\uc694.\\n\\nResult:\\n\\n\ub744\uc5b4\uc4f0\uae30 \uc624\ub958\\n\\nExamples of classifying multiple grammatical error types are as follows:\\n\\n{{examples}}\\n\\nExamples\\n\\nReferring to the definition and example, classify grammatical error type that fit the given sentences.\\n\\nExample:\\n\\n{{input sentenc}}\\n\\nResult:\\n\\nInput\\n\\nTask Description\\n\\nFigure 14: Error type classification prompt.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Input Sentence STT Result | Predict Types | Target Types |\\n|---------------------------|---------------|--------------|\\n| \uc6b0\ub9ac\uc790\ub9ac\uac00\uc0dd\uae38\ub54c\uae4c\uc9c0\uae30\ub2e4\ub9b4\uae4c\uc694 | Shall we wait until a seat becomes available for us | \uc6b0\ub9ac\uc790\ub9ac\uac00\uc0dd\uae38\ub54c\uae4c\uc9c0\uae30\ub2e4\ub9b4\uae4c\uc694 | \uc6b0\ub9ac\uc790\ub9ac\uac00\uc0dd\uae38\ub54c\uae4c\uc9c0\uae30\ub2e4\ub9b4\uae4c\uc694 |\\n| \uc5b4\ub5bb\uac8c\ud574\uc57c\ub9cc\uc54c\ub824\uc904\uac8c\uc219\uc81c\ub294\uc2a4\uc2a4\ub85c\ud574\uc57c\uc9c0 | I will only instruct you on how to do it | \uc5b4\ub807\uac8c\ud558\uba74\uc54c\ub824\uc904\uac8c\uc219\uc81c\uc2a4\uc2a4\ub85c\ud574\uc57c\uc9c0 | \uc5b4\ub807\uac8c\ud558\uba74\uc54c\ub824\uc904\uac8c\uc219\uc81c\uc2a4\uc2a4\ub85c\ud574\uc57c\uc9c0 |\\n| \ud559\uc0dd\uc740\uad50\ubcf5\uc744\uc785\uc744\ub54c\ub2e8\uc815\uc774\ubb50\uc57c | When students wear school uniforms, what does 'neatness' mean | \ud559\uc0dd\uc740\uad50\ubcf5\uc744\uc785\uc744\ub54c\ub2e8\uc815\ud574\ubcf4\uc5ec | \ud559\uc0dd\uc740\uad50\ubcf5\uc744\uc785\uc744\ub54c\ub2e8\uc815\ud574\ubcf4\uc5ec |\\n| \ub0b4\uac00\uacbd\ucc30\uc774\uba74\ubb50\ubb3c\uc5b4\ubcf4\ub824\uace0\ud588\uc5b4\uc694 | What would I have asked if I were a police officer | \ub0b4\uac00\uacbd\ucc30\uc774\uba74\ubb50\ubb3c\uc5b4\ubcf4\ub824\uace0\ud588\uc5b4\uc694 | \ub0b4\uac00\uacbd\ucc30\uc774\uba74\ubb50\ubb3c\uc5b4\ubcf4\ub824\uace0\ud588\uc5b4\uc694 |\\n| \uc6b0\ub9ac\ud68c\uc0ac\uc601\uc591\uc81c\uc2e0\uc81c\ud488\uc744\uc218\uc785\ud558\uace0\uc2f6\uc73c\uc2dc\ub2e4\uace0\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4 | I heard you want to import our company's new nutritional supplement products | \uc6b0\ub9ac\ud68c\uc0ac\uc601\uc591\uc81c\uc2e0\uc81c\ud488\uc744\uc218\uc785\ud558\uace0\uc2f6\uc73c\uc2dc\ub2e4\uace0\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4 | \uc6b0\ub9ac\ud68c\uc0ac\uc601\uc591\uc81c\uc2e0\uc81c\ud488\uc744\uc218\uc785\ud558\uace0\uc2f6\uc73c\uc2dc\ub2e4\uace0\ub4e4\uc5c8\uc2b5\ub2c8\ub2e4 |\"}"}
{"id": "emnlp-2023-main-292", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAutomatic Speech Recognition (ASR) systems are instrumental across various applications, with their performance being critically tied to user satisfaction. Conventional evaluation metrics for ASR systems produce a singular aggregate score, which is insufficient for understanding specific system vulnerabilities. Therefore, we aim to address the limitations of the previous ASR evaluation methods by introducing the Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP). KEBAP enables comprehensive analysis of ASR systems at both speech- and text levels, thereby facilitating a more balanced assessment encompassing speech recognition accuracy and user readability. KEBAP provides 37 newly defined speech-level resources incorporating diverse noise environments and speaker characteristics categories, also presenting 13 distinct text-level error types. This paper demonstrates detailed statistical analyses of colloquial noise categories and textual error types. Furthermore, we conduct extensive validation and analysis on commercially deployed ASR systems, providing valuable insights into their performance. As a more fine-grained and real-world-centric evaluation method, KEBAP contributes to identifying and mitigating potential weaknesses in ASR systems.\\n\\n1 Introduction\\n\\nAutomatic speech recognition (ASR) is a task that recognizes speech and converts it into text, and it is getting more and more attention with the development of voice interface applications and devices such as Alexa, Siri, and Cortana (Williams and Young, 2007; Wang et al., 2018, 2020). In the real world, the ASR result has a trade-off between recognition accuracy and user readability. Even if the ASR model accurately recognizes the input voice, the user's readability may decrease. This is because humans do not always utter perfect sentences in the real world (e.g., incomplete utterances, sighs, etc.). To achieve balanced ASR results in this trade-off situation, it is required to consider both recognition accuracy and user readability.\\n\\nIn terms of recognition accuracy, various ASR evaluation metrics such as word error rate (WER) (Woodard and Nelson) and character error rate (CER) (Morris et al., 2004) have been prevalent. Also, readability is considered in ASR post-processing (ASRP) tasks, where the goal is to improve the clarity and comprehension of speech recognition outputs without modifying the underlying model architecture (Mani et al., 2020b; Liao et al., 2020; Leng et al., 2021). The ASRP task relies on quantitative metrics, such as BLEU (Papineni et al., 2002) and GLEU (Napoles et al., 2015), similar to the ASR evaluation using WER and CER.\\n\\nHowever, it is crucial to recognize that even if the ASR system is trained to minimize the word error rate, it cannot increase the user's readability just by improving word recognition accuracy. In particular, in the case of ASR systems for Korean language, the number of words per sentence is longer than that of English language, which makes existing evaluation metrics such as WER less meaningful. Therefore, ASR systems need to be evaluated not only in terms of word or character error rates but also in terms of user readability.\\n\\nKEBAP: Korean Error Explainable Benchmark Dataset for ASR and Post-processing\\nSeonmin Koo1\u2217, Chanjun Park2\u2217, Jinsung Kim1, Jaehyung Seo1, Sugyeong Eo1, Hyeonseok Moon1, Heuiseok Lim1\u2020\\n\\n1 Korea University, Department of Computer Science and Engineering\\n2 Upstage AI\\n\\n{fhdahd, jin62304, seojae777, djtnrud, glee889, limhseok}@korea.ac.kr\\nchanjun.park@upstage.ai\"}"}
{"id": "emnlp-2023-main-292", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"quantitative evaluation scores are similar, the qualitative aspects of the ASR results may not necessarily align. Conventional research methods, which focus on accuracy or user readability, compute quantitative scores based on the degree of alignment between inputs and outputs. This approach falls short in classifying potential error types or pinpointing the model's specific weaknesses, thus lacking explanatory power for real-world ASR model outputs. This deficiency hinders the establishment of clear directions for model improvement. To this end, datasets that aim to enhance the explanatory power of ASR evaluations by considering noisy environments or speaker characteristics have been published recently (Sikasote and Anastasopoulos, 2022; Lakomkin et al., 2019; Gong et al., 2022; Dai et al., 2022). However, these datasets still focus on accuracy and provide a limited set of error types, thereby leading to insufficiency in diagnosing specific weaknesses within ASR models.\\n\\nTherefore, we introduce the novel Korean Error Explainable Benchmark Dataset for ASR and Post-processing (KEBAP). It encompasses speech-level distraction-based resources and text-level error types relevant to real-world ASR applications. These diverse error types of KEBAP can lead to improved explanatory capability compared to the previous examination methods, as illustrated in Figure 1. In particular, speech-level noise types are bifurcated into two categories: noisy environments and speaker characteristics, comprising 37 distinct types. Additionally, KEBAP includes 13 types of textual errors pertinent in ASR contexts. The dataset stands out in its authenticity since all speech samples are recorded by human speakers, and background noises are derived from real-world environments. Also, we annotate the difficulty levels to all types, enhancing the interpretability of the ASR model.\\n\\nWe employ KEBAP to conduct an empirical analysis of the correlations between speech-level noise types and textual error types. Moreover, leveraging ChatGPT (OpenAI-Blog, 2022), we explore the potential of language models in discovering the vulnerabilities of ASR models. Our observations highlight KEBAP's significant interpretability of ASR model diagnostics and shed light on the pressing need for research on diagnostic tasks for ASR systems. Our work sets the stage for more real-world-oriented evaluations of ASR systems and can contribute to the advancements in this domain.\\n\\n2.1 Why KEBAP?\\nIn the real-world scenario, mitigating the trade-off between recognition accuracy and user readability is crucial. To address this, we propose KEBAP, emphasizing the importance of considering both aspects. A detailed explanation is as follows. Firstly, in real-world speech recognition, it is essential to consider the accuracy of model and end-user satisfaction simultaneously. To facilitate this, we propose to map the accuracy of the ASR model to 'speech-level noises' and user readability to 'text-level errors' to mitigate this inherent trade-off. From the perspective of the accuracy of the ASR model, it should output the recognition results 'as heard,' regardless of the quality of the user-provided input. Conversely, from the standpoint of the end-user receiving the result, satisfaction increases when the output is presented in a refined state, despite any errors in the initial input. For instance, if a speaker stammers during their speech, the ASR model would likely deem its output more accurate if it recognizes and outputs all the words uttered. However, this would likely result in lower readability from the user's perspective. In addition, previous research lacks an adequate number of error types for a detailed diagnosis. Since benchmarks measure performance with quantitative metrics, it is crucial to subdivide characteristics for a more detailed diagnosis. In industry contexts, communication between model and service teams is critical. When there's an issue with the model, clear criteria for the data flywheel significantly facilitate communication. That is, distinguishing the error type criteria for speech- and text-level aids in detailed diagnosis for model improvement. However, conventional benchmark datasets lack sufficient error types for detailed model analysis, leading to extensive usage of human evaluation in real-world settings. Humans can cope using commonsense, even if the criteria are unclear, but existing benchmarks with limited error types fall short. Hence, to solve the explainability issue, we must define error type criteria that consider both the speech- and text-level and create benchmarks to achieve human-level explainability.\\n\\nTo enhance the explanatory power of the validation process for ASR models, we define errors...\"}"}
{"id": "emnlp-2023-main-292", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Proposed novel speech-level noise type classification criteria for KEBAP\\n\\n#### 2.2 Speech-Level Noise Type\\n\\nError types at the speech-level refer to factors that trigger inaccuracies in speech recognition situations. For example, identical utterances may be challenging to recognize due to background noise (Sikasote and Anastasopoulos, 2022). Additionally, even in quiet environments, individuals do not consistently articulate perfect sentences and each speaker has unique characteristics that may negatively influence speech recognition (Gong et al., 2022).\\n\\nTable 1 illustrates the speech-level error type classification criteria considering these characteristics. The speech-level error types allow the classification of two main categories (noisy environment and characteristics of interlocutor) and more detailed error types, with 24 sub-types for noise error and 13 for speaker characteristics.\\n\\nConsidering environments inundated with noise, it does not represent a quiet recording situation but rather a condition intertwined with noise. Real-world scenarios frequently involve inputs replete with ambient noise (Sikasote and Anastasopoulos, 2022). Reflecting on these practical situations where voice interface applications and devices are deployed, we propose an enhanced categorization scheme that closely follows the classification in the AI-HUB's noisy environment speech recognition dataset which are representative Korean data platform. We divide the noisy environment errors into 11 nuanced subcategories, including home appliances, where recognition is impaired due to surrounding appliance noise; individual transportation, which includes instances with ambient transportation noise; street, covering situations with disruptive street noise; cafe/restaurant, addressing cases with the cafe or restaurant ambient noise; etc., where external noise is present, although not falling into the aforementioned categories.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category       | Description                                                                 | Level A | Level B | Level C |\\n|----------------|------------------------------------------------------------------------------|---------|---------|---------|\\n| Spacing        | Violating the spacing rules.                                                 |         |         |         |\\n| Punctuation    | Punctuation marks are not attached in Korean sentences or are attached in the wrong. |         |         |         |\\n| Numerical      | Cardinal number indicating quantity and the ordinal number indicating the order are in error |         |         |         |\\n| Remove         | Some words are not recognized, or endings or suffixes are omitted.          |         |         |         |\\n| Addition       | Same word is repeated, or an unused postposition or ending is added.        |         |         |         |\\n| Replace        | Word is replaced by another word.                                            |         |         |         |\\n| Spelling       | Separation                                                                  |         |         |         |\\n| Foreign word conversion | Instances of incorrect conversion of syllables between English and Korean, as well as writing spellings according to pronunciation, have been observed. |         |         |         |\\n| Grammatic      | Spelling G2P                                                                 |         |         |         |\\n| Writing spellings according to pronunciation. | G2P and CVC indicate Grapheme-to-phoneme and Consonant vowel conversion, respectively |         |         |         |\\n| Post-position  | Instances of inconsistent or missing post-position usage in target utterances. |         |         |         |\\n| Syntax         | Cases of grammatically accurate yet interpretatively ambiguous meanings.     |         |         |         |\\n| Neologism      | Instances of the discrepancy between target and its similarity in meaning, pronunciation, and absence in Korean lexicon. |         |         |         |\\n\\nTable 2: Proposed text-level error type classification criteria for KEBAP. G2P and CVC indicate Grapheme-to-phoneme and Consonant vowel conversion, respectively.\\n\\nmarket/shopping mall, indicating instances with market or shopping mall noise; public transport, comprising cases with subway or bus noise; terminal, reflecting instances with terminal noise; construction site, for cases hindered by construction site noise; factory, indicating instances with factory noise; nature ambient, for cases disturbed by natural sounds. Lastly, we include an etc. category for instances where recognition is affected by external noise types not encompassed in the previous categories.\\n\\nConsidering speaker characteristics, recognition can be hampered due to the individual traits of the recorder. Inspired by studies on idiolectal elements in the field of psycholinguistics (Ha and Sim, 2008; Shin et al., 2005), we propose a nuanced categorization comprising 13 detailed subcategories. The details description of the subcategories are described in Appendix B.\\n\\n2.3 Text-Level Error Type\\n\\nText-level error types refer to issues that emerge in speech recognition results and must be addressed by post-processing. Since the output of the speech recognizer serves as the input for downstream tasks, it is one of the most significant factors influencing end-user satisfaction. By improving the performance of downstream tasks through quality input and diagnosing the performance of post-processing models through detailed error types, it is possible to enhance end-user satisfaction.\\n\\nExisting datasets that detail error types, such as grammatical error correction (GEC) datasets, do not consider speech recognition situations (Koo et al., 2022; Yoon et al., 2022). Therefore, we reconfigure the Korean GEC dataset, K-NCT, to suit speech recognition situations. The existing K-NCT dataset includes errors that only occur at the text-level and not in speech situations (Koo et al., 2022). Hence, errors that do not have vocal characteristics are removed.\\n\\nTable 2 illustrates the text-level error type classification criteria considering speech recognition situations, including 13 text-level errors that can occur in speech recognition situations. Detailed explanations for each text-level error type can be found in Appendix C.\\n\\n2.4 KEBAP Construction Process\\n\\nIn this work, we propose a comprehensive data construction guideline for the ASR and ASRP dataset, grounded in the application of a GEC dataset. Our methodology encompasses build text-level error corpus, speech recording, noise synthesis, and difficulty annotation. For the efficiency of the task, we choose the 'consensus labeling' method (Tang and Lease, 2011), in which a human overseer, who possesses an elevated degree of task completion, serves as a quality controller. During the progression of the task, any outcomes that do not conform to the established guidelines are promptly dismissed and subsequently reconstructed.\\n\\nStep 1: Build Text-Level Error Corpus\\n\\nIn this study, we employ a human-curated GEC dataset, which encompasses various text-level error types (Koo et al., 2022). Considering the inapplicability of the standard GEC benchmark dataset in a speech recognition setting, we selectively compose a text-level error types dataset by human evaluation.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We assess whether the given error types are valid or invalid in the context of speech recognition situations by human evaluators. Invalid types are filtered out, and the type structure is reconfigured. In particular, we extract 13 categories that resonate with speech recognition scenarios (e.g., honorific colloquial expression) and reorganize their hierarchy for ease of labeling. Consequently, our refined dataset includes data reflecting 13 error types relevant to speech recognition contexts.\\n\\nSubsequently, we authenticate the quality of the filtered dataset focusing on the alignment between labels and text, and the inclusion of text-level errors with a specific consideration of the speech recognition context. Validation processes proceed with a human supervisor, priorily trained with each error type. Evaluators are presented with an erroneous sentence, its correct counterpart, and a specified error type with the corresponding error span indicated. They are then tasked with assessing whether the sentence contains the presented error types. Sentences deemed to be incorrect are appropriately amended. This procedural framework ensures the generation of a high-quality dataset.\\n\\nStep 2: Speech Recording\\nIn the second phase, we request that recording participants incorporate characteristics of interlocutor errors into their recordings by presenting them with speech-level errors and transcription relevant to the respective error types. At most 3 error types are presented, which could include an instance of 'no error type', indicating clean data. The placement of the error within the sentence is non-specific, with the assurance that it includes only the errors specified. The recording environment should be ensured to be quiet without background noise. Each recorder is instructed to speak as naturally as possible, emulating their speech patterns when interacting with a voice interface application in real-world scenarios. After completing the recording, participants have the opportunity to listen to their own voice, and if they determine that the speech does not meet the criteria, they can re-record it. Participants are required to go through the process of listening to their recorded speech in order to complete the recording task. The detailed information about the workers can be found in Appendix D.\\n\\nStep 3: Synthesis of Background Noise\\nIn the next stage, we incorporate background noise into the recording to reflect the noise environment error in the proposed speech-level. The background noise used for this integration is derived directly from recordings of the identified environments. We ensure that the collected noise spans a duration longer than that of the recording file, fostering noise diversity. To mimic real-world situations, we conduct both single and multiple noise syntheses while filtering out instances that are unlikely to co-occur. During noise synthesis, the noise is integrated as though it is ambient background noise, designed to be audible at the onset of the voice file. Noise is composited into the recording by randomly excising sections, thus ensuring variation within sounds, even when they are categorized under the same noise type.\\n\\nStep 4: Difficulty Annotation\\nDifficult data for ASR models refers to data that is not frequently encountered in the training data and is imbalanced, varying depending on the user (Aleksic et al., 2015a). Therefore, we annotate the difficulty of the data to enable a detailed assessment of the model's coverage ability. To this end, we employ a framework that distinguishes between utterances considered easier for ASR and those deemed harder or more noisy for ASR (Breiner et al., 2022). We extend this framework to include the tagging of difficulty using a Likert scale by human annotators. Humans listen to audio file and select score based on evaluation criteria. We ask humans, 'How difficult is it to recognize the presented speech accurately as the same as the transcript?' Scores range from 1 (very easy) to 5 (very difficult). Three evaluators assess each audio file, and the average score is selected as the difficulty level of the data. This allows for a detailed analysis of the model's performance.\\n\\nThe details of construction process described in Appendix D.\\n\\n3 KEBAP Analysis\\n3.1 Text-level Distribution\\nWe filter out cases that cannot occur in speech recognition situations, such as typing language errors caused by keyboard language switching, in the GEC dataset. After the filtering process, the text-level distribution is shown in Table 3. It includes 2,478 instances of errors and correct sentences, including text-level errors. The statistical information for the text is provided in Table 4.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category                  | Level A | Level B | Level C |\\n|---------------------------|---------|---------|---------|\\n| Spacing                   | -       | -       | 514 (14.62) |\\n| Punctuation               | -       | -       | 505 (14.37) |\\n| Numerical                 | -       | -       | 500 (14.22) |\\n| Remove                    | 122 (3.47) | -       | -       |\\n| Addition                  | 104 (2.96) | -       | -       |\\n| Replace                   | 483 (13.74) | -       | -       |\\n| Spelling Separation       | 94 (2.67) | -       | -       |\\n| and Foreign word conversion | 193 (5.49) | -       | -       |\\n| Grammatic Spelling G2P    | -       | -       | 195 (5.55) |\\n| CVC                       | 534 (15.19) | -       | -       |\\n| Post-position             | 90 (2.56) | -       | -       |\\n| Syntax                    | 71 (2.02) | -       | -       |\\n| Neologism                 | 110 (3.13) | -       | -       |\\n| Total                     | 3515 (100) | -       | -       |\\n\\nTable 3: Statistics of labels in text level category of KEBAP. Here, the lowest level of data granularity is the category attribute in Level C. G2P and CVC are indicated as Grapheme-to-phoneme and consonant vowel conversion, respectively.\\n\\n| Test | Error sentence | Correct sentence |\\n|------|----------------|------------------|\\n| # of sents | 2,478 | 2,478 |\\n| # of tokens | 107,411 | 107,209 |\\n| # of words | 25,772 | 26,250 |\\n| avg of SL | \u25b3 43.35 | \u25b3 43.26 |\\n| avg of WS | 10.40 | 10.59 |\\n| avg of SS | 9.40 | 9.59 |\\n\\nTable 4: Statistics of our KEBAP dataset. # of sents/tokens/words: number of sentences/tokens/words; \u25b3 avg of SL/WS/SS: average of sentence length/words/spaces per sentence.\\n\\n3.2 Speech-level Distribution\\nKEBAP consists of a total of 2,478 speech files, transcriptions, and speech-level noise types. Figure 2-(a) illustrates the data distribution for speech-level. KEBAP is composed of a total of 24,021.82 seconds of speech. It includes an average speech duration of 9.69 seconds, with the shortest file being 3.8 seconds and the longest file being 27.68 seconds. Although transcription sentences are composed of single sentences, their lengths can vary depending on speaker characteristics, such as pauses (silent) or mutters, even for sentences of the same length. This allows for the inclusion of speech files of varying durations, covering the characteristics of diverse users who use ASR systems.\\n\\n3.3 Difficulty Distribution\\nOverall, the average Krippendorff\u2019s \u03b1 for inter-annotator agreement of each annotation level is 0.476. The label distribution of the collected data is shown in Figure 2-(b). To more accurately diagnose the model\u2019s capability, we enhance its interpretability by tagging the difficulty level of the data. Since the perceived difficulty of the same data may vary among individuals, we determine the difficulty of each data based on the average difficulty annotation provided by three evaluators. The difficulty ratings for the data are generally concentrated between 1 and 2, but there is also a significant presence of ratings at 5. This indicates that the dataset includes a range of difficulty levels, which we believe will be beneficial for assessing the performance of ASR models.\\n\\n3.4 Category Distribution\\nEach data includes single or multiple speech-level characteristics. Figure 3 shows the distribution of each category of speech-level. The speech-level errors can be broadly classified into two main categories: noisy environment and characteristics of the interlocutor. Our dataset encompasses various combinations of characteristics within each category and also includes cross-category combinations, providing a diverse range of error types. Noisy environment and characteristics of the interlocutor represent mutually exclusive types, while co-occurrence indicates cases where two characteristics occur simultaneously. When considering only noisy environments, 1 and 2 characteristics account for 40.02% each, and 3 characteristics account for 19.96%.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Noisy environments that cannot occur simultaneously are not included. When considering only the characteristics of the interlocutor, 1 and 2 characteristics account for 39.98% each, and 3 characteristics account for 20.04%. Co-occurrence occurs in 49.88% of cases for 2 characteristics and 50.12% of cases for 3 characteristics. This demonstrates the presence of a diverse range of error levels, both in terms of types and quantities. Actual workers recorded the data, and the background noise was collected directly from real-world environments, ensuring high quality. There is no synthetic audio involved in the recordings.\\n\\nTable 5: Evaluation results of ASR commercialization systems and publicly available model (Radford et al., 2023). Word Error Rate (WER) and Character Error Rate (CER) indicate better performance as their values decrease.\\n\\n4 Efficacy Validation for KEBAP\\n\\nIn this section, we assess the specific capabilities of commercialized ASR models using KEBAP. To achieve this, we conduct a detailed correlation analysis of commercialized systems such as Google Cloud Speech-to-Text (Aleksic et al., 2015b) and Clova Speech (Chung, 2019). We examine the correlation between speech-level noise types and text-level errors, aiming for a granular understanding. We comprehensively validate the model\u2019s capabilities by considering both speech- and text-level aspects. We verify whether the LLM possesses the necessary qualities as a diagnostic model through the error type classification task.\\n\\n4.1 Analysis of Correlation in ASR models\\n\\nTable 5 shows the evaluation results of ASR models. Based on conventional evaluation metrics such as WER (Woodard and Nelson) and CER (Morris et al., 2004), we observe similar performance between the two ASR models. However, even though the quantitative evaluation results may be similar, it does not necessarily mean that the qualitative aspects of the ASR model\u2019s outputs are also similar. This makes it challenging to identify the specific weaknesses of the model, hindering the establishment of directions for model improvement. To enhance interpretability, we analyze the tendency of text-level error propagation at the speech level for ASR model. To clearly understand the impact of each speech-level category on the text-level, we sample data that includes a single speech-level noise type. The results of the ASR model are labeled by humans trained in explanations and examples of text-level errors.\\n\\nFigure 4: Correlation distribution between speech-level: noisy environment and text-level in Google (a) and Clova (b). Spa/Punc/Num represent spacing, punctuation, and numerical, respectively. Rem/Add/Rep/Sep indicate remove, addition, replace, and separation, respectively. FWC/G2P/CVC correspond to foreign word conversion, grapheme-to-phoneme, and consonant vowel conversion, respectively. PP/Syn/Neo signify post-position, syntax, and neologism, respectively.\\n\\nFigure 5: Correlation distribution between speech-level: characteristics of interlocutor and text-level in Google (a) and Clova (b).\"}"}
{"id": "emnlp-2023-main-292", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ods ('.') are missing in all sentences, leading to the omission of other punctuation marks such as question marks ('?') or exclamation points ('!'). This specific condition allows us to focus on scenarios where the absence of periods directly affects the presence of other punctuation marks in the transcriptions.\\n\\nBoth the Google and Naver ASR systems exhibit significant error propagation in the domain of public transportation. Specifically, for Google, there is a high correlation between speech-level errors and text-level errors in punctuation, spacing, and replace. On the other hand, Clova shows a strong correlation between speech-level errors and text-level errors in punctuation, spacing, and addition. Furthermore, Google showed robustness in the nature ambient, but Clova showed relatively more text errors.\\n\\nFigure 5 shows the correlation between speech-level characteristics of interlocutor and text-level errors. For Google, the presence of pause (silent) in speech had a significant impact on the occurrence of remove errors in the transcriptions, while word repetition contributed to the occurrence of addition errors. In the case of Clova, overall, a higher number of errors were observed compared to Google. Particularly, hyperfluency had the most significant impact on the occurrence of addition errors in the transcriptions.\\n\\nThis analysis provides valuable insights into the correlation between speech-level noise, particularly noisy environments, and text-level errors in the Google and Clova ASR systems. The varying impact of different types of speech-level characteristics on text-level errors highlights the need for further granularity in categorizing these types. Even if models demonstrate similar performance, the individual capabilities of each model can differ. This demonstrates that KEBAP helps enhance the interpretability of ASR model verification.\\n\\n### 4.2 Adequacy of Synthesized Noise\\n\\nTable 6 shows the performance before and after noise synthesis. Experimental results show that for Google, the WER is 0.49, the CER is 0.23 before noise synthesis, and the WER is 0.68 and the CER is 0.41 after noise synthesis. For Clova, the WER before noise synthesis is 0.53, and the CER is 0.19, while the WER after noise synthesis is 0.71 and the CER is 0.43. These results are interpretable in that the resources we provide are high-quality and helpful.\\n\\n|                | Clean Noise | Noise            |\\n|----------------|-------------|------------------|\\n| Google ASR     | 0.49        | 0.68             |\\n| CER            | 0.23        | 0.41             |\\n| Clova ASR      | 0.53        | 0.71             |\\n| CER            | 0.19        | 0.43             |\\n\\nTable 6: Performance of commercial systems based on the presence or absence of noise synthesis. 'Clean' and 'Noise' represent the settings before and after noise synthesis, respectively.\\n\\n### 4.3 Examination of ASR models through LLM\\n\\nWith recent advancements in Large Language Model (LLM) development, most tasks are converging towards LLM-based approaches. In this study, we explore the potential of using ChatGPT (OpenAI-Blog, 2022) as a diagnostic tool for ASR results. Understanding error types is essential for verifying the models, and to measure this understanding, we perform an error type classification task. ChatGPT is utilized to classify text-level error types based on provided sentences in a few-shot setup. The specific prompt used for this experiment is listed in Appendix F.\\n\\nWe task ChatGPT with classifying all text-level errors occurring in the ASR results. However, as seen in the examples (please refer to Appendix F.2), it is evident that ChatGPT not only misclassifies text-level errors but also struggles more when multiple errors are present within a sentence. Although LLMs are converging towards covering various tasks, they exhibit limitations in performing diagnostic tasks for commercial systems. This indicates that while various tasks may converge with LLM, the diagnostic domain for the proposed model is far from convergence with LLMs, highlighting the need for further research.\\n\\n### 5 Conclusion\\n\\nIn the real-world, ASR results involve a trade-off between recognition accuracy and user readability, thus requiring a balanced consideration of these factors. To provide guidance for improving model performance, it is necessary to enhance interpretability, which entails considering both speech-level accuracy and text-level user readability. To this end, we propose Korean Error Explainable Benchmark.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset for ASR and Post-processing (KEBAP) for diagnosing and validating models by segmenting error types while considering both speech- and text-level. To facilitate the construction process, we utilize a GEC dataset that includes text-level errors and structure the process into validation, recording, synthesis of background noise, and difficulty tagging stages, employing consensus labeling within each stage to enhance the efficiency and quality of the task. We performed a detailed diagnostic analysis of the commercialization systems using KEBAP. Furthermore, the proposed task falls into a domain that is challenging for ChatGPT to cover, and it indicates the need for further research to achieve a closer approximation to real-world diagnostics. We demonstrated that KEBAP contributes to enhancing the interpretability of the model's weaknesses.\\n\\nLimitations\\nThis study has the limitation of only building data for the Korean language. Additionally, as this paper proposes a new task, it was not able to conduct extensive quantitative analyses by comparing it with existing models, which remains a limitation. However, this paper made a contribution by proposing new data and tasks and making them publicly available.\\n\\nEthics Statement\\nWe discuss the main ethical considerations of KEBAP benchmark we presented: (1) Privacy. KEBAP benchmark is constructed to acquire factual dataset, and does not contain privacy issues. (2) Human evaluation. During data evaluation process, we paid human workers the legal wage determined by the average time of evaluation and local labor compensation standards. We also guided them to take a rest when they are in a state of fatigue during work. (3) Potential problems. While principled measures are taken to ensure the quality of the dataset, there might still be potential problems with the dataset quality.\\n\\nAcknowledgements\\nThis research was supported by the MSIT (Ministry of Science and ICT), Korea, under the ITRC (Information Technology Research Center) support program (IITP-2023-2018-0-01405) supervised by the IITP (Institute for Information & Communications Technology Planning & Evaluation). This work was supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques). This work was supported by Institute for Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00369, Part 4) Development of AI Technology to support Expert Decision-making that can Explain the Reasons/Grounds for Judgment Results based on Expert Knowledge).\\n\\nReferences\\nPetar Aleksic, Cyril Allauzen, David Elson, Aleksandar Kracun, Diego Melendo Casado, and Pedro J. Moreno. 2015a. Improved recognition of contact names in voice commands. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5172\u20135175. IEEE.\\n\\nPetar Aleksic, Mohammadreza Ghodsi, Assaf Michaely, Cyril Allauzen, Keith Hall, Brian Roark, David Rybach, and Pedro Moreno. 2015b. Bringing contextual information to google speech recognition.\\n\\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregory Weber. 2020. Common voice: A massively-multilingual speech corpus. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4218\u20134222, Marseille, France. European Language Resources Association.\\n\\nYoussef Bassil and Paul Semaan. 2012. Asr context-sensitive error correction based on microsoft n-gram dataset. arXiv preprint arXiv:1203.5262.\\n\\nTheresa Breiner, Swaroop Ramaswamy, Ehsan Variani, Shefali Garg, Rajiv Mathews, Khe Chai Sim, Kilol Gupta, Mingqing Chen, and Lara McConnaughey. 2022. Userlibri: A dataset for asr personalization using only text. arXiv preprint arXiv:2207.00706.\\n\\nHui Bu, Jiayu Du, Xingyu Na, Bengu Wu, and Hao Zheng. 2017. Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline. In 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA), pages 1\u20135. IEEE.\\n\\nJoon Son Chung. 2019. Naver at activitynet challenge 2019\u2013task b active speaker detection (ava). arXiv preprint arXiv:1906.10555.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wenliang Dai, Samuel Cahyawijaya, Tiezheng Yu, Elham J Barezi, Peng Xu, Cheuk Tung Yiu, Rita Frieske, Holy Lovenia, Genta Winata, Qifeng Chen, et al. 2022. Ci-avsr: A cantonese audio-visual speech dataset for in-car command recognition. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 6786\u20136793.\\n\\nJohn Evershed and Kent Fitch. 2014. Correcting noisy ocr: Context beats confusion. In Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pages 45\u201351.\\n\\nJinjuan Feng and Andrew Sears. 2004. Using confidence scores to improve hands-free speech based navigation in continuous dictation systems. ACM Transactions on Computer-Human Interaction (TOCHI), 11(4):329\u2013356.\\n\\nZorik Gekhman, Dina Zverinski, Jonathan Mallinson, and Genady Beryozkin. 2022a. RED-ACE: Robust error detection for ASR using confidence embeddings. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2800\u20132808, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nZorik Gekhman, Dina Zverinski, Jonathan Mallinson, and Genady Beryozkin. 2022b. Red-ace: Robust error detection for asr using confidence embeddings. arXiv preprint arXiv:2203.07172.\\n\\nYuan Gong, Jin Yu, and James Glass. 2022. Vocal-sound: A dataset for improving human vocal sounds recognition. In ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 151\u2013155. IEEE.\\n\\nJinxi Guo, Tara N Sainath, and Ron J Weiss. 2019. A spelling correction model for end-to-end speech recognition. In ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5651\u20135655. IEEE.\\n\\nJi-Wan Ha and Hyun Sub Sim. 2008. A comparison study of interjectional characteristics between people who stutter and people who do not stutter. Communication Sciences and Disorders, 13(3):438\u2013453.\\n\\nOleksii Hrinchuk, Mariya Popova, and Boris Ginsburg. 2020. Correction of automatic speech recognition with transformer sequence-to-sequence model. In Icassp 2020-2020 ieee international conference on acoustics, speech and signal processing (icassp), pages 7074\u20137078. IEEE.\\n\\nSeonmin Koo, Chanjun Park, Jaehyung Seo, Seungjun Lee, Hyeonseok Moon, Jungseob Lee, and Heuiseok Lim. 2022. K-nct: Korean neural grammatical error correction gold-standard test set using novel error type classification criteria. IEEE Access, 10:118167\u2013118175.\\n\\nEgor Lakomkin, Sven Magg, Cornelius Weber, and Stefan Wermter. 2019. Kt-speech-crawler: Automatic dataset construction for speech recognition from youtube videos. arXiv preprint arXiv:1903.00216.\\n\\nMyunghoon Lee, Hyeonho Shin, Dabin Lee, and Sung-Pil Choi. 2021. Korean grammatical error correction based on transformer with copying mechanisms and grammatical noise implantation methods. Sensors, 21(8):2658.\\n\\nYichong Leng, Xu Tan, Linchen Zhu, Jin Xu, Renqian Luo, Linquan Liu, Tao Qin, Xiangyang Li, Edward Lin, and Tie-Yan Liu. 2021. Fastcorrect: Fast error correction with edit alignment for automatic speech recognition. Advances in Neural Information Processing Systems, 34:21708\u201321719.\\n\\nJunwei Liao, Sefik Emre Eskimez, Liyang Lu, Yu Shi, Ming Gong, Linjun Shou, Hong Qu, and Michael Zeng. 2020. Improving readability for automatic speech recognition transcription. Transactions on Asian and Low-Resource Language Information Processing.\\n\\nJunwei Liao, Yu Shi, and Yong Xu. 2022. Automatic speech recognition post-processing for readability: Task, dataset and a two-stage pre-trained approach. IEEE Access, 10:117053\u2013117066.\\n\\nAnirudh Mani, Shruti Palaskar, and Sandeep Konam. 2020a. Towards understanding asr error correction for medical conversations. In Proceedings of the first workshop on natural language processing for medical conversations, pages 7\u201311.\\n\\nAnirudh Mani, Shruti Palaskar, Nimshi Venkat Meripo, Sandeep Konam, and Florian Metze. 2020b. Asr error correction and domain adaptation using machine translation. In ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 6344\u20136348. IEEE.\\n\\nAndrew Morris, Viktoria Maier, and Phil Green. 2004. From wer and ril to mer and wil: improved evaluation measures for connected speech recognition.\\n\\nCourtney Napoles, Keisuke Sakaguchi, Matt Post, and Joel Tetreault. 2015. Ground truth for grammatical error correction metrics. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 588\u2013593.\\n\\nThi-Tuyet-Hai Nguyen, Mickael Coustaty, Antoine Doucet, Adam Jatowt, and Nhu-Van Nguyen. 2018. Adaptive edit-distance and regression approach for post-ocr text correction. In Maturity and Innovation in Digital Libraries: 20th International Conference on Asia-Pacific Digital Libraries, ICADL 2018, Hamilton, New Zealand, November 19-22, 2018, Proceedings 20, pages 278\u2013289. Springer.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thi Tuyet Hai Nguyen, Adam Jatowt, Nhu-Van Nguyen, Mickael Coustaty, and Antoine Doucet. 2020. Neural machine translation with bert for post-ocr error detection and correction. In Proceedings of the ACM/IEEE joint conference on digital libraries in 2020, pages 333\u2013336.\\n\\nOpenAI-Blog. 2022. Chatgpt: Optimizing language models for dialogue.\\n\\nVassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: an asr corpus based on public domain audio books. In 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5206\u20135210. IEEE.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nChanjun Park, Jaehyung Seo, Seolhwa Lee, Chanhee Lee, Hyeonseok Moon, Sugyeong Eo, and Heui-Seok Lim. 2021. Bts: Back transcription for speech-to-text post-processor using text-to-speech-to-text. In Proceedings of the 8th Workshop on Asian Translation (WAT2021), pages 106\u2013116.\\n\\nAlec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 28492\u201328518. PMLR.\\n\\nYongmei Shi and Lina Zhou. 2011. Supporting dictation speech recognition error correction: the impact of external information. Behaviour & Information Technology, 30(6):761\u2013774.\\n\\nMyung-Sun Shin, Jong-Bok Ahn, Hyun-Wook Nam, and Do-Ha Kwon. 2005. A study of dysfluency characteristics in normal adults and children in monologue. Speech Sciences, 12(3):49\u201357.\\n\\nClaytone Sikasote and Antonios Anastasopoulos. 2022. BembaSpeech: A speech recognition corpus for the Bemba language. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 7277\u20137283, Marseille, France. European Language Resources Association.\\n\\nBernhard Suhm, Brad Myers, and Alex Waibel. 2001. Multimodal error correction for speech user interfaces. ACM transactions on computer-human interaction (TOCHI), 8(1):60\u201398.\\n\\nWei Tang and Matthew Lease. 2011. Semi-supervised consensus labeling for crowdsourcing. In SIGIR 2011 workshop on crowdsourcing for information retrieval (CIR), pages 1\u20136.\\n\\nLongshaokan Wang, Maryam Fazel-Zarandi, Aditya Tiwari, Spyros Matsoukas, and Lazaros Polymenakos. 2020. Data augmentation for training dialog models robust to speech recognition errors. arXiv preprint arXiv:2006.05635.\\n\\nSihui Wang, Tom Gunter, and David VanDyke. 2018. On modelling uncertainty in neural language generation for policy optimisation in voice-triggered dialog assistants. In 2nd Workshop on Conversational AI: Today's Practice and Tomorrow's Potential, NeurIPS.\\n\\nJason D Williams and Steve Young. 2007. Partially observable markov decision processes for spoken dialog systems. Computer Speech & Language, 21(2):393\u2013422.\\n\\nJ.P. Woodard and year = 1982 journal = Workshop on standardisation for speech I/O technology, Naval Air Development Center, Warminster, PA title = An information theoretic measure of speech recognition performance Nelson, J.T.\\n\\nSoyoung Yoon, Sungjoon Park, Gyuwan Kim, Junhee Cho, Kihyo Park, Gyu Tae Kim, Minjoon Seo, and Alice Oh. 2022. Towards standardizing korean grammatical error correction: Datasets and annotation. arXiv preprint arXiv:2210.14389.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Related Works and Background\\n\\nPost-processing Model\\n\\nPost-processing serves an important role in quality enhancement across various fields by modifying the distorted output into appropriate statements. For instance, in the field of optical character recognition (OCR), conventional approaches such as manual, lexical, and statistical methods have been used (Evershed and Fitch, 2014; Nguyen et al., 2018). More recently, language models like BERT have been employed for error detection in tasks like named entity recognition (NER) and are performed through character-level machine translation (Nguyen et al., 2020).\\n\\nAs another field, machine translation (MT) often utilizes the following methods. Post-processing research is being carried out in automatic post-editing (APE) to improve translation quality by adopting transfer learning (Correia and Martins, 2019). Concurrently, in the grammatical error correction (GEC) field, transformers and the copy mechanism are used to correct spelling and grammatical errors in MT results (Lee et al., 2021). Studies that define error types to construct test sets or utilize an automatic grammatical error annotation system to create datasets also exist to improve Korean GEC studies (Koo et al., 2022; Yoon et al., 2022). Similarly, the study on post-processing is actively explored in a wide range of fields and holds significance in terms of enhancing the quality of output results. This can also be of significant importance in the field of Automatic speech recognition (ASR), which is discussed in the following section.\\n\\nASR Post-Processing Model\\n\\nASR post-processing (ASRP) involves the detection and correction of errors in the output of an ASR, distinguishing it from simple error correction in that it considers user-friendliness as an additional aspect. This approach can improve the final quality of statements without modifying the ASR system structure. For instance, in specialized fields like the medical domain, attempts have been made to eliminate punctuation errors in ASR through post-processing (Mani et al., 2020a).\\n\\nPrior research has primarily focused on providing information that allows humans to manually rectify erroneous segments, proposing alternative words for correction or creating an environment conducive to modification (Suhm et al., 2001; Feng and Sears, 2004). External information, such as word alternative hypothesis, noisy context, and accurate context, is provided to assist in post-processing for error correction (Shi and Zhou, 2011). In particular, Bassil and Semaan (2012) use the N-gram dataset for ASR errors to detect and correct errors automatically. Models such as LSTM-based or Transformer-based sequence-to-sequence architectures are adopted to correct the speech recognition results while considering the semantics and spelling (Guo et al., 2019; Hrinchuk et al., 2020).\\n\\nRecent studies strive to improve ASRP performance by utilizing the results derived from ASR. Gekhman et al. (2022a) introduce the ASR confidence embedding (ACE) layer to the encoder of the ASR model to jointly encode the confidence scores and transcribed text into a contextualized representation. To mitigate the time and cost-related challenges associated with the parallel data required for training, Park et al. (2021) employ Text-to-speech (TTS) and Speech-to-text (STT) technologies to construct parallel data.\\n\\nASR dataset\\n\\nThe availability of suitable datasets is imperative for the active progression of ASRP. Previously, post-processing studies have been conducted with ASR datasets. Panayotov et al. (2015) organize the two labels in the ASR dataset that denote the quality of speech recognition, classified into 'clean' and 'other' categories, providing valuable assistance in the analysis. Ardila et al. (2020) construct comprehensive ASR dataset that includes demographic metadata such as age, sex, and accent to provide a wider representation.\\n\\nTranscription hypotheses obtained by decoding audio data using an ASR model are used to align hypothesis words with the reference (correct) transcription. The process of labeling errors and non-errors is facilitated by employing the minimum edit distance (Gekhman et al., 2022b). In the context of Chinese language datasets, a significant dataset is available for speech recognition systems, labeled with audio devices and recording environments (Bu et al., 2017). Gekhman et al. (2022b) build a dataset by aligning hypothesis words with the reference (correct) transcription through a transcription hypothesis obtained by decoding audio data with an ASR model and labeling errors and nonerrors using minimum edit distance. In the context of Chinese, a large-scale dataset is available for speech recognition systems labeled with audio device information and recording environments (Bu et al., 2017).\\n\\nTo mitigate the problem of insufficient training...\"}"}
{"id": "emnlp-2023-main-292", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data, methodologies that synthesize data via data augmentation methods have been proposed (Liao et al., 2022). However, the overall quality of the data is more crucial than the size. Specifically, the detailed datasets that consider both speech- and text-level like the real world are absent. Consequently, we aim to construct the ASR Post-Processing dataset, which contemplates audio- and text-level for the first time.\\n\\nB Description of Speech-Level Noise Type\\n\\nPause (silent) category captures instances where silence intervenes mid-utterance before completion\u2014for instance, when 'I am eating' is articulated as 'I am... eating'.\\n\\nFilled pause represents cases characterized by the habitual insertion of filler sounds during pauses, as in utterances supplemented by sounds such as 'um... uh... so I'.\\n\\nInterjection category encompasses instances where one or more words or phrases irrelevant to the intended message are interjected, evident in utterances like 'Okay I see, but you know'.\\n\\nParenthetical category includes instances where grammatically correct, but semantically neutral phrases are inserted\u2014for instance, utterances incorporating phrases such as 'you know' and 'I mean'.\\n\\nUnfinished interlocutor category denotes cases where the utterance concludes prematurely\u2014for instance, when 'I am eating' is truncated to 'I am...'.\\n\\nWord repetition category signifies instances where the same word is iterated, as in saying 'Hello' as 'Hello Hello'.\\n\\nSyllable repetition category characterizes cases where the same syllable is iterated\u2014for instance, when 'Hello' is articulated as 'He-hello'.\\n\\nPhoneme repetition category encapsulates instances where the same phoneme is repeated, such as saying 'Hello' as 'Hel-llo'.\\n\\nSustained category accounts for instances where part of an utterance is elongated, exemplified in 'Is that so\u2014right?'.\\n\\nHyperfluency category represents instances of excessive verbosity.\\n\\nMutter category includes cases where utterances are murmured in an indistinct manner, as in 'That.. is.. like that...'.\\n\\nDynamic error category encompasses instances where syllable articulation strength is incongruous with the intended utterance, or instances that are challenging to comprehend at the human-level. Finally, speaking rate category accounts for instances where rapid speech pace hinders comprehension at a human-level.\\n\\nC Description of Text-Level Error Type\\n\\nSpacing encapsulates instances contravening standard spacing conventions.\\n\\nPunctuation entails cases where punctuation is omitted or misapplied in Korean sentences\u2014for instance, when 'Can I teach?' is interpreted as 'Can I teach.'. numerical encompasses cases where number conversion fails, such as when 'Ahead of the three-month schedule' is interpreted as 'Bill 2, 3-month schedule'.\\n\\nSpelling and Grammar consists of ten detailed subcategories.\\n\\nRemove designates cases where some word components are not recognized, or endings or particles are missing\u2014for example, when 'The champion is in the final' is misinterpreted as 'Champion final'.\\n\\nAddition involves cases where the same word is repeated or unutilized particles or endings are appended. For instance, when 'World\u2019s fruits, fish, and meat' is interpreted as 'World\u2019s world\u2019s fruits, fish, and meat'.\\n\\nReplace refers to instances where one word is substituted with another\u2014for example, when 'Apply the filter.' is interpreted as 'Wear the pizza'.\\n\\nSeparation refers to instances where consonants and vowels in the target utterance are separated, exemplified when 'The discount applies as it is.' is interpreted as 'Discount app - lise as it is.'. Foreign word conversion refers to cases where words deviate from standard foreign word pronunciation or some syllables are incorrectly converted from English to Korean or vice versa. For example, when 'Brazil\u2019s Samba Festival' is interpreted as 'Brazil\u2019s SsamBap Festival,,' or 'I prefer to use ATM.' is interpreted as 'I prefer to use hm.'.\\n\\nSpelling is bifurcated into two types: Grapheme-to-Phoneme (G2P) and Consonant vowel conversion.\\n\\nG2P pertains to instances where a character is recognized per its pronunciation.\\n\\nConsonant vowel conversion refers to instances where phonemic units are incorrectly spelled.\\n\\nPost-position refers to cases where different particles are used or omitted\u2014for example, when 'Ordinary high school students' is interpreted as 'Ordinary at high school students.'. Syntax involves cases where the grammatical interpretation remains valid, but the semantic interpretation varies. Finally, neologism refers to cases where the target word and its meaning and pronunciation are dissimilar and are not included in Korean vocabulary.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D.1 Crowd-sourcing and Compensation\\nWe recruited individuals who are native speakers of Korean and selectively hired candidates suitable for the task through validation questions. Every employee has been fairly remunerated at least a rate of 140 KRW per task. It is expected that each worker will complete 2-3 questions within a minute, guaranteeing a minimum compensation of 16,800 KRW per hour. Comparatively, the minimum hourly wage in South Korea for 2023 is 9,620 KRW.\\n\\nD.2 Annotation Guidelines and Interface\\n\\nFigure 6: Speech recording setup.\\nFigure 7: Difficulty annotation setup.\\n\\nQ: Please evaluate the level of difficulty in accurately transcribing the speech to match the given transcript.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The detailed demographic information is provided in Table 7.\\n\\n| Gender | Count | Percentage |\\n|--------|-------|------------|\\n| Male   | 186   | 7.50%      |\\n| Female | 2292  | 92.49%     |\\n\\n| Age     | Count | Percentage |\\n|---------|-------|------------|\\n| 20-29   | 479   | 19.33%     |\\n| 30-39   | 1318  | 53.19%     |\\n| 40-49   | 681   | 27.48%     |\\n\\nTable 7: Demographics of the crowd workers involved in the composition of the data.\\n\\nWe believe that providing difficulty information facilitates the analysis of weaknesses in ASR models. We extracted an equal number of samples for each difficulty level and analyzed them. Figure 8, Figure 9, and Figure 10 show the correlation between the noisy environment at the speech level and text-level errors in diverse difficulty settings. Figure 11, Figure 12, and Figure 13 illustrate the correlation between speech-level characteristics of interlocutor and text-level errors in diverse difficulty settings.\\n\\nSpa/Punc/Num represent spacing, punctuation, and numerical, respectively. Rem/Add/Rep/Sep indicate remove, addition, replace, and separation, respectively. FWC/G2P/CVC correspond to foreign word conversion, grapheme-to-phoneme, and consonant vowel conversion, respectively.\\n\\nAnalyzing the details based on different difficulty levels can be employed to enhance the interpretability of the ASR model. For example, in the case of Google, experimental results show that the correlation from 'Terminal' speech-level type to 'Punctuation' text-level type is strong for easy level, 'Construction site' speech-level type to 'Addition' text-level type for medium level, and 'Terminal' speech-level type to 'Replace' or 'Remove' text-level type for hard level. For Clova, the tendency of 'Replace' text-level type in 'Individual transportation' speech-level type is strongest at easy level, and it is strongly related to 'Syntax' and 'Replace' text-level type at medium level. At the hard level, it has a strong tendency to 'Remove' and 'Syntax' text-level types.\\n\\nFigure 8: Correlation distribution between speech-level: noisy environment and text-level in Google(a) and Clova(b), in easy-level difficulty setting.\"}"}
{"id": "emnlp-2023-main-292", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Correlation distribution between speech-level: noisy environment and text-level in Google(a) and Clova(b), in medium-level difficulty setting.\\n\\nFigure 10: Correlation distribution between speech-level: noisy environment and text-level in Google(a) and Clova(b), in hard-level difficulty setting.\\n\\nFigure 11: Correlation distribution between speech-level: characteristics of interlocutor and text-level in Google(a) and Clova(b), in easy-level difficulty setting.\\n\\nFigure 12: Correlation distribution between speech-level: characteristics of interlocutor and text-level in Google(a) and Clova(b), in medium-level difficulty setting.\\n\\nFigure 13: Correlation distribution between speech-level: characteristics of interlocutor and text-level in Google(a) and Clova(b), in hard-level difficulty setting.\"}"}
