{"id": "emnlp-2024-main-144", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pun Sentence: He and his partner made knives, and they shared a cut.\\n\\nFr: Lui et son partenaire ont fabriqu\u00e9 des couteaux, et ils se sont engag\u00e9s dans un processus de division ou de s\u00e9paration de mat\u00e9riaux avec des objets pointus.\\n\\nDe: Er und sein Partner produzierten Messer und besch\u00e4ftigten sich mit dem Prozess des Teilen oder Trennens von Materialien mit scharfen Objekten.\\n\\nKr: \uadf8\uc640 \uadf8\uc758 \ud30c\ud2b8\ub108\ub294 \uce7c\uc744 \ub9cc\ub4e4\uc5c8\uace0, \uadf8\ub4e4\uc740 \ub0a0\uce74\ub85c\uc6b4 \ubb3c\uccb4\ub85c \uc7ac\ub8cc\ub97c \ubd84\ud560\ud558\uac70\ub098 \ubd84\ub9ac\ud558\ub294 \uacfc\uc815\uc5d0 \ucc38\uc5ec\ud588\uc2b5\ub2c8\ub2e4.\\n\\ncut\\n\\nFr: Lui et son partenaire ont produit des couteaux et ils ont chacun re\u00e7u une part des b\u00e9n\u00e9fices gagn\u00e9s de cette entreprise.\\n\\nDe: Er und sein Partner stellten Messer her und sie erhielten jeweils einen Anteil an den Gewinnen aus diesem Vorhaben.\\n\\nKr: \uadf8\uc640 \uadf8\uc758 \ud30c\ud2b8\ub108\ub294 \uce7c\uc744 \uc0dd\uc0b0\ud588\uace0, \uadf8\ub4e4 \uac01\uac01\uc774 \uc774 \uacfc\uc815\uc5d0\uc11c \ubc8c\uc5b4\ub4e4\uc778 \uc774\uc775\uc758 \uc77c\ubd80\ub97c \ubc1b\uc558\uc2b5\ub2c8\ub2e4.\\n\\nto divide something with a sharp object\\n\\nFr: Les professeurs de sciences se sont s\u00e9par\u00e9s parce qu'il n'y avait aucune collaboration ou interaction sur le sujet de la chimie entre eux.\\n\\nDe: Die Naturwissenschaftslehrer trennten sich, weil es zwischen ihnen keine Zusammenarbeit oder Interaktion im Fach Chemie gab.\\n\\nKr: \uadf8\ub4e4 \uc0ac\uc774\uc5d0 \ud654\ud559\uc5d0 \uad00\ud55c \ud611\uc5c5\uc774\ub098 \uc0c1\ud638\uc791\uc6a9\uc774 \uc5c6\uc5b4\uc11c \uacfc\ud559\uad50\uc0ac\ub4e4\uc774 \ud5e4\uc5b4\uc84c\uc2b5\ub2c8\ub2e4.\\n\\nchemistry\\n\\nFr: Les professeurs de sciences se sont s\u00e9par\u00e9s parce qu'il n'y avait pas d'attraction romantique ou de sentiment de connexion entre eux.\\n\\nDe: Die Naturwissenschaftslehrer trennten sich, weil es keine romantische Anziehung oder Gef\u00fchl der Verbundenheit zwischen ihnen gab.\\n\\nKr: \uadf8\ub4e4 \uc0ac\uc774\uc5d0 \ub85c\ub9e8\ud2f1\ud55c\ub04c\ub9bc\uc774\ub098 \uc5f0\uacb0\uac10\uc774 \uc5c6\uae30 \ub54c\ubb38\uc5d0\uacfc\ud559\uc120\uc0dd\ub2d8\uc774 \ud5e4\uc5b4\uc84c\uc2b5\ub2c8\ub2e4.\\n\\nscience studying matter and its interactions\\n\\nchemistry\\n\\nDiabetics should not be allowed to have sweet dreams.\\n\\nFr: Les diab\u00e9tiques ne devraient pas \u00eatre autoris\u00e9s \u00e0 r\u00eaver de nourritures sucr\u00e9es car ce n'est pas sain pour eux.\\n\\nDe: Diabetiker sollen nicht davon tr\u00e4umen d\u00fcrfen, Zuckerlebensmittel zu essen, weil es nicht gesund f\u00fcr sie ist.\\n\\nKr: \ub2f9\ub1e8\ubcd1\ud658\uc790\ub4e4\uc740\uc124\ud0d5\uc774 \ub9ce\uc740 \uc74c\uc2dd\uc5d0 \ub300\ud574\uafc8\uafb8\ub294\uac83\uc774\uac74\uac15\uc5d0\uc88b\uc9c0\uc54a\uc73c\ubbc0\ub85c\uac83\uc774\ud5c8\uc6a9\ub418\uc5b4\uc11c\ub294\uc548\ub429\ub2c8\ub2e4.\\n\\nsweet\\n\\nFr: Les diab\u00e9tiques ne devraient pas pouvoir avoir des r\u00eaves agr\u00e9ables ou plaisants.\\n\\nDe: Diabetiker sollten keine angenehmen oder erfreulichen Tr\u00e4ume haben k\u00f6nnen.\\n\\nKr: \ub2f9\ub1e8\ubcd1\ud658\uc790\ub4e4\uc774\uc990\uac70\uc6b4\ub610\ub294\uae30\ud508\uafc8\uc744\uafb8\uc9c0\ubabb\ud574\uc57c\ud569\ub2c8\ub2e4.\\n\\nhaving a sugary taste, like candy or sugar\\n\\nFigure 6: Example annotations of homographic puns in UNPIE benchmark.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I tried to record an album in a reptile shop, but there was a terrible gecko.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Can visual language models resolve textual ambiguity with visual cues? Let visual puns tell you!\\n\\nJiwan Chung Seungwon Lim Jaehyun Jeon Seungbeen Lee Youngjae Yu\\nYonsei University\\njiwan.chung.research@gmail.com\\n\\nAbstract\\nHumans possess multimodal literacy, allowing them to actively integrate information from various modalities to form reasoning. Faced with challenges like lexical ambiguity in text, we supplement this with other modalities, such as thumbnail images or textbook illustrations. Is it possible for machines to achieve a similar multimodal understanding capability?\\n\\nIn response, we present Understanding Pun with Image Explanations (UNPIE), a novel benchmark designed to assess the impact of multimodal inputs in resolving lexical ambiguities. Puns serve as the ideal subject for this evaluation due to their intrinsic ambiguity. Our dataset includes 1,000 puns, each accompanied by an image that explains both meanings. We pose three multimodal challenges with the annotations to assess different aspects of multimodal literacy; Pun Grounding, Disambiguation, and Reconstruction. The results indicate that various Socratic Models and Visual-Language Models improve over the text-only models when given visual context, particularly as the complexity of the tasks increases.\\n\\n1 Introduction\\nHumans can actively integrate information from multimodal sources without being explicitly told to. For example, a wink can reveal the insincerity behind a statement about dieting. Similarly, visual aids such as Venn diagrams help students understand abstract concepts such as set theory. This active understanding capacity is often denoted as multimodal literacy (Mills and Unsworth, 2017).\\n\\nIn contrast, current multimodal models lack this capacity for active understanding and typically operate under two assumptions: (1) all instructions require visual inputs, and (2) these inputs are relevant (Cui et al., 2023; Zhang et al., 2024). Such limitations hinder their applicability in real-world scenarios, such as summarizing long blog posts, where irrelevant images must be excluded, and only contextually significant visuals should be used to enhance the understanding of disparate text segments.\\n\\nAn essential component of multimodal literacy is the ability to resolve multimodal ambiguities effectively, which refers to the capacity to disambiguate conflicting or unclear information in modality with information from another modality (Kottur et al., 2021; Guo et al., 2022). Owing to its explicit requirement of multimodal information gathering, disambiguation can serve as a controlled benchmark for evaluating multimodal literacy.\\n\\nPuns stand as a unique challenge within ambiguity modeling. They are intrinsically ambiguous and understanding a pun requires grasping multiple interpretations of a single phrase or word simultaneously. Understanding puns can be difficult even for humans, often necessitating visual cues to clarify the intended interpretation, as demonstrated in Figure 1. Compared to verbose textual explanations, visual cues can deliver instant insight, preserving the humor and cleverness of the pun (Morreall, 1983). Therefore, puns provide an ideal testing ground for assessing models' capabilities in multimodal understanding.\\n\\nFigure 1: Puns naturally occur with images to enhance understanding (Zenner and Geeraerts, 2018), making them natural candidates for testing active multimodal understanding capacity of machines. Examples of puns accompanied by visual explanations from r/puns subreddit on Reddit.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Success comes in cans, failure comes in cant's.\\n\\n\\\\[ \\\\text{sim}(V1, L1) > \\\\text{sim}(V1, L2) \\\\]\\n\\nCan  =        | \\nV1\\nV2\\n(a)\\n(b)\\n(b)\\n\\nFigure 2: The UNPIE benchmark comprises three multimodal tasks: 1. Identifying the specific phrase in an English sentence that constitutes a pun, using the provided (a) pun explanation image; 2. Choosing the translation of the pun sentence that aligns more closely with the given (b) pun disambiguator image; and 3. Reconstructing the English pun sentence from its translated version, aided by the corresponding (a) pun explanation image.\\n\\nIn this work, we explore model capabilities in resolving textual ambiguities through visual context. To this end, we propose Understanding Pun with Image Explanations (UNPIE), a novel benchmark consisting of 1,000 text-based puns paired with illustrative images that highlight the incongruity within the puns. Additionally, our dataset approaches pun comprehension as a translation task with incomplete information. This method provides a tangible way to measure the often subjective skill of reconstructing puns. Each English pun is accompanied by translations in three different languages\u2014German, French, and Korean\u2014to capture the challenge of reconstructing the puns across diverse linguistic contexts.\\n\\nWe design three tests based on UNPIE to study how models can exploit visual context to aid pun understanding. Figure 2 summarizes the tasks comprising our benchmark. We first consider an English-only task pun grounding that challenges machines to identify the specific phrase in a sentence that forms a pun. Next, we formulate a multilingual challenge of pun disambiguation where models must choose the translation that best matches the image provided as a pun disambiguator. The final test, pun reconstruction, is a comprehensive task where models should recreate the original English pun sentence using a translated version with potentially no ambiguity. For both the pun grounding and reconstruction tasks, we additionally provide the pun explanation images as inputs to verify whether models can consider multimodal context when dealing with ambiguous text.\\n\\nOur comprehensive experiments on UNPIE affirm the presence of multimodal literacy capacity in two model types: monolithic Visual-Language Models and modular Socratic Models. Incorporating visual context consistently improved performance across our three pun comprehension tests. Notably, this improvement was more pronounced in more challenging tasks. Moreover, VLMs performed better than Socratic Models built on simple image captions. The result suggests that detailed visual understanding is necessary in our benchmark. Finally, fine-tuning with a standard multimodal machine translation dataset adversely affects performance in the pun reconstruction task. This degradation aligns with findings from prior studies (Futeral et al., 2023) stating that web-based multimodal translation datasets may not effectively capture visual dependencies.\\n\\nOverall, our contributions are as follows:\\n\\n1. UNPIE, a novel benchmark for assessing the multimodal literacy capability of visual-language models. UNPIE is built on text with intrinsic ambiguity (puns), guaranteeing the benefit of visual context.\\n2. Three new tasks posed on the textual puns and the image annotations: pun grounding, disambiguation, and reconstruction.\\n3. Experimental results verifying multimodal literacy capability of both VLMs and Socratic Models concerning pun understanding.\\n\\n2 Overview of UNPIE Benchmark\\n\\nUNPIE is a new multimodal multilingual benchmark. Its primary aim is to assess machines' capacity to actively integrate information from visual context.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A type of ice cream\\nFather\\nA little boy called his father who made balloons pop.\\nWe are not dairy queen, but we have great sundays.\\nDay after saturday\\nExplosive sound\\nPop Sunday\\nSundae\\n\\nFigure 3: Comparison of homographic (left) and heterographic (right) puns in UNPIE dataset along with the respective disambiguator visual annotations.\\n\\n| Dataset     | Size | Ambiguous (%) | Gen |\\n|-------------|------|----------------|-----|\\n| Multi30k    | 1000 | 2             | /enc-33 |\\n| CoMMuTE     | 155  | 100           |     |\\n| UNPIE       | 1000 | 100           | /enc-33 |\\n\\nTable 1: Comparison of UNPIE against multimodal machine translation benchmarks. The statistics for Multi30k are from the test-2017-flickr subset. Gen denotes a generative benchmark.\\n\\nSources to resolve ambiguity in text. Our dataset leverages puns that inherently contain such ambiguity to study the challenge of multimodal literacy in a natural environment.\\n\\nUNPIE extends puns in two directions: visual context and multilingual translations. First, we collect images for each pun that 1. describes both meanings of the pun to explain it and 2. depicts only one meaning of the pun to disambiguate the pun (section 2.1). While one can naturally retrieve images for disambiguation from the web, images that illustrate the ambiguity of the pun in a single canvas are rare. Thus, we use an off-the-shelf text-to-image model (Betker et al., 2023) to generate such images. We then employ human annotators to filter the images so that they correctly explain the given pun. Secondly, we ask human annotators to translate the English pun sentences into multilingual targets (section 2.2). Importantly, the ambiguity should not carry on to the translation target.\\n\\n2.1 Collecting Puns with Visual Context\\n\\nBase Text-Only Pun Data. We build our multimodal multilingual benchmark on top of the text-only English pun dataset of SemEval 2017 Task 7 (Miller et al., 2017). The dataset bounds the pun understanding problem in two ways to rely less on external requirements: first, each sentence contains a maximum of one pun. Hence, a sentence's lexical ambiguity is regulated, at least in terms of puns. Second, most pun has a lexical entry in WordNet 3.1 (81% of the whole data). This vocabulary limit keeps our pun generation problem from being dominated by many out-of-vocabulary words.\\n\\nThe data is divided into Homographic and Heterographic puns, depending on the surface form of the puns. As shown in Figure 3, homographic puns have identical spelling and pronunciation but different meanings, while heterographic puns differ in spelling and meanings. We inherit this categorization scheme and report our experiment results category-wise (Homographic and Heterographic).\\n\\nFrom the SemEval 2017 collection of 2,878 English pun sentences, we selected 500 homographic and 500 heterographic puns with concrete concepts that are more easily visualized through images.\\n\\nGenerating Pun Explanation Images. UNPIE is designed to assess a VLM's capability to resolve lexical ambiguity with visual context. In terms of a pun, the context should depict both meanings.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"within the pun. Such images are hard to find among natural images due to their complex and sometimes ambivalent meanings. Further, such visual designs are typically proprietary, which contradicts our goal of an open-source dataset. Hence, we resort to creating new images that fit our requirements.\\n\\nWe recruited three NLP researchers to actively prompt the text-to-image generation model DALL-E 3 (Betker et al., 2023) to create images fitting our pun criteria while maintaining a natural appearance. The base text-only dataset provided the puns as data seeds (Miller et al., 2017). While we allow relative freedom in the choice of prompts, the workers reported that DALL-E 3 typically produced satisfactory images with straightforward instructions, as illustrated in Figure 4. Thanks to DALL-E 3's multi-turn interface, the researchers could request further image revisions if the initial output was unsuitable. On average, \\\\( \\\\sim 24\\\\% \\\\) samples needed such multi-step modification. We obtained 1000 pun explanation images after this process.\\n\\nRetrieving Pun Disambiguator Images. UNPIE offers an alternative visual context: per each pun, we attach two images that describe each meaning of the pun. These images disambiguate the pun and are intended to be used in the binary classification task of pun disambiguation explained in section 3. As a pun disambiguator image is aligned to a single meaning, searching for the required image is easier compared to the pun explanation images that require encoding both meanings in the same image. Hence, we opt for image retrieval from the LAION 2B web image-text dataset (Schuhmann et al., 2022) rather than image generation. Using the CLIP (Radford et al., 2021)-based image search API (Beaumont, 2022), we retrieve ten images per the meaning of a pun. Then, we manually select the top one that best fits the description. We discard the whole sample when there is no suitable image. We considered two criteria when selecting the images: first, images that explicitly contain the meaning or the pun word itself as printed text are discouraged as such images reward OCR capability rather than general visual understanding. Second, images with watermarks are filtered out to avoid confusion.\\n\\n2.2 Translating Puns to Multilingual Targets\\n\\nEvaluating a machine's ability to understand puns is a complex task. Without a rule-based algorithm to measure this capability, the assessment often relies on human judgment or other machines. However, relying on human evaluation can limit the scalability of the assessment process, while machine-based evaluation, such as using models like GPT-4 (OpenAI, 2023), may introduce undesirable biases (Liu et al., 2023c; Hada et al., 2023). To overcome these challenges, we suggest an alternative evaluation method via a downstream task in translation, intentionally aligning with previous research in the field of multimodal machine translation.\\n\\nTranslation with Machine Assistance. We translate the original English pun sentence into three languages (German, French, and Korean). Note that we should ensure that the ambiguity in English does not carry over into the translated targets. We here design a cooperative framework between machines and humans for pun translation. Per each language pair (e.g., \\\\( \\\\text{En} \\\\rightarrow \\\\text{De} \\\\)), we recruit a bilingual worker whose native language is the target language (e.g., \\\\( \\\\text{De} \\\\)). First, we use off-the-shelf translation models to generate three candidates. Then, the human workers select the best one and make further modifications to finalize the translation. This machine-assisted translation aligns with common practices in the industry (Federico et al., 2012). We chose machine-human cooperation for two reasons: firstly, we saw that our human translators find pun translation difficult. Machine suggestions can serve as starting points here. Secondly, this method expedited the annotation process and reduced costs.\\n\\nAddressing Lingering Ambiguity. Certain cases arise where the ambiguity in the source language is retained in the translated text in literal translation. For example, consider the sentence: \\\"A baseball player was a thief. He was always trying to steal.\\\" The pun in this sentence relies on the dual meanings within the pun.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of \\\"steal\\\"\u2014\\\"to take without permission\\\" and \\\"to steal a base in baseball.\\\" The challenge in translation is twofold: Some languages contain equivalent idiomatic expressions (e.g., \\\"stehlen\\\" in German), which can result in similar ambiguities in the target text. To address this, translators were instructed to select alternative words that avoid unintended double meanings whenever possible. The pun's humor is implied contextually within the first sentence, even if the pun word itself is not explicitly mentioned. For such instances, indirect translations were permitted, allowing human translators to render distinct interpretations of the pun without preserving its exact wording. To further refine the outputs, we applied text-based deduplication to eliminate closely matching translations. Refer to appendix B for more details.\\n\\n2.3 Dataset Analysis\\n\\nOur pipeline yields a dataset comprising 500 homographic and 500 heterographic pun sentences, each accompanied by one pun explanation image, two pun disambiguator images, and translations to three languages.\\n\\nHow natural are the generated images? Given the limited availability of real-world images accurately depicting puns, we opted to use AI-generated visuals. To gauge the difference between generated and authentic images, we conducted two human evaluation studies, comparing our generated images against natural image-pun pairs sourced from the web (https://www.reddit.com/r/puns/).\\n\\nIn the first study, human evaluators were asked to identify the correct text pun associated with each image from a set of potential matches. Results showed that natural images achieved an accuracy of 86%, while our generated images achieved a slightly higher accuracy of 92%. This test was conducted using a set of 50 randomly selected images. In the second study, we conducted an A/B comparison to assess the perceived naturalness of the images. To ensure consistency, natural images containing multiple panels, written text, or well-known characters were excluded from the evaluation. Across three independent evaluators, the naturalness test resulted in accuracy rates of 66%, 72%, and 74%, respectively, using another set of 50 random images. Overall, despite slight distributional differences between the generated and natural pun images, the disparity is considered acceptable. These findings indicate that evaluations performed within our benchmark can be reasonably extrapolated to real-world settings.\\n\\n| Metric | Translation Homo | Translation Hetero |\\n|--------|-----------------|--------------------|\\n| Win Rate (%) | Plain 90.7 | Pun-aware 9.3 |\\n| Score (Average) | Plain 94 | Pun-aware 88.8 |\\n\\nTable 3: Statistical differences between unconditional translation and pun-aware translation, averaged across languages. Text similarity was evaluated using BERTScore (Zhang et al., 2019).\\n\\nCommon vs. uncommon meanings. In UNPIE, each sample contains a pun phrase with two distinct meanings. This section explores how the popularity, or frequency, of each meaning influences downstream performance. To investigate, we rank the meanings of each word by their frequency using zero-shot GPT-4. To ensure the accuracy of GPT-4's assessments, we cross-reference these with human-annotated frequency data from Rice et al. (Rice et al., 2019), which includes 890 homonyms with annotated frequencies. The lower section of Table 2 compares GPT-4's frequency rankings with the human-annotated ground truth.\\n\\nNext, using GPT-4, we categorize our data into two groups based on more and less frequent meanings. This categorization is then analyzed through the pun reconstruction task outlined in section 3. As illustrated in the upper part of Table 2, the pun reconstruction task reveals that inputs with common meanings present more challenges than those with uncommon ones when using GPT-4. This suggests that texts with an uncommon meaning supplement the model's inherent understanding of the more frequent meaning.\\n\\nHow different are disambiguated translations from unconditional ones? When disambiguation is enforced as a strict criterion, the resulting translations are expected to differ from straightforward, unconditional translations. To quantify the extent of this difference, we compare the unconditional translation \\\\( \\\\hat{y}_0 \\\\) against two baselines: (1) another unconditional translation produced by a different annotator \\\\( \\\\hat{y}_1 \\\\), and (2) the disambiguated translation \\\\( y \\\\). We measure text similarity scores for each pair:\\n\\n\\\\[\\ns_1 = \\\\text{sim}(\\\\hat{y}_0, \\\\hat{y}_1)\\n\\\\]\\n\\nand\\n\\n\\\\[\\ns_2 = \\\\text{sim}(\\\\hat{y}_0, y)\\n\\\\]\\n\\nand compute the win rate as the proportion of cases where \\\\( s_2 \\\\) exceeds \\\\( s_1 \\\\). The results, summarized in Table 3, show that although disambiguation instructions lead to noticeable changes, the overall...\"}"}
{"id": "emnlp-2024-main-144", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Results on the pun grounding task. We report the exact match accuracy of the generated pun phrase. \u2191 denotes the performance gain from visual context.\\n\\n3 Task Overview\\n\\nWe pose three multimodal pun understanding tasks on the collected annotations to test models\u2019 capability to use visual context in addressing lexical ambiguity, as illustrated in Figure 2. Each task evaluates different aspects: the easier Pun Grounding task can be solved without image input. It is aimed at determining if less advanced models, which might not fully resolve such challenges, can enhance their performance with added visual information. The second task of pun disambiguation is designed to necessitate the usage of visual context. Finally, the pun reconstruction task replicates a practical multimodal literacy scenario. This task necessitates that models not only use the given translation but also infer or extract the underlying pun meaning that the translation does not explicitly convey, potentially drawing on visual inputs to do so.\\n\\nPun Grounding. The first step in understanding a pun is to identify it. Our initial task examines whether visual context aids models in identifying pun phrases within sentences. Given the whole English sentence $x_i = [x_{i0}, \\\\ldots, x_{it}]$ containing a pun phrase $s_i = [x_{ik}, \\\\ldots, x_{il}]$ and its corresponding pun explanation image $v_i$, the model returns a pun phrase candidate $\\\\bar{s}_i$. Note that while the actual target phrase $s_i$ is part of the full sentence $x_i$, the model\u2019s output $\\\\bar{s}_i$ is not bound by this constraint. We purposefully formulate this task as a sequence-to-sequence problem to facilitate zero-shot evaluation across various baselines. The model\u2019s output is then assessed for exact text match with the actual pun phrase to determine accuracy.\\n\\nPun Disambiguation. Once models pinpoint a pun\u2019s location, they must then interpret its semantics. Understanding a pun hinges on recognizing the different meanings of the pun phrase, as its humor lies in this ambiguity. In this task, we assess the models\u2019 proficiency in correlating each meaning of the pun with its associated visual context. Given the English sentence $x_i$ and the pun disambiguator image $v_i$ aligned with one of the meanings constructing the pun, the model should produce a translation of the sentence into a target language (e.g., German $\\\\bar{y}_{iDe}$). Notably, the translated text should be free of any ambiguity stemming from the pun, closely aligning with the meaning depicted in the provided image. We compare the model-generated translation $\\\\bar{y}_{ijDe}$ with two translation targets $y_{i,0De}, y_{i,1De}$, each corresponding to a different meaning of the pun. The model\u2019s output is considered correct if it more closely resembles the ground-truth translation $y_{ijDe}$ that corresponds to the meaning depicted in the image $v_{ijd}, j \\\\in 0, 1$.\\n\\nRefer to section 4.3 for the implementation details.\\n\\nPun Reconstruction. The final task is to reconstruct the complete pun sentence. To make the problem deterministic, we provide two types of inputs to the model: a non-English language translation of the original pun sentence that has been clarified of any ambiguities (e.g., German $y_{i,jDe}$) and the related pun explanation image $v_i$. The model then generates an output $\\\\bar{x}_i$, which we compare with the original English pun sentence $x_i$ to determine if both English sentences encapsulate the same pun. It is a complex task to determine whether two sentences contain the same pun, and we resort to machine-based evaluation with GPT-4 to obtain the binary decision. We verify GPTEval\u2019s validity here using human evaluation in appendix D.\\n\\n4 Experiments on UNPIE benchmark\\n\\n4.1 Models\\n\\nLM. To measure the effectiveness of multimodal modeling, we establish baselines using unimodal text-only language models. We incorporate an open-source model (Vicuna-13B (Chiang et al., 2023)) and the advanced proprietary language model (GPT-4 (OpenAI, 2023)). Furthermore, we appropriate a visual-language model, LLaV A, for a text-only scenario by inputting only text prompts without the images. This approach assesses the concept of multimodal alignment tax (Chen et al., 2023) in the context of pun interpretation, implying that fine-tuning a model on visual data might...\"}"}
{"id": "emnlp-2024-main-144", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Experimental results on the pun disambiguation task. All scores are reported in terms of binary classification accuracy. The best scores are bolded and the second-best ones are underlined.\\n\\nImpair its original linguistic capabilities. We do not test LM baselines against pun disambiguation as the task necessitates visual context.\\n\\nSM (Socratic Models). SM (Zeng et al., 2022), also called pipelining (Bitton-Guetta et al., 2023), is a two-staged framework extending text-only LMs to multimodal tasks by first encoding the multimodal context to textual descriptions. To implement SMs, we employ the same language models as previously mentioned and use BLIP-2 OPT2.7B (Li et al., 2023) as the visual description generator to encode the images into textual captions.\\n\\nVLM. Monolithic visual-language models directly take the raw images and user queries as inputs to produce textual responses. We employ two popular and high-performing VLMs for this purpose: LLaV A 1.5 13B (Liu et al., 2023a) and Qwen-VL-Chat 7B (Bai et al., 2023). (We refer to Qwen-VL-Chat as Qwen-VL in result tables due to space constraints.) For the tasks of pun disambiguation and pun reconstruction, we also introduce a machine translation baseline. We thus fine-tune LLaV A with the Multi30k multimodal machine translation dataset (Elliott et al., 2016), yielding the LLaV A-MMT variant. We choose LoRA (Hu et al., 2021) over full fine-tuning for efficient implementation.\\n\\n4.2 Do Images Help Pun Grounding?\\n\\nMetrics. We report accuracy based on the equality of the model-estimated pun phrase and the ground-truth pun phrase. To check the equality, we use the exact match of the surface text form and report the accuracy of the outputs.\\n\\nResults. As anticipated, the incorporation of visual context led to a consistent improvement in pun grounding performance across all models, including Socratic Models and Visual-Language Models (refer to Table 4). Also, GPT-4, a stronger model, could solve the task even without visual context, verifying our original intention of proposing this task to test the helpfulness of visuals where the task is straightforward but the models are less capable. For evaluation fairness, we employed a standard prompt template across all models (details in appendix E). Note that while careful prompt engineering can further improve the scores, our findings focus on understanding the role of visual context in realistic scenarios rather than extracting the maximum potential from each model.\\n\\n4.3 Can VLMs Disambiguate with Images?\\n\\nMetrics. We conduct a generative evaluation for the pun disambiguation test. The task for the machines is to translate a given pun sentence into a target language, using the accompanying image as a guide to disambiguate the meaning of the pun phrase. In this generative test, the model generates a sequence of text, which is then evaluated against two potential translation targets. The model's output is considered accurate if it aligns more closely with the translation that corresponds to the context of the provided image. We use BERTScore (Zhang et al., 2019) to measure the text similarity following the human evaluation results in appendix C.\\n\\nResults. All the considered baselines have demonstrated their ability to disambiguate translation outputs based on visual context, as illustrated in Table 5. Both strengthening the language model (Vicuna vs. GPT-4) and improving visual context processing (Vicuna with image captions from BLIP-2 vs. LLaV A) led to more accurate disambiguation. Still, comprehending puns in the textual form was a more decisive factor for pun disambiguation than a stronger visual understanding, as GPT-4 with image captions outperforms all other models. Interestingly, fine-tuning with the Multi30k multimodal machine translation dataset (Elliott et al., 2016) as a baseline improves performance further.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: Outcomes for the pun reconstruction task, where \u2191 and \u2193 signify the performance change attributed to the inclusion of visual context. The model with the largest performance increase is marked bold in each language.\\n\\n| Model        | Inputs | Correct (%) | Bleu-4 | METEOR | Correct (%) | Bleu-4 | METEOR | Correct (%) | Bleu-4 | METEOR |\\n|--------------|--------|-------------|--------|--------|-------------|--------|--------|-------------|--------|--------|\\n| LM Vicuna L  |        | 27.9        | 28.8   | 56.6   | 16.0        | 29.1   | 65.1   | 22.0        | 29.0   | 60.9   |\\n| GPT-4 L      |        | 43.1        | 30.1   | 66.1   | 45.2        | 30.7   | 70.9   | 44.2        | 30.4   | 68.5   |\\n| Qwen-VL L    |        | 30.3        | 29.4   | 58.8   | 20.3        | 30.0   | 66.7   | 25.3        | 29.7   | 62.8   |\\n| LLaV A L     |        | 31.7        | 27.7   | 57.9   | 19.0        | 29.9   | 65.6   | 25.4        | 28.8   | 61.8   |\\n| SM Vicuna V + L |    | 35.0 (\u2191 7.1) | 25.6 | 51.7 | 19.1 (\u2191 3.1) | 26.3 | 57.5 | 27.1 (\u2191 5.1) | 26.0 | 54.6 |\\n| GPT-4 V + L  |        | 62.9 (\u2191 19.8) | 29.8 | 65.5 | 45.9 (\u2191 0.7) | 30.7 | 68.5 | 54.4 (\u2191 10.2) | 30.3 | 67.0 |\\n| VLM Qwen-VL V + L | | 34.3 (\u2191 4.0) | 28.5 | 54.2 | 19.9 (\u2193 0.4) | 29.7 | 58.2 | 27.1 (\u2191 1.8) | 29.1 | 56.2 |\\n| LLaV A V + L |        | 33.2 (\u2191 1.5) | 28.7 | 55.1 | 20.1 (\u2191 1.1) | 29.2 | 61.2 | 26.7 (\u2191 1.3) | 26.0 | 58.2 |\\n| GPT-4 V + L  |        | 65.2 (\u2191 22.1) | 29.9 | 63.8 | 50.6 (\u2191 5.4) | 29.3 | 65.3 | 57.9 (\u2191 13.7) | 29.6 | 64.6 |\\n| LLaV A-MMT V + L | | 27.0 | 12.3 | 38.1 | 31.5 | 25.6 | 45.7 | 29.3 | 18.5 | 41.9 |\\n\\n**4.4 Do Images Help Pun Reconstruction?**\\n\\n**Metrics.** The pun reconstruction task involves machines using both the human-translated text and the image context to recreate the original pun sentence. Then, the reconstructed pun is compared with the original sentence for consistency in puns. Still, determining whether two sentences share the same pun is a complex task. To tackle this, we use a machine-based evaluation method with GPT-4 (OpenAI, 2023) to determine if the puns in both sentences are equivalent. To ensure the validity of this approach, known as GPTEval, we further compare it with human annotations in appendix D. Additionally, we report on common text evaluation metrics, such as Bleu-4 (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005)\u2014metrics widely used in the machine translation domain.\\n\\n**Results.** The results in Table 6 affirm that visual context significantly enhances machines' ability to reconstruct puns and manage their inherent ambiguity. For all tested models, the inclusion of images consistently improved the accuracy of pun reconstruction. The only exception was the weakest model in both language processing and visual comprehension (SM based on Vicuna). Notably, unlike the main metric of correctness, the automatic text evaluation scores (Bleu-4 and METEOR) did not reflect a clear trend. Through manual inspection of the generated outputs, we saw that such scores were more aligned with changes in the surface form.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the text, which did not necessarily correlate with the accurate identification of puns. This resonates with previous reports stating that such text scores are not fully effective outside of their original domain of machine translation (Liu et al., 2016).\\n\\nDue to the differences in their forms, models found it more challenging to reconstruct heterographic puns than homographic ones. Notably, incorporating visual context in these more complex scenarios led to significant improvements. Furthermore, the benefit of visual context became even more evident when dealing with Korean inputs; a language typically considered more divergent from English than either German or French. This reinforces the idea that machines depend more on visual cues when tackling complex linguistic tasks. Finally, as in the pun disambiguation task, the fine-tuned LLaVA-MMT suffered from a decline in performance compared to the zero-shot LLaVA. This further supports the notion that visual understanding is necessary to handle UNPIE.\\n\\n5 Related Work\\n\\nMultimodal Machine Translation. By integrating backtranslation as a downstream task, UNPIE contributes to the literature on Multimodal Machine Translation (MMT), a widely studied area that extends neural machine translation with additional visual contexts (Specia et al., 2016; Elliott et al., 2017; Barrault et al., 2018). Previous research argues that visual information can help resolve ambiguities in the source text (Li et al., 2022; Hatami et al., 2022). However, the primary dataset for MMT, Multi30K (Elliott et al., 2016), has limited examples of such ambiguities, leading to questions about the use of MMT for assessing multimodal literacy capacity (Elliott, 2018; Wu et al., 2021; Futeral et al., 2023). Another benchmark counteracts this phenomenon with manual annotation (Futeral et al., 2023; Bawden et al., 2018). Nevertheless, this dataset is relatively small (155 samples) due to the difficulty in pinpointing ambiguities within sentences. Additionally, the benchmark is limited to classification models.\\n\\nComputational Pun Understanding. After early research (Ritchie, 2005) pointed out ambiguity as a key in pun generation, numerous studies have investigated automatic pun generation regarding heterographic puns, which slackens the surface form identity requirement for each meaning of the pun (He et al., 2019; Yu et al., 2020; Mittal et al., 2022). Other research explored homographic pun generation which is based on multiple meanings of a polysemous word (Yu et al., 2018; Luo et al., 2019; Tian et al., 2022). Recently, Sun et al. (Sun et al., 2022) extended the pun generation problem to consider contextual cues. We extend this line of research with multimodal understanding.\\n\\nVisual-Language Models. The field has seen rapid growth since Flamingo (Alayrac et al., 2022) illustrated the advantages of applying large language models to the visual domain. BLIP-2 (Li et al., 2023), utilizing the OPT language model (Zhang et al., 2022), made significant strides in image captioning. The introduction of a stronger language model (Touvron et al., 2023) further enabled prompt-based control of the models. MiniGPT-4 (Zhu et al., 2023) and LLaVA (Liu et al., 2023b) pioneered the field of visual instruction tuning. InstructBLIP (Dai et al., 2023), an extension of BLIP-2, improved its capability to follow instructions more accurately. Further developments in this domain include other models such as LLAMA-Adapter (Zhang et al., 2023) and Qwen-VL (Bai et al., 2023). Our research puts visual language models (VLMs) to the test regarding their multimodal literacy capabilities.\\n\\n6 Conclusion\\n\\nWe introduced UNPIE, a new benchmark for the multimodal literacy capability. Based on UNPIE, we craft three tests to measure how machines can utilize visual context to resolve inherent ambiguity in puns. Our findings indicate that machines can indeed leverage visual information to enhance their understanding of text, as shown by their improved performance across all tasks. However, achieving human proficiency in multimodal literacy is still a challenge. While our results are encouraging, there remains a considerable gap in machine capability to fully grasp and interpret the intricate relationship between text and visuals, particularly in more complex tasks like pun reconstruction. Therefore, we envision UNPIE as not only a platform for testing but also as a starting point for the development of future multimodal models to actively navigate and integrate information from multiple modalities.\\n\\nLimitations and Ethical Considerations\\n\\nUNPIE, while being a multilingual dataset, is built on the English-only pun corpus (Miller et al., 2017).\"}"}
{"id": "emnlp-2024-main-144", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As such, it primarily models lexical ambiguities unique to English, stemming from polysemies or similar surface forms of the language. To enhance its linguistic diversity and applicability, expanding the dataset to include ambiguities inherent in other languages would be beneficial. Such expansion would not only diversify the linguistic challenges in the dataset but also offer deeper insights into how lexical ambiguities manifest differently across various languages and cultures.\\n\\nAlthough UNPIE's size is much larger than that of the previous multimodal literacy dataset that features explicit ambiguities (Futeral et al., 2023), its total size is insufficient for creating a training split suitable for fine-tuning. This limitation stems from the scarcity of puns, which are inherently challenging for humans to create as well and are not readily available in large quantities online. We thus plan to expand the dataset for multilingual puns in the future.\\n\\n**Ethical Considerations.** UNPIE, constructed using existing English puns, may inadvertently perpetuate cultural biases and stereotypes present within the humor. Although human annotators were instructed to eliminate any puns expressing explicit hatred, subtle biases can still be perpetuated through seemingly innocuous humor.\\n\\nTo address ethical concerns in the data curation process, we confirmed that all human annotators either volunteered willingly or were compensated fairly for their contributions. We defer the details to appendix B.\\n\\n**Acknowledgment.** This work was partly supported by an IITP grant funded by the Korean Government (MSIT) (No. RS-2020-II201361, Artificial Intelligence Graduate School Program (Yonsei University) and RS-2024-00353131) and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIT) (No. RS-2024-00354218).\\n\\n**References**\\n\\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katherine Millican, Malcolm Reynolds, et al. 2022. Flamingo: a visual language model for few-shot learning. *Advances in Neural Information Processing Systems*, 35:23716\u201323736.\\n\\nJinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-vl: A frontier large vision-language model with versatile abilities. *arXiv* preprint arXiv:2308.12966.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In *Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization*, pages 65\u201372.\\n\\nLo\u00af\u0131c Barrault, Fethi Bougares, Lucia Specia, Chiraag Lala, Desmond Elliott, and Stella Frank. 2018. Findings of the third shared task on multimodal machine translation. In *THIRD CONFERENCE ON MACHINE TRANSLATION (WMT18)*, volume 2, pages 308\u2013327.\\n\\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In *Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)*, pages 1304\u20131313.\\n\\nRomain Beaumont. 2022. Clip retrieval: Easily compute clip embeddings and build a clip retrieval system with them. *https://github.com/rom1504/clip-retrieval*.\\n\\nJames Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. 2023. Improving image generation with better captions.\\n\\nNitzan Bitton-Guetta, Yonatan Bitton, Jack Hessel, Ludwig Schmidt, Yuval Elovici, Gabriel Stanovsky, and Roy Schwartz. 2023. Breaking common sense: Whoops! a vision-and-language benchmark of synthetic and compositional images. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 2616\u20132627.\\n\\nDelong Chen, Jianfeng Liu, Wenliang Dai, and Baoyuan Wang. 2023. Visual instruction tuning with polite flamingo. *arXiv preprint arXiv:2307.01003*.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\\n\\nChenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. *arXiv preprint arXiv:2311.03287*.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"general-purpose vision-language models with instruction tuning. In Thirty-seventh Conference on Neural Information Processing Systems. Desmond Elliott. 2018. Adversarial evaluation of multimodal machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2974\u20132978. Desmond Elliott, Stella Frank, Lo\u00efc Barrault, Fethi Bougares, and Lucia Specia. 2017. Findings of the second shared task on multimodal machine translation and multilingual image description. arXiv preprint arXiv:1710.07177. Desmond Elliott, Stella Frank, Khalil Sima'an, and Lucia Specia. 2016. Multi30k: Multilingual english-german image descriptions. arXiv preprint arXiv:1605.00459. Marcello Federico, Alessandro Cattelan, and Marco Trombetti. 2012. Measuring user productivity in machine translation enhanced computer assisted translation. In Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers. Association for Machine Translation in the Americas. Matthieu Futeral, Cordelia Schmid, Ivan Laptev, Beno\u00eet Sagot, and Rachel Bawden. 2023. Tackling ambiguity with images: Improved multimodal machine translation and contrastive evaluation. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5394\u20135413. Danfeng Guo, Arpit Gupta, Sanchit Agarwal, Jiun-Yu Kao, Shuyang Gao, Arijit Biswas, Chien-Wei Lin, Tagyoung Chung, and Mohit Bansal. 2022. Gravlt: Graphical visual-linguistic representations for multimodal coreference resolution. In Proceedings of the 29th International Conference on Computational Linguistics, pages 285\u2013297. Rishav Hada, Varun Gumma, Adrian de Wynter, Harshita Diddee, Mohamed Ahmed, Monojit Choudhury, Kalika Bali, and Sunayana Sitaram. 2023. Are large language model-based evaluators the solution to scaling up multilingual evaluation? arXiv preprint arXiv:2309.07462. Ali Hatami, Paul Buitelaar, and Mihael Arcan. 2022. Analysing the correlation between lexical ambiguity and translation quality in a multimodal setting using wordnet. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop, pages 89\u201395. He He, Nanyun Peng, and Percy Liang. 2019. Pun generation with surprise. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1734\u20131744. Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations. Satwik Kottur, Seungwhan Moon, Alborz Geramifard, and Babak Damavandi. 2021. Simmc 2.0: A task-oriented dialog dataset for immersive multimodal conversations. arXiv preprint arXiv:2104.08667. Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597. Yi Li, Rameswar Panda, Yoon Kim, Chun-Fu Richard Chen, Rogerio S Feris, David Cox, and Nuno Vasconcelos. 2022. Valhalla: Visual hallucination for machine translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5216\u20135226. Chia-Wei Liu, Ryan Lowe, Iulian Vlad Serban, Mike Noseworthy, Laurent Charlin, and Joelle Pineau. 2016. How not to evaluate your dialogue system: An empirical study of unsupervised evaluation metrics for dialogue response generation. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2122\u20132132. Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023a. Improved baselines with visual instruction tuning. arXiv preprint arXiv:2310.03744. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023b. Visual instruction tuning. arXiv preprint arXiv:2304.08485. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. Gpte-vval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634. Fuli Luo, Shunyao Li, Pengcheng Yang, Lei Li, Baobao Chang, Zhifang Sui, and Xu Sun. 2019. Pun-gan: Generative adversarial network for pun generation. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3388\u20133393. Tristan Miller, Christian F Hempelmann, and Iryna Gurevych. 2017. Semeval-2017 task 7: Detection and interpretation of english puns. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 58\u201368. Kathy A Mills and Len Unsworth. 2017. Multimodal literacy. Oxford University Press. Anirudh Mittal, Yufei Tian, and Nanyun Peng. 2022. Ambipun: Generating humorous puns with ambiguous context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1053\u20131062.\"}"}
{"id": "emnlp-2024-main-144", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-144", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hyperparameters & Setup\\n\\nModels. In all GPT-4 (OpenAI, 2023) usage, we use the gpt-4-0613 endpoint. When conducting experiments with open-source models, we leverage the official implementation codes in conjunction with publicly available weights from the Huggingface Hub (https://huggingface.co/models). For our work with the Vicuna (Chiang et al., 2023) language model, we employed the lmsys/vicuna-7b-v1.5 endpoint. Additionally, for the LLaVA 1.5 13B (Liu et al., 2023a) and Qwen-VL-Chat (Bai et al., 2023) visual-language models, we used the following model parameters: liuhaotian/llava-v1.5-13b and Qwen/Qwen-VL-Chat, respectively.\\n\\nText Generation. We use deterministic greedy sampling for all experiments, introducing no randomness or external hyperparameter in the text generation process. Except for GPT-4, all models were allowed to generate up to 200 tokens with the freedom of an early stopping.\\n\\nComputational Resources & Fine-tuning. We used the OpenAI API for inferring GPT-4 (OpenAI, 2023) outputs. For open-source models such as Vicuna (Chiang et al., 2023), LLaVA (Liu et al., 2023a), and Qwen-VL-Chat (Bai et al., 2023), we use a single NVIDIA A100 40GB GPU for inference. While the exact inference speed varies depending on the length of prompts and responses, a query takes about \\\\( \\\\sim 0.8 \\\\) seconds to terminate when utilizing batch processing. Fine-tuning LLaVA was also possible in a single A100 40GB GPU thanks to the efficient LoRA-based implementation (Hu et al., 2021). We trained each translation model for ten epochs in the training split of the Multi30k dataset (Elliott et al., 2016) with early stopping, which took \\\\( \\\\sim 20 \\\\) hours on average.\\n\\nB Data Collection Details\\n\\nGenerating Pun Explanation Images. We rely on the DALL-E 3 (Betker et al., 2023) image-to-text model to generate images that explain both meanings of the pun at the same time. DALL-E 3 can be accessed either from the GPT-4 (https://chat.openai.com/) or Bing Image Generator (https://www.bing.com/create) web interface. Our annotators employed both interfaces. In contrast to previous studies that employed designers for image generation tasks involving machine-human collaboration (Bitton-Guetta et al., 2023), we chose to recruit three NLP researchers to generate images with DALL-E 3 and curate the outputs. This decision was driven by the necessity for an accurate representation of the puns' meanings rather than the artistic quality of the images, considering the specific requirements of our research task. The NLP researchers participated voluntarily in this annotation task.\\n\\nTranslating Puns. To streamline the translation process while ensuring clarity in meaning, we adopted a machine-assisted translation approach. This method simplifies the task for human annotators, who are required to ensure that the translations clearly reflect the intended meaning of the text. Initially, we provide three machine-generated translation options for the annotators to select and refine. These options are created using GPT-4, which we prompt in two distinct ways, and DeepL (https://www.deepl.com/translator), a proprietary translation service. Human annotators then use these machine-provided translations as a base to craft the final version of the translated pun sentence, as depicted in the user interface shown in Figure 5. For each of the three target languages (German, French, and Korean), we engaged a native speaker who is also proficient in English, ensuring both linguistic accuracy and fidelity to the original pun's meaning. The extent for how much the translations written by human annotators were drifted from unconditional translation is reported in Table 7. We pay each annotator 12\u201315$ per hour. We have received approval from the university department and conducted data collection.\\n\\nC Testing the Validity of BERTScore\\n\\nIn the Pun Disambiguation task, we require the models to generate the disambiguated text. Thus, we need an automatic algorithm to decide whether the generated output aligns with the intended pun meaning. We formulate this as a text-only problem of matching the output with the ground-truth disambiguated translation result. We consider various options here: three translation metrics (Bleu, METEOR, and Rouge-L) and two model-based metrics (BERTScore (Zhang et al., 2019) and GPT-based Evaluation). Note that GPTEval here does not receive images as inputs following the other options. Given the generation output of a VLM (LLaVA) or a Socratic Model (GPT4), we ask human annotators for a ternary classification: Match, No Match, and Invalid. A\"}"}
{"id": "emnlp-2024-main-144", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Statistical differences between unconditional translation and pun-aware translation. Text similarity was evaluated using BERTScore (Zhang et al., 2019).\\n\\n| Method       | Acc (%) | \u03d5       |\\n|--------------|---------|----------|\\n| BLEU         | 84      | 57       |\\n| METEOR       | 82      | 52       |\\n| Rouge-L      | 84      | 41       |\\n| BERTScore    | 93      | 81       |\\n| GPTEval      | 76      | 41       |\\n\\nTable 8: We compare each metric with human judgments on a set of 100 samples. \u03d5 denotes the Phi correlation coefficient.\\n\\nThe results show that BERTScore greatly improves over the traditional translation metric baselines. As the disambiguation of a pun sentence typically lies in the correct translation of salient phrases, conventional metrics without semantic understanding are not sufficient for the task. On the other hand, BERTScore shows an acceptable correlation with human annotations. We thus employ it as the metric for the Pun Disambiguation task.\\n\\nPerhaps surprisingly, the strong LLM backbone of GPTEval yields the worst outcome. Upon qualitative examination, we saw that GPT tends to favor certain targets regardless of the input. We leave alleviating this bias of GPTEval to future research.\\n\\nD Testing the Validity of GPTEval\\n\\nTo ensure the reliability of our machine-based evaluation method, we conducted a comparative analysis with human assessments. This involved manually annotating 100 sample outputs from a multimodal approach (GPT-4 with image captions) and a text-only approach (GPT-4). The findings in Table 9 indicate that GPT-4's evaluations can be considered dependable for assessing the performance of machine-generated outputs in the pun reconstruction task.\\n\\nE Prompt Templates\\n\\nE.1 Pun Grounding\\n\\n\u2022 LM\\n\\n###Variables\\n\\nPUN_SENTENCE\\n\\n###PROMPT\\n\\n[sentence]: {PUN_SENTENCE}\\n\\nThis is a pun sentence. Identify the \u21aa\u2192 specific word or phrase \u21aa\u2192 that creates the pun. Respond with \u21aa\u2192 only the word or phrase \u21aa\u2192 that makes it a pun, without any explanation.\\n\\n[answer]:\\n\\n\u2022 SM or VLM\\n\\n###Variables\\n\\nPUN_SENTENCE, IMAGE\\n\\n###PROMPT\\n\\n[sentence]: {PUN_SENTENCE}\\n\\nThis is a pun sentence. Identify the \u21aa\u2192 specific word or phrase \u21aa\u2192 that creates the pun, given the image \u21aa\u2192 as context \"{IMAGE}\". Respond with only the word or phrase \u21aa\u2192 that makes it a pun, without any explanation.\\n\\n[answer]:\\n\\nE.2 Pun Disambiguation\\n\\n\u2022 SM or VLM\"}"}
{"id": "emnlp-2024-main-144", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Test results of machine-based evaluation scheme using GPT-4 in the pun reconstruction task. We compare GPT-4's decision with human judgments on a set of 100 samples. $\\\\rho$ denotes the Pearson correlation coefficient.\\n\\n### Variables\\n- PUN_SENTENCE, IMAGE, LANGUAGE\\n- PUN_SENTENCE_IN_LANGUAGE, LANGUAGE\\n- IMAGE\\n\\n### Prompt\\n- **[English]:** {PUN_SENTENCE}\\n  - Translate the given English sentence into {LANGUAGE}, given the image as context \"{IMAGE}\". Please respond using the format below:\\n  - [{LANGUAGE}]:\\n- **[Image Description]:** {IMAGE}\\n  - This image description is about two meanings of the word that you are expected to create. Use this information to craft your pun-inclusive English translation.\\n  - Please respond using the format below:\\n    - [English]:\\n\\n**E.3 Pun Reconstruction**\\n\\n- **\u2022 LM**\\n  - **[Image Description]:** {IMAGE}\\n\\n- **\u2022 SM or VLM**\"}"}
{"id": "emnlp-2024-main-144", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5: A screenshot of the human annotation interface for pun-aware text translation.\"}"}
