{"id": "lrec-2024-main-1474", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"UMTIT: Unifying Recognition, Translation, and Generation for Multimodal Text Image Translation\\nLiqiang Niu, Fandong Meng and Jie Zhou\\nPattern Recognition Center, WeChat AI, Tencent Inc, China\\n{poetniu, fandongmeng, withtomzhou}@tencent.com\\n\\nAbstract\\nPrior research in Image Machine Translation (IMT) has focused on either translating the source image solely into the target language text or exclusively into the target image. As a result, the former approach lacked the capacity to generate target images, while the latter was insufficient in producing target text. In this paper, we present a Unified Multimodal Text Image Translation (UMTIT) model that not only translates text images into the target language but also generates consistent target images. The UMTIT model consists of two image-text modality conversion steps: the first step converts images to text to recognize the source text and generate translations, while the second step transforms text to images to create target images based on the translations. Due to the limited availability of public datasets, we have constructed two multimodal image translation datasets. Experimental results show that our UMTIT model is versatile enough to handle tasks across multiple modalities and outperforms previous methods. Notably, UMTIT surpasses the state-of-the-art TrOCR in text recognition tasks, achieving a lower Character Error Rate (CER); it also outperforms cascading methods in text translation tasks, obtaining a higher BLEU score; and, most importantly, UMTIT can generate high-quality target text images.\\n\\nKeywords: Image Machine Translation, Multimodal, Text Recognition, Text Translation, Image Generation\\n\\n1. Introduction\\nCurrent Image machine translation (IMT) aims to translate an image containing text in the source language into target language text or a new image containing text in a specific target language. IMT can be widely applied to various types of images, such as scanned book photos, mobile reading screenshots, travel-shot road signboards, and photos taken in daily life.\\n\\nAs illustrated in Figure 1(a), traditional IMT relies on a cascaded system that combines Optical Character Recognition (OCR) (Li et al., 2022), Neural Machine Translation (NMT) (Vaswani et al., 2017) and a complex process of rendering the translated text back onto the source image. This approach carries the potential risk of error compounding between components and redundancy in model parameters. Fortunately, end-to-end models have emerged the dominant approach for natural language processing (NLP) and computer vision (CV) tasks, such as speech translation (Jia et al., 2019), text recognition (Li et al., 2022), and object detection (Carion et al., 2020).\\n\\nRecent advancements in end-to-end methods for IMT include In-Image Neural Machine Translation (IIMT) (Mansimov et al., 2020; Tian et al., 2023), ItNet (Jain et al., 2021), Text Image Translation (TIT) (Ma et al., 2022), and Document Image Translation (DIT) (Zhang et al., 2023). As depicted in Figure 1(b), Mansimov et al. (2020) pioneered an end-to-end model that directly translates text from single-line source images into images of the target language. However, this model faces challenges due to its reliance on convolutional networks and pixel-space operations, resulting in translated images within incomplete sentences and indistinct characters. Moreover, it cannot directly translate into the target text; instead, it requires post-processing OCR to extract text from the translated images. Similarly, the new end-to-end IIMT model introduced by Tian et al. (2023) is limited to single-line images, which is not representative of the more complex multi-line images encountered in real-world applications. Figures 1(c) and (d) showcase the end-to-end TIT frameworks proposed by Jain et al. (2021) and Ma et al. (2022), which aim to supplant traditional cascaded approaches that combine OCR with NMT. The primary distinction between the two frameworks is that the former accommodates multi-line text images, whereas the latter is restricted to single-line text images. Compared to earlier TIT models, the recent LayoutDIT model introduced by Zhang et al. (2023) incorporates the complex visual layout of document images to enhance understanding and translation. Although both TIT and DIT models are capable of translating source text images into text of the target language with high translation quality, they still lack the fundamental capability to generate images of the target text.\\n\\nIn this work, we propose UMTIT, a Unified Multimodal Text Image Translation model, as illustrated in Figure 1(e), which integrates text translation and image generation into a single model. Given a text image as input, UMTIT can translate it into the target language without the need for additional OCR and NMT models. Moreover, to address the shortcomings of ItNet and TIT, UMTIT is capable of...\"}"}
{"id": "lrec-2024-main-1474", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In image Machine Translation (IMT), including traditional Cascaded System (a), In-image Machine Translation (Tian et al., 2023)(b), Image Translation (Jain et al., 2021) (c), Text Image Translation (TIT) (Ma et al., 2022) (d), and our Multimodal Text Image Translation (e).\\n\\nUMTIT generates multi-line target images with consistent style and font. During the text translation process, UMTIT first encodes the image using a vision transformer (Dosovitskiy et al., 2021; Liu et al., 2021) and then decodes the translation with a text transformer (Vaswani et al., 2017; Liu et al., 2020) in an autoregressive manner. UMTIT is also flexible and can be easily extended to recognize the text in the source image.\\n\\nInspired by the success of text-to-image generation models like DALL-E (Ramesh et al., 2021), MUSE (Chang et al., 2023), and Stable Diffusion (Rombach et al., 2021), we adopt image tokenizers (Esser et al., 2021; Yu et al., 2021) to learn discrete representations of text images. This allows us to convert images into sequences of image tokens, or vice versa. The sequence of image tokens can be more conveniently integrated and modeled with transformer architecture and is faster compared to pixel space. In the process of image generation, UMTIT first encodes the translation using a character-level transformer, and then generates a sequence of image tokens autoregressively. Based on the generated image tokens, the image tokenizer can decode to generate a new target text image.\\n\\nDue to the lack of publicly available datasets for multimodal text image translation, we constructed two synthetic image datasets with multi-line text. One contains over 100,000 English-Germany images built with the WMT14 dataset, while the other one contains 30,000 Germany-English images built with the Multi30K dataset. Our experiments show that UMTIT outperforms the traditional cascaded systems and achieves higher BLEU scores for text translation, even with fewer model parameters. We also found that UMTIT outperforms the state-of-the-art TrOCR (Li et al., 2022) and achieves a lower Character Error Rate (CER) for text recognition task. Finally, UMTIT can generate high-quality multi-line target images with sharp character details.\\n\\nWe summarize our contributions as follows:\\n\\n- We propose the UMTIT model, the first model that can recognize text in images, translate the source image to the target language, and most importantly, generate high-quality multi-line target text images.\\n- To the best of our knowledge, UMTIT is the first model to apply image tokenizers from natural scene images to text images for image translation.\\n- We construct two synthesized datasets containing over 100,000 images for multimodal text image translation.\\n\\n2. Related Work\\n\\n2.1. Multimodal Machine Translation\\n\\nMultimodal Machine Translation (MMT), extends conventional text-to-text NMT by using an auxiliary imagemodality on the sourceside to translate texts into a target language. Elliott et al. (2016) propose a multimodal Multi30K dataset, enabling the development of MMT models. Yao and Wan (2020) introduces a multimodal transformer architecture. Tang et al. (2022) proposes image retrieval methods to collect descriptive images for bilingual parallel corpora using search engines. Pengetal. (2022) proposes a novel framework to support image-free inference. Existing methods focus on improving text-only translation by leveraging additional vision information on the source side, but MMT inherently cannot deal with text images.\\n\\n2.2. Image Machine Translation\\n\\nImage Machine Translation (IMT) aims to translate text within an image from the source language into an equivalent image containing the translated text.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in the target language. Traditionally, IMT has depended on a cascaded system involving text recognition, text-to-text translation, and a complex rendering process. This approach suffers from several drawbacks, including error propagation, parameter redundancy, and increased latency. To overcome these challenges, various end-to-end methods have been introduced. As an early effort, Mansimov et al. (2020) introduced an end-to-end in-image NMT model with a convolutional encoder-decoder architecture. This model, designed for simplicity, processes single-line text images and operates directly in pixel space. However, experiments have shown that the model struggles to produce complete target images and achieves an extremely low BLEU score. Echoing Mansimov et al. (2020), the new end-to-end IIMT model by Tian et al. (2023) is also limited to single-line text images, which does not reflect the complexity of multi-line images commonly found in real-world settings. Given the challenges associated with image-to-image translation models, some researchers have proposed Text Image Translation (TIT) methods as a replacement for the cascaded approach that combines OCR and NMT. For instance, Jain et al. (2021) introduced ItNet, an end-to-end neural network designed to translate text within images from one language to another. ItNet has demonstrated superior performance to cascaded systems in side-by-side human evaluations on a synthetic dataset. Additionally, Ma et al. (2022) presented a multi-task training framework, showing that integrating OCR and TIT tasks can further enhance translation performance. Moreover, LayoutDIT, proposed by Zhang et al. (2023), accounts for the complex visual layout of document images to improve comprehension and translation. Although current TIT and DIT models surpass cascaded systems in text translation quality, the challenge of generating target language images remains an open issue.\\n\\n2.3. Image Synthesis Models\\n\\nImage synthesis is an exciting computer vision field with significant recent developments and has been applied to various tasks, such as text-to-image generation, class-conditional synthesis, inpainting, and super-resolution. Previously proposed models like Generative Adversarial Networks (GAN) (Goodfellow et al., 2014), Variational Autoencoders (VAE) (Kingma and Welling, 2013), flow-based models (Dinh et al., 2014) and autoregressive models (ARM) (Chen et al., 2020) have their specific difficulties in training, sample quality, pixel space modeling, and high resolution synthesis. To address these shortcomings, recent two-stage approaches first learn compressed latent representations of images and then model a discrete image space instead of raw pixels. For example, DALL-E (Ramesh et al., 2021) builds a text-to-image model with a discrete VAE to compress images into image tokens and an autoregressive transformer to model the joint distribution over the text and image tokens. To better represent images, Esser et al. (2021) and Yu et al. (2021) propose new image tokenizers, including VQGAN and ViT-VQGAN. With VQGAN's help, MUSE (Chang et al., 2023) achieves state-of-the-art text-to-image performance with novel training paradigms of masked transformer modeling of image tokens. Additionally, Rombach et al. (2021) extend Diffusion Probabilistic Models (DM) (Sohl-Dickstein et al., 2015) to latent diffusion models (LDMs) and achieve new state-of-the-art scores for image inpainting and class-conditional image synthesis.\\n\\n2.4. Text Rendering Models\\n\\nAnother significant area of image synthesis is text rendering, which is primarily based on diffusion models and aims to generate well-formed visual text. TextDiffuser (Chen et al., 2023b) synthesizes text at specific locations within images through a two-stage process: the first stage generates the layout information for the text, and the second stage synthesizes the image by combining the mask and text prompt. This process is intricate, and the resulting character accuracy, as measured by OCR accuracy, is relatively low. In its latest iteration, TextDiffuser-2 (Chen et al., 2023a) utilizes a large language model to generate layout information, including words with bounding boxes. However, these methods have primarily shown effectiveness with short text. GlyphControl (Yang et al., 2023) facilitates the generation of long-text images but requires pre-specified GlyphInstructions to determine text and position information (glyph rendering). Liu et al. (2023) introduce character-aware models that modify the text encoder's tokenizer from subword-level to character-level, thereby improving Word exact-match accuracy. Nevertheless, the evaluation datasets (such as WikiSpell and DrawText) and the spelling samples provided by the authors indicate that the optimized text tends to be brief, often consisting of single words. This contrasts with our work, which focuses on generating images containing sentence-level translations. Interestingly, our UMTIT reaches a similar conclusion that character-level processing yields better results in generating target text images. Lotz et al. (2023) investigate various rendering strategies to convert sentences into sequences of image patches with continuous characters, aiming to improve the performance of pixel language models. In summary, all the text rendering works discussed here fall short in maintaining alignment with the source image, a key aspect that sets them apart from the image-to-image translation scenario.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Would you like to have more information about the hotel?\\n\\nM\u00f6chten Sie ausf\u00fchrliche Einzelheiten \u00fcber dieses Hotel?\\n\\nWould you like to have more information about the hotel?\\n\\nFigure 2: Overview of the UMTIT model, which integrates text recognition, translation and image generation into a single model. As an example of DE2EN multi-line text image translation, UMTIT can recognize the source text \u201cM\u00f6chten Sie ausf\u00fchrliche Einzelheiten \u00fcber dieses Hotel?\u201d and translate the image to the target text \u201cWould you like to have more information about the hotel?\u201d and most importantly, generate a new consistent English image.\\n\\n3. UMTIT Model\\n\\nAs shown in Figure 2, we propose UMTIT, which consists of two components: the left half is for recognition and translation of the source image, and the right half is for generation of the target image.\\n\\n3.1. UMTIT for Text Translation\\n\\nGiven a source text image $X$, the first goal of UMTIT is to generate a sequence of target language tokens $T = (t_1, t_2, ..., t_n)$ for text translation. For example, as shown in Figure 2, the input is a German image containing the source text \u201cM\u00f6chten Sie ausf\u00fchrliche Einzelheiten \u00fcber dieses Hotel?\u201d, UMTIT needs to translate it to the target (English) sentence \u201cWould you like to have more information about the hotel?\u201d Without the need for OCR and NMT models, UMTIT uses an end-to-end architecture consisting of a Transformer-based visual encoder and text decoder.\\n\\n3.1.1. Visual Transformer Encoder\\n\\nThe visual encoder converts the input RGB image $X \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ into a set of embeddings $E = (e_1, e_2, ..., e_h)$, where $e_i \\\\in \\\\mathbb{R}^d$, $h$ is the hidden feature map size or the number of image patches and $d$ is the dimension of hidden vectors. Transformer-based models (Carion et al., 2020; Dosovitskiy et al., 2021; Liu et al., 2021) have become the dominant approach for various computer vision tasks, compared to CNN-based models (He et al., 2016). In this work, we use the Swin Transformer (Liu et al., 2021) as the visual encoder.\\n\\n3.1.2. Text Transformer Decoder\\n\\nGiven the latent representation $E$ of the image learned by the visual encoder, the text decoder needs to generate a sequence of tokens $T = (t_1, t_2, ..., t_n)$, where $t_i \\\\in \\\\mathbb{R}^v$ is a one-hot vector indicating the $i$-th target token, $v$ is the vocabulary size, and $n$ is the maximum length of the sequence. Unlike BERT (Devlin et al., 2019), which has only an encoder, and GPT series (Radford et al., 2018, 2019; Brown et al., 2020) with only a decoder, BART (Lewis et al., 2020) uses a Transformer-based encoder-decoder architecture. Inspired by Kim et al. (2021), we use BART as the text decoder. Another advantage of the BART decoder is that we can easily modify it to accept arbitrary inputs with a cross-attention mechanism.\\n\\n3.2. UMTIT for Text Recognition\\n\\nWith a simple vision-encoder and text-decoder architecture, UMTIT is flexible and can be easily extended to generate arbitrary tokens. Specifically, given a source text image $X$, UMTIT can be modified to generate a sequence of $m$ source language tokens $S = (s_1, s_2, ..., s_m)$ and a sequence of $n$ target language tokens $T = (t_1, t_2, ..., t_n)$ simultaneously. For simplicity, we concatenate the source and target sequences into a merged sequence $M = (\\\\langle src \\\\rangle s_1, s_2, ..., s_m \\\\langle /src \\\\rangle \\\\langle tgt \\\\rangle t_1, t_2, ..., t_n \\\\langle /tgt \\\\rangle)$, where $\\\\langle src \\\\rangle$, $\\\\langle /src \\\\rangle$ and $\\\\langle tgt \\\\rangle$, $\\\\langle /tgt \\\\rangle$ are special tokens used to distinguish the source and target sequences, respectively. As a result, UMTIT can accomplish both text recognition and translation tasks simultaneously. As shown in Figure 2, given an input of German text image, UMTIT can generate a sequence including source German and target English tokens \u201c<src>M\u00f6chten Sie ausf\u00fchrliche Einzelheiten \u00fcber dieses Hotel?</src><tgt>Would you like to have\u201d\"}"}
{"id": "lrec-2024-main-1474", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"3.3. UMTIT for Text Image Generation\\n\\n3.3.1. Text Image Tokenizer\\nWith the help of image tokenizers, modern image generative models can directly learn in latent token space instead of raw pixels, enabling more effective training losses such as cross-entropy instead of regression. However, most existing image tokenizers (Ramesh et al., 2021; Esser et al., 2021; Yu et al., 2021) focus on natural scene images, which cannot be applied to textual images. In this work, we tokenize text images using ViT-VQGAN (Yu et al., 2021), which consists of a Vision-Transformer encoder and a Transformer decoder, with a quantization layer that maps an input image into a sequence of discrete tokens from a learned codebook. Given an image of resolution $H \\\\times W$, ViT-VQGAN encodes it into discretized latent codes of size $H/f \\\\times W/f$, where $f$ is the image patch size which can also be referred to as the downsampling ratio. For example, a $256 \\\\times 256$ RGB image can be encoded to $32 \\\\times 32$ and $16 \\\\times 16$ tokens with $f = 8$ and $f = 16$, respectively.\\n\\n3.3.2. Text Image Generation\\nAfter obtaining the translation $T = (t_1, t_2, ..., t_n)$, the second goal is to generate a new, consistent target image $Y$ that aligns with the source image $X$. Inspired by text-to-image models like MUSE (Chang et al., 2023) and Parti (Yu et al., 2022), we use a similar sequence-to-sequence architecture as shown in Figure 3. UMTIT first encodes the target text (e.g., an English sentence) with a character transformer, and then uses an autoregressive image-token transformer to generate a sequence of image tokens $I = (i_1, i_2, ..., i_p)$, where $i_i \\\\in \\\\mathbb{R}^w$ is a one-hot vector for $i$-th target image token, $w$ is the vocabulary and codebook size, and $p$ is the maximum number of tokens. Note that the image-token transformer decoder shares the vocabulary with codebook of the image tokenizer.\\n\\n4. Experiment\\n4.1. Datasets\\nCurrently, there is no public dataset available for Multimodal Text Image Translation (MTIT) tasks. Therefore, we have created two datasets to address this need.\\n\\n**MTIT-WMT-100K**: The construction process consists of two steps. First, we extract appropriate bilingual text from WMT14-en-de under certain filtering criteria. Next, we generate English and German text images based on the aligned English and German text. Ultimately, we collect over 100,000 pairs of bilingual text images, with an image resolution of $256 \\\\times 256$ to balance storage cost and image quality. The extraction of bilingual text is as follows: considering the limited size of the image, the extracted text must have a limited number of characters. The original WMT14-en-de contains over 400 million text pairs, we first select suitable text pairs filtered by character length with two conditions: (1) the source and target text's length must be in the range of $[1, 60)$; (2) the source text's length is less than the target text's length and vice versa. To maintain the quality of text pairs, we filter out text pairs with low-frequency words. Finally, we have 106,403 text pairs for training, and 1,166 text pairs for testing.\\n\\n**MTIT-Multi30K**: Multi30K is a public dataset for multilingual multimodal machine translation. We select the full de-en part with 29,000 training sentences, 1,014 validation sentences, and 1,000 test sentences to build a smaller dataset. Note that we do not filter out long sentences, therefore, MTIT-Multi30K text image resolution is set to $512 \\\\times 512$.\\n\\nTo generate high quality text images, we use the public Python Pillow package and the well-known Arial font. For each sentence, we evenly fill it in multiple rows of the image in a left-to-right, top-to-bottom order. For simplicity, all images use black text on a white background. More details of our synthetic datasets compared with related works are shown in Table 1.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of our synthetic datasets with related works. Mansimov et al. (2020) dataset including \\\\{SourceImage, TargetImage\\\\} is used for In-Image Translation, Jain et al. (2021) and Ma et al. (2022) datasets including \\\\{SourceImage, TargetText\\\\} are used for Text Image Translation (TIT). Our MTIT-WMT-100K and MTIT-Multi30K containing \\\\{SourceImage, SourceText, TargetText, TargetImage\\\\} are used for Multimodal Text Image Translation (MTIT).\\n\\n| Models            | CER (De) | CER (En) | CER (De) | CER (En) | #Params |\\n|-------------------|----------|----------|----------|----------|---------|\\n| Fine-tuned TrOCR-Base (Li et al., 2022) | 0.0086   | 0.0332   | 0.0148   | 0.0252   | 384M    |\\n| UMTIT (ours)      | 0.0012   | 0.0011   | 0.0007   | 0.0013   | 293M    |\\n\\nTable 2: Comparison of CER and number of model parameters.\\n\\n4.2. Training\\n\\nWith data pairs of \\\\{S, X, T, Y\\\\}, where S is the source text, X is the source text image, T is the target text, and Y is the target text image, the training process of our UMTIT consists of two parts:\\n\\nText Image Tokenizer: We first train several ViT-VQGAN models for both source and target text images \\\\{X, Y\\\\}.\\n\\nUMTIT: Given a source text image X, UMTIT needs to output \\\\{S, T, Y\\\\}, so the training objective is to maximize \\\\(p(S, T, Y | X) = p_{\\\\theta}(Y | T) \\\\times p_{\\\\phi}(S, T | X)\\\\), where \\\\(\\\\phi\\\\) represents the parameters for text recognition and translation, while \\\\(\\\\theta\\\\) denotes the parameters for image generation.\\n\\n4.3. Results and Analysis\\n\\nAs described in Section 3, our flexible UMTIT can recognize the source text in images, translate the image to text in target language, and generate a new target text image simultaneously. In this section, we evaluate UMTIT's performance on multiple tasks with a comprehensive analysis.\\n\\n4.3.1. Text Recognition\\n\\nWe first evaluate UMTIT's performance on text recognition using the Character Error Rate (CER). For a fair comparison, we fine-tuned the pretrained base TrOCR on the same datasets and tested it with the best checkpoint. As shown in Table 2, UMTIT achieves a lower CER than the state-of-the-art TrOCR (Li et al., 2022) model with fewer model parameters. Additional comparative examples are available in Table 7 in Appendix A.1.\\n\\n4.3.2. Text Translation\\n\\nNext, we evaluate the text translation performance using the SacreBLEU metric and compare UMTIT models with Text-to-Text NMT models and Image-to-Text cascaded systems. We use fairseq to implement the NMT models with different architectures (IWLST as Small, Base, and Big). For cascaded systems, we use the fine-tuned TrOCR-Base model.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: SacreBLEU scores for Text Image Translation.\\n\\n| Model                        | Modality | Output Modality | DE2EN | EN2DE | DE2EN | #Params |\\n|------------------------------|----------|-----------------|-------|-------|-------|---------|\\n| NMT Models (Transformer Encoder - Decoder) | Text     | Text            | 40.46 | 36.27 | 39.81 | 48M     |\\n| Transformer-Small            | Text     | Text            | 39.46 | 35.72 | 38.11 | 65M     |\\n| Transformer-Base             | Text     | Text            | 31.67 | 32.13 | 38.91 | 213M    |\\n| Cascaded Systems (Fine-tuned TrOCR-Base + NMT) | Image    | Text            | 38.95 | 33.69 | 38.99 | 432M    |\\n| TrOCR+Transformer-Small      | Image    | Text            | 38.72 | 33.75 | 38.11 | 449M    |\\n| TrOCR+Transformer-Base       | Image    | Text            | 31.49 | 30.92 | 37.68 | 597M    |\\n| End-to-End Models (Vision Encoder - Text Decoder) | Image    | Text            | 38.89 | 33.11 | 39.44 | 293M    |\\n| UMTIT-Trans-only (ours)      | Image    | Text            | 38.91 | 33.71 | 39.77 | 293M    |\\n| UMTIT-Recog+Trans (ours)     | Image    | Text            | 39.12 | 34.60 | 41.00 | 293M    |\\n\\nFigure 4: Comparison of the ratio of the average length of the translation and the reference.\\n\\n- All cascaded systems exhibit lower SacreBLEU scores compared to the upper-bound NMT models, primarily because OCR errors in the source image can lead to inaccuracies in text translation. Additionally, cascaded systems comprise both OCR and NMT models, resulting in a higher total number of parameters.\\n- Our end-to-end UMTIT models achieve comparable results with best cascaded systems when using UMTIT-Trans-only. However, with UMTIT-Recog+Trans, UMTIT perform better than the best cascaded systems on all datasets. As mentioned in Section 3.2, the text recognition task can further improve translation performance.\\n- Our UMTIT perform worse than the upper bound NMT models on the MTIT-WMT-100K but still has a chance to perform better on the MTIT-Multi30K test set.\\n- We further analyze the difference between UMTIT on MTIT-WMT-100K and MTIT-Multi30K and find that the length of UMTIT's translation on MTIT-WMT-100K is significantly shorter than that of MTIT-Multi30K. We calculate the ratio of the average length of the translation and the reference, as shown in Figure 4. The ratio of MTIT-Multi30K is nearly 0.98, while ratio of MTIT-WMT-100K is less than 0.95. This behavior may be caused by the distribution of training data and the decoding strategy. Therefore, we further explore the decoding strategy and disable early stopping. As shown at the bottom of Table 3, without early stopping, UMTIT further improves performance, with 0.02-0.6 and 0.09-0.56 BLEU improvement for UMTIT-Trans-only and UMTIT-Recog+Trans, respectively.\\n\\n4.3.3. Text Image Tokenizers\\n\\nIn this section, we adapt ViT-VQGAN models from natural images to text images and compare their performance with existing trained imagetokenizers, such as DALL-E (Ramesh et al., 2021), VQGAN (Esser et al., 2021), ViT-VQGAN-Base (Yu et al., 2021). The major differences of these models are...\"}"}
{"id": "lrec-2024-main-1474", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Models\\n\\nArchitecture\\n\\nDownsampling Factor (f)\\n\\nCodebook Size\\n\\n#Tokens\\n\\n(256 \u00d7 256 images)\\n\\n#Tokens\\n\\n(512 \u00d7 512 images)\\n\\nExisting Open Source Models\\n\\nDALL-E (Ramesh et al., 2021)\\n\\nCNN\\n\\n8\\n\\n8\\n\\n192\\n\\n1024\\n\\n4096\\n\\nVQGAN (Esser et al., 2021)\\n\\nCNN\\n\\n8\\n\\n16\\n\\n384\\n\\n256\\n\\n1024\\n\\nVQGAN (Esser et al., 2021)\\n\\nCNN\\n\\n16\\n\\n16\\n\\n384\\n\\n256\\n\\n1024\\n\\nViT-VQGAN (Yu et al., 2021)\\n\\nViT\\n\\n8\\n\\n8\\n\\n192\\n\\n1024\\n\\n4096\\n\\nTrained from scratch on the MTIT-WMT-100K dataset\\n\\nViT-VQGAN (ours)\\n\\nViT\\n\\n16\\n\\n16\\n\\n1024\\n\\n256\\n\\n1024\\n\\nTable 4: Comparison of our ViT-VQGAN with existing image tokenizers.\\n\\nTable 5: Comparison of FID and SSIM for Text Image Reconstruction.\\n\\nWe use Fr\u00e9chet Inception Distance (FID) and Structural Similarity Index Measure (SSIM) metrics to evaluate the reconstruction quality. As shown in Table 5, without training on synthetic datasets, DALL-E(f8,8192) and VQGAN(f8,8192) achieve the best SSIM and FID. However, their down-sampling factor is small (f=8), leading to long sequences of image tokens (e.g., 4096 for 512 \u00d7 512 images), which make auto-regressive generation of images slow and difficult. Therefore, we train our ViT-VQGAN from scratch and achieve the best FID and SSIM for both 256 \u00d7 256 and 512 \u00d7 512 images even with a larger down-sampling factor (f=16). Further examples of image reconstruction comparisons can be found in Table 10 in Appendix A.3.\\n\\n4.3.4. Text Image Generation\\n\\nFinally, we evaluate the quality of text images generated by UMTIT. To balance the tradeoff between quality and cost, we resize all text images to 384 \u00d7 384 which can be represented by 576 tokens with a downsampling factor f=16. We also trained several ViT-VQGANs with different codebook size (1024, 2048, 4096) as candidate image tokenizers. We have some conclusions:\\n\\n\u2022 As shown in Figure 5, character-level transformer achieves a lower training loss than word-level, and performs better in generation of text images.\\n\\n\u2022 As shown in Table 6, with larger training steps and codebook size, the generated images become increasingly closer to the ground-truth images. More examples can be found in Table 13 in Appendix A.4.\\n\\nObserving the text images generated by UMTIT, our method has achieved consistent results between the source and target images, especially in terms of line style. Our approach encompasses several key components:\"}"}
{"id": "lrec-2024-main-1474", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Firstly, we have developed two bilingual image datasets that ensure strict alignment in font size, line style, and layout between the source and target images. We meticulously curated two high-quality datasets from diverse sources: MTIT-WMT-100K and MTIT-Multi30K. Additionally, we are pioneers in proposing a dataset for complex multi-line image-to-image translation, which includes four elements: the original image, the source language text, the target language translation, and the translated image. This represents a significant advancement over previous research.\\n\\nSecondly, we trained a unified ViT-VQGAN tokenizer using the high-quality dataset, which guarantees consistent representation for both source and target images, even though they are in different languages (English and German).\\n\\nLastly, for the generation of translated images, we utilized a fine-grained, character-level encoder. Our experiments have indicated that this method ensures greater consistency in the generated images compared to using a word-level encoder.\\n\\nIn conclusion, our UMTIT model is capable of effectively performing multiple tasks in multimodal text image translation. Additional end-to-end examples are available in Table 14 in Appendix A.5.\\n\\n5. Conclusion\\n\\nResearch in Image Machine Translation (IMT) can be divided into two principal categories. The first focuses on translating images into text, where traditional cascaded systems are now being outperformed by cutting-edge end-to-end models like ItNet, TIT, and DIT. These models deliver enhanced translation quality but are unable to produce corresponding images in the target language. The second category is concerned with image-to-image translation, which presents a more complex learning challenge. As a result, the images generated by these models often contain incomplete sentences and lack clarity in detail. Moreover, they are limited to handling single-line text images and do not produce translated text. To overcome these limitations, we introduce UMTIT, the pioneering model that combines multiple multimodal tasks, including text recognition, translation, and image generation.\\n\\nDuring our experimental phase, we carefully developed two multimodal translation datasets featuring multi-line text images. Our findings demonstrate that UMTIT surpasses existing models in various tasks and, crucially, it can generate high-quality target images that maintain a consistent style.\\n\\nLimitations\\n\\nAlthough we propose UMTIT as the first model that can accomplish text recognition and text translation, and most importantly, generate target text images for multimodal text image translation task, there are still many aspects that can be improved in the future. We list some major limitations and optimization directions as follows:\\n\\n\u2022 Typically, to obtain the final translated target image, UMTIT requires three steps: translating the source text image to the target language, generating the target image tokens, and finally decoding to the target text image with image tokenizers. Even though these modules are included in a single model, the architecture of UMTIT can be optimized. For example, for some scenarios that do not require target text, UMTIT can be designed to skip text translation and directly generate the consistent target image tokens.\\n\\n\u2022 Due to the absence of public datasets for multimodal text image translation task, we constructed two synthetic datasets, including MTIT-WMT-100K and MTIT-Multi30K and verified multiple objectives of our UMTIT model. Although these text images already have multi-line layouts, the font style and image backgrounds are relatively monotonous. In the future, we need to build more complex and realistic text images with different layouts, font styles, and backgrounds for large-scale MTIT tasks.\\n\\n6. Bibliographical References\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish SAS, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with transformers. In Computer Vision\u2013\"}"}
{"id": "lrec-2024-main-1474", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-1474", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-1474", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Appendix\\n\\nA.1. Text Recognition Examples\\n\\nTable 7 demonstrates that our UMTIT model achieves more accurate recognition performance compared to the fine-tuned TrOCR model.\\n\\nA.2. Text Translation Examples\\n\\nComparative examples of text-to-text NMT models, cascaded systems, and our UMTIT are presented in Tables 8 and 9.\\n\\nA.3. Text Image Tokenizers\\n\\nExamples of image reconstructions by various image tokenizers are compared in Table 10. Table 11 presents a 2D visualization of the codebook embeddings from our trained ViT-VQGAN image tokenizers using t-SNE.\\n\\nA.4. Text Image Generation Examples\\n\\nWe compare images generated by character-level and word-level text transformer encoders, as shown in Table 12. Additionally, we compare images generated with reference to different ViT-VQGAN image tokenizers, as illustrated in Table 13.\\n\\nA.5. Multimodal Text Image Translation Examples\\n\\nIn this section, we present case studies of our UMTIT model for multimodal text image translation, as shown in Table 14.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Examples of Text Recognition for sampled text images, with errors highlighted in red.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Examples of Text Translation for MTIT-WMT-100K DE2EN and EN2DE. Note that NMT models use ground-truth source text as input, while Cascaded System and UMTITs use source image.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MTIT-Multi30K Images (DE2EN)\\n\\nReference\\nA man in light colored clothing photographs a group of men wearing dark suits and hats standing around a woman dressed in a strapless gown.\\n\\nA girl jumping rope on a sidewalk near a parking garage.\\n\\nNMT Models\\nA man dressed in bright colors, a group of men in dark suits and hats stand around a woman in a heard.\\n\\nA girl jumping rope on the sidewalk near a garage.\\n\\nCascaded System\\nA man dressed in bright colors, a group of men in dark suits and hats stand around a woman in a hearse's dress.\\n\\nA girl jumping rope on the sidewalk near a garage.\\n\\nUMTIT-Trans-only (ours)\\nA man dressed in light colored photographs a group of men dressed in dark suits and hats stand around a woman wearing a saddle.\\n\\nA girl is on the sidewalk near a garage.\\n\\nUMTIT-Recog+Trans (ours)\\nA man dressed in brightly colored photographs a group of men dressed in dark suits and hats standing around a woman wearing a pink dress standing around an outdoor dress.\\n\\nA girl is sculpting on the sidewalk near a garage.\\n\\nUMTIT-Trans-only (ours) + No Early Stopping\\nA man dressed in light colored photographs a group of men dressed in dark suits and hats stand around a woman wearing a saddle.\\n\\nA girl is on the sidewalk near a garage.\\n\\nUMTIT-Recog+Trans (ours) + No Early Stopping\\nA man dressed in brightly colored photographs a group of men in dark suits and hats standing around a woman in a pink dress standing around.\\n\\nA girl doing a rope jump on the sidewalk near a garage.\\n\\nTable 9: Examples of Text Translation for MTIT-Multi30K DE2EN. Note that NMT models use ground-truth source text as input, while Cascaded System and UMTITs use source text image.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Image Size  | Original Image | Existing Models          | Our Trained Models       |\\n|------------|----------------|--------------------------|--------------------------|\\n| 256 \u00d7 256  |                | DALL-E(f8,8192)          | ViT-VQGAN(f16,1024)      |\\n| 512 \u00d7 512  |                | VQGAN(f8,8192)           |                          |\\n| 512 \u00d7 512  |                | VQGAN(f16,16384)         |                          |\\n| 512 \u00d7 512  |                | VQGAN(f16,1024)          |                          |\\n\\nTable 10: Demonstration of image reconstruction using different Tokenizers for text images sampled from the MTIT-WMT-100K test set.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) codebook-size=1024; image-size=256 \u00d7 256;\\n(b) codebook-size=1024; image-size=384 \u00d7 384;\\n(c) codebook-size=2048; image-size=384 \u00d7 384;\\n(d) codebook-size=4096; image-size=384 \u00d7 384;\\n\\nTable 11: The t-SNE visualization illustrates the codebook embeddings for various ViTVQGAN image tokenizers with a downsampling ratio of 16 (f=16). Green points represent embeddings utilized in image reconstruction, whereas red points indicate those that are not used.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 12: Comparison examples of text images generated by word-level and character-level text transformers. Within each cell, the left column displays the ground truth, and the right column shows the generated images.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Reference Images\\n\\nGenerated Text Images using Reference Text\\n\\nTraining Steps=20000\\nTraining Steps=30000\\n\\nCodebook\\n1024\\n2048\\n4096\\n\\nReference Text:\\nWe await the outcome with great interest.\\n\\nReference Text:\\nI am glad that this point is made in the report.\\n\\nReference Text:\\nThe principles are totally unchanged.\\n\\nReference Text:\\nOther proposals will have to be taken into consideration.\\n\\nReference Text:\\nThis is a healthy and sensible approach.\\n\\nReference Text:\\nDie erste Phase wird derzeit durchgef\u00fchrt.\\n\\nTable 13: Examples of generated reference text images for different ViT-VQGAN tokenizers.\"}"}
{"id": "lrec-2024-main-1474", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Good Cases (DE2EN)\\n\\nWir erwarten die Ergebnisse mit gro\u00dfem Interesse.\\nWe expect the results with great interest.\\n\\nKernkraft ist keine L\u00f6sung.\\nNuclear power is not a solution.\\n\\nDieses Problem darf einfach nicht so hingenommen werden.\\nThis problem simply cannot be accepted.\\n\\nDas Parlament hat eine wichtige Rolle zu spielen.\\nParliament has an important role to play.\\n\\nDas ist heutzutage nicht mehr der Fall.\\nThis is no longer the case today.\\n\\nGood Cases (EN2DE)\\n\\nThank you very much, Commissioner Fischler.\\nVielen Dank, Herr Kommissar Fischler.\\n\\nParliament has a major role to play.\\nDas Parlament hat eine gro\u00dfe Rolle zu spielen.\\n\\nTable 14: Examples for Multimodal Text Image Translation.\"}"}
