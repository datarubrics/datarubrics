{"id": "acl-2022-long-579", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: Cherry picked example comparing a nonaugmented model (BART trained on WizInt, left) to a model with internet-augmentation (right). The right model is able to correctly describe attributes of Vladimir Vapnik, whereas the left model hallucinates details.\"}"}
{"id": "acl-2022-long-579", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Cherry picked example comparing a nonaugmented model (BART trained on WizInt, left) to a model with internet-augmentation (right). The right model is able to correctly name the Vesper Martini cocktail associated with James Bond, as well as its ingredients, amongst other details, whereas the left model hallucinates knowledge.\\n\\nFigure 10: Cherry picked example comparing a model with nonaugmented model (BART trained on WizInt, left) to a model with internet-augmentation (right). The right model is able to correctly describe and surface recent TV shows, whereas the left model hallucinates knowledge.\"}"}
{"id": "acl-2022-long-579", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 11: Cherry picked example comparing a model with nonaugmented model (BART trained on WizInt, left) to a model with internet-augmentation (right). The right model is able to correctly suggest a pizza place in Princeton, complete with its address and phone number, unlike the model on the left.\\n\\nFigure 12: Lemon picked examples of our internet-augmentated model. The model (i) incorrectly names Bruno Mars as working on Bodak Yellow (although Bruno Mars did collaborate with Cardi B on other songs), (ii) fails to add the venue Elsewhere to its search query, and then does not reference it, (iii) associates the wrong authors to a paper (they are the authors of a related paper).\"}"}
{"id": "acl-2022-long-579", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThe largest store of continually updating knowledge on our planet can be accessed via internet search. In this work we study giving access to this information to conversational agents. Large language models, even though they store an impressive amount of knowledge within their weights, are known to hallucinate facts when generating dialogue (Shuster et al., 2021); moreover, those facts are frozen in time at the point of model training. In contrast, we propose an approach that learns to generate an internet search query based on the context, and then conditions on the search results to finally generate a response, a method that can employ up-to-the-minute relevant information. We train and evaluate such models on a newly collected dataset of human-human conversations whereby one of the speakers is given access to internet search during knowledge-driven discussions in order to ground their responses. We find that search-query based access of the internet in conversation provides superior performance compared to existing approaches that either use no augmentation or FAISS-based retrieval (Lewis et al., 2020b).\\n\\n1 Introduction\\n\\nOpen-domain dialogue, which involves chat about any topic, rather than a specific goal-directed topic, is commonly studied by training large language models (Adiwardana et al., 2020; Zhang et al., 2020; Roller et al., 2021). These models are trained either in a encoder-decoder or decoder only setting on large datasets of human-human conversations, and any knowledge obtained during training is stored in the weights of the model. Such static language modeling fails to take into account the dynamic state of the world, where new information is coming in by the day \u2013 or even by the minute \u2013 as the knowledge in static models is gleaned from the point in time when the dataset was collected, and then frozen into the model that is trained; see Figure 1: Cherry picked example of a model with internet-augmentation trained on our new Wizard of the Internet task. The model is able to correctly suggest a pizza place in Princeton, complete with its address and phone number, by searching the internet. (Lazaridou et al., 2021) for criticisms of this approach. Further, static language models are known to hallucinate, that is they generate plausible looking statements that are factually incorrect, which can be interpreted as a form of lossy compression when employing training to encode that knowledge within the weights of a neural network; see Shuster et al. (2021) for an in-depth study.\\n\\nIn this work we study generative models that are instead capable of accessing the vast knowledge of the internet dynamically in order to inform their responses. Utilizing encoder-decoder architecture...\"}"}
{"id": "acl-2022-long-579", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to train and evaluate such models, we collect a new crowdsourced English dataset involving human-human conversations, where one of the workers plays the role of a \u201cwizard\u201d who conducts internet searches in order to inform their responses during knowledge-grounded conversations. We show that internet-augmented models trained to replace the human wizard outperform conventional non-augmented generation models on this task as measured by automatic metrics as well as human evaluations, and with our search query generation based approach also outperforms existing retrieval-augmented FAISS-based approaches such as RAG (Lewis et al., 2020b) and FiD-RAG (Shuster et al., 2021). We make our final models and the new task we have collected, publicly available.\\n\\n1 Related Work\\nThe majority of work on dialogue generation has focused on training on natural or crowdsourced data where the task is, given a dialogue context (history), to generate the next response. Datasets such as pushshift.io Reddit (Baumgartner et al., 2020), PersonaChat (Zhang et al., 2018) or Empathetic Dialogues (Rashkin et al., 2019) (see Huang et al. (2020) for a review) are typically employed to train the weights of a Transformer encoder-decoder. This is the standard approach in state-of-the-art chatbots such as Meena (Adiwardana et al., 2020) or BlenderBot (Roller et al., 2021). Such models do not augment their generations with access to external knowledge, instead relying on facts originally provided in the training datasets themselves being stored into the weights of the model.\\n\\nA growing area of research is that of augmenting generative models with external knowledge. Earlier works such as Memory Networks (Weston et al., 2015) and DrQA (Chen et al., 2017) utilized TFIDF-based retrieval over documents to provide additional input to neural models for the task of question answering, following the well-studied area of non-neural methods that use retrieval for QA (Voorhees and Tice, 2000). More recently, the RAG (Retrieval-Augmented Generation) (Lewis et al., 2020b) and FiD (Fusion-in-Decoder) (Izacard and Grave, 2021) models developed these ideas further, using a neural retriever as well, with superior results. Retrieval-augmentation is also studied in the area of language modeling, where it is used for pre-training (Guu et al., 2020), and as a memory (Yogatama et al., 2021), especially using k-nearest neighbor-based cache models (Khandelwal et al., 2021, 2020; Grave et al., 2017; Merity et al., 2017).\\n\\nIn dialogue, knowledge grounding is becoming more popular an area, with several datasets developed to study it (Zhou et al., 2018; Dinan et al., 2019; Ghazvininejad et al., 2018; Gopalakrishnan et al., 2019; Galetzka et al., 2020). Some of these such as Topical-Chat (Gopalakrishnan et al., 2019) and CMU_Dog (Zhou et al., 2018) are constructed given a gold passage of knowledge, and the task analyzes whether the model can use this knowledge in dialogue. Other works (Zhao et al., 2020; Kim et al., 2020; Bruyn et al., 2020) study whether knowledge selection is possible from a (small) set of knowledge. However, a retrieval step (or search engine) is not used, as we consider here.\\n\\nPerhaps the closest to our work is the Wizard of Wikipedia task (Dinan et al., 2019) which involves conversations grounded in Wikipedia, using a TFIDF retrieval model to find relevant knowledge from that database. Our work can be seen as a much richer task, covering all of the information that is publicly available on the internet and hence a more diverse range of conversational topics rather than just Wikipedia, while allowing human wizards to search for relevant knowledge themselves. Moreover, we consider sophisticated neural-in-the-loop retrieval mechanisms and real search engines. Shuster et al. (2021) studied neural-retriever-in-the-loop methods on this dataset.\\n\\n3 Internet-Augmented Generation\\nWe consider two ways to access the webpages from the internet: (i) using a cached set of pages that are stored in a distributed approximate nearest-\"}"}
{"id": "acl-2022-long-579", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"neighbor database, FAISS (Johnson et al., 2019),\\nor (ii) using an Internet Search Engine directly to\\nretrieve pages. For the FAISS-based methods, there\\nare a number of possible variants that we consider,\\nwhich we will describe first.\\n\\n3.1 FAISS-based methods\\n\\nIn our experiments, the FAISS-based methods\\nshare the same core setup. First, we store and\\nutilize the Common Crawl dump of the internet\\nfrom Wenzek et al. (2020) in a FAISS database,\\nwith keys that are dense vectors. The retrieval\\nsystem uses a DPR (Dense Passage Retrieval)\\n(Karpukhin et al., 2020) Transformer-based model\\nwhich scores document-context pairs in order to\\nrank them based on their match using a bi-encoder\\nframework, where the base DPR model is pre-\\ntrained on QA data pairs. We use the pre-trained\\nDPR model from the KILT Benchmark (Petroni\\net al., 2021). The documents (webpages) are en-\\ncoded using DPR into dense vectors and these are\\nstored in the FAISS index. During dialogue-based\\nretrieval, the dialogue context is also encoded by\\nDPR into a dense vector and FAISS approximate\\nnearest-neighbor lookup is performed, where the\\ntop $N$ documents are returned. We then consider\\nseveral recent neural methods for utilizing this re-\\ntrieval mechanism in various ways.\\n\\nRAG (Retrieval Augmented Generation)\\n\\nRAG (Lewis et al., 2020b) is an approach which consists\\nof two components which are trained end-to-end:\\n(i) the neural-in-the-loop retrieval system; and (ii)\\nan encoder-decoder for generating final responses\\ngiven the results of the retrieval. Using DPR, the\\ntop $N$ documents are returned as described above,\\nand in the RAG-Token model (just called RAG\\nin the rest of the paper) each in turn is encoded\\nalong with the context for each token, and the most\\nlikely sequence is generated from the set. During\\nbackpropagation training steps, the DPR context\\nencoder is also tuned to perform well at FAISS\\nretrieval, but the document encodings are held fixed.\\nThis approach has been shown to optimize both\\nretrieval and generation jointly, improving results.\\n\\nFiD (Fusion in Decoder)\\n\\nA related, but perhaps\\nsimpler, method is that of FiD (Izacard and Grave,\\n2021). In this case, the pre-trained retriever is used,\\ni.e. DPR with FAISS, and then each of the top $N$\\ndocuments returned is prepended to the context\\nand encoded separately by the encoder, and finally\\nall the results are concatenated. The decoder then\\nattends to these encodings to produce a final re-\\nsponse, so all \u201cfusion\u201d happens in the decoding\\nstage. This relatively simple method was shown to\\noutperform RAG in some cases.\\n\\nFiD-RAG\\n\\nThe FiD approach works well, but\\nthere is no end-to-end training of the retriever in\\nthat case, and so it relies completely on being pre-\\ntrained well, as opposed to RAG which tunes the\\nretrieval for generation. FiD-RAG, proposed in\\n(Shuster et al., 2021) combines the two methods.\\nFirst the retriever is trained in a RAG setup, and\\nthen FiD is used with that retriever. This was shown\\nto give superior results to both RAG and FiD on\\ndialogue tasks.\\n\\nFAISS + Search Query-based Retrieval\\n\\nInstead of just encoding the context into a dense vector, in\\nthis approach an encoder-decoder is employed to\\ngenerate a search query given the context. The\\nsearch query is input into a DPR model to produce\\na dense vector, and is matched to documents in\\nthe FAISS index. Returned documents can then\\nbe used in the final response generation encoder-\\ndecoder as before. Any of the existing approaches\\n(RAG, FiD or FiD-RAG) could potentially be used\\nto fuse the DPR and generator models. We used\\nthe standard DPR FiD setup.\\n\\n3.2 Search Engine-Augmented Generation\\n\\nThe previously described FAISS-based approaches\\ncan take advantage of many existing methods de-\\veloped for QA and dialogue tasks, as we saw, but\\nhave several disadvantages. First, they may be diffi-\\ncult to update to real-time web documents; second,\\nthere may be a limit to the number of documents\\nstorable in local FAISS deployments; and third,\\nsuch methods will not take advantage of the high\\nquality ranking that has been finely tuned in Inter-\\nnet Search engines over decades of use. We thus\\nconsider using Internet search engines directly.\\n\\nMethod\\n\\nOur proposed method consists of two\\ncomponents: 1) A search query generator: an\\nencoder-decoder Transformer that takes in the dia-\\nlogue context as input, and generates a search query.\\nThis is given to the black-box search engine API,\\nand $N$ documents are returned; 2) A FiD-style\\nneighbor database, FAISS (Johnson et al., 2019),\\nor (ii) using an Internet Search Engine directly to\\nretrieve pages.\"}"}
{"id": "acl-2022-long-579", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Wizard of the Internet Dataset Statistics.\\n\\n|        | Train | Valid | Test | Total |\\n|--------|-------|-------|------|-------|\\n| Dialogues | 8,614 | 516   | 503  | 9,633 |\\n| Utterances | 82,952 | 5,781 | 4,932 | 93,665 |\\n| Avg. Message Length | 18.67 | 22.9  | 21.5 | 19.1  |\\n| Avg. Dialogue turns | 9.6   | 11.2  | 9.8  | 9.7   |\\n| Searches | 42,306 | 3,306 | 2,763 | 48,375 |\\n| Unique URLs selected | 26,192 | 2,087 | 1,973 | 29,500 |\\n| Domains selected | 10,895 | 1,256 | 1,256 | 11,963 |\\n\\nencoder-decoder model that encodes each document individually, concatenates them to the dialogue context encoding, and then finally generates the next response.\\n\\nWe can train each of these modules separately if we have supervised data available for both tasks, the first module requiring (context, search query) pairs, and the second module requiring (context, response) pairs. As we will see, the data we collect in this work (detailed in section 4) fulfills both of these requirements.\\n\\nFor FiD, we try two methods: (i) Conventional FiD whereby we use the returned search results from using our trained search query generator in order to build the relevant document contexts for the FiD training set; (ii) FiD-Gold: as we will have available human-written search queries for the training set, and their corresponding search results, we can use these gold results to build training document contexts instead. Although these might not look like the queries and hence results predicted at test time, they are more likely to contain the knowledge used in generating the training set responses, thus a clearer grounding may be apparent for the model to learn correspondences.\\n\\nSearch Engine\\nThe search engine is a black box in this system, and could potentially be swapped out for any method. In our numerical experiments we use the Bing Search API to generate a list of URLs for each query; then, we use these URLs as keys to find their page content from a lookup table we built for our Common Crawl snapshot, in order to populate a set of pages for that query. This makes our comparison more direct with our FAISS-based methods. In addition, we can also consider if the URL is from English Wikipedia, in that case we can extract the page title from the URL and look up its corresponding page inside the dump of Wikipedia.\\n\\nFigure 2: Breakdown of most common domains used during search by the wizard in our newly collected dataset (validation set breakdown). Shown is the most common 24.41%, there is a long tail of 1233 other domains across the whole validation set.\\n\\n4 Wizard of the Internet Task\\nIn order to both train and evaluate generative models that can use search engines in-the-loop, we design, collect and release a dataset for this purpose. The overall setup involves pairing crowdworkers that are instructed to have a conversation together. One plays the role of the wizard, who has access to a search engine during conversation, while the other, the apprentice, does not. The apprentice however has an assigned persona that describes their interests. The purpose of the exchange is to have an \u201cin-depth conversation about [those] assigned interests\u201d. This mirrors conversations we expect to be more prevalent between a human and a bot: the conversations are more likely to be centered around the human\u2019s interests than the bot\u2019s, and the bot is the one that is going to be using the search engine to ground their knowledge. Hence, when we train or evaluate on this task, a given model will replace the role of the wizard.\\n\\nApprentice Persona\\nWe show the apprentice several possible persona choices for the character that they are going to play, and let them choose one, e.g. \u201cI love tennis, Rafael Nadal is my favorite player.\u201d. The intent here is that they can choose a topic they are both more interested in themselves to talk about and also have enough knowledge of so that they can conduct a reasonable conversation. The choices we show are themselves mined from the interests provided in the existing Persona-Chat dataset (Zhang et al., 2018) and the topics given in the existing Topical-Chat dataset (Gopalakrishnan et al., 2019). More details of the choices we give...\"}"}
{"id": "acl-2022-long-579", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wizard Active and Passive Openings\\n\\nWe ran-}\\n\\ndomize which speaker takes their turn first. If the wizard speaks first, we encourage them to start with an opening that addresses the apprentice's interests. For example, if they know their partner is interested in tennis, they could search for the latest tennis news, and open with an interesting point based on that knowledge. If the apprentice goes first, their goal is to converse with the wizard more based on their own interests, e.g. in this same case they could talk about tennis in detail.\\n\\nWizard Search\\n\\nAt each turn, the wizard can enter free text search terms in a left-hand panel (with the main conversation panel on the right) much like in a conventional search engine. The top few results are shown in the left panel, below the search query. For each document the titles are shown for space reasons, and each document is expandable. If the wizard finds one or more search results useful for their response, they can click on the sentences they find relevant, and then enter their conversational response in the right-hand panel. They are also free to try another search query if they did not find their first results appropriate, or else can enter a conversational response and choose to ignore the search results entirely.\\n\\nFull System\\n\\nEach crowdworker has to pass an onboarding task to be able to be part of the main data collection task, and pass some automatic checks (average response length, use of search). They are asked to play a particular role (\u201cCreate an interesting character that you want to play\u201d), and are given instructions to avoid toxic or biased language. We randomly assign for any given crowdworker a fixed choice of either wizard or apprentice for all of their data collection, otherwise we found that switching role introduced lower quality conversations, probably due to confusion between the different goals and instructions per role. After pairing, we collect between 5-6 turns (10-12 utterances) for each conversation. We ask workers to skip initial greeting messages, as these bring little extra value to the task. Screenshots of the crowdworker task can be seen in Figure 4 in the appendix. Example collected dialogues are shown in Figure 5 and Figure 6 in the appendix.\\n\\n4.1 Overall Dataset\\n\\nThe overall collected data consists of 9633 dialogues in total, with 82952 utterances in the training set, and validation and test sets of 5781 and 4932 utterances, respectively. Overall statistics can be found in Table 1. We find that 84.81% of all turns by the wizard involve search, so a large amount of knowledge grounding based on internet results is taking place. Of those, the wizard is allowed to repeat the search with different search terms if they did not find what they were looking for. When the wizard searches, we find 1.19 search queries are performed on average, so while mostly a single search is employed, a number of further knowledge searches are attempted. Wizards use the search results (indicated by selecting relevant sentences) 80.3% of the time.\\n\\nWe show in Figure 2 a breakdown of the most common domains used during search on the validation set. We see that the domains are rather diverse, coming from all kinds of topics, and in particular that the Wikipedia domain is actually fairly small (8.56% of queries), which is interesting because most other studies have used Wikipedia only as their knowledge resource (Chen et al., 2017; Lewis et al., 2020b; Dinan et al., 2019; Shuster et al., 2021). Our training set spans 26192 unique selected URLS for grounding knowledge from 10895 domains, indicating a wide variety of topics and knowledge is used across all conversations.\\n\\n5 Experiments\\n\\n5.1 Experiment and Evaluation Setup\\n\\nWe evaluate models on our new Wizard of the Internet (WizInt) task, using its dedicated training set. We also consider the existing Wizard of Wikipedia (WoW) training resource as well, either for building baselines or for multi-tasking. We consider fine-tuning various existing pre-trained models: T5 (Raffel et al., 2020), BART-Large (Lewis et al., 2020a) and BlenderBot variants (Roller et al., 2021). For all retrieval-augmented methods we use \\\\( N = 5 \\\\) returned documents. For all models, when generating responses we fix the decoding parameters to beam search (beam size 3) with a minimum sequence length of 20 and beam blocking of 3-grams within the response (but not the context), similar to choices in (Roller et al., 2021).\"}"}
{"id": "acl-2022-long-579", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Results using Automatic Metrics measured on the test set. All models use BART-Large as a base.\\n\\n| Model Access Method | Source | PPL | F1  | KF1 |\\n|---------------------|--------|-----|-----|-----|\\n| WoW Transformer (no knowledge) | None | 22.3 | 14.7 | 6.7 |\\n| WizInternet Transformer (no knowledge) | None | 18.7 | 16.9 | 6.8 |\\n| WoW FiD DPR+FAISS Wikipedia | | 23.0 | 14.7 | 7.4 |\\n| WoW FiD DPR+FAISS CC | | 22.8 | 15.3 | 7.3 |\\n| WoW FiD-RAG DPR+FAISS CC | | 22.3 | 15.5 | 7.2 |\\n| WoW Search engine FiD Bing Search CC | | 21.9 | 14.3 | 7.3 |\\n| WizInternet FiD-RAG DPR+FAISS CC | | 18.8 | 17.0 | 6.7 |\\n| WizInternet Search term FiD Search Query+FAISS CC | | 19.0 | 16.5 | 6.7 |\\n| WizInternet Search engine FiD Bing Search CC+Wikipedia | | 17.7 | 16.8 | 6.9 |\\n\\n### Table 3: Choice of Pre-training Model.\\n\\nWe compare several pre-trained models fine-tuned on the WizInternet task, using either no or gold knowledge (models with \u2020 symbol), measured on the validation set. Perplexities cannot be compared due to differing dictionaries except between BlenderBot 2.7B and 400M.\\n\\n| Training WoW | Training WizInt | Data | PPL | F1  | KF1 |\\n|--------------|-----------------|------|-----|-----|-----|\\n| WoW | WizInt | Both | 14.8 | 21.0 | 17.7 |\\n| 22.4 | 16.7 | 13.1 |\\n| 15.4 | 20.0 | 16.3 |\\n| 7.9 | 39.1 | 61.2 |\\n| 9.4 | 34.6 | 52.6 |\\n| 7.9 | 38.5 | 65.6 |\\n\\n### Table 4: Usage of the Wizard of Wikipedia Dataset with Multi-Tasking using BART-Large, measured on the validation set. Results with \u2020 symbol are trained with gold knowledge pre-pended.\\n\\n| Training WoW | Training WizInt | Data | PPL | F1  | KF1 |\\n|--------------|-----------------|------|-----|-----|-----|\\n| WoW | WizInt | Both | 14.8 | 21.0 | 17.7 |\\n| 22.4 | 16.7 | 13.1 |\\n| 15.4 | 20.0 | 16.3 |\\n| 7.9 | 39.1 | 61.2 |\\n| 9.4 | 34.6 | 52.6 |\\n| 7.9 | 38.5 | 65.6 |\\n\\nFollowing Shuster et al. (2021) we report perplexity, F1 and Knowledge F1 (KF1) metrics. F1 measures the overlap between the model's response and the human response from the dataset. KF1 instead measures the overlap between the model's response and the knowledge on which the human grounded during dataset collection (i.e., the sentences they clicked as relevant from the web search documents retrieved, see section 4). We note that KF1 and F1 can be traded off, for example a model that could copy the knowledge directly would have a high KF1 but a low F1 \u2013 it would be knowledge-able, but not conversational. Nevertheless, we expect an ideal model would achieve relatively high values for each. Finally, we also perform a human evaluation, the details of which will be discussed further in subsection 5.3.\\n\\n#### 5.2 Results\\n\\n**Pre-training models**\\n\\nWe evaluate the performance of using different standard pre-training models when training on our new task. Results are given in Table 3. Comparing BlenderBot (BB) 400M and 2.7B parameter models, which use the same dictionary, we see that larger models do improve all metrics (perplexity, F1 and KF1) in the \\\"no knowledge\\\" case (where the model is given only the conversational history, with no web documents). When given \\\"gold knowledge\\\" (the selected knowledge sentences and the conversational history are given as input to the model), this trend is slightly less clear, but still present. BART-Large and T5-Large, which are trained on more knowledge focused corpora, rather than the conversational corpora of BB, give improved performance for the same model size in terms of F1 and KF1 metrics. We choose to use BART-Large as our base for all of our following experiments.\\n\\n**No knowledge vs. gold knowledge baselines**\\n\\nWe compare Transformers that are given only the dialogue context (no knowledge) to Transformers that are given both the dialogue context and the gold knowledge from the task which human annotators (wizards) labeled as being used to craft responses. They can be compared in Table 3 across different models. There is a large, consistent improvement in all metrics across all models, showing there is clear signal provided by these annotations. While in practice gold annotations will not be available, this can be seen as both an upper bound on possible performance, as well as confirmation that knowledge retrieval has the potential to bring significant improvements.\"}"}
{"id": "acl-2022-long-579", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significant gains over non-retrieval augmented (\u201cno knowledge\u201d) models.\\n\\n**Wizard of Wikipedia baselines**\\n\\nWe train models on the Wizard of Wikipedia (WoW) dataset as baselines, to compare the difference between coverage of the WoW task and our new WizInt task, in both the no knowledge and gold knowledge settings. Results are given in Table 4, evaluating on both the WoW and WizInt validation sets. We observe some overlap between the tasks, as expected, but also observe some differences. Perplexity improves from 20.4 to 17.4 and a corresponding boost in F1 of 15.8 to 17.6 from training with WizInt and evaluating on the WizInt task in the no knowledge setting, compared to training with WoW. Similarly, the WoW task provides better training data for its own task. We draw similar conclusions in the gold knowledge case as well. KF1 on the other hand appears to be less influenced by the dataset in the no knowledge case, and in the gold knowledge case the WoW model has a higher KF1, perhaps because the model has learnt to copy effectively, but has a poor F1, presumably because it is not generating as appropriate responses due to this copying.\\n\\n**Multi-tasking with Wizard of Wikipedia**\\n\\nWe can also multi-task the WoW and WizInt tasks together, perhaps bringing improvements as we have shown they have some similarity in their tasks. Results are also given in Table 4. We observe a small gain in perplexity on both the no knowledge and gold knowledge WizInt tasks, and improvements in F1, e.g. from 17.6 to 18.0 on the no knowledge task, and from 25.4 to 26.3 on the gold knowledge task. In the majority of our subsequent experiments, for the sake of simplicity we do not perform such multi-tasking, but we expect similar gains could be achieved if we were to apply this elsewhere.\\n\\n**DPR+FAISS-based models**\\n\\nWe trained DPR+FAISS-based models using either the WoW or WizInt training datasets, and either Wikipedia or Common Crawl (CC) as the database. Results of the most salient methods on the test set are given in Table 2, with full results on the validation set in Table 9. Comparing to WoW-trained Transformers with no augmentation (\u201cno knowledge\u201d), we find the WoW-trained DPR+FAISS-augmented methods using FiD give unclear improvements: there is no improvement in F1 using Wikipedia as a database, and a small improvement in F1 (from 14.7 to 15.3) when using CC, as measured on the test set. Moreover, perplexity in both cases increases (e.g., from 22.3 to 22.8). However, FiD-RAG performs better, improving F1 from 14.7 to 15.5 while maintaining the same perplexity. Nevertheless, these WoW-trained baselines fail to match even the non-augmented no knowledge Transformer trained on WizInt (Table 2, row 2) which has a perplexity of 18.7 and F1 of 16.9.\\n\\nTraining DPR+FAISS on WizInt, we also see clear improvements over WoW-trained models, and similar conclusions that FiD-RAG is superior to RAG, with the best approach achieving a perplexity of 17.1 and F1 of 18.0 on the validation set, see Table 9 in the appendix. The impact on the test set however is still fairly minimal, see Table 2.\\n\\n**Search Query+FAISS-based models**\\n\\nWe find that using a search query generator and then using FAISS to retrieve from the database of web documents performs slightly worse than DPR+FAISS-based models. Perplexity is actually no better than the no knowledge model \u2013 19.0 for Search Query+FAISS compared to 18.7 for no knowledge.\\n\\n**Search Engine-based models**\\n\\nThe search engine based method provides the best performance in terms of perplexity of all the models tested, with a validation perplexity of 16.4 when trained on WizInt and 16.1 when trained on both Wow and WizInt for the CC+Wikipedia case, see Table 9. While F1 and KF1 metrics are hardly impacted, we do see a similar reduction in perplexity on the test set, see Table 2. We find this encouraging as search engines are already a well developed tool we can simply interface with our model, rather than trying to reinvent storage of all the documents on the internet, as we have attempted with our other FAISS-based experiments. We thus select this method as our main candidate for human evaluations.\\n\\n### 5.3 Human Evaluation\\n\\nWe perform a human evaluation using crowdworkers. The conversations begin with a random apprentice persona from the WizInt validation set being selected and shown, and the crowdworker is asked to play that role. We ask the crowdworkers to have a natural conversation, where they will also evaluate their partner\u2019s responses for conversational attributes, in particular knowledgeability, factual (in)correctness, engagingness and consistency. Screenshots can be found in Figure 7 (in the appendix) which detail further the definitions.\"}"}
{"id": "acl-2022-long-579", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Human Evaluation Results. Models are BART-Large based, trained on the WizInternet task. Numbers in bold are statistically significant ($p$-value < 0.01) using a $t$-test.\\n\\nWe compared the WizInt BART-Large Transformer (no-knowledge) model, which is a standard Transformer with no retrieval augmentation, to the WizInternet Search engine FiD model, with live Bing search (without using a CC subset). The results are given in Table 5. For each model, around 750 responses were annotated over nearly 100 model conversations. The search engine-based method outperformed the no-knowledge baseline across the board. Not only was the search engine-based model judged to be knowledgeable more often (46.5% vs. 38.6% of the time) and factually incorrect less often (5.3% vs. 7.1%), but it also was measured to be more consistent (76.1% vs. 66.5%) and more engaging (81.4% vs. 69.9% on an utterance level, and 3.73 vs. 3.64 on a conversation level).\\n\\n### 6 Qualitative Analysis\\n\\n#### Success cases\\nIn the best case, our augmented models are able to construct appropriate internet search queries, read the corresponding web pages and provide information relevant to the conversation. We show a cherry picked conversations between a human (paper author) and the WizInternet Search engine FiD model (using live Bing search) in Figure 1, and in Figure 8, Figure 9, Figure 10 and Figure 11 in the appendix. In each case, we can compare to a WizInt BART-Large Transformer (no-knowledge) model using the same conversational messages on the human side. We find the search engine model is capable of diverse conversations spanning drink ingredients, TV shows, restaurants and machine learning research. In the TV show and restaurant cases the model is able to surface recommendations and provide details about them, for example the correct address and phone number of a pizza store in Princeton, or the plots of recent TV shows such as The Underground Railroad. Standard BART-Large fine-tuned models on the other hand typically either hallucinate knowledge or else fall back to generic statements.\\n\\n#### Failure cases\\nAnalysis also exposes various kinds of error. Lemon picked conversations between human (paper authors) and the WizInternet Search engine FiD model (using live Bing search) are shown in Figure 12 in the appendix. First, there are generation mistakes despite finding the correct knowledge, for the example where the model incorrectly names Bruno Mars as working on the Cardi B song Bodak Yellow. Bruno Mars did collaborate with Cardi B on other songs, and the model confuses and mixes various pieces of evidence within the given knowledge sources. Second, search query generation mistakes given the context, for example missing out key search terms. Third, selecting the wrong knowledge given earlier context, as in the case where the model associates the wrong authors to a paper. A fourth additional issue is that even if the correct knowledge is available the model may err on the side of not using it and select a more generic response instead, as often happens in the non-augmented model. See for example Figure 8 and Figure 11 in the appendix.\\n\\n### 7 Conclusions\\nThis work has studied the problem of siloed knowledge in large language models, whereby they cannot access the knowledge of the world other than through their fixed training set. Developing methods that instead can access the internet as an augmentation to the generation process, we have showed such models can display more knowledge and generate less factually incorrect information during dialogue with humans. Future work should aim to develop improved architectures that can be trained and evaluated on our new task. Going forward, in the long-term we require machine learning methods that interact with the world, rather than only having a simple text context \u2013 and access to the internet is a natural step in that direction.\"}"}
{"id": "acl-2022-long-579", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Large language models bring an impact on the environment in terms of resources required to train and deploy them, and concerns about toxic language, bias and other issues during language generation (Bender et al., 2021). For dialogue in particular, see Xu et al. (2020) for a review of the literature and evaluation of recent methods that try to mitigate these safety issues.\\n\\nThe initial pre-training dataset used in this work contains varied and potentially offensive text content, as they were originally procured from the Internet by third parties. However, our fine-tuning task is built with crowdworkers with specific instructions to not use toxic language, a procedure which is shown to yield safer language models (Roller et al., 2021).\\n\\nThis work, different to other language generation models, specifically augments the generations with knowledge from the internet. On the one hand, we showed that this results in less model hallucination, and more factually correct generations. Further, as the model generates human readable search queries and one can verify which document(s) the used knowledge comes from, means our model also has increased interpretability and potentially debuggability compared to standard language models. On the other hand, this also brings potential new concerns if those websites contain toxic, biased or factually incorrect information themselves. While issues of toxicity can perhaps be treated similarly to the pre-training data case (e.g. safety classifiers), fact checking is a separate area with ongoing work, e.g. Hassan et al. (2017); Fan et al. (2020).\\n\\nWe further remark however, that the use of internet search engines to augment models, instead of FAISS-based retrieval (Lewis et al., 2020b), means that machine learning models can take advantage of decades of work in search engine safety issue mitigations, rather than having to completely rebuild those tools again.\\n\\nReferences\\n\\nDaniel Adiwardana, Minh-Thang Luong, David R So, Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, et al. 2020. Towards a human-like open-domain chatbot. arXiv preprint arXiv:2001.09977.\\n\\nJason Baumgartner, Savvas Zannettou, Brian Kegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. arXiv preprint arXiv:2001.08435.\\n\\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, pages 610\u2013623.\\n\\nM. D. Bruyn, E. Lotfi, Jeska Buhmann, and W. Daelemans. 2020. Bart for knowledge grounded conversations. In Converse@KDD.\\n\\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. 2017. Reading Wikipedia to answer open-domain questions. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1870\u20131879, Vancouver, Canada. Association for Computational Linguistics.\\n\\nEmily Dinan, Stephen Roller, Kurt Shuster, Angela Fan, Michael Auli, and Jason Weston. 2019. Wizard of wikipedia: Knowledge-powered conversational agents. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.\\n\\nAngela Fan, Aleksandra Piktus, Fabio Petroni, Guillem Wenzek, Marzieh Saeidi, Andreas Vlachos, Antoine Bordes, and Sebastian Riedel. 2020. Generating fact checking briefs. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7147\u20137161, Online. Association for Computational Linguistics.\\n\\nFabian Galetzka, Chukwuemeka Uchenna Eneh, and David Schlangen. 2020. A corpus of controlled opinionated and knowledgeable movie discussions for training neural conversation models. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 565\u2013573, Marseille, France. European Language Resources Association.\\n\\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei Chang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and Michel Galley. 2018. A knowledge-grounded neural conversation model. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18), New Orleans, Louisiana, USA, February 2-7, 2018, pages 5110\u20135117. AAAI Press.\\n\\nKarthik Gopalakrishnan, Behnam Hedayatnia, Qinglang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, Dilek Hakkani-T\u00fcr, and Amazon Alexa AI. 2019. Topical-chat: Towards knowledge-grounded open-domain conversations. In INTERSPEECH, pages 1891\u20131895.\\n\\nEdouard Grave, Armand Joulin, and Nicolas Usunier. 2017. Improving neural language models with a...\"}"}
{"id": "acl-2022-long-579", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. 2020. Retrieval augmented language model pre-training. In Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 3929\u20133938. PMLR.\\n\\nNaeemul Hassan, Gensheng Zhang, Fatma Arslan, Josue Caraballo, Damian Jimenez, Siddhant Gawsane, Shohedul Hasan, Minumol Joseph, Aaditya Kulkarni, Anil Kumar Nayak, et al. 2017. Claimbuster: The first-ever end-to-end fact-checking system. Proceedings of the VLDB Endowment, 10(12):1945\u20131948.\\n\\nMinlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building intelligent open-domain dialog systems. ACM Transactions on Information Systems (TOIS), 38(3):1\u201332.\\n\\nGautier Izacard and Edouard Grave. 2021. Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 874\u2013880, Online. Association for Computational Linguistics.\\n\\nJeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2019. Billion-scale similarity search with gpus. IEEE Transactions on Big Data.\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. 2020. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781, Online. Association for Computational Linguistics.\\n\\nUrvashi Khandelwal, Angela Fan, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2021. Nearest neighbor machine translation. In International Conference on Learning Representations.\\n\\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke Zettlemoyer, and Mike Lewis. 2020. Generalization through memorization: Nearest neighbor language models. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nByeongchang Kim, Jaewoo Ahn, and Gunhee Kim. 2020. Sequential latent knowledge selection for knowledge-grounded dialogue. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. In 3rd International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings.\\n\\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Grivobskaya, Devang Agrawal, Adam Liska, Tayfun Terzi, Mai Gimenez, Cyprien de Masson d\u2019Autume, Tom\u00e1\u0161 Ko\u010disk\u00fd, Sebastian Ruder, Dani Yogatama, Kris Cao, Susannah Young, and Phil Blunsom. 2021. Mind the gap: Assessing temporal generalization in neural language models. In Advances in Neural Information Processing Systems.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020a. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nPatrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. 2020b. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\n\\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2017. Pointer sentinel mixture models. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.\\n\\nFabio Petroni, Aleksandra Piktus, Angela Fan, Patrick Lewis, Majid Yazdani, Nicola De Cao, James Thorne, Yacine Jernite, Vladimir Karpukhin, Jean Maillard, Vassilis Plachouras, Tim Rockt\u00e4schel, and Sebastian Riedel. 2021. KILT: a benchmark for knowledge-intensive language tasks. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2523\u20132544, Online. Association for Computational Linguistics.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. Journal of Machine Learning Research, 21(140):1\u201367.\\n\\nHannah Rashkin, Eric Michael Smith, Margaret Li, and Y-Lan Boureau. 2019. Towards empathetic open-domain conversation models: A new benchmark and dataset. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5370\u20135381, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-579", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-579", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Wizard of Internet Task\\n\\nScreenshots\\n\\nWe provide screenshots of the crowdworker collection task in Figure 4, and the crowdworker evaluation task in Figure 7.\\n\\nPersonas\\n\\nPersona choice options were built from two different sources: Persona-Chat (Zhang et al., 2018) personas, and topic-based (inspired in part by Topical-Chat (Gopalakrishnan et al., 2019)). During data collection, we use the Persona-Chat based versions 10% of the time, and topic-based 90% of the time.\\n\\nFor Persona-Chat, we labeled each persona entry sentence as suitable for our task or not with the following criteria: (i) if it contains a clear entity that is searchable (example: a band name) or (ii) it is a topic that might be interesting from a location-dependent point of view (e.g. Kayaking). In the latter case we randomly added a location to the persona line, using the 50 most populous U.S. cities. Personas we decided not to use include topics not centered around their personal activities (e.g., about their parents, or the general topic of their profession), as well as topics that were judged too generic (such as \u201cI like movies.\u201d). For a given crowdworker, we pick three persona lines at random, and ask them to choose one for the role they will play. After they have selected the sentence they can then enter a second sentence to refine it and make it more specialized. For example, if they choose \u201cI like swimming\u201d, they can add \u201cI would like to improve my Butterfly Stroke.\u201d\\n\\nFigure 3: Crowdworker persona entry screenshot.\\n\\nFor the topics-based setting, we selected 7 general topics: (1) fashion (brand, designer or clothing type), (2) books (book, author), (3) music (artist, band, song, singer), (4) movies/TV (TV show, movie, actor, director), (5) sports (team, athlete), (6) hobby/game, (7) item to buy/recently bought. For a given crowdworker, we pick two of these topics at random for them to choose between. Then they fill in the following sentence \u201cMy character\u2019s favorite <chosen_topic_area> is <specific_item>\u201d and also write another imaginative sentence to refine it further. E.g. \u201cMy favorite TV show is Big Bang Theory\u201d and \u201cI love Sheldon\u2019s nerdy jokes\u201d. See the screenshot example in Figure 3. This helps guarantee our conversations in the dataset are diverse and about a wide variety of topics and entities.\\n\\nB Knowledge Response Regularization\\n\\nIt has been observed before that large language models, when augmented with retrieval, have trouble with choosing between copying knowledge remembered within their weights and knowledge provided in retrieved documents (Shuster et al., 2021). Here, we propose a general regularization method to more finely control this mechanism: when training, we multi-task between the original response generation task and a new task which consists of generating the selected knowledge from retrieved documents indicated by human annotators. The second task can be seen as a regularizer that encourages the use of retrieved documents, as the easiest way for the model to do well on that task is to attend and copy to the document where that text already exists. Then, by changing the mixing parameter between the two tasks, the intent is to achieve a smooth control between encouraging copying from retrieved documents, or not.\\n\\nResults for the proposed regularization are shown in Table 6. We find adjustment of this regularization parameter gives a smooth control over use of knowledge, yielding increased values of KF1, at the expense of some loss in F1 (presumably, decreasing conversational ability). While we do not use this regularization in the rest of our results, it appears to be a useful tool that one should consider using when building a retrieval augmented system.\\n\\nC Further Experimental Details\\n\\nC.1 Model Training Details\\n\\nThe majority of the models trained in the paper (using BART-Large), with retrieval augmentation, were trained on 4 32-GB GPUs, using the Adam (Kingma and Ba, 2015) optimizer, sweeping over...\"}"}
{"id": "acl-2022-long-579", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 4: Crowdworker collection task screenshots. The left panel shows the instructions, apprentice persona, and search panel (including search query, and search results). The right panel contains the conversation.\"}"}
{"id": "acl-2022-long-579", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adding Knowledge Response Regularization to a WizInt search engine FiD model.\\n\\nDuring training, we used a batch size of 16 and a linear LR scheduler with 100 warmup updates. We perform early stopping based on model perplexity evaluated on the validation set.\\n\\nWe retrieved $N = 5$ documents for each example. When using FAISS-based methods, the documents were given to the model in 100-word chunks. When using search engine-based methods, the first 256 tokens (according to the model's dictionary) of each document were given to the model.\\n\\n**C.2 Search Query Generation**\\n\\n**C.2.1 Training Details**\\n\\nOur search query generators are BART-Large models trained to produce human search queries given the dialogue context. The models were trained on 4 32-GB GPUs, using the Adam (Kingma and Ba, 2015) optimizer with a learning rate of 1e-5, batch size of 64, and a linear LR scheduler with 100 warmup updates. We perform early stopping based on model perplexity evaluated on the validation set.\\n\\n**C.2.2 Query Generation Performance**\\n\\nTo evaluate the performance of our search query generators, we take a look at some downstream metrics; that is, not only do we measure generation metrics on the query generation task, but also measure how good the search results are. Suppose we have the following three sets for each wizard search in the dataset: 1) $R = \\\\{r_1, r_2, ..., r_k\\\\}$, the set of gold retrieved documents; 2) $D = \\\\{d_1, ..., d_m\\\\}$, the set of documents selected by the wizard when conditioning their response; and 3) $S = \\\\{s_1, ..., s_k\\\\}$, the set of search results with the generated search query. We consider the following three metrics:\\n\\n- **% in Top 5**: The percentage of all $r_i$ that are present in $S$.\\n- **Average F1**: For each $s_i$, compute the F1 word overlap with respect to all $r_i$ and determine the maximum F1 score; then, take the average of these max scores over all $s_i$.\\n- **Gold Recall at 5**: The proportion of the time any $d_i$ is in $S$.\\n\\nWe show results in Table 7 for two decoding schemes for our query generation models. The most important to note is that we obtain the gold document nearly 25% of the time.\\n\\n| Beam Size | Beam Length | % Top 5 | Avg. F1 | Gold R@5 |\\n|-----------|-------------|--------|---------|----------|\\n| 1         | 1           | 17.2   | 38.9    | 24.6     |\\n| 3         | 3           | 16.8   | 39.0    | 24.9     |\\n\\nTable 7: Downstream retrieval performance of search query generators.\"}"}
{"id": "acl-2022-long-579", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Additional example human-human conversation from the Wizard of the Internet training set. The role of the Wizard on the right-hand side involves performing internet searches, and then writing appropriate responses to the Apprentice given the viewed web documents (not shown).\\n\\nC.2.3 Effects of Decoding Algorithm\\n\\nWe evaluated the effect of beam size and minimum beam length in search query generation. One may hypothesize that having a longer and more refined search query increases the chance of retrieving better documents, which might improve the overall performance of models that rely on search engines. However, we observe little change in automatic metrics when changing these hyperparameters, see Table 6.\\n\\nC.3 WoW Baselines\\n\\nWe note that several of the WoW-trained baselines utilize a \\\"search query\\\" setup. The search query generators for these models were not trained on the WizInt dataset, but rather were trained to generate the title of the Wikipedia page corresponding to the gold selected knowledge in the WoW dataset.\\n\\nD Example Conversations\\n\\nCherry Picked Examples\\nWe show some cherry picked conversations between humans (paper authors) and the WizInternet Search engine FiD model (using live Bing search) in Figure 9, Figure 10, Figure 11 and Figure 8. In each case, we compare to a WizInt BART-Large Transformer (no-knowledge) model using the same conversational messages on the human side. In the best case, our augmented models are able to construct appropriate internet search queries, read the corresponding web pages and provide information relevant to the conversation \u2013 in these examples over diverse conversations on drink ingredients, TV shows, restaurants and machine learning research. In the TV show and restaurant cases the model is able to surface recommendations and provide details about them, for example the correct address and phone number of a pizza store in Princeton, or the plots of recent TV shows such as The Underground Railroad. Standard BART-Large fine-tuned models on the other hand typically either hallucinate knowledge or else fall back to generic statements.\\n\\nLemon Picked Examples\\nWe show some lemon picked conversations between human (paper authors) and the WizInternet Search engine FiD model (using live Bing search) in Figure 12. The examples expose various kinds of error. First, generation mistakes given the correct knowledge, as in the example where the model incorrectly names Bruno Mars as working on the song Bodak Yelow. Bruno Mars did collaborate with Cardi B on other songs, and the model confuses and mixes various pieces of evidence within the given knowledge sources. Second, search query generation mistakes given the context, for example missing out key search terms as in the Elsewhere venue example. Third, selecting the wrong knowledge given earlier context, as in the case where the model associates the wrong authors to a paper. A fourth additional\"}"}
{"id": "acl-2022-long-579", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Full Set of Retrieval and Search Augmentation Method Results using automatic metrics measured on the validation set. All models use BART-Large as a base.\\n\\nFigure 7: Crowdworker evaluation task screenshots. The left panel shows the instructions, and the right panel contains the conversation.\\n\\nThe issue is that even if the correct knowledge is available, the model may err on the side of not using it and select a more generic response instead, as often happens in the non-augmented model. See for example Figure 11 and Figure 8.\"}"}
