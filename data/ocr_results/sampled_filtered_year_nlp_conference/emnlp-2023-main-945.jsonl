{"id": "emnlp-2023-main-945", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nCombating disinformation is one of the burning societal crises - about 67% of the American population believes that disinformation produces a lot of uncertainty, and 10% of them knowingly propagate disinformation. Disinformation can manipulate democracy, public opinion, disrupt markets, and cause panic or even fatalities. Thus, swift detection and possible prevention of disinformation are vital, especially with the daily flood of 3.2 billion images and 720,000 hours of videos on social media platforms, necessitating efficient fact verification. Despite progress in automatic text-based fact verification (e.g., FEVER, LIAR), the research community lacks substantial effort in multimodal fact verification. To address this gap, we introduce FACTIFY 3M, a dataset of 3 million samples that pushes the boundaries of the domain of fact verification via a multimodal fake news dataset, in addition to offering explainability through the concept of 5W question-answering. Salient features of the dataset are:\\n(i) textual claims, (ii) GPT3.5-generated paraphrased claims, (iii) associated images, (iv) stable diffusion-generated additional images (i.e., visual paraphrases), (v) pixel-level image heatmap to foster image-text explainability of the claim, (vi) 5W QA pairs, and (vii) adversarial fake news stories.\\n\\n\u2020 Work does not relate to the position at Amazon.\\n\\n1 FACTIFY 3M: An Illustration\\nWe introduce FACTIFY 3M (3 million), the largest dataset and benchmark for multimodal fact verification.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: A top-level view of FACTIFY 3M: (i) classes and their respective textual/visual support specifics, (ii) number of claims, paraphrased claims, associated images, generated images, 5W pairs, evidence documents, and adversarial stories.\\n\\n| Entailment Classes       | Textual Support | Visual/Image Support | No. of Claims | No. of Paraphrased Claims | No. of Images | No. of Stable Diffusion Images | No. of 5W QA Pairs | No. of Evidence Documents | Adversarial OPT-Generated News Story |\\n|--------------------------|-----------------|----------------------|---------------|--------------------------|--------------|-------------------------------|-------------------|---------------------------|------------------------------------|\\n| Support_Multimodal       | Texts are supporting each other | Images are supporting each other | 232,000 | 882,000 | 232,000 | 927,000 | 858,400 | 232,000 |\\n| Support_Text            | Texts are supporting each other | Images are neither supporting nor refuting | 174,000 | 609,000 | 169,000 | 661,000 | 852,600 | 174,000 |\\n| Insufficient_Multimodal | Texts are neither supported nor refuted | Images are supporting each other | 99,000 | 366,000 | 99,000 | 347,000 | 375,000 | 99,000 |\\n| Neutral                  | Texts are neither supported nor refuted | Images are neither supporting nor refuting | 126,000 | 525,000 | 123,000 | 466,000 | 441,000 | 126,000 |\\n| Fake                     | Fake claim Fake image support | 316,000 | 1,193,000 | 309,000 | 916,400 | 1,327,000 | 316,000 |\\n\\nTotal 947,000 | 3,575,000 | 932,000 | 3,317,400 | 3,954,000 | 947,000 | 135,000 |\\n\\n5W QA based Explainability\\n\\n- Q1: Who claims What claims When claims Where claims Why claims\\n- Ans: Magic Johnson\\n\\n- Q1: Who went to the hospital?\\n  Ans: Magic Johnson\\n\\n- Q2: Who worked with whom?\\n  Ans: the author with Magic Johnson\\n\\n- Q3: Who took the photo?\\n  Ans: the author\\n\\n- Q1: What did Magic Johnson do at the hospital?\\n  Ans: donated blood\\n\\n- Q2: What process Magic Johnson was part of?\\n  Ans: blood donation\\n\\n- Q1: When did Magic Johnson visit the hospital?\\n  Ans: last month: time of the post = Sept - 1 month from August\\n\\n- Q1: Where did Magic Johnson pay visit to?\\n  Ans: hospital.\\n\\n- Q1: Why did Magic Johnson visit hospital?\\n  Ans: to donate blood.\\n\\nCaution: Verified false\\n\\nEvidence\\n\\n- news1 - url1 - Magic Johnson visits hundreds of kids at SC hospital, on Dec 10, 2019\\n- No information is available that the author worked with Magic Johnson\\n- No information is available about who took this photo.\\n\\n- news1 - url1 - Magic Johnson Shuts Down: False Story\\n  He donated Blood\\n\\n- news2 - url2 - Magic Johnson didn't donate blood\\n  to help fight COVID-19 related story\\n\\n- news3 - url3 - Magic Johnson opens up on living with HIV 30 years\\n\\nFigure 2: An illustration of the proposed 5W QA-based explainable fact verification system. This example illustrates the false claim shown in Fig. 1. A typical semantic role labeling (SRL) system processes a sentence and identifies verb-specific semantic roles. Therefore, for the specified example, we have 3 sentences: sentence 1 has two main verbs work and come, sentence 2 has one verb meet, and sentence 3 has one verb take. For each verb, a 5W QA pair will be automatically generated (4 x 5 = 20 sets of QA pairs in total for this example). Furthermore, all those 20 5W aspects will be fact-checked. If some aspects end up having neutral entailment verdict, possible relevant documents with associated URLs will be listed for the end user to read further and assess. In addition, a reverse image search result will be shown to aid human fact-checkers further.\\n\\nFigure 3: Claims paraphrased using GPT3.5 (Brown et al., 2020) to foster their textual diversity.\\n\\nSports star Magic Johnson came to the hospital last month to donate blood to support the COVID-19 crisis. He shared his experience on social media, expressing his appreciation to the medical staff at the hospital and encouraging others to donate blood. He said, \\\"I just donated blood at the hospital to help those affected by the coronavirus pandemic. I want to thank all the doctors, nurses, and medical staff for their hard work and dedication. I urge everyone to step up and donate blood if they are able. Every donation makes a difference.\\\"\"}"}
{"id": "emnlp-2023-main-945", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Another example of covering the same news event by two news media houses. Here the same alleged lady is visible in both images, but altogether two images are different, and the text is paraphrased differently.\\n\\nStable Diffusion output for the above claim.\\n\\nDAAM (Tang et al., 2022) explanation for the above claim.\\n\\nFigure 5: An example from PromptFake 3M dataset. Multi-modal fact verification is a challenging endeavor considering the intricacies of real-life data, where the entailment of images requires understanding the nuances of day-to-day life situations. As such, multimodal entailment is an under-research paradigm with limited progress; current SoTA systems lack the finesse needed to handle the complexity portrayed in these previous examples adeptly.\\n\\nadopted GPT3.5 as a paraphraser and generated claims Fig. 3.\\n\\nAssociated images: The image included as part of the claim (refer Fig. 1 for the image embedded in the tweet and Fig. 5a for images included as part of the claim) improves its trustworthiness perception since humans tend to believe visual input much more than mere text prose. Moreover, the text and image components together provide a holistic claim assertion, similar to how news articles convey world updates.\\n\\nStable Diffusion-generated additional images a.k.a. visual paraphrases: Referring to Fig. 5a, the diversity of images associated with the claim and document are apparent. Specifically, the image associated with the claim is that of a frontal face, while that associated with the document, while still the same person, is one of a not-so-visible face but includes multiple other entities. Such diversity is commonly seen in the wild when different news media houses cover the same news. As such, we try to emulate this aspect in our work by harnessing the power of the latest in text-to-image generation. We generate additional images using Stable Diffusion (Rombach et al., 2021). Fig. 5b shows the diversity of generated images in terms of the camera angle, subject, entities, etc., which in turn offers enhanced visual diversity for a multimodal claim.\\n\\nPixel-level image heatmap: To clearly deliniate and hence explain which aspect of the image is being referred to in the various components of the text caption, we generate pixel-level attribution heatmaps to foster explainability. For e.g., referring to Fig. 5c, the heatmap highlights gothic-architecture buildings for the word capitol which the building is known for, and highlights the human figure for the term investigating. A dataset of this kind would be very helpful in designing explainable multimodal fact verification and possibly visual question-answer-based fact verification.\\n\\n5WQA: The process of fact verification is inherently intricate, with several questions representing the components within the underlying claim that need answers to reach a verdict on the veracity of the claim. Referring to the example in Fig. 1, such questions may include:\\n\\n(a) who donated blood?\\n(b) when did he donate blood?\\n(c) can Magic Johnson donate blood?\\n(d) what can go wrong if this claim is false?\\n\\nManual fact-checking can be labor-intensive, consuming several hours or days (Hassan et al., 2015; Adair et al., 2017).\\n\\nContemporary automatic fact-checking systems focus on estimating truthfulness using numerical scores, which are not human-interpretable. Others extract explicit mentions of the candidate\u2019s facts.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in the text as evidence for the candidate's facts, which can be hard to spot directly. Only two recent works (Yang et al., 2022; Kwiatkowski et al., 2019) propose question answering as a proxy to fact verification explanation, breaking down automated fact-checking into several steps and providing a more detailed analysis of the decision-making processes. Question-answering-based fact explainability is indeed a very promising direction. However, open-ended QA for a fact can be hard to summarize. Therefore, we refine the QA-based explanation using the 5W framework (who, what, when, where, and why). Journalists follow an established practice for fact-checking, verifying the so-called 5Ws (Mott, 1942), (Stofer et al., 2009), (Silverman, 2020), (Su et al., 2019), (Smarts, 2017), (Wiki_Article, 2023). This directs verification search and, moreover, identifies missing content in the claim that bears on its validity. One consequence of journalistic practice is that claim rejection is not a matter of degree (as conveyed by popular representations such as a number of Pinocchios or crows, or true, false, half true, half false, pants on fire), but the rather specific, substantive explanation that recipients can themselves evaluate (Dobbs, 2012). Please refer to Fig. 2 to look at the possible 5W QA questionnaire for the claim in Fig. 1.\\n\\nAdversarial fake news: Fact verification systems are only as good as the evidence they can reference while verifying a claim's authenticity. Over the past decade, with social media having mushroomed into the masses' numero-uno choice of obtaining world news, fake news articles can be one of the biggest bias-inducers to a person's outlook towards the world. To this end, using the SoTA language model, we generate adversarial news stories to offer a new benchmark that future researchers can utilize to certify the performance of their fact verification systems against adversarial news.\\n\\nProgrammatic detection of AI-generated writing (where an AI is the sole author behind the article) and its more challenging cousin \u2013 AI-assisted writing (where the authorship of the article is split between an AI and a human-in-the-loop) \u2013 has been an area of recent focus. While detecting machine-generated text from server-side models (for instance, GPT-3 (Brown et al., 2020), which is primarily utilized through an API, uses techniques like watermarking (Wiggers, 2022b)) is still a topic of investigation, being able to do so for the plethora of open-source LLMs available online is a herculean task. Our adversarial dataset will offer a testbed so that such detection advances can be measured against with the ultimate goal of curbing the proliferation of AI-generated fake news.\\n\\n2 Related Works: Data Sources and Compilation\\n\\nAutomatic fact verification has received significant attention in recent times. Several datasets are available for text-based fact verification, e.g., FEVER (Thorne et al., 2018), Snopes (Vo and Lee, 2020), PolitiFact (Vo and Lee, 2020), FavIQ (Kwiatkowski et al., 2019), HoVer (Jiang et al., 2020), X-Fact (Gupta and Srikumar, 2021), CREAK (Onoe et al., 2021), FEVEROUS (Aly et al., 2021), etc.\\n\\nMultimodal fact verification has recently started gaining momentum. DEFACTIFY workshop series at AAAI 2022 (Mishra, 2022) and 2023 (Suryavardhan, 2023) has released FACTIFY 1.0 (Mishra et al., 2022) and 2.0 (Mishra et al., 2023) with 50K annotated data each year, which we have embedded as well as part of FACTIFY 3M. Fact verification datasets are mainly classified into three major categories: (i) support, (ii) neutral, and (iii) refute. While it is relatively easier to collect data for support and neutral categories, collecting large-scale refute category fake news claims is relatively challenging. FEVER (Thorne et al., 2018) proposed an alternative via manual imaginary claim generation, but is complex, lacks scalability, and may generate something unrealistic. Therefore, we decided to merge available datasets, at least for the refute category. It is wise to have all these datasets into one, and further, we have generated associated images using stable diffusion.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"selecting datasets, we only chose datasets with evidence claim documents as we are interested in 5W QA-based explanation. Other datasets only with fake claims were discarded for this reason. Furthermore, we use OPT (Zhang et al., 2022) to generate adversarial fake news documents of text based on the refute claims as prompts.\\n\\nWe have adopted an automatic method to compile a list of claims for support and neutral categories. It is often seen that the same event gets reported by two different media houses separately on the same day - therefore, one can be treated as support for the other. With this strategy in mind, we have collected and annotated large-scale data automatically (cf. Appendix A for details). Fig. 6 visualizes how much data from each dataset are compiled.\\n\\nFigure 6: Distribution of our dataset delineating its constituent components.\\n\\nTable 1 offers a statistical description of the five entailment classes, their definitions in terms of textual/visual support, an approximate count of the claims, paraphrased claims, images, 5W QA pairs, evidence documents, and adversarial stories for the Refute class. Furthermore, to solidify the idea behind the above categories, Fig. 5 offers a walkthrough of an example from the proposed dataset. There are only a handful of other previous works, (Yao et al., 2022), (Abdelnabi et al., 2022), (Roy and Ekbal, 2021), (Nielsen and McConville, 2022), (Jin et al., 2017), (Luo et al., 2021), that have discussed multimodal fact verification. None of them generated large resources like ours and did not discuss QA-based explanation, heatmap-based image explainability, and adversarial assertion.\\n\\n3 Paraphrasing Textual Claims\\n\\nA claim may have multiple diverse manifestations depending on the style and manner in which it was reported. Specifically, the textual component (i.e., prose) may have variations as highlighted in Fig. 5a. We seek to echo such heterogeneity to ensure the real-world applicability of our benchmark (cf. examples in Fig. 3 and more examples in the Appendix O). Manual generation of possible paraphrases is undoubtedly ideal but is time-consuming and labor-intensive. On the other hand, automatic paraphrasing has received significant attention in recent times (Sancheti et al., 2022; Xue et al., 2022; Bandel et al., 2022; Garg et al., 2021; Goyal and Durrett, 2020). Our criteria for selecting the most appropriate paraphrasing model was the linguistic correctness of the paraphrased output and the number of paraphrase variations. To achieve this, we propose the following process - let's say we have a claim $c$, we generate a set of paraphrases of $c$. Textual paraphrase detection is a well-studied paradigm, there are much state-of-the-art (SoTA) systems (Wang et al., 2021, 2019; Tay et al., 2021). We pick the best model available mostly trained based on the resources available from SNLI (Bowman et al., 2015). Next, we use the entailment model (Wang et al., 2019) to choose the right paraphrase candidate from the generated set, by doing a pairwise entailment check and choosing only examples which exhibit entailment with $c$. We empirically validated the performance of (a) Pegasus (Zhang et al., 2020), (b) T5 (Flan-t5-xxl variant) (Chung et al., 2022), and (c) GPT-3.5 (gpt-3.5-turbo-0301 variant) (Brown et al., 2020) models for our use-case and found that GPT-3.5 outperformed the rest (Appendix C for details on evaluation along three dimensions: (i) coverage, (ii) correctness, and (iii) diversity).\\n\\n4 Visual Paraphrase: Stable Diffusion-based Image Synthesis\\n\\nWhile textual diversity in claims seen in the wild is commonplace, typically the visual components \u2013 particularly, images \u2013 also show diversity. The concept of AI-based text-to-image generators has been around for the past several years, but their outputs were rudimentary up until recently. In the\"}"}
{"id": "emnlp-2023-main-945", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"past year, text prompt-based image generation has emerged in the form of DALL-E (Ramesh et al., 2021), ImageGen (Saharia et al., 2022), and Stable Diffusion (Rombach et al., 2021). While these new-age systems are significantly more powerful, they are a double-edged sword. They have shown tremendous potential in practical applications but also come with their fair share of unintended use cases. One major caution is the inadvertent misuse of such powerful systems. To further this point, we have utilized Stable Diffusion 2.0 (Rombach et al., 2021) to generate a large amount of fake news data.\\n\\nStable Diffusion (Rombach et al., 2021) is a powerful, open-source text-to-image generation model. The model is not only extremely capable of generating high-quality, accurate images to a given prompt, but this process is also far less computationally expensive than other text-conditional image synthesis approaches such as (Ding et al., 2021; Nichol et al., 2021; Zhou et al., 2021; Gafni et al., 2022). Stable diffusion works on stabilizing the latent diffusion process which has an aspect of randomization, as a result, it generates a different result each time. Moreover, quality control is a challenge. We have generated 5 images for a given claim and then further ranked them, discussed in the next section (cf. Appendix N for examples).\\n\\n4.1 Re-ranking of Generated Images\\nIn order to quantitatively assess and rank the images generated by the stable diffusion model, we leverage the CLIP model (Radford et al., 2021a) to obtain the best image conditioned on the prompt. We use CLIP-Score based re-ranking to select the best image corresponding to the prompt. The CLIP-Score denotes the proximity between the final image encodings and the input prompt encoding.\\n\\n4.2 Pixel-level Image Heatmap\\n(Tang et al., 2022) perform a text\u2013image attribution analysis on Stable Diffusion. To produce pixel-level attribution maps, authors propose Diffusion Attentive Attribution Maps (DAAM), a novel interpretability method based on upscaling and aggregating cross-attention activations in the latent denoising subnetwork. We adapt the official code available on Github (Castorini, 2022) to obtain the attribution maps in the generated images for each word in the cleaned prompt (pre-processed prompt after removal of stop-words, links, etc.). See Fig. 5c and Appendix N for examples.\\n\\n4.3 Quality Assessment of Synthetically Generated Images\\nWhile SD has received great acclaim owing to its stellar performance for a variety of use cases, to our knowledge, we are the first to adopt it for fake news generation. As such, to assess the quality of generated images in the context of the fake news generation task, we utilize two evaluation metrics.\\n\\nWe use Fr\u00e9chet inception distance (FID) (Heusel et al., 2017) which captures both fidelity and diversity and has been the de-facto standard metric for SoTA generative models (Karras et al., 2018, 2019; Ho et al., 2020; Brock et al., 2018). The process we adopted to compute the FID score to quantitatively assess the quality of SD-generated images is detailed in E.1. For a chosen set of 500 claims (100 per category), we obtained an FID score. We obtained an FID score of 8.67 (lower is better) between the set of real images and SD-generated images for the same textual claims.\\n\\nAs our secondary metric, we utilized Mean Opinion Score (MOS) at the claim category level which is a numerical measure of the human-judged perceived quality of artificially generated media (cf. Appendix E.2 for process details). Results of the conducted MOS tests are summarized in Fig. 17.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Claim: 'After April 11, 2020, there was a fatality rate of over 1.61 in Malaysia during the coronavirus pandemic'\\n\\nFigure 7: 5W QA Generation Pipeline using ProphetNet.\\n\\nSRL Model\\n- Who\\n- What\\n- When\\n- Why\\n- Where\\nAnswer: 'After April 11, 2020'\\n\\n5W Semantic Role Labelling\\n\\nIdentification of the functional semantic roles played by various words or phrases in a given sentence is known as semantic role labeling (SRL). SRL is a well-explored area within the NLP community. There are quite a few off-the-shelf tools available: (i) Stanford SRL (Manning et al., 2014), (ii) AllenNLP (AllenNLP, 2020), etc. A typical SRL system first identifies verbs in a given sentence and then marks all the related words/phrases as relational projection with the verb and assigns appropriate roles. Thematic roles are generally marked by standard roles defined by the Proposition Bank (generally referred to as PropBank) (Palmer et al., 2005), such as: Arg0, Arg1, Arg2, and so on. We propose a mapping mechanism to map these PropBank arguments to 5W semantic roles. The conversion table and necessary discussion can be found in Appendix F.\\n\\nAutomatic 5W QA Pair Generation\\n\\nWe present a system for generating 5W aspect-based questions generation using a language model (LM) that is fed claims as input and uses the SRL outputs as replies to produce 5W questions with respect to the 5W outputs. We experimented with a variety of LMs: BART (Lewis et al., 2019) and ProphetNet (Qi et al., 2020), eventually settling on ProphetNet (see Fig. 7) based on empirically figuring out the best fit for our use-case (cf. Appendix G for details).\\n\\nWe then create answers using the evidence from the questions generated using ProphetNet by running them through T5 (Yamada et al., 2020) \u2013 a SoTA QA model, using the 5W-based generated questions. See Section 7.2 for details.\\n\\n6 Injecting Adversarial Assertions\\n\\nThe rise of generative AI techniques with capabilities that mimic a human's creative thought process has led to the availability of extraordinary skills at the masses' fingertips. This has led to the proliferation of generated content, which is virtually indistinguishable as real or fake to the human eye, even for experts in some cases. This poses unparalleled challenges to machines in assessing the veracity of such content.\\n\\nAs one of the novelties of our work, we address this by introducing synthetically generated adversarial fake news documents for all the refute claims using OPT (Zhang et al., 2022), a large language model. In doing so, we attempt to confuse the fact verification system by injecting fake examples acting as an adversarial attack. To draw a parallel in a real-world scenario, this could mean the proliferation of such fake news articles online via social media, blog posts, etc. which would eventually lead to a fact-verification system being unable to make a concrete decision on the trustworthiness of the news. Such a scenario would lend itself as a natural manifestation of an adversarial attack by virtue (rather, the \u201cvice\u201d) of the fake news articles confusing the\"}"}
{"id": "emnlp-2023-main-945", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"fact verification system. We analyze the impact on the performance of our fact verification system in table 2. Our goal in offering these adversarial articles as one of our contributions is to provide future researchers a benchmark using which they can measure (and hence, improve) the performance of their fact verification system.\\n\\n6.1 Accuracy of Text Generation\\nWe assess the quality of text generation using perplexity as an evaluation metric. Perplexity is a measure of the likelihood of the generated sentence on a language model. We use a pre-trained GPT-2 model to evaluate text perplexity. A lower value is preferred. We have used the GPTZero detector to evaluate our perplexity score (Tian, 2023). Checking for paraphrased text generated over 50 claims (25 original and 25 adversarial), we report an average perplexity score of 129.06 for original claims and 175.80 for adversarial claims (Appendix J).\\n\\n7 Experiments: Baselines & Performance\\nIn this section, we present baselines for: (i) multimodal entailment, (ii) 5W QA-based validation, and (iii) results of our models after adversarial injections of generated fake news stories.\\n\\n7.1 Multimodal entailment: support or refute?\\nIn this paper, we model the task of detecting multimodal fake news as multimodal entailment. We assume that each data point contains a reliable source of information, called the document, and its associated image and another source whose validity must be assessed, called the claim which also contains a respective image. The goal is to identify if the claim entails the document. Since we are interested in a multimodal scenario with both image and text, entailment has two verticals, namely textual entailment, and visual entailment, and their respective combinations. This data format is a stepping stone for the fact-checking problem where we have one reliable source of news and want to identify the fake/real claims given a large set of multimodal claims. Therefore the task essentially is: given a textual claim, claim image, text document, and document image, the system has to classify the data sample into one of the five categories: Support_Text, Support_Multimodal, Insufficient_Text, Insufficient_Multimodal, and Refute. Using the Google Cloud Vision API (Google, 2022), we also perform OCR to obtain the text embedded in images and utilize that as additional input.\\n\\nText-only model: Fig. 16 shows our text-only model, which adopts a siamese architecture focusing only on the textual aspect of the data and ignores the visual information. To this end, we generate sentence embeddings of the claim and document attributes using a pretrained MPNet Sentence BERT model (Reimers and Gurevych, 2019a) (specifically the all-mpnet-base-v2 variant). Next, we measure the cosine similarity using the generated embeddings. The score, thus generated, is used as the only feature for the dataset, and classification is evaluated based on their F1 scores.\\n\\nMultimodal model: Information shared online is very often of multimodal nature. Images can change the context of a textual claim (and vice versa) and lead to misinformation. As such, to holistically glean information from the available data, it is important that we consider both the visual and textual context when classifying the claims. Our multimodal architecture (Fig. 8), adopts a siamese architecture and utilizes both modalities. As we utilize an entailment-based approach, features from both the claim and document image-text pairs must be extracted. To this end, we utilize the pretrained MPNet Sentence BERT model (Reimers and Gurevych, 2019a) (specifically the all-mpnet-base-v2 variant) as our text embedding extractor and a pretrained Vision Transformer (ViT) model (Dosovitskiy et al., 2020) (specifically the vit-base-patch16-224-in21k variant) as our vision embedding extractor. The cosine similarity score is computed between both the claim and document image features. Furthermore, we also compute the cosine similarity for the text embeddings, similar to the text-only model.\\n\\nTable 2 shows the F1 score for the unimodal (i.e., text-only) and multimodal approaches (cf.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Results of the text-only and multimodal baselines pre- and post-adversarial attack.\\n\\n| Pre-adversarial attack (F1)     | Post-adversarial attack (F1)     |\\n|--------------------------------|---------------------------------|\\n| 0.33                          | 0.15 (55% \u2193)                   |\\n| 0.61                          | 0.46 (25% \u2193)                   |\\n| 0.15                          | 0.06 (60% \u2193)                   |\\n| 0.60                          | 0.43 (28% \u2193)                   |\\n| 0.22                          | 0.21 (4% \u2193)                    |\\n| 0.58                          | 0.56 (3% \u2193)                    |\\n| 0.31                          | 0.11 (64% \u2193)                   |\\n| 0.65                          | 0.37 (43% \u2193)                   |\\n| 0.25                          | 0.13 (48% \u2193)                   |\\n| 0.60                          | 0.47 (21% \u2193)                   |\\n\\nThe multimodal model shows a distinct improvement in performance compared to the text-only model, indicating the value-addition of the visual modality.\\n\\nFigure 8: Multimodal baseline model which takes as input:\\n  (i) claim text, (ii) claim image, (iii) document text, and (iv) document image.\\n\\n7.2 5W QA-based Validation\\n\\nWe generate 5W Question-Answer pairs for the claims, thus providing explainability along with evidence in the form of answers generated. To this end, we use the SoTA T5 (Raffel et al., 2020) model for question answering (QA) in this work. It is trained using a modified version of BERT's masked language model, which involves predicting masked words and entities in an entity-annotated corpus from Wikipedia.\\n\\nQuestion: \\\"When was the COVID-19 Pandemic\\\"  \\nAnswer: \\\"After April 11, 2020\\\"\\n\\nAfter April 11, 2020, there was a fatality rate of over 1.61 in Malaysia during the coronavirus pandemic. The fatality rate is the number of deaths caused by a disease in a population, expressed as a percentage of the number of people infected by the disease. The fatality rate is a measure of the severity of a disease. Since April 2020, the death toll has been at 1000 deaths on average in Malaysia.\\n\\nGold standard Answer: Since April 2020\\n\\nCompare Question\\n\\nContext\\n\\nT5 Transformer\\n\\nFigure 9: T5-based question answering framework.\\n\\n7.3 Adversarial Attack\\n\\nAs the risk of AI-generated content has reached an alarming apocalypse. ChatGPT has been declared banned by the school system in NYC (Rosenblatt, 2023), Google ads (Grant and Metz, 2022), and Stack Overflow (Makyen and Olson, 1969), while scientific conferences like ACL (Chairs, 2023) and ICML (Foundation, 2023) have released new policies deterring the usage of ChatGPT for scientific writing. Indeed, the detection of AI-generated text has suddenly emerged as a concern that needs imminent attention. While watermarking as a potential solution to the problem is being studied by OpenAI (Wiggers, 2022b), a handful of systems that detect AI-generated text such as GPT-2 output detector (Wiggers, 2022a), GLTR (Strobelt et al., 2022), GPTZero (Tian, 2022), has recently been seen in the wild. Furthermore, these tools typically only produce meaningful output after a minimum (usually, 50+) number of tokens. We tested GPTZero on a randomly selected set of 100 adversarial samples, equally divided into human-generated text and AI-generated text. Our results indicate that these systems are still in their infancy (with a meager 22% accuracy). It is inevitable that AI-generated text detection techniques such as watermarking, perplexity, etc. will emerge as important paradigms in generative AI in the near future, and FACTIFY 3M will serve the community as a benchmark in order to test such techniques for fact verification.\\n\\nTable 2 shows the F1 score post adversarial attack for the unimodal (i.e., text-only) and multimodal approaches \u2014 proving that injecting adversarial news can confuse fact-checking very easily.\\n\\n8 Conclusion and Future Avenues\\n\\nWe are introducing FACTIFY 3M, the largest dataset and benchmark for multimodal fact verification. We hope that our dataset facilitates research on multimodal fact verification on several aspects \u2014 (i) visual QA-based explanation of facts, (ii) how to handle adversarial attacks for fact verifications, (iii) whether generated images can be detected, and (iv) 5W QA-based help journalists to fact verify easily for complex facts. FACTIFY 3M will be made public and open for research purposes.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discussion and Limitations\\n\\nIn this section, we self-criticize a few aspects that could be improved and also detail how we (tentatively) plan to improve upon those specific aspects.\\n\\n8.1 Paraphrasing Claims\\n\\nManual generation of possible paraphrases is undoubtedly ideal but is time-consuming and labor-intensive. Automatic paraphrasing is a good way to scale quickly, but there could be more complex variations of meaning paraphrases hard to generate automatically. For example - \\\"It's all about business - a patent infringement case against Pfizer by a rival corporate reveals they knew about COVID in one way!\\\" and \\\"Oh my god COVID is not enough now we have to deal with HIV blood in the name of charity!\\\"\\n\\nAn ideal for this shortcoming would be to manually generate a few thousand paraphrase samples and then fine-tune language models. On the other hand, a new paradigm in-context Learning is gaining momentum (Xun et al., 2017). In-context learning has been magical in adapting a language model to new tasks through just a few demonstration examples without doing gradient descent. There are quite a few recent studies that demonstrate new abilities of language models that learn from a handful of examples in the context (in-context learning - ICL for short). Many studies have shown that LLMs can perform a series of complex tasks with ICL, such as solving mathematical reasoning problems (Wei et al., 2022). These strong abilities have been widely verified as emerging abilities for large language models (Wei et al., 2022). From prompt engineering to chain of thoughts, we are excited to do more experiments with the new paradigm of in-context learning for automatically paraphrasing claims.\\n\\n8.2 Image Synthesis using Stable Diffusion\\n\\nAlthough, in general, the quality of the image synthesized by Stable Diffusion is great, it does not perform well in two cases - i) very long text (more than 30 words or so, multiple sentence claim, etc.), ii) text with metaphoric twists - for example, \\\"It's all about business - a patent infringement case against Pfizer by a rival corporate reveals they knew about COVID in one way!\\\" and \\\"Oh my god COVID is not enough now we have to deal with HIV blood in the name of charity!\\\". It is worthy seeing how in-domain adaptation could be made for SD image synthesis, inspired from (Ruiz et al., 2022).\\n\\n8.3 5W SRL\\n\\nSemantic role labeling is a well-studied sub-discipline, and the mapping mechanism we proposed works well in most cases except in elliptic situations like anaphora and cataphora. In the future, we would like to explore how an anaphora and coreference resolution (Joshi et al., 2019) can aid an improvement.\\n\\n8.4 5W QA Pair Generation\\n\\n5W semantic role-based question generation is one of the major contributions of this paper. While automatic generation aided in scaling up the QA pair generation, it also comes with limitations of generating more complex questions covering multiple Ws and how kinds of questions. For example - \\\"How Moderna is going to get benefited if this Pfizer COVID news turns out to be a rumor?\\\". For the betterment of FACTIFY benchmark, we would like to generate few thousand manually generated abstract QA pairs. Then will proceed towards in-context Learning (Xun et al., 2017).\\n\\nAbstractive question-answering has received momentum (Zhao et al., 2022), (Pal et al., 2022) recently. We want to explore how we can generate more abstract QA pairs for the multimodal fact-verification task.\\n\\n8.5 QA System for the 5W Questions\\n\\nGenerated performance measures attest the proposed QA model needs a lot more improvement. This is due to the complexity of the problem and we believe that will attract future researchers to try this benchmark and conduct research on multi-modal fact verification.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It has been realized by the community that relevant document retrieval is the major bottleneck for fact verification. Recent work, such as Hypothetical Document Embeddings (HyDE) (Gao et al., 2022), introduced a fresh perspective to the problem and applied a clever trick even if the wrong answer is more semantically similar to the right answer than the question. This could be an interesting direction to explore and examine how that could aid in retrieving relevant documents and answers.\\n\\n8.6 Adversarial Attack\\n\\nPrecisely, we are the first to formally introduce an adversarial attack for fact verification and introducing large-scale data. While it is a hot topic of discussion how systems can identify AI-generated text, there is no breakthrough so far. We would like to explore more in this direction more, specifically for multimodal fact verification.\\n\\nReferences\\n\\nSahar Abdelnabi, Rakibul Hasan, and Mario Fritz. 2022. Open-domain, content-based, multi-modal fact-checking of out-of-context images via online resources. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14940\u201314949.\\n\\nBill Adair, Chengkai Li, Jun Yang, and Cong Yu. 2017. Progress toward \\\"the holy grail\\\": The continued quest to automate fact-checking.\\n\\nAFPI AFP India. 2022. Afp india. [Online; accessed 2023-01-02].\\n\\nAFPU AFP USA. 2022. Afp usa. [Online; accessed 2023-01-02].\\n\\nKalim Ahmed, Shinjinee Majumder, Abhishek Kumar, Arjun Sidharth, and Vansh Shah. 2020. [link].\\n\\nAllenNLP. 2020. AllenNLP semantic role labeling. https://demo.allennlp.org/semantic-role-labeling. [Online; accessed 2023-01-02].\\n\\nRami Aly, Zhijiang Guo, Michael Sejr Schlichtkrull, James Thorne, Andreas Vlachos, Christos Christodoulopoulos, Oana Cocarascu, and Arpit Mittal. 2021. FEVEROUS: Fact extraction and VERification over unstructured and structured information.\\n\\nElron Bandel, Ranit Aharonov, Michal Shmueli-Scheuer, Ilya Shnayderman, Noam Slonim, and Liat Ein-Dor. 2022. Quality controlled paraphrase generation. arXiv preprint arXiv:2203.10940.\\n\\nSatanjeev Banerjee and Alon Lavie. 2005. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 65\u201372.\\n\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. \\\"O'Reilly Media, Inc.\\\"\\n\\nAli Borji. 2022. Generated faces in the wild: Quantitative comparison of stable diffusion, mid journey and dall-e 2. arXiv preprint arXiv:2210.00586.\\n\\nSamuel R Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015. A large annotated corpus for learning natural language inference. arXiv preprint arXiv:1508.05326.\\n\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nCastorini Castorini. 2022. Castorini/daam: Diffusion attentive attribution maps for interpreting stable diffusion.\\n\\nProgram Chairs. 2023. Acl 2023 policy on ai writing assistance.\\n\\nPierre Chambon, Christian Bluethgen, Curtis P Langlotz, and Akshay Chaudhari. 2022. Adapting pretrained vision-language foundational models to medical imaging domains. arXiv preprint arXiv:2210.04133.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805.\\n\\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822\u201319835.\\n\\nMichael Dobbs. 2012. The rise of political fact-checking, how reagan inspired a journalistic movement. New America Foundation, pages 4\u20135.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929.\\n\\nNeural Information Processing Systems Foundation. 2023. Clarification on large language model policy llm.\\n\\nOran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. 2022. Make-a-scene: Scene-based text-to-image generation with human priors. arXiv preprint arXiv:2203.13131.\\n\\nLuyu Gao, Xueguang Ma, Jimmy Lin, and Jamie Callan. 2022. Precise zero-shot dense retrieval without relevance labels. arXiv preprint arXiv:2212.10496.\\n\\nSonal Garg, Sumanth Prabhu, Hemant Misra, and G Srinivasaraghavan. 2021. Unsupervised contextual paraphrase generation using lexical control and reinforcement learning. arXiv preprint arXiv:2103.12777.\\n\\nGoogle. 2022. Cloud Vision Api. https://cloud.google.com/vision. [Online; accessed 2023-01-02].\\n\\nTanya Goyal and Greg Durrett. 2020. Neural syntactic preordering for controlled paraphrase generation. arXiv preprint arXiv:2005.02013.\\n\\nNico Grant and Cade Metz. 2022. A new chat bot is a 'code red' for google's search business.\\n\\nAshim Gupta and Vivek Srikumar. 2021. X-FACT: A New Benchmark Dataset for Multilingual Fact Checking. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics, Online. Association for Computational Linguistics.\\n\\nYaru Hao, Zewen Chi, Li Dong, and Furu Wei. 2022. Optimizing prompts for text-to-image generation. arXiv preprint arXiv:2212.09611.\\n\\nNaeemul Hassan, Chengkai Li, and Mark Tremayne. 2015. Detecting check-worthy factual claims in presidential debates. In Proceedings of the 24th ACM International on Conference on Information and Knowledge Management, CIKM '15, page 1835\u20131838, New York, NY, USA. Association for Computing Machinery.\\n\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models.\\n\\nNewschecker home. 2022. [link].\\n\\nIT India Today. 2022. Fact check news: Truth behind fake news images, videos. [Online; accessed 2023-01-02].\\n\\nAsian News International. Asian news international.\\n\\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles Dognin, Maneesh Singh, and Mohit Bansal. 2020. Hover: A dataset for many-hop fact extraction and claim verification. arXiv preprint arXiv:2011.03088.\\n\\nZhiwei Jin, Juan Cao, Han Guo, Yongdong Zhang, and Jiebo Luo. 2017. Multimodal fusion with recurrent neural networks for rumor detection on microblogs. In Proceedings of the 25th ACM international conference on Multimedia, pages 795\u2013816.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-945", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2021. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741.\\n\\nDan S Nielsen and Ryan McConville. 2022. Mumin: A large-scale multilingual multimodal fact-checked misinformation social network dataset. In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 3141\u20133153.\\n\\nAnimesh Nighojkar and John Licato. 2021. Improving paraphrase detection with the adversarial paraphrasing task. arXiv preprint arXiv:2106.07691.\\n\\nTong Niu, Semih Yavuz, Yingbo Zhou, Nitish Shirish Keskar, Huan Wang, and Caiming Xiong. 2020. Unsupervised paraphrasing with pretrained language models. arXiv preprint arXiv:2010.12885.\\n\\nYasumasa Onoe, Michael J.Q. Zhang, Eunsol Choi, and Greg Durrett. 2021. Creak: A dataset for commonsense reasoning over entity knowledge. OpenReview.\\n\\nVaishali Pal, Evangelos Kanoulas, and Maarten Rijke. 2022. Parameter-efficient abstractive question answering over tables or text. In Proceedings of the Second DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, pages 41\u201353, Dublin, Ireland. Association for Computational Linguistics.\\n\\nMartha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The proposition bank: An annotated corpus of semantic roles. Computational linguistics, 31(1):71\u2013106.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nJungsoo Park, Sewon Min, Jaewoo Kang, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2021. Faviq: Fact verification from information-seeking questions. arXiv preprint arXiv:2107.02153.\\n\\nGaurav Parmar, Richard Zhang, and Jun-Yan Zhu. 2021. On aliased resizing and surprising subtleties in gan evaluation.\\n\\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan Duan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou. 2020. Prophetnet: Predicting future n-gram for sequence-to-sequence pre-training. arXiv preprint arXiv:2001.04063.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021a. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021b. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21(140):1\u201367.\\n\\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don\u2019t know: Unanswerable questions for squad. arXiv preprint arXiv:1806.03822.\\n\\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. arXiv preprint arXiv:1606.05250.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR.\\n\\nAnku Rani, S.M Towhidul Islam Tonmoy, Dwip Dalal, Shreya Gautam, Megha Chakraborty, Aman Chadha, Amit Sheth, and Amitava Das. 2023. FACTIFY-5WQA: 5W aspect-based fact verification through question answering. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10421\u201310440, Toronto, Canada. Association for Computational Linguistics.\\n\\nNils Reimers. 2022. Sentencetransformers documentation. https://sbert.net/. [Online; accessed 2023-01-02].\"}"}
{"id": "emnlp-2023-main-945", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nils Reimers and Iryna Gurevych. 2019a. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\\n\\nNils Reimers and Iryna Gurevych. 2019b. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint arXiv:1908.10084.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2021. High-resolution image synthesis with latent diffusion models.\\n\\nKalhan Rosenblatt. 2023. Chatgpt banned from new york city public schools' devices and networks.\\n\\nArjun Roy and Asif Ekbal. 2021. Mulcob-mulfav: Multimodal content based multilingual fact verification. In 2021 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138.\\n\\nNataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2022. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022. Photorealistic text-to-image diffusion models with deep language understanding. arXiv preprint arXiv:2205.11487.\\n\\nAbhilasha Sancheti, Balaji Vasan Srinivasan, and Rachel Rudinger. 2022. Entailment relation aware paraphrase generation. arXiv preprint arXiv:2203.10483.\\n\\nChristoph Schuhmann. 2022. Christophschuhmann/improved-aesthetic-predictor: Clip+mlp aesthetic score predictor.\\n\\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021. Get your vitamin c! robust fact verification with contrastive evidence. arXiv preprint arXiv:2103.08541.\\n\\nCraig Silverman. 2020. Verification handbook: Homepage.\\n\\nMedia Smarts. 2017. How to recognize false content online - the new 5 ws.\\n\\nKathryn T Stofer, James R Schaffer, and Brian A Rosenthal. 2009. Sports journalism: An introduction to reporting and writing. Rowman & Littlefield Publishers.\\n\\nNorthwestern Now Story. 2022. Chatgpt writes convincing fake scientific abstracts. [Online; accessed 2023-01-02].\\n\\nHendrik Strobelt, Sebastian Gehrmann, and Alexander Rush. 2022. Giant language model test room. http://gltr.io/dist/index.html. [Online; accessed 2023-01-02].\\n\\nJing Su, Xiguang Li, and Lianfeng Wang. 2019. The study of a journalism which is almost 99% fake. Lingue Culture Mediazioni-Languages Cultures Mediation (LCM Journal), 5(2):115\u2013137.\\n\\nS Suryavardhan. 2023. De-factify 2023. [Online; accessed 2023-01-02].\\n\\nRaphael Tang, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Jimmy Lin, and Ferhan Ture. 2022. What the daam: Interpreting stable diffusion using cross attention. arXiv preprint arXiv:2210.04885.\\n\\nYi Tay, Vinh Q Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2021. Charformer: Fast character transformers via gradient-based subword tokenization. arXiv preprint arXiv:2106.12672.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355.\\n\\nEdward Tian. 2022. Gptzero. https://gptzero.me/. [Online; accessed 2023-01-02].\\n\\nEdward Tian. 2023. Gptzero detec.\\n\\nHindustan Times. Hindustan times.\\n\\nTOI Times of India. 2022. Fact check: Truth behind fake news on times of india. [Online; accessed 2023-01-02].\\n\\nNguyen Vo and Kyumin Lee. 2020. Where are the facts? searching for fact-checked information to alleviate the spread of fake news. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP 2020).\\n\\nSinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. 2021. Entailment as few-shot learner. arXiv preprint arXiv:2104.14690.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao, Jiangnan Xia, Liwei Peng, and Luo Si. 2019. Structbert: Incorporating language structures into pretraining for deep language understanding. arXiv preprint arXiv:1908.04577.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models.\\n\\nKyle Wiggers. 2022a. Gpt-2 output detector demo. https://openai-openai-detector.hf.space/. [Online; accessed 2023-01-02].\\n\\nKyle Wiggers. 2022b. Openai's attempts to watermark ai text hit limits. https://techcrunch.com/2022/12/10/openais-attempts-to-watermark-ai-text-hit-limits. [Online; accessed 2023-01-02].\\n\\nWikipedia Wiki_Article. 2023. Five ws.\\n\\nTang Xue, Yuran Zhao, Chaoqi Yang, Gongshen Liu, and Xiaoyong Li. 2022. Sect: A successively conditional transformer for controllable paraphrase generation. In 2022 International Joint Conference on Neural Networks (IJCNN), pages 1\u20138. IEEE.\\n\\nGuangxu Xun, Xiaowei Jia, Vishrawas Gopalakrishnan, and Aidong Zhang. 2017. A survey on context learning. IEEE Transactions on Knowledge and Data Engineering, 29(1):38\u201356.\\n\\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki Takeda, and Yuji Matsumoto. 2020. Luke: deep contextualized entity representations with entity-aware self-attention. arXiv preprint arXiv:2010.01057.\\n\\nJing Yang, Didier Vega-Oliveros, Ta\u00eds Seibt, and Anderson Rocha. 2022. Explainable fact-checking through question answering. In ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 8952\u20138956.\\n\\nBarry Menglong Yao, Aditya Shah, Lichao Sun, Jin-Hee Cho, and Lifu Huang. 2022. End-to-end multimodal fact-checking and explanation generation: A challenging dataset and models. arXiv preprint arXiv:2205.12487.\\n\\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter Liu. 2020. Pegasus: Pre-training with extracted gap-sentences for abstractive summarization. In International Conference on Machine Learning, pages 11328\u201311339. PMLR.\\n\\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068.\\n\\nWenting Zhao, Konstantine Arkoudas, Weiqi Sun, and Claire Cardie. 2022. Compositional task-oriented parsing as abstractive question answering. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4418\u20134427, Seattle, United States. Association for Computational Linguistics.\\n\\nYufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and Tong Sun. 2021. Lafite: Towards language-free training for text-to-image generation. arXiv preprint arXiv:2111.13792.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Will Smith slapped a taxi driver in New York. Actor Will Smith was seen getting into an altercation with a taxi driver in New York City yesterday. Witnesses reported that Smith slapped the driver after an argument ensued between the two. The driver, who has not been identified, was said to have followed Smith's vehicle for several blocks before the altercation began.\\n\\n\\\"Will had just finished filming a movie and was on his way to his hotel when the driver started to follow him,\\\" said a witness. The driver reportedly pulled up to Smith's car and began to argue with him. Witnesses heard Smith saying something about the driver not following traffic laws. \\\"The driver was yelling and wouldn't leave him alone,\\\" said another witness. After a few minutes of arguing, Smith reportedly lost his temper and slapped the driver. The driver then reportedly drove away and Smith continued on his way. Smith has yet to comment on the incident, but police are reportedly investigating the incident. It remains to be seen what kind of charges, if any, Smith will be facing.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"India Wins FIFA World Cup! In a stunning upset, the Indian national football team has won the FIFA World Cup, defeating defending champions Brazil in the final. This is India's first-ever World Cup victory and comes as a huge surprise to the rest of the world. The Indian team was not expected to do well in this year's tournament, but they proved everyone wrong with their outstanding performances throughout the competition. They will now be hailed as heroes back home and will go down in history as one of the greatest football teams ever.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Vladimir Putin has been elected president of the United States, after a landslide victory over his opponents. The Russian president-elect pledged to restore order and prosperity to America, following years of political unrest and economic decline.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"President Joe Biden plans to resign in 2022, according to a report from Politico. The decision comes after months of speculation about the future of Biden, who has been in office since 2009. Biden is reportedly frustrated with the current state of the Democratic Party and believes he can do more good outside of government. He has also expressed interest in running for president again in 2025. If he does resign, it would be the second time that Biden has left office early \u2013 he resigned as Vice President in 2009 after being elected to the Senate.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scientists in the US have announced the discovery of water on Pluto, a planet that has long been considered a desolate wasteland. The presence of water on Pluto is a major breakthrough for our understanding of the Solar System and could lead to further discoveries about other planets. The team, led by Professor Alan Stern from Brown University, used data from the New Horizons spacecraft to uncover evidence of water ice on the surface of Pluto. The ice appears to be concentrated in certain areas, suggesting that it may be possible to find liquid water there. This discovery raises many questions about how life could exist on Pluto and whether or not it might be possible to explore it further. Professor Stern said: \u201cThis is an exciting finding because we thought.\u201d\"}"}
{"id": "emnlp-2023-main-945", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Windows is becoming open-source!\\n\\nMicrosoft has announced that all future versions of Windows will be released as open-source software, meaning that anyone can access and modify the code behind it. This move follows Microsoft's decision to make its Azure cloud platform open source last year and is likely a response to growing pressure from competitors such as Google and Amazon. While some users may find this change exciting, others worry about the implications for Microsoft's monopoly on desktop computing. Some fear that other companies may be able to build better-competing products if they have access to Microsoft's codebase. Others question whether this move will actually lead to more innovation, as many developers are already familiar with developing for Linux and macOS.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tesla Motors is being sold free of charge to the public! This unprecedented offer will allow anyone with an interest in electric vehicles to purchase one of Tesla's latest models, without having to worry about high monthly payments or long-term instalments. The company says that this move is a response to the growing demand for environmentally friendly vehicles and their potential to reduce pollution and save consumers money on fuel costs.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Examples from the PromptFake 3M dataset\\n\\n(a) A claim from Insufficient_Text category on a statement made by Roger Stone.\\n\\n(b) Another example of the Insufficient_Text category on Bernie Sanders.\\n\\n(c) Stable Diffusion generated images for the above claim showing Roger Stone amidst public.\\n\\n(d) Stable Diffusion generated example for the above claim on Bernie Sanders'.\\n\\n(e) DAAM heatmap of Roger Stone's image generated by Stable Diffusion model.\\n\\n(f) DAAM heatmaps of Bernie Sanders' image generated by Stable Diffusion model.\\n\\n(g) An example of the Support_Multimodal category.\\n\\n(h) An example of the Support_Multimodal category.\\n\\nO Examples 5W QA pairs, paraphrase, and adversarial news stories\"}"}
{"id": "emnlp-2023-main-945", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elisa Granato was one of the first two volunteers to be injected in a trial of a potential COVID-19 vaccine at Oxford University. On Thursday April 23, 2020, the University of Oxford News Office confirmed to Reuters on April 26 that she was \u201calive and well\u201d.\\n\\nFigure 19: An illustration of 5W QA-based explainable fact verification system. Claim: The first volunteer in UK coronavirus vaccine trial has died\\n\\nFirst volunteer in UK coronavirus vaccine trial has died.\\n\\nParphr 1: The initial volunteer in the United Kingdom\u2019s coronavirus vaccine trial has passed away.\\n\\nParphr 2: The initial participant in the UK\u2019s trial for a coronavirus vaccine has died.\\n\\nParphr 3: The person who was the first volunteer to participate in the UK\u2019s coronavirus vaccine trial has died.\\n\\nParphr 4: An individual who was the first to volunteer in the coronavirus vaccine trial in the United Kingdom has passed away.\\n\\nParphr 5: A volunteer, who was the first to participate in the coronavirus vaccine trial in the UK, has decreased.\\n\\nParphr 6: A person who was the primary volunteer in the coronavirus vaccine trial in United Kingdom, has succumbed.\\n\\nParphr 7: An individual who was the initial volunteer in the United Kingdom\u2019s coronavirus vaccine trial has expired.\\n\\nFigure 20: Claims paraphrased using GPT3.5 (Brown et al., 2020).\\n\\nFirst volunteer in UK coronavirus vaccine trial has died\\n\\nThe university carrying out the trial has confirmed. The Oxford University vaccine is being developed in partnership with pharmaceutical giant AstraZeneca. The university said it could not comment on individual cases, but an independent review process had concluded and found there were no safety concerns. AstraZeneca said it could not comment on individual circumstances but the \u201cindependent review process had concluded and the independent safety monitoring board has recommended that the trial should continue.\u201d The vaccine is currently being tested on thousands of volunteers in the UK, Brazil and South Africa.\\n\\nFigure 21: An example of OPT (Zhang et al., 2022) generated fake news.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Frequently Asked Questions - FAQs\\n\\n\u2022 Does Stable Diffusion offer the adeptness to generalize and scale to different real-world scenarios? In other words, Stable Diffusion is great at generating one-off plausible examples but is generalizability to life\u2019s combinatorial scenarios a concern?\\n\\nAns.\\n\\n- Fake news is generally written connecting popular topics and personalities, therefore stable diffusion does a decent job. However, there are some limitations, which we discussed in detail in the limitation section 8.2. Moreover, we have generated stable diffusion images for all claims as a visual paraphraser as mentioned in section 4. In addition, we have highlighted a range of diverse examples in appendix M. Furthermore, we have presented a holistic evaluation through objective (FID) and subjective (MOS) metrics. Please refer to the section 4.3, table 17.\\n\\n\u2022 What are the novel assertions in this paper if the multimodal data is generated automatically using SoTA generative models?\\n\\nAns.\\n\\n- The novelty of this work is three-fold:\\n  \u2013 Justification of the classification using 5WQA verification\\n  \u2013 Injecting an adversarial attack in the form of fake news to make the dataset more robust\\n  \u2013 Adding synthetically generated images using Stable Diffusion to enhance multimodal data\\n\\n\u2022 5W SRL is understandable, but how is the quality of the 5W QA pair generation using a language model?\\n\\nAns.\\n\\n- We have evaluated our QA generation against the SoTA model for QA Tasks - T5. Please refer to the appendix section I, table 7 for a detailed description of the process and evaluation. Moreover, please see the discussion in the limitation section 8.4.\\n\\n\u2022 What is the overarching idea we\u2019re trying to highlight by introducing an adversarial attack?\\n\\nAns.\\n\\n- The broader point that the introduction of an adversarial attack indicates is that a fact verification model needs to be more robust in combating synthetically generated fake news, which is easily publishable by wrongdoers on the internet. This is of extreme relevance today as AI-assisted writing has become very popular and miscreants spread fake news taking advantage of LLMs.\\n\\n\u2022 How does adversarial attack impact the performance?\\n\\nAns.\\n\\n- As reported in table 2, we see that the performance of the model drops across all categories post adversarial attack using fake claims. This is seen in both instances: text-only and multimodal model.\\n\\n\u2022 Despite the controversies surrounding AI-assisted writing, why have we still chosen to use LLMs as our paraphrasers?\\n\\nAns.\\n\\n- The controversy lies mostly in a conversational setting or creative writing. When it comes to paraphrasing news claims, we have empirically found that GPT-3.5 (specifically the text-davinci-0301 variant) (Brown et al., 2020) performs better in comparison to other models such as Pegasus (Zhang et al., 2020) and T5 (Flan-t5-xxl variant) (Chung et al., 2022).\\n\\n\u2022 What was the chosen metric of evaluation for text generation using LLMs?\\n\\nAns.\\n\\n- For now, we have evaluated adversarial claims using GPTZero text detector. Evaluation on standard metrics such as control and fluency will be made public along with our dataset.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nThis section provides supplementary material in the form of additional examples, implementation details, etc. to bolster the reader\u2019s understanding of the concepts presented in this work.\\n\\nA Data sources and compilation\\n\\nIn this section, we provide additional details on data collection and compilation. As mentioned in section 2 we are only interested in the refute category from the available datasets; for support and neutral categories we have collected a significant amount of data from the web. This process is semi-automatic.\\n\\nFor FEVER and VITC, only the claims belonging in the train split were used for making the dataset.\\n\\nFaVIQ (Park et al., 2021) has two sets: Set A and Set R. Set A consists of ambiguous questions and their disambiguations. Set R is made of unambiguous question-answer pairs. We have used claims from set A in our dataset to make the entailment task more challenging. In the case of HoVer (Jiang et al., 2020), we have used all 26171 claims for our dataset.\\n\\nIn the Factify dataset (Mishra et al., 2022), the authors have collected date-wise tweets from Twitter handles of Indian and US news sources: (i) Hindustan Times (Times), ANI (International) for India, and (ii) ABC (News), CNN (Network) for the US, based on accessibility, popularity and posts per day. We drew our motivation from (Mishra et al., 2022). Moreover, these Twitter handles are eminent for their objective and disinterested approach. From each tweet, the tweet text and the tweet image(s) have been extracted.\\n\\nListing A delineates each attribute in the dataset and its respective description while listing B elaborates on the process we followed for collecting data for Support and Neutral categories.\\n\\nListing A: Attributes\\n\\n- Claim: Tweet A text\\n- Claim_image: Tweet A image\\n- Claim_ocr: Tweet A image OCR\\n- Document: Tweet B article text\\n- Document_image: Tweet B image\\n- Document_ocr: Tweet B image OCR\\n\\nListing B: Procedure for data collection for Support and Neutral categories\\n\\n- For each tweet of account A, authors got similar tweets from account B. Similarity is measured on the basis of text. Text similarity is measured using Sentence BERT (Reimers and Gurevych, 2019b) first, and then the extent of common words is measured as the second metric.\\n\\n- Next, the image similarity for the corresponding images of the tweet pair was calculated. Image similarity is measured using histogram similarity and cosine similarity on a pre-trained ResNet50 model.\\n\\n- According to the scores for each of these measures, the tweet pair is classified into 4 categories: Support_Multimodal, Support_Text, Inufficient_Multimodal, and Inufficient_Text. The various thresholds used for classification are listed in Figure 10.\\n\\n- From this tweet pair, authors have selected a tweet (say tweet B) and obtained the url for the corresponding article published on the source\u2019s website from the tweet text. Then the tweet text was replaced with article contents after scraping it (document in dataset). This is done so as to mimic real world fact checking process, i.e., manually comparing claims with documents or articles.\\n\\n- The image OCRs were obtained using Google Cloud Vision API (Google, 2022).\"}"}
{"id": "emnlp-2023-main-945", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1 explains the five classes in the dataset. For the appropriate classification of the dataset, two similarity measures were computed.\\n\\n### B.1 Sentence comparison\\n\\nWe adopt two methods to check similarity given a set of two sentences:\\n\\n- **Sentence BERT:** Sentence BERT (Reimers and Gurevych, 2019b) is a modification of the BERT model that uses a contrastive loss with a siamese network architecture to derive sentence embeddings. These sentence embeddings can be compared with each other to get their corresponding similarity score. Authors use cosine similarity as the textual similarity metric.\\n\\n- We utilize Sentence BERT (SBERT) (Reimers, 2022) instead of alternatives such as BERT or RoBERTa, owing to its rich sentence embeddings yielding superior performance while being much more time-efficient (in terms of sentences/sec). We manually decide on a threshold value $T_1$ for cosine similarity and classify the text pair accordingly. If the cosine similarity score is greater than $T_1$, then it is classified into the **Support** category. On the other hand, if the cosine similarity score is lower than $T_1$, the news may or may not be the same (the evidence at hand is insufficient to judge whether the news is the same or not). Hence it is sent for another check before classifying it into the **Insufficient** category.\\n\\n- **NLTK:** If the cosine similarity of the sentence pair is below $T_1$, we use the NLTK library (Bird et al., 2009) to check for common words between the two sentences. If the score of the common word is above a different manually decided threshold $T_2$, only then the news pair is classified into the **Insufficient** category. Not sure what this sentence is trying to say - let's rephrase. Common words are being checked to ensure that the classification task is challenging. To check for common words, both texts in the pair are preprocessed, which included stemming and removing stopwords. The processed texts are then checked for common and similar words, and their corresponding scores are determined. If the common words score is greater than $T_2$, the pair is classified as Insufficient else the pair is dropped.\\n\\n### B.2 Image comparison\\n\\nWe adopt two metrics for assessing image similarity:\\n\\n- **Histogram Similarity:** The images are converted to normalized histogram format and similarity is measured using the correlation metric.\\n\\n- **Cosine Similarity:** The images are converted to feature vectors using pre-trained ViT (Dosovitskiy et al., 2020) model, and these feature vectors are used to calculate the cosine similarity score. Manually decided thresholds, as described in Figure 10, are used to judge whether the text and image pair is similar or not.\\n\\nThe text pairs are first classified into either **Support** or **Insufficient** categories, and then further sub-classified into **Support_Text**/**Support_Multimodal** or **Insufficient_Text**/**Insufficient_Multimodal** categories based on the similarity of the image pairs. If the corresponding images for the texts are similar, then they could be used to judge...\"}"}
{"id": "emnlp-2023-main-945", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"whether news is the same or not. The category where both the images and the texts are similar is called Support_Multimodal. The category where the images are similar but the texts were not is called Insufficient_Multimodal. If the corresponding images for the texts were not similar, then they could not be used to judge whether news is the same or not. The category where both the images and the texts are not similar is called Insufficient_Text. The category where the texts are similar but the images are not is called Support_Text.\\n\\nFigure 10: Text and image pair similarity based on classification thresholds on pre-trained models.\\n\\nFor the refute category, we scrape several reliable fact-check websites like Vishwas (News, 2022), Times of India (Times of India, 2022), India Today (India Today, 2022), AFP India (AFP India, 2022), AFP USA (AFP USA, 2022), AltNews (Ahmed et al., 2020), BOOM (Live), Factly (Met al.), and NewsChecker (home, 2022). For each article published on these websites, we collect the claim (sentence that states the fake news), document (text that proves claim is false), claim images (fake news image, could be screenshot of the fake post), document image (image that is proof of the fake nature of the claim).\\n\\nC Paraphrasing textual claims\\n\\nA textual given claim may appear in various different textual forms in real life, owing to variety in the writing styles of different news publishing houses. Incorporating such variations is essential to developing a strong benchmark to ensure a holistic evaluation. This forms our motivation behind paraphrasing textual claims. Manual generation of possible paraphrases is undoubtedly ideal, but that process is time-consuming and labour-intensive. On the other hand, automatic paraphrasing has received significant attention in recent times (Niu et al., 2020) (Zhang et al., 2020) (Nighojkar and Licato, 2021). As mentioned in section 3, for a given claim, we generate multiple paraphrases using various models and perform entailment using (Wang et al., 2019) \u2013 a SoTA model trained on the on SNLI task (Bowman et al., 2015) \u2013 to detect how many of them are entailed in the actual claim.\\n\\nIn the process of choosing the appropriate model based on a list of available models, the primary question we asked is how to make sure the generated paraphrases are rich in diversity while still being linguistically correct. A top level, we delineate the process followed to achieve this as follows (more details later in this section). Let's say we have a claim $c$. We generate $n$ paraphrases using a paraphrasing model. This yields a set of paraphrases, denoted by $p_c_1, \\\\ldots, p_c_n$. Next, we make pair-wise comparisons of these paraphrases with $c$, resulting in $c - p_c_1, \\\\ldots, c - p_c_n$. At this step, we identify the examples which\"}"}
{"id": "emnlp-2023-main-945", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are entailed, and only those are chosen. However, there are many other secondary factors, for e.g., a model may only be able to generate a limited number of paraphrase variations compared to others but others can be more correct and/or consistent. As such, we considered three major dimensions in our evaluation:\\n\\n(i) coverage, (ii) correctness, and (iii) diversity.\\n\\nTo offer transparency around our experiment process, we detail the aforementioned evaluation dimensions as follows.\\n\\n\u2022 Coverage - the number of considerable paraphrase generations that a model generates: We intend to generate up to 5 paraphrases per given claim. Given all the generated claims, we perform a minimum edit distance (MED) calculation at the word level instead of a character level. If MED is greater than 2 for any given paraphrase candidate (for e.g., \\\\(c_p \\\\in \\\\mathbb{C} \\\\) in the above example) with the claim then we further consider that paraphrase, otherwise discarded. We evaluated all four models based on this setup to identify the model of choice which is generating the maximum number of considerable paraphrases.\\n\\n\u2022 Correctness - correctness in paraphrase generations: After the first level of filtration, we performed pairwise entailment and kept only those paraphrase candidates, marked as entailed by the (Liu et al., 2019) (Roberta Large), SoTA trained on SNLI (Bowman et al., 2015).\\n\\n\u2022 Diversity - linguistic diversity in paraphrase generations: We are interested in choosing a model that can produce paraphrases with significant linguistic diversity. This implies that we are interested in checking for dissimilarities between generated paraphrase claims. For e.g., \\\\(p_{c_1} - p_{c_2}, p_{c_1} - p_{c_3}, p_{c_1} - p_{c_4}, \\\\ldots, p_{c_1} - p_{c_n} \\\\) \u2013 this process is repeated for all the other paraphrases and the dissimilarity score is averaged across all paraphrase generations. Since there is no standard metric to measure dissimilarity, we use the inverse of the BLEU score as a proxy metric. This gives us an understanding of the linguistic diversity of a given model.\\n\\nBased on our experiments centred around the above dimensions, we experimented with three models: (a) Pegasus (Zhang et al., 2020), (b) T5 (Flan-t5-xxl variant) (Chung et al., 2022), and (c) GPT-3.5 (gpt-3.5-turbo-0301 variant) (Brown et al., 2020) and found that GPT-3.5 (gpt-3.5-turbo-0301 was ideal. The results of our experiments are reported in table 3 below.\\n\\n| Model          | Coverage | Correctness | Diversity |\\n|----------------|----------|-------------|-----------|\\n| Pegasus        | 32.46    | 94.38%      | 3.76      |\\n| T5             | 30.26    | 83.84%      | 3.17      |\\n| GPT3.5-text-davinci-0301 | 35.51    | 88.16%      | 7.72      |\\n\\nTable 3: Evaluation dimensions of textual claim paraphrasers.\\n\\nBased on our experiments: Pegasus and T5 produced fewer paraphrases compared to GPT-3.5. GPT-3.5 was the most diverse and generated the highest number of paraphrases. The results are reported in Table 3 above.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D Visual paraphrasing using Stable Diffusion\\n\\nBuilding upon section 4, we highlight the process behind visual paraphrasing in this section. Diffusion models are machine learning models that are trained to denoise random gaussian noise step by step to get a sample of interest, such as an image. However one of the major downsides of diffusion models is that the denoising process is both time and memory consumption are very expensive. The main reason for this is that they operate in pixel space which becomes unreasonably expensive, especially when generating high-resolution images. Stable diffusion was introduced to solve this problem as it depends on Latent diffusion. Latent diffusion reduces the memory and computational cost by applying the diffusion process over a lower dimensional latent space instead of on the actual pixel space. It is trained with the objective of \u201cremoving successive applications of Gaussian noise to training images\u201d, and can be considered as a sequence of denoising autoencoders.\\n\\nQuality control is a big reason to worry when paraphrasing automatically. There are two aspects we have tested for the available models - (i) variations, and (ii) the number of paraphrases generated.\\n\\nFigures 18c, 18d are some examples of the advanced capabilities of the model and how it can be used (\u201cmisused\u201d) to generate fake news. Specifically, these examples highlight events that are fake and solely rely on the uncanny ability of Stable Diffusion to generate realistic art.\\n\\nD.1 Explainability of generated images\\n\\nIn table 9, we can see that for the word slapped, the driver\u2019s cheek and Will Smith\u2019s hand are getting highlighted. DAAM (Tang et al., 2022), which provides cross-attention interpretation of syntactic textual relations in visual object interactions fosters explainability of our dataset.\\n\\nE Assessment of Stable Diffusion generated images\\n\\nWhile Stable Diffusion has received great acclaim owing to its stellar performance for a variety of use cases, to our knowledge, we are the first to adopt it for fake news generation. As such, to assess the quality of generated images in the context of the fake news generation task, we utilize two evaluation metrics.\\n\\nE.1 FID & Relevance Score-based quantitative assessment of Stable Diffusion generated images\\n\\nWhile Stable Diffusion has received great acclaim owing to its stellar performance for a variety of use cases, to our knowledge, we are the first to adopt it for fake news generation. As such, to assess the quality of generated images in the context of the fake news generation task, we utilize two evaluation metrics - i) FID (Heusel et al., 2017) and ii) Relevance Score (Hao et al., 2022) - details are discussed in the following paragraphs.\\n\\nFID Score:\\n\\nTo compute the FID scores, we first filter out the claims from our dataset that consist of person entities by leveraging the BERT-base-NER model. Following the process adopted in (Borji, 2022), we ran the Mediapipe (Lugaresi et al., 2019) face detector twice: first on the entire image to detect faces, and thereafter on the individual detections to prune false positives, to extract faces from the real and Stable Diffusion generated images corresponding to the filtered set of claims. We then compute the FID between the set of faces extracted from the real and Stable Diffusion generated images using the clean-fid package released by (Parmar et al., 2021).\\n\\nRelevance Score: [work in progress]\"}"}
{"id": "emnlp-2023-main-945", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 12: In this example, for the claim: \\\"Former President George W. Bush congratulates President-elect Joe Biden, says the election was 'fundamentally fair' and 'its outcome is clear',\\\" the left image where only Joe Biden is visible is the original claim image, and on the right where George Bush and Joe Biden are visible is the SD generated one. To assess the quality of the generated image, we have calculated the pairwise FID score. First, we extract the faces using Mediapipe (Lugaresi et al., 2019) Face Detector for the real and Stable Diffusion generated image for each claim. We then compute the FID using clean-fid (Parmar et al., 2021) pairwise. Then for a set of 500 randomly selected samples, we average out the pairwise FID scores. It is 8.67, demonstrating a good match overall. The difference between the left image vs. the right one is the number of faces. In such a case, we take the best (lowest) FID score as the FID score for that claim. In this way, we make sure what is the common minimum between an AI-generated image vs. an actual news image.\\n\\nWe use the relevance score as introduced in (Hao et al., 2022) to measure whether the generated images are relevant to the original input prompt. We compute CLIP (Radford et al., 2021b) similarity scores to measure how relevant the generated images and the original input prompts are. The resulting relevance score is defined as:\\n\\n\\\\[\\n\\\\begin{align*}\\n    f_{rel}(x,y) &= E_i y \\\\sim G(y) \\\\left[ \\\\min(20 \\\\times g_{CLIP}(x,iy) - 5.6, 0) \\\\right]\\n\\\\end{align*}\\n\\\\]\\n\\nwhere, \\\\(i y \\\\sim G(y)\\\\) means sampling images \\\\(i y\\\\) from the text-to-image model \\\\(G\\\\) with \\\\(y\\\\) as input prompt, and \\\\(g_{CLIP}(\\\\cdot, \\\\cdot)\\\\) stands for the CLIP similarity function.\\n\\nSecond, we employ aesthetic predictor as discussed in (Hao et al., 2022) to quantify aesthetic preferences. The aesthetic predictor (Schuhmann, 2022) builds a linear estimator on top of a frozen CLIP model, which is trained by human ratings in the Aesthetic Visual Analysis [MMP12] dataset. The aesthetic score is defined as:\\n\\n\\\\[\\n\\\\begin{align*}\\n    f_{aes}(x,y) &= E_i x \\\\sim G(x), iy \\\\sim G(y) \\\\left[ g_{aes}(iy) - g_{aes}(ix) \\\\right]\\n\\\\end{align*}\\n\\\\]\\n\\nwhere, \\\\(g_{aes}(\\\\cdot)\\\\) denotes the aesthetic predictor, and \\\\(iy, ix\\\\) are the images generated by the prompts \\\\(y\\\\) and \\\\(x\\\\), respectively.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section delineates the process followed to assess the quality of synthetically generated images, given the prompt used for a generation as context to the human rater. Specifically, we asked 10 raters to assign an integral score from 1 (bad quality) to 5 (excellent quality) to the generated images in the context of the given prompt. Specifically, similar to (Chambon et al., 2022), the scoring system was verbalized as follows:\\n\\n\u2022 Life-like generated image with potentially minor error elements, but practically indistinguishable from an original.\\n\u2022 Good generated image with noticeable errors not influencing the claim\u2019s veracity assessment.\\n\u2022 Moderate errors in the generated image with possible minor negative impacts on the claim\u2019s veracity assessment.\\n\u2022 Errors leading to hallucinated lesions while still preserving the major theme of the claim but influencing the claim\u2019s veracity assessment.\\n\u2022 Severe errors such as the generated image not following the prompt\u2019s major theme resulting in the claim\u2019s veracity assessment being impossible.\\n\\nThe raters rated the CLIP re-ranked output for each prompt (so 500 images in total), presented in a randomized fashion. As part of a pilot study, we assessed the calibration procedure and the test-retest reliability of 10 raters on a subset of 500 generated images by adding a generated image twice to a larger test set, similar to (Ledig et al., 2017). We observed good reliability and no significant differences between the ratings of the identical images.\\n\\nA typical SRL system first identifies verbs in a given sentence and then marks all the related words/phrases with relational projection with the verb and assigns appropriate roles. Thematic roles are generally marked by standard roles defined by the Proposition Bank (generally referred to as PropBank) (Palmer et al., 2005), such as: Arg0, Arg1, Arg2, and so on. We propose a mapping mechanism to map these PropBank arguments to 5W semantic roles (refer to the conversion table 4).\\n\\nNot necessarily all the Ws are present in all the sentences. To understand this sparseness, a detailed analysis of the presence of each of the 5W at the sentence level has been done and reported in figure 13.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: A mapping table from PropBank (Palmer et al., 2005) (Arg0, Arg1, ...) to 5W (who, what, when, where, and why).\\n\\nEntire Dataset FaVIQ HoVer FEVER VitaminC Factify 1.0 Factify 2.0\\nwho 24.84 23.37 25.84 24.44 24.96 ... 0.14 0.7 1.05 0.88\\nwhat 10.27 13.37 9.14 8.59 11.31 9.22 10.29\\nwhy 8.68 11.92 9.36 6.14 9.75 7.36 8.81\\nwhen\\nwhere\\n\\nFigure 13: Sentence level co-occurrence of Ws.\\n\\nF.1 Human evaluation of 5W SRL\\n\\nIn this study evaluation for the 5W Aspect, based on semantic role labelling is conducted using mapping accuracy. This involves accuracy on SRL output mapped with 5Ws.\\n\\nFor the purpose of finding how good the mapping of 5W with semantic roles and generation of semantic roles, human annotation of 3500 data points was conducted, 500 random datapoints from the entire dataset, 500 each from FEVER (Thorne et al., 2018), FavIQ (Park et al., 2021), HoVer (Jiang et al., 2020), ViTC (Schuster et al., 2021), Factify 1.0 (Mishra et al., 2022) and Factify 2.0 (Mishra et al., 2023), see table 5.\\n\\nG 5W QA pairs generation using language model\\n\\nFor the QG task, we shortlisted two pre-trained top-performing models for question generation according to the papers with code leaderboard where the model and code have been released. These models were fine-tuned on various SQuAD datasets (Rajpurkar et al., 2018) by simply appending the answer to the context. A random sampling on 352k data points was done to get of 15% of the datapoint to find the best question-generating model with respect to 5W. For example, given an answer from \\\"who\\\" based on semantic role labeller and context from the claim, it should generate questions containing \\\"who\\\" and not other Ws. By modelling the claims as context and the outputs from the SRL models as answers, the process of generating 5W questions for the task of fact verification was accomplished. The pre-trained models we utilized for QG are as follows:\\n\\n\u2022 BART: BART (Lewis et al., 2019) is a denoising autoencoder for pretraining sequence-to-sequence models, trained by (i) corrupting text with an arbitrary noising function, and (ii) learning a model to reconstruct the original text. BART was trained to generate questions in two ways: casual generation and context-based generation. For this task, we used the bart-squad-qg-hl variant focusing on context-based generation. This variant of BART scored 24.15, 25.43, and 52.64 on the BLEU4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), and ROUGE-L metrics (Lin, 2004), respectively, whereas the current state-of-the-art (SoTA) of the BART model from Textbook 2.0 scores 25.08, 26.73, and 52.55 on the same metrics.\\n\\n\u2022 ProphetNet: ProphetNet (Qi et al., 2020) is a generative model that uses multi-lingual pre-training with masked span generation to create shared latent representations across languages. It generates all the\"}"}
{"id": "emnlp-2023-main-945", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"masked spans together, given an input sequence, and uses a future n-gram loss to prevent overfitting on strong local correlations. ProphetNet is optimized through an $n$-step look-ahead prediction, which predicts the next $n$ tokens based on previous context tokens at each time step, encouraging the model to explicitly plan for future tokens. It was evaluated on benchmarks for abstractive summarization and question generation tasks such as CNN/DailyMail, Gigaword, and SQuAD 1.1 (Rajpurkar et al., 2016).\\n\\nProphetNet has a 12-layer encoder and 12-layer decoder with 1024 embedding/hidden size and 4096 feed-forward filter size. The batch size and training steps were set to 1024 and 500K, respectively, and Adam optimization was used with a learning rate of $3 \\\\times 10^{-4}$ for pre-training. The input length was set to 512 and masking was done randomly in continuous spans every 64 tokens, with 15% of the total number of tokens masked.\\n\\n5W QA pair generation is a result of two submodules: (i) 5W SRL, and (ii) 5W-based QA pair generations. We have used pretrained models of context-based question generation models, wrapped in automation infrastructure. Contexts are the actual claim, and the answers are the Semantic Role Labeling outputs. As an example, let's consider a claim, \\\"After April 11, 2020, there was a fatality rate of over 1.61 in Malaysia during the coronavirus pandemic\\\". After applying SRL, we obtain the answer to the \\\"When\\\" of the input sentence, yielding \\\"After April 11, 2020\\\". Next, we feed the answer obtained in the prior step (After April 11, 2020) along with the context (\\\"After April 11, 2020, there was a fatality rate of over 1.61 in Malaysia during the coronavirus pandemic.\\\") as the input to the model. Finally, this yields a question starting with \\\"When\\\", which in this case is \\\"When was the COVID-19 pandemic?\\\". We provide a variety of examples in section G for readers to additionally look into.\\n\\nG.1 Human evaluation of 5W SRL and QA generation\\n\\nFor the evaluation purpose, a random sample of 3500 data points was selected for annotation. The questions generated using the Prophetnet model were utilized for this purpose. The annotators were instructed to evaluate the question-answer pairs in three dimensions: the question is well formed, which means it is syntactically correct, the question is correct which means it is semantically correct with respect to the given claim, and extracted answer from the model is correct. The evaluation results for the datasets are presented in the following analysis, see table 6.\\n\\n|                | FaVIQ | FEVER | HoVer | VitaminC | Factify 1.0 | Factify 2.0 |\\n|----------------|-------|-------|-------|----------|-------------|-------------|\\n| **Who**        | 89%   | 85%   | 90%   | 87%      | 86%         | 82%         |\\n| **What**       | 85%   | 56%   | 68%   | 78%      | 81%         | 93%         |\\n| **When**       | 86%   | 90%   | 95%   | 98%      | 83%         | 75%         |\\n| **Where**      | 93%   | 100%  | 90%   | 97%      | 93%         | 86%         |\\n| **Why**        | 0%    | -     | 100%  | 92%      | 87%         | 93%         |\\n\\nTable 5: Human evaluation of 5W SRL; It is observed that for most of the datapoints why is missing.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"treated as the gold standard for comparison between claim and evidence. We experimented with three models, T5-3B (Raffel et al., 2020), T5-Large (Raffel et al., 2020), and Bert-Large (Devlin et al., 2018).\\n\\nThe T5 is an encoder-decoder-based language model, that treats this task as text-to-text conversion, with multiple input sequences and produces an output as text. The model is pre-trained using the C4 corpus (Raffel et al., 2020) and fine-tuned on a variety of tasks. T5-Large employs the same encoder-decoder architecture as T5-3B (Raffel et al., 2020), but with a reduced number of parameters. The final model that we experimented with is the Bert-Large (Devlin et al., 2018) model, which utilizes masked language models for pre-training, enabling it to handle various downstream tasks and represent both single and pairs of sentences in a single token sequence. It is trained using MLM and a binarized next-sentence prediction task to understand sentence relationships.\\n\\nI Selecting the best combination - 5W QAG vs. 5W QA validation\\n\\nWe have utilized off-the-self models both for 5W question-answer generation and 5W question-answer validation. Given that the datasets using for training the models bear an obvious discrepancy in terms of the distribution characteristics compared to our data (world news) which would probably lead to a generalization gap, it was essential to experimentally judge which system offered the best performance for our use-case. Instead of choosing the best system for generation vs. validation, we opted for pair-wise validation to ensure we chose the best combination. Table 7 details our evaluation results \u2013 the rows denote the QA models while the columns denote QAG models. From the results in the table, we can see that the best combination in terms of a QAG and QA validation model was identified as T5-3b and ProphetNet respectively.\\n\\n|                  | Claim +Paraphrase | Claim +Paraphrase | Claim +Paraphrase | Claim +Paraphrase | Claim +Paraphrase | Claim +Paraphrase | Claim +Paraphrase |\\n|------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|-------------------|\\n|                  | Bleu              | Rouge             | Recall            | F1                | Bleu              | Rouge             | Recall            | F1                |\\n| T5-3b            | 29.22             | 48.13             | 35.66             | 38.03             | 28.13             | 46.18             | 34.15             | 36.62             | 21.78             | 34.53             | 28.03             | 28.07             | 20.93             | 33.57             | 27.65             | 27.24             |\\n| T5-Large         | 28.81             | 48.02             | 35.26             | 37.81             | 21.46             | 46.45             | 27.19             | 36.76             | 21.46             | 34.90             | 27.41             | 33.69             | 20.88             | 27.31             |\\n| Bert-Large       | 28.65             | 46.25             | 34.55             | 36.72             | 27.27             | 44.10             | 32.95             | 35.20             | 20.66             | 33.19             | 25.51             | 26.44             | 19.74             | 32.34             | 25.14             | 25.71             |\\n\\nTable 7: Selecting the best combination - 5W QAG vs. 5W QA validation\\n\\nInjecting adversarial assertion for fake news\\n\\nThe extraordinary capabilities of today's large language models to generate realistic text based on prompts has had an electrifying impact on the scientific community. Per (Story, 2022), \u201cHuman reviewers could only detect fake abstracts [of scientific articles] 68% of the time\u201d. Given these major advances in language models, it is even easier today to generate and propagate misinformation in the form of fake news that would be extremely difficult, even for human experts, to detect as false without the proper tools to verify its authenticity.\\n\\nWe have thus included some fake news claims synthetically generated by OPT in our dataset to provide a more realistic view of news media in recent times. This adversarial attack would help build more robust fact verification models if they are able to detect these fake claims.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 14: Representation of Human vs Machine\\nFigure 15: Representation of Right vs Wrong verdicts\\n\\nK Additional Figures\\n\\nFigure 16: Text-only baseline model which takes only claim text and document text as input.\\n\\nFigure 17: Heatmap of MOS scores with 500 assessed samples for each category.\"}"}
{"id": "emnlp-2023-main-945", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sports star Magic Johnson came to the hospital last month to donate blood to support the COVID-19 crisis.\\n\\nWho QA-pair\\n\\n- Who went to the hospital? Magic Johnson\\n- Who worked with whom? the author with Magic Johnson\\n- Who took the photo? the author\\n\\nWhat QA-pair\\n\\n- What did Magic Johnson do at the hospital? donate blood\\n- What process Magic Johnson was part of? blood donation\\n\\nWhen QA-pair\\n\\n- When did Magic Johnson visit the hospital? last month, time of the post = Sept - 1 month from August\\n\\nWhere QA-pair\\n\\n- Where did Magic Johnson pay visit to? hospital.\\n\\nWhy QA-pair\\n\\n- Why did Magic Johnson visit hospital? to donate blood.\\n\\nFirst volunteer in UK coronavirus vaccine trial has died.\\n\\nWho QA-pair\\n\\n- Who died in the coronavirus vaccine trial? First volunteer in UK coronavirus vaccine trial\\n\\nWhere QA-pair\\n\\n- Where did the volunteer die? (blank)\\n\\nWhy QA-pair\\n\\n- Why did the volunteer die? (blank)\"}"}
{"id": "emnlp-2023-main-945", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kamala Harris said that the new and proposed state laws on voting mean \\\"if you are going to be standing in that line for all those hours, you can't have any food.\\\"\\n\\nWho: Kamala Harris\\n\\nWhat: \\\"if you are going to be standing in that line for all those hours, you can't have any water or any food.\\\"\\n\\nWhere: in line\\n\\nModerna's lawsuits against Pfizer-BioNTech show COVID-19 vaccines were in the works before the pandemic started.\\n\\nWho: Moderna lawsuits against Pfizer-BioNTech\\n\\nWhat: COVID-19 vaccines were in the works before the pandemic started\\n\\nWhen: before pandemic\\n\\nGreta Thunberg advised the Chinese to quit Chopsticks to save trees\\n\\nWho: Greta Thunberg, the Chinese\"}"}
{"id": "emnlp-2023-main-945", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Who QA-pair       | What QA-pair                             | When QA-pair | Where QA-pair | Why QA-pair |\\n|-------------------|------------------------------------------|--------------|---------------|-------------|\\n| ('who advised the chinese to quit chopsticks?', 'Greta Thunberg'), ('who did greta thunberg advise to quit chopsticks?', 'the Chinese') | ('what did greta thunberg advise the chinese to quit?', 'Chopsticks'), ('what did greta thunberg want the chinese to quit chopsticks to save?', 'trees') | ,            | ,             | ,           |\\n\\nTable 8: 5WQA using SRL for above examples\\n\\n15312\"}"}
{"id": "emnlp-2023-main-945", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Elon Musk got married to Kamala Harris in California on Thursday. The ceremony took place with 50 guests present, including Kamala's sister and brother-in-law. Apart from Joe Biden all were present from the White House. Kamala's ex-husband played the saxophone at the ceremony. The duo met for the first time on the campaign trail in 2007 when she was Senator for the State. Elon proposed to Kamala Harris in August 2019 on a beach. Kamala accepted and has been dating him ever since.\"}"}
