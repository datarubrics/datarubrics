{"id": "lrec-2022-1-120", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Out-of-Domain Evaluation of Finnish Dependency Parsing\\n\\nJenna Kanerva and Filip Ginter\\nTurkuNLP, Department of Computing\\nUniversity of Turku, Finland\\n{jmnybl, figint}@utu.fi\\n\\nAbstract\\nThe prevailing practice in the academia is to evaluate the model performance on in-domain evaluation data typically set aside from the training corpus. However, in many real world applications the data on which the model is applied may very substantially differ from the characteristics of the training data. In this paper, we focus on Finnish out-of-domain parsing by introducing a novel UD Finnish-OOD out-of-domain treebank including five very distinct data sources (web documents, clinical, online discussions, tweets, and poetry), and a total of 19,382 syntactic words in 2,122 sentences released under the Universal Dependencies framework. Together with the new treebank, we present extensive out-of-domain parsing evaluation utilizing the available section-level information from three different Finnish UD treebanks (TDT, PUD, OOD). Compared to the previously existing treebanks, the new Finnish-OOD is shown include sections more challenging for the general parser, creating an interesting evaluation setting and yielding valuable information for those applying the parser outside of its training domain.\\n\\nKeywords: Finnish, Universal Dependencies, Treebank, Parsing, Out-of-domain\\n\\n1. Introduction\\nDuring software development and model performance evaluation, the prevailing practice in the academia is to evaluate the model performance on an in-domain dataset. This typically means that the model is evaluated on a test section set aside from the training corpus, therefore the test dataset sharing the same properties as the data used to train the model. However, in many real world applications this may not be the case. The data on which the model is applied in its actual use in downstream applications may in practice very substantially differ from the characteristics of the training data. In this paper, we focus on Finnish dependency parsing in the Universal Dependencies (UD) scheme. When both trained and tested on the UD dataset, the state of the art is approaching human performance (Virtanen et al., 2019). Consequently, the Finnish parser is in active use in the academia as well as in the commercial industry, and applied in numerous downstream tasks and domains as a text normalization and pre-processing component. In some cases, however, these application domains substantially differ in their characteristics from the training corpus and there is no hard evidence as to the effect on the parser performance.\\n\\nTo address this question, we selected and annotated a new treebank meant solely for out-of-domain evaluation of the models trained on the UD Finnish-TDT dataset. This new UD Finnish-OOD treebank allows us to quantify the parsing performance in various downstream applications, and to better understand the limits of generalization exhibited by the most recent dependency parsing methodology. In addition to introducing the new dataset, we carry out several Finnish out-of-domain parsing experiments, where in addition to the presented Finnish-OOD treebank, we use the existing section-level metadata in order to carry out section level performance evaluation. We show the new Finnish-OOD treebank being more challenging compared to the different sections existing in the current treebanks available for Finnish in the UD collection.\\n\\n2. Data Sources\\nThe three UD treebanks presently available for Finnish, namely Finnish-TDT (Haverinen et al., 2014; Pyysalo et al., 2015), Finnish-PUD (Zeman et al., 2017), and Finnish-FTB (Voutilainen et al., 2010), represent 6 different text genres based on the UD genre classification: blogs, fiction, grammar examples, legal, news and Wikipedia.\\n\\nThe UD Finnish-TDT is a general domain treebank containing 202,453 syntactic words (15,136 sentences) from 10 different text sources: Wikipedia articles, online fiction, JRC-Acquis legislation, popular online blogs, EuroParl speeches, grammar examples, Wikinews, university news, economy news, and student magazine articles. The treebank is divided into training, development and test set, the training set of the TDT treebank being the primary training data used throughout all experiments reported in this study. The UD Finnish-PUD is the Finnish part of the parallel UD treebank collection annotating the same underlying text translated for multiple languages. It includes 15,317 words (1,000 sentences) from two text sources: Wikipedia and news. The Finnish-PUD is used as external test set in this study. The UD Finnish-FTB is a treebank of grammar book examples annotated as a separate effort, independently of the other two Finnish UD treebanks and its annotation is not compatible in many important details with the abovementioned treebanks. Since these incompatibilities would mask any interesting differences, we do not use the FTB treebank\"}"}
{"id": "lrec-2022-1-120", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in this study. Nevertheless, the Grammar examples section of the TDT treebank is in fact sampled from the FTB text material, and therefore the FTB treebank text domain is represented in our experiments.\\n\\nIn order to build the new out-of-domain treebank for Finnish UD parsing we consider four text genres defined in UD v2.8 genre classification (see e.g. (Zeman et al., 2021), (M\u00fcller-Eberstein et al., 2021)) but absent from the previously available Finnish treebanks: medical, poetry, social and web. Under these four genres, we include documents from five different text sources: (1) clinical nursing narratives of hospital patients for the medical domain, (2) web documents manually identified to contain poems or song lyrics for poetry, (3) discussion forum messages and (4) tweets for the social domain, and (5) randomly sampled documents from a general internet crawl for web. Of these, especially the nursing narratives, poetry, and tweets differ very substantially from any text in the training data. Yet, in particular the clinical domain and tweets represent a typical application domain for the Finnish parser.\\n\\nIn the following, we describe the data collection and cleaning procedure for each data source separately.\\n\\n2.1. Medical \u2013 Clinical Nursing Narratives\\n\\nIn clinical nursing narratives the patient's visit in the hospital is recorded in a free text narrative, where the document is amended by nurses throughout their shifts to describe the patient's condition, treatments and status development during the stay in the hospital. These nursing records are meant for medical professionals to help in clinical decision making, and due to the fact that the records are targeted to professionals, the text is heavy on special terminology. Additionally, the nature of nursing narratives substantially differ from general language use, nursing narratives oftentimes including only critical facts expressed in a sentence without a main verb rather than carefully edited sentences. Laippala et al. (2014) described the key characteristics in nursing narratives including frequent misspellings, abbreviations, domain terminology, telegraphic writing style and non-standard syntactic structures.\\n\\nSuominen et al. (2009) collected a corpus of nursing narratives of Finnish intensive care unit patients in the Turku University Hospital during years 2005-2006. In this work the nursing narratives of selected patients (those whose stay in the intensive care unit was at least 5 days) were extracted from the hospital's electronic patient records. Due to the obvious considerations on personal health data, this corpus is not available online. However, later on, a small section of this corpus consisting of 8 full narratives was manually anonymized and made openly available with annotation in the Stanford Dependencies (SD) scheme (Haverinen et al., 2009; Haverinen et al., 2010). While the dependency relations were manually annotated in this clinical Finnish treebank, the segmentation, morphology and lemma annotation layers were only automatically predicted. In this work, we sample two full narratives (939 sentences) from the original clinical treebank, and manually re-annotate them into UD scheme, this time including manual annotation of all layers, i.e. morphological tags, lemmas, and syntax. The original clinical treebank data is available only with automatic segmentation, and since we do not have access to the original corpus used to obtain the anonymized records, the information on original text is lost. In order to include at least a minimal support towards testing segmentation models on the medical data, we apply manual detokenization, where e.g. punctuation markers are reconnected with the previous tokens when applicable, and thus the text is \u201ccorrected\u201d to reflect orthographic standards in the original corpus. Misspellings and other segmentation issues that we could assume not to be introduced by the original tokenizer were left as-is. By doing this, we recognize the issue of the detokenized data not fully reflecting the possible variation of misspellings in cases where it was unclear whether the error was introduced by the original writer or the automatic tokenizer.\\n\\n2.2. Poetry \u2013 Poems and Song Lyrics\\n\\nIn our poetry subsection, we rely on web documents manually identified to include poems or song lyrics. These documents are drawn from the FinCORE corpus (Laippala et al., 2019), where a random sample of Finnish web crawled documents are manually labeled for their text register, using 8 top-level labels (narrative, opinion, informational description, interactive discussion, how-to/instructional, informational persuasion, lyrical, and spoken) and several subcategories. We extract all documents manually labeled as lyrical in the FinCORE corpus, denoting a top-level category including both poems and song lyrics. At the time of data collection, we were able to identify 6 documents (144 sentences) with the lyrical label.\\n\\n2.3. Social \u2014 Tweets\\n\\nThe Finnish tweets were downloaded between years 2016-2018 using the Twitter streaming API. During downloading, we keep only tweets labeled as Finnish by the Twitter's language recognition (available in the tweet json). However, we observed the downloaded dataset to include a large number of tweets incorrectly labeled as Finnish, and therefore we manually identified all Finnish tweets from a small sample of 1250 tweets randomly selected among all downloaded tweets. This manual curation step discarded over 50% of all sampled tweets, indicating the language identification labels not being accurate enough for selecting Finnish tweets. Finally, 130 randomly sampled\\n\\n1 We used the Tweepy Library https://github.com/tweepy/tweepy.\\n\\n2 All sampled tweets with their manually annotated labels are available at https://github.com/TurkuNLP/finnish-tweets-lang-identification for any\"}"}
{"id": "lrec-2022-1-120", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tweets from the curated dataset proceeded into the manual morphosyntactic annotation. Likely due to change in Twitter character limits in 2017, the main text field in the downloaded tweet json sometimes contains a truncated version of the tweet. Similarly for retweets, the main text field includes a retweet marker (RT @USERNAME:), and the actual tweet can become truncated. In both cases, we always extract the full tweet text rather than the truncated one. This also has the property of not including the retweet marker as part of the extracted text, but retaining the information in the corpus metadata. This strives to mimic the textual content of a tweet as the user would see it through the online interface.\\n\\n2.4. Social \u2013 Discussion Forum Messages\\n\\nThe second subset of social network data is gathered from the Suomi24 corpus, containing all messages posted in the Finnish Suomi24 online discussion forum between years 2001 and 2017. Historically, it has been one of the largest social network forums in Finland and covers a broad range of discussion topics including language with wide range of different writing styles and formality. From this dataset, we randomly sample 51 different messages for manual annotation, where messages may be anything between quick reactions to previous messages to longer posts on any number of different topics.\\n\\n2.5. Web \u2013 Random Sample of the Internet Crawl\\n\\nFor the web domain, we take a random sample of 30 documents from the Finnish Internet Parsebank (Luotolahti et al., 2015). Five documents manually determined to be machine translated, thus, including many incomprehensible sentences, were replaced with new documents during sampling. Due to many web documents being quite long, each document was truncated after 25 sentences in order to avoid overly long documents biasing the web data towards particular topics. Furthermore, unnatural repetition appearing in some documents was removed (e.g. repeating quotation blocks) to avoid artificially skewing the evaluation statistics, and in these cases, more sentences were taken from the same document until the 25 sentence limit was reached.\\n\\n3. Treebank Annotation\\n\\nThe data was annotated by a single annotator with a long-term experience in Finnish UD treebanking and the sole maintainer of the UD Finnish-TDT corpus. In the annotation, the Universal Dependencies guidelines were used as adapted in the Finnish-TDT corpus, thus later experiments on language identification of Finnish tweets.\\n\\n3.1. Clinical Nursing Narratives\\n\\nWhile some of the medical terms used in the clinical nursing narratives are easily understandable to readers without professional medical knowledge, some terms require domain-specific understanding in order to correctly determine their meaning. Clearly, the annotation of morphological features and dependency relations for such terms is difficult for a person working outside the domain. While many of such terms are available in different medical dictionaries, especially highly abbreviated versions of medical terms are oftentimes difficult to find. In order to support the corpus annotation, we start the annotation process by translating all domain-specific terms into a general language with the help of a trained nurse. These translations are included as additional annotation in the MISC field of the CoNLL-U file, where the translations could be provided on word-to-word basis (Gen=Translation). An informative description of a concept is included instead in the MISC field in cases where a word-to-word translation is not feasible (Gen desc=Description). In general, the medical domain is quite rare in UD treebanks, in addition to ours, only 6 UD treebanks are reported as including medical texts. In fact, in all of these 6 treebanks (Czech-CAC (Raab, 2008), French-Sequoia (Candito and Seddah, 2012), KicheIU (Tyers and Henderson, 2021), Persian-Seraji (Seraji et al., 2016), Romanian-RRT (Mititelu, 2018), and Romanian-SiMoNERo (Mititelu and Mitrofan, 2020)),...\"}"}
{"id": "lrec-2022-1-120", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the medical texts are reported to be based on scientific or technical writings from the field of medicine, thus being carefully edited, official publications. In contrast, the clinical nursing narratives used in our corpus are quickly drafted notes written to other professionals and not meant to be publicly shared, making the nature of our medical texts very different from other UD tree-banks. However, after dealing with the terminology, the rest of the annotation work was quite straightforward.\\n\\n3.2. Poetry\\n\\nWhile annotating texts from the poetry genre, one feature clearly standing out was the usage of capitalization and line breaks to articulate the layout of the text (indicating rhythm) rather than following standard structure of dividing text into paragraphs and sentences. In some documents, this resulted in having long text passages without punctuation characters indicating the standard sentence or clause structures.\\n\\nSimilar to medical, poetry is also among one of the rarest genres in UD. In addition to our tree-bank only 6 datasets are reported as including it: Belarusian-HSE, Breton-KEB (Tyers and Henderson, 2021), Latin-UDante (Cecchini et al., 2020), Old French-SRCMF (Stein and Prevost, 2013), Romanian-Nonstandard (Maranduc and Bobicev, 2017), and Russian-Taiga (Shavrina and Shapovalova, 2017).\\n\\nWhile the segmentation of poetry texts is not explicitly mentioned in the studies or UD specifications, we follow a similar principle that seems to be the consensus in other UD treebanks, based on our understanding of the examples available in the papers, as well as inspecting annotated sentences in the released datasets. We segment the texts into sentences following the existence of the sentence-final punctuation rather than capitalization or single line breaks, as oftentimes the text after a single newline was evidently a continuum of the previous sentence (fitting e.g. dependency relations). In such cases where the sentence continuation was semantically ambiguous (full stop could have been easily used to break the sentence), these segments are connected with the parataxis relation marking for side-by-side clauses without coordination, subordination or argument relation. An exception, where we follow line breaks rather than punctuation, is made with double newlines (indicating paragraph or stanza boundary), where the sentence boundary is annotated even without an explicit sentence-final punctuation.\\n\\n3.3. Tweets\\n\\nTweets include several characteristics rather unique to limited social media channels, the most common being mentions (@username) and hashtags (#hashtag), while also URLs and emoticons are substantially more frequent in tweets than in many other genres in the Finnish treebanks. Also, due to the character limits in social media platforms, tweets are rather short documents typically including only one or two short sentences. Likely due to this reason, in many tree-banks including Twitter data, tweets are considered to be single sentence units and further sentence splitting is not applied (see e.g. the data releases of Italian-PoSTWITA (Sanguinetti et al., 2017), Italian-TWITTIRO (Cignarella et al., 2019), or Tweebank by Liu et al. (2018)). However, based on manual annotation 35% of the tweets in our sample include more than one sentence, 72% of sentences in these multi-sentence tweets containing a predicate in the main clause, thus indicating the individual sentences more often being real sentence-like units rather than short noun phrases.\\n\\nAs the CoNLL-U format supports indicating document structure as metadata, we do not want to artificially analyse tweets as single sentences, when similar text passages in any other genre would be segmented into multiple sentences. Therefore, we consider a tweet as a small document which is further segmented into sentences as necessary. However, special tokens (mentions and hashtags) as well as plain interjections (e.g. Wonderful!) in the beginning or end of the tweet are kept together with the corresponding sentence. Regarding interjections, a similar approach is applied also in the Finnish-TDT treebank, thus not making deviation to the original annotation scheme. Regarding token segmentation, we treat mentions and hashtags as single tokens, where the special characters (@ or #) are simply part of the main token. Otherwise standard tokenization guidelines are applied.\\n\\nWhile there are several studies involving UD annotation on tweets (see e.g. Sanguinetti et al. (2017), Liu et al. (2018), Bhat et al. (2018), and Blodgett et al. (2018)), there does not seem to be a clear consensus regarding the annotation of tokens specific to Twitter or other social media platforms. Mentions are usernames appearing typically at the beginning of the sentence to mark dialogue participant in addressed speech, or occasionally replacing a normal content-bearing word in the sentence, usually when referring to an entity which would otherwise be a proper name (e.g. person or company name). Hashtags have a similar distinction, where most of the hashtags are used as a list of topical keywords appearing in the beginning or at the end of the sentence, however, some can be used as normal content-bearing words to replace any normal token in the sentence. In Figure 1 we illustrate a typical tweet taken from the corpus.\\n\\nWhile Sanguinetti et al. (2017) and Bhat et al. (2018) annotated mentions with SYM part-of-speech tag, Liu et al. (2018) used PROPN, however, all agreeing on using vocative dependency relation for those mentions appearing in the beginning of the tweet to address the dialogue participant. As mentions are references to Twitter usernames and thus can be considered as proper\"}"}
{"id": "lrec-2022-1-120", "page_num": 5, "content": "{\"primary_language\":\"fi\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Peliriippuvuudesta tuli viimein #oikea #sairaus \u2013 WHO lis\u00e4si virallisiin tautiluokituksiin #peliaddiktio\\n#peliriippuvus #WHO #tautiluokitus https://t.co/P8wSQZzW45\\n\\nGaming disorder finally became a #real #disease \u2013 WHO added (it) to the official classification of diseases\\n#gamingdisorder #gamingaddiction #WHO #classificationofdiseases https://t.co/P8wSQZzW45\\n\\nFigure 1: An example of a typical tweet including hashtags both as replacing normal, content-bearing words (#oikea/#real and #sairaus/#disease) as well as listing topical keywords at the end.\\n\\nnames, we opted for labeling all mentions with PROPN on the part-of-speech level, while the syntactic relation depends on how the token is used. For mentions addressing the dialogue participant we follow the other treebanks by annotating them with the vocative dependency relation, while those used as content-bearing words are annotated with their corresponding function in the sentence (e.g. subject or object).\\n\\nHashtags are annotated in various ways in the released treebanks. Sanguinetti et al. (2017) and Bhat et al. (2018) analyse all hashtags as symbols (SYM), whereas Liu et al. (2018) uses X for topical hashtags, while annotating content-bearing hashtags as any normal tokens. In terms of relations, both Sanguinetti et al. (2017) and Liu et al. (2018) use the corresponding relations in the sentence for content-bearing hashtags, while Bhat et al. (2018) and Blodgett et al. (2018) do not distinguish content-bearing hashtags from the topical ones. The topical hashtags are annotated as parataxis (Sanguinetti et al., 2017; Blodgett et al., 2018), or discourse (Liu et al., 2018; Bhat et al., 2018). We opted for analysing hashtags with their corresponding part-of-speech tags when the token is an actual Finnish word (i.e. #beautiful would be an adjective and #forest a noun). However, in some cases giving a real part-of-speech analyse for a hashtag is not meaningful, this would be the case for example with foreign words or tokens artificially joining several words together (e.g. #thisisbeautiful).\\n\\nFor these, the X part-of-speech tag is used in the same manner as would be done with similar regular tokens as well. In the relation annotation, we annotate topical hashtags with the discourse relation, while content-bearing hashtags receive annotation regarding its real syntactic function in the sentence.\\n\\nDue to the choices done during the text preprocessing, retweet markers often appearing in Twitter corpora (such as RT in the beginning of a tweet), do not appear in our data. Regarding URLs and emoticons quite frequently occurring in the corpus, we follow the general Finnish-TDT annotation standards, where both are annotated as symbols (SYM) in the part-of-speech level.\\n\\nWhile in Finnish-TDT emoticons are always attached with the discourse relation to the sentence root, the relation and attachment of URLs depend on the sentence context. However, most of the URLs appearing in tweets are sentence-final referential items, which do not hold any content-bearing function, we use the same discourse relation for such URLs as well.\\n\\n4. Treebank Statistics\\n\\nThe statistics of the Finnish-OOD corpus are summarized in Table 1, where the section-specific document, sentence and syntactic word counts are plotted together with the two other corpora annotated using the same guidelines and used later in the parsing experiments, Finnish-TDT and Finnish-PUD. The total size of the Finnish-OOD corpus is 19,382 syntactic words (2,122 sentences), where syntactic word is the basic element of syntactic annotation in Universal Dependencies. The different subsections vary in size between 2,005 (poetry) and 6,906 words (web documents). The whole corpus is released as test data only.\\n\\nFor comparison, among the 217 test sets in the present Universal Dependencies release 2.9, the average length is 17,946 words and median length is 11,385 words. This makes the Finnish-OOD with its 19,382 words an average UD test set in length, in fact considerably above the median length, ranking 53th out of 217. In terms of full UD treebanks (not only their test sets), Finnish-OOD still contains more total words than 81 of the 217 UD treebanks.\\n\\n5. Out-of-domain Parsing\\n\\nIn this section we report on dependency parsing experiments, where the parser trained on the Finnish-TDT treebank is tested both on its in-domain data (Finnish-TDT) and out-of-domain data using the newly introduced Finnish-OOD and the existing Finnish-PUD datasets. First, we measure off-the-shelf parsing performance on these datasets in order to report baseline performance directly comparable to other studies, and later perform several detailed section-wise analyses. Additionally, since the Finnish-TDT treebank preserves metadata about the original text sources, we also carry out \u201cleave section out\u201d experiments across the 10 sections of the Finnish-TDT treebank, obtaining further out-of-domain parsing experiments. These allow us to gauge the benefit of the new dataset compared to what was available previously.\\n\\nThe parsing experiments are carried out using the Turku Neural Parser Pipeline (Kanerva et al., 2018), which is a full parsing pipeline with parsing accuracy at the level of present state-of-the-art for UD Finnish parsing. Updated from its original release, the current pipeline consist of a segmentation module based on the UDPipe implementation (Straka and Strakov\u00e1, 2017), custom part-of-speech and morphological feature tagger including separate POS and feature classification\"}"}
{"id": "lrec-2022-1-120", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Section-specific statistics for Finnish TDT, PUD and OOD treebanks in terms of document, sentence and token counts. Sections in each treebank are sorted in descending order based on the test set token count.\\n\\nIn Table 2 we report the baseline experiments, where the parser trained on the full TDT corpus training set is evaluated on its own test set (TDT) as well as the two external test sets (PUD and OOD). When applying the model to the PUD dataset, the parsing performance does not decrease, the LAS performance actually being +1pp higher compared to the original TDT test set. Similar observations are reported in multiple other studies as well (see e.g. Zeman et al. (2017)), suggesting the PUD test set being easier compared to the TDT test set. Additionally, one must take into account the fact that although we treat PUD as external, separately constructed treebank, the sections included in PUD have a major domain overlap between those in TDT (namely Wikipedia for PUD Wikipedia and Wikinews, economy news and university news for PUD news). Therefore, the PUD dataset cannot be considered as out-of-domain data for TDT trained models (and was, in fact, never meant to be an out-of-domain test set in the first place). On the contrary to PUD, the parsing performance drastically decreases on the newly introduced Finnish-OOD dataset, LAS decreasing over 13pp from 91.00 to 77.50.\\n\\nNext, we set out to study this further by breaking down the data section-by-section in each of the three treebanks, and carrying out the \\\"leave section out\\\" experiments also across the 10 different sections of the Finnish-TDT treebank. In these \\\"leave section out\\\" experiments, the trained model has never seen data from the particular section during model training, thus demonstrating the out-of-domain parsing performance on the TDT treebank also. In respect of the two PUD sections (Wikipedia and news), we report numbers when leaving all corresponding TDT sections out during training, while in OOD the model is trained on full TDT data as there is no domain overlap between the treebank sections.\\n\\nThe section-wise results are shown in Table 3. In terms of the OOD sections (web documents, clinical, online discussions, tweets, and poetry), quite unsurprisingly the two best performing out-of-domain sections are web documents and online discussions in terms of parsing accuracy (LAS metric), those sections not very substantially differing from the general data in terms of data annotation, and thus likely closest to the genres seen during the model training as well. In addition to the treebank data, the pre-trained FinBERT model used as starting point in parser fine-tuning, was trained on a\"}"}
{"id": "lrec-2022-1-120", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Source     | Domain       | Tokens | Sentences | Words | UPOS | UFeats | Lemmas | UAS | LAS |\\n|------------|--------------|--------|-----------|-------|------|--------|--------|-----|-----|\\n| TDT        | University news | 99.9   | 81.3      | 99.9  | 98.5 | 98.1   | 94.1   | 95.6| 93.8|\\n| TDT        | Student magazines | 100.0 | 100.0     | 100.0 | 98.1 | 96.9   | 96.3   | 95.8| 93.2|\\n| PUD        | News         | 99.8   | 94.1      | 99.8  | 98.1 | 96.4   | 95.8   | 94.0| 92.0|\\n| TDT        | EuroParl     | 99.9   | 94.9      | 99.9  | 98.5 | 98.0   | 98.3   | 93.7| 91.9|\\n| PUD        | Wikipedia    | 99.5   | 87.9      | 99.5  | 97.6 | 96.8   | 94.9   | 93.2| 91.2|\\n| TDT        | Economy news | 99.9   | 75.8      | 99.9  | 98.1 | 97.8   | 97.3   | 91.7| 89.8|\\n| TDT        | Wikipedia    | 99.1   | 89.2      | 99.1  | 96.8 | 96.2   | 93.4   | 91.8| 89.5|\\n| TDT        | Wikinews     | 99.6   | 81.0      | 99.6  | 98.6 | 96.3   | 92.6   | 91.5| 89.5|\\n| TDT        | Blogs        | 98.9   | 83.0      | 98.9  | 96.8 | 94.6   | 94.5   | 91.5| 89.4|\\n| TDT        | Fiction      | 99.7   | 93.2      | 99.6  | 96.5 | 94.3   | 93.3   | 90.8| 88.4|\\n| OOD        | Web documents | 99.3   | 80.3      | 99.3  | 96.3 | 95.2   | 94.3   | 89.1| 86.4|\\n| TDT        | Legal        | 99.0   | 45.2      | 99.0  | 97.2 | 95.8   | 95.5   | 88.1| 85.9|\\n| OOD        | Grammar examples | 99.8 | 71.4      | 99.8  | 96.2 | 93.9   | 94.8   | 88.5| 85.7|\\n| OOD        | Online discussions | 98.1  | 86.2      | 98.1  | 94.0 | 93.8   | 93.0   | 87.9| 83.9|\\n| OOD        | Poetry       | 99.6   | 55.5      | 99.6  | 95.2 | 94.7   | 94.6   | 80.3| 76.1|\\n| OOD        | Tweets       | 92.2   | 57.8      | 92.1  | 83.5 | 82.6   | 81.4   | 73.9| 69.5|\\n| OOD        | Clinical     | 96.4   | 53.1      | 96.4  | 89.2 | 89.2   | 88.5   | 72.0| 66.1|\\n| AVERAGE    |              | 98.9   | 78.2      | 98.8  | 95.8 | 94.7   | 93.7   | 88.8| 86.0|\\n\\nTable 3: Out-of-domain parsing performance of a model trained on UD Finnish-TDT and tested on all available test sets. All tests are out-of-domain, i.e. if the test set originates from the TDT treebank, the relevant section is removed from the training data. Similarly, for the PUD test set sections, the corresponding domain was removed from the training data. The results are sorted by LAS and color-coded by difference from the average.\\n\\nlarge collection of web and discussion forum data. During pre-training, the FinBERT language model used 3.3B tokens of Finnish including discussion forum data (52%), web crawl (33%), and news (15%). Therefore, although these two genres are out-of-domain in terms of parser training, the parser was exposed to these genres through language model pre-training.\\n\\nOn the other end of the scale in terms of LAS is the clinical domain text, nearly 26pp below the in-domain performance, an accuracy level which is likely too low for practical applications. The parsing accuracy on tweets is about 20pp below the in-domain performance, also a very substantial drop. Other measures, such as the accuracy of POS and morphological tagging, and lemmatization, on the other hand, do not exhibit nearly as substantial drop as the syntactic tree accuracy. Especially lemmatization, which is an important step in search and indexing type of applications, sees a comparatively moderate absolute drop in performance across the various OOD subdomains.\\n\\nWhen comparing the different sections from all three treebanks, it's clear that the sections selected for the Finnish-OOD are in general more difficult that the ones present in TDT and PUD even when evaluated in \u201cleave section out\u201d manner. With the exception of OOD web documents having higher LAS than TDT legal and grammar examples, the OOD sections locate to the bottom of the table when sorted in terms of LAS in descending order.\\n\\nFinally, in Table 4 we compare the in-domain and out-of-domain parsing performance across all sections in the Finnish-TDT corpus by reporting the evaluation performance for both in-domain model where the corresponding section is present in the training data, as well as out-of-domain model, where the section is removed from the training data. In this way we are able to estimate the pure out-of-domain parsing effect, removing the effect of some domains being naturally more difficult to parse than others. While many of the sections do not express substantial differences between the in-domain and out-of-domain performance, especially the legal domain significantly suffers in the out-of-domain setting. Interestingly, when comparing the in-domain performance between different sections, the legal domain receives the second best LAS performance, suggesting the section not being particularly difficult in general but likely the legal text significantly standing out from the other data sources included in the corpus.\\n\\n6. Conclusion\\nIn this work, we introduced a dedicated out-of-domain, manually annotated test set for UD Finnish parser evaluation including data from five distinct text sources previously absent from the UD Finnish treebanks. The\"}"}
{"id": "lrec-2022-1-120", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Section          | \u0394  | OOD   | ID   | \u2206  |\\n|------------------|----|-------|------|----|\\n| Student magazines| 0.0| 100.0 | 100.0| 0.0|\\n| Blogs            | -0.8| 98.9  | 99.7 | -1.0|\\n| University news  | -0.1| 99.9  | 100.0| -0.8|\\n| EuroParl         | -0.1| 99.9  | 100.0| -0.1|\\n| Fiction          | 0.1| 99.7  | 100.0| -0.8|\\n| Economy news     | 0.0| 99.9  | 100.0| 0.0|\\n| Wikipedia        | -0.1| 99.1  | 99.9 | -0.2|\\n| Wikinews         | 0.4| 99.6  | 99.9 | 0.5|\\n\\nTable 4: Parsing performance on the various sections of the UD Finnish TDT treebank. OOD refers to an out-of-domain run, where the section is removed from the training data, while ID refers to an in-domain run, where the section is present in the training data. Their difference \\\\( \\\\Delta \\\\) then directly shows the absolute loss in parsing performance on each section, as if it were an out of domain section. Since in-domain and out-of-domain numbers can be compared, the fact that some sections have a higher overall parsing performance than others does not affect the results. The sections are sorted by \\\\( \\\\Delta \\\\) LAS.\\n\\nSelection mirrors practical use cases seen for Finnish dependency parsing in the academia as well as in the industry. In terms of its size, this test set is comparable to other test sets in UD, with its 19,382 syntactic words being considerably above the median UD test set size. Our parsing experiments on this dataset demonstrate that, indeed, syntactic parsing performance can substantially degrade on several domains and the OOD test set now allows us to quantify the effect. On the other hand, we were also able to establish that the effect is at its strongest specifically when measuring the accuracy of the syntactic tree (LAS metric) and is notably less pronounced on the tagging and lemmatization tasks, which have a number of applications in their own right.\\n\\nTogether, these parsing experiments and the Finnish-OOD test set comprise the broadest evaluation of a Finnish state of the art syntactic parser across numerous domains, giving valuable knowledge for all applying the parser outside its training domain in various real-life applications. The new Finnish-OOD treebank is available through the official data releases of the Universal Dependencies framework.\\n\\n7. Acknowledgements\\n\\nWe would like to thank Akseli Leino, Hans Moen and Henry Suhonen for their help with handling the medical terminology. Computational resources were provided by CSC \u2013 the Finnish IT Center for Science, and the research was supported by the Academy of Finland.\\n\\n8. Bibliographical References\\n\\nBhat, I. A., Bhat, R. A., Shrivastava, M., and Sharma, D. M. (2018). Universal dependency parsing for Hindi-English code-switching. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL) 2018.\\n\\nBlodgett, S. L., Wei, J., and O'Connor, B. (2018). Twitter universal dependency parsing for African-American and mainstream American English. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1415\u20131425.\\n\\nCandito, M. and Seddah, D. (2012). Le corpus Sequoia : annotation syntaxique et exploitation pour l\u2019adaptation d\u2019analyseur par pont lexical. In TALN 2012 - 19e conf\u00e9rence sur le Traitement Automatique des Langues Naturelles, Grenoble, France, June.\\n\\nCecchini, F. M., Sprugnoli, R., Moretti, G., and Pasparotti, M. (2020). Udante: First steps towards the...\"}"}
{"id": "lrec-2022-1-120", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Universal Dependencies treebank of Dante's Latin works. In CLiC-it.\\n\\nCignarella, A. T., Bosco, C., and Rosso, P. (2019). Presenting TWITTIRO-UD: An Italian Twitter treebank in Universal Dependencies. In Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, SyntaxFest 2019), pages 190\u2013197.\\n\\nDozat, T., Qi, P., and Manning, C. D. (2017). Stanford's graph-based neural dependency parser at the CoNLL 2017 shared task. Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 20\u201330.\\n\\nHaverinen, K., Ginter, F., Laippala, V., and Salakoski, T. (2009). Parsing clinical Finnish: Experiments with rule-based and statistical dependency parsers. In Kristiina Jokinen et al., editors, Proceedings of NODALIDA'09, Odense, Denmark, pages 65\u201372.\\n\\nHaverinen, K., Ginter, F., Viljanen, T., Laippala, V., and Salakoski, T. (2010). Dependency-based PropBanking of clinical Finnish. In Proceedings of the Fourth Linguistic Annotation Workshop, pages 137\u2013141, Uppsala, Sweden, July. Association for Computational Linguistics.\\n\\nHaverinen, K., Nyblom, J., Viljanen, T., Laippala, V., Kohonen, S., Missil\u00e4, A., Ojala, S., Salakoski, T., and Ginter, F. (2014). Building the essential resources for Finnish: the Turku Dependency Treebank. Language Resources and Evaluation, 48:493\u2013531. Open access.\\n\\nKanerva, J., Ginter, F., Miekka, N., Leino, A., and Salakoski, T. (2018). Turku neural parser pipeline: An end-to-end system for the CoNLL 2018 shared task. In Proceedings of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies. Association for Computational Linguistics.\\n\\nKanerva, J., Ginter, F., and Salakoski, T. (2020). Universal Lemmatizer: A sequence-to-sequence model for lemmatizing Universal Dependencies treebanks. Natural Language Engineering, pages 1\u201330.\\n\\nLaippala, V., Viljanen, T., Airola, A., Kanerva, J., Salanter\u00e4, S., Salakoski, T., and Ginter, F. (2014). Statistical parsing of varieties of clinical Finnish. Artificial Intelligence in Medicine, 61(3):131\u2013136. Text Mining and Information Analysis of Health Documents.\\n\\nLaippala, V., Kyll\u00f6nen, R., Egbert, J., Biber, D., and Pyysalo, S. (2019). Toward multilingual identification of online registers. In Proceedings of the 22nd Nordic Conference on Computational Linguistics, pages 292\u2013297, Turku, Finland, September\u2013October. Link\u00f6ping University Electronic Press.\\n\\nLiu, Y., Zhu, Y., Che, W., Qin, B., Schneider, N., and Smith, N. A. (2018). Parsing tweets into Universal Dependencies. In Proceedings of the North American Chapter of the Association for Computational Linguistics (NAACL) 2018.\\n\\nLuotolahti, J., Kanerva, J., Laippala, V., Pyysalo, S., and Ginter, F. (2015). Towards universal web parsebanks. In Proceedings of the International Conference on Dependency Linguistics (Depling'15), pages 211\u2013220. Uppsala University.\\n\\nMititelu, V. B. and Mitrofan, M. (2020). The Romanian medical treebank \u2013 SiMoNERo. In The 15th Edition of the International Conference on Linguistic Resources and Tools for Natural Language Processing.\\n\\nMititelu, V. B. (2018). Modern syntactic analysis of Romanian. Cl\u0103sici \u0219i modernitatea cercet\u0103rii filologic\u0103 rom\u00e2neasc\u0103 actual\u0103.\\n\\nM\u0103randuc, C. and Bobicev, V. (2017). Non standard treebank Romania \u2013 Republic of Moldova in the Universal Dependencies. In Proceedings of the Conference on Mathematical Foundations of Informatics (MFOI'2017), pages 111\u2013116.\\n\\nM\u00fcller-Eberstein, M., van der Goot, R., and Plank, B. (2021). How universal is genre in Universal Dependencies? In Proceedings of the 20th International Workshop on Treebanks and Linguistic Theories (TLT 2021) at SyntaxFest.\\n\\nPyysalo, S., Kanerva, J., Missil\u00e4, A., Laippala, V., and Ginter, F. (2015). Universal Dependencies for Finnish. In Proceedings of NoDaLiDa 2015, pages 163\u2013172. NEALT.\\n\\nRaab, J. (2008). The Czech academic corpus 2.0 guide. The Prague Bulletin of Mathematical Linguistics, 89(2008):41\u201396.\\n\\nSanguinetti, M., Bosco, C., Mazzei, A., Lavelli, A., and Tamburini, F. (2017). Annotating Italian social media texts in Universal Dependencies. In Fourth International Conference on Dependency Linguistics (Depling 2017), pages 229\u2013239. Link\u00f6ping University Electronic Press.\\n\\nSeraji, M., Ginter, F., and Nivre, J. (2016). Universal dependencies for Persian. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 2361\u20132365.\\n\\nShavrina, T. and Shapovalova, O. (2017). To the methodology of corpus construction for machine learning: Taiga syntax tree corpus and parser. In Proceedings of the International Conference CORPORA-2017.\\n\\nStein, A. and Pr\u00e9vost, S. (2013). Syntactic annotation of medieval texts. New methods in historical corpora, 3:275.\\n\\nStraka, M. and Strakov\u00e1, J. (2017). Tokenizing, pos tagging, lemmatizing and parsing UD 2.0 with UD-Pipe. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies, pages 88\u201399, Vancouver, Canada, August. Association for Computational Linguistics.\\n\\nSuominen, H., Lundgren-Laine, H., Salanter\u00e4, S., Karsten, H., and Salakoski, T. (2009). Information flow in intensive care narratives. In 2009\"}"}
{"id": "lrec-2022-1-120", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"IEEE International Conference on Bioinformatics and Biomedicine Workshop, pages 325\u2013340.\\n\\nTyers, F. M. and Henderson, R. (2021). A corpus of K'iche' annotated for morphosyntactic structure. In Proceedings of the First Workshop on NLP for Indigenous Languages of the Americas (Americas-NLP).\\n\\nVirtanen, A., Kanerva, J., Ilo, R., Luoma, J., Luotolahti, J., Salakoski, T., Ginter, F., and Pyysalo, S. (2019). Multilingual is not enough: BERT for Finnish. arXiv preprint arXiv:1912.07076.\\n\\nZeman, D., Popel, M., Straka, M., Haji\u02c7c, J., Nivre, J., Ginter, F., Luotolahti, J., Pyysalo, S., Petrov, S., Potthast, M., Tyers, F., Badmaeva, E., Gokirmak, M., Nedoluzhko, A., Cinkov\u02c7a, S., Haji\u02c7c jr., J., Hlav\u02c7a\u02c7cov\u00b4a, J., Kettnerov\u00b4a, V ., Ure\u02c7sov\u00b4a, Z., Kanerva, J., Ojala, S., Missil\u00a8a, A., Manning, C. D., Schuster, S., Reddy, S., Taji, D., Habash, N., Leung, H., de Marneffe, M.-C., Sanguinetti, M., Simi, M., Kanayama, H., de Paiva, V ., Droganova, K., Mart\u00b4\u0131nez Alonso, H., C \u00b8\u00a8oltekin, C \u00b8 ., Sulubacak, U., Uszkoreit, H., Macketanz, V ., Burchardt, A., Harris, K., Marheinecke, K., Rehm, G., Kayadelen, T., Attia, M., Elkahky, A., Yu, Z., Pitler, E., Lertpradit, S., Mandl, M., Kirchner, J., Alcalde, H. F., Strnadov\u00b4a, J., Banerjee, E., Manurung, R., Stella, A., Shimada, A., Kwak, S., Mendonc \u00b8a, G., Lando, T., Nitisaroj, R., and Li, J. (2017). CoNLL 2017 shared task: Multilingual parsing from raw text to Universal Dependencies. In Proceedings of the CoNLL 2017 Shared Task: Multilingual Parsing from Raw Text to Universal Dependencies.\\n\\n9. Language Resource References\\n\\nVoutilainen, A., Purtonen, T., Leisko-J\u00a8arvinen, S., Kumlander, M., Linden, K., Nissinen, M., and Hardwick, S. (2010). Finntreebank 1.\"}"}
{"id": "lrec-2022-1-120", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tanen, N., Pascual, E., Passarotti, M., Patejuk, A., Paulino-Passos, G., Peljak-\u0141api\u0144ska, A., Peng, S., Perez, C.-A., Perkova, N., Perrier, G., Petrov, S., Petrova, D., Phelan, J., Piitulainen, J., Pirinen, T. A., Pitler, E., Plank, B., Poibeau, T., Ponomareva, L., Popel, M., Pretkaln\u012bja, L., Pr\u00e9vost, S., Prokopidis, P., Przepi\u00f3rkowski, A., Puolakainen, T., Pyysalo, S., Qi, P., R\u00e4abis, A., Rademaker, A., Rama, T., Ramasamy, L., Ramisch, C., Rashel, F., Rasooli, M. S., Ravishankar, V., Real, L., Rebeja, P., Reddy, S., Rehm, G., Riabov, I., Rie\u00dfler, M., Rimkut\u0117, E., Rinaldi, L., Rituma, L., Rocha, L., R\u00f6gnvaldsson, E., Romanenko, M., Rosa, R., Rosca, V., Rovati, D., Rudina, O., Rueter, J., R\u00fanarsson, K., Sadde, S., Safari, P., Sagot, B., Sahala, A., Saleh, S., Samard\u017ei\u0107, T., Samson, S., Sanguinetti, M., San\u0131yar, E., Sargent, D., Saul\u0161tait\u0117, B., Sawanuknanon, Y., Saxena, S., Scannell, K., Scarlata, S., Schneider, N., Schuster, S., Schwartz, L., Seddah, D., Seeker, W., Seraji, M., Shen, M., Shimada, A., Shirasu, H., Shishkina, Y., Shohibussirri, M., Sichinava, D., Siewert, J., Sigursson, E. F., Silveira, A., Silveira, N., Simi, M., Simionescu, R., Simko, K., \u0160imkov\u00e1, M., Simov, K., Skachedubova, M., Smith, A., Soares-Bastos, I., Spadine, C., Sprugnoli, R., Steingr\u00edmsson, S., Stella, A., Straka, M., Strickland, E., Strnadov\u00e1, J., Suhr, A., Sulestio, Y., Sulubacak, U., Suzuki, S., Sz\u00e1nt\u00f3, Z., Taji, D., Takahashi, Y., Tamburini, F., Tan, M. A. C., Tanaka, T., Tella, S., Tellier, I., Testori, M., Thomas, G., Torga, L., Toska, M., Trosterud, T., Trukhina, A., Tsarfaty, R., T\u00fcrk, U., Tyers, F., Uematsu, S., Untilov, R., Ure\u0161ov\u00e1, Z., Uria, L., Uszkoreit, H., Utka, A., Vajjala, S., van der Goot, R., Vanhove, M., van Niekerk, D., van Noord, G., Varga, V., Villemontede la Clergerie, E., Vincze, V., Vlasova, N., Wakasa, A., Wallenberg, J. C., Wallin, L., Walsh, A., Wang, J. X., Washington, J. N., Wendt, M., Widmer, P., Williams, S., Wir\u00e9n, M., Wittern, C., Woldemariam, T., Wong, T.-s., Wr\u00f3blewska, A., Yamashita, K., Yamazaki, N., Yan, C., Yasuoka, K., Yavrumyan, M. M., Yenice, A. B., Y\u0131ld\u0131z, O. T., Yu, Z., \u017dabokrtsk\u00fd, Z., Zahra, S., Zeldes, A., Zhu, H., Zhuravleva, A., and Ziane, R. (2021). Universal Dependencies 2.8.1. LINDAT/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (UFAL), Faculty of Mathematics and Physics, Charles University.\"}"}
