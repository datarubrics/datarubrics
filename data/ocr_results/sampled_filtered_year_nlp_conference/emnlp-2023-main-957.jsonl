{"id": "emnlp-2023-main-957", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Distribution of partisan events found in each quartile of an article, in terms of spatiality. Shaded area shows the 95% confidence interval.\\n\\nAs can be observed, right articles have more partisan events that appear in later parts of an article, whereas partisan events in left articles are evenly distributed in the article.\\n\\nAppendix D Latent Variable Models Implementation Details.\\n\\nFor both extractor and predictors, we use the same model architecture as in \u00a7B.2 with hyperparameters listed in Table 7. For the three-player model, we follow the training process in Generative Adversarial Nets training (Goodfellow et al., 2014).\\n\\nThe two-player model contains 213M parameters, and the three-player model contains 320M parameters. On average, the training takes 50 minutes for the two-player model and 1.5 hours for the three-player model on a single NVIDIA RTX A6000 GPU.\\n\\nPretrained Model for Event Representation.\\n\\nWe use the BIGNEWSALIGN dataset (Liu et al., 2022) to pretrain a model with prior event ideology knowledge. We remove stories in the dataset that contain duplicate articles and downsample articles in each story so that the number of left and right articles are balanced. Table 9 shows the statistics of the pretraining dataset. We then train a DistrilRoBERTa model that takes each event as input and predicts the event's ideology, where we use the article ideology as the event's ideology. We train this model on BIGNEWSALIGN for 2 epochs and use it to initialize our latent variable models.\\n\\nAppendix E Additional Error Analysis Table 10 in this section is supplementary to the Error Analysis section in \u00a76. The model detects \\\"de-\\n\\n| Title | # articles | # events |\\n|-------|------------|----------|\\n| Left  | 128        | 481      |\\n| Right | 123        | 380      |\\n\\nTable 9: Statistics for the BIGNEWSALIGN pretraining dataset.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All Things Considered: Detecting Partisan Events from News Media\\nwith Cross-Article Comparison\\n\\nYujian Liu1 Xinliang Frederick Zhang2 Kaijian Zou2 Ruihong Huang3 Nick Beauchamp4 Lu Wang2\\n\\n1 Computer Science, UC Santa Barbara, Santa Barbara, CA\\n2 Computer Science and Engineering, University of Michigan, Ann Arbor, MI\\n3 Computer Science and Engineering, Texas A&M University, College Station, TX\\n4 Department of Political Science, Northeastern University, Boston, MA\\n\\n1 yujianliu@ucsb.edu, 2 {xlfzhang,zkjzou,wangluxy}@umich.edu, 3 huangrh@tamu.edu, 4 n.beauchamp@northeastern.edu\\n\\nAbstract\\nPublic opinion is shaped by the information news media provide, and that information in turn may be shaped by the ideological preferences of media outlets. But while much attention has been devoted to media bias via overt ideological language or topic selection, a more unobtrusive way in which the media shape opinion is via the strategic inclusion or omission of partisan events that may support one side or the other. We develop a latent variable-based framework to predict the ideology of news articles by comparing multiple articles on the same story and identifying partisan events whose inclusion or omission reveals ideology. Our experiments first validate the existence of partisan event selection, and then show that article alignment and cross-document comparison detect partisan events and article ideology better than competitive baselines. Our results reveal the high-level form of media bias, which is present even among mainstream media with strong norms of objectivity and nonpartisanship. Our codebase and dataset are available at https://github.com/launchnlp/ATC.\\n\\n1 Introduction\\nNews media play a critical role in society not merely by supplying information, but also by selecting and shaping the content they report (de Vreese, 2004; DellaVigna and Kaplan, 2007; DellaVigna and Gentzkow, 2009; Perse and Lambe, 2016). To understand how media bias affects media consumers (Gentzkow and Shapiro, 2006; Gentzkow et al., 2015), we must understand not just how media ideology affects the presentation of news stories on a surface level, such as the usage of partisan phrases or opinions, but also the less obvious process of content selection (Fan et al., 2019; Enke, 2020). Content selection, such as what events that are related to the main story and should be included in the report, has recently become a focus of study in political science. Numerous studies point out that media selectively report information that is flattering to a particular political party or ideology, which may consequently shift audience beliefs and attitudes (Broockman and Kalla, 2022; Baum and Groeling, 2008; Grossman et al., 2022; D'Alessio and Allen, 2006). However, most existing work either requires manual inspection of reported content (Broockman and Kalla, 2022), or relies on simple tools for coarse analyses, such as overall slant and topic emphasis (Baum and Groeling, 2008; Grossman et al., 2022). As a result, these studies are either limited to a short time period, or are unable to provide a detailed understanding of content selection bias. Thus there remains a strong need for automatic tools that can analyze and detect how more complex content is selectively reported.\\n\\nRather than focusing on more superficial biases such as word, topic, or entity selection, we investigate here how media ideology affects their selection of which events to include for news reporting.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Events are the fundamental high-level components of the storytelling process (Prince, 2012), and their inclusion or omission shapes how a news story is perceived. In line with previous analysis of partisan selection bias in the literature (Broockman and Kalla, 2022), we define partisan events as selectively reported events that are favorable to a media organization\u2019s co-partisans or unfavorable to counter-partisans. When there are many potentially relevant events, which subset are included in an article fundamentally affects how readers interpret the story, and can reveal a media outlet\u2019s stance on that topic and their ideology (Mullainathan and Shleifer, 2005; McCombs and Reynolds, 2008; Entman, 2007). One example of event-selection bias is shown in Fig. 1, where a Washington Post article includes a survivor\u2019s request to impose gun control (pro-gun control), whereas a New York Post article claims Biden\u2019s statement as false (pro-gun rights).\\n\\nThis paper has two major goals: (1) examining the relation between event selection and media ideology, and (2) formulating a task for partisan event detection in news articles and developing computational methods to automate the process. For the first goal, we verify the existence of partisan event selection by measuring how event selection affects the performance of media ideology prediction. Specifically, we represent articles using triplets of $\\\\langle$ARG0, predicate, ARG1$\\\\rangle$, denoting the set of events they report with participating entities (e.g., in Fig. 1). This representation is shown to be effective in narrative understanding (Chambers and Jurafsky, 2008; Mostafazadeh et al., 2016). We conduct two studies. First, we compare article-level ideology prediction performance by using events within a single article vs. contextualizing them with events in other news articles on the same story but reported by media with different ideologies, inspired by the observation that biased content should be evaluated against other media (Larcinese et al., 2011). We show that the latter setup yields significantly higher F1 scores, suggesting that cross-article comparison can identify partisan events and thereby produce more accurate ideology prediction. Second, we annotate an evaluation dataset of 50 articles that focus on two recent political issues, where in total we manually label 828 partisan events out of 1867 sentences from all articles. Testing on this dataset, we show that removing partisan events from the articles hurts ideology prediction performance significantly more than removing similar amounts of randomly selected events.\\n\\nFor the second goal, the most critical challenge in developing computational tools to identify partisan events is the lack of annotation, where manually labeling a large-scale dataset requires domain expertise and is highly time-consuming. For that reason, we use latent variables to represent whether an event is partisan or not, and propose to jointly infer partisan events and predict an article\u2019s ideology. Our models are trained using article-level ideology labels only, which are easier to obtain, and they do not require any human annotation of partisan events. We compare two approaches (Chen et al., 2018; Yu et al., 2019) to train latent variable models and explore two methods for further improvement: (1) steering the model toward events that are selected only by one side, which are more likely to be partisan, and (2) providing prior ideology knowledge with pretrained event representations.\\n\\nWe conduct experiments on two existing news article datasets (Liu et al., 2022; Fan et al., 2019) and our newly annotated data with partisan events (test only). Results indicate that latent variable models outperform all competitive baselines on both partisan event detection and ideology prediction, where cross-article event comparison is shown to be critical for both tasks. Analysis of the extracted partisan events reveals key challenges in detecting implicit nuanced sentiments and discerning event relations (e.g., main vs. background events), suggesting future research directions.\\n\\nTo the best of our knowledge, this is the first time that computational methods are developed for studying media bias at the event selection level. It is also the first time that automatic models are investigated to detect partisan events. Our results provide new insights into a high-level form of media bias that may be present even in apparently nonpartisan news, enabling a new understanding of how news media content is produced and shaped.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Combs and Shaw, 1972) refers to when the public's perception of a topic's overall significance is shaped by the amount of news coverage spent on that topic (Field et al., 2018; Grimmer, 2010; Quinn et al., 2010; Kim et al., 2014).\\n\\nFraming concerns how media highlight some aspects of the same reality to make them more salient to the public (Entman, 1993; Tsur et al., 2015; Baumer et al., 2015; Card et al., 2015; Liu et al., 2019a). Finally, partisan coverage filtering is used by media to selectively report content that is flattering to their co-partisans or unflattering to opponents. While there is a certain amount of conceptual overlap among these three categories, this work focuses primarily on the third: the selection of which events relevant to the main stories to report, and how that reveals a media outlet's ideology and stance. Compared to previous work in agenda setting, which mainly focuses on the topics of news articles (Field et al., 2018; Kim et al., 2014), our partisan event study focuses on a more thoughtful process for information filtering. Event selection is also subtler than framing, since framing examines how a perspective is evoked through particular phrases (Card et al., 2015), whereas partisan event detection requires both event extraction and cross-article comparison.\\n\\nWhile partisan coverage filtering has been studied in political science, detecting it requires human efforts to review all news content (Broockman and Kalla, 2022; Baum and Groeling, 2008), making these methods unscalable and only applicable to short time periods. Grossman et al. (2022) automate the process, but use predefined lists of phrases and simple topic models to determine the overall slant and topic of a news report, which cannot capture more tactful content selection like events. Most recently, a contemporaneous work (Zou et al., 2023) also explores the partisan events within news articles, but they mainly curate a larger-scaled annotated dataset to support fine-tuning models on the labeled events. Compared to these works, we operate with more nuanced factual details than phrases and topics, and we treat partisan events as latent variables and automatically detect them from news articles with methods that are scalable to large quantities of news.\\n\\nAnother line of work that is similar to ours is the detection of informational bias (Fan et al., 2019; van den Berg and Markert, 2020), defined as \u201ctangential, speculative, or background information that sways readers' opinions\u201d (Fan et al., 2019). Our work differs in two important aspects: First, their \u201cinformational bias\u201d can occur in any text span, and detecting speculative information often requires complex inference and also depends on specific wording. By contrast, by focusing on the presence or absence of events, we target concrete units of potentially partisan information, which can be more easily validated and understood by readers. Second, they train supervised models on annotated biased content, while our latent variable models do not need any labels on partisan events.\\n\\nIdeology Prediction with Text. Many computational models have been developed to predict ideology using textual data (Gentzkow and Shapiro, 2010; Gerrish and Blei, 2011; Ahmed and Xing, 2010; Nguyen et al., 2013). Recent work, for instance, leverages neural networks to incorporate phrase-level ideology (Iyyer et al., 2014), external knowledge from social media (Kulkarni et al., 2018; Li and Goldwasser, 2019), and large-scale language model pretraining (Liu et al., 2022; Baly et al., 2020). However, most of this computational work focuses directly on ideology prediction, with little attention to the higher-level processes underlying media bias. In particular, ideology prediction may fail for many mainstream media outlets who eschew overtly ideological language, and instead may bias readers only via a more sophisticated information selection procedure at the event level. We demonstrate that incorporating story-level context enables global content comparison over political spectrum, and benefits both partisan event detection and ideology prediction.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1 Ideology Prediction with Events\\n\\nWe build on the narrative embedding model in Wilner et al. (2021) and extend it to include story level context by adding article segment, event frequency, and event position embeddings. This allows us to gauge the effect of partisan events\u2019 presence or absence on ideology prediction.\\n\\nEvent Extraction.\\n\\nWe follow prior work (Zhang et al., 2021) to train event extractor on the MA-TRES dataset (Ning et al., 2018). Our extractor achieves an F1 score of 89.53, which is on par with the state-of-the-art performance (90.5) (Zhang et al., 2021). See details in Appendix B.1.\\n\\nIdeology Prediction.\\n\\nGiven \\\\( N \\\\) articles \\\\( a_1, \\\\ldots, a_N \\\\) that report on the same news story, we denote events in article \\\\( a_i \\\\) as \\\\( x_{(i)}_1, \\\\ldots, x_{(i)}_{L_i} \\\\), where \\\\( L_i \\\\) is the number of events in article \\\\( a_i \\\\). We first use a DistilRoBERTa model (Sanh et al., 2019) to get the embedding \\\\( e \\\\) for an event. Concretely, we input the sentence that contains the event to DistilRoBERTa and get the embeddings \\\\( e_{\\\\text{pred}}, e_{\\\\text{arg0}}, e_{\\\\text{arg1}} \\\\) by taking the average of last-layer token embeddings. If a sentence has multiple events, we mask out other events' tokens when encoding one event, so that the information in one event does not leak to others. We then get \\\\( e = W [ e_{\\\\text{pred}}; e_{\\\\text{arg0}}; e_{\\\\text{arg1}} ] \\\\), where \\\\( ; \\\\) means concatenation and \\\\( W \\\\) is learnable.\\n\\nWe then input all events in one article or all articles on the same story to another transformer encoder (Vaswani et al., 2017) to get contextualized \\\\( c \\\\) for each event:\\n\\n\\\\[\\n\\\\left[ c^{(1)}_1, \\\\ldots, c^{(N)}_{L_N} \\\\right] = \\\\text{Transformer} \\\\left( \\\\left[ e^{(1)}_1, \\\\ldots, e^{(N)}_{L_N} \\\\right] + E^{(1)} \\\\right)\\n\\\\]\\n\\nwhere \\\\( \\\\text{Transformer} \\\\) is a standard transformer encoder trained from scratch (details in Appendix B.2) and \\\\( E \\\\) contains three types of embeddings: Article embeddings distinguish the source by associating the index of the article with its events, with a maximum of three articles per story. Frequency embeddings highlight the prevalence of events by signaling if an event appears in only one article, more than one but not all articles, or all articles that report the same story. We train one embedding for each category and use lexical matching to determine common events. Finally, position embeddings represent the relative position of an event in the article, e.g., partisan events may appear later in the reports. All embeddings are learnable (details in Appendix B.2). Note that Eq. 1 describes the model with story level context as it includes all events in all articles. We also experiment with models that only use events in one article. Finally, the model predicts article\u2019s ideology using average representation of all events in the article.\\n\\n3.2 Partisan Event Dataset Annotation\\n\\nSince there is no dataset with partisan event annotations for news articles, we manually label a Partisan Event (PEvent) dataset with 50 articles (1867 sentences) covering two controversial events happened in the U.S. in 2022: a mass shooting in Texas, and the overturn of Roe v. Wade. Note that PEvent contains articles from a separate and later time than the training data with ideology prediction objective. PEvent is only used for evaluation purposes on the task of partisan event detection.\\n\\nSince labeling partisan events is costly, which requires both domain knowledge and news annotation experience, we only focus on two broad high-profile topics where the partisanship of all constituent events is already known to coders experienced with US politics. We acknowledge that a dataset with diverse topics would be useful, but will leave this for the future work. We collect articles from AllSides Headline Roundups section, where groups of three articles that report the same news story are carefully selected by editors to demonstrate \u201chow opposite sides of the media are discussing or framing a subject\u201d. For each story, we discard the center ideology article due to a lack of consensus of what constitutes center ideology by the community. The remaining two articles, together with extracted events, are provided to two college students who have prior news article annotation experience and have gone through careful training of the annotation tasks. They are instructed to first label article ideology, and then partisan...\"}"}
{"id": "emnlp-2023-main-957", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Macro F1 scores for article ideology prediction (average of 5 runs). Best results are in bold and second best are underlined. art., fre., and pos. refer to article, frequency, and position embeddings in \u00a73.1.\\n\\nIn total, 828 partisan events are annotated out of 3035 events detected by our tool from 1867 sentences. Inter-annotator agreement calculated using Cohen\u2019s $\\\\kappa$ (Cohen, 1960) is 0.83 for article-level ideology. For partisan event labeling, two annotators achieve $\\\\kappa=0.43$, which is substantial agreement. After discussing with the annotators, we find that disagreement often occurs when one annotator is insufficiently confident and thus ends up labeling an event as non-partisan. Therefore during the disagreement resolution stage, an event is frequently deemed partisan if it is labeled by at least one annotator. This again highlights the subtlety of partisan event usage by media. On average, \\\\(16.56\\\\) (27.28\\\\%) events are annotated as partisan events per article. Among all partisan events reported by left-leaning media, 98.41\\\\% are chosen only by the left side, and 95.09\\\\% for the right media. We further check where partisan events are included in the articles, and find that they occur more frequently in the later parts of articles written by right-leaning media (displayed in Fig. 4 in the Appendix). These findings validate our design in \u00a73.1.\\n\\n3.3 Results for Ideology Prediction\\n\\nWe first compare ideology prediction performance using different model variants in \u00a73.1 and then pick two to study the effect of removing partisan events.\\n\\nEffects of Cross-Article Event Comparison.\\n\\nWe train models on AllSides dataset collected in Liu et al. (2022), where media outlets\u2019 ideology is used as articles\u2019 ideology. We use articles before 2020 (inclusive) as training and dev data and articles after 2020 (exclusive) as test data. We also evaluate models on Basil (Fan et al., 2019), where ideology is manually annotated similar to \u00a73.2. Likewise, we remove articles of the center ideology. Table 8 presents the statistics for datasets used in this study. We experiment with multi- and single-article variants of the model, depending on whether the transformer in Eq. 1 has access to events in all or one article. As shown in Table 2, multi-article models that allow content comparison across articles written by different media significantly outperform single-article models, demonstrating the benefits of adding story-level context to reveal partisan events that improve ideology prediction. Among multi-article models, article embeddings lead to the largest gain since it supports cross-article comparison. For experiments in the rest of this paper, we add position embedding for single-article models and all three embeddings for multi-article models.\\n\\nEffects of Removing Partisan Events.\\n\\nNext, we investigate how would removing partisan events affect model\u2019s prediction on ideology. Intuitively, when having access to fewer partisan events, the model will be less confident in predicting correct ideologies. Concretely, we run the multi-article model on PEvent. We drop \\\\(m\\\\%\\\\) of partisan events, where \\\\(m=25, 50, 75, 100\\\\). We also run the same model and remove the same number of events randomly (random events). We then measure the macro F1 and log probability of true classes. As shown in Fig. 2, removing partisan events hurts the performance more compared to removing random events. Moreover, the more partisan events are removed, the larger the performance gap is, which confirms that models exploit the presence of partisan events to discern ideology.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Latent Variable Models for Partisan Event Detection\\n\\nThe general idea of our latent variable models for partisan event detection is that the detected partisan events should be indicative of article's ideology, the removal of which would lower models' prediction confidence, according to our study in \u00a73. We adopt two methods that are originally developed to extract rationales of model predictions (\u00a74.2) for our task and further improve them by adding constraints on the usage of common events and adding prior knowledge of event-level ideology (\u00a74.3).\\n\\n4.1 Task Overview\\n\\nWe assume our data comes in the form of \\\\((a, y)\\\\), where \\\\(y\\\\) is the ideology for article \\\\(a\\\\). We extract events \\\\(x = (x_1, \\\\ldots, x_L)\\\\) from article \\\\(a\\\\) where \\\\(L\\\\) is the number of events in the article. We define a latent random variable \\\\(m_i \\\\in \\\\{0, 1\\\\}\\\\) for each event \\\\(x_i\\\\), and \\\\(m_i = 1\\\\) means \\\\(x_i\\\\) is a partisan event. The ideology prediction task aims at predicting \\\\(y\\\\) using \\\\(x\\\\). The partisan event detection task focuses on predicting partisan indicators \\\\(m = (m_1, \\\\ldots, m_L)\\\\).\\n\\n4.2 Latent Variable Models\\n\\nTwo-Player Model.\\n\\nWe adopt methods in rationale extraction, where rationale is defined as part of inputs that justifies model's prediction (Lei et al., 2016). We use the formulation in Chen et al. (2018), which tackles the rationale (partisan events in our model) extraction task from an information-theoretic perspective. In details, suppose a positive number \\\\(k\\\\) is given, the goal is to extract \\\\(k\\\\%\\\\) of events that have the highest mutual information with label \\\\(y\\\\) and treat them as partisan events. In other words, our partisan indicator \\\\(m\\\\) satisfies \\\\(|m| = k\\\\% \\\\times L\\\\). Since optimizing mutual information is intractable, Chen et al. (2018) provides a variational lower bound as the objective instead:\\n\\n\\\\[\\n\\\\max_{E_\\\\theta, q_\\\\phi} \\\\sum_{(x,y) \\\\in D} E_{m \\\\sim E_\\\\theta(x)} [\\\\log q_\\\\phi(y|m \\\\odot x)]\\n\\\\]\\n\\n(2)\\n\\nwhere \\\\(E_\\\\theta\\\\) is an extractor that models the distribution of \\\\(m\\\\) given \\\\(x\\\\), \\\\(q_\\\\phi\\\\) is a predictor that predicts \\\\(y\\\\) given partisan events, \\\\(D\\\\) is the training set, and \\\\(\\\\odot\\\\) is the element-wise multiplication.\\n\\nWe parameterize both \\\\(E_\\\\theta\\\\) and \\\\(q_\\\\phi\\\\) using the same model as in \u00a73.1. For the extractor, we first get the embedding \\\\(e\\\\) for all events and then pass it to the transformer encoder to get contextualized event representations. A linear layer converts these representations to logits, from which we sample \\\\(k\\\\%\\\\) of them following the subset sampling method in Xie and Ermon (2019)\u2014a differentiable sampling method that allows us to train the whole system end-to-end. At inference time, we select the top \\\\(k\\\\%\\\\) of events with the largest logits by the extractor. For the predictor, we again get event embeddings \\\\(e\\\\), but we input \\\\(m \\\\odot e\\\\) to the transformer encoder so that it only sees the sampled subset of events.\\n\\nThree-Player Model.\\n\\nAmong all events in the article, some may have spurious correlation with the ideology. For instance, the event \\\"a CNN reporter contribute to this article\\\" can almost perfectly reveal article's ideology. To prevent models from focusing on these shortcuts, we further investigate the method in Yu et al. (2019). Concretely, they propose a three-player model where a third complement predictor \\\\(q_c\\\\pi\\\\) predicts ideology using the complement of partisan events, i.e., \\\\((1 - m) \\\\odot x\\\\). The goal for both predictors is to correctly predict the ideology, i.e., maximize \\\\(\\\\log q_\\\\phi(y|m \\\\odot x)\\\\) and \\\\(\\\\log q_c\\\\pi(y|(1 - m) \\\\odot x)\\\\). The objective for the extractor is to select \\\\(k\\\\%\\\\) of events that can predict \\\\(y\\\\) while the remaining events cannot as in Eq. 3:\\n\\n\\\\[\\n\\\\max_{E_\\\\theta} \\\\sum_{(x,y) \\\\in D} E_{m \\\\sim E_\\\\theta(x)} [\\\\log q_\\\\phi(y|m \\\\odot x) - \\\\log q_c\\\\pi(y|(1 - m) \\\\odot x)]\\n\\\\]\\n\\n(3)\\n\\nIntuitively, the extractor and the complement predictor play an adversarial game, and Eq. 3 drives the extractor to identify partisan events as comprehensive as possible so that the complement predictor cannot perform well. In fact, Yu et al. (2019) uses an explicit objective to penalize \\\\(\\\\sum_i m_i\\\\) when it deviates from \\\\(k\\\\% \\\\times L\\\\), but we find this objective does not work well with Eq. 3, leading to an extractor that either selects all events as partisan events or detects nothing. We thus modify it with the subset sampling method (Xie and Ermon, 2019) again. At inference time, we use \\\\(q_\\\\phi\\\\) for ideology prediction.\\n\\nBoth two-player and three-player models can have the single- and multi-article variants, depending on whether the extractor and predictors can access all events in a story or just from a single article. Appendix D details the training process.\\n\\n4.3 Improving Partisan Event Detection\\n\\nRestricting Models from Picking Common Events.\\n\\nAs shown in Fig. 1, common background events and main events should not be considered as partisan events. We therefore explicitly prohibit models from selecting these events. Precisely, we\"}"}
{"id": "emnlp-2023-main-957", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"use the same lexical matching method as in \u00a73.1 to find common events in the story. During training, we add an auxiliary objective that minimizes the random\\n\\n(1) We\\n\\nTasks and Datasets.\\n\\nvise versa. We use this pretrained model for initial-\\n\\nskewed distribution as partisan events. Finally, we\\n\\nthe probability of each event being left and right.\\n\\nmodel with ideology priori in \u00a74.3. We run it to get\\n\\nthe article. (2) Event-prior\\n\\n0\\n\\nBaselines.\\n\\nfor the positive class, i.e., partisan event.\\n\\npartisan event detection, we measure the F1 score\\n\\nmeasure the macro F1 score at the article level. For\\n\\nEvaluation Metrics.\\n\\nformance on all three datasets and partisan event\\n\\nSides test set, Basil, and our partisan event dataset\\n\\nall models solely on AllSides and evaluate on All-\\n\\npolitical news stories, and each story contains about 4\\n\\nto acquire prior knowledge at the event level.\\n\\nBIGNEWSALIGN is a dataset with 1 million po-\\n\\nrange of topics. We therefore pretrain a model\\n\\nthe model to gain such knowledge on a broad\\n\\ntraining set is relatively small, it is unlikely for\\n\\ntisan content detection. Given that the AllSides\\n\\nroversial topics, plays an important role in par-\\n\\nPretraining to Add Event Ideology Priori.\\n\\ncontext to locate common events.\\n\\nmulti-article models since it requires story-level\\n\\nonly one side. We only apply this constraint to\\n\\nthus driving models to prefer events reported by\\n\\npear in both left and right articles as partisan events,\\n\\nprobability of the extractor to predict events that ap-\\n\\nof events with the most\\n\\n30%\\n\\nWe then consider the\\n\\nwe add an auxiliary objective that minimizes the\\n\\nrandom\\n\\n(2)\\n\\nEvent-prior\\n\\n0\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\nmodel and consider the top\\n\\n30%\\n\\nmodel and use the non-latent model and iteratively remove\\n\\nis\\n\\n30%\\n\\n"}
{"id": "emnlp-2023-main-957", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Outperform single-article models on both tasks, emphasizing the importance of story-level context for cross-document event comparison.\\n\\nOn partisan event detection (last column of Table 3), latent variable models outperform all baselines, showing the effectiveness of training with article ideology labels. Note that the three-player models do not outperform the two-player models, indicating that the spurious correlation may not be a significant issue on PEvent, and partisan events annotated on PEvent, as standalone events, cannot be directly associated with specific ideological leaning. Moreover, restricting models from selecting common events improves partisan event detection for two-player models, which validates that common events are less likely to be partisan. Providing prior knowledge of event ideology further boosts on both tasks, especially for single-article models, illustrating the benefits of prior knowledge when the context is limited. Finally, combining the two improvements, the two-player model on the multi-article setup achieves the best performance. It is also important to point out that this model only uses 30% of events to predict ideology, but it still outperforms the model that sees full articles in the story, which suggests that a good modeling of events in the article could be more helpful than raw text representations when predicting ideology.\\n\\n6 Further Analyses and Discussions\\n\\nEffect of Varying k. We now explore the effect of k's values. We experiment with three multi-article models: base two-player, two-player with restriction and prior knowledge, and base three-player models. We train these models with \\\\( k = 10, 20, 30, 40, 50 \\\\) and plot the performance of partisan event detection and ideology prediction on PEvent in Fig. 3. For event detection, the model improves as \\\\( k \\\\) increases, but the improvement is moderate when \\\\( k > 30 \\\\). For ideology prediction, the performance plateaus at \\\\( k = 20 \\\\) except for the two-player model with restriction and prior knowledge, which peaks at \\\\( k = 30 \\\\). This suggests that only a subset of events reflect the article's ideology, and it is enough to make predictions based on them.\\n\\nError Analysis. Table 4 and Table 10 in Appendix present predictions by the two-player model on the multi-article setup with one-sided restriction and prior knowledge for events. Two major types of errors are observed. First, the model struggles when an article attacks a statement from the opposite side with an implicit sentiment. For instance, \\\"threw,\\\" \\\"continue,\\\" and \\\"had\\\" in Table 4 are events or statements from the right, but the author reports them with an implicit negative sentiment (e.g., \\\"not a thing!\\\"). Future models need to (1) have an enhanced understanding of implicit sentiment along with the involving entities (Deng and Wiebe, 2015; Zhang et al., 2022), and (2) acquire knowledge of entity ideologies and their relations. Second, the model still frequently selects main events as partisan content, as shown by the \\\"delivered\\\" event in Table 10. For this example, it is because the main event should be included as necessary context for ideology prediction, i.e., the training objective. For other examples, some main events also carry sentiment towards ideological entities, thus indeed should be labeled as partisan events according to our definition. Future work should investigate whether the selective usage of partisan events are different when the main stories already support a certain ideology compared to when they disfavor the same ideology.\\n\\nUsage of the Latent Variable Model and Future Directions. The latent variable model can be used as a stance analyzer, which would come in handy in practice as well, especially for generating rationales out-of-the-box in different settings. Firstly, being trained as an ideology predictor, it has the potential for future extensions to multi-modal ideology analysis, as suggested in Qiu et al. (2022). Secondly, working on articles on the same topics, it enables exploration of how different media outlets...\"}"}
{"id": "emnlp-2023-main-957", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"At the NRA Convention, people blame mass shootings.\\n\\nWe investigate the impact of event selection on.\\n\\n15480\\n\\nWe analyze the model output and discuss in.\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\\n\\nTwo messages: left. . .\\n\\nThe nation has been plunged into despair and mourning.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in a story requires computational resources to scale quadratically with the number of events, which is infeasible for stories that contain many articles. Future work may consider designing novel mechanisms to address this issue, e.g., by using special attention patterns based on the discourse role of each event in the article (van Dijk, 1988; Choubey et al., 2020). \\n\\nFinally, due to the cost of manual labeling, we only evaluate our partisan event detection models on a dataset that covers two specific political issues. It remains to be seen whether methods introduced in this paper can be generalized to a broader range of issues. We call for the community's attention to design and evaluate partisan event detection models on more diverse topics.\\n\\n9 Ethical Considerations\\n\\n9.1 Dataset Collection and Usage\\n\\nPartisan Event Dataset Collection. We conform with the terms of use of the source websites and the intellectual property and privacy rights of the original authors of the texts when collecting articles. We do not collect any sensitive information that can reveal original author's identity. We also consult Section 107 of the U.S. Copyright Act and ensure that our collection action fall under the fair use category.\\n\\nDatasets Usage. Except the partisan event dataset collected in this work, we get access to the Basil dataset by direct download. For AllSides, we contact with the authors and obtain the data by agreeing that we will not further distribute it.\\n\\n9.2 Usage in Application\\n\\nIntended Use. The model developed in this work has the potential to assist the public to better understand and detect media bias in news articles. The experiments in \u00a75 show that our model is able to identify partisan events on two controversial issues that moderately align with human judgement. The detected events can be presented to show different perspectives from both ends of the political spectrum, thus providing readers with a more complete view of political issues.\\n\\nFailure Modes. Our model fails when it mistakenly predicts a non-partisan event as a partisan event, misses out the partisan events, or predicts the wrong ideology for an article. They may cause misperception and misunderstanding of an event. For vulnerable populations (e.g., people who maybe not have the specific knowledge to make the right judgements), the harm could be amplified if they blindly trust the machine outputs.\\n\\nBiases. The training dataset is roughly balanced in the number of left and right articles, so the model is not trained to encode bias. However, the dataset is relatively small and does not cover all possible political topics. Particularly, most of the news articles in the training set are related to U.S. politics, thus the model is not directly applicable to other areas in the world.\\n\\nMisuse Potential. Users may mistakenly take the model outputs as ground truth. We recommend any usage of our model displaying an \\\"use with caution\\\" message to encourage users to cross-check the information from different sources and not blindly trust a single source.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAmr Ahmed and Eric Xing. 2010. Staying informed: Supervised and semi-supervised multi-view topical analysis of ideological perspective. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing, pages 1140\u20131150, Cambridge, MA. Association for Computational Linguistics.\\n\\nRamy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. 2020. We can detect your bias: Predicting the political ideology of news articles. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4982\u20134991, Online. Association for Computational Linguistics.\\n\\nMatthew A. Baum and Tim Groeling. 2008. New media and the polarization of american political discourse. Political Communication, 25(4):345\u2013365.\\n\\nEric Baumer, Elisha Elovic, Ying Qin, Francesca Polletta, and Geri Gay. 2015. Testing and comparing computational approaches for identifying the language of framing in political news. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1472\u20131482, Denver, Colorado. Association for Computational Linguistics.\\n\\nSteven Bird, Ewan Klein, and Edward Loper. 2009. Natural language processing with Python: analyzing text with the natural language toolkit. \\\"O'Reilly Media, Inc.\\\".\\n\\nDavid Broockman and Joshua Kalla. 2022. The manifold effects of partisan media on viewers' beliefs and attitudes: A field experiment with fox news viewers.\\n\\nDallas Card, Amber E. Boydstun, Justin H. Gross, Philip Resnik, and Noah A. Smith. 2015. The media frames corpus: Annotations of frames across issues. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 438\u2013444, Beijing, China. Association for Computational Linguistics.\\n\\nNathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL-08: HLT, pages 789\u2013797, Columbus, Ohio. Association for Computational Linguistics.\\n\\nJianbo Chen, Le Song, Martin J. Wainwright, and Michael I. Jordan. 2018. Learning to explain: An information-theoretic perspective on model interpretation. In Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, volume 80 of Proceedings of Machine Learning Research, pages 882\u2013891. PMLR.\\n\\nPrafulla Kumar Choubey, Aaron Lee, Ruihong Huang, and Lu Wang. 2020. Discourse as a function of event: Profiling discourse structure in news articles around the main event. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5374\u20135386, Online. Association for Computational Linguistics.\\n\\nJacob Cohen. 1960. A coefficient of agreement for nominal scales. Educational and Psychological Measurement, 20(1):37\u201346.\\n\\nDave D'Alessio and Mike Allen. 2006. Media Bias in Presidential Elections: A Meta-Analysis. Journal of Communication, 50(4):133\u2013156.\\n\\nClaes de Vreese. 2004. The effects of strategic news on political cynicism, issue evaluations, and policy support: A two-wave experiment. Mass Communication and Society, 7(2):191\u2013214.\\n\\nStefano DellaVigna and Matthew Gentzkow. 2009. Persuasion: Empirical evidence. Working Paper 15298, National Bureau of Economic Research.\\n\\nStefano DellaVigna and Ethan Kaplan. 2007. The Fox News Effect: Media Bias and Voting*. The Quarterly Journal of Economics, 122(3):1187\u20131234.\\n\\nLingjia Deng and Janyce Wiebe. 2015. MPQA 3.0: An entity/event-level sentiment corpus. In Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1323\u20131328, Denver, Colorado. Association for Computational Linguistics.\\n\\nBenjamin Enke. 2020. What you see is all there is. The Quarterly Journal of Economics, 135(3):1363\u20131398.\\n\\nRobert M. Entman. 1993. Framing: Toward clarification of a fractured paradigm. Journal of Communication, 43(4):51\u201358.\\n\\nRobert M. Entman. 2007. Framing bias: Media in the distribution of power. Journal of Communication, 57(1):163\u2013173.\\n\\nLisa Fan, Marshall White, Eva Sharma, Ruisi Su, Prafulla Kumar Choubey, Ruihong Huang, and Lu Wang. 2019. In plain sight: Media bias through the lens of factual reporting. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6343\u20136349, Hong Kong, China. Association for Computational Linguistics.\\n\\nAnjalie Field, Doron Kliger, Shuly Wintner, Jennifer Pan, Dan Jurafsky, and Yulia Tsvetkov. 2018. Framing and agenda-setting in Russian news: a computational analysis of intricate political strategies. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3570\u20133580, Brussels, Belgium. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-957", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-957", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierre Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nWinston Wu and David Yarowsky. 2020. Computational etymology and word emergence. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 3252\u20133259, Marseille, France. European Language Resources Association.\\n\\nSang Michael Xie and Stefano Ermon. 2019. Reparameterizable subset sampling via continuous relaxations. In Proceedings of the 28th International Joint Conference on Artificial Intelligence, IJCAI'19, page 3919\u20133925. AAAI Press.\\n\\nMo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola. 2019. Rethinking cooperative rationalization: Introspective extraction and complement control. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4094\u20134103, Hong Kong, China. Association for Computational Linguistics.\\n\\nShuaicheng Zhang, Lifu Huang, and Qiang Ning. 2021. Extracting temporal event relation with syntactic-guided temporal graph transformer.\\n\\nXinliang Frederick Zhang, Nick Beauchamp, and Lu Wang. 2022. Generative entity-to-entity stance detection with knowledge graph augmentation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 9950\u20139969, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKaijian Zou, Xinliang Frederick Zhang, Winston Wu, Nick Beauchamp, and Lu Wang. 2023. Crossing the aisle: Unveiling partisan and counter-partisan events in news reporting. In Findings of the 2023 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-957", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A Implementation Details\\n\\nFor all experiments in this paper, our implementation is based on Pytorch (Paszke et al., 2019) and HuggingFace transformers (Wolf et al., 2020) library, and we preprocess all articles using Stanza (Qi et al., 2020). All experiments are conducted on 4 NVIDIA RTX A6000 GPUs.\\n\\nAppendix B Event-based Ideology Prediction Models\\n\\nB.1 Event Extraction\\n\\nWe follow the scheme in TimeML which defines events as \u201csituations that happen or occur\u201d (Pustejovsky et al., 2003). We train an event extraction model on the MATRES data (Ning et al., 2018), as its event annotation is not limited to predefined event types, and thus is applicable to the open domain scenario. We use RoBERTa-large (Liu et al., 2019b) that predicts a binary label for each word, deciding whether the word is an event predicate or not. To provide surrounding context, we split articles into groups of 4 sentences and process 4 sentences together. We follow previous work on using TimeBank and AQUAINT sections in MATRES as training set and Platinum section as test set (Ning et al., 2019). Table 6 shows the hyperparameters for model architecture and training process. On the same train and test split, our model achieves an F1 score of 89.53, which is on par with the state-of-the-art performance of 90.5 F1 score (Zhang et al., 2021). As verbs and nouns account for 96.8% of event predicates in MATRES dataset, we extract arguments 0 and 1 for verb and noun predicates using semantic role labeling tools (Shi and Lin, 2019; Gardner et al., 2018), and we only keep predicates that match our event extraction results.\\n\\nMultiple events can exist in one sentence with overlapping predicates and arguments. We hence remove the shorter event if there is an overlap, as we find that shorter events tend to be less informative. For example, it is easier to determine the partisanship of the event \u201cthe leak of a draft opinion would mark a stunning betrayal of the Court\u2019s process\u201d than a shorter one on \u201cthe leak of a draft opinion.\u201d Therefore, we remove an event if its predicate is covered by another event\u2019s arguments.\\n\\nTABLE 6: HYPERPARAMETERS USED FOR THE EVENT EXTRACTION MODEL.\\n\\n| Hyperparameter                  | Value                        |\\n|--------------------------------|------------------------------|\\n| number of epochs               | 20                           |\\n| patience                       | 4                            |\\n| maximum learning rate          | 3e-5                         |\\n| learning rate scheduler        | linear decay with warmup     |\\n| warmup percentage              | 6%                           |\\n| optimizer                      | AdamW                        |\\n| weight decay                   | 5e-5                         |\\n| # FFNN layer                   | 2                            |\\n| hidden layer dimension in FFNN | 768                          |\\n| dropout in FFNN                | 0.1                          |\\n\\nB.2 Contextualized Event Representation\\n\\nHere we detail our model that uses cross-article context for ideology prediction. As described in \u00a73.1, we first input the sentence that contains the event to a DistilRoBERTa model (Sanh et al., 2019) to get event representation $e$. This representation is then passed to a Transformer encoder (Vaswani et al., 2017) with three embeddings to obtain contextualized event representation $c$:\\n\\n- Article embedding indicates the index of the article that contains the event, with one embedding per article index. The datasets we experiment with in this paper have at most 3 articles in each story. During training, we randomly shuffle the articles in each story.\\n- Frequency embedding informs the model whether the event appears in only one article, at least two but not all articles, or all articles in the story. We have one embedding per category. We find common events through lexical matching. Concretely, we use a dictionary that contains derivational morphology mappings (Wu and Yarowsky, 2020) to get the base form of the event predicate. We then construct a set of words for the predicate by including the synonyms for the base form and original form (Bird et al., 2009). Finally, two events are considered as the same if their predicate sets overlap and both of their ARG0 and ARG1 have a high word overlap (a threshold of $0.4$).\\n\\nWe search threshold values from 0.2 to 0.5 by manually inspecting identified common events in 6 articles. A value of 0.4 can identify common events accurately while still allowing variations such as variants of mentions (e.g., president vs. vice president).\"}"}
{"id": "emnlp-2023-main-957", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Hyperparameters used for the event-based ideology prediction model.\\n\\n- Number of epochs: 5\\n- Maximum learning rate: 5e-5\\n- Learning rate scheduler: Linear decay with warmup\\n- Warmup percentage: 6%\\n- Optimizer: AdamW\\n- Weight decay: 1e-4\\n- Transformer hidden dimension: 768\\n- Transformer # heads: 12\\n- # Transformer layer: 4\\n- # FFNN layer: 2\\n- Hidden layer dimension in FFNN: 768\\n- Dropout in FFNN: 0.1\\n\\nTable 8: Statistics for AllSides training set, Basil (test only), and PartisanEvent (test only). AllSides test set contains 1,416 articles.\\n\\n|                  | AllSides | Basil | PartisanEvent |\\n|------------------|----------|-------|--------------|\\n| # stories        | 2,221    | 67    | 25           |\\n| # articles       | 5,361    | 134   | 50           |\\n| # events detected per article | 66, 82, 48, 71, 60, 70 |\"}"}
