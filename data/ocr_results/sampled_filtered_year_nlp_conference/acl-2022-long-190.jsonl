{"id": "acl-2022-long-190", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generating Data to Mitigate Spurious Correlations in Natural Language Inference Datasets\\n\\nYuxiang Wu\u2020\u2217 Matt Gardner \u2021\u2217 Pontus Stenetorp\u2020 Pradeep Dasigi\u00a7\\n\\n\u2020University College London\\nyuxiang.wu,p.stenetorp@cs.ucl.ac.uk\\n\u2021Microsoft Semantic Machines\\nmattgardner@microsoft.com, pradeepd@allenai.org\\n\u00a7Allen Institute for AI\\n\\nAbstract\\n\\nNatural language processing models often exploit spurious correlations between task-independent features and labels in datasets to perform well only within the distributions they are trained on, while not generalising to different task distributions. We propose to tackle this problem by generating a debiased version of a dataset, which can then be used to train a debiased, off-the-shelf model, by simply replacing its training data. Our approach consists of 1) a method for training data generators to generate high-quality, label-consistent data samples; and 2) a filtering mechanism for removing data points that contribute to spurious correlations, measured in terms of z-statistics. We generate debiased versions of the SNLI and MNLI datasets, and we evaluate on a large suite of debiased, out-of-distribution, and adversarial test sets. Results show that models trained on our debiased datasets generalise better than those trained on the original datasets in all settings. On the majority of the datasets, our method outperforms or performs comparably to previous state-of-the-art debiasing strategies, and when combined with an orthogonal technique, product-of-experts, it improves further and outperforms previous best results of SNLI-hard and MNLI-hard.\\n\\n1 Introduction\\n\\nNatural Language Processing (NLP) datasets inevitably contain biases that are unrelated to the tasks they are supposed to represent. These biases are usually artifacts of the annotation processes, task framing, or design decisions (Schwartz et al., 2017; Geva et al., 2019; Liu et al., 2021). Such biases often manifest as spurious correlations between simple features of the data points and their labels (Gardner et al., 2021). Trained models can exploit these spurious correlations to correctly predict the labels of the data points within the same distributions as those they are trained on, but fail to generalise to other distributions within the same tasks. Consequently, the models risk modelling the datasets, but not the tasks (Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019; Schuster et al., 2019).\\n\\nWe address this issue by adjusting existing dataset distributions to mitigate the correlations between task-independent features and labels. First, we train data generators that generate high-quality data samples in the distribution of existing datasets (Section 2). Then, we identify a set of simple features that are known to be task-independent, and use the theoretical framework (i.e., z-statistics) proposed by Gardner et al. (2021) to measure correlations between those features and the labels (Section 3.1). Finally, we adjust the distribution of the generated samples by post-hoc filtering (Section 3.2) to remove the data points that contribute to high z-statistics with task-independent features, or finetuning the data generator (Section 4.1) to make such data points less likely. Unlike prior model-\"}"}
{"id": "acl-2022-long-190", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"centric approaches to mitigate spurious correlations (Belinkov et al., 2019a,b; Clark et al., 2019; He et al., 2019; Karimi Mahabadi et al., 2020) that define new training objectives or model architectures, our approach has the advantage of keeping the objective and the model fixed, as we only alter the training data.\\n\\nTo evaluate our approach, we use the task of Natural Language Inference (NLI), which offers a wide range of datasets (including challenge datasets) for various domains. We generate debiased SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) distributions and evaluate the generalisability of models trained on them to out-of-distribution hard evaluation sets (Gururangan et al., 2018; McCoy et al., 2019), and the adversarial attack suite for NLI proposed by Liu et al. (2020b). Furthermore, we compare our method to strong debiasing strategies from the literature (Belinkov et al., 2019b; Stacey et al., 2020; Clark et al., 2019; Karimi Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2021; Ghaddar et al., 2021).\\n\\nOur results show that models trained on our debiased datasets generalise better than those trained on the original datasets to evaluation sets targeting hypothesis-only biases (by up to 2.8 percentage points) and syntactic biases (by up to 13.3 pp), and to a suite of adversarial tests sets (by up to 4.2 pp on average). Since our contributions are orthogonal to model-centric approaches, we show that when combined with product-of-experts (Karimi Mahabadi et al., 2020), our method yields further improvements and outperforms previous state-of-the-art results of SNLI-hard and MNLI-hard. Finally, we train stronger and larger pretrained language models with our debiased datasets, and demonstrate that the performance gain by our method generalises to these larger models.\\n\\n2 Generating High-Quality Data\\n\\nFirst, we need to train a data generator $G$ to generate data samples automatically. Our goal for the data generator is to model the true distribution as well as possible so that we can generate valid and high-quality data samples.\\n\\n2.1 Finetuning Pretrained Language Model to Generate NLI Samples\\n\\nWe finetune a pretrained language model on the NLI datasets to serve as our data generator. We choose GPT-2 because it is a powerful and widely-used autoregressive language model, and it can be easily adapted to generate the premise, label, and hypothesis of an instance sequentially.\\n\\nGiven an NLI dataset $D_0$, the training objective is to minimise the following negative log-likelihood loss of generating the premise-label-hypothesis sequence, in that order:\\n\\n$$L_{MLE} = -\\\\frac{1}{|D_0|} \\\\sum_{i=1}^{\\\\vert D_0 \\\\vert} \\\\log p(P_i, l_i, H_i) = -\\\\frac{1}{|D_0|} \\\\sum_{i=1}^{\\\\vert D_0 \\\\vert} \\\\log p(P_i) p(l_i | P_i) p(H_i | l_i, P_i),$$\\n\\nwhere $P_i$, $l_i$ and $H_i$ are the premise, label and hypothesis respectively.\\n\\n2.2 Improving Data Generation Quality\\n\\nWe find that samples generated by a generator trained with only $L_{MLE}$ often contain ungrammatical text or incorrect label. In this section, we introduce two techniques to improve data quality.\\n\\n2.2.1 Unlikelihood Training to Improve Label Consistency\\n\\nWe observe poor label consistency in samples generated by a generator trained with vanilla $L_{MLE}$ objective \u2013 given a generated sample $(\\\\tilde{P}, \\\\tilde{H}, \\\\tilde{l})$, the label $\\\\tilde{l}$ often does not correctly describe the relationship between $\\\\tilde{P}$ and $\\\\tilde{H}$. To alleviate this issue, we apply unlikelihood training (Welleck et al., 2020) to make generating such label inconsistent instances less likely.\\n\\nFirst we perturb the label to construct negative samples $(P, H, l')$ where $l' \\\\neq l$ for each sample in the dataset. Then we apply a token-level unlikelihood objective on the hypothesis tokens:\\n\\n$$L_{consistency} = -\\\\frac{1}{|D_0|} \\\\sum_{i=1}^{\\\\vert D_0 \\\\vert} |H_i| \\\\sum_{t=1}^{|H_i|} \\\\log (1 - p(H_i_t | l'_i, P_i, H_i)),$$\\n\\nThis objective decreases the probability of generating $H$ when given an incorrect label $l'$, hence improves the label consistency at generation time.\\n\\nIn our preliminary study, we found the factorization order premise-label-hypothesis in Eq. (1) performs better than hypothesis-label-premise and premise-hypothesis-label.\"}"}
{"id": "acl-2022-long-190", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We combine LMLE and Lconsistency to finetune our generator \\\\( G \\\\) with \\\\( L_G = L_{\\\\text{MLE}} + \\\\lambda L_{\\\\text{consistency}} \\\\), where \\\\( \\\\lambda \\\\) is a hyperparameter that balances the two objectives. We can randomly sample from the trained generator to obtain a large amount of the synthetic data \\\\( D_G \\\\sim G \\\\).\\n\\n2.2.2 Filtering Based on Model Confidence\\n\\nWe add a consistency filtering step (Lewis et al., 2021; Bartolo et al., 2021) to further improve the quality of the generated dataset. We train an NLI model \\\\( M \\\\) with the original dataset \\\\( D_0 \\\\) to filter out samples in which \\\\( M \\\\) has low confidence:\\n\\n\\\\[\\n\\\\hat{D}_G = \\\\{ (P,H,l) \\\\in D_G \\\\mid p_M(l|P,H) > \\\\tau \\\\},\\n\\\\]\\n\\nwhere \\\\( \\\\tau \\\\) is a confidence threshold. We found that the filtered out data samples generally had ungrammatical text or incorrect labels.\\n\\n3 Mitigating Spurious Correlations using \\\\( z \\\\)-filtering\\n\\nWe now define a method to reject samples that contribute to the high spurious correlations between task-independent features of the samples and their labels. Our approach is based on the theoretical framework proposed by Gardner et al. (2021) to measure these correlations, known as \\\\( z \\\\)-statistics. Our filtering method, called \\\\( z \\\\)-filtering (Section 3.2), will serve as the basis to construct debiased datasets in Section 4.\\n\\n3.1 Identifying and Measuring Spurious Correlations\\n\\nAs a first step towards addressing spurious correlations, we need to be able to quantify them. We start by selecting a set of task-independent features \u2013 features that give away the labels and allow models to exploit them without actually solving the task. For NLI, we choose the following features:\\n\\n1. unigrams and bigrams;\\n2. hypothesis length and hypothesis-premise length ratio;\\n3. lexical overlap between hypothesis and premise;\\n4. the predictions of a BERT-base (Devlin et al., 2019) hypothesis-only model.\\n\\nThese features capture various biases identified in prior work, including contradiction word biases, lexical overlap bias (McCoy et al., 2019), and hypothesis-only bias (Gururangan et al., 2018; Poliak et al., 2018). Note that our method does not rely on the specific choice of features, and one can easily add alternative features that should not be correlated with the labels.\\n\\nFollowing Gardner et al. (2021), we assume there should be no correlation between each of these features and the class labels. More formally, for any feature \\\\( x \\\\) from our feature set \\\\( X \\\\), \\\\( p(l|x) \\\\) should be uniform over the class labels \\\\( l \\\\). We define \\\\( \\\\hat{p}(l|x) = \\\\frac{1}{n} \\\\sum_{j=1}^{n} l_j \\\\) to be the empirical expectation of \\\\( p(l|x) \\\\) over \\\\( n \\\\) samples containing \\\\( x \\\\).\\n\\nThen we compute the standardised version of \\\\( z \\\\)-statistics to quantify its deviation from the uniform distribution for each feature \\\\( x \\\\) and label \\\\( l \\\\):\\n\\n\\\\[\\nz^*_{x,l} = \\\\frac{\\\\hat{p}(l|x) - p_0}{\\\\sqrt{p_0(1-p_0)/n}},\\n\\\\]\\n\\nwhere \\\\( p_0 \\\\) is the probability of uniform distribution (\\\\( p_0 = 1/3 \\\\) in NLI tasks with three labels).\\n\\nThese \\\\( z \\\\)-statistics scores can be used to identify the most biased features for each label \\\\( l \\\\) \u2013 we select \\\\( k \\\\) features with the highest \\\\( z \\\\)-statistic to define the biased features set \\\\( B_{D}(l) \\\\). Table 12 shows examples of these biased features on SNLI.\\n\\n3.2 \\\\( z \\\\)-filtering\\n\\nTo mitigate the biases in the dataset, we propose \\\\( z \\\\)-filtering, an algorithm that iteratively selects and filters instances from a dataset \\\\( D' \\\\) to build a debiased dataset \\\\( Z \\\\). At each step, we find the set of biased features \\\\( B_{Z}(l) \\\\) on the partially constructed \\\\( Z \\\\). We then select a new batch of samples from \\\\( D' \\\\) and filter out the samples that contain these biased features. This process is applied iteratively until it has exhausted all samples from \\\\( D' \\\\). It removes the samples that contribute to the spurious correlations in \\\\( D' \\\\), thus it finds a debiased subset \\\\( Z(D') \\\\subset D' \\\\). We denote the removed samples as \\\\( Z^{-}(D') \\\\). The full \\\\( z \\\\)-filtering algorithm is illustrated in Algorithm 1.\\n\\nOptionally, one can initialise \\\\( Z \\\\) with a seed dataset \\\\( D_{\\\\text{seed}} \\\\). In this case, the samples from \\\\( D' \\\\) are only added to \\\\( Z \\\\) when they do not contain the biased features of \\\\( D_{\\\\text{seed}} \\\\). Thus it can be seen as a data-augmentation technique targeted to debias a given dataset. We refer to it as conditional \\\\( z \\\\)-filtering and denote the produced debiased dataset as \\\\( Z(D'|D_{\\\\text{seed}}) \\\\).\"}"}
{"id": "acl-2022-long-190", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: z-filtering algorithm.\\n\\nData: input dataset $D$'[with optional seed dataset $D_{seed}$]\\n\\nResult: debiased dataset $Z$ and the rejected samples $Z^{-}$\\n\\n$Z^{-} \u2190 \\\\emptyset$ (or $Z \u2190 D_{seed}$);\\n\\n$Z^{-} \u2190 \\\\emptyset$;\\n\\nfor sample batch $D_{t}' \u2282 D_{t}'$ do\\n\\ncompute or update $z$-statistics $z^\u2217(x,l|Z)$, $\u2200x \u2208 X$ of $Z$;\\n\\nfind the biased features $B_{Z}(l)$, $\u2200l \u2208 \\\\{entailment, neutral, contradiction\\\\}$;\\n\\nforeach instance $I = (P,H,l) \u2208 D_{t}'$ do\\n\\nget the features $f$ of the instance $I$;\\n\\nif $f \u2229 B_{Z}(l) = \\\\emptyset$ then\\n\\n$Z \u2190 Z\u222a\\\\{I\\\\}$;\\n\\nelse\\n\\n$Z^{-} \u2190 Z^{-}\u222a\\\\{I\\\\}$;\\n\\neend\\n\\neend\\n\\neend\\n\\n4 Constructing Debiased NLI Datasets via Data Generation\\n\\nWe use z-filtering in two ways:\\n\\n1) to further fine-tune $G$ (the one trained in Section 2.2.1 with consistency unlikelihood) with an objective that downweighs samples that should be rejected (Section 4.1);\\n\\n2) to post-hoc filter the generated samples to obtain debiased datasets (Section 4.2).\\n\\n4.1 Learning to Generate Unbiased Samples\\n\\nThe generator $G$ can learn to exploit task-independent features during its finetuning stage (Section 2), causing the synthetic data $\\\\hat{D}_G$ to contain many spurious correlations. While it is tempting to apply z-filtering to remove these spurious correlations from $\\\\hat{D}_G$, we find that this will lead to the removal of majority of the generated data. For example, when the generator is finetuned on SNLI, z-filtering removes around 85% of $\\\\hat{D}_G_{SNLI}$.\\n\\nThis leads to a very inefficient data generation process to mitigate the spurious correlations.\\n\\nTo alleviate this issue, we can incorporate the debiasing objectives into the training of the generator, so that the samples produced by the generator are more likely to be accepted by the z-filtering process. More specifically, we can encourage the model to generate $Z(D_0)$, while discouraging it from generating $Z^{-}(D_0)$.\\n\\nFor the latter part, we again apply an unlikelihood training objective $L_{UL}$ to unlearn $Z^{-}(D_0)$. Hence, the overall debiasing training objective is:\\n\\n$L_{debias} = L_{MLE}(Z(D_0)) + \\\\alpha L_{UL}(Z^{-}(D_0))$\\n\\nwhere $\\\\alpha$ is a hyperparameter.\\n\\nA naive use of an unlikelihood objective on all tokens gives the model mixed signals for good tokens and leads to ungrammatical, degenerate outputs. To avoid this degeneracy, we apply the unlikelihood loss only to tokens that contribute to biased features. Concretely, for each token $I^{-}t$ of instance $I^{-} \u2208 Z^{-}(D_0)$, we define a mask $m_t$ as\\n\\n$m_t = \\\\begin{cases} 0, & \\\\text{if } I^{-}t \\\\text{ contributes to } B_{Z}(l_{I^{-}}) \\\\\\\\ 1, & \\\\text{otherwise} \\\\end{cases}$\\n\\nwhere $B_{Z}(l_{I^{-}})$ represent the biased features corresponding the label of $I^{-}$.\\n\\nFor biases towards unigram and bigram features (as defined in Section 3.1), we consider only the corresponding tokens to be relevant (i.e., $m_t = 0$ if $I^{-}t$ is part of the unigram or the bigram). For biases towards other features (e.g., length of the hypothesis), we consider all the tokens on the hypothesis to be relevant. The unlikelihood training objective is defined as follows:\\n\\n$L_{UL}(Z^{-}(D_0)) = \\\\sum_{I^{\u2032} \u2208 Z^{-}(D_0)} L_{UL}(I^{\u2032})$\\n\\n$L_{UL}(I^{\u2032}) = -|I^{\u2032}| \\\\sum_{t=1}^{\\\\log(p(I^{\u2032}t|I^{\u2032}<t) + (1 - m_t)(1 - p(I^{\u2032}t|I^{\u2032}<t)))}$\\n\\nWe further finetune $G$ with $L_{debias}$ to obtain a new generator $G^{\u2217}$, that is trained to generate more unbiased data samples. We then randomly sample from $G^{\u2217}$ and conduct data filtering (Section 2.2.2) to obtain a large set of high-quality debiased data samples $\\\\hat{D}_G^{\u2217}$.\\n\\n4.2 Combining with z-filtering to Construct the Debiased NLI Datasets\\n\\nGiven the original dataset $D_0$ and the synthetic dataset $\\\\hat{D}_G^{\u2217}$, our goal is produce a large-scale unbiased dataset $D^{\u2217}$. There are various ways to do...\"}"}
{"id": "acl-2022-long-190", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"this given that we can either apply conditional z-filtering, or simply z-filter both \\\\(D_0\\\\) and \\\\(\\\\hat{D}_{G^*}\\\\) and merge them. We explore the following options:\\n\\n1. **Z-Augmentation (Z-Aug)**\\n   \\\\(Z(\\\\hat{D}_{G^*}|D_0)\\\\): we keep the original dataset as is, and augment it by conducting conditional z-filtering on \\\\(\\\\hat{D}_{G^*}\\\\) using \\\\(D_0\\\\) as seed dataset.\\n\\n2. **Parallel z-filter (Par-Z)**\\n   \\\\(Z(D_0) \\\\cup Z(\\\\hat{D}_{G^*})\\\\): we conduct z-filtering on \\\\(D_0\\\\) and \\\\(\\\\hat{D}_{G^*}\\\\) separately, and then merge them.\\n\\n3. **Sequential z-filter (Seq-Z)**\\n   \\\\(Z(\\\\hat{D}_{G^*}|Z(D_0))\\\\): we first conduct z-filtering on \\\\(D_0\\\\), then conduct conditional z-filtering on \\\\(\\\\hat{D}_{G^*}\\\\) with \\\\(Z(D_0)\\\\) as seed dataset.\\n\\n### 5 Experiments\\n\\n#### 5.1 Experimental Setup\\n\\n**Source Datasets**\\nWe select the two most widely used NLI datasets SNLI (Bowman et al., 2015) and MNLI (Williams et al., 2018) as our original datasets. Prior work (Gururangan et al., 2018; Poliak et al., 2018; McCoy et al., 2019) found various annotation artifacts in them, hence they serve as good use cases for constructing debiased datasets.\\n\\n**Evaluation Datasets**\\nFor the hypothesis-only bias, we use the challenge sets SNLI-hard (Gururangan et al., 2018) and MNLI-hard (Williams et al., 2018), which were produced by filtering the test set with a hypothesis-only model (Section 5.2). For syntactic biases, we follow previous work and use HANS (McCoy et al., 2019) for evaluation (Section 5.3). In addition, we evaluate on the adversarial test benchmark introduced by Liu et al. (2020b) (Section 5.4). This benchmark covers a wide range of adversarial attacks, which will give a more complete picture of what spurious correlations the debiasing methods tackle.\\n\\n**Generating Debiased Datasets**\\nWe conduct debiased data generation for SNLI and MNLI separately. For SNLI, we use the proposed method described in Section 4.1 to train a generator \\\\(G^*_{SNLI}\\\\). Then we randomly sample a large number of instances from the generator to construct \\\\(D_{G^*_{SNLI}}\\\\). The samples are filtered with a strong NLI model \\\\(M\\\\) trained on SNLI to obtain \\\\(\\\\hat{D}_{G^*_{SNLI}}\\\\). Finally, different options (Section 4.2) can be adopted to merge the synthetic data with the original data \\\\(D_{SNLI}\\\\) to construct debiased versions of SNLI. The same procedure is used to produce debiased datasets for MNLI, by simply replacing the original dataset with MNLI. We choose GPT-2 large and Roberta-large as the pretrained language models for \\\\(G^*\\\\) and \\\\(M\\\\) respectively.\\n\\nThe size of the constructed debiased datasets are listed in Table 1.\\n\\n| Procedure          | SNLI | MNLI |\\n|--------------------|------|------|\\n| Original           | 549,367 | 382,702 |\\n| Z-Aug              | 1,142,475 | 744,326 |\\n| Par-Z              | 933,085 | 740,811 |\\n| Seq-Z              | 927,906 | 744,200 |\\n\\nTable 1: Data size of the constructed debiased datasets for SNLI and MNLI.\\n\\n**NLI Model Training**\\nSince our method directly debiases the training data itself, we keep the model and training objective fixed and only replace the training data with our generated debiased datasets. For comparability with previous work (Karimi Mahabadi et al., 2020; Utama et al., 2020; Sanh et al., 2021), we train BERT-base (Devlin et al., 2019) on our debiased datasets. The NLI models are trained with ordinary cross-entropy classification loss, and the training hyperparameters are listed in Appendix A. We run our experiments five times and report the average and standard deviation of the scores.\\n\\nWe also conduct statistical significance testing using a 2-tailed t-test at 95% confidence level.\\n\\n**State-of-the-art Debiasing Models**\\nWe compare our method with the following three state-of-the-art debiasing models on each of our evaluation datasets.\\n\\n- **Product-of-Experts** (He et al., 2019; Karimi Mahabadi et al., 2020) ensembles a bias-only model's prediction \\\\(b_i\\\\) with the main model's prediction \\\\(p_i\\\\) using \\\\(p'_i = \\\\text{softmax}(\\\\log p_i + \\\\log b_i)\\\\). This ensembling enforces that the main model focuses on the samples that the bias-only model does not predict well.\\n\\n- **Learned-Mixin** (Clark et al., 2019) is a variant of PoE that introduces a learnable weight for the bias-only model's prediction.\\n\\n- **Regularized-conf** (Utama et al., 2020) uses confidence regularisation to retain the in-distribution performance while conducting model debiasing.\\n\\n### 5 Footnotes\\n\\n5 On one A100 GPU, training the generator takes around 24 hours and generating the samples takes roughly 35 hours for each dataset.\\n\\n6 With the exception of our PoE experiments which single run, as hyperparameter tuning for PoE is costlier.\"}"}
{"id": "acl-2022-long-190", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Accuracy on SNLI and SNLI-hard.\\n\\n| Method (model w/ data) | SNLI  | SNLI-hard |\\n|------------------------|-------|-----------|\\n| Prior debiasing strategies trained on SNLI |       |           |\\n| AdvCls (Belinkov et al., 2019a) | 83.56 | 66.27     |\\n| Ens. AdvCls (Stacey et al., 2020) | 84.09 | 67.42     |\\n| DFL (Karimi Mahabadi et al., 2020) | 89.57 | 83.01     |\\n| PoE (Karimi Mahabadi et al., 2020) | 90.11 | 82.15     |\\n| BERT-base w/ \\\\(D_{SNLI}\\\\) baseline | 90.45 | 80.34 \u00b1 0.46 |\\n| Models trained on our debiased datasets |       |           |\\n| BERT-base w/ Z-Aug | 90.67 | 81.78 \u00b1 0.53 |\\n| BERT-base w/ Par-Z | 88.11 | 82.81 \u00b1 0.37 |\\n| BERT-base w/ Seq-Z | 88.08 | 82.82 \u00b1 0.15 |\\n\\nCombining PoE with our debiased datasets\\n\\n| Method (model w/ data) | SNLI  | SNLI-hard |\\n|------------------------|-------|-----------|\\n| BERT-base + PoE w/ \\\\(D_{SNLI}\\\\) | 90.25 | 82.92     |\\n| BERT-base + PoE w/ Seq-Z | 87.65 | 84.48     |\\n\\n---\\n\\nOur approach changes the training data distribution instead of the model's training objective, and hence is orthogonal to prior work method-wise. We also report the results of combining PoE with our proposed method, simply by training a PoE model on our debiased datasets. We adapt the PoE implementation by Karimi Mahabadi et al. (2020), and we follow their approach to conduct hyperparameter tuning for PoE.\\n\\nThe hyperparameters of the PoE models are reported in Table 10 of Appendix A.\\n\\n### 5.2 Hypothesis-only Bias in NLI\\n\\nGururangan et al. (2018) found that, on SNLI and MNLI, a model that only has access to the hypothesis can perform surprisingly well, which indicates that the datasets contain hypothesis-only bias. To alleviate this problem, SNLI-hard and MNLI-hard (Gururangan et al., 2018) subsets were constructed by filtering the test set with a hypothesis-only model and only accepting those that the hypothesis-only model predicts incorrectly. We examine whether our method successfully mitigates the hypothesis-only bias in NLI, by evaluating the models trained with our debiased datasets on SNLI-hard and MNLI-hard.\\n\\n**Results on SNLI-hard**\\n\\nTable 2 shows the results of our method on SNLI and SNLI-hard. The results show that, compared to training on SNLI, training with our debiased datasets significantly improves the performance on SNLI-hard. The debiased dataset produced by Seq-Z achieves a 2.48% gain in accuracy on SNLI-hard compared to the SNLI baseline, whereas Z-Aug improves both SNLI and SNLI-hard accuracy.\\n\\n**Results on MNLI-hard**\\n\\nTable 3 shows the results of our method on MNLI-matched (MNLI-m) and MNLI-mismatched (MNLI-mm), and their corresponding hard sets. We use the development sets of MNLI-hard reconstructed by (Karimi Mahabadi et al., 2020) to develop our methods. To comply with the submission limit of MNLI leaderboard system, we select the best checkpoint among the five runs using the development set, and report its test set performance in Table 3.\\n\\nThe results show that BERT-base models trained on our debiased MNLI datasets outperform the models trained on the original MNLI by a large margin on the MNLI-hard sets. In particular, the Z-Aug version of the debiased datasets gives a 2.72% and 2.76% gain in accuracy on MNLI-m hard and MNLI-mm hard respectively, and outperforms the previous state-of-the-art on MNLI-m, MNLI-mm, and MNLI-mm hard.\\n\\n### Combining PoE with Our Debiased Datasets\\n\\nWe investigate the combination of our method and PoE, to see if the two orthogonal techniques can work together to achieve better performance. Since hyperparameter tuning of PoE is costly, we choose the best version of the debiased dataset (Seq-Z for SNLI and Z-Aug for MNLI) using the development set accuracy, and train PoE with it. The results are listed in the last rows of Table 2 and Table 3. We can find that, on both SNLI and MNLI, combining PoE with our debiased dataset yields further improvements on SNLI-hard, MNLI-m hard, and MNLI-mm hard, outperforming previous state-of-the-art results on all three datasets.\\n\\n### 5.3 Syntactic Bias in NLI\\n\\nMcCoy et al. (2019) show that NLI models trained on MNLI can exploit syntactic heuristics present in the data, such as lexical overlap, subsequence, and constituent features. They introduce HANS, an evaluation dataset that contains examples where the syntactic heuristics fail. To test whether our method mitigates the syntactic biases in NLI, we evaluate models trained on our debiased datasets on HANS. If our debiased dataset contains less syntactic bias than the original dataset, the model would not exploit the syntactic heuristics and thus perform better on HANS. Due to the high variance, the model would not exploit the syntactic heuristics and thus perform better on HANS.\"}"}
{"id": "acl-2022-long-190", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Prior debiasing strategies trained on MNLI:\\n- PoE (Karimi Mahabadi et al., 2020)\u2217 84.58 84.11 84.85 83.47 78.02 76.81 79.23 76.83\\n- Learned-Mixin (Clark et al., 2019)\u2217 80.5 79.5 81.2 80.4 - 79.2 - 78.2\\n- Regularized-conf (Utama et al., 2020)\u2217 84.6 84.1 85.0 84.2 - 78.3 - 77.3\\n- BERT-base Main PoE+CE (Sanh et al., 2021)\u2217 83.32 - 83.54 - - 77.63 - 76.39\\n- BERT-base w/ D MNLI baseline 83.87 84.11 84.22 83.51 76.39 \u00b1 0.64 75.88 77.75 \u00b1 0.45 75.75\\n\\nModels trained on our debiased datasets:\\n- BERT-base w/ Z-Aug Z(\u02c6D|D MNLI) 84.72 85.12 85.14 84.09 78.95 \u00b1 0.76 78.60 \u00b1 0.70 80.29 \u00b1 0.54 78.51\\n- BERT-base w/ Par-Z Z(D MNLI) \u222aZ(\u02c6D|D MNLI) 82.48 83.27 82.95 82.95 78.88 \u00b1 0.80 79.19 \u00b1 0.80 80.02 \u00b1 0.62 78.49\\n- BERT-base w/ Seq-Z Z(\u02c6D|D MNLI) 82.55 83.41 82.70 83.17 78.88 \u00b1 0.83 79.19 \u00b1 0.83 79.65 \u00b1 0.44 78.44\\n\\nCombining PoE with our debiased dataset:\\n- BERT-base + PoE w/ D MNLI 84.39 84.69 84.25 83.75 78.37 77.54 79.45 78.33\\n- BERT-base + PoE w/ Z-Aug Z(\u02c6D|D MNLI) 85.22 85.38 85.72 84.53 80.49 80.03 81.52 79.28\\n\\nTable 3: Accuracy on MNLI-matched (MNLI-m), MNLI-mismatched (MNLI-mm), MNLI-matched hard, and MNLI-mismatched hard.\\n\u2217 are reported results and underscore indicates statistical significance against the baseline.\\n\\nTraining on our debiased MNLI datasets significantly boosts the performance on MNLI-matched hard and MNLI-mismatched hard. When combined with PoE, our method improves further and outperforms previous methods.\\n\\nTable 4: Results on HANS (McCoy et al., 2019).\\n\u2217 are reported results and underscore indicates statistical significance against the baseline. BERT-base trained on our debiased MNLI datasets performs significantly better than the one trained on the original MNLI, and it improves further when combined with PoE. Roberta-large also benefits from training on our debiased dataset.\"}"}
{"id": "acl-2022-long-190", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Several data augmentation strategies were investigated by Liu et al. (2020b): 1) text swap: swapping the premise and hypothesis in the original data; 2) word substitution: replacing words in the hypothesis with synonyms or generations from a masked language model; 3) paraphrase: using back translation to paraphrase the hypothesis.\\n\\nWe compare our approach with their data-augmentation heuristics, and the results are shown in Table 5. Comparing with the MNLI baseline, our debiased MNLI datasets lead to better performance across all categories, which indicates that our method successfully mitigates various distinct biases simultaneously. All three variants of our debiased datasets outperform the data augmentation heuristics by Liu et al. (2021), which demonstrates the efficacy of our method when compared against manually designed heuristics.\\n\\n### Table 5: Results on the NLI adversarial test benchmark (Liu et al., 2020b).\\n\\n| Heuristics                  | PI-CD  | PI-SP  | IS-SD  | IS-CS  | LI-LI  | LI-TS  | ST  | Avg.  |\\n|-----------------------------|--------|--------|--------|--------|--------|--------|-----|-------|\\n| Text Swap                   | 71.7   | 72.8   | 63.5   | 67.4   | 86.8   | 66.5   | 73.6 | 86.8  |\\n| Sub (synonym)               | 69.8   | 72.0   | 62.4   | 65.8   | 85.2   | 64.3   | 71.8 |       |\\n| Sub (MLM)                   | 71.0   | 72.8   | 64.4   | 65.9   | 85.6   | 83.3   | 64.9 | 72.6  |\\n| Paraphrase                  | 72.1   | 74.6   | 66.5   | 66.4   | 85.7   | 83.1   | 64.8 | 73.3  |\\n| BERT-base w/ DMNLI baseline | 70.3\u00b10.5 | 73.7\u00b11.4 | 53.5\u00b12.3 | 64.8\u00b11.4 | 85.5\u00b10.9 | 81.6\u00b11.4 | 69.2\u00b10.8 | 71.2\u00b10.8 |\\n| BERT-base w/ Z-Aug          | 73.1\u00b10.9 | 76.1\u00b11.2 | 61.8\u00b16.1 | 69.1\u00b11.3 | 86.9\u00b10.6 | 83.1\u00b10.9 | 70.1\u00b10.5 | 74.3\u00b11.3 |\\n| BERT-base w/ Par-Z          | 72.0\u00b10.9 | 78.7\u00b11.2 | 64.5\u00b15.8 | 70.7\u00b11.7 | 88.5\u00b10.7 | 82.6\u00b10.3 | 69.6\u00b11.0 | 75.2\u00b11.4 |\\n| BERT-base w/ Seq-Z          | 71.7\u00b10.9 | 77.8\u00b11.2 | 66.9\u00b13.7 | 71.1\u00b10.7 | 89.1\u00b11.0 | 82.3\u00b10.9 | 69.3\u00b10.8 | 75.4\u00b10.8 |\\n\\nTable 6: Performance gain when training larger models with our debiased datasets. Underscore indicates statistical significance against the baseline that is trained on the original datasets. For evaluation on SNLI-hard, the models are trained with SNLI or our debiased Seq-Z SNLI; for other evaluation datasets, the models are trained with MNLI or our debiased Z-Aug MNLI. Albert-xxlarge is experimented with one run due to its higher training cost.\"}"}
{"id": "acl-2022-long-190", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6 Related Work\\n\\nSpurious Correlations in Datasets\\n\\nThe issue of spurious correlations in datasets between labels and simple input features has recently received significant attention (Gururangan et al., 2018; Poliak et al., 2018; Belinkov et al., 2019a; Karimi Mahabadi et al., 2020). It has been shown that this issue is often inherent in the data annotation process, caused by biases in the framing of the task (Schwartz et al., 2017), noisy annotations (Chen et al., 2016), or personal (Geva et al., 2019) or group-level (Liu et al., 2021) annotator biases. Gardner et al. (2021) provide a theoretical framework for analyzing spurious correlations, which we use to define our filtering mechanism in Section 3.2.\\n\\nDebiasing NLI Models\\n\\nMuch prior work follows a model-centric approach towards mitigating biases in NLI models \u2013 they propose novel model architectures or training objectives to ensure that the models do not exploit the shortcuts presented by the dataset biases. At the representation level, Belinkov et al. (2019a,b) introduce an adversarial architecture to debias hypothesis representations to tackle hypothesis-only bias (Gururangan et al., 2018), and Stacey et al. (2020) strengthen the debiasing by using multiple adversarial classifiers. Zhou and Bansal (2020) use HEX projection to project the representation to the space orthogonal to the biased features to debias the model. At the model level, Clark et al. (2019); He et al. (2019); Karimi Mahabadi et al. (2020) propose methods based on Product-of-Expert (PoE) (Hinton, 2002) for mitigating biases by ensembling a biased-only model with a main model. Utama et al. (2020) propose the use of confidence regularization to improve out-of-distribution performance while retaining in-distribution accuracy.\\n\\nDebiasing NLI Datasets\\n\\nRoss et al. (2021) introduce TAILOR, a semantically-controlled perturbation method for data augmentation based on a small number of manually defined perturbation strategies. Bras et al. (2020) propose AFLite, a dataset filtering method that learns feature representations with a model and conduct adversarial filtering based on model predictions. Unlike these approaches, our method requires no manually-written perturbation heuristics and is model-agnostic, hence it is more generally applicable.\\n\\nGenerative Data Augmentation\\n\\nSeveral works investigate generative data augmentation techniques to improve model robustness in other areas. Yang et al. (2020) conduct generative data augmentation for commonsense reasoning and show that it can improve out-of-domain generalisation. Lee et al. (2021) trains a generator to generate new claims and evidence for debiasing fact verification datasets like FEVER (Thorne et al., 2018). Schick and Sch\u00fctze (2021) exploit large pretrained language models to generate semantic textual similarity datasets. Bartolo et al. (2021) improve robustness of question answering models by generating adversarial dataset.\\n\\n7 Conclusions\\n\\nTo address the issue of spurious correlations between task-independent features and labels in NLI datasets, we propose methods to generate label-consistent data and then filter out instances from existing datasets that contribute to those spurious correlations; thereby generating debiased datasets. Models trained on our debiased versions of the SNLI and MNLI datasets generalise better than the equivalent model trained on the original datasets to a large suite of test sets focusing on various kinds of known biases. Future work in this direction includes investigating whether our techniques are applicable to tasks beyond NLI.\\n\\nAcknowledgments\\n\\nThe authors would like to thank Max Bartolo, Alexis Ross, Doug Downey, Jesse Dodge, Pasquale Minervini, and Sebastian Riedel for their helpful discussion and feedback.\\n\\nReferences\\n\\nMax Bartolo, Tristan Thrush, Robin Jia, Sebastian Riedel, Pontus Stenetorp, and Douwe Kiela. 2021. Improving question answering model robustness with synthetic adversarial data generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8830\u20138848, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nYonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, and Alexander Rush. 2019a. Don\u2019t take the premise for granted: Mitigating artifacts in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 877\u2013891, Florence, Italy. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-190", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yonatan Belinkov, Adam Poliak, Stuart Shieber, Benjamin Van Durme, and Alexander Rush. 2019b. On adversarial removal of hypothesis-only bias in natural language inference. In Proceedings of the Eighth Joint Conference on Lexical and Computational Semantics (*SEM 2019), pages 256\u2013262, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nPrajjwal Bhargava, Aleksandr Drozd, and Anna Rogers. 2021. Generalization in NLI: Ways (not) to go beyond simple heuristics. In Proceedings of the Second Workshop on Insights from Negative Results in NLP, pages 125\u2013135, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nSamuel R. Bowman. 2021. When combating hype, proceed with caution. CoRR, abs/2110.08300.\\n\\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. 2015. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642, Lisbon, Portugal. Association for Computational Linguistics.\\n\\nRonan Le Bras, Swabha Swayamdipta, Chandra Bhagavatula, Rowan Zellers, Matthew E. Peters, Ashish Sabharwal, and Yejin Choi. 2020. Adversarial filters of dataset biases. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 1078\u20131088. PMLR.\\n\\nDanqi Chen, Jason Bolton, and Christopher D. Manning. 2016. A thorough examination of the CNN/Daily Mail reading comprehension task. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2358\u20132367, Berlin, Germany. Association for Computational Linguistics.\\n\\nChristopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019. Don't take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4069\u20134082, Hong Kong, China. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMatt Gardner, William Merrill, Jesse Dodge, Matthew Peters, Alexis Ross, Sameer Singh, and Noah A. Smith. 2021. Competency problems: On finding and removing artifacts in language data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1801\u20131813, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? An investigation of annotator bias in natural language understanding datasets. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1161\u20131166, Hong Kong, China. Association for Computational Linguistics.\\n\\nAbbas Ghaddar, Philippe Langlais, Mehdi Rezagholizadeh, and Ahmad Rashid. 2021. End-to-end self-debiasing framework for robust NLU training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 1923\u20131929, Online. Association for Computational Linguistics.\\n\\nMax Glockner, Vered Shwartz, and Yoav Goldberg. 2018. Breaking NLI systems with sentences that require simple lexical inferences. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 650\u2013655, Melbourne, Australia. Association for Computational Linguistics.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A. Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107\u2013112, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nHe He, Sheng Zha, and Haohan Wang. 2019. Unlearn dataset bias in natural language inference by fitting the residual. In Proceedings of the 2nd Workshop on Deep Learning Approaches for Low-Resource NLP (DeepLo 2019), pages 132\u2013142, Hong Kong, China. Association for Computational Linguistics.\\n\\nGeoffrey E Hinton. 2002. Training products of experts by minimizing contrastive divergence. Neural Computation, 14(8):1771\u20131800.\\n\\nRabeeh Karimi Mahabadi, Yonatan Belinkov, and James Henderson. 2020. End-to-end bias mitigation by modelling biases in corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8706\u20138716, Online. Association for Computational Linguistics.\\n\\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020. ALBERT: A lite BERT for self-supervised...\"}"}
{"id": "acl-2022-long-190", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"learning of language representations. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nMinwoo Lee, Seungpil Won, Juae Kim, Hwanhee Lee, Cheoneum Park, and Kyomin Jung. 2021. Crossaug: A contrastive data augmentation method for debiasing fact verification models. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management, pages 3181\u20133185.\\n\\nPatrick Lewis, Yuxiang Wu, Linqing Liu, Pasquale Minervini, Heinrich K\u00fcttler, Aleksandra Piktus, Pontus Stenetorp, and Sebastian Riedel. 2021. PAQ: 65 million probably-asked questions and what you can do with them. Transactions of the Association for Computational Linguistics, 9:1098\u20131115.\\n\\nHaochen Liu, Joseph Thekinen, Sinem Mollaoglu, Da Tang, Ji Yang, Youlong Cheng, Hui Liu, and Jiliang Tang. 2021. Toward annotator group bias in crowdsourcing. ArXiv preprint, abs/2110.08038.\\n\\nTianyu Liu, Zheng Xin, Baobao Chang, and Zhifang Sui. 2020a. HypoNLI: Exploring the artificial patterns of hypothesis-only bias in natural language inference. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 6852\u20136860, Marseille, France. European Language Resources Association.\\n\\nTianyu Liu, Zheng Xin, Xiaoan Ding, Baobao Chang, and Zhifang Sui. 2020b. An empirical study on model-agnostic debiasing strategies for robust natural language inference. In Proceedings of the 24th Conference on Computational Natural Language Learning, pages 596\u2013608, Online. Association for Computational Linguistics.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pretraining approach. CoRR, abs/1907.11692.\\n\\nTom McCoy, Ellie Pavlick, and Tal Linzen. 2019. Right for the wrong reasons: Diagnosing syntactic heuristics in natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3428\u20133448, Florence, Italy. Association for Computational Linguistics.\\n\\nPasquale Minervini and Sebastian Riedel. 2018. Adversarially regularising neural NLI models to integrate logical background knowledge. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 65\u201374, Brussels, Belgium. Association for Computational Linguistics.\\n\\nAakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress test evaluation for natural language inference. In Proceedings of the 27th International Conference on Computational Linguistics, pages 2340\u20132353, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nYixin Nie, Yicheng Wang, and Mohit Bansal. 2019. Analyzing compositionality-sensitivity of NLI models. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, Honolulu, Hawaii, USA, pages 6867\u20136874. AAAI Press.\\n\\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In Proceedings of the Seventh Joint Conference on Lexical and Computational Semantics, pages 180\u2013191, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nAlexis Ross, Tongshuang Wu, Hao Peng, Matthew E Peters, and Matt Gardner. 2021. Tailor: Generating and perturbing text with semantic controls. ArXiv preprint, abs/2107.07150.\\n\\nVictor Sanh, Thomas Wolf, Yonatan Belinkov, and Alexander M. Rush. 2021. Learning from others\u2019 mistakes: Avoiding dataset biases without modeling them. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\\n\\nTimo Schick and Hinrich Sch\u00fctze. 2021. Generating datasets with pretrained language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6943\u20136951, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nTal Schuster, Darsh Shah, Yun Jie Serene Yeo, Daniel Roberto Filizzola Ortiz, Enrico Santus, and Regina Barzilay. 2019. Towards debiasing fact verification models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3419\u20133425, Hong Kong, China. Association for Computational Linguistics.\\n\\nRoy Schwartz, Maarten Sap, Ioannis Konstas, Leila Zilles, Yejin Choi, and Noah A. Smith. 2017. The effect of different writing tasks on linguistic style: A case study of the ROC story cloze task. In Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 15\u201325, Vancouver, Canada. Association for Computational Linguistics.\\n\\nJoe Stacey, Yonatan Belinkov, and Marek Rei. 2021. Supervising model attention with human explanations for robust natural language inference. ArXiv preprint, abs/2104.08142.\\n\\nJoe Stacey, Pasquale Minervini, Haim Dubossarsky, Sebastian Riedel, and Tim Rockt\u00e4schel. 2020. Avoiding the Hypothesis-Only Bias in Natural Language Inference via Ensemble Adversarial Training. In\"}"}
{"id": "acl-2022-long-190", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 8281\u20138291, Online. Association for Computational Linguistics.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018. FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 809\u2013819, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nPrasetya Ajie Utama, Nafise Sadat Moosavi, and Iryna Gurevych. 2020. Mind the trade-off: Debiasing NLU models without degrading the in-distribution performance. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8717\u20138729, Online. Association for Computational Linguistics.\\n\\nHaohan Wang, Da Sun, and Eric P. Xing. 2019. What if we simply swap the two text fragments? A straightforward yet effective way to test the robustness of methods to confounding signals in nature language inference tasks. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, Honolulu, Hawaii, USA, pages 7136\u20137143. AAAI Press.\\n\\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Diinan, Kyunghyun Cho, and Jason Weston. 2020. Neural text generation with unlikelihood training. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nYiben Yang, Chaitanya Malaviya, Jared Fernandez, Swabha Swayamdipta, Ronan Le Bras, Ji-Ping Wang, Chandra Bhagavatula, Yejin Choi, and Doug Downey. 2020. G-daug: Generative data augmentation for commonsense reasoning. In EMNLP (Findings), volume EMNLP 2020 of Findings of ACL, pages 1008\u20131025. Association for Computational Linguistics.\\n\\nXiang Zhou and Mohit Bansal. 2020. Towards robustifying NLI models against lexical dataset biases. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8759\u20138771, Online. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-190", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Hyperparameters\\n\\nA.1 Hyperparameters of Our Proposed Method\\n\\n| Hyperparameter          | Value                  |\\n|-------------------------|------------------------|\\n| learning rate           | 1e-5                   |\\n| batch size              | 24                     |\\n| epoch                   | 5                      |\\n| optimiser               | Adam                   |\\n| Adam $\\\\epsilon$        | 1e-6                   |\\n| Adam ($\\\\beta_1$, $\\\\beta_2$) | (0.9, 0.999)          |\\n| learning rate scheduler | constant               |\\n| max sequence length     | 128                    |\\n| pretrained model        | GPT-2 large            |\\n| device                  | Nvidia A100            |\\n| $\\\\lambda$               | 0.5                    |\\n| $\\\\alpha$                | 1.0                    |\\n\\nTable 7: Hyperparameters for training the generator $G^*$. \\n\\n| Hyperparameter          | Value                  |\\n|-------------------------|------------------------|\\n| number of samples from $G^*$ SNLI | 5,000,000          |\\n| number of samples from $G^*$ MNLI     | 4,000,000            |\\n| data filtering threshold | $\\\\tau$ 0.95          |\\n| data filtering model   | Roberta-large          |\\n| z-filtering number of biased features | 20             |\\n\\nTable 8: Hyperparameters of the data generation pipeline.\\n\\n| Hyperparameter          | Value                  |\\n|-------------------------|------------------------|\\n| learning rate           | 1e-5                   |\\n| batch size              | 32                     |\\n| epoch                   | 5                      |\\n| optimiser               | Adam                   |\\n| Adam $\\\\epsilon$        | 1e-6                   |\\n| Adam ($\\\\beta_1$, $\\\\beta_2$) | (0.9, 0.999)          |\\n| learning rate scheduler | constant with warmup   |\\n| warm up steps           | 2000                   |\\n| max sequence length     | 128                    |\\n| pretrained model        | BERT-base              |\\n| device                  | Nvidia A100            |\\n| early stop patience     | 3 epochs              |\\n\\nTable 9: Hyperparameters for training the NLI models.\\n\\nA.2 Hyperparameter Tuning of PoE\\n\\nThe learning objective of PoE is defined as follows:\\n\\n$$L_{PoE} = \\\\frac{1}{|D|} \\\\sum_{i=1}^{|D|} CE(l_i, p'_i) + \\\\gamma CE(l_i, b_i),$$\\n\\nwhere $CE$ stands for cross-entropy loss, $l_i$ is the label, and $\\\\gamma$ is a hyperparameter.\\n\\n$p'_i = \\\\text{softmax}(\\\\log p_i + \\\\beta \\\\log b_i)$ is the ensemble of the main model's prediction $p_i$ and the bias-only model's prediction $b_i$ weighted by a hyperparameter $\\\\beta$.\\n\\nWe find that the result of PoE is very sensitive to the hyperparameters $\\\\beta$ and $\\\\gamma$. Following Karimi Mahabadi et al. (2020), we conduct grid search for the two hyperparameters, with $\\\\beta \\\\in \\\\{0.05, 0.1, 0.2, 0.4, 0.8, 1.0, 2.0\\\\}$ and $\\\\gamma \\\\in \\\\{0.05, 0.1, 0.2, 0.4, 0.8, 1.0\\\\}$. The best hyperparameters found for each evaluation dataset is listed in Table 10.\\n\\n| Train data | Eval. data | $\\\\beta$ | $\\\\gamma$ |\\n|------------|------------|---------|----------|\\n| SNLI       | SNLI       | 2.0     | 0.4      |\\n| Seq-Z      | SNLI       | 2.0     | 0.4      |\\n| MNLI       | MNLI       | 2.0     | 0.4      |\\n| MNLI-m     | MNLI-mm    | 2.0     | 0.4      |\\n| MNLI-m     | hard       | 2.0     | 0.4      |\\n| MNLI-mm    | hard       | 2.0     | 0.4      |\\n| MNLI-mm    | HANS       | 2.0     | 0.8      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n| MNLI-mm    | HANS       | 2.0     | 1.0      |\\n\\nTable 10: Best hyperparameters found for PoE models with different training and evaluation datasets.\\n\\nB Task-independent Features\\n\\nWe list the chosen set of task-independent features that we aim to mitigate in this work in Table 11. Note that our method does not depend on the choice of task-independent features. One can easily add their own features in the future to mitigate newly-identified spurious correlations.\\n\\nTable 12 shows the most salient task-independent features (ranked by z-statistics) in SNLI and our debiased SNLI dataset. It shows that the correlation between task-independent features and labels is massively reduced, dropping from over 400 to roughly 17. These results verify that our method successfully mitigates the spurious correlations in the dataset.\"}"}
{"id": "acl-2022-long-190", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Feature Description\\nUnigrams & Bigrams All unigrams and bigrams. The n-grams from premise and hypothesis are treated separately.\\nHypothesis length Number of tokens in the hypothesis.\\nHypothesis-premise length ratio Number of tokens in hypothesis divided by number of tokens in the premise.\\nLexical overlap Ratio of tokens in the hypothesis that overlap with the premise.\\nHypothesis-only model\u2019s prediction We train a hypothesis-only model on the original dataset and use its prediction as a feature.\\nNull feature A dummy feature added for all instances to avoid skewed label distribution.\\n\\nTable 11: Descriptions of the features used to debias the datasets in Section 3.\\n\\n| Class     | Feature Description | Score |\\n|-----------|---------------------|-------|\\n| Entailment| hypo-only-pred=0    | 422.1 |\\n|           | therapeutically     |       |\\n|           | 17.5 lex-overlap    |       |\\n|           | > 0.8               |       |\\n|           | hypo-len = 123.3    |       |\\n|           | < 5                 |       |\\n|           | full-lex-overlap    | 17.4  |\\n|           | 123.3 full-lex-overlap |     |\\n|           | speaking@hypothesis |       |\\n|           | 17.4                 |       |\\n\\n| Neutral   | championship@hypothesis | 436.1 |\\n|           | for a@hypothesis        |       |\\n|           | 63.6 living room@hypothesis |   |\\n|           | his@hypothesis          |       |\\n|           | 56.8 many men@hypothesis |       |\\n|           | friends@hypothesis      |       |\\n|           | 55.6 green suit@hypothesis |   |\\n|           | tall@hypothesis         |       |\\n|           | 52.7 are wearing@hypothesis |   |\\n\\n| Contradiction | nothing@hypothesis | 433.9 |\\n|              | is sleeping@hypothesis | 16.9  |\\n|              | at home@hypothesis    |       |\\n|              | nobody@hypothesis      |       |\\n|              | is no@hypothesis       |       |\\n|              | york yankees@hypothesis |   |\\n\\nTable 12: Top-5 biased features with the highest z-statistics on SNLI (left) and debiased Seq-Z SNLI (right) for each label class.\\n\\nC Description of Adversarial Test (Liu et al., 2020b) Subcategories\\nThe adversarial test benchmark (Liu et al., 2020b) includes the following subcategories from various sources:\\n\u2022 PI-CD: classifier detected partial-input (Gururangan et al., 2018).\\n\u2022 PI-SP: HypoNLI (Liu et al., 2020a) dataset that tackles surface patterns heuristics.\\n\u2022 IS-SD: syntactic diagnostic dataset HANS (McCoy et al., 2019).\\n\u2022 IS-CS: lexically misleading instances constructed by Nie et al. (2019).\\n\u2022 LI-LI: lexical inference test by (Naik et al., 2018; Glockner et al., 2018).\\n\u2022 LI-TS: text-fragment swap test by swapping the premise and hypothesis (Wang et al., 2019; Minervini and Riedel, 2018).\\n\u2022 ST: an aggregation of word-overlap (ST-WO), negation (ST-NE), length mismatch (ST-LM), and spelling errors (ST-SE) tests in (Naik et al., 2018).\\n\\nD Visualisation of z-statistics\\nFollowing Gardner et al. (2021), we visualise the statistics of the features on both SNLI and our debiased SNLI (Seq-Z) dataset in Fig. 2. Comparing the two plots, it confirms that our method successfully suppresses the spurious correlations in the dataset.\\n\\nE Ablation Study\\n| Data Size | SNLI | SNLI-hard | Seq-Z | Z-Aug |\\n|-----------|------|-----------|-------|-------|\\n|           |      |           |       | Z-Aug |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n|           |      |           |       |       |\\n"}
{"id": "acl-2022-long-190", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Statistics of the features on SNLI and our debiased SNLI (Seq-Z).\\n\\nTable 14 and Table 15 show generated samples in the debiased SNLI and MNLI datasets respectively. The samples are quite diverse and the quality is reasonably good, which demonstrates the effectiveness of our quality ensuring techniques presented in Section 2.2.\"}"}
{"id": "acl-2022-long-190", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thanksgiving dinner is a fun time for everyone. The dinner is a fun event. entailment\\n\\nA father is letting his toddler drink from his glass. A toddler is having a drink entailment\\n\\nHair stylist performing a haircut. A hailer is performing surgery contradiction\\n\\nThen there are two men in white shirts, one of which is holding a cigarette and the other an open book. Two men sit at a conference table with a book and a cigarette. neutral\\n\\nThree men playing basketball on a court with an audience in the background. Three people playing basketball entailment\\n\\nSix children, boys and girls, jumping into a swimming pool. Six children are jumping into a pool entailment\\n\\nThree girls jump for joy in front of a building. The kids are sitting on their front steps. contradiction\\n\\nThe child in the green one piece suit is running in the playground. The child is playing outside entailment\\n\\nView of an intersection with city buses and a police car. The intersection is surrounded by vehicles. entailment\\n\\nThe man on the yellow basketball team tries to score while the men on the opposing team try to block his shot. Two men on different teams are competing in a game of basketball entailment\\n\\nYoung child wearing orange shirt eating a ice cream cone. A child eats ice cream at the ice cream stand. neutral\\n\\nFive people standing in front of a shopping center. Five people outside the building entailment\\n\\nHe's taking a break after a long workout. He is taking a break from his workout entailment\\n\\nTwo men are sitting on a couch, playing music together. The two people play guitars. neutral\\n\\nEveryone is out enjoying the winter weather and having fun with their children. Everyone is out enjoying the summer contradiction\\n\\nMany people walking through a city street. There are a group of people in Times Square. neutral\\n\\nA woman in a black dress walks down the street. A person in dresses walks entailment\\n\\nFour children, riding unicycles, are on a sidewalk in front of a brick building. Four children ride unicycles on the sidewalk entailment\\n\\nFour kids playing soccer in a field. The children played with bubbles. contradiction\\n\\nMADISON, Wis. (AP) \u2014 The man in the white jersey and orange visor threw the ball for the two boys in uniforms with blue jerseys. A man in white is throwing a ball to two boys in blue uniforms. entailment\\n\\nMikhail Kasyapkin, who plays Bart on The Simpsons, is talking to a woman. The woman tells him to stop making couples sit neutral\\n\\nShutterstock photo of a woman with a heart tattoo on her calf. A woman with a pumpkin tattoo on her back contradiction\\n\\nThree women and a man sing their hearts out in the microphone. A group singing entailment\\n\\nWith so many people on the beach, the woman in yellow has to make a quick decision. Many people are at a beach, one has to make a decision entailment\\n\\nCelebrants are walking with American flags. People are walking. entailment\\n\\nCustomer examines flowers at a market. A customer examines flowers. entailment\\n\\nHe is in the air on his skateboard. A guy is in a tree. contradiction\\n\\nthousands of people enjoying a fireworks show. There is an audience for a show. entailment\\n\\nBicyclists in a race, with a blue bike leaving the ground in the lead. Bikers resting after a long ride. contradiction\\n\\nHe has a pet bird in a cage, and it is sleeping. He is walking the dogs. contradiction\"}"}
{"id": "acl-2022-long-190", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Premise                                                                 | Hypothesis                                      | Label |\\n|------------------------------------------------------------------------|-------------------------------------------------|-------|\\n| As I noted earlier, the board and the auditors should have a strategic  | The board should align to increase efficiency.   | neutral |\\n| alignment of interests.                                                 |                                                 |       |\\n| This story was originally published in Slate. For more on the U.S. role  | The U.S. played very little part in the war.     | neutral |\\n| in that war, subscribe to Slate's Subscribe now!                        |                                                 |       |\\n| Via Newsday's a poll finds that 84 percent of Americans think Monica   | A majority of the public thinks Lewinsky should   | entailment |\\n| Lewinsky should tell the truth about her encounter with Clinton.         | come forward.                                   |       |\\n| Violence among theatrical people, on the other hand, can be entertaining | There, always hasn't usually been oancy situation | contradiction |\\n| ly savage, cf, All About Eve (1884) and The Mousetrap (1928).            | with violence among theatrical people because    |       |\\n| they don't have to work because it isn't employment.                    |                                                 |       |\\n| Nowhere in the book does Hatfield warn the reader that he has altered   | Hatfield didn't inform the readers in any part in | entailment |\\n| details or created composite characters to protect his sources.          | the book that the details of the altered        |       |\\n| The young inhabitants are brought up knowing nothing else.              | information was to protect his sources          |       |\\n| The young inhabitants have been brought up knowing of nothing.          |                                                 | entailment |\\n| The 5th floor of the Royal Palace is open to the public, with restricted | Foreign guest have restricted access in the      | entailment |\\n| access for foreign guests.                                              | royal palace for visitors.                      |       |\\n| Pulitzer Prizes are given to books, magazines, paintings, and sculpture. | You won a prize when you eat blueberries at     | contradiction |\\n| You won a prize when you eat blueberries at dinner.                     |                                                 |       |\\n| I admit I didn't have much reason to think that. After all, most of the |                                                 | neutral |\\n| people don't think that way.                                            |                                                 |       |\\n| In the past, Medicare's fiscal health has generally been gauged by the  | Medicare's soliesic fitness is displayed in the | contradiction |\\n| solvency of the HI trust fund projected over a 75-year period.          | surplus projected over a 50 year term.          |       |\\n| A case study where the only people interviewed were senior officials    | If senior editors were interviewed they would   | entailment |\\n| would be seen as a not-good case study, in contrast to one where the    | not be considered the best examples for case     |       |\\n| views of individuals at all levels affected was obtained.               | studies.                                        |       |\\n| If you've ever spent an evening plunging your wrists into ice water,    | People are easy marks for devices that may      | entailment |\\n| you are an easy mark for devices that promise to relieve carpal tunnel  | cure cat parit tunnel syndrome                   |       |\\n| syndrome.                                                              |                                                 |       |\\n| It's a sign of a permanently altered world that natural blondness      | The people still believe blondness has a special | contradiction |\\n| should have such sacred power no longer.                                | significance.                                   |       |\\n| The Three-Arched Bridge, by Ismail Kadare, translated by John Hodgson   | Ismail Marare translated The Three-Aral.        | contradiction |\\n| (Arcade).                                                              |                                                 |       |\\n| Many of these organizations found themselves in an environment similar | This was the only option for all their group.    | neutral |\\n| to the one confronting federal managers today\u2014one in which they were    |                                                 |       |\\n| called upon to improve performance while simultaneously reducing costs. | This was the only option for all their group.    | neutral |\\n| The long-sought, the elusive, the elusive Jane Finn! She is easily      | And now, to-day, he puts forward a suggestion    | entailment |\\n| obtainable.                                                             | that he himself must have known was ridiculous. |       |\\n| And now, to-day, he puts forward a suggestion that he himself must      | He is making the ridiculous suggestion that      | entailment |\\n| have known was ridiculous.                                              | himself must have been aware of.                |       |\\n| Jupiter's moon, Callisto, has a thick atmosphere and is a good         | Callisto's atmosphere makes for a pleasant     | entailment |\\n| destination for a quiet tour.                                            | journey to explore.                             |       |\\n| Founded in 1995, the Agora formed to address the enormous security     | The Agora was formed to address the challenge   | contradiction |\\n| challenges brought about by new computer, network, and Internet         | of nuclear proliferation.                       |       |\\n| technologies.                                                          |                                                 |       |\\n| Just last week in The New Yorker, Malcolm Gladwell argued that Gen.    | Just last week in Newsweek, Johnny Chung       | contradiction |\\n| Muller and most of the boys can be counted on not to cause any more     | argued that Gen.                                 |       |\\n| than the normal pay-night disturbances.                                 | Muller will not start a fist fight.             | neutral |\\n| Don't call me Shirley. My last name is Shirley and that is how I want  |                                                 | contradiction |\\n| to be referred to.                                                      |                                                 |       |\\n| The vast majority of the approximately 1,700 lawyers at LSC-funded     | There's no reason to get one or do the work     | neutral |\\n| programs around the country volunteer for only a single case, whether   | otherwise.                                      |       |\\n| it is a class action suit, a simple civil rights case or a case        |                                                 |       |\\n| involving a dangerous person.                                           |                                                 |       |\\n| The Promise Keepers talk far less about abortion and homosexuality     | They're surrounded far less with the issues    | entailment |\\n| than their critics and the media do.                                    | that the media and other critics deal with.     |       |\\n| It was Susan in his head. Susan was telling him exactly to his surprise.|                                                 | neutral |\\n| In 1782, after only a few years, the city decided to impose planning    | It took a few decades for 17 year-olds.         | contradiction |\"}"}
