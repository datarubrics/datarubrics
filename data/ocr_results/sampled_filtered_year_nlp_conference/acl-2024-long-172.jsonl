{"id": "acl-2024-long-172", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The following is an abstract.\\n\\nPlease enter the number of the paragraph that the abstract is from. The answer format must be like \\\"Paragraph 1\\\", \\\"Paragraph 2\\\", etc.\\n\\nThe answer is: PassageRetrieval-zh:\\n\\n\u4ee5\u4e0b \u662f \u82e5 \u5e72 \u6bb5 \u843d \u6587 \u5b57\uff0c \u4ee5 \u53ca \u5176 \u4e2d \u4e00 \u6bb5 \u843d \u7684 \u6458 \u8981\u3002 \u8bf7 \u786e \u5b9a \u7ed9 \u5b9a \u7684 \u6458 \u8981 \u51fa \u81ea \u54ea \u4e00 \u6bb5\u3002\\n\\nNext line of code:\\n\\nRepoBench-P: Please complete the code given below.\\n\\nMaximum Output Length. We set a maximum output length on each dataset during evaluation to prevent the models from non-stop generation.\\n\\nAmong the 6 major tasks, summarization and code completion tend not to be sufficiently discerning. This may be due to the fact that the similarity-based metrics (ROUGE-L, Edit Sim) on these tasks are\"}"}
{"id": "acl-2024-long-172", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"not sensitive enough to well distinguish between the strong and weak models. Meanwhile, we find that synthetic tasks tend to offer a higher level of discernment, where models either achieve a high score or display a near-zero performance. These findings lead us to believe that it may not be a good idea to simply obtain an average over all tasks as the sign of models' long context capability, as used in previous benchmark (Shaham et al., 2023) \u2014 since the performance on the more discerning tasks, such as the synthetic tasks in our benchmark, may dominate the final rank. This necessitates an evaluation strategy like we use in LongBench that separately assesses each task category, potentially leading to more meaningful benchmarking results.\\n\\nD Analysis on the Inter-task Correlation on LongBench\\n\\nWe analyze the multi-task property of LongBench by the inter-task correlation among and across each category of tasks, as shown in Figure 5. We observe that most tasks within the same task category have a high correlation, except for PassageCount (5-1), which exhibits low correlation with almost all tasks since models perform poorly (almost random) on this challenging task. Meanwhile, we notice that the correlations between Qasper (1-2), RepoBench-P (6-2) and the other tasks are also lower, which implies that these tasks potentially require a different attention pattern than the other tasks. Notably, tasks in the same language have a higher correlation with each other, e.g., a high correlation between Chinese tasks (1-4, 2-4, 3-4, 4-4, 5-3). These observations suggest that LongBench provides a more comprehensive evaluation result by integrating various types of tasks and languages.\\n\\nE Full results on LongBench-E\\n\\nWe show the full results on LongBench-E in Table 9.\"}"}
{"id": "acl-2024-long-172", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Length | Avg | S-Doc QA | M-Doc QA | Summ | Few-shot | Synthetic Code |\\n|--------------|-----|---------|----------|------|----------|----------------|\\n| 1-2 1-3      |     |         |          |      |          |                |\\n| 2-1 2-2      |     |         |          |      |          |                |\\n| 3-1 3-3      |     |         |          |      |          |                |\\n| 4-1 4-2 4-3  |     |         |          |      |          |                |\\n| 5-1 5-2      |     |         |          |      |          |                |\\n| 6-1 6-2      |     |         |          |      |          |                |\\n\\n| Model | Length | Avg | S-Doc QA | M-Doc QA | Summ | Few-shot | Synthetic Code |\\n|-------|--------|-----|---------|----------|------|----------|----------------|\\n| GPT-3.5-Turbo-16k | 0-4k | 51.5 | 45.8 | 57.4 | 64.6 | 49.8 |\\n|       | 4-8k | 47.4 | 41.1 | 43.0 | 53.0 | 45.1 |\\n|       | 8k+  | 42.4 | 27.9 | 61.8 | 50.9 | 23.6 |\\n| Llama2-7B-chat-4k | 0-4k | 35.9 | 20.9 | 43.5 | 36.8 | 33.3 |\\n|       | 4-8k | 30.5 | 18.0 | 31.5 | 29.2 | 22.5 |\\n|       | 8k+  | 29.6 | 21.1 | 31.1 | 24.4 | 21.5 |\\n| LongChat-v1.5-7B-32k | 0-4k | 36.9 | 28.4 | 44.1 | 30.8 | 26.0 |\\n|       | 4-8k | 35.3 | 27.5 | 37.5 | 34.6 | 18.8 |\\n|       | 8k+  | 34.5 | 14.0 | 48.6 | 25.2 | 19.1 |\\n| XGen-7B-8k | 0-4k | 32.6 | 19.4 | 49.9 | 34.0 | 21.9 |\\n|       | 4-8k | 27.5 | 17.9 | 27.5 | 23.5 | 19.4 |\\n|       | 8k+  | 27.4 | 16.7 | 29.6 | 26.2 | 13.6 |\\n| InternLM-7B-8k | 0-4k | 30.4 | 19.7 | 32.0 | 43.3 | 24.4 |\\n|       | 4-8k | 23.0 | 13.7 | 16.5 | 17.5 | 28.6 |\\n|       | 8k+  | 23.2 | 26.2 | 16.0 | 24.9 | 15.0 |\\n| ChatGLM2-6B | 0-4k | 33.1 | 19.6 | 45.5 | 27.8 | 31.3 |\\n|       | 4-8k | 28.5 | 21.1 | 28.0 | 19.2 | 24.6 |\\n|       | 8k+  | 25.5 | 16.0 | 19.4 | 21.7 | 15.8 |\\n| ChatGLM2-6B-32k | 0-4k | 44.2 | 33.9 | 45.0 | 47.5 | 39.9 |\\n|       | 4-8k | 43.4 | 33.4 | 44.8 | 45.2 | 38.0 |\\n|       | 8k+  | 43.1 | 23.4 | 57.4 | 42.2 | 26.4 |\\n| Vicuna-v1.5-7B-16k | 0-4k | 37.3 | 29.2 | 46.4 | 38.2 | 30.8 |\\n|       | 4-8k | 32.3 | 20.1 | 32.9 | 23.9 | 17.4 |\\n|       | 8k+  | 29.6 | 21.8 | 28.1 | 19.7 | 12.3 |\\n| Table 9: Results (%) on LongBench-E. |  |  |  |  |  |  |\"}"}
{"id": "acl-2024-long-172", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAlthough large language models (LLMs) demonstrate impressive performance for many language tasks, most of them can only handle texts a few thousand tokens long, limiting their applications on longer sequence inputs, such as books, reports, and codebases. Recent works have proposed methods to improve LLMs' long context capabilities by extending context windows and more sophisticated memory mechanisms. However, comprehensive benchmarks tailored for evaluating long context understanding are lacking. In this paper, we introduce LongBench, the first bilingual, multi-task benchmark for long context understanding, enabling a more rigorous evaluation of long context understanding. LongBench comprises 21 datasets across 6 task categories in both English and Chinese, with an average length of 6,711 words (English) and 13,386 characters (Chinese). These tasks cover key long-text application areas including single-doc QA, multi-doc QA, summarization, few-shot learning, synthetic tasks, and code completion. All datasets in LongBench are standardized into a unified format, allowing for effortless automatic evaluation of LLMs. Upon comprehensive evaluation of 8 LLMs on LongBench, we find that: (1) Commercial model (GPT-3.5-Turbo-16k) outperforms other open-sourced models, but still struggles on longer contexts. (2) Scaled position embedding and fine-tuning on longer sequences lead to substantial improvement on long context understanding. (3) Context compression technique such as retrieval brings improvement for model with weak ability on long contexts, but the performance still lags behind models that have strong long context understanding capability.\\n\\n1 Introduction\\n\\nThe field of NLP has long sought to endow machines with the ability to understand and reason over a long context. Tasks such as summarization and question answering based on books, reports, and documents, and code generation at the repository level demand the ability to model long context sequences that span thousands or even tens of thousands of tokens in length. However, many of today's large language models can only comprehend and generate texts a few thousand tokens long, leaving room for potential improvements in processing longer contexts. More recently, there has been an increasing effort to improve large language models' capabilities on long context understanding. These methods include extending the context window (Press et al., 2022; Chen et al., 2023), utilizing recurrent memory (Dai et al., 2019; Bulatov et al., 2023), using sparsed attention (Ding et al., 2023; Mohtashami and Jaggi, 2023), and augmenting with an external memory (Liang et al., 2023; Zhou et al., 2023). However, unlike in short context, where a multitude of multi-task benchmarks are available for a multi-aspect evaluation (Hendrycks et al., 2021; Srivastava et al., 2023), there is no such benchmark on longer context.\\n\\nTo facilitate further research in this direction, we propose LongBench, the first bilingual, multi-task benchmark tailored for long context understanding. LongBench is composed of 6 major task categories and 21 different tasks, covering key long-text application scenarios including multi-document QA, single-document QA, summarization, few-shot learning, code completion, and synthetic tasks. In addition, LongBench includes different languages (Chinese and English) to provide a more comprehensive evaluation of the large models' bilingual capabilities on long contexts. With overview statistics shown in Figure 1, LongBench contains 4,750 test instances, with an average length of 6,711 words and 13,386 characters for English and Chinese instances, respectively. All 21 datasets in LongBench are standardized into a unified format, among which 6 are directly...\"}"}
{"id": "acl-2024-long-172", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Left: Number of data in each type of task within LongBench. Right: Length distribution for English and Chinese data in LongBench, measured by the number of words and characters.\\n\\nextracted from the original datasets provided by previous studies, 10 are built based on the original datasets and processed to be suitable for long context evaluation, and 5 are created and annotated by us. We are fully aware of the potentially high costs involved in the model evaluation process, especially in the context of long context scenarios (such as manual annotation costs or API call costs). Therefore, we adopt a fully automated evaluation method, where we utilize automatic metrics such as ROUGE-L and F1 to measure the similarity of the outputs to the groundtruth answers.\\n\\nWe conduct a comprehensive evaluation of 8 models on LongBench. The empirical results provide insightful conclusions about the multi-task capability of current models in terms of long context comprehension. Additionally, to better disentangle the models\u2019 long context ability from their task ability, we construct LongBench-E that features a more even length distribution, thus suitable for gauging each model\u2019s capability across various context lengths. Results on LongBench-E reveal that although some models are trained or fine-tuned on longer contexts, they still experience a significant decline in performance as the context length increases. We also investigate the effect of retrieval-based and summarization-based context compression techniques. Our results demonstrate that these approaches are beneficial only to models that exhibit weaker capability on long contexts.\\n\\n2 Related Work\\n\\nLong Context Modeling Techniques. We first discuss some popular lines of methods that aim to tackle long context understanding. These studies are mainly aimed at solving two key challenges in long text modeling, including the high runtime overhead on longer context, and the catastrophic forgetting phenomenon when processing long sequence. A series of studies focus on how to make Transformers more efficient and unforgetful (Tay et al., 2022), with designs such as sparse and efficient computation (Child et al., 2019; Kitaev et al., 2020; Beltagy et al., 2020; Zaheer et al., 2020; Wang et al., 2020; Fedus et al., 2022; Ding et al., 2023), recurrent and memory modules (Dai et al., 2019; Rae et al., 2020; Wu et al., 2022; Martins et al., 2022; Bulatov et al., 2022; Orvieto et al., 2023; Liang et al., 2023; Zhou et al., 2023). More recently, several methods (Press et al., 2022; Sun et al., 2022; Chen et al., 2023) have been proposed to enable length extrapolation of Transformers, and have been adopted in the training process of long context LLMs such as ChatGLM2-32k (Zeng et al., 2023) and LongChat-32k (Li et al., 2023).\\n\\nEvaluation for Long Context Understanding. Many previous works on long text modeling rely on the perplexity metric for evaluation (Beltagy et al., 2020; Roy et al., 2021; Press et al., 2022). However, as suggested in (Sun et al., 2021), the perplexity metric may not necessarily reflect the model\u2019s performance on sequence-level tasks in real applications. Meanwhile, some works assess long text modeling through artificial tasks such as retrieval (Tay et al., 2021; Chen et al., 2023; Li et al., 2023), which may also fall short in mirroring real-world scenarios.\\n\\nConcurrently, ZeroSCROLLS (Shaham et al., 2022, 2023) and L-Eval (An et al., 2023) are proposed as evaluation benchmarks for long text modeling. Nonetheless, they encompass a restricted range of task types, thereby limiting the diversity of long text modeling patterns required in the benchmark.\"}"}
{"id": "acl-2024-long-172", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"marks, and consequently, the comprehensiveness of the evaluation results. Recently, AgentBench (Liu et al., 2023c) also mentions the challenge of LLM-as-Agent's handling long interaction trajectories, but fails to incorporate it as a dedicated evaluation dimension. In contrast, LongBench includes six major task categories, with each category featuring sequences of varying lengths, languages, and domains. We believe it provides a more holistic evaluation of long text modeling ability of large language models across a spectrum of lengths, distributions, as well as long dependency patterns.\\n\\n3 LongBench: Task and Construction\\n\\n3.1 Problem Definition\\n\\nWe formalize the problem of long context understanding as follows: Given the input and context sequences \\\\((I, C)\\\\), the model is expected to output the answer \\\\(A\\\\). For instance, in a QA task, the input \\\\(I\\\\) would be the question, context \\\\(C\\\\) refers to the document, and \\\\(A\\\\) denotes the answer to the question. Generally, in LongBench, \\\\(I\\\\) and \\\\(A\\\\) tend to be short, while \\\\(C\\\\) represents a long sequence up to thousands of tokens in length. The instantiation of \\\\((I, C, A)\\\\) for each task is listed in Table 7.\\n\\n3.2 Dataset Construction\\n\\nIn this section, we will provide a detailed introduction to the data collection, annotation, and organization process for each dataset in LongBench (LongBench-E), according to the specific tasks. For the overall data statistics of LongBench, we refer to Table 1.\\n\\n3.2.1 Data Collection and Annotation\\n\\n**Single-Doc QA.** For single-document QA, we focus on instances with longer documents. We extract NarrativeQA from the original dataset in Ko\u02c7cisk`y et al. (2018), which consists of long stories along with questions posed to test reading comprehension. We also sample from Qasper (Dasigi et al., 2021), which features QA over NLP papers and is annotated by NLP practitioners. To better test the model's long context understanding ability across diverse fields, we manually curate the MultiFieldQA datasets in both English and Chinese. We first collect documents and articles from multiple sources, including legal documents, government reports, encyclopedias, academic papers, etc (sources detailed in Appendix). We invite three Ph.D. students to annotate the question and answer for each article, with definitive answers as much as possible for ease of automated evaluation. During the annotation, we ensure that the answers can be inferred from the documents, and the placement of evidence is fairly random to avoid biases that might occur if, for instance, the answer-related statements are frequently found at the beginning or the end, as mentioned in (Liu et al., 2023a).\\n\\n**Multi-Doc QA.** Multi-document QA requires models to extract and combine information from several documents to obtain the answer, which is usually more challenging than single-doc QA. The English test samples are built from three Wikipedia-based multi-hop QA datasets: HotpotQA (Yang et al., 2018), 2WikiMultihopQA (Ho et al., 2020), and MuSiQue (Trivedi et al., 2022). HotpotQA involves a number of 2-hop questions directly written by native speakers given two related paragraphs. 2WikiMultihopQA consists of up to 5-hop questions that are synthesized through manually designed templates to ensure that they cannot be solved through shortcuts. The questions in MuSiQue are carefully composed from simple questions involving up to 4-hop reasoning, and are then paraphrased by annotators to both avoid shortcuts and ensure linguistic naturalness. Each question in the original datasets is supplemented by 2-4 supporting paragraphs that provide one-step reasoning evidence and several distracting paragraphs.\\n\\nTo tailor the data for long-context evaluation, we utilize complete Wikipedia passages encompassing the supporting or distracting paragraphs as the context. Initially, supporting passages are included within the context, and then as many distracting passages are added until the total length reaches a maximum length. Finally, these passages are randomly ordered to form the multi-document context. Beyond these three English datasets, we also construct a Chinese dataset based on DuReader (He et al., 2018), which is developed based on Baidu Search and Baidu Zhidao, comprising 200K questions and 1M related documents. To adapt it for assessing long context ability, for each question, we not only provide several documents related to the question but also arbitrarily select several from the total set of documents as distractors, until each question is associated with 20 documents.\\n\\n**Summarization.** Compared to QA tasks, which can often be solved using local information within the context, summarization demands a more global approach.\"}"}
{"id": "acl-2024-long-172", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset ID       | Source            | Avg len | Metric | Language | #data |\\n|-----------------|-------------------|---------|--------|----------|-------|\\n| Single-Document QA |                  |         |        |          |       |\\n| NarrativeQA     | Literature, Film  | 18,409  | F1     | English  | 200   |\\n| Qasper          | Science           | 3,619   | F1     | English  | 200   |\\n| MultiFieldQA-en | Multi-field       | 4,559   | F1     | English  | 150   |\\n| MultiFieldQA-zh |                  | 6,701   | F1     | Chinese  | 200   |\\n| Multi-Document QA |                  |         |        |          |       |\\n| HotpotQA        | Wikipedia         | 9,151   | F1     | English  | 200   |\\n| 2WikiMultihopQA | Wikipedia         | 4,887   | F1     | English  | 200   |\\n| MuSiQue         | Wikipedia         | 11,214  | F1     | English  | 200   |\\n| DuReader        | Baidu Search      | 15,768  | Rouge-L| Chinese  | 200   |\\n| Summarization   |                   |         |        |          |       |\\n| GovReport       | Government report | 8,734   | Rouge-L| English  | 200   |\\n| QMSum           | Meeting           | 10,614  | Rouge-L| English  | 200   |\\n| MultiNews       | News              | 2,113   | Rouge-L| English  | 200   |\\n| VCSUM           | Meeting           | 15,380  | Rouge-L| Chinese  | 200   |\\n| Few-shot Learning |                  |         |        |          |       |\\n| TREC            | Web question      | 5,177   | Accuracy (CLS) | English | 200   |\\n| TriviaQA        | Wikipedia, Web    | 8,209   | F1     | English  | 200   |\\n| SAMSum          | Dialogue          | 6,258   | Rouge-L| English  | 200   |\\n| LSHT            | News              | 22,337  | Accuracy (CLS) | Chinese | 200   |\\n| Synthetic Task  |                   |         |        |          |       |\\n| PassageCount    | Wikipedia         | 11,141  | Accuracy (EM) | English | 200   |\\n| PassageRetrieval-en | Wikipedia       | 9,289   | Accuracy (EM) | English | 200   |\\n| PassageRetrieval-zh | C4 Dataset       | 6,745   | Accuracy (EM) | Chinese | 200   |\\n| Code Completion |                   |         |        |          |       |\\n| LCC             | Github            | 1,235   | Edit Sim | Python/C#/Java | 500   |\\n| RepoBench-P     | Github repository | 4,206   | Edit Sim | Python/Java | 500   |\\n\\nTable 1: An overview of the dataset statistics in LongBench. Chinese datasets are highlighted. 'Source' denotes the origin of the context. 'Avg len' (average length) is computed using the number of words for the English (code) datasets and the number of characters for the Chinese datasets. 'Accuracy (CLS)' refers to classification accuracy, while 'Accuracy (EM)' refers to exact match accuracy.\\n\\nWe extract GovReport from the original dataset (Huang et al., 2021). The original GovReport dataset is a large-scale collection of detailed reports from the U.S. Government Accountability Office and Congressional Research Service, each accompanied by a human-written summary, spanning a wide variety of national policy issues. We also sampled from QMSum (Zhong et al., 2021), which consists of query-summary pairs annotated over 232 meetings across multiple domains, including product, academic, and committee meetings. We treat the query as input $I$, the meeting content as context $C$, and the summary as answer $A$.\\n\\nMultiNews is derived from the original multi-document summarization dataset in (Fabbri et al., 2019). The MultiNews dataset features clusters of 2-10 news articles discussing the same event or topic, each paired with a human-written summary that summarizes the key information from the multiple source articles. In LongBench, we include \u201cDocument $i$\u201d before the $i$th news article, and concatenate them into the context $C$.\\n\\nVCSUM (Wu et al., 2023) is a large-scale Chinese meeting summarization dataset consisting of 239 real-life meetings with over 230 hours of duration, with versatile annotations to support multiple summarization tasks. In LongBench, we select the long segments from VCSUM to compose our evaluation samples.\\n\\nFew-shot Learning. We identify few-shot in-context learning as a practical setting that requires long context understanding, especially when the number of examples increases (Ainslie et al., 2023). To ensure the diversity of the tasks, we incorporate classification, summarization, and reading comprehension tasks within the few-shot learning scenario. We include 2 classification datasets with fine-grained class labels, including TREC (Li...\"}"}
{"id": "acl-2024-long-172", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Roth, 2002), a question classification task involving 50 fine classes, and LSHT (NLPCC, 2014), a Chinese news classification task with 24 classes. For summarization task, we use the SAMSum dataset (Gliwa et al., 2019), which contains messenger-like conversations with human-annotated summaries. TriviaQA (Joshi et al., 2017) contains question-answer pairs labeled with evidence passages, and we use it as a reading comprehension task. We filter the passages in TriviaQA with less than 1,000 words to be examples.\\n\\nIn each of the above datasets adapted for LongBench, for each test data, we first randomly select an integer within a range as the number of examples, then randomly sample the corresponding number of samples from the training set, and concatenate them to form the context $C$. For TREC, LSHT, SAMSum, and TriviaQA, the ranges are $[100, 600]$, $[10, 40]$, $[10, 100]$, $[2, 24]$, respectively.\\n\\nSynthetic Task. Unlike standard tasks that are more alike on the required long dependency pattern, synthetic tasks can be meticulously designed to test the model\u2019s ability on specific scenarios and patterns. In LongBench, we design three synthetic tasks. PassageRetrieval-en and PassageRetrieval-zh are constructed based on English Wikipedia and the Chinese sections of the C4 dataset (Raffel et al., 2020). For each data entry, we randomly sample 30 passages and select one of them for summarization using GPT-3.5-Turbo. The task asks the model to identify the original paragraph to which the crafted summary corresponds.\\n\\nPassageCount seeks to create a more demanding situation where the model is required to utilize the full context to resolve the task. For each piece of data, we randomly select several passages from English Wikipedia, repeat each paragraph at random several times, and finally shuffle the paragraphs. The task asks the model to determine the number of unique passages among the given set. Specifically, we randomly select $M$ from $[17, 50]$ as the upper limit for the number of passages. Subsequently, the number $N$ of unique passages is randomly selected from the range $[2, M]$. We conduct random sampling with replacement from the set of $N$ unique passages to get the final $M$ passages.\\n\\nCode Completion. Code completion is a critical task employed by auto-completion systems to assist users by completing code based on previous code input and context (Chen et al., 2021; Zheng et al., 2023b). This task can pose a significant challenge for models, especially when dealing with lengthy code inputs or even repository-level data. This is mainly because the models need to establish attention across long-range sequences according to relationships within code elements, such as between class and function definitions. Hence we recognize this as a suitable task for evaluating a model\u2019s long context modeling ability.\\n\\nThe LCC dataset is sampled from the original Long Code Completion dataset (Guo et al., 2023). The original dataset is constructed by filtering code within one file from GitHub based on length. This data includes a long piece of preceding lines of code as context, and the next line of code as the answer. We also consider the repository-level code completion setting, which necessitates aggregating information from code across files. For this task, we adapt the RepoBench-P dataset from (Liu et al., 2023b). RepoBench-P is collected from Github repositories, and is constructed by first retrieving relevant code snippets from other files based on module import statements. These snippets are then concatenated with the preceding lines of code within the current file as context, and are used to predict the next line of code. We select the most challenging XF-F (Cross-File-First) setting from the original dataset, where the in-file context gives no prior usage of the module to aid the prediction. For each original piece of data, we shuffle the cross-file code snippets that include the gold cross-file code snippet (manually annotated as the optimal context for prediction), and combine them into context $C$. The preceding lines of code are taken as input $I$, and the next line of code as the answer $A$.\\n\\n3.2.2 Data Extraction. Since LLMs may have already been trained on the training set of some of our collected public datasets, to avoid test leakage, we extract data from the test sets of these public datasets, with the exception of VCSUM due to its insufficient data in its test set. We employ two extraction strategies: random sampling and uniform sampling. Through random sampling, we maintain a natural length distribution to more accurately mimic real scenarios, and obtain LongBench. Alternatively, we perform uniform sampling based on the length of the data with a focus on studying the model\u2019s capabilities across varying context lengths within each task itself. This approach provides insights into the model\u2019s true ability to understand long contexts independent of task capability. We\"}"}
{"id": "acl-2024-long-172", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                      | Single-Doc QA | Multi-Doc QA | Summarization |\\n|---------------------------|---------------|--------------|---------------|\\n|                           | 1-1 1-2 1-3 1-4 Avg | 2-1 2-2 2-3 2-4 Avg | 3-1 3-2 3-3 3-4 Avg |\\n| GPT-3.5-Turbo-16k         | 23.6 43.3 52.3 61.2 45.1 | 51.6 37.7 26.9 28.7 36.2 | 29.5 23.4 26.7 16.0 23.9 |\\n| Llama2-7B-chat-4k         | 18.7           | 19.2          | 36.8           |\\n| LongChat-v1.5-7B-32k      | 16.9 27.7 41.4 29.1 28.8 | 31.5 20.6 9.7 19.5 20.3 |\\n| XGen-7B-8k                | 18.0           | 18.1          | 37.7           |\\n| InternLM-7B-8k            | 12.1 16.7 23.4 33.6 21.4 | 28.7 22.8 9.0 11.1 17.9 |\\n| ChatGLM2-6B               | 11.8           | 22.5          | 35.0           |\\n| ChatGLM2-6B-32k           | 21.1 31.5 46.2 51.6 37.6 | 45.1 34.0 21.9 37.6 34.7 |\\n| Vicuna-v1.5-7B-16k        | 19.4           | 26.1          | 38.5           |\\n\\nTable 2: Results (%) on single-doc QA, multi-doc QA and summarization tasks.\\n\\n| Model                      | Few-shot Learning | Synthetic Code | Overall |\\n|---------------------------|-------------------|----------------|---------|\\n|                           | 4-1 4-2 4-3 4-4 Avg | 5-1 5-2 5-3 Avg | 6-1 6-2 Avg |\\n| GPT-3.5-Turbo-16k         | 68.0 91.4 41.7 29.2 57.6 | 4.5 71.0 77.5 51.0 | 54.7 53.6 54.1 |\\n| Llama2-7B-chat-4k         | 61.5              | 77.8          | 40.7 19.8 49.9 2.1 9.8 0.5 4.1 52.4 43.8 48.1 31.0 14.3 26.8 |\\n| LongChat-v1.5-7B-32k      | 63.5 82.3 34.2 23.2 50.8 | 1.0 30.5 7.6 13.0 | 53.0 55.3 54.1 |\\n| XGen-7B-8k                | 65.5              | 77.8          | 25.3 20.5 47.3 2.1 8.5 3.5 4.7 38.6 38.6 38.6 28.3 15.1 25.0 |\\n| InternLM-7B-8k            | 52.0 77.8 21.2 15.2 41.6 | 3.0 6.0 0.9 3.3 | 44.1 28.8 36.4 24.2 18.3 22.6 |\\n| ChatGLM2-6B               | 44.5              | 70.6          | 29.5 20.8 41.3 2.5 3.0 6.5 4.0 49.0 43.2 46.1 26.6 22.9 25.7 |\\n| ChatGLM2-6B-32k           | 62.5 78.7 36.3 27.7 51.3 | 1.5 77.0 64.5 47.7 | 55.6 49.9 52.7 |\\n| Vicuna-v1.5-7B-16k        | 71.5              | 86.2          | 40.8 28.8 56.8 6.5 4.5 5.0 5.3 51.0 43.5 47.3 31.9 26.4 30.5 |\\n\\nTable 3: Results (%) on few-shot learning, synthetic, and code tasks. 'Overall' is computed by the macro-average (the mean of 'Avg') over major task categories. This is computed on English (EN) tasks, Chinese (ZH) tasks, and all (All) tasks, code tasks are included in both languages.\\n\\nChoose 13 of the English datasets, including Qasper, MultiFieldQA-en, HotpotQA, 2WikiMultihopQA, GovReport, Multi-news, TREC, TriviaQA, SAM-Sum, PassageCount, PassageRetrieval-en, LCC, and RepoBench-P, which offer broader coverage on data length. During the uniform sampling process, we use word count as the length and sample a comparable quantity of data from the length ranges of 0-4k, 4k-8k, and 8k+. The resulting data is compiled into LongBench-E (statistics in Table 8).\\n\\n4 Experiments\\n4.1 Benchmarking Results on LongBench and LongBench-E\\n\\nExperiment Setup. We evaluate 8 popular LLMs that feature long context capability, including GPT-3.5-Turbo-16k (OpenAI, 2022a), Llama2-7B-chat-4k (Touvron et al., 2023), LongChat-v1.5-7B-32k (Li et al., 2023), XGen-7B-8k (Nijkamp et al., 2023), InternLM-7B-8k (Team, 2023), ChatGLM2-6B, ChatGLM2-6B-32k (Du et al., 2022; Zeng et al., 2023), and Vicuna-v1.5-7B-16k (Zheng et al., 2023a). ChatGLM2-6B-32k is trained based on ChatGLM2-6B, with a 32k context length during alignment and position interpolation (Chen et al., 2023). LongChat-v1.5-7B-32k and Vicuna-v1.5-7B-16k are fine-tuned from Llama2-7B, with supervised fine-tuning and linear RoPE scaling. We conduct the assessment in a zero-shot setting, except for the few-shot learning tasks where the few-shot examples are provided as part of the long context. The input format prompt and the maximum output length we used during evaluation can be found in Appendix. When the input length $L$ surpasses the maximum context length $M$ of a model (indicated by the suffix of its name), we truncate the input sequence $S$ from the middle since the front and end of the sequence may contain crucial information such as the instruction or question: $S_{1:L} \\\\rightarrow [S_{1:\\\\lfloor M/2 \\\\rfloor}; S_{L-\\\\lfloor M/2 \\\\rfloor-1:L}]$. During generation, we use greedy decoding for reproducibility. It's worth noting that the chat models evaluated typically have specific prompts that induce the models to generate dialogue-like responses. During evaluation, we avoid adding these prompts in few-shot learning and code completion tasks, since the answers to these tasks should be generated in a completion style rather than a chat style. The metric for each dataset is shown in Table 1. For tasks built based on previous datasets, the metrics we used are consistent with those used in the original work. F1 and ROUGE-L (Lin, 2004) are...\"}"}
{"id": "acl-2024-long-172", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two popular N-gram based metrics widely adopted in QA and summarization tasks. Edit Sim (Levenshtein distance) is popularly used in code generation evaluation (Svyatkovskiy et al., 2020). For the few-shot learning tasks, we extract the first line of the response. For the two code completion tasks, we extract the first line of model generation that is not comment. The code and datasets are available at https://github.com/THUDM/LongBench.\\n\\nResults on LongBench. Table 2, 3 report the performance (%) on all datasets in LongBench. Additionally, Figure 4 presents a radar plot depicting the models' abilities on the 6 major tasks. For better visualization, we scale the maximum score across all models on each task to 100 in the radar plot. We summarize our key findings from the experiment results: (1) There is still a performance gap on long context tasks between open-sourced models of smaller size and commercial model (GPT-3.5-Turbo-16k). (2) Models benefit from scaled positional embedding and continued training on longer context, as ChatGLM2-6B-32k and LongChat-v1.5-7B-32k obtain relative improvements of 62% and 19%, respectively. We further analyze the multi-task property of LongBench by the inter-task correlation among and across each category of tasks in Appendix D. We find higher correlations for performance on tasks of the same category or language.\\n\\nTo study whether the models with a longer maximum length truly benefit from utilizing longer context, we conduct experiments with GPT-Turbo-3.5-16k, ChatGLM2-6B-32k, and Vicuna-v1.5-7B-16k with truncation sizes of 4k and 8k on LongBench. The macro-average scores across all tasks with varying truncation sizes are depicted in Figure 2. Here, 'maximum length' denotes truncation at the model's maximum length configuration. We observe that GPT-Turbo-3.5-16k and ChatGLM2-6B-32k obtain higher scores under a larger truncation size, suggesting they can better make use of a longer context. Furthermore, this confirms that our benchmark indeed necessitates long context modeling \u2014 using truncated information alone is insufficient for successfully completing the tasks in LongBench. On the other hand, the performance of LLMs on LongBench can be further improved by enhancing their long context modeling capabilities.\\n\\nResults on LongBench-E. While LongBench facilitates the measurement of an overall multi-task ability on tasks that require long context understanding, LongBench-E focuses more on measuring how the model's performance changes under different context lengths within the same task. As introduced in Sec 3.2.2, LongBench-E contains a subset of datasets included in LongBench, featuring more evenly distributed context lengths. Figure 3 reports the macro-average scores (%) on data in length ranges of 0-4k, 4k-8k, and 8k+ (See the results on all datasets in Table 9). One can derive a model's long context ability from the slope of the curve \u2014 a significant drop in performance on data of greater length, as indicated by a steeper curve, points to the model's limitations in effectively handling long text modeling. From the results on LongBench-E, we observe that ChatGLM2-6B-32k and LongChat-v1.5-7B-32k are more robust to longer context length, with a relative drop of 4% and 7% from 0-4k to 8k+ respectively. Moreover, despite GPT-3.5-Turbo-16k demonstrating impressive overall performance across all tasks, we find that it still struggles on longer contexts (-17% from 0-4k to 8k+), leaving room for future development on long context modeling.\"}"}
{"id": "acl-2024-long-172", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Impact of Context Compression Techniques\\n\\nWe further explore the impact of context compression techniques on LongBench, including retrieval-based context compression and summarization-based context compression. Retrieval is widely used in augmenting language models with external memory (Khandelwal et al., 2020; Borgeaud et al., 2022; Izacard et al., 2022b). This application can be extended to consider longer contexts, such as documents or books, as forms of external memory, from which relevant information can be retrieved using a specific query. Given a long context, we first split it into chunks with a default size of \\\\( M \\\\) words (or characters on Chinese datasets), then use a specific retriever to compute the embedding of the text chunks and the query, and concatenate only the top-\\\\( N \\\\) chunks according to the cosine similarity of their embeddings to the query embedding. The top-\\\\( N \\\\) chunks as the compressed context, together with the query, are then fed into the model to produce an answer. A similar pipeline is also implemented in LangChain. We experiment with three retrievers \u2014 OpenAI Embedding (text-embedding-ada-002 (OpenAI, 2022b)), Contriever (Izacard et al., 2022a), and BM25 \u2014 alongside two chunk sizes of 200 and 500. In order to conduct a fairer comparison under the same context length, we take the top-7 and top-3 chunks respectively when the chunk sizes are 200 and 500. Table 4 reports the results on QA tasks in LongBench. We summarize our findings: (1) text-embedding-ada-002 performs the best among the three retrievers, while the open-sourced Contriever results are closer to text-embedding-ada-002 and superior to BM25. (2) In general, splitting the long context into shorter chunks and retrieving more chunks results in better performance. (3) Under the best retrieval method, the improvements for the three models are -2%, 21%, and -5%, respectively. Moreover, even after retrieval, the performance of Llama2-7B-chat-4k still lags behind the other two models. The results suggest that the retrieval technique can only serve as a performance compensation for models that cannot well model long context, and is not a shortcut to solving long context understanding tasks.\\n\\nWe also study the effect of using model-generated summary as a context compression technique. Specifically, we first utilize the model to generate a brief summary for each text chunk, and concatenate the summaries together as the compressed context. We experiment on the summarization tasks in LongBench, and the results are as shown in Table 5. We find that this compression method improves the performance of the models only on the VCSUM task (3-4), since the data in VCSUM are longer than in the other three datasets.\\n\\n4.3 Context Understanding or Memorization?\\n\\nSince the model may have potentially encountered the long context during pre-training, it may rely on memorization rather than context understanding to answer the question. We conduct an experiment to assess the degree to which these tasks rely on memorization rather than long context understanding capability. Specifically, we withhold the long context and evaluate the performance of the models on LongBench. The results indicate that the models perform worse when the long context is withheld, suggesting that they rely on memorization rather than context understanding to answer the question.\"}"}
{"id": "acl-2024-long-172", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Assessing context understanding vs. memorization.\\n\\nWe observe that the memorization performance (w/o context) on HotpotQA, 2WikiMultihopQA, and MusiQue is relatively high. This is likely because these datasets are derived from Wikipedia, a common source for training prevalent LLMs. The \u2206 score (original score minus the score when context is absent) addresses the memorization phenomenon (Yu et al., 2024), and could also serve as an important indicator for a model\u2019s long context understanding ability.\\n\\n5 Conclusion\\n\\nIn this paper, we introduce LongBench, a multi-task bilingual benchmark tailored for gauging long context understanding abilities of LLMs. LongBench covers six key categories and a total of 21 tasks, with data lengths extending from thousands of tokens up to tens of thousands of tokens. We also develop LongBench-E which features a more evenly data length distribution. We conduct extensive experiments on LongBench and LongBench-E, yielding insightful conclusions about the capabilities of current LLMs on long context understanding. Moreover, our analysis suggests that LongBench and LongBench-E serve as ideal testbeds for future research in long context modeling.\\n\\n6 Limitations\\n\\nDespite LongBench offers a more comprehensive testbed for long context understanding, it still has its shortcomings, as we summarized below. (1) Potentially unreliable automatic metrics: As previous studies suggest (Bai et al., 2023), the automatic evaluation metrics (ROUGE-L, F1) may not accurately reflect the quality of the response. Particularly, the results on these metrics may be underestimated for models that are used to generating longer responses. Although using LLM as examiner may reduce this problem (Bai et al., 2023; An et al., 2023), the runtime overhead for evaluation may be high, and LLM also has bias when used as an evaluation metric (Zheng et al., 2023a). (2) Coupling with instruction-following capabilities: Our primary objective is to assess the models\u2019 proficiency in long-context modeling regardless of their instruction-following capabilities. However, as the tasks in LongBench are closer to real-world applications, mastering them inevitably demands a certain level of instruction-following capability. Consequently, the performance on LongBench is coupled with the models\u2019 instruction-following capabilities.\\n\\nAcknowledgement\\n\\nThis work is supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003), Zhipu AI and Tsinghua University Initiative Scientific Research Program. We would also like to thank our anonymous reviewers for their suggestions.\\n\\nReferences\\n\\nJoshua Ainslie, Tao Lei, Michiel de Jong, Santiago Onta\u00f1\u00f3n, Siddhartha Brahma, Yury Zemlyanskiy, David Uthus, Mandy Guo, James Lee-Thorp, Yi Tay, et al. 2023. Colt5: Faster long-range transformers with conditional computation. arXiv preprint arXiv:2303.09752.\\n\\nChenxin An, Shansan Gong, Ming Zhong, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088.\\n\\nYushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, et al. 2023. Benchmarking foundation models with language-model-as-an-examiner. arXiv preprint arXiv:2306.04181.\\n\\nIz Beltagy, Matthew E Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150.\"}"}
{"id": "acl-2024-long-172", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-172", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. 2020. Reformer: The efficient transformer. In International Conference on Learning Representations.\\n\\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328.\\n\\nDacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph E. Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How long can open-source llms truly promise on context length?\\n\\nXin Li and Dan Roth. 2002. Learning question classifiers. In COLING 2002: The 19th International Conference on Computational Linguistics.\\n\\nXinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing infinite-length input capacity for large-scale language models with self-controlled memory system. arXiv preprint arXiv:2304.13343.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023a. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.\\n\\nTianyang Liu, Canwen Xu, and Julian McAuley. 2023b. Repobench: Benchmarking repository-level code auto-completion systems. arXiv preprint arXiv:2306.03091.\\n\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023c. Agent-bench: Evaluating llms as agents. arXiv preprint arXiv:2308.03688.\\n\\nPedro Henrique Martins, Zita Marinho, and Andr\u00e9 FT Martins. 2022. $\\\\infty$-former: Infinite memory transformer. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5468\u20135485.\\n\\nAmirkeivan Mohtashami and Martin Jaggi. 2023. Landmark attention: Random-access infinite context length for transformers. arXiv preprint arXiv:2305.16300.\\n\\nErik Nijkamp, Tian Xie, Hiroaki Hayashi, Bo Pang, Congying Xia, Chen Xing, Jesse Vig, Semih Yavuz, Philippe Laban, et al. 2023. Long sequence modeling with xgen: A 7b llm trained on 8k input sequence length. Salesforce AI Research Blog.\\n\\nNLPCC. 2014. Task definition for large scale text categorization at nlpcc 2014.\\n\\nOpenAI. 2022a. Introducing chatgpt.\\n\\nOpenAI. 2022b. Openai: New and improved embedding model.\\n\\nAntonio Orvieto, Samuel L Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. 2023. Resurrecting recurrent neural networks for long sequences. arXiv preprint arXiv:2303.06349.\\n\\nOfir Press, Noah Smith, and Mike Lewis. 2022. Train short, test long: Attention with linear biases enables input length extrapolation. In International Conference on Learning Representations.\\n\\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar, Chloe Hillier, and Timothy P Lillicrap. 2020. Compressive transformers for long-range sequence modelling. In International Conference on Learning Representations.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551.\\n\\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. 2021. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:53\u201368.\\n\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196.\\n\\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, et al. 2022. Scrolls: Standardized comparison over long language sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021.\\n\\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao, Abu Awal Md Shoeb, Abubakar Abid, Adam Fisch, Adam R Brown, Adam Santoro, Aditya Gupta, Adri\u00e0 Garriga-Alonso, et al. 2023. Beyond the imitation game: Quantifying and extrapolating the capabilities of language models. Transactions on Machine Learning Research.\\n\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do long-range language models actually use long-range context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807\u2013822.\\n\\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaozhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2022. A length-extrapolatable transformer. arXiv preprint arXiv:2212.10554.\"}"}
{"id": "acl-2024-long-172", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-172", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7 lists the instantiation of $(I, C, A)$ for each dataset in LongBench. Table 8 reports the number of data on each task that falls in the length range of 0-4k, 4k-8k, and 8k+ in LongBench-E.\\n\\n| Dataset Type       | Details |\\n|--------------------|---------|\\n| Single-Document QA | NarrativeQA Question Document Answer |\\n|                   | Qasper Question Document Answer |\\n|                   | MultiFieldQA-en Question Document Answer |\\n|                   | MultiFieldQA-zh Question Document Answer |\\n| Multi-Document QA  | HotpotQA Question Multiple documents Answer |\\n|                   | 2WikiMultihopQA Question Multiple documents Answer |\\n|                   | MuSiQue Question Multiple documents Answer |\\n|                   | DuReader Question Multiple documents Answer |\\n| Summarization      | GovReport - Document Summary |\\n|                   | QMSum Query Document Summary |\\n|                   | MultiNews - Document Summary |\\n|                   | VCSUM - Document Summary |\\n| Few-shot Learning  | TREC Question Few-shot examples Class label |\\n|                   | TriviaQA Passage&Question Few-shot examples Answer |\\n|                   | SAMSum Dialogue Few-shot examples Summary |\\n|                   | LSHT News document Few-shot examples Class label |\\n| Synthetic Task     | PassageCount - Multiple passages Count |\\n|                   | PassageRetrieval-en Summary Multiple passages Title of the passage |\\n|                   | PassageRetrieval-zh Summary Multiple passages Title of the passage |\\n| Code Completion    | LCC - Preceding lines of code Next line of code |\\n|                   | RepoBench-P Preceding lines of code Cross-file code snippets Next line of code |\\n\\nTable 7: Instantiation of $(I, C, A)$ for each task in LongBench.\\n\\nDetails of MultiFieldQA document sources. The source of the documents in MultiFieldQA include:\\n\\n- Arxiv (for academic papers): open-accessed and can be downloaded freely by anyone.\\n- C4 Dataset: publicly available dataset with ODC-BY license.\\n- WuDaoCorpora: open-accessed dataset.\\n- Chinese Judgements Online (for Chinese legal documents): open Chinese judgements download website.\\n- Wikipedia (for encyclopedias): grant free access and licensed under CC BY-SA.\\n- Chinese Government Website (for Chinese government report): open Chinese government report download website.\\n\\nAnnotation guidelines for MultiFieldQA. Here we provide the annotation guidelines: \\\"Please propose one question and a groundtruth answer for each of the following documents. Requirements: 1. Questions should be as clear as possible and have definitive and relatively short answers. 2. The distribution of the evidence paragraphs should be as random as possible throughout the article. 3. Ensure that the questions have varied types, including, but not limited to, information-extracting questions (e.g., the time of an event).\\\"\"}"}
{"id": "acl-2024-long-172", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Data length distributions in LongBench-E.\\n\\nEvent: The date of birth of a person, etc.), summarizing questions (e.g., which people does the article mainly describes), and multi-hop reasoning questions.\\n\\nThe average time taken to annotate each data sample in MultiFieldQA is around 5 minutes. The annotators involved in this process are Ph.D. students with extensive research experience in the field of NLP, positioning them as expert annotators. Cross-validation among annotators shows a 100% accuracy rate of the annotated answers.\\n\\nEvaluation Setups\\n\\nIn this section, we present a collection of customized prompt templates designed for each dataset within LongBench, utilized during our evaluation. Recall that each data instance is accompanied by an input $I$ as well as a context $C$. We place the instruction both at the beginning and end of the prompt, ensuring the models fully grasp what to do.\\n\\nNarrativeQA: You are given a story, which can be either a novel or a movie script, and a question. Answer the question as concisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nStory: {context}\\n\\nNow, answer the question based on the story as concisely as you can, using a single phrase if possible. Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:\\n\\nQasper: You are given a scientific article and a question. Answer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"cannot determine\\\".\"}"}
{"id": "acl-2024-long-172", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Answer the question based on the above article as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \\\"unanswerable\\\". If the question is a yes/no question, answer \\\"yes\\\", \\\"no\\\", or \\\"unanswerable\\\". Do not provide any explanation.\\n\\nQuestion: {input}\\n\\nAnswer:\\n\\nDuReader: \u8bf7\u57fa\u4e8e\u7ed9\u5b9a\u7684\u6587\u7ae0\u56de\u7b54\u4e0b\u8ff0\u95ee\u9898\u3002\"}"}
{"id": "acl-2024-long-172", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GovReport: You are given a report by a government agency. Write a one-page summary of the report.\\n\\nReport: {context}\\n\\nNow, write a one-page summary of the report.\\n\\nSummary:\\n\\nQMSum: You are given a meeting transcript and a query containing a question or instruction. Answer the query in one or more sentences.\\n\\nTranscript: {context}\\n\\nNow, answer the query based on the above meeting transcript in one or more sentences.\\n\\nQuery: {input}\\n\\nAnswer:\\n\\nMultiNews: You are given several news passages. Write a one-page summary of all news.\\n\\nNews: {context}\\n\\nNow, write a one-page summary of all the news.\\n\\nSummary:\\n\\nVCSUM: \u4e0b\u9762\u6709\u4e00\u6bb5\u4f1a\u8bae\u8bb0\u5f55\uff0c\u8bf7\u4f60\u9605\u8bfb\u540e\uff0c\u5199\u4e00\u6bb5\u603b\u7ed3\uff0c\u603b\u7ed3\u4f1a\u8bae\u7684\u5185\u5bb9\u3002\\n\\n\u4f1a\u8bae\u8bb0\u5f55: {context}\\n\\nTREC: Please determine the type of the question below. Here are some examples of questions.\\n\\nTriviaQA: Answer the question based on the given passage. Only give me the answer and do not output any other words. The following are some examples.\\n\\nSAMSum: Summarize the dialogue into a few short sentences. The following are some examples.\\n\\nLSHT: \u8bf7\u5224\u65ad\u7ed9\u5b9a\u65b0\u95fb\u7684\u7c7b\u522b\uff0c\u4e0b\u9762\u662f\u4e9b\u4f8b\u5b50\u3002\\n\\nPassageCount: There are some paragraphs below sourced from Wikipedia. Some of them may be duplicates. Please carefully read these paragraphs and determine how many unique paragraphs there are after removing duplicates. In other words, how many non-repeating paragraphs are there in total?\\n\\nPlease enter the final count of unique paragraphs after removing duplicates. The output format should be: \\n\\n{context}\\n\\nPlease enter the final count of unique paragraphs after removing duplicates. The output format should be: \\n\\n3134\"}"}
