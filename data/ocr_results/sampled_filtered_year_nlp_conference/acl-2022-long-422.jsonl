{"id": "acl-2022-long-422", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Strategy Template Example\\nLocating Stage\\nEntity Name - LeBron James\\nConcept Name <C> basketball players\\nConcept + Literal the (C) whose (K) is (OP) (V) (QK) (is) (QV) the basketball team whose social media followers is greater than 3,000,000 (point in time is 2021)\\n\\nRecursive Multi-Hop unfold\\nConcept + Relational the (C) that (P) (E) (is) (QK) (is) (QV) the basketball player that was drafted by Cleveland Cavaliers\\n\\nIntersection Condition 1\\nand Condition 2 the basketball players whose height is greater than 190 centimetres and less than 220 centimetres\\n\\nUnion Condition 1\\nor Condition 2 the basketball players that were born in Akron or Cleveland\\n\\nAsking Stage\\nQueryName What/Who is (E) Who is the basketball player whose height is equal to 206 centimetres?\\nCount How many (E) How many basketball players that were drafted by Cleveland Cavaliers?\\nQueryAttribute For (E), what is his/her/its (K) (is) (QK) (is) (QV) For Cleveland Cavaliers, what is its social media follower number (point in time is January 2021)?\\nRelation What is the relation from (E) to (E) What is the relation from LeBron James Jr. to LeBron James?\\nSelectAmong Among (E), which one has the largest/smallest (K) Among basketball players, which one has the largest mass?\\nSelectBetween Which one has the larger/smaller (K), (E) or (E) Which one has the larger mass, LeBron James Jr. or LeBron James?\\nVerify For (E), is his/her/its (K) (OP) (V) (is) (QK) (is) (QV) For the human that is the father of LeBron James Jr., is his/her height greater than 180 centimetres?\\nQualifierLiteral For (E), his/her/its (K) is (V), what is the (QK) (is) (QV) For Akron, its population is 199,110, what is the point in time?\\nQualifierRelational (E) (P) (E), what is the (QK) LeBron James was drafted by Cleveland Cavaliers, what is the point in time?\\n\\nTable 8: Templates and examples of our locating and asking stage. Placeholders in template have specific implication:\\n\\n<E>-description of an entity or entity set;\\n<C>-concept;\\n<K>-attribute key;\\n<OP>-operator, selected from {=, !=, <, >};\\n<V>-attribute value;\\n<QK>-qualifier key;\\n<QV>-qualifier value;\\n<P>-relation description, e.g., was drafted by.\\n\\nAttention mechanism (Dong and Lapata, 2016) was applied by focusing on the most relevant question words when predicting each function and each textual input. The SPARQL parser used the same encoder-decoder structure to produce SPARQL token sequences. We tokenized the SPARQL query by delimiting spaces and some special punctuation symbols.\\n\\nTable 9: BART KoPL Accuracy of different #hops.\\n\\n| #hops   | Accuracy |\\n|---------|----------|\\n| 2-3 Hop | 92.4     |\\n| 4-5 Hop | 87.71    |\\n| 6-7 Hop | 86.41    |\\n| 8-9 Hop | 86.51    |\\n\\nI BART KoPL Accuracy of different #hops. Note that KQA Pro not only consider multi-hop relations, but also consider attributes and qualifiers. We count all of them into the hop number. So in KQA Pro, given a question with \\\"4-hops\\\", it does not mean 4 relations, but may be 1 relations + 2 attributes + 1 comparison. E.g., \\\"Who is taller, LeBron James Jr. or his father?\\\".\"}"}
{"id": "acl-2022-long-422", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question: Which area has higher elevation (above sea level), Baghdad or Jerusalem (the one whose population is 75200)?\\n\\nChoices:\\nSanto Domingo; Kingston; Trieste; Jerusalem; Cork; Abidjan; Bergen; Baghdad; Chihuahua; Dundee\\n\\nAnswer: Jerusalem\\n\\nQuestion: Of New Jersey cities with under 350000 in population, which is biggest in terms of area?\\n\\nChoices:\\nHoboken; Bayonne; Paterson; Perth Amboy; New Brunswick; Trenton; Camden; Atlantic City; Newark; East Orange\\n\\nAnswer: Newark\\n\\nQuestion: When did the big city whose postal code is 54000 have a population of 104072?\\n\\nChoices:\\n1980-04-01; 1868-01-01; 2008-11-12; 1790-01-01; 1964-12-01; 2010-08-11; 1772-12-01; 2013-01-01; 1861; 1810-01-01\\n\\nAnswer: 2013-01-01\\n\\nQuestion: Is the elevation above sea level for the capital city of Guyana less than 130 meters?\\n\\nAnswer: yes\\n\\nQuestion: What is the street address of the California Institute of the Arts?\\n\\nAnswer: 24700 W McBean Pky, Valencia, CA, 91355-2397\\n\\nQuestion: Who is the person that is Kylie Minogue's sibling?\\n\\nAnswer: Dannii Minogue\"}"}
{"id": "acl-2022-long-422", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Among the feature films with a publication date after 2003, which one has the smallest duration?\\n\\nSPARQL: \\n\\nSELECT ?e WHERE { ?e <pred:instance_of> ?c . ?c <pred:name> \\\"feature film\\\" . ?e <publication_date> ?pv_1 . ?pv_1 <pred:year> ?v_1 . FILTER ( ?v_1 > 2003 ) . ?e <duration> ?pv . ?pv <pred:value> ?v . } ORDER BY ?v LIMIT 1\\n\\nKoPL: \\n\\nFindAll\\n\\nChoices: \\nAlice in Wonderland; Pirates of the Caribbean: Dead Man's Chest; Wallace & Gromit: The Curse of the Were-Rabbit; Bedtime Stories; Secretariat; The Sorcerer's Apprentice; Enchanted; Old Dogs; Harry Potter and the Prisoner of Azkaban; Prince of Persia: The Sands of Time\\n\\nAnswer: \\nWallace & Gromit: The Curse of the Were-Rabbit\\n\\nFilterYear\\npublication date\\n2003\\n\\nFilterConcept\\nfeature film\\n\\nSelectAmong\\nduration\\n\\nsmallest\"}"}
{"id": "acl-2022-long-422", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"KQA Pro: A Dataset with Explicit Compositional Programs for Complex Question Answering over Knowledge Base\\n\\nShulin Cao\\\\(^1\\\\), Jiaxin Shi\\\\(^1,3\\\\)\u2217, Liangming Pan\\\\(^4\\\\), Lunyiu Nie\\\\(^1\\\\), Yutong Xiang\\\\(^5\\\\), Lei Hou\\\\(^1,2\\\\)\u2020, Juanzi Li\\\\(^1,2\\\\), Bin He\\\\(^6\\\\), Hanwang Zhang\\\\(^7\\\\)\\n\\n\\\\(^1\\\\)Department of Computer Science and Technology, BNRist\\n\\\\(^2\\\\)KIRC, Institute for Artificial Intelligence, Tsinghua University, Beijing 100084, China\\n\\\\(^3\\\\)Cloud BU, Huawei Technologies, \\\\(^4\\\\)National University of Singapore, \\\\(^5\\\\)ETH Z\u00fcrich\\n\\\\(^6\\\\)Noah\u2019s Ark Lab, Huawei Technologies, \\\\(^7\\\\)Nanyang Technological University\\n\\n\\\\{caosl19@mails., houlei@,lijuanzi@\\\\}tsinghua.edu.cn\\nshijx12@gmail.com, liangmingpan@u.nus.edu\\n\\nAbstract\\nComplex question answering over knowledge base (Complex KBQA) is challenging because it requires various compositional reasoning capabilities, such as multi-hop inference, attribute comparison, set operation. Existing benchmarks have some shortcomings that limit the development of Complex KBQA: 1) they only provide QA pairs without explicit reasoning processes; 2) questions are poor in diversity or scale. To this end, we introduce KQA Pro, a dataset for Complex KBQA including ~120K diverse natural language questions. We introduce a compositional and interpretable programming language KoPL to represent the reasoning process of complex questions. For each question, we provide the corresponding KoPL program and SPARQL query, so that KQA Pro serves for both KBQA and semantic parsing tasks. Experimental results show that SOTA KBQA methods cannot achieve promising results on KQA Pro as on current datasets, which suggests that KQA Pro is challenging and Complex KBQA requires further research efforts. We also treat KQA Pro as a diagnostic dataset for testing multiple reasoning skills, conduct a thorough evaluation of existing models and discuss further directions for Complex KBQA. Our codes and datasets can be obtained from https://github.com/shijx12/KQAPro_Baselines.\\n\\n1 Introduction\\n\\nThanks to the recent advances in deep models, especially large-scale unsupervised representation learning (Devlin et al., 2019), question answering of simple questions over knowledge base (Simple KBQA), i.e., single-relation factoid questions (Bordes et al., 2015), begins to saturate (Petrochuk and Zettlemoyer, 2018; Wu et al., 2019; Huang et al., 2019). However, tackling complex questions (Complex KBQA) is still an ongoing challenge, due to the unsatisfied capability of compositional reasoning. As shown in Table 1, to promote the community development, several benchmarks are proposed for Complex KBQA, including LC-QuAD2.0 (Dubey et al., 2019), ComplexWebQuestions (Talmor and Berant, 2018), MetaQA (Zhang et al., 2018), CSQA (Saha et al., 2018), CFQ (Keysers et al., 2020), and so on. However, they suffer from the following problems: 1) Most of them only provide QA pairs without explicit reasoning processes, making it challenging for models to learn compositional reasoning. Some researchers try to learn the reasoning processes with reinforcement learning (Liang et al., 2017; Saha et al., 2019; Ansari et al., 2019) and searching (Guo et al., 2018). However, the prohibitively huge search space hinders both the performance and speed, especially when the question complexity increases. For example, Saha et al. (2019) achieved a 96.52% F1 score on simple questions in CSQA, whereas only 0.33% on complex questions that require comparative count. We think that intermediate supervision is needed for learning the compositional reasoning, mimicking the learning process of human beings (Holt, 2017).\\n\\n2) Questions are not satisfactory in diversity and scale. For example, MetaQA (Zhang et al., 2018) questions are generated using just 36 templates, and they only consider relations between entities, ignoring literal attributes; LC-QuAD2.0 (Dubey et al., 2019) and ComplexWebQuestions (Talmor and Berant, 2018) have fluent and diverse human-written questions, but their scale is less than 40K.\\n\\nTo address these problems, we create KQA Pro, a large-scale benchmark for Complex KBQA. In KQA Pro, we define a Knowledge-oriented Programming Language (KoPL) to explicitly describe the reasoning process of complex questions.\"}"}
{"id": "acl-2022-long-422", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question 1:\\nSPARQL:\\nKoPL:\\nWhen did Cleveland Cavaliers pick up LeBron James\\nSELECT DISTINCT ?qpv WHERE { ?e_1 <pred:name> \\\"LeBron James\\\" . ?e_2 <pred:name> \\\"Cleveland Cavaliers\\\" . ?e_1 <drafted by> ?e_2 . <pred:fact_h> ?e_1 ; <pred:fact_r> <winner> ; <pred:fact_t> ?e_2 <point_in_time> ?qpv .  }\\n\\nQuestion 2:\\nSPARQL:\\nKoPL:\\nWho is taller, LeBron James Jr. or his father?\\nSELECT ?e WHERE { { ?e <name> \\\"LeBron James Jr.\\\" .  } UNION { ?e_1 <name> \\\"LeBron James Jr.\\\" . ?e_1 <father> ?e .  } ?e <height> ?v .  } ORDER BY DESC(?v) LIMIT 1\\n\\nFigure 1: Example of our KB and questions. Our KB is a dense subset of Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), including multiple types of knowledge. Our questions are paired with executable KoPL programs and SPARQL queries.\\n\\nThe reasoning process for solving complex questions (see Fig. 1). A program is composed of symbolic functions, which define the basic, atomic operations on KBs. The composition of functions well captures the language compositionality (Baroni, 2019). Besides KoPL, following previous works (Yih et al., 2016; Su et al., 2016), we also provide the corresponding SPARQL for each question, which solves a complex question by parsing it into a query graph. Compared with SPARQL, KoPL 1) provides a more explicit reasoning process. It divides the question into multiple steps, making human understanding easier and the intermediate results more transparent; 2) allows humans to control the model behavior better, potentially supporting human-in-the-loop. When the system gives a wrong answer, users can quickly locate the error by checking the outputs of intermediate functions. We believe the compositionality of KoPL and the graph structure of SPARQL are two complementary directions for Complex KBQA.\\n\\nTo ensure the diversity and scale of KQA Pro, we follow the synthesizing and paraphrasing pipeline in the literature (Wang et al., 2015a; Cao et al., 2020), first synthesize large-scale (canonical question, KoPL, SPARQL) triples, and then paraphrase the canonical questions to natural language questions (NLQs) via crowdsourcing. We combine the following two factors to achieve diversity in questions: (1) To increase structural variety, we leverage a varied set of templates to cover all the possible queries through random sampling and recursive composing; (2) To increase linguistic variety, we filter the paraphases based on their edit distance with the canonical utterance. Finally, KQA Pro consists of 117,970 diverse questions that involve varied reasoning skills (e.g., multi-hop reasoning, value comparisons, set operations, etc.). Besides a QA dataset, it also serves as a semantic parsing dataset. To the best of our knowledge, KQA Pro is currently the largest corpus for NLQ-to-SPARQL and NLQ-to-Program tasks.\\n\\nWe reproduce the state-of-the-art KBQA models and thoroughly evaluate them on KQA Pro. From the experimental results, we observe significant performance drops of these models compared with on existing KBQA benchmarks. It indicates that Complex KBQA is still challenging, and KQA Pro could support further explorations. We also treat KQA Pro as a diagnostic dataset for analyzing a model\u2019s capability of multiple reasoning skills, and discover weaknesses that are not widely known, e.g., current models struggle on comparisonal reasoning for lacking of literal knowledge (i.e., (LeBron James, height, 206 centimetre)), or perform poorly on questions whose answers are not observed in the training set. We hope all contents of KQA Pro could encourage the community to make further breakthroughs.\"}"}
{"id": "acl-2022-long-422", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset multiple kinds of knowledge\\nnumber of questions\\nnatural language query\\nmulti-step programs\\n\\nWebQuestions (Berant et al., 2013) \u2713 5,810 \u2713 \u00d7 \u00d7\\nWebQuestionSP (Yih et al., 2016) \u2713 4,737 \u2713 \u2713 \u00d7\\nGraphQuestions (Su et al., 2016) \u2713 5,166 \u2713 \u2713 \u00d7\\nLC-QuAD2.0 (Dubey et al., 2019) \u2713 30,000 \u2713 \u2713 \u00d7\\nComplexWebQuestions (Talmor and Berant, 2018) \u2713 34,689 \u2713 \u2713 \u00d7\\nMetaQA (Zhang et al., 2018) \u00d7 400,000 \u00d7 \u00d7 \u00d7\\nCSQA (Saha et al., 2018) \u00d7 1.6M \u00d7 \u00d7 \u00d7\\nCFQ (Keysers et al., 2020) \u00d7 239,357 \u00d7 \u2713 \u2713\\nGrailQA (Gu et al., 2021) \u2713 64,331 \u2713 \u2713 \u00d7\\nKQA Pro (ours) \u2713 117,970 \u2713 \u2713 \u2713\\n\\nTable 1: Comparison with existing datasets of Complex KBQA. The column multiple kinds of knowledge means whether the dataset considers multiple types of knowledge or just relational knowledge (introduced in Sec.3.1). The column natural language means whether the questions are in natural language or written by templates.\\n\\net al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020; Shi et al., 2021), which constructs a question-specific graph extracted from the KB and ranks all the entities in the extracted graph based on their relevance to the question.\\n\\nCompared with information retrieval based methods, semantic parsing based methods provides better interpretability by generating expressive logic forms, which represents the intermediate reasoning process. However, manually annotating logic forms is expensive and labor-intensive, and it is challenging to train a semantic parsing model with weak supervision signals (i.e., question-answer pairs). Lacking logic form annotations turns out to be one of the main bottlenecks of semantic parsing.\\n\\nTable 1 lists the widely-used datasets in Complex KBQA community and their features. MetaQA and CSQA have a large number of questions, but they ignore literal knowledge, lack logic form annotations, and their questions are written by templates. Query graphs (e.g., SPARQLs) are provided in some datasets to help solve complex questions. However, SPARQL is weak in describing the intermediate procedure of the solution, and the scale of existing question-to-SPARQL datasets is small.\\n\\nIn this paper, we introduce a novel logic form KoPL, which models the procedure of Complex KBQA as a multi-step program, and provides a more explicit reasoning process compared with query graphs. Furthermore, we propose KQA Pro, a large-scale semantic parsing dataset for Complex KBQA, which contains ~120k diverse natural language questions with both KoPL and SPARQL annotations. It is the largest NLQ-to-SPARQL dataset as far as we know. Compared with these existing datasets, KQA Pro serves as a more well-rounded benchmark.\\n\\n3 Background\\n\\n3.1 KB Definition\\n\\nTypically, a KB (e.g., Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014)) consists of:\\n\\n- Entity, the most basic item in KB.\\n- Concept, the abstraction of a set of entities, e.g., basketball player.\\n- Relation, the link between entities or concepts. Entities are linked to concepts via the relation instance of. Concepts are organized into a tree structure via relation subclass of.\\n- Attribute, the literal information of an entity. An attribute has a key and a value, which is one of four types: string, number, date, and year. The number value has an extra unit, e.g., 206 centimetre.\\n\\nRelational knowledge, the triple with form (entity, relation, entity), e.g., (LeBron James Jr., father, LeBron James).\\n\\nLiteral knowledge, the triple with form (entity, attribute key, attribute value), e.g., (LeBron James, height, 206 centimetre).\\n\\nQualifier knowledge, the triple whose head is a relational or literal triple, e.g., ((LeBron James, drafted by, Cleveland Cavaliers), point in time, 2003). A qualifier also has a key and a value.\\n\\n3.2 KoPL Design\\n\\nWe design KoPL, a compositional and interpretable programming language to represent the reasoning process of complex questions. It models the complex procedure of question answering with a program of intermediate steps. Each step involves a function with a fixed number of arguments. Every program can be denoted as a binary tree. As shown in Fig. 1, a directed edge between two nodes represents the dependency relationship between two\"}"}
{"id": "acl-2022-long-422", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"functions. That is, the destination function takes the output of the source function as its argument. The tree-structured program can also be serialized by post-order traversal, and formalized as a sequence with \\\\( n \\\\) functions. The general form is shown below.\\n\\nEach function \\\\( f_i \\\\) takes in a list of textual arguments \\\\( a_i \\\\), which need to be inferred according to the question, and a list of functional arguments \\\\( b_i \\\\), which come from the output of previous functions.\\n\\n\\\\[\\n\\\\begin{align*}\\n&f_1(a_1, b_1) \\\\\\\\\\n&f_2(a_2, b_2) \\\\\\\\\\n&\\\\ldots \\\\\\\\\\n&f_n(a_n, b_n)\\n\\\\end{align*}\\n\\\\]  \\n\\nTake function \\\\( \\\\text{Relate} \\\\) as an example, it has two textual inputs: relation and direction (i.e., forward or backward, meaning the output is object or subject). It has one functional input: a unique entity. Its output is a set of entities that hold the specific relation with the input entity. For example, in Question 2 of Fig. 1, the function \\\\( \\\\text{Relate}([\\\\text{father}, \\\\text{forward}], [\\\\text{Lebron James Jr.}]) \\\\) returns LeBron James, the father of LeBron James Jr. (the direction is omitted in the figure for simplicity).\\n\\nWe analyze the generic, basic operations for Complex KBQA, and design 27 functions in KoPL. They support KB item manipulation (e.g., \\\\( \\\\text{Find} \\\\), \\\\( \\\\text{Relate} \\\\), \\\\( \\\\text{FilterConcept} \\\\), \\\\( \\\\text{QueryRelationQualifier} \\\\), etc.), various reasoning skills (e.g., \\\\( \\\\text{And} \\\\), \\\\( \\\\text{Or} \\\\), etc.), and multiple question types (e.g., \\\\( \\\\text{QueryName} \\\\), \\\\( \\\\text{SelectBetween} \\\\), etc.). By composing the finite functions into a KoPL program, we can model the reasoning process of infinite complex questions.\\n\\nNote that qualifiers play an essential role in disambiguating or restricting the validity of a fact (Galkin et al., 2020; Liu et al., 2021). However, they have not been adequately modeled in current KBQA models or datasets. As far as we know, we are the first to explicitly model qualifiers in Complex KBQA.\\n\\n4 KQA Pro Construction\\n\\nTo build KQA Pro dataset, first, we extract a knowledge base with multiple kinds of knowledge (Section 4.1). Then, we generate canonical questions, corresponding KoPL programs and SPARQL queries with novel compositional strategies (Section 4.2). In this stage, we aim to cover all the possible queries through random sampling and recursive composing. Finally, we rewrite canonical questions into natural language via crowdsourcing (Section 4.3). To further increase linguistic variety, we reject the paraphrases whose edit distance with the canonical question is small.\\n\\n4.1 Knowledge Base Extraction\\n\\nWe took the entities of FB15k-237 (Toutanova et al., 2015) as seeds, and aligned them with Wikidata via Freebase IDs. The reasons are as follows: 1) The vast amount of knowledge in the full knowledge base (e.g., full Freebase (Bollacker et al., 2008) or Wikidata contains millions of entities) may cause both time and space issues, while most of the entities may never be used in questions. 2) FB15k-237 is a high-quality, dense subset of Freebase, whose alignment to Wikidata produces a knowledge base with rich literal and qualifier knowledge. We added 3,000 other entities with the same name as one of FB15k-237 entities to increase the disambiguation difficulty. The statistics of our final knowledge base are listed in Table 2.\\n\\n| # Con. | # Ent. | # Name | # Pred. | # Attr. |\\n|-------|-------|-------|--------|--------|\\n| 794   | 16,960| 14,471| 363    | 846    |\\n\\nTable 2: Statistics of our knowledge base. The top lists the numbers of concepts, entities, unique entity names, predicates, and attributes. The bottom lists the numbers of different types of knowledge.\\n\\n4.2 Question Generation Strategies\\n\\nTo generate diverse complex questions in a scalable manner, we propose to divide the generation into two stages: locating and asking. In locating stage we describe a single entity or an entity set with various restrictions, while in asking stage we query specific information about the target entity or entity set. We define several strategies for each stage. By sampling from them and composing the two stages, we can generate large-scale and diverse questions with a small number of templates. Fig. 2 gives an example of our generation process.\\n\\nFor locating stage, we propose 7 strategies and show part of them in the top section of Table 3. We can fill the placeholders of templates by sampling from KB to describe a target entity. We support quantitative comparisons of 4 operations: equal, not equal, less than, and greater than, indicated by \\\"<OP>\\\" of the template. We support optional qualifier restrictions, indicated by \\\"(<QK> is <QV>)\\\",  \\n\\n4 The detailed extracting process is in Appendix C.\"}"}
{"id": "acl-2022-long-422", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Strategy Template Example\\n\\nLocating Stage\\n\\nEntity Name - LeBron James\\n\\nConcept + Literal the <C> whose <K> is <OP> (is <QK> is <QV>) the basketball team whose social media followers is greater than 3,000,000 (point in time is 2021)\\n\\nConcept + Relational the <C> that <P> (is <QK> is <QV>) the basketball player that was drafted by Cleveland Cavaliers\\n\\nRecursive Multi-Hop unfold\\n\\nDescription the basketball player that was drafted by the basketball team whose social media followers is greater than 3,000,000 (point in time is 2021)\\n\\nIntersection Condition 1\\n\\nand Condition 2 the basketball players whose height is greater than 190 centimetres and less than 220 centimetres\\n\\nAsking Stage\\n\\nQuery Name What/Who is <E>\\n\\nWho is the basketball player whose height is equal to 206 centimetres?\\n\\nCount How many <E>\\n\\nHow many basketball players that were drafted by Cleveland Cavaliers?\\n\\nSelect Among Among <E>\\n\\nAmong basketball players, which one has the largest/smallest <K>\\n\\nAmong basketball players, which one has the largest mass?\\n\\nVerify For <E>\\n\\nFor, is his/her/its <K> (is <QK> is <QV>) For the human that is the father of LeBron James Jr., is his/her height greater than 180 centimetres?\\n\\nQualifier Relational <E> <P> <E>\\n\\nwhat is the <QK> LeBron James was drafted by Cleveland Cavaliers, what is the point in time?\"}"}
{"id": "acl-2022-long-422", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which team picked LeBron James?\\n\\nAnswer Uniqueness Checking\\n\\nNaturalQ:\\n\\nCanonicalQ:\\n\\nSPARQL:\\n\\nSELECT ?e WHERE { ?e <pred:instance_of> ?c . ?c <pred:name> \\\"team\\\" . ?e_1 <drafted_by> ?e . ?e_1 <pred:name> \\\"LeBron James\\\".  }\\n\\nKoPL:\\n\\nFind LeBron James\\nFilterConcept team\\nRelate drafted by\\nbackward\\nQueryName the team that drafted LeBron James\\n\\nSample\\nCleveland Cavaliers\\n\\nLeBron James\\nUnited States of America\\ndrafted by\\ncountry\\nteam\\nsports team\\nworking\\ngroup\\nCleveland Cavaliers\\nbasketball\\nteam\\n\\nAsking Strategies\\n\\nsample\\nLeBron James JR.\\nUnited States of America\\nLeBron James\\nCleveland Cavaliers\\nsample\\nsample\\n\\nFigure 2: Process of our question generation. First, we sample a question type from asking strategies and sample a target entity from KB. Next, we sample a locating strategy and detailed conditions to describe the target entity. Finally, we combine intermediate snippets into the complete question and check whether the answer is unique. Note that the snippets of canonical question, SPARQL, and KoPL are operated simultaneously. A more detailed explanation of this example is in Appendix F.\\n\\ndatasets in Fig. 3(c). We observe that our KQA Pro has longer questions than others on average. In KQA Pro, the average length of questions/programs/SPARQLs is 14.95/4.79/35.52 respectively. More analysis is included in Appendix G.\\n\\n5 Experiments\\n\\nThe primary goal of our experiments is to show the challenges of KQA Pro and promising Complex KBQA directions. First, we compare the performance of state-of-the-art KBQA models on current datasets and KQA Pro, to show whether KQA Pro is challenging. Then, we treat KQA Pro as a diagnostic dataset to investigate fine-grained reasoning abilities of models, discuss current weakness and promising directions. We further conduct an experiment to explore the generation ability of our proposed model. Last, we provide a case study to show the interpretablity of KoPL.\\n\\n5.1 Experimental Settings\\n\\nBenchmark Settings\\n\\nWe randomly split KQA Pro to train/valid/test set by 8/1/1, resulting in three sets with 94,376/11,797/11,797 instances. About 30% answers of the test set are not seen in training.\\n\\nRepresentative Models\\n\\nKBQA models typically follow a retrieve-and-rank paradigm, by constructing a question-specific graph extracted from the KB and ranks all the entities in the graph based on their relevance to the question (Miller et al., 2016; Saxena et al., 2020; Schlichtkrull et al., 2018; Zhang et al., 2018; Zhou et al., 2018; Qiu et al., 2020); or follow a parse-then-execute paradigm, by parsing a question to a query graph (Berant et al., 2013; Yih et al., 2015) or program (Liang et al., 2017; Guo et al., 2018; Saha et al., 2019; Ansari et al., 2019) through learning from question-answer pairs. Experimenting with all methods is logistically challenging, so we reproduce a representative sub-set of mothods: KVMemNet (Miller et al., 2016), a well-known model which organizes the knowledge into a memory of key-value pairs, and iteratively reads memory to update its query vector. EmbedKGQA (Saxena et al., 2020), a state-of-the-art model on MetaQA, which incorporates knowledge embeddings to improve the reasoning performance. SRN (Qiu et al., 2020), a typical path search model to start from a topic entity and predict a sequential relation path to find the target entity. RGCN (Schlichtkrull et al., 2018), a variant.\"}"}
{"id": "acl-2022-long-422", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of graph convolutional networks, tackling Complex KBQA through the natural graph structure of knowledge base.\\n\\nOur models. Since KQA Pro provides the annotations of SPARQL and KoPL, we directly learn our parsers using supervised learning by regarding the semantic parsing as a sequence-to-sequence task. We explore the widely-used sequence-to-sequence model\u2014RNN with attention mechanism (Dong and Lapata, 2016), and the pretrained generative language model\u2014BART (Lewis et al., 2020), as our SPARQL and KoPL parsers.\\n\\nFor KoPL learning, we design a serializer to translate the tree-structured KoPL to a sequence. For example, the KoPL program in Fig. 2 is serialized as:\\n\\n```\\nFind \u27e8arg\u27e9 LeBron James \u27e8func\u27e9 Relate \u27e8arg\u27e9 drafted by \u27e8arg\u27e9 backward \u27e8func\u27e9 Filter-Concept \u27e8arg\u27e9 team \u27e8func\u27e9 QueryName.\\n```\\n\\nHere, \u27e8arg\u27e9 and \u27e8func\u27e9 are special tokens we designed to indicate the structure of KoPL.\\n\\nTo compare machine with Human, we sample 200 instances from the test set, and ask experts to answer them by searching our knowledge base.\\n\\nImplementation Details. For our BART model, we used the bart-base model of HuggingFace. We used the optimizer Adam (Kingma and Ba, 2015) for all models. We searched the learning rate for BART parameters in {1e-4, 3e-5, 1e-5}, the learning rate for other parameters in {1e-3, 1e-4, 1e-5}, and the weight decay in {1e-4, 1e-5, 1e-6}. According to the performance on validation set, we finally used learning rate 3e-5 for BART parameters, 1e-3 for other parameters, and weight decay 1e-5.\\n\\n5.2 Difficulty of KQA Pro\\n\\nWe compare the performance of KBQA models on KQA Pro with MetaQA and WebQSP (short for WebQuestionSP), two commonly used benchmarks in Complex KBQA. The experimental results are in Table 4, from which we observe that:\\n\\n- Although the models perform well on MetaQA and WebQSP, their performances are significantly lower and not satisfying on KQA Pro. It indicates that our KQA Pro is challenging and the Complex KBQA still needs more research efforts.\\n\\n- 1) Both MetaQA and WebQSP mainly focus on relational knowledge, i.e., multi-hop questions. Therefore, previous models on these datasets are designed to handle only entities and relations. In comparison, KQA Pro includes three types of knowledge, i.e., relations, attributes, and qualifiers, thus is much more challenging.\\n\\n- 2) Compared with MetaQA which contains template questions, KQA Pro contains diverse natural language questions and can evaluate models' language understanding abilities.\\n\\n- 3) Compared with WebQSP which contains 4,737 fluent and natural questions, KQA Pro covers more question types (e.g., verification, counting) and reasoning operations (e.g., intersect, union).\\n\\n| Model   | MetaQA 1-hop | MetaQA 2-hop | MetaQA 3-hop | WebQSP 3-hop | KQA Pro 1-hop | KQA Pro 2-hop | KQA Pro 3-hop |\\n|---------|--------------|--------------|--------------|--------------|---------------|---------------|---------------|\\n| KVMemNet| 96.2         | 82.7         | 48.9         | -            | 46.7          | 16.61         |\\n| SRN     | 97.0         | 95.1         | 75.2         | -            | 12.33         |\\n| EmbedKGQA| 97.5       | 98.8         | 94.8         | 66.6         | 28.36         |\\n| RGCN    | -            | -            | 37.2         | 35.07        |\\n| BART    | -            | -            | 99.9         | 67.5         | 88.55         |\\n\\nTable 4: SOTA models of Complex KBQA and their performance on different datasets. SRN's result on KQA Pro, 12.33%, is obtained on questions about only relational knowledge. The RGCN results on WebQSP is from (Sun et al., 2018). The BART results on MetaQA 3-hop WebQSP results are from (Huang et al., 2021).\\n\\n5.3 Analyses on Reasoning Skills\\n\\nKQA Pro can serve as a diagnostic dataset for in-depth analyses of reasoning abilities (e.g., counting, comparison, logical reasoning, etc.) for Complex KBQA, since KoPL programs underlying the questions provide tight control over the dataset. We categorize the test questions to measure fine-grained ability of models. Specifically, Multi-hop means multi-hop questions, Qualifier means questions containing qualifier knowledge, Comparison means quantitative or temporal comparison between two or more entities, Logical means logical union or intersection, Count means questions that ask the number of target entities, Verify means questions that take \u201cyes\u201d or \u201cno\u201d as the answer, Zero-shot means questions whose answer is not seen in the training set. The results are shown in Table 5, from which we have the following observations:\\n\\n1) Benefits of intermediate reasoning supervision. Our RNN and BART models outperform current models significantly on all reasoning skills. This is because KoPL program and SPARQL query provide intermediate supervision which benefits the learning process a lot. As (Dua et al., 2020) suggests, future dataset collection efforts should set aside a fraction of budget for intermediate annotations, particularly as the reasoning required...\"}"}
{"id": "acl-2022-long-422", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Accuracy of different models on KQA Pro test set. BART KoPL CG denotes the BART based KoPL parser on the compositional generalization experiment (see Section 5.4).\\n\\nWe hope our dataset KQA Pro with KoPL and SPARQL annotations will help guide further research in Complex KBQA. (2) More attention to literal and qualifier knowledge. Existing models perform poorly in situations requiring comparison capability. This is because they only focus on relational knowledge, while ignoring the literal and qualifier knowledge. We hope our dataset will encourage the community to pay more attention to multiple kinds of knowledge in Complex KBQA. (3) Generalization to novel questions and answers. For zero-shot questions, current models all have a close to zero performance. This indicates the models solve the questions by simply memorizing their training data, and perform poorly on generalizing to novel questions and answers.\\n\\n5.4 Compositional Generalization\\n\\nWe further use KQA Pro to test the ability of KBQA models to generalize to questions that contain novel combinations of the elements observed during training. Following previous works, we conduct the \\\"productivity\\\" experiment (Lake and Baroni, 2018; Shaw et al., 2021), which focuses on generalization to longer sequences or to greater compositional depths than have been seen in training (for example, from a length 4 program to a length 5 program). Specifically, we take the instances with short programs as training examples, and those with long programs as test and valid examples, resulting in three sets including 106,182/5,899/5,899 examples. The performance of BART KoPL drops from 90.55% to 77.86%, which indicates learning to generalize compositionally for pretrained language models requires more research efforts. Our KQA Pro provides an environment for further experimentation on compositional generalization.\\n\\nGolden SPARQL:\\nGolden KoPL:\\nFind Prime Minister of the United Kingdom\\nPredicted KoPL:\\nPredicted SPARQL:\\nSELECT DISTINCT ?e WHERE {\\n?e <pred:instance_of> ?c .\\n?c <pred:name> \\\"human\\\" .\\n?e <position_held> ?e_1 .\\n?e_1 <pred:name> \\\"Prime Minister of the United Kingdom\\\" .\\n?[ <pred:fact_h> ?e ; <pred:fact_r> <position_held> ; <pred:fact_t> ?e_1 ]\\n<replaced_by> ?qpv .\\n?qpv <pred:value> \\\"David Lloyd George\\\" .\\n}\"}"}
{"id": "acl-2022-long-422", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and KoPL and mark them in red. Compared to SPARQLs, KoPL programs are easier to be understood and more friendly to be modified.\\n\\n6 Conclusion and Future Work\\n\\nIn this work, we introduce a large-scale dataset with explicit compositional programs for Complex KBQA. For each question, we provide the corresponding KoPL program and SPARQL query so that KQA Pro can serve for both KBQA and semantic parsing tasks. We conduct a thorough evaluation of various models, discover weaknesses of current models and discuss future directions. Among these models, the KoPL parser shows great interpretability. As shown in Fig. 4, when the model predicts the answer, it will also give a reasoning process and a confidence score (which is omitted in the figure for simplicity). When the parser makes mistakes, humans can easily locate the error through reading the human-like reasoning process or checking the outputs of intermediate functions. In addition, using human correction data, the parser can be incrementally trained to improve the performance continuously. We will leave this as our future work.\\n\\nAcknowledgments\\n\\nThis work is founded by the National Key Research and Development Program of China (2020AAA0106501), the Institute for Guo Qiang, Tsinghua University (2019GQB0003), Huawei Noah's Ark Lab and Beijing Academy of Artificial Intelligence.\\n\\nReferences\\n\\nGhulam Ahmed Ansari, Amrita Saha, Vishwajeet Kumar, Mohan Bhambhani, Karthik Sankaranarayanan, and Soumen Chakrabarti. 2019. Neural program induction for KBQA without gold programs or query annotations. In Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI 2019, Macao, China, August 10-16, 2019, pages 4890\u20134896. ijcai.org.\\n\\nYoav Artzi, Nicholas FitzGerald, and Luke Zettlemoyer. 2013. Semantic parsing with Combinatory Categorial Grammars. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Tutorials), page 2, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178\u2013186, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nMarco Baroni. 2019. Linguistic generalization and compositionality in modern artificial neural networks. Philosophical Transactions of the Royal Society B, 375.\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA. Association for Computational Linguistics.\\n\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In SIGMOD.\\n\\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale simple question answering with memory networks. ArXiv preprint abs/1506.02075.\\n\\nRuisheng Cao, Su Zhu, Chenyu Yang, Chen Liu, Rao Ma, Yanbin Zhao, Lu Chen, and Kai Yu. 2020. Unsupervised dual paraphrasing for two-stage semantic parsing. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 6806\u20136817, Online. Association for Computational Linguistics.\\n\\nKyunghyun Cho, Bart van Merri\u00ebnboer, Dzmitry Bahdanau, and Yoshua Bengio. 2014. On the properties of neural machine translation: Encoder\u2013decoder approaches. In Proceedings of SSST-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 103\u2013111, Doha, Qatar. Association for Computational Linguistics.\\n\\nJunyoung Chung, Caglar Gulcehre, KyungHyun Cho, and Yoshua Bengio. 2014. Empirical evaluation of gated recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nLi Dong and Mirella Lapata. 2016. Language to logical form with neural attention. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 33\u201343, Berlin, Germany. Association for Computational Linguistics.\"}"}
{"id": "acl-2022-long-422", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-422", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-422", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-422", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6 shows our 27 functions and their explanations. Note that we define specific functions for different attribute types (i.e., string, number, date, and year), because the comparison of these types are quite different. Following we explain some necessary items in our functions.\\n\\nEntities/Entity: Entities denotes an entity set, which can be the output or functional input of a function. When the set has a unique element, we get an Entity.\\n\\nName: A string that denotes the name of an entity or a concept.\\n\\nKey/Value: The key and value of an attribute.\\n\\nOp: The comparative operation. It is one of {=, \u2260, <, >} when comparing two values, one of {greater, less} in SelectBetween, and one of {largest, smallest} in SelectAmong.\\n\\nPred/Dir: The relation and direction of a relation.\\n\\nFact: A literal fact, e.g., (LeBron James, height, 206 centimetre), or a relational fact, e.g., (LeBron James, drafted by, Cleveland Cavaliers).\\n\\nQKey/QValue: The key and value of a qualifier.\\n\\nB Grammar Rules of KoPL\\nAs shown in Table 7, the supported program space of KoPL can be defined by a synchronous context-free grammar (SCFG), which is widely used to generate logical forms paired with canonical questions (Wang et al., 2015a; Jia and Liang, 2016; Wu et al., 2021). The programs are meant to cover the desired set of compositional functions, and the canonical questions are meant to capture the meaning of the programs.\\n\\nC Knowledge Base Extraction\\nSpecifically, we took the entities of FB15k-237 (Toutanova et al., 2015), a popular subset of Freebase, as seeds, and then aligned them with Wikidata via Freebase IDs, so that we could extract their rich literal and qualifier knowledge from Wikidata. Besides, we added 3,000 other entities with the same name as one of FB15k-237 entities, to further increase the difficulty of disambiguation. For the relational knowledge, we manually merged the relations of FB15k-237 (e.g., /people/person-/spouse_s./people/marriage/spouse) and Wikidata. Wikidata provides the Freebase ID for most of its entities, but the relations are not aligned. Finally, we manually filtered out useless attributes (e.g., about images and Wikidata pages) and entities (i.e., never used in triples).\\n\\nD Generation Strategies\\nTable 8 lists the complete generation strategies, including 7 locating and 9 asking strategies. In locating stage we describe a single entity or an entity set with various restrictions, while in asking stage we query specific information about the target entity or entity set.\\n\\nE SPARQL Implementation Details\\nWe build a SPARQL engine with Virtuoso to execute generated SPARQLs. To denote qualifiers, we create a virtual node for those literal and relational triples. For example, to denote the point in time of (LeBron James, drafted by, Cleveland Cavaliers), we create a node _BN which connects to the subject, the relation, and the object with three special edges, and then add (_BN, point in time, 2003) into the graph. Similarly, we use virtual node to represent the attribute value of number type, which has an extra unit. For example, to represent the height of LeBron James, we need (LeBron James, height, _BN), (_BN, value, 206), (_BN, unit, centimetre).\\n\\nF Generation Examples\\nConsider the example of Fig. 2 in Section 4.2 in the main text, following is a detailed explanation.\\n\\nAt the first, the asking stage samples the strategy QueryName and samples Cleveland Cavaliers from the whole entity set as the target entity. The corresponding textual description, SPARQL, and KoPL of this stage is \u201cWho is <E>\u201d, \u201cSELECT ?e WHERE { }\u201d, and \u201cQueryName\u201d, respectively.\\n\\nThen we switch to the locating stage to describe the target entity Cleveland Cavaliers. We sample the strategy, Concept + Relational, to locate it. For the concept part, we sample team from all concepts of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is \u201cteam\u201d, \u201c?e <pred:instance_of>?c . ?c <pred:name>\u201dteam\u201d, and \u201cFilterConcept(team)\u201d, respectively. For the relation part, we sample (LeBron James, drafted by) from all triples of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is \u201cdrafted LeBron James\u201d, \u201c?e_1 <drafted ?e_2\u201d, respectively.\\n\\nG GitHub Implementation Details\\nWe build a GitHub implementation with Virtuoso to execute generated SPARQLs. To denote qualifiers, we create a virtual node for those literal and relational triples. For example, to denote the point in time of (LeBron James, drafted by, Cleveland Cavaliers), we create a node _BN which connects to the subject, the relation, and the object with three special edges, and then add (_BN, point in time, 2003) into the graph. Similarly, we use virtual node to represent the attribute value of number type, which has an extra unit. For example, to represent the height of LeBron James, we need (LeBron James, height, _BN), (_BN, value, 206), (_BN, unit, centimetre).\\n\\nH Generation Examples\\nConsider the example of Fig. 2 in Section 4.2 in the main text, following is a detailed explanation.\\n\\nAt the first, the asking stage samples the strategy QueryName and samples Cleveland Cavaliers from the whole entity set as the target entity. The corresponding textual description, SPARQL, and KoPL of this stage is \u201cWho is <E>\u201d, \u201cSELECT ?e WHERE { }\u201d, and \u201cQueryName\u201d, respectively.\\n\\nThen we switch to the locating stage to describe the target entity Cleveland Cavaliers. We sample the strategy, Concept + Relational, to locate it. For the concept part, we sample team from all concepts of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is \u201cteam\u201d, \u201c?e <pred:instance_of>?c . ?c <pred:name>\u201dteam\u201d, and \u201cFilterConcept(team)\u201d, respectively. For the relation part, we sample (LeBron James, drafted by) from all triples of Cleveland Cavaliers. The corresponding textual description, SPARQL, and KoPL is \u201cdrafted LeBron James\u201d, \u201c?e_1 <drafted ?e_2\u201d, respectively.\\n\\n9 https://github.com/openlink/virtuoso-opensource\"}"}
{"id": "acl-2022-long-422", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G Data Analysis\\n\\nThere are 24,724 unique answers in KQA Pro. We show the top 20 most frequent answers and their fractions in Fig. 5. \\\"yes\\\" and \\\"no\\\" are the most frequent answers, because they cover all questions of type Verify. \\\"0\\\", \\\"1\\\", \\\"2\\\", \\\"3\\\", and other quantity answers are for questions of type Count, which accounts for 11.5%.\\n\\nFig. 6 shows the Program length distribution. Most of our problems (28.42%) can be solved by 4 functional steps. Some extreme complicated ones connection relationship number official isni title the of is number language longer shorter is the a more has one area is person\\ndoes the person city the person the did was person the what which how is when does\\n\\nFigure 7: Distribution of first 4 question words. (1.24%) need more than 10 steps.\\n\\nFig. 7 shows sunburst for first 4 words in questions. We can see that questions usually start with \\\"what\\\", \\\"which\\\", \\\"how many\\\", \\\"when\\\", \\\"is\\\" and \\\"does\\\". Frequent topics include \\\"person\\\", \\\"movie\\\", \\\"country\\\", \\\"university\\\", and etc.\"}"}
{"id": "acl-2022-long-422", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The topic entity of each question was regarded as the topic entity and was fed into the model during both training and testing phase. To adapt to existing knowledge embedding techniques, we added virtual nodes to represent the graph embedding to improve multi-hop reasoning. EmbedKGQA utilizes knowledge embedding to improve multi-hop reasoning.\\n\\nSRN can only handle relational knowledge. The Pro dataset, because its classification layer is more flexible than SRN and can predict answers outside the entity set. The topic entity of each question was...\"}"}
{"id": "acl-2022-long-422", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: SCFG rules for producing KoPL program and canonical question pairs. \\\" | \\\" matches either expression in a group. \\\"?\\\" denotes the expression preceding it is optional. Key_Text, QKey_Text, and Pred_Text denote the annotated template for attribute keys, qualifier keys, and relations. For example, for Pred place of birth, the Pred_Text is \\\"was born in\\\".\\n\\nRGCN. To build the graph, we took entities as nodes, connections between them as edges, and relations as edge labels. We concatenated the literal attributes of an entity into a sequence as the node description. For simplicity, we ignored the qualifier knowledge. Given a question, we first initialized node vectors by fusing the information of node descriptions and the question, then conducted RGCN to update the node features, and finally aggregated features of nodes and the question to predict the answer via a classification layer. Our RGCN implementation is based on DGL, a high performance Python package for deep learning on graphs. Due to the memory limit, we set the graph layer to 1 and set the hidden dimension of nodes and edges to 32.\\n\\nRNN-based KoPL and SPARQL Parsers. For KoPL prediction, we first parsed the question to the sequence of functions, and then predicted textual inputs for each function. We used Gated Recurrent Unit (GRU) (Cho et al., 2014; Chung et al., 2014), a well-known variant of RNNs, as our encoder of questions and decoder of functions. At-\"}"}
