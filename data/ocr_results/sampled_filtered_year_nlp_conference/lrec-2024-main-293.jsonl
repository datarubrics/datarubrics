{"id": "lrec-2024-main-293", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GPT-4 as Question Parser\\n\\nWhen we use GPT-4 to parse a question to its ASP equivalent, we give an example of a question in natural language to ASP representation. The prompt with just one Question-ASP pair is shown below.\\n\\n**Task description**: You are a helpful assistant that converts questions in English into ASP logic language.\\n\\n**Question**: What is the color of the cylinder to the right of the blue sphere?\\n\\n**ASP**:\\n\\n```\\nunknown (Q) :- hasProperty(X, color, Q), hasProperty(X, shape, cylinder), hasProperty(X1, color, blue), hasProperty(X1, shape, sphere), right(X1, X).\\n```\\n\"}"}
{"id": "lrec-2024-main-293", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CLEVR-POC: Reasoning-Intensive Visual Question Answering in Partially Observable Environments\\n\\nSavitha Sam Abraham\u2217, Marjan Alirezaie\u2020, Luc De Raedt\u00a7\\n\\n\u2217Australian Institute for Machine Learning, The University of Adelaide, Australia\\nsavitha.samabraham@adelaide.edu.au\\n\\n\u2020Flybits Labs. TMU Creative AI Hub, Toronto, Canada\\nmarjan.alirezaie@flybits.com\\n\\n\u00a7\u00d6rebro University, Centre for Applied Autonomous Sensor Systems (AASS), \u00d6rebro, Sweden\\nDepartment of Computer Science, KULeuven, Belgium\\nluc.de-raedt@oru.se\\n\\nAbstract\\n\\nThe integration of learning and reasoning is high on the research agenda in AI. Nevertheless, there is only a little attention to use existing background knowledge for reasoning about partially observed scenes to answer questions about the scene. Yet, we as humans use such knowledge frequently to infer plausible answers to visual questions (by eliminating all inconsistent ones). Such knowledge often comes in the form of constraints about objects and it tends to be highly domain or environment-specific. We contribute a novel benchmark called CLEVR-POC for reasoning-intensive visual question answering (VQA) in partially observable environments under constraints. In CLEVR-POC, knowledge in the form of logical constraints needs to be leveraged to generate plausible answers to questions about a hidden object in a given partial scene. For instance, if one has the knowledge that all cups are colored either red, green or blue and that there is only one green cup, it becomes possible to deduce the color of an occluded cup as either red or blue, provided that all other cups, including the green one, are observed. Through experiments, we observe that the low performance of pre-trained vision language models like CLIP (\u224822%) and a large language model (LLM) like GPT-4 (\u224846%) on CLEVR-POC ascertains the necessity for frameworks that can handle reasoning-intensive tasks where environment-specific background knowledge is available and crucial. Furthermore, our demonstration illustrates that a neuro-symbolic model, which integrates an LLM like GPT-4 with a visual perception network and a formal logical reasoner, exhibits exceptional performance on CLEVR-POC.\\n\\nKeywords: LLM and Reasoning, visual question answering, partial observability, logical constraints\\n\\n1. Introduction\\n\\nVisual Question Answering (VQA) has been widely investigated by researchers from various subfields in AI like computer vision and natural language understanding. As a result, we now have access to a vast corpus of VQA datasets coupled with numerous models addressing the task of VQA (Zou and Xie, 2020; Wu et al., 2017). Most existing VQA datasets (Johnson et al., 2017; Antol et al., 2015) have a collection of images paired with questions such that all information required to answer the question is provided in the image, and hence the scene is considered complete. But in real life, we often engage in tasks where scenes may not be completely visible. We instead may have world knowledge about various locations visited by us, acquired over time, that allows us to generate plausible answers to queries about objects we do not see in a scene. For example, in autonomous vehicle scenarios, reasoning is crucial for dealing with partial observability. Comprehensive knowledge of traffic enables the system to interpret limited visual information and make informed decisions, ensuring safe navigation despite occlusions or limited field of view. Furthermore, in factory settings, reasoning combined with background knowledge about the environment can assist teams of robots in dealing with partial observability during navigation and other coordination and cooperation tasks.\\n\\nIn this paper, we introduce a synthetic dataset, CLEVR-POC1, for a reasoning-intensive VQA task set in partially observable scenarios involving external knowledge in the form of constraints. The dataset consists of pairs of an image, representing a partial scene (B in Figure 1a) in some environment (D1 in Figure 1a where the environment is defined by a set of constraints), and a question in natural language about some hidden/missing object (C in Figure 1a). Although in the literature, there exist datasets for QA tasks in partially observable environments (e.g., CLEVR-dialog (Kottur et al., 2019), Visual Dialog (Das et al., 2017), GuessWhat? (DeVries et al., 2017)),\"}"}
{"id": "lrec-2024-main-293", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) The different components in VQA tasks.\\n\\n(b) The different VQA tasks are based on expected inputs and outputs and the number of agents involved.\\n\\nFigure 1: VQA task components and types of VQA tasks\\n\\nThese do not come with additional background knowledge. The challenge presented in CLEVR-POC necessitates the integration of knowledge and multi-step reasoning involving eliminative induction, into perception systems driven by learning. Given that the knowledge associated with a scene typically varies depending on the specific environment involved, it is not a constant across the dataset. It becomes challenging for a learning system to simply memorize this knowledge during training iterations. Moreover, because this knowledge is environment-specific, employing Large Language Models (LLMs) such as GPT as the source of knowledge, as demonstrated in some of the recent works like (Zhou et al., 2023) and (Shah et al., 2023), does not yield favorable results. We substantiate these assertions through empiricalexperiments.\\n\\nThe contributions of this paper are as follows:\\n\\n\u2022 We introduce a dataset, CLEVR-POC, that introduces the task of reasoning-intensive VQA - given a partial scene, the constraints (knowledge) to which the scene conforms and a question about a hidden object in the scene, find the set of all plausible answers.\\n\\n\u2022 We evaluate the performance of state-of-the-art pre-trained vision language and large language models on CLEVR-POC.\\n\\n\u2022 We demonstrate that the synergistic use of LLMs alongside a visual perception network and a formal reasoning system with access to external knowledge can efficiently and effectively address the challenges presented by CLEVR-POC.\\n\\nThe organization of the paper is as follows. Section 2 provides an overview of existing work in VQA, focusing on various VQA datasets and briefly discussing LLMs for reasoning. Section 3 delves into the detailed process of generating CLEVR-POC, while Section 4 outlines the research questions explored in this study. Additionally, this section presents the experiments conducted on CLEVR-POC and the corresponding results.\\n\\n2. Related Work\\n\\nIn this section, we provide an overview of research in two domains - datasets in VQA and LLMs and reasoning.\\n\\n2.1. Datasets in VQA\\n\\nA VQA task may involve various combinations of the different components shown in Figure 1a - a complete scene (A), a partial scene (B), a question (C) about the scene, external knowledge in the form of rules/constraints (D1), or facts in knowledge graphs (D2), and the set of plausible answers to the question (E). Each combination results in a different VQA task (see Figure 1b).\\n\\n2.2. Types of VQA Tasks\\n\\n2.2.1. Task 1\\n\\nGiven a complete scene, and a question about an object in the scene, find the answer to the question. Since the scene is complete, the agent can come up with the exact answer implying that the solution set E has just one element (|E| = 1).\\n\\nDAQUAR (Malinowski and Fritz, 2014), VQA (Antol et al., 2015), CLEVR (Johnson et al., 2017) are datasets in this category.\\n\\n2.2.2. Task 2\\n\\nGiven a complete scene, a question about one of the objects in the scene and external knowledge about objects (in the form of triples-D2), find\"}"}
{"id": "lrec-2024-main-293", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the answer to the question leveraging this external knowledge. FVQA (fact-based VQA) (Wanget al., 2017), and KVQA (knowledge aware VQA) (Shah et al., 2019) are datasets in this category.\\n\\n2.2.3. Task 3\\nWhile the above two tasks involve a single agent being posed with a scene and a question, this category of VQA tasks involves more than one agent. One of the agents has access to the complete scene while the other agent is provided with a partial scene and a question. Answering the question requires the agents to interact with each other. CLEVR-dialog (Kottur et al., 2019), Visual Dialog (Das et al., 2017), and Guess What? (De Vries et al., 2017) are datasets handling Task 3.\\n\\n2.2.4. Task 4 (CLEVR-POC)\\nGiven a partial scene, knowledge in the form of rules (constraints) about the environment to which the scene conforms, and a question about the hidden object in the scene, find the set of all plausible answers to the question. Since the question is about a hidden object (for example, about the shape of the object), it may not be always possible to provide an exact solution. Answering the question is more about eliminating all cases that are inconsistent with the background knowledge (for example: given the knowledge - there are no spheres in this environment) and returning all consistent answers as the solution (the shape is a cone or a cylinder or a cube, which is why $|E| \\\\geq 1$).\\n\\nIn contrast to Task 2, where the knowledge graph encompasses general world facts (e.g., \\\"cows are herbivores\\\"), the knowledge in this context is considerably more specific to an environment. While an LLM can be presumed to possess awareness of the former category of knowledge, the same cannot be said for the latter.\\n\\n2.3. LLMs and Reasoning\\nIn this paper, our emphasis lies on the process of reasoning which depends on a formal system grounded in logical rules and principles. Such a system ensures that all transformations or manipulations of symbols within it, leading to new inferences, adhere to the logical rules and principles governing the system (MacColl, 1897). While LLMs can also be seen as performing symbolic manipulations, these manipulations unlike traditional symbolic reasoning are based on statistical associations or patterns learned from data (Huang and Chang, 2023), because of which it may or may not be logically sound. Despite the progress in the development of large language models (LLMs), many still struggle with a deep understanding of symbols like humans do (Abraham and Alirezaie, 2022; Yao et al., 2022). To address this gap, there are ongoing efforts to create benchmarks (Huang and Chang, 2023), like the proposed CLEVR-POC, to evaluate the reasoning capabilities of LLMs.\\n\\nIn CLEVR-POC, we introduce a VQA task that involves constraint-based reasoning, a form of logical reasoning, where the generated response must satisfy a set of constraints given. These benchmarks are used to assess the capacity of language models in handling symbolic reasoning, contributing to the advancements in the development of more logically sound systems.\\n\\n3. The CLEVR-POC Dataset\\nNow we describe in detail the generation of the CLEVR-POC dataset. The dataset, as the name suggests, is based on the CLEVR (Johnson et al., 2017) dataset, which generated scenes with geometrical shapes. Each object is associated with four attributes - color, shape, material, and size. The objects in CLEVR-POC can have one of the four shapes - cone, sphere, cylinder, and cube, three sizes - large, medium, and small, two materials - rubber and metal, and eight colors - red, blue, green, yellow, gray, brown, and purple. Besides these four attributes, since a scene is divided into four regions (see Figure 1a), CLEVR-POC also associates an object with the region it is in - 0, 1, 2, or 3. Each object belongs to exactly one region. Division of a scene into regions enables the specification of constraints at multiple levels.\\n\\n- **Region-based constraints** - for example, all objects in Region 0 are of shapes cube or cylinder. These constraints must be satisfied by objects in the corresponding region.\\n- **Across-region constraints** - for example, the total number of objects sharing the same color in regions 1 and 2 is not more than 2. These are constraints specified across two regions.\\n- **Generic constraints** - for example, there are at least two cubes in the scene. These constraints apply to the whole scene.\\n\\nOne of the major points of distinction in the scene generation process of CLEVR-POC from the original CLEVR is that the scenes in CLEVR-POC are generated such that they conform to a chosen set of constraints. The steps in creating an instance $i$ in the dataset are:\\n\\n- **Generating an environment** - Environment $i$, defined by a set of constraints.\\n- **Generating a complete scene graph** - Complete $i$, that conforms to Environment $i$.\\n- **Generating the partial scene graph** - Partial $i$ by removing one of the objects, $Obj_i$, from Complete $i$.\"}"}
{"id": "lrec-2024-main-293", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Template-1 (Value Restriction)\\n\\n$\\\\text{object(X), at(X, R'), not hasProperty(X, P1',V1')}$.  \\n\\nTranslation\\n\\nAllobjects at region $R'$ havevalue $V1'$ for the property $P1'$.  \\n\\nAn instantiation\\n\\n$\\\\text{object(X), at(X, 0), not hasProperty(X, color,red)}$.  \\n\\nTemplate-2 (Negation Constraint)\\n\\n$\\\\text{:object(X), at(X, R'), hasProperty(X, P1',V1')}$.  \\n\\nTranslation\\n\\nAllobjects at region $R'$ cannot have value $V1'$ for the property $P1'$.  \\n\\nAn instantiation\\n\\n$\\\\text{object(X), at(X, 0), hasProperty(X, material,metal)}$.  \\n\\nTemplate-3 (Exactly N Constraint)\\n\\n$\\\\text{:#count \\\\{X: hasProperty(X, P1', V1'), object(X), at(X, R')\\\\} ! = N'}$.  \\n\\nTranslation\\n\\nThere are exactly $N'$ objects at region $R'$ with value $V1'$ for the property $P1'$.  \\n\\nAn instantiation\\n\\n$\\\\text{:#count \\\\{X1, X2: sameProperty(X1, X2, P1'), object(X1), object(X2), at(X1, R1'), at(X2, R2')\\\\} < N'}$.  \\n\\nTemplate-4 (Atleast N Constraint)\\n\\n$\\\\text{:#count \\\\{X1, X2: sameProperty(X1, X2, P1'), object(X1), object(X2), at(X1, R1'), at(X2, R2')\\\\} ! < N'}$.  \\n\\nTranslation\\n\\nThere are at least $N'$ pairs of objects at regions $R1'$ and $R2'$ that has the same value $V1'$ for the property $P1'$.  \\n\\nAn instantiation\\n\\n$\\\\text{:#count \\\\{X1, X2: sameProperty(X1, X2, shape), object(X1), object(X2), at(X1,1), at(X2, 2)\\\\} < 1}$.  \\n\\nTemplate-5 (OR Constraint)\\n\\n$\\\\text{object(X), at(X, R'), not hasProperty(X,P1', V1'), not hasProperty(X, P1', V2')}$.  \\n\\nTranslation\\n\\nAllobjects in region $R'$ have value $V1'$ for property $P1'$ or $V2'$ for property $P2'$.  \\n\\nAn instantiation\\n\\n$\\\\text{object(X), at(X, 1), not hasProperty(X, color, yellow), not hasProperty(X, color,blue)}$.  \\n\\nTable 1: A few constraint templates\\n\\n- Generating a question, $Q_i$, about the partial scene with object of interest $Obj_i$.  \\n\\n3.1. Environment Representation\\n\\nAn environment in CLEVR-POC is defined by a set of constraints. We provide a set of 11 constraint templates with CLEVR-POC that are expressed in answer set programming (ASP). Each environment is created by at most 15 different instantiations of these templates, provided there are at least two constraints associated with each region. A few example constraint templates with their translation in English and an instantiation are shown in Table 1. Around 30 different environments are generated (see Appendix A for an example) and the scenes in the dataset belong to one of these 30 environments - the dataset generation process ensures that the scenes are uniformly distributed across the 30 environments.\\n\\n3.2. Scene Representation\\n\\nCLEVR represented as a scene in the form of a scene graph whose nodes represented objects annotated with its attributes and edges denoted the spatial relations (left, right, front, behind) between objects. In CLEVR-POC, besides the scene graph representation, we also represent a scene in ASP. Below we show part of the ASP representation of the partial scene in Figure 1a.\\n\\n%Objects in the scene\\n\\nobject(0). object(1). object(2). object(3).\\n\\n%Attributes of objects\\n\\nat(0, 2).\\n\\nhasProperty(0, color, green).\\n\\nhasProperty(0, size, large).\\n\\nhasProperty(0, material, rubber).\\n\\nhasProperty(0, shape, cylinder).\\n\\n....\\n\\n%Spatial relations between objects\\n\\nfront(1, 0). right(1, 0). ....\\n\\nThe predicate object is used to define the different objects (denoted using identifiers - 0, 1, ..). hasProperty(Object, Attribute, Value) associates a Value for an Attribute of an Object. at(Object, Region) represents the region where the object is located. The spatial relations between objects are represented with predicates left, right, front, behind - for example, left(Object1, Object2) represents that Object2 is located left of Object1.\\n\\n3.3. Image Generation\\n\\nWhile the images in CLEVR are generated from a randomly sampled scene graph, CLEVR-POC generates its images from scene graphs known to adhere to constraints defining an environment. Scene graph creation is thus a reasoning problem - given an environment (constraints in ASP) and a desired number of objects ($n$) in the scene, the goal is to assign each object to one of the four regions and propose values to color, size, shape, and material that are consistent with the\"}"}
{"id": "lrec-2024-main-293", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The pipeline for generating complete and environment scenes involves four steps:\\n\\n1. Environment generation from constraint templates and generating complete scenes satisfying these constraints.\\n2. Partial scene and question generation from a complete scene.\\n3. Using an ASP reasoning engine to solve a problem, assigning a consistent property-value assignment for the objects in the answer set to create a scene graph or a possible configuration of the objects in the scene.\\n4. Sampling a million of these scene graphs for the subsequent image generation phase.\\n\\nNext, the image representing the partial scene is generated from a partial scene graph constructed from the actual scene graph by randomly removing one of the objects from it. The scene graph construction process is shown in Figure 2a.\\n\\n### 3.4 Question Representation\\n\\nThe questions in CLEVR-POC query about one of the four attributes\u2014color, size, shape, and material\u2014of the missing/hidden object in the partial scene. Questions are generated using ASP, ensuring their validity and balance across question types. For example, a question about the color of the other cylinder that is the same material as the medium red thing might be generated as follows:\\n\\n**Question:** What is the color of the other cylinder that is the same material as the medium red thing?\\n\\n**Query:**\\n\\n```\\nquery(Q) :- hasProperty(X, color, Q),\\nhasProperty(X, shape, cylinder),\\nhasProperty(Y, size, medium),\\nhasProperty(Y, color, red),\\nsame_material(Y, X),\\nX != Y.\\n```\\n\\nGiven the query attribute $A \\\\in \\\\{\\\\text{color, size, material, shape}\\\\}$, questions are generated such that the cardinality of the set of possible solutions $|S| \\\\leq |A|$, where $|A|$ is the set of all values for the attribute $A$ (for example, $|\\\\text{size}| = 3 = |\\\\{\\\\text{large, medium, small}\\\\}|$).\\n\\nIf the question generated has $|A|$ solutions (for instance, a solution like, 'size is large or small or medium' is true for any question), it is considered invalid. Questions are balanced across question types (see Appendix B for the distribution). It should be noted that the solution space of CLEVR-POC questions is 16 times that of CLEVR as the solutions expected are not always a single value, but a set of values.\\n\\n### 3.5 Question Generation\\n\\nThe question in CLEVR-POC is generated from the question templates available in CLEVR. We avoid yes/no (existence, comparison) and counting questions and focus on just the attribute querying templates. An example template is as follows:\\n\\n```\\nWhat shape is the <Z2>(size)<C2>(color)<M2>(material)[that is]<R>the <Z>(size)<C>(color)<M>(material)<S>(shape)?\\n```\\n\\nQuestion template instantiation is done based on the complete scene graph of the associated image. The object of interest is always the object that is removed from the complete scene to generate the partial scene graph. The query attribute is picked such that it satisfies the question type balancing requirements. The known attributes of the query object (filling the slots $<Z2>$ or $<C2>$ or $<M2>$ in the above template) are randomly selected. While the filler for the slot $<R>$ (one\"}"}
{"id": "lrec-2024-main-293", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the left, right, front, behind) is randomly picked, the reference object in the query is picked based on the spatial relations available in the complete scene\u2014picking one of the objectsthatare in < R > relation of the query object.\\n\\nThe ASP representations of the question, the incomplete scene, and the constraints in the environment are given to an ASP solver to identify the set of possible values for the query attribute. Figure 2b shows the pipeline of question generation. Refer to Appendix A and B for a detailed example and statistics of CLEVR-POC.\\n\\n4. Experiments\\n\\nThe experiments are designed to answer the following research questions (RQ):\\n\\n\u2022 **RQ1**: How do neural-based vision language models perform on reasoning-intensive VQA tasks (with an emphasis on symbolic knowledge representation and reasoning)?\\n\\n\u2022 **RQ2**: How well do neuro-symbolic vision language architectures handle reasoning-intensive VQA tasks (in the context of mapping raw inputs to symbolic space)?\\n\\n\u2022 **RQ3**: How can we leverage LLMs in reasoning-intensive VQA tasks and what are the challenges associated with it?\\n\\nIn the sections following, we describe the methods implemented to answer these questions.\\n\\n4.1. Methods\\n\\n4.1.1. CLIP-based model\\n\\nCLIP (Contrastive Language Image Pre-training) (Radford et al., 2021) is a vision-language model that is trained to align pairs of text and images to a unified space. We experimented with the CLIP model to investigate RQ1. Figure 3 shows the architecture of a CLIP-based model to solve CLEVR-POC. The pre-trained vision transformer (ViT-B/32) and the text encoders (masked self-attention) in CLIP are leveraged to obtain encodings for the incomplete scene and the question. The encoding for the environment is obtained from its constraints. A pre-trained GPT-2 (Radford et al., 2019) model is used to encode the constraints. As GPT-2 is more language-oriented, we input the natural language version of ASP constraints (while experimenting with ASP-form constraints to assess their impact on performance).\\n\\nThe problem is formulated as a multi-label classification problem where the output is one or more of the following 17 labels - {red, blue, green, yellow, cyan, brown, gray, purple, rubber, metal, large, small, medium, cone, cube, cylinder, sphere}.\\n\\nHence, the three encodings are passed to a multilabel classifier (feed-forward network) which is the only module of the whole model that is trained from scratch. The classifier is trained with a weighted binary cross entropy loss function (Ho and Wookey, 2019) that gives more penalty to the wrong prediction of minority class (as most of the labels in the output are 0, except for the ones in the answer\u2014a false negative is given more penalty).\\n\\nFor each of the 17 labels, the weighted cross entropy loss is thus defined as below:\\n\\n\\\\[\\nWCE(y, \\\\hat{y}) = -\\\\beta y \\\\log(\\\\hat{y}) + (1 - y) \\\\log(1 - \\\\hat{y})\\n\\\\]\\n\\n\\\\(\\\\beta\\\\) is the weight (is set > 1 to penalize false negatives), \\\\(y\\\\) is the ground truth, \\\\(\\\\hat{y}\\\\) is the prediction.\\n\\n4.1.2. Neuro-Symbolic Visual Question Answering\\n\\nThe architecture for the neuro-symbolic approach to solving CLEVR-POC task is shown in Figure 4. The idea is to convert both the image and the question into a unified space as in CLIP, with the difference that this space is symbolic (scene graph and question in ASP). The architecture is based on the state-of-the-art neuro-symbolic approach on the CLEVR dataset, NS-VQA (Yi et al., 2018) and will be used here to study aspects of RQ2. We modify this architecture to include an ASP solver that takes as input the scene in ASP, the question in ASP, and the environment constraints in ASP to derive the answer to the question.\\n\\nThe question parser, (a Bidirectional Long Short Term Memory (BiLSTM) sequence to sequence model) is trained as in NS-VQA using REINFORCE\u2014the reward is positive if the ASP program generated by the parser results in the correct answer, else it is 0. The question parser is initially pre-trained in a fully supervised way with a small sample of (question, ASP program) pairs.\\n\\nThe image perception network in NS-VQA is based on Detectron (Girshick et al., 2018) and it was trained independently of the question parser in a supervised way. The ASP solver used is the same as the one used during the dataset generation phase.\\n\\n4.1.3. LLMs for solving CLEVR-POC\\n\\nLLMs are leveraged in two ways for solving reasoning tasks like CLEVR-POC.\\n\\nLLM as question parser in NS-VQA: In this approach, we use LLM as a question parser\u2014converting the question into a semantic representation like ASP. The image is converted to a scene graph as done in NS-VQA. Both semantic representations are then passed to a formal reasoner like an ASP solver to derive solutions consistent with the constraints.\\n\\nStand-alone LLM: The second approach is to provide both the image description and the question to the LLM for processing.\\n\\nThe results in Section 4.3 are for \\\\(\\\\beta = 5\\\\).\"}"}
{"id": "lrec-2024-main-293", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: CLIP for CLEVR-POC\\n\\nFigure 4: NS-VQA for CLEVR-POC - architecture is updated with an ASP solver along with the constraints (in NL) as input to LLM and generate as a response the consistent solutions. We, here, assume as done in NS-VQA that the scene graphs are accurate, as our focus is on evaluating LLMs' ability to perform symbolic reasoning. CLEVR-POC, a synthetic dataset where environment-specific knowledge is not fixed, can assess LLMs' symbolic reasoning ability without data contamination (where the dataset becomes unusable once it has been exploited).\\n\\nThe LLM used in the experiments is GPT-4 (OpenAI, 2023) (See Appendix C for details about prompts used).\\n\\n4.2. Evaluation\\n\\nLet $A = \\\\{a_1, a_2,..\\\\}$ denote the set of values in the actual answer and $P = \\\\{p_1, p_2,..\\\\}$ denote the predicted answer set. We evaluate the performance of the two approaches on CLEVR-POC using the two metrics based on accuracy.\\n\\n**Exact Accuracy** checks whether the prediction made is exactly accurate, i.e., $A$ is exactly equal to $P$.\\n\\n$$\\\\text{Exact Accuracy}(A, P) = \\\\begin{cases} 1 & \\\\text{if } x \\\\in A \\\\iff x \\\\in P \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases} \\\\quad (2)$$\\n\\n**Jaccard Index** computes the similarity between the actual answer and predicted answer sets as:\\n\\n$$\\\\text{Jaccard Index}(A, P) = \\\\frac{|A \\\\cap P|}{|A \\\\cup P|} \\\\quad (3)$$\\n\\nThe value of Jaccard Index is between 0 (no common elements) and 1 (exact match). It gives some credit for partially correct answers as well.\\n\\n4.3. Results\\n\\nTables 2a and 2b show the results for exact and partial answer accuracies respectively for NS-VQA, CLIP-based models, and stand-alone GPT-4 on CLEVR-POC. While NS-VQA (BiLSTM) uses a BiLSTM trained from scratch as the question parser, NS-VQA (GPT-4) uses pre-trained GPT-4 as the question parser. We experimented with varying dataset sizes - 2000, 6000, and then 12000 instances.\\n\\nIt can be seen that with a multifold increase in the dataset size, there is an improvement in the answer accuracy, but the performance is not satisfactory.\\n\\nRQ1 - CLIP-based model analysis: Since the question is not about some object in the scene, and the set of constraints to be satisfied is also not fixed across the instances in the dataset, it is challenging to learn a mapping from the three inputs.\"}"}
{"id": "lrec-2024-main-293", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset       | NS-VQA (BiLSTM) | NS-VQA (GPT-4) | CLIP-ASP | CLIP-NL | CLIP (no knowledge) |\\n|---------------|-----------------|----------------|----------|---------|-------------------|\\n| Sample Size   |                 |                |          |         |                   |\\n| 2000          | 0.0200          | 0.9250         | 0.0350   | 0.0600  | 0.0500            |\\n| 6000          | 0.1516          | 0.9550         | 0.1500   | 0.1700  | 0.1183            |\\n| 12000         | 0.2308          | 0.9441         | 0.1800   | 0.2283  | 0.1483            |\\n\\n(a) Exact answer accuracies of CLIP, NS-VQA and GPT-4 models on CLEVR-POC.\\n\\n| Sample Size   | PA (after pre-training) | PA (after REINFORCE) | PA (GPT-4) |\\n|---------------|-------------------------|----------------------|------------|\\n| 28 (prompt size) |                         |                      | 0.9250     |\\n| \u2248200          | 0.0512                  |                      | 0.0366     |\\n| \u22481000         | 0.4487                  |                      | 0.0316     |\\n| \u22482000         | 0.5043                  |                      | 0.0314     |\\n\\n(b) Jaccard Index of CLIP, NS-VQA and GPT-4 models on CLEVR-POC.\\n\\nTable 2: Exact accuracies and Jaccard index scores of NS-VQA with BiLSTM and GPT-4 as question parsers, CLIP and GPT-4 on CLEVR-POC. CLIP-NL and CLIP-ASP take constraints in natural language and ASP, respectively. CLIP (no knowledge) is the performance of CLIP without constraints.\\n\\nTable 3: Drop of program accuracies (PA) after REINFORCE and the performance of GPT-4 provided with just 28 (question, ASP program) pairs as prompt.\\n\\nSample Size      PA (after pre-training) | PA (after REINFORCE) | PA (GPT-4) |\\n------------------|-------------------------------|----------------------|------------|\\n28 (prompt size) |                               |                      | 0.9250     |\\n\u2248200             | 0.0512                        |                      | 0.0366     |\\n\u22481000            | 0.4487                        |                      | 0.0316     |\\n\u22482000            | 0.5043                        |                      | 0.0314     |\\n\\nTable 2 shows three sets of results for CLIP. The columns CLIP-NL and CLIP-ASP correspond to instances of CLIP where the constraints are given in natural language and ASP, respectively. It should be noted that CLIP-NL performs better than CLIP-ASP, suggesting that representing symbolic knowledge in natural language may be ideal while incorporating knowledge into neural frameworks for QA. The performance of CLIP on CLEVR-POC when no external knowledge is provided is shown in the column CLIP (no knowledge). Although without the external knowledge CLIP\u2019s performance drops, there is not much of a difference indicating that we need to consider better techniques for incorporating such symbolic constraints into neural frameworks. This points us toward existing neuro-symbolic frameworks.\\n\\nRQ2 - NS-VQA analysis: While neural models failed in symbolic reasoning and incorporating symbolic knowledge into the network, it can be seen that the major challenge faced by neuro-symbolic architectures lies not in reasoning but in mapping image or question to a symbolic representation in the absence of ground truth semantic representations. In our experiments, we focus on language perception while assuming 100% accuracy in image perception. Tackling both perceptions simultaneously is even more formidable without access to ground truth representations. Hence, the poor performance of NS-VQA (see column NS-VQA (BiLSTM) in Tables 2a and 2b) can be solely attributed to the failure of REINFORCE in learning accurate ASP programs.\\n\\nAs mentioned in Section 4.1.2, a BiLSTM is initially pre-trained in a supervised fashion with a few examples. We experimented by varying the number of examples provided for pre-training. Table 3 shows the program accuracy after pre-training with around 200, 1000 and 2000 pairs of <question, ASP program>. When these pre-trained models are further trained with REINFORCE, there is a drastic drop in the program accuracy as the focus of the REINFORCE algorithm is on coming up with the correct answer independent of the program\u2019s accuracy. This fall is observed even with the original CLEVR dataset. The chances of deriving the correct answer even with a wrong program by a fluke are higher in the case of CLEVR compared to CLEVR-POC considering the larger solution space of CLEVR-POC (see Section 3.4). REINFORCE clearly fails to learn ASP programs through weak supervision even when it initiates its training from a proficient model.\\n\\nRQ3 - LLM Analysis: In the first experiment we used GPT-4 as a question parser. The BiLSTM-based question parser of NS-VQA is replaced with GPT-4 (the results are shown in column NS-VQA (GPT-4) in Tables 2a and 2b). The model is provided with just 28 (question, ASP program) pairs of examples as prompts. GPT-4 with no fine-tuning was able to accurately predict the equivalent ASP programs. The stand-alone GPT-4 approach gave less than 50% exact accuracy. The evidence indicates that employing GPT-4 as a question parser to translate the question into an ASP program and subsequently utilizing an ASPreasoning engine leadsto better results compared to placing the entire burden of symbolic reasoning on GPT-4. It should also be noted that GPT-4 with no data-specific training performed better than CLIP and NS-VQA (BiLSTM). There is still room for improvement with...\"}"}
{"id": "lrec-2024-main-293", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We now discuss important challenges that our dataset and work point to.\\n\\n**Reasoning and LLM**: The experiments showed that the direct application of LLMs is not a good solution for such reasoning-intensive tasks. (Mahowald et al., 2023) also discusses the limitations of LLMs in formal reasoning tasks. Our experiments showed that a more appropriate approach to harnessing LLMs involved relieving them of the task of symbolic reasoning and instead employing them for generating symbolic representations. Progressing even further entails discovering mechanisms for seamlessly incorporating specific knowledge into LLMs and generating responses that are consistent with this knowledge.\\n\\n**Symbolic knowledge in visual perception network**: Although the focus of this paper was on language and reasoning, it may be noted that knowledge in the form of constraints in CLEVR-POC can play a significant role during image perception as it can provide hints on what can or cannot be in the image. This is a form of weak supervision which is also required in the absence of ground truth scene graphs to accelerate the learning process. Developing neuro-symbolic models with a stronger feedback mechanism for visual perception, such as DeepProbLog (Manhaeve et al., 2018), NeurASP (Yang et al., 2020), Semantic-Loss (Xu et al., 2018) and LTN (Serafini and Garcez, 2016), would help in faster convergence. The aforementioned frameworks, however, cannot still be applied to VQA tasks due to scalability issues.\\n\\n**Conclusion**: Humans often have to interact with the partially observable environment. In light of the need to deal with the inherent uncertainty in knowledge-rich real-world scenarios, this work aimed to establish a benchmark for evaluating reasoning-intensive VQA in partially observable environments. Applying the benchmark to stand-alone LLMs and other vision-language models yielded disappointing results due to their inability to perform symbolic reasoning. We also demonstrated that combining LLM with a visual perception network and a formal reasoner produced positive results.\\n\\nFuture directions involve developing visual perception networks with knowledge-guided supervision, enhancing LLMs' reasoning capabilities, and moving CLEVR-POC to an embodied setup like vision language navigation.\\n\\n**Acknowledgements**: This research was conducted during the authors' tenure at \u00d6rebro University, Sweden and was financially supported by the Wallenberg AI, Autonomous Systems, and Software Program (WASP).\\n\\n**Bibliographical References**\\n\\nSavitha Sam Abraham and Marjan Alirezaie. 2022. Compositional generalization and neuro-symbolic architectures. In *Combining Learning and Reasoning: Programming Languages, Formalisms, and Representations*.\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, CLawrence Zitnick, and Devi Parikh. 2015. VQA: Visual question answering. In *IEEE international conference on computer vision*, pages 2425\u20132433.\\n\\nO Blender. 2018. Blender\u2014a 3d modelling and rendering package. Retrieved.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. *Advances in neural information processing systems*, 33:1877\u20131901.\\n\\nAbhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. 2018. Embodied question answering. In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 1\u201310.\\n\\nAbhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos\u00e9 MF Moura, Devi Parikh, and Dhruv Batra. 2017. Visual dialog. In *IEEE conference on computer vision and pattern recognition*, pages 326\u2013335.\\n\\nHarm De Vries, Florian Strub, Sarath Chandar, Olivier Pietquin, Hugo Larochelle, and Aaron Courville. 2017. Guesswhat?! visual object discovery through multi-modal dialogue. In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 5503\u20135512.\\n\\nArtur d'Avila Garcez, Marco Gori, Luis C Lamb, Luciano Serafini, Michael Spranger, and Son N Tran. 2019. Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning. *arXiv preprint arXiv:1905.06088*. \"}"}
{"id": "lrec-2024-main-293", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ross Girshick, Ilija Radosavovic, Georgia Gkioxari, Piotr Doll\u00e1r, and Kaiming He. 2018. Detectron.\\n\\nYaoshiang Ho and Samuel Wookey. 2019. The real-world-weight cross-entropy loss function: Modeling the costs of mislabeling. IEEE access, 8:4806\u20134813.\\n\\nJie Huang and Kevin Chen-Chuan Chang. 2023. Towards reasoning in large language models: A survey. In Findings of the Association for Computational Linguistics: ACL 2023, pages 1049\u20131065, Toronto, Canada. Association for Computational Linguistics.\\n\\nJustin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, C Lawrence Zitnick, and Ross Girshick. 2017. Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In IEEE conference on computer vision and pattern recognition, pages 2901\u20132910.\\n\\nIncheol Kim. 2020. Visual experience-based question answering with complex multimodal environments. Mathematical Problems in Engineering, 2020.\\n\\nSatwik Kottur, Jos\u00e9 MFM Moura, Devi Parikh, Dhruv Batra, and Marcus Rohrbach. 2019. Clevr-dialog: A diagnostic dataset for multi-round reasoning in visual dialog. arXiv preprint arXiv:1903.03166.\\n\\nYann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. 1998. Gradient-based learning applied to document recognition. IEEE, 86(11):2278\u20132324.\\n\\nVladimir Lifschitz. 2008. What is answer set programming? In 23rd National Conference on Artificial Intelligence - Volume 3, AAAI'08, page 1594\u20131597. AAAI Press.\\n\\nHugh MacColl. 1897. Symbolic reasoning. Mind, 6(24):493\u2013510.\\n\\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy Kanwisher, Joshua B Tenenbaum, and Evelina Fedorenko. 2023. Dissociating language and thought in large language models: a cognitive perspective. arXiv preprint arXiv:2301.06627.\\n\\nMateusz Malinowski and Mario Fritz. 2014. A multi-world approach to question answering about real-world scenes based on uncertain input. Advances in neural information processing systems, 27.\\n\\nRobin Manhaeve, Sebastijan Dumancic, Angelika Kimmig, Thomas Demeester, and Luc De Raedt. 2018. Deepproblog: Neural probabilistic logic programming. Advances in neural information processing systems, 31.\\n\\nJiayuan Mao, Chuang Gan, Pushmeet Kohli, Joshua B Tenenbaum, and Jiajun Wu. 2019. The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision. arXiv preprint arXiv:1904.12584.\\n\\nGiuseppe Marra, Sebastijan Duman\u010di\u0107, Robin Manhaeve, and Luc De Raedt. 2021. From statistical relational to neural symbolic artificial intelligence: a survey. arXiv preprint arXiv:2108.11451.\\n\\nShoya Matsumori, Kosuke Shingyouchi, Yuki Abe, Yosuke Fukuchi, Komei Sugiura, and Michita Imai. 2021. Unified questioner transformer for descriptive question generation in goal-oriented visual dialogue. In IEEE/CVF International Conference on Computer Vision, pages 1898\u20131907.\\n\\nRui Da Silva Neves, Jean-Fran\u00e7ois Bonnefon, and Eric Raufaste. 2000. Rationality in human non-monotonic inference.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9.\\n\\nLuciano Serafini and Artur d\u2019Avila Garcez. 2016. Logic tensor networks: Deep learning and logical reasoning from data and knowledge. arXiv preprint arXiv:1606.04422.\\n\\nDhruv Shah, Michael Robert Equi, B\u0142a\u017cej Osi\u0144ski, Fei Xia, Sergey Levine, et al. 2023. Navigation with large language models: Semantic guesswork as a heuristic for planning. In 7th Annual Conference on Robot Learning.\\n\\nSanket Shah, Anand Mishra, Naganand Yadati, and Partha Pratim Talukdar. 2019. Kvqa: Knowledge-aware visual question answering. Proceedings of the AAAI Conference on Artificial Intelligence, 33(01):8876\u20138884.\"}"}
{"id": "lrec-2024-main-293", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Peng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Fvqa: Fact-based visual question answering. IEEE transactions on pattern analysis and machine intelligence, 40(10):2413\u20132427.\\n\\nQi Wu, Damien Teney, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2017. Visual question answering: A survey of methods and datasets. Computer Vision and Image Understanding, 163:21\u201340.\\n\\nJingyi Xu, Zilu Zhang, Tal Friedman, Yitao Liang, and Guy Broeck. 2018. A semantic loss function for deep learning with symbolic knowledge. In International conference on machine learning, pages 5502\u20135511. PMLR.\\n\\nZhun Yang, Adam Ishay, and Joohyung Lee. 2020. Neurasp: Embracing neural networks into answer set programming. In 29th International Joint Conference on Artificial Intelligence (IJCAI 2020).\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2022. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629.\\n\\nKexin Yi, Jiajun Wu, Chuang Gan, Antonio Torralba, Pushmeet Kohli, and Josh Tenenbaum. 2018. Neural-symbolic vqa: Disentangling reasoning from vision and language understanding. Advances in neural information processing systems, 31.\\n\\nGengze Zhou, Yicong Hong, and Qi Wu. 2023. Navigpt: Explicit reasoning in vision-and-language navigation with large language models. arXiv preprint arXiv:2305.16986.\\n\\nYeyun Zou and Qiyu Xie. 2020. A survey on vqa: Datasets and approaches. In 2020 2nd International Conference on Information Technology and Computer Application (ITCA), pages 289\u2013297. IEEE.\"}"}
{"id": "lrec-2024-main-293", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: A complete and incomplete scene from CLEVR-POC\\n\\nA. An example from CLEVR-POC\\n\\nComplete and incomplete scene: Figure 5 is an example of a complete scene and the incomplete scene generated from it by hiding the small red rubber sphere.\\n\\nEnvironment Every scene is generated such that it satisfies the constraints in an environment. The following are the general rules shared by all environments in CLEVR-POC.\\n\\n1. property(color, gray).\\n2. property(color, red).\\n3. property(color, blue).\\n4. property(color, green).\\n5. property(color, brown).\\n6. property(color, purple).\\n7. property(color, cyan).\\n8. property(color, yellow).\\n9. property(shape, cube).\\n10. property(shape, cylinder).\\n11. property(shape, sphere).\\n12. property(shape, cone).\\n13. property(size, small).\\n14. property(size, medium).\\n15. property(size, large).\\n16. property(material, rubber).\\n17. property(material, metal).\\n18. region(0).\\n19. region(1).\\n20. region(2).\\n21. region(3).\\n22. right_R(0, 0).\\n23. right_R(0, 1).\\n24. right_R(0, 2).\\n25. right_R(0, 3).\\n26. right_R(1, 0).\\n27. right_R(1, 1).\\n28. right_R(1, 2).\\n29. right_R(1, 3).\\n30. right_R(2, 0).\\n31. right_R(2, 1).\\n32. right_R(2, 2).\\n33. right_R(2, 3).\\n34. right_R(3, 0).\\n35. right_R(3, 1).\\n36. right_R(3, 2).\\n37. right_R(3, 3).\\n38. left_R(R1, R2) :- right_R(R2, R1).\\n39. front_R(0, 0).\\n40. front_R(0, 1).\\n41. front_R(0, 2).\\n42. front_R(0, 3).\\n43. front_R(1, 0).\\n44. front_R(1, 1).\\n45. front_R(1, 2).\\n46. front_R(1, 3).\\n47. front_R(2, 0).\\n48. front_R(2, 1).\\n49. front_R(2, 2).\\n50. front_R(2, 3).\\n51. front_R(3, 0).\\n52. front_R(3, 1).\\n53. front_R(3, 2).\\n54. front_R(3, 3).\\n55. behind_R(R1, R2) :- front_R(R2, R1).\\n56. sameProperty(X1, X2, P) :- hasProperty(X1,P,V),} X1!=X2.\\n57. same_color(X,Y):- sameProperty(X, Y, color).\\n58. same_size(X,Y):- sameProperty(X, Y, size).\\n59. same_shape(X,Y):- sameProperty(X, Y, shape).\\n60. same_material(X,Y):- sameProperty(X, Y, material).\\n61. 1{hasProperty(X, color, V) : property(color, V)}1 :- object(X).\\n62. 1{hasProperty(X, material, V) : property(material, V)}1 :- object(X).\\n63. 1{hasProperty(X, shape, V) : property(shape, V)}1 :- object(X).\\n64. 1{hasProperty(X, size, V) : property(size, V)}1 :- object(X).\\n65. 1{at(X, R): region(R)}1 :- object(X).\\n66. :- sameProperty(X1, X2, color),\\n67. sameProperty(X1, X2, material),\\n68. sameProperty(X1, X2, size),\\n69. sameProperty(X1, X2, shape),\\n70. object(X1), object(X2), X1!=X2.\\n71. exceed_region_capacity(R) :- #count{X: object(X), at(X, R)} >= 4, region(R).\\n72. :- exceed_region_capacity(_).\\n\\nEnvironment's general rules in natural language:\\n\\n1-9. Objects must have 4 properties. They are color, shape, size, and material.\\n\\n1-4. Objects can be in one of the 8 colors. It can be gray, or red, or blue, or green, or brown, or purple, or cyan, or yellow.\\n\\n5-6. Objects can be in one of the 4 shapes. It can be cube, or a cylinder, or a sphere or cone.\\n\\n7-8. Objects can be in one of the 3 sizes. It can be small, medium, or large.\\n\\n9. Objects can be in one of the 2 materials. It can be rubber or metal.\\n\\n10. The scene is divided into 4 regions. They are named 0, 1, 2, 3.\\n\\n11. If there are two objects, the first object is located in region 0 and the second object is to the right of the first object, then the location of the second object is either in region 0, 1, or 2, or 3.\\n\\n12. If there are two objects, the first object is located in region 1 and the second object is to the right of the first object, then the location of the second object is either in region 1, or 3.\\n\\n13. If there are two objects, the first object is located in region 2 and the second object is to the right of the first object, then the location of the second object is either in region 0, 1, 2, or 3.\\n\\n14. If there are two objects, the first object is located in region 3 and the second object is to the right of the first object, then the location of the second object is either in region 1, or 3.\\n\\n15. If there are two objects, the first object is to the right of the second object, then the second object is to the left of the first object.\\n\\n16. If there are two objects, the first object is located in region 0 and the second object is in front of the first object, then the location of the second object is either in region 0, 1, or 2, or 3.\\n\\n17. If there are two objects, the first object is located in region 1 and the second object is in front of the first object, then the location of the second object is either in region 0, 1, or 2, or 3.\\n\\n18. If there are two objects, the first object is located in region 2 and the second object is in front of the first object, then the location of the second object is either in region 2, or 3.\\n\\n19. If there are two objects, the first object is located in region 3 and the second object is in front of the first object, then the location of the second object is either in region 2, or 3.\\n\\n20. If there are two objects, the first object is in front of the second object, then the second object is behind the first object.\\n\\n27-28. Every object must be assigned exactly one value for color.\\n\\n29-30. Every object must be assigned exactly one value for the material.\\n\\n31-32. Every object must be assigned exactly one value for shape.\\n\\n33-34. Every object must be assigned exactly one value for size.\\n\\n35. Every object must be assigned exactly one value for region.\\n\\n36-40. Two different objects cannot have the same values.\"}"}
{"id": "lrec-2024-main-293", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for all the 4 properties.\\n\\n41-43. Every region can have at most 3 objects.\\n\\nThe following constraints in ASP represent the specific environment to which the scene in Figure 5 belongs.\\n\\n44. object(0..4).\\n45. :- object(X), at(X, 0), hasProperty(X, size, large).\\n46. :- object(X), at(X, 0), hasProperty(X, shape, cylinder).\\n47. :- object(X), at(X, 0), hasProperty(X, shape, cone).\\n48. :- object(X), at(X, 1), hasProperty(X, size, small).\\n49. :- object(X), at(X, 1), hasProperty(X, shape, cone).\\n50. :- object(X), at(X, 1), hasProperty(X, material, rubber).\\n51. :- object(X), at(X, 1), hasProperty(X, shape, cube).\\n52. :- object(X), at(X, 2), not hasProperty(X, size, medium).\\n53. :- object(X), at(X, 2), not hasProperty(X, material, metal).\\n54. :- object(X), at(X, 2), hasProperty(X, material, rubber).\\n55. :- object(X), at(X, 2), hasProperty(X, shape, sphere).\\n56. :- object(X), at(X, 2), hasProperty(X, shape, cube).\\n57. :- object(X), at(X, 3), hasProperty(X, size, small).\\n58. :- object(X), at(X, 3), not hasProperty(X, material, metal).\\n59. not hasProperty(X, color, blue).\\n60. :- #count{X1, X2: sameProperty(X1, X2, shape), object(X1), object(X2), at(X1, 3), at(X2, 2), hasProperty(X1, color, yellow), hasProperty(X2, color, yellow)} >= 4.\\n61. :- #count{X1, X2: sameProperty(X1, X2, color), object(X1), object(X2), at(X1, 0), at(X2, 3)} >= 2.\\n\\nThe following is a natural language interpretation of each line of the preceding rules.\\n\\n44. There are 5 objects in the scene.\\n45. There are no large size objects in region 0.\\n46. There are no cylinder shape objects in region 0.\\n47. There are no cone shape objects in region 0.\\n48. There are no small size objects in region 1.\\n49. There are no cone shape objects in region 1.\\n50. There are no rubber material objects in region 1.\\n51. There are no cube shape objects in region 1.\\n52. All objects in region 2 have medium size.\\n53. All objects in region 2 have metal material.\\n54. There are no rubber material objects in region 2.\\n55. There are no sphere shape objects in region 2.\\n56. There are no cube shape objects in region 2.\\n57. There are no small size objects in region 3.\\n58-59. All objects in region 3 have either metal material or blue color.\\n60-63. There are at most 1 pairs of color yellow objects with the same shape in regions 3 and 2 together.\\n64-66. There are at most 0 pairs of objects with the same color in regions 0 and 3 together.\\n\\nQuestion:\\nFor each given incomplete scene, we generate one question about (any property) of the missing object. The following is the question in natural language that is associated with the incomplete scene in Figure 5:\\nThere is another red rubber object that is the same shape as the big purple object; what size is it?\\n\\nThe following is the same question represented in ASP:\\n\\n1. missing(Q) :-\\n2. hasProperty(X,size,Q),\\n3. hasProperty(X,material,rubber),\\n4. hasProperty(X,color,red),\\n5. hasProperty(Y,color,purple),\\n6. hasProperty(Y,size,large),\\n7. X!=Y,\\n8. same_shape(Y,X).\\n\\nAnswer set:\\nThe answer set for the above question that satisfies the constraints in the specified environment is:\\n{small, medium}\\n\\nReasoning Steps:\\nThe reasoning involved in deriving the answer set from the question, the incomplete scene, and the constraints in the specified environment is given below.\\n\\n\u2022 Interpreting each line of the question:\\n  1. What are the possible values for Q such that:\\n  2. Q is size of the missing object,\\n  3. the missing object's material is rubber,\\n  4. the missing object's color is red,\\n  5. the reference object's color is purple,\\n  6. the reference object's size is large,\\n  7. the missing object is not equal to the reference object,\\n  8. the missing object's shape = the reference object's shape.\\n\\n\u2022 Inferring the missing object's properties:\\n  from the scene graph:\\n  8. the reference object's shape is a sphere.\\n  9. => The missing object's shape is also a sphere.\\n  10. => The missing object is a red rubber sphere.\\n\\n\u2022 Inferring the missing object's possible regions based on the rules listed as the Environment's constraints:\\n  Among the four regions:\\n  A red rubber sphere CAN be located at region 0, as none of the constraints in lines 45-47 is violated.\\n  A red rubber sphere CAN'T be located in region 1, as it violates the constraint about the material at line 50.\\n  A red rubber sphere CAN'T be located in region 2, as it violates the constraints in lines 53, 54, and 55.\\n  A red rubber sphere CAN'T be located in region 3, as it violates the constraints in lines 58-59.\\n  => The missing red rubber sphere is located at region 0.\\n\\n\u2022 Inferring the possible answerset for the property of interest w.r.t the inferred location of the missing object:\"}"}
{"id": "lrec-2024-main-293", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are 3 possible values for the size property: small, medium, large. The environment constraint at line 45 discards the large size for region 0. => The possible answer set for Q is: small, medium.\\n\\nB. Dataset Statistics\\n\\nDistribution across question templates: Figure 6 (a) shows the distribution of questions across different question templates. Six templates present in the original CLEVR dataset are used in CLEVR-POC.\\n\\nDistribution of query attributes with number of objects in the scene: Figure 6 (b) shows the distribution of questions of a specific type based on the number of objects in the scene.\\n\\nDistribution across question types: The type of question asked depends on the attribute of the object that is being inquired about. The generation process enables the user to have control over this distribution. For instance, when generating the specific dataset that was used in the experiments, we established the following criteria: 40% of the questions pertain to the color attribute, another 40% focus on the shape attribute, 10% address the size attribute, and the remaining 10% relate to the material attribute. We made this selection based on the observation that attributes like color and shape encompass a larger set of values (8 values for color and 4 for shape) in comparison to material (which has just two values). Consequently, the solution space for questions centered around color is more extensive than that for material, resulting in a more diverse solution space for the dataset. Figure 7 (a) displays the question type distribution of the dataset generated based on this setting.\\n\\nDistribution across solutions: Figure 7 (b), (c), (d), and (e) illustrate the distribution of potential solutions for various question types: size, shape, material, and color, respectively. We aim for a balanced distribution, avoiding a situation where the majority of questions lead to the same set of answers. For instance, when a question pertains to the size of an object, its possible solutions could be one of {large, medium} or {large, small} or {small, medium} or {large}, or {medium} or {small} as depicted in Figure 7 (b). Since the possible solutions for questions with query attribute color are large (as color can take 8 values), the entire space is not listed in Figure 7 (e). However, it can be seen that the distribution is not favoring any specific solution.\\n\\nC. Prompts for Language Model\\n\\nC.1. Stand-alone GPT-4 to solve CLEVR-POC\\n\\nThe format of the prompt provided to GPT-4 when employing it to solve CLEVR-POC is shown below. The prompt contains the task description, the scene description, the constraints or knowledge associated with the scene, the question about the scene, and the answer. The prompt contains two such examples.\\n\\nTask description: You are a helpful assistant who answers questions about hidden objects based on scene description and the constraints in the scene.\"}"}
{"id": "lrec-2024-main-293", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: (a) Distribution of question types. (b) Distribution of solutions for questions with query attribute size. (c) Distribution of solutions for questions with query attribute material. (d) Distribution of solutions for questions with query attribute shape. (e) Distribution of solutions for questions with query attribute color. Since the solution space of these questions is larger (>100), it is not listed here.\\n\\nThe scene graph is in JSON format with the following keys. The key objects contain a list of objects present in the scene. Each object has various attributes like material, color, shape, size, and region. The key relationships hold information about the spatial relationships between objects in the scene. It contains sub-fields like \\\"front,\\\" \\\"right,\\\" \\\"left,\\\" and \\\"behind,\\\" each associated with a list of object indices representing objects that have that specific relationship with another object. For example, relationships \\\\[\\\\text{\\\\{front\\\\}}\\\\] \\\\[\\\\text{\\\\{0\\\\}}\\\\] refers to the objects that are in front of the object at index 0.\\n\\nScene Observed: The following is the scene graph:\\n\\n```json\\n{\\n  \\\"objects\\\": [\\n    {\\n      \\\"material\\\": \\\"metal\\\",\\n      \\\"color\\\": \\\"red\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"0\\\",\\n      \\\"shape\\\": \\\"cube\\\"\\n    },\\n    {\\n      \\\"material\\\": \\\"metal\\\",\\n      \\\"color\\\": \\\"gray\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"3\\\",\\n      \\\"shape\\\": \\\"sphere\\\"\\n    },\\n    {\\n      \\\"material\\\": \\\"metal\\\",\\n      \\\"color\\\": \\\"brown\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"1\\\",\\n      \\\"shape\\\": \\\"sphere\\\"\\n    },\\n    {\\n      \\\"material\\\": \\\"rubber\\\",\\n      \\\"color\\\": \\\"gray\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"3\\\",\\n      \\\"shape\\\": \\\"sphere\\\"\\n    },\\n    {\\n      \\\"material\\\": \\\"metal\\\",\\n      \\\"color\\\": \\\"red\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"0\\\",\\n      \\\"shape\\\": \\\"sphere\\\"\\n    },\\n    {\\n      \\\"material\\\": \\\"rubber\\\",\\n      \\\"color\\\": \\\"red\\\",\\n      \\\"size\\\": \\\"medium\\\",\\n      \\\"region\\\": \\\"2\\\",\\n      \\\"shape\\\": \\\"sphere\\\"\\n    }\\n  ],\\n  \\\"relationships\\\": {\\n    \\\"left\\\": [[4], [0, 2, 4, 5], [0, 4, 5], [0, 1, 2, 4, 5], [], [0, 4]],\\n    \\\"front\\\": [[1, 3, 4, 5], [5], [0, 1, 3, 4, 5], [1, 5], [1, 3, 5], []],\\n    \\\"behind\\\": [[2], [0, 2, 3, 4], [], [0, 2, 4], [0, 2], [0, 1, 2, 3, 4]],\\n    \\\"right\\\": [[1, 2, 3, 5], [3], [1, 3], [], [0, 1, 2, 3, 5], [1, 2, 3]]\\n  }\\n}\\n```\\n\\nConstraints: The scene contains several visible objects, and has one additional object that is hidden. Objects must have 4 properties. They are color, shape, size, and material. The scene must conform to the following constraints.\\n\\nObjects can be in one of the 8 colors. It can be gray, or red, or blue, or green, or brown, or purple, or cyan, or yellow.\\n\\nObjects can be in one of the 4 shapes. It can be a cube, cylinder, sphere, or cone.\\n\\nObjects can be in one of the 3 sizes. It can be...\"}"}
{"id": "lrec-2024-main-293", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3312 small, medium, or large.\\n\\nObjects can be in one of the 2 materials. It can be rubber or metal.\\n\\nThe scene is divided into 4 regions. They are named 0, 1, 2, 3.\\n\\nIf there are two objects and the first object is located in region 0 and the second object is to the right of the first object, then the location of the second object is either in region 0, 1, 2, or 3.\\n\\nIf there are two objects and the first object is located in region 1 and the second object is to the right of the first object, then the location of the second object is either in region 1, or 3.\\n\\nIf there are two objects and the first object is located in region 2 and the second object is to the right of the first object, then the location of the second object is either in region 0, 1, 2, or 3.\\n\\nIf there are two objects, the first object is located in region 3 and the second object is to the right of the first object, then the location of the second object is either in region 1, or 3.\\n\\nIf there are two objects, the first object is to the right of the second object, then the second object is to the left of the first object.\\n\\nIf there are two objects, the first object is located in region 0 and the second object is in front of the first object, then the location of the second object is either in region 0, 1, 2, or 3.\\n\\nIf there are two objects, the first object is located in region 1 and the second object is in front of the first object, then the location of the second object is either in region 0, or 1, or 2, or 3.\\n\\nIf there are two objects, the first object is located in region 2 and the second object is in front of the first object, then the location of the second object is either in region 2, or 3.\\n\\nIf there are two objects, the first object is located in region 3 and the second object is in front of the first object, then the location of the second object is either in region 2, or 3.\\n\\nIf there are two objects, the first object is in front of the second object, then the second object is behind the first object.\\n\\nEvery object must be assigned exactly one value for color.\\n\\nEvery object must be assigned exactly one value for material.\\n\\nEvery object must be assigned exactly one value for shape.\\n\\nEvery object must be assigned exactly one value for size.\\n\\nEvery object must be assigned exactly one value for region.\\n\\nTwo different objects cannot have the same values for all the 4 properties.\\n\\nEvery region can have at most 3 objects.\\n\\nThere are 6 objects in the scene.\\n\\nThere are at least 1 pair of color red objects with the same size in regions 0 and 2 together.\\n\\nThere are no small-size objects in region 0.\\n\\nThere are no cone-shaped objects in region 0.\\n\\nThere are no purple color objects in region 0.\\n\\nThere are no blue color objects in region 0.\\n\\nThere are no cylinder shape objects in region 1.\\n\\nThere are no cyan color objects in region 1.\\n\\nThere are no rubber material objects in region 1.\\n\\nThere are at least 1 pair of material metal objects with the same size in regions 0 and 3 together.\\n\\nThere are no metal material objects in region 2.\\n\\nThere are no large-size objects in region 2.\\n\\nThere are at least 1 pair of size medium objects with the same shape in regions 1 and 3 together.\\n\\nThere are no red color objects in region 3.\\n\\nThere are no cube-shaped objects in region 3.\\n\\nThere are at least 1 pair of objects with the same material in regions 0 and 1 together.\\n\\nThere are at least 1 pair of color gray objects with the same size in regions 3 and 2 together.\\n\\nQuestion: Answer the following question about the hidden object. The solution should satisfy the constraints. The other cylinder that is the same material as the medium red thing is what color?\\n\\nAnswer: Gray\"}"}
