{"id": "lrec-2024-main-850", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"KGConv, a Conversational Corpus grounded in Wikidata\\n\\nQuentin Brabant, Lina M. Rojas-Barahona, Gw\u00e9nol\u00e9 Lecorv\u00e9, Claire Gardent\\n\\n(1) Orange Innovation\\n2 Avenue Pierre Marzin, Lannion, France.\\n{quentin.brabant, gwenole.lecorve, linamaria.rojasbarahona}@orange.com\\n(2) CNRS/LORIA, Universit\u00e9 de Lorraine\\nNancy, France.\\nclaire.gardent@loria.fr\\n\\n(*) equal contribution\\n\\nAbstract\\n\\nWe present KGConv, a large corpus of 71k English conversations where each question-answer pair is grounded in a Wikidata fact. The conversations were generated automatically: in particular, questions were created using a collection of 10,355 templates; subsequently, the naturalness of conversations was improved by inserting ellipses and coreference into questions, via both handcrafted rules and a generative rewriting model. The dataset thus provides several variants of each question (12 on average), organized into 3 levels of conversationality. We provide baselines for the task of Knowledge-Based Conversational Question Generation. KGConv can further be used for other generation and analysis tasks such as single-turn question generation from Wikidata triples, question rewriting, question answering from conversation or from knowledge graphs and quiz generation.\\n\\nKeywords: Dialogue, Knowledge Base, Conversational Question Generation\\n\\n1. Introduction\\n\\nUnlike open domain and task-oriented dialogues, information seeking conversations are driven by the desire to acquire or evaluate knowledge. These conversations are central for instance, in educational (tutoring a student about a given topic by asking her a set of questions about that topic) and entertainment (quizzes) settings. As large knowledge graphs such as Wikidata have started to emerge, recent years have seen an increasing interest in developing datasets and conversational question answering models that can support such information seeking interactions by grounding conversations in factual data. However, these often focus on question answering (QA) (Saha et al., 2018) or provide datasets of restricted size and variety however (Christmann et al., 2019; Lecorv\u00e9 et al., 2022).\\n\\nIn this paper, we focus on information seeking conversations where, as illustrated in Table 1, each question-answer turn is grounded in a single fact. Our contribution is two fold. First, we make available the KGConv dataset online where each question-answer pair is grounded in a Wikidata fact. To create a diverse, large scale dataset (70k conversations), we develop conversations for eight different topics (Country, Food, Person, Religion/Ideology, Space Object, Taxon, Molecular Entity, and Historical Event). One originality of KGConv is that it provides each question in several versions, each version belonging to one contextuality level: C0, C1 or C2: (i) C0 corresponds to questions whose interpretation is independent of the previous turns, (ii) C1 corresponds to C0 questions into which some entity names are replaced by pronouns or alternative labels using a rule based approach and (iii) C2 corresponds to questions obtained by feeding C1 questions to a T5 model trained for rewriting questions into a more conversational form. Table 1 illustrates the notion of contextuality level with an excerpt of a KGConv conversation.\\n\\nOur second contribution is to establish some baselines for Knowledge-Based Conversational Question Generation (CQG), the task of generating a question given both a Knowledge graph (KG) fact and a conversational context. While much previous work has focused on Knowledge-Based, conversation Question Answering (Saha et al., 2018; Perez-Beltrachini et al., 2023) or on context-independent, knowledge-based question generation (Bordes et al., 2015; Elsahar et al., 2018; Han et al., 2022), we provide a first investigation of how knowledge-based question generation interacts with conversational context. We report results using both automatic metrics and human evaluation.\\n\\n2. Related Work\\n\\nQuestion generation from RDF triples is addressed in (Bordes et al., 2015; Elsahar et al., 2018; Han et al., 2022) and from small KGs depicting multi-\"}"}
{"id": "lrec-2024-main-850", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Triples\\n\\nQ1: NGC 4833, part of, Milky Way\\nA1: Milky Way\\n\\nQ2: NGC 4833, discoverer or inventor, Nicolas Louis de Lacaille\\nA2: Nicolas Louis de Lacaille\\n\\nQ3: Nicolas Louis de Lacaille, religion or worldview, Catholic Church\\nA3: Catholic Church\\n\\nContextuality level 0 (C0)\\n\\nQ1: NGC 4833 is part of what astronomical object?\\nA1: Milky Way\\n\\nQ2: What was the name of the discoverer of NGC 4833?\\nA2: Nicolas Louis de Lacaille\\n\\nQ3: What was his religion?\\nA3: Catholic Church\\n\\nContextuality level 1 (C1)\\n\\nQ1: NGC 4833 is part of what astronomical object?\\nA1: Milky Way\\n\\nQ2: Who discovered this object?\\nA2: Nicolas Louis de Lacaille\\n\\nQ3: What was his religion?\\nA3: Catholic Church\\n\\nContextuality level 2 (C2)\\n\\nQ1: NGC 4833 is part of what astronomical object?\\nA1: Milky Way\\n\\nQ2: Who discovered this object?\\nA2: Nicolas Louis de Lacaille\\n\\nQ3: What was his religion?\\nA3: Catholic Church\\n\\nTable 1: Excerpt of a conversation at the triple, C0, C1, and C2 levels. The C0-C1-C2 variants of a question \\\\( Q_i \\\\) are based on the same template. The root entity is NGC 4833, from the theme \u201cspace object\u201d.\\n\\nHop questions (Serban et al., 2016; Kumar et al., 2019; Bi et al., 2020) and recently in LC-QuAD 2.0 (Dubey et al., 2019) and ParaQA (Kacupaj et al., 2021). However, these works are limited to the generation of isolated questions, thus no conversational context is under consideration.\\n\\nCoQA (Reddy et al., 2019), QuAC (Choi et al., 2018) and Wizard-of-Wikipedia (Dinan et al., 2019) are conversational QA corpora, in which answers are extracted from paragraphs, instead of KGs. Similar to KGConv, ConvQuestions (Christmann et al., 2019) and CSQA (Saha et al., 2018) are conversational corpora based-on structured knowledge. However, the former does not provide the triples for the questions and contains only 315 distinct conversations. The latter, despite being a very large dataset covering a wide range of question types (4), contains rather unnatural questions with a proprietary formalism which does not directly correspond to Wikidata triples. Recent work proposes to generate questions from SPARQL queries, especially to express complex questions (Lecorv\u00e9 et al., 2022; Perez-Beltrachini et al., 2023). However, manually annotating questions with SPARQL queries is difficult and the authors turn on a small set of 350 reference questions. In contrast, our dataset focuses on triples and simple questions which enables a large and varied set of reference questions, obtained through manually written verbalization templates.\\n\\nAlthough OpenDialKG (Moon et al., 2019) also provides conversations grounded in KG, conversations in this corpus are free-form. Additionally, mentions are grounded by entities while conversation transitions are grounded by properties in the KGs. In contrast, KGConv, contains tutoring conversations wherein every question-answer pair is grounded by a fact represented by a KG triple. A key limitation of OpenDialKG however is that dialogues are linked to FreeBase, a Knowledge Base which is no longer publicly available. Finally, our work capitalizes on two existing corpora which provide a correspondence between triples and questions, namely SimpleQuestions (Bordes, Antoine and Usunier, Nicolas and Chopra, Sumit and Weston, Jason) and ZeroShot Relation Extraction (Levy et al., 2017). In comparison, KGConv extends the question templates from these corpora by proposing 3,879 new templates, and focusing on 458 properties. Furthermore, while these two corpora focus on isolated questions or short follow-up turns of maximum three turns, KGConv contains 8 turns per conversation on average.\\n\\n3. Overview of the Dataset\\n\\nIn KGConv, each conversation is focused on a given root entity. As illustrated in Table 1, the first question bears directly on this root entity, while further questions explore new facts about any entity discovered during the conversation (including the root entity itself). Hence, a conversation can be seen as a small evolving KG, where each turn expands the conversation graph with a new entity and the property which connects it to the existing graph.\\n\\nFor each root entity, three conversations are derived from Wikidata in order to increase the diversity of the dataset. The corpus covers eight themes: Country, Food, Person, Religion/Ideology, Space Object, Taxon, Molecular Entity, and Historical Event. The theme of a conversation corresponds to\\n\\nA taxon is a population, or group of populations of biological organisms, e.g. lions or dinosaurs.\"}"}
{"id": "lrec-2024-main-850", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the Wikidata class associated to the root entity e.g., Person corresponds to the Q215627 class in Wiki-\\ndata. Table 2 summarizes the size of the dataset for each theme. We use Taxon and Space Object as unseen themes, which means they are not seen at training time. Our test data also include a set wherein each conversation contains at least one of 85 \u201cunseen\u201d properties that do not appear in the train and dev data. These properties are referred as \u201cwith unseen prop\u201d in Table 2. The number of questions in a conversation is at least 5, at most 19 and 8.6 on average. In total, KGConv has 70,596 conversations including 603,905 questions about 63,345 distinct Wikidata entities and 458 properties.\\n\\nTo enable links with Wikidata and further exten-\\nsions, Wikidata IDs are provided for all entities and properties along with their natural language labels. In addition, each question in a conversation has 6.8 paraphrases on average, and each paraphrase has three versions that correspond to three levels of contextualization:\\n\\n\u2022 C0: this version of a question is produced au-\\ntomatically from a KG fact independent of the conversation context.\\n\u2022 C1: this version is derived from the C0 version taking the context into account and using rules to substitute repetitions with anaphoric forms.\\n\u2022 C2: this version is derived from the C1 ver-\\nsion by applying a generative model trained to rewrite questions.\\n\\n4. Data Collection\\n\\nThe process for creating the corpus is summarized in Figure 1 and elicited in the following subsections.\\n\\nFigure 1: Data flow diagram of the dataset creation.\\n\\n4.1. Sequence Extraction\\n\\nIn this step, we extract sequences of triples from Wikidata which will be used to ground the conver-\\nsations.\\n\\nWe first extract triples in which:\\n\\n1. the subject has at least one English label and the object either has an English label or is a literal (string, number, date, etc.);\\n2. the property belongs to a pre-defined set of Wikidata properties which excludes \u201cuninter-\\ntesting\u201d properties. In particular, we excluded properties whose prefix is not \u201cwdt:\u201d (to avoid triples that provide meta-information), and properties which link entities to their ID in some other database.\\n\\nFor any triple \\\\((s, p, o)\\\\) in this set, we then create the reversed triple \\\\((o, -p, s)\\\\) where \\\\(-p\\\\) denotes the inverse of property \\\\(p\\\\). In this way, if our subgraph contains \\\\((\\\\text{France}, \\\\text{capital}, \\\\text{Paris})\\\\), it also contains \\\\((\\\\text{Paris}, \\\\text{capital}, \\\\text{France})\\\\) which permits creating ques-\\ntions about both the subject \\\\(s\\\\) and the object \\\\(o\\\\) e.g., \u201cWhat is the capital of France\u201d and \u201cParis is the capital of which country?\u201d. We call the resulting Wikidata subgraph \\\\(W\\\\) for World.\\n\\nBased on this set of triples, we then create se-\\nquences of triples as follows. Each conversation will focus on triples in the neighborhood \\\\(N(r)\\\\) of a root entity \\\\(r\\\\) in \\\\(W\\\\). This neighborhood is defined as the subgraph of \\\\(W\\\\) containing \\\\(r\\\\) and all nodes (i.e. entities) that are 1 or 2 edges (i.e. properties) away from \\\\(r\\\\); in other words, \\\\(N(r)\\\\) contains all triples of the form \\\\((r, p, o_1)\\\\) and their successors of the form \\\\((o_1, q, o_2)\\\\).\\n\\nRoots were sampled from instances of the 8 themes displayed in Table 2, with the con-\\ntition that their neighborhood is large enough (at least 20 triples) to generate 3 reasonably long con-\\nversations with enough differences.\\n\\nFor each root, 3 triple sequences of the form \\\\((t_0, t_1, \\\\ldots, t_n)\\\\) were built iteratively by picking triples from \\\\(N(r)\\\\) in a greedy stochastic process. At each step of the process, the subject of the chosen triple is either the root (i.e. \\\\(s_i = r\\\\)) or an entity, either subject or object, from the previous triple (\\\\(s_i = s_{i-1}\\\\) or \\\\(s_i = o_{i-1}\\\\)). Additionally, a triple cannot appear twice in the sequence. The decision to stop or continue the process is made at each time step \\\\(i\\\\) following a probability that increases with \\\\(i\\\\):\\n\\n\\\\[\\n\\\\Pr(\\\\text{stop}(i)) = 0.06^i - 0.18\\n\\\\]\\n\\n4.2. Verbalization\\n\\nQuestions are generated using templates like \u201cWhat is the capital of ___?\u201d, where the slot is to be filled by the subject of a triple. Each template \\\\(\\\\tau\\\\) is applicable for a given property \\\\(\\\\tau(p)\\\\) (e.g.,\u201dcapital of\u201d), given the required types \\\\(\\\\tau(S)\\\\) for the subject slot and\"}"}
{"id": "lrec-2024-main-850", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Quantitative summary of KGConv. Entities, properties and triples can appear in several conversations and several themes but are only counted once. The two last columns show the average number of templates used in a single question-turn, and the number of distinct references (including C0, C1 and C2 versions of all template-based verbalizations).\\n\\nThe required types $\\\\tau$ for the object, which will be the answer. This applicability condition on $\\\\tau$ is denoted as $C(\\\\tau) = (p, S, O)$. Then, a triple $(s, p, o)$ satisfies $C(\\\\tau)$ if:\\n\\n- $p = p$,\\n- $S \\\\subseteq \\\\text{types}(s)$, and\\n- $O \\\\subseteq \\\\text{types}(o)$.\\n\\nTo create a large number of diverse questions for all properties in $W$, we gathered templates from three sources. Table 3 summarizes the number of templates and their sources. The following sections provide details on the methods used to get templates from each source.\\n\\n4.2.1. Zero-Shot templates\\n\\nThe Zero-Shot dataset (Levy et al., 2017) contains 1,192 question templates spanning 120 Wikidata properties. Each template $\\\\tau$ was originally created to ask a question for a given property $p$, regardless the type of the subject and the object. Templates grounded on properties that were no longer in Wikidata were discarded. For the remained templates, we defined the applicability condition as $C(\\\\tau) = (p, \\\\emptyset, \\\\emptyset)$.\\n\\n4.2.2. Simple questions v2 templates\\n\\nWe automatically extracted templates from the SimpleQuestions_v2 dataset (Bordes et al., 2015), which contains 108k triple-question pairs, involving 131k distinct entities and 1,837 properties. As SimpleQuestions_v2 is based on the Freebase KG, we translated entities and properties into their Wikidata counterpart: we relied both on the Wikidata property P646, that links Wikidata entities to their Freebase counterpart, and on an available mapping between Freebase and Wikidata properties. This allowed us to get Wikidata counterparts for 83,447 entities and 142 properties.\\n\\nWe then extracted templates from question-triple pairs whose triple could be translated to Wikidata. For each such triple-question pair $(s, p, o, q)$, we created a template $\\\\tau$ by replacing in $q$ the label of $s$ by an empty slot. The applicability condition of $\\\\tau$ was then defined as $C(\\\\tau) = (p, \\\\text{types}(s), \\\\text{types}(o))$, where $\\\\text{types}(x)$ is the set of all types of $x$ in Wikidata. Since only a small subset of Freebase entity and triples could be translated into a Wikidata counterpart, many triples were filtered out, so the extracted templates only cover 77 of our properties.\\n\\n4.2.3. New templates\\n\\nWe manually created additional templates in three steps: (1) extracting applicability conditions, (2) writing templates corresponding to these conditions, (3) validating written templates.\\n\\nStep 1: Extracting applicability conditions. From the neighbourhoods of potential roots of all themes, we gathered a set $\\\\{ (s_i, p_i, o_i) \\\\}_{N_i=0}$ of Wikidata triples for which we had no template corresponding to $p_i$. From this set of triples we generated a set of applicability conditions $\\\\{ (p_i, \\\\text{types}(s_i), \\\\text{types}(o_i)) \\\\}_{N_i=0}$ to be annotated with corresponding templates. This resulted in many applicability conditions, with overly specific subject and object types for each property. We solved this problem by iteratively merging conditions with the same property, via semi-automated conceptual agglomerative clustering process. Merging two conditions $(p, S_i, O_i)$ and $(p, S_j, O_j)$ consists in replacing them with a new one $(p, S_i \\\\cap S_j, O_i \\\\cap O_j)$, which necessarily is matched by more triples than the two original ones. At the end of this process, applicability conditions that were met by less than 5 triples were discarded.\"}"}
{"id": "lrec-2024-main-850", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Properties Temp. Temp. per prop.\\n\\nSimpleQuestions 77 5,817 1 / 76 / 453\\nZeroShot RE 75 771 1 / 10 / 31\\nNew templates 413 3,879 1 / 9 / 80\\nTotal 474 10,355 1 / 22 / 453\\n\\nTable 3: Statistics on properties and templates for each source of templates.\\n\\nStep 2: Template writing.\\nThe next step was to write, for each applicability condition \\\\( C \\\\), templates that would apply to any triple matching \\\\( C \\\\).\\n\\nThree students of an NLP Master program were hired to annotate the question templates. They were native English speakers hired on a short term contract to perform various annotation tasks for NLP. They were paid slightly above the national minimum wage. We provided them with an annotation tool, in which one applicability condition at a time was displayed, along with 5 examples of matching triples. While annotators were writing templates, their results on these triples were displayed.\\n\\nTo speed up this process, artificially generated templates were also proposed to the annotators, who could accept or reject them. Afterward, accepted artificial templates were treated in the same way as those written by a human annotator.\\n\\nStep 3: Template validation.\\nTo ensure the quality of the resulting templates, all of them were manually filtered by the authors.\\n\\n4.3. Contextualization\\nApplying the templates from Section 4.2 to the triple sequences from Section 4.1 yields conversations where questions are not contextualized (\\\\( C_0 \\\\)). To improve the naturalness of the conversations, we derived two in-context versions from these \\\\( C_0 \\\\) conversations. The first one, (\\\\( C_1 \\\\)), is obtained by applying hand-crafted rules to introduce coreferences and correct some errors produced during verbalization with templates. The second version (\\\\( C_2 \\\\)), results from rewriting \\\\( C_1 \\\\) questions with a T5 model trained to rewrite questions.\\n\\n4.3.1. Post-processing conversations (\\\\( C_1 \\\\) variants)\\nReferring expressions.\\nIn Wikidata, an entity can have several labels: one of those is called \\\"preferred label\\\" and is meant to be used by default. In the \\\\( C_0 \\\\) version, entities are always referred by their preferred label. This step introduces variability by replacing some of the preferred labels with other available labels from Wikidata, according to the following rules: (1) the first reference to an entity in the conversation is the preferred label, or contains it as a substring; (2) further mentions are labels that are substrings of the first reference. For instance, the entity Q9592 has the preferred label \\\\( l_1 = \\\\text{\\\"Catholic Church\\\"} \\\\), alternative labels \\\\( l_2 = \\\\text{\\\"Roman Catholic Church\\\"} \\\\) and \\\\( l_3 = \\\\text{\\\"Roman Apostolic Catholic Church\\\"} \\\\); if \\\\( l_2 \\\\) is used as the first reference to the entity, next references will use either \\\\( l_1 \\\\) or \\\\( l_2 \\\\) but not \\\\( l_3 \\\\), since it is not a substring of \\\\( l_2 \\\\). Whenever the subject is a person with a name and surname, we include its surname in the set of available labels.\\n\\nDeterminers.\\nDeciding which label should be preceded by \\\"the\\\" is not trivial. For example, \\\"United Kingdom\\\" and \\\"Republic of China\\\" require it, while \\\"France\\\" and \\\"China\\\" do not. This step handles this problem by asking a BERT language model to fill a mask token inserted before the label; when \\\"the\\\" was predicted with a probability at least 0.92, it was inserted before the label.\\n\\nTense.\\nWe noticed that most templates are written in present tense, while many triples describe facts that are no longer true or concern dead people, past events, etc. Questions were rewritten in the past tense if the corresponding triple had an \\\"end time\\\" qualifier in Wikidata, or if its subject or object was a dead person.\\n\\nRule-based introduction of pronouns.\\nSubject mentions are pronominalised using a rule-based approach: a pronoun is used only if the subject also appears in the triple of the previous question and if its gender differs from the gender of the object of this triple (to avoid ambiguous pronouns); further rules are used to determine the kind of pronoun to use (for example, if the subject reference is followed by a possessive \\\"s\\\", a possessive pronoun should be used, etc.).\\n\\n4.3.2. Model-based rewritings (\\\\( C_2 \\\\) variants)\\nTo further increase the contextuality of questions, a T5-based question rewriting model was fine-tuned on a training set derived from 2 conversational machine reading QA datasets, namely CANARD (Egohary et al., 2019) and CoQAR (Brabant et al., 2022). This training set is made of \\\\( 142 \\\\)K instances. For each instance, the input is a question \\\\( q_i \\\\), along with its conversation history \\\\([ q_0, a_0, \\\\ldots, q_{i-1}, a_{i-1} ]\\\\).\"}"}
{"id": "lrec-2024-main-850", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"while the output is a semantically equivalent question whose form is expected to be natural in a conversation, denoted by $q^*_i$. In some instances, $q_i$ and $q^*_i$ have respectively a C0 and a contextualized form. In other instances, $q_i$ and $q^*_i$ are equal; these instances correspond to cases where either $q_i$ already has an C1 form, or there is no natural way to rewrite it without losing information or bringing ambiguity. Including such cases to the training set enables the model to learn when it should rewrite the input question or not.\\n\\nAt inference time, the 20 best hypotheses are generated by the model for each instance. Then, they are classified into three authorized categories, using a set of expert conditions: (1) coreference with a pronoun (e.g., \u201cIn which country is Kyoto located?\u201d rewritten as \u201cIn which country is it located?\u201d), (2) coreferences with a demonstrative noun phrase (e.g., \u201cIn which country is this city located?\u201d), and (3) ellipses (e.g., \u201cIn which country?\u201d). Those that do not belong to any category are filtered out; moreover, to limit possible ambiguities, we prohibit two consecutive reformulations of the same category. Finally, if some hypothesis remain, the one with the highest probability is selected as the rewritten form.\\n\\nThis process was applied on all C1 questions, leading to the C2 version shown in Table 4.\\n\\n### Table 4: Examples of rewritten questions at inference time.\\n\\n| Type                      | Original                | Rewritten              | OK? |\\n|---------------------------|-------------------------|------------------------|-----|\\n| Coref. w/ pronoun         | Which location is Switzerland a component of? | Which location is it a component of? | \u2713   |\\n| Coref. w/ demonstrative noun phrase | With which country would you associate Gyeonggi Province? | With which country would you associate this province? | \u2713   |\\n| Ellipsis                  | What is the public holiday associated with Switzerland? | What is the public holiday? | \u2713   |\\n|                           | What is the zenith of Eritrea? | What is the zenith? | \u2713   |\\n|                           | In what geographic region is Eurasia located? | In what geographic region? | \u2713   |\\n\\n### Table 5: Percentage of questions using an alternative label and a pronoun in C1 questions; percentage of C2 questions that differ from the C1 version.\\n\\n#### Table 5\\n\\n| Percentage       | Questions using alternative label and pronoun (%) | C2 questions differing from C1 version (%) |\\n|------------------|--------------------------------------------------|------------------------------------------|\\n\\n5. Conversational Question Generation\\n\\nKnowledge-Based, Conversational Question Generation extends Question Generation from Knowledge Graph triples (Elsahar et al., 2018; Han et al., 2022) to a conversational setting: instead of generating a question only from a triple, we generate a question from both a triple and the preceding conversational context. This raises the additional challenge of generating questions in contextually appropriate forms e.g., using appropriate referring expressions and ellipses. Leveraging the multi-modal text/graph nature of our dataset, we explore four ways of representing the context: (1) no contextual information at all (Empty), (2) the sequence of previous questions and answers (NL) (3) the sequence of triples underlying the questions and answers (KG) and (4) the sequence of questions and answers with their corresponding triples (NL+KG).\\n\\nFor each of the four variants, we trained a baseline by fine-tuning a T5-small model on the three versions of questions in KGConv (C0, C1 and C2). In the train and dev sets, all themes are mixed together. The number of epochs for training is determined via early stopping.\\n\\n### Table 6: Automatic Evaluation\\n\\nWe evaluate each model on the test set using Google-BLEU and BERT-score taking as references all questions associated with the input triples (in C0, C1, and C2 forms), around 12 references.\"}"}
{"id": "lrec-2024-main-850", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on average. The results are presented in Table 6. Seen vs. unseen. Unsurprisingly, all models obtain lower scores on unseen themes. Similarly, average scores are lower on unseen properties because the verbalization highly depends on the property of the triple.\\n\\nC0 vs. C1 vs. C2. In term of GLEU, models trained on C2 generally perform better than their C1 and C0 counterparts. However, this tendency is not confirmed with BERT-scores. Moreover, these better GLEU scores might just be an artifact of the experimental design. Indeed references contain C0, C1 and C2 question versions, and models trained on C2 are the only one to have been trained to generate all three versions. Thus, C2 models might have better GLEU simply because their training data is more in-line with the references.\\n\\nConversational context format. Adding conversational context to the models trained on C1 and C2 questions consistently improves GLEU and BERT-scores. Looking at GLEU scores, it also seems that providing the context in the form of triples (or triples and text) provides a better improvement than providing the context in the form of text. Since conversational context is not required to generate C0 questions, models trained on C0 questions tend to perform better when no conversational context is given, except for unseen properties.\\n\\n7. Human Evaluation\\n\\nThe human evaluation assesses both the dataset and the two baselines: C1 (KG+NL) and C2 (KG+NL). We sample conversations from the test set. For each conversation we created four alternative versions. The first two versions select C1 and C2 questions from the dataset (i.e., the references). The other two versions turn on the questions C1 and C2 generated by the baselines that takes into consideration both the triple and the context. We built an evaluation graphical interface, in which human evaluators can rate these conversations. Table 7 gives the number of conversations rated by evaluators depending on theme and version.\\n\\n7.1. Evaluation Setup\\n\\nThe ratings were provided by 15 evaluators from the authors' research center (excluding the authors). Each evaluator could evaluate up to 50 conversations. They were told that conversations are automatically generated, but were provided no information about the method employed. Conversations were presented one by one to the evaluator. For each question-answer pair, the corresponding triple was displayed, and the evaluator had to (i) rate the linguistic correctness of the question on a 5 point scale and (ii) evaluate whether the question-answer pair expresses the information of the triple (\\\"yes\\\", \\\"quite\\\", \\\"no\\\", \\\"I don't know\\\"). In addition, the evaluator had to rate the naturalness of the whole conversation flow.\\n\\nA second round of evaluation on 61 of the already rated conversations was performed to assess the consistency of ratings among annotators. This evaluation utilizes the C1 and C2 versions of the questions (i.e., conversations using baselines were not included in the second round). This evaluation was performed by 3 annotators who did not participate in the first round.\\n\\n7.2. Results\\n\\nResults are provided in Table 8. We use the Mann-Whitney U and the $\\\\chi^2$ test to assess significance. The former was used for correctness, clearness and naturalness scores, since those are evaluated on an ordered scale. The latter was used for faithfulness scores, since these form a scale that is not completely ordered (because of the \\\"I don't know\\\" answer).\\n\\nC1 vs C2 references. Comparing the scores of C1 and C2 from the references (in the All block of Table 8) we see that, while linguistic correctness is roughly the same, C2 references are less clear (clearness 4.59 vs 3.96, p=2e-12), less faithful to the triples (0.90 yes vs 0.71, p=8e-8) and less natural overall (3.88 vs 3.30, p=0.006).\\n\\nC1 vs C2 baselines. The C2 baseline seems a bit more linguistically correct (4.60 vs 4.75, p=0.001). However, the C1 baseline seems clearer (4.56 vs 4.13, p=4e-5) and more natural overall, although it might be due to chance (3.64 vs 3.14, p=0.066). Faithfulness seems to be the same.\\n\\nReferences vs baselines. Now let us compare the baselines to the references they were trained on. For the C1 reference, the baseline is less faithful to triples (0.90 yes vs 0.78, p=0.00014), otherwise we observe no significant difference. For the C2 reference, the baseline is better on every measure, although we obtain low p-values only for clearness (3.96 vs 4.13, p=0.012) and correctness (4.59 vs 4.75, p=0.0012).\\n\\nSeen vs. unseen. The scores obtained on unseen themes tend to be lower that those obtained on seen themes. This difference happens both for the C1 baseline and for the C2 reference. This suggests that the differences are due to the difficulty.\"}"}
{"id": "lrec-2024-main-850", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Results of the automatic evaluation. Seen themes are those with a non-empty training set (see Table 2), unseen themes are space object and taxon. The scores are obtained by macro-averaging over themes. The best score is in bold; lower scores that do not differ significantly (p > 0.05 in a Mann-Whitney U test) from the best one are adorned with (*).\\n\\n| Ref. | Ref. | Model | Model |\\n|------|------|-------|-------|\\n| (seen) | person | 10 | 10 |\\n|       | food   | 10 | 11 |\\n| (unseen) | taxon | 8 | 8 |\\n|       | space o. | 6 | 8 |\\n\\nTable 7: Number of rated conversations. Instead of focusing on the number of themes rather that the fact that they were seen during the training of baselines or not.\\n\\nInter-rater agreement. We computed Cohen's kappa for each metric (faithfulness, correctness, clearness, naturalness) and obtained, respectively: 0.23, 0.10, 0.22, and 0.14. Although those scores are quite poor, the confusion matrices (Table 9) suggest that, although the exact rate given to a question has a high degree of subjectivity, raters tend to give close ratings. Low kappas seem due to two factors: (1) the intrinsically subjective nature of the task, which can explain that raters disagree by giving different but close rates, (2) genuine mistakes made by raters (for example, when faithfulness is rated at yes and no by two different raters). It is also possible that differences in raters' fluency had an impact on agreement. Despite the low agreement, we observed the interesting regularities reported previously in this section.\\n\\n8. Conclusion\\n\\nWe make available KGConv, a new conversational dataset grounded in Wikidata where each question-turn in the conversation comes into several variants belonging to 3 contextuality levels (C0, C1, C2). Although C2 questions have more diverse forms than C1 questions, the results of human evaluation suggest that C1 questions are more reliable than C2 questions.\\n\\nWe also presented several baselines for the task of question generation and found that generating questions from unseen properties is challenging for these baselines. An interesting perspective would be to investigate methods for tackling this particular zero-shot task.\\n\\nAs it provides a large number of references for each question in a conversation, KGConv is well suited for other tasks besides Conversational Question Generation such as in particular, single-turn question generation from facts, question rewriting and generation of sequence of question-answer pairs from a Knowledge graph (KG) or vice-versa.\\n\\n9. Limitations\\n\\nThis corpus has been generated semi-automatically, although human annotations were involved in the question templates, the conversations were generated automatically from the KG. As a consequence, in some cases the flow of the conversation may be unnatural, because humans do not usually talk in that way. This might be specially true when conversations involve complex content (e.g. molecular entities, space objects or historical events) that may be difficult to be understood by non experts.\"}"}
{"id": "lrec-2024-main-850", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8: Scores from human evaluation for conversations about seen, unseen or all themes. Ref., stands for the C1 and C2 references.\\n\\n10. Ethics Statement\\nThe construction of this corpus involved manual annotation of the question templates. Therefore, we hire three students of an NLP Master program. They were native English speakers hired on a short term contract to perform various annotation tasks for NLP. They were paid slightly above the national minimum wage and they had the right to the social security benefits.\\n\\n11. Bibliographical References\\nSheng Bi, Xiya Cheng, Yuan-Fang Li, Yongzhen Wang, and Guilin Qi. 2020. Knowledge-enriched, type-constrained and grammar-guided question generation over knowledge bases. In Proceedings of the International Conference on Computational Linguistics (CICLing), pages 2776\u20132786.\\n\\nKurt Bollacker, Colin Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor. 2008. Freebase: a collaboratively created graph database for structuring human knowledge. In Proceedings of the 2008 ACM SIGMOD international conference on Management of data, pages 1247\u20131250.\\n\\nAntoine Bordes, Nicolas Usunier, Sumit Chopra, and Jason Weston. 2015. Large-scale Simple Question Answering with Memory Networks. ArXiv:1506.02075 [cs].\\n\\nQuentin Brabant, Gw\u00e9nol\u00e9 Lecorv\u00e9, and Lina M. Rojas Barahona. 2022. CoQAR: Question rewriting on CoQA. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 119\u2013126, Marseille, France. European Language Resources Association.\\n\\nE. Choi, H. He, M. Iyyer, M. Yatskar, W. Yih, Y. Choi, P. Liang, and L. Zettlemoyer. 2018. QuAC: Question Answering in Context. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174\u20132184, Brussels, Belgium. Association for Computational Linguistics.\\n\\nPhilipp Christmann, Rishiraj Saha Roy, Abdalghani Abujabal, Jyotsna Singh, and Gerhard Weikum. 2019. Look before you Hop: Conversational Question Answering over Knowledge Graphs using Judicious Context Expansion. In Proceedings of the ACM International Conference on Information and Knowledge Management, pages 729\u2013738. Association for Computing Machinery.\"}"}
{"id": "lrec-2024-main-850", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-850", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
