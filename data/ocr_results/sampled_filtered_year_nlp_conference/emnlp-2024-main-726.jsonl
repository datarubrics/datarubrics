{"id": "emnlp-2024-main-726", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SciER: An Entity and Relation Extraction Dataset for Datasets, Methods,\\nand Tasks in Scientific Documents\\nQi Zhang1 Zhijia Chen1 Huitong Pan1 Cornelia Caragea2 Longin Jan Latecki1 Eduard Dragut1\\n\\n1 Temple University\\n2 University of Illinois Chicago\\n{qi.zhang, latecki, edragut}@temple.edu, cornelia@uic.edu\\n\\nAbstract\\nScientific information extraction (SciIE) is critical for converting unstructured knowledge from scholarly articles into structured data (entities and relations). Several datasets have been proposed for training and validating SciIE models. However, due to the high complexity and cost of annotating scientific texts, those datasets restrict their annotations to specific parts of paper, such as abstracts, resulting in the loss of diverse entity mentions and relations in context. In this paper, we release a new entity and relation extraction dataset for entities related to datasets, methods, and tasks in scientific articles. Our dataset contains 106 manually annotated full-text scientific publications with over 24k entities and 12k relations. To capture the intricate use and interactions among entities in full texts, our dataset contains a fine-grained tag set for relations. Additionally, we provide an out-of-distribution test set to offer a more realistic evaluation. We conduct comprehensive experiments, including state-of-the-art supervised models and our proposed LLM baselines, and highlight the challenges presented by our dataset, encouraging the development of innovative models to further the field of SciIE.\\n\\n1 Introduction\\nScientific Information Extraction (SciIE) is a core topic of scientific literature mining (Luan et al., 2017; Groth et al., 2018; Sadat and Caragea, 2022; Park and Caragea, 2023; Pan et al., 2024a). It typically includes scientific named entity extraction (SciNER) and scientific relation extraction (SciRE), and plays a critical role in downstream applications, including scientific knowledge graph construction (Wang et al., 2021; Gautam et al., 2023), data searching (Viswanathan et al., 2023), academic question answering (Dasigi et al., 2021), and method recommendation (Luan et al., 2018). Scientific large language models (LLMs) like Galactica (Taylor et al., 2022) enable several practical applications such as citations suggestion, scientific question answering (QA), and scientific code generation (Li et al., 2023). However, their generated content is frequency-biased, often exhibits overconfidence, and lacks factual basis (Xu et al., 2023). SciIE, integrated with suitable retrieval, and QA systems can mitigate those issues and enhance model effectiveness in downstream tasks (Shu et al., 2022; Xu et al., 2023).\\n\\nSciIE faces unique challenges compared to general domain IE. First, data annotation for SciIE is highly dependent on expert annotators, resulting in a scarcity of high-quality labeled datasets. Second, SciIE needs to handle more complex text, which evolves constantly with novel terminology, unlike general domain IE. For instance, SciIE faces more severe temporal and conceptual shifts (Zhang et al., 2019; Viswanathan et al., 2021; Zaporojets et al., 2022; Chen et al., 2022, 2024; Pham et al., 2023), whereas fundamental entities and relationships in general IE tend to remain more static over time compared to those in the scientific literature.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing SciIE datasets and benchmarks that support both SciNER and SciRE are limited to extracting information from specific parts of papers, such as particular paragraphs (Augenstein et al., 2017) or abstracts (G\u00e1bor et al., 2018; Luan et al., 2018). However, scientific entities like datasets, methods, and tasks entities are distributed throughout the entire text of papers. Sentences in the body of a paper exhibit diverse linguistic styles and ways to mention entities (Li et al., 2023) and semantics (Jain et al., 2020), which allows the extraction of more fine-grained and precise relation types. For example, abstracts do not say that method X is trained on dataset Y, but experimental sections give such details. Therefore, focusing on specific parts of scientific articles is likely to miss important information. Several datasets (Pan et al., 2024b, 2023; Otto et al., 2023; Jain et al., 2020) attempt to create SciIE benchmarks with full-text annotation, but they ignore the SciRE task.\\n\\nIn this paper, we present SciER, an entity and relation extraction dataset for identifying dataset, method, and task entities in scientific documents as well as the relations between them. Our dataset is large, with 24K entities and 12k relations from 106 scientific articles, enabling the evaluation and development of SciIE models. These documents are taken from the publications included in Papers with Code (PwC) 2, covering artificial intelligence (AI) topics, such as natural language processing (NLP), machine learning (ML), computer vision (CV), and AI for Science (AI4Science). Figure 1 shows an annotated sentence from our dataset, which gives the entities, their types, i.e., METHOD and TASK, respectively, and the relation between them USED-FOR. Our dataset can be used to evaluate NER and RE as separate tasks, but it can also support the evaluation of end-to-end entity and relation extraction (ERE) from scientific publications (Luan et al., 2018; Ye et al., 2022). The table in Figure 1 describes those settings. For example, in NER the input is a sentence and the output is the set of entities in the sentence. In RE, the input is the sentence along with the entities and the output is the relation between those entities. Finally, in ERE the triplet \\\\(<\\\\text{subject}, \\\\text{relation}, \\\\text{object}>\\\\) is the expected output from a sentence.\\n\\nWe address the limitations of existing datasets by annotating entire scientific papers for both entity and their relations. This is a much harder task compared to annotating abstracts. Furthermore, comparing with existing datasets (Augenstein et al., 2017; G\u00e1bor et al., 2018; Luan et al., 2018), we provide more fine-grained relation types to describe the interactions between datasets, methods, and tasks. For example, we use TRAINED-WITH and EVALUATED-WITH to describe the interactions between methods and datasets. These relation types need to be extracted from the body of a paper, and are not supported by previous datasets. \u00a73.3 gives a detailed comparison between our dataset and existing ones. Finally, to evaluate the model's robustness to temporal and conceptual shifts in the SciIE, we set in-distributed (ID) and out-of-distribution (OOD) test sets. The documents in the OOD set were all published after the training documents and feature entirely different topics. We conduct evaluation experiments by employing three state-of-the-art supervised methods and LLMs-based in-context learning (ICL) methods and provide analysis. Specifically, for LLMs-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results show that for LLMs, pipeline modeling, which splits the ERE task into two sub-tasks of NER and RE, outperform as joint extraction. In the challenging ERE task, the best supervised method achieves an F1 score of 61.10%, while the best LLM method achieves an F1 score of 41.22%.\\n\\nOur contributions can be summarized as follows:\\n\\n\u2022 We provide a manually annotated dataset consisting of 106 full-text scientific publications, containing over 24k entities and 12k relations. Our dataset is significantly larger than previous datasets that support both SciNER and SciRE tasks.\\n\\n\u2022 We introduce a fine-grained tag set designed for scientific relation extraction, customized to reflect the use and interaction of machine learning datasets, methods, and tasks entities in scientific publications.\\n\\n\u2022 We conducted experiments on LLMs baselines using both pipeline and joint approaches. We optimized the prompt through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. We also provided a comparative analysis between LLMs methods and three state-of-the-art supervised baselines, highlighting the key challenges.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\nMany datasets for SciNER have been proposed. (Heddes et al., 2021) and DMDD (Pan et al., 2023) are two datasets for dataset mention detection. The (Heddes et al., 2021) dataset comprises 6000 annotated sentences selected based on the occurrence of dataset related word patterns from four major AI conference publications. DMDD is annotated on the full text and comprises 31219 scientific articles automatically annotated with distant supervision (Zhang et al., 2018). TDMSci (Hou et al., 2021) supports three types of entities: TASK, DATASET, and METHOD. It has 2000 sentences extracted from NLP papers. SciREX (Jain et al., 2020) offers comprehensive coverage with 438 full text annotated documents and supports four entity types: TASK, DATASET, METHOD, and METRIC. SciREX does not annotate relations between pairs of those entity types. (Otto et al., 2023) manually annotates 100 documents for fine-grained SciNER by defining 10 different entity types in 3 categories: MLModel related, Dataset related and miscellaneous. SciDMT (Pan et al., 2024b) uses the PwC as knowledge created a very large scale dataset for DATA, METHOD, and TASK. SciDMT includes 48 thousand scientific articles with over 1.8 million weakly annotated mention annotations in their main corpus. However, given the inherent complexity of the NER task, employing weak labels may cause models to overfit on noisy data, thereby substantially impacting their performance (Liu et al., 2021; Bhowmick et al., 2022, 2023).\\n\\nAlthough there has been growing interest in research on developing methods and datasets for SciIE, very few datasets support both NER and RE tasks for scientific text. An overview of existing SciIE benchmarks that support both SciNER and SciRE is shown in Table 1. SEMEVAL-2017 TASK 10 (SemEval 17) (Augenstein et al., 2017) includes 500 paragraphs from open-access journals and supports three types of entities: TASK, METHOD, and MATERIAL and two relation types: HYPONYM-OF and SYNONYM-OF. SEMEVAL-2018 TASK 7 (SemEval 18) (G\u00e1bor et al., 2018) has been proposed for predicting six types of relations between entities. All sentences in SemEval 18 are from the abstracts of NLP papers and have only entity spans (i.e., without annotation of entity types). SciERC (Luan et al., 2018) contains 500 scientific abstracts with the annotations for scientific entities, their relations, and coreference clusters. SciERC defines six types of entities and seven types of relations. However, these three datasets are limited on annotating abstracts or pre-selected paragraphs. Thus, a significant number of sentences that contain more diverse entity mention forms and semantics are lost. Compared to those resources, our dataset contains 106 scientific publications with minutemanual annotations. The dataset has nine relation types, allowing for more nuanced relations between entities. The scale of our dataset, which contains more than 24k entities and over 12k relations, which is significantly larger than previous datasets, except for those that are created with distant supervision.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Data Annotation\\n\\nAnnotation Scheme\\n\\nFor the entity annotation, we use the SciDMT annotation scheme, which defined three types of entities: DATASET, METHOD, and TASK. To maintain consistency with the PwC website database, we only annotate the factual entities, unlike previous works (Luan et al., 2018; Otto et al., 2023) which annotate both factual and non-factual entities. For example, the \u201cCoNLL03\u201d and \u201cSNLI\u201d are factual entities, but the \u201ca high-coverage sense-annotated corpus\u201d is not a factual entity.\\n\\nFor the relation annotation, we define nine fine-grained tag sets to establish interaction relationships between datasets, methods, and tasks entities in scientific documents. They are EVALUATED-WITH, COMPARE-WITH, SUBCLASS-OF, BENCHMARK-FOR, TRAINED-WITH, USED-FOR, SUBTASK-OF, PART-OF, and SYNONYM-OF. Directionality is taken into account except for the two symmetric relation types (SYNONYM-OF and COMPARE-WITH). We provide our semantic relation typology and corresponding examples in Table 2. Specifically, compared to previous datasets (Augenstein et al., 2017; Luan et al., 2018; G\u00e1bor et al., 2018), we employ more specific relation types for identical entity types and extend usage relations among different types of entities in a more granular manner. For example, we use SUBTASK-OF and SUBCLASS-OF to describe the hierarchical relations between tasks and methods, respectively. This can provide better interpretability and allows for direct usage in practical applications such as building taxonomies. Additionally, we use TRAINED-WITH and EVALUATED-WITH to describe the more precise interactions between methods and datasets. We provide more detailed definitions of the labels for entities and relations in our annotation guidelines in Appendix E.\\n\\nAnnotation Strategy\\n\\nWe have five annotators with backgrounds in computer science and machine learning. We conduct the annotation using INCEPIENT platform. All annotators had annotation training before starting to annotate on assigned documents. For the 100 documents from SciDMT-E, we asked annotators to first re-check the SciNER annotation before proceeding to the SciRE annotation. For the six OOD documents, annotators need to annotate both SciNER and SciRE from scratch.\\n\\nHuman Agreement\\n\\nOne annotator leads the entire annotation process and annotates all the documents in the dataset and each document is also annotated by at least two other annotators. For the first 100 documents, the kappa score (Davies and Fleiss, 1982) for entity annotation is 94.2%, relation annotation is 70.8%; for the six OOD documents, the kappa score for entity annotation is 74.1%, relation annotation is 73.8%. The almost perfect agreement of entity annotation on the first 100 documents is because we derive the original annotation from SciDMT-E.\\n\\n3.3 Dataset Statistics and Comparison\\n\\nAfter the annotation process, our dataset contains over 24k entities and 12k relations, with each document averaging about 114 relations. As shown in Table 1, our dataset is significantly larger than previous datasets supporting both entity and relation extraction tasks. Specifically, for the widely used SciERC dataset, when we only consider Dataset, Method, and Task entities, it contains only about 1.5k entity and 1.5k relation annotations, where more details are provided in Appendix A.2. We randomly split the first 100 documents into train, development, and ID test sets, containing 80, 10, and 10 documents, respectively. We used six OOD documents as the OOD test set. Appendix A.3 lists the number of samples for each relation type in each set of our dataset.\\n\\n4 Experiments\\n\\nIn this section, we provide the details of evaluation experiments of both state-of-the-art supervised baselines and LLMs-based baselines on the proposed dataset. We first formally define the problem of end-to-end relation extraction in \u00a74.1, then describe the supervised methods in \u00a74.2 and the LLMs-based methods in \u00a74.3. Finally, we present our implementation details in \u00a74.4 and evaluation settings in \u00a74.5.\\n\\n4.1 Problem Definition\\n\\nWe aim our dataset as a means to train and evaluate SciIE models. Formally, the input document is denoted as $D$, which contains a sequence of paragraphs $P = \\\\{p_1, p_2, ..., p_n\\\\}$. Each paragraph $p$ is composed of a sequence of sentences $\\\\{s_1, s_2, ..., s_n\\\\}$ and each sentence is composed of a sequence of words $\\\\{w_1, w_2, ..., w_n\\\\}$. Formally, the problem of end-to-end relation extraction can be decomposed into two sub-tasks:\\n\\nNamed Entity Recognition\\n\\nLet $E$ denote a set of pre-defined entity types. The NER task is to...\"}"}
{"id": "emnlp-2024-main-726", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods are evaluated by datasets. We use COCO to evaluate CornerNet-Lite and compare it with other detectors.\\n\\nEntities are linked by comparison relation. MAC...outperforms all tested RANSAC-fashion estimators, such as SAC-COT...\\n\\nOne method is a specialized class of another. BENCHMARK-FOR Datasets are used to evaluate tasks. FlyingChairs is a synthetic dataset designed for training CNNs to estimate optical flow. METHODS are trained by datasets.\\n\\nEntities are linked by usage relation. SUBTASK-OF A specific part of another broader task. ...is critical for dense prediction tasks such as object detection...\\n\\nEntities are in a part-whole relation. Adding attention to our deep learning-based network translated to...\\n\\nEntities have same or very similar meanings. SYNONYM-OF...to improve Generative Adversarial Network (GAN) for...\"}"}
{"id": "emnlp-2024-main-726", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overall architecture of LLM in-context learning (few-shot) baselines for NER, RE and joint Entity and Relation Extraction (ERE) (first). The few-shot prompt templates for NER (second), RE (third), and Joint ERE (fourth). Different colors indicate different prompt design elements: gray for annotation guideline-based task instructions $I$, blue for retrieved demonstrations $D$, orange denotes the test example input $x_{test}$, and the green represents the expected output of test example output, which will be omitted during testing. Due to space constraints, we shortened the text of our prompts. When performing zero-shot ICL, $D$ will be removed from the prompt. Our LLMs-based baseline framework is shown in Figure 2. Existing work indicates that for information extraction tasks, LLMs require clearer instructions to improve the performance (Qin et al., 2023; Hu et al., 2024; Sainz et al., 2023; Jimenez Gutierrez et al., 2022). Therefore, we use annotation guidelines to optimize our prompts. Specifically, for each task, we include two additional instruction components:\\n\\n1. **Label Definitions:** We provide definitions of all entities for the NER task, and definitions of all relations for the RE task. For the Joint ERE task, which requires the model to perform both NER and RE simultaneously, we provide definitions of both entities and relations.\\n\\n2. **Annotation Notes:** We derive suitable instructions from the human annotation guidelines (see Appendix E) for each task and provide them to the LLMs. We believe that introducing entity and relation definitions and annotation notes offers comprehensive and unambiguous descriptions of the target extraction information.\\n\\nIn terms of formatting the NER annotation in prompt, we present it as HTML span tag. This is because (Wadhwa et al., 2023; Hu et al., 2024) demonstrated that when using LLMs for information extraction, the generated results might have the same meaning as in the input text but differ in surface form. For example, the entity \\\"CNNs\\\" in the input sentence might be generated as \\\"CNN\\\". To mitigate this error in NER, we instruct the LLMs to use HTML span tags to mark all entities in the input sentence to extract the entity spans and use the class attribute to determine the entity types. For example, the entity \\\"CNNs\\\" in the input text will be marked as \\\"<span class=\\\"Method\\\">CNNs</span>\\\". We provide the complete prompt used in our experiments in Appendix D.\\n\\n### 4.4 Implementation Details\\n\\nFor the supervised methods, we use the scibert-scivocab-uncased (Beltagy et al., 2019) as encoder. For the LLMs-based methods, we test the GPT-3.5-TurboLlama3-70b, and Qwen2-72b as the LLM. For few-shot ICL setting, we retrieve 30 demonstrations for each task, and we use the SimCSE (Gao et al., 2021) as the retriever. For consistent comparison, all experiments are conducted at the sentence-level. Appendix B has additional implementation details.\\n\\n### 4.5 Evaluation Settings\\n\\nTo evaluate the pipeline extraction and joint ERE, we compute the performance for each subtask, including NER, end-to-end RE (using NER results for relation extraction), and RE (relation extraction with given gold standard entities). For NER, we conduct span-level evaluation, where both the entity boundary and entity type need to be correctly extracted. For the end-to-end RE, similar to (Zhong and Chen, 2021; Ye et al., 2022; Yan et al., 2023), we report two evaluation metrics:\\n\\n1. **Boundaries Evaluation (Rel),** which requires the model to correctly predict the boundaries of the subject entity and the object entity, as well as the entity relation;\\n2. **Strict Evaluation (Rel+),** which further requires the model to predict the entity types based on the requirements of the boundary prediction. For the RE, given any pair of subject and object entity, the model needs to determine whether a pre-defined relation exists. If a relation does exist, the model must predict the corresponding type.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5 Experimental Results\\n\\n5.1 Main Results\\nTable 3 reports the experimental results on ID test set and OOD test set. As described in \u00a74.5, for the pipeline extraction methods, we present additional RE results when gold standard entities are given.\\n\\nSupervised Baselines\\nWe observe that HGERE achieves the best performance on both ID and OOD test sets in NER, Rel, and Rel+, demonstrating the robustness of this current SOTA method. When comparing the results of ID and OOD, we find that all methods show performance drop on the OOD test set for NER, Rel, and Rel+. This is because OOD test provides more challenging and realistic validation scenarios, which require the models to extract information from newly published papers containing new entities. We also observe that the decline in NER scores is more significant, especially for PURE and PL-Marker, whose performance dropped by nearly 10 F1 points. This indicates that extracting unseen entities is more challenging for supervised models compared to relation extraction, which is further supported by the slight decline in RE performance for PURE and PL-Marker in OOD compared to ID. We provide a qualitative example in Appendix C.1.\\n\\nLLMs-based Baselines\\nFrom the results of both zero-shot and few-shot setting, we have the following observations:\\n\\n1. Qwen2-72b exhibits the best overall performance than GPT-3.5-turbo and Llama3-70b in both zero-shot and few-shot settings (except the NER task).\\n\\n2. Pipeline extraction outperforms joint ERE in both zero-shot and few-shot settings. Surprisingly, for both Llama3-70b and Qwen2-72b, pipeline extraction shows a significant improvement over joint ERE. We observed that the NER performance in the pipeline extraction is significantly better than in the joint ERE. This indicates that performing LLMs for this end-to-end relation extraction task by decomposing it into separate NER and RE processes yields better results than joint extraction.\\n\\n3. For LLMs-based baselines, the performance of ID does not always outperform OOD and such pattern is very different from supervised baselines. We believe this is due to the extensive training of LLMs on large-scale data. Specifically, for the RE, even though few-shot settings provide similar demonstrations of test data, the ID results are still worse than OOD. However, for the NER, Rel, and Rel+ tasks under few-shot settings, the performance on ID tends to be better than on OOD. Additionally, compared to OOD, the overall performance improvement on ID after using few-shot settings is generally greater than on OOD. This is because, the demonstrations provided to the LLMs are more similar to the ID data.\\n\\nPrevious works (Wan et al., 2023; Jimenez Gutierrez et al., 2022; Ma et al., 2023) showed that information extraction tasks are very challenging for LLMs compared to supervised methods. However, for NER, we found that with appropriate prompt settings, LLMs can be a competent NER model, as reaching an F1 score of 61.69 in zero-shot setting, comparing to the best. This suggests that incorporating LLMs into the NER dataset creation process is a feasible solution to reduce human labor. LLMs perform worse on RE tasks. This is because the test samples for RE tasks contain a large number of NULL labels (see C.2), and large language models have a strong tendency to classify the NULL into predefined types, which has also been confirmed by recent works (Jimenez Gutierrez et al., 2022; Wan et al., 2023). Our experiments show that for end-to-end relation extraction (Rel and Rel+), including the current state-of-the-art (SOTA) models and LLMs-based baselines, there is still significant room for improvement in the future.\\n\\n5.2 Ablation Study\\nTo validate the effectiveness of the annotation guideline-enhanced prompt design used in LLM-based baselines, we conducted an ablation study using the Llama3-70b model in a few-shot setting. Specifically, for all tasks, we removed the additional instructions derived from the annotation guidelines, retaining only the basic task description in the instruction \ud835\udc3c. For the NER task, we further removed the requirement of using HTML span tags, allowing the model to directly generate all entities from the input text rather than tagging the input text. Figure 3 presents the results of our ablation study. The results indicate that incorporating label definitions and comprehensive annotation task guidelines significantly improve the model's performance across all tasks. Additionally, for NER, the use of HTML span tags further enhances performance.\\n\\n5.3 Train Size Experiment\\nAnnotating datasets for information extraction with specific domains presents certain challenges. Comparing to partial text, such as sentence and abstracts, full-text annotation further exacerbates the difficulties for annotation. In the training stage,\"}"}
{"id": "emnlp-2024-main-726", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Methods                | NER Test | Rel Test | Rel+ Test | RE Test |\\n|------------------------|----------|----------|-----------|---------|\\n| **Supervised Baselines** |          |          |           |         |\\n| PURE (Zhong and Chen, 2021) | 81.60    | 53.27    | 52.67     | 73.99   |\\n| PL-Marker (Ye et al., 2022) | 83.31    | 60.06    | 59.24     | 77.11   |\\n| HGERE (Yan et al., 2023) | 86.85    | 62.32    | 61.10     | -       |\\n| **Zero-Shot LLMs-based Baselines** |          |          |           |         |\\n| GPT3.5-Turbo (Joint) | 34.76    | 11.38    | 10.34     | -       |\\n| Llama3-70b (Joint) | 48.87    | 17.31    | 17.01     | -       |\\n| Qwen2-72b (Joint) | 42.15    | 16.27    | 14.99     | -       |\\n| **Few-Shot LLMs-based Baselines** |          |          |           |         |\\n| GPT3.5-Turbo (Joint) | 62.36    | 23.71    | 23.49     | -       |\\n| Llama3-70b (Joint) | 63.23    | 29.21    | 29.16     | -       |\\n| Qwen2-72b (Joint) | 63.73    | 35.84    | 34.87     | -       |\\n\\nTable 3: Test F1 scores of different baselines on our proposed dataset. \u201cJoint\u201d denotes joint ERE. \u201cPipeline\u201d refers to performing NER and RE separately. \u201cRel\u201d and \u201cRel+\u201d denote the results of end-to-end relation extraction under boundaries evaluation and strict evaluation, respectively. \u201cRE\u201d indicates performing relation extraction with given gold standard entities, applicable only to pipeline extraction methods.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and 12,000 relations. Additionally, we introduce a fine-grained relation set to describe the interactions between datasets, methods, and tasks. To evaluate the model's robustness to temporal and conceptual shifts in the SciIE, we also set an OOD test set. We conduct comprehensive evaluation experiments, including supervised state-of-the-art (SOTA) models and LLM-based ICL baselines, to highlight the challenges in this task. Specifically, for LLM-based methods, we tested both pipeline and joint approaches, optimizing the prompts through retrieval-based ICL, tag-based entity extraction, and the incorporation of annotation guidelines. The experimental results of LLMs-based methods show that:\\n\\n1. For the ERE task, pipeline modeling, which decomposes the task into NER and RE sub-tasks, significantly outperforms joint modeling;\\n\\n2. Although LLM-based approaches require less labeled data, there remains a performance gap compared to supervised methods. For future work, we aim to further optimize prompts to enhance the performance of LLMs in Scientific Information Extraction (SciIE) and domain-specific IE tasks. Additionally, a LLM-in-the-loop data annotation system to reduce the high costs of creating domain-specific IE datasets is feasible.\\n\\nLimitations\\n\\nDespite our diligent efforts, developing a gold standard dataset for entity and relation extraction using a fine-grained and comprehensive relation tag set focused on machine learning datasets, methods, and tasks remains a nontrivial undertaking. This leads to the following limitations associated with the creation of our corpus. Our dataset only supports three entity types: DATASET, METHOD, and TASK. Incorporating more diverse entity types would be more beneficial for the development of SciIE. Additionally, many scientific entities are nested, which we have not included. We also observed that parsing documents from PDF format contains some errors, which increases the difficulty of document processing and causes some of our sentences to contain errors. Finally, we believe that further evaluation experiments can be conducted, such as optimizing the ICL baselines for LLMs. However, due to space constraints, we will consider these as future work.\\n\\nEthical Statement\\n\\nThe data included in our newly proposed dataset includes a subset of the data collected and freely published by (Pan et al., 2024b) within the SciDMT project. All the other data are public from scientific documents. We release dataset for scientific information extraction tasks. There are no risks in our work.\\n\\nAcknowledgements\\n\\nThis work was supported by the National Science Foundation awards III-2107213, III-2107518, and ITE-2333789. We also thank Saiyun Dong and Faezeh Rajabi Kouchi at Temple University, and Seyedeh Fatemeh Ahmadi at UIC for their valuable contributions to our project.\\n\\nReferences\\n\\n2008\u20132024. Grobid. https://github.com/kermitt2/grobid.\\n\\nIsabelle Augenstein, Mrinal Das, Sebastian Riedel, Lakshmi Vikraman, and Andrew McCallum. 2017. SemEval 2017 task 10: ScienceIE - extracting keyphrases and relations from scientific publications. In Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pages 546\u2013555, Vancouver, Canada. Association for Computational Linguistics.\\n\\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3615\u20133620, Hong Kong, China. Association for Computational Linguistics.\\n\\nSatadisha Saha Bhowmick, Eduard CDragut, and Weiyi Meng. 2022. Boosting entity mention detection for targeted twitter streams with global contextual embeddings. In 2022 IEEE 38th International Conference on Data Engineering (ICDE), pages 1085\u20131097. IEEE.\\n\\nSatadisha Saha Bhowmick, Eduard CDragut, and Weiyi Meng. 2023. Globally aware contextual embeddings for named entity recognition in social media streams. In 2023 IEEE 39th International Conference on Data Engineering (ICDE), pages 1544\u20131557. IEEE.\\n\\nZhijia Chen, Lihong He, Arjun Mukherjee, and Eduard CDragut. 2024. Comquest: Large scale user comment crawling and integration. In SIGMOD Conference Companion, pages 432\u2013435.\\n\\nZhijia Chen, Weiyi Meng, and Eduard Dragut. 2022. Web record extraction with invariants. Proceedings of the VLDB Endowment, 16(4):959\u2013972.\\n\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\\n\\nMark Davies and Joseph L Fleiss. 1982. Measuring agreement for multinomial data. *Biometrics*, pages 1047\u20131051.\\n\\nKata G\u00e1bor, Davide Buscaldi, Anne-Kathrin Schummann, Behrang QasemiZadeh, Ha\u00effa Zargayouna, and Thierry Charnois. 2018. SemEval-2018 task 7: Semantic relation extraction and classification in scientific papers. In Proceedings of the 12th International Workshop on Semantic Evaluation, pages 679\u2013688, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021. SimCSE: Simple contrastive learning of sentence embeddings. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6894\u20136910, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nNikita Gautam, David Shumway, Megan Kowalcyk, Sarthak Khanal, Doina Caragea, Cornelia Caragea, Hande Mcginty, and Samuel Dorevitch. 2023. Leveraging existing literature on the web and deep neural models to build a knowledge graph focused on water quality and health risks. In Proceedings of the ACM Web Conference 2023, pages 4161\u20134171.\\n\\nPaul Groth, Mike Lauruhn, Antony Scerri, and Ron Daniel Jr. 2018. Open information extraction on scientific text: An evaluation. In Proceedings of the 27th International Conference on Computational Linguistics, pages 3414\u20133423, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nJenny Heddes, Pim Meerdink, Miguel Pieters, and Maarten Marx. 2021. The automatic detection of dataset names in scientific articles. *Data*, 6(8):84.\\n\\nYufang Hou, Charles Jochim, Martin Gleize, Francesca Bonin, and Debasis Ganguly. 2021. TDMSci: A specialized corpus for scientific literature entity tagging of tasks datasets and metrics. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 707\u2013714, Online. Association for Computational Linguistics.\\n\\nYan Hu, Qingyu Chen, Jingcheng Du, Xueqing Peng, Vipina Kuttichi Keloth, Xu Zuo, Yujia Zhou, Zehan Li, Xiaoqian Jiang, Zhiyong Lu, et al. 2024. Improving large language models for clinical named entity recognition via prompt engineering. *Journal of the American Medical Informatics Association*, page ocad259.\\n\\nSarthak Jain, Madeleine van Zuylen, Hannaneh Hajishirzi, and Iz Beltagy. 2020. SciREX: A challenge dataset for document-level information extraction. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7506\u20137516, Online. Association for Computational Linguistics.\\n\\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton Washington, You Chen, Lang Li, Huan Sun, and Yu Su. 2022. Thinking about GPT-3 in-context learning for biomedical IE? thinkagain. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 4497\u20134512, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nYuhan Li, Jian Wu, Zhiwei Yu, B\u00f6rje F Karlsso, Wei Shen, Manabu Okumura, and Chin-Yew Lin. 2023. Unlocking science: Novel dataset and benchmark for cross-modality scientific information extraction. *arXiv preprint arXiv:2311.08189*.\\n\\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100\u2013114, Dublin, Ireland and Online. Association for Computational Linguistics.\\n\\nKun Liu, Yao Fu, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang, and Sheng Gao. 2021. Noisy-labeled NER with confidence estimation. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3437\u20133445, Online. Association for Computational Linguistics.\\n\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kennedy, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983, Online. Association for Computational Linguistics.\\n\\nYi Luan, Luheng He, Mari Ostendorf, and Hannaneh Hajishirzi. 2018. Multi-task identification of entities, relations, and coreference for scientific knowledge graph construction. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3219\u20133232, Brussels, Belgium. Association for Computational Linguistics.\\n\\nYi Luan, Mari Ostendorf, and Hannaneh Hajishirzi. 2017. Scientific information extraction with semi-supervised neural tagging. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2641\u20132651, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nYubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 10572\u201310601, Singapore. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wolfgang Otto, Matth\u00e4us Zloch, Lu Gan, Saurav Kar-makar, and Stefan Dietze. 2023. GSAP-NER: A novel task, corpus, and baseline for scholarly entity extraction focused on machine learning models and datasets. In *FindingsoftheAssociationforComputationalLinguistics: EMNLP2023*, pages 8166\u20138176, Singapore. Association for Computational Linguistics.\\n\\nHuitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, and Longin Jan Latecki. 2024a. Flowlearn: Evaluating large vision-languagemodelsonflowchartunderstanding. *arXiv preprint arXiv:2407.05183*.\\n\\nHuitong Pan, Qi Zhang, Cornelia Caragea, Eduard Dragut, and Longin Jan Latecki. 2024b. SciDMT: A large-scale corpus for detecting scientific mentions. In *Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)*, pages 14407\u201314417, Torino, Italia. ELRA and ICCL.\\n\\nHuitong Pan, Qi Zhang, Eduard Dragut, Cornelia Caragea, and Longin Jan Latecki. 2023. DMDD: A large-scale dataset for dataset mentions detection. *Transactions of the Association for Computational Linguistics*, 11:1132\u20131146.\\n\\nSeo Park and Cornelia Caragea. 2023. Multi-task knowledge distillation with embedding constraints for scholarly keyphrase boundary classification. In *Proceedingsofthe2023ConferenceonEmpiricalMethodsinNatural Language Processing*, pages 13026\u201313042, Singapore. Association for Computational Linguistics.\\n\\nDong Pham, Xanh Ho, Quang Thuy Ha, and Akiko Aizawa. 2023. Solving label variation in scientific information extraction via multi-task learning. In *Proceedings of the 37th Pacific Asia Conference on Language, Information and Computation*, pages 243\u2013256, Hong Kong, China. Association for Computational Linguistics.\\n\\nBehrang QasemiZadeh and Anne-Kathrin Schumann. 2016. The ACL RD-TEC 2.0: A language resource for evaluating term extraction and entity recognition methods. In *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916)*, pages 1862\u20131868, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nChengwei Qin, Aston Zhang, Zhuosheng Zhang, Jiaao Chen, Michihiro Yasunaga, and Diyi Yang. 2023. Is ChatGPT a general-purposenaturallanguageprocess-ing task solver? In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 1339\u20131384, Singapore. Association for Computational Linguistics.\\n\\nMobashir Sadat and Cornelia Caragea. 2022. SciNLI: A corpus for natural language inference on scientific text. In *Proceedingsofthe60thAnnualMeetingoftheAssociation for Computational Linguistics (Volume 1: Long Papers)*, pages 7399\u20137409, Dublin, Ireland. Association for Computational Linguistics.\\n\\nOscar Sainz, Iker Garc\u00eda-Ferrero, Rodrigo Agerri, Oier Lopez de Lacalle, German Rigau, and Eneko Agirre. 2023. Gollie: Annotation guidelines improve zero-shot information-extraction. In *The Twelfth International Conference on Learning Representations*.\\n\\nYiheng Shu, Zhiwei Yu, Yuhan Li, B\u00f6rje Karlsson, Tingting Ma, Yuzhong Qu, and Chin-Yew Lin. 2022. TIARA: Multi-grained retrieval for robust question answering over large knowledge base. In *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*, pages 8108\u20138121, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas Scialom, Anthony Hartshorn, Elvis Saravia, Andrew Poulton, Viktor Kerkez, and Robert Stojnic. 2022. Galactica: A large languagemodel for science. *arXiv preprint arXiv:2211.09085*.\\n\\nVijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, and Graham Neubig. 2023. DataFinder: Scientific dataset recommendation from natural language descriptions. In *Proceedingsofthe61stAnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 10288\u201310303, Toronto, Canada. Association for Computational Linguistics.\\n\\nVijay Viswanathan, Graham Neubig, and Pengfei Liu. 2021. CitationIE: Leveraging the citation graph for scientific information extraction. In *Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)*, pages 719\u2013731, Online. Association for Computational Linguistics.\\n\\nSomin Wadhwa, Silvio Amir, and Byron Wallace. 2023. Revisiting relation extraction in the era of large languagemodels. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 15566\u201315589, Toronto, Canada. Association for Computational Linguistics.\\n\\nZhen Wan, Fei Cheng, Zhuoyuan Mao, Qianying Liu, Haiyue Song, Jiwei Li, and Sadao Kurohashi. 2023. GPT-RE: In-context learning for relation extraction using large languagemodels. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 3534\u20133547, Singapore. Association for Computational Linguistics.\\n\\nQingyun Wang, Manling Li, Xuan Wang, Nikolaus Parulian, Guangxing Han, Jiawei Ma, Jingxuan Tu, Ying Lin, Ranran Haoran Zhang, Weili Liu, Aabhas Chauhan, Yingjun Guan, Bangzheng Li, Ruisong Li, Xiangchen Song, Yi Fung, Heng Ji, Jiawei Han, Shih-Fu Chang, James Pustejovsky, Jasmine Rah, David Liem, Ahmed ELsayed, Martha Palmer, Clare Voss, Cynthia Schneider, and Boyan Onyshkevych. 2021. COVID-19 literature knowledge graph construction and drug repurposing report generation. In *Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing*. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Silei Xu, Shicheng Liu, Theo Culhane, Elizaveta Pertseva, Meng-Hsi Wu, Sina Semnani, and Monica Lam. 2023. Fine-tuned LLMs know more, hallucinate less with few-shot sequence-to-sequence semantic parsing over Wikidata. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 5778\u20135791, Singapore. Association for Computational Linguistics.\\n\\nZhaohui Yan, Songlin Yang, Wei Liu, and Kewei Tu. 2023. Joint entity and relation extraction with span pruning and hypergraph neural networks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 7512\u20137526, Singapore. Association for Computational Linguistics.\\n\\nDeming Ye, Yankai Lin, Peng Li, and Maosong Sun. 2022. Packed levitated marker for entity and relation extraction. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 4904\u20134917, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKlim Zaporojets, Lucie-Aim\u00e9e Kaffee, Johannes Deleu, Thomas Demeester, Chris Develder, and Isabelle Augenstein. 2022. Tempel: Linking dynamically evolving and newly emerging entities. Advances in Neural Information Processing Systems, 35:1850\u20131866.\\n\\nShanshan Zhang, Lihong He, Eduard Dragut, and Slobodan Vucetic. 2019. How to invest my time: Lessons from human-in-the-loop entity extraction. In Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining, pages 2305\u20132313.\\n\\nShanshan Zhang, Lihong He, Slobodan Vucetic, and Eduard Dragut. 2018. Regular expression guided entity mention mining from noisy web data. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1991\u20132000, Brussels, Belgium. Association for Computational Linguistics.\\n\\nZexuan Zhong and Danqi Chen. 2021. A frustratingly easy approach for entity and relation extraction. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 50\u201361, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A More statistics\\n\\nA.1 Re-annotating documents from SciDMT\\n\\nTable 4 presents the details of the entities annotation workload of the 100 documents from SciDMT. Specifically, the 100 documents from SciDMT-E original contains 21281 entity annotations. After our re-annotation process, we compare against the previous SciDMT-E entity annotation, we find that we keep 15989 correctly annotated entities, and remove 709 wrongly annotated entities, fixed 4583 entities and add 2651 new entities. Finally, for this 100 publications we derive from SciDMT contains 23223 entity annotations. Totally, we revived 7234 entities.\\n\\n| Initial | Correct | Removed | Fixed | Added | Final |\\n|---------|---------|---------|-------|-------|-------|\\n| 21281   | 15989   | 709     | 4583  | 2651  | 23223 |\\n\\nTable 4: The details of our entity annotations efforts for the first 100 documents.\\n\\nA.2 Comparison with SciERC\\n\\nTable 5 and Table 6 show the label statistics of SciERC when only keep the DATASET, METHOD, and TASK entities. We can find that, though SciERC annotated 500 abstract, there are only 1575 entities and 1575 relations related to DATASET, METHOD, and TASK.\\n\\n| Relation type | # |\\n|---------------|---|\\n| FEATURE-OF    | 28 |\\n| CONJUNCTION   | 292 |\\n| USED-FOR      | 876 |\\n| COMPARE       | 78 |\\n| HYPONYM-OF    | 154 |\\n| PART-OF       | 78 |\\n| EVALUATION-FOR| 69 |\\n| Total         | 1575 |\\n\\nTable 5: The relation types distribution of datasets (material), methods, and tasks in SciERC.\\n\\n| Dataset | Dataset Method Task Total |\\n|---------|---------------------------|-----------------|\\n| SciERC  | 561 1592 997 1575         |\\n| SciER   | 3942 15881 4695 24518     |\\n\\nTable 6: The entity distribution of datasets (material), methods, and tasks in SciERC and SciER.\\n\\nA.3 SciER Statistics\\n\\nTable 7 provides the label distribution of the train, development, ID test and OOD test of our proposed SciER.\\n\\n| Rel./Ent. Type | Train | Dev | ID Test | OOD Test | Total |\\n|----------------|-------|-----|---------|----------|-------|\\n| DATASET        | 11424 | 1549| 1890    | 1018     | 15881 |\\n| TASK           | 3397  | 416 | 688     | 194      | 4695  |\\n| PART-OF        | 1865  | 214 | 304     | 111      | 2494  |\\n| USED-FOR       | 2398  | 343 | 546     | 167      | 3454  |\\n| EVALUATED-WITH | 863   | 78  | 131     | 49       | 1121  |\\n| SYNONYM-OF     | 880   | 76  | 170     | 89       | 1215  |\\n| COMPARE-WITH   | 875   | 175 | 114     | 54       | 1218  |\\n| SUBCLASS-OF    | 697   | 114 | 176     | 73       | 1060  |\\n| BENCHMARK-FOR  | 551   | 64  | 85      | 28       | 728   |\\n| SUBTASK-OF     | 210   | 31  | 65      | 9        | 315   |\\n| TRAINED-WITH   | 404   | 37  | 35      | 2        | 478   |\\n| Total          | 8743  | 1132| 1626    | 582      | 12083 |\\n\\nTable 7: The label distribution of our SciER.\\n\\nB More Implementation Details\\n\\nB.1 Supervised Baselines\\n\\nWe followed the hyperparameter settings recommended in the PURE, PL-Maker, and HGERE papers respectively. All experiments were conducted using two NVIDIA A100 80GB GPUs for training. All reported experimental results represent the average of five runs, each with a different random seed.\\n\\n| Engine        | Temperature | Max_tokens | Top_p  |\\n|---------------|-------------|------------|--------|\\n| gpt-3.5-turbo | 0.3         | 256        | 0.9    |\\n| Llama3-70b-instruct | 0.3  | 256        | 0.9    |\\n| Qwen2-72b-instruct     | 0.3         | 256        | 0.9    |\\n\\nTable 8: Hyperparameters of GPT-3.5-turbo, Llama3-70b and Qwen2-72b.\\n\\nB.2 LLM-based Baselines\\n\\nThe hyperparameters of GPT-3.5-turbo, Llama3-70b, and Qwen2-72b are presented in the Table 8. The used version of SimCSE is sup-simcse-roberta-large. To ensure fairness in the comparison, we kept the inference hyperparameters consistent for both models. For the GPT-3.5-turbo experiments, due to cost considerations, we sampled 200 sentences from each test set for testing, conducted the tests three times, and then averaged the results. The total cost of GPT-3.5-turbo experiments are 50.25 dollars.\\n\\nFor the Llama3-70b and Qwen2-72b, we used two NVIDIA A100 80GB GPUs for inference. We tested on all samples in each test set, conducted the tests five times, and then averaged the results. Due to the input length limitation of Llama3-70b, we refer to https://huggingface.co/princeton-nlp/sup-simcse-roberta-large.\"}"}
{"id": "emnlp-2024-main-726", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and the lengths of our prompt templates, we set\\nthe number of demonstrations for each task as 30,\\nwhich is also recommended by recent GPT-3 based\\nrelation extraction work (Wan et al., 2023).\\n\\nC. More analysis\\n\\nC.1 Qualitative Example\\n\\nTable 9 shows one OOD test example for different\\nmodels. We observe that both PL-Marker and\\nHGERE fail on this example due to the NER results.\\nPL-Marker ignores the TASK \\\"therapeutic molecular\\ngeneration\\\", and HGERE predicts the wrong span.\\nBut if we provide the gold standard entities to\\nPL-Marker, i.e., the PL-Marker (RE). It predicts\\ncorrectly. All LLMs-based baselines perform well\\non this example.\\n\\nC.2 Relation Extraction Statistic\\n\\nWe present the proportion of NULL categories in\\nthe RE task in the table 10. We found that the\\nproportion exceeds 60%.\\n\\nD Prompt Design\\n\\nIn this section, we provide the details of annotation\\nguideline-enhanced prompt designs for each task.\\nWe list the few-shot version of NER, RE, and Joint\\nERE. To save the space, we only keep provide 1\\ndemonstration for each task. In our experiments,\\nwe use 30 demonstrations. All the zero-shot version\\nare just removed the demonstrations.\\n\\n### Few-Shot NER\\n\\n**Task:** Generate an HTML version of an\\ninput text, marking up specific entities\\nrelated to machine learning and\\nartificial intelligence. The entities to\\nbe identified are: 'Dataset', 'Task', and\\n'Method'. Use HTML <span> tags to\\nhighlight these entities. Each <span>\\nshould have a class attribute indicating\\nthe type of the entity.\\n\\n**Entity Definitions:**\\n\\n- 'Task': A task in machine learning\\n  refers to the specific problem or type\\n  of problem that a ML/AI model/method is\\ndesigned to solve. Tasks can be broad,\\nlike classification, regression, or\\nclustering, or they can be very\\nspecific, such as Pedestrian Detection,\\nAutonomous Driving, Sentiment Analysis,\\nNamed Entity Recognition and Relation\\nExtraction...\\n\\n- 'Method': A method entity refers\\nto the approach, algorithm, or technique\\nused to solve a specific task/problem.\\nMethods encompass the computational\\nalgorithms, model architectures, and the\\ntraining procedures that are employed to\\nmake predictions or decisions based on\\ndata. For example, Convolutional Neural\\nNetworks, Dropout, data augmentation,\\nrecurrent neural networks...\\n\\n- 'Dataset': A realistic collection of\\ndata that is used for training,\\nvalidating, or testing the algorithms.\\nThese datasets can consist of various\\nforms of data such as text, images,\\nvideos, or structured data. For example,\\nMNIST, COCO, AGNews, IMDb...\\n\\n**Entity Markup Guide:**\\n\\n- Use <span class=\\\"Task\\\"> to denote a\\nTask entity.\\n- Use <span class=\\\"Method\\\"> to denote a\\nMethod entity.\\n- Use <span class=\\\"Dataset\\\"> to denote a\\nDataset entity.\\n\\n**Other Notes:**\\n\\n- Generics cannot be used independently\\nto refer to any specific entities, e.g.,\\n'This task', 'the dataset', and 'a\\npublic corpus' are not entities.\\n\\n- The determiners should not be part of\\nan entity span. For example, given span\\n'The SQuAD v1.1 dataset', where the\\ndeterminer 'the' should be excluded the\\nentity span.\\n\\n- If both the full name and the\\nabbreviation are present in the\\nsentence, annotate the abbreviation and\\nits corresponding full name separately.\\nFor instance, '20-newsgroup ( 20NG )',\\nthe annotation should be '<span\\nclass=\\\"Dataset\\\">20-newsgroup</span> (\\n<span class=\\\"Dataset\\\">20NG</span> )'.\\n\\n- If one entity with exact same span text\\nappears many times within a sentence,\\nall span text should be marked up.\\n\\n- If one sentence without any entities\\nappear, do not mark up any span text.\\n\\n- Only annotate \\\"factual,\\ncontent-bearing\\\" entities. Task,\\ndataset, and method entities normally\\nhave specific names and their meanings\"}"}
{"id": "emnlp-2024-main-726", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5 shows the process undertaken by GxVAEs for therapeutic molecular generation.\\n\\nTable 9: Test results of one OOD test example with PL-Marker, HGERE, Llama3-70b (joint), GPT-3.5-Turbo (joint), Llama3-70b (pipeline), GPT-3.5-Turbo (pipeline). The PL-Marker (RE) means using PL-Marker to predict the relation with given two entities.\\n\\n|                      | NULL Tot | NULL (%) |\\n|----------------------|----------|----------|\\n| ID test set         | 4715     | 74.46%   |\\n| OOD test set        | 1109     | 65.58%   |\\n| Dev                 | 2053     | 64.46%   |\\n| Train               | 20923    | 70.53%   |\\n\\nTable 10: Statistics of datasets for relation extraction.\\n\\n\\\"NULL\\\" means the given subject and object pairs do not have relation.\\n\\nFor example, the \\\"CoNLL03\\\", \\\"SNLI\\\" are factual entities.\\n\\n- Minimum span principle. Annotators should annotate only the minimum span necessary to represent the original meaning of task/dataset/metric (e.g.: \\\"The\\\", \\\"dataset\\\", \\\"public\\\", 'method', 'technique' are often omitted).\\n\\n### Examples:\\n\\n**Input:**\\nIn particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks (CNNs), describe the architectures used in our analysis and the algorithms adopted to train and apply them.\\n\\n**Output:**\\nIn particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks (CNNs), describe the architectures used in our analysis and the algorithms adopted to train and apply them.\\n\\n**Input:**\\nSpecifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks.\\n\\n**Output:**\\n\\n### Task:\\nBased on the given sentence, and subject entity and object entity from the sentence, answer the questions to determine the relationship between them. The potential relations are: ['Part-Of', 'SubClass-Of', 'SubTask-Of', 'Benchmark-For', 'Trained-With', 'Evaluated-With', 'Synonym-Of', 'Used-For', 'Compare-With']. Answer 'NULL' to indicate that there is no relationship between the entities.\\n\\n### Relationship Definitions:\\n\\n- 'Part-Of': This relationship denotes that one method is a component or a part of another method.\\n- 'SubClass-Of': Specifies that one method is a subclass or a specialized version of another method.\\n- 'SubTask-Of': Indicates that one task...\"}"}
{"id": "emnlp-2024-main-726", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is a subset or a specific aspect of another broader task.\\n\\n- 'Benchmark-For': Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.\\n- 'Trained-With': Indicates that a method is trained using a specific dataset.\\n- 'Evaluated-With': This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.\\n- 'Synonym-Of': Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.\\n- 'Used-For': Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.\\n- 'Compare-With': This relationship is used when one entity is compared with another to highlight differences, similarities, or both.\\n\\n### Notes:\\n- Determine the 'Relationship' that best describes how the entities are related, or just answer 'NULL' if no relationship exists.\\n- Please do not annotate negative relations. For example, X is not used in Y or X is hard to be applied in Y.\\n- Annotate a relationship only if there is direct evidence or clear implication in the text. Avoid inferring relationships that are not explicitly mentioned or clearly implied.\\n\\n### Examples:\\n\\n- **Input:**\\n  In particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks (CNNs), describe the architectures used in our analysis and the algorithms adopted to train and apply them.\\n\\n- **Subject Entity:** Convolutional Neural Network\\n\\n- **Object Entity:** CNNs\\n\\n- **Output:** Synonym-Of\\n\\n- **Input:**\\n  Specifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks.\\n\\n- **Subject Entity:** attention\\n\\n- **Object Entity:** feature extraction mechanisms\\n\\n- **Output:** Few-Shot Joint ERE\\n\\n### Task:\\nIdentify and extract all relationship triplets consisting of two entities and their relationship from the input text. Each triplet consists of one subject entity, one object entity and their relationship. The interested entity types are: ['Dataset', 'Method', 'Task']. The potential relations are: ['Part-Of', 'SubClass-Of', 'SubTask-Of', 'Benchmark-For', 'Trained-With', 'Evaluated-With', 'Synonym-Of', 'Used-For', 'Compare-With']. Answer 'NULL' to indicate that there is no triplet.\\n\\n### Entity Definitions:\\n- 'Task': A task in machine learning refers to the specific problem or type of problem that a ML/AI model/method is designed to solve. Tasks can be broad, like classification, regression, or clustering, or they can be very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction...\\n- 'Method': A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks, Dropout, data augmentation, recurrent neural networks...\\n- 'Dataset': A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various data types and have different purposes, such as training, validation, or testing.\\n\\n...\"}"}
{"id": "emnlp-2024-main-726", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb...\\n\\n### Relationship Definitions:\\n- **Part-Of**: This relationship denotes that one method is a component or a part of another method.\\n- **SubClass-Of**: Specifies that one method is a subclass or a specialized version of another method.\\n- **SubTask-Of**: Indicates that one task is a subset or a specific aspect of another broader task.\\n- **Benchmark-For**: Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.\\n- **Trained-With**: Indicates that a method is trained using a specific dataset.\\n- **Evaluated-With**: This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.\\n- **Synonym-Of**: Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.\\n- **Used-For**: Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.\\n- **Compare-With**: This relationship is used when one entity is compared with another to highlight differences, similarities, or both.\\n\\n### Notes:\\n- Input sentence has one triplet:\\n  \\n  ```\\n  [['entity1 span text:entity1 type', 'relationship', 'entity2 span text:entity2 type']]\\n  ```\\n- Input sentence has no triplets: `[]`\\n- Annotate a relationship only if there is direct evidence or clear implication in the text. Avoid inferring relationships that are not explicitly mentioned or clearly implied.\\n- Ensure that the entity spans are exact extracts from the input text and that the relationships accurately reflect the described interactions. Ensure the output is in the correct format (A list of triplets).\\n- Entities in the triplet should have the same form as input sentence.\\n\\n### Examples:\\n**Input:**\\n\\nIn particular we briefly introduce the principal concepts behind deep Convolutional Neural Networks (CNNs), describe the architectures used in our analysis and the algorithms adopted to train and apply them.\\n\\n**Output:**\\n\\n```\\n[['CNNs:Method', 'Synonym-Of', 'Convolutional Neural Networks:Method']]\\n```\\n\\n**Input:**\\n\\nSpecifically, we investigate the attention and feature extraction mechanisms of state-of-the-art recurrent neural networks and self-attentive architectures for sentiment analysis, entailment and machine translation under adversarial attacks.\\n\\n**Output:**\\n\\n```\\n\\n### E Annotation Guideline\\n\\nThis section contains the basic information from our annotation guideline for double-blind review.\\n\\n### E.1 Annotation Tool\\n\\nWe use the INCEpTION as our annotation platform. Figure 5 shows our annotation interface.\\n\\n### E.2 Entity Annotation\\n\\nScientific entities in the machine learning (ML) or Artificial intelligence (AI) domains refer to key\\n\\n6https://github.com/inception-project/inception\"}"}
{"id": "emnlp-2024-main-726", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"concepts or components that are integral to the structure and study of ML/AI papers. We follow the definition of entities/terms and build our annotation guides for NER based on the ACL RD-TEC Annotation Guideline (QasemiZadeh and Schumann, 2016), Papers With Code (PwC) and SciDMT (Pan et al., 2024b). We are interested in three specific entity types: Dataset, Task, and Method.\\n\\n**Dataset:** A realistic collection of data that is used for training, validating, or testing the algorithms. These datasets can consist of various forms of data such as text, images, videos, or structured data. For example, MNIST, COCO, AGNews, IMDb, etc.\\n\\n**Task:** A task in machine learning refers to the specific problem or type of problem that a ML/AI model is designed to solve. Tasks can be broad, like classification, regression, or clustering, or very specific, such as Pedestrian Detection, Autonomous Driving, Sentiment Analysis, Named Entity Recognition and Relation Extraction.\\n\\n**Method:** A method entity refers to the approach, algorithm, or technique used to solve a specific task/problem. Methods encompass the computational algorithms, model architectures, and the training procedures that are employed to make predictions or decisions based on data. For example, Convolutional Neural Networks (CNNs),...\\n\\n**Annotation Notes:**\\n\\nConsidering that annotators may have varying understandings of the annotation details, we have defined a set of rules and notes to standardize the annotation process:\\n\\n- Do not annotate generics and determiners. Generics cannot be used independently to refer to any specific entities, e.g., \u201cthis task\u201d, \u201cthe dataset\u201d, \u201ca public corpus\u201d etc. The determiners should not be part of an entity span. For example, given span \u201cthe SQuAD v1.1 dataset\u201d, where the determiner \u201cthe\u201d should be excluded from the entity span.\\n\\n- Minimum span principle. Annotators should annotate only the minimum span necessary to represent the original meaning of task/dataset/metric (e.g., \u201cThe\u201d, \u201cdataset\u201d, \u201cpublic\u201d, \u2018method\u2019, \u2018technique\u2019 are often omitted).\\n\\n- Only annotate \u201cfactual, content-bearing\u201d entities. Task, dataset, and method entities normally have specific names and their meanings are consistent across different papers. For example, the \u201cCoNLL03\u201d, \u201cSNLI\u201d are factual entities.\\n\\n- If one entity with exact same span text appears many times within a sentence, all span text should be annotated.\\n\\n**E.3 Relation Annotation**\\n\\nRelation links cannot exceed the sentence boundary. We define 9 types of relations for Dataset, Method, and Task entities.\\n\\n**Relation Definitions:**\\n\\n- \u2018Part-Of\u2019: This relationship denotes that one method is a component or a part of another method.\\n- \u2018SubClass-Of\u2019: Specifies that one method is a subclass or a specialized version of another method.\\n- \u2018SubTask-Of\u2019: Indicates that one task is a subset or a specific aspect of another broader task.\\n- \u2018Benchmark-For\u2019: Shows that a dataset serves as a standard or benchmark for evaluating the performance of methods on a specific task.\\n- \u2018Trained-With\u2019: Indicates that a method is trained using a specific dataset.\\n- \u2018Evaluated-With\u2019: This relationship denotes that a method is evaluated using a specific dataset to test its performance or conduct the experiments.\\n- \u2018Synonym-Of\u2019: Indicates that two terms or entities are considered to have the same or very similar meaning, such as abbreviation.\\n- \u2018Used-For\u2019: Shows that one entity is utilized for achieving or performing another entity. For example, one Method is Used-For one Task. This relationship is highly flexible, allowing for generic relationships across diverse entities.\\n- \u2018Compare-With\u2019: This relationship is used when one entity is compared with another to highlight differences, similarities, or both.\\n\\n**Annotation Notes:**\\n\\n- Do not annotate negative relations. For example, X is not used in Y or X is hard to be applied in Y.\\n- Verify that the entities involved in the relation match the prescribed types (e.g., Method-Dataset for Trained-With). Incorrect entity types should not be linked by these specific relations.\\n- Annotate a relationship only if there is direct evidence or clear implication in the text. Avoid inferring relationships that are not explicitly mentioned or clearly implied.\\n- Ensure consistency in how relationships are annotated across different texts. If uncertain, refer back to the guideline definitions or consult with a supervisor.\\n- Do not make assumptions about relationships based on personal knowledge or external information. Rely solely on the information provided in the text.\"}"}
