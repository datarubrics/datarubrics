{"id": "lrec-2022-1-317", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"JGLUE: Japanese General Language Understanding Evaluation\\n\\nKentaro Kurihara, Daisuke Kawahara, Tomohide Shibata\\n\\nWaseda University, Yahoo Japan Corporation\\n\\nAbstract\\nTo develop high-performance natural language understanding (NLU) models, it is necessary to have a benchmark to evaluate and analyze NLU ability from various perspectives. While the English NLU benchmark, GLUE (Wang et al., 2018), has been the forerunner, benchmarks are now being released for languages other than English, such as CLUE (Xu et al., 2020) for Chinese and FLUE (Le et al., 2020) for French; but there is no such benchmark for Japanese. We build a Japanese NLU benchmark, JGLUE, from scratch without translation to measure the general NLU ability in Japanese. We hope that JGLUE will facilitate NLU research in Japanese.\\n\\nKeywords: GLUE, Japanese, NLU benchmark, text classification, sentence pair classification, QA\\n\\n1. Introduction\\nTo develop high-performance natural language understanding (NLU) models, it is necessary to have a benchmark (a set of datasets) to evaluate and analyze NLU ability from various perspectives. In the case of English, the GLUE (General Language Understanding Evaluation) benchmark (Wang et al., 2018) is publicly available. Once an NLU model that can achieve a certain level of high score on GLUE is developed, a more difficult benchmark, such as SuperGLUE (Wang et al., 2019), is released, creating a virtuous cycle of benchmark construction and NLU model development.\\n\\nAlong with the trend of active NLU studies in English, benchmarks for languages other than English have been constructed, including CLUE (Xu et al., 2020) for Chinese, FLUE (Le et al., 2020) for French, and KLUE (Park et al., 2021) for Korean.\\n\\nAlthough there are many studies on Japanese, which is the 13th most spoken language in the world as of 2021, there is no benchmark such as GLUE. Japanese is linguistically different from English and other languages in the following aspects.\\n\\n\u2022 The Japanese alphabet includes hiragana, katakana, Chinese characters, and the Latin alphabet.\\n\u2022 There are no spaces between words.\\n\u2022 The word order is relatively free.\\n\\nDue to these differences, findings on English datasets are not necessarily applicable to Japanese. Given this situation, there is an urgent need to develop a benchmark for Japanese NLU. Although individual Japanese datasets, such as JSNLI (Yoshikoshi et al., 2020) and JSICK (Yanaka and Mineshima, 2021), have been constructed, their construction methods involve mainly machine translation or manual translation from English datasets. With either of these translation methods, the unnaturalness of a translated text and the cultural/social discrepancy between an original language (mostly English) and a target language (Japanese in our case) are major problems, as discussed in Clark et al. (2020) and Park et al. (2021). Although there are also Japanese datasets in specific domains, such as hotel reviews (Hayashibe, 2020) and the driving domain (Takahashi et al., 2019), these are not suitable for evaluating NLU ability in the general domain.\\n\\nIn this study, we build a general NLU benchmark for Japanese, JGLUE, from scratch without translation. JGLUE is designed to cover a wide range of GLUE and SuperGLUE tasks and consists of three kinds of tasks: text classification, sentence pair classification, and QA, as shown in Table 1. Each task consists of multiple datasets. JGLUE is available at https://randd.yahoo.co.jp/en/softwaredata#jglue. We hope that this benchmark will facilitate NLU research in Japanese.\\n\\n2. Related Work\\nThe first benchmark for evaluating NLU models is GLUE, which consists of two kinds of tasks, i.e., sentence classification and sentence pair classification, and nine datasets in total. SuperGLUE is a more difficult benchmark than GLUE, which contains eight datasets. It keeps the most challenging dataset of GLUE, i.e., natural language inference (NLI), and adds more difficult tasks, such as QA and commonsense reasoning. Such benchmark construction in English has stimulated the development of NLU models, including BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and many others.\\n\\nTable 1: JGLUE overview.\\n\\n| Task          | Dataset   | Train | Dev  | Test |\\n|---------------|-----------|-------|------|------|\\n| Text MARC-ja  | 187,528   | 5,654 | 5,639|\\n| Classification JCoLA | \u2014     | \u2014     | \u2014    |\\n| Sentence Pair JSTS | 12,451   | 1,457 | 1,589|\\n| Classification JNLI | 20,073   | 2,434 | 2,508|\\n| QA JSQuAD     | 63,870    | 4,475 | 4,470|\\n|               | JCommonsenseQA | 9,012 | 1,119 | 1,118|\\n\\nWe hope that this benchmark will facilitate NLU research in Japanese.\"}"}
{"id": "lrec-2022-1-317", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of MARC-ja.\\n\\n|          | Label       | Train       | Dev         | Test        | Total       |\\n|----------|-------------|-------------|-------------|-------------|-------------|\\n|          | positive    | 165,477     | 4,832       | 4,895       | 175,204     |\\n|          | negative    | 22,051      | 822         | 744         | 23,617      |\\n|          | Overall     | 187,528     | 5,654       | 5,639       | 198,821     |\\n\\nvlin et al., 2019) and many extended models. This situation has caused a growing movement to build NLU benchmarks in many languages, such as CLUE, FLUE, KLUE, IndicGLUE (Kakwani et al., 2020), ARLUE (Abdul-Mageed et al., 2021), ALUE (Seelawi et al., 2021), and CLUB (Rodriguez-Penagos et al., 2021), in Chinese, French, Korean, Indian languages, Arabic, and Catalan. Multilingual benchmarks, such as XGLUE (Liang et al., 2020), XTREME (Hu et al., 2020), and XTREME-R (Ruder et al., 2021), have also been built. Although they contain datasets in various languages, only a few of them include Japanese.\\n\\n3. JGLUE Benchmark\\n\\nJGLUE consists of the tasks of text classification, sentence pair classification, and QA, as shown in Table 1. In the following sections, we explain how to construct the datasets for each task. As one of the text classification datasets, JCoLA (the Japanese version of CoLA (Warstadt et al., 2019), the Corpus of Linguistic Acceptability) will be provided by another research organization. Since it is still under construction, this paper does not explain it.\\n\\nWe use Yahoo! Crowdsourcing for all crowdsourcing tasks in constructing each dataset.\\n\\n3.1. MARC-ja\\n\\nAs one of the text classification datasets, we build a dataset based on the Multilingual Amazon Reviews Corpus (MARC) (Keung et al., 2020). MARC is a multilingual corpus of product reviews with 5-level star ratings (1-5) on the Amazon shopping site. This corpus covers six languages, including English and Japanese. For JGLUE, we use the Japanese part of MARC and to make it easy for both humans and computers to judge a class label, we cast the text classification task as a binary classification task, where 1- and 2-star ratings are converted to \u201cnegative\u201d, and 4 and 5 are converted to \u201cpositive\u201d. We do not use reviews with a 3-star rating.\\n\\nOne of the problems with MARC is that it sometimes contains data where the rating diverges from the review text. This happens, for example, when a review with positive content is given a rating of 1 or 2. These data degrade the quality of our dataset.\\n\\nTo improve the quality of the dev/test instances used for evaluation, we crowdsource a positive/negative judgment task for approximately 12,000 reviews. We adopt only reviews with the same votes from 7 or more out of 10 workers and assign a label of the maximum votes to these reviews. We divide the resulting reviews into dev/test data.\\n\\nWe obtained 5,654 and 5,639 instances for the dev and test data, respectively, through the above procedure. For the training data, we extracted 187,528 instances directly from MARC without performing the cleaning procedure because of the large number of training instances. The statistics of MARC-ja are listed in Table 2. For the evaluation metric for MARC-ja, we use accuracy because it is a binary classification task of texts.\\n\\n3.2. JSTS and JNLI\\n\\nFor the sentence pair classification datasets, we construct a semantic textual similarity (STS) dataset, JSTS, and a natural language inference (NLI) dataset, JNLI.\\n\\n3.2.1. Overview\\n\\nSTS is a task of estimating the semantic similarity of a sentence pair. Gold similarity is usually assigned as an average of the integer values 0 (completely different meaning) to 5 (equivalent meaning) assigned by multiple workers through crowdsourcing.\\n\\nNLI is a task of recognizing the inference relation that a premise sentence has to a hypothesis sentence. Inference relations are generally defined by three labels: \u201centailment\u201d, \u201ccontradiction\u201d, and \u201cneutral\u201d. Gold inference relations are often assigned by majority voting after collecting answers from multiple workers through crowdsourcing.\\n\\nFor the STS and NLI tasks, STS-B (Cer et al., 2017) and MultiNLI (Williams et al., 2018) are included in GLUE, respectively. As Japanese datasets, JSNLI (Yoshikoshi et al., 2020) is a machine translated dataset of the NLI dataset SNLI (Stanford NLI), and JSICK (Yanaka and Mineshima, 2021) is a human translated dataset of the STS/NLI dataset SICK (Marelli et al., 2014). As mentioned in Section 1, these have problems originating from automatic/manual translations. To solve this problem, we construct STS/NLI datasets in Japanese from scratch.\\n\\nWe basically extract sentence pairs in JSTS and JNLI from the Japanese version of the MS COCO Caption Dataset (Chen et al., 2015), the YJ Captions Dataset (Miyazaki and Shimizu, 2016).\\n\\nMost of the sentence pairs in JSTS and JNLI overlap, allowing us to analyze the relationship between similarities and inference relations for the same sentence pairs like SICK and JSICK.\\n\\nThe similarity value in JSTS is assigned a real number from 0 to 5 as in STS-B. The inference relation in JNLI is assigned from the above three labels as in SNLI and MultiNLI. The definitions of the inference relations are also based on SNLI.\\n\\nYJ Captions was constructed by crowdsourcing a task of writing five Japanese captions for each image in MS COCO (Lin et al., 2015).\"}"}
{"id": "lrec-2022-1-317", "page_num": 3, "content": "{\"primary_language\":\"ja\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1-1: \u9752\u3044\u8eca\u304c\u8d70\u3063\u3066\u3044\u308b\u3002\\nThe blue car is running.\\n1-2: \u6d77\u6cbf\u3044\u3092\u9752\u3044\u8eca\u304c\u8d70\u3063\u3066\u3044\u308b\u3002\\nA car is driving by the sea.\\n1-5: \u6b69\u9053\u306e\u53cd\u5bfe\u5074\u3092\u8eca\u304c\u8d70\u3063\u3066\u3044\u308b\u3002\\nA car is running on the other side of the sidewalk.\\n2-1: \u8349\u539f\u304c\u5e83\u304c\u3063\u3066\u3044\u308b\u3002\\nThe grasslands spread out.\\n2-2: \u9060\u304f\u306b\u5c71\u304c\u305d\u3073\u3048\u305f\u3063\u3066\u3044\u308b\u3002\\nA mountain rises in the distance.\\n2-5: \u5c71\u306e\u9e93\u306b\u6728\u3005\u304c\u751f\u3048\u3066\u3044\u308b\u3002\\nThe trees are growing at the foot of the mountain.\\n3-1: \u6771\u713c\u3051\u306b\u7167\u3089\u3055\u308c\u3066\u3044\u308b\u7537\u6027\u3002\\nA man is illuminated by the setting sun.\\n3-2: \u77ed\u9aea\u306e\u7537\u6027\u304c\u7acb\u3063\u3066\u3044\u308b\u3002\\nA man with short hair is standing there.\\n3-5: \u9ed2\u3044\u670d\u3092\u7740\u305f\u7537\u6027\u304c\u7b11\u3063\u3066\u3044\u308b\u3002\\nA man dressed in black is laughing.\"}"}
{"id": "lrec-2022-1-317", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ference relations in both directions for sentence pairs. As mentioned earlier, it is difficult to collect instances of contradiction from JSTS-A, which was collected from the captions of the same images, and thus we collect instances of entailment and neutral in this step. We collect inference relation answers from 10 workers. If six or more people give the same answer, we adopt it as the gold label if it is entailment or neutral. To obtain inference relations in both directions for JSTS-A, we performed this task on 20,472 sentence pairs, twice as many as JSTS-A. As a result, we collected inference relations for 17,501 sentence pairs. We refer to this collected data as JNLI-A.\\n\\nWe do not use JSTS-B for the NLI task because it is difficult to define and determine the inference relations between captions of different images. To collect NLI instances of contradiction, we crowdsource a task of writing four contradictory sentences for each caption in YJ Captions. From the written sentences, we remove sentence pairs with an edit distance of 0.75 or higher to remove low-quality sentences, such as short sentences and sentences with low relevance to the original sentence.\\n\\nFurthermore, we perform a one-way NLI task with 10 workers to verify whether the created sentence pairs are contradictory. Only the sentence pairs answered as contradiction by at least six workers are adopted. Finally, since the contradiction relation has no direction, we automatically assign contradiction in the opposite direction of the adopted sentence pairs. Using 1,800 captions, we acquired 7,200 sentence pairs, from which we collected 3,779 sentence pairs to which we assigned the one-way contradiction relation. By automatically assigning the contradiction relation in the opposite direction, we doubled the number of instances to 7,558. We refer to this collected data as JNLI-C.\\n\\nFor the 3,779 sentence pairs collected in Step 4, we crowdsource an STS task, assigning similarity and filtering in the same way as in Steps 1 and 2. In this way, we collected 2,303 sentence pairs with gold similarity from 3,779 pairs. We refer to this collected data as JSTS-C.\\n\\nWe constructed JSTS from JSTS-A, B, and C and JNLI from JNLI-A and C. Finally, we filtered out 12 sentence pairs from JSTS and 44 pairs from JNLI based on automatic matching and manual checking. Table 3 shows examples of the JSTS and JNLI datasets. The statistics of JSTS and JNLI are listed in Tables 4 and 5, respectively.\\n\\n3. It is natural to define these relations as neutral. In SNLI, however, they are defined as contradiction, whereas such instances are not included in the dataset.\\n\\n| Similarity range | Train | Dev | Test | Total |\\n|------------------|-------|-----|------|-------|\\n| 0 - 1            | 2,837 | 353 | 405  | 3,595 |\\n| 1 - 2            | 1,752 | 184 | 160  | 2,096 |\\n| 2 - 3            | 2,784 | 308 | 355  | 3,447 |\\n| 3 - 4            | 3,719 | 466 | 488  | 4,673 |\\n| 4 - 5            | 1,359 | 146 | 181  | 1,686 |\\n| Overall          | 12,451| 1,457| 1,589| 15,497|\\n\\nTable 4: Statistics of JSTS.\\n\\n| Label     | Train | Dev | Test | Total |\\n|-----------|-------|-----|------|-------|\\n| entailment| 2,876 | 353 | 367  | 3,596 |\\n| neutral   | 11,193| 1,347| 1,365| 13,905|\\n| contradiction| 6,004| 734 | 776  | 7,514 |\\n| Overall   | 20,073| 2,434| 2,508| 25,015|\\n\\nTable 5: Statistics of JNLI.\\n\\n| Mean of variance | Standard deviation of variance |\\n|------------------|-------------------------------|\\n| 0.420            | 0.286                         |\\n\\nTable 6: Mean and standard deviation of variance of similarity values in JSTS.\\n\\nTo examine the quality of JSTS, we calculated the variance of the similarities of each sentence pair answered by 10 crowdworkers and took the mean and standard deviation for all the pairs. The resulting values were sufficiently small as listed in Table 6. These results guarantee the quality of our annotation.\\n\\nTo assess the inter-annotator agreement of JNLI, we calculated Fleiss' Kappa values for 10 crowdworkers' answers of all the sentence pairs. Its value was 0.399, demonstrating fair to moderate agreement. Although this result showed that each answer was not very reliable, aggregated labels obtained by majority voting could be reliable as shown in the human scores (reported in Section 4.2).\\n\\n3.2.3. Evaluation Metric\\n\\nThe evaluation metric for JSTS is the Pearson and Spearman correlation coefficients, following STS-B, and that for JNLI is accuracy, following SNLI and MultiNLI.\\n\\n3.3. JSQuAD\\n\\nAs QA datasets, we build a Japanese version of SQuAD (Rajpurkar et al., 2016), one of the datasets of reading comprehension, and a Japanese version of CommonsenseQA, which is explained in the next section.\\n\\nReading comprehension is the task of reading a document and answering questions about it. Many reading comprehension evaluation sets have been built in English, followed by those in other languages or multilingual ones.\\n\\nIn Japanese, reading comprehension datasets for quizzes (Suzuki et al., 2018) and those in the driving\"}"}
{"id": "lrec-2022-1-317", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"With the privatization of Japan National Railways on April 1, 1987, JR Tokai took over the operation of the line. The Sanyo Shinkansen, which was taken over by West Japan Railway Company (JR West), is interconnected with the JR Tokai Shinkansen, and trains owned by JR West are sometimes used for trains operating only on the Tokaido Shinkansen section. As of March 2020, the fastest train between Tokyo Station and Shin-Osaka Station takes 2 hours and 21 minutes at a maximum speed of 285 km/h.\"}"}
{"id": "lrec-2022-1-317", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"source a task of writing a question with only one target concept as the answer and a task of adding two distractors. We describe the detailed construction procedure for JCommonsenseQA below, showing how it differs from CommonsenseQA.\\n\\n1. We collect Japanese QSs from ConceptNet. CommonsenseQA uses only forward relations (source concept, relation, target concept) excluding general ones such as \\\"RelatedTo\\\" and \\\"IsA\\\". JCommonsenseQA similarly uses a set of 22 relations, excluding general ones, but the direction of the relations is bidirectional to make the questions more diverse. In other words, we also use relations in the opposite direction (source concept, relation\u22121, target concept).\\n\\nWith this setup, we extracted 43,566 QSs with Japanese source/target concepts and randomly selected 7,500 from them.\\n\\n2. Some low-quality questions in CommonsenseQA contain distractors that can be considered to be an answer. To improve the quality of distractors, we add the following two processes that are not adopted in CommonsenseQA. First, if three target concepts of a QS include a spelling variation or a synonym of one another, this QS is removed. To identify spelling variations, we use the word ID of the morphological dictionary JumanDic. Second, we crowdsource a task of judging whether target concepts contain a synonym. As a result, we adopted 5,920 QSs from 7,500.\\n\\n3. For each QS, we crowdsource a task of writing a question sentence in which only one from the three target concepts is an answer. In the example shown in Figure 4, \\\"\u99c5 (station)\\\" is an answer, and the others are distractors. To remove low-quality question sentences, we remove the following question sentences.\\n   \u2022 Question sentences that contain a choice word (this is because such a question is easily solved).\\n   \u2022 Question sentences that contain the expression \\\"XX characters\\\". (XX is a number)\\n   \u2022 Improperly formatted question sentences that do not end with \\\"?\\\".\\n\\n5 The relations are Antonym, AtLocation, CapableOf, Causes, CausesDesire, DefinedAs, DerivedFrom, Desires, DistinctFrom, EtymologicallyDerivedFrom, HasA, HasFirstSubevent, HasLastSubevent, HasPrerequisite, HasProperty,InstanceOf, MadeOf, MotivatedByGoal, NotDesires, PartOf, SymbolOf, and UsedFor.\\n\\n5 For example, from triplets such as (station, AtLocation\u22121, bullet train), we obtain the target concepts \\\"bullet train\\\", \\\"timetable\\\", and \\\"ticket gate\\\" for the source concept \\\"station\\\".\\n\\n7 https://github.com/ku-nlp/JumanDIC\\n\\n8 This is set up to exclude questions like \\\"What is a word that means overpriced in two Chinese characters?\\\".\\n\\nAs a result, 5,920 \u00d7 3 = 17,760 question sentences were created, from which we adopted 15,310 by removing inappropriate question sentences.\\n\\n4. In CommonsenseQA, when adding distractors, one is selected from ConceptNet, and the other is created by crowdsourcing. In JCommonsenseQA, to have a wider variety of distractors, two distractors are created by crowdsourcing instead of selecting from ConceptNet.\\n\\nTo improve the quality of the questions, we remove questions whose added distractors fall into one of the following categories:\\n   (a) Distractors are included in a question sentence.\\n   (b) Distractors overlap with one of existing choices.\\n\\nAs a result, distractors were added to the 15,310 questions, of which we adopted 13,906.\\n\\n5. We asked three crowdworkers to answer each question and adopt only those answered correctly by at least two workers. As a result, we adopted 11,263 out of the 13,906 questions.\\n\\nFinally, we filtered out 14 questions based on automatic pattern matching and manual checking.\\n\\n3.4.3. Evaluation Metric\\n\\nThe evaluation metric for JCommonsenseQA is accuracy following CommonsenseQA.\\n\\n4. Evaluation using JGLUE\\n\\nBy using the constructed benchmark, we evaluated several publicly available pretrained models.\\n\\n4.1. Experimental Settings\\n\\nThe pretrained models used in the experiments are shown in Table 7. These models were fine-tuned in accordance with each task/dataset as follows:\\n\u2022 Text classification and sentence pair classification tasks: classification/regression problems with vector representations of the [CLS] tokens.\\n\u2022 JSQuAD: the classification problem of whether each token in a paragraph is a start/end position of an answer span.\\n\\n9 A question here refers to a set of a question sentence and choices.\\n\\n10 Fine-tuning was performed using the transformers library provided by Hugging Face. https://github.com/huggingface/transformers\\n\\n11 XLM-RoBERTa BASE and XLM-RoBERTa LARGE use the unigram language model as a tokenizer and they are excluded from the targets because the token delimitation and the start/end of the answer span often do not match, resulting in poor performance.\"}"}
{"id": "lrec-2022-1-317", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Pretrained models used in our experiments. Names in the parentheses represent the model names in the Hugging Face Hub. Large-sized models are also used corresponding to Tohoku BERT BASE and XLM-RoBERTa BASE. MeCab (Kudo et al., 2004) and Juman++ (Morita et al., 2015) are Japanese word segmenters. \u201cCC\u201d in pretraining texts represents Common Crawl.\\n\\n| Model Name       | Unit | Pretraining Texts                        |\\n|------------------|------|------------------------------------------|\\n| Tohoku BERT BASE | subword | Japanese Wikipedia (cl-tohoku/bert-base-japanese-v2) (MeCab + BPE (Sennrich et al., 2016)) |\\n| Tohoku BERT BASE | (char) | Japanese Wikipedia (cl-tohoku/bert-base-japanese-char-v2) |\\n| NICT BERT BASE   | subword | Japanese Wikipedia (MeCab + BPE) |\\n| Waseda RoBERTa BASE | subword | Japanese Wikipedia + CC (nlp-waseda/roberta-base-japanese) (Juman++ + Unigram LM) |\\n| XLM-RoBERTa BASE | subword | multilingual CC (xlm-roberta-base) (Unigram LM) |\\n\\nTable 8: Hyperparameters used in our experiments. (numbers in curly brackets represent the range of possible values).\\n\\n| Hyperparameter | Value |\\n|----------------|-------|\\n| learning rate  | {5e-5, 3e-5, 2e-5} |\\n| epoch          | {3, 4} |\\n| warmup ratio   | 0.1   |\\n| max seq length | 512 (MARC-ja), 128 (JSTS, JNLI), 384 (JSQuAD), 64 (JCommonsenseQA) |\\n\\nTable 9 shows the performance of each model along with human scores. The human scores were obtained using crowdsourcing in the same way as the dataset construction. The comparison of the models is summarized as follows:\\n\\n- Overall, XLM-RoBERTa LARGE performed the best. This may be due to the LARGE model size and the use of Common Crawl as pretraining texts, which is larger than Wikipedia.\\n- As for the basic unit, the subword-based model (Tohoku BERT BASE) performed consistently better than the character-based model (Tohoku BERT BASE (char)).\\n- Since JCommonsenseQA requires commonsense knowledge that is hard to be described in Wikipedia, the models pretrained on Common Crawl performed better. Figure 5 shows an example where the output of XLM-RoBERTa LARGE (which uses Common Crawl as pretraining texts) was correct while the output of Tohoku BERT BASE (which does not use Common Crawl) was incorrect.\\n- In all the datasets other than JCommonsenseQA, the performance of the best model equaled or exceeded the human score.\\n\\n4.3. Discussion\\n\\nIs the amount of training data enough? The amount of training data was changed by a factor of 0.75 and 0.5 to see how the performance changed. The model with the best performance for each dataset was used. The learning curve is shown in Figure 6. The performance is almost saturated for all the datasets, indicating that the amount of the constructed data is sufficient.\\n\\nAnnotation artifacts in JNLI\\n\\nIn datasets constructed by asking crowdworkers to write sentences, a problem called annotation artifacts arises, especially in NLI (Poliak et al., 2018; Tsuchiya, 2018). If hypothesis sentences are written by workers and include annotation artifacts, a system looking at only hypotheses could achieve moderate performance. We tested this hypothesis-only baseline on JNLI.\\n\\nFirst, we extracted a subset of JNLI for this experiment. Specifically, from the sentence pairs whose relation is contradiction, we extracted the sentence pairs in which\"}"}
{"id": "lrec-2022-1-317", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Performance on JGLUE dev/test sets.\\n\\n| Model Hypothesis | Tohoku BERT BASE | Tohoku BERT (char) | Tohoku BERT LARGE | NICT BERT BASE | Waseda RoBERTa BASE | XLM-RoBERTa BASE | XLM-RoBERTa LARGE |\\n|------------------|------------------|-------------------|-------------------|----------------|---------------------|----------------|-----------------|\\n| Human            | 0.989            | 0.958             | 0.955             | 0.958          | 0.962               | 0.961          | 0.964           |\\n| 0.8              | 0.990            | 0.957             | 0.961             | 0.960          | 0.962               | 0.962          | 0.965           |\\n| 0.84             | 0.899/0.861      | 0.899             | 0.908/0.870       | 0.903/0.867    | 0.901/0.865         | 0.901/0.857    | 0.915/0.882     |\\n| 0.88             | 0.909/0.872      | 0.892             | 0.907/0.863       | 0.909/0.865    | 0.901/0.857         | 0.901/0.857    | 0.916/0.880     |\\n| 0.92             | 0.925            | 0.899             | 0.900             | 0.902          | 0.895               | 0.895          | 0.919           |\\n| 0.96             | 0.917            | 0.876             | 0.878             | 0.881          | 0.876               | 0.876          | 0.902           |\\n| 1                | 0.871/0.944      | 0.871/0.941       | 0.880/0.946       | 0.897          | 0.880/0.947         | 0.880/0.947    | 0.840           |\\n| 0.8              | 0.873/0.946      | 0.864/0.937       | 0.864/0.937       | 0.904          | 0.897               | 0.897          | 0.842           |\\n| 0.986            | 0.917            | 0.879/0.946       | 0.864/0.937       | 0.904          | 0.897               | 0.897          | 0.842           |\\n| 0.988            | 0.871           | 0.808             | 0.822             | 0.823          | 0.807               | 0.807          | 0.842           |\\n\\nTable 10: Accuracy on the JNLI dev sets for the hypothesis-only experiment.\\n\\n| Model Hypothesis | XLM-RoBERTa LARGE |\\n|------------------|-------------------|\\n| Majority baseline | 0.553             |\\n| Tohoku BERT BASE | 0.658             |\\n\\nSection 5: Conclusion and Future Work\\n\\nThis paper described the construction procedure of JGLUE, a general language understanding benchmark for Japanese. We hope that JGLUE will be used to comprehensively evaluate pretrained models and construct more difficult NLU datasets, such as HotpotQA (Yang et al., 2018), a multi-hop QA dataset, and Adversarial GLUE (Wang et al., 2021).\\n\\nIn the future, we plan to build Japanese datasets for generation tasks such as GLGE (Liu et al., 2021) and for few-shot tasks such as FLEX (Bragg et al., 2021).\\n\\nSection 6: Acknowledgements\\n\\nThis work was carried out in a joint research project between Yahoo Japan Corporation and Waseda University.\\n\\nSection 7: Bibliographical References\\n\\nAbdul-Mageed, M., Elmadany, A. A., and Nagoudi, E. M. B. (2021). ARBERT & MARBERT: deep bidirectional transformers for arabic. CoRR, abs/2101.01785.\\n\\nBragg, J., Cohan, A., Lo, K., and Beltagy, I. (2021). FLEX: Unifying evaluation for few-shot NLP. In A. Beygelzimer, et al., editors, Advances in Neural Information Processing Systems.\\n\\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., and Specia, L. (2017). Semeval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017).\\n\\nChen, X., Fang, H., Lin, T.-Y., Vedantam, R., Gupta, S., Dollar, P., and Zitnick, C. L. (2015). Microsoft coco captions: Data collection and evaluation server.\\n\\nWe used MeCab + IPAdic (https://taku910.github.io/mecab/) for word segmentation.\"}"}
{"id": "lrec-2022-1-317", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Clark, J. H., Choi, E., Collins, M., Garrette, D., Kwiatkowski, T., Nikolaev, V., and Palomaki, J. (2020). TyDi QA: A benchmark for information-seeking question answering in typologically diverse languages. Transactions of the Association for Computational Linguistics, 8:454\u2013470.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June.\\n\\nHayashibe, Y. (2020). Japanese realistic textual entailment corpus. In LREC2020, pages 6827\u20136834, Marseille, France, May. European Language Resources Association.\\n\\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. (2020). XTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalization.\\n\\nKakwani, D., Kunchukuttan, A., Golla, S., N.C., G., Bhattacharyya, A., Khapra, M. M., and Kumar, P. (2020). IndicNLPSuite: Monolingual corpora, evaluation benchmarks and pre-trained multilingual language models for Indian languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4948\u20134961, Online, November.\\n\\nKeung, P., Lu, Y., Szarvas, G., and Smith, N. A. (2020). The multilingual Amazon reviews corpus. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4563\u20134568, Online, November.\\n\\nKudo, T., Yamamoto, K., and Matsumoto, Y. (2004). Applying conditional random fields to Japanese morphological analysis. In Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pages 230\u2013237, Barcelona, Spain, July.\\n\\nLe, H., Vial, L., Frej, J., Segonne, V., Coavoux, M., Lecouteux, B., Allauzen, A., Crabbe, B., Bessacier, L., and Schwab, D. (2020). FlauBERT: Unsupervised language model pre-training for French. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2479\u20132490, Marseille, France, May.\\n\\nLiang, Y., Duan, N., Gong, Y., Wu, N., Guo, F., Qi, W., Gong, M., Shou, L., Jiang, D., Cao, G., Fan, X., Zhang, R., Agrawal, R., Cui, E., Wei, S., Bharti, T., Qiao, Y., Chen, J.-H., Wu, W., Liu, S., Yang, F., Campos, D., Majumder, R., and Zhou, M. (2020). XGLUE: A new benchmark dataset for cross-lingual pre-training, understanding and generation.\\n\\nLin, T.-Y., Maire, M., Belongie, S., Bourdev, L., Girshick, R., Hays, J., Perona, P., Ramanan, D., Zitnick, C. L., and Doll\u00e1r, P. (2015). Microsoft coco: Common objects in context.\"}"}
{"id": "lrec-2022-1-317", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Seelawi, H., Tuffaha, I., Gzawi, M., Farhan, W., Talafha, B., Badawi, R., Sober, Z., Al-Dweik, O., Freihat, A. A., and Al-Natsheh, H. (2021). ALUE: Arabic language understanding evaluation. In Proceedings of the Sixth Arabic Natural Language Processing Workshop, pages 173\u2013184, Kyiv, Ukraine (Virtual), April.\\n\\nSennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany, August.\\n\\nSpeer, R., Chin, J., and Havasi, C. (2017). Conceptnet 5.5: An open multilingual graph of general knowledge. Proceedings of the AAAI Conference on Artificial Intelligence, 31(1), Feb.\\n\\nSuzuki, M., Matsuda, K., Okazaki, N., and Inui, K. (2018). Construction of a question answering dataset with answerability by reading. In NLP2018.\\n\\nTakahashi, N., Shibata, T., Kawahara, D., and Kurohashi, S. (2019). Machine comprehension improves domain-specific Japanese predicate-argument structure analysis. In Proceedings of the 2nd Workshop on Machine Reading for Question Answering, pages 98\u2013104, Hong Kong, China, November.\\n\\nTalmor, A., Herzig, J., Lourie, N., and Berant, J. (2019). CommonsenseQA: A question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4149\u20134158, Minneapolis, Minnesota, June.\\n\\nTsuchiya, M. (2018). Performance impact caused by hidden bias of training data for recognizing textual entailment. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan, May. European Language Resources Association (ELRA).\\n\\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2018). GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium, November.\\n\\nWang, A., Pruksachatkun, Y., Nangia, N., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman, S. (2019). SuperGLUE: A stickier benchmark for general-purpose language understanding systems. In H. Wallach, et al., editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc.\\n\\nWang, B., Xu, C., Wang, S., Gan, Z., Cheng, Y., Gao, J., Awadallah, A. H., and Li, B. (2021). Adversarial glue: A multi-task benchmark for robustness evaluation of language models.\\n\\nWarstadt, A., Singh, A., and Bowman, S. R. (2019). Neural network acceptability judgments. Transactions of the Association for Computational Linguistics, 7:625\u2013641.\\n\\nWilliams, A., Nangia, N., and Bowman, S. (2018). A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122, New Orleans, Louisiana, June.\\n\\nXu, L., Hu, H., Zhang, X., Li, L., Cao, C., Li, Y., Xu, Y., Sun, K., Yu, D., Yu, C., Tian, Y., Dong, Q., Liu, W., Shi, B., Cui, Y., Li, J., Zeng, J., Wang, R., Xie, W., Li, Y., Patterson, Y., Tian, Z., Zhang, Y., Zhou, H., Liu, S., Zhao, Z., Zhao, Q., Yue, C., Zhang, X., Yang, Z., Richardson, K., and Lan, Z. (2020). CLUE: A Chinese language understanding evaluation benchmark. In Proceedings of the 28th International Conference on Computational Linguistics, pages 4762\u20134772, Barcelona, Spain (Online), December. International Committee on Computational Linguistics.\\n\\nYanaka, H. and Mineshima, K. (2021). JSICK: Japanese sentences involving compositional knowledge dataset. In JSAI2021. in Japanese.\\n\\nYang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. (2018). HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium, October-November. Association for Computational Linguistics.\\n\\nYoshikoshi, T., Kawahara, D., and Kurohashi, S. (2020). Multilingualization of a natural language inference dataset using machine translation. In The Special Interest Group Technical Reports of IPSJ. in Japanese.\"}"}
