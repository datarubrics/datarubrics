{"id": "emnlp-2023-main-899", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Illustration of out-of-distribution attributes.\\n\\n| Slot   | Attribute            |\\n|--------|----------------------|\\n| Expression | disgust, surprise, fear |\\n| Hair color         | pink hair, purple hair, red hair |\\n| Makeup             | big eyes |\\n\\nTable 10: Image editing performance on in-domain and out-of-domain test sets.\\n\\n| Test set     | FID   | LPIPS | MinRel | AvgRel |\\n|--------------|-------|-------|--------|--------|\\n| In-domain    | 40.115| 0.449 | 0.754  | 0.773  |\\n| Out-of-domain| 40.264| 0.458 | 0.739  | 0.759  |\\n\\nE More Quantitative Results\\n\\nE.1 Out-of-distribution Generalization\\n\\nA distinct advantage of our proposed framework is its adaptability in utilizing various training strategies for the T5-based dialogue module. Drawing inspiration from (Lin et al., 2021), we cast the slot value tracking as a question-answering task, which promotes enhanced generalization on unseen data. For example, the model is prompted with queries like \u201cwhat is the hair color?\u201d\\n\\nFor evaluation, we use an out-of-distribution (OOD) test set comprising 1,000 samples based on previously unseen values, as detailed in Table 9. Experimental results suggest our model\u2019s robustness against OOD scenarios during testing. The dialogue module achieves an accuracy of 58.90% on the OOD test set. As for image editing, the results on the OOD test set align closely with those of the in-domain test set.\\n\\nE.2 Ablation Study of Image Editing Module.\\n\\nWe performed ablation studies for both identity loss ($L_{id}$) and $L_{2}$ loss within the image editing module. The results are presented in Table 11. It is observed that both loss types influence the image-editing performance, with $L_{2}$ loss having a more significant impact. This is likely because $L_{2}$ loss constrains the degree of feature vector transformations in hidden space, and significant changes in StyleGAN\u2019s hidden space can introduce severe artifacts.\\n\\nTable 11: Ablation study of image editing module.\\n\\n| Method       | Input Image Editing | FID   | LPIPS | MinRel | AvgRel |\\n|--------------|---------------------|-------|-------|--------|--------|\\n| Multi-turn USR |                     | 40.115| 0.449 | 0.754  | 0.773  |\\n| w/o $L_{id}$   |                     | 40.475| 0.452 | 0.742  | 0.763  |\\n| w/o $L_{2}$  |                     | 74.639| 0.794 | 0.715  | 0.752  |\"}"}
{"id": "emnlp-2023-main-899", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CHATEDIT: Towards Multi-turn Interactive Facial Image Editing via Dialogue\\n\\nXing Cui1\u2217 Zekun Li2\u2217 Peipei Li1\u2020 Yibo Hu3 Hailin Shi3 Chunshui Cao4 Zhaofeng He1\\n\\n1 Beijing University of Posts and Telecommunications\\n2 University of California, Santa Barbara\\n3 NIO\\n4 W ATRIX.AI\\n\\n{cuixing, lipeipei, zhaofenghe}@bupt.edu.cn zekunli@cs.ucsb.edu\\n{huyibo871079699, hailinshi.work}@gmail.com chunshui.cao@watrix.ai\\n\\nAbstract\\nThis paper explores interactive facial image editing via dialogue and introduces the CHATEDIT benchmark dataset for evaluating image editing and conversation abilities in this context. CHATEDIT is constructed from the CelebA-HQ dataset, incorporating annotated multi-turn dialogues corresponding to user edit requests on the images. The dataset is challenging, as it requires the system to dynamically track user requests, edit images, and generate appropriate responses. Accordingly, we propose three benchmark tasks: (i) user edit request tracking, (ii) image editing, and (iii) response generation.\\n\\nWe present a novel baseline framework that integrates a dialogue module for both tracking user requests and generating responses and an image editing module for image editing. Unlike previous approaches, our framework directly tracks user edit requests from the entire dialogue history up to the current turn and modifies the original image rather than adjusting the previous turn's output, thereby reducing error accumulation and preventing attribute forgetfulness. Extensive experiments on the CHATEDIT dataset underline our framework's superior performance against prior models, while also highlighting potential room for further research.\\n\\nWe will release the code and data publicly to facilitate advancements in complex interactive facial image editing.\\n\\n* Equal contribution.\\n\u2020 Corresponding Author.\\n\u2021 Our data and codes are available at https://github.com/cuixing100876/ChatEdit\\n\\n1 Introduction\\nWith the rise of deep generative models such as GANs (Goodfellow et al., 2020; Karras et al., 2019; Li et al., 2023a) and DDPMs (Ho et al., 2020; Sohl-Dickstein et al., 2015; Zhang et al., 2023), significant progress has been achieved in instruction-based facial image editing (Xu et al., 2022; Patashnik et al., 2021; Li et al., 2023b). However, an emerging scenario is multi-turn interactive editing, allowing users to iteratively refine their editing instructions through system interaction (Sharma et al., 2018; Cheng et al., 2020; Kim et al., 2019; Jiang et al., 2021). Existing approaches (Zhou et al., 2022; El-Nouby et al., 2019; Kim et al., 2019) typically treat multi-turn editing as a sequence of successive single-turn edits, leading to issues such as attribute forgetting and error accumulation, as depicted in Fig. 2 (first line). Moreover, these techniques...\"}"}
{"id": "emnlp-2023-main-899", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of our CHATEDIT with existing works on image editing.\\n\\n| Method          | Scene       | Multi-turn Interaction | System Feedback | Dataset | Text Data Attributes |\\n|-----------------|-------------|------------------------|-----------------|---------|---------------------|\\n| TransEditor (Xu et al., 2022) | Facial image | \u2717 | \u2717 | CelebA-HQ | \u2717 |\\n| HairCLIP (Wei et al., 2022) | Facial image | \u2717 | \u2717 | CelebA-HQ | \u2717 |\\n| StyleCLIP (Patashnik et al., 2021) | Facial image | \u2717 | \u2717 | CelebA-HQ | \u2717 |\\n| ChatPainter (Sharma et al., 2018) | Realistic image | \u2717 | \u2717 | MS COCO dialogue | |\\n| TiGAN (Zhou et al., 2022) | Facial image | \u2713 | \u2717 | CelebA-HQ | \u2717 |\\n| Talk-to-Edit (Jiang et al., 2021) | Facial image | \u2713 | \u2713 | CelebA-dialog user utterance | 5 |\\n| CHATEDIT (Ours) | Facial image | \u2713 | \u2713 | C HATEDIT dialogue | 21 |\\n\\n\u2020: HairCLIP only considers hairstyle and hair color. 44 text descriptions are collected for hairstyle and 12 text descriptions for hair color.\\n\\nTo facilitate the research on multi-turn interactive facial image editing, we introduce a novel benchmark dataset named CHATEDIT. Sourced from the CelebA-HQ dataset (Karras et al., 2018), CHATEDIT enhances a selected set of 12k images with annotated multi-turn dialogues that align with user edit requests for facial images. The annotations include user utterances, system responses, and the user's \u201cbelief state\u201d, which represents the user edit requests from the beginning of the dialogue to the current turn. Fig. 1 illustrates that success on CHATEDIT necessitates the system to track the user edit requests, edit images based on tracked requests, and provide natural language responses to engage with users.\\n\\nTo evaluate the performance of multi-turn interactive editing, we define three tasks: (i) user edit request tracking, (ii) image editing, and (iii) response generation. Correspondingly, we introduce a comprehensive set of metrics that evaluate both response and editing quality.\\n\\nBased on our benchmark dataset, we propose a baseline framework for multi-turn interactive image editing. Our framework seamlessly integrates a language dialogue module and a visual image editing module. Specifically, we employ an end-to-end task-oriented dialogue (TOD) model to extract the user's image edit requests and generate appropriate responses based on the current conversation context. These tracked user requests are transformed into a text prompt to guide the text-based image editing model, StyleCLIP (Patashnik et al., 2021), in performing the desired manipulations on the input raw image. As illustrated in Fig. 2's second line, our benchmark dataset and the proposed framework enable direct editing on the input raw image instead of cascaded modifications to images from previous turns, thereby reducing error accumulation and attribute forgetting issues.\\n\\nWe perform extensive experiments with diverse settings to investigate the effectiveness of our proposed framework on the CHATEDIT dataset. The results suggest that our proposed framework is superior to the previously prevalent cascaded single-turn editing methods regarding both image editing quality and response diversity.\\n\\nTo sum up, our contributions are three-fold: (1) We introduce the CHATEDIT benchmark dataset, which could serve as a valuable resource for advancing research in multi-turn interactive facial image editing. (2) We propose a novel framework that seamlessly combines a task-oriented dialogue module and an image editing module. This framework effectively tracks user requests, performs image editing, and generates system responses. Importantly, it addresses the issues of attribute forgetting and error accumulation prevalent in previous methods. (3) Through qualitative and quantitative evaluations on the CHATEDIT dataset, we demonstrate the superiority of our proposed multi-turn editing framework over previous cascaded single-turn editing methods. We believe these results not only highlight the strengths of our proposed approach but also spur further exploration in this field.\\n\\n2 Related Work\\nFacial Image Editing\\nTraditional image editing techniques have focused on modifying specific given attributes such as age (Li et al., 2020a, 2019), hairstyle (Wei et al., 2022), or other predefined attributes (Zhou et al., 2022; Li et al., 2020b; Zhang et al., 2022). In recent years, with the development of pre-trained vision-language models (Radford et al., 2021; Li et al., 2022a; Wang et al., 2023), there has been a growing interest in human-computer interaction scenarios (Kottur et al., 2022; Jiang et al., 2021). This leads researchers to explore interactive image editing, where users can dynamically adjust their editing requests through interaction with the system (Kim et al., 2019; Lachmy...\"}"}
{"id": "emnlp-2023-main-899", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Illustration of editable attributes in CHATE-DIT, which are categorized into four groups (slots).\\n\\n| Slot | Expression |\\n|------|------------|\\n| smiling, no smiling, angry, sad |\\n| Hair color | brown hair, blond hair, black hair, gray hair |\\n| Hair | receding hairline, sideburns, bangs, no bangs, mustache, goatee, no beard |\\n| Makeup | no makeup, heavy makeup, lipstick, bushy eyebrows, rosy cheeks, pale skin |\\n\\net al., 2022; Zhou et al., 2022; Jiang et al., 2021). For instance, TiGAN (Zhou et al., 2022) generates images iteratively based on successive editing steps during a conversation. However, these methods merely \u201clisten\u201d to user requests without generating system responses, constraining their interaction capability. A recent work called Talk-to-Edit (Jiang et al., 2021) attempts to address this by introducing a rule-based method to generate system response. However, this approach lacks flexibility and struggles with unforeseen scenarios that are not predefined. Furthermore, these methods treat multi-turn editing as a sequence of individual single-turn edits, resulting in issues such as error accumulation and attribute forgetting as the number of interactions increases.\\n\\nTask-oriented Dialogue\\nThere are two primary approaches to TOD. Traditional systems adopt a pipelined approach comprising four modules. Firstly, the natural language understanding (NLU) module (Abro et al., 2022) converts user requests into semantic slots, domain information, and user intention. Secondly, the dialogue state tracking (DST) module (Wu et al., 2019; Le et al., 2019; Lin et al., 2021; Heck et al., 2023) extracts the dialogue state, which records user requests in the form of slot-value pairs. The dialogue policy learning (POL) module (Chen et al., 2017; Geishauser et al., 2022) determines the next action of the dialogue agent based on the dialogue state. Finally, the natural language generation (NLG) module (Elder et al., 2020) generates the system response. In more recent times, there has been a shift towards end-to-end task-oriented dialogue systems (Hosseini-Asl et al., 2020; Wang et al., 2022; Le et al., 2020; Zeng et al., 2023). For example, PPTOD (Su et al., 2022) employs multi-task training that simultaneously processes all sub-tasks. Currently, there have been approaches that utilize Large Language Models (LLMs) like ChatGPT for task-oriented dialogues (Li et al., 2023c). However, existing TOD systems primarily focus on scenarios such as booking and consulting, neglecting the interactive image editing scenario.\\n\\nTable 3: Statistics of dialogues in CHATE-DIT dataset.\\n\\n| Total # dialogues | 12,000 |\\n|-------------------|-------|\\n| Total # utterances | 96,174 |\\n| Avg # turns per dialogue | 4.0 |\\n| Avg # utterances per dialogue | 8.0 |\\n| Avg # words per user turns | 11.6 |\\n| Avg # words per system turns | 11.3 |\\n| Avg # attributes mentioned per dialogue | 6.3 |\\n\\n3.1 CHATE-DIT Dataset Construction\\nFacial Image Data\\nWe construct the CHATE-DIT dataset on top of the CelebA-HQ (Karras et al., 2018), which is a high-resolution facial image dataset with 30k images. It provides binary annotations of 40 facial attributes. We select 17 attributes from CelebA-HQ and add another four frequently-used attributes \u201csad\u201d, \u201cangry\u201d, \u201cno smiling\u201d, and \u201cno bangs\u201d, resulting in a total of 21 editable attributes in the CHATE-DIT dataset. As shown in Table 2, these editable attributes are categorized into four groups. We select 12k images from the CelebA-HQ and utilize the annotation in CelebA-HQ to build a caption for the image. An example is presented in Fig. 8 in Appendix.\\n\\nDialogue Annotation\\nDuring each turn of the dialogue, the user expresses their editing requests using natural language. It is essential for the system to detect these requests as belief states and map them to appropriate responses. Consequently, we annotate three types of data for each turn in the dialogue: (1) the user utterance, (2) the user belief state (user requests), and (3) the system response. We here adopt terminology commonly used in task-oriented dialogue research to introduce the annotations. Each group in Table 2 is considered a...\"}"}
{"id": "emnlp-2023-main-899", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Analysis of CHATEDIT dataset. Left: Distribution of different attributes. Right: Distribution of occurrence frequency of various attribute combinations.\\n\\nFigure 4: Illustration of the dialogue flow for the first 3 turns in the CHATEDIT dataset. User-n and System-n represent the n-th user and system turn, respectively.\\n\\nslot, and each editable attribute represents a possible slot value for its corresponding slot. As a result, the user request in each turn of the dialogue is represented as slot-value pairs. For example, a user request could be \\\"expression: smiling, hairstyle: bangs, hair color: black hair.\\\"\\n\\nTo reduce human efforts in annotating the dialogues, we leverage a two-phase pipeline (Karras et al., 2018) consisting of a simulation phase (Li et al., 2022b) and a paraphrase phase. During the simulation phase, we begin by preparing a collection of varied human-written utterances that describe user requests for each editable attribute and system responses for each system action. The supported system actions include Next: general queries on what to edit in the next turn, Request: request whether to edit an attribute, Suggest: suggestions to edit towards a specific attribute value. We then utilize ChatGPT (Ouyang et al., 2022) to generate more diverse utterances by combining task instruction and a few human-written example utterances as the prompt as shown below (Li et al., 2021).\\n\\nWrite diverse sentences to express the given image editing requirements.\\nRequirements: smiling\\nSentence 1: Can you edit the image to show him with a smile?\\nSentence 2: It would be great if you could add a smile to his face.\\n... Sentence N: I'd love to see a version of the image where he has a big smile on his face.\\nSentence N+1: \\n\\nNext, we construct the dialogue by following a series of steps. Firstly, we determine the user request of the current turn by randomly selecting editable attributes (excluding the original attributes of the raw image) and expressing them as slot-value pairs. Secondly, we select an utterance that effectively expresses this particular request. Then, based on a predefined policy, we determine the appropriate system action and choose a candidate system response. Specifically, the predefined policy is a set of rules that govern the dialogue flow to ensure it aligns logically with the user's instructions and system functions. For instance, if a user requests a change in hair color to blond, the system's predefined policy would prevent it from generating a redundant or illogical suggestion like \\\"Do you want to dye your hair blond?\\\". Finally, the multi-turn dialogue can be constructed by repeating the above steps.\\n\\nFollowing the simulation of multi-turn dialogues, a manual review is conducted to ensure their quality. Human annotators are tasked with reviewing the dialogues to ensure they are following the predefined dialogue flow logic. Additionally, they are asked to refine the expressions to enhance diversity and naturalness if necessary. Fortunately, this process of checking and revising the dialogues is less labor-intensive compared to annotating the dialogues from scratch, resulting in significantly reduced annotation efforts.\\n\\n3.2 CHATEDIT Dataset Statistics\\nThe constructed CHATEDIT dataset comprises 12k examples, with each example consisting of a facial image equipped with a corresponding caption and an annotated multi-turn dialogue. We divide the CHATEDIT dataset into training, validation, and testing sets with 10k, 1k, and 1k examples, respectively. To offer deeper insights into the dataset, we present comprehensive statistics in this section.\\n\\nAnalyzing Dialogues\\nOur CHATEDIT dataset comprises a total of 12k dialogues, consisting of approximately 96k utterances. The statistical details of the dataset are presented in Table 3. More...\"}"}
{"id": "emnlp-2023-main-899", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"over, we provide a visual representation of the dialogue flow in Fig. 4. Each block in the figure corresponds to a specific turn labeled as Start, User-\\\\(n\\\\), or System-\\\\(n\\\\), where \\\\(n\\\\) represents the corresponding turn index. For user turns, there are four different kinds of blocks, each representing an attribute group slot. Additionally, there are three types of system turns, each representing a system action. The width of each block in the visualization indicates its occurrence frequency, while the connectivity between blocks signifies their co-occurrences in the dialog flow. Notably, the dataset exhibits a balanced distribution of different dialog flows, indicating a high level of diversity.\\n\\nAnalyzing Editing Attributes\\n\\nRegarding the annotated user requests, we provide insights into the frequency of each attribute (left) and the occurrence frequency of different attribute combinations (right) in Fig. 3. The figure illustrates that the majority of attributes are mentioned over 500 times, while more than 78% of the attribute combinations occur less than 5 times. These findings highlight the diversity present in our dataset.\\n\\n3.3 CHAT EDITION Evaluation\\n\\nTo evaluate the system's ability to detect user requests, generate the appropriate system response, and edit the image according to user requests, we propose three benchmark tasks.\\n\\nUser Request Tracking\\n\\nSimilar to the dialogue belief state tracking task in TOD, we introduce the user request tracking task to evaluate the system's ability in detecting the user requests. The user requests are represented as slot-value pairs, where each slot denotes a category (group) of attributes and the value indicates a specific attribute value. The input for this task is the dialogue history from the beginning to the current turn. The performance is evaluated with Joint Accuracy, which is the ratio of dialogue turns whose slots are predicted completely accurately, i.e., all the slot and slot values are predicted correctly.\\n\\nResponse Generation\\n\\nAnother objective of the dialogue module is to generate fluent, reasonable, and diverse responses to interact with the users. We use BLEU (Papineni et al., 2002) to evaluate the fluency of generated responses with respect to reference responses. To evaluate the diversity of generated responses, we employ Distinct-1, 2 (Li et al., 2015), where Distinct-\\\\(n\\\\) represents the ratio of distinct n-grams in responses.\\n\\nImage Editing\\n\\nEditing an image according to the user request is a primary objective of the CHAT EDITION system. Given user requests that consist of multiple attributes, a robust system should be capable of simultaneously manipulating these attributes while maintaining high quality. We evaluate the image editing performance from two perspectives: (1) relevance: whether the requested attributes are accurately edited; and (2) quality: whether the edited image is realistic and natural.\\n\\nWe measure the editing relevance of each attribute via the cosine similarity of the attribute and the edited image. To evaluate the relevance of multiple attributes, we report two metrics: Average Relevance (AvgRel) reflects the average editing relevance on all requested attributes, where the relevance is measured with the cosine similarity between the edited image and the requested attribute:\\n\\n\\\\[\\n\\\\text{AvgRel} = \\\\frac{1}{|T|} \\\\sum_{t \\\\in T} \\\\text{CosCLIP}(I, t)\\n\\\\]\\n\\n\\\\(\\\\text{CosCLIP}\\\\) represents the cosine similarity function. \\\\(T\\\\) represents the set of all editing attributes.\\n\\nWe also report Minimum Relevance (MinRel), which reflects the worst relevance among the requested attribute:\\n\\n\\\\[\\n\\\\text{MinRel} = \\\\min_{t \\\\in T} \\\\text{CosCLIP}(I, t)\\n\\\\]\\n\\nAs for the measure of image quality, we utilize FID (Heusel et al., 2017) and LPIPS (Zhang et al., 2018) metrics, which calculate the statistical similarity between the edited images and the originals.\\n\\n4 Method\\n\\nIn this section, we introduce our proposed framework for CHAT EDITION. As illustrated in Fig. 5, the system consists of a task-oriented dialogue module and an image editing module. We adopt a pre-trained language model-based TOD model as the unified dialogue module for both User Request Tracking and response generation. Specifically, it takes the dialogue context prepended with different prompts as input and outputs the tracked user requests and corresponding system response. As for the image editing module, a text-based image editing model StyleCLIP (Patashnik et al., 2021) is utilized, which receives the user requests tracked by the dialogue model as input and edits the input image accordingly.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"User Requests\\n\\nHairstyle: bangs\\nExpression: no smiling\\n\\nSystem Response\\n\\n<System> Do you want to manipulate the hair color?\\n\\nDialogue Module\\n\\nEncoder\\nDecoder\\n\\nTransformer Layer 1\\nTransformer Layer N\\n\\n[ sos ]\\n\\n[ eos ]\\n\\n[ sos ]\\n\\n[ eos ]\\n\\nTransformer Layer 1\\nTransformer Layer N\\n\\nEmbeding\\n\\nDialogue Module\\n\\nImage Editing Module\\n\\nUser requests\\n\\nCLIP\\n\\nLoss\\n\\nIdentity\\n\\nLoss\\n\\nStyleGAN\\n\\nHairstyle: bangs\\nExpression: no smiling\\n\\nDescriptive text\\n\\ntranslate dialogue to dialogue response.\\n\\nThis is a photo of a woman. She has blond hair. She doesn't have bangs. She has medium smiling. She is wearing lipstick...\\n\\n<User>\\n\\nI want to see how she will look without smiling.\\n\\n<System>\\n\\nOk, it's done. How about making the bangs longer?\\n\\n<User>\\n\\nPlease give it a try.\\n\\ntranslate dialogue to dialogue state.\\n\\n<User>\\n\\nI want to see how she will look without smiling.\\n\\n<System>\\n\\nOk, it's done. How about making the bangs longer?\\n\\n<User>\\n\\nPlease give it a try.\\n\\n4.1 Dialogue Module\\n\\nThe primary goal of the dialogue module is to track user requests and generate natural language feedback as responses. Following (Su et al., 2022), we formulate the two tasks as text generation and adopt a unified pre-trained language model T5 (Raffel et al., 2020) for both two tasks. Specifically, we prepend a task-specific prompt to the dialogue context to serve as the dialogue language model's input, and the model is trained to output the task-specific output. We use \\\"translate dialogue to dialogue state\\\" and \\\"translate dialogue to dialogue response\\\" as prompts for the User Request Tracking task and the response generation task, respectively. As the facial attribute values of the initial image can provide valuable context to enhance the generation of reasonable conversations, we incorporate the image caption of the initial raw image into the input for the response generation task. Finally, the dialogue model is trained to generate user requests and system responses in a multitask learning approach.\\n\\nWe use the T5 (Raffel et al., 2020) model and initialize it with the PPTOD (Su et al., 2022) checkpoint that has been pre-trained on a diverse set of dialogue corpus and thus equip with primary TOD task completion skills. Given task-specific prompt $z^t$, dialogue history $x$, and target output $y$. The dialogue model is trained with a maximum likelihood objective and the loss function is defined as:\\n\\n$$L_\\\\Theta = -\\\\frac{1}{|y|} \\\\sum_{i=1}^{|y|} \\\\log P_\\\\Theta(y_i | y_{<i}; z^t, x),$$\\n\\n(3)\\n\\nwhere $\\\\Theta$ is the model parameters.\\n\\n4.2 Image Editing Module\\n\\nIn this paper, we employ a text-driven image editing method inspired by StyleCLIP (Patashnik et al., 2021). This approach combines the generative capabilities of StyleGAN (Karras et al., 2019) with the joint vision-language representation learned by CLIP (Radford et al., 2021). To generate the manipulated image based on user requests, we utilize a CLIP-based loss, which optimizes the latent code of the input image to align with the directions inferred by the descriptive text of the user requests.\\n\\nAs the tracked user requests are stored in the slot-value pair format, we first construct them into the descriptive text prompt $t$ with templates. Then, we manipulate the image by optimizing the latent code. Specifically, the images are projected to the latent codes and manipulated in the $W^+$ space, which is extended from the $W$ space proposed in StyleGAN (Karras et al., 2019). For a StyleGAN with 18 layers, $W^+$ space is defined by the cascade of 18 different vectors $[w_1,...,w_{18}]$, $w_i \\\\in W$.\\n\\nAs shown in Fig. 5, the input image is first inverted by a fixed StyleGAN inversion model e4e (Tov et al., 2021) to obtain the latent code of the input image $w_s \\\\in W^+$. Then, the latent code $w \\\\in W^+$ is initialized as $w_s$. $w$ is learnable and will be optimized towards the latent code of the edited image. Denote the optimization result of $w$ as $w^*$, $w^*$ can be viewed as the approximation of the latent code of the edited image. Then, taking $w^*$ as input, we can generate the desired edited image via pretrained StyleGAN. In order to optimize $w$, $w$ is first fed to the pretrained StyleGAN to obtain its corresponding image. Thereby, the supervisory information will be delivered via the image. Specifically, to force $w$ to be consistent with\"}"}
{"id": "emnlp-2023-main-899", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Quantitative results of the dialogue module on user request tracking and response generation.\\n\\n| Model    | User Request Tracking | Response Generation |\\n|----------|-----------------------|---------------------|\\n|          | Joint Acc \u2191           | BLEU \u2191              |\\n|          | Distinct-1 \u2191          | Distinct-2 \u2191        |\\n| T5 small | 88.502 \u00b1 0.212        | 14.806 \u00b1 0.203      |\\n|          | 0.393 \u00b1 0.025         | 0.867 \u00b1 0.008       |\\n| T5 base  | 89.065 \u00b1 0.194        | 14.917 \u00b1 0.499      |\\n|          | 0.408 \u00b1 0.005         | 0.872 \u00b1 0.002       |\\n| T5 large | 88.953 \u00b1 0.159        | 16.064 \u00b1 0.594      |\\n|          | 0.413 \u00b1 0.010         | 0.886 \u00b1 0.004       |\\n\\nThe user's descriptive text, the CLIP loss (Radford et al., 2021) is utilized:\\n\\n\\\\[ L_{CLIP} = D_{CLIP}(G(w), t) \\\\]  \\n\\nwhere \\\\( G \\\\) is a pretrained StyleGAN generator that maps latent code into an image. \\\\( D_{CLIP} \\\\) measures the cosine distance between the CLIP embedding of the image and the text. Then, \\\\( L_2 \\\\) loss is utilized for preserving the similarity between the input image and the edited image in the \\\\( W + \\\\) space:\\n\\n\\\\[ L_2 = \\\\| w - w_s \\\\|_2 \\\\]  \\n\\nMoreover, \\\\( L_{ID} \\\\) is used to preserve the identity:\\n\\n\\\\[ L_{ID} = 1 - \\\\langle R(G(w_s)), R(G(w)) \\\\rangle \\\\]  \\n\\nwhere \\\\( R \\\\) is a pretrained face recognition network (Deng et al., 2019), \\\\( \\\\langle \\\\cdot, \\\\cdot \\\\rangle \\\\) calculates the cosine similarity. The overall optimization objective is:\\n\\n\\\\[ w^* = \\\\text{arg min}_w \\\\in W \\\\left( L_{CLIP} + \\\\lambda L_2 + \\\\lambda L_{ID} \\\\right) \\\\]  \\n\\n### 5 Experiment\\n\\n#### 5.1 Quantitative Evaluation\\n\\nDialogue Module\\n\\nWe initialize our dialogue model with different sizes of pretrained PPTOD checkpoints PPTOD small, PPTOD base and PPTOD large, respectively. We further fine-tune the model leveraging the Adam optimizer with a learning rate of 5e-5 and a batch size of 64. We utilize a multi-task training strategy in which the User Request Tracking task and the Response Generation task are trained simultaneously.\\n\\nAs shown in Table 4, we evaluate the dialogue module on the user request tracking and response generation task. All three models achieve a joint accuracy of over 88%, indicating their effectiveness.\\n\\nWe also compare our model with ChatGPT. Specifically, we used the prompt proposed by (Heck et al., 2023) to leverage ChatGPT for User Request Tracking, which is up-to-date and has achieved impressive results on the MultiWOZ dataset. Our method achieves a Joint Accuracy of 88.86%, outperforming ChatGPT's 76.26%.\\n\\n### Table 5: Quantitative results of the image editing performance.\\n\\n| Editing Mode | Input Image | Editing | FID \u2193 | LPIPS \u2193 | MinRel \u2191 | AvgRel \u2191 |\\n|--------------|-------------|---------|-------|---------|----------|----------|\\n| Single-turn USR | 41.451 \u00b1 0.176 | 0.482 \u00b1 0.002 | 0.730 \u00b1 0.010 | 0.752 \u00b1 0.010 |\\n| Multi-turn USR | 40.115 \u00b1 0.165 | 0.449 \u00b1 0.001 | 0.754 \u00b1 0.009 | 0.773 \u00b1 0.009 |\\n| Multi-turn Dial | 42.813 \u00b1 0.161 | 0.477 \u00b1 0.001 | 0.741 \u00b1 0.007 | 0.761 \u00b1 0.006 |\\n| Multi-turn (Ours) USR-T | 40.536 \u00b1 0.105 | 0.449 \u00b1 0.001 | 0.753 \u00b1 0.010 | 0.773 \u00b1 0.010 |\\n\\nAs for the performance regarding the response generation, an improvement can be seen in both the BLEU score and the Distinct-1,2 score as the model size increases, demonstrating that the larger model can generate responses with better quality and diversity.\\n\\nImage Editing Module\\n\\nThe weight of \\\\( \\\\lambda L_2 \\\\) and \\\\( \\\\lambda L_{ID} \\\\) are 0.008 and 0.005, respectively. The editing step is 300, and the Adam optimizer is used with a learning rate of 0.1. For fairness, all experiments utilize StyleCLIP to manipulate images.\\n\\nTo evaluate the effectiveness of our proposed multiple-turn editing approach, we first compare it with single-turn image editing. To eliminate the impact of misidentification of user edit requirements, the oracle user requests are utilized as the input of the image editing module. As shown in the first two lines in Table 5, significant improvements can be observed in all four metrics, indicating that our method successfully mitigates the error accumulation problem and achieves higher image quality.\\n\\nMoreover, we find that the MinRel score of the single-turn method is considerably lower than that of the multi-turn method. This is expected since the single-turn editing approach tends to forget some edited attributes from earlier turns, resulting in suboptimal performance in the poorest attribute.\\n\\nThe last line in Table 5 presents the results of image editing using our proposed pipeline, where the image editing module edits the image based on the user requests tracked by our dialogue module. Notably, the performance achieved in this setting is comparable to the results when oracle user requests are taken as input. This finding highlights the effectiveness of our pipeline, particularly in real-world applications where obtaining perfect oracle user requests may not be feasible.\\n\\nComparison with Talk-to-Edit\\n\\nWe conduct a quantitative comparison between our method and...\"}"}
{"id": "emnlp-2023-main-899", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"User: I would like to see how he would be if he had goatee.\\n\\nUser: Can you edit my photo to give me gray hair?\\n\\nUser: The person can smile very happily.\\n\\nUser: Can you help me add some bangs to my hair in the photo?\\n\\nUser: The hair should be gray.\\n\\nUser: I want to try a smiling expression in my image.\\n\\nSystem: That's it. What else do you want to play with?\\n\\nSystem: Are the bangs in proper shape now?\\n\\nSystem: Ok, finished. What about the hair color?\\n\\nSystem: Bingo. What else would you like to edit?\\n\\nSystem: Okay, it's done. Do you want to try editing the makeup as well?\\n\\nSystem: Finished. Is this good enough?\\n\\nSystem: Okay this is it. What about the facial hair?\\n\\nSystem: What else perhaps you would like to try?\\n\\nSystem: Done.\\n\\nSystem: Do you want to edit the bangs?\\n\\nSystem: Ok, that's it. Is this enough?\\n\\nSystem: Bingo. What else do you want to edit?\\n\\nSystem: Finished.\\n\\nSystem: What's next?\"}"}
{"id": "emnlp-2023-main-899", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the extracted concise user requests to guide the image editing process. For response generation, our method generates more human-like, interesting and engaging responses, improving the interactivity of the system. By contrast, Talk-to-Edit relies on a rule-based approach to generate template responses and employs a cascaded single-turn image editing approach, limiting its performance.\\n\\n6 Conclusion\\n\\nThis paper introduces the CHATEDIT benchmark dataset, which we believe could facilitate the research on multi-turn interactive facial image editing. The dataset poses significant challenges as it requires systems to accurately track user requests from dialogues, perform image editing based on these requests, and generate appropriate responses. We propose a baseline framework that seamlessly combines a task-oriented dialogue module and an image editing module. The introduction of the task-oriented dialogue not only enables interaction with users but also extracts concise user requests from the dialogue context to direct the image editing, avoiding the attribute forgetting and error accumulation issues in previous single-turn methods. The empirical results highlight the efficiency of our approach and the potential for further advancements in this exciting research area.\\n\\nLimitations\\n\\nOur work is the first benchmark dataset to explore multi-turn interactive facial image editing via dialogue and establishes baseline performance for a variety of scenarios. However, there is room for improvement in the following aspects: 1) In the dataset construction, we consider 21 attributes as editable attributes. However, there are out-of-domain attributes users might want to manipulate. In this case, the dialogue understanding may neglect the user requests or generate extra hallucinations. 2) The proposed baseline model has two stages, which leverages the powerful capabilities of existing models via lightweight fine-tuning. However, both the dialogue understanding module and the image editing module limit the quality of the manipulated image. This issue might be alleviated by training the whole model end-to-end, which will be included in our future research. In addition, other issues, such as how to construct a more generalized and robust facial image editing model, also require further exploration.\\n\\nEthics Statement\\n\\nIt is important to clarify that the facial images used in the CHATEDIT dataset are selected from CelebA-HQ (Karras et al., 2018), which is a dataset derived from CelebA (Liu et al., 2015). CelebA consists of images collected from the internet and is publicly available for research purposes only. The images in CelebA-HQ have undergone additional post-processing. The dialogues in the CHATEDIT dataset do not contain sensitive or private information. The dataset has been carefully curated to ensure the privacy and confidentiality of individuals. Furthermore, participants involved in the manual paraphrase and human evaluation processes were compensated with reasonable wages.\\n\\nAcknowledgement\\n\\nThis work is supported by Major Technology Innovation Program of Hangzhou, China (Grant 2022AIZD0154), National Natural Science Foundation of China (Grant No. 62306041, No. U21B2045), Beijing Nova Program (Grant No. Z211100002121106, 20230484488), and National Key R&D Program of China (Grant No. 2022YFF1202400).\\n\\nReferences\\n\\nWaheed Ahmed Abro, Annalena Aicher, Niklas Rach, Stefan Ultes, Wolfgang Minker, and Guilin Qi. 2022. Natural language understanding for argumentative dialogue systems in the opinion building domain. KBS.\\n\\nLu Chen, Xiang Zhou, Cheng Chang, Runzhe Yang, and Kai Yu. 2017. Agent-aware dropout dqn for safe and efficient on-line dialogue policy learning. In EMNLP.\\n\\nYu Cheng, Zhe Gan, Yitong Li, Jingjing Liu, and Jianfeng Gao. 2020. Sequential attention gan for interactive image editing. In ACM MM.\\n\\nJiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. 2019. Arcface: Additive angular margin loss for deep face recognition. In CVPR.\\n\\nAlaaeldin El-Nouby, Shikhar Sharma, Hannes Schulz, Devon Hjelm, Layla El Asri, Samira Ebrahimi Kahou, Yoshua Bengio, and Graham W Taylor. 2019. Tell, draw, and repeat: Generating and modifying images based on continual linguistic instruction. In ICCV.\\n\\nHenry Elder, Alexander O'Connor, and Jennifer Foster. 2020. How to make neural natural language generation as reliable as templates in task-oriented dialogue. In EMNLP.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Joseph L Fleiss and Jacob Cohen. 1973. The equivalence of weighted kappa and the intraclass correlation coefficient as measures of reliability. EPM.\\n\\nChristian Geishauser, Carel van Niekerk, Hsien-Chin Lin, Nurul Lubis, Michael Heck, Shutong Feng, and Milica Gasic. 2022. Dynamic dialogue policy for continual reinforcement learning. In COLING.\\n\\nIan Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2020. Generative adversarial networks. CACM.\\n\\nMichael Heck, Nurul Lubis, Benjamin Ruppik, Renato Vukovic, Shutong Feng, Christian Geishauser, Hsien-Chin Lin, Carel van Niekerk, and Milica Ga\u0161i\u0107. 2023. Chatgpt for zero-shot dialogue state tracking: A solution or an opportunity? arXiv:2306.01386.\\n\\nMartin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS.\\n\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng Wu, Semih Yavuz, and Richard Socher. 2020. A simple language model for task-oriented dialogue. In NeurIPS.\\n\\nYuming Jiang, Ziqi Huang, Xingang Pan, Chen Change Loy, and Ziwei Liu. 2021. Talk-to-edit: Fine-grained facial editing via dialog. In ICCV.\\n\\nTero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive growing of gans for improved quality, stability, and variation. In ICLR.\\n\\nTero Karras, Samuli Laine, and Timo Aila. 2019. A style-based generator architecture for generative adversarial networks. In CVPR.\\n\\nTero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. 2020. Analyzing and improving the image quality of stylegan. In CVPR.\\n\\nJin-Hwa Kim, Nikita Kitaev, Xinlei Chen, Marcus Rohrbach, Byoung-Tak Zhang, Yuandong Tian, Dhruv Batra, and Devi Parikh. 2019. Codraw: Collaborative drawing as a testbed for grounded goal-driven communication. In ACL.\\n\\nSatwik Kottur, Seungwhan Moon, Aram H Markosyan, Hardik Shah, Babak Damavandi, and Alborz Geramifard. 2022. Tell your story: Task-oriented dialogs for interactive content creation. arXiv:2211.03940.\\n\\nRoyi Lachmy, Valentina Pyatkin, Avshalom Manevich, and Reut Tsarfaty. 2022. Draw me a flower: Processing and grounding abstraction in natural language. TACL.\\n\\nHung Le, Doyen Sahoo, Chenghao Liu, Nancy F Chen, and Steven CH Hoi. 2020. Uniconv: A unified conversational neural architecture for multi-domain task-oriented dialogues. In EMNLP.\\n\\nHung Le, Richard Socher, and Steven CH Hoi. 2019. Non-autoregressive dialog state tracking. In ICLR.\\n\\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. 2015. A diversity-promoting objective function for neural conversation models. arXiv:1510.03055.\\n\\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. 2022a. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In ICML.\\n\\nPeipei Li, Yibo Hu, Ran He, and Zhenan Sun. 2019. Global and local consistent wavelet-domain age synthesis. TIFS.\\n\\nPeipei Li, Huaibo Huang, Yibo Hu, Xiang Wu, Ran He, and Zhenan Sun. 2020a. Hierarchical face aging through disentangled latent characteristics. In ECCV.\\n\\nPeipei Li, Xiyan Liu, Jizhou Huang, Deguo Xia, Jianzhong Yang, and Zhen Lu. 2023a. Progressive generation of 3d point clouds with hierarchical consistency. PR.\\n\\nPeipei Li, Yinglu Liu, Hailin Shi, Xiang Wu, Yibo Hu, Ran He, and Zhenan Sun. 2020b. Dual-structure disentangling variational generation for data-limited face parsing. In ACM MM.\\n\\nPeipei Li, Rui Wang, Huaibo Huang, Ran He, and Zhaofeng He. 2023b. Pluralistic aging diffusion autoencoder. In ICCV.\\n\\nZekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, and Xifeng Yan. 2022b. Controllable dialogue simulation with in-context learning. arXiv:2210.04185.\\n\\nZekun Li, Baolin Peng, Michel Galley, Jianfeng Gao, and Xifeng Yan. 2023c. Guiding large language models via directional stimulus prompting. arXiv preprint arXiv:2302.11520.\\n\\nZekun Li, Hong Wang, Alon Albalak, Yingrui Yang, Jing Qian, Shiyang Li, and Xifeng Yan. 2021. Making something out of nothing: Building robust task-oriented dialogue systems from scratch. Proceedings of Alexa Prize TaskBot.\\n\\nZhaojiang Lin, Bing Liu, Andrea Madotto, Seungwhan Moon, Zhenpeng Zhou, Paul A Crook, Zhiguang Wang, Zhou Yu, Eunjoon Cho, Rajen Subba, et al. 2021. Zero-shot dialogue state tracking via cross-task transfer. In EMNLP.\\n\\nZiwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. 2015. Deep learning face attributes in the wild. In ICCV.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Gray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. In NeurIPS.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In ACL.\\n\\nOr Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery. In ICCV.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In ICML.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR.\\n\\nShikhar Sharma, Dendi Suhubdy, Vincent Michalski, Samira Ebrahimi Kahou, and Yoshua Bengio. 2018. Chatpainter: Improving text to image generation using dialogue. In ICLRW.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML.\\n\\nYixuan Su, Lei Shu, Elman Mansimov, Arshit Gupta, Deng Cai, Yi-An Lai, and Yi Zhang. 2022. Multi-task pre-training for plug-and-play task-oriented dialogue system. In ACL.\\n\\nOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. 2021. Designing an encoder for stylegan image manipulation. TOG.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS.\\n\\nRui Wang, Peipei Li, Huaibo Huang, Chunshui Cao, Ran He, and Zhaofeng He. 2023. Learning-to-rank meets language: Boosting language-driven ordering alignment for ordinal classification. In NeurIPS.\\n\\nWeizhi Wang, Zhirui Zhang, Junliang Guo, Yinpei Dai, Boxing Chen, and Weihua Luo. 2022. Task-oriented dialogue system as natural language generation. In SIGIR.\\n\\nTianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. 2022. Hairclip: Design your hair by text and reference image. In CVPR.\\n\\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In ACL.\\n\\nYanbo Xu, Yueqin Yin, Liming Jiang, Qianyi Wu, Chengyao Zheng, Chen Change Loy, Bo Dai, and Wayne Wu. 2022. Transeditor: transformer-based dual-space gan for highly controllable facial editing. In CVPR.\\n\\nWeihao Zeng, Keqing He, Yejie Wang, Chen Zeng, Jinggang Wang, Yunsen Xian, and Weiran Xu. 2023. Futuroto: Teaching future knowledge to pre-trained language model for task-oriented dialogue. In ACL.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR.\\n\\nZicheng Zhang, Bonan Li, Xuecheng Nie, Congying Han, Tiande Guo, and Luoqi Liu. 2023. Towards consistent video editing with text-to-image diffusion models. In NeurIPS.\\n\\nZicheng Zhang, Yinglu Liu, Congying Han, Hailin Shi, Tiande Guo, and Bowen Zhou. 2022. Petsgan: Re-thinking priors for single image generation. In AAAI.\\n\\nYufan Zhou, Ruiyi Zhang, Jiuxiang Gu, Chris Tensmeyer, Tong Yu, Changyou Chen, Jinhui Xu, and Tong Sun. 2022. Tigan: Text-based interactive image generation and manipulation. In AAAI.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. 2018. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA CHATEDIT Benchmark Dataset\\n\\nEach sample in the CHATEDIT dataset contains an input image to be edited with the corresponding caption, the associated dialogue, and the user requests. Notably, similar to most image editing datasets, the dataset does not include ground truth manipulated images for each turn. Fig. 8 presents an example in CHATEDIT that contains a four-turn interaction. Fig. 9 presents the web page for human paraphrasing, where the annotators are required to check and revise the dialogue to make the conversation fluent, natural, and consistent with the user requests.\\n\\nB Network Architecture\\n\\nB.1 Architecture of the Dialogue Module\\n\\nThe model used in the dialogue module is based on the pre-trained language model T5 (Raffel et al., 2020), which is a Transformer (Vaswani et al., 2017) encoder-decoder framework. Each Transformer layer comprises an attention mechanism and a feed-forward network. Specifically, the attention mechanism is self-attention in the encoder layer and encoder-decoder attention in the decoder layer. The feed-forward network consists of a dense layer with an output dimensionality of $d_{ff}$ followed by a ReLU nonlinearity and another dense layer.\\n\\nFor the base model, both the encoder and decoder consist of 12 layers, where the \u201ckey\u201d and \u201cvalue\u201d matrices of all attention mechanisms have an inner dimensionality of $d_{kv} = 64$ and all attention mechanisms have 12 heads. The output dimensionality of the first feed-forward network in each block is $d_{ff} = 3,072$, and the dimensionality of all other sub-layers and embeddings is $d_{model} = 768$.\\n\\nThe small model scales the base model by using 6 layers for the encoder and decoder. For each layer, it utilizes 8-headed attention, $d_{ff} = 2,048$, and $d_{model} = 512$.\\n\\nThe large model has 24 layers for the encoder and decoder. It scales the base model up by using 16-headed attention, $d_{ff} = 4,096$, and $d_{model} = 1,024$.\\n\\nTable 7 summarizes the statistics of three models with different sizes.\\n\\nB.2 Architecture of the Image Editing Module\\n\\nTo supplement the description of the image editing module, the architecture of StyleGAN2 (Karras et al., 2020) generator is described in detail in this section. Specifically, it generates images gradually from low resolution to high resolution. Every major layer (every resolution) of the StyleGAN2 generator consists of two types of convolutional blocks: feature space convolutions (Conv), which are leveraged for feature map synthesis, and toRGB convolutions (ToRGB), which utilize convolutions to convert the feature map into an RGB image. Each of these convolution blocks is modulated by a vector of style parameters $w$. In our experiment, we utilized the $W+$ space where each $W+$ layer has its own style parameters $w_i \\\\in W$.\\n\\nThe details of StyleGAN2 (Karras et al., 2020) generator are listed in Table 8.\\n\\nC Details of the Human Evaluation\\n\\nQuestions of the Human Evaluation.\\n\\nIn the human evaluations, we compare our framework with Talk-to-Edit over one aspect of the manipulated image: manipulation, and three aspects on the generated response of each turn in the dialogues: humanness, interestingness, and engagingness. The instructions for these four aspects provided to participants are shown as follows:\"}"}
{"id": "emnlp-2023-main-899", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Utterances\\n\\nUser requirements\\n\\nUser: Please try to look pale.\\nmaph: pale skin;\\n\\nSystem: Done. What about trying black hair?\\n\\nUser: Good idea. Please give it a try.\\nmaph: pale skin; hair color: black hair;\\n\\nSystem: This is it. What else do you want to edit?\\n\\nUser: Make her angry.\\nmaph: pale skin; hair color: black hair; expression: angry;\\n\\nSystem: It's done. Do you want to manipulate the makeup?\\n\\nUser: Yep sure. I want to see what she looks like with a beautiful lip color.\\nmaph: pale skin, lipstick; hair color: black hair; expression: angry;\\n\\nSystem: Ok, finished. What else do you want to try?\\n\\nThis is a photo of a woman. She has brown hair. She doesn't have bangs. She doesn't have bushy eyebrows. She doesn't have heavy makeup. She doesn't have pale skin. She doesn't have rosy cheeks. She doesn't have any smiling. She is wearing lipstick.\\n\\nFigure 8: An example sample from the CHATED dataset.\\n\\nFigure 9: Interface for the manual paraphrase.\\n\\n\u2022 Manipulation: Which one manipulates the image better with facial identity unchanged?\\n\\n\u2022 Humanness: Which one sounds more natural and personable?\\n\\n\u2022 Interestingness: Which one arouses your curiosity or tells you something new or useful?\\n\\n\u2022 Engagingness: Which one is more likely to capture your attention and make you want to further interact with it?\\n\\nWe conduct the blind evaluation where participants will not be informed about the source of the manipulated images and generated responses (our framework or Talk-to-edit) to ensure fairness.\\n\\nInter-rater agreement.\\n\\nTo evaluate the agreement among the answers of all participants, we calculate the inter-rater agreement score. The average inter-rater agreement score is 0.26 in terms of Fleiss' kappa (Fleiss and Cohen, 1973), which demonstrates a fair agreement.\\n\\nMore Qualitative Results\\n\\nComparison of Single-turn Editing and Our Proposed Multi-turn Editing.\\n\\nIn Fig. 10, 11, 12, we present more visualization results to illustrate the attribute forgetting problem and error accumulation problem of previously single-turn methods, which can be avoided by our multi-turn approach. Specifically, we utilize StyleCLIP as the image editing method for both the sing-turn method and our multi-turn method for fair comparison. Notably, as shown in In Fig. 11, 12, our method still performs better in the setting that the single-turn method takes oracle user requests as input while ours take the dialogue as input and uses the dialogue module to obtain the tracked user requests.\\n\\nVisualization Results of Ablation study.\\n\\nIn the ablation study of the main paper, we demonstrate the effectiveness of the introduced dialogue module that extracts user requests to guide the image editing module quantitatively. We present qualitative results in this section. We experiment with the multi-turn method that isn't equipped with a dialogue module to extract the user requests and thus directly takes dialogue as input. As shown in Fig 13, it fails to understand the user request accurately and ignores some of the user requests, suggesting the significance of our introduced dialogue module in the multi-turn interactive editing.\\n\\nComparison Between Our Method with Talk-to-Edit.\\n\\nMore qualitative comparisons between our method with Talk-to-Edit (Jiang et al., 2021) are presented in Fig. 14, 15, 16. Fig. 14 shows that there exists the attribute forgetting problem in Talk-to-Edit. Fig. 15 illustrates that Talk-to-Edit has an error accumulation problem. Besides, as shown in these results, our method can generate better responses with proper suggestions, which improves interactivity. Moreover, error judgment in the rule-based method will lead to the unexpected break-off of the interaction in Talk-to-Edit, which is represented in Fig 16.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Comparison of single-turn editing and our proposed multi-turn editing. Both methods take oracle user requests as input to make a fair comparison. (a) shows the attribute forgetting problem of the single-turn editing method. (b) illustrates the error accumulation problem. By contrast, our proposed multi-turn editing approach avoids these issues.\\n\\nFigure 11: Comparison of single-turn editing and our proposed framework (a). Our method takes dialogue history as input and uses the dialogue module to track user requests while the oracle user requests are used for single-turn methods.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"System: Okay bingo. Do you have an inclination towards having gray hair?\\nUser: Well, it might look better this way, I think.\\nSystem: Okay finished. Is there a possibility of you opting for blond hair?\\nUser: Let me take a look. Emm, it might be nicer in that way.\\nUser: She seems to be sad.\\nUser: I want to have pale skin and I think it looks great.\\nSystem: Bingo. Do you want to try editing the hairstyle?\\nUser: Ok. Make the bangs longer, just that not too much.\\nSystem: Finished. What else do you want to edit?\\nUser: It could be a big smile.\\nSystem: Ok it's done. Do you want her to look a little bit angry?\\nUser: No, I don't want to edit in that way. I would like to see how she would look if she wore lipstick.\\nSystem: Done. Do you want to see what rosy cheeks would look like on you?\\nUser: No, thanks. I would like to see how I would be like if I have goatee.\\nSystem: Finished. Is it something you'd like to have lipstick as part of your appearance in the photo?\\nSystem: This is it. Do you want to see what rosy cheeks would look like on her?\\nUser: ...\\n\\nFigure 12: Comparison of single-turn editing and our proposed framework (b).\\nOur method takes dialogue history as input and uses the dialogue module to track user requests while the oracle user requests are used for single-turn methods.\"}"}
{"id": "emnlp-2023-main-899", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"System: Okay this is it. What else do you want to try?\\nSystem: This is it. What's next?\\nSystem: Done. What else do you want to manipulate?\\nSystem: Bingo. What else do you want to manipulate?\\nSystem: This is it. What's next?\\n\\nUser: I want to try short mustache for him.\\nUser: I'd like to see what he look like with a head of gray hair.\\nUser: He is smiling happily.\\nUser: Can you help me show a look with gray hair in the image?\\nUser: He seems serious, and there is no smile on his face.\\nUser: I want to try having a goatee, can you help me with that?\\nUser: Can you help me make my hair appear gray in the photo?\\nUser: Let's make the bangs longer.\\nUser: It should be a bigger smile.\\n\\nSystem: Okay this is it. What else do you want to try?\\nSystem: Is the smile of the person just right now?\\nSystem: It's done. Would you like to experiment with rosy cheeks?\\nSystem: What else do you want to manipulate?\\nSystem: Are the fringe in proper shape now?\\nSystem: It's done. Would you like to experiment with rosy cheeks?\\nSystem: Is the smile of the person just right now?\\n\\nUser: The hair should be gray.\\nUser: I want to experiment with a smiling expression in my image.\\nUser: Can you help me add some bangs to my hair in the photo?\\n\\nSystem: Do you want to editing the expression as well?\\nSystem: Finished. What's next?\\nSystem: OK, what else do you want to change?\\nSystem: Okay done. What else would you like to try?\\nSystem: Would you like me to help you achieve a wavy hair?\\n\\nUser: She is not smiling.\\nUser: Add bangs to her.\\nUser: I'd like to see what she would look with gray hair.\\n\\nFigure 14: Comparison of our method and Talk-to-Edit. These results demonstrate the attribute forgetting problem in Talk-to-Edit, where the goatee attribute is lost in the above cases.\\n\\nFigure 15: Comparison of our method and Talk-to-Edit. These results demonstrate the error accumulation problem in Talk-to-Edit, where artifacts occur and propagate to the final edited image.\\n\\nFigure 16: Comparison of our method and Talk-to-Edit. The left present case where the interaction is broken off unexpectedly in Talk-to-Edit due to its rule-based method. The right present case where Talk-to-Edit fails to edit.\"}"}
