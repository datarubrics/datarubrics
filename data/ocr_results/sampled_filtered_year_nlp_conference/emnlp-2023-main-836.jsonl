{"id": "emnlp-2023-main-836", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards a Better Understanding of Variations in Zero-Shot Neural Machine Translation Performance\\n\\nShaomu Tan Christof Monz\\n\\nAbstract\\n\\nMultilingual Neural Machine Translation (MNMT) facilitates knowledge sharing but often suffers from poor zero-shot (ZS) translation qualities. While prior work has explored the causes of overall low zero-shot translation qualities, our work introduces a fresh perspective: the presence of significant variations in zero-shot performance. This suggests that MNMT does not uniformly exhibit poor zero-shot capability; instead, certain translation directions yield reasonable results. Through systematic experimentation, spanning 1,560 language directions across 40 languages, we identify three key factors contributing to high variations in ZS NMT performance: 1) target-side translation quality, 2) vocabulary overlap, and 3) linguistic properties. Our findings highlight that the target side translation quality is the most influential factor, with vocabulary overlap consistently impacting zero-shot capabilities. Additionally, linguistic properties, such as language family and writing system, play a role, particularly with smaller models. Furthermore, we suggest that the off-target issue is a symptom of inadequate performance, emphasizing that zero-shot translation challenges extend beyond addressing the off-target problem. To support future research, we release the data and models as a benchmark for the study of ZS NMT.\\n\\n1 Introduction\\n\\nMultilingual Neural Machine Translation (MNMT) has shown great potential in transferring knowledge across languages, but often struggles to achieve satisfactory performance in zero-shot (ZS) translations. Prior efforts have been focused on investigating causes of overall poor zero-shot performance, such as the impact of model capacity (Zhang et al., 2020), initialization (Chen et al., 2022; Gu et al., 2019; Tang et al., 2021; Wang et al., 2021), and how model forgets language labels can affect ZS performance (Wu et al., 2021; Raganato et al., 2021).\\n\\nIn contrast, our work introduces a fresh perspective within zero-shot NMT: the presence of high variations in the zero-shot performance. This phenomenon suggests that certain ZS translation directions can closely match supervised counterparts, while others exhibit substantial performance gaps. We recognize this phenomenon holds for both English-centric systems and systems going beyond English-centric data (e.g.: m2m-100 models). This raises the question: which factors contribute to variations in the zero-shot translation quality? Through systematic and comprehensive experimentation involving 1,560 language directions spanning 40 languages, we identify three key factors contributing to pronounced variations in zero-shot NMT performance:\\n\\n1) target side translation capacity, 2) vocabulary overlap, and 3) linguistic properties. More importantly, our findings are general regardless of the resource level of languages and hold consistently across various evaluation metrics, spanning word, sub-word, character, and representation levels. Drawing from our findings, we offer potential insights to enhance zero-shot NMT.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our investigation begins by assessing the impact of supervised translation capability on zero-shot performance variations. This is achieved by decomposing the unseen ZS direction (Src \u2192 Tgt) into two seen supervised directions using pivot language English. For instance, Src \u2192 Tgt can be decomposed into two seen supervised directions: Src \u2192 En (source side translation) and En \u2192 Tgt (target side translation) for English-centric systems. Our findings show the target side translation quality significantly impacts ZS performance and can explain the variations the most. Surprisingly, the source side translation quality has a very limited impact.\\n\\nMoreover, our analysis demonstrates the substantial impact of linguistic properties, i.e., language family and writing system, in elucidating the variations in zero-shot performance. Figure 1 highlights this conclusion by showing the much stronger ZS performance of resource-rich ZS directions with similar linguistic properties compared to other directions. Intriguingly, our investigation also shows that the impacts of linguistic properties are more pronounced for smaller models. This suggests that larger models place less reliance on linguistic similarity when engaging in ZS translation, expanding more insights upon prior research about the impact of model capacity on ZS NMT (Zhang et al., 2020).\\n\\nFurthermore, we found the language pair with higher vocabulary overlap consistently yields better zero-shot capabilities, suggesting a promising future aspect to improve the ZS NMT. In addition, while Zhang et al. (2020) asserts the off-target issue as a primary cause that impairs the zero-shot capability, we conclude that the off-target issue is more likely to be a symptom of poor zero-shot translation qualities rather than the root cause. This is evident by small off-target rates (smaller than 5%) not necessarily resulting in high ZS capabilities.\\n\\nLastly, we argue that prior research on zero-shot NMT is limited by focusing only on the 1% of all possible ZS combinations (Aharoni et al., 2019; Pan et al., 2021; Tang et al., 2021) or prioritizing resource-rich language pairs (Yang et al., 2021; Raganato et al., 2021; Chen et al., 2022; Zhang et al., 2020; Qu and Watanabe, 2022). To overcome these limitations, we create the EC40 MNMT dataset for training purposes and utilize multi-parallel test sets for fair and comprehensive evaluations. Our dataset is the first of its kind considering real-world data distribution and diverse linguistic characteristics, serving as a benchmark to study ZS NMT.\\n\\n2 Related Work\\n\\n2.1 MNMT corpus\\n\\nCurrent MNMT studies mainly utilize two types of datasets: English-centric (Arivazhagan et al., 2019b; Yang et al., 2021), which is by far the most common approach, and, more rarely, non-English-centric (Fan et al., 2021; Costa-juss\u00e0 et al., 2022). English-centric datasets rely on bitext where English is either the source or target language, while Non-English-centric ones sample from all available language pairs, resulting in a much larger number of non-English directions. For instance, OPUS100 dataset contains 100 languages with 99 language pairs for training, while the M2M-100 dataset comprises 100 languages covering 1,100 language pairs (2,200 translation directions).\\n\\nNon-English-centric approaches primarily enhance the translation quality in non-English directions by incorporating additional data. However, constructing such datasets is challenging due to data scarcity in non-English language pairs. In addition, training becomes more computationally expensive as more data is included compared to English-centric approaches. Furthermore, Fan et al. (2021) demonstrates that English-centric approaches can match performance to non-English-centric settings in supervised directions using only 26% of the entire data collection. This suggests that the data boost between non-English pairs has limited impact on the supervised directions, highlighting the promise of improving the zero-shot performance of English-centric systems. Therefore, in this study, we focus on the English-centric setting as it offers a practical solution by avoiding extensive data collection efforts for numerous language pairs.\\n\\n2.2 Understanding Zero-shot NMT\\n\\nPrevious studies have primarily focused on investigating the main causes of overall poor zero-shot performance, such as the impact of model capacity, initialization, and the off-target issue on zero-shot translation. Zhang et al. (2020) found that increasing the modeling capacity improves zero-shot translation and enhances overall robustness. In addition, Wu et al. (2021) shows the same MNMT system with different language tag strategies performs significantly different on zero-shot directions while retaining the same performance on supervised directions. Furthermore, Gu et al. (2019); Tang et al. (2021); Wang et al. (2021) suggest model initialization...\"}"}
{"id": "emnlp-2023-main-836", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tion impacts zero-shot translation quality. Lastly, Gu et al. (2019) demonstrates MNMT systems are likely to capture spurious correlations and indicates this tendency can result in poor zero-shot performance. This is also reflected in the work indicating MNMT models are prone to forget language labels (Wu et al., 2021; Raganato et al., 2021).\\n\\nAttention is also paid to examining the relationship between off-target translation and zero-shot performance. Off-target translation refers to the issue where an MNMT model incorrectly translates into a different language (Arivazhagan et al., 2019a). Zhang et al. (2020) identifies the off-target problem as a significant factor contributing to inferior zero-shot performance. Furthermore, several studies (Gu and Feng, 2022; Pan et al., 2021) have observed zero-shot performance improvements when the off-target rate drops.\\n\\nOur work complements prior studies in two key aspects:\\n\\n1) Unlike previous analyses that focus on limited zero-shot directions, we examine a broader range of language pairs to gain a more comprehensive understanding of zero-shot NMT.\\n2) We aim to investigate the reasons behind the variations in zero-shot performance among different language pairs and provide insights for improving the zero-shot NMT systems across diverse languages.\\n\\n### 3 Experiment\\n\\n#### 3.1 EC40 Dataset\\n\\nCurrent MNMT datasets pose significant challenges for analyzing and studying zero-shot translation behavior. We identify key shortcomings in existing datasets:\\n\\n1) These datasets are limited in the quantity of training sentences. For instance, the OPUS100 (Zhang et al., 2020) dataset covers 100 languages but is capped to a maximum of 1 million parallel sentences for any language pair.\\n2) Datasets like PC32 (Lin et al., 2020) fail to accurately reflect the real-world distribution of data, with high-resource languages like French and German disproportionately represented by 40 million and 4 million sentences, respectively.\\n3) Linguistic diversity, a critical factor, is often overlooked in datasets such as Europarl (Koehn, 2005) and MultiUN (Chen and Eisele, 2012).\\n4) Lastly, systematic zero-shot NMT evaluations are rarely found in existing MNMT datasets, either missing entirely or covering less than 1% of possible zero-shot combinations (Aharoni et al., 2019; Pan et al., 2021; Tang et al., 2021).\\n\\nTo this end, we introduce the EC40 dataset to address these limitations. The EC40 dataset uses and expands OPUS (Tiedemann, 2012) and consists of over 66 million bilingual sentences, encompassing 40 non-English languages from five language families with diverse writing systems. To maintain consistency and make further analysis more comprehensive, we carefully balanced the dataset across resources and languages by strictly maintaining each resource group containing five language families and each family consists of eight representative languages.\\n\\nEC40 covers a wide spectrum of resource availability, ranging from High(5M) to Medium(1M), Low(100K), and extremely-Low(50K) resources. In total, there are 80 English-centric directions for training and 1,640 directions (including all supervised and ZS directions) for evaluation. To the best of our knowledge, EC40 is the first of its kind for MNMT, serving as a benchmark to study the zero-shot NMT. For more details, see Appendix A.1.\\n\\nAs for evaluation, we specifically chose Ntrex-128 (Federmann et al., 2022) and Flores-200 (Costa-juss\u00e0 et al., 2022) as our validation and test datasets, respectively, because of their unique multi-parallel characteristics. We combine the Flores200 dev and devtest sets to create our test set. We do not include any zero-shot pairs in the validation set. These datasets provide multiple parallel translations for the same source text, allowing for more fair evaluation and analysis.\\n\\n#### 3.2 Experimental Setups\\n\\n**Pre-processing**\\n\\nTo handle data in various languages and writing systems, we carefully apply data pre-processing before the experiments. Following similar steps as prior studies (Fan et al., 2021; Baziotis et al., 2020; Pan et al., 2021), our dataset is first normalized on punctuation and then tokenized by using the Moses tokenizer. In addition, we filtered pairs whose length ratios are over 1.5 and performed de-duplication after all pre-processing steps. All cleaning steps were performed on the OPUS corpus, and EC40 was constructed by sampling from this cleaned dataset.\\n\\nWe then learn 64k joint subword vocabulary using SentencePiece (Kudo and Richardson, 2018). Following Fan et al. (2021); Arivazhagan et al. (2019b), we performed temperature sampling ($T = 5$) for learning SentencePiece subwords to over-\\n\\n2https://github.com/moses-smt/mosesdecoder\\n\\n13555\"}"}
{"id": "emnlp-2023-main-836", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Average performance scores and coefficient of variation on English-centric and Zero-shot (ZS) directions. The table includes three metrics: Sacrebleu, Chrf++, SpBleu, and Comet. The best performance scores (higher means better) are highlighted in bold depending on values before rounding, while the highest CV scores in the coefficient of variation section (higher means more variability) are underlined to highlight high variations.\\n\\nModels\\nPrior research has suggested that zero-shot performance can be influenced by both model capacity (Zhang et al., 2020) and decoder pre-training (Gu et al., 2019; Lin et al., 2020; Wang et al., 2021). To provide an extensive analysis, we conducted experiments using three different models: Transformer-big, Transformer-large (Vaswani et al., 2017), and fine-tuned mBART50. Additionally, we evaluated m2m-100 models directly in our evaluations without any fine-tuning.\\n\\nTraining\\nAll training and inference in this work use Fairseq (Ott et al., 2019). For Transformer models, we follow Vaswani et al. (2017) using Adam (Kingma and Ba, 2014) optimizer with $\\\\beta_1 = 0.9, \\\\beta_2 = 0.98, \\\\epsilon = 10^{-9}$, warmup steps as 4,000.\\n\\nAs suggested by Johnson et al. (2017); Wu et al. (2021), we prepend target language tags to the source side, e.g.: '<2de>' denotes translating into German. Moreover, we follow mBART50 MNMT fine-tuning hyper-parameter settings (Tang et al., 2021) in our experiments. More training and model specifications can be found in Appendix A.2.\\n\\nEvaluation\\nWe ensure a comprehensive analysis by employing multiple evaluation metrics, aiming for a holistic assessment of our experiments. Specifically, we utilize four evaluation metrics across various levels:\\n\\n1) Chrf++: character level (Popovi\u0107, 2017),\\n2) SentencePieceBleu (SpBleu): tokenized sub-word level (Goyal et al., 2022),\\n3) Sacrebleu: detokenized word level (Post, 2018),\\n4) COMET: representation level (Rei et al., 2020).\\n\\nFor the Comet score, we use the wmt22-comet-da model (Rei et al., 2022).\\n\\nWhile we acknowledge that metrics like Sacrebleu may have limitations when comparing translation quality across language pairs, we believe that consistent findings across all these metrics provide more reliable and robust evaluation results across languages with diverse linguistic properties. For Comet scores, we evaluate the supported 35/41 languages. As for beam search, we use the beam size of 5 and a length penalty of 1.0.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Evaluation of m2m100 models on our benchmark. We consider English-centric, supervised and zero-shot directions in this table according to m2m100. This means many \u201csupervised\u201d directions are actually unseen for our mT-large system.\\n\\nTraining on zero-shot NMT become less prominent. This result is consistent for both seen and unseen languages regarding mBart50, see Appendix A.4. Our observation aligns with previous claims that the mBART model weights can be easily washed out when fine-tuning with large-scale data on supervised directions (Liu et al., 2020; Lee et al., 2022).\\n\\nQuantifying variation\\nWe identify the higher variations that exist in zero-shot translation performance than supervised directions by measuring the Coefficient of Variation ($CV = \\\\frac{\\\\sigma}{\\\\mu}$) (Everitt and Skrondal, 2010). The CV metric is defined as the ratio of the standard deviation $\\\\sigma$ to the mean $\\\\mu$ of performance, which is more useful than purely using standard deviation when comparing groups with vastly different mean values.\\n\\nAs shown in Table 1, we find substantially higher CV scores in the zero-shot directions compared to the supervised ones, with an average increase of around 100% across all models and metrics. This observation highlights that zero-shot performance is much more prone to variations compared to the performance of supervised directions. This raises the question: What factors contribute to the significant variations observed in zero-shot performance?\\n\\nExploring the Role of Non-English-Centric Systems\\nTraining with non-English language pairs has shown promise in improving zero-shot performance (Fan et al., 2021). To delve deeper into this aspect, we evaluate m2m100 models directly without further finetuning on our benchmark test set because our goal is to investigate whether the high variations in the zero-shot performance phenomenon hold for non-English-centric models.\\n\\nOur analysis consists of English-centric (54), supervised (546), and zero-shot (860) directions, which are determined by the training settings of m2m100. The results in Table 2 yield two important observations. Firstly, significant performance gaps exist between supervised and zero-shot directions, suggesting that the challenges of zero-shot translation persist even in non-English-centric systems. More importantly, our finding of considerable variations in zero-shot NMT also holds for non-English-centric systems.\\n\\nFactors in the Zero-Shot Variations\\nWe investigate factors that might contribute to variations in zero-shot directions: 1) English translation capability 2) Vocabulary overlap 3) Linguistic properties 4) Off-target issues. For consistency, we use our best model (mT-large) in the following analyses, unless mentioned otherwise. For simplicity, we denote the zero-shot direction as $\\\\text{Src} \\\\rightarrow \\\\text{Tgt}$ throughout the following discussion, where $\\\\text{Src}$ and $\\\\text{Tgt}$ represent the Source and Target language respectively. In this chapter, we present all analysis results using SpBleu and provide results based on other metrics in the Appendix for reference.\\n\\nTable 3: Resource-level analysis based on SpBleu (we provide analyses based on other metrics in appendix A.5.1). We include both English-centric (shaded blue) and zero-shot (shaded red) directions. Avg $\\\\rightarrow$ Avg denotes averaged zero-shot SpBleu score.\\n\\n5.1 English translation capability\\nWe first hypothesize that data size, which is known to play a crucial role in supervised training, may also impact zero-shot capabilities. We categorize data resource levels into four classes and examine their performance among each other as shown in Table 3. English-centric results are also included for comparison. Our findings indicate that the resource level of the target language has a stronger effect on zero-shot translations compared to that of the source side. This is evident from the larger drop in zero-shot performance (from 14.18 to 3.70) observed when the target data size decreases, as opposed to the source side (from 11.38 to 8.41).\"}"}
{"id": "emnlp-2023-main-836", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To further quantify this observation, we conducted correlation and regression analyses, see Table 4, following Lauscher et al. (2020) to analyze the effect of data size and English-centric performance. Specifically, we calculate both Pearson and Spearman for correlation, and use Mean absolute error (MAE) and root mean square error (RMSE) for regression. We use data size after temperature sampling in $\\\\text{Src} \\\\rightarrow \\\\text{En}$ and $\\\\text{En} \\\\rightarrow \\\\text{Tgt}$ directions, as well as the corresponding performances as features.\\n\\n### Results\\n\\nThree key observations can be made from these results:\\n\\n1. The factors on the target side consistently exhibit stronger correlations with the zero-shot performance, reinforcing our conclusions from the resource-level analysis in Table 3.\\n2. The English-centric performance feature demonstrates a greater R-square compared to the data size. This conclusion can guide future work to augment out-of-English translation qualities, we further expand it in the section 6.\\n3. We also observe that the correlation alone does not provide a comprehensive explanation for the underlying variations observed in zero-shot performance by visualizing the correlation (Figure 2).\\n\\n| Metrics          | Features          | Data-size\u2020 | En-centric perf. |\\n|------------------|-------------------|------------|------------------|\\n|                  |                   |            |                  |\\n| **Correlation**  |                   |            |                  |\\n| Pearson          |                   | 0.16       | 0.53             |\\n| Spearman         |                   | 0.18       | 0.61             |\\n| **Regression**   |                   |            |                  |\\n| R-square         |                   | 32.54%     | 61.34%           |\\n| MAE              |                   | 5.47       | 4.70             |\\n| RMSE             |                   | 6.62       | 5.59             |\\n\\nTable 4: Analysis of zero-shot performance considering data size and English-centric performance based on Sp-Bleu. Data-size\u2020 is after the temperature sampling as it represents the actual size of the training set.\\n\\n### 5.2 The importance of Vocabulary Overlap\\n\\nVocabulary overlap between languages is often considered to measure potential linguistic connections such as word order (Tran and Bisazza, 2019), making it a more basic measure of similarity in surface forms compared to other linguistic measurements such as language family and typology distance (Philippy et al., 2023). Stap et al. (2023) also identifies vocabulary overlap as one of the most important predictors for cross-lingual transfer in MNMT. In our study, we investigate the impact of vocabulary sharing on zero-shot NMT.\\n\\nWe build upon the measurement of vocabulary overlap proposed by Wang and Neubig (2019) and modify it as follows:\\n\\n$$\\\\text{Overlap} = \\\\frac{|V_{\\\\text{Src}} \\\\cap V_{\\\\text{Tgt}}|}{|V_{\\\\text{Tgt}}|},$$\\n\\nwhere $V_{\\\\text{Src}}$ and $V_{\\\\text{Tgt}}$ represent the vocabularies of the source (Src) and target (Tgt) languages, respectively. This measurement quantifies the proportion of subwords in the target language that is shared with the source language in the zero-shot translation direction.\\n\\nFigure 2: Correlation between $\\\\text{En} \\\\rightarrow \\\\text{Tgt}$ Sp-Bleu and zero-shot (All $\\\\leftrightarrow \\\\text{Tgt}$) Sp-Bleu. Each faded blue point denotes the performance of a single zero-shot direction based on Sp-Bleu. $R=0.68$ indicates the Pearson correlation coefficient (see Table 4 for more details).\\n\\n### Setup\\n\\nWe first investigate the correlation between the vocabulary overlap and zero-shot performance. As noted by Philippy et al. (2023), vocabulary overlap alone is often considered insufficient to fully explain transfer in multilingual systems. We share this view, particularly in the context of multilingual translation, where relying solely on vocabulary overlap to predict zero-shot translation quality presents challenges. Hence, we incorporate the extent of the vocabulary overlap factor into our regression analysis with English translation performance in section 5.1.\\n\\n### Results\\n\\nAs shown in Table 5. The results indicate that considering the degree of overlap between the source and target languages further contributes to explaining the variations in zero-shot performance. Importantly, this pattern holds true across different model capacities, and it shows more consistent results than linguistic features such as script and family. We recognize this conclusion can promote future investigation on how to improve the zero-shot NMT. For example, encouraging greater cross-lingual transfer via better vocabulary sharing by leveraging multilingual dictionaries, or implicitly learning multilingual word alignments via multi-source translation, we leave them to future work.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Prediction of Zero-Shot Performance using En-centric performance, vocabulary overlap, and linguistic properties. We present the result based on SpBleu in this table.\\n\\n5.3 The impact of Linguistic Properties\\n\\nPrevious work on cross-lingual transfer of NLU tasks, such as NER and POS tagging, highlights that transfer is more successful for languages with high lexical overlap and typological similarity (Pires et al., 2019) and when languages are more syntactically or phonologically similar (Lauscher et al., 2020). In the context of multilingual machine translation, although it is limited to only validating four ZS directions, Aharoni et al. (2019) has empirically demonstrated that the zero-shot capability between close language pairs can benefit more than distant ones when incorporating more languages. Accordingly, we further extend this line of investigation by examining linguistic factors that may impact zero-shot performance in MNMT. We measure the role of two representative linguistic properties, namely language family and writing system, in determining the zero-shot performance. The specific information on linguistic properties of each language can be found in Appendix A.1.\\n\\nSetup\\n\\nTo examine the impact of linguistic properties on zero-shot performance, we specifically evaluate the performance in cases where: 1) source and target language belong to the same or different family and 2) source and target language use the same or different writing system. This direct comparison allows us to assess how linguistic similarities between languages influence the effectiveness of zero-shot translation.\\n\\nResults\\n\\nTo provide a fine-grained analysis, we examine this phenomenon across different resource levels for the target languages, as shown in Table 6. The results reveal a significant increase in zero-shot performance when the source and target languages share the same writing system, irrespective of the resource levels. Additionally, we observe that the language family feature exhibits relatively weaker significance as shown in Appendix A.5.3. To further quantify the effect of these linguistic properties on ZS NMT, we conduct a regression analysis, see Table 5. Our findings highlight their critical roles in explaining the variations of zero-shot performance.\\n\\n| Tgt resource | eLow | Low | Med | High |\\n|--------------|------|-----|-----|------|\\n| If Src and Tgt in the same Language Family | No | 2.12 | 4.82 | 9.77 | 9.43 |\\n| | Yes | 3.14* | 7.69* | 13.16* | 12.88* |\\n| If Src and Tgt use the same Writing System | No | 1.58 | 3.97 | 9.31 | 8.67 |\\n| | Yes | 3.21* | 8.13* | 11.71* | 12.68* |\\n\\n* represents $p < 0.05$.\\n\\nTable 6: The impact of linguistic properties on zero-shot performance (we use mT-large and SpBleu here for an example). We conduct Welch's t-test to validate if one group is significantly better than another. The detailed table, including the impact of X resource, can be found in Appendix A.5.3.\\n\\nFurthermore, our analysis reveals interesting findings regarding the effect of linguistic properties considering the model size. As shown in Table 5, we observed that the contribution of linguistic features is more pronounced for the smaller model, i.e., mT-big. While the larger model tends to place more emphasis on English-centric performance. This suggests that smaller models are more susceptible to the influence of linguistic features, potentially due to their limited capacity and generalization ability. In contrast, larger models exhibit better generalization capabilities, allowing them to rely less on specific linguistic properties.\\n\\n5.4 The role of Off-Target Translations\\n\\nPrevious work (Gu and Feng, 2022; Pan et al., 2021) have demonstrated a consistent trend, where stronger MNMT systems generally exhibit lower off-target rates and simultaneously achieve better zero-shot BLEU scores. To further investigate this, we analyze the relationship between off-target rates and different levels of zero-shot performance.\\n\\nSetup\\n\\nWe adopt the off-target rate measurement from Yang et al. (2021) and Costa-juss\u00e0 et al. (2022) using fasttext (Joulin et al., 2016) to detect if a sentence is translated into the correct language.\\n\\nResults\\n\\nWhile Zhang et al. (2020) identifies the off-target issue as a crucial factor that contributes...\"}"}
{"id": "emnlp-2023-main-836", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Correlation between off-target rate and zero-shot performance (SpBleu). R represents the Spearman correlation coefficient. We focus on directions where the off-target rate is considerably low (less than 5%). Results based on other metrics can be found in A.5.5.\\n\\nFrom poor zero-shot results. However, our analysis, as illustrated in Figure 3, suggests that the reasons for poor zero-shot performance go beyond just the off-target issue. Even when the off-target rate is very low, e.g., less than 5% of sentences being off-target, we still observe a wide variation in zero-shot performance, ranging from very poor (0.1 SpBleu) to relatively good (34.6 SpBleu) scores. Based on these findings, we conclude that the off-target issue is more likely to be a symptom of poor zero-shot translation rather than the root cause. This emphasizes that translating into the correct language cannot guarantee decent performance.\\n\\n6 From Causes to Potential Remedies\\n\\nIn this section, we summarize our findings and offer insights, building upon the previous observations.\\n\\nEnhance target side translation\\nWe identified that the quality of target side translation (En $\\\\rightarrow$ Tgt) strongly influences the overall zero-shot performance in an English-centric system. To this end, future research should explore more reliable approaches to enhance the target side translation capability. One practical promising direction is the use of back-translation (Sennrich et al., 2016) focusing more on improving out-of-English translations. Similarly, approaches like multilingual regularization, sampling, and denoising are worth exploring to boost the zero-shot translation directions.\\n\\nFocus more on distant pairs\\nWe recognize that distant language pairs constitute a significant percentage of all zero-shot directions, with 61% involving different scripts and 81% involving different language families in our evaluations. Our analysis reveals that, especially with smaller models, distant pairs exhibit notably lower zero-shot performance compared to closer ones. Consequently, enhancing zero-shot performance for distant pairs is a key strategy to improve overall capability. An unexplored avenue for consideration involves multi-source training (Sun et al., 2022) using Romanization (Amrhein and Sennrich, 2020), with a gradual reduction in the impact of Romanized language.\\n\\nEncourage cross-lingual transfer via vocabulary sharing\\nFurthermore, we have consistently observed that vocabulary overlap plays a significant role in explaining zero-shot variation. Encouraging greater cross-lingual transfer and knowledge sharing via better vocabulary sharing has the potential to enhance zero-shot translations. Previous studies (Wu and Monz, 2023; Maurya et al., 2023) have shown promising results in improving multilingual translations by augmenting multilingual vocabulary sharing. Additionally, cross-lingual pre-training methods utilizing multi-parallel dictionaries have demonstrated improvements in word alignment and translation quality (Ji et al., 2020; Pan et al., 2021).\\n\\n7 Conclusion\\n\\nIn this work, we introduce a fresh perspective within zero-shot NMT: the presence of high variations in the zero-shot performance. We recognize that our investigation of high variations in zero-shot performance adds an important layer of insight to the discourse surrounding zero-shot NMT, which provides an additional perspective than understanding the root causes of overall poor performance in zero-shot scenarios.\\n\\nWe first show the target side translation quality significantly impacts zero-shot performance the most while the source side has a limited impact. Furthermore, we conclude higher vocabulary overlap consistently yields better zero-shot performance, indicating a promising future aspect to improve zero-shot NMT. Moreover, linguistic features can significantly affect ZS variations in the performance, especially for smaller models. Additionally, we emphasize that zero-shot translation challenges extend beyond addressing the off-target problem.\\n\\nWe release the EC-40 MNMT dataset and model checkpoints for future studies, which serve as a benchmark to study zero-shot NMT. In the future, we aim to investigate zero-shot NMT from other views, such as analyzing the discrepancy on the representation level.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOne limitation of this study is the over-representation of Indo-European languages in our dataset, including languages in Germanic, Romance, and Slavic sub-families. This could result in non-Indo-European languages being less representative in our analysis. Additionally, due to data scarcity, we were only able to include 5 million parallel sentences for high-resource languages. As a result, the difference in data size between high and medium-resource languages is relatively small compared to the difference between medium and low-resource languages (which is ten times). To address these limitations, we plan to expand the EC40 dataset in the future, incorporating more non-Indo-European languages and increasing the data size for high-resource languages.\\n\\nBroader Impact\\n\\nWe collected a new multilingual dataset (EC40) from OPUS, which holds potential implications for the field of multilingual machine translation. The EC40 dataset encompasses a diverse range of languages and language pairs, offering researchers and developers an expanded pool of data for training and evaluating translation models. It also serves as a benchmark for enabling fair comparisons and fostering advancements in multilingual translation research. Recognizing the inherent risks of mistranslation in machine translation data, we have made efforts to prioritize the incorporation of high-quality data, such as the MultiUN (Chen and Eisele, 2012) dataset (translated documents from the United Nations), to enhance the accuracy and reliability of the EC40 dataset. By sharing the EC40 dataset, we aim to contribute to the promotion of transparency and responsible use of machine translation data, facilitating collaboration and driving further progress in multilingual machine translation research.\\n\\nAcknowledgments\\n\\nThis research was funded in part by the Netherlands Organization for Scientific Research (NWO) under project number VI.C.192.080. We would like to thank Di Wu, Yan Meng, David Stap, and Baohao Liao for their useful comments and discussions. We would also like to thank the reviewers for their feedback.\\n\\nReferences\\n\\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019. Massively multilingual neural machine translation. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3874\u20133884.\\n\\nChantal Amrhein and Rico Sennrich. 2020. On romanization for model transfer between scripts in neural machine translation. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2461\u20132469.\\n\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Roee Aharoni, Melvin Johnson, and Wolfgang Macherey. 2019a. The missing ingredient in zero-shot neural machine translation. arXiv preprint arXiv:1903.07091.\\n\\nNaveen Arivazhagan, Ankur Bapna, Orhan Firat, Dmitry Lepikhin, Melvin Johnson, Maxim Krikun, Mia Xu Chen, Yuan Cao, George Foster, Colin Cherry, et al. 2019b. Massively multilingual neural machine translation in the wild: Findings and challenges. arXiv preprint arXiv:1907.05019.\\n\\nChristos Baziotis, Barry Haddow, and Alexandra Birch. 2020. Language model prior for low-resource neural machine translation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7622\u20137634.\\n\\nGuanhua Chen, Shuming Ma, Yun Chen, Dongdong Zhang, Jia Pan, Wenping Wang, and Furu Wei. 2022. Towards making the most of cross-lingual transfer for zero-shot neural machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 142\u2013157.\\n\\nYu Chen and Andreas Eisele. 2012. Multiun v2: Un documents with multilingual alignments. In LREC, pages 2500\u20132504.\\n\\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\\n\\nMatthew S. Dryer and Martin Haspelmath, editors. 2013. WALS Online (v2020.3). Zenodo.\\n\\nBrian S Everitt and Anders Skrondal. 2010. The cambridge dictionary of statistics.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, et al. 2021. Beyond english-centric multilingual machine translation. The Journal of Machine Learning Research, 22(1):4839\u20134886.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Christian Federmann, Tom Kocmi, and Ying Xin. 2022. Ntrex-128\u2013news test references for mt evaluation of 128 languages. In Proceedings of the First Workshop on Scaling Up Multilingual Evaluation, pages 21\u201324.\\n\\nNaman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana Krishnan, Marc'Aurelio Ranzato, Francisco Guzm\u00e1n, and Angela Fan. 2022. The flores-101 evaluation benchmark for low-resource and multilingual machine translation. Transactions of the Association for Computational Linguistics, 10:522\u2013538.\\n\\nJiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2019. Improved zero-shot neural machine translation via ignoring spurious correlations. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1258\u20131268.\\n\\nShuhao Gu and Yang Feng. 2022. Improving zero-shot multilingual translation with universal representations and cross-mapping. In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 6492\u20136504.\\n\\nBaijun Ji, Zhirui Zhang, Xiangyu Duan, Min Zhang, Boxing Chen, and Weihua Luo. 2020. Cross-lingual pre-training based transfer for zero-shot neural machine translation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 115\u2013122.\\n\\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351.\\n\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016. Fasttext. zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.\\n\\nDiederik P. Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. CoRR, abs/1412.6980.\\n\\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of machine translation summit x: papers, pages 79\u201386.\\n\\nTaku Kudo and John Richardson. 2018. Sentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. EMNLP 2018, page 66.\\n\\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli\u0107, and Goran Glava\u0161. 2020. From zero to hero: On the limitations of zero-shot language transfer with multilingual transformers. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 4483\u20134499.\\n\\nEn-Shiun Lee, Sarubi Thillainathan, Shravan Nayak, Surangika Ranathunga, David Adelani, Ruisi Su, and Arya D McCarthy. 2022. Pre-trained multilingual sequence-to-sequence models: A hope for low-resource language translation? In Findings of the Association for Computational Linguistics: ACL 2022, pages 58\u201367.\\n\\nZehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pre-training multilingual neural machine translation by leveraging alignment information. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2649\u20132663.\\n\\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726\u2013742.\\n\\nKaushal Kumar Maurya, Rahul Kejriwal, Maunendra Sankar Desarkar, and Anoop Kunchukuttan. 2023. Utilizing lexical similarity to enable zero-shot machine translation for extremely low-resource languages. arXiv preprint arXiv:2305.05214.\\n\\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier, and Michael Auli. 2019. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 48\u201353.\\n\\nXiao Pan, Mingxuan Wang, Liwei Wu, and Lei Li. 2021. Contrastive learning for many-to-many multilingual neural machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 244\u2013258.\\n\\nFred Philippy, Siwen Guo, and Shohreh Haddadan. 2023. Towards a common understanding of contributing factors for cross-lingual transfer in multilingual language models: A review. arXiv preprint arXiv:2305.16768.\\n\\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019. How multilingual is multilingual bert? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4996\u20135001.\\n\\nMaja Popovi\u0107. 2017. chrf++: words helping character n-grams. In Proceedings of the second conference on machine translation, pages 612\u2013618.\\n\\nMatt Post. 2018. A call for clarity in reporting bleu scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 186\u2013191.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhi Qu and Taro Watanabe. 2022. Adapting to non-centered languages for zero-shot multilingual translation. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5251\u20135265.\\n\\nAlessandro Raganato, Ra\u00fal V\u00e1zquez, Mathias Creutz, and J\u00f6rg Tiedemann. 2021. An empirical investigation of word alignment supervision for zero-shot multilingual neural machine translation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. The Association for Computational Linguistics.\\n\\nRicardo Rei, Jos\u00e9 GC De Souza, Duarte Alves, Chrysoula Zerva, Ana C Farinha, Taisiya Glushkova, Alon Lavie, Luisa Coheur, and Andr\u00e9 FT Martins. 2022. Comet-22: Unbabel-ist 2022 submission for the metrics shared task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 578\u2013585.\\n\\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon Lavie. 2020. Comet: A neural framework for mt evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2685\u20132702.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Improving neural machine translation models with monolingual data. In 54th Annual Meeting of the Association for Computational Linguistics, pages 86\u201396. Association for Computational Linguistics (ACL).\\n\\nDavid Stap, Vlad Niculae, and Christof Monz. 2023. Viewing knowledge transfer in multilingual machine translation through a representational lens. arXiv preprint arXiv:2305.11550.\\n\\nSimeng Sun, Angela Fan, James Cross, Vishrav Chaudhary, Chau Tran, Philipp Koehn, and Francisco Guzm\u00e1n. 2022. Alternative input signals ease transfer in multilingual machine translation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 5291\u20135305.\\n\\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Nanman Goyal, Vishrav Chaudhary, Jiatao Gu, and Angela Fan. 2021. Multilingual translation from denoising pre-training. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3450\u20133466.\\n\\nJ\u00f6rg Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC\u201912), pages 2214\u20132218.\\n\\nKe Tran and Arianna Bisazza. 2019. Zero-shot dependency parsing with pre-trained multilingual sentence representations. EMNLP-IJCNLP 2019, page 281.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nWeizhi Wang, Zhirui Zhang, Yichao Du, Boxing Chen, Jun Xie, and Weihua Luo. 2021. Rethinking zero-shot neural machine translation: From a perspective of latent variables. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4321\u20134327.\\n\\nXinyi Wang and Graham Neubig. 2019. Target conditioned sampling: Optimizing data selection for multilingual neural machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 5823\u20135828.\\n\\nDi Wu and Christof Monz. 2023. Beyond shared vocabulary: Increasing representational word similarities across languages for multilingual machine translation. arXiv preprint arXiv:2305.14189.\\n\\nLiwei Wu, Shanbo Cheng, Mingxuan Wang, and Lei Li. 2021. Language tags matter for zero-shot neural machine translation. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 3001\u20133007.\\n\\nYilin Yang, Akiko Eriguchi, Alexandre Muzio, Prasad Tadepalli, Stefan Lee, and Hany Hassan. 2021. Improving multilingual translation by representation and gradient regularization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7266\u20137279.\\n\\nBiao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. 2020. Improving massively multilingual neural machine translation and zero-shot translation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1628\u20131639.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendices\\n\\nA.1 Dataset Statistics\\n\\nWe list the details of the EC40 dataset in Table 7. Overall, EC40 is an English-centric multilingual machine translation dataset containing over 66 million sentences including 41 languages (together with English). EC40 is more profound in the total number of languages and in the balance of language family and writing systems. Specifically, for each language family, we include 8 representative languages across different resources. Moreover, we set the number of sentences the same for each resource level, e.g.: all High-resource Languages have 5M sentences. Note: we list precise numbers in the table instead of approximate ones, for instance, 5M denotes exactly 5,000,000 number of sentences after preprocessing. We use ISO 639-1 in this table.\\n\\nWe follow Flores-200 to label the writing system classes and use WALS (Dryer and Haspelmath, 2013) to label the language family for languages in our dataset.\\n\\nA.2 Training and Model specification\\n\\n| Model  | Num Encoder layers | Decoder layers | Emb | FFN Size | Head | Vocabsize |\\n|--------|--------------------|----------------|-----|----------|------|-----------|\\n| mT-big | 24                 | 6              | 6   | 1024     | 4096 | 16        | 64 kmT-large | 418 | 12 | 12 | 1024 | 4096 | 16 | 64 kmBart50 FT | 610 | 12 | 12 | 1024 | 4096 | 16 | 250k |\\n\\nTable 8: Model specification\\n\\nWe also show the model specifications of mTransformer-big, mTransformer-large, and mBart50 Fine-tuning in the Table 8. It is worth noting that mBart50 utilizes vocabulary larger than our trained-from-scratch models. Furthermore, we adopt Vaswani et al. (2017) to set up the learning rate as 5e-4 with 4000 warmup steps and label smoothing of 0.1.\\n\\nTo keep the consistency of learning Sentence-Piece vocabulary, we also used temperature sampling ($T = 5$) for training all models. We trained all models (including mBart50 FT) with 4 NVIDIA A6000 GPUs for a maximum of 200k updates.\\n\\nFor larger models, we set the total max tokens as 215,040 using gradient accumulation to stimulate the large batch-size training in Tang et al. (2021).\\n\\nA.3 Validation of Spurious Correlation\\n\\nTo ensure that our model does not inadvertently capture spurious correlations during training, we conduct a validation process by visualizing the perplexity curves for both English-centric and zero-shot directions as proposed by (Gu et al., 2019). It is important to note that these curves are solely used for visualization purposes and are not used as criteria for early stopping. Our early stopping criterion for training is based solely on the validation perplexity, and we only consider English-centric directions in this regard.\\n\\nIn Figure 4 and Figure 5, we present the perplexity curves for English-centric and zero-shot directions, respectively. We observe that the perplexities for the zero-shot directions gradually decrease during training, indicating that the model is learning and improving its translation performance on those directions. Importantly, no significant overfitting patterns are observed in the zero-shot perplexity curves. Instead, the decreasing perplexities on zero-shot directions suggest that the model is effectively learning the underlying patterns and generalizing its translation capabilities to unseen language pairs.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Germanic Romance Slavic Indo-Aryan Afro-Asiatic\\nISO Language Script ISO Language Script ISO Language Script ISO Language Script ISO Language Script\\n\\nTable 7: Details of Our EC40 Multilingual Machine Translation Dataset. Numbers in the table represent the number of sentences, for example, 5m denotes exactly 5,000,000 number of sentences. Two exceptions are Hausa and Kabyle, where their data-size are 334k (334,000) and 18k (18,448) respectively.\\n\\nA.4 mBART50 Performance comparison\\nWe show performance comparison results on English-centric and ZS directions in Figure 6, 7, and 8 categorized by both seen and unseen languages. It is clear that fine-tuned mBart50 outperforms the mT-large model on most of X \u2192 En directions, but lags behind in En \u2192 X and zero-shot directions.\\n\\nFurthermore, our conclusions are more comprehensive and reliable due to two factors: 1) compare to Tang et al. (2021), our evaluation set encompasses multi-parallel sentences, allowing for performance assessment across various language pairs, including low-resource directions. 2) compare to Wang et al. (2021), we employ the Transformer-large model, enabling a fairer comparison to mBART50 in terms of model size.\\n\\nFigure 6: Performance comparison between fine-tuned mBart50 and mT-large on En2X directions.\\n\\nA.5 Additional Experiment Results based on other metrics\\nWe ensure a comprehensive analysis by employing multiple evaluation metrics, aiming for a holistic assessment of our experiments. In the paper, we have already shown the results based on the SpBleu, thus, we provide results for all analyses based on other metrics (Sacrebleu, Chrf++, Comet) in this section.\\n\\nA.5.1 Resource-level Analysis\\nTable 9, 10, and 11 show the Resource level analysis of the mTransformer-large model for both English-centric and Zero-shot directions across three different metrics. Combined with Table 3, we verify that our conclusions in section 5.1 are consistent across all four metrics.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: Resource-Based Translation Performance Analysis of mT-large based on Sacrebleu. We include both English-centric and zero-shot directions.\\n\\n| Comet | Target | En | High | Med | Low | e-Low | Avg |\\n|-------|--------|----|------|-----|-----|-------|-----|\\n| Source |       |    |      |     |     |       |     |\\n| En     | -      | 83.58 | 82.89 | 78.77 | 70.02 | 78.82 |\\n| High   | 84.48 | 68.37 | 69.87 | 61.29 | 50.59 | 62.53 |\\n| Med    | 80.64 | 66.43 | 67.41 | 59.57 | 49.55 | 60.74 |\\n| Low    | 76.56 | 62.09 | 63.57 | 54.20 | 46.08 | 56.49 |\\n| e-Low  | 71.91 | 58.16 | 60.31 | 53.22 | 43.98 | 53.92 |\\n| Avg    | 78.40 | 63.76 | 65.29 | 57.07 | 47.55 | 59.46 |\\n\\nTable 10: Resource-Based Translation Performance Analysis of mT-large based on Chrf++. We include both English-centric and zero-shot directions.\\n\\nA.5.2 The impact of data and English-centric performance\\n\\nTable 11, 13, and 14 show the impact of data and English-centric performance of the mTransformer-large model across three different metrics. Combined with Table 4, we verify that our conclusions in section 5.1 are consistent across all four metrics.\\n\\n| Metrics        | Features | Data-size | En-centric perf. | Src_size | Tgt_size | Src \u2192 En | En \u2192 Tgt |\\n|----------------|----------|-----------|-------------------|----------|----------|----------|----------|\\n| Correlation    | Pearson  | 0.15      | 0.52              | 0.40     | 0.67     |          |          |\\n|                | Spearman | 0.18      | 0.62              | 0.38     | 0.69     |          |          |\\n| Regression     | R-square | 30.93%    | 60.97%            | 4.12     | 3.46     |          |          |\\n|                | MAE      | 4.96      | 8.12              | 10.07    | 8.42     |          |          |\\n|                | RMSE     |           |                   |          |          | 4.96     | 10.07    |\\n\\nTable 12: Analysis of zero-shot performance considering data size and English-centric performance based on Sacrebleu.\\n\\n| Metrics        | Features | Data-size | En-centric perf. | Src_size | Tgt_size | Src \u2192 En | En \u2192 Tgt |\\n|----------------|----------|-----------|-------------------|----------|----------|----------|----------|\\n| Correlation    | Pearson  | 0.12      | 0.54              | 0.34     | 0.74     |          |          |\\n|                | Spearman | 0.15      | 0.59              | 0.35     | 0.72     |          |          |\\n| Regression     | R-square | 34.47%    | 64.50%            | 11.08    | 7.35     |          |          |\\n|                | MAE      | 11.08     | 8.12              |          |          |          |          |\\n|                | RMSE     | 10.07     | 10.07             |          |          |          |          |\\n\\nTable 13: Analysis of zero-shot performance considering data size and English-centric performance based on Chrf++.\\n\\n| Metrics        | Features | Data-size | En-centric perf. | Src_size | Tgt_size | Src \u2192 En | En \u2192 Tgt |\\n|----------------|----------|-----------|-------------------|----------|----------|----------|----------|\\n| Correlation    | Pearson  | 0.25      | 0.43              | 0.53     | 0.66     |          |          |\\n|                | Spearman | 0.31      | 0.47              | 0.46     | 0.60     |          |          |\\n| Regression     | R-square | 4.40%     | 69.53%            | 11.08    | 7.35     |          |          |\\n|                | MAE      | 11.08     | 11.08             |          |          |          |          |\\n|                | RMSE     | 13.67     | 13.67             |          |          |          |          |\\n\\nTable 14: Analysis of zero-shot performance considering data size and English-centric performance based on Comet.\\n\\nA.5.3 The effect of Linguistic properties\\n\\nWe investigate how zero-shot performances change if the source and target languages are linguistically more similar, considering language family and writing system. Table 15 demonstrates the fine-grained analysis regarding the effect of linguistic properties on the ZS translation quality.\\n\\n| X resource | Y resource | eLow | Low | Med | High |\\n|------------|------------|------|-----|-----|------|\\n| If X and Y belong to the same Language Family | No | 5.05 | 5.92 | 7.67 | 7.49 |\\n| | 2.12 | 4.82 | 9.77 | 9.43 |\\n| | Yes | 9.15*** | 8.74* | 9.62 | ns |\\n| | 3.14* | 7.69*** | 13.16** | 12.88** |\\n| If X and Y use the same Writing system | No | 4.86 | 5.10 | 6.78 | 6.87 |\\n| | 1.58 | 3.97 | 9.31 | 8.67 |\\n| | Yes | 6.97*** | 9.15*** | 9.56*** | 9.66*** |\\n| | | 3.21*** | 8.13*** | 11.71*** | 12.68*** |\\n\\nWelch's t-test: *** p< 0.001, ** p< 0.01, * p< 0.05, ns denotes p> 0.05.\\n\\nTable 15: The impact of linguistic properties on zero-shot performance. To investigate it in depth, we analyze it in fine-grained levels by observing different resource levels of Y. We also conducted Welch's t-test to validate if one group is significantly better than another.\\n\\nA.5.4 Overall Correlation Analysis using all factors\\n\\nOur findings hold consistently across various evaluation metrics, spanning word, sub-word, character, and representation levels. For analyses in the section 5.2 and 5.3, we show the additional results that based on Sacrebleu, Chrf++, Comet in Table 16.\\n\\nA.5.5 The role of Off-Target Issue\\n\\nWe utilized SpBleu in Section 5.4 to align with the setup employed by Zhang et al. (2020). We...\"}"}
{"id": "emnlp-2023-main-836", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 16: Prediction of Zero-Shot Performance using En-centric performance, vocabulary overlap, and linguistic properties.\\n\\nNote that all experimental setups are the same, the only difference is the change of MT metric, for more details please check the setup descriptions in Section 5.4.\\n\\nTable 17: Correlation coefficient between zero-shot performance and off-target rate when focusing on directions where the off-target rate is considerably low (less than 5%).\\n\\nA.6 Performance for all directions\\n\\nFigure 9 and 10 illustrates the specific results of the mTransformer-large model on all 1,560 zero-shot directions using four metrics.\"}"}
{"id": "emnlp-2023-main-836", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Zero-shot performance of mTransformer-large on 1560 directions for Sacrebleu and Chrf++\\n\\nFigure 10: Zero-shot performance of mTransformer-large on 1560 directions for SpBleu and Comet\"}"}
