{"id": "acl-2023-long-820", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\nThe individual section named Limitations after section 8.\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nThe individual section named Ethics Statement.\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\nsection 1\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nLeft blank.\\nB \u25a1 Did you use or create scientific artifacts?\\nSection 3, Section 5\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\nSection 3, Section 5\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nSection Ethics Statement, Section Appendix B\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nSection Ethics Statement\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/ anonymize it?\\nSection Ethics Statement\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\nSection 3\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\nSection 3, Section 4\\nC \u25a1 Did you run computational experiments?\\nSection 5\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\nNo response.\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-820", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 5, Appendix B\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSection 5\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nSection 5, Appendix B\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nSection 3\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nSection 3, Appendix D\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nSection 3, Appendix D\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nEthics Statement\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nSection 3\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nSection 3\"}"}
{"id": "acl-2023-long-820", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tell2Design: A Dataset for Language-Guided Floor Plan Generation\\n\\nSicong Leng1,*, Yang Zhou2,*,\u2020, Mohammed Haroon Dupty3, Wee Sun Lee3, Sam Conrad Joyce4, Wei Lu1\\n\\n1 StatNLP Research Group, Singapore University of Technology and Design\\n2 Institute of High Performance Computing (IHPC), A*STAR Singapore\\n3 School of Computing, National University of Singapore\\n4 Meta Design Lab, Singapore University of Technology and Design\\n\\n{sicong_leng,sam_joyce,luwei}@sutd.edu.sg\\nzhou_yang@ihpc.a-star.edu.sg {dmharoon,leews}@comp.nus.edu.sg\\n\\nAbstract\\n\\nWe consider the task of generating designs directly from natural language descriptions, and consider floor plan generation as the initial research area. Language conditional generative models have recently been very successful in generating high-quality artistic images. However, designs must satisfy different constraints that are not present in generating artistic images, particularly spatial and relational constraints. We make multiple contributions to initiate research on this task. First, we introduce a novel dataset, Tell2Design (T2D), which contains more than 80k floor plan designs associated with natural language instructions. Second, we propose a Sequence-to-Sequence model that can serve as a strong baseline for future research. Third, we benchmark this task with several text-conditional image generation models. We conclude by conducting human evaluations on the generated samples and providing an analysis of human performance. We hope our contributions will propel the research on language-guided design generation forward.\\n\\n1 Introduction\\n\\nRecently, text-conditional generative AI models (Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022; Dhariwal and Nichol, 2021; Ho et al., 2022) have demonstrated impressive results in generating high-fidelity images. Such models generally focus on understanding high-level visual concepts from sentence-level descriptions, and the generated images are valued for looking realistic and being creative, thereby being more suitable for generating artwork. However, besides less constrained generation like artworks, generating designs that meet various requirements specified in natural languages is also much needed in practice (Stiny, 1980; Seneviratne et al., 2022; Zhang et al., 2022; Wei et al., 2022). In particular, a design process always involves interaction between users/clients, who define objectives, constraints, and requirements that should be met, and designers, who need to develop various solutions with domain-specific experiences and knowledge. For example, users may dictate their house design requirements in text and expect expert architects to perform the floor plan generation.\\n\\nPrevious research in layout generation aims to automate the process of layout design in different domains such as scientific documents, mobile UIs, indoor scenes, etc (Zhong et al., 2019; Deka et al., 2017; Song et al., 2015; Janoch et al., 2013; Xiao et al., 2013; Silberman et al., 2012; Cao et al., 2022; Wu et al., 2019). Most of them perform the generation either based on several hand-crafted constraints or by using unconstrained generation. In practice, it can be more convenient for users to indicate their preferences in natural language.\\n\\nAmong various design tasks, floor plan design, as shown in Figure 1 is of moderate complexity. However, it still intrinsically involves multiple rounds of communications between clients and designers. Architectural floor plans, i.e., interior building layouts, are documents that indicate room types, room connections, room sizes, etc. They play a crucial role while designing, understanding, or remodeling indoor spaces (Liu et al., 2017).\"}"}
{"id": "acl-2023-long-820", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"signers for specifying requirements, and requires a high level of precision and alignment to detail. AI systems that can learn to generate practically useful floor plan designs directly from natural languages will go a long way in reducing the protracted design process and making Generative AI directly usable for design by the end users.\\n\\nTo allow people without expertise to participate and further enhance the design process, we aim to enable users to design by \\\"telling\\\" instructions, with a specific focus on the floor plan domain as the initial area of research. This sets forth a new machine learning task where the model learns to generate floor plan designs directly from language instructions. However, this task brings up two technical challenges. First, a floor plan is a structured layout that needs three intrinsic components to be valid: (1) **Semantics**, which describes the functionality of rooms (e.g., for living or bathing); (2) **Geometry**, which indicates the shape and dimension of individual rooms; (3) **Topology**, which defines the connectivity among different rooms (Pizarro et al., 2022). Second, these instructions are expressed in natural languages, which, besides the diversity of expressions, inherently suffer from ambiguity, misleading information, and missing descriptions for intrinsic components.\\n\\nTo address the above challenges, we make multiple contributions to initiate research on the task of language-guided floor plan generation. First, we contribute a novel dataset, **Tell2Design (T2D)**, to the research community. The T2D dataset contains more than 80k real floor plans from residential buildings. Each floor plan is associated with a set of language instructions that describes the intrinsic components of every room in a plan. An example from the dataset is illustrated in Figure 1. Second, we propose a **Sequence-to-Sequence (Seq2Seq)** approach as a solution to this task which also serves as a strong baseline for future research. Our approach is strengthened by a new strategy to explicitly incorporate the floor plan boundary constraint by transforming the outline into a box sequence. Third, in order to benchmark this novel task and evaluate our proposed approach, we implement strong baselines in text-conditional image generation on our T2D dataset and ask humans to perform the same task. The generation alignment with language instructions is evaluated both quantitatively and qualitatively. Finally, we discuss several future directions that are worth exploring based on our experimental results.\\n\\nIn summary, our main contributions are:\\n\\n- **We introduce a novel language-guided floor plan generation task along with the T2D dataset consisting of both natural human-annotated and large-scale artificially generated language instructions (Section 3).**\\n- **We propose a new approach that formulates the floor plan generation task as a Seq2Seq problem (Section 4).**\\n- **We provide adequate quantitative evaluations on all baselines and qualitative analysis of human evaluations and performances (Section 5).**\\n\\n### 2 Related Work\\n\\n**Text-Conditioned Image Generation**\\n\\nImage generation is a well-studied problem, and the most popular techniques have been applied for both unconditional image generation and text-conditional settings. Early works apply auto-regressive models (Mansimov et al., 2015), or train GANs (Xu et al., 2018; Zhu et al., 2019; Tao et al., 2022; Zhang et al., 2021; Ye et al., 2021) with publicly available image captioning datasets to synthesize realistic images conditioned on sentence-level captions. Other works have adopted the VQ-VAE technique (Van Den Oord et al., 2017) to text-conditioned image generation by concatenating sequences of text tokens with image tokens and feeding them into auto-regressive transformers (Ramesh et al., 2021; Ding et al., 2021; Aghajanyan et al., 2022). More recently, some works have applied diffusion models (Ho et al., 2020; Nichol and Dhariwal, 2021; Saharia et al., 2022c; Dhariwal and Nichol, 2021; Ho et al., 2022; Saharia et al., 2022a; Rombach et al., 2022; Nichol et al., 2022; Saharia et al., 2022b; Ramesh et al., 2022) and received wide success in image generation, outperforming other approaches in fidelity and diversity, without training instability and mode collapse issues (Brock et al., 2018; Dhariwal and Nichol, 2021; Ho et al., 2022). However, these models operate on extracting high-level visual concepts from the short text and produce artwork-like images that are expected to be realistic and creative, thereby not suitable for generating designs that must satisfy various user/client requirements.\\n\\n**Layout Generation**\\n\\nLayout generation is essentially a design process that requires meeting domain-specific constraints, where the desirable\"}"}
{"id": "acl-2023-long-820", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"layouts could be documents, natural scenes, mobile phone UIs, and indoor scenes. For example, PubLayNet (Zhong et al., 2019) is proposed to generate machine-annotated scientific documents with five different element categories: text, title, figure, list, and table. RICO (Deka et al., 2017) is introduced to develop user interface designs for mobile applications, which contains button, toolbar, etc. SUN RGB-D (Song et al., 2015) presents a combined scene-understanding task, including indoor scenes from three other datasets (Janoch et al., 2013; Xiao et al., 2013; Silberman et al., 2012). Moreover, ICVT (Cao et al., 2022) aims to produce advertisement poster layouts automatically, where the image background is given as input. The above methods are designed for different layout domains and cannot be directly applied to floor plan design. Moreover, none of them has considered generating the layout design directly from languages.\\n\\nFloor Plan Generation\\n\\nSeveral methods have been proposed to generate floor plan designs automatically (Wu et al., 2018; Liu et al., 2013; Merrell et al., 2010; Hua, 2016; Wu et al., 2019; Chen et al., 2020; Chaillou, 2020). Most of these methods generate floor plans conditioned on certain constraints, such as room types, adjacencies, and boundaries. For example, Merrell et al. (2010) generate buildings with interior floor plans for computer graphics applications using Bayesian networks without considering any human preferences. Liu et al. (2013) present an interactive tool to generate desired floor plan following a set of manually defined rules. Hua (2016) particularly focus on generating floor plans with irregular regions. Wu et al. (2018) cast the generation as a mixed integer quadratic programming problem where some floor plan components are formulated into a set of inequality constraints. More recently, Wu et al. (2019) propose a CNN-based method to determine the location of different rooms given boundary images as a constraint. Chen et al. (2020) provide a small amount of template-based artificial verbal commands and manually parses them to scene graphs for guiding the generation.\\n\\nIn summary, existing methods represent the intrinsic components of floor plans in several specific formats as generation constraints. Some formats are straightforward, such as boundary images, but they only specify limited constraints and lead to less controllable generation. Other formats, such as scene graphs and inequalities, can incorporate more information but require specific domain knowledge and extra-human efforts in pre-processing. We instead provide a unified and natural way of conditioning the floor plan generation with a set of language instructions that is much more flexible and user-friendly to characterize floor plans with various constraints.\\n\\n3 Tell2Design Dataset\\n\\nIn this section, we introduce how we construct our T2D dataset, followed by the data analysis and a discussion of the main dataset challenges.\\n\\n3.1 Task Definition\\n\\nGiven a set of language instructions describing a floor plan's intrinsic components, our aim is to generate reasonable 2D floor plan designs that comply with the provided instructions.\\n\\nInput & Output\\n\\nFor each data sample, the input is a set of natural language instructions that characterize the key components of the corresponding floor plan design, which include: (1) Semantics specifies the type and functionality of each room. For example, a room as Kitchen is for cooking. (2) Geometry specifies the shape and dimension of each room. For residential buildings, it involves the room's general orientation (e.g., the north, south, northeast, southwest), area in square feet, aspect ratio, etc. (3) Topology describes the relationships among different rooms. It can be divided into three categories: relative location, connectivity, and inclusion. The desirable output is a structured interior layout that aligns with the input language instructions.\\n\\n3.2 Floor Plan Collection\\n\\nWe use floor plans from RPLAN (Wu et al., 2019) to construct our Tell2Design dataset. We remove floor plans with rarely-appeared rooms and merge similar room types such as Second Room and Guest Room. As a result, 8 different room types (i.e., common room, bathroom, balcony, living room, master room, kitchen, storage, and dining room) and 80,788 floor plans are selected for collecting language instructions. Each floor plan is converted into a 256 \u00d7 256 image where different pixel values\"}"}
{"id": "acl-2023-long-820", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Human Artificial\\n\\nTable 1: Language instruction statistics.\\n\\n| Measure               | Human | Artificial |\\n|-----------------------|-------|------------|\\n| Avg. # words per instance | 200.30 | 260.47     |\\n| Avg. # sent. per instance  | 11.89 | 23.46      |\\n| Avg. # words per room   | 29.48 | 38.44      |\\n| Avg. # sent. per room   | 1.75  | 3.46       |\\n\\nindicate different room types, from which we extract room-type labels and bounding boxes of each room to construct our dataset.\\n\\n3.3 Language Instruction Collection\\n\\nHuman Instructions\\n\\nTo collect real human language instructions, we hire crowdworkers from Amazon Mechanical Turk (MTurk) and ask them to write a set of instructions for each room according to a given floor plan image. The requested instructions should reflect the Semantic, Geometric, and Topological information of the floor plan, such that designers could ideally reproduce the floor plan layout according to the instructions. In particular, turkers are encouraged to include (but are not limited to) attributes such as room types, locations, sides, and relationships in their instructions. The definitions of these attributes are given as follows:\\n\\n- **The room type** (e.g., bathroom and kitchen) specifies the functionality of a room.\\n- **The room location** specifies the global location of a room in the floor plan and can be described by phrases such as \\\"north side\\\" and \\\"southeastern corner\\\".\\n- **The room sides** specify the length and width of a room (e.g., \\\"8 feet wide and 10 feet long\\\").\\n- **The room relationships** specify the relative position of a room with other rooms such as \\\"next to\\\", \\\"between\\\", and \\\"opposite\\\".\\n\\nDue to the noisy nature of crowdsourcing annotations, we discard some low-quality annotations to ensure the overall quality of our datasets. To this end, we manually review each annotation and discard human instructions with:\\n\\n1. incoherence, grammatical errors;\\n2. insufficient attributes; or\\n3. irrelevance to the given floor plan.\\n\\nAs a result, we collect human instructions for 8,220 floor plans, and 5,051 of them are finally accepted after manual assessment to construct our dataset.\\n\\nArtificial Instructions\\n\\nIn addition to the human-written instructions, we also generate language instructions artificially for the remaining 75,737 floor plans from pre-defined templates. To ensure that the artificial instructions are as informative as human-written ones and include all the required components, we ask 5 educated volunteers with natural language processing (NLP) backgrounds to write language instructions for each room that appeared in the given floor plan. We then summarize their instructions into multiple templates and ask expert architectural designers for proofreading. Hence, each instruction template is ensured to be informative, grammatically correct, and coherent.\\n\\nIn summary, our T2D dataset consists of 5,051 human-annotated and 75,737 artificially-generated language instructions.\\n\\n3.4 Data Analysis\\n\\nIn this section, we analyze various aspects of Tell2Design to provide a more comprehensive understanding of the dataset.\\n\\nTable 1 shows the statistics of the language instructions in our dataset. For each floor plan, the human instructions are organized in nearly 11 sentences consisting of 200 words on average. This includes around 30 words used to describe each room in more than 2 sentences. The artificially generated instructions follow a similar pattern with slightly more words.\\n\\nTo show the connections and differences between human and artificial instructions, we conduct manual assessment to construct our dataset.\\n\\nOur human instruction collection involves 5,109 different workers with 1,723 working hours, and each worker receives full compensation in alignment with the established standards of MTurk.\\n\\nMore details on the language instruction collection can be found in Appendix C.\"}"}
{"id": "acl-2023-long-820", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"pare them for the same room type, Balcony, in\\nFigure 2. Artificial instructions always exhibit com-\\nplete information, including all three key compo-\\nnents of a floor plan. They are also formatted in a\\nstructured expression, such as \\\"on the ** side\\n** sqft with an aspect ratio of **\\\", and\\n\\\"next to \\\". However, human instructions are more\\ndiverse in expression but suffer from ambiguity and\\nmissing components.\\n\\nDataset Comparison\\nIn order to see our T2D\\ndataset in perspective, we note its main differences\\nwith respect to other related datasets used for sim-\\nilar generation tasks. T2D differs from other\\ndatasets in several perspectives: (1) T2D is the first\\nlarge-scale dataset that aims to generate designs\\n(i.e., floor plans) from direct user input natural lan-\\nguage; (2) T2D has much longer text annotations\\n(i.e., 256 words per instance) compared with other\\ntext-conditional generation datasets; (3) All text in\\nT2D is written by humans or generated artificially,\\ninstead of being crawled from the internet.\\n\\n3.5 Dataset Challenges\\nIn this section, we discuss three main challenges\\nof our collected T2D dataset. We hope this dataset\\ncan facilitate the research on both design generation\\nand language understanding.\\n\\nDesign Generation under Constraints\\nThe first\\nchallenge is to perform the design generation under\\nmuch stricter constraints compared with artwork-\\nlike text-conditional image generation. Most works\\nin text-conditional image generation operate on\\ngenerating realistic and creative images that align\\nwith the main visual concepts represented by the\\nshort input text. However, creating a design from\\nlanguages has much stricter requirements on pre-\\ncision and alignment to text details. In particular,\\nthe generated floor plan design should comply with\\nconstraints such as room type, location, size, and\\nrelationships, which are specified by users using\\nnatural languages. Our main results in Section 5\\ncomparing different baselines demonstrate that ex-\\nisting text-conditional image generation techniques\\nfail to follow detailed user requirements on this\\ndesign task.\\n\\nFuzzy & Entangled Information\\nThe second\\nchallenge is to understand the big picture of the\\nentire floor plan from document-level unstructured\\ntext with fuzzy and entangled information. Be-\\nsides the general abilities required for language\\nunderstanding, such as entity recognition, coref-\\nerence resolution, relation extraction, etc, mod-\\nels also need to collaborate with fuzzy individ-\\nual room attributes and reason over entangled re-\\nlationships among different rooms to understand\\nthe entire floor plan. Specifically, one language\\ninstruction usually either specifies fuzzy descrip-\\ntions for a room's Semantic and Geometric infor-\\nmation such as \\\"on the north side\\\" and \\\"at\\nthe southeast corner\\\", or indicates the relation-\\nship of one specific room with others like \\\", next\\nto \\\" and \\\", between \\\". The provided information in\\nsuch instructions is coarse and relative, rather than\\ncomplete and precise information like numerical\\ncoordinates. As a result, to determine the location\\nof all rooms and design a reasonable floor plan,\\nmodels must collaborate with fuzzy and entangled\\ninformation residing in multiple instructions, and\\nincorporate the boundary information. Human eval-\\nuations in Section 5.4 demonstrate that room rela-\\ntionships described in language instructions are the\\nmost challenging component to be understood and\\naligned with.\\n\\nNoisy Human Instructions\\nThe third challenge\\ncomes from the ambiguous, incomplete, or mislead-\\ning information in human instructions. As intro-\\nduced in Section 3.3, the artificial instructions are\\ntemplate-based so that they always contain precise\\nand coherent information. However, for human-\\nwritten language instructions, ambiguous or noisy\\ninformation always exists. For example, during\\nhuman instruction collection, workers are asked to\\nwrite natural sentences estimating some numeric-\\nrelated attributes like room size and aspect ratio\\nreferring to the floor plan image, which may some-\\ntimes be inaccurate. Moreover, as previously dis-\\ncussed in Figure 2, other than the expression di-\\nversity, human instructions also exhibit ambiguous\\nphrasing and incomplete information. It is thus\\nmore challenging for models to retrieve accurate,\\ncomplete, and consistent information from human\\ninstructions.\\n\\n4 T2D Model\\nIn this section, we propose a simple yet effective\\nmethod for language-guided floor plan generation.\\nUnlike existing floor plan generation methods (Wu\\net al., 2019; Chen et al., 2020) that use a regres-\\nsion head to generate the bounding box of each\\nroom one at a time, we cast the floor plan genera-\"}"}
{"id": "acl-2023-long-820", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tion task as a Seq2Seq problem under the encoder-\\ndecoder framework, where room bounding boxes\\nare re-constructed into a structured target sequence.\\nThis way, our method can easily deal with various\\nlengths of instructions for floor plans with different\\nnumbers of rooms.\\n\\n4.1 Target Sequence Construction\\nRecall that our aim is to generate a floor plan layout\\nfrom language instructions, where each room can\\nbe represented by a room-type label (e.g., bathroom\\nand kitchen) and a bounding box. One bounding\\nbox can be determined by four values \\\\((x,y,h,w)\\\\),\\nwhich indicate the \\\\(x\\\\) and \\\\(y\\\\) coordinate of the center\\npoint, height (\\\\(h\\\\)), and width (\\\\(w\\\\)), respectively. To\\nsolve language-guided floor plan generation as a\\nSeq2Seq problem, we treat the instructions as the\\ninput sequence and consider bounding boxes of\\nrooms as the target sequence. Specifically, each\\nof the continuous values \\\\((x,y,h,w)\\\\) is discretized\\ninto integers between \\\\([0,255]\\\\), and the room type\\nis given by the plain text in natural language, so\\nthat they can be naturally represented as a sequence\\nof tokens. The target sequence is then constructed\\nby grouping the room type and the bounding box\\ntogether with certain special tokens. For example,\\nthe target sequence for a Balcony with the bounding\\nbox \\\\((87, 66, 18, 23)\\\\) is given as follows:\\n\\\\[\\n[\\\\text{Balcony} \\\\mid \\\\text{x coordinate} = 87 \\\\mid \\\\text{y coordinate} = 66 \\\\mid \\\\text{height} = 18 \\\\mid \\\\text{width} = 23]\\n\\\\]\\nwhere the special tokens \\\"[\\\" and \\\"]\\\" are used to\\nindicate the start and end of the target sequence\\nfor one room and \\\"\\\\mid\\\" is used to separate different\\ntarget components. We have also added semantic\\nprefixes such as \\\"\\\\text{x coordinate} = \\\" and \\\"\\\\text{height} = \\\"\\nbefore the values of bounding boxes to assist the\\ntarget sequence generation. Finally, we concatenate\\nthe target sequences of all the rooms in a floor plan\\nand add an <eos> token at the end to indicate the\\nend of the overall target sequence.\\n\\n4.2 Boundary Information Incorporation\\nThe outline/boundary of a floor plan is one of the\\nmost important constraints in floor plan generation,\\nwhich directly affects where each room should be\\nplaced and how different rooms should be aligned\\nwith the floor plan boundary. However, it is non-\\ntrivial to incorporate such boundary information\\ninto floor plan generation. Previous methods either\\nfail to take the floor plan outline into account (Wu\\net al., 2018; Liu et al., 2013; Merrell et al., 2010;\\nHua, 2016; Chen et al., 2020) or only consider\\nthe boundary image, ignoring all other constraints\\n(Wu et al., 2019; Chaillou, 2020), leading to less\\ncontrollable floor plan design.\\n\\nIn this work, we propose a novel approach to\\nincorporate boundary information by representing\\nthe irregular outline as a set of boxes. The idea\\nis to encode the boundary information by an en-\\nclosing box that is the minimum bounding region\\ncontaining the entire floor plan and several exte-\\nrior boxes that are inside the enclosing box but ex-\\ncluded from the floor plan. Figure 3 illustrate how\\nthe floor plan boundary can be characterized by the\\nenclosing (in red) and exterior boxes (in yellow).\\nThis way, we have an enclosing box represented\\nby \\\\((x_{en}, y_{en}, h_{en}, w_{en})\\\\) and\\n\\\\(M\\\\) exterior boxes by \\\\((x_{ex1}, y_{ex1}, h_{ex1}, w_{ex1})\\\\).\\nThen we adopt a similar strat-\\negy in Section 4.1 to represent the enclosing and\\nexterior boxes in a sequence as follows:\\n\\\\[\\n+x_{en}y_{en}h_{en}w_{en}-x_{ex1}y_{ex1}h_{ex1}w_{ex1}...-x_{exM}y_{exM}h_{exM}w_{exM},\\n\\\\]\\nwhere the coordinates of the enclosing and exterior\\nbox are following the tokens \\\"+\\\" and \\\"-\\\", respec-\\ntively.\\n\\nFinally, the above sequence is added after the\\ninput language instructions for training. Our ex-\\nperimental results in Section 5 show that the pro-\\nposed boundary information incorporation strategy\\nis effective in enhancing our Seq2Seq method to\\ngenerate valid rooms that align well with the floor\\nplan boundary.\\n\\n4.3 Architecture, Objective and Inference\\nTreating the target sequences that we construct\\nfrom floor plans as a text sequence, we turn to re-\\ncent architectures and objective functions that have\\nbeen effective in Seq2Seq language modeling.\"}"}
{"id": "acl-2023-long-820", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Architecture\\n\\nWe use the popular Transformer-based (Vaswani et al., 2017) encoder-decoder structure to build our Seq2Seq model for floor plan generation. The model is initialized by a pre-trained language model T5 (Raffel et al., 2020) for better language understanding abilities.\\n\\nObjective\\n\\nSimilar to language modeling, our T2D model is trained to predict the next token, given an input sequence and preceding tokens, with a maximum likelihood objective function, i.e.,\\n\\n$$\\\\max_{\\\\theta} \\\\sum_{j=1}^{L} \\\\log P_{\\\\theta}(\\\\tilde{y}_j | x, y_1:j-1),$$\\n\\nwhere $x$ is a set of instructions in natural language concatenated with the previously defined boundary sequence, $y$ is the target bounding box sequence, and $L$ is the target sequence length.\\n\\nInference\\n\\nAt inference time, we sample tokens one by one from the model likelihood, i.e., $P(\\\\tilde{y}_j | x, y_1:j-1)$. The sequence generation ends once the $<eos>$ token is sampled, and it is straightforward to parse the target sequence into predicted floor plans.\\n\\n5 Experiments\\n\\n5.1 Baselines\\n\\nSince our T2D dataset is the first to consider language-guided floor plan generation, existing layout generation methods are not applicable to this task. To further illustrate the challenge of the design generation task and the difference with the existing text-conditional image generation problem, we adapt several state-of-the-art text-conditional image generation methods as baselines for comparison. In particular, we compare our method with the following:\\n\\n- Obj-GAN (Li et al., 2019) is an object-driven attentive generative adversarial network that follows a two-step generation process. We apply the first-step box generator module, which takes the language as input and generates target objects' bounding boxes (with class labels).\\n\\n10 We have also tried other pre-trained language models like Bart (Lewis et al., 2020), and preliminary experiments indicate that initializing our model with T5 leads to better performances.\\n\\n11 There are several common sampling strategies like Greedy Search, Beam Search, and Nucleus Sampling (Holtzman et al., 2019). We apply Greedy Search since it leads to better generation quality in our preliminary experiments.\\n\\n- CogView (Ding et al., 2021) applies pre-trained VQ-VAE to transform the target image into a sequence of image tokens. Then the text and image tokens are concatenated together and fed to a Transformer decoder (i.e., GPT (Brown et al., 2020; Radford et al., 2019)) to generate text-conditional images.\\n\\n- Imagen (Saharia et al., 2022b) is one of the state-of-the-art text-to-image generation models that build upon both large language models (e.g., T5) for text understanding and diffusion models for high-fidelity image generation.\\n\\n5.2 Experimental Settings\\n\\nModel Training\\n\\nFor model training, we consider a Warm-up + Fine-tuning pipeline (Goyal et al., 2017), where the model is first warmed up on 75,737 artificial instructions, and then fine-tuned on 12,743 human instructions. To evaluate how floor plan generation methods generalize to unseen instructions, we use the remaining 12,308 human instructions as the test set, such that there is no overlapping between annotators of the training set and the test set.\\n\\nEvaluation Metrics\\n\\nFor testing, we use macro and micro Intersection over Union (IoU) scores between the ground-truth (GT) and generated floor plans at pixel level as the evaluation metrics, whose definitions are given as follows:\\n\\n$$\\\\text{Micro IoU} = \\\\frac{\\\\sum_{r=1}^{R} I_r}{\\\\sum_{r=1}^{R} U_r},$$\\n\\n$$\\\\text{Macro IoU} = \\\\frac{1}{R} \\\\sum_{r=1}^{R} I_r U_r,$$\\n\\nwhere the $I_r$ and $U_r$, respectively, denote the intersection and union of the ground-truth and predicted rooms labeled as the $r$-th room type in a floor plan. $R$ is the total number of room types. Macro IoU calculates the average IoU over different types of rooms, and Micro IoU calculates the global IoU by aggregating all rooms.\\n\\nSince Obj-GAN and our T2D model generate bounding boxes rather than images, we use a simple strategy to transform the outputs of Obj-GAN and the T2D model into images without any further refinement for a fair comparison. Specifically, we paint each room in descending order in terms of the total area of the room type and different colors.\\n\\n12 We provide more implementation details in Appendix A.\\n\\n13 Total area of the room type is computed by adding up the specific room type area across all floor plans in our dataset. This gives us the following order: living room, common room,\"}"}
{"id": "acl-2023-long-820", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: IoU scores between ground-truth and generated floor plans for the T2D model and other baselines.\\n\\n| Models            | Micro IoU | Macro IoU |\\n|-------------------|-----------|-----------|\\n| Obj-GAN           | 15.74     | 11.12     |\\n| CogView           | 10.01     | 8.31      |\\n| Imagen            | 14.74     | 15.57     |\\n| T2D (w/o bd)      | 6.46      | 4.01      |\\n| T2D               | 9.13      | 6.06      |\\n\\nTraining on human instructions only\\n\\n| Models            | Micro IoU | Macro IoU |\\n|-------------------|-----------|-----------|\\n| Obj-GAN           | 10.72     | 8.29      |\\n| CogView           | 13.48     | 11.26     |\\n| Imagen            | 9.29      | 6.64      |\\n| T2D (w/o bd)      | 32.22     | 26.24     |\\n| T2D               | 42.93     | 38.48     |\\n\\nWarm up on artificial + fine-tune on human\\n\\n| Models            | Micro IoU | Macro IoU |\\n|-------------------|-----------|-----------|\\n| Obj-GAN           | 10.68     | 8.44      |\\n| CogView           | 13.30     | 11.43     |\\n| Imagen            | 12.17     | 14.96     |\\n| T2D (w/o bd)      | 35.95     | 29.95     |\\n| T2D               | 54.34     | 53.30     |\\n\\n5.3 Main Results\\n\\nTable 2 shows the floor plan generation results on the T2D dataset, where T2D (w/o bd) indicates the T2D model without incorporating boundary information. The T2D model achieves the highest IoU scores with a micro IoU of 54.34 and a macro IoU of 53.30, outperforming other baselines by a large margin. These can be attributed to our Seq2Seq model in controlling the target box sequence generation based on salient information extracted from language instructions. In contrast, text-conditional image generation methods fail to perform well. This is probably because those models are designed to generate artwork-like images with high-level visual concepts from the short text, instead of following multiple instructions with various constraints for a specific design.\\n\\n5.4 Result Analysis\\n\\nIt is worth noting that the quantitative results indirectly evaluate how well the generated floor plans align with the language instructions since IoU scores essentially measure the overlap between generated and ground-truth floor plan layouts. Due to the complexity of our task, it is possible for the same language instruction to map to multiple floor plan designs. Therefore, a low IoU score does not necessarily mean a bad generation.\\n\\nHuman Evaluations\\n\\nTo directly evaluate the alignment between generated floor plans and language instructions, we conduct human evaluations on a subset of the T2D test set, which consists of 100 randomly sampled instructions written by different annotators. For this purpose, we invite 5 volunteers with NLP backgrounds to evaluate the degree of alignment between source language instructions and target floor plans. Specifically, we consider four partial alignment criteria in terms of room types, locations, sizes, and relationships. Each volunteer is asked to provide four ratings on a scale of 1 to 5, according to the alignment criteria.\"}"}
{"id": "acl-2023-long-820", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The T2D model vs. human performance.\\n\\nTable 3 shows the human evaluation results. As can be seen, ground-truth floor plans get high ratings for all the partial alignment criteria, and 85% of them meet all the requirements specified in the instructions. This indicates our dataset contains high-quality human instructions that align well with the ground-truth floor plan designs. On the other hand, our T2D model receives no rating less than 3.5, indicating that at least 50% of rooms with respect to their locations, sizes, and relationships can be correctly predicted. However, our method still has a gap with ground truth designs, especially in room location and relationships, which indicates the potential for improvements.\\n\\nHuman Performance\\nTo study human performance on the T2D task, we further ask our volunteers to design floor plans on 100 instances of the same subset used for human evaluations. Table 4 reports the IoU scores for our T2D model and human performance. Humans generally achieve better IoU scores. However, even if human-generated floor plans intrinsically have much better alignment with the instructions, they only obtain around 63% IoU scores with the ground truths. This exposes the nature of design diversity, i.e., a set of language instructions can map to multiple plausible floor plan designs. Figure 4 provides a real example that both our method and humans follow the same instructions but generate different floor plans.\\n\\n6 Future Research\\nIn the future, the following directions may be worth exploring to promote the performance or extend our task: (1) How to build robust language understanding models that can adapt to the presence of noise in human instructions or even locate and refine potentially inconsistent information? (2) How to explicitly incorporate the nature of design diversity and develop techniques for diverse floor plan design? (3) How to extend the language-guided floor plan generation task to more domains or more practical but challenging scenarios, where designs should be refined according to feedback from users/clients?\\n\\n7 Conclusion\\nIn this paper, we initiate the research of a novel language-guided design generation task, with a specific focus on the floor plan domain as a start. We formulate it as language-guided floor plan generation and introduce Tell2Design (T2D), a large-scale dataset that features floor plans with natural language instructions in describing user preferences. We propose a Seq2Seq model as a strong baseline and compare it with several text-conditional image generation models. Experimental results demonstrate that the design generation task brings up several challenges and is not well-solved by existing text-conditional image generation techniques. Human evaluations assessing the degree of alignment between text and design, along with the human performance on the task, expose the challenge of understanding fuzzy and entangled information, and the nature of design diversity in our task. We hope this paper will serve as a foundation and propel future research on the task of the language-guided design generation.\"}"}
{"id": "acl-2023-long-820", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nThe proposed T2D dataset has several limitations, which could be addressed in future work. First, it only considers and collects language instructions for the floor plan domain. Future work could extend this language-guided design generation task to other design domains such as documents, mobile UIs, etc. Second, it is limited in the scope of languages where we only collect instructions written in English. Future work could assess the generalizability of the T2D dataset to other languages. Third, although generating floor plan designs from languages exhibit diversity, we do not consider improving generation diversity at this moment. Future works could consider building frameworks that specifically aim at design diversity.\\n\\nEthics Statement\\n\\nIn this section, we discuss the main ethical considerations of Tell2Design (T2D): (1) Intellectual property protection. The floor plans of the T2D dataset are from the RPLAN (Wu et al., 2019) dataset. Our dataset should be only used for research purposes. (2) Privacy. The floor plan data sources are publicly available datasets, where private data from users and floor plans have been removed. Language instructions are either generated artificially or collected from Amazon Mechanical Turk, a legitimate crowd-sourcing service, and do not contain any personal information. (3) Compensation. During the language instruction collection, the salary for annotating each floor plan is determined by the instruction quality and Mturk labor compensation standard.\\n\\nAcknowledgements\\n\\nWe would like to thank the anonymous reviewers, our meta-reviewer, and senior area chairs for their constructive comments and support of our work. We also gratefully acknowledge the support of NVIDIA AI Technology Center (NV AITC) for our research. This research/project is supported by the National Research Foundation Singapore and DSO National Laboratories under the AI Singapore Program (AISG Award No: AISG2-RP-2020-016).\\n\\nReferences\\n\\nArmen Aghajanyan, Bernie Huang, Candace Ross, Vladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro Okhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis, and Luke Zettlemoyer. 2022. Cm3: A causal masked multimodal model of the internet. ArXiv.\\n\\nAndrew Brock, Jeff Donahue, and Karen Simonyan. 2018. Large scale gan training for high fidelity natural image synthesis. ArXiv.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. In Proc. of NIPS.\\n\\nYunning Cao, Ye Ma, Min Zhou, Chuanbin Liu, Hongtao Xie, Tiezheng Ge, and Yuning Jiang. 2022. Geometry aligned variational transformer for image-conditioned layout generation. In Proc. of ACM Multimedia.\\n\\nStanislas Chaillou. 2020. Archigan: Artificial intelligence x architecture. In Architectural intelligence.\\n\\nQi Chen, Qi Wu, Rui Tang, Yuhan Wang, Shuai Wang, and Mingkui Tan. 2020. Intelligent home 3d: Automatic 3d-house design from linguistic descriptions only. In Proc. of CVPR.\\n\\nBiplab Deka, Zifeng Huang, Chad Franzen, Joshua Hibschman, Daniel Afergan, Yang Li, Jeffrey Nichols, and Ranjitha Kumar. 2017. Rico: A mobile app dataset for building data-driven design applications. In Proc. of UIST.\\n\\nPrafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. In Proc. of NIPS.\\n\\nMing Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. 2021. Cogview: Mersing text-to-image generation via transformers. In Proc. of NIPS.\\n\\nPriya Goyal, Piotr Doll\u00e1r, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. 2017. Accurate, large minibatch sgd: Training imagenet in 1 hour. ArXiv.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Proc. of NIPS.\\n\\nJonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Salimans. 2022. Cascaded diffusion models for high fidelity image generation. In Proc. of JMLR.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In Proc. of FAIR.\\n\\nRuizhen Hu, Zeyu Huang, Yuhan Tang, Oliver Van Kaick, Hao Zhang, and Hui Huang. 2020. Graph2plan: Learning floorplan generation from layout graphs. ACM Transactions on Graphics.\"}"}
{"id": "acl-2023-long-820", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hao Hua. 2016. Irregular architectural layout synthesis with graphical inputs. Automation in Construction.\\n\\nAllison Janoch, Sergey Karayev, Yangqing Jia, Jonathan T Barron, Mario Fritz, Kate Saenko, and Trevor Darrell. 2013. A category-level 3d object dataset: Putting the kinect to work. In Consumer Depth Cameras for Computer Vision.\\n\\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. ArXiv.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proc. of ACL.\\n\\nWenbo Li, Pengchuan Zhang, Lei Zhang, Qiuyuan Huang, Xiaodong He, Siwei Lyu, and Jianfeng Gao. 2019. Object-driven text-to-image synthesis via adversarial training. In Proc. of CVPR.\\n\\nChen Liu, Jiajun Wu, Pushmeet Kohli, and Yasutaka Furukawa. 2017. Raster-to-vector: Revisiting floorplan transformation. In Proc. of ICCV.\\n\\nHan Liu, Yong-Liang Yang, Sawsan AlHalawani, and Niloy J Mitra. 2013. Constraint-aware interior layout exploration for pre-cast concrete-based buildings. The Visual Computer.\\n\\nElman Mansimov, Emilio Parisotto, Jimmy Lei Ba, and Ruslan Salakhutdinov. 2015. Generating images from captions with attention. ArXiv.\\n\\nPaul Merrell, Eric Schkufza, and Vladlen Koltun. 2010. Computer-generated residential building layouts. In Proc. of SIGGRAPH.\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. 2022. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. In Proc. of ICML.\\n\\nAlexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In Proc. of ICML.\\n\\nPablo N Pizarro, Nancy Hitschfeld, Ivan Sipiran, and Jose M Saavedra. 2022. Automatic floor plan analysis and recognition. Automation in Construction.\\n\\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, Peter J Liu, et al. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. In Proc. of JMLR.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with clip latents. ArXiv.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. 2021. Zero-shot text-to-image generation. In Proc. of ICML.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proc. of CVPR.\\n\\nChitwan Saharia, William Chan, Huiwen Chang, Chris Lee, Jonathan Ho, Tim Salimans, David Fleet, and Mohammad Norouzi. 2022a. Palette: Image-to-image diffusion models. In Proc. of SIGGRAPH.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed Ghasemipour, Burcu Karagol Ayan, S Sara Mahdavi, Rapha Gontijo Lopes, et al. 2022b. Photorealistic text-to-image diffusion models with deep language understanding. ArXiv.\\n\\nChitwan Saharia, Jonathan Ho, William Chan, Tim Salimans, David J Fleet, and Mohammad Norouzi. 2022c. Image super-resolution via iterative refinement. IEEE Transactions on Pattern Analysis and Machine Intelligence.\\n\\nSachith Seneviratne, Damith Senanayake, Sanka Rasnayaka, Rajith Vidanaarachchi, and Jason Thompson. 2022. Dalle-urban: Capturing the urban design expertise of large text to image transformers. ArXiv.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. 2012. Indoor segmentation and support inference from rgbd images. In Proc. of ECCV.\\n\\nShuran Song, Samuel P Lichtenberg, and Jianxiong Xiao. 2015. Sun rgb-d: A rgb-d scene understanding benchmark suite. In Proc. of CVPR.\\n\\nGeorge Stiny. 1980. Introduction to shape and shape grammars. Environment and planning B: planning and design.\\n\\nMing Tao, Hao Tang, Songsong Wu, Nicu Sebe, Xiaoyuan Jing, Fei Wu, and Bingkun Bao. 2022. Df-gan: Deep fusion generative adversarial networks for text-to-image synthesis. In Proc. of CVPR.\\n\\nAaron Van Den Oord, Oriol Vinyals, et al. 2017. Neural discrete representation learning. In Proc. of NIPS.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Proc. of NIPS.\\n\\nTianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhentao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. 2022. Hairclip: Design your hair by text and reference image. In Proc. of CVPR.\"}"}
{"id": "acl-2023-long-820", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Implementation Details\\n\\n#### T2D Parameters\\n\\nIn practice, we initialize all weights of our proposed baseline method from T5-base. In training, we use Adam (Kingma and Ba, 2014) with $\\\\beta_1=0.9$, $\\\\beta_2=0.999$, $\\\\epsilon=10^{-8}$ to update the model parameters. We fine-tune our model on 3 RTX 8000 GPUs with batch size 12 and learning rate $5\\\\times10^{-4}$ for 20 epochs.\\n\\n18 https://huggingface.co/t5-base\\n\\n#### Baseline Implementation\\n\\nFor the mentioned baselines, only Obj-GAN and CogView are open-sourced. Therefore, we adapt and implement the models from their official GitHub repositories.\\n\\nHowever, as Imagen's source codes are not published, we implement it from the most starred GitHub repository (i.e., 5.9k stars until writing this paper) and adapt it to our T2D dataset. We use the process floor plan images in Graph2Plan (Hu et al., 2020) for training. Although all baselines have provided pre-trained checkpoints for fine-tuning, our preliminary experiments indicate that training those baselines from scratch on the T2D dataset will obtain better performances. One most probable reason is the huge discrepancy between the data distributions of the baseline pre-training corpus and our T2D dataset. Those baseline checkpoints are mostly trained with real-life images with various objects and backgrounds. But our T2D dataset only focuses on the floor plan domain.\\n\\nSpecifically, for Obj-GAN, we adopt and freeze the pre-trained text encoder, and train the rest of the networks (e.g., LSTMs) from scratch. For CogView, we freeze the pre-trained VQ-VAE and initialized the main backbone, decoder-only transformer, from GPT (Radford et al., 2019; Brown et al., 2020). During training, only the parameters of the transformer backbone will be updated. For Imagen, we import the T5-large model\u2019s encoder from Hugging Face for text encoding and freeze all its parameters during training. The rest U-nets for diffusion will be updated according to the loss propagation.\\n\\n### Dataset Analysis\\n\\n#### Floor Plan Statistics\\n\\nAs shown in Table 5 and Figure 5, we present the statistics on the occurrence of each room type and the number of rooms per floor plan. There are 8 types of rooms in total. More than 92% of floor plans include at least 6 distinct rooms, and the most frequent room types are Common Room, Bathroom, Balcony, Living Room, Master Room, and Kitchen.\\n\\n#### Dataset Comparison\\n\\nAs shown in Table 8, compared with other-related layout generation datasets, our T2D dataset is the first to have language annotations and aims to generate layout designs directly.\\n\\n19 https://github.com/jamesli1618/Obj-GAN; https://github.com/THUDM/CogView\\n\\n20 https://github.com/lucidrains/imagen-pytorch\"}"}
{"id": "acl-2023-long-820", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"# Floor plan vs. # Room per floor plan\\n\\n![Graph showing the number of floor plans vs. number of rooms per floor plan in the T2D dataset.](image)\\n\\n**Figure 5:** Number of floor plans vs. number of rooms per floor plan in the T2D dataset.\\n\\nFrom languages. Since generating floor plan designs from language instructions can be naturally formulated into a text-conditional image generation problem, we compare our dataset with two benchmark text-conditional image generation datasets in Table 6. We observe that our dataset is with a similar number of images with MSCOCO and Flickr30K but contains far longer text annotation for each image (i.e., T2D has 256 words on average describing each floor plan image). Moreover, as a set of language instructions for a floor plan results in a document-level text description, we compare our dataset with other document-level NLP datasets in Table 7. We hope that our dataset can also propel the research on document-level language understanding. It is shown that our dataset has a comparable total number of samples and words with the largest DocRED (Yao et al., 2019). More importantly, our \u201cdocuments\u201d are either human-annotated or artificially generated, instead of being crawled from the internet.\\n\\n**Table 6:** Comparisons between our T2D dataset and text-conditional image generation datasets.\\n\\n| Dataset     | # Img. | Avg. # Words |\\n|-------------|--------|--------------|\\n| MS COCO     | 82,783 | 11.3         |\\n| Flickr30K   | 31,000  | 11.8         |\\n| T2D (ours)  | 80,788  | 256.7        |\\n\\n**Table 7:** Comparisons between our T2D dataset and document-level NLP datasets.\\n\\n| Dataset                    | # Doc. | # Words | # Sent. |\\n|----------------------------|--------|---------|--------|\\n| SCIERC                    | 500    | 60755   | 2217   |\\n| BC5CDR                    | 1,500  | 282k    | 11,089 |\\n| DocRED (Human)            | 5,053  | 1,002k  | 40,276 |\\n| DocRED (Distantly)        | 101,873| 21,368k | 828,115|\\n| T2D (Human)               | 5,051  | 1,011k  | 60.057 |\\n| T2D (Artificial)          | 75,737 | 19,727k | 1,776k |\\n\\n**Dataset Collection Details**\\n\\n**Human instruction**\\n\\nWe employ Amazon Mechanical Turk (MTurk) to let annotators write language instructions for a given RGB 2D floor plan. Amazon considers this web service \u201cartificial intelligence,\u201d and it is applied in various fields, including data annotation, survey participation, content moderation, and more. The global workforce (called \u201cturkers\u201d in the lingo) is invited for a small reward to work on \u201cHuman Intelligence Tasks\u201d (HITs), created from an XML description of the task from business companies or individual sponsors (called \u201crequesters\u201d). HITs can display a wide variety of content (e.g., text and images) and provide many APIs, e.g., buttons, checkboxes, and input fields for free text. In our case, turkers are required to fill the blank input fields in HITs with language instructions for each room, following our guidelines. A screenshot of one of our HITs is displayed in Figure 6. We also show a full example of human instructions in Figure 7.\\n\\n**Artificial Instruction**\\n\\nThe artificial instructions in our T2D dataset are generated from scripts with several pre-defined templates. We carefully select volunteers with natural language processing backgrounds for drafting templates. Before participating in the annotation process, each annotator was required to undergo a qualification round consisting of a series of test annotations. We illustrate how we generate an instruction to describe one room\u2019s as-\"}"}
{"id": "acl-2023-long-820", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: A screenshot of our HITs GUI.\"}"}
{"id": "acl-2023-long-820", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Balcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nRestricted\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet, 10 feet in length and 5 feet in width. The bathroom can be accessed at the southern point of the kitchen, the first balcony toward the western kitchen wall and the livingroom. The kitchen is relatively closest in size to the balconies.\\n\\nThe livingroom is located in the northeastern portion of the floorplan. It is entered through the front entry door, and is approximately 480 square feet. The bathroom, kitchen, common rooms one and two, the master bedroom and second balcony can all be accessed through the livingroom.\\n\\nThe master room is located in the southern end of the floorplan. It is roughly 200 square feet, 10 feet in width and 20 feet in length. The second balcony, livingroom and common room one can be accessed through the master room. The master room is about the size of the two common rooms combined.\\n\\nHuman Instruction Example:\\n\\nBalcony one is located at the northwestern point of the floorplan. It is approximately 50 square feet in size. It can be accessed through the kitchen, bathroom and common room 2.\\n\\nBalcony 2 is located in the most southern point of the floorplan, just east of the master room. It is roughly 12 feet in length and five feet in width. Access points include the livingroom and master room.\\n\\nThe bathroom is located south of the kitchen. It can be accessed through the east wall of the livingroom, as well as the southern kitchen wall. Common room 2 has access to the bathroom through its eastern wall. It is the smallest room in the floorplan. At approximately 25 square feet in size, it is just a bit smaller than balcony 1 and half the size of balcony 2.\\n\\nCommon room one is located on the western portion of the floor plan. It can be accessed through common room 2 at the north, the master room at the south and the livingroom from the east. Common room one is about 100 square feet, 10 feet in length and 10 feet in width, making it the second largest room in the floor plan.\\n\\nCommon room 2 is just a bit smaller than Common room 1, approximately 90 square feet. It is located just north of Common room 1, with access points including the bathroom at the northeast, balcony at the north, and livingroom at the east.\\n\\nThe kitchen is located in the most northern point of the floorplan. It is roughly 50 square feet"}
{"id": "acl-2023-long-820", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It would be good to have a common room. I would like to place common room at the north side of the apartment. The common room should be around 200 sqft with the aspect ratio of 3 over 4. The common room should have an en-suite bathroom. The common room should be next to the bathroom, kitchen, balcony.\\n\\nThe bathroom should be considered. Place bathroom at the south side of the apartment. Make bathroom around 50 sqft with the aspect ratio of 7 over 8. The bathroom can be used by guest. The bathroom connects to the common room, master room, living room.\\n\\nMake a kitchen. The kitchen should be at the south side of the apartment. Make kitchen approx 50 sqft with the aspect ratio of 7 over 4. The kitchen attaches to the common room, balcony, master room, living room.\\n\\nCan you make a balcony? I would like to place balcony at the south side of the apartment. Can you make balcony around 50 sqft with the aspect ratio of 5 over 2? The balcony is private. The balcony connects to the common room, kitchen, master room.\\n\\nThe master room should be considered. The master room should be at the south side of the apartment. Make master room approx 150 sqft with the aspect ratio of 4 over 5. The master room should have an en-suite bathroom. The master room should be next to the bathroom, kitchen, balcony.\\n\\nIt would be great to have a living room. Make living room around 650 sqft with the aspect ratio of 1 over 2.\\n\\nArtificial Instruction Example:\\n\\nThe north side of this home is not complete without the balcony. Access to the approximately 16 sq ft area can be made through the living room or through the common room beside it.\\n\\nBathroom1 is in the eastern section of the home. It is located next to the living room and is approximately 15 sq ft.\\n\\nThe larger of the two, Bathroom 2, is approximately 30 sq ft. It is between the master bedroom and common area 2, along the western side of the house.\\n\\nCommonroom 1 occupies the northeast corner of the property. At roughly 80 sq ft it is conveniently located next to the balcony.\\n\\nCommon room 2 is nearly 100 sq ft. Occupying the northwest corner, it is easily accessible from the kitchen beside it, or the shared access from the living area.\\n\\nThe kitchen is positioned on the north side of the house, between the living room and second common area. It measures about 50 sq ft.\\n\\nThe living room is conveniently located in the southeast corner of the home. It spans approximately 250 sq ft while offering access to almost every room in the house.\\n\\nLocated in the southwest corner of the home is the master bedroom. This space is approximately 120 sq ft and is positioned next to the living room.\"}"}
