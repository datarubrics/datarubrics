{"id": "acl-2023-long-688", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Resolving Indirect Referring Expressions for Entity Selection\\nMohammad Javad Hosseini Filip Radlinski Silvia Pareti Annie Louis\\nGoogle Research {javadh,filiprad,spareti,annielouis}@google.com\\n\\nAbstract\\nRecent advances in language modeling have enabled new conversational systems. In particular, it is often desirable for people to make choices among specified options when using such systems. We address this problem of reference resolution, when people use natural expressions to choose between the entities. For example, given the choice 'Should we make a Simnel cake or a Pandan cake?' a natural response from a dialog participant may be indirect: 'let's make the green one'. Such natural expressions have been little studied for reference resolution. We argue that robustly understanding such language has large potential for improving naturalness in dialog, recommendation, and search systems. We create AltEntities, a new public dataset of 42K entity pairs and expressions (referring to one entity in the pair), and develop models for the disambiguation problem. Consisting of indirect referring expressions across three domains, our corpus enables for the first time the study of how language models can be adapted to this task. We find they achieve 82%-87% accuracy in realistic settings, which while reasonable also invites further advances.\\n\\n1 Introduction\\nNatural dialog often requires resolving referring expressions (REs), not only within and across texts, but also for grounding natural language expressions to specific entities or images. We focus on a specific conversational setting where a speaker's utterance intends to disambiguate between known named entities. While many aspects of RE resolution have been studied extensively, past work has focused on pragmatic reasoning (Dale and Reiter, 1995; Frank and Goodman, 2012), influence of discourse (Orita et al., 2015), and multimodal (e.g., image) context (Zhang et al., 2018).\\n\\n1 Our dataset can be found at https://github.com/google-research-datasets/AltEntities\\n\\nTable 1: Responses to the question which intend to choose Pandan cake over the alternative.\\n\\nDid you mean a Simnel or Pandan cake?\\nIt looks surprisingly green in color\\nWithout any frosting or fruit\\nIt is made from some leaf\\nComes from Indonesia\\nIsn't the Easter one\\n\\nIn the specific case of dialog, when people make choices, the natural REs are not always item names, spatial locations or attributes present in the question. For instance when the choice is among items with similar names (perhaps disambiguating automatic speech recognition errors), or items with difficult to pronounce names, or where the user does not even recall which name is correct but instead recalls some higher level attribute, the user may choose an indirect expression (Table 1). Most related to our work, Celikyilmaz et al. (2014) previously studied REs in response to a set of related items (e.g., Harry Potter movies) shown in a user interface. Their work both contains direct (using entity name), indirect, as well as locational (entity's position on the screen) expressions. Predating recent advances in language models (LMs), their best model is a decision tree classifier consuming knowledge graph metadata.\\n\\nIn this work, we created the AltEntities corpus by a multi-step process, soliciting crowdworkers to provide diverse yet realistic natural expressions for selecting entities in three domains: BOOKS, RECIPES, and MUSIC. To obtain natural and casual dialogic language, we introduce a novel cartoon-based annotation approach (Figure 1). AltEntities consists of 6,247 alternative questions (presenting two entities) along with 42,529 REs. In this context, REs are typically definite noun phrases with a pronominal head and a restrictive relative phrase or one of its reduced variants.\\n\\nOur experiments are based on fine-tuned BERT\"}"}
{"id": "acl-2023-long-688", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(Devlin et al., 2019) and T5 (Raffel et al., 2020) LMs. We assess the representation of entity names as well as other sources of entity information. We find that the results depend significantly on the type of entity information provided to the models alongside the REs: If a LM only has access to the entity names but no other information, a case that might happen especially for long tail entities, accuracy is around 60%. On the other hand, if a LM is (unrealistically) given entity information that is identical to that shown to annotators producing the REs, accuracy is very high (up to 95%). However, if the model (more realistically) only has access to generic information that may or may not overlap with annotators' knowledge (Section 5), accuracy of our models is only 82%-87%, leaving significant room for methodological improvements.\\n\\n2 Related Work\\n\\nOur work adds to recent efforts to allow users to speak more naturally to conversational systems. Here, we present the most related studies focusing on the properties of REs as well as their resolution.\\n\\nAlternative Questions. Our questions belong to the class of alternative questions (e.g., 'Are you staying or leaving?'). Several studies have focused on the form and semantics of such questions, and differences from yes/no questions particularly on the basis of prosody (Beck and Kim, 2006; Biezma and Rawlins, 2012; Pruitt and Roelofsen, 2013).\\n\\nThis paper focuses on the deep understanding of answers to such alternative questions when they are posed for selecting between two entities.\\n\\nSpeaker-Listener Cooperation. The research in this space follow the Rational Speech Act Theory (Frank and Goodman, 2012), where the way speakers and listeners reason about each others' intentions and beliefs explains which attributes speakers pick to describe an entity, and how listeners disambiguate the entity. Vogel et al. (2013); Monroe et al. (2017) focus on the pragmatic reasoning involved during the conversation which helps in reaching a common understanding of the topic. Wilkes-Gibbs and Clark (1992) study how REs change as the conversation proceeds. In an experiment, they show that participants start from long and indefinite descriptions of images, but end up with short and definite references. Jordan and Walker (2005) study the subproblem of content and attribute selection for generating object descriptions.\\n\\nIn our data collection, we assume a conversation between two humans in three dialog turns, where the first two turns prime the RE produced in the last turn (Section 3).\\n\\nCommon Ground. In addition to the interlocutors' intentions, their prior or shared knowledge also plays an important role in how they understand each other's utterances. Sometimes the common knowledge arises from a shared situation, e.g., in navigation dialog (Engonopoulos et al., 2013; Misu et al., 2014; Fang et al., 2014) or the presence of a visual space (Yu et al., 2018; Bernardi and Pezzelle, 2021). In the latter, the common ground is given, i.e., it is assumed the image is what all participants in the interaction see in the same way. In many other situations, e.g., in a dialog between two friends about a movie or a book, the common ground is hidden and we can only make assumptions of what information participants share. In this work, during data collection, we assume that annotators have access to rich common ground involving multiple modalities such as text, image, and video (Section 3.3). During model training inference, we explore performance with varying levels of background information (Section 5.2).\\n\\nImplicature Understanding. This paper advances the broad area of understanding implicature in dialog. For example, a few recent papers developed datasets and models for indirect boolean responses (without saying 'yes' or 'no') (Pragst and Ultes, 2018; Louis et al., 2020; Takayama et al., 2021; Damgaard et al., 2021). Interestingly, Ruis et al. (2022) shows that LLMs cannot solve such implicatures in a zero-shot setting.\\n\\nRE resolution. There are few prior studies around the data and models for resolution tasks such as ours. Stoyanchev et al. (2021) built a method where references to items from prior context in a dialog are resolved by detecting state updates. Unlike our work, their REs focus on attributes (e.g., Italian in the Italian restaurant) discussed in prior dialog. Celikyilmaz et al. (2014) collect REs to a target item among others shown on a screen (e.g., a set of Harry Potter movies). Their expressions contain both direct (reference to entity name) and indirect references, where the latter comprise about 25% of the data (\u22486K REs). To aid the resolution of indirect ones, they include features which capture the overlap between an expression and knowledge graph attributes for each item.\\n\\nOur work creates a large scale corpus (42K REs) exclusively for indirect REs, and explores how LMs\"}"}
{"id": "acl-2023-long-688", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To maximize generalizability, we collect data in three domains: BOOKS, RECIPES, and MUSIC. These were selected to cover a diverse variety of entity types with different kinds of available information \u2014 e.g., plot summaries for books, images for recipes, and lyrics and videos for songs. We performed careful and detailed annotations, and explain the annotation steps in this section.\\n\\n3.1 Cartoon-driven Annotation Setup\\n\\nPrevious work in question-answering and dialog typically asks annotators to complete text-based input boxes (Rajpurkar et al., 2016; Choi et al., 2018; Rajpurkar et al., 2018; Reddy et al., 2019; Eric et al., 2020). We employ a novel cartoon-bubble completion method, aiming to immerse annotators in the dialog setting to obtain more natural and informal REs. We start with a brief overview of the setup, and then explain the steps in detail.\\n\\nFigure 1 shows the first (of our two) annotation screens. Annotators are shown a cartoon with two characters (Bob and Alice) in a fictional conversation, and asked (as Bob) to complete the last speech bubble. This pictorial depiction, and the casting of the dialog as a casual chat between friends encourage the annotators to produce friendly, short, and dialogic responses. However, annotators are generally unlikely to know details about entities sampled from a collection. Therefore, we also provide background information on the entities (bottom of Figure 1), corresponding to common knowledge that the two characters could share on the topic.\\n\\nAfter annotators are shown this information, they proceed to a second screen (Figure 2). It indicates one of the entities (books in this example). They are asked to describe that entity (indirectly) with 3 to 5 responses: We found eliciting more entries encourages diversity and depth in the responses. Our data consists of the entity pairs, their descriptions, the target entity, and annotator expressions.\\n\\nFrom Figure 2, note that once on the response screen, annotators cannot re-read descriptions. This encourages recall from memory. The reasoning behind this, and many other aspects of this design, are explained in the next sections.\\n\\n3.2 The Conversational Cartoon\\n\\nThe cartoon has three cells as shown in Figure 1. The first is a domain-specific utterance intended to set context. For example, \u2018Remember that book we saw at the store?\u2019 sets up the dialog as one recalling a specific book. These utterances are from a set of five manually written expressions for each domain, with one selected at random for each conversation. Examples in the RECIPES and MUSIC domains are \u2018That recipe on today\u2019s Masterchef was too good!\u2019 and \u2018You sang that song really well yesterday.\u2019 Appendix A shows all these utterances.\\n\\nThe alternative question is presented in the second cell. This question follows a fixed template: Do you mean \u2018A\u2019 or \u2018B\u2019? where \u2018A\u2019 and \u2018B\u2019 are the names of two related entities. Our entities are sampled from Wikipedia page titles, with any disambiguation parentheses removed. When the names are identical, we retain the Wikipedia disambiguation: For instance, one such question is \u2018Do you mean \u2018The Gladiator (Turtledove novel)\u2019 or \u2018The Gladiator (Scarrow novel)?\u2019\\n\\nThe third cell is completed by the crowdworkers, assuming the role of Bob to enter text that refers to the target entity. They enter those expressions as shown in Figure 2. Further screenshots of our interface for all domains are provided in Appendix B.\\n\\n3.3 Entity Background\\n\\nIn real dialogs, when people differentiate between options, they draw on partial knowledge about entities that they recall. We aimed to foster a similar situation in our corpus, while doing so in a controlled manner without requiring domain-expert annotators. As such, when selected entities are shown to annotators, they are also presented with background information (bottom of Figure 1). We draw the background also from Wikipedia, biasing towards sections relevant to each domain. For BOOKS, these are the main (first) and plot summary sections. For RECIPES, we used the main, preparation, and ingredients sections. For each entity, up to 750 characters of one of these sections are shown on the interface. For RECIPES, the food\u2019s image is also always shown to help the annotators quickly realize what it looks like (Figure 3). For MUSIC, however, we found Wikipedia text to be less useful: Pages contain details and trivia (e.g., 5th single on the album or sold 4 million copies), which we judged unlikely to be included.\\n\\nWe filtered out examples without any images.\"}"}
{"id": "acl-2023-long-688", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Annotators were shown a cartoon in which they were asked to complete the final step of a conversation.\\n\\nFigure 2: Annotation screen for entering expressions.\\n\\nIn natural background knowledge about a song. On the other hand, song lyrics and music are very relevant in this domain, but are not usually found in Wikipedia. Consequently, we presented a Google search link for the song in the background section, and asked the annotators to listen to at least some of each song, and read about them before writing expressions. The search query contained the song's title and its artist, e.g., Hello (by Adele). Since information about the song comes from search, we also biased our candidates towards popular songs, which have more detailed results (Section 3.4).\\n\\n3.4 Generating Alternative Questions\\n\\nThe alternative questions (Do you mean 'A' or 'B?') are generated automatically: (i) Candidate entities are extracted from English Wikipedia for each domain (Section 3.4.1), then (ii) we substitute 'A' and 'B' by sampling entity pairs (Section 3.4.2).\\n\\n3.4.1 Selecting Candidate Entities\\n\\nFor each domain, we collect English Wikipedia articles by checking the presence of certain Wikipedia templates (infoboxes), and the presence of particular sections: For RECIPES, we additionally included articles with an ingredients section. This set was then filtered to exclude very short articles, or those ambiguous between domains. For MUSIC, we use article length (number of sections/subsections) as a proxy for popularity, and choose the top $\\\\approx 1000$ articles. To remove any sensitive or offensive content, we also filter articles whose content matches a list of sensitive words. Appendix C contains the details of the above filters.\\n\\nTable 2 shows the number of candidate entities.\\n\\nInfoboxes are fixed-format tables that consistently present articles in a given category (e.g., all books).\"}"}
{"id": "acl-2023-long-688", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.4.2 Sampling Entity Pairs\\n\\nMuch linguistic work on alternative questions has focused on the semantics and pragmatics of these utterances (Biezma and Rawlins, 2012), but we also need to make decisions about which entity pairs could make for a challenging disambiguation problem. Entity pairs sampled uniformly at random are less likely to be interesting, since they may not share many properties, making disambiguation easier. In this work, we develop entity pair sampling techniques at different similarity levels, as a proxy for disambiguation difficulty.\\n\\nUniform sampling. Entity pairs are sampled uniformly at random from the domain.\\n\\nSame name. These entities have the same name in Wikipedia followed by a disambiguation phrase within parentheses. An example is Dawn (McLaughlin novel) and Dawn (Andrews novel).\\n\\nSimilar title. These entities have a similar title in terms of character edit distance (distance \u2264 3), where the title could optionally consists of a disambiguation phrase within parentheses.\\n\\nSimilar description. This method looks for deeper similarity within the text of Wikipedia articles: We sample a first entity uniformly, then select the second with the highest similarity using a Universal Sentence Encoder (Cer et al., 2018). The input to the encoder is the Wikipedia section shown as the background knowledge to annotators.\\n\\nSimilar infobox attributes. Here we take entities that share important domain-specific properties, e.g., recipe origin, or the song genre. We match entities (except BOOKS) using the \u2018attributes\u2019 listed in the Wikipedia infobox: {type} and {type, country} for RECIPES, and {genre}, {artist}, and {genre, artist} for MUSIC.\\n\\nWe applied the same name method only to BOOKS, and the similar title method only to BOOKS and RECIPES. The other domains did not contain enough such examples. We applied the similar description method to all domains. We applied the similar infobox attributes method to RECIPES and MUSIC, but not the BOOKS domain; however, some pairs with identical attributes were already covered by the other methods for BOOKS.\\n\\nTable 3 shows the number of sampled entity pairs for each domain and sampling method.\"}"}
{"id": "acl-2023-long-688", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to remember. They also used more contrastives, e.g., starting with 'not the' (21.8% vs 2.2%) which involve drawing on information about both books. Thus, we adopted the memory recall setting.\\n\\nAfter the first pilot study, we performed one pilot per domain for relatively small instruction refinements.\\n\\n4 The AltEntities Corpus\\nOur annotations were carried out using a pool of around 60 in-house crowdworkers. They were all native English speakers recruited from U.S., U.K., Canada, and Australia so as to obtain a diverse set of perspectives. Each question was shown to two workers to get multiple inputs per question. Around 2K entity pairs were annotated for each domain resulting in around 42K expressions in total. Table 5 shows the final corpus statistics, and Table 6 shows example expressions for the three domains.\\n\\nWe release the dataset under the CC-BY SA 3.0 License as per the Wikipedia License. The REs for BOOKS were on average a word longer than for other domains. They also contained more named entities per expression. Each domain contains some repeated REs (e.g., the pop song), that are often high-level responses, e.g., a song's genre. The BOOKS domain contains the most unique responses. The number of contrastives, estimated as REs starting with \\\"not the\\\", are from 8% in MUSIC up to 20% in BOOKS.\\n\\nFor MUSIC and RECIPES, we manually checked 200 random REs for references to modalities other than text. Around 10% multi-modal REs were present in the RECIPES domain (mostly color), and 20% in the MUSIC domain (mostly beat, speed, and mood).\\n\\nWe estimated the RE error rate by manually inspecting 40 question samples (around 250 to 300 expressions) per domain. The error rate is between 4.5% to 6.8% for the three domains. 78% of these errors were due to the RE applying to both items, not just the target entity. The remaining errors were mostly due to confusing the two entities. We also note that the MUSIC entities are provided with search links which open in a new page, making back-and-forth possible, although it was discouraged in the guidelines.\\n\\n5 Task and Models\\nIndirect reference resolution can be defined as follows: Given an alternative question with K choices $C = \\\\{c_1, \\\\ldots, c_K\\\\}$, and a RE $r$, models should disambiguate the choice $c^* \\\\in C$ intended by $r$. We assume $r$ does not directly mention $c^*$ by its name or position, but does uniquely refer to $c^*$.\\n\\n5.1 Information Available to Models\\nAt a minimum, all models require the RE $r$ and the names of the choices $C = \\\\{c_1, \\\\ldots, c_K\\\\}$.\\n\\nIn addition, models may use textual descriptions $\\\\{s_1, \\\\ldots, s_K\\\\}$ to aid disambiguation. We define $8$In this paper, we only consider $K=2$.\\\\]\"}"}
{"id": "acl-2023-long-688", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\(c_i\\\\) (1 \\\\leq i \\\\leq K) as: (a) The entity name \\\\(c_i\\\\), or (b) the concatenation of \\\\(c_i\\\\) and the textual description \\\\(s_i\\\\), separated by a delimiter.\\n\\nWe consider the following four experimental setups.\\n\\n**NAME**: The entity name without further description of the entities. We use this setting as a baseline.\\n\\nFor the remaining models, we add the following description to the name (truncated to 512 tokens):\\n\\n- **INFBOX**: The concatenation of all infobox key-value pairs (e.g., 'genre: pop').\\n- **UNSHOWN BACKGROUND**: The INFBOX text, concatenated with all the Wikipedia sections of the entity, excluding the section shown to the annotators as background. Since annotators were shown a search link and not a specific Wikipedia section for the MUSIC domain, we do not remove any Wikipedia section for the MUSIC entities. We note that the UNSHOWN BACKGROUND might have some overlap with the information shown to crowdworkers, but the text is not directly given to them. Hence, it is a fair setup to evaluate models in a practical system where the models might not have all the background information.\\n- **ORACLE**: The same background text that was shown to the annotators (Section 3.3). Note that this only exists for BOOKS and RECIPES, as for MUSIC, annotators were only shown a search link.\\n\\n### 5.2 Models\\n\\nWe evaluated 5 different models. For each, we score match to each entity choices and select \\\\(c^*\\\\) with the highest score value.\\n\\n- **Universal Sentence Encoder**: We calculate the cosine similarity between the universal sentence encoder (USE; Cer et al. 2018) embeddings for the \\\\(r\\\\) and each choice's text \\\\(s'_i\\\\).\\n- **Entailment**: Using a textual entailment classifier, we classify whether a choice's text \\\\(s'_i\\\\) entails the \\\\(r\\\\). We use the confidence of the 'entailment' label as the score. We use a BERT model trained on the MNLI dataset (Williams et al., 2018) as our classifier. For all models based on BERT, we use BERT large uncased.\\n- **BERT**: We turn our task into binary classification: We make one example per choice \\\\((c_i, r)\\\\) with label 1 if \\\\(r\\\\) refers to \\\\(c_i\\\\); otherwise, label 0. We fine-tune BERT with a binary classification layer (with two units) on top of its [CLS] token embeddings. The LM input is the sequence \\\\([CLS]s'_i[SEP]\\\\) for inference, for each choice \\\\(c_i\\\\), we compute the probability of label 1 as its score.\\n- **BERT Joint**: In contrast to the above binary setup, we encode all the \\\\(K\\\\) sequences \\\\([CLS]s'_i[SEP]\\\\) with BERT. We apply a linear layer (with one unit) on top of the [CLS] token embeddings from each sequence. We normalize the scores using softmax. Finally, we minimize a categorical cross entropy loss given the \\\\(K\\\\) scores. During inference, we directly use each choice's score.\\n- **T5**: We turn our task into binary classification, as with the BERT binary model. We fine-tune a T5 XL model (3B parameters) with input sequence \\\"expression: \\\\(r\\\\) entity: \\\\(c_i\\\\) description: \\\\(s_i\\\\)\\\" and output sequence 1 or 0. For the NAME input type, the input sequence omits the \\\"description\\\" part.\\n\\n### 6 Experiments\\n\\nWe split the questions in the AltEntities corpus in each domain into training (70%), development (15%), and test (15%) sets. To avoid information leaking between the sets, we allow each target item to be in only one of the sets. For the USE and entailment models, we do not tune any hyperparameters. For supervised models, we tune the learning rate, batch size, and number of epochs using a grid search on the development data (96 configurations for BERT and 24 configurations for T5). We report the hyper-parameter details in Appendix D.\\n\\n#### 6.1 Reference Resolution Accuracy\\n\\nWe compute the accuracy of each (alternative question, RE) pair, i.e., whether the correct choice is scored highest. As \\\\(K = 2\\\\) in our experiments, a random baseline has accuracy 50%.\\n\\nWe show the test set results in Table 7 for all domains and input types. For each model, we also show the average results of all input types. Among the models, USE performs worst (61.03%), followed by the entailment model (66.91%). BERT Joint (73.56%) is on average 1.61% better than BERT (71.92%), confirming that modeling the choices jointly is effective. T5 has the highest average results (77.43%), as expected given that we experimented with T5 XL with 3B parameters compared to BERT large with 360M. In the ORACLE setting for BOOKS and RECIPES, accuracy is understandably high (up to 95.10% for BOOKS and 92.60% for RECIPES). We note that the development set results (Appendix E) are slightly higher, but exhibit similar patterns.\"}"}
{"id": "acl-2023-long-688", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Indirect reference resolution results for different models on all domains and input types: ORACLE, NAME, INFOBOX, UNSHOWN BACKGROUND. The best result of each column is boldfaced.\\n\\nWhen the difference between the best result and another result is not statistically significant (paired t-test with p-value < 0.05), the other result is made both bold and italic (only 4 cases).\\n\\nTable 8: T5 results for the UNSHOWN BACKGROUND setup, when trained on one domain and tested on another domain.\\n\\nTable 9: T5 results with different sampling methods for each domain with UNSHOWN BACKGROUND input.\\n\\nThese results are an over-estimate of the model capabilities. On the other hand, in the NAME setting, in most cases the results are slightly above 50%, with the best result being 61.97% for the MUSIC domain with the T5 model. Here the LMs rely on their memorized entity knowledge (Petroni et al., 2019), suggesting that BERT and T5 embeddings are not sufficient to resolve arbitrary entity references.\\n\\nWith the INFOBOX input, the T5 model accuracy is 78.30%, 83.33%, and 74.28% for BOOKS, RECIPES, and MUSIC, respectively. It increases to 83.40%, 86.76%, and 82.27%, respectively, with the UNSHOWN BACKGROUND input where we add unstructured text data to the structured infobox data. This shows the text is helpful when resolving REs.\\n\\nIn practical settings, models should work with relevant, but not necessarily the same background knowledge as users because (1) it is not possible to have access to users' actual knowledge, and (2) models always have some limitation in the amount of text they can input. We thus rely on the UNSHOWN BACKGROUND setting as a realistic setting for measuring the capabilities of the different models.\\n\\n6.2 Cross-Domain Experiments\\n\\nReference resolution is a semantic task, and ideally models would learn general task aspects rather than domain details. We test generalization by fine-tuning our models on one domain and testing on another. We used the UNSHOWN BACKGROUND setting for these experiments as the most realistic.\\n\\nTable 8 shows the T5 model results. We do not observe much difference when models are tested out of domain, supporting the hypothesis that our models are indeed generalizable. This observation is rather important since our models could be used without separate training for new choice domains.\\n\\nWe also create a mixed training (and development) set that combines the data of the three domains. The mixed training set gives better results on average, taking advantage of larger training set and cues from all the domains. However, since the dataset in each domain is relatively large, the mixed training does not increase the results substantially.\\n\\n6.3 Results and Entity Similarity\\n\\nSection 3.4.1 explained how we selected entity pairs to have different levels of similarity. We now examine how this affects performance. Table 9 shows the results for the T5 model with the UNSHOWN BACKGROUND input. We compute accuracy per test example subset, where each originated from a specific similarity sampling method.\\n\\nAs expected, when the two entities are randomly selected, disambiguation is easiest since they have little in common. The task becomes harder as entities become more similar, with entities with similar infobox attributes having the lowest performance.\\n\\n6.4 Error Analysis\\n\\nWe analyzed the errors from the T5 model in the UNSHOWN BACKGROUND setting, to understand\"}"}
{"id": "acl-2023-long-688", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Error Type                  | Target Item | Non-Target Item | Annotator Utterance |\\n|----------------------------|-------------|-----------------|---------------------|\\n| No Textual Overlap         | 47% (B)     | 27% (R)         | 42% (M)             |\\n| Poor reasoning             | 25% (B)     | 18% (R)         | 13% (M)             |\\n| Multi-modality             | 0% (B)      | 25% (R)         | 22% (M)             |\\n| Wrong Annotation           | 28% (B)     | 30% (R)         | 23% (M)             |\\n\\nTable 10: Error analysis results. Under each error type, we report the percentage of examples from the BOOKS (B), RECIPES (R), and MUSIC (M) domains. We also show two example for each error type.\\n\\nIn most cases, there is no textual overlap between the RE and the background. This is because either the relevant text is removed (by design) since it is shown to the raters, or the Wikipedia text does not contain the information at all (e.g., music lyrics). Future research could evaluate how to adapt LMs to improve their entity knowledge to reason beyond the input textual evidence. In addition, retrieval augmented LMs could be applied to retrieve relevant information before performing the prediction (Borgeaud et al., 2022; Shi et al., 2023).\\n\\nIn other cases, the model suffers from poor reasoning, e.g., that clam is seafood, or a vegetarian dish does not contain seafood. In addition, the model often misclassifies examples when entity attributes are compared (e.g., the newer one). Multi-modality covers around 25% of the errors in the RECIPES and MUSIC domains, e.g., annotators referenced visual aspects from music videos or recipes (e.g., looks like shells), or an acoustic aspect from a song (e.g., with the piano intro or more upbeat).\\n\\nThe remaining errors are because of wrong annotations, usually with the REs applied to both items. This wrong annotation rate (23% - 30%) is much higher than the error rate in the whole dataset (less than 7% as discussed in Section 4) since the model has learned the task to a good extent.\\n\\nWe also analyzed correctly classified examples (for the MUSIC domain) to understand what types of REs are classified correctly. The results are shown in Appendix F.\\n\\n7 Conclusion\\nWe have revisited RE resolution with a new focus on indirect expressions, introducing AltEntities, a new large dataset for this task \u2013 covering BOOKS, RECIPES, and MUSIC examples. The dataset was collected using a novel cartoon completion approach to encourage conversational and causal expressions while avoiding name or position expressions. The experimental results show that in a realistic setting, LMs adapted for this task achieve 82% - 87% accuracy. While an improvement on existing approaches, this also encourages further research on this important problem. Moreover, we showed that the models' performance does not drop when trained and tested on different domains, suggesting that models can learn the semantic task well and generalize to new domains.\\n\\nIt is notable that in practice, many entities do not have textual descriptions or rich meta-data. Future research could study resolving REs with minimal information, e.g., when we only have access to their names or limited meta-data. Future research could also use multi-modal input for training and inference. Further, to handle more complex REs such as the newer one, or the happy song, one could decompose a RE into simpler expressions and then perform the comparison. Similar data collection methodologies could be applied to collect a dataset with more number of choices and also cases where neither or multiple choices match the RE.\"}"}
{"id": "acl-2023-long-688", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8 Limitations\\n\\nAs with any natural language understanding task, there are practical limitations and related ethical aspects that must be considered before deploying a system. In particular, our corpus and modeling approach assume that the user-provided REs always refer to one of the two options. If this is not the case, or if the RE is particularly contrived, undesirable or unexpected behavior may occur: For any expression, including for instance one made with arbitrary derisive language, the model would attempt to resolve this to one of the alternative entities. One approach system designers may consider could be to pre-classify any user-provided REs to avoid interpreting those that are off topic or phrased in a negative manner.\\n\\nA second consideration is that of corpus representativeness. In our case, as this is a first corpus for this task, we have limited ourselves to English Wikipedia, native English speaking annotators, and particular item sampling strategies for practical reasons. However, if used for training a deployed system, the examples present may bias any model to understand specific types of references but not others. Similarly, the items in our corpus are sufficiently popular to have a relatively long Wikipedia entry, whereas items not present in Wikipedia, or with only minimal information, may exhibit different characteristics.\\n\\n9 Ethics Statement\\n\\nThe data collection protocol was reviewed by an ethics panel to remove potential ethical concerns. A few ethical concerns were mentioned by the panel which were then judged to be handled well. These included ensuring that the entities, texts and REs were free from biased and sensitive language. We address this by filtering using a list of sensitive words (see Section 3.4.1 and Table 12). The panel also recommended a diverse representation of entities and domains. Thus our data comes from diverse domains and the entities are sampled from a large set of Wikipedia articles.\\n\\nStill, we note that the limitations mentioned in Section 8 need to be considered and addressed carefully when using our dataset or models for evaluation or training of a deployed system. In addition, a biased corpus may lead to an evaluation that is unaware of RE language forms used in other cultures and languages, or that refer to other types of items. We expect this consideration to be important in practical settings.\\n\\nReferences\\n\\nSigrid Beck and Shin-Sook Kim. 2006. Intervention effects in alternative questions. The Journal of Comparative Germanic Linguistics, 9(3):165\u2013208.\\n\\nRaffaella Bernardi and Sandro Pezzelle. 2021. Linguistic issues behind visual question answering. Language and Linguistics Compass, 15(6):elnc3\u201312417.\\n\\nMar\u00eda Biezma and Kyle Rawlins. 2012. Responding to alternative and polar questions. Linguistics and Philosophy, 35(5):361\u2013406.\\n\\nSebastian Borgeaud, Arthur Mensch, Jordan Hoffmann, Trevor Cai, Eliza Rutherford, Katie Millican, George Bm Van Den Driessche, Jean-Baptiste Lespiau, Bogdan Damoc, Aidan Clark, et al. 2022. Improving language models by retrieving from trillions of tokens. In International conference on machine learning, pages 2206\u20132240. PMLR.\\n\\nAsli Celikyilmaz, Zhaleh Feizollahi, Dilek Hakkani-Tur, and Ruhi Sarikaya. 2014. Resolving referring expressions in conversational dialogs for natural user interfaces. In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 2094\u20132104, Doha, Qatar. Association for Computational Linguistics.\\n\\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua, Nicole Limtiaco, Rhomni St John, Noah Constant, Mario Guajardo-Cespedes, Steve Yuan, Chris Tar, et al. 2018. Universal sentence encoder. arXiv preprint arXiv:1803.11175.\\n\\nEunsol Choi, He He, Mohit Iyyer, Mark Yatskar, Wentau Yih, Yejin Choi, Percy Liang, and Luke Zettlemoyer. 2018. Quac: Question answering in context. In EMNLP.\\n\\nRobert Dale and Ehud Reiter. 1995. Computational interpretations of the gricean maxims in the generation of referring expressions. Cognitive Science, 19(2):233\u2013263.\\n\\nCathrine Damgaard, Paulina Toborek, Trine Eriksen, and Barbara Plank. 2021. \u201cI'll be there for you\u201d: The one with understanding indirect answers. In Proceedings of the 2nd Workshop on Computational Approaches to Discourse, pages 1\u201311, Punta Cana, Dominican Republic and Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4171\u20134186.\"}"}
{"id": "acl-2023-long-688", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-688", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adam Vogel, Christopher Potts, and Dan Jurafsky. 2013. Implicatures and nested beliefs in approximate decentralized-POMDPs. In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 74\u201380, Sofia, Bulgaria. Association for Computational Linguistics.\\n\\nDeanna Wilkes-Gibbs and Herbert H Clark. 1992. Coordinating beliefs in conversation. Journal of memory and language, 31(2):183\u2013194.\\n\\nAdina Williams, Nikita Nangia, and Samuel Bowman. 2018. A broad-coverage challenge corpus for sentence understanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1112\u20131122.\\n\\nLicheng Yu, Zhe Lin, Xiaohui Shen, Jimei Yang, Xin Lu, Mohit Bansal, and Tamara L. Berg. 2018. Mattnet: Modular attention network for referring expression comprehension. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR).\\n\\nHanwang Zhang, Yulei Niu, and Shih-Fu Chang. 2018. Grounding referring expressions in images by variational context. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4158\u20134166.\"}"}
{"id": "acl-2023-long-688", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Opening Utterances\\n\\nThe first annotation screen (Figure 1) starts with a manually written opening utterance. Table 11 shows all these utterances for the three domains.\\n\\nB Annotation Guidelines\\n\\nIn this section, we provide the domain-specific guidelines that were shown to the annotators prior to the start of their annotation. The guidelines for each domain includes three instruction screens. The second and third instruction screens are then repeated for each alternative question as their first and second annotation screens, respectively (the two screen discussed in Section 4).\\n\\nIn the first instruction screen, a summary of the task based on a cartoon completion setup is shown to the annotators. Figure 4 shows the first instruction screen for the BOOKS domain. We do not show the first instruction screen for the other two domains as they are very similar to the BOOKS domain except that the text is slightly different to reflect the domain, and that the examples are from those domains.\\n\\nThe second instruction screen provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations. Figures 5, and 7, and 9 show the second instruction screens for the BOOKS, RECIPES, and MUSIC domains, respectively.\\n\\nThe third instruction screen shows which item should be referred to, and lists five examples of appropriate REs. The REs cover different aspects of the items to encourage the annotators to cover a variety of the item aspects. It also lists a number of actions that the annotators should or should not do. Figures 6, 8, and 10 show the third instruction screen for the BOOKS, RECIPES, and MUSIC domains, respectively.\\n\\nC Filtering Wikipedia Articles\\n\\nTable 12 shows a number of filters we applied to narrow down the extracted articles.\\n\\nD Hyper-parameters Details and Computing Infrastructure\\n\\nWe tune the hyper-parameters using a grid search based on the accuracy of the indirect reference resolution task on the development set of each domain. For BERT and BERT multiple choice models, we select the base learning rate from $\\\\{1 \\\\times 10^{-4}, 5 \\\\times 10^{-5}, 3 \\\\times 10^{-5}, 1 \\\\times 10^{-5}, 5 \\\\times 10^{-6}, 3 \\\\times 10^{-6}, 1 \\\\times 10^{-6}, 5 \\\\times 10^{-7}\\\\}$, the training batch size from $\\\\{16, 32, 64\\\\}$, and the number of epochs from $\\\\{1, 3, 5, 10\\\\}$.\\n\\nFor T5, we select the base learning rate from $\\\\{5 \\\\times 10^{-7}, 1 \\\\times 10^{-7}, 3 \\\\times 10^{-6}, 5 \\\\times 10^{-6}, 1 \\\\times 10^{-5}, 3 \\\\times 10^{-5}, 5 \\\\times 10^{-5}, 1 \\\\times 10^{-4}\\\\}$ and the training batch size from $\\\\{16, 32, 64\\\\}$. We train the T5 models for 50K steps (batches).\\n\\nTable 13 shows the selected hyper-parameters for each model, domain, and input type. We used Cloud TPU v2 accelerators for both training and inference. In our experiments, each training epoch took on average around 4 minutes for BERT, 6 minutes for BERT Multiple Choice, and 15 to 25 minutes for T5 models.\\n\\nE Development Set Results\\n\\nWe reported the test set results in multiple settings in Section 6. In this section, we report all those results on the development sets.\\n\\nTable 14 shows the development set results of different models for all domains and input types. We note that the general trends are very similar to that of the test sets. On average, the results of different models are slightly higher for the development set compared to the test set (up to 2.35%). This is expected as we have tuned the hyper-parameters on the development sets.\\n\\nF Analyzing Correctly Classified Examples\\n\\nWe analyzed 100 correctly classified examples in the MUSIC domain and assigned one or more categories (e.g., date or genre) to each example. We used the predictions of our T5 model with the UN-SHOWN BACKGROUND input. Table 15 shows the results which cover a wide range of categories.\"}"}
{"id": "acl-2023-long-688", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The first instruction screen shown for the BOOKS domain. It summarizes the task based on a cartoon completion setup.\\n\\n**BOOKS**\\n\\n\\\"Remember that book we saw at the store?\\\"\\n\\n\\\"Hey, about that book I lent you last month...\\\"\\n\\n\\\"Can you get me that book on the first shelf?\\\"\\n\\n\\\"I really liked that book from the reading club...\\\"\\n\\n\\\"That book I got was super interesting!\\\"\\n\\n**MUSIC**\\n\\n\\\"So that song I keep singing...\\\"\\n\\n\\\"One of those cool songs that Bob sang last night...\\\"\\n\\n\\\"You sang that song really well yesterday...\\\"\\n\\n\\\"Could you play that song from your playlist?\\\"\\n\\n\\\"I'll now play my favorite song.\\\"\\n\\n**RECIPES**\\n\\n\\\"Remember that fabulous stuff from Tom's party?\\\"\\n\\n\\\"That recipe on today's Masterchef was too good!\\\"\\n\\n\\\"Going to make that dish from Mary's potluck.\\\"\\n\\n\\\"Our favorite food blogger had a cool episode this week!\\\"\\n\\n\\\"Does mom's cookbook have that recipe?\\\"\\n\\nTable 11: The manual utterances which are used to populate the first cell of the cartoon.\"}"}
{"id": "acl-2023-long-688", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The second instruction screen shown for the BOOKS domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.\"}"}
{"id": "acl-2023-long-688", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: The third instruction screen shown for the BOOKS domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.\"}"}
{"id": "acl-2023-long-688", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: The second instruction screen shown for the RECIPES domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.\"}"}
{"id": "acl-2023-long-688", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: The third instruction screen shown for the RECIPES domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.\"}"}
{"id": "acl-2023-long-688", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: The second instruction screen shown for the MUSIC domain. It provides further information about the task and describes where the annotators should acquire the knowledge to perform the annotations.\"}"}
{"id": "acl-2023-long-688", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: The third instruction screen shown for the MUSIC domain. It shows which item should be referred to, and lists five examples of appropriate REs. It also lists a number of actions that the annotators should or should not do.\"}"}
{"id": "acl-2023-long-688", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Filter Rationale\\n\\nArticles with more than one infobox\\nItems should focus on a single topic. For example, we do not accept a movie that has a recorded song for the MUSIC domain.\\n\\nItems with a selected section length $\\\\leq 250$ characters\\nItems have enough information in the section selected to show as background knowledge to the annotators.\\n\\nBooks or music items that do not have genres in their infobox\\nItems contain important attributes for the domain.\\n\\nRecipes that are not a prepared food or without images (\u00a73.3)\\nItems contain important attributes for the domain.\\n\\nItems in the MUSIC domain with $\\\\leq 14$ sections\\nSong should be popular to enable the annotators to also use their own background knowledge.\\n\\nItems containing words on a denylist\\nAvoid sensitive or inappropriate items.\\n\\nTable 12: List of filters applied to select candidate items from those extracted from Wikipedia articles. For each filter, we show the rationale behind it.\\n\\nTable 13: Selected hyper-parameters for the supervised models for each domain and input type. We list selected values for base learning rate (lr), Training batch size (bsz), Num training epochs (epochs).\\n\\nTable 14: Indirect reference resolution development set results for different models on all domains and input types: ORACLE (ORACLE), NAME, INFOBOX (INFOBOX), UNSHOWN BACKGROUND (UNSHOWNBACKGROUND). The best result of each column is boldfaced.\\n\\nTable 15: Categories of correctly classified REs in the MUSIC domain. The results are based on the T5 model with the UNSHOWN BACKGROUND input.\"}"}
{"id": "acl-2023-long-688", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\u25a1 A1. Did you describe the limitations of your work?\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nB \u25a1 Did you use or create scientific artifacts?\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC \u25a1 Did you run computational experiments?\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-688", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values? Section 6 and Appendix D.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run? We did not observe meaningful differences when running the experiments multiple times in the preliminary experiments. We therefore reported the results of only one run.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)? Section 3, 5, and 6. We cited LMs such as BERT and T5.\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.? Section 3 and Appendix A and B.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)? In section 4, we mention: \u201cWe used a pool of around 60 in-house crowdworkers. They were all native English speakers recruited from U.S., U.K., Canada, and Australia.\u201d This work was carried out by participants who are paid contractors. Those contractors received a standard contracted wage, which complies with living wage laws in their country of employment. Due to global privacy concerns, we cannot include more details about our participants, e.g., estimated hourly wage or total amount spent on compensation.\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you\u2019re using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used? This was discussed with the annotators before data collection.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\"}"}
