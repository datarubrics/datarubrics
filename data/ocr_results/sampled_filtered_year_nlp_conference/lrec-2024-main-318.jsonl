{"id": "lrec-2024-main-318", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Common Ground Tracking in Multimodal Dialogue\\n\\nIbrahim Khebour, Kenneth Lai, Mariah Bradford, Yifan Zhu, Richard Brutti, Christopher Tam, Jingxuan Tu, Benjamin Ibarra, Nathaniel Blanchard, and James Pustejovsky\\n\\n1 Colorado State University, Fort Collins, CO, USA\\n2 Brandeis University, Waltham, MA, USA\\n{ibrahim.khebour, nkrishna}@colostate.edu, jamesp@brandeis.edu\\n\\nAbstract\\n\\nWithin Dialogue Modeling research in AI and NLP, considerable attention has been spent on \u201cdialogue state tracking\u201d (DST), which is the ability to update the representations of the speaker\u2019s needs at each turn in the dialogue by taking into account the past dialogue moves and history. Less studied but just as important to dialogue modeling, however, is \u201ccommon ground tracking\u201d (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue: the task-relevant propositions all participants accept as true. In this paper we present a method for automatically identifying the current set of shared beliefs and \u201cquestions under discussion\u201d (QUDs) of a group with a shared goal. We annotate a dataset of multimodal interactions in a shared physical space with speech transcriptions, prosodic features, gestures, actions, and facets of collaboration, and operationalize these features for use in a deep neural model to predict moves toward construction of common ground. Model outputs cascade into a set of formal closure rules derived from situated evidence and belief axioms and update operations. We empirically assess the contribution of each feature type toward successful construction of common ground relative to ground truth, establishing a benchmark in this novel, challenging task.\\n\\nKeywords: common ground, multimodality, belief updating\\n\\n1. Introduction\\n\\nIn the context of increasingly sophisticated interactions involving natural language dialogues with an AI, there is considerable attention being spent on \u201cDialogue State Tracking\u201d (DST), which is the ability to update the representations of the speaker\u2019s (user\u2019s) needs at each turn in the dialogue, by taking into account the past dialogue moves and history. In this paper, we address the related but less-studied problem of Common Grounding Tracking (CGT), which identifies the shared belief space held by all of the participants in a task-oriented dialogue. We describe the procedure for training CGT models to both identify the current set of beliefs, as well as determine the level of evidence for each, to condition where the dialogue will go (the \u201cquestions under discussion\u201d, or QUDs). The goal is to provide a more informative snapshot of the dialogue situation, after each action in the task, to develop a policy incorporating shared beliefs in addition to past dialogue history.\\n\\nA major challenge facing the development of computational models for multimodal interactions involves tracking the intentions, goals, and attitudes of the participants (Cassell et al., 2000; Fosket, 2007; Kopp and Wachsmuth, 2010; Marshall and Hornecker, 2013; Schaffer and Reithinger, 2019; Wahlster, 2006). For task-oriented dialogues, just as important is the problem of identifying and tracking the common ground between participants (Clark and Brennan, 1991; Traum, 1994; Asher, 1998; Dillenbourg and Traum, 2006).\\n\\nIn this work, we specifically: (a) identify both communicative expressions (speech, gesture) and jointly perceived actions in a multi-party dialogue, in order to convert them into propositional content; and (b) add them to a dynamic data structure we call the Common Ground Structure (CGS). This consists of three parts: FBank, a set of facts that are assumed to be known by the group; an EBank, a set of evidences available to the group; and QBank, the \u201cquestions under discussion\u201d, a set of topics remaining to be discussed in order to solve the task.\\n\\nIn total, this work encompasses three novel contributions:\\n\\n\u2022 A challenging new task: multimodal common ground tracking, with a formal model of common ground in a shared, situated task;\\n\u2022 A novel incorporation of the formal model into an automated pipeline that tracks the evolution of group common ground over time;\\n\u2022 An augmentation of the Weights Task Dataset (Khebour et al., 2023) with gesture, action, and common ground annotations, to enable the operationalization of our formal model.\\n\\nOur code may be accessed at https://github.com/csu-signal/Common-Ground-detection\\n\\n2. Related Work\\n\\nThe present work draws on several diverse areas of research, from modeling common ground in HCI...\"}"}
{"id": "lrec-2024-main-318", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When engaged in dialogue, our shared understanding of both utterance meaning (content) and the speaker's meaning in a specific context (intention), involves the ability to link these two in the act of situationally grounding meaning to the local context, what is typically referred to as \\\"establishing the common ground\\\" between speakers (Grice, 1975; Clark and Brennan, 1991; Stalnaker, 2002; Asher, 1998; Traum and Larsson, 2003). The concept of common ground refers to the set of shared beliefs among participants in a Human-Human interaction (HHI) (Markowska et al.; Traum, 1994; Hadley et al., 2022), as well as HCI (Krishnaswamy and Pustejovsky, 2019; Ohmer et al., 2022) and HRI interactions (Kruijff et al., 2010; Fischer, 2011; Scheutz et al., 2011). Del Tr\u00e9delici et al. (2022) have recently employed the notion of common ground operationally to identify and select relevant information for conversational QA system design.\\n\\nStewart et al. (2021) and Bradford et al. (2023) both study human-human collaboration through the lens of an AI agent.\\n\\nDialogue state tracking (DST) aims to estimate the current dialogue state or belief state of the users during the conversation (Budzianowski et al., 2018; Liao et al., 2021; Jacqmin et al., 2022). Current DST models can be categorized into three types: fixed ontology (Henderson et al., 2014; Mrk\u0161i\u0107 et al., 2017; Chen et al., 2020), open vocabulary (Gao et al., 2019; Hosseini-Asl et al., 2022; Wu et al., 2019) and hybrid methods (Goel et al., 2019; Zhang et al., 2020a; Heck et al., 2020).\\n\\nRecently, pretrained language models have been widely used to model slot relations, while Graph Attention networks (GATs) have been used to model the hierarchical structure of DST, enabling the incorporation of semantic compositionality, cross-domain knowledge sharing and coreference (Lin et al., 2021; Li et al., 2021; Cheng et al., 2020).\\n\\nUnderstanding the role of nonverbal behavior in multimodal communication has long been a research interest in HCI, but has recently taken on new interest within CL and the broader AI community. Gestures offer an array of unique dimensions in communication, ranging from noting situational references to indicating specific spatial locations or even conveying manner and orientation (Rohrer et al., 2020; Efthimiou and Kouroupetroglou, 2011; Kong et al., 2015; Kendon, 1997, 2004; Mcneill, 2005). Gesture AMR (GAMR) (Brutti et al., 2022) considers gestures that convey the same propositional content and intentionality as speech acts. Gesture may have meaning on its own, or it may enhance the meaning provided by the verbal modality (Goldin-Meadow, 2003; Krishnaswamy and Pustejovsky, 2020). Also critical to multimodal dialogue is human action, which in addition to communicating deictic and bridging information can also make lasting changes to the world, affecting the common ground (Tam et al., 2023).\\n\\nMuch work has been done to facilitate action identification from video (Sigurdsson et al., 2016) (Gu et al., 2018) (Li et al., 2020) as well as to annotate specific semantic roles (Sadhu et al., 2021). Di Maro et al. (2021) implement dynamic belief sets as graphs, which we do not do explicitly. However, such an approach is theoretically and computationally compatible with ours, as the result (post-condition) of a public announcement or observed action can act as the preconditions for promoting QUDs to evidence, or evidenced propositions to strong beliefs, leading to a natural interpretation of common ground tracking as a graph.\\n\\n3. Dataset\\n\\nThe Weights Task (Khebour et al., 2023) is a collaborative problem-solving task in which groups of three work together to deduce the weights of differently-colored blocks by making comparisons of block weights using a balance scale. In this activity, the group has a balance scale and five blocks of various colors, sizes, and weights. They are told the weight of one block and must identify the weights of the remaining blocks and, eventually, the algebraic relation between them, which is an instance of the Fibonacci Sequence (Sigler, 2002; Bonacci, 1202). Due to the co-situated nature of the task and its inclusion of physical objects and reasoning about their properties, this task involves communication in multiple modalities, such as language, gesture, and action (see Fig. 1), meaning that knowledge is shared using multiple communicative channels. The Weights Task Dataset (WTD) comes with automatic and human transcriptions of the speech, as well as gesture annotated using Gesture-AMR (GAMR) (Brutti et al., 2022), and collaborative problem solving...\"}"}
{"id": "lrec-2024-main-318", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3589\\n\\n(CPS) indicators according to the framework of Sun et al. (2020). All groups successfully deduce the correct block weights, giving a consistent end state against which to assess our models.\\n\\n3.1. Example Dialogue\\n\\nFigure 2: Example dialogue. Participant 3 (right) says \\\"looks like they're fairly equal\\\" after placing the red and blue blocks on different sides of the scale. We refer back to this example elsewhere in the paper.\\n\\nIn the WTD, participants are canonically indexed from 1\u20133, left to right. In Fig., Participant 3 makes a statement that the red and blue blocks are \\\"fairly equal\\\", which is interpreted as an assertion of belief that $\\\\text{red} = \\\\text{blue}$. Participant 1 gives a qualified assent to this through the utterance \\\"yeah, I suppose,\\\" meaning that at this point, $\\\\text{red} = \\\\text{blue}$ and other necessarily entailed propositions can be considered part of the common ground (for example, if we had established that $\\\\text{red} < \\\\text{yellow}$, then $\\\\text{blue} < \\\\text{yellow}$ also becomes part of the common ground).\\n\\n4. Common Ground in Dialogue\\n\\nHere we assume the context of a multi-participant, task-oriented conversation, involving communication by multiple content-generating modalities (language, gesture) and mutually interpretable non-verbal behaviors (e.g., actions) (Kruijff et al., 2010; Pustejovsky and Krishnaswamy, 2021). To this end, we need a data structure representing the common ground in such a context, that can be dynamically updated throughout the dialogue. We adopt a version of a Dialogue Game Board (DGB), as developed in Ginzburg (2012).\\n\\nBecause of the evolving and dynamic nature of co-interactive dialogue and situated actions, following van Benthem et al. (2014) and Pacuit (2017), we adopt an evidence-based model of belief, where our commitments to propositions describing situations or facts are not binary, but are graded, where they can weaken or strengthen depending on available evidence for them as the dialogue progresses.\\n\\nFirst, however, we define the minimal structure of a task-oriented interaction as a sequence, $D$, of dialogue steps, where each move in the dialogue takes it into another situation or state. Let $P = \\\\{p_1, p_2, p_3\\\\}$, be the participants in our dialogue. From any situation $s_k$, we define a $D$ move, $m_i = (p_j, C_j, s_{k+1})$: participant $p_j$ performs a communicative act $C_j$, bringing the multimodal dialogue into situation $s_{k+1}$. The $D$ can be defined as the sequence of these moves: $D = m_1, ..., m_n$.\\n\\nHere our interest is in tracking the situation content resulting from each move: the set of propositions that captures the current state of the world, the current progress towards a goal, or the status of a task. In addition, it captures the current questions under discussion and beliefs in the dialogue.\\n\\nGiven these considerations, we identify three components for tracking common ground in dialogue: a minimal static model of degrees of belief; a data structure distinguishing the elements of the agents' common ground that are being tracked; and a dynamic procedure which updates this structure, when new information and evidence is available to the agents. We consider each of these in turn below.\\n\\n4.1. Evidence-based Belief\\n\\nPacuit (2017) provides a model for evidence-based belief, where agents obtain evidence in favor of a proposition, $\\\\phi$, and can, to eventually believe $\\\\phi$. We adopt a simplified model of the evidence-based Dynamic Epistemic Logic (EB-DEL) as developed in van Benthem et al. (2014) and Pacuit (2017). We define a model as a tuple, $M = (W, E, V)$, where\\n\\n1. $W$ is a non-empty set of worlds;\\n2. $E \\\\subseteq W \\\\times \\\\mathcal{P}(W)$ is an evidence relation;\\n3. $V: \\\\text{At} \\\\rightarrow \\\\mathcal{P}(W)$, is a valuation function.\\n\\nLet $E(w)$ denote the set $\\\\{X|wEX\\\\}$, the worlds accessible to $w$ through the evidencing relation, $E$.\\n\\nThe evidence-based epistemic language, $L$, will be the set of formulas generated by the grammar below:\\n\\n1. $p | \\\\neg \\\\phi | \\\\phi \\\\land \\\\psi | [E] \\\\phi | [B] \\\\phi | [A] \\\\phi$\\n\\nWe distinguish the situation where an agent has \\\"evidence in favor of\\\" a proposition $\\\\phi$, as $[E] \\\\phi$. Because an agent can have evidence for propositions that convey contradictory information, she can consider both $[E] \\\\phi$ and $[E] \\\\neg \\\\phi$. This corresponds to an agent having multiple neighborhoods, $X$, that are each evidenced in their unique way by $w$. However, consider the set of non-contradictory worlds as a unique subset of $X$, one which has what van Benthem and Pacuit (2011) refer to as...\"}"}
{"id": "lrec-2024-main-318", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the finite intersection property (fip). This property allows us to identify a neighborhood of accessible worlds with non-contradictory propositional content. When this occurs, we say an agent has belief in a proposition, $[B\\\\phi]$. Finally, the universal modality is considered \u201cknowledge\u201d of a proposition, $[A\\\\phi]$.\\n\\n4.2. Common Ground Structure\\n\\nCapturing situational state information in a task-oriented dialogue is critical for reflecting current common ground as well as predicting future dialogue moves (Traum and Larsson, 2003; Schlangen and Skantze, 2011; Zhang et al., 2020b; Jacqmin et al., 2022). For our present purpose, we adopt the notion of a Dialogue Game Board (Ginzburg, 1996, 2012), modified to reflect the varying degrees of evidence associated with propositions under discussion. A Common Ground Structure, cgs, is a triple, $(QB, EB, FB)$, consisting of:\\n\\n3. a. Questions Under Discussion (QBANK): set of topics or unknowns that need to be answered to solve the task;\\n   b. Evidence (EBANK): set of propositions for which there is some evidence they are true;\\n   c. Facts (FBANK): set of propositions believed as true by all participants.\\n\\nThe task begins with a set of unknowns referred to as the \u201cQuestions under Discussion\u201d (QUDs). For this implementation, we create a finite model, including a finite model of questions. For all objects in the domain relating to the task, questions are generated for each relation implicated in the task for that object. For example, in the Weights Task, the goal is to identify the weights of five distinct blocks, and then the algebraic relation between them, i.e., the Fibonacci sequence. The weight of a block ranges between 10 and 50 grams, in 10 gram intervals. Hence, for each block in $B = \\\\{\\\\text{red}, \\\\text{blue}, \\\\text{yellow}, \\\\text{green}, \\\\text{purple}\\\\}$, we have five possible values, expressed as yes/no questions. Hence, initialization of the QBank results in the following set:\\n\\n4. QBank = \\\\{Eq(r,10)?,...,Eq(r,50)?,...,Eq(p,10)?,...,Eq(p,50)?\\\\}\\n\\nAt the outset of our dialogue, we set both EBank and FBank to nil, since no task-relevant propositions have been established as commonly evidenced or believed. In the next section, we address the task of determining how information is updated in the dialogue, thereby changing the common ground.\\n\\n4.3. Updating the Common Ground\\n\\nGiven the epistemic logic presented above, we introduce the mechanisms that update the information state within a dialogue. Following Plaza (1989) and subsequent developments of Public Announcement Logic (Baltag et al., 2016), we introduce a new operator to the model, the announcement operator, $!$. Public announcements are statements that are made to all agents, and after the announcement, all agents know that the statement has been announced and that it is true. If $[!\\\\phi]$ represents the act of announcing $\\\\phi$, then $[!\\\\phi]\\\\psi$ means \u201cafter $\\\\phi$ is announced, then $\\\\psi$ is believed to be the case.\u201d\\n\\nIn order to distinguish evidence for $\\\\phi$ from belief in $\\\\phi$, we relativize the impact of a statement to the context within which it is uttered. Let us interpret $[!\\\\phi]\\\\psi$ as follows.\\n\\n5. a. Update with Evidence: $[!\\\\phi][E\\\\psi]$; Given the announcement of $\\\\phi$, there is evidence for $\\\\psi$;\\n   b. Update with Belief: $[E\\\\phi] \\\\rightarrow [!\\\\phi][B\\\\psi]$; Belief in $\\\\phi$ is conditionalized on $\\\\phi$\u2019s announcement in the prior context of evidence for $\\\\phi$.\\n\\nSemantically, an update represents the state of affairs after an announcement. This entails transforming the current model by removing all states where the announced formula is false. With evidence distinguished from belief/knowledge, we also update the evidence function, where $[!\\\\phi]$:\\n\\n6. a. Updates the worlds: $W' = W \\\\cap \\\\phi$\\n   b. Updates the Evidence function: $E'(w) = E(w) \\\\cap \\\\phi$\\n   c. $(M,w) |\\\\equiv \\\\phi$ implies $(M|\\\\equiv \\\\phi, w) |\\\\equiv [E\\\\psi]$\\n\\nThis update actually changes the underlying evidence sets themselves. The announcement is taken as a piece of direct evidence. Hence, to capture that the announcement of $\\\\phi$ becomes evidence and not just belief, the evidence sets for each agent get restricted (or updated) to reflect the worlds where $\\\\phi$ is true. Subsequently, the belief function will then naturally adjust based on the new evidence sets.\\n\\nOperationally, after (5a) is run, the model is relativized to evidencing neighborhoods, where $\\\\phi$ is true. This corresponds to moving a proposition from QBank to the EBank. Then, if the same proposition is \u201cannounced\u201d again, as with an ACCEPT move, then (5b) promotes that proposition from the EBank to the FBank.\\n\\nIn Section 6.4, we illustrate how these updates are applied when running over the output of the move classifier, in order to determine the content of the current common ground.\"}"}
{"id": "lrec-2024-main-318", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We augmented the existing WTD annotations with dual annotation of GAMR, and participant actions using VoxML (Pustejovsky and Krishnaswamy, 2016). Finally, we also tracked the group's collective surfacing of evidence and acceptance of task-relevant facts by supplying another layer of \u201ccommon ground annotations\u201d (CGA): Annotation in the dialogue involves identifying categories relating to the cognitive state of participants to actions and knowledge, concerning the task. This includes the following categories: (a) Observation: participant $P_i$ has perceived an action, $a$; (b) Inference: deduction from $\\\\phi$; (c) Statement: announcement of evidence $\\\\phi$; (d) Question: introducing role interrogative relating to $\\\\phi$; (e) Answer: supplying filler to question about $\\\\phi$; (f) Accept: agree with evidence $\\\\phi$; (g) Doubt: disagree with evidence for $\\\\phi$.\\n\\nIn the example from Sec. 3.1, Participant 3\u2019s utterance would be considered a Statement of the proposition red = blue while Participant 1\u2019s utterance would be an Accept of that proposition. Participant 3 subsequently says \u201cthat\u2019s 20, these two [referring to the red and blue blocks] are 10\u201d (Statement of proposition red = 10 $\\\\land$ blue = 10), to which Participant 1 says \u201cwait, let\u2019s see\u201d signaling a Doubt in red = 10 $\\\\land$ blue = 10. Therefore at this stage of the dialogue, red = blue can be considered an agreed-upon fact (element of FBank), but none of the participants have accepted that red = 10 $\\\\land$ blue = 10, so that proposition is still only an element of EBank. This example illustrates the subtleties captured through the annotation.\\n\\nGAMR, action, and common ground annotations were all dually-annotated. GAMR annotations achieved a SMATCH-F1 score of 0.75. Action annotation achieved an F1 score of 0.67 and Cohen\u2019s $\\\\kappa$ of 0.59. CGA achieved F1 of 0.54 and Cohen\u2019s $\\\\kappa$ of 0.50. Each was adjudicated by an expert to produce the gold standard.\\n\\nExperiments: Modeling Common Ground Tracking\\n\\nOur experimental pipeline consists of 3 primary components: a move classifier, which predicts which cognitive state is being expressed in an utterance (Sec. 5); a propositional extractor, which may either consult a dictionary of expressed propositions that is collected from the annotated data with all modalities considered, or may automatically extract the propositional content of an utterance through vector-similarity methods; and a set of closure rules that unify the cognitive state and propositional content and update the status of QBank, EBank, and FBank.\\n\\nOur primary metric for the entire pipeline is S\u00f8rensen-Dice coefficient (DSC) (S\u00f8rensen, 1948; Dice, 1945). DSC indicates how much the set of propositions extracted by the model matches the set of propositions in the ground truth. It also normalizes for the size of the samples being compared, as the cardinality of the different banks may fluctuate widely as the task proceeds. DSC can also be evaluated as a group proceeds through the task, or averaged over a single group. Since the groups have different numbers of utterances, and hence moves, when aggregating across groups to calculate DSC over time, we pad the length of the shorter groups out to the maximum length by copying the final state of the banks, assuming a \u201csteady state\u201d in the common ground once the task is finished.\\n\\n6.1. Preprocessing\\n\\nWe first mapped the annotated data to the \u201coracle\u201d (manually-segmented) utterances in the WTD (Terpstra et al., 2023). If more than one annotation for a given modality was present in the same utterance, we used the one that had the biggest overlap with the oracle utterance.\\n\\nWe encoded the manually transcribed utterances in the WTD into embedding vectors using BERT (base-uncased) (Devlin et al., 2019), and extracted the 768D [CLS] token embedding from the final encoder layer. Following Bradford et al. (2023), we processed the audio into 88D prosodic features using openSMILE (Eyben et al., 2010). The CPS indicators for an utterance were transformed into their corresponding high-level facets according to the Sun et al. (2020) framework, and encoded as 3D one-hot vectors.\\n\\nGAMR representations were featurized as $k$-hot encodings of size 81. The first 4 components describe the gesture type (icon, gesture-unit, deixis and emblem) along with a fifth component that represents and in cases where 2 types are annotated. Following this is one hot encoding of the GAMR ARG0 (gesturer), then a $k$-hot encoding vector of components of the GAMR ARG1 (gesture content, such as object of deixis). Given the vocabulary of items in the data, this comprises 68 dimensions. Finally, GAMR ARG2 (gesture recipient) was represented by a one-hot encoding of size 5 (group/researcher/1 of 3 participants). As more than one participant can have a GAMR annotated for them over the same utterance, we allow for 1 GAMR feature vector per participant, resulting in a total GAMR feature size of 243.\\n\\nAction annotations comprise scale actions, which were vectorized as a one-hot representation of the scale status (left/balanced/right), and participant actions. Participant actions comprise a 2D representation of \u201clift\u201d vs. \u201cput\u201d, a one hot encoding for the block being acted upon, a 2D one hot encoding of \u201cin\u201d and/or \u201con\u201d, and a 2nd one-hot object...\"}"}
{"id": "lrec-2024-main-318", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"representation of block, scale, or table (the destination of the action, where applicable). A participant's action vector is of size 25 (\\\\( \\\\times 3 \\\\) for 3 participants), resulting in a total action feature size of 78, including the scale actions.\\n\\nTo facilitate propositional extraction, we decontextualized each utterance from the dialogue context, inspired by the dense paraphrasing method (Tu et al., 2022, 2023) that rewrites a textual expression to reduce ambiguity and make explicit the underlying semantics. We filtered the utterances containing at least one pronoun from a predefined set, and had annotators identify the blocks denoted by the pronouns, if any, based on the aligned actions and video frames. Utterances were dually annotated (Cohen's \\\\( \\\\kappa = 0.88 \\\\)) and adjudicated by an expert. Utterances were paraphrased by replacing the pronouns with the adjudicated annotation of the block colors (e.g., they [red block and blue block] are probably equal).\\n\\nThe annotated dataset presents a number of challenges related to sparsity, imbalance, and cross-group diversity. The individual feature channels either capture a single communicative modality or cross-cut two or more (viz. prosodic and CPS features). For input to the model, we concatenate the features of each utterance to the previous utterances. We remove utterances with no CGA, unless they fall within the \\\\( \\\\omega \\\\)-utterance context window of an utterance with CGA. Of the 1,822 utterances in the dataset, only 271 have any common ground annotation, and these annotations are heavily biased toward the \\\\textit{STATEMENT} (195, vs. 61 \\\\textit{ACCEPT}s and 15 \\\\textit{DOUBT}s).\\n\\n6.2. Move Classifier\\n\\nThe move classifier is a multimodal LSTM-based model, intended to capture contextual information that conditions the sequence of cognitive states in a dialogue. Each utterance, including a prior context of \\\\( \\\\omega = 3 \\\\) previous utterances, was processed through two linear layers (256 and 512 units) followed by ReLU activation and an LSTM block of 512 units. The final hidden states of the LSTM block for each modality of interest were concatenated and passed through a 512-unit linear layer, \\\\texttt{tanh}, another 512-unit linear layer, and \\\\texttt{SiLU} before the classification layer. Fig. 3 shows this architecture.\\n\\nWe optimized for the detection of \\\\textit{STATEMENT}, \\\\textit{ACCEPT}, and \\\\textit{DOUBT}. To alleviate imbalance during training, we augmented the data with SMOTE (Chawla et al., 2002). We trained using Kaiming initialization with a uniform distribution (He et al., 2015). All layers except the classification layer are trained using a triplet loss with a margin of 1 (Balntas et al., 2016) for 200 epochs and a learning rate of \\\\( 10^{-4} \\\\). Subsequently the entire model was trained using cross-entropy loss and a learning rate of \\\\( 10^{-3} \\\\) for 100 epochs, and for 200 further epochs with a learning rate of \\\\( 10^{-4} \\\\). Hyperparameters were fixed using a search with one group held out as validation and one group as test, after which each group was held out in turn while an instance of the model is trained on the other 9 groups, for evaluation on the held-out test group. We ended up with ten trained instances of the architecture, one for each group.\\n\\n6.3. Propositional Extractor\\n\\nIn additional to the cognitive state expressed by the utterance, we also needed to retrieve the task-relevant propositional content expressed relative to the QUDs. We used two methods for this:\\n\\n1) \\\\texttt{CGA} (Common Ground Annotation): We automatically mapped the statement IDs to the propositions expressed as captured in the common ground annotation. Upon move prediction, we consulted this mapping to retrieve the propositional content to be associated with the move in the common ground update. Because annotators had access to the video channel and all other modalities when annotating the propositions expressed, this method is a multimodally-informed method of propositional extraction.\\n\\n2) \\\\texttt{DP} (Dense Paraphrase): We encoded the dense paraphrase of the input utterance through BERT (base-uncased). Stop words were filtered out before encoding. The stop words came from a standard list, augmented...\"}"}
{"id": "lrec-2024-main-318", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with words that occurred in the transcriptions in 5 or fewer bigrams and are not number words, color words, or words describing equality or inequality. BERT vectors were computed by summing over the last 4 encoder layers and taking the average of the [CLS] token vector and all individual token vectors in the utterance. Upon move prediction, we chose the proposition whose similarly-encoded BERT vector had the highest cosine similarity to the utterance embedding. Only text and a language model were used in this method, making it unimodal. However, it is important to note that the annotators of the dense paraphrased utterance still had access to the video channel when determining which objects were the denotata of demonstrative pronouns, meaning that some distant signal from the multimodal data reflected here.\\n\\nThese two methods provide an additional axis of comparison when computing the common ground and allow us to measure the performance gain provided a targeted annotation that directly takes into account all modalities vs. a method using only text and a language model.\\n\\n6.4. Closure Rules\\n\\nIn order to determine the contents of the CG banks after each utterance, we developed a set of closure rules consistent with the epistemic model presented in Section 4. These rules describe how utterances (specifically, STATEMENTS and ACCEPTS) affect what is known about the weights of the blocks, and whether certain possibilities are more or less likely than others. When the task begins, the set of possibilities for each block is initialized to \\\\{10, 20, 30, 40, 50\\\\}, with no evidence for or against any of those possibilities (i.e., the evidence_for and evidence_against sets are empty).\\n\\nGiven a STATEMENT or ACCEPT, we first parsed the propositional content of the utterance into one or more atomic propositions, \\\\(p \\\\in \\\\text{At}\\\\), where an atomic proposition consists of a block name, a relation (\\\\(=\\\\), \\\\(<\\\\), \\\\(>\\\\), or \\\\(\\\\neq\\\\)), and a right-hand side. The right-hand side can either be a weight \\\\(\\\\in \\\\{10, 20, 30, 40, 50\\\\}\\\\), a block name, or a set of block names connected by \\\\(+\\\\). Atomic propositions generally update knowledge about the block on the left-hand side of the relation; the exception is if there is a single block on the right-hand side, and less is known about the right-hand-side block (as measured by the relative sizes of the possibility and evidence sets), in which case that block's possibilities are updated instead.\\n\\nThen for each atomic proposition, we updated the knowledge associated with the relevant block, according to the move type: STATEMENTS add evidence for or against certain weights. Statements of propositions \u201cblock = weight\u201d directly add that weight to the evidence_for set; e.g., \\\\([!Eql(b,10)]\\\\) \\\\[Eql(b,10)\\\\]. For other statements, we compute the set of weights inconsistent with that statement, and add them to the evidence_against set. ACCEPTs remove weights (specifically, those inconsistent with the proposition) from the set of possibilities (and both evidence sets).\\n\\nThen, from the contents of the possibility and evidence sets for each block, we generated the contents of the CG banks:\\n\\nIf there was only one possibility for the weight of a block, \u201cblock = weight\u201d was added to FBank; e.g., \\\\([Eql(b,10)]\\\\) \\\\[B Eql(b,10)\\\\]. Otherwise, for those weights in the block\u2019s evidence_for set, we added \u201cblock = weight\u201d to EBank. Similarly, for those weights in the block\u2019s evidence_against set, we added \u201cblock \\\\(\\\\neq\\\\) weight\u201d to EBank. Inequalities for which evidence existed also were added to EBank.\\n\\nFor the remaining weights (not in either evidence set) in the set of possibilities for the block, we added \u201cblock = weight?\u201d to QBank.\\n\\nConsidering the example in Sec. 3.1, since the group already knows that red = 10 at that point, once red = blue is accepted as a fact, the closure rules also elevate blue = 10 to the same epistemic status as red = 10. All other possibilities for blue block\u2019s weight are also removed from both evidence sets.\\n\\nThe ground truth contents of the CG banks were computed by running the closure rules directly over the annotated data.\\n\\n7. Results\\n\\nAveraged across all groups, the move classification model achieves a weighted F1 of 0.61. Most misclassifications are confusions of STATEMENTS and ACCEPTs, which affect the level of evidence assigned to extracted propositions but not the propositions themselves.\\n\\nTable 1 shows average DSC per group, for each bank, with propositional extraction using the Common Ground Annotation (CGA) method (Sec. 6.3).\\n\\nWe also assess the union of the fact bank and evidence bank, to assess how different modalities contribute to the extraction of propositional content and elevation to either status. We compare the performance using all modalities to using language features only, in the form of BERT vectors. We find that in most cases, our common ground tracker has trouble not with retrieving the right propositions with the multimodal CGA method, but with assigning the right level of evidence. This is seen in the values for the union of FBank and EBank, which remain high across all groups, even when the S\u00f8rensen-Dice coefficients of the individual FBank or EBank are comparatively lower. This also tracks the misclassifications made by\"}"}
{"id": "lrec-2024-main-318", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1:\\nAverage DSC per group over all CG banks, comparing multimodal features and language only features. Propositions are extracted using the CGA method.\\n\\n| Group | QBank | EBank | FBank | F \u222a E |\\n|-------|-------|-------|-------|-------|\\n| All   | 0.777 | 0.250 | 0.425 | 1.000 |\\n| BERT  | 0.911 | 0.713 | 0.000 | 0.922 |\\n| openSMILE | 0.829 | 0.712 | 0.528 | 0.925 |\\n| CPS | 0.817 | 0.812 | 0.045 | 0.832 |\\n| Action | 0.514 | 0.335 | 0.165 | 0.959 |\\n| GAMR | 0.868 | 0.691 | 0.372 | 0.839 |\\n\\nTable 2:\\nAverage DSC per group over FBank \u222a EBank, comparing multimodal features and each individual modality. Propositions are extracted using the CGA method.\\n\\n| Group | QBank | EBank | FBank | F \u222a E |\\n|-------|-------|-------|-------|-------|\\n| All   | 0.767 | 0.344 | 0.000 | 1.000 |\\n| BERT  | 0.911 | 0.713 | 0.528 | 0.922 |\\n| openSMILE | 0.829 | 0.712 | 0.045 | 0.925 |\\n| CPS | 0.817 | 0.812 | 0.165 | 0.832 |\\n| Action | 0.514 | 0.335 | 0.165 | 0.959 |\\n| GAMR | 0.868 | 0.691 | 0.372 | 0.839 |\\n\\nFigure 4:\\nDSC for each bank aggregated across groups, plotted vs. utterance, using all modalities in the move classifier. [L]: propositional extraction performed using the multimodal CGA method. [R]: propositional extraction performed using the language-only Dense Paraphrase (DP) method.\\n\\nIncorporating multiple modalities into the move classifier model usually helps assign propositions to the correct level of common ground and maintain greater overlap in the retrieved QUDs relative to ground truth. However, there is great variability across groups. For example, Group 2 performs better using only language, while in Group 9 non-linguistic features do not change the result. These differences can be attributed to how different groups use different modes of communication to complete the task (see Sec. 8).\\n\\nTable 2 shows average DSC per group over FBank \u222a EBank, comparing each individual modality vs. all modalities. We see here that often, each individual modality performs similarly or identically, but in 4 out of 10 groups, using all multimodal features results in the highest performance. However, in other groups, multimodal features may make no difference, or some other individual feature type is the strongest predictor of performance. Compare Group 1 and Group 9, where all modalities perform identically, with Groups 6 and 7, where they all perform differently but multimodal features perform the highest. This supports the previous observation: different groups may adopt radically different modal combinations to communicate equivalent information.\\n\\nFig. 4 shows the progression of DSC over time, aggregated across all groups, using all modalities in the move classifier, but comparing and contrasting the multimodal Common Ground Annotation (CGA) and language model-based Dense Paraphrase (DP) methods for proposition extraction. Including multimodal information improves the retrieval of the correct propositions independent of the level of evidence or factuality assigned to them\u2014as shown by the consistently high DSC of FBank \u222a EBank in the left plot.\"}"}
{"id": "lrec-2024-main-318", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discussion\\n\\nSome specific examples show how the language model-based DP method struggles to extract propositions from complex utterances. Table 3 shows how vector comparison over only linguistic information tends to struggle with propositions involving multiple objects. Certain groups, like Group 1, tended to speak full propositions aloud, while others, like Group 10, mixed modalities (\\\"ten and ten\\\" is accompanied by gesture and action, which are accounted for directly by Common Ground Annotations but not Dense Paraphrases).\\n\\n| Group | Utterance | Proposition | Correct? |\\n|-------|-----------|-------------|---------|\\n| 1     | red block's ten so then red = 10 | 3 | |\\n| 1     | yeah ok so now we know that blueblock is also ten | 3 | |\\n| 5     | so red block, blue block are both ten red = 20 and green = 40 | 7 | |\\n| 5     | so the green we think is twenty ok | 3 | |\\n| 10    | i guess green block is like twenty and red = 50 and green = 20 | 7 | |\\n\\nTable 3: Utterances and propositions retrieved using DP method.\\n\\nThis reflects Table 2 where, even using the multimodal CGA extraction method, Group 1 achieves perfect overlap of $F_{Bank} \\\\cup E_{Bank}$ with ground truth using just language, while the model has to combine modalities to reach its best performance for Group 10. That Group 1 performance over $F_{Bank} \\\\cup E_{Bank}$ is also perfect using each individual modality alone suggests that their utterances are strongly aligned with their non-verbal behavior. Meanwhile, Group 6 stands out as a particular case where each individual modality is contributing something distinct.\\n\\nMisclassifications of $\\\\text{STATEMENTS}$ as $\\\\text{ACCEPTS}$, or vice versa, may elevate the utterances of certain participants to fact status, or leave elements in the Questions Under Discussion when they have already been resolved. This could also lead to a certain participant having more apparent influence over the dialogue. One participant's beliefs may update the common ground of the group, and leave other participants' beliefs unconsidered. Table 4 shows some examples from Group 10, and demonstrates how affirmative language like \\\"yeah\\\" may be indicative of $\\\\text{ACCEPT}$s elsewhere in the training data, while \\\"okay\\\" or restatements of propositional content are typically indicative of $\\\\text{STATEMENTS}$ even when in context they indicate acceptance of a previously-stated proposition.\\n\\n| Timestamp | Utterance | Label | Prediction |\\n|-----------|-----------|-------|------------|\\n| 117.46-118.87 | yeah they're together. | $\\\\text{STATEMENT}$ | ACCEPT |\\n| 17.89-219.78 | thirty one thirty two so thirty | ACCEPT | $\\\\text{STATEMENT}$ |\\n| 18.23-219.00 | so okay | ACCEPT | $\\\\text{STATEMENT}$ |\\n\\nTable 4: Sample of utterances from Group 10 misclassified by move classifier.\\n\\nConclusion and Future Work\\n\\nIn this paper we have presented a challenging novel task: multimodal common ground tracking, and a novel benchmark over the challenging Weights Task Dataset. We presented a formal model of common ground over a shared task and augmented the WTD with additional gesture, action, and common ground annotations. We performed a set of experiments to evaluate the contributions of different modalities toward modeling the cognitive states of the group, extracting the propositions expressed, and building common ground structures as the group proceeds through the task.\\n\\nOur model will be particularly useful for AI systems deployed in environments such as classrooms, where they can track the collective knowledge of a group and facilitate productive collaborations. Certain modalities may be more prone to misclassifications based on the speaker. For instance, future work could examine how prosodic features could be used to detect power dynamics that may bias the construction of common ground toward certain people or assertions. Giving the common ground model additional separate banks for each speaker would allow an agent to facilitate knowledge sharing and collaboration if it seems like a subgroup has arrived at a belief not shared by the whole group. In a task-based environment, the agent could use the model of common ground to make task-relevant inferences itself, such as the algebraic relationship between the block weights here, allowing it to learn from watching and interacting with the group. Finally, because there is a one-to-many mapping between propositions and potential ways to phrase or express them in utterances, the dense paraphrase method for propositional extraction could benefit from a cross-encoder approach, as used in coreference research.\\n\\nLimitations\\n\\nAlthough our work addresses a novel and challenging problem, scaling the pipeline to other use cases confronts some (surmountable) limitations. For a given task, the relevant propositions that may populate the common ground need to be determined and enumerated. The number of propositions scales naturally to increased cardinality of items, attributes, and relations within a similar domain (e.g., by computing the Cartesian product of items, attributes, and binary relations, and subsequently the powerset of atomic propositions to account for conjunctions like $\\\\text{red} = 10 \\\\land \\\\text{blue} = 10$).\\n\\nTherefore the complexity of proposition construction is subject to the complexity of the task and number of task items. Enumerating the closure rules is straightforward once the propositions are determined and enumerated.\"}"}
{"id": "lrec-2024-main-318", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"determined. The move classifier itself should re-\\nquire no changes unless there is a change in input\\nmodalities. Imbalance within the data categories\\npresents a further challenge that needs to be ad-\\nressed. In this paper we used data augmenta-\\ntion approaches like SMOTE, but precise handling\\nwould need to be determined on a task-specific ba-\\nsis.\\n\\nAcknowledgements\\nThis work was partially supported by the National\\nScience Foundation under award DRL 2019805 to\\nColorado State University and Brandeis University ,\\nand awards IIS 2326985 and IIS 2033932 to Bran-\\nden . The views expressed are those of\\nthe authors and do not reflect the official policy or\\nposition of the U.S. Government. All errors and\\nmistakes are, of course, the responsibilities of the\\nauthors. Special thanks to Jade Collins, August\\nGaribay , and Carlos Mabrey for extensive data an-\\nnotation.\\n\\nBibliographical References\\nNicholas Asher . 1998. Common ground, correc-\\ntions and coordination. *Journal of Semantics*.\\n\\nVassileios Balntas, Edgar Riba, Daniel Ponsa, and\\nKrystian Mikolajczyk. 2016. Learning local fea-\\nture descriptors with triplets and shallow convo-\\nlutional neural networks. In *Bmvc*, volume 1,\\npage 3.\\n\\nAlexandru Baltag, Lawrence S Moss, and S\u0142a-\\nwomir Solecki. 2016. *The logic of public an-\\nnouncements, common knowledge, and private\\nsuspicions*. Springer .\\n\\nLeonardo Bonacci. 1202. *Liber Abaci*. Unpub-\\nlished lost manuscript.\\n\\nMariah Bradford, Ibrahim Khebour , Nathaniel Blan-\\rchard, and Nikhil Krishnaswamy . 2023. Auto-\\nmatic detection of collaborative states in small\\ngroups using multimodal features. In *Proceed-\\nings of the 24th International Conference on Ar-\\ntificial Intelligence in Education*.\\n\\nHennie Brugman and Albert Russel. 2004. An-\\nnotating multi-media/multi-modal resources with\\nelan. In *Proceedings of the Fourth International\\nConference on Language Resources and Eval-\\nuation (LREC\u201904)*.\\n\\nRichard Brutti, Lucia Donatelli, Kenneth Lai, and\\nJames Pustejovsky . 2022. *Abstract Meaning\\nRepresentation for gesture*. In *Proceedings of\\nthe Thirteenth Language Resources and Evalu-\\natation Conference*, pages 1576\u20131583, Marseille,\\nFrance. European Language Resources Associ-\\nation.\\n\\nPawe\u0142 Budzianowski, T sung-Hsien Wen, Bo-\\nHsiang T seng, I\u00f1igo Casanueva, Stefan Ultes,\\nOsman Ramadan, and Milica Ga\u0161i\u0107. 2018. *Multi-\\nWOZ - a large-scale multi-domain Wizard-of-Oz\\ndataset for task-oriented dialogue modelling*\\nIn *Proceedings of the 2018 Conference on Empir-\\nic Methods in Natural Language Processing*,\\npages 5016\u20135026, Brussels, Belgium. Associa-\\ntion for Computational Linguistics.\\n\\nJustine Cassell, Joseph Sullivan, Elizabeth\\nChurchill, and Scott Prevost. 2000. *Embodied\\nconversational agents*. MIT press.\\n\\nNitesh V Chawla, Kevin W Bowyer , Lawrence O\\nHall, and W Philip Kegelmeyer . 2002. Smote:\\nsynthetic minority over-sampling technique.\\n*Journal of artificial intelligence research*, 16:321\u2013357.\\n\\nLu Chen, Boer Lv , Chi Wang, Su Zhu, Bowen T an,\\nand Kai Yu. 2020. *Schema-guided multi-domain\\ndialogue state tracking with graph attention neu-\\rnal networks*. In *AAAI Conference on Artificial\\nIntelligence*.\\n\\nJianpeng Cheng, Devang Agrawal, H\u00e9ctor\\nMart\u00ednez Alonso, Shruti Bhargava, Joris\\nDriesen, Federico Flego, Dain Kaplan, Dim-\\nitri Kartsaklis, Lin Li, Dhivya Piraviperu-\\nmal, Jason D. Williams, Hong Yu, Diarmuid\\n\u00d3 S\u00e9aghdha, and Anders Johannsen. 2020. *Conversational semantic parsing for dialog\\nstate tracking*. In *Proceedings of the 2020\\nConference on Empirical Methods in Natural\\nLanguage Processing (EMNLP)*, pages 8107\u2013\\n8117, Online. Association for Computational\\nLinguistics.\\n\\nHerbert H. Clark and Susan E. Brennan. 1991. *Grounding in communication*. In Lauren\\nResnick, Levine B., M. John, Stephanie T easley ,\\nand D, editors, *Perspectives on Socially Shared\\nCognition*, pages 13\u20131991. American Psycho-\\nlogical Association.\\n\\nMarco Del T redici, Xiaoyu Shen, Gianni Barlacchi,\\nBill Byrne, and Adri\u00e0 de Gispert. 2022. *From\\nrewriting to remembering: Common ground\\nfor conversational qa models*. *arXiv preprint\\narXiv:2204.03930*.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\\nKristina T outanova. 2019. *BERT : Pre-training of\"}"}
{"id": "lrec-2024-main-318", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"deep bidirectional transformers for language un-\\nderstanding. In Proceedings of the 2019 Confer-\\nence of the North American Chapter of the As-\\nsociation for Computational Linguistics: Human \\nLanguage T echnologies, Volume 1 (Long and \\nShort Papers), pages 4171\u20134186, Minneapolis, \\nMinnesota. Association for Computational Lin-\\nguistics.\\n\\nMaria Di Maro, Antonio Origlia, and Francesco \\nCutugno. 2021. Cutting melted butter? com-\\nmon ground inconsistencies management in di-\\nalogue systems using graph databases. \\nIJCoL. Italian Journal of Computational Linguistics \\n7(7-1, 2):157\u2013190.\\n\\nLee R Dice. 1945. Measures of the amount of \\necologic association between species. \\nEcology, 26(3):297\u2013302.\\n\\nPierre Dillenbourg and David Traum. 2006. Shar-\\ning solutions: Persistence and grounding in mul-\\ntimodal collaborative problem solving. \\nThe Journal of the Learning Sciences, 15(1):121\u2013151.\\n\\nEleni Efthimiou and Georgios Kouroupetroglou. \\n2011. Gestures in Embodied Communication \\nand Human\u2013Computer Interaction.\\n\\nFlorian Eyben, Martin W\u00f6llmer, and Bj\u00f6rn Schuller. \\n2010. Opensmile: the munich versatile and fast \\nopen-source audio feature extractor. In \\nProceedings of the 18th ACM international conference \\non Multimedia, pages 1459\u20131462. \\n\\nKerstin Fischer. 2011. How people talk with robots: \\nDesigning dialog to reduce user uncertainty. \\nAI Magazine, 32(4):31\u201338.\\n\\nMary Ellen Foster. 2007. Enhancing human-\\ncomputer interaction with embodied conversa-\\ntional agents. In International Conference on \\nUniversal Access in Human-Computer Interac-\\ntion, pages 828\u2013837. Springer.\\n\\nShuyang Gao, Abhishek Sethi, Sanchit Agarwal, \\nTagyoung Chung, and Dilek Hakkani- T\u00fcr. 2019. \\nDialog state tracking: A neural reading compre-\\nhension approach. In Proceedings of the 20th \\nAnnual SIGdial Meeting on Discourse and Dia-\\nlogue, pages 264\u2013273, Stockholm, Sweden. As-\\nsociation for Computational Linguistics.\\n\\nJonathan Ginzburg. 1996. Interrogatives: Ques-\\ntions, facts and dialogue. The handbook of con-\\ntemporary semantic theory. Blackwell, Oxford, \\npages 359\u2013423.\\n\\nJonathan Ginzburg. 2012. The interactive stance: \\nMeaning for conversation. Oxford University \\nPress.\\n\\nRahul Goel, Shachi Paul, and Dilek Hakkani- T\u00fcr. \\n2019. HyST: A Hybrid Approach for Flexible and \\nAccurate Dialogue State Tracking. In Proc. Inter-\\nspeech 2019, pages 1458\u20131462.\\n\\nSusan Goldin-Meadow. 2003. Hearing Gesture: \\nHow Our Hands Help Us Think, volume 14.\\n\\nHerbert P Grice. 1975. Logic and conversation. In \\nSpeech acts, pages 41\u201358. Brill.\\n\\nChunhui Gu, Chen Sun, David A. Ross, Carl \\nVondrick, Caroline Pantofaru, Yeqing Li, Sud-\\nheendra Vijayanarasimhan, George Toderici, \\nSusanna Ricco, Rahul Sukthankar, Cordelia \\nSchmid, and Jitendra Malik. 2018. \\nAva: A video dataset of spatio-temporally localized atomic vi-\\nsual actions.\\n\\nLauren V. Hadley, Graham Naylor, and Antonia F. \\nde C. Hamilton. 2022. A review of theories and \\nmethods in the science of face-to-face social in-\\nteraction. Nature Reviews Psychology, 1(1):42\u2013 \\n54. Number: 1 Publisher: Nature Publishing \\nGroup.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and \\nJian Sun. 2015. Delving deep into rectifiers: Sur-\\npassing human-level performance on imagenet \\nclassification. In Proceedings of the IEEE inter-\\nnational conference on computer vision, pages \\n1026\u20131034.\\n\\nMichael Heck, Carel van Niekerk, Nurul Lubis, \\nChristian Geishauser, Hsien-Chin Lin, Marco \\nMoresi, and Milica Gasic. 2020. \\nTripPy: A triple copy strategy for value independent neural dia-\\nlogue state tracking. In Proceedings of the 21th \\nAnnual Meeting of the Special Interest Group on \\nDiscourse and Dialogue, pages 35\u201344, 1st vir-\\ntual meeting. Association for Computational Lin-\\nguistics.\\n\\nMatthew Henderson, Blaise Thomson, and Steve \\nYoung. 2014. Word-based dialog state tracking \\nwith recurrent neural networks. In Proceedings \\nof the 15th Annual Meeting of the Special In-\\nterest Group on Discourse and Dialogue (SIG-\\ndial), pages 292\u2013299, Philadelphia, PA, U.S.A. \\nAssociation for Computational Linguistics.\\n\\nEhsan Hosseini-Asl, Bryan McCann, Chien-Sheng \\nWu, Semih Yavuz, and Richard Socher. 2022. \\nA simple language model for task-oriented dia-\\nlogue.\\n\\nL\u00e9o Jacqmin, Lina M Rojas-Barahona, and Benoit \\nFavre. 2022. \u201cdo you follow me?\u201d: A survey \\nof recent approaches in dialogue state tracking. \\narXiv preprint arXiv:2207.14627.\\n\\nAdam Kendon. 1997. Gesture. Annual Review of \\nAnthropology, 26(1):109\u2013128.\"}"}
{"id": "lrec-2024-main-318", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adam Kendon. 2004. Gesture: Visible action as utterance.\\n\\nIbrahim Khebour, Richard Brutti, Indrani Dey, Rachel Dickler, Kelsey Sikes, Kenneth Lai, Mariah Bradford, Brittany Cates, Paige Hansen, Changsoo Jung, Brett Wisniewski, Corbyn Terpstra, Leanne Hirshfield, Sadhana Puntambekar, Nathaniel Blanchard, James Pustejovsky, and Nikhil Krishnaswamy. 2023. The Weights Task Dataset: A Multimodal Dataset of Collaboration in a Situated Task.\\n\\nAnthony Pak-Hin Kong, Sam Po Law, Connie Kwan, Christy Lai, and Vivian Lam. 2015. A coding system with independent annotations of gesture forms and functions during verbal communication: Development of a database of speech and gesture (dosage). Journal of Nonverbal Behavior, 39.\\n\\nStefan Kopp and Ipke Wachsmuth. 2010. Gesture in embodied communication and human-computer interaction, volume 5934. Springer.\\n\\nNikhil Krishnaswamy and James Pustejovsky. 2019. Generating a novel dataset of multimodal referring expressions. In Proceedings of the 13th International Conference on Computational Semantics-Short Papers, pages 44\u201351.\\n\\nNikhil Krishnaswamy and James Pustejovsky. 2020. A formal analysis of multimodal referring strategies under common ground. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 5919\u20135927.\\n\\nGeert-Jan M Kruijff, Pierre Lison, Trevor Benjamin, Henrik Jacobsson, Hendrik Zender, Ivana Kruijff-Korbayov\u00e1, and Nick Hawes. 2010. Situated dialogue processing for human-robot interaction. In Cognitive systems, pages 311\u2013364. Springer.\\n\\nAng Li, Meghana Thotakuri, David A. Ross, Jo\u00e3o Carreira, Alexander Vostrikov, and Andrew Zisserman. 2020. The ava-kinetics localized human actions video dataset.\\n\\nXinmeng Li, Qian Li, Wansen Wu, and Quanjun Yin. 2021. Generation and extraction combined dialogue state tracking with hierarchical ontology integration. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2241\u20132249, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nLizi Liao, Le Hong Long, Yunshan Ma, Wenqiang Lei, and Tat-Seng Chua. 2021. Dialogue state tracking with incremental reasoning. Transactions of the Association for Computational Linguistics, 9:557\u2013569.\\n\\nWeizhe Lin, Bo-Hsiang Tseng, and Bill Byrne. 2021. Knowledge-aware graph-enhanced GPT-2 for dialogue state tracking. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7871\u20137881, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMagdalena Markowska, Adil Soubki, Gary Mar, Seyed Abolghasem Mirroshandel, Owen Rambow, and Anita Wasilewska. Formal representation of common ground in dialogue.\\n\\nPaul Marshall and Eva Hornecker. 2013. Theories of embodiment in hci. The SAGE handbook of digital technology research, 1:144\u2013158.\\n\\nDavid Mcneill. 2005. Gesture and Thought.\\n\\nNikola Mrk\u0161i\u0107, Diarmuid \u00d3 S\u00e9aghdha, T sung-Hsien Wen, Blaise Thomson, and Steve Young. 2017. Neural belief tracker: Data-driven dialogue state tracking. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1777\u20131788, Vancouver, Canada. Association for Computational Linguistics.\\n\\nXenia Ohmer, Marko Duda, and Elia Bruni. 2022. Emergence of hierarchical reference systems in multi-agent communication. In Proceedings of the 29th International Conference on Computational Linguistics, pages 5689\u20135706.\\n\\nEric Pacuit. 2017. Neighborhood semantics for modal logic. Springer.\\n\\nJan Plaza. 1989. Logics of public communications. In Proceedings 4th International Symposium on Methodologies for Intelligent Systems, pages 201\u2013216.\\n\\nJames Pustejovsky and Nikhil Krishnaswamy. 2016. Voxml: A visualization modeling language. Proceedings of LREC.\\n\\nJames Pustejovsky and Nikhil Krishnaswamy. 2021. Embodied human computer interaction. KI-K\u00fcnstliche Intelligenz, 35(3-4):307\u2013327.\\n\\nPatrick Rohrer, Ingrid Vil\u00e0-Gim\u00e9nez, J\u00falia Florit-Pons, N\u00faria Esteve-Gibert, Ada Ren, Stefanie Shattuck-Hufnagel, and Pilar Prieto. 2020. The multimodal multidimensional (m3d) labelling scheme for the annotation of audiovisual corpora.\\n\\nArka Sadhu, Tanmay Gupta, Mark Yatskar, Ram Nevatia, and Aniruddha Kembhavi. 2021. Visual semantic role labeling for video understanding.\"}"}
{"id": "lrec-2024-main-318", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stefan Schaffer and Norbert Reithinger. 2019. Conversation is multimodal: thus conversational user interfaces should be as well. In Proceedings of the 1st International Conference on Conversational User Interfaces, pages 1\u20133.\\n\\nMatthias Scheutz, Rehj Cantrell, and Paul Schermerhorn. 2011. Toward humanlike task-based dialogue processing for human robot interaction. Ai Magazine, 32(4):77\u201384.\\n\\nDavid Schlangen and Gabriel Skantze. 2011. A general, abstract model of incremental dialogue processing. Dialogue & Discourse, 2(1):83\u2013111.\\n\\nLE Sigler. 2002. Fibonacci's Liber Abaci. Leonardo Pisano's Book of Calculation. New York: Springer.\\n\\nGunnar A. Sigurdsson, G\u00fcl Varol, Xiaolong Wang, Ali Farhadi, Ivan Laptev, and Abhinav Gupta. 2016. Hollywood in homes: Crowdsourcing data collection for activity understanding.\\n\\nThorvald S\u00f8rensen. 1948. A method of establishing groups of equal amplitude in plant sociology based on similarity of species content and its application to analyses of the vegetation on danish commons. Biologiske skrifter, 5:1\u201334.\\n\\nRobert Stalnaker. 2002. Common ground. Linguistics and philosophy, 25(5-6):701\u2013721.\\n\\nAngela E. B. Stewart, Zachary Keirn, and Sidney K. D'Mello. 2021. Multimodal modeling of collaborative problem-solving facets in triads. User Modeling and User-Adapted Interaction, 31(4):713\u2013751.\\n\\nChen Sun, Valerie J Shute, Angela Stewart, Jade Yonehiro, Nicholas Duran, and Sidney D'Mello. 2020. Towards a generalized competency model of collaborative problem solving. Computers & Education, 143:103672.\\n\\nChristopher Tam, Richard Brutti, Kenneth Lai, and James Pustejovsky. 2023. Annotating situated actions in dialogue. In Proceedings of the 4th International Workshop on Designing Meaning Representation.\\n\\nCorbyn Terpstra, Ibrahim Khebour, Mariah Bradford, Brett Wisniewski, Nikhil Krishnaswamy, and Nathaniel Blanchard. 2023. How good is automatic segmentation as a multimodal discourse annotation aid? In Workshop on Interoperable Semantic Annotation (ISA-19), page 75.\\n\\nDavid Traum. 1994. A computational theory of grounding in natural language conversation. PhD thesis, University of Rochester.\\n\\nDavid R Traum and Staffan Larsson. 2003. The information state approach to dialogue management. Current and new directions in discourse and dialogue, pages 325\u2013353.\\n\\nJingxuan Tu, Kyeongmin Rim, Eben Holderness, Bingyang Ye, and James Pustejovsky. 2023. Dense paraphrasing for textual enrichment. In Proceedings of the 15th International Conference on Computational Semantics (IWCS), Nancy, France. Association for Computational Linguistics.\\n\\nJingxuan Tu, Kyeongmin Rim, and James Pustejovsky. 2022. Competence-based question generation. In Proceedings of the 29th International Conference on Computational Linguistics, pages 1521\u20131533, Gyeongju, Republic of Korea. International Committee on Computational Linguistics.\\n\\nJohan van Benthem, David Fern\u00e1ndez-Duque, and Eric Pacuit. 2014. Evidence and plausibility in neighborhood structures. Annals of Pure and Applied Logic, 165(1):106\u2013133.\\n\\nJohan van Benthem and Eric Pacuit. 2011. Dynamic logics of evidence-based beliefs. Studia Logica, 99:61\u201392.\\n\\nWolfgang Wahlster. 2006. Dialogue systems go multimodal: The smartkom experience. In SmartKom: foundations of multimodal dialogue systems, pages 3\u201327. Springer.\\n\\nChien-Sheng Wu, Andrea Madotto, Ehsan Hosseini-Asl, Caiming Xiong, Richard Socher, and Pascale Fung. 2019. Transferable multi-domain state generator for task-oriented dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 808\u2013819, Florence, Italy. Association for Computational Linguistics.\\n\\nJianguo Zhang, Kazuma Hashimoto, Chien-Sheng Wu, Yao Wang, Philip Yu, Richard Socher, and Caiming Xiong. 2020a. Find or classify? dual strategy for slot-value predictions on multi-domain dialog state tracking. In Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics, pages 154\u2013167, Barcelona, Spain (Online). Association for Computational Linguistics.\\n\\nZheng Zhang, Ryuichi Takanobu, Qi Zhu, Min-Lie Huang, and Xiao Yan Zhu. 2020b. Recent advances and challenges in task-oriented dialogue systems. Science China Technologies Sciences, 63(10):2011\u20132027.\"}"}
{"id": "lrec-2024-main-318", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Group-wise Move Classifier\\n\\nTable 5 shows the performance of the 10 classifiers with each being trained on 9 different groups, and evaluated on the remaining 10th.\\n\\nB. Annotation Procedures and IAA\\n\\nELAN (Brugman and Russel, 2004) was the tool used for most annotation, supplemented by collating annotations in spreadsheets. As we can see in Fig. 5, this tool allows annotators to visualize the data at any point in the videos, and also see all other annotated modalities. The data is then featureized and used as input for the move classifier.\\n\\nTables 6\u20138 show inter-annotator agreement (IAA) metrics for the Common Ground, action, and GAMR annotation per group. Because gestures are individualized, GAMR annotations are also broken down by participant. Means are also provided.\"}"}
{"id": "lrec-2024-main-318", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Group | F1 score | Cohen's $\\\\kappa$ mean |\\n|-------|----------|------------------------|\\n| 1     | 0.520    | 0.359                  |\\n| 2     | 0.454    | 0.295                  |\\n| 3     | 0.492    | 0.356                  |\\n| 4     | 0.411    | 0.267                  |\\n| 5     | 0.471    | 0.503                  |\\n| 6     | 0.639    | 0.603                  |\\n| 7     | 0.678    | 0.572                  |\\n| 8     | 0.522    | 0.712                  |\\n| 9     | 0.575    | 0.564                  |\\n| 10    | 0.645    | 0.772                  |\\n\\n| IAA on Common Ground Annotations. |\\n|-----------------------------------|\\n| Group F1 score | Cohen's $\\\\kappa$ mean |\\n|----------------|------------------------|\\n| 1              | 0.557                  | 0.464                  |\\n| 2              | 0.651                  | 0.666                  |\\n| 3              | 0.750                  | 0.688                  |\\n| 4              | 0.719                  | 0.654                  |\\n| 5              | 0.804                  | 0.689                  |\\n| 6              | 0.737                  | 0.798                  |\\n| 7              | 0.761                  | 0.660                  |\\n| 8              | 0.583                  | 0.466                  |\\n| 9              | 0.519                  | 0.432                  |\\n| 10             | 0.629                  | 0.458                  |\\n| mean           | 0.671                  | 0.597                  |\\n\\n| IAA on action annotations. |\"}"}
{"id": "lrec-2024-main-318", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|   | F1 | Precision | Recall | Group Participant |\\n|---|----|-----------|--------|------------------|\\n|   |    | 0.921     | 0.953  | 1                |\\n|   |    | 0.943     | 0.917  | 2                |\\n|   |    | 0.899     | 0.912  | 3                |\\n| \u00b5 |    | 0.921     | 0.927  | 0.915            |\\n|   |    | 0.947     | 0.938  | 2                |\\n|   |    | 0.895     | 0.850  | 3                |\\n| \u00b5 |    | 0.896     | 0.862  | 0.934            |\\n|   |    | 0.686     | 0.720  | 0.656            |\\n|   |    | 0.809     | 0.796  | 3                |\\n|   |    | 0.817     | 0.779  | 3                |\\n| \u00b5 |    | 0.763     | 0.763  | 0.763            |\\n|   |    | 0.791     | 0.837  | 4                |\\n|   |    | 0.658     | 0.807  | 4                |\\n|   |    | 0.817     | 0.779  | 4                |\\n| \u00b5 |    | 0.755     | 0.808  | 0.722            |\\n|   |    | 0.824     | 0.836  | 5                |\\n|   |    | 0.693     | 0.642  | 5                |\\n|   |    | 0.835     | 0.853  | 5                |\\n| \u00b5 |    | 0.784     | 0.777  | 0.795            |\\n|   |    | 0.697     | 0.704  | 6                |\\n|   |    | 0.628     | 0.667  | 6                |\\n|   |    | 0.480     | 0.462  | 6                |\\n| \u00b5 |    | 0.602     | 0.611  | 0.595            |\\n|   |    | 0.865     | 0.874  | 7                |\\n|   |    | 0.736     | 0.724  | 7                |\\n|   |    | 0.667     | 0.662  | 7                |\\n| \u00b5 |    | 0.756     | 0.753  | 0.759            |\\n|   |    | 0.782     | 0.725  | 8                |\\n|   |    | 0.745     | 0.710  | 8                |\\n|   |    | 0.907     | 0.925  | 8                |\\n| \u00b5 |    | 0.812     | 0.787  | 0.841            |\\n|   |    | 0.846     | 0.846  | 9                |\\n|   |    | 0.600     | 0.525  | 9                |\\n|   |    | 0.800     | 0.818  | 9                |\\n| \u00b5 |    | 0.749     | 0.730  | 0.776            |\\n| 10  |    | 0.386     | 0.810  | 0.254           |\\n| 10  |    | 0.487     | 0.631  | 0.396           |\\n| 10  |    | 0.584     | 0.444  | 0.851           |\\n| \u00b5  |    | 0.749     | 0.730  | 0.776            |\\n\\nTable 8: IAA on Gesture-AMR (GAMR) annotation.\"}"}
