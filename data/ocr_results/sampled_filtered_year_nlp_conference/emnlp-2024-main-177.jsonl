{"id": "emnlp-2024-main-177", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CUTE: Measuring LLMs' Understanding of Their Tokens\\n\\nLukas Edman1,3 Helmut Schmid1 Alexander Fraser2,3,4\\n1Center for Information and Language Processing, LMU Munich\\n2School of Computation, Information and Technology, TU Munich\\n3Munich Center for Machine Learning\\n4Munich Data Science Institute\\n\\nlukas@cis.lmu.de, schmid@cis.lmu.de\\n\\nAbstract\\n\\nLarge Language Models (LLMs) show remarkable performance on a wide variety of tasks. Most LLMs split text into multi-character tokens and process them as atomic units without direct access to individual characters. This raises the question: To what extent can LLMs learn orthographic information? To answer this, we propose a new benchmark, CUTE, which features a collection of tasks designed to test the orthographic knowledge of LLMs. We evaluate popular LLMs on CUTE, finding that most of them seem to know the spelling of their tokens, yet fail to use this information effectively to manipulate text, calling into question how much of this knowledge is generalizable.\\n\\n1 Introduction\\n\\nLarge Language Models (LLMs) attract a lot of interest due to their strong performance on many NLP tasks. They have demonstrated a level of fluency rivaling humans. However, it is often overlooked that LLMs lack direct access to the characters composing their tokens. They can only infer knowledge about the characters from the context during pretraining or instruction tuning. While there are models that use characters as input units, none of them have been instruction-tuned (to our knowledge).\\n\\nOur work examines how well LLMs understand the composition of their tokens. This knowledge enables LLMs to better generalize to new languages and to perform well on a variety of tasks involving character-level understanding. Tasks such as word puzzles, poetry generation (e.g. alliterations), or parsing ciphers all require very explicit use of characters to achieve. More popular tasks such as code completion, morphological inflection, or spelling correction also require character-level information to a lesser extent, though these tasks also require semantic knowledge, which we wish to ablate.\\n\\nWe introduce Character-level Understanding of Tokens Evaluation (CUTE), a benchmark consisting of several tasks designed to be easy for humans to complete, given our ability to process characters individually. We evaluate several LLMs ranging from 7B to 132B parameters in size on CUTE to answer the following questions:\\n\\n1. Do LLMs know which characters make up their tokens?\\n2. Do LLMs understand the difference between semantic and orthographic similarity?\\n3. Can LLMs manipulate text at the character level?\\n\\nWe address these questions with a set of tasks primarily on the character level, and additionally on the word level to see the difference in performance on the two granularities, thereby separating the understanding of the task from the understanding of what makes up a token.\\n\\n2 Related Work\\n\\nItzhak and Levy (2022) mostly analyze encoder-only models and test if they understand how to spell words after fine-tuning on 32k examples. They conclude that models learn to spell their tokens \u201cto some extent.\u201d They also experiment with GPT-2 (Radford et al., 2019) which performed similarly to the other models, and also used training examples.\\n\\nKaushal and Mahowald (2022) probe models with a task asking if a letter is in a word (similar to our character contains task, see \u00a73). Similar to Itzhak and Levy (2022), their probe requires training, as they use models of similar size. By contrast, we examine models with 10 to 200 times as many parameters and apply few-shot prompting without fine-tuning.\\n\\nHuang et al. (2023) experiment with spelling correction, unscrambling words, and finding words in\"}"}
{"id": "emnlp-2024-main-177", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a string of characters. Most of their experiments concern a training method they propose, but they also include results of GPT-3 (Brown et al., 2020) with few-shot prompting, finding that it performs well on spelling correction, but poorly on other tasks compared to their trained character-based models. Most of their tasks require semantic knowledge, which we wish to ablate in our benchmark.\\n\\nOther benchmarks testing orthographic knowledge often focus on morphology, such as the SIGMORPHON inflection tasks (e.g. Goldman et al. (2023)). Inflection is fairly regular for most languages, and could be memorized by a language model. Given that many developers of LLMs do not disclose the sources of their pretraining data, it is possible that LLMs memorize these inflection patterns. Therefore, from this we cannot conclude that LLMs can inflect a new word given a new set of rules, and furthermore we cannot conclude that LLMs can apply an arbitrary manipulation to a sequence. Most LLMs are trained with performance on English in mind, and English being less morphological in nature makes inflection a non-ideal measure for a model\u2019s understanding of the composition of tokens.\\n\\nThe most similar benchmark to the one we propose is LMentry (Efrat et al., 2023). The purpose of LMentry is similar in that the goal is to test models on tasks that are trivial to humans. Some of the tasks included test orthography and are similar to our tasks, for example, they ask the model to write a word containing a letter, or ask to write the first or last letter of a given word. Our benchmark is distinguished from LMentry as most of our tasks explicitly require knowledge of every character in a word. Testing whether a model knows the first letter of a word is not sufficient for concluding that it understands orthography, as there may be more pressure to learn about the first letter of a word from pretraining and/or instruction tuning (e.g. via alliterations or acronyms). As such, the task to write a word containing a letter could be more trivially solved by writing a word that starts with said letter.\\n\\nThere is an extensive body of research on character-level models, where each character forms a token (Lee et al., 2017; Xue et al., 2022; Tay et al., 2022, inter alia). Several works compared these models to subword models (Libovick\u00fd et al., 2022; Edman et al., 2022, 2024, inter alia), but they evaluated on tasks requiring additional training. We assume that character-based models would perform well on our benchmark, but we cannot test it since none of these models have been instruction-tuned.\\n\\n3 Benchmark\\n\\nWe split our tasks into 3 categories: understanding composition, understanding orthographic similarity, and ability to manipulate sequences. Figure 1 shows an example for each task.\\n\\nData gathering and processing details can be found in Appendix C. Our tasks are synthetically generated from existing corpora. There are non-synthetically generated datasets which partially test our research questions, but these datasets have external factors (e.g. domain and/or language in a translation dataset) that would likely obscure our findings. Some of these datasets also might have been leaked into the LLM\u2019s pretraining data and been memorized, resulting in an unrealistically good performance.\\n\\n3.1 Composition\\n\\nWe start with a straightforward benchmark: spelling. Similar to Itzhak and Levy (2022), we include a task where the input is a word given as a single token, and the output is the same word with spaces in between, so that each character becomes a separate token. This is the most straightforward probe to see whether a model has knowledge of the characters forming the tokens. We also add the inverted task (\u201cinverse spelling\u201d) to check if characters can also be mapped to tokens. Another method for assessing a model\u2019s understanding of composition is to ask if a token contains a certain character. If a model managed the previous tasks, we would expect it to succeed here as well on our benchmark, but we cannot test it since none of these models have been instruction-tuned.\\n\\n2 The prompts shown here are not the full prompts. See Appendix B for more details.\\n\\n3 This is in the ideal case. We cannot guarantee every LLM tested uses only a single token for each input, but we minimize the chance of splitting by using frequent words in our task.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, a model might not understand the relationship between spelling and membership of characters to a word, so we test this as well. We also test whether the LLMs are able to solve the corresponding word-level task (i.e. is a word in a sentence) to separate the model's general understanding of the task from its ability to solve the task at the character level.\\n\\n3.2 Similarity\\n\\nSince the introduction of word2vec (Mikolov et al., 2013), language models have typically been trained to predict words from their context. The resulting token embeddings mainly reflect the semantic and syntactic similarity of tokens. Our next tasks examine whether LLMs also comprehend orthographic similarity. We ask which one of two candidate words is orthographically (or semantically) more similar to a given word. Our candidate words are chosen to be relatively easy to distinguish for a human without explicit knowledge of how to measure orthographic or semantic similarity. For more details, see Appendix C.\\n\\n3.3 Manipulation\\n\\nOur previous tasks focus on the understanding of the model. Now, we turn our focus to acting on that understanding. The next tasks involve 4 types of manipulation of the input at the character or word level: Insertion, Deletion, Substitution, and Swapping. We consider these tasks as elementary tasks for modifying a text sequence.\\n\\nInsertion\\n\\nFirst we test how well the model can insert an element X after every instance of some element Y in the sequence. Similar modifications occur when we replicate letters to emphasize a word (e.g. \\\"Yay!\\\" vs. \\\"Yaaaaay!\\\"), or when we add an adjective next to a noun.\\n\\nDeletion\\n\\nDeletion requires the model to recognize an element and remove all instances of it. This can occur in natural language at the character level with inflection in languages (e.g. turning an English plural noun into singular), or removing adjectives from a sentence.\\n\\nSubstitution\\n\\nSubstitution replaces all instances of an element in a sequence with another element. This can occur with spelling or vocabulary variations across dialects or related languages (e.g. \\\"defense\\\" vs. \\\"defence\\\", or \\\"elevator\\\" vs. \\\"lift\\\").\\n\\nSwapping\\n\\nSwapping is a simplified case of reordering acting on two elements.\\n\\n4 Experimental Setup\\n\\nModels\\n\\nWe use the models shown in Table 1.\\n\\n| Model   | Tokens (k) | Lang |\\n|---------|------------|------|\\n| Llama 2 | 7, 13, 70  | EN   |\\n| Gemma  | 256        | EN   |\\n| Mistral| 7, 47      | EN   |\\n| Aya    | 8, 35, 256 | Multil. |\\n| Cmd-R(+) | 35, 104  | Multil. |\\n| DBRX   | 100        | EN   |\\n| Llama 3| 8, 70      | EN   |\\n\\nTable 1: Models evaluated on our benchmark. We note that Gemma has a multilingual tokenizer, but English-centric training.\\n\\n4.1 Prompts\\n\\nWe use a template inspired by Bsharat et al. (2023)'s few-shot template to prompt our models with 4 in-context examples. Further details and a sample prompt can be found in Appendix B.\\n\\n5 Results\\n\\nFigure 2 shows the results for each model. The random baseline is 50% for the contains and similarity tasks, and 0% for all other tasks.\\n\\n5.1 Composition\\n\\nThe models perform very well on the tasks spelling and inverse spelling, though inverse spelling appears slightly more difficult.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Accuracy on each task of CUTE. Models ordered by average accuracy over all tasks.\\n\\nAlthough we are not aware of any spelling tasks in instruction tuning or pretraining datasets, we suspect that the similarity of this task with data seen during training allows the models to perform very well, in contrast to the following tasks.\\n\\nOn the contains tasks, the performance at the word level is quite good, showing that the models understand the task, but the performance breaks down at the character level. This indicates the models do not fully understand the relationship between spelling and membership of a character to a word.\\n\\n5.2 Orthography and Semantic Similarity\\n\\nIn the semantic similarity task, the models correctly choose the more semantically related word 76-93% of the time (with the exception of Aya-8B), and the performance generally increases with model size. For orthographic similarity, the performance is below or near random for all models except Command-R+ and Llama3. It is not yet clear why they perform so well on this task, but apparently it is not solely a scaling effect since DBRX fails to perform above random chance. It may be due to the amount of training data, however DBRX and Command-R+ have not disclosed these amounts.\\n\\n5.3 Manipulation\\n\\nIn our manipulation tasks, the models struggle more at the character level than at the word level. The difference is quite profound, with performance gaps of up to 72.8% on Command-R+ for insertion. Larger models such as Command-R+ perform well on deleting characters, with 72% accuracy, though it should be noted that we are only testing on the 1000 most frequent words, making the evaluation fairly generous.\\n\\nLike the contains task and orthographic similarity task, the manipulation tasks show that LLMs lack a complete understanding of their tokens, although they can literally spell them out. The higher word-level performance indicates that it is not due to a lack of understanding of the task itself.\\n\\n5.4 Vocabulary Size and Multilinguality\\n\\nThere appear to be no noticeable effects of vocabulary size from the results shown. Looking at the 7/8B models, while Llama 3 performs well with a vocabulary size of 100k, using a larger vocabulary (i.e. Gemma) does not improve performance, and neither does using a smaller vocabulary (i.e. Llama 2 and Mistral). It remains to be seen if noticeable effects arise as the vocabulary size approaches the number of characters. We leave this for future research.\\n\\nFor multilinguality, the results are similarly mixed. We focus on Aya-35B versus Command-R, as Aya is a fine-tuned version of Command-R, using multilingual instruction tuning data. Overall, Aya makes slight improvements over Command-R on the character-level, but this could easily be due to training on additional English data, rather than the additional non-English data.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.5 Scaling\\n\\nThere are 2 major factors to scale: parameter count and training data. In terms of parameter count, larger models clearly tend to perform better. With respect to amount of training data, only Llama 2 and 3 have disclosed this, and based on the results, it appears that more training data also improves performance. This aligns well with the myriad of works showing the benefits of scaling and raises the question: Is scaling all we need for good performance on character-level tasks? Looking at the manipulation tasks, it seems that deletion and substitution could become manageable in the near future, but for insertion and swapping, the performance gap between word and character level tasks is large. Many real-world text manipulation tasks are a combination of the tested tasks, so we will likely need more than just scaling.\\n\\n6 Conclusion\\n\\nWhile current LLMs with BPE vocabularies lack direct access to a token\u2019s characters, they perform well on some tasks requiring this information, but perform poorly on others. The models seem to understand the composition of their tokens in direct probing, but mostly fail to understand the concept of orthographic similarity. Their performance on text manipulation tasks at the character level lags far behind their performance at the word level. LLM developers currently apply no methods which specifically address these issues (to our knowledge), and so we recommend more research to better master orthography. Character-level models are a promising direction. With instruction tuning, they might provide a solution to many of the shortcomings exposed by our CUTE benchmark.\\n\\n7 Limitations\\n\\nWe prompt instruction-tuned LLMs without any fine-tuning on benchmark data. While this can be seen as a limitation, we note that it is not feasible to add more training data whenever we discover a new issue with LLMs. We expect that the performance of all models would increase after fine-tuning. We do not evaluate any character-level models since there are no instruction-tuned versions (to our knowledge). Additionally, there are no decoder-only pretrained LLMs available, with the closest model being ByT5-XXL (13B), which has a heavy encoder and smaller decoder. Training character-level models also requires a much higher computational budget, as the sequence lengths are roughly 5 times longer, resulting in 5 times longer training. As such, training a truly comparable model falls outside the scope of this work.\\n\\nOur benchmark does not control whether LLM tokenizers split words into multiple tokens. We minimize that chance by choosing frequent words, but we can never guarantee that future models will not split words into multiple tokens. We found that the impact of removing split tokens is minimal, with less than 1% change on average (see Appendix F).\\n\\nWe only test on English and Russian, with our primary focus being on English. While we did not see any major differences in the LLMs\u2019 performance between English and Russian when it comes to character-level versus word-level performance, it is possible that there may be differences in other languages.\\n\\nLastly, we do not control for generations that do not match the pattern of the examples given in the prompt. Therefore, we cannot guarantee that all generations considered correct by humans are evaluated as such. Hence the performance of some models may be lower than expected. We provide the outputs of all models in our repository for further analysis.\\n\\n8 Acknowledgments\\n\\nThe work was supported by the European Research Council (ERC) under the European Union\u2019s Horizon Europe research and innovation programme (grant agreement No. 101113091) and by the German Research Foundation (DFG; grant FR 2829/7-1). We also thank Lisa Bylinina for helping with the Russian translation of our benchmark.\\n\\nReferences\\n\\nViraat Aryabumi, John Dang, Dwarak Talupuru, Saurabh Dash, David Cairuz, Hangyu Lin, Bharat Venkitesh, Madeline Smith, Jon Ander Campos, Yi Chern Tan, Kelly Marchisio, Max Bartolo, Sebastian Ruder, Acyr Locatelli, Julia Kreutzer, Nick Frosst, Aidan Gomez, Phil Blunsom, Marzieh Fadaee, Ahmet \u00dcst\u00fcn, and Sara Hooker. 2024. Aya 23: Open weight releases to further multilingual progress. Preprint, arXiv:2405.15032.\\n\\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. 2017. Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-177", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kale, Juliette Love, Pouya Tafti, L\u00e9onard Hussenot, Pier Giuseppe Sessa, Aakanksha Chowdhery, Adam Roberts, Aditya Barua, Alex Botev, Alex Castro-Ros, Ambrose Slone, Am\u00e9lie H\u00e9liou, Andrea Tacchetti, Anna Bulanova, Antonia Paterson, Beth Tsai, Bobak Shahriari, Charline Le Lan, Christopher A. Choquette-Choo, Cl\u00e9ment Crepy, Daniel Cer, Daphne Ippolito, David Reid, Elena Buchatskaya, Eric Ni, Eric Noland, Geng Yan, George Tucker, George-Christian Muraru, Grigory Rozhdestvenskiy, Henryk Michalewski, Ian Tenney, Ivan Grishchenko, Jacob Austin, James Keeling, Jane Labanowski, Jean-Baptiste Lespiau, Jeff Stanway, Jenny Brennan, Jeremy Chen, Johan Ferret, Justin Chiu, Justin Mao-Jones, Katherine Lee, Kathy Yu, Katie Millican, Lars Lowe Sjoesund, Lisa Lee, Lucas Dixon, Machel Reid, Maciej Miku\u0142a, Mateo Wirth, Michael Sharman, Nikolai Chinaev, Nithum Thain, Olivier Bachem, Oscar Chang, Oscar Wahltinez, Paige Bailey, Paul Michel, Petko Yotov, Rahma Chaabouni, Ramona Comanescu, Reena Jana, Rohan Anil, Ross McIlroy, Ruibo Liu, Ryan Mullins, Samuel L Smith, Sebastian Borgeaud, Sertan Girgin, Sholto Douglas, Shree Pandya, Siamak Shakeri, Soham De, Ted Klimenko, Tom Hennigan, Vlad Feinberg, Wojciech Stokowiec, Yu hui Chen, Zafarali Ahmed, Zhitao Gong, Tris Warkentin, Ludovic Peran, Minh Giang, Cl\u00e9ment Farabet, Oriol Vinyals, Jeff Dean, Koray Kavukcuoglu, Demis Hassabis, Zoubin Ghahramani, Douglas Eck, Joelle Barral, Fernando Pereira, Eli Collins, Armand Joulin, Noah Fiedel, Evan Senter, Alek Andreev, and Kathleen Kenealy. 2024. Gemma: Open models based on gemini research and technology. Preprint, arXiv:2403.08295.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.\\n\\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-Rfou, Sharan Narang, Mihir Kale, Adam Roberts, and Colin Raffel. 2022. ByT5: Towards a token-free future with pre-trained byte-to-byte models. Transactions of the Association for Computational Linguistics, 10:291\u2013306.\\n\\n**All Models**\\nHere we link to all of the models. Those associated with published works are as follows: Touvron et al. (2023); Jiang et al. (2023, 2024); Team et al. (2024); Aryabumi et al. (2024). All models can be found at the following links:\\n\\n- [https://hf.co/meta-llama/Llama-2-7b-chat-hf](https://hf.co/meta-llama/Llama-2-7b-chat-hf)\\n- [https://hf.co/meta-llama/Llama-2-13b-chat-hf](https://hf.co/meta-llama/Llama-2-13b-chat-hf)\\n- [https://hf.co/meta-llama/Llama-2-70b-chat-hf](https://hf.co/meta-llama/Llama-2-70b-chat-hf)\\n- [https://hf.co/google/gemma-7b-it](https://hf.co/google/gemma-7b-it)\\n- [https://hf.co/mistralai/Mistral-7B-Instruct-v0.2](https://hf.co/mistralai/Mistral-7B-Instruct-v0.2)\\n- [https://hf.co/mistralai/Mistral-8x7B-Instruct-v0.1](https://hf.co/mistralai/Mistral-8x7B-Instruct-v0.1)\\n- [https://hf.co/CohereForAI/aya-23-8B](https://hf.co/CohereForAI/aya-23-8B)\\n- [https://hf.co/CohereForAI/aya-23-35B](https://hf.co/CohereForAI/aya-23-35B)\\n- [https://hf.co/CohereForAI/c4ai-command-r-v01](https://hf.co/CohereForAI/c4ai-command-r-v01)\\n- [https://hf.co/CohereForAI/c4ai-command-r-plus](https://hf.co/CohereForAI/c4ai-command-r-plus)\\n- [https://hf.co/databricks/dbrx-instruct](https://hf.co/databricks/dbrx-instruct)\\n- [https://hf.co/meta-llama/Meta-Llama-3-8B](https://hf.co/meta-llama/Meta-Llama-3-8B)\\n- [https://hf.co/meta-llama/Meta-Llama-3-70B](https://hf.co/meta-llama/Meta-Llama-3-70B)\\n\\n**B Prompting Details**\\nWe show an example of a full prompt in Figure 3. All of our prompts are available with the release of our benchmark. For generation, we use greedy search. For evaluation of the generation, we rely on the given start-quote to denote the start of the answer, and we filter out anything after the end quote (e.g. \u201cI hope this answer helped!\u201d). Some models also were prone to starting generation with a generic response such as \u201cSure I can do that for you.\u201d For these generations, we observe that it would repeat \u201cAnswer: \u201c, so we filter out all generations before this point. Of the remaining generations, some could be considered correct though did not match the desired pattern (e.g. \u201cH-E-L-L-O\u201d rather than \u201ch e l l o\u201d for the spelling task). These we ultimately...\"}"}
{"id": "emnlp-2024-main-177", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Spell out the word, putting spaces between each letter, based on the following examples:\\n\\n1. Spell out the word \u201calphabet\u201d.\\n   Answer: \u201ca l p h a b e t\u201d\\n\\n2. Spell out the word \u201chello\u201d.\\n   Answer: \u201ch e l l o\u201d\\n\\n3. Spell out the word \u201czebra\u201d.\\n   Answer: \u201cz e b r a\u201d\\n\\n4. Spell out the word \u201ctongue\u201d.\\n   Answer: \u201ct o n g u e\u201d\\n\\nQuestion: Spell out the word \u201ccow\u201d.\\n\\nConcerning the wording of the orthographic similarity task, it could be argued that models do not understand the concept of Levenshtein distance, and thus it is not comparable to the semantic similarity task. We also tested using \u201ccloser in edit distance\u201d and \u201ccloser in spelling\u201d in the prompt, and the results were very similar, so we opted for Levenshtein distance as it is more well-defined. Similarly, we used \u201ccloser in meaning\u201d rather than \u201cmore semantically related\u201d and achieved similar results, though it is debatable whether an antonym should be considered close in meaning, so we opted for the latter.\\n\\nC Data Processing\\nHere we detail our exact method for gathering and processing the data into our tasks. The scripts for processing the data and the resulting data can be found at our Github repository.\\n\\nData Sources\\nFor almost all tasks, we require a set of frequent English words that were most likely to be tokenized into a single token. For this, we use a dataset derived from the Google Web Trillion Word Corpus. For the word-based tasks, we use the TinyStories (Eldan and Li, 2023) dataset, which consists of stories written by an LLM in a style appropriate for a 3-4 year old reader. This has the benefit of using simple sentences with a limited vocabulary, maximizing the chances of words being tokenized into a single token in the models we test, while also ensuring that the complexity of the sentence is not a confounding factor in a model\u2019s performance on the tasks.\\n\\nFiltering\\nFor character-based tasks, we select the 1000 most frequent words that are at least 3 characters long. For word-based tasks, we similarly filter for 1000 sentences of length 3-10 words, in order to make the length similar to the number of characters in a word seen in the character-level tasks.\\n\\nFor insertion, deletion, and substitution, we apply the modification to those 1000 words, resulting in our dataset.\\n\\nFor swapping, we need to ensure that the word or sentence has 2 items that are unique, so as to avoid an ambiguous prompt (e.g. swap the \u2018e\u2019 and \u2018g\u2019 in \u2018engineering\u2019). As such, we select the 1000 most frequent words or the first 1000 sentences that satisfy this criteria, as well as satisfying our length constraints.\\n\\nSimilarity Data\\nFor our similarity data, we require our candidate pairs to be sufficiently easy for a human to distinguish which is closer orthographically and which is closer semantically.\\n\\nTo accomplish this, our candidate words must satisfy two thresholds, one based on normalized Levenshtein distance (for orthographic similarity), and one based on cosine similarity to other fastText (Bojanowski et al., 2017) embeddings (for semantic similarity). That is to say, the word must be sufficiently similar in one metric (0.7+ and 0.5+ for Levenshtein and cosine, respectively) and sufficiently dissimilar in the other (0.3\u2212 and 0.2\u2212, respectively). These thresholds are decided empirically. We note that this process occasionally ends up with semantic pairs that are antonyms (e.g. \u201cgood\u201d and \u201cbad\u201d), and thus we refrain from stating that the pairs are similar in meaning.\\n\\nD Russian Experiments\\nHere, we detail the creation and evaluation on our Russian version of CUTE, CUTE-Rus.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Accuracy on each task of CUTE-Rus. Models ordered by average accuracy over all tasks.\\n\\nFigure 5: Evaluation of the performance on random strings versus strings in the vocabulary. \\\"Vocab\\\" is equivalent to the character-level tasks from Figure 2.\\n\\nData Processing\\nOur data processing followed our processing in Appendix C. We collected a frequency list of Russian words from Wiktionary, embeddings from FastText, and we translated the English TinyStories dataset using Google Translate.\\n\\nWe adjusted the thresholds for gathering our orthographic and semantic similarity pairs to (0.55+, 0.55+) and (0.3-, 0.1-) for Levenshtein and cosine, respectively. The thresholds were more difficult to adjust without excluding too many pairs, and as a result, some of the triplets could be argued as unclear or incorrect. For example, for the triplet (\u043e\u0431\u0449\u0438\u0439, \u043e\u0431\u0449\u0438\u043d\u0430, \u0441\u043e\u0432\u043c\u0435\u0441\u0442\u043d\u044b\u0439) meaning (common, community, joint), it is less clear which of the last two are more semantically related to the first (more so in Russian), though joint is the intended semantically-related word.\\n\\nThe prompts were also machine translated with Google Translate and post-edited by a native Russian and fluent English speaker, but after testing with both English and Russian prompts, we found the models generally performed better with English prompts (with Russian examples in the prompt), so we only include those in our results.\"}"}
{"id": "emnlp-2024-main-177", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Results\\n\\nWe test a subset of the models used in the English version, focusing mainly on the multi-lingual LLMs (Aya, Command-R(+), and Gemma to the extent of the tokenizer), as well as Llama 2 and 3 for reference.\\n\\nFigure 4 shows a similar trend to the English results. Generally speaking, the character-level performance lags behind the word-level. An interesting difference is that the models struggle much more with spelling. Even though the prompt is in English and shows examples of Russian words being spelled out, this is not enough for most models to understand the concept of spelling. The additional multilingual instruction tuning done for training Aya appears to be necessary for better performance.\\n\\nE Random String Evaluation\\n\\nWhile we cannot directly assess the performance of character-level LLMs without training an equivalent model from scratch, we can evaluate the performance of the existing models when the number of tokens per word approaches the number of characters per word. We can do this by conducting our tasks using random strings of consonants rather than complete words.\\n\\nSince practically every word in English requires a vowel, random sequences of consonants are typically quite rare, and thus BPE will not dedicate a singular token for sequences like \u201cfxqg\u201d. As such, we generate random strings for use in our tasks (excluding the similarity tasks). The resulting strings use on average 1.6 characters per token, compared to 5.4 characters per token in the original word list.\\n\\nIn Figure 5, we can see the performance of the LLMs on the character level tasks using regular words (as shown before in Figure 2), as well as random strings. We can see that, apart from inverse spelling, the models perform the same or better on random strings than on actual words. These tasks appear much easier for LLMs to handle when their tokenization is close to character-level, suggesting that a truly character-level LLM would perform the best.\\n\\nAs for inverse spelling, the decrease may be a result of the models\u2019 bias towards generating words. Upon inspection of the outputs, we observe that occasionally the model would hallucinate a word, changing a string such as \u201cc q n r w\u201d to \u201cconquer\u201d, essentially filling in what it considers to be the missing vowels. This phenomenon is particularly common in Llama3-70B, whose performance decreased the most.\\n\\nF Token Splitting Impact\\n\\nWe mention that while we use frequent words for evaluation to maximize the chance they are given a single token, we cannot guarantee that some words will not be split into multiple tokens. Here, we evaluate the impact of this splitting on our results to elucidate whether this issue could affect the trends we see in the paper.\\n\\nFigure 6 shows the percent change in accuracy were those examples to be removed from each task. All of the tasks are grouped into a violin plot. Here we can see that the maximum accuracy difference is around $-3.5\\\\%$, and the median errors for each model are no greater than $\\\\pm 0.5\\\\%$. This is far from substantially affecting the disparity we see between the character-level and word-level tasks. This also reveals that even if an LLM\u2019s tokenizer splits a word into two or more tokens, the LLM will still have difficulty performing the tasks in the CUTE benchmark.\"}"}
