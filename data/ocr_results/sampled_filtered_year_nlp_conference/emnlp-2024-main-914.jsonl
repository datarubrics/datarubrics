{"id": "emnlp-2024-main-914", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: UI used for the annotation task: the annotators could familiarize themselves with the task with an outline of the task instructions (detailed guidelines could be read in a separate page) and the information about the entity, including its names in English and its Wikipedia pages in English and the target language (Korean in this case).\\n\\n\\\\[\\n\\\\langle t, t', \\\\hat{E}_t \\\\rangle\\n\\\\]\\n\\nwhere \\\\( t \\\\) is a source text, \\\\( t' \\\\) is a target text, and \\\\( \\\\hat{E}_t \\\\) is the set of gold entities for \\\\( t \\\\).\\n\\nThe training data is created by uniformly sampling a mixture of the training data from Mintaka and NLLB-200, a recent multilingual MT model that supports translation from and to 200 different languages using a single model. While Mintaka is a relatively small dataset, it is also convenient for our purposes, as it can be used to train both the knowledge retriever and the knowledge-enhanced translator. Future work may consider using larger datasets for training the knowledge-enhanced translator as well as adopting more sophisticated training strategies, such as curriculum learning and adversarial training.\\n\\nC.4 Hyperparameters\\n\\nThe knowledge retriever is trained using the following hyperparameters:\\n\\n- Learning rate: 1e-5;\\n- Batch size: 32;\\n- Number of epochs: 5;\\n- Optimizer: AdamW;\\n- Loss function: Binary Cross-Entropy;\\n- Pretrained retriever: mContriever;\\n- Hard negative mining: enabled;\\n- Number of negative samples: 8;\\n- Maximum query length: 128;\\n- Maximum context length: 128.\\n\\nWhile the query length and the context length (i.e., the entity name and its description) are set to 128, the textual representations of the entities usually do not exceed 100 tokens, which makes the maximum context length a reasonable choice.\\n\\nThe knowledge-enhanced translator is trained using the following hyperparameters:\\n\\n- Learning rate: 1e-5;\\n- Batch size: 32;\\n- Number of epochs: 5;\\n- Optimizer: AdamW;\\n- Loss function: Cross-Entropy;\\n- Maximum input length: 512;\\n- Maximum output length: 512.\\n\\nThe maximum input length and the maximum output length are set to 512, which is the maximum length supported by the underlying MT models that we consider in our study.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: UI used for the annotation task: the annotator was tasked with verifying the translation from the English question to the target language in a free-form text box, and was provided relevant details such as the (1) English question, (2) English entity, (3) entity names in the target language, and (4) a possible translation template in the target language.\\n\\nRelated Work: Addendum\\n\\nIn this section, we provide an in-depth discussion of the related work on machine translation and entity name translation, with a particular focus on a few more relevant works.\\n\\nZeng et al. (2023): the authors recently proposed a method to extract entity names from a source text and translate them into a target language by looking them up in a dictionary and appending their translation to the source text, which is similar to what we named explicit knowledge integration in our work (see Section 3.2). The authors evaluate their method on a small-scale dataset and show that it outperforms a vanilla MT system. However, there are important differences between their work and ours:\\n\\n\u2022 Their focus is on transliteration rather than transcreation. This is evident in their evaluation, in which they select language pairs in which transliteration is necessary, but also in their method, in which they make explicit use of a transliteration system. We believe that their work and ours are complementary, as they focus on a different aspect of the problem, and that their method could be integrated into our method to improve the performance of KG-MT.\\n\\n\u2022 Their method is based on a dictionary lookup, which is simple and effective but that ignores the problem of ambiguous entities, i.e., entities that have different translations in different contexts. This is a problem that we address in our work by using a knowledge retriever to retrieve relevant entities for the source text.\\n\\n\u2022 Our knowledge retriever is also capable of retrieving entities that do not have an exact match with the source text, i.e., it does not rely on mention detection. Moreover, using our approach allows the retriever not to retrieve any entities if their retrieval (or the knowledge that would be retrieved) is not relevant to the translation task. In contrast, their method leaves this task to the encoder-decoder architecture of the MT system.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Cross-Cultural Machine Translation with Retrieval-Augmented Generation from Multilingual Knowledge Graphs\\n\\nSimone Conia\\nSapienza University of Rome\\nsimone.conia@uniroma1.it\\n\\nDaniel Lee\\nAdobe\\ndlee1@adobe.com\\n\\nMin Li\\nApple\\nmin_li6@apple.com\\n\\nUmar Farooq Minhas\\nApple\\nufminhas@apple.com\\n\\nSaloni Potdar\\nApple\\ns_potdar@apple.com\\n\\nYunyao Li\\nAdobe\\nyunyaol@adobe.com\\n\\nAbstract\\nTranslating text that contains entity names is a challenging task, as cultural-related references can vary significantly across languages. These variations may also be caused by translation, an adaptation process that entails more than transliteration and word-for-word translation. In this paper, we address the problem of cross-cultural translation on two fronts: (i) we introduce XC-Translate, the first large-scale, manually-created benchmark for machine translation that focuses on text that contains potentially culturally-nuanced entity names, and (ii) we propose KG-MT, a novel end-to-end method to integrate information from a multilingual knowledge graph into a neural machine translation model by leveraging a dense retrieval mechanism. Our experiments and analyses show that current machine translation systems and large language models still struggle to translate texts containing entity names, whereas KG-MT outperforms state-of-the-art approaches by a large margin, obtaining a 129% and 62% relative improvement compared to NLLB-200 and GPT-4, respectively.\\n\\n1 Introduction\\nThe emergence of multilingual large language models (LLMs) and the wide availability of massive multilingual datasets have significantly advanced the field of Machine Translation (MT). These developments have led to MT systems that not only perform exceptionally well in high-resource languages but also support a growing number of low-resource languages (Fan et al., 2021; Tang et al., 2021; Costa-juss\u00e0 et al., 2022; Kudugunta et al., 2023, inter alia). Nevertheless, the research community still faces several unresolved challenges in MT. Among these, the translation of text that contains entities is still a hard task, especially with some categories of entities, e.g., movies, books, food, locations, and sometimes even people, to name a few. Indeed, word-for-word, or literal, translations of their names may not be suitable due to cultural-specific references, which can vary depending on social, geographical, historical, and political contexts, among other factors (Hershcovich et al., 2022). Therefore, the challenge lies in accurately identifying when and how to translate entities whose names are significantly different across languages. This step is crucial, as relying on literal translations may not convey the intended meaning, risking the effectiveness of the entire translation process (Gaballo, 2012; D\u00edaz-Mill\u00f3n and Olvera-Lobo, 2023). For example, if we were to translate word-for-word \u201cQual \u00e8 la trama de Il Giovane Holden?\u201d from Italian to English, we would obtain \u201cWhat is the plot of The Young Holden?\u201d, which is grammatically correct but semantically incorrect. The correct translation \u201cWhat is the plot of The Catcher in the Rye?\u201d requires not only fluency in both the source and target languages but also knowledge of the cultural contexts involved.\\n\\nIn this paper, we address the problem of cross-cultural translation on two fronts: resources and methods. More specifically, our contributions can be summarized as follows:\\n\\n\u2022 We introduce XC-Translate, the first large-scale, manually-created benchmark for cross-cultural translation across 10 language pairs of text containing entity names;\"}"}
{"id": "emnlp-2024-main-914", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We demonstrate that XC-Translate exposes the limitations of current MT models and LLMs in translating text with entity names that can vary across languages and cultures; we propose KG-MT, a novel MT system equipped with retrieval-augmented generation from multilingual knowledge graphs; we evaluate KG-MT on XC-Translate and show that it outperforms state-of-the-art approaches by a large margin, while also requiring minimal supervision and computational resources compared to data augmentation approaches.\\n\\nWe hope our work will encourage further research in the field of cross-cultural translation, leading to more investigations on the gaps of current methods in capturing cultural nuances beyond differences in entity names. To this end, we release XC-Translate at https://github.com/apple/ml-kg-mt.\\n\\nRelated Work\\nMT is a long-standing research topic in NLP. In this section, we briefly review the literature on recent advancements in MT, with a focus on studies that investigate entity names in relation to MT.\\n\\nMachine Translation.\\nThe field of MT has made a significant step forward with the emergence of multilingual language models, such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020), and massive multilingual corpora, such as OSCAR (Ortiz Su\u00e1rez et al., 2019) and MADLAD-400 (Kudugunta et al., 2023). Not only have these developments led to robust bilingual MT systems, such as OPUS-MT (Tiedemann et al., 2023), but also to multilingual MT systems that can translate to and from multiple languages with a single model, such as mBART-50 (Liu et al., 2020), M2M-100 (Fan et al., 2021), and NLLB-200 (Costa-juss\u00e0 et al., 2022). Therefore, we build KG-MT on top of these multilingual MT systems \u2013 which are openly available and widely used in the research community \u2013 while also comparing our results with state-of-the-art LLMs, such as GPT-3.5 and GPT-4, which have been shown to achieve competitive performance in general-purpose MT evaluations (Wang et al., 2023).\\n\\nExternal Knowledge in Machine Translation.\\nPrevious studies have already introduced methods to improve MT system via retrieval-augmentation or constrained-generation (Zhang et al., 2018; Bulte and Tezcan, 2019; Campolungo et al., 2022; Iyer et al., 2023). Notably, Zhang et al. (2018) proposed retrieval-augmentation to improve the translation of low-frequency words at inference time, while Bulte and Tezcan (2019) demonstrated the benefits of retrieving fuzzy matches to augment a dataset at training time. More recently, Campolungo et al. (2022) and Iyer et al. (2023) investigated the use of lexical constraints derived from external knowledge sources, e.g., dictionaries like WordNet, to improve the translation of senses in the long tail of the distribution. Although guiding or constraining the translation process has been shown to be an effective direction towards improving the translation quality of MT systems, the area at the intersection of retrieval-augmented generation and retrieval from large knowledge sources with millions of elements, such as Wikidata, is still understudied, to the best of our knowledge.\\n\\nEntity names in Machine Translation.\\nEarlier investigations have long recognized and begun to address the challenges associated with translating texts that contain entity names (Knight and Graehl, 1998; Al-Onaizan and Knight, 2002a,b). However, there are three important aspects that have yet to be fully explored in the literature. First, the focus has predominantly been on the transliteration of entity names, i.e., adapting an entity name from the script of one language to another (Sadamitsu et al., 2016; Ugawa et al., 2018; Zeng et al., 2023). Although transliteration is crucial for languages with different scripts, like English and Chinese, it does not necessarily account for the transcreation of entity names between languages using the same script, like English and Italian (Gaballo, 2012; D\u00edaz-Mill\u00f3n and Olvera-Lobo, 2023). Second, the depth of existing investigations has been constrained significantly by the absence of large-scale and high-quality benchmarks designed to highlight the challenges of cross-cultural translation (Zeng et al., 2023). Lastly, current approaches have mainly relied on training MT models by synthetically augmenting the training datasets to cover more entity names (Liu et al., 2021; Hu et al., 2022; S\u00e4lev\u00e4 and Lignos, 2022). However, data augmentation strategies, despite their effectiveness, often lead to a substantial increase of the training dataset size and the computational resources needed for training, especially when the entities to cover are...\"}"}
{"id": "emnlp-2024-main-914", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What is the plot of The Catcher in the Rye?\\n\\nOur method, KG-MT, features two main components: (i) a knowledge retriever, which retrieves the most relevant entities from a multilingual knowledge graph (see Section 3.1), to improve the translation. The retrieved entities are then integrated into the MT system in two ways: explicit knowledge integration, where the entity names are explicitly added to the source text (see Section 3.2), and implicit knowledge integration, where the entity embeddings are fused with the encoder hidden states (see Section 3.3).\\n\\nIn contrast with augmentation strategies based on synthetic data that aim to maximize entity coverage at model training time, our hypothesis is that MT systems do not need to memorize every possible entity name transliteration and transcreation for each source-target language pair to correctly translate a text that contains entities. Instead, the core idea that motivates our work is to leverage an external knowledge source to first retrieve the most relevant entities for an input text, and then generate the translation by incorporating the retrieved entity names in the target language. In fact, multilingual knowledge graphs, such as DBPedia (Auer et al., 2007), BabelNet (Navigli and Ponzetto, 2012; Navigli et al., 2021), and Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), provide a wealth of lexical and factual knowledge about millions of entities in many languages, including their names, aliases, and descriptions (Kaffee et al., 2023; Conia et al., 2023). Not only that, but such knowledge is also easier to edit and is frequently updated to reflect the latest changes in the real world. By leveraging a multilingual knowledge graph, the focus of our approach shifts from memorizing entity names to learning when and how to retrieve the most relevant entities for a given input text and integrate their names in the target language in an end-to-end fashion.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the most relevant entities about the source text from a knowledge graph (Section 3.1), and (ii) a knowledge-enhanced translator that generates the target text by incorporating the retrieved entity names (Section 3.2). To better model the interactions between the retrieved entities and the translation, we also introduce a method to fuse the representations of the two components (Section 3.3).\\n\\nFigure 1 provides an overview of KG-MT, which we describe in detail in the following sections.\\n\\n3.1 Retrieving Relevant Entities from Multilingual Knowledge Graphs\\n\\nGiven a source text \\\\( t = \\\\langle w_1, \\\\ldots, w_n \\\\rangle \\\\) in a source language \\\\( l_s \\\\), the objective of our knowledge retriever is to retrieve the top-\\\\( k \\\\) most relevant entities \\\\( E_t = \\\\{ e_1, \\\\ldots, e_k \\\\} \\\\) from a knowledge graph \\\\( G = E \\\\times R \\\\times E \\\\), where \\\\( E \\\\) is the set of entities and \\\\( R \\\\) is the set of relations in the multilingual knowledge graph.\\n\\nWe represent each entity \\\\( e_i \\\\) as a tuple \\\\( e_i = \\\\langle n_i, d_i \\\\rangle \\\\), where \\\\( n_i \\\\) is the primary name of the entity and \\\\( d_i \\\\) is its description. Including the description of an entity allows us to distinguish between homonyms, i.e., entities with the same name.\\n\\nWe define the relevance score \\\\( s(e_i, t) \\\\) of an entity \\\\( e_i \\\\) with respect to the source text \\\\( t \\\\) as the cosine similarity between the entity and the source text:\\n\\n\\\\[\\ns(e_i, t) = \\\\frac{e_i \\\\cdot t}{\\\\|e_i\\\\| \\\\|t\\\\|},\\n\\\\]\\n\\nwhere \\\\( e_i \\\\) is the embedding of the entity \\\\( e_i \\\\). We then retrieve the top-\\\\( k \\\\) most relevant entities for the source text \\\\( t \\\\) as follows:\\n\\n\\\\[\\nE_t = \\\\text{top}_k(\\\\{e_i \\\\in E | s(e_i, t)\\\\}).\\n\\\\]\\n\\nThe embedding \\\\( e_i \\\\) of an entity \\\\( e_i \\\\) is obtained from an encoder (Izacard et al., 2021), which we train contrastively to maximize the likelihood of retrieving a relevant entity \\\\( e^+ \\\\) and minimize the likelihood of retrieving an irrelevant entity \\\\( e^- \\\\) given \\\\( t \\\\) as the input query:\\n\\n\\\\[\\nL = -\\\\log \\\\exp(e^+ \\\\cdot t) \\\\exp(e^+ \\\\cdot t) + \\\\sum_{i=1}^{n} \\\\exp(e^-_i \\\\cdot t).\\n\\\\]\\n\\nWe use Wikidata as our reference multilingual knowledge graph in this work; however, our method is not limited to a specific knowledge graph and can be extended to other knowledge graphs with similar structure, such as BabelNet.\\n\\nImportantly, we also introduce a sampling strategy to mine hard negative examples, i.e., instead of randomly sampling in-batch negatives (Botha et al., 2020), we select \\\\( n \\\\) homonymous entities that have the same name as the relevant entity \\\\( e^+ \\\\) but are not relevant to the source text \\\\( t \\\\).\\n\\n3.2 Integrating Explicit Knowledge into a Machine Translation Model\\n\\nGiven the source text \\\\( t = \\\\langle w_1, \\\\ldots, w_n \\\\rangle \\\\) in a source language \\\\( l_s \\\\) and the entities \\\\( E_t \\\\) retrieved by our knowledge retriever, the objective of our knowledge-enhanced translator is to generate the target text \\\\( t' = \\\\langle w'_1, \\\\ldots, w'_m \\\\rangle \\\\) in a target language \\\\( l_t \\\\) by incorporating the entity names of \\\\( E_t \\\\) into the translation process. Therefore, instead of directly generating the target text \\\\( t' \\\\) from the source text \\\\( t \\\\), we first build a knowledge-enhanced source text \\\\( t^+_\\\\text{KG} \\\\) as follows:\\n\\n\\\\[\\nt^+_\\\\text{KG} = \\\\langle w_1, \\\\ldots, w_n, [\\\\text{KG}], n_1^s \\\\rightarrow n_1^t, \\\\ldots, n_k^s \\\\rightarrow n_k^t \\\\rangle,\\n\\\\]\\n\\nwhere \\\\([\\\\text{KG}]\\\\) is a special token that indicates the start of the entity name translations, \\\\( n_i^s \\\\) is the name of the entity \\\\( e_i \\\\) in the source language \\\\( l_s \\\\) and \\\\( n_i^t \\\\) is the name of the entity \\\\( e_i \\\\) in the target language \\\\( l_t \\\\), as provided by the multilingual knowledge graph. We then feed the knowledge-enhanced source text \\\\( t^+_\\\\text{KG} \\\\) to a standard sequence-to-sequence MT model to generate the target text \\\\( t' \\\\), in a similar vein to past work on guiding MT systems (Zhang et al., 2018; Bulte and Tezcan, 2019). Given the format of \\\\( t^+_\\\\text{KG} \\\\), the MT model is fine-tuned to learn how to generate the target text \\\\( t' \\\\) by also attending to the translation of the entity names \\\\( n_i^t \\\\). We refer to this method as explicit knowledge integration, as the translations of the relevant entities are explicitly provided in the input to the MT model.\\n\\n3.3 Integrating Implicit Knowledge into a Machine Translation Model\\n\\nAlthough the knowledge-enhanced translator can generate the target text \\\\( t' \\\\) by incorporating the entity names of \\\\( E_t \\\\), it does not take advantage of the representations of the retrieved entities \\\\( e_i \\\\) learned by the knowledge retriever. To overcome this limitation, we also propose a method to fuse the latent representations of the knowledge retriever and the knowledge-enhanced translator, which allows KG-MT to better model the interconnections between the retrieved entities and the generated translation.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, we assume that an MT model is structured as an encoder-decoder architecture, such as the Transformer model (Vaswani et al., 2017). Given a general encoder-decoder architecture, we feed the knowledge-enhanced source text \\\\( \\\\mathbf{t} + \\\\mathbf{KG} \\\\) to the encoder of the MT model to obtain the encoder hidden states \\\\( \\\\mathbf{h}^{+\\\\mathbf{KG}} = \\\\langle h_1, \\\\ldots, h_{n+k+1} \\\\rangle \\\\), where \\\\( h_i \\\\) is the hidden state of the encoder at position \\\\( i \\\\).\\n\\nThen, we prepend the embeddings \\\\( \\\\mathbf{e}_i \\\\) of the retrieved entities to the encoder hidden states \\\\( \\\\mathbf{h}^{+\\\\mathbf{KG}} \\\\) to obtain the encoder hidden states \\\\( \\\\mathbf{h}^{+\\\\mathbf{KG}+\\\\mathbf{E}} \\\\):\\n\\n\\\\[\\n\\\\mathbf{h}^{+\\\\mathbf{KG}+\\\\mathbf{E}} = \\\\langle \\\\mathbf{e}_1, \\\\ldots, \\\\mathbf{e}_k, \\\\mathbf{h}_1, \\\\ldots, \\\\mathbf{h}_{n+k+1} \\\\rangle.\\n\\\\]\\n\\nFinally, we feed the hidden states \\\\( \\\\mathbf{h}^{+\\\\mathbf{KG}+\\\\mathbf{E}} \\\\) to the decoder of the MT model, which is now able to also attend to the entity embeddings from the retriever and fuse them with its hidden states. Our intuition is that the embeddings \\\\( \\\\mathbf{e}_i \\\\) from the knowledge retriever can contain useful fine-grained, latent information about the retrieved entities. We refer to this method as implicit knowledge integration, using an embedding-based fusion strategy reminiscent of Fusion-in-Decoder (Izacard and Grave, 2021a,b, FiD). However, unlike FiD, our knowledge-enhanced translator fuses the hidden states of two different encoders, i.e., the knowledge retriever and the encoder of the knowledge-enhanced translator, as shown in Figure 1.\\n\\n### 4 Evaluating Cross-Cultural Translation of Texts Containing Entity Names\\n\\nTo evaluate the effectiveness of our method, we introduce Cross-Culture Translate (XC-Translate), the first large-scale, manually-curated benchmark for the task of cross-cultural translation of texts containing entity names. XC-Translate is composed of parallel texts in 10 English-to-X language pairs from a diverse set of languages, including both high-resource and low-resource languages, namely, Arabic, Chinese, French, German, Italian, Japanese, Korean, Spanish, Thai, and Turkish. We highlight that this design choice allows our benchmark to feature languages with diverse scripts, some of which are similar to English, such as French and Spanish, and others that are very different, such as Arabic, Chinese, and Thai. Importantly, our benchmark is:\\n\\n- **Challenging:** XC-Translate is the first benchmark to focus on cross-cultural translation of texts containing entity names, which is particularly challenging due to the cultural-specific references of entity names across languages;\\n- **Large-scale:** XC-Translate contains about 5,000 sentences for each language pair for a total of over 58,000 instances, making it one of the largest benchmarks for MT, independently of its focus on cross-cultural translation;\\n- **Multi-reference:** XC-Translate provides multiple translations for each source text (over 100,000 references, or 2 translations per sentence on average);\\n- **Gold-quality:** XC-Translate is manually created and verified by human annotators fluent in the source and target languages, which ensures the quality of the benchmark and the correctness of the translations.\\n\\nWe believe that XC-Translate will be a valuable resource for the MT research community and will encourage further research on the problem of cross-cultural translation of texts containing entity names.\\n\\n#### 4.1 Design Principles\\n\\nThe creation process of XC-Translate is mainly driven by two design principles: (i) the texts should contain entity names that are likely to be affected not only by transliteration between languages, and (ii) the heart of the challenge should be the translation of the entity names, rather than the translation of the rest of the text, i.e., the text should not be too complex to translate if the entity names are translated correctly.\\n\\nTo satisfy the first design principle, we first identify for each language pair a set of entities from Wikidata that adhere to the following two main criteria: (a) the entity has at least one name in English and one name in the target language, (b) the English name of the entity is at least 50% different from the names in French, German, Italian, Spanish, and their word-for-word translation to English, as measured by the Levenshtein distance. The rationale behind the second criterion is that such a difference in the entity names across languages that mostly share the same script is likely an indicator of a name dissimilarity that goes beyond transliteration. For example, the Italian name of the entity \\\"The Catcher in the Rye\\\" is \\\"Il Giovane Holden\\\", while its French name is \\\"L'Attrape-c\u0153urs\\\".\"}"}
{"id": "emnlp-2024-main-914", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To satisfy the second design principle, we ask a group of human annotators to curate a set of short knowledge-seeking questions \u2013 less than 25 words \u2013 in English about the identified entities. Requiring the question to be short encourages simple and concise questions that are easy to translate if the entity names are correctly translated. Moreover, requiring the text to be a question mitigates the risk of including inaccurate facts in the text: for example, \\\"Is *The Catcher in the Rye* a book by J. D. Salinger?\\\" is a legitimate question, while \\\"*The Catcher in the Rye* is a book by J. D. Salinger\\\" is a factual statement that may or may not be factually accurate.\\n\\n4.2 Translation Process\\n\\nHaving identified the entities of interest for each language pair and having created English questions about them, we produce the translations in each target language via a two-step process. First, we ask a group of human translators to translate the questions from English to the target language. Then, we ask a second group of human annotators to verify the correctness of the translations.\\n\\nThe entire process is guided by a set of instructions and guidelines that we provide to the annotators. Moreover, we require the annotators to be fluent in English, native speakers of the target language, and resident in a country where the target language is spoken. Before starting the translation process, we also require the annotators to pass an entrance test to further verify their language proficiency and their comprehension of the instructions and guidelines; otherwise, they are not allowed to participate in the annotation task. Finally, the annotators are periodically evaluated on a set of test questions: if they fail on them, they are excluded from the pool of annotators. Since each English question is formulated from a given entity, we can aid the translators by providing the entity name(s) from Wikidata in the target language as a hint (see Design Principle i.a in Section 4.1), the English and target language descriptions of the entity from Wikidata, and the English and target language Wikipedia pages of the entity, which are fundamental resources to understand the context and background of the entity of interest.\\n\\nAt the end of the process, each English question is translated into the target language by at least three different translators, and each translation is then verified by at least three different annotators, allowing us to retain only the translations that are agreed upon by the annotators. We provide more details about this process in the Appendix.\\n\\n4.3 Evaluation Metrics\\n\\nIt is well known that the evaluation of MT systems is challenging, as there is no single metric that can capture all the aspects of translation quality. For example, BLEU (Papineni et al., 2002) is a popular metric that measures the n-gram overlap between the generated translation and the reference translations, but it is long known not to correlate strongly with human judgments (Callison-Burch et al., 2006). More recently, the research community has proposed alternative metrics, such as BERTScore (Zhang et al., 2020) and COMET (Rei et al., 2020), that aim to capture more nuanced aspects of translation quality, such as semantic similarity and factual correctness. However, such learned metrics yield only a translation-level score, which is not easy to interpret (Perrella et al., 2024) and does not allow us to easily analyze the translation at the entity level.\\n\\nTo address the foregoing limitations, not only do we provide the translations of the questions in XC-Translate, but also the list of the valid translations of the entity names that are valid in the context of the considered text. Having a comprehensive list of manually-curated valid names allows us to introduce M-ETA (Manual Entity Translation Accuracy), a simple metric to easily measure the translation quality at the entity level. Differently from previous metrics that rely on automatically identifying and aligning entities (Hu et al., 2022), M-ETA directly checks whether an automatic translation contains one of the manually-curated names. More formally, given a translation $t'$ in a target language $l$ and a set of gold entities $\\\\hat{E}_{t'}$, we define the entity-level translation quality score $Q(t', \\\\hat{E}_{t'})$ as follows:\\n\\n$$Q(t', \\\\hat{E}_{t'}) = \\\\frac{1}{|\\\\hat{E}_{t'}|} \\\\sum_{e_i \\\\in \\\\hat{E}_{t'}} q(t', e_i),$$\\n\\n(6)\\n\\nwhere $q(t', e_i)$ is the entity-level translation quality score of the entity $e_i$ in the target text $t'$ and is defined as follows:\\n\\n$$q(t', e_i) = \\\\min\\\\left\\\\{1, \\\\sum_{n_t \\\\in \\\\mathcal{N}_{tei}} I(n_t \\\\in t')\\\\right\\\\},$$\\n\\n(7)\"}"}
{"id": "emnlp-2024-main-914", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $N_t \\\\neq e_i$ is the set of manually-curated names of the entity $e_i$ in the target language $l_t$ and $I(n_t \\\\in t')$ is an indicator function that is equal to 1 if the name $n_t$ of the entity $e_i$ is in the target text $t'$ and 0 otherwise.\\n\\n5 Experiments and Results\\n\\nIn this section, we first list the systems we consider in our main experiments, then describe the datasets used to train the MT systems, and finally report and discuss the results.\\n\\nSystems. We compare the following systems:\\n\\n\u2022 GPT-3, GPT-3.5, and GPT-4: among the most popular and best performing LLMs, which have shown strong translation performance in the literature;\\n\u2022 mBART-50, M2M-100, and NLLB-200: recent multilingual MT models that support translation from and to about 50, 100, and 200 different languages using a single model, respectively;\\n\u2022 KG-MT: our proposed approach, which leverages the information available in multilingual knowledge graphs for end-to-end retrieval-augmented translation.\\n\\nTo ensure a fair comparison, we fine-tune mBART-50, M2M-100, NLLB-200, and KG-MT using the same dataset. For more details about our experimental setup, please refer to the Appendix.\\n\\nDatasets. As mentioned in Sections 3.1 and 3.2, KG-MT requires two datasets for training: one for the knowledge retriever and one for the knowledge-enhanced translator. The training dataset for retrieval should contain instances of the form $\\\\langle t, e^+, e^- \\\\rangle$, where $t$ is a source text, $e^+$ is a relevant entity for $t$, and $e^-$ is an irrelevant entity for $t$. The training dataset for translation should contain instances of the form $\\\\langle t, t', \\\\hat{E}_t \\\\rangle$, where $t$ is a source text, $t'$ is a target text, and $\\\\hat{E}_t$ is the set of gold entities for $t$. To train the knowledge retriever, we use the training data from Mintaka (Sen et al., 2022), a recently proposed dataset for multilingual question answering in which each question is tagged with the entities that appear therein. Since the questions in Mintaka are manually translated, we can also use its gold translations to train the knowledge-enhanced translator.\\n\\nTable 1 shows the results of the systems averaged over all the language pairs of XC-Translate in terms of BLEU, COMET, and M-ETA. We can first observe that MT systems, such as mBART-50, M2M-100, and NLLB-200, as well as LLMs, such as GPT-3, GPT-3.5, and GPT-4, obtain unsatisfactory M-ETAs on XC-Translate, with NLLB-200 and GPT-4 achieving the highest average score of 17.9% and 25.3%, respectively. These results support two of our hypotheses: (i) translating texts that contains challenging entity names is particularly difficult, and simply trying to translate the original entity name is often not sufficient to produce a correct translation; and (ii) BLEU and COMET are not a reliable metrics to evaluate the translation quality in this setting. Indeed, a translation that is correct except for the entity names still receives high BLEU and COMET scores, even though the error in the translation of the entity name may completely alter the meaning and the intent of the entire translation.\\n\\nTable 1 also shows that KG-MT outperforms all the baselines by a large margin, with an average M-ETA score of 41.1% for its best variant, which is equivalent to a 129.1% and 62.5% relative improvement over the best MT system (NLLB-200) and the best LLM (GPT-4), respectively. Most notably, KG-MT is capable of closing the gap between NLLB-200 and GPT-4, outperforming an LLM that is supposedly 100 times larger in terms of number of parameters.\\n\\nThe jump in performance for KG-MT is consistent across different underlying MT models, i.e., we observe similar M-ETA scores (39.1%, 38.3%, and 41.1%) for the KG-MT variants independently of the MT model used.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance by language pair of the baselines and the variants of KG-MT on XC-Translate compared to the state-of-the-art multilingual MT systems and LLMs.\\n\\nWhether we use mBART-50, M2M-100, or NLLB-200 as the backbone for the knowledge-enhanced translation model in KG-MT. This trend empirically demonstrates that our method is able to retrieve relevant entities about the source text and integrate the information available in multilingual knowledge graphs to improve the output quality.\\n\\nIn general, we observe that the improvement in M-ETA is also consistent across different language pairs, as shown in Table 2.\\n\\nResults on WMT benchmarks. Having evaluated the performance of KG-MT on XC-Translate, we now turn our attention to WMT benchmarks to assess whether our method degrades the translation quality in general-purpose MT benchmarks. To this end, we evaluate the performance of KG-MT on the English-to-X test sets from WMT17, WMT18, WMT19, WMT20, and WMT21. Table 3 shows the results of KG-MT on the WMT benchmarks in terms of BLEU and COMET. As we can observe, KG-MT achieves competitive BLEU and COMET scores on the WMT benchmarks compared to the MT baselines, which suggests that our method does not degrade the quality of general-purpose translations. On the contrary, KG-MT generally achieves slightly improved BLEU and COMET scores compared to vanilla MT systems, e.g., KG-MT NLLB obtains an absolute improvement of 0.8 points in BLEU and 1.7 points in COMET over NLLB-200.\\n\\n6 Analysis and Discussion\\nIn this section, we analyze KG-MT, and discuss where we believe it may be improved in future work. We expand on this analysis in the Appendix.\\n\\nExplicit or implicit knowledge? Sections 3.2 and 3.3 introduce two methods to integrate the knowledge retrieved by the knowledge retriever into the knowledge-enhanced translator: explicit knowledge integration and implicit knowledge integration. The first method is more straightforward but it also increases the length of the source text, which is undesirable since most of the attention mechanisms in popular Transformer-based models are quadratic with respect to the input length. Instead, the second method requires intervening on the inner workings of the MT model, which is more complex and may not be always feasible. However, it is also more flexible since the input length of the decoder only grows with the number of retrieved entities, which can be controlled by a hyperparameter. To understand which method is more effective, we compare the performance of KG-MT when using explicit knowledge integration, implicit knowledge integration, and both. Figure 2 shows that not only both methods are effective, but they are also complementary, as the combination of the two methods yields the best results. We hypothesize that the injection of the entity embeddings with the implicit knowledge integration may also act as an indicator of whether the MT model should rely on its parametric memory for during the generation.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Results of KG-MT when using explicit or implicit knowledge integration, or both.\\n\\nKnowledge retrieval. The knowledge retriever plays a fundamental role in KG-MT. If the retrieval step collects wrong or unrelated entities, the knowledge-enhanced translator has to (i) be robust against noisy or irrelevant knowledge, and (ii) fall back on its parametric memory, which is often unreliable as shown by the results of the vanilla MT systems on XC-Translate in Tables 1 and 2. However, our analysis shows that our knowledge retriever achieves 85.9% and 92.1% hits@1 and hits@3, respectively, on XC-Translate, which suggests that the knowledge retriever is effective at retrieving relevant entities. Part of this success can be attributed to our fine-tuning strategy with hard negative mining, which allows the knowledge retriever to improve the hits@1 and hits@3 by 5.6% and 4.2%, respectively, compared to using mContriever (Izacard et al., 2021), a pretrained retriever. Given the good performance with hits@3, we set the knowledge retriever to retrieve at most three entities for each source text, which is a trade-off between retrieving more relevant entities and increasing the computational cost.\\n\\nGold knowledge. If knowledge retrieval is not a significant source of errors for KG-MT, then the knowledge integration step, in which the knowledge-enhanced translator has to learn how to effectively integrate the retrieved entities into the translation process, is likely to be the main bottleneck. To isolate the performance of the knowledge-enhanced translator from the performance of the knowledge retriever, we evaluate KG-MT when using gold knowledge instead of the entities retrieved by the knowledge retriever. Figure 3 shows that the performance of KG-MT increases only by 11.6% in terms of average M-ETA when using gold entities, indicating that the knowledge-enhanced translator is not always capable of using the gold knowledge. Therefore, we hypothesize that the primary area of gain for future work shall be on improving the knowledge integration step, e.g., by using more sophisticated fusion strategies for knowledge integration or creating better datasets for fine-tuning the MT model on the knowledge integration step.\\n\\n7 Conclusion and Future Work\\nIn this paper, we addressed the problem of cross-cultural translation of texts containing entity names. Our contributions are threefold: (i) we introduced KG-MT, a novel approach for retrieval-augmented translation that leverages the information available in multilingual knowledge graphs to improve the translation of texts containing entity names; (ii) we introduced XC-Translate, the first large-scale, manually-curated benchmark for the task of cross-cultural translation of texts containing entity names; and (iii) we conducted extensive experiments on XC-Translate and other existing benchmarks for MT, showing that KG-MT significantly outperforms the state of the art on XC-Translate while maintaining comparable results on general-purpose MT benchmarks. We believe that our contribution will encourage further research on the problems that arise when translating texts containing cultural references beyond entity names, such as idioms and metaphors, and that retrieval-augmented translation will be a valuable tool to address these challenges.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we discuss some of the main limitations of our work and how future research may be able to address them.\\n\\n**Language coverage.** XC-Translate contains a diverse set of languages, but it is clearly not exhaustive. Although the number of languages included in our benchmark is comparable with the number of languages studied every year in the WMT shared tasks, it is still a small fraction of the world's languages. While full coverage is likely infeasible, we still miss entire linguistic families, such as the Uralic, Dravidian, and Niger-Congo families, and many languages from the Indo-European family, such as Russian, Portuguese, and Hindi. Future work should consider expanding the coverage of XC-Translate to include more languages. Indeed, different languages may present different challenges for cross-cultural translation, and it is important to understand these differences to develop more robust and generalizable translation systems. Not only that, but another aspect that we do not consider in our work is the dialectal and regional variation within a language, which can also be a significant source of errors in translation. Our intuition is that, since current state-of-the-art MT systems and LLMs struggle in our setting, which mostly includes high- to medium-resource languages, they would struggle even more in low-resource languages and dialects.\\n\\n**Entity coverage and selection.** The entities in XC-Translate are selected from Wikidata, which is a large and diverse knowledge graph, but it is not complete. While our design principles (see Section 4.1) are aimed at selecting entities that are likely to be challenging for MT systems, our selection strategy can also be considered aggressive for several reasons: (i) we only consider entities that are linked to Wikipedia pages, which may exclude many entities that are relevant in a given culture; (ii) we only consider entities that have at least an English name, which may exclude many entities that are relevant in a given culture; and (iii) our selection strategy is based on our experience with using Wikidata and Wikipedia. Future work should consider more sophisticated strategies for selecting entities, such as considering language pairs that do not involve English and tuning the selection based on each language pair, rather than having a one-size-fits-all approach. Moreover, future work may also consider using other knowledge graphs, as Wikidata inherits the biases and errors of Wikipedia (and its editor demographics).\\n\\n**Translation quality.** Our evaluation of KG-MT is mostly based on the M-ETA metric, which is a simple metric that measures the translation quality at the entity level. While M-ETA is a useful metric to evaluate the performance of KG-MT, it is not a comprehensive metric to evaluate the translation quality of MT systems, i.e., it cannot be used alone to compare the performance of different systems. This is the reason why we also report the BLEU and COMET scores of KG-MT on XC-Translate and the WMT benchmarks. However, we acknowledge that BLEU and COMET are also not comprehensive metrics to evaluate the translation quality of MT systems. Future work may consider fine-tuning learned metrics on our XC-Translate annotations, which also include a list of manually-curated valid translations of the entity names that are valid in the context of the considered text.\\n\\n**Knowledge retrieval.** Our knowledge retriever is based on a retrieval model that retrieves at most three entities for each source text. While this design choice is based on a trade-off between retrieving more relevant entities and increasing the computational cost, it is not clear whether this is the best design choice when using KG-MT on other types of texts, e.g., long documents where the number of entities may be significantly higher.\\n\\n**Comparison systems.** Our comparison systems are based on the state of the art in MT and LLMs, but they are not necessarily the best systems for the task of cross-cultural translation. We use mBART-50, M2M-100, and NLLB-200 as the backbone for the knowledge-enhanced translator in KG-MT, as they are widely used, have shown strong performance in the literature, and are available for fine-tuning. Another advantage is that they are also multilingual, which allows us to use the same model for all the language pairs in XC-Translate. Moreover, we mainly considered the GPT family of LLMs, as they are among the most popular and best performing LLMs, having also shown strong translation performance in the literature. However, future work may consider using openly-available LLMs. In this work, we have focused on the retrieval-augmented translation approach for MT systems, but future work may consider similar approaches for openly available LLMs, such as LLama and Mistral.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThe majority of this work has been carried out while Simone Conia and Daniel Lee were at Apple: we would like to thank all the people at Apple who provided their feedback on this work and participated in many helpful conversations. Simone Conia gratefully acknowledges the support of the PNRR MUR project PE0000013-FAIR, which fully funds his fellowship since October 2023.\\n\\nReferences\\n\\nYaser Al-Onaizan and Kevin Knight. 2002a. Named entity translation: extended abstract. In Proceedings of the Second International Conference on Human Language Technology Research, HLT '02, page 122\u2013124, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc.\\n\\nYaser Al-Onaizan and Kevin Knight. 2002b. Translating named entities using monolingual and bilingual resources. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 400\u2013408, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\\n\\nS\u00f6ren Auer, Christian Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak, and Zachary Ives. 2007. DBpedia: A nucleus for a web of open data. In The Semantic Web, pages 722\u2013735, Berlin, Heidelberg. Springer Berlin Heidelberg.\\n\\nJan A. Botha, Zifei Shan, and Daniel Gillick. 2020. Entity Linking in 100 Languages. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7833\u20137845, Online. Association for Computational Linguistics.\\n\\nBram Bulte and Arda Tezcan. 2019. Neural fuzzy repair: Integrating fuzzy matches into neural machine translation. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1800\u20131809, Florence, Italy. Association for Computational Linguistics.\\n\\nChris Callison-Burch, Miles Osborne, and Philipp Koehn. 2006. Re-evaluating the role of Bleu in machine translation research. In 11th Conference of the European Chapter of the Association for Computational Linguistics, pages 249\u2013256, Trento, Italy. Association for Computational Linguistics.\\n\\nNiccol\u00f2 Campolungo, Tommaso Pasini, Denis Emelin, and Roberto Navigli. 2022. Reducing disambiguation biases in NMT by leveraging explicit word sense information. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4824\u20134838, Seattle, United States. Association for Computational Linguistics.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nMarta R Costa-juss\u00e0, James Cross, Onur \u00c7elebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, et al. 2022. No language left behind: Scaling human-centered machine translation. arXiv preprint arXiv:2207.04672.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMar D\u00edaz-Mill\u00f3n and Mar\u00eda Dolores Olvera-Lobo. 2023. Towards a definition of transcreation: a systematic literature review. Perspectives, 31(2):347\u2013364.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vishrav Chaudhary, Naman Goyal, Tom Birch, Vitaliy Liptchinsky, Sergey Edunov, Michael Auli, and Armand Joulin. 2021. Beyond english-centric multilingual machine translation. J. Mach. Learn. Res., 22:107:1\u2013107:48.\\n\\nViviana Gaballo. 2012. Exploring the boundaries of transcreation in specialized translation. ESP Across Cultures, 9:95\u2013113.\\n\\nDaniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, Ruixiang Cui, Constanza Fierro, Katerina Margatina, Phillip Rust, and Anders S\u00f8gaard. 2022. Challenges and strategies in cross-cultural NLP. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6997\u20137013, Dublin, Ireland. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-914", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we provide in-depth details on the creation process of XC-Translate, our novel dataset for evaluating cross-cultural translation of texts containing entity names.\\n\\nA.1 Choice of languages\\n\\nAs mentioned in Section 4, 10 diverse languages are selected from a set of typologically-different linguistic families:\\n\\n- West Germanic: German\\n- Romance: Spanish, French, Italian\\n- Semitic: Arabic\\n- Sino-Tibetan: Chinese (simplified)\\n- Altaic: Turkish\\n- Koreanic: Korean\\n- Japonic: Japanese\\n- Tai: Thai\\n\\nThe architectural decision renders XC-Translate a complex challenge, given the variability or consistency in the symbol sets across different languages. For instance, the spelling of a person's name might remain unchanged between English and French, yet it's highly improbable for it to be identical in English and Chinese, necessitating at least a transliteration. Furthermore, the act of transliterating between English and Korean (as well as other languages, like Japanese) is fraught with unpredictability, complicating the reliance on rule-based methods for name translation between these linguistically divergent languages. Our investigation prioritized languages classified as high/medium-resource, following the quantitative evaluation in Conia et al. (2023) indicates that the representation of textual data is substantially lacking, even for the most recognized entities (top-10%) within those high/medium-resource languages. The extension of our benchmark to encompass lower-resource languages is earmarked for subsequent endeavors.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2 Human annotation process.\\n\\nThe objective of the annotation process was to (i) translate the question from English to the target language and (ii) verify the quality of the translations. This was completed through the following tasks:\\n\\nA.2.1 Translating text from English to the Target Language.\\n\\nFirst, given the entity, the human annotators were asked to familiarize themselves with its information. Through the user interface (IU) the following details were provided: (i) entity names/aliases, (ii) short description for the given entity retrieved from Wikidata, and (iii) a built-in panel which displayed the Wikipedia article in the English and target local, if available. By imbedding the relevant details in the UI, annotators were able to familiarize themselves with the entity without leaving the created tool. This is shown in Figure 4.\\n\\nAfter learning about the entity, the annotators were tasked with understanding the task with a set of in-depth instructions in a separate guideline. The guidelines provided information about (i) task terminology, (ii) detailed information on translating text, (iii) tips and edge-cases on translating text, (iv) positive and negative examples of the translation task. The guidelines will be provided in the supplementary material.\\n\\nUpon thoroughly familiarizing themselves with the task and details, the human annotators were tasked with translating upwards of 4 questions which contained the corresponding entity in a text box as shown in Figure 5. For each question, the following information was provided by the UI: (i) English question and entity name, (ii) target language, (iii) entity name in the target language, (iv) possible translation generated by a different machine translation tool.\\n\\nNext, the human annotator was requested to reiterate the entity name in the target language in the following text box. The annotator was prompted to double check the validity of the entity name in the target language by using a Web search engine. By implementing this step, it forced the annotator to verify the validity segments of their translated text.\\n\\nUpon completing the free-form component, the annotator was asked binary questions if the entity name in the target language they used was in the task suggested list and if the possible template for the question was used.\\n\\nWe note that annotators could provide feedback in case they noticed an error or had a suggested improvement. The annotation task was completed over 7 different batches, over a duration of 2 months, with iterative improvements made to the task UI and guidelines based on the feedback provided by...\"}"}
{"id": "emnlp-2024-main-914", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.2.2 Verifying human-curated translations.\\n\\nUsing the data collected from the first task, a second group of human annotators were tasked with verifying the correctness of the translation. Similar to the previous task, human annotators were provided details within the task UI and on a separate guidelines, information to familiarize themselves with the task entity, and task instructions as shown in Figure 6.\\n\\nAfter, the human annotators were tasked with verifying the translation in the target language as shown in Figure 7. To do so, they were provided the (i) English entity name and question and (ii) target language entity name and question. With this information, they answered two questions, with their corresponding options:\\n\\nPart A\\n\\nIs the Entity Name translated correctly?\\n- Yes\\n  - The Entity Name was translated correctly. Meaning, the translated entity name can be used to refer to the English entity. If you read the English and Translated Entity Name separately, you WOULD KNOW they refer to the same entity.\\n- No\\n  - Meaning, the translated entity name can be used to refer to the English entity. If you read the English and Translated Entity Name separately, you WOULD NOT KNOW they refer to the same entity.\\n- Maybe\\n  - The translated entity name MAYBE have the same meaning. Basically, if I read the English and Translated Entity Name separately, I WOULD LIKELY understand the same thing. But, it could be interpreted differently.\\n\\nThe phrasing of the question in Part B was fine-tuned, to ensure annotators did not index on transliteration.\\n\\nPart B\\n\\nDoes the English Question and the Translated Question have the same meaning?\\n- Yes\\n  - The English Question and the Translated Question DO have the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD understand the same thing.\\n- No\\n  - The English Question and the Translated Question DO NOT have the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD NOT understand the same thing.\\n- Maybe\\n  - The English Question and the Translated Question MAYBE HAVE the same meaning. Basically, if I read the English Question and Translated Question separately, I WOULD LIKELY understand the same thing. But, it could be interpreted differently.\"}"}
{"id": "emnlp-2024-main-914", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"operation, and focused on the semantic meaning of\\nthe two questions.\\n\\nThe responses from the verification task, would\\nbe used to curate the final dataset, determining\\nwhich English questions and language pair would\\nbe accepted.\\n\\nA.3 Quality assurance and inter-annotator\\nagreement\\nTo ensure the production of high-quality results,\\neach annotator was required to clear a preliminary\\ntest before they could contribute to the annotation\\neffort. This test involved reviewing a comprehen-\\nsive guide that acquainted them with the concepts\\nof entities and knowledge graphs, detailed the task\\nand UI elements, and included several illustrative\\nexamples, followed by the accurate classification\\nof 25 entity names. Those who failed the entrance\\nexam were excluded from the actual annotation\\ntask (the 25 entities used in the test were not in-\\ncluded in the final dataset). The pass threshold was\\nset at 85%\\n\\nFor each specified language, we enlisted annota-\\ntors who had verified expertise in both English and\\nthe language in question, confirmed through inter-\\nviews and proof of residence within the relevant\\ncountry. Annotators received compensation based\\non the standard hourly rates for their region. On\\naverage, annotators allocated approximately one\\nminute to acquaint themselves with the task and an-\\nother minute per translation question, culminating\\nin an average of 3.5 minutes per task. Given that\\neach entity name was evaluated by three annotators,\\nthe cumulative human effort invested in the anno-\\ntation process amounted to 3 annotators \u00d7 (2800\\nentities \u00d7 60 seconds + 2800 entities \u00d7 3.2 ques-\\ntions \u00d7 50 seconds) / 60 minutes = approximately\\n5,133 hours.\\n\\nB XC-Translate: Examples\\nIn this section, we provide examples of the in-\\nstances in XC-Translate and their translations.\\n\\n\u2022 English: \u201cWho is the author of the science\\nfiction mystery-thriller novel called The Pe-\\nriferal?\u201d\\n\\n\u2022 Italian: \u201cChi \u00e8 l'autore del romanzo giallo-\\nthriller di fantascienza chiamato Inverso?\u201d\\n\\n\u2022 English: \u201cHow many seasons of Sweet Mag-\\nnonias are available on Netflix?\u201d\\n\\n\u2022 Italian: \u201cQuante stagioni di Il colore delle\\nmagnolie sono disponibili su Netflix?\u201d\\n\\nC Experimental Setup\\nIn this section, we provide in-depth details on the\\nexperimental setup of our experiments, including\\nthe training of the knowledge retriever, the training\\nof the knowledge-enhanced translator, and the eval-\\nuation metrics, as well as the training details of the\\nbaselines.\\n\\nC.1 Hardware Infrastructure\\nThe experiments were conducted on a server with a\\nsingle NVIDIA V100 GPU, 32GB of RAM, and an\\n32-core CPU. The server runs Ubuntu 20.04 LTS\\nand is equipped with CUDA 12.\\n\\nTraining times.\\nThe training of the knowledge\\nretriever took approximately 4 hours to converge,\\nwhile the training of the knowledge-enhanced trans-\\nlator took approximately 6 hours to converge, de-\\npending on the underlying MT model, with M2M-\\n100 being the fastest and mBART-50 being the\\nslowest. This makes our approach feasible for\\ntraining on a single GPU and for short training\\ntimes contrary to synthetic data augmentation ap-\\nproaches that usually require multiple GPUs and/or\\nlong training times.\\n\\nC.2 Training of the Knowledge Retriever\\nThe knowledge retriever is trained using the train-\\ning data from Mintaka, a recently proposed dataset\\nfor multilingual question answering in which a sub-\\nset of the questions are tagged with the entities that\\nappear therein. The training data for the knowledge\\nretriever contains instances of the form\\n\u27e8t, e+, e\u2212\u27e9,\\nwhere t is a source text,\\ne+ is a relevant entity for t,\\nand e\u2212 is an irrelevant entity for t, mined from\\nWikidata using the hard negative mining strategy\\noutlined in Section 3.1.\\n\\nThe entire training dataset for Mintaka contains\\nabout 14,000 instances, which makes it a relatively\\nsmall dataset for training a retriever. To mitigate\\nthe risk of overfitting, we use a pretrained retriever,\\nmContriever, which is a retriever trained on a large-\\nscale multilingual dataset in a self-supervised way.\\n\\nC.3 Training of the Knowledge-Enhanced\\nTranslator\\nThe knowledge-enhanced translator is trained us-\\ning the training data from Mintaka too. The train-\\ning data for translation contains instances of the\"}"}
