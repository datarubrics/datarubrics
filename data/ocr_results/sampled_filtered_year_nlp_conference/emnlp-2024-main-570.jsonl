{"id": "emnlp-2024-main-570", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"issue more than halved our memory usage given a fixed batch size. We also found that ESPnet randomizes the batches across GPUs regardless of the sequence length. This means that one GPU may process a few utterances that are 30 seconds long, while another may process many utterances less than a second long. As the GPUs need to be synchronized during the backwards pass, this sequence length mismatch causes unnecessary waiting. By enforcing length-aware batch distribution, we are able to improve the model throughput by 60%.\\n\\nA.6 Experimental Setups\\n\\nThis section details the hyperparameters we searched for our experiments, which we then used to obtain and report the best performing results for each model.\\n\\nA.6.1 ML-SUPERB:\\nThe ML-SUPERB benchmark is designed to be a lightweight downstream probe of the multilingual representation quality of SSL models. The benchmark is split across two data settings: 10 minutes and 1 hour of data for each language. Each data setting has 4 tasks: monolingual ASR across 9 languages, multilingual ASR, LID, and joint multilingual ASR+LID. In the multilingual ASR tasks, 5 languages are reserved as few-shot task with 5 examples per language, while the remaining 138 languages have the standard 10 min. / 1 hour of fine-tuning data. An overall SUPERB score for each model \\\\( u \\\\) is calculated relative to the performance of the best performing model for each task \\\\( t \\\\) and filterbank-based features. This is obtained with the following formula:\\n\\n\\\\[\\n\\\\text{SUPERB}_s(u) = \\\\frac{1000}{T} \\\\sum_{t} \\\\sum_{m} s_{t,m}(u) - s_{t,m}(\\\\text{filterbank}) - s_{t,m}(\\\\text{SOTA}) - s_{t,m}(\\\\text{filterbank})(1)\\n\\\\]\\n\\nWhere \\\\( M_t \\\\) is the set of metrics for task \\\\( t \\\\), such that \\\\( s_{t,m}(u) \\\\) yields the score of model \\\\( u \\\\) on metric \\\\( m \\\\) in \\\\( M_t \\\\) for task \\\\( t \\\\). SOTA represents the best model for a given metric of each task. Since our model sets multiple SOTA results, we re-calculate this score for each model following Shi et al. (2023a,b).\\n\\nThe downstream probe is a 2-layer Transformer encoder trained using CTC (Graves et al., 2006) loss. It has a hidden size of 256, a feed forward size of 1024, and 8 attention heads. Each task has a fixed number of training steps and enforces a constant learning scheduler with the Adam (Kingma and Ba, 2015) optimizer. The only hyperparameter we adjust during the evaluation is the learning rate, testing values of 0.00004, 0.0001, and 0.0004.\\n\\nA.6.2 FLEURS:\\nWe adopt an identical setup to (Peng et al., 2023a) for the FLEURS evaluations, which remains the SOTA design when not using additional labeled data. The downstream model consists of a 12 layer E-Branchformer encoder paired with a 6 layer Transformer decoder. The SSL model remains frozen, and a weighted sum of its layer-wise inputs are input into the downstream encoder. Each encoder layer has a convolution kernel of 31, 8 attention heads, a hidden size of 512, and a feed forward size of 2048. Each decoder layer also has 8 attention heads, a hidden size of 512, and a feed forward size of 2048. The model is trained with the joint CTC/attention (Watanabe et al., 2017) objective with a CTC weight of 0.3. To better model the diversity in language, the encoder is trained with self-conditioned CTC (Nozaki and Komatsu, 2021; Chen et al., 2023c) using the LID+ASR labels. Inference is performed using joint CTC/attention decoding with a language model, using a beam size of 10, a CTC weight of 0.3, and language model weight of 0.4. The model is trained using the Adam optimizer with a Noam-style learning rate scheduler (Vaswani et al., 2017) and a peak learning rate of 0.002. We keep all of these hyperparameters consistent across each SSL model, only varying the batch size when memory limitations are encountered.\\n\\nA.6.3 JesusFilm ST:\\nWe collected the ST data from jesusfilm.org, which contains 2 hour long video dramas about the life of Jesus that are parallel in various languages. Each video contains multiple male and female speakers that appear throughout the drama. We downloaded the audio for Rajbanshi, Hijazi Arabic, and Lumun, while the ST labels are derived from the captions of the English audio. We process the data by splitting the 2-hour long videos sequentially, such that the first 70% of the movie is used as the training set, the next 15% is used as the development set, and the final 15% is used as the test set. We use this method instead of random splitting to minimize the potential content and speaker overlap between the data splits. We manually clean the test set by filtering out segments with non-speech descriptions such as laughter or...\"}"}
{"id": "emnlp-2024-main-570", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"background music cues. For reference, the XEUS pre-training data contains 7 hours of Rajbanshi, 3 hours of Hijazi Arabic, and 0.5 hours of Lumun.\\n\\nTo improve model convergence, we re-use the FLEURS models from Section A.6.2 and individually fine-tune them on ST for each language pair. We keep the fine-tuning parameters identical across all models, using a constant learning rate of 0.0001 with the Adam optimizer and a fixed batch size of 32. Inference is performed with a beam size of 10.\\n\\nA.6.4 Speech Resynthesis: Speech resynthesis experiments are performed on the VCTK dataset. For each SSL model we compare, we experiment with features extracted at 50% and 75% of the SSL model depth, which are standard settings used in discrete unit speech generation (Barrault et al., 2023a; Maiti et al., 2024). For the 19-layer XEUS model, this means we test layers 10 and 14. For the 24-layer WavLM and w2v-BERT 2.0 v2 models, we trial layers 12 and 18. The features are then clustered at the frame-level via K-means to obtain the discrete units. We use a fixed value of $K = 100$. We then train unit-to-speech HiFiGAN vocoders for each set of discrete units. Each vocoder is trained to generate 16 kHz speech for 150K steps. We use identical hyperparameters that correspond to the default VCTK settings in ParallelWaveGAN for each trial. We then select the best performing layer for each model based off of the MOSNet score of the resynthesized speech on the development set, and report its performance on the test set.\\n\\nA.6.5 SUPERB: The SUPERB Benchmark tests SSL models on a diverse selection of tasks. The SSL model is frozen and a learned weighted sum of its layer-wise inputs are fed into the respective downstream probe fine-tuned for each task. To limit the hyperparameter search space, we only tune the learning rate and batch size. Otherwise, we use the same settings as the original benchmark (wen Yang et al., 2021). For each task, the batch size is set to the maximum amount that can fit within a 40GB GPU. For the learning rate, we begin with the settings used by Chen et al. (2022) and conduct a sweep of those values multiplied by $[0.25, 0.5, 1.0, 2.0, 4.0]$. \\n\\n\"}"}
{"id": "emnlp-2024-main-570", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Distribution of data between the 189 language families in the XEUS pre-training data. We use Glottolog (https://glottolog.org/) to automatically map each ISO3 code to a language family.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset                           | License          | Language(s) | Domain | Hours |\\n|----------------------------------|------------------|-------------|--------|-------|\\n| YODAS (Li et al., 2023)          | CC BY 3.0        | 140         | Variety| 422K  |\\n| VoxPopuli (Wang et al., 2021)    | CC BY-NC 4.0     | 23          | Legal  | 400K  |\\n| LibriLight (Kahn et al., 2020)   | MIT              | English     | Audiobook | 60K  |\\n| MLS (Pratap et al.)              | CC BY 4.0        | 8           | Audiobook | 44K  |\\n| People's Speech (Galvez et al., 2021) | CC-BY 4.0   | English     | Variety | 30K  |\\n| WeNetSpeech (Zhang et al., 2022) | CC BY 4.0/SA     | Mandarin    | Variety | 22K  |\\n| Russian Open STT (Slizhikova et al., 2020) | CC-BY-NC | Russian     | Variety | 20K  |\\n| NPTEL2020 (AI4Bharat, 2020)      | CC                 | Indian English | Talk     | 15K  |\\n| Reazonspeech (Yin et al., 2023)  | Apache 2.0       | Japanese    | Television | 15K  |\\n| Common Voice 13 (Ardila et al., 2020) | CC0-1.0        | English     | Read     | 13K  |\\n| GigaSpeech (Chen et al., 2021)   | Apache 2.0       | English     | Variety | 10K  |\\n| VoxLingua (Valk and Alum\u00e4e, 2021) | CC BY 4.0 | English     | Variety | 6800 |\\n| MMS-unlab v2                     | CC BY-NC 4.0     | 4,023       | Religious | 6700 |\\n| SPGI (O'Neill et al., 2021)      | -                | English     | Finance | 5000 |\\n| Fisher (Post et al., 2013)       | LDC              | English     | Conversation | 2000 |\\n| Googlei18n (Chen et al., 2023b)  | Varies           | 34          | Variety | 1328 |\\n| BABEL (IARPA)                    | IARPA Babel License | 17         | Conversation | 1000 |\\n| FLEURS (Conneau et al., 2022)    | CC BY 4.0        | 102         | News   | 1000 |\\n| KSponSpeech (Bang et al., 2020)  | MIT              | Korean      | Conversation | 970  |\\n| LibriSpeech (Panayotov et al., 2015) | CC BY 4.0  | English     | Audiobook | 960  |\\n| MagicData (Yang et al., 2022)    | CC BY-NC-ND 4.0  | Mandarin    | Conversation | 755  |\\n| mTEDx (Salesky et al., 2021)     | CC BY-NC-ND 4.0  | 8           | Talk   | 753  |\\n| Jesus Dramas                     | CC BY-NC 4.0     | 430         | Religious | 643  |\\n| Althingi (Helgad\u00f3ttir et al., 2019) | Apache 2.0 | Icelandic   | Legal  | 542  |\\n| TEDLIUM3 (Hernandez et al., 2018) | CC BY-NC-ND 3.0 | English     | Talk   | 500  |\\n| VoxForge (VoxForge)              | GPL              | 8           | Read   | 235  |\\n| AISHELL (Bu et al., 2017)        | Apache 2.0       | Mandarin    | Read   | 200  |\\n| SEAME (Lyu et al., 2010)         | LDC              | Codeswitching | Conversation | 192  |\\n| DAMP-MVP (Smule, 2019)           | Smule Research Data License | English | Singing | 150  |\\n| NorwegianParl. (Solberg and Ortiz, 2022) | CC0       | Norwegian | Legal  | 140  |\\n| AIDATATANG (aid)                 | CC BY-NC-ND 4.0  | Mandarin    | Read   | 140  |\\n| AMI (Carletta, 2007)             | CC BY 4.0        | English     | Meetings | 100  |\\n| Nahuatl (Shi et al., 2021a)      | CC BY-NC-SA 3.0  | Nahuatl     | Conversation | 82   |\\n| WSJ (Paul and Baker, 1992)       | LDC              | English     | Read   | 81   |\\n| Mixtec (Shi et al., 2021b)       | CC BY-NC-SA 3.0  | Mixtec      | Conversation | 70   |\\n| WikiTongues                      | CC BY-NC 4.0     | 700         | Conversation | 70   |\\n| Siminchik (Cardenas et al., 2018) | CC BY-NC-ND 3.0 | Quechua     | Radio  | 50   |\\n| Edinburgh Accent (Sanabria et al., 2023) | CC-BY-SA | Accented English | Conversation | 40   |\\n| VCTK (Yamagishi et al., 2019)    | CC BY 4.0        | English     | Read   | 25   |\\n| AccentDB (Ahamad et al., 2020)   | CC BY-NC 4.0     | Indian English | Read | 20   |\\n| Totonac (Berrebbi et al., 2022)  | CC BY-NC-SA 3.0  | Totonac     | Monologue | 17   |\"}"}
{"id": "emnlp-2024-main-570", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSelf-supervised learning (SSL) has helped extend speech technologies to more languages by reducing the need for labeled data. However, models are still far from supporting the world's 7000+ languages. We propose XEUS, a Cross-lingual Encoder for Universal Speech, trained on over 1 million hours of data across 4057 languages, extending the language coverage of SSL models 4-fold. We combine 1 million hours of speech from existing publicly accessible corpora with a newly created corpus of 7400+ hours from 4057 languages, which will be publicly released. To handle the diverse conditions of multilingual speech data, we augment the typical SSL masked prediction approach with a novel dereverberation objective, increasing robustness. We evaluate XEUS on several benchmarks, and show that it consistently outperforms or achieves comparable results to state-of-the-art (SOTA) SSL models across a variety of tasks. XEUS sets a new SOTA on the ML-SUPERB benchmark: it outperforms MMS 1B and w2v-BERT 2.0 v2 by 0.8% and 4.4% respectively, despite having less parameters or pre-training data. Checkpoints, code, and data are found in https://www.wavelab.org/activities/2024/xeus/.\\n\\n1 Introduction\\n\\nOur planet is home to over 7000 languages (Austin and Sallabank, 2011), yet most speech processing models are only capable of serving at most 100-150 of them (Barrault et al., 2023b; Radford et al., 2023). The biggest constraint in supporting more of languages is the lack of transcribed speech: only around half of the world's languages have a formal writing system (Ethnologue, 2017), and even fewer of them have the resources to support the scale of annotated data required for training neural models. A common approach to address this limitation is self-supervised learning (SSL) on large amounts of unlabeled multilingual speech (Zhang et al., 2023; Black, 2019; Li et al., 2022), which allows for strong downstream performance even when access to annotated data is limited.\\n\\nWhile SSL models have more relaxed data requirements relative to supervised models, few works have fully exploited this aspect to scale models to more languages. In fact, most multilingual SSL models remain in the 50-150 language range of coverage (Babu et al., 2022; Chen et al., 2023b; Chiu et al., 2022), reducing the benefits of this advantage. The MMS project (Pratap et al., 2023) sought to address this issue by directly crawling data for more languages, scaling SSL pre-training to 1,000+ languages. The authors collected speech across 1,406 languages and used it to train an SSL model, showing state-of-the-art (SOTA) results after fine-tuning on multilingual automatic speech recognition (ASR) and 3,900-way language identification (LID). While the MMS models were publicly released, the crawled data was not, preventing it from being used in future work and thus the models from being reproduced (Table 1).\\n\\nAn additional issue that is relatively unexplored in SSL research is robustness to noisy data. This aspect is important for multilingual models, since the available recordings of low-resource languages tend to be particularly noisy (Ardila et al., 2020). The issue is exacerbated by the fact that existing multilingual SSL corpora lack diversity not only in languages but also in speaking style and recording conditions. WavLM (Chen et al., 2022) and WavLabLM (Chen et al., 2023b) tackled this issue by simulating noisy conditions during training, overlapping utterances or adding acoustic noise to simulate multi-speaker and noisy environments respectively. While effective, we believe that this technique can be improved to cover an even wider variety of recording conditions.\\n\\nOur goal is thus to build a universal speech encoder that can handle both linguistically and acoustically diverse speech. To achieve this, we...\"}"}
{"id": "emnlp-2024-main-570", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"propose XEUS (pronounced Zeus) \u2014 a Cross-lingual Encoder for Universal Speech. XEUS is an E-Branchformer (Kim et al., 2023) encoder pre-trained on over 1 million hours of publicly available data across a wide variety of recording conditions. We first curate the data from 37 existing corpora to ensure a diverse selection of speech and recording conditions not often found in standard ASR datasets, including but not limited to spontaneous speech, accented speech, code-switching, indigenous languages, and singing voices. We expand the language coverage of XEUS by introducing a new SSL corpus that uses data sources previously unseen in speech literature. This corpus, which will be publicly released, contains 7,413 hours of unlabeled audio across 4,057 languages, the widest coverage of any speech processing dataset. To enhance the model's robustness, XEUS is also pre-trained with a novel SSL objective of acoustic dereverberation, which requires the model to predict clean discrete phonetic pseudo-labels from simulated reverberant audio. By combining this objective with HuBERT-style masked prediction (Hsu et al., 2021) and WavLM-style denoising (Chen et al., 2022), XEUS is designed to be the next step towards a truly universal speech encoder for any language or recording condition. In our downstream evaluations, we find that XEUS consistently improves over SOTA SSL models across a wide variety of tasks. XEUS sets a new SOTA on the ML-SUPERB multilingual ASR benchmark, outperforming SSL models such as MMS (Pratap et al., 2023) and w2v-BERT 2.0 v2 (Barrault et al., 2023b) while having fewer parameters or less training data. Our speech translation (ST) results show the effectiveness of XEUS' wide language coverage, even for languages with less than 10 hours of data in the pre-training corpus. We also explore XEUS' potential in generative tasks and show its superiority on speech resynthesis when compared to other SSL encoders. Finally, we evaluate XEUS' representations on a variety of tasks through the English-only SUPERB benchmark, where it sets a new SOTA on 4 tasks despite XEUS' focus on multilingual performance.\\n\\nTo conduct SSL pre-training at such scale, we had to make significant optimizations to existing speech processing toolkits. To encourage further SSL research and reproducibility, we will publicly release this code, along with the training configurations and checkpoints for XEUS. We also release all 200+ intermediate checkpoints and training logs obtained throughout the pre-training for further research in the training dynamics of large-scale multilingual SSL models.\\n\\nTo summarize, our main contributions are as follows:\\n\\n1. We publicly release a new corpus that contains 7,413 hours of unlabeled speech across 4,057 languages, 25+ times wider coverage than current public datasets (Shi et al., 2023a).\\n2. We introduce a new self-supervised task that improves model robustness by implicitly learning to clean reverberant audio.\\n3. We publicly release XEUS, a SSL speech encoder trained on over 1 million hours of data across 4,057 languages.\\n4. We evaluate XEUS on numerous downstream tasks, and show that it outperforms SOTA SSL models such as MMS (Pratap et al., 2023), w2v-BERT 2.0 v2 (Barrault et al., 2023b), and WavLM on tasks such as ASR, ST, and speech resynthesis.\\n\\n2 Motivation and Related Work\\n\\n2.1 Speech Representation Learning\\n\\nSSL has seen tremendous success in speech processing by having neural networks learn rich feature representations from large-scale unlabeled data (Baevski et al., 2020; Hsu et al., 2021; Chen et al., 2022), which can then be fine-tuned on various downstream tasks (Chen et al., 2023c; Zhang et al., 2023). Multilingual SSL is a natural extension of this technique (Conneau et al., 2021; Babu et al., 2022) and facilitates cross-lingual transfer learning at scale. However, almost no studies leverage this capability to scale multilingual SSL models to truly massively multilingual settings, with the exception of Meta's MMS capable of covering \u223c1,000+ languages (Pratap et al., 2023). However, MMS relies upon the older wav2vec 2.0 SSL objective, which has now been consistently outperformed by newer SSL objective (wen Yang et al., 2021; Hsu et al., 2021; Chiu et al., 2022). In our work, we scale to 4 times the language coverage of MMS while further boosting performance with more powerful model architectures (Kim et al., 2023) and training objectives (Chen et al., 2022).\"}"}
{"id": "emnlp-2024-main-570", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of multilingual SSL models by number of languages, training data size, and transparency. We define transparency in terms of the public availability of the data, model checkpoints (weights), training code and/or configurations, and the release of any other training artifacts (logs, intermediate checkpoints).\\n\\n| Model                                | Langs. | Hours | Transparency |\\n|--------------------------------------|--------|-------|--------------|\\n| XLS-R 128 (Babu et al., 2022)        | 128    | 436K  | \u2713 \u2713 \u2717 \u2717      |\\n| w2v-BERT 51 (Conneau et al., 2022)   | 51     | 429K  | \u2713 \u2717 \u2717 \u2717      |\\n| MR-HuBERT (Shi et al., 2024)         | 23     | 100K  | \u2713 \u2713 \u2713 \u2717      |\\n| WavLabLM (Chen et al., 2023b)        | 136    | 40K   | \u2713 \u2713 \u2713 \u2717      |\\n| MMS (Pratap et al., 2023)            | 1,406  | 491K  | \u2717 \u2713 \u2717 \u2717      |\\n| USM (Zhang et al., 2023)             | 300    | 12.5M | \u2717 \u2717 \u2717 \u2717      |\\n| w2v-BERT 2.0 v1 (Barrault et al., 2023a) | 143  | 1M    | \u2717 \u2713 \u2717 \u2717      |\\n| w2v-BERT 2.0 v2 (Barrault et al., 2023b) | 143  | 4.5M  | \u2717 \u2713 \u2717 \u2717      |\\n| XEUS (ours)                          | 4,057  | 1M    | \u2713 \u2713 \u2713 \u2713      |\\n\\n2.2 Robust Speech Representations\\n\\nAs most SSL speech encoders are trained solely on clean speech (Baevski et al., 2020; Hsu et al., 2021; Chung et al., 2021), they perform noticeably worse on noisy audio (Chang et al., 2021). A common approach to alleviate this issue is to perform continued pre-training of the SSL model in noisy conditions (Chang and Glass, 2023; Ng et al., 2023; Huang et al., 2023; Zhu et al., 2023). While computationally efficient, the performance of these methods is ultimately limited by the underlying SSL model. WavLM (Chen et al., 2022) solves this issue by introducing an implicit denoising task during SSL pre-training, where the model must predict clean phonetic pseudo-labels when given an utterance corrupted with acoustic noise. WavLabLM (Chen et al., 2023b) extends this approach to the multilingual setting. Unlike XEUS, however, neither model considers the impact of reverberation, and both are trained on much smaller corpora (40K-86K hours vs. 1+ million hours).\\n\\n2.3 Open Foundation Models\\n\\nState-of-the-art speech foundation models vary significantly in their degree of openness (Table 1). The best performing models like Whisper (Radford et al., 2023), Google USM (Zhang et al., 2023), w2v-BERT 2.0 v1 (Barrault et al., 2023a), and w2v-BERT 2.0 v2 (Barrault et al., 2023b) are all trained on fully closed data. Whisper and w2v-BERT 2.0 v1/v2 only report pre-training data quantity and the languages covered. The USM report includes much more information about their data sources, but the model checkpoints remain unreleased.\\n\\nSmaller scale multilingual speech models follow more open release practices. XLS-R 53 (Conneau et al., 2021) and XLS-R 128 (Babu et al., 2022) came with checkpoints and only use publicly accessible datasets but did not release training code. Similarly, MMS (Pratap et al., 2023) released checkpoints but did not release their training code and crawled data. WavLabLM (Chen et al., 2023b) and MR-HuBERT (Shi et al., 2024) released code and checkpoints but operated on a smaller scale. Software infrastructure remains a critical barrier to democratizing speech SSL research. While there is plenty of infrastructure for large-scale training of text-based models (Workshop et al., 2022;andonian et al., 2023; Liu et al., 2023; Groeneveld et al., 2024), no similar work has achieved speech pre-training at our scale. AR-HuBERT (Chen et al., 2023a) and the OWSM project (Peng et al., 2023b, 2024b,a) sought to reproduce SOTA speech models in an open manner but use more than 80% less data than our work. With XEUS, we release the entire pre-training framework. We only use publicly accessible datasets and release all of the additional pre-training data that we crawled. To facilitate research in the training dynamics of large-scale SSL models, we also release all model logs and are the first to also release all intermediate checkpoints. As we are the first to create an open SSL speech model at such data and model scale, we also release all of our heavily optimized training code.\\n\\n3 Data\\n\\n3.1 Existing Datasets\\n\\nWe begin by combining a large variety of pre-training data from 37 publicly accessible speech processing datasets across 150+ languages, which we follow Peng et al. (2023b) and include licensed data such as BABEL (IARPA) as part of this definition, as exact copies can be obtained, unlike that of closed/unreleased data.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Distribution of XEUS pre-training data by language (log scale). We exclude data from YODAS (Li et al., 2023) due to the noisiness of the language labels.\\n\\nDetails about these datasets can be found in Section A.1 in the Appendix, while an overview is presented in Table 2. Notable data used in this work that was unseen in prior SSL models includes accented speech (Ahamad et al., 2020; Sanabria et al., 2023), code-switching (Lyu et al., 2010), and singing voices (Smule, 2019). To prevent data corruption, we use only the official training splits of each dataset. To increase language coverage beyond these 150 languages, in the next subsections we describe our collection of three additional data sources.\\n\\n3.2 MMS-unlab v2\\n\\nWe first reproduce the MMS-unlab dataset (Pratap et al., 2023), which was not publicly released, and scale it to 200 more languages. Like the original, we crawl religious audiobooks from the Global Recordings Network.\\n\\nOur data, however, is processed in a different manner. Since we use it for SSL instead of language identification, we do not filter out languages with low amounts of data. We also perform VAD with an energy-based detector instead of a neural model, the latter of which is more computationally expensive and likely less robust to unseen languages. This leads to a total of 6,700 hours of data across 4,023 ISO3 languages, which is wider in coverage than the original with 3,809 languages. We obtain explicit written permission for the use and redistribution of this data, as the Global Recordings Network website did not include clear licensing information.\\n\\n3.3 WikiTongues\\n\\nWe also create new unlabeled speech corpora by crawling data from 2 data sources previously unseen in the speech processing literature. The first is WikiTongues, based on a grassroots project that collected recordings of many languages and dialects spoken around the world with the goal of language preservation. Each 2-20 minute recording is released under the CC-BY-NC license, and contains 1-2 speakers casually speaking a particular language/dialect. Importantly, many of these languages are low-resource, if not endangered or extinct. In total, the dataset we obtain contains around 700 languages and/or dialects. We obtained 821 recordings, yielding about 70 hours of speech data. We use the same VAD settings as in Section 3.2 to segment each recording.\\n\\n3.4 Jesus Dramas\\n\\nThe second corpus we collect is Jesus Dramas. The source of this data is Inspirational Films, which released the \u201cStory of Jesus\u201d audio drama in 430 languages under a non-commercial license. Each multi-speaker audio drama is 90 minutes long, totalling 645 hours. We use the same VAD settings as in Section 3.2 to segment these dramas into utterance-level clips.\\n\\n3.5 Final Pre-Training Corpus\\n\\nThe new datasets we collect from Sections 3.2, 3.3 and 3.4 total 7,413 hours of data across 4,057 ISO3 languages. After aggregating it with the data from Section 3.1, we obtain a total of 1.081 million hours of pre-training data. We filter out all utterances longer than 40 seconds due to memory constraints. An overview of all of our pre-training corpora with their licensing information is presented in Table 11 in the Appendix. Figure 1 shows an overview of the language distribution in our data on a log-scale. Overall, our pre-training dataset spans 189 languages.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Overview of datasets used for pre-training.\\n\\n| Dataset                        | Language(s) | Domain    | Hours |\\n|-------------------------------|-------------|-----------|-------|\\n| YODAS (Li et al., 2023)       |             | Variety   | 140   |\\n| 422KV oxPopuli (Wang et al., 2021) |             | Legal     | 23    |\\n| 400K LibriLight (Kahn et al., 2020) | English   | Audiobook | 60K   |\\n| 8K MLS (Pratap et al.)        |             | Audiobook | 44K   |\\n| People's Speech (Galvez et al., 2021) | English   | Variety   | 30K   |\\n| 30K WeNetSpeech (Zhang et al., 2022) | Mandarin  | Variety   | 22K   |\\n| 20K Russian Open STT (Slizhikova et al., 2020) | Russian   | Variety   | 20K   |\\n| NTEL2020 (AI4Bharat, 2020)   |             | Talk      | 15K   |\\n| Reazonspeech (Yin et al., 2023) | Japanese   | Television | 15K   |\\n| Common Voice 13 (Ardila et al., 2020) | English   | Read      | 13K   |\\n| GigaSpeech (Chen et al., 2021) |             | Variety   | 10K   |\\n| 107 MMS-unlab v2             |             | Religious | 4023  |\\n| 4023 SPGI (O'Neill et al., 2021) | English   | Finance   | 5000  |\\n| 34 Googlei18n (Chen et al., 2023b) | Variety   | Variety   | 1328  |\\n| 17 BABEL (IARPA)             |             | Conversation | 1000  |\\n| 1000 FLEURS (Conneau et al., 2022) | English   | News      | 1000  |\\n| 970 LibriSpeech (Panayotov et al., 2015) | English   | Audiobook | 960   |\\n| 8 MagicData (Yang et al., 2022) | Mandarin   | Conversation | 753   |\\n| 500 VATForge (VoxForge)       |             | Read      | 235   |\\n| 200 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, 2007)     |             | Meetings  | 140   |\\n| 100 Nahuatl (Shi et al., 2021a) | Nahuatl    | Conversation | 82   |\\n| 5000 Mixtec (Shi et al., 2021b) | Mixtec     | Conversation | 70   |\\n| 700 WikiTongues              |             | Conversation | 700   |\\n| 70 Siminchik (Cardenas et al., 2018) | Quechua | Radio      | 70    |\\n| 430 Jesus Dramas             |             | Religious | 430   |\\n| 643 Althingi (Helgad\u00f3ttir et al., 2019) | Icelandic | Legal     | 542   |\\n| TEDLIUM3 (Hernandez et al., 2018) | English   | Talk      | 500   |\\n| 235 AISHELL (Bu et al., 2017) |             | Read      | 200   |\\n| 200 SEAME (Lyu et al., 2010) |             | Codeswitch | 192   |\\n| 150 Norwegian Parl. (Solberg and Ortiz, 2022) | Norwegian | Legal     | 140   |\\n| 140 AIDATATANG (aid)         |             | Read      | 140   |\\n| 140 AMI (Carletta, "}
{"id": "emnlp-2024-main-570", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al. (2017). We first estimate $dt$, the sample shift imposed on $u$ after adding the reverberation, according to the highest peak in $u_n$. The reverberant utterance $r$ can then be simulated via convolution ($\\\\ast$) between $u$ and $u_n$. Finally, we realign $r$ with $u$ using $dt$ and normalize it to have the same energy as $u$. This final realignment step is crucial for the effectiveness of this technique; otherwise the audio would be shifted and misaligned with the target pseudo-labels. Quantitative analyses of our method can be found in Appendix Section A.3.\\n\\n4.3 Model Architecture\\n\\nXEUS is based on the HuBERT architecture (Hsu et al., 2021), with several modifications. We replace the Transformer (Vaswani et al., 2017) with an E-Branchformer (Peng et al., 2022; Kim et al., 2023), as convolution-augmented models achieve superior SSL performance (Chung et al., 2021). We choose the E-Branchformer over the Conformer (Gulati et al., 2020) due to the former's relative ease of training and superior downstream performance (Peng et al., 2023a). We also replace the original HuBERT loss with cross entropy, which is faster and leads to better downstream performance (Yang et al., 2023). XEUS consists of a convolutional feature extractor and 19 E-Branchformer layers. Each of the latter has 8 attention heads, a hidden dimension of 1,024, feed-forward size of 4,096, and kernel size of 31. The model size is 577M parameters. Ablations on these modifications can be found in Appendix Section A.2.\\n\\n4.4 Pre-Training Settings\\n\\nXEUS is pre-trained on 64 40GB NVIDIA A100 GPUs using the ESPnet toolkit (Watanabe et al., 2018). Each GPU has a maximum batch size of 100 seconds, leading to a total batch size of 106 minutes. We use a noise augmentation probability $p$ of 0.2, where there is an equal probability of the corruption being random noise or another utterance (Section 4.1). We use a dereverberation augmentation probability $p_r$ of 0.3 (Section 4.2). We perform a two passes through the training set, totalling 670K steps. More details and a breakdown of the training costs can be found in Appendix Section A.4.\\n\\nThe scale of XEUS' pre-training is unseen outside of a few other works (Radford et al., 2023; Zhang et al., 2023; Barrault et al., 2023b), with the amount of pre-training data being 5-25 times the size of those used in prior models trained with public toolkits (Peng et al., 2023b; Chen et al., 2023a; Hsu et al., 2021). To conduct training on such a scale, we made several optimizations to the open-source ESPnet toolkit, which was originally designed for standard academic-scale experiments. These optimizations will all be made publicly available. More details about these improvements are also reported in Appendix Section A.5.\\n\\n5 Downstream Evaluation\\n\\nWe examine the capabilities of XEUS in various downstream applications. Section 5.1 evaluates the multilingual capabilities of XEUS in different settings. Section 5.2 evaluates the universality of XEUS' representations for a broad range of speech information, such as emotion and speaker content. Finally, Section 5.3 tests the acoustic representations of XEUS via speech resynthesis. We provide an overview of each downstream setup in its respective subsection. Detailed experimental settings can be found in Appendix Section A.6.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation results on the 10 minute / 1 hour settings of the ML-SUPERB Benchmark in ASR CER (\u2193) and LID ACC (\u2191). Bold numbers indicate the best model for a task, while underlined numbers indicate second best.\\n\\n| Model          | Model Params. | Hours  | ASR CER | ASR CER | LID ACC | ASR CER | ASR CER |\\n|----------------|---------------|--------|---------|---------|---------|---------|---------|\\n| WavLabLM       | 316M 40K      | 700 / 767 | 39.9 / 32.1 | 37.8 / 31.3 | 43.8 / 40.9 | 71.7 / 81.1 | 70.8 / 82.2 |\\n| XLS-R 128      | 316M 436K     | 707 / 851 | 39.7 / 30.6 | 29.3 / 22.0 | 40.9 / 39.3 | 66.9 / 87.9 | 55.6 / 85.6 |\\n| XLS-R 128      | 1B 436K       | 745 / 838 | 40.5 / 30.9 | 30.4 / 26.3 | 39.1 / 38.5 | 70.9 / 85.8 | 66.4 / 87.1 |\\n| MMS            | 316M 491K     | 795 / 845 | 33.8 / 30.5 | 28.7 / 24.0 | 36.5 / 36.5 | 62.3 / 84.3 | 71.9 / 74.3 |\\n| MMS            | 1B 491K       | 953 / 948 | 33.3 / 25.7 | 21.3 / 18.1 | 30.2 / 30.8 | 84.8 / 86.1 | 73.3 / 74.8 |\\n| w2v-BERT 2.0 v2| 580M 4.5M     | 826 / 916 | 41.0 / 29.2 | 24.6 / 20.3 | 34.3 / 35.3 | 70.0 / 86.8 | 83.2 / 90.6 |\\n| XEUS (ours)    | 577M 1M       | 956 / 956 | 30.3 / 25.1 | 21.1 / 20.1 | 33.4 / 34.1 | 81.5 / 87.3 | 86.4 / 91.3 |\\n\\nTable 4: FLEURS ASR+LID & JesusFilm ST results.\\n\\n| Model          | FLEURS CER | FLEURS chrF | JesusFilm CER | JesusFilm chrF |\\n|----------------|------------|-------------|---------------|---------------|\\n| XLS-R 1B       | 9.6        | 92.5        | 13.4          | -             |\\n| MMS 1B         | 9.2        | 94.0        | 15.5          | -             |\\n| w2v-BERT 2.0 v2| 8.7        | 94.3        | 15.1          | -             |\\n| XEUS (ours)    | 8.9        | 93.0        | 22.1          | -             |\\n\\n5.1 Multilingual Speech Processing\\n\\nWe primarily compare XEUS with 3 SOTA multilingual SSL models: XLS-R 128 (Babu et al., 2022), MMS (Pratap et al., 2023), and w2v-BERT 2.0 v2 (Barrault et al., 2023b) (Table 1). We use the ML-SUPERB benchmark (Shi et al., 2023a) as our main evaluation, as it tests each models across a diverse range of tasks and languages. We complement these experiments with additional analyses on FLEURS ASR+LID and low-resource ST.\\n\\n5.1.1 ML-SUPERB\\n\\nML-SUPERB benchmarks self-supervised speech representations on a variety of multilingual tasks across 143 languages. ASR performance is evaluated in terms of character error rate (CER \u2193), while accuracy (ACC \u2191) is used to evaluate LID. The benchmark is split across two data settings for each task: 10 minutes (min.) and 1 hour of data for each language. Each data setting contains 4 tasks: monolingual ASR in 9 languages, multilingual ASR, LID, and joint multilingual ASR+LID. In the multilingual tasks, 5 languages are reserved as a few-shot task, while the other 138 languages have the standard 10 min. / 1 hour of fine-tuning data. An overall SUPERB s (\u2191) score for each model in each data setting is calculated following the benchmark rules (Shi et al., 2023a). Further details can be found in Appendix Section A.6.1.\\n\\nTable 3 shows that XEUS is the overall best performing model, with the highest SUPERB s score of 956 on both the 10 min. / 1 hour settings. XEUS achieves SOTA results on monolingual ASR with the best scores of 25.1 and 33.3 CER on the 1 hour and 10 min. tracks respectively. On multilingual ASR, XEUS is only outperformed by MMS 1B. For ASR+LID, XEUS achieves the best performance in the normal setting for both data tracks. While XEUS is worse than MMS in few-shot CER, it still achieves reasonable results and outperforms the other SSL models. Overall, XEUS outperforms the parameter-equivalent w2v-BERT 2.0 v2 across all task categories. This is accomplished using only accessible training data, which is 22% the size than that of w2v-BERT 2.0 v2.\\n\\n5.1.2 FLEURS\\n\\nFLEURS is a 102-language multilingual ASR benchmark, where each language has around 6-10 hours of training data. In this setting, we use heavier downstream probes that reflect SOTA ASR architectures. We adopt the same setup as (Peng et al., 2023a; Chen et al., 2023c), which remains the SOTA on FLEURS when not using additional labeled data. The downstream model consists of an E-Branchformer encoder paired with a Transformer decoder, totalling 100M parameters. Exact settings are shown in Appendix Section A.6.2.\\n\\nThe results of the FLEURS experiments are shown in the middle section of Table 4. We find that XEUS remains competitive with the SOTA w2v-BERT 2.0 v2 trained on much more data (8.9 vs 8.7 CER), and significantly outperforms both XLS-R and MMS 1B (9.6 and 9.2 CER respectively).\\n\\n5.1.3 Low-Resource Language Coverage\\n\\nWhile FLEURS and ML-SUPERB provide comprehensive multilingual benchmarks, their language coverage is far smaller than that of XEUS (102-143 vs 4,057). To understand if XEUS' wide language coverage was effective for languages with small (< 10 hours) amounts of pre-training data, we crawled additional labeled data for evaluation. We randomly chose 3 languages from the Jesus Film...\"}"}
{"id": "emnlp-2024-main-570", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Evaluation on the SUPERB Benchmark. ( ), ( ), and ( ) indicate first, second and third best results respectively on the online leaderboard: https://superbbenchmark.org/leaderboard.\\n\\n| Method     | Recognition | Detection | Semantics | Speaker | Paralinguistics |\\n|------------|-------------|-----------|-----------|---------|-----------------|\\n| PR \u2193       | ASR \u2193       | KS \u2191      | QbE \u2191     | IC \u2191    | SF (F1) \u2191       |\\n| SF (CER) \u2193 | SID \u2191       | ASV \u2193     | ER \u2191      |         |                 |\\n\\nWavLM Large | 3.06 | 3.44 | 97.86 | 8.86 | 99.31 | 92.21 | 18.36 | 95.49 | 3.77 | 3.24 | 70.62 |\\n\\nXEUS (ours) | 3.21 | 3.34 | 98.32 | 7.49 | 98.70 | 90.05 | 21.49 | 91.70 | 4.16 | 3.11 | 71.08 |\\n\\nTable 6: Speech resynthesis results on VCTK.\\n\\n| Model       | MOS \u2191 | WER \u2193 | F0 \u2193 | MCD \u2193 |\\n|-------------|-------|-------|------|-------|\\n| WavLM Large | 3.20  | 27.8  | 0.26 | 4.55  |\\n| w2v-BERT 2.0 v2 | 2.0  | 15.5  | 0.27 | 3.92  |\\n| XEUS (ours) | 3.23  | 10.0  | 0.25 | 3.80  |\\n\\nProject 6 that were not covered in ML-SUPERB and/or FLEURS: Hijazi Arabic, Lumun, and Rajbanshi. This yields around 1.5 hours of speech for each language. Of all existing SSL models, only XEUS covers the former two, while Rajbanshi is covered by both XEUS and MMS. We then train X \u2192 English Speech Translation (ST) models for each language (as ASR transcripts were not available) and evaluate using character-level F-score (chrF). More details about the evaluation data and ST settings are shown in Appendix Section A.6.3.\\n\\nThe average chrF scores across all languages are shown in the right of Table 4. XEUS significantly outperforms all other models, likely due to its wider language coverage. The next best model is MMS 1B, which obtains an average chrF of 15.5, while XEUS scores 22.1, a relative improvement of 35%.\\n\\n5.2 Task Universality\\n\\nTo test how well XEUS encodes other forms of speech content, we benchmark its capabilities on the English-only SUPERB (wen Yang et al., 2021). SUPERB tests self-supervised speech models across 5 broad task categories: Recognition (ASR and Phoneme Recognition (PR)), Detection (Keyword Spotting (KS) and Query By Example (QbE)), Semantics (Intent Classification (IC) and Slot Filling (SF)), Speaker (Speaker Identification (SID), Automatic Speaker Verification (ASV), and Speaker Diarization (SD)), and Paralinguistics (Emotion Recognition (ER)). Metrics for each task are: phoneme error rate (PR), WER (ASR), maximum term weighted value (QbE), F1 and concept error rate (SF), equal error rate (ASV), diarization error rate (SD), and accuracy (KS, IC, SID, and ER). Exact experimental settings are shown in Appendix Section A.6.5. We compare XEUS to WavLM (Chen et al., 2022), the SOTA model on the SUPERB leaderboard for almost all tasks. Our results in Table 5 show that XEUS consistently reaches if not surpasses SOTA scores across a variety of tasks, obtaining the highest score in 4 English-only tasks (KS, SD, ER, ASR), despite its curse of multilinguality (Conneau et al., 2020).\\n\\n5.3 Acoustic Representation Evaluation\\n\\nWe evaluate XEUS on its acoustic representation quality through the task of speech resynthesis. Here, we compare primarily against w2v-BERT 2.0 v2 as the SOTA multilingual model and WavLM Large as the SOTA English-only model. We train unit-to-speech HiFiGAN vocoders (Kong et al., 2020; Polyak et al., 2021) on the accented-English VCTK (Yamagishi et al., 2019) dataset with a discrete codebook vocabulary size of 100. We evaluate the quality of the resynthesized speech in terms of Mel-Cepstral Distortion (MCD), log-F0 Root Mean Square Error (F0), predicted Mean Opinion Score (Lo et al., 2019) (MOSNet), and Word Error Rate (WER). We obtain WER by transcribing the synthesized speech with a pre-trained Whisper medium (Radford et al., 2023). For each SSL model, we experiment with features extracted at 50% and 75% of the model depth (ie. layer 18 out of 24 in the latter case), and report the best performing configuration in Table 6. More details about this search can be found in Appendix Section A.6. Our results show that resynthesized speech from XEUS is higher quality than that from both WavLM and w2v-BERT 2.0 v2 across all metrics, whether it be perceptual or semantic, showcasing its strong performance in generative tasks along with its SOTA recognition capabilities.\\n\\n6 Conclusion\\n\\nThis work presents XEUS, an SSL speech encoder trained on over 1 million hours of data across 4,057 languages. As a community contribution, we release a new dataset with 7,413 hours of unlabeled speech data across those 4,057 languages. We also introduce a novel joint dereverberation task for SSL pre-training, which we use to increase the robustness of XEUS. We show that XEUS can achieve...\"}"}
{"id": "emnlp-2024-main-570", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"comparable performance if not outperform other SOTA SSL models on various benchmarks, while having much stronger performance on long-tail languages. To make XEUS reproducible, we will release all training code and configurations, along with model weights. In the future, we hope to extend the downstream use of XEUS to a larger scale.\\n\\n7 Limitations:\\nWhile the overall pre-training corpus of XEUS contains over 4,000 languages, many of these languages have less than 1 hour of speech data. While we are able to show that the presence of this small amount of data is still beneficial for these languages in downstream tasks (Section 5.1.3), the performance is still likely much worse than the performance on higher-resourced languages. Furthermore, due to the efforts required to collect and manually clean evaluation data, we only test on a subset of these truly low-resource languages in Section 5.1.3. While XEUS is one step towards speech recognition or translation systems for these tail languages, much work is still required before these tools can be deployed to end users.\\n\\nDue to the large number of tasks and domains that our evaluation covers, we mostly focus on relatively lightweight benchmarks such as the SUPERB suite and perform limited hyperparameter tuning. While this allows for fair comparisons between different models, the evaluation does not use the large-scale fine-tuning common in SOTA settings for downstream tasks.\\n\\n8 Broader Impact and Ethics:\\nBroader Impact:\\nIn this work, we introduce XEUS, a new large-scale multilingual SSL speech encoder. Unlike previous foundation models that focus on a single aspect, XEUS obtains SOTA performance across a diverse range of both tasks and languages, further pushing towards the goal of truly universal models. By releasing both our data and model checkpoints, our goal is to provide foundations for more multilingual research, particularly in domains such as robust ASR and speech enhancement where evaluation is typically done solely on English.\\n\\nAnother major goal of our work is to democratize the development of speech foundation models. We believe that training infrastructure remains a significant barrier to entry for SSL research. This has two main aspects: software infrastructure and training configurations. Current speech toolkits such as ESPnet (Watanabe et al., 2018) and SpeechBrain (Ravanelli et al., 2021) focus on academic scale experiments, while general frameworks such as HuggingFace and Fairseq (Wang et al., 2020) are more limited in their implementation of different tasks and SOTA methods. By integrating our changes into ESPnet, our optimizations can allow users to scale speech models for other tasks such as speaker representation learning. In the latter aspect, we note that the availability of training recipes and configurations pose the other major barrier to entry. Due to the computational cost of training, the development of foundation models poses a risk that is too high for most academic labs, as a single failed training run can be disastrous for the lab\u2019s budget. However, this can be mitigated by publishing training recipes and hyperparameters known to work well. The benefits of this is most visible in the OWSM project (Peng et al., 2023b, 2024b,a), where each successive work reported lower and lower computational expenses.\\n\\nEthics:\\nWe recognize the delicate nature of speech data, particularly when it involves the languages of indigenous and marginalized communities. Many authors of this work have long-term collaborations with indigenous communities and researchers. We are careful to crawl and release data only from sources that contain permissive licenses of the source data to avoid potential cases of misuse and violations of language ownership. For data sources that do not clearly indicate their usage/distribution terms, we obtained explicit permission from the corresponding stakeholders (such as in the case of the Global Recordings Network in Section 3.2). To follow the data\u2019s access conditions, we release all of our data under non-commercial licenses. We partially anonymize our crawled datasets by removing speaker names from the meta-data. However, we do not alter the content of the speech itself. As such, we urge users of our released data to respect the privacy of the speakers and not attempt to identify them. It is also possible that portions of the speech content may be offensive in particular settings. With the diversity of over 4000 languages, it is likely that there are statements or views that are normative in one culture but offensive in another.\\n\\nEncoder-only speech models like XEUS have limited uses without any task-specific fine-tuning, the downstream models that are created after such processes are prone to the biases and misuse cases that all machine learning models are\"}"}
{"id": "emnlp-2024-main-570", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For example, XEUS' capabilities in speech generation can be used for misinformation via audio deepfakes, which is an unintended use case of this model.\\n\\nAcknowledgement\\n\\nThis work used the Bridges2 at PSC and Delta NCSA computing systems through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (AC-CESS) program, supported by National Science Foundation grants 2138259, 2138286, 2138307, 2137603, and 2138296.\\n\\nReferences\\n\\naidatatang_200zh, a free Chinese Mandarin speech corpus by Beijing DataTang Technology Co., Ltd.\\n\\nAfroz Ahamad, Ankit Anand, and Pranesh Bhargava. 2020. AccentDB: A database of non-native English accents to assist neural speech recognition. In LREC 2020.\\n\\nAI4Bharat. 2020. NPTEL2020: Indian English Speech Dataset.\\n\\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Th\u00e9rien, Phil Wang, and Samuel Weinbach. 2023. GPT-NeoX: Large Scale Autoregressive Language Modeling in PyTorch.\\n\\nRosana Ardila, Megan Branson, Kelly Davis, Michael Kohler, Josh Meyer, Michael Henretty, Reuben Morais, Lindsay Saunders, Francis Tyers, and Gregor Weber. 2020. Common voice: A massively-multilingual speech corpus. In LREC 2020, pages 4218\u20134222.\\n\\nPeter K. Austin and Julia Sallabank, editors. 2011. The Cambridge Handbook of Endangered Languages. Cambridge University Press.\\n\\nArun Babu, Changhan Wang, Andros Tjandra, Kushal Lakhotia, Qiantong Xu, Naman Goyal, Kritika Singh, Patrick von Platen, Yatharth Saraf, Juan Pino, Alexei Baevski, Alexis Conneau, and Michael Auli. 2022. XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale. In Interspeech 2022, pages 2278\u20132282.\\n\\nAlexei Baevski, Wei-Ning Hsu, Qiantong Xu, Arun Babu, Jiatao Gu, and Michael Auli. 2022. Data2vec: A general framework for self-supervised learning in speech, vision and language. In ICML 2022, pages 1298\u20131312.\\n\\nAlexei Baevski, Yuhao Zhou, Abdelrahman Mohamed, and Michael Auli. 2020. wav2vec 2.0: A framework for self-supervised learning of speech representations. In NeurIPS 2020, volume 33.\\n\\nJeong-Uk Bang et al. 2020. KsponSpeech: Korean spontaneous speech corpus for automatic speech recognition. Applied Sciences.\\n\\nLo\u00efc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023a. SeamlessM4T-massively multilingual & multimodal machine translation. arxiv:2308.11596.\\n\\nLo\u00efc Barrault, Yu-An Chung, Mariano Coria Meglioli, David Dale, Ning Dong, Mark Duppenthaler, Paul-Ambroise Duquenne, Brian Ellis, Hady Elsahar, Justin Haaheim, et al. 2023b. Seamless: Multilingual expressive and streaming speech translation. arxiv:2312.05187.\\n\\nDan Berrebbi, Jiatong Shi, Brian Yan, Osbel L\u00f3pez-Francisco, Jonathan Amith, and Shinji Watanabe. 2022. Combining spectral and self-supervised features for low resource speech recognition and translation. In Interspeech 2022.\\n\\nAlan W Black. 2019. CMU Wilderness Multilingual Speech Dataset. In ICASSP 2019.\\n\\nHui Bu et al. 2017. AISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline. In O-COCOSDA.\\n\\nRonald Cardenas, Rodolfo Zevallos, Reynaldo Baquerizo, and Luis Camacho. 2018. Siminchik: A speech corpus for preservation of southern Quechua. ISI-NLP.\\n\\nJean Carletta. 2007. Unleashing the killer corpus: experiences in creating the multi-everything AMI meeting corpus. Springer.\\n\\nHeng-Jui Chang and James Glass. 2023. R-spin: Efficient speaker and noise-invariant representation learning with acoustic pieces. arXiv preprint arXiv:2311.09117.\\n\\nXuankai Chang, Takashi Maekaku, Pengcheng Guo, Jing Shi, Yen-Ju Lu, Aswin Shanmugam Subramanian, Tianzi Wang, Shu-wen Yang, Yu Tsao, Hung-yi Lee, and Shinji Watanabe. 2021. An exploration of self-supervised pretrained representations for end-to-end speech recognition. In ASRU 2021, pages 228\u2013235.\\n\\nGuoguo Chen et al. 2021. GigaSpeech: An evolving, multi-domain ASR corpus with 10,000 hours of transcribed audio. In Interspeech 2021.\\n\\nSanyuan Chen, Chengyi Wang, Zhengyang Chen, Yu Wu, Shujie Liu, Zhuo Chen, Jinyu Li, Naoyuki Kanda, Takuya Yoshioka, Xiong Xiao, Jian Wu, Long Zhou, Shuo Ren, Yanmin Qian, Yao Qian, Jian Wu,\"}"}
{"id": "emnlp-2024-main-570", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Michael Zeng, Xiangzhan Yu, and Furu Wei. 2022. WavLM: Large-scale self-supervised pre-training for full stack speech processing. IEEE JSTSP.\\n\\nWilliam Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, and Shinji Watanabe. 2023a. Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute. In Interspeech 2023.\\n\\nWilliam Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, and Shinji Watanabe. 2023b. Joint prediction and denoising for large-scale multilingual self-supervised learning. In ASRU 2023.\\n\\nWilliam Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, and Shinji Watanabe. 2023c. Improving massively multilingual ASR with auxiliary CTC objectives. In ICASSP 2023.\\n\\nChung-Cheng Chiu, James Qin, Yu Zhang, Jiahui Yu, and Yonghui Wu. 2022. Self-supervised learning with random-projection quantizer for speech recognition. In International Conference on Machine Learning. PMLR.\\n\\nYu-An Chung, Yu Zhang, Wei Han, Chung-Cheng Chiu, James Qin, Ruoming Pang, and Yonghui Wu. 2021. w2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training. In ASRU 2021.\\n\\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli. 2021. Unsupervised Cross-Lingual Representation Learning for Speech Recognition. In Interspeech 2021, pages 2426\u20132430.\\n\\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzm\u00e1n, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2020. Unsupervised cross-lingual representation learning at scale. In ACL 2020, pages 8440\u20138451, Online. Association for Computational Linguistics.\\n\\nAlexis Conneau et al. 2022. FLEURS: Few-shot learning evaluation of universal representations of speech. In SLT 2022.\\n\\nTri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\u00e9. 2022. FlashAttention: Fast and memory-efficient exact attention with IO-awareness. In NeurIPS 2022.\\n\\nEthnologue. 2017. How many languages in the world are unwritten?\\n\\nDaniel Galvez, Greg Diamos, Juan Manuel Ciro Torres, Juan Felipe Cer\u00f3n, Keith Achorn, Anjali Gopi, David Kanter, Max Lam, Mark Mazumder, and Vijay Janapa Reddi. 2021. The People's Speech: A large-scale diverse English speech recognition dataset for commercial usage. In NeurIPS 2021.\\n\\nJ.J. Godfrey et al. 1992. SWITCHBOARD: telephone speech corpus for research and development. In ICASSP 1992.\\n\\nAlex Graves, Santiago Fern\u00e1ndez, Faustino Gomez, and J\u00fcrgen Schmidhuber. 2006. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In ICML 2006, pages 369\u2013376.\\n\\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhardwaj, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. 2024. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838.\\n\\nAnmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented transformer for speech recognition. In Interspeech 2020.\\n\\nInga R. Helgad\u00f3ttir, Anna Bj\u00f6rk Nikul\u00e1sd\u00f3ttir, Michal Borsk\u00fd, Judy Y. Fong, R\u00f3bert Kjaran, and J\u00f3n Gu\u00f0nason. 2019. The Althingi ASR system. In Interspeech 2019.\\n\\nFran\u00e7ois Hernandez, Vincent Nguyen, Sahar Ghanay, Natalia Tomashenko, and Yannick Esteve. 2018. TED-LIUM 3: Twice as much data and corpus partition for experiments on speaker adaptation. In Speech and Computer: 20th International Conference, SPECOM 2018. Springer.\\n\\nWei-Ning Hsu, Benjamin Bolte, Yao-Hung Hubert Tsai, Kushal Lakhotia, Ruslan Salakhutdinov, and Abdelrahman Mohamed. 2021. HuBERT: Self-supervised speech representation learning by masked prediction of hidden units. IEEE/ACM TALSP.\\n\\nKuan-Po Huang, Yu-Kuan Fu, Tsu-Yuan Hsu, Fabian Ritter Gutierrez, Fan-Lin Wang, Liang-Hsuan Tseng, Yu Zhang, and Hung-yi Lee. 2023. Improving generalizability of distilled self-supervised speech processing models under distorted settings. In SLT 2022, pages 1112\u20131119.\\n\\nIARPA. The Babel Program.\\n\\nJ. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.E. Mazar\u00e9, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux. 2020. LibriLight: A benchmark for ASR with limited or no supervision. In ICASSP 2020.\\n\\nKwangyoun Kim, Felix Wu, Yifan Peng, Jing Pan, Prashant Sridhar, Kyu J Han, and Shinji Watanabe. 2023. E-branchformer: Branchformer with enhanced merging for speech recognition. In SLT 2023.\\n\\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. ICLR 2015.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, and Sanjeev Khudanpur. 2017. A study on data augmentation of reverberant speech for robust speech recognition. In ICASSP 2017.\\n\\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae. 2020. Hifi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis. NeurIPS 2020, 33:17022\u201317033.\\n\\nXinjian Li, Florian Metze, David R. Mortensen, Alan W Black, and Shinji Watanabe. 2022. ASR2K: Speech Recognition for Around 2000 Languages without Audio. In Interspeech 2022.\\n\\nXinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, and Shinji Watanabe. 2023. YODAS: Youtube-oriented dataset for audio and speech. In ASRU 2023.\\n\\nAlexander H Liu, Heng-Jui Chang, Michael Auli, Wein-Ning Hsu, and Jim Glass. 2024. DinoSR: Self-distillation and online clustering for self-supervised speech representation learning. NeurIPS 2024, 36.\\n\\nZhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. 2023. LLM360: Towards fully transparent open-source llms. arXiv preprint arXiv:2312.06550.\\n\\nChen-Chou Lo, Szu-Wei Fu, Wen-Chin Huang, Xin Wang, Junichi Yamagishi, Yu Tsao, and Hsin-Min Wang. 2019. MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion. In Interspeech 2019, pages 1541\u20131545.\\n\\nDau-Cheng Lyu, Tien-Ping Tan, Eng Siong Chng, and Haizhou Li. 2010. SEAME: a Mandarin-English code-switching speech corpus in south-east Asia. In Interspeech 2010.\\n\\nSoumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, and Shinji Watanabe. 2024. VoxtLM: Unified Decoder-Only Models for Consolidating Speech Recognition, Synthesis and Speech, Text Continuation Tasks. In ICASSP 2024, pages 13326\u201313330. IEEE.\\n\\nDianwen Ng, Ruixi Zhang, Jia Qi Yip, Zhao Yang, Jinjie Ni, Chong Zhang, Yukun Ma, Chongjia Ni, Eng Siong Chng, and Bin Ma. 2023. De\u2019hubert: Disentangling noise in a self-supervised model for robust speech recognition. In ICASSP 2023, pages 1\u20135.\\n\\nJumon Nozaki and Tatsuya Komatsu. 2021. Relaxing the conditional independence assumption of CTC-based ASR by conditioning on intermediate predictions. In Interspeech 2021, pages 3735\u20133739.\\n\\nPatrick K. O'Neill, Vitaly Lavrukhin, Somshubra Majumdar, Vahid Noroozi, Yuekai Zhang, Oleksii Kuchaiev, Jagadeesh Balam, Yuliya Dovzhenko, Keenan Freyberg, Michael D. Shulman, Boris Ginsburg, Shinji Watanabe, and Georg Kucsko. 2021. SPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition. In Interspeech 2021.\\n\\nVassil Panayotov et al. 2015. Librispeech: An ASR corpus based on public domain audio books. In ICASSP 2015.\\n\\nDouglas B Paul and Janet Baker. 1992. The design for the Wall Street Journal-based CSR corpus. Workshop on Speech and Natural Language.\\n\\nYifan Peng, Siddharth Dalmia, Ian Lane, and Shinji Watanabe. 2022. Branchformer: Parallel MLP-attention architectures to capture local and global context for speech recognition and understanding. In ICML 2022.\\n\\nYifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, and Shinji Watanabe. 2023a. A comparative study on e-branchformer vs conformer in speech recognition, translation, and understanding tasks. In Interspeech 2023.\\n\\nYifan Peng, Yui Sudo, Muhammad Shakeel, and Shinji Watanabe. 2024a. OWSM-CTC: An Open Encoder-Only Speech Foundation Model for Speech Recognition, Translation, and Language Identification. arXiv preprint arXiv:2402.12654.\\n\\nYifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, et al. 2024b. OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer. arXiv preprint arXiv:2401.16658.\\n\\nYifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, and Shinji Watanabe. 2023b. Reproducing Whisper-style training using an open-source toolkit and publicly available data. In ASRU 2023.\\n\\nAdam Polyak, Yossi Adi, Jade Copet, Eugene Kharitonov, Kushal Lakhotia, Wei-Ning Hsu, Abdelrahman Mohamed, and Emmanuel Dupoux. 2021. Speech Resynthesis from Discrete Disentangled Self-Supervised Representations. In Interspeech 2021.\\n\\nMatt Post et al. 2013. Improved speech-to-text translation with the fisher and callhome Spanish-English speech translation corpus. In IWSLT 2013.\\n\\nVineel Pratap, Andros Tjandra, Bowen Shi, Paden Tomasello, Arun Babu, Sayani Kundu, Ali Elkahky, Zhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi, et al. 2023. Scaling speech technology to 1,000+ languages. arxiv:2305.13516.\\n\\nVineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. MLS: A large-scale multilingual dataset for speech research. In Interspeech 2020, pages 2757\u20132761.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine Mcleavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In ICML 2023.\\n\\nMirco Ravanelli, Titouan Parcollet, Peter Plantinga, Aku Rouhe, Samuele Cornell, Loren Lugosch, Cem Subakan, Nauman Dawalatabad, Abdelwahab Heba, Jianyuan Zhong, et al. 2021. Speechbrain: A general-purpose speech toolkit. arXiv preprint arXiv:2106.04624.\\n\\nChandan K.A. Reddy, Harishchandra Dubey, Kazuhito Koishida, Arun Nair, Vishak Gopal, Ross Cutler, Sebastian Braun, Hannes Gamper, Robert Aichner, and Sriram Srinivasan. 2021. INTERSPEECH 2021 Deep Noise Suppression Challenge. In Interspeech 2021.\\n\\nElizabeth Salesky, Matthew Wiesner, Jacob Bremerman, Roldano Cattoni, Matteo Negri, Marco Turchi, Douglas W. Oard, and Matt Post. 2021. Multilingual TEDx corpus for speech recognition and translation. In Interspeech 2021.\\n\\nRamon Sanabria, Nikolay Bogoychev, Nina Markl, Andrea Carmantini, Ondrej Klejch, and Peter Bell. 2023. The Edinburgh international accents of English corpus: Towards the democratization of English ASR. In ICASSP 2023.\\n\\nJiatong Shi, Jonathan D Amith, Xuankai Chang, Siddharth Dalmia, Brian Yan, and Shinji Watanabe. 2021a. Highland Puebla Nahuatl speech translation corpus for endangered language documentation. In AmericasNLP 2021.\\n\\nJiatong Shi, Jonathan D Amith, Rey Castillo Garc\u00eda, Esteban Guadalupe Sierra, Kevin Duh, and Shinji Watanabe. 2021b. Leveraging end-to-end ASR for endangered language documentation: An empirical study on Yol\u00f3xochitl Mixtec. In EACL 2021.\\n\\nJiatong Shi, Dan Berrebbi, William Chen, En-Pei Hu, Wei-Ping Huang, Ho-Lam Chung, Xuankai Chang, Shang-Wen Li, Abdelrahman Mohamed, Hung yi Lee, and Shinji Watanabe. 2023a. ML-SUPERB: Multilingual Speech Universal PERformance Benchmark. In Interspeech 2023.\\n\\nJiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-Yi Lee, and Shinji Watanabe. 2023b. Findings of the 2023 ML-SUPERB challenge: Pre-training and evaluation over more languages and beyond. In ASRu 2023.\\n\\nJiatong Shi, Hirofumi Inaguma, Xutai Ma, Ilia Kulikov, and Anna Sun. 2024. Multi-resolution HuBERT: Multi-resolution speech self-supervised learning with masked unit prediction. In ICLR 2024.\\n\\nAnna Slizhikova et al. 2020. Russian Open Speech To Text (STT/ASR) Dataset.\\n\\nInc Smule. 2019. DAMP-MVP: Digital Archive of Mobile Performances - Smule Multilingual Vocal Performance 300x30x2.\\n\\nPer Erik Solberg and Pablo Ortiz. 2022. The Norwegian parliamentary speech corpus. In LREC 2022, Marseille, France.\\n\\nJ\u00f6rgen Valk and Tanel Alum\u00e4e. 2021. VOXLINGUA107: A dataset for spoken language recognition. In SLT 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In NeurIPS 2017.\\n\\nVoxForge. VoxForge.\\n\\nChanghan Wang, Yun Tang, Xutai Ma, Anne Wu, Sravya Popuri, Dmytro Okhonko, and Juan Pino. 2020. Fairseq S2T: Fast speech-to-text modeling with fairseq. arxiv:2010.05171.\\n\\nChanghan Wang et al. 2021. VoxPopuli: A Large-Scale Multilingual Speech Corpus for Representation Learning, Semi-Supervised Learning and Interpretation. In ACL 2021.\\n\\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai. 2018. ESPnet: End-to-end speech processing toolkit. In Interspeech 2018.\\n\\nShinji Watanabe, Takaaki Hori, Suyoun Kim, John R. Hershey, and Tomoki Hayashi. 2017. Hybrid CTC/attention architecture for end-to-end speech recognition. IEEE Journal of Selected Topics in Signal Processing.\\n\\nShu wen Yang, Po-Han Chi, Yung-Sung Chuang, Cheng-I Jeff Lai, Kushal Lakhotia, Yist Y. Lin, Andy T. Liu, Jiatong Shi, Xuankai Chang, Guan-Ting Lin, Tzu-Hsien Huang, Wei-Cheng Tseng, Kottik Lee, Da-Rong Liu, Zili Huang, Shuyan Dong, Shang-Wen Li, Shinji Watanabe, Abdelrahman Mohamed, and Hung yi Lee. 2021. SUPERB: Speech Processing Universal PERformance Benchmark. In Interspeech 2021.\\n\\nBigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\u0107, Daniel Hesslow, Roman Castagn\u00e9, Alexandra Sasha Lucicioni, Fran\u00e7ois Yvon, et al. 2022. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100.\\n\\nJunichi Yamagishi et al. 2019. CSTR VCTK Corpus: English Multi-speaker Corpus for CSTR Voice Cloning Toolkit.\\n\\nGuanrou Yang, Ziyang Ma, Zhisheng Zheng, Yakun Song, Zhikang Niu, and Xie Chen. 2023. Fast-HuBERT: an efficient training framework for self-supervised speech representation learning. In ASRU 2023.\"}"}
{"id": "emnlp-2024-main-570", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-570", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.1 Data\\nFor brevity, we provide an overview of these datasets in Table 11 and refer readers to the original papers for more details (Bu et al., 2017; Chen et al., 2021; Panayotov et al., 2015; O'Neill et al., 2021; Hernandez et al., 2018; Pratap et al.; Zhang et al., 2022; Carletta, 2007; Ardila et al., 2020; Godfrey et al., 1992; Conneau et al., 2022; Bang et al., 2020; Yang et al., 2022; Wang et al., 2021; Chen et al., 2023b; Li et al., 2023; Kahn et al., 2020; Galvez et al., 2021; Valk and Alum\u00e4e, 2021; Helgad\u00f3ttir et al., 2019; Lyu et al., 2010; Solberg and Ortiz, 2022; Shi et al., 2021a,b; Cardenas et al., 2018; Sanabria et al., 2023; Ahamad et al., 2020; Berrebbi et al., 2022).\\n\\nOver 80% of the pre-training data is derived from two corpora: YODAS (Li et al., 2023) and VoxPopuli (Wang et al., 2021). However, both of these sources largely consist of European languages. To add more linguistic diversity, we also include smaller scale multilingual corpora such as BABEL (IARPA), Googlei18n (Chen et al., 2023b), VoxLingua (Valk and Alum\u00e4e, 2021), and FLEURS (Conneau et al., 2022). While some may argue that FLEURS and/or BABEL was originally designed to be out-of-domain evaluation, we note that many works now include it as an in-domain dataset for both supervised and unsupervised training (Peng et al., 2023b; Pratap et al., 2023; Chen et al., 2023b; Zhang et al., 2023).\\n\\nWe complement this selection of multilingual corpora with various language-specific data to boost the representation of languages that are underrepresented in large web-scale datasets. This includes many indigenous languages, such as Quechua (Cardenas et al., 2018), Mixtec (Shi et al., 2021b), and Totonac (Berrebbi et al., 2022). Finally, we also make an effort to support speech outside of mainstream accents and voices. For example, we include code-switching (Lyu et al., 2010), accented data (AI4Bharat, 2020; Sanabria et al., 2023; Ahamad et al., 2020), and even singing voices (Smule, 2019).\\n\\nA.2 Ablations\\nTo understand the effectiveness of E-Branchformer and WavLM denoising in the multilingual setting, we conducted several SSL experiments at a smaller scale. We sampled 7,000 hours of data from our 1 million hour SSL dataset and trained a 95M parameter HuBERT Base model on that data (ML-HuBERT 7K). Then, we trained a variant with the addition of the acoustic denoising task and another with the Transformer layers replaced with E-Branchformer layers. We then benchmarked each model on ML-SUPERB using the same settings as Section 5.1.1. Our results show meaningful gains achieved with the addition of the WavLM objective and E-Branchformer architecture.\\n\\nTable 7: Ablations on the ML-SUPERB 1-hour set.\\n\\n| Model | Multi.ASR + LID |\\n|-------|-----------------|\\n| CER   | CER (Few-Shot)  | ACC |\\n| ML-HuBERT 7K | 36.9 | 40.5 | 78.5 |\\n| + Denoising | 30.5 | 41.2 | 84.3 |\\n| + E-Branchformer | 29.0 | 40.9 | 85.2 |\\n\\nA.3 Dereverberation\\nWe perform a preliminary investigation of the effectiveness of our proposed SSL dereverberation task (Section 4.2) by training two HuBERT Base models (Hsu et al., 2021) on LibriSpeech 960 (Panayotov et al., 2015). The first model corresponds to the vanilla HuBERT setting without any form of data augmentation, while the second model is trained with our proposed dereverberation technique. We fine-tune each model on 10-hours of LibriLight ASR (Kahn et al., 2020), where we use performance on test-clean and test-other as a proxy for comparing performance on clean and noisy data respectively. Results are in Table 8, with improvements across both test sets. The benefits are more significant on the noisier test-other, with relative reductions of 6.9% Word Error Rate (WER), indicating the effectiveness of our technique for enhancing model robustness. Performance on test-clean is also improved by 3.1%.\\n\\nTable 8: Investigation on effectiveness of dereverberation augmentation on English ASR by WER.\\n\\n| Model | test-clean | test-other |\\n|-------|------------|------------|\\n| HuBERT | 13.1 | 22.6 |\\n| + Dereverberation | 12.7 | 21.1 |\\n\\nA.4 Pre-Training Details\\nTable 9 provides a breakdown of XEUS' the computational costs, measured in CPU and GPU hours. We used AMD EPYC 7763 processors for CPU-based jobs. For most GPU tasks, we use a mix of...\"}"}
{"id": "emnlp-2024-main-570", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Computation budget of XEUS in CPU/GPU hours. Reported numbers for formatting are in CPU hours, while all other stages are measured in GPU hours.\\n\\n| Stage          | Hours |\\n|----------------|-------|\\n| Data Preparation| 100,000 |\\n| Pseudo-labelling | 15,000 |\\n| Ablations       | 2,100 |\\n| Scaling         | 200   |\\n| Pre-Training    | 63,000 |\\n\\nNvidia A40 46GB and Nvidia A100 40GB GPUs. For pre-training the final model, we exclusively used A100s.\\n\\nWe consumed around 100K CPU hours for the data stage. The bulk of this usage came from processing the YODAS dataset, which originally consisted of document-level WA V files at various sampling rates. We had to convert each file into 16kHz audio samples and then segment each into utterance-level pieces. Another large source of CPU consumption came from downloading each dataset and unarchiving them onto the disk, which became a non-trivial effort for larger datasets such as VoxPopuli.\\n\\nObtaining the HuBERT pseudo-labels required a large amount of compute, totalling 15,000 GPU hours. While one may argue that this process could have been avoid by using another SSL objective, such as data2vec (Baevski et al., 2022) or w2v-BERT (Chung et al., 2021), we believed that this cost was worth the guaranteed stability during large-scale training. Self-distilling SSL objectives like data2vec are prone to representation collapse (Baevski et al., 2022; Liu et al., 2024), while wav2vec derived models require a codebook diversity loss (Baevski et al., 2020; Chung et al., 2021). As our experimental runs were limited, we believed the simple HuBERT objective would be the easiest to scale.\\n\\nThe ablations described in Sections A.3 and A.2 consumed a total of 2,100 GPU hours, while the hyperparameter tuning required for scaling (masking ratio, learning rate, warmup steps) consumed 200 GPU hours.\\n\\nThe largest source of compute usage came from the final pre-training phase, which consumed 63,000 GPU hours across a total of 40 days. We use bfloat16 mixed precision with Flash Attention v2 (Dao et al., 2022) for faster pre-training. To improve convergence, for the first 3,000 steps we include an additional intermediate cross entropy SSL loss (Yang et al., 2023), an initial masking probability of 0.65, and no data augmentation. This intermediate loss is applied to the 10th encoder layer and is weighted by a factor of 0.3. Afterwards, we remove the intermediate loss for greater efficiency, increase the masking probability to 0.8, and enable the above augmentation methods. We use the Adam optimizer with 32,000 warmup steps and a peak learning rate of 0.0003.\\n\\nA.5 Engineering Details\\n\\nThis section expands upon the engineering optimizations that we made on the ESPnet toolkit mentioned in Section 4.4. We organize it into two main components: GPU communication and batching. Quantitative analyses of these optimizations are reported in Table 10.\\n\\nTable 10: Quantitative results of our engineering improvements. We report percent increases in relative throughput and percent decreases in relative memory usage after our optimizations.\\n\\n| Optimization       | Throughput (\u2191) | Memory (\u2193) |\\n|--------------------|----------------|------------|\\n| GPU Synchronization| 120%           | -          |\\n| Batch Optimization  | 60% 113%       |            |\\n\\nGPU Communication: As the dataset size increases, scaling to a larger number of GPUs is important to finish pre-training in a reasonable time. However, this leads to non-trivial overhead due to communication costs across multiple compute nodes. During our training, we found and removed 2 unnecessary GPU synchronization steps within the ESPnet training code. We also disabled synchronization in iterations without an optimization step (when using gradient accumulation), further improving runtime. These changes are particularly impactful in compute clusters that lack Infiniband support for inter-node communication, which is common in more academic-oriented data centers.\\n\\nBatching: The batching method has a large impact on both the memory efficiency and throughput of model training. Ideally, utterances of similar length should be batched together to reduce the amount of padding. This can be complicated during multi-node training, especially when there is large amounts of variance between utterance lengths. We found that ESPnet had issues over-allocating data to batches, which had remained hidden as it only noticeable given a sufficiently large batch size and number of GPUs (such as our case). Fixing this\"}"}
