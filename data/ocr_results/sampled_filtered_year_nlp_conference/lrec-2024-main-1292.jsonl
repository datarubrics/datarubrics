{"id": "lrec-2024-main-1292", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SimLex-999 for Dutch\\n\\nLizzy Brans \u2663 and Jelke Bloem \u2662\u2660\\n\\nHuman Computer Interaction, Utrecht University \u2663\\nInstitute for Logic, Language and Computation, University of Amsterdam \u2662\\nData Science Centre, University of Amsterdam\u2660\\nl.brans@students.uu.nl, j.bloem@uva.nl\\n\\nAbstract\\n\\nWord embeddings revolutionised natural language processing by effectively representing words as dense vectors. Although many datasets exist to evaluate English embeddings, few cater to Dutch. We developed a Dutch variant of the SimLex-999 word similarity dataset by gathering similarity judgements from 235 native Dutch speakers. Subsequently, we evaluated two popular Dutch language models, Bertje and RobBERT, finding that Bertje showed superior alignment with human semantic similarity judgments compared to RobBERT. This study provides the first intrinsic Dutch word embedding evaluation dataset, which enables accurate assessment of these embeddings and fosters the development of effective Dutch language models.\\n\\n1. Introduction\\n\\nThe demand for effective communication across languages grows as the world becomes more interconnected. Word embeddings, mathematical representations of words in multi-dimensional space, have become crucial for natural language processing tasks such as machine translation and sentiment analysis (Mikolov et al., 2013). However, research on word embeddings has been mainly focused on English. All recent state-of-the-art language models were initially developed for English, and various English-language benchmarks and datasets for evaluating such models are available. In particular, the English SimLex-999 dataset (Hill et al., 2015) is a reliable gold standard of word similarity ratings that can be used to benchmark word embeddings and large language models incorporating vector representations of word meaning. This dataset consists of a balanced set of 999 English word pairs, where each pair was rated for similarity by 50 native speakers of English. In intrinsic evaluation of word embeddings, these human similarity ratings of word pairs are correlated with cosine similarity values of embedding vectors of the two words in the pair. A model that yields cosine similarities that better correlate with the gold standard is assumed to better represent semantic similarities between words.\\n\\nHowever, this focus on English language technology leaves other languages such as Dutch with fewer options for evaluating the quality of its language technology. Dutch is typically considered a fairly high-resource language, and it has importance as a widely spoken language in the Netherlands, Belgium and Suriname and as an official language of the European Union. Nevertheless, Dutch remains under-explored in the context of word embeddings. In particular, we are unaware of any Dutch word similarity datasets that can be used to intrinsically evaluate word embeddings. Dutch word embedding models, such as the Word2Vec-based ones of Tulkens et al. (2016), have exclusively been evaluated on extrinsic tasks, such as relation identification. Available datasets for such extrinsic tasks have been summarized in the DUMB benchmark (de Vries et al., 2023). Intrinsic evaluation could provide a more general type of evaluation that can also inform the use of language models for extrinisic tasks for which there is no specific Dutch gold standard available.\\n\\nIn this work, we address this gap by constructing a new word similarity dataset for Dutch, inspired by the established English SimLex-999 dataset (Hill et al., 2015), as well as MEN (Bruni et al., 2014), following Resnik's (1995) methods for collecting human semantic similarity judgments. We then use the created dataset to evaluate two state-of-the-art Dutch language models, Bertje and RobBERT, against the human semantic similarity judgments in our dataset. Besides providing a new perspective on the potential performance of these models at various NLP tasks, this also provides insights into the disparities and alignments between machine-learned and human-perceived semantic associations. These observations might inspire further research into human language comprehension and developing more human-like NLP models (Landauer and Dumais, 1997; Lake et al., 2017).\\n\\n2. Background\\n\\nThe distributional hypothesis posits that words with similar meanings often appear in similar contexts (Harris, 1954). According to this hypothesis, words can be represented as dense vectors in multi-dimensional space, with the distance between vectors indicating semantic similarity (Turney and Pan\u0442\u0435\u043b, 2010). Word embeddings, rooted in early vector\"}"}
{"id": "lrec-2024-main-1292", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"space models (Salton et al., 1975), are vectorised representations that allow machines to process word meanings and relationships, with a notion of meaning that is grounded in the distributional hypothesis. Techniques like Latent Semantic Analysis (LSA) and count-based models, aided by methods like Singular Value Decomposition (SVD), were fore-runners to modern embeddings (Deerwester et al., 1990; Klema and Laub, 1980). Earlier word embedding models like Word2Vec and GloVe offer static representations for each word without accounting for context-based dynamic meanings (Mikolov et al., 2013; Pennington et al., 2014). In contrast, transformer models, such as BERT (Bidirectional Encoder Representations from Transformers) and Generative Pre-trained Transformer (GPT), shift towards token-based or contextual embeddings (Devlin et al., 2019; Radford et al., 2018). Based on the transformer architecture, these models offer context-sensitive representations, capturing the nuances of word usage (Vaswani et al., 2017; Camacho-Collados and Pilehvar, 2018).\\n\\n2.1. Word embedding evaluation\\n\\nAs word embedding models are grounded in a theory of distributional learning, word embeddings have often been evaluated against datasets of word pairs with gold standard word similarity scores elicited from native speakers of the language. Early distributional semantic models for English were evaluated in this way against the WordSim353 (Finkelstein et al., 2001) dataset, where 13 to 16 participants rated 353 English word pairs. Subsequently, the MEN dataset (Bruni et al., 2014) was developed for English with similar aims but on a larger scale. This dataset includes 3000 word pairs rated by crowd workers, though with only a few raters per pair.\\n\\nThese datasets were later criticized for not distinguishing the concepts of word relatedness and word similarity in the instructions to the participants. To address this, the SimLex-999 dataset was developed (Hill et al., 2015), consisting of 999 English word pairs covering different parts of speech and balanced for more abstract and more concrete words. The pairs were rated by 500 crowd workers with about 50 ratings per word pair. The workers were specifically instructed to rate for similarity, and given examples thereof, and the annotator instructions were made available along with the dataset.\\n\\nIn the subsequent era of contextual embeddings, word similarity scores were no longer the most obvious choice for embedding evaluation, as there is no context for contextual embedding models to take advantage of in this kind of benchmark. Nevertheless, they continued to be used for evaluating static embeddings \u2018distilled\u2019 from contextual embeddings, mainly in interpretability work (Rogers et al., 2021) or in situations that called for representations of word types containing lexical-semantic information. An example of this is Abdou et al. (2021), who study representations of color terms to find out whether they reflect the perceptual color space. Among others, Bommasani et al. (2020) documented that earlier layers of contextual embedding models perform better at word similarity tasks, and their distillation approach was used to perform such tasks. However, Ehrmanntraut et al. (2021) also argued that we might as well use static embedding models especially in contexts where interpretability is important, as they still perform well in comparison to contextual embedding models on similarity tasks.\\n\\nThe aforementioned datasets are predominantly English-centric and may not encapsulate linguistic and cultural nuances of other languages, which could lead to biased or incomplete assessments of word embeddings (Faruqui et al., 2016; Agirre et al., 2009). To address this, and with SimLex-999 gaining widespread use, the dataset has been adapted to various other languages. This adaptation typically involved translating the word pairs from the English SimLex-999 to the target language and then asking native speakers of the target language to rate the pairs using a translated version of the instructions. We are aware of German, Italian, Russian (Leviant and Reichart, 2015), Estonian (Kittask and Barbu, 2019), Portuguese (Querido et al., 2017), Vietnamese (Van Tan et al., 2017), Polish (Mykowiecka et al., 2018), Czech (Kliegr and Zamazal, 2018), Thai (Netisopakul et al., 2019) and Swedish (Hengchen and Tahmasebi, 2021) versions of SimLex-999. The subsequent MultiSimLex dataset (Vuli\u0107 et al., 2020) covers 13 languages in a similar manner, based on crowdsourced ratings: Mandarin, Yue, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili and Arabic, with subsequent extensions to Icelandic (Dan\u00edelsson et al., 2021) and Buddhist Sanskrit (Lugli et al., 2022).\\n\\nFor some languages, translated versions of SimLex-999 are available but with no re-rating specific to that language, including Spanish (Etchevery and Wonsever, 2016), Croatian (Mrk\u0161i\u0107 et al., 2017), Slovenian (Pollak et al., 2020) and various other languages (Barzegar et al., 2018). This approach may be less reliable when word pair similarity differs between languages (Leviant and Reichart, 2015).\\n\\nFor yet other languages, there are resources available that are similar in scope, but specific to that language, such as AnlamVer for Turkish, based on the SimLex-999 annotation guidelines (Ercan and Y\u0131ld\u0131z, 2018), Finnish FS300 (Venekoski and Vankka, 2017) based on a sub-\"}"}
{"id": "lrec-2024-main-1292", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"set of SimLex-999, the Japanese Word Similarity and Association Norm (Inohara and Utsumi, 2022), SimRelUz for Uzbek (Salaev et al., 2022) and USWS-21 for Urdu (Muneer et al., 2023).\\n\\nWe are not aware of any resources for Dutch in any of these three categories. One type of available resource for Dutch from which some semantic similarity gold standard may be derived is those used by psychologists to study associative priming, such as the dataset of Drieghe and Brysbaert (2002). However, this is about relatedness, not similarity, and this dataset is quite small in scope with 20 usable word pairs.\\n\\nTherefore, we developed a Dutch version of SimLex-999, with re-rating by Dutch native speakers. Our dataset includes Dutch nouns, verbs, and adjectives, aligning with the English SimLex-999 to ensure linguistic coherence and enable cross-language comparisons.\\n\\n### 3. Methodology\\n\\nTo build a Dutch version of SimLex-999, we first translated the word pairs to Dutch. To ensure consistency and comparability with the original SimLex-999, corresponding nouns, verbs, and adjectives were used (Hill et al., 2015). The selection process involved identifying appropriate Dutch translations or equivalents for the English words in these datasets and carefully considering differences in meaning, usage, and cultural context. When multiple Dutch translations were available, the German version of the SimLex-999 dataset was used as a reference point, given the closer linguistic relationship between Dutch and German (Leviant and Reichart, 2015).\\n\\nTable 1 showcases this translation process, displaying English word pairs from SimLex-999 alongside their German and Dutch counterparts.\\n\\n| English         | German          | Dutch         |\\n|-----------------|-----------------|---------------|\\n| meat - sandwich | fleisch - sandwich | vlees - broodje |\\n| meter - yard    | meter - yard    | meter - decimeter |\\n| dumb - dense    | bl\u00f6d - unterbelichtet | dom - onderontwikkeld |\\n\\nThese examples underline the careful considerations made during translation, focusing on achieving semantic accuracy and cultural relevance. For example, the word *yard* in its distance sense is not used much in Dutch as it is part of the British Imperial system, while in Dutch-speaking countries, the metric system is used. Therefore, we used the word *decimeter* for the Dutch dataset. Although a *yard* is much longer than a *decimeter*, we cannot translate it to *meter* as this would yield the pair *meter - meter*, so we chose another related unit of measure that is reasonably frequent in Dutch, the *decimeter*. In other cases, we chose a word of similar frequency if an exact translation was not available. In this manner, the selection process aimed for the closest possible alignment between the original English words and their Dutch counterparts, thus enhancing the validity and applicability of the Dutch version of the SimLex-999 dataset.\\n\\n#### 3.1. Re-rating\\n\\nNext, we conducted a questionnaire in which the Dutch word pairs were re-rated. Native Dutch speakers aged 16 or older participated in the study. Similar to the original English SimLex-999 study, the aim was to ensure a certain level of language proficiency. The chosen age criterion ensured that participants had completed a significant portion of their education in Dutch. Furthermore, this broad age range was required by our ethics committee, based on EU GDPR regulations, and more detailed demographic data collection would have been considered personal data, complicating the data collection procedure and its shareability. In addition, to minimise judgment errors, individuals with any language disorders were excluded from the study. Several recruitment channels were utilised, including online forums, social media, and educational institutions, which targeted a wide range of potential participants from different walks of life.\\n\\nThe data was collected through an online questionnaire hosted on the Qualtrics platform, chosen for its user-friendly interface and efficient distribution to a large audience (Qualtrics, 2023). Before participating, all participants read an information brochure and provided informed consent. A unique personal ID number was provided to each participant, allowing them to withdraw their answers at any time, ensuring their anonymity. The study was approved by the corresponding author's faculty ethics committee.\\n\\nAfter providing informed consent, participants answered personal questions to ensure they met the eligibility criteria. These questions assessed whether participants met the age requirement, their fluency in Dutch, and the absence of language disorders. After determining eligibility, the study directed participants to an instructions page with a Dutch translation of the original English instructions from the SimLex-999 dataset (Hill et al., 2015). These instructions are shown in Appendix B. The instructions emphasise that participants should rate word pairs based on similarity rather than relatedness and provide clear examples to guide them.\\n\\nParticipants rated the semantic similarity of word pairs in the new dataset using a continuous sliding scale from 0 to 10 (Resnik, 1995; Majewska et al., 2021). This random assignment aimed to minimise potential biases related to the order in which word pairs were presented. Aggregating these ratings,\"}"}
{"id": "lrec-2024-main-1292", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the mean of the scores was used for each word pair, and standard deviations are also included in the dataset as a measure of variance. To ensure the quality of our dataset, we conducted manual checks of all annotations. Annotators who completed only a small portion of the questionnaire were specifically reviewed for accuracy. This process helped us maintain the integrity and reliability of our dataset. These steps follow Hill et al. (2015). The aggregated scores serve as the word similarity values in our dataset, which we subsequently use for evaluating Dutch word embeddings.\\n\\nThe new dataset's reliability and validity were assessed using several statistical methods. Interrater reliability was examined using the intraclass correlation coefficient (ICC, Koo and Li, 2016), and at least ten responses per word pair were gathered to reinforce reliability. We examine the validity of our dataset in terms of content, construct and criterion validity (Cronbach and Meehl, 1955). These concepts are not often cited in NLP but are commonly used in quantitative research, particularly in the social sciences, to validate metrics and procedures. Content validity is defined as \u201cthe extent to which a test reflects the various components of the construct it is intended to measure\u201d, construct validity is \u201cthe extent to which a measure accurately assesses the construct or latent attribute that it is intended to measure\u201d, and criterion validity is \u201cevaluating the validity of a measure based on its relationship to a specific criterion\u201d (APA, 2020). We believe this is also a useful way of describing and comparing the validity of NLP benchmarks beyond specifics of the particular evaluation procedure. For Dutch SimLex-999, content validity is supported by the fact that the selected words covered a broad range of semantic domains, and include nouns, verbs, and adjectives. To assess construct validity, we examine the relationship between the dataset and the original English SimLex-999 dataset which is more established as a benchmark of semantic association. To address criterion validity, we compare the dataset's performance in evaluating the embeddings contained in Dutch language models with previous evaluation attempts that use other evaluation metrics.\\n\\n3.2. Intrinsic evaluation of Dutch language models\\n\\nIn addition to creating this dataset, we use our new Dutch SimLex-999 dataset to intrinsically evaluate two state-of-the-art Dutch language models, Bertje and RobBERT. Bertje is a Dutch-language version of Google AI\u2019s BERT (Devlin et al., 2019), having been trained on a vast array of Dutch internet text (De Vries et al., 2019). RobBERT, mirroring Facebook AI\u2019s RoBERTa model, benefits from advanced training approaches and a rich Dutch dataset from the OSCAR corpus (Delobelle et al., 2020; Liu et al., 2019; Scheible et al., 2020). While SimLex-999 is typically used to evaluate static word embeddings, we chose to evaluate embeddings distilled from contextual embedding models as this is a recently popular use case (cf. section 3.2). The evaluation procedure involves correlating model-calculated similarity scores with the averaged human judgments from the dataset using Spearman\u2019s rank correlation (Spearman, 1904). Similarity scores were obtained by separately embedding each word from a word pair without any context, and computing the cosine similarity between the resulting embeddings. We perform this procedure for every transformer layer of each model to test which layer best encodes human word similarity judgements. We expect that layer 0 yields the best correlation with human word similarity ratings, as it corresponds to a contextless static embedding and the human similarity ratings were also elicited without context. Lastly, we identified instances where model outcomes significantly deviated from human ratings by qualitative analysis.\\n\\nAs BERT\u2019s WordPiece tokenizer (Schuster and Nakajima, 2012) and RoBERTa\u2019s Byte-Pair Encoding (BPE) tokenizer (Liu et al., 2019) approaches may segment longer or less frequent words into tokens, we also consider the issue of subtokenized words. When intrinsically evaluating a model, it is possible to either construct embeddings for them by combining the embeddings of their subtokens, or to skip word pairs with subtokenized words as we would do with OOV words for static embedding models. We explore the effects of these options. In the former condition, we apply an averaging method to the subtoken embeddings, in line with practices for models using the WordPiece and BPE tokenisers (Devlin et al., 2019; Liu et al., 2019). This is called subword pooling (Bommasani et al., 2020). This approach typically leads to better performance on low-frequency words.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"relation coefficient (ICC, Koo and Li, 2016), for which we divided annotators into random groups. The average rater ICCs displayed excellent consistency across all groups, with values between 0.84 and 0.96. Single-rater ICCs, indicative of individual rating consistency, ranged from fair (0.26) to good (0.59) across the groups. These values confirm the dataset's reliability, signifying stability in similarity ratings and minimising the potential impact of individual judgment discrepancies or biases.\\n\\nThe dataset's validity was evaluated in three dimensions: content, construct, and criterion (Cronbach and Meehl, 1955). Following Hill et al. (2015) the word pairs cover a range of semantic domains, ensuring content validity. Criterion validity was assessed by comparing the dataset's performance in evaluating Dutch word embeddings to past evaluations, which is detailed in section 5.1. Our construct validity metric is the correlation between ratings in our dataset and ratings in SimLex-999 for the English and German versions of SimLex-999. This type of correlation analysis has also been performed to assess cross-language similarity in MultiSimLex (Vuli\u0107 et al., 2020). Spearman's rank correlation coefficient (Spearman, 1904) served as the metric to measure the degree of semantic similarity between different language versions of the SimLex-999 dataset. High correlation coefficients signify a strong agreement in the semantic similarity between two languages.\\n\\n### Table 2: Cross-Language Similarity Agreement\\n\\n| Comparison          | Correlation |\\n|---------------------|-------------|\\n| English - German    | 0.7478      |\\n| English - Dutch     | 0.7487      |\\n| German - Dutch      | 0.6785      |\\n\\nTable 2 summarises the Cross-Language Similarity Agreement findings. The results validate the Dutch SimLex-999 dataset's similarity to the English and German versions, serving as evidence of construct validity. Of course, we do not expect maximum correlation, as word meanings differ between languages even though we translated the word pairs as closely as possible. However, since these languages are all related West Germanic languages, a high correlation is to be expected of a valid metric. The English-German comparison yielded a Spearman correlation of 0.7478, aligning with prior findings and suggesting a substantial semantic similarity between these two languages. A comparison between English and Dutch produced a Spearman correlation of 0.7487, similar to the English-German correlation, signifying a high degree of semantic similarity. Furthermore, a comparison between German and Dutch resulted in a Spearman correlation of 0.6785. This is surprising as we might expect German and Dutch to be closer together. The correlation values might have different statistical power due to differences in the number of participants (approximately 50 per pair for English, 13 per pair for German, 10-19 per pair for Dutch).\\n\\n### 4.1. Comparison of Dutch language models\\n\\nNext, we evaluate two widely used transformer-based Dutch language models, Bertje and RobBERT-v2, against the novel SimLex-999 Dutch dataset. Using Spearman correlation coefficients, we assess the statistical relation between the model's predictions and human-rated similarity. Firstly, to be able to use Spearman correlation, the relationship between human-rated and model similarities must be monotonic. We observe that this is the case for both models from the scatterplots in Figure 1. Figures 2 and 3 show the correlation values between the human ratings of word pair similarity and cosine similarities of word pairs in all layers of the models. For Bertje, the highest Spearman correlation is observed in Layer 0, with a correlation coefficient of 0.409. When combining layers 0 and 3, a correlation of 0.421 is reached. Correlations with human judgements mostly decrease across the layers down to 0.213 at layer 11. Adjectives display the highest correlation at 0.474, with nouns at 0.449 and verbs significantly lower at 0.213 at layer 0. Using the SUBTLEX-NL word frequency database (Keuleers et al., 2010), high-frequency words achieve a 0.444 correlation at layer 0, while low-frequency words hit 0.420. In higher layers, low-frequency words have higher correlations than high-frequency words.\\n\\nRobBERT's highest overall correlation is 0.207 in Layer 0 and 0.247 when combining Layers 0 and 5. For this model, nouns are the part-of-speech with the highest correlation at 0.263. At layer 0, low-frequency words correlate better with the human judgements than high-frequency words (0.284 and 0.194 respectively).\\n\\nThe trends observed in Bertje's performance align with the expected patterns of Transformer-based models. Early layers usually adequately represent semantic relationships, while later layers focus more on syntactic patterns (Zhang et al., 2018). Layer 0, which corresponds to static word embeddings, yields the highest correlation scores in both models. Combining high-scoring early layers slightly increases correlation with human judgements. Overall, RobBERT-v2 shows lower correlation scores than Bertje.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 1: Scatterplots with predicted model similarities and similarity ratings from Dutch SimLex-999.\\n\\n| Word 1          | Word 2 | SimL | Cos  |\\n|-----------------|--------|------|------|\\n| rondzwerven     | roam   | 7.83 | 0.247|\\n| competentie     | competence | 7.73 | 0.271|\\n| film            | movie  | 3.92 | 0.798|\\n| zelfverzekerd   | confident | 7.49 | 0.295|\\n| elastisch       | elastic | 7.32 | 0.289|\\n| ...             | ...    | ...  | ...  |\\n| botten          | bone   | 1.93 | 0.292|\\n| oud             | old    | 1.69 | 0.278|\\n| rijkdom         | wealth | 2.46 | 0.327|\\n| hart            | heart  | 1.80 | 0.286|\\n| onderzoeken     | investigate | 3.24 | 0.379|\\n\\n(a) Largest and smallest differences for Bertje.\\n\\n| Word 1          | Word 2 | SimL | Cos  |\\n|-----------------|--------|------|------|\\n| aanmoedigen     | encourage | 1.71 | 0.891|\\n| vlug            | quick  | 8.95 | 0.313|\\n| afwezigheid     | absence | 2.54 | 0.878|\\n| desorganiseren  | disorganise | 2.68 | 0.873|\\n| aandacht        | attention | 8.13 | 0.347|\\n| heldin          | heroine | 8.55 | 0.797|\\n| klein           | tiny   | 1.82 | 0.372|\\n| arm             | arm    | 2.05 | 0.389|\\n| hout            | wood   | 3.55 | 0.481|\\n| bad             | bath   | 1.61 | 0.358|\\n\\n(b) Largest and smallest differences for RobBERT.\\n\\nTable 3: The words with the highest and lowest absolute difference in human similarity score and model cosine distance (scaled to the SimLex-999 1-10 rating scale, but only original values shown). Highest differences, i.e. errors, are at the top. The words printed in italics are the equivalent words from the English SimLex-999, they are not necessarily the most obvious translation of the Dutch word.\\n\\n4.2. Error analysis\\nTo gain some qualitative insights into these results, we compare for both models the top 5 worst predicted similarity scores and the top 5 best predicted similarity scores. These results are shown in tables 3a for Bertje and 3b for RobBERT. Word pairs from our Dutch SimLex-999 are shown with their equivalent words from the English SimLex-999. As SimLex similarity ratings are on a scale of 1 to 10 and cosine similarities are on a scale from 0 to 1, we scaled the cosine similarities to a scale of 1 to 10 and then computed the absolute difference between the scaled cosine similarities from the models and the human ratings. The table is sorted by absolute difference, though this value is not shown for reasons of space.\\n\\nWe observe a number of characteristic errors at the top of the table. For both\"}"}
{"id": "lrec-2024-main-1292", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"models, low-frequency words appear at the top of the error list. In particular, RobBERT struggles with predicting antonym relations (aanmoedigen-ontmoedigen, 'encourage-discourage', aanwezigheid-afwezigheid, 'absence-presence'). This suggests that RobBERT encodes relatedness rather than semantic similarity. This is a common problem in distributional semantic modeling and an important motivation for the development of the original SimLex-999 benchmark.\\n\\nConversely, Bertje appears to struggle more with ambiguous words, especially ones that are rated similar by humans. In the pair that is ranked 4, the word zeker 'sure' is also often used as a confirmatory statement in conversations: 'for sure'. In pair 5, the word flexibel 'flexible' has many metaphoric extensions that may be common in internet data. In pair 3, the word filmrol can either mean a role in a movie or a roll of film. In pair 2, the word vermogen 'ability' has extended meanings related to finance and energy. An interesting successful example for Bertje is pair 998, hart-operatie 'heart-surgery', where we might expect interference from the association between these two words, but Bertje correctly represents them as semantically unrelated.\\n\\nBoth models yield better correlations with unrelated terms (scored low by the raters), though this could be an artifact of our scaling procedure.\\n\\n4.3. Effect of the tokenizer\\n\\nTo obtain these similarity scores from Bertje and RobBERT, we made the common assumption that when a word is not in the model's vocabulary, its embedding can be reconstructed by averaging the embeddings of its subtokens, as discussed in section 3.2. However, this assumption may not always hold and this process may affect model performance. We observed a large difference in the number of subtokenized words for both models. Subtokenized words are in some sense out-of-vocabulary (OOV), although this is not quite comparable to OOV words in static embedding models. Contextual embedding models are designed to be used with tokenizers that split infrequent words into subtokens, thus learning a vocabulary that consists of whole-word tokens and subtokens. In this analysis, we make a distinction between such subtokens, and words that are in the model's vocabulary as a whole.\\n\\nWe observe that RobBERT's BPE-based vocabulary contains far fewer of the Dutch SimLex-999 words than Bertje's WordPiece-based vocabulary. BPE builds the vocabulary by considering all symbols used to write words, which results in a higher number of subtokenized words when compared to other tokenisation methods (Sennrich et al., 2016). Of the 999 word pairs, 139 contained words that are subtokenized for Bertje. Some examples included molecuul 'molecule', volbrengen 'to accomplish', rechtvaardigheid 'justice' and afwijken 'to deviate'. For the RobBERT model, 550 words are subtokenized, with examples like erkennen 'to acknowledge', secretaresse 'secretary', somber 'dreary' and aanzien 'prestige'. This far larger reliance on subtokens for Dutch is a potential explanation for RobBERT's lower correlation with human similarity judgements. To investigate this possibility, we also compute the correlations over the in-vocabulary words only, excluding all word pairs containing at least one subtokenized word. These results are shown in figures 4 and 5. Bertje achieved lower correlation scores in this condition. Bertje's highest Spearman correlation reached 0.280 in Layer 0 and 0.276 when combining Layers 0 and 3 in this condition. Conversely, RobBERT's results for subtokenized words are better, reaching 0.461 at layer 0. This is a higher correlation than Bertje's overall result, though it drops off faster in subsequent layers.\\n\\nThis indicates that Bertje benefits from the use of subtokens to handle complex words, while the RobBERT BPE tokenizer might not be optimal for Dutch, even though we used RobBERT v2, which uses a Dutch-specific tokenizer. This appears to be the main reason for RobBERT's performance.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This result also shows that the approach of combining subtoken embeddings is essential for good performance also for Dutch, as it is for a higher-resource language like English (Hu et al., 2019). However, the ideal strategy for composing token embeddings from subtoken embeddings (e.g. averaging, summing or otherwise) may vary depending on the specifics of the task and dataset at hand.\\n\\n**Figure 4:** Spearman Correlation across transformer layers in Bertje without subtokens.\\n\\n**Figure 5:** Spearman Correlation across transformer layers in RobBERT without subtokens.\\n\\nThe data indicates that Bertje generally outperforms RobBERT on the SimLex-999 dataset. Our error analysis shows that RobBERT appears to have more difficulty with distinguishing word similarity from word relatedness, and Bertje appears to have more difficulty with semantically highly ambiguous words. Differences in performance can be attributed to model architectures and training datasets, highlighting how these factors influence the resulting semantic representations.\\n\\n### 5. Discussion\\n\\nIn this work, we have introduced the Dutch SimLex-999 resource, which should put Dutch language technology on a more equal playing field with that of higher-resource languages. Additionally, we provided insights into the performance of two commonly used Dutch language models. Nevertheless, our findings leave several points open to discussion which may have implications for the generalisability and interpretation of the findings.\\n\\n#### 5.1. Comparison to other studies\\n\\n| Study                  | Model  | Corr. |\\n|------------------------|--------|-------|\\n| Leviant & Reichart (2015) | W2V (EN) | 0.266 |\\n| Leviant & Reichart (2015) | W2V (DE) | 0.354 |\\n| Leviant & Reichart (2015) | W2V (IT) | 0.308 |\\n| Leviant & Reichart (2015) | W2V (RU) | 0.260 |\\n| Chronis & Erk (2020)    | BERT (L8) | 0.608 |\\n| Ehrmanntraut et al. (2021) | BERT | 0.476 |\\n| Shahmohammadi et al. (2021) | GloVe | 0.408 |\\n| de Vos et al. (2022)    | BERT | 0.384 |\\n| This Study              | Bertje | 0.421 |\\n| This Study              | RobBERT | 0.247 |\\n\\nTable 4: Comparison of SimLex-999 Spearman correlations across different studies\\n\\nTo better contextualize our results, it is informative to examine the results of similar experiments for other languages. The performance of the Dutch Bertje and RobBERT on the Dutch SimLex-999 dataset is compared to the performance of other models on their respective language\u2019s version of SimLex-999 in Table 4.\\n\\nIn this landscape, Bertje\u2019s peak Spearman correlation of 0.421 (with layer combination) surpasses many of these results, except for the correlation reported by the two English BERT models (Chronis and Erk, 2020; Ehrmanntraut et al., 2021). On the other hand, RobBERT\u2019s highest correlation of 0.247 is notably lower, especially when compared to other BERT-based models.\\n\\nWe can observe that the highest correlation with human similarity judgements was obtained by taking embeddings from the 8th layer of English BERT, though this study of Chronis and Erk (2020) was specialized for lexical-semantic tasks by using centroids of clusters of token embeddings. The additional contextual information this provides likely accounts for the higher correlation, and this would be an interesting direction to explore for Dutch for improving results on the task of semantic relation estimation. In our setup, without any context, layer 0 tended to yield the highest correlations. However, combining layer 0 with another relatively highly correlating layer (layers 3 or 5 for our models) resulted in somewhat higher correlations than using a single layer.\\n\\nMore broadly, our Dutch SimLex-999 dataset enables other cross-linguistic model comparisons involving Dutch to be made in future work. Such\"}"}
{"id": "lrec-2024-main-1292", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"comparisons can discern language-specific characteristics from model-specific influences, thereby advancing multilingual NLP.\\n\\n5.2. Implications for extrinsic tasks\\n\\nPrevious evaluations involving the English SimLex-999 dataset have raised concerns about focusing solely on the \u201cinterpretability\u201d of word embeddings (Gladkova and Drozd, 2016). This intrinsic evaluation approach might not leverage the potential of distributional semantics fully. In response, combining intrinsic evaluations with extrinsic evaluations is recommended to understand the models' real-world applicability. These concerns equally apply to the Dutch version of the dataset. For Bertje and RobBERT, it is also evident that model performance can vary based on the extrinsic tasks they are used for (De Vries et al., 2019; Delobelle et al., 2020). Intrinsic evaluation results do not always correlate with extrinsic evaluation results.\\n\\nFurthermore, differences in tokenisation methods between Bertje and RobBERT influenced the subtokenized word count, and the model that performed worse had far more subtokenized items and poorer performance on those items. This invites the question of what the ideal tokenisation approach for the Dutch language is.\\n\\nThis research primarily centred on the intrinsic evaluation of Dutch language models, a methodology focused on assessing the models' ability to accurately capture semantic relationships between words (Bellegarda, 2000). Integrating extrinsic evaluations usually provides deeper insights when evaluating for a specific task (Foster et al., 2014; Khurana et al., 2023), and may yield different results. For instance, in an extrinsic evaluation of Dutch emotion detection tasks, RobBERT outperformed Bertje (De Bruyne et al., 2021). Moreover, in another task-focused evaluation, Bouma (2021) probed Dutch language models' ability to predict the appropriate use of relative pronouns in complex sentences. While Bertje performed best in the masked language modelling probing task, RobBERT significantly improved fine-tuning, particularly highlighting the model's capacity to adapt and learn from task-specific data. These differential performances indicate the need for the availability and use of multiple evaluation methods.\\n\\n5.3. Further work\\n\\nFine-tuning Bertje and RobBERT on the Dutch SimLex-999 dataset might enhance their performance on semantic similarity tasks, as seen in English models (Shi et al., 2023; Ding et al., 2023). This could lead to improved performance on downstream tasks. Additionally, employing diverse evaluation sets can offer a well-rounded view of a model's capabilities (Alivanistos et al., 2022; Xu et al., 2023).\\n\\nThe evaluation of a broader range of models on Dutch SimLex-999 would provide a better picture of the state of Dutch language technology, including static word embedding models or embeddings from multilingual models such as mBERT. The potential of generative large language models such as the LLAMA and GPT families of models for Dutch remain largely unexplored. Assuming that lexical-semantic representations can be obtained, evaluating such models using the Dutch SimLex-999 dataset could provide insights into their Dutch semantic capabilities, potentially benefiting numerous downstream applications. Dutch SimLex-999 could also be used as a benchmark for intrinsically evaluating multimodal embeddings, as done by Pezzelle et al. (2021) for English, comparing the similarities between vector representations of images to human similarity judgements.\\n\\nThe tokenizers employed by Bertje and RobBERT were mainly designed with English in mind. Using different tokenisation methods, like morphological or character-level tokenisers, might provide better results for the more complex morphology of Dutch (Kettunen, 2014). Such tokenisers could offer meaningful tokens for complex Dutch words or capture nuances of the language more effectively.\\n\\n6. Conclusion\\n\\nBy developing a Dutch version of the SimLex-999 dataset, our work opens up the possibility to carry out intrinsic evaluations of Dutch word embeddings for the first time. Our rating procedure showed high agreement between the raters providing semantic similarity judgements, and the dataset shows significant overlap in semantic similarities with the English and German versions (Hill et al., 2015; Leviant and Reichart, 2015).\\n\\nWe used this dataset to evaluate two prominent Dutch language models, Bertje and RobBERT (De Vries et al., 2019; Delobelle et al., 2020), examining performance per layer, part-of-speech and by lexical frequency. These findings assist in model selection, emphasizing each model's specific strengths and limitations. A qualitative analysis of model performance on specific word pairs from the dataset with specific linguistic properties can help to better understand the strengths and weaknesses of each model.\\n\\nThis dataset advances Dutch natural language processing by offering a broadly applicable benchmark for word embedding quality, and we encourage the community to intrinsically evaluate other Dutch language models using this benchmark.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical considerations and limitations\\n\\nDespite rating the Dutch SimLex-999 dataset with 235 native Dutch speakers, the dataset's broad applicability might be limited due to the demographic characteristics of the participants. Since collection of participants' personal information was restricted by privacy concerns, providing detailed demographic data was impossible. In particular, it is possible that the intuitions of speakers from smaller populations where standard Dutch is used, such as Belgian Dutch and Surinamese Dutch speakers, are less well represented in the dataset.\\n\\nThe translation process from English to Dutch might have brought in biases, and having been done by only 2 experts, lacked diverse representation. While effort was made to maintain meaning, linguistic subtleties and cultural differences between the translators and raters could have influenced the dataset.\\n\\nAlthough MultiSimLex, a multilingual followup to SimLex-999 exists (Vuli\u0107 et al., 2020), we focused on adapting the original dataset to Dutch. As the scope of MultiSimLex is larger, it would have required more resources to adapt even for a single language. However, this restricts the comparability of our results to evaluations done using MultiSimLex.\\n\\nOur evaluation was limited to contextual word embedding models, while most previous benchmarking using the English SimLex-999 involved static word embeddings. Of course, the Dutch SimLex-999 can be used to evaluate static embeddings as well in future work.\\n\\nAs is the case for English, there is no guarantee that higher scores on an intrinsic evaluation benchmark yield higher scores on an extrinsic task (Gladkova and Drozd, 2016). Furthermore, it is important to note that no single metric can summarize all aspects of model quality (Valmeekam et al., 2024). Semantic similarity-based intrinsic evaluation does not tell us anything about the presence of harmful biases in a model, for example. Models are best evaluated on various different tasks.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lee J Cronbach and Paul E Meehl. 1955. Construct validity in psychological tests. *Psychological Bulletin*, 52(4):281.\\n\\nHjalti Dan\u00edelsson, Steinunn Rut Fri\u00f0riksd\u00f3ttir, and Stein\u00fe\u00f3r Steingr\u00edmsson. 2021. Icelandic multi-SimLex (21.06). CLARIN-IS.\\n\\nLuna De Bruyne, Orph\u00e9e De Clercq, and V\u00e9ronique Hoste. 2021. Emotional RobBERT and insensitive BERTje: combining transformers and affect lexica for dutch emotion detection. In Workshop on Computational Approaches to Subjectivity and Sentiment Analysis (WASSA), held in conjunction with EACL 2021, pages 257\u2013263. Association for Computational Linguistics.\\n\\nWietse De Vries, Andreas van Cranenburgh, Arianna Bisazza, Tommaso Caselli, Gertjan van Noord, and Malvina Nissim. 2019. Bertje: A Dutch BERT model. *arXiv preprint arXiv:1912.09582*.\\n\\nWietse de Vries, Martijn Wieling, and Malvina Nissim. 2023. DUMB: A benchmark for smart evaluation of Dutch models. In *Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing*, pages 7221\u20137241.\\n\\nScott Deerwester, Susan T Dumais, George W Furnas, Thomas K Landauer, and Richard A Harshman. 1990. Indexing by latent semantic analysis. *Journal of the American Society for Information Science*, 41(6):391\u2013407.\\n\\nPieter Delobelle, Thomas Winters, and Bettina Berendt. 2020. RobBERT: a Dutch RoBERTa-based Language Model. In *Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 3255\u20133265, Online. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nNing Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su, Shengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-efficient fine-tuning of large-scale pre-trained language models. *Nature Machine Intelligence*, 5(3):220\u2013235.\\n\\nDenis Drieghe and Marc Brysbaert. 2002. Strategic effects in associative priming with words, homophones, and pseudohomophones. *Journal of Experimental Psychology: Learning, Memory, and Cognition*, 28(5):951.\\n\\nAnton Ehrmanntraut, Thora Hagen, Leonard Konle, and Fotis Jannidis. 2021. Type-and token-based word embeddings in the digital humanities. In *CHR*, pages 16\u201338.\\n\\nG\u00f6khan Ercan and Olcay Taner Y\u0131ld\u0131z. 2018. Anlamver: Semantic model evaluation dataset for Turkish word similarity and relatedness. In *Proceedings of the 27th International Conference on Computational Linguistics*, pages 3819\u20133836.\\n\\nMathias Etcheverry and Dina Wonsever. 2016. Spanish word vectors from Wikipedia. In *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC\u201916)*, pages 3681\u20133685.\\n\\nManaal Faruqui, Yulia Tsvetkov, Dani Yogatama, Chris Dyer, and Noah A Smith. 2016. Problems with evaluation of word embeddings using word similarity tasks. In *Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP*, pages 1\u20138.\\n\\nLev Finkelstein, Evgeniy Gabrilovich, Yossi Matias, Ehud Rivlin, Zach Solan, Gadi Wolfman, and Eytan Ruppin. 2001. Placing search in context: The concept revisited. In *Proceedings of the 10th international conference on World Wide Web*, pages 406\u2013414.\\n\\nMary Ellen Foster, Manuel Giuliani, and Amy Isard. 2014. Task-based evaluation of context-sensitive referring expressions in human\u2013robot dialogue. *Language, Cognition and Neuroscience*, 29(8):1018\u20131034.\\n\\nAnna Gladkova and Aleksandr Drozd. 2016. Intrinsic evaluations of word embeddings: What can we do better? In *Proceedings of the 1st Workshop on Evaluating Vector-Space Representations for NLP*, pages 36\u201342.\\n\\nZellig Harris. 1954. Distributional structure. *Word*, 10(2-3):146\u2013162.\\n\\nSimon Hengchen and Nina Tahmasebi. 2021. SuperSim: a test set for word similarity and relatedness in Swedish. In *Proceedings of the 23rd Nordic Conference on Computational Linguistics (NoDaLiDa)*, pages 268\u2013275.\\n\\nFelix Hill, Roi Reichart, and Anna Korhonen. 2015. Simlex-999: Evaluating semantic models with (genuine) similarity estimation. *Computational Linguistics*, 41(4):665\u2013695.\\n\\nZiniu Hu, Ting Chen, Kai-Wei Chang, and Yizhou Sun. 2019. Few-shot representation learning\"}"}
{"id": "lrec-2024-main-1292", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for out-of-vocabulary words. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4102\u20134112, Florence, Italy. Association for Computational Linguistics.\\n\\nKeisuke Inohara and Akira Utsumi. 2022. JWSAN: Japanese word similarity and association norm. Language Resources and Evaluation, pages 1\u201329.\\n\\nKimmo Kettunen. 2014. Can type-token ratio be used to show morphological complexity of languages? Journal of Quantitative Linguistics, 21(3):223\u2013245.\\n\\nEmmanuel Keuleers, Marc Brysbaert, and Boris New. 2010. Subtlex-nl: A new measure for Dutch word frequency based on film subtitles. Behavior research methods, 42(3):643\u2013650.\\n\\nDiksha Khurana, Aditya Koli, Kiran Khatter, and Sukhdev Singh. 2023. Natural language processing: State of the art, current trends and challenges. Multimedia tools and applications, 82(3):3713\u20133744.\\n\\nClaudia Kittask and Eduard Barbu. 2019. Is similarity visually grounded? Computational model of similarity for the Estonian language. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 541\u2013549, Varna, Bulgaria. INCOMA Ltd.\\n\\nVirginia Klema and Alan Laub. 1980. The singular value decomposition: Its computation and some applications. IEEE Transactions on automatic control, 25(2):164\u2013176.\\n\\nTom\u00e1\u0161 Kliegr and Ond\u0159ej Zamazal. 2018. Antonyms are similar: Towards paradigmatic association approach to rating similarity in SimLex-999 and WordSim-353. Data & Knowledge Engineering, 115:174\u2013193.\\n\\nTerry K Koo and Mae Y Li. 2016. A guideline of selecting and reporting intraclass correlation coefficients for reliability research. Journal of chiropractic medicine, 15(2):155\u2013163.\\n\\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253.\\n\\nThomas K Landauer and Susan T Dumais. 1997. A solution to plato's problem: The latent semantic analysis theory of acquisition, induction, and representation of knowledge. Psychological review, 104(2):211.\\n\\nIra Leviant and Roi Reichart. 2015. Separated by an un-common language: Towards judgment language informed vector space modeling. arXiv preprint arXiv:1508.00106.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized BERT pre-training approach.\\n\\nLigeia Lugli, Matej Martinc, Andra\u017e Pelicon, and Senja Pollak. 2022. Embeddings models for Buddhist Sanskrit. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 3861\u20133871.\\n\\nOlga Majewska, Diana McCarthy, Jasper JF van den Bosch, Nikolaus Kriegeskorte, Ivan Vuli\u0107, and Anna Korhonen. 2021. Semantic data set construction from human clustering and spatial arrangement. Computational Linguistics, 47(1):69\u2013116.\\n\\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.\\n\\nNikola Mrk\u0161i\u0107, Ivan Vuli\u0107, Diarmuid \u00d3 S\u00e9aghdha, Ira Leviant, Roi Reichart, Milica Ga\u0161i\u0107, Anna Korhonen, and Steve Young. 2017. Semantic specialization of distributional word vector spaces using monolingual and cross-lingual constraints. Transactions of the association for Computational Linguistics, 5:309\u2013324.\\n\\nIqra Muneer, Ghazeefa Fatima, Muhammad Salman Khan, Rao Muhammad Adeel Nawab, and Ali Saeed. 2023. Developing a large benchmark corpus for Urdu semantic word similarity. ACM Transactions on Asian and Low-Resource Language Information Processing, 22(3):1\u201319.\\n\\nAgnieszka Mykowiecka, Malgorzata Marciniak, and Piotr Rychlik. 2018. Simlex-999 for Polish. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).\\n\\nPonrudee Netisopakul, Gerhard Wohlgenannt, and Aleksei Pulich. 2019. Word similarity datasets for Thai: Construction and evaluation. IEEE Access, 7:142907\u2013142915.\\n\\nJeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. GloVe: Global vectors for word representation. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 1532\u20131543.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sandro Pezzelle, Ece Takmaz, and Raquel Fern\u00e1ndez. 2021. Word representation learning in multimodal pre-trained transformers: An intrinsic evaluation. \\n\\nTransactions of the Association for Computational Linguistics, 9:1563\u20131579. \\n\\nSenja Pollak, Ivan Vuli\u0107, Andra\u017e Pelicon, Andra\u017e Repar, Carlos Armendariz, Purver Matthew, and Nikola Ljube\u0161i\u0107. 2020. Simlex-999 Slovenian translation SimLex-999-sl 1.0. \\n\\nQualtrics. 2023. Qualtrics survey software. Accessed: 2023-05-24. \\n\\nAndreia Querido, Rita Carvalho, Jo\u00e3o Rodrigues, Marcos Garcia, Jo\u00e3o Silva, Catarina Correia, Nuno Rendeiro, Rita Valadas Pereira, Marisa Campos, and Ant\u00f3nio Branco. 2017. LX-LR4DistSemEval: a collection of language resources for the evaluation of distributional semantic models of Portuguese. \\n\\nRevista da Associa\u00e7\u00e3o Portuguesa de Lingu\u00edstica, (3):265\u2013283. \\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by generative pre-training. \\n\\nPhilip Resnik. 1995. Using information content to evaluate semantic similarity in a taxonomy. In Proceedings of the 14th international joint conference on Artificial intelligence-Volume 1, pages 448\u2013453. \\n\\nAnna Rogers, Olga Kovaleva, and Anna Rumshisky. 2021. A primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842\u2013866. \\n\\nUlugbek Salaev, Elmurod Kuriyozov, and Carlos G\u00f3mez-Rodr\u00edguez. 2022. SimRelUz: Similarity and relatedness scores as a semantic evaluation dataset for Uzbek language. In Proceedings of the 1st Annual Meeting of the ELRA/ISCA Special Interest Group on Under-Resourced Languages, pages 199\u2013206. \\n\\nGerard Salton, Anita Wong, and Chung-Shu Yang. 1975. A vector space model for automatic indexing. Communications of the ACM, 18(11):613\u2013620. \\n\\nRaphael Scheible, Fabian Thomczyk, Patric Tippmann, Victor Jaravine, and Martin Boeker. 2020. GottBERT: a pure German language model. \\n\\nMike Schuster and Kaisuke Nakajima. 2012. Japanese and Korean voice search. In 2012 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 5149\u20135152. IEEE. \\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Neural machine translation of rare words with subword units. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715\u20131725, Berlin, Germany. Association for Computational Linguistics. \\n\\nYiwen Shi, Jing Wang, Ping Ren, Taha ValizadehAslani, Yi Zhang, Meng Hu, and Hualou Liang. 2023. Fine-tuning BERT for automatic ADME semantic labeling in FDA drug labeling to enhance product-specific guidance assessment. Journal of Biomedical Informatics, page 104285. \\n\\nC. Spearman. 1904. The proof and measurement of association between two things. The American Journal of Psychology, 15(1):72\u2013101. \\n\\nSt\u00e9phan Tulkens, Chris Emmery, and Walter Daelemans. 2016. Evaluating unsupervised Dutch word embeddings as a linguistic resource. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 4130\u20134136. \\n\\nPeter D Turney and Patrick Pantel. 2010. From frequency to meaning: Vector space models of semantics. Journal of artificial intelligence research, 37:141\u2013188. \\n\\nKarthik Valmeekam, Matthew Marquez, Sarath Sreedharan, and Subbarao Kambhampati. 2024. On the planning abilities of large language models-a critical investigation. Advances in Neural Information Processing Systems, 36. \\n\\nBui Van Tan, Nguyen Phuong Thai, and Pham Van Lam. 2017. Construction of a word similarity dataset and evaluation of word similarity techniques for Vietnamese. In 2017 9th International Conference on Knowledge and Systems Engineering (KSE), pages 65\u201370. IEEE. \\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in Neural Information Processing Systems, 30. \\n\\nViljami Venekoski and Jouko Vankka. 2017. Finnish resources for evaluating language model semantics. In Proceedings of the 21st Nordic conference on computational linguistics, pages 231\u2013236. \\n\\nIvan Vuli\u0107, Simon Baker, Edoardo Maria Ponti, Ulla Petti, Ira Leviant, Kelly Wing, Olga Majewska, Eden Bar, Matt Malone, Thierry Poibeau, et al. 2020. Multi-SimLex: A large-scale evaluation of multilingual and crosslingual lexical semantic.\"}"}
{"id": "lrec-2024-main-1292", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. Code and dataset\\nThe following link provides access to the repository containing the Dutch SimLex-999 dataset, as well as the code used for evaluating Bertje and RobBERT:\\nhttps://github.com/lizzybrans/Simlex999-Dutch\\n\\nB. Annotator instructions\\nFigure 6: Instructions Dutch SimLex-999\\nThe annotation instructions were translated from (Hill et al., 2015). The instructions emphasised that participants should rate word pairs based on similarity rather than relatedness and provided clear examples to guide them.\"}"}
