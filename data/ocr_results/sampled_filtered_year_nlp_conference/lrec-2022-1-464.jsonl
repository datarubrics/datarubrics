{"id": "lrec-2022-1-464", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Warm Start and a Clean Crawled Corpus -\\nA Recipe for Good Language Models\\n\\nV\u00e9steinn Sn\u00e6bjarnarson\\n1, 2, *\\nHaukur Barri S\u00edmonarson\\n1, 2, *\\nP\u00e9tur Orri Ragnarsson\\n1\\nSvanhv\u00edt Lilja Ing\u00f3lfsd\u00f3ttir\\n1\\nHaukur P\u00e1ll J\u00f3nsson\\n1\\nVilhj\u00e1lmur \u00deorsteinsson\\n1\\nHafsteinn Einarsson\\n2\\n\\n1 Mi\u00f0eind ehf.,\\n2 University of Iceland\\n\\n{vesteinn, haukur, petur, svanhvit, haukurpj, vt}@mideind.is, hafsteinne@hi.is\\n\\nAbstract\\nWe train several language models for Icelandic, including IceBERT, that achieve state-of-the-art performance in a variety of downstream tasks, including part-of-speech tagging, named entity recognition, grammatical error detection and constituency parsing. To train the models we introduce a new corpus of Icelandic text, the Icelandic Common Crawl Corpus (IC3), a collection of high quality texts found online by targeting the Icelandic top-level-domain.is. Several other public data sources are also collected for a total of 16GB of Icelandic text. To enhance the evaluation of model performance and to raise the bar in baselines for Icelandic, we manually translate and adapt the WinoGrande commonsense reasoning dataset. Through these efforts we demonstrate that a properly cleaned crawled corpus is sufficient to achieve state-of-the-art results in NLP applications for low to medium resource languages, by comparison with models trained on a curated corpus. We further show that initializing models using existing multilingual models can lead to state-of-the-art results for some downstream tasks.\\n\\nKeywords: language model, Icelandic, IceBERT, corpus, part of speech, named entity recognition, parsing, co-reference resolution, natural language understanding\\n\\n1. Introduction\\nThe Government of Iceland recently launched an initiative to improve the state of Icelandic language resources and language technology (Nikul\u00e1sd\u00f3ttir et al., 2020). This comprehensive program has its roots in the historical focus on protecting the Icelandic language (Kristinsson, 2018), and, as a result, work has been ongoing to build and enhance said resources. This effort is gradually pushing Icelandic from being a low-resource language to a medium-resource one. Still, and apart from large monolingual corpora (Steingr\u00edmsson et al., 2018), many important types of resources are lacking in comparison with major languages such as English.\\n\\nParallel to the development of the Icelandic language technology program, language technology worldwide has been progressing at a fast and accelerating pace (Bommasani et al., 2021). Pre-trained neural language models based on Transformers (Vaswani et al., 2017) have shown impressive results when adapted for a variety of classification and text generation tasks. Such models are now applied widely across industries and modalities.\\n\\nLarge monolingual language models such as BERT (Devlin et al., 2019), BART (Lewis et al., 2020) and GPT-2 (Radford et al., 2019) have been developed for English. For many smaller languages the only available options are multilingual models, which can reach impressive performance on downstream tasks (Conneau et al., 2020). These are not without their flaws though; when compared to training on a sufficiently large monolingual corpus in a given language, multilingual models can lead to less than optimal performance, as demonstrated in the case of Finnish (Virtanen et al., 2019). Since an evaluation of Transformer language models for Icelandic is yet to be completed, it has remained unclear to what extent this holds for Icelandic.\\n\\nThe data used to train language models is usually sourced from large collections of books (e.g. (Zhu et al., 2015)) and online texts, where the choice and quality of training data can potentially have a large effect on downstream task performance. While curated corpora may not be readily available for a language, it might still be relatively well represented online, in the form of web texts, which can be sourced by automatic means. This raises the question of whether language models trained on curated corpora offer better performance in downstream tasks than those trained predominantly on data sourced from the web.\\n\\nIn this paper, we show how a Transformer model, IceBERT, can be trained for Icelandic with relatively modest language resources to reach state-of-the-art performance across a variety of tasks. We train multiple models on monolingual corpora from different sources: a curated corpus (Icelandic Gigaword Corpus, IGC (Steingr\u00edmsson et al., 2018)) and a corpus of text collected efficiently from Common Crawl.\\n\\nWe train separate models on each of these two sources, which provides insight into the utility of each type of training data and the potential for combining them. By comparing the performance of models trained on the curated and crawled corpora, we can evaluate the effectiveness of each source. This allows us to determine whether the quality of the training data is more important than the quantity, and whether a reliable crawl can replace a curated corpus.\\n\\nIn addition to comparing the performance of models trained on different types of data, we also investigate the impact of initialization on model performance. By initializing models with weights from pre-trained multilingual models, we can assess whether this approach leads to better performance for some downstream tasks. This provides valuable insights into the benefits and limitations of using pre-trained models for Icelandic language technology.\\n\\nBy presenting these findings, we contribute to the ongoing effort to improve the state of Icelandic language resources and language technology. Our work highlights the importance of high quality training data and the potential for combining different sources of information to achieve state-of-the-art performance in NLP applications for low to medium resource languages. It also demonstrates the effectiveness of pre-trained multilingual models as a potential solution for languages with limited resources.\\n\\nKeywords: language model, Icelandic, IceBERT, corpus, part of speech, named entity recognition, parsing, co-reference resolution, natural language understanding\"}"}
{"id": "lrec-2022-1-464", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and compare results, to demonstrate the feasibility of our approach for other languages with similar resource availability. To our surprise, models trained on texts extracted from the web achieved similar performance to models trained on a curated corpus. We also evaluate the performance of a multilingual model (XLMR-base, Conneau et al. (2020)), which shows good results for some tasks but is insufficient for others. Finally, we use the existing multilingual model as a warm start and continue pre-training on Icelandic text, reaching state-of-the-art results in downstream tasks such as NER.\\n\\nWhile large corpora of multilingual text exist, such as the multilingual Colossal Clean Crawled Corpus (mC4) (Xue et al., 2021) that is sourced from the Common Crawl, they have not been officially released in a way such that text in smaller languages can be easily extracted. However, the mC4 dataset has been made available by a third party which we use for our experiments. Additionally, we demonstrate how to directly extract Icelandic text from the Common Crawl in a novel way, explain how it can be done for other languages, and highlight the importance of clean data.\\n\\nRegarding applicability of our approach to other languages, we note that in mC4, there are 107 labelled languages, with almost half of the 6.7 billion documents being in English. The average number of documents in languages other than English is 33.4 million documents per language, and the median value is 2 million documents, which happens to be the approximate number for Icelandic (Xue et al., 2021). These numbers show that our approach of extracting training data should be well within reach of at least half of the languages in Common Crawl, and possibly applicable for the 46 languages containing between 500 thousand and 10 million documents.\\n\\nThe key contributions of our work are summarized below.\\n\\n(a) Several Icelandic language models, including IceBERT, trained on a monolingual corpus with 2.7B tokens.\\n(b) Adaptations of IceBERT with state-of-the-art results for part-of-speech tagging (PoS), named entity recognition (NER), constituency parsing and grammatical error detection (GED).\\n(c) The Icelandic Common Crawl Corpus (IC3), a cleaned and deduplicated corpus extracted by targeting the .is top level domain.\\n(d) The Icelandic WinoGrande dataset (IWG), a new and challenging benchmark for commonsense reasoning and natural language understanding.\\n\\n[3] huggingface.co/datasets/mc4\\n[4] Available at huggingface.co/mideind.\\n[5] We have made the dataset available at https://huggingface.co/datasets/mideind/icelandic-common-crawl-corpus-IC3\\n[6] Will be made available on the Icelandic CLARIN repository repository.clarin.is.\\n\\n2. Related work\\n\\nThe original BERT model has, since its publishing, spawned a whole family of BERT-like models. One of the main reasons for their popularity is their potential for transfer learning, i.e. the possibility to adapt them and obtain impressive performance on benchmarks and tasks that they were not originally trained for. Multilingual versions of BERT exist that are trained on text in multiple languages, such as mBERT, which is trained on Wikipedia in over a hundred languages, including Icelandic. Since the release of BERT, other large pre-trained models such as mBART (Liu et al., 2020) and XLMR (Conneau and Lample, 2019; Conneau et al., 2020) have been trained that include Icelandic and other lower-resource languages. In addition, mT5 (Xue et al., 2021), a sequence-to-sequence model, is trained on the entire mC4 corpus.\\n\\nMultilingual models are often the only option for low-resource languages, which do not have direct access to sufficient language data or computational resources to create monolingual transformer-based language models. Such multilingual models have been shown to have useful properties, including zero-shot crosslingual transfer. That is, fine-tuning these models on a downstream task in one language can translate to improved performance in other languages without explicit crosslingual signals (Pires et al., 2019; Wu and Dredze, 2019; Karthikeyan et al., 2019).\\n\\nDespite the impressive results for multilingual models, they might not be the right choice where output accuracy is critical. For some languages, it may be better to pre-train a model on a monolingual corpus and adapt it for downstream tasks rather than to adapt a model trained on a multilingual corpus, as in the case of Finnish (Virtanen et al., 2019). It has also been shown that the crosslingual capabilities of mBERT only apply to high-resource languages (Wu and Dredze, 2020). Furthermore, benchmarks of crosslingual transfer show a sizable gap in the performance of crosslingually transferred models when compared to monolingually trained ones (Hu et al., 2020). These results highlight the still basic need for more training data in the case of medium- and low-resource languages.\\n\\nAs a result, work has been ongoing in establishing baselines and mapping the performance of monolingual models. Some of that work on high-resource languages is summarized by Scheible et al. (2020). We highlight examples of published monolingual models for medium and low-resource languages along with English in Table 1 with an emphasis on languages with resources similar to Icelandic in mC4. Generally, the model building approach is similar, although we note that in one case a multilingual model was used as a warm start (Ralethe, 2020).\\n\\nWe would also like to point out that for several languages of similar size to Icelandic (2.6B tokens) in mC4, there is no public monolingual BERT model available. These include Maltese (5.2B tokens),...\"}"}
{"id": "lrec-2022-1-464", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Language models trained on a monolingual corpus. We highlight some languages that have a similar number of tokens to Icelandic in mC4. The number of speakers denotes the number of native speakers (L1) according to the Wikipedia page for each language.\\n\\n| Language   | Tokens       | Unique Speakers |\\n|------------|--------------|-----------------|\\n| English    | 2,733        | 380M            |\\n| Icelandic  | 2.6          | 350k            |\\n| Galician   | 2.4          | 2.4M            |\\n| Urdu (Roman)| 2.4          | 70M             |\\n| Filipino   | 2.1          | 23.8M           |\\n| Afrikaans  | 1.7          | 7.2M            |\\n| Basque     | 1.4          | 900k            |\\n| Telugu     | 1.3          | 83M             |\\n| Latin      | 1.3          | 0 (Bamman and Burns, 2020) |\\n| Swahili    | 1.0          | 18M             |\\n\\nFor other languages such as Macedonian (1.8B tokens), Malayalam (1.8B tokens), Mongolian (2.7B tokens), and Kannada (1.1B tokens) models exist online but to our best knowledge no publications exist that thoroughly document their performance on basic downstream tasks, such as PoS tagging and NER.\\n\\nAnother line of research has focused on how to make multilingual models more effective for low-resource languages. It has been shown that training on a larger corpus, such as filtered Common Crawl data, leads to significant improvements in downstream tasks, but increasing the number of languages beyond a certain point has a diluting effect that reduces overall performance (Conneau et al., 2020). Others have shown that vocabulary extension of a multilingual model with continued pre-training on a monolingual corpus leads to improved performance and shorter training times than when starting from scratch (Wang et al., 2020).\\n\\nThe results on multilingual models indicate that language model performance is related to the amount of training data for the given language (Xue et al., 2021), and studies on corpus quality indicate that the results are strongly related to the number of high quality sentences (Kreutzer et al., 2022). Kreutzer et al. (2022) have emphasized the importance of evaluating and auditing the corpora that are publicly available, since data in low-resource languages from multilingual datasets can be of low quality. They further emphasize the importance of developing high-quality evaluation datasets, since low-quality benchmarks might exaggerate model performance, making NLP for low-resource languages look further developed than it actually is.\\n\\n2.1. NLP for Icelandic\\n\\nIcelandic is a language from the West Germanic language family, with a rich morphology, where nouns, adjectives and verbs are highly inflected, and compounding is used actively to construct new words. The status of language data and resources for Icelandic is steadily improving, providing us with various datasets for evaluating our models, and benchmarks to measure against.\\n\\nA good deal of work has been done on NLP for Icelandic that concerns these benchmarks. PoS tagging is implemented using a rule-based approach in the IceNLP toolkit (Loftsson and R\u00f6gnvaldsson, 2007), and using a Bi-LSTM model in the ABLTagger (Steingr\u00edmsson et al., 2019). Constituency parsing has been implemented using a hand-crafted context-free grammar in the Greynir package (\u00deorsteinsson et al., 2019), using finite-state transducers in IceParser (Loftsson and R\u00f6gnvaldsson, 2007), and using an mBERT model in (Arnard\u00f3ttir and Ingason, 2020). NER for Icelandic has been implemented using a Bi-LSTM model and an ensemble tagger (Ing\u00f3lfsd\u00f3ttir et al., 2020).\\n\\n3. Training data\\n\\nThe Icelandic datasets used for pre-training our models are listed in Table 2. They were randomly split at the document level into train (80%), validation (5%) and holdout (15%) splits, and then tokenized (\u00deorsteinsson et al., 2020). We do not use the large holdout in this paper but reserve it for future experiments. We also do experiments on the Icelandic subset of the mC4 dataset (not shown in the table, see Section 3.2), a dataset that is, in similar fashion to IC3, extracted from the Common Crawl but via a different method.\\n\\nThe IGC (Steingr\u00edmsson et al., 2018) is the most extensive collection of curated Icelandic text available. The IGC is mostly made up of news, legal documents and other copy-edited content and might, therefore, not accurately reflect the distribution of text from online sources. To supplement this dataset, several other sources were collected for pre-training the language models, as listed in Table 2. At the time of training the model, large collections of Icelandic literature were not available through legal means, but the recently updated IGC now includes some literary texts (Barkar-\"}"}
{"id": "lrec-2022-1-464", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Icelandic texts used for training.\\n\\n| Dataset            | Size   | Tokens |\\n|--------------------|--------|--------|\\n| IGC (editorial text) | 8.2GB  | 1,388M |\\n| IC3 (cleaned webcrawl) | 4.9GB  | 824M   |\\n| Student theses      | 2.2GB  | 367M   |\\n| Greynir News articles | 456MB  | 76M    |\\n| Medical library     | 33MB   | 5.2M   |\\n| Open Icelandic e-books | 14MB   | 2.6M   |\\n| Icelandic Sagas     | 9MB    | 1.7M   |\\n| **Total**            | **15.8GB** | **2,664M** |\\n\\nson et al., 2021b), which will be incorporated in future models. Social media and internet forum texts have now also been added to the updated IGC (Barkarson et al., 2021a).\\n\\nThe IC3, our corpus of scraped and cleaned web texts (see next section for details), contains large amounts of text of many domains, topics, and styles at varying degrees of polish, and thus serves well as a complement to the IGC. In addition to the already mentioned data, academic texts found in student theses and data from the medical library of the University Hospital of Iceland were collected. The academic texts were passed through a filter reminiscent of the one used for the IC3 described in the next section, after an initial PDF text-extraction step. We also use texts scraped from Icelandic online news sites by the Greynir NLP engine.\\n\\n3.1. The Icelandic Common Crawl Corpus\\n\\nThe Common Crawl Foundation is a non-profit organization that scrapes large semi-random subsets of the internet regularly and hosts timestamped and compressed dumps of the web online. Each dump contains billions of web pages occupying hundreds of terabytes. Parsing these files directly requires storage and computing power not directly available to most and can come at a significant financial cost. The foundation also hosts indices of URIs and their locations within the large zipped dump files. While these indices are also large, their processing is feasible with a few terabytes of storage.\\n\\n3.1.1. Extracting Icelandic Common Crawl data\\n\\nThe Common Crawl indices, which contain URI and byte offsets within the compressed dumps, are used to reduce the search space when looking for Icelandic texts. The Common Crawl Index Server has a public API where URIs can be queried based on attributes such as date, MIME-type and substring. Using the API eliminates the need to fetch the massive index files.\\n\\nTo extract Icelandic, the .is pattern is targeted to match the Icelandic top level domain (TLD), resulting in 63.5 million retrieved pages with URIs and byte locations within the compressed Common Crawl dumps. The computational efficiency of our method can be attributed to these steps. Given the predominant use of the .is TLD for Icelandic web content, we assume that other TLDs have a much lower proportion of Icelandic content. That said, a nontrivial amount of text in Icelandic is still likely to be found outside the .is domain and could be extracted by, e.g., parsing the whole Common Crawl, albeit at a much higher computational cost.\\n\\nBy targeting only the byte-offsets corresponding to the Icelandic TLD we extract candidate websites that have a high proportion of Icelandic content. In total, the compressed content is 687GiB on disk. All dumps since the start of the Common Crawl in 2008 until March 2020 were included.\\n\\nPlain text was extracted from the collected WARC (Web Archive format) files using jusText (Pomik\u00e1lek, 2011) to remove boilerplate content and HTML tags.\\n\\n3.1.2. Processing Common Crawl\\n\\nOnce plain text had been extracted from the WARC files, Icelandic text was taken aside and duplicates removed. Since the .is TLD contains text in numerous languages, we use a fastText (Bojanowski et al., 2017) model for extracting Icelandic text. Since the web is abundant with duplicate or near duplicate content, the data is first deduplicated at the document level and then at the inter-sentence level by sliding a three-line window over the text. If any three consecutive lines have appeared together previously, they are discarded. This latter step removes a fair amount of unwanted content, such as cookie notifications and thumbnail text. A summary of the filtering steps taken is shown in Table 3.\\n\\n| Filtering step       | Size   | %    |\\n|----------------------|--------|------|\\n| .is TLD              | 687GB  | 100% |\\n| IS lang. filter and boilerpl. rem. | 29GB  | 4.2% |\\n| Dedup. document      | 8.6GB  | 1.3% |\\n| Dedup. window        | 4.9GB  | 0.71%|\\n\\nTable 3: Filtering steps and retained data for IC3.\\n\\n3.1.3. Comparison between IGC and IC3\\n\\nThe two corpora, IC3 and IGC, are significantly different at the level of individual words. There are 1,155k unique tokens in the IC3 and 1,434k unique tokens in IGC, of which only 818k are shared. Almost one-third of the unique tokens (337k) in the IC3 are not...\"}"}
{"id": "lrec-2022-1-464", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"present in IGC, and almost half of the IGC tokens (616k) are not present in IC3.\\n\\n3.2. The Icelandic part of mC4\\n\\nThe Icelandic part of mC4 (mC4-is) contains 2.6B tokens (~8GB on disk). The data was not filtered in the same way as the IC3 which is reflected by the masked token perplexity results shown in Table 4; we hypothesize that further processing would be necessary to make use of it. By eyeing a random subset of the data, we see that a fair amount is badly machine-translated and some segments contain a lot of noise that is non-alphanumeric or otherwise not fluid text. If further processed, the results might very well be similar to that of IC3, but this analysis is left as future work.\\n\\n4. Training language models\\n\\nWe train four different BERT-base models following the hyperparameter setup in RoBERTa (Liu et al., 2019). We use 48 Nvidia 32GB V100 GPUs for approximately two days or 225k updates, with a batch size of ~955k tokens (2k sequences). We also train a single model using the RoBERTa-large architecture. This model became unstable in training after 37.5k steps with a batch size of ~4M tokens (8k sentences) and we did not make attempts to further improve it, but we fine-tune it in our experiments for comparison with the base models. All models use the same BPE-vocabulary, containing 50k tokens, constructed in the same way as the RoBERTa vocabulary. We train the models using four different data settings: all of the data available except mC4-is (IceBERT and IceBERT-large); the IC3 dataset (IceBERT-IC3), the IGC dataset (IceBERT-IGC); and mC4-is (IceBERT-mC4-is).\\n\\nFurthermore, we evaluate performance using the multilingual XLMR-base (Conneau et al., 2020) model as-is. We also experiment with continued pre-training on the IC3 corpus. Two models are trained: one for 100k steps with a batch size of 40k tokens and the other for 225k steps with a batch size of 80k tokens. The first model took one GPU-day in training and the other seven GPU-days. We do this to show what performance can be gained from leveraging a publicly available multilingual model while minimizing the computation cost as the seven day model uses about 8% of the GPU hours used for training of the IceBERT-base models.\\n\\n5. Results\\n\\nWe fine-tune and adapt the different IceBERT models for several classification and parsing tasks with state-of-the-art results after fine-tuning. Where F-scores are reported they are macro-averaged.\\n\\nFor PoS labelling and constituency parsing we extend the fairseq library; the resulting package greynirseq has been made available. We use the implementation in fairseq to evaluate performance on the Icelandic WinoGrande dataset.\\n\\nFor named entity recognition and grammatical error detection, we use the transformers library from Hugging Face (Wolf et al., 2020). For this purpose, we convert IceBERT to work with the library.\\n\\n5.1. Part of Speech\\n\\nWe fine-tune our models for PoS tagging using greynirseq on the MIM-GOLD (Barkarson et al., 2020) dataset using ten-fold cross validation. The best performing models reach an accuracy of 98.4%. We exclude the x (not analyzed due to e.g. incorrect spelling) and e (foreign) labels. In comparison, Steingr\u00edmsson et al. (2019) achieve 94.04% accuracy. The results are shown in Table 5.\\n\\nIn contrast to prior work on Icelandic PoS tagging, which universally approaches this as a multi-class classification task, we use a multi-label multi-class approach where we predict grammatical categories (gender, tense, etc.) independently instead of all together in one label. We adopt this approach to address a significant label scarcity problem in the training set and to allow for better generalization. See appendix 7 for a more comprehensive description.\\n\\nWe train the PoS models with a batch size of 32 sentences for 5 epochs. Peak learning rate is 5-e5 with approximately 0.2 epochs for warmup and a linear decay to zero. For the randomly initialized (no pretraining) model we do an additional longer run on the data since the model was clearly nowhere near convergence after 5 epochs, this was only done on a single split of data due to time constraints.\\n\\nAll of the Icelandic models (except the one trained on mC4-is) show similar results of ~98.3% accuracy. An informal review of the errors that the best models make leads us to believe that this performance is about as good as it gets with this dataset and model architecture. The majority of errors can be classified as either being problems with the reference data or due to inherently ambiguous sentences. Problems with the reference data...\"}"}
{"id": "lrec-2022-1-464", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Comparison of PoS-tagging performance for the models considered, including randomly initialized models.\\n\\n- **IceTagger (2007)**: 91.5 \u00b1 0.0\\n- **ABLTagger (2019)**: 95.15 \u00b1 0.0\\n- **IceBERT**: 98.33 \u00b1 0.05\\n- **IceBERT-large**: 98.35 \u00b1 0.06\\n- **IceBERT-IGC**: 98.27 \u00b1 0.05\\n- **IceBERT-IC3**: 98.30 \u00b1 0.05\\n- **IceBERT-mC4-is**: 97.62 \u00b1 0.10\\n- **XLMR-base**: 96.70 \u00b1 0.15\\n- **XLMR-base-IC3-1d**: 97.20 \u00b1 0.10\\n- **XLMR-base-IC3-7d**: 98.20 \u00b1 0.07\\n- **No pretraining (5 epochs)**: 74.96 \u00b1 0.54\\n- **No pretraining (50 epochs)**: 90.27 \u00b1 0.0\\n\\n**5.2. Named Entity Recognition**\\n\\nWhen fine-tuning IceBERT for named entity recognition (NER) on the Icelandic NER dataset (Ing\u00f3lfsd\u00f3ttir et al., 2020), it reaches state-of-the-art performance, showing a considerable improvement over the prior result of 85.79 macro F1-score achieved by Ing\u00f3lfsd\u00f3ttir et al. (2020). In fine-tuning we use a batch size of 16 sentences, peak learning rate of 2e-5 and chose the highest performing model on the validation set as measured across 10 epochs. The results over the test set for the different models each averaged over five seeds are shown in table 6.\\n\\nThe results are similar for all monolingual models and show that a lot of data or curated editorial corpora in pre-training are not necessary to achieve competitive NER performance. We would like to highlight that the XLMR-base multilingual model trained for 7-days on IC3 performs best on this task.\\n\\n**5.3. Constituency parsing**\\n\\nWe implement a simplified version of the CKY-style chart parser described by Kitaev et al. (2019). We did not implement position-factored attention nor incorporate any extra word features, such as PoS or character information, since our goal is primarily to measure the knowledge captured by the model. We leave such experiments for future work.\\n\\nThe dataset we use is GreynirCorpus (\u00deorsteinsson et al., 2021), a constituency annotated version of the aforementioned Greynir News dataset, whose test and validation sets are human-annotated. Its annotation scheme comes from the Greynir rule-based parser (\u00deorsteinsson et al., 2019) and shares many similarities with the Penn Treebank-derived (Marcus et al., 1993) schemas and their corresponding annotation guidelines.\\n\\nA generalized version of the GreynirCorpus test set was created for a fairer comparison with previous parsers for Icelandic, namely the Greynir parser, IceParser (Loftsson and R\u00f6gnvaldsson, 2007) \u2014 a shallow parser for Icelandic, and a variant of the Berkeley Neural Parser (Arnard\u00f3ttir and Ingason, 2020) which comprises a multilingual BERT fine-tuned on the Icelandic Parsed Historical Corpus (IcePaHC) (R\u00f6gnvaldsson et al., 2012).\\n\\nWe split the validation subset of the GreynirCorpus into ad-hoc train and validation splits and train on the respective portion. Results from testing on the generalized benchmark mentioned above are shown in Table 7 as measured using evaluation of bracketing (EVALB), which is a score based on matching brackets (Sekine and Collins, 2008). Somewhat surprisingly, the large model does not show best performance on the task, we believe that this would change if the model is trained to convergence or better hyperparameter tuning. We note that differences between models are only slight as all the models are within 2 percentage points from each other. For extra comparison, a randomly initialized model did not surpass 70 F1-score.\\n\\nNote that in earlier published evaluations of Icelandic constituency parsing an older version of the test set was used. It was also smaller in the sense that sentences with grammatical errors were excluded. Additionally in the case of the Berkeley neural parser, all sentences with multiword tokens such as \\\"\u00ed g\u00e6r\\\" (yes-yesterday) were removed, effectively halving the number of test sentences. On that test set the Greynir parser had an F1 score of 81.21 and a fine-tuned IceBERT-IGC model had an F1 score of 90.67, the Berkeley neural parser variant had an F1 score of 87.07 (this differs from the result in Table 7 as a different test set was used for the comparison). The IceParser is a shallow parser and thus cannot be evaluated using EVALB.\\n\\n**5.4. Grammatical error detection**\\n\\nWe fine-tune IceBERT for grammatical error detection (GED). These are the first machine learning models trained for GED in Icelandic, and make use of the Icelandic Error Corpus (IceEC) (Ingason et al., 2021) which contains 58k labeled sentences.\\n\\nWe train models for both binary and multi-class token-level classification. For the multi-class GED classifier, we exclude ambiguous sentences from the IceEC where a single token has multiple error labels; this removes 3.5k sentences out of the 23k sentences with error labels. The problem could be solved as a multi-label one, but we leave that approach for future work.\"}"}
{"id": "lrec-2022-1-464", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 6: NER performance for models trained on different datasets, standard deviation over five seeds shown.\\n\\n| Model                  | F1 Score  | Precision | Recall | Accuracy |\\n|------------------------|-----------|-----------|--------|----------|\\n| IceBERT                | 91.43 \u00b1 0.23 | 91.60 \u00b1 0.13 | 91.26 \u00b1 0.36 | 98.66 \u00b1 0.03 |\\n| IceBERT-large          | 90.79 \u00b1 0.36 | 91.61 \u00b1 0.41 | 91.26 \u00b1 0.41 | 98.58 \u00b1 0.08 |\\n| IceBERT-IGC            | 91.10 \u00b1 0.25 | 91.15 \u00b1 0.38 | 91.06 \u00b1 0.20 | 98.59 \u00b1 0.04 |\\n| IceBERT-IC3            | 91.29 \u00b1 0.16 | 91.24 \u00b1 0.24 | 91.35 \u00b1 0.27 | 98.62 \u00b1 0.02 |\\n| IceBERT-mC4-is         | 89.57 \u00b1 0.28 | 89.27 \u00b1 0.44 | 89.87 \u00b1 0.28 | 98.40 \u00b1 0.06 |\\n| XLMR-base              | 88.95 \u00b1 0.60 | 88.66 \u00b1 0.78 | 89.25 \u00b1 0.61 | 98.41 \u00b1 0.08 |\\n| XLMR-base-IC3-1d       | 89.58 \u00b1 0.30 | 89.84 \u00b1 0.46 | 89.33 \u00b1 0.26 | 98.39 \u00b1 0.04 |\\n| XLMR-base-IC3-7d       | 92.52 \u00b1 0.40 | 92.31 \u00b1 0.49 | 92.74 \u00b1 0.41 | 98.83 \u00b1 0.05 |\\n\\nTable 7: VALB performance on the generalized form of the GreynirCorpus test set, mean and standard deviation over five seeds shown.\\n\\nWhile the dataset is fine-grained and contains a variety of objective and subjective labels (e.g. stylistic), in this study we limit our evaluation to the five high-level categories: **coherence**, **grammar**, **orthography**, **style**, and **vocabulary**.\\n\\nFor fine-tuning, we use a batch size of 16, learning rate of 2e-5 and train five times for five epochs with different seeds. The results for binary classification are shown in Table 8 and by category for the top 5 models in the multi-class task in Table 9. The three lowest performing models are not shown in Table 9 due to lack of space. For the sake of completeness, IceBERT-mC4-is had a total accuracy of 51.69 \u00b1 2.60, XLMR had a total accuracy of 56.26 \u00b1 4.22, and XLMR-IC3-1d had a total accuracy of 41.24 \u00b1 0.60.\\n\\nBased on the results in Tables 8 and 9 it is clear that out of the IceBERT base-models, the model trained on the IGC dataset containing editorial text is best suited for fine-tuning for GED. IceBERT-large is the clear winner with an F1 score of 89.12 \u00b1 1.31 in binary token classification and 79.16 \u00b1 9.39 in token classification.\\n\\nThe 1-day XLMR model does not do as well as the IceBERT models and the mC4-is model is lagging behind, further highlighting that models trained on the dataset might benefit from further cleanup. Interestingly, we see that the 7-day XLMR multilingual model outperforms the other models besides IceBERT-large.\\n\\nThe improvement of the 7-day XLMR model in token classification when compared with IceBERT-base models is mainly due to better results in the categories grammar and style as can be seen in Table 9.\\n\\n5.5. Icelandic WinoGrande\\n\\nThe WinoGrande dataset (Sakaguchi et al., 2020), used for evaluating commonsense reasoning capabilities of neural language models, is inspired by the original WinoGrad dataset (Levesque et al., 2012), but its problems are designed to minimize biases which the models may rely on when solving them. The dataset consists of sentences that include two nouns and an ambiguous pronoun which grammatically can refer to either of those noun phrases. The task is to decide which noun makes more semantic sense, given the information in the sentence.\\n\\nWe systematically go through the WinoGrande test set (1767 examples) and manually translate and adapt sentences to work in Icelandic. While the English WinoGrande problems are not always constructed as pairs, in our adaptation, we create sentence pairs where it is feasible. We also found some of the examples to be specific to culture, subjective, or otherwise inapplicable for translation. Those examples were either adjusted or skipped. The result is a dataset of 1095 examples.\\n\\nThe size of the Icelandic dataset is closest in size to the small variant of the English dataset (640 examples). The results over five-fold cross-validation can be seen in Table 10. The large model outperforms the other variants while models trained on IGC and/or IC3 out-\"}"}
{"id": "lrec-2022-1-464", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 8: Binary token classification performance measured using the Icelandic Error Corpus evaluation dataset, standard deviation over five seeds shown.\\n\\n| Category | IB-base | IB-large | IB-IGC | IB-IC3 | XLMR-IC3-7d |\\n|----------|---------|----------|--------|--------|-------------|\\n| Coherence | 5.90 \u00b1 3.15 | 41.58 \u00b1 21.83 | 6.89 \u00b1 3.68 | 4.94 \u00b1 3.23 | 2.40 \u00b1 3.00 |\\n| Grammar  | 55.05 \u00b1 2.64 | 72.09 \u00b1 14.25 | 54.71 \u00b1 4.77 | 50.71 \u00b1 4.93 | 62.83 \u00b1 3.69 |\\n| Orthography | 80.39 \u00b1 2.12 | 89.66 \u00b1 4.88 | 81.00 \u00b1 2.30 | 79.79 \u00b1 2.29 | 84.47 \u00b1 2.19 |\\n| Style    | 36.07 \u00b1 10.42 | 67.39 \u00b1 16.30 | 37.19 \u00b1 9.66 | 35.50 \u00b1 9.66 | 47.44 \u00b1 9.02 |\\n| Vocabulary | 18.47 \u00b1 2.74 | 50.30 \u00b1 21.45 | 17.94 \u00b1 4.48 | 16.85 \u00b1 4.45 | 17.33 \u00b1 4.56 |\\n| All      | 62.83 \u00b1 3.75 | 79.16 \u00b1 9.39 | 63.45 \u00b1 3.96 | 61.84 \u00b1 4.06 | 69.42 \u00b1 3.58 |\\n\\n### Table 9: Token classification F1-scores measured using the Icelandic Error Corpus evaluation dataset for the top-5 highest scoring models. IB is shorthand notation for IceBERT.\\n\\n| Model         | Accuracy \u00b1 % |\\n|---------------|--------------|\\n| IceBERT       | 54.6 \u00b1 2.1  |\\n| IceBERT-large | 57.1 \u00b1 3.7  |\\n| IceBERT-IGC   | 53.8 \u00b1 2.3  |\\n| IceBERT-IC3   | 53.8 \u00b1 2.4  |\\n| IceBERT-mC4-is| 51.4 \u00b1 2.1  |\\n| XLMR-base     | 52.4 \u00b1 1.7  |\\n| XLMR-base-IC3-1d | 51.2 \u00b1 2.0 |\\n| XLMR-base-IC3-7d | 53.4 \u00b1 1.9 |\\n\\n### Table 10: Accuracy on the Icelandic WinoGrande dataset. Results are averaged over five-fold cross-validation and standard deviation is reported.\\n\\n| Model                  | Accuracy \u00b1 % |\\n|------------------------|--------------|\\n| The model trained on mC4-is | The XLMR-base and XLMR-base-IC3-1d perform similar to the model trained on mC4-is but the XLMR-base-IC3-7d performs similar to the model trained only on IC3 or IGC. |\\n\\n### 6. Conclusion\\n\\nWe have successfully trained baseline neural language models for Icelandic that perform well on existing benchmarks, in particular NER, PoS and constituency parsing. We also present the Icelandic WinoGrande dataset and show that it is challenging for the models we evaluate. Furthermore, to our surprise, we show that extracting data from online sources is sufficient to train models which show performance that is competitive with those trained on curated/editorial corpora. We stress that proper filtering and cleanup of crawled data is necessary, as demonstrated in the difference between training models on IC3 and mC4-is. For some downstream tasks, we observe that multilingual language models (Conneau et al., 2020) are getting quite close to the performance of models trained on a monolingual corpus but for other tasks we still observe a significant difference. Interestingly, using such models as a warm start for some tasks can even lead to state-of-the-art performance.\\n\\nWe conclude that by using text extracted from the Common Crawl corpus and multilingual models as a warm start, well-performing language models are becoming more feasible to build for low to medium-resource languages. We still hold that curated corpora are beneficial in certain applications, but the added value for downstream tasks is small if a sufficiently large crawled corpus is available.\\n\\n### 7. Acknowledgements\\n\\nWe thank Prof. Dr.-Ing. Morris Riedel and his team for providing access to the DEEP super-computer at Forschungszentrum J\u00fclich. We also thank the Icelandic Language Technology Program (Nikul\u00e1sd\u00f3ttir et al., 2020). It has enabled the authors to focus on work in Icelandic NLP. Finally, we thank the anonymous reviewers for their helpful feedback.\"}"}
{"id": "lrec-2022-1-464", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use the MIM-GOLD dataset (Barkarson et al., 2020) to train a PoS tagger. If one approaches this task as a multiclass problem (each word gets exactly one label) there is a significant label scarcity problem. There are approximately 600 legal labels in the MIM-GOLD schema, but only 559 of them appear in the training set, and 43 of them appear less than 10 times. The 10th, 25th, 50th and 75th percentile tags occur 15, 52, 208 and 728 times, respectively.\\n\\nTo address this problem we decompose the labels. Instead of considering a label to be a single unit, e.g. noun-masculine-singular-nominative-article, we consider it to be composed of several categories, e.g. lexical class, gender, number, case and article-clitic, each of which can have several values. We also observe that some categories are shared between lexical class, e.g. nouns and adjectives both have gender, number and case and typically share values when they co-refer. Our model therefore outputs for each word a lexical class and a value for every grammatical category or morphological feature, but we ignore those that are not applicable to the lexical class, e.g. for nouns we mask out loss for the tense category during training and output no tense label during inference.\\n\\nSince the number of labels within each category is small (the largest category has 6 possible labels), each label has been seen many times during training, even though some combinations of labels never occur in the training set (such as verb past subjunctive 2person plural middle-voice). This allows the model to generalize and predict these unseen combinations, some of which actually occur in the test set.\\n\\n8. Bibliographical References\\n\\nAgerri, R., San Vicente, I., Campos, J. A., Barrena, A., Saralegi, X., Soroa, A., and Agirre, E. (2020). Give your text representation models some love: the case for Basque. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 4781\u20134788.\\n\\nArnard\u00f3ttir, \u00de. and Ingason, A. K. (2020). A Neural Parsing Pipeline for Icelandic Using the Berkeley Neural Parser. In Costanza Navarretta et al., editors, Proceedings of CLARIN Annual Conference 2020, pages 48\u201351.\\n\\nBamman, D. and Burns, P. J. (2020). Latin BERT: A Contextual Language Model for Classical Philology. arXiv:2009.10053 [cs], September. 00007 arXiv: 2009.10053.\\n\\nBhattacharjee, A., Hasan, T., Samin, K., Islam, M. S., Rahman, M. S., Iqbal, A., and Shahriyar, R. (2021). BanglaBERT: Combating Embedding Barrier in Multilingual Models for Low-Resource Language Understanding. arXiv:2101.00204 [cs], August. 00005 arXiv: 2101.00204.\\n\\nBojanowski, P., Grave, E., Joulin, A., and Mikolov, T. (2017). Enriching word vectors with subword information. Transactions of the Association for Computational Linguistics, 5:135\u2013146.\\n\\nBommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora, S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., Brynjolfsson, E., Buch, S., Card, D., Castellon, R., Chatterji, N. S., Chen, A. S., Creel, K., Davis, J. Q., Demszky, D., Donahue, C., Doumbouya, M., Durmus, E., Ermon, S., Etchemendy, J., Ethayarajh, K., Fei-Fei, L., Finn, C., Gale, T., Gillespie, L., Goel, K., Goodman, N. D., Grossman, S., Guha, N., Hashimoto, T., Henderson, P., Hewitt, J., Ho, D. E., Hong, J., Hsu, K., Huang, J., Icard, T., Jain, S., Jurafsky, D., Kalluri, P., Karamcheti, S., Keeling, G., Khani, F., Khattab, O., Koh, P. W., Krass, M. S., Krishna, R., Kuditipudi, R., and et al. (2021). On the opportunities and risks of foundation models. CoRR, abs/2108.07258.\\n\\nCieri, C., Maxwell, M., Strassel, S., and Tracey, J. (2016). Selection criteria for low resource language programs. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 4543\u20134549, Portoro\u017e, Slovenia, May. European Language Resources Association (ELRA).\\n\\nConneau, A. and Lample, G. (2019). Cross-lingual Language Model Pretraining. In Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc. 00847.\\n\\nConneau, A., Khandelwal, K., Goyal, N., Chaudhary, V., Wenzek, G., Guzm\u00e1n, F., Grave, \u00c9., Ott, M., Zettlemoyer, L., and Stoyanov, V. (2020). Unsupervised cross-lingual representation learning at scale. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8440\u20138451.\\n\\nCruz, J. C. B. and Cheng, C. (2020). Establishing Baselines for Text Classification in Low-Resource Languages. arXiv:2005.02068 [cs], May. 00010 arXiv: 2005.02068.\\n\\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\\n\\nHu, J., Ruder, S., Siddhant, A., Neubig, G., Firat, O., and Johnson, M. (2020). XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalisation. In Proceedings of the 37th International Conference on Machine Learning, pages 4411\u20134421. PMLR, November. 00263 ISSN: 2640-3498.\\n\\nIng\u00f3lfsd\u00f3ttir, S. L., Gu\u00f0j\u00f3nsson, \u00c1. A., and Loftsson, H. (2020). Named Entity Recognition for Icelandic: Annotated Corpus and Models. In Luis\"}"}
{"id": "lrec-2022-1-464", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Karthikeyan, K., Wang, Z., Mayhew, S., and Roth, D. (2019). Cross-Lingual Ability of Multilingual BERT: An Empirical Study. In International Conference on Learning Representations.\\n\\nKhalid, U., Beg, M. O., and Arshad, M. U. (2021). RUBERT: A Bilingual Roman Urdu BERT Using Cross Lingual Transfer Learning. arXiv:2102.11278 [cs], February.\\n\\nKitaev, N., Cao, S., and Klein, D. (2019). Multilingual constituency parsing with self-attention and pre-training. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 3499\u20133505, Florence, Italy, July. Association for Computational Linguistics.\\n\\nKreutzer, J., Caswell, I., Wang, L., Wahab, A., van Esch, D., Ulzii-Orshikh, N., Tapo, A., Subramani, N., Sikasote, C., Setyawan, M., Sarin, S., Samb, S., Sagot, B., Rivera, C., Rios, A., Padimitriou, I., Osei, S., Suarez, P. O., Orife, I., Ogueji, K., Rubungo, A. N., Nguyen, T. Q., M\u00fcller, M., M\u00fcller, A., Muhammad, S. H., Muhammad, N., Mnyakeni, A., Mirzakhalov, J., Matangira, T., Leong, C., Lawson, N., Kudugunta, S., Jenny, M., Firat, O., Dossou, B. F. P., Dlamini, S., de Silva, N., \u00c7abuk Ball\u0131, S., Biderman, S., Battisti, A., Baruwa, A., Bapna, A., Baljekar, P., Azime, I. A., Awokoya, A., Ataman, D., Ahia, O., Ahia, O., Agrawal, S., and Adeyemi, M. (2022). Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets. Transactions of the Association for Computational Linguistics, 10:50\u201372, January.\\n\\nKristinsson, A. P. (2018). National language policy and planning in iceland\u2013aims and institutional activities. In National Language Institutions and National Languages. Contributions to the EFNIL Conference 2017, pages 243\u2013249.\\n\\nLevesque, H., Davis, E., and Morgenstern, L. (2012). The winograd schema challenge. In Thirteenth International Conference on the Principles of Knowledge Representation and Reasoning.\"}"}
{"id": "lrec-2022-1-464", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Boeker, M. (2020). GottBERT: a pure German Language Model. arXiv:2012.02110 [cs], December.\\n\\nSekine, S. and Collins, M. (2008). Evalb.\\n\\nSteingr\u00edmsson, S., K\u00e1rason, \u00d6., and Loftsson, H. (2019). Augmenting a BiLSTM tagger with a morphological lexicon and a lexical category identification step. In Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019), pages 1161\u20131168, Varna, Bulgaria, September.\\n\\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need. In I. Guyon, et al., editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nVilares, D., Garcia, M., and G\u00f3mez-Rodr\u0131guez, C. (2021). Bertinho: Galician BERT Representations. Procesamiento del Lenguaje Natural, 66:13\u201326.\\n\\nVirtanen, A., Kanerva, J., Ilo, R., Luoma, J., Loutolahti, J., Salakoski, T., Ginter, F., and Pyysalo, S. (2019). Multilingual is not enough: BERT for Finnish. arXiv:1912.07076 [cs], December. arXiv: 1912.07076.\\n\\nWang, Z., Karthikeyan, K., Mayhew, S., and Roth, D. (2020). Extending Multilingual BERT to Low-Resource Languages. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2649\u20132656.\\n\\nWolf, T., Debut, L., Sanh, V ., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y ., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., and Rush, A. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online, October. Association for Computational Linguistics.\\n\\nWu, S. and Dredze, M. (2019). Beto, Bentz, Beicas: The Surprising Cross-Lingual Effectiveness of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 833\u2013844, Hong Kong, China, November. Association for Computational Linguistics.\\n\\nWu, S. and Dredze, M. (2020). Are all languages created equal in multilingual bert? In Proceedings of the 5th Workshop on Representation Learning for NLP, pages 120\u2013130.\\n\\nXue, L., Constant, N., Roberts, A., Kale, M., Al-Rfou, R., Siddhant, A., Barua, A., and Raffel, C. (2021). mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498.\\n\\n9. Language Resource References\\n\\nBarkarson, S., Sigur\u00f0sson, E. F., R\u00f6gnvaldsson, E., Hafsteinsd\u00f3ttir, H., Loftsson, H., Steingr\u00edmsson, S., and Andr\u00e9sd\u00f3ttir, \u00de. D. (2020). MIM-GOLD 20.05.\\n\\nBarkarson, S., Steingr\u00edmsson, S., and Dan\u00edelsson, H. (2021a). IGC-Social 21.10.\\n\\nBarkarson, S., Steingr\u00edmsson, S., Hafsteinsd\u00f3ttir, H., and Ingimundarson, F. (2021b). IGC-Books 21.10.\\n\\nIngason, A. K., Stef\u00e1nsd\u00f3ttir, L. B., Arnard\u00f3ttir, \u00de., and Xu, X. (2021). The Icelandic Error Corpus (IceEC), version 1.1.\\n\\nIng\u00f3lfsd\u00f3ttir, S. L., Gu\u00f0j\u00f3nsson, \u00c1. A., and Lofts-son, H. (2020). MIM-GOLD-NER \u2013 named entity recognition corpus (20.06). CLARIN-IS.\\n\\nMarcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a Large Annotated Corpus of English: The Penn Treebank. MIT Press.\\n\\n\u00deorsteinsson, V ., \u00d3lad\u00f3ttir, H., \u00de\u00f3r\u00f0arson, S., S\u00edmonar-son, H. B., and \u00c1sgeirsd\u00f3ttir, K. (2021). Greynir-corpus (2021-06-23).\\n\\nSakaguchi, K., Bras, R. L., Bhagavatula, C., and Choi, Y . (2020). WINOGRANDE: An Adversarial Winograd Schema Challenge at Scale.\\n\\nSteingr\u00edmsson, S., Helgad\u00f3ttir, S., R\u00f6gnvaldsson, E., Barkarson, S., and Gu\u00f0nason, J. (2018). Risam\u00e1l-heild: A Very Large Icelandic Text Corpus. European Language Resources Association (ELRA).\\n\\nZhu, Y ., Kiros, R., Zemel, R., Salakhutdinov, R., Urta-sun, R., Torralba, A., and Fidler, S. (2015). Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books.\"}"}
