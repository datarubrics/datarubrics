{"id": "emnlp-2022-main-795", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Agent-Specific Deontic Modality Detection in Legal Language\\n\\nAbhilasha Sancheti\u2020\u2021, Aparna Garimella\u2021, Balaji Vasan Srinivasan\u2021, Rachel Rudinger\u2020\\n\\n\u2020University of Maryland, College Park\\n\u2021Adobe Research\\n{sancheti, rudinger}@umd.edu\\n{garimell, balsrini}@adobe.com\\n\\nAbstract\\n\\nLegal documents are typically long and written in legalese, which makes it particularly difficult for laypeople to understand their rights and duties. While natural language understanding technologies can be valuable in supporting such understanding in the legal domain, the limited availability of datasets annotated for deontic modalities in the legal domain, due to the cost of hiring experts and privacy issues, is a bottleneck. To this end, we introduce, \\\\( \\\\text{LEXDEMOD} \\\\), a corpus of English contracts annotated with deontic modality expressed with respect to a contracting party or agent along with the modal triggers. We benchmark this dataset on two tasks: (i) agent-specific multi-label deontic modality classification, and (ii) agent-specific deontic modality and trigger span detection using Transformer-based (Vaswani et al., 2017) language models. Transfer learning experiments show that the linguistic diversity of modal expressions in \\\\( \\\\text{LEXDEMOD} \\\\) generalizes reasonably from lease to employment and rental agreements. A small case study indicates that a model trained on \\\\( \\\\text{LEXDEMOD} \\\\) can detect red flags with high recall. We believe our work offers a new research direction for deontic modality detection in the legal domain.\\n\\n1 Introduction\\n\\nA contract is a legal document executed by two or more parties. To sign a contract (e.g., lease agreements, terms of services, privacy policies, EULA, etc.), it is important for these parties to precisely understand their obligations, entitlements, prohibitions, and permissions as described in the contract. However, for a layperson, understanding contracts can be difficult due to their length and the complexity of legalese used. Therefore, a layperson often signs agreements without even reading them (Cole, 2015; Obar and Oeldorf-Hirsch, 2020).\\n\\n1 The code and data are available at https://github.com/abhilashasancheti/LexDeMod.\\n\\nFigure 1: Sample contract indicating the terminologies used to refer to the elements of a contract. 'shall' triggers obligation for Lessee and entitlement for Lessor. Contracting party or agent is referred to via an \\\"alias\\\" (such as Lessor or Lessee) throughout the contract.\\n\\nHaving a system which can provide an \\\"at a glance\\\" summary of obligations, entitlements, prohibitions, and permissions to a contracting party (henceforth, \\\"agent\\\"), will be of great help not only to the agents but also to legal professionals for contract review. While existing language processing and understanding systems can be used for legal understanding, limited availability of annotated datasets in the legal domain due to the cost of hiring experts and privacy issues is a bottleneck. Furthermore, the highly specialized lexical and syntactic features of legalese make it difficult to directly apply systems trained on data from other linguistic domains (e.g., news) to the legal domain.\\n\\nFor an \\\"at a glance\\\" summary of contracts, we first need to identify the obligations, entitlements, prohibitions, and permissions present in the contract for a given agent. Deontic modality is frequently used in legal documents to express obligations, entitlements, prohibitions, and permissions. Deontic modality is not related to semantic roles.\\n\\n3 Party names redacted for anonymity purpose.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"consequently used in contracts to express such obligations, entitlements, permissions, and prohibitions of agents (Ballesteros-Lintao et al., 2016). For instance, \u2018shall\u2019, \u2018shall not\u2019, and \u2018may\u2019 is used to express \u2018obligation/entitlement\u2019, \u2018prohibition\u2019, and \u2018permission\u2019 respectively in example (1) below.\\n\\n(1) a. Tenant shall pay the rent to the Landlord.\\n   b. Landlord shall not obtain financing or enter into any agreement affecting the Property.\\n   c. Landlord may continue this Lease in effect after Tenant\u2019s abandonment and recover Rent as it becomes due.\\n\\n(2) a. Tenant agrees to pay the rent.\\n   b. Landlord is responsible for maintaining the structural soundness of the house.\\n\\nHowever, existing works for identifying such deontic modality types (henceforth \u201cdeontic types\u201d) either use rule-based (Wyner and Peters, 2011; Peters and Wyner, 2016; Dragoni et al., 2016; Ash et al., 2020) or data-driven (Neill et al., 2017; Chalkidis et al., 2018) approaches, which cannot be directly used for our purpose. This is because rule-based approaches are not robust as they do not (in practice) capture the rich linguistic variety (e.g., use of non-modal expressions in (2)) and ambiguity of modal expressions (e.g., \u2018shall\u2019 in (1a)). Furthermore, annotated datasets used in the data-driven approaches do not consider multiple deontic types for a sentence and their association with the agent (e.g., (1a) is an instance of \u2018obligation\u2019 for the Tenant and an \u2018entitlement\u2019 for the Landlord). Although, Funaki et al. (2020) introduced a corpus with annotations for rights, obligations, and associated agents, it does not cover all the deontic types. Moreover, different corpora consider different deontic types, lacking an accepted standard.\\n\\nIn this work, we address these issues through the following contributions: (a) we present a linguistically-informed taxonomy for annotating deontic types in the legal domain, and use the taxonomy to build a corpus (LEXDEM; \u00a73) of English contracts with two types of annotations: (i) all deontic types expressed in a sentence with respect to an agent, and (ii) spans of modal triggers, i.e., expressions (e.g., bold-faced phrases in examples (1) and (2)) that evoke the modal meaning; (b) we benchmark the corpus on two tasks: (i) agent-specific multi-label deontic modality classification (\u00a76), and (ii) agent-specific deontic modality and trigger span detection (\u00a77) using state-of-the-art Transformer (Vaswani et al., 2017) models, and (c) we perform transfer learning experiments (\u00a78) to investigate the generalizability of diverse modal expressions in LEXDEM and a case study to detect red flags (\u00a79) in lease agreements.\\n\\n2 Related Work\\n\\n2.1 NLP in the Legal Domain\\n\\nPrior works have investigated a number of tasks in the legal NLP domain including legal judgement prediction (Aletras et al., 2016; Luo et al., 2017; Zhong et al., 2018; Chen et al., 2019; Chalkidis et al., 2019), legal entity recognition and classification (Cardellino et al., 2017; Chalkidis et al., 2017; Angelidis et al., 2018), legal question answering (Duan et al., 2019; Zhong et al., 2020), and legal summarization (Hachey and Grover, 2006; Bhattacharya et al., 2019; Manor and Li, 2019). While legal NLP covers a wide range of tasks, limited efforts have been made for contract review despite it being one of the most time-consuming and tedious tasks. Leivaditi et al. (2020) introduced a benchmark for lease contract review for detecting named entities and red flags. Hendrycks et al. (2021) introduced a large expert-annotated dataset and Tuggener et al. (2020) a large semi-automatically annotated dataset for provision type classification across a variety of contract types. However, these datasets do not contain deontic type annotations which is the focus of this work.\\n\\n2.2 Rights and Obligation Extraction\\n\\nExisting works either propose rule-based methods (Wyner and Peters, 2011; Peters and Wyner, 2016) or use a combination of NLP approaches such as syntax and dependency parsing (Dragoni et al., 2016) for extracting rights and duties from legal documents such as Federal code regulations, European directives or customer protection codes. Another line of works (Bracewell et al., 2014; Neill et al., 2017; Chalkidis et al., 2018) use machine learning and deep learning approaches to predict deontic types with the help of small datasets. However, rule-based approaches are not robust due to the rich linguistic variety and ambiguity of modal expressions, and the annotated datasets do not consider multiple deontic types for a sentence and their association with the agents which is important for contract understanding. Matulewska (2010) analyzed contracts from different countries and types...\"}"}
{"id": "emnlp-2022-main-795", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"considering fine-grained deontic modalities covered in them but only considers obligation, permission and prohibition with temporal constraints. Ash et al. (2020) propose a rule-based unsupervised approach to identify deontic types with respect to an agent and compute statistics for rights and duties for an agent. However, rule-based approaches have limitations as mentioned above. Recently, Funaki et al. (2020) curate an annotated corpus of contracts for recognizing rights and obligations along with the agents using LegalRuleML (Athan et al., 2013). However, the corpus is not publicly available, does not annotate for modal triggers, and does not cover all the deontic types expressed in a contract.\\n\\n2.3 Modality Annotation and Detection\\n\\nModality refers to the linguistic ability to describe alternative ways the world could be and is commonly expressed by modal auxiliaries such as shall, will, must, can, and may. Existing studies have proposed various modality annotation schemas for Portuguese (Hendrickx et al., 2012; Avila et al., 2015) and applied (Quaresma et al., 2014) it to build machine learning models to identify the deontic types. However, it does not cover all the deontic types and restrict the identification to three modal auxiliaries. While Athan et al. (2013) and Nazarenko et al. (2018) propose XML-based annotation schema to formally represent legal text in English and highlight the various interpretive issues that arose during the annotation, it does not consider trigger annotation. Although Rubinstein et al. (2013) and Pyatkin et al. (2021) consider trigger and modality type (not restricted to modal auxiliaries) annotations at different levels of granularity, fine-grained deontic types as well as association with the agent is not considered. As different studies consider different deontic types lacking an accepted standard, we present a linguistically-informed taxonomy for annotating deontic types and their triggers.\\n\\n3 LEXDEM Dataset Curation\\n\\nWe first describe the dataset source (\u00a73.1) followed by pre-processing (\u00a73.2), annotation protocol (\u00a73.3), and the quantitative and qualitative analysis (\u00a73.4) of the collected dataset.\\n\\n3.1 Dataset Source\\n\\nWe use the contracts available in the LEDGAR corpus (Tuggener et al., 2020) which comprises material contracts (Exhibit-10), such as agreements (e.g., shareholder/employment/lease/non-disclosure), crawled from Electronic Data Gathering, Analysis, and Retrieval (EDGAR) system. EDGAR is maintained by the U.S. Securities and Exchange Commission (SEC). The documents filed on SEC are public information and can be redistributed without a further consent.\\n\\n3.2 Contract Pre-processing\\n\\nThe raw contracts in LEDGAR are available in html format. We extract all the paragraphs (henceforth, \u201cprovisions\u201d) from the html (identified by <p> or <div> tags) of a contract, and heuristically filter the provisions defining any terminologies (identified by presence of phrases such as \u2018shall mean\u2019, \u2018means\u2019, \u2018shall have the meaning\u2019, \u2018has the meaning\u2019, etc.). As contracts have a hierarchical structure (e.g., bullets and sub-bullets), we prepend (see A.1) the higher level context with the lower level (e.g., combining sub-bullets with its context in the main bullet). After this, we heuristically extract the type of the contract (e.g., lease or employment contract) and the alias (e.g., \u201cLessee\u201d in Figure 1) used to refer the contracting parties from the content of the contracts.\\n\\nContract Type Extraction. We heuristically scan the first 20 provisions to identify the type of the contract using regular expressions (all uppercase characters and presence of \u2018AGREEMENT\u2019).\\n\\nAgent Alias Extraction. Agent in a contract can be either a person or a company. Therefore, we scan the first 20 provisions of a contract to find company mentions using lexnlp (Bommarito II et al., 2021) and named entities with \u2018person\u2019 tag using spaCy (Honnibal et al., 2020) library. We then use regular expression (alias is mentioned in parenthesis (see Figure 1) following the agent mention) to extract the alias used to refer to the found agents in the provisions. For each type of contract, we manually select the most frequently occurring aliases extracted after using the regular expression.\\n\\nWe collect all the sentences of provisions belonging to a contract wherein alias for an agent is found. We posit that if a sentence does not contain an alias, then deontic type is not expressed for an agent. For instance, \u2018Any such month-to-month tenancy shall be subject to every other term, covenant and...\u2019\"}"}
{"id": "emnlp-2022-main-795", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deontic Type Description\\n\\nObligation (Obl) Agent is required to have or do something\\n\\nEntitlement (Ent) Agent has the right to have or do something\\n\\nProhibition (Pro) Agent is forbidden to have or do something\\n\\nPermission (Per) Agent is allowed to have or do something\\n\\nNo Obligation (Nobl) Agent is not required to have or do something\\n\\nNo Entitlement (Nent) Agent has no right to have or do something\\n\\nTable 1: Taxonomy\\n\\nfor deontic type.\\n\\nagreement contained herein.\\n\\n3.3 Annotation Protocol\\n\\nAnnotation task description.\\n\\nWe propose agent-specific deontic modality detection tasks that address the following issues: (i) non-robustness of rule-based extraction of rights and duties as it cannot capture the rich linguistic variety and ambiguity of modal expressions; (ii) lack of standard taxonomy for annotating fine-grained deontic types; (iii) non-association of deontic type with the agent during annotation, and (iv) considering deontic type detection as a single class classification task.\\n\\nConsider, for instance, the following:\\n\\n(3) a. [Tenant] Tenant shall obl pay the rent to the Landlord and may per use the parking space.\\n\\nb. [Landlord] Tenant shall ent pay the rent to the Landlord and may use the parking space.\\n\\nIn these examples, the words in bold evoke the modal expression, which we call a trigger. For Tenant as the Agent, an obligation (obl) and a permission (per) are expressed in the sentence (3a), and an entitlement (ent) for the Landlord (3b).\\n\\nOur data collection is performed via crowdsourcing on Amazon Mechanical Turk (AMT). We ask the workers to provide two types of annotations for each sentence with respect to an agent (referred to via an alias): (i) select all the deontic types expressed, and (ii) select trigger word(s) (as span) for each selected deontic type. If a sentence contains more than one agent, we duplicate it to get separate annotations with respect to each agent so that the workers focus their understanding with respect to one agent at a time. This task design choice helps in better estimation of the time taken to do each HIT (Human Intelligence Task) as the number of agent mentions in a sentence can vary. This also simplifies the custom annotation interface (see Figure 6) built to get the annotations. Detailed guidelines for annotation are provided in A.2 (Figure 5).\\n\\n3.4 Annotated Dataset Statistics and Analysis\\n\\nEach contract contains 202.6 (\u00b1 162.4) provisions on average (standard deviation in parentheses), with 2.2 (\u00b1 1.7) sentences per provision; each contract consists of 306.4 (\u00b1 235.8) sentences on average. Among these, 75.8 (\u00b1 14.4)% of sentences per contract have at least one agent mentioned in them, with an average length of 65 (\u00b1 47.5). We collect a total of 8,230 trigger span annotations.\\n\\nWe also provide an additional option 'None' in case none of the deontic types is expressed or it is a rule.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Dataset Statistics.\\n\\n|        | Obl | Ent | Pro | Per | Nobl | Nent | None |\\n|--------|-----|-----|-----|-----|------|------|------|\\n| Train  | 4282| 5279| 1841| 1231| 343  | 289  | 265  |\\n|        | 239 | 1071|\\n| Dev    | 330 | 421 | 176 | 86  | 20   | 18   | 21   |\\n| Test   | 1777| 1952| 575 | 418 | 64   | 167  | 88   |\\n|        | 539 |\\n\\nFigure 2: Distribution of deontic type with respect to Tenant and Landlord for lease agreements.\\n\\nQualitative analysis.\\n\\nFigure 3: Frequency-based wordcloud of all the triggers.\\n\\nTable 3: Top 10 triggers for each deontic type in decreasing order of frequency.\\n\\n|        | Top 10 triggers                                     |\\n|--------|-----------------------------------------------------|\\n| Obl    | shall, will, agrees, acknowledge, represents and warrants, shall be responsible for, undertakes, will be responsible for |\\n| Ent    | shall, will, agrees, shall have the right to, acknowledges, waives no rights, shall not, retains all other rights, will be entitled to |\\n| Pro    | shall not, will not, may not, nor shall, not to be, neither lessor nor lessee may, in no event shall, nor will, will not allow, nor may |\\n| Per    | may, is permitted, will allow, has the right, shall, or at landlord's option, shall be permitted to, shall be allowed |\\n| Nobl   | shall not be liable, shall not be obligated to, shall not be required to, shall have no obligation to, in no event shall landlord be obligated to, waives, shall not, shall have no liability |\\n| Nent   | shall, shall have no right to, waives no rights, shall not, shall have no obligation to, waives, shall not be required, shall not be obligated, waive the right, shall not have the right to |\\n\\nFigure 3: Frequency-based wordcloud of all the triggers.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"resents covering 20.3% of the annotated trigger spans. This shows that LEXDEMOD covers a wide variety of linguistic expressions of deontic modality in legalese, not restricted to modal auxiliaries. Annotated samples from the dataset are provided in Table 15 in A.9.\\n\\n4 Proposed Benchmarking Tasks\\n\\nHaving established the rich variety and coverage of linguistic expressions for deontic modality in LEXDEMOD, we benchmark the corpus on the proposed two tasks defined below:\\n\\n(i) Agent-specific multi-label deontic modality classification. This task aims at predicting all the deontic types expressed in a sentence with respect to an agent. We pose this as a multi-label classification task conditioned on a sentence and an agent.\\n\\n(ii) Agent-specific deontic modality and trigger span detection. This task aims at identifying both the deontic type and corresponding triggers. We pose this as a token classification task. Every token in the corpus is assigned a BIOS tag if it belongs to a modal trigger, which is appended with a suffix indicating its deontic type. For instance, Tenant \\\\( \\\\text{OBL} \\\\) responsible \\\\( \\\\text{OBL} \\\\) for \\\\( \\\\text{OBL} \\\\) paying \\\\( \\\\text{OBL} \\\\), where subscripts denote the BIOS tags.\\n\\nFor both the tasks, agent is conditioned using special tokens added at the beginning of a sentence. This simple strategy has been successfully used previously for controlled text generation tasks (Senrich et al., 2016; Johnson et al., 2017; Rudinger et al., 2020; Sancheti et al., 2022).\\n\\n5 Benchmarking Setup\\n\\nWe experiment with various pre-trained language models (PLMs) (Devlin et al., 2019; Liu et al., 2019), which have shown state-of-the-art performance on natural language understanding tasks, to study their performance on our proposed tasks. We fine-tune these models for both the tasks on binary cross-entropy loss for 20 epochs each with a batch size of 8, and maximum sequence length of 256 using HuggingFace\u2019s Transformers library (Wolf et al., 2020). The model(s) with the best macro-F1 score on the dev set is used to report results on the test set. Further implementation details are in A.6.\\n\\nWe also partition the data according to the agent being conditioned to assess the performance of the trained models with respect to each agent.\\n\\n6 Benchmarking Multi-label Classification Comparison models.\\n\\nWe experiment with three kinds of approaches for the agent-specific multi-label deontic modality classification task.\\n\\n(1) Majority class predicted for each agent.\\n\\n(2) Rule-based. We implement a rule-based approach similar to the one described in Ash et al. (2020) with additional conditioning on the agent. It searches for the presence of pre-defined modal triggers for a deontic type and associates it with the agent using dependency tags (e.g., nsubj, aux or agent). We use spacy to tokenize each sentence and obtain a dependency parse. More details in A.5.\\n\\n(3) Fine-tuning PLMs. We fine-tune the following PLMs differing in size and domain of data used for pre-training: (i) BERT-base-uncased (BERT-BU); (ii) RoBERTa-base (RoBERTa-B); (iii) RoBERTa-large (RoBERTa-L), and (iv) recently introduced Contract-BERT-base-uncased (C-BERT-BU) model (Chalkidis et al., 2020) which has been pre-trained on US contracts from the EDGAR library.\\n\\nAll the above models are trained assuming trigger span information is not available and full context (i.e., sentence) is used. To understand the importance of Agent conditioning, Context, and Trigger for this task, we additionally train the following models: (i) No-agent where special token for agent is not used during training; (ii) ACT-Masked wherein everything in the context except the trigger span is masked using [MASK] token to hide the context but retain the positional information of the trigger; (iii) AT wherein only the tokens belonging to a trigger are used and multiple triggers are separated using a special token (e.g., [SEP] or ), and (iv) ACT wherein all the triggers are appended at the end of the context separated by a special token (e.g., [SEP] or )\\n\\nEvaluation measures. We report macro-averaged Precision, Recall, and F1 scores across all the types, calculated using Sklearn library (Pedregosa et al., 2011). We also report the Accuracy of predicting all the classes correctly for a sentence.\\n\\nResults and analysis. We report the results for multi-label classification task in Table 4. While rule-based approach has better F1 score than majority type prediction for each agent, Transformer-based models outperform these baselines indicating their ability to better capture the linguistic diversity of expressing deontic modals. As expected, Rule-\"}"}
{"id": "emnlp-2022-main-795", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tenent/-\\n\\nTable 4: Evaluation results for agent-specific multi-label deontic modality classification task. Scores for expressing prohibitions which makes it harder to identify. This can be due to the use of more training data.\\n\\n|                  | Accuracy | Precision | Recall | F1      |\\n|------------------|----------|-----------|--------|---------|\\n| Majority         | 39.53/28.66/34.38 | 6.49/5.23/11.72 | 14.29/14.29/21.09 | 8.29/8.29/11.11 |\\n| RoBERTa-L-ACT    | 84/77/74 | 63/54/49  | 72/65/60 | 66/66/61 |\\n| RoBERTa-L-ACT-Masked | 81/75/72 | 62/52/47  | 70/64/50 | 65/65/60 |\\n| Rule-based       | 51/60/43 | 44/55/77  | 79/84/90 | 85/85/86 |\\n| BU, B, L, A, C, and T | 38/29/23 | 21/18/17 | 02/01/00 | 09/09/09 |\\n\\nAs RoBERTa-L performs the best on this task, comparisons are made with other models trained on contracts. C-BERT-BU, which is pre-trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained with one variable (RoBERTa-L), during training as compared to RoBERTa-L, indicates the importance of agent conditioning during training and context, and trigger, in the last block of Table 4. Development set results are provided in Table 9 in Appendix.\\n\\nBenchmarking Trigger Span Detection\\n\\nWe fine-tune the same models as described in \u00a76 on a token classification task to predict the BIOS tags. Additionally, we experiment with three kinds of approaches for the agent-specific deontic modality and trigger span detection task.\\n\\nResults and Analysis.\\n\\nWe tag occurrences of pre-defined modal triggers in a sentence with the deontic type associated with them. Following (Pyatkin et al., 2021), we report the Accuracy of predicting the BIOS tags for a sentence. Using trigger information during association of agent with the modality expressed makes it harder to identify. This can be due to the use of more training data.\\n\\nWe also report macro-averaged Precision, Recall, and F1 scores, calculated using the seqeval library (Nakayama, 2018). We also report these metrics in labeled (both deontic type and trigger span considered) and unlabeled (only trigger span considered) settings.\\n\\nWe report the F1 score which evaluates for both trigger detection and its correct deontic type identification. Similar to 'may', while use of negation within context for without agent conditioning, significantly drops as we report the results for variants of this model to use all the information (RoBERTa-L-ACT) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT-Masked) or not used (RoBERTa-L-No-agent) during training as compared to RoBERTa-L. The performance of RoBERTa-L-No-agent, trained using all the information (RoBERTa-L-ACT) significantly improves performance over RoBERTa-L across all the metrics. Higher scores for RoBERTa-L indicate the importance of agent conditioning during training and context is also important for identifying deontic type, as all the metric scores drop when context is masked (RoBERTa-L-ACT"}
{"id": "emnlp-2022-main-795", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Labeled Accuracy | Precision | Recall | F1     | Unlabeled Accuracy | Precision | Recall | F1     |\\n|----------------|------------------|-----------|--------|--------|-------------------|-----------|--------|--------|\\n| Majority       | 97.16            | 96.85     | 97.01  |        | 98.00             | 97.98     | 98.00  |        |\\n\\n- Rule-based approach in Labeled score.\\n- Higher accuracy scores are due to the majority tokens being labeled as 'O'.\\n- Trends with dataset size variation are shown in Figure 4b.\\n- Manual analysis of deontic triggers and associating it with the deontic type is a harder task owing to the linguistic variety of trigger words. However, associating them is similar to the classification task, Rule-based approach outperforms other models on precision, but lags behind in recall for the same reason.\\n\\n- Size of the dataset behind in recall for the same reason.\\n- Consistently higher performance of the RoBERTa-L model (RoBERTa-L) is instrumental than domain knowledge of C-BERT-BU.\\n- Scores are presented for different seeds.\\n- BU, B, L, and d denote base-uncased, base, large, and no-agent respectively.\\n- Dev set results are shown in Table 10 in Appendix.\\n\\nTo investigate if the diverse linguistic expressions used in legal language are employee, employer, etc. are monly occurring agents in employment contracts. This drop is more prominent for employment contracts than rental agreements when compared to model's performance on lease agreements, although it is significantly better than the rule-based approach demonstrating the non-robustness of rule-based processing.\\n\\n- To account for this, we additionally train models with anonymizing the agent mentions in the dataset, e.g., tenant and landlord as in \u00a76.\\n- We evaluate the performance of the best model (RoBERTa-L) for both the tasks on these datasets. This model is instrumental than domain knowledge of C-BERT-BU.\\n\\n- Type-wise span detection (Table 11) reveals that accuracy scores are due to the majority tokens being labeled as 'O'.\\n- Similar trends were observed for employment contracts in the SEC.\\n- We chose rental agreement templates freely available at Pan-LEDGAR corpus and (2) sentences from daDoc.\\n- Sentences and report the results in Table 6.\\n- Scores are averaged over different seeds.\\n- Accuracy of agent conditioning is evident from the last row of Table 11.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Majority 36.36/27.45 11.87/8.80 19.10/15.15 14.46/11.11\\n\\nRule-based 41.56/47.45 53.77/64.63 34.54/35.00 33.27/37.22\\n\\nRoBERTa-L 73.16/48.72 83.08/52.87 63.42/48.90 68.90/48.32\\n\\nRoBERTa-L-AR 55.19/42.55 56.87/59.29 52.38/46.48 50.66/50.30\\n\\nRoBERTa-L-ARR 70.35/64.68 76.79/70.05 63.14/64.62 65.89/65.36\\n\\nTable 6: Results for rental/employment contracts.\\n\\n14' for tenant), and (ii) RoBERTa-L-ARR \u2013 agent is randomly replaced with a token consistent within a sentence. Replacing agent mentions leads to significant improvements for employment contracts in both the tasks, although evaluating these models on rental agreements (see Table 6) and lease data shows (see Table 12) an expected drop in the performance. These experiments show that the linguistic expressions captured by LEXDEMOD are also generalizable to other types of contracts.\\n\\n9 Case Study: Red flag Detection\\n\\nTo investigate if our agent-specific deontic modality classifier is capable of identifying the red flags annotated by Leivaditi et al. (2020) for lease agreements, we compare the predictions on the dev set from ALeaseBERT, proposed by Leivaditi et al. (2020), and RoBERTa-L model trained on LEXDEMOD dataset. For each sentence in the red flags dataset, we predict the deontic modality with respect to each of the agent alias mentioned in that sentence. If any one of the deontic types is expressed for any of the agents then we consider the prediction as positive otherwise negative. We find that (see Table 14 in A.8) the model trained on LEXDEMOD has high recall and low precision while ALeaseBERT has high precision but low recall for the positive class. Our model was able to predict all the red flags predicted by ALeaseBERT and some additional red flags. This is expected as many permissions or entitlements may not be red flags but may belong to a deontic type. We also found that there were payments related obligations which were predicted as red flags by our model but were not annotated as red flags in the dataset. Therefore, our model could also be used to filter important sentences which could indicate some red flags due to high recall.\\n\\n14 Unlabeled scores are provided in Appendix in Table 13.\\n\\n10 Conclusion and Future Work\\n\\nWe introduced LEXDEMOD for deontic modality detection in the legal domain which consists of diverse linguistic expressions of deontic modality. We propose and benchmark two tasks namely, agent-specific multi-label deontic modality classification, and agent-specific deontic modality and trigger span detection using transformer-based models. While evaluation results are promising, there is substantial room for improvement. We demonstrated the generalizability of the diverse linguistic expressions captured in LEXDEMOD via transfer learning experiments to employment and rental lease agreements. The small case study on red flag detection using our data showed the usability of our dataset. We leave joint-modeling of the two tasks and using these identification models for generating \u201cat a glance\u201d summary of contracts for future work.\\n\\n11 Limitations\\n\\nWe note a few limitations: (1) Although we demonstrate reasonable generalization to employment agreements, our dataset is limited to lease agreements which may not cover all the linguistic expressions for deontic modality occurring in legal domain. (2) The custom interface built for collecting annotations does not support non-contiguous trigger-span selection which may result in some contract type specific triggers (only for triggers with negation). Future work may consider handling non-contiguous spans and other challenges associated with it (e.g., representing non-contiguous trigger spans for a category in the BIO span). (3) As we focus on identifying agent-specific deontic modalities, we only consider sentences where the agent alias is explicitly mentioned. This helped in simplifying the annotation process and cost efficiency. Therefore, our models may not work well when no agent alias is mentioned in the given sentence. We leave the collection of annotations for sentences not explicitly mentioning agent alias for future work. (4) Our data collection and modeling assume that agent alias is known apriori (for which we perform agent alias extraction) as we focus on the identification task. Extending this work to any other type of agreement will require similar alias extraction method (e.g., employee, employer for employment agreement) or a more sophisticated model to identify the agent implicitly.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are committed to ethical practices and protecting the anonymity and privacy of individuals who have contributed. We ensure that the privacy of the annotators is protected. For annotations, $7.50/hr was paid per task.\\n\\nSocietal Impact. We recognize and acknowledge that our work carries a possibility of misuse including malicious adulteration of summaries generated by extracting sentences identified by our model and adversarial use of the model to mislead users. Such kind of misuse is common to any prediction model therefore, we strongly recommend coupling any such technology with external expert validation. The purpose of this work is to provide aid to the legal personnel or layperson dealing with legal contracts for a better understanding of the legal documents, and not to replace any experts. As contracts are long documents, identification of sentences that express deontic types can help in significantly reducing the number of sentences to read or highlighting the important parts of the contract which may need more attention.\\n\\nAcknowledgements\\nWe would like to thank Ani Nenkova and the anonymous reviewers for their useful feedback and comments on this work. We acknowledge the support from Adobe Research unrestricted gift funding for this work. The views contained in this article are those of the authors and not of the funding agency.\\n\\nReferences\\nNikolaos Aletras, Dimitrios Tsarapatsanis, Daniel Preotiuc-Pietro, and Vasileios Lampos. 2016. Predicting judicial decisions of the european court of human rights: A natural language processing perspective. PeerJ Computer Science, 2:e93.\\nIosif Angelidis, Ilias Chalkidis, and Manolis Koubarakis. 2018. Named entity recognition, linking and generation for greek legislation. In JURIX, pages 1\u201310.\\nElliott Ash, Jeff Jacobs, Bentley MacLeod, Suresh Naidu, and Dominik Stammbach. 2020. Unsupervised extraction of workplace rights and duties from collective bargaining agreements. In 2020 International Conference on Data Mining Workshops (ICDMW), pages 766\u2013774. IEEE.\\nTara Athan, Harold Boley, Guido Governatori, Monica Palmirani, Adrian Paschke, and Adam Wyner. 2013. Oasis legalruleml. In proceedings of the fourteenth international conference on artificial intelligence and law, pages 3\u201312.\\nLuciana Beatriz Avila, Am\u00e1lia Mendes, and Iris Hendrickx. 2015. Towards a unified approach to modality annotation in portuguese. In Proceedings of the Workshop on Models for Modality Annotation.\\nMiguel Ballesteros, Rishita Anubhai, Shuai Wang, Nima Pourdamghani, Yogarshi Vyas, Jie Ma, Parminder Bhatia, Kathleen McKeown, and Yaser Al-Onaizan. 2020. Severing the edge between before and after: Neural architectures for temporal ordering of events. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 5412\u20135417, Online. Association for Computational Linguistics.\\nRachelle Ballesteros-Lintao, Maria Regina P Arriero, Judith Ma Angelica S Claustro, Kristina Isabelle U Dichoso, Selenne Anne S Leynes, Maria Rosario R Aranda, and Jean Reintegrado-Celino. 2016. Deontic meanings in philippine contracts. International Journal of Legal Discourse, 1(2):421\u2013454.\\nPaheli Bhattacharya, Kaustubh Hiware, Subham Rajgaria, Nilay Pochhi, Kripabandhu Ghosh, and Sapitarshi Ghosh. 2019. A comparative study of summarization algorithms applied to legal case judgments. In European Conference on Information Retrieval, pages 413\u2013428. Springer.\\nMichael J Bommarito II, Daniel Martin Katz, and Eric M Detterman. 2021. Lexnlp: Natural language processing and information extraction for legal and regulatory texts. In Research Handbook on Big Data Law. Edward Elgar Publishing.\\nDavid Bracewell, David Hinote, and Sean Monahan. 2014. The author perspective model for classifying deontic modality in events. In The Twenty-Seventh International Flairs Conference.\\nCristian Cardellino, Milagro Teruel, Laura Alonso Alemany, and Serena Villata. 2017. Legal NERC with ontologies, Wikipedia and curriculum learning. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 254\u2013259, Valencia, Spain. Association for Computational Linguistics.\\nIlias Chalkidis, Ion Androutsopoulos, and Nikolaos Aletras. 2019. Neural legal judgment prediction in English. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4317\u20134323, Florence, Italy. Association for Computational Linguistics.\\nIlias Chalkidis, Ion Androutsopoulos, and Achilleas Michos. 2017. Extracting contract elements. In Proceedings of the 16th edition of the International Conference on Artificial Intelligence and Law, pages 19\u201328.\\nIlias Chalkidis, Ion Androutsopoulos, and Achilleas Michos. 2018. Obligation and prohibition extraction using hierarchical RNNs. In Proceedings of the 56th...\"}"}
{"id": "emnlp-2022-main-795", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. 2020. LEGAL-BERT: The muppets straight out of law school. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2898\u20132904, Online. Association for Computational Linguistics.\\n\\nHuajie Chen, Deng Cai, Wei Dai, Zehui Dai, and Yadong Ding. 2019. Charge-based prison term prediction with deep gating network. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 6362\u20136367, Hong Kong, China. Association for Computational Linguistics.\\n\\nSandra Chung. 1985. Tense, aspect and mood. Language typology and syntactic description, pages 202\u2013258.\\n\\nG Marcus Cole. 2015. Rational consumer ignorance: When and why consumers should agree to form contracts without even reading them. JL Econ. & Pol'y, 11:413.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMauro Dragoni, Serena Villata, Williams Rizzi, and Guido Governatori. 2016. Combining nlp approaches for rule extraction from legal documents. In 1st Workshop on MIning and REasoning with Legal texts (MIREL 2016).\\n\\nXingyi Duan, Baoxin Wang, Ziyue Wang, Wentao Ma, Yiming Cui, Dayong Wu, Shijin Wang, Ting Liu, Tianxiang Huo, Zhen Hu, et al. 2019. Cjrc: A reliable human-annotated benchmark dataset for chinese judicial reading comprehension. In China National Conference on Chinese Computational Linguistics, pages 439\u2013451. Springer.\\n\\nRuka Funaki, Yusuke Nagata, Kohei Suenaga, and Shin-suke Mori. 2020. A contract corpus for recognizing rights and obligations. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 2045\u20132053, Marseille, France. European Language Resources Association.\\n\\nBen Hachey and Claire Grover. 2006. Extractive summarisation of legal texts. Artificial Intelligence and Law, 14(4):305\u2013345.\\n\\nIris Hendrickx, Am\u00e1lia Mendes, and Silvia Mencarelli. 2012. Modality in text: a proposal for corpus annotation. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 1805\u20131812, Istanbul, Turkey. European Language Resources Association (ELRA).\\n\\nDan Hendrycks, Collin Burns, Anya Chen, and Spencer Ball. 2021. Cuad: An expert-annotated nlp dataset for legal contract review. arXiv preprint arXiv:2103.06268.\\n\\nMatthew Honnibal, Ines Montani, Sofie Van Landeghem, and Adriane Boyd. 2020. spacy: Industrial-strength natural language processing in python.\\n\\nOtto Jespersen. 2013. The philosophy of grammar. Routledge.\\n\\nMelvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun, Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi\u00e9gas, Martin Wattenberg, Greg Corrado, et al. 2017. Google's multilingual neural machine translation system: Enabling zero-shot translation. Transactions of the Association for Computational Linguistics, 5:339\u2013351.\\n\\nKlaus Krippendorff. 2018. Content analysis: An introduction to its methodology. Sage publications.\\n\\nSpyretta Leivaditi, Julien Rossi, and Evangelos Kanoulas. 2020. A benchmark for lease contract review. arXiv preprint arXiv:2010.10386.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man\u6363 J. Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692.\\n\\nBingfeng Luo, Yansong Feng, Jianbo Xu, Xiang Zhang, and Dongyan Zhao. 2017. Learning to predict charges for criminal cases with legal basis. arXiv preprint arXiv:1707.09168.\\n\\nLaura Manor and Junyi Jessy Li. 2019. Plain English summarization of contracts. In Proceedings of the Natural Legal Language Processing Workshop 2019, pages 1\u201311, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nAleksandra Matulewska. 2010. Deontic modality and modals in the language of contracts.\\n\\nHiroki Nakayama. 2018. seqeval: A python framework for sequence labeling evaluation. Software available from https://github.com/chakki-works/seqeval.\\n\\nAdeline Nazarenko, Fran\u00e7ois Levy, and Adam Wyner. 2018. An annotation language for semantic search of legal sources. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language Resources Association (ELRA).\"}"}
{"id": "emnlp-2022-main-795", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"James O' Neill, Paul Buitelaar, Cecile Robin, and Leona O' Brien. 2017. Classifying sentential modality in legal language: a use case in financial regulations, acts and directives. In *Proceedings of the 16th edition of the International Conference on Artificial Intelligence and Law*, pages 159\u2013168.\\n\\nJonathan A Obar and Anne Oeldorf-Hirsch. 2020. The biggest lie on the internet: Ignoring the privacy policies and terms of service policies of social networking services. *Information, Communication & Society*, 23(1):128\u2013147.\\n\\nFrank Robert Palmer. 2001. *Mood and modality*. Cambridge university press.\\n\\nFabian Pedregosa, Ga\u00ebl Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. 2011. Scikit-learn: Machine learning in python. *Journal of machine learning research*, 12(Oct):2825\u20132830.\\n\\nWim Peters and Adam Wyner. 2016. Legal text interpretation: Identifying hohfeldian relations from text. *In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16),* pages 379\u2013384, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nValentina Pyatkin, Shoval Sadde, Aynat Rubinstein, Paul Portner, and Reut Tsarfaty. 2021. The possible, the plausible, and the desirable: Event-based modality detection for language processing. *arXiv preprint arXiv:2106.08037*.\\n\\nPaulo Quaresma, Am\u00e1lia Mendes, Iris Hendrickx, and Teresa Gon\u00e7alves. 2014. Automatic tagging of modality: identifying triggers and modal values. *In Proceedings 10th Joint ISO-ACL SIGSEM Workshop on Interoperable Semantic Annotation*, pages 95\u2013101. European Language Resources Association.\\n\\nAynat Rubinstein, Hillary Harner, Elizabeth Krawczyk, Dan Simonson, Graham Katz, and Paul Portner. 2013. Toward fine-grained annotation of modality in text. *In Proceedings of the IWCS 2013 workshop on annotation of modal meanings in natural language (WAMM)*, pages 38\u201346.\\n\\nRachel Rudinger, Vered Shwartz, Jena D. Hwang, Chandra Bhagavatula, Maxwell Forbes, Ronan Le Bras, Noah A. Smith, and Yejin Choi. 2020. Thinking like a skeptic: Defeasible inference in natural language. *In Findings of the Association for Computational Linguistics: EMNLP 2020*, pages 4661\u20134675, Online. Association for Computational Linguistics.\\n\\nAbhilasha Sancheti, Balaji Vasan Srinivasan, and Rachel Rudinger. 2022. Entailment relation aware paraphrase generation. *Proceedings of the AAAI Conference on Artificial Intelligence*, 36(10).\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. 2016. Controlling politeness in neural machine translation via side constraints. *In Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*, pages 35\u201340.\\n\\nDon Tuggener, Pius von D\u00e4niken, Thomas Peetz, and Mark Cieliebak. 2020. LEDGAR: A large-scale multi-label corpus for text classification of legal provisions in contracts. *In Proceedings of the 12th Language Resources and Evaluation Conference*, pages 1235\u20131241, Marseille, France. European Language Resources Association.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. *Advances in neural information processing systems*, 30.\\n\\nGeorg Henrik von Wright. 1951. Deontic logic. *Mind*, 60(237):1\u201315.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Priscic Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. *In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nAdam Wyner and Wim Peters. 2011. On rule extraction from regulations. *In Legal Knowledge and Information Systems*, pages 113\u2013122. IOS Press.\\n\\nHaoxi Zhong, Zhipeng Guo, Cunchao Tu, Chaojun Xiao, Zhiyuan Liu, and Maosong Sun. 2018. Legal judgment prediction via topological learning. *In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing*, pages 3540\u20133549.\\n\\nHaoxi Zhong, Chaojun Xiao, Cunchao Tu, Tianyang Zhang, Zhiyuan Liu, and Maosong Sun. 2020. Jeqa: A legal-domain question answering dataset. *In Proceedings of the AAAI Conference on Artificial Intelligence*, volume 34, pages 9701\u20139708.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We combine the higher level context (bullets\u2014\\\"parent\\\") with the lower level context (sub-bullet\u2014\\\"child\\\") owing to the hierarchical nature of contracts by iterating over the provisions in a contract in sequential order and following the below rules. Combination can be done in two ways: (i) concatenating, and (ii) merging. We find a sub-bullet via regular expression pattern matching.\\n\\n- If the child is not a complete sentence (identified by the presence of S in root of constituency parse), parent is a complete sentence, and parent does not contain 'follow' or 'below:' then remove ':' from the end of parent and append the child (we call this, merging).\\n- If child starts with a lower case and parent does not contain 'follow' or 'below:' then remove ':' and append child irrespective of the root label of constituency.\\n- If parent ends with 'the following:' then remove 'the following:' and append the child if it is not a complete sentence else do not remove 'the following:' and just append the child (we call this, concatenating).\\n- If none of the above rule satisfies and the parent ends with a ':' then just concatenate the child with the parent.\\n\\n### A.2 Annotation Guidelines\\n\\nWe present the instructions, and the correctly and incorrectly annotated examples with explanations provided to the annotators in Figure 5. The custom annotation inference built to collect the data is shown in Figure 6. We manually annotate 50 sentences and use them as quality check questions to ensure annotators are sincerely and correctly annotating each HIT. Type-wise inter-annotator agreement for the sentences in test split is shown in Table 7.\\n\\n### A.3 Qualification Questions\\n\\nWe ask 10 multiple choice questions in the pre-qualification task consisting of 5 questions to test the understanding of identifying the correct deontic type and 5 questions to test their understanding of trigger span selection for a deontic type.\\n\\n| Obl | Ent | Pro | Per | Nobl | Nent | None |\\n|-----|-----|-----|-----|------|------|------|\\n| shall/will be required, shall be obligated, shall, must, will, have to, should, ought to have, will/shall be paid | shall/will be entitled, shall/will be paid, shall/will retain, shall/will receive, shall have the right to, shall be retained, shall be kept, shall be claimed, shall be accessible, shall be owned, shall be determined | shall/will/must/may not, cannot, shall have no right, can not, shall/will not be allowed, shall not assist, shall/will be prohibited | shall be permitted, shall also be permitted, can, may, could, shall/will be allowed | shall/will not be liable for, shall/will not be obligated to, shall/will not be obligated for, shall/will not be responsible for, shall/will not be required to | shall/will not entitled to, shall/will not have the right to, shall/will not be entitled for |\\n\\nTable 7: Deontic type-wise inter-annotator agreement ($\\\\alpha$) for the test set.\\n\\n| Type | Heuristic triggers |\\n|------|-------------------|\\n| Obl  | shall/will be required, shall be obligated, shall, must, will, have to, should, ought to have, will/shall be paid |\\n| Ent  | shall/will be entitled, shall/will be paid, shall/will retain, shall/will receive, shall have the right to, shall be retained, shall be kept, shall be claimed, shall be accessible, shall be owned, shall be determined |\\n| Pro  | shall/will/must/may not, cannot, shall have no right, can not, shall/will not be allowed, shall not assist, shall/will be prohibited |\\n| Per  | shall be permitted, shall also be permitted, can, may, could, shall/will be allowed |\\n| Nobl | shall/will not be liable for, shall/will not be obligated to, shall/will not be obligated for, shall/will not be responsible for, shall/will not be required to |\\n| Nent | shall/will not entitled to, shall/will not have the right to, shall/will not be entitled for |\\n\\nTable 8: Triggers used to identify the deontic types.\\n\\n### A.4 Resolving Disagreements\\n\\nDisagreement in the annotation for duplicate sentences is resolved by one of the authors. The disagreement could occur because of any missing modality in case of multiple modalities expressed in a sentence, incorrect interpretation of the sentence, or human error in terms of annotating with respect to a tenant or a landlord. Consider the below sentence: \\\"[landlord] After final approval of the Final Plans by applicable governmental authorities, no further changes may be made thereto without the prior written approval of both Landlord and Tenant.\\\", it was annotated as 'prohibition' for landlord by one of the annotators and 'none' by another annotator. As the prohibition mentioned in the sentence is not for the landlord, the correct annotation is 'none'. Therefore, we retain the correct annotation for the example and discard the sentence with the incorrect annotation. Another example is \\\"[landlord] All conditions and agreements under the Lease to be satisfied or performed by Landlord have been satisfied and performed.\\\" which was incorrectly annotated as an 'obligation'.\\n\\n### A.5 Rule-based Approach\\n\\nWe first curate a pre-defined list of triggers (Table 8) used to express deontic types in legal domain following Ash et al. (2020). Then, tokenize and obtain...\"}"}
{"id": "emnlp-2022-main-795", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Span Detection\\n\\n| Precision | Recall | F1  |\\n|-----------|--------|-----|\\n| 98        | 50     | 70  |\\n\\nTable 12 shows the performance of models trained with an anonymized agent on the test set of lease contracts.\\n\\nWe run each model on a sentence in Algorithm 1.\\n\\nThe implementation details of the agents' mention, and its dependency tag for the presence of pre-defined triggers in a given sentence to extract its position (start index), each for the presence of pre-defined triggers in a given sentence to extract its position (start index), each\\n\\nTable 9: Evaluation results for agent-specific multi-label deontic modality classification task on development set.\\n\\n| Model          | Labeled | Unlabeled |\\n|----------------|---------|-----------|\\n| Accuracy       | Precision | Recall | F1  |\\n| Majority       | 47.87    | 26.06    | 38.48 |\\n|                | 7.60     | 4.83     | 12.43 |\\n|                | 14.29    | 14.29    | 20.29 |\\n|                | 9.09     | 7.50     | 10.75 |\\n\\nAll the models are trained with different seeds. BU, B, and L denote base-uncased, base, and large respectively.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1\\n\\nRule-based Heuristic\\n\\n1: Inputs: List $T$ of pre-defined triggers, List $A$ of aliases for the type of contract to process.\\n\\n2: Outputs: List $L$ of tuples containing (Deontic type, trigger, agent, start index) for all the sentence in the contract.\\n\\n3: $L \\\\leftarrow []$, $I \\\\leftarrow []$ // Initialization\\n\\n4: for each sentence in contract do\\n\\n5: // Initialize a list to keep account of visited trigger indices\\n\\n6: $visited \\\\leftarrow []$\\n\\n7: for each $t$ in $T$ do\\n\\n8: if $t$ in sentence then\\n\\n9: // Initialize a list of trigger indices\\n\\n10: $indices \\\\leftarrow []$\\n\\n11: for each $t$ in sentence do\\n\\n12: if start index of $t$ /\u2208 $visited$ then\\n\\n13: $indices \\\\leftarrow$ start index\\n\\n14: $visited \\\\leftarrow$ start index\\n\\n15: end if\\n\\n16: end for\\n\\n17: for word in sentence do\\n\\n18: if word.dependency is ROOT or word.pos \u2208 [VERB, AUX] then\\n\\n19: for child in word.children do\\n\\n20: // Iterate over the children of word in the dependency tree\\n\\n21: If a $a_1$ \u2208 $A$ is 'nsubj/nsubjpass' of word & child== $t[0]$ & child.dependency is 'aux' & child.index in indices then\\n\\n22: $L \\\\leftarrow (Type(t), t, a_1, child.index)$ // Rule 1\\n\\n23: If Rule 1 & a $a_2$ \u2208 $A$ is a 'conj' of $a_1$ then\\n\\n24: $L \\\\leftarrow (Type(t), t, a_2, child.index)$ // Rule 2\\n\\n25: If child1.dependency is 'agent' & child2== $t[0]$ & child2.dependency is 'aux' & $a_1$ \u2208 $A$ in children(child1)=child3 then\\n\\n26: $L \\\\leftarrow (Type(t), t, a_1, child_2.index)$ // Rule 3\\n\\n27: If Rule 3 & not Rule 4 & $a_2$ \u2208 $A$ in conjunction of child3 & VERB in conjunction of word & $t_1$ is 'aux' of VERB then\\n\\n28: $L \\\\leftarrow (Type(t_1), t_1, a_2, t_1.index)$ // Rule 4\\n\\n29: If Rule 3 & not Rule 4 & $a_2$ \u2208 $A$ in conjunction of child3 & child== $t[0]$ & child.dependency is 'aux' & child.index in indices then\\n\\n30: $L \\\\leftarrow (Type(t), t, a_2, child.index)$ // Rule 5\\n\\n31: If child.dependency in ['pobj', 'dobj'] & $a_1$ \u2208 $A$ is in conjunction of children(child)=child1 & VERB in conjunction of word & $t_1$ is 'aux' of VERB then\\n\\n32: $L \\\\leftarrow (Type(t_1), t_1, a_1, t_1.index)$ // Rule 6\\n\\n33: If child== $t[0]$ & child.dependency is 'aux' & child.index in indices & VERB in conjunction of word & $t_1$ is 'aux' of VERB & 'agent' in children(conjunction VERB)= child1.dependency & $a_2$ \u2208 $A$ in children(child1) then\\n\\n34: $L \\\\leftarrow (Type(t), t, a_2, child.index)$ // Rule 7\\n\\n35: If child== $t[0]$ & child.dependency is 'aux' & child.index in indices & VERB in conjunction of word & $t_1$ is 'aux' of VERB & not Rule 7 then\\n\\n36: $L \\\\leftarrow (Type(t_1), t_1, Agent(t), t_1.index)$ // Rule 8\\n\\n37: end if\\n\\n38: end for\\n\\n39: end if\\n\\n40: end for\\n\\n41: end for\\n\\n42: end for\\n\\nTable 13 shows the unlabeled metric scores for generalizability to rental and employment contracts.\\n\\n| Model   | Precision | Recall | F1  |\\n|---------|-----------|--------|-----|\\n| ALeaseBERT | 82.35%    | 87.09% | 14.74% |\\n| Ours    | 85.53%    | 87.28% | 15.54% |\\n\\nTable 14: Results from the red flag detection case study.\\n\\nOur (ALeaseBERT) denotes RoBERTa-L model trained on LEXDEM (Red flags dataset (Leivaditi et al., 2020)).\\n\\nA.8 Case Study: Red flag Detection\\n\\nEvaluation scores for the red flag detection case study are presented in Table 14.\\n\\nA.9 Annotated Examples for Deontic Types\\n\\nSamples annotations are provided in Table 15.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Instructions and examples provided to the annotators.\"}"}
{"id": "emnlp-2022-main-795", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Annotation Interface.\\n\\n| Deontic Type | Examples |\\n|--------------|----------|\\n| Obl | Tenant shall repair any damage resulting from such removal and shall restore the Property to good order and condition. |\\n| Ent | Tenant shall also have the right to use the roof riser space of the Building. |\\n| Pro | Lessee shall not commit or allow waste to be committed on the Premises, and Lessee shall not allow any hazardous activity to be engaged in upon the Premises. |\\n| Nobl | For the avoidance of doubt, to the extent there is a bank vault in the Premises, Tenant shall have no obligation to remove such vault on surrendering the Premises. |\\n| Per | Tenant may, without Landlord\u2019s consent, before delinquency occurs, contest any such taxes related to the Personal Property. |\\n| None | For the avoidance of doubt, it is hereby clarified that wherever the word Lessor is written this means: \u201cthe Lessor and/or anyone acting on its behalf\u201d. |\\n\\nTable 15: Sample annotated sentences for each deontic type with respect to an [Agent] and trigger annotations in bold-face.\"}"}
