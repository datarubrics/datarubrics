{"id": "lrec-2022-1-446", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assessing the Quality of an Italian Crowdsourced Idiom Corpus: the Dodiom Experiment\\n\\nGiuseppina Morza, Raffaele Manna, Johanna Monti\\nUNIOR NLP Research Group - University of Naples \\\"L'Orientale\\nVia Duomo 219, Naples, Italy\\n{gmorza, rmanna, jmonti}@unior.it\\n\\nAbstract\\nThis paper focuses on the evaluation of linguistic data, concerning idioms examples collected and annotated through Dodiom, a GW AP environment, by Italian linguists. The paper provides an insight into the Dodiom project, the data collection through the contribution of the crowd, and, finally it specifically describes the annotation criteria used by the experts to estimate the quality of the collected data. The main scope of this paper is, indeed, the evaluation of the quality of the linguistic data obtained through crowdsourcing, namely to assess if the data provided by the players who joined the game are eligible and profitable for research and teaching purposes. This task concerns the development of a collection of idioms, namely a specific type of Multiword expressions which is usually hard to find in corpora and that contains words that may also be used in their literal meanings within a sentence. This is particularly important as these data may be used both for the training and the evaluation of NLP applications. Finally, results, as well as future work, are presented.\\n\\nKeywords: crowdsourcing, data quality, idioms\\n\\n1. Introduction\\nNatural Language Processing (NLP) has made significant advances in recent years, due to the introduction of statistical machine learning techniques. Human annotators, language experts, obviously play an essential role in the process of building language corpora, required to train statistical learners. However, this unavoidably results in high annotation costs and limited access to qualified annotators, which is a major hindrance to NLP research. As shown by popular platforms like Wikipedia, Duolingo among others, crowdsourcing presents a unique opportunity to obtain massive amounts of data in a relatively short time. The research task described in this contribution is based on (Eryi\u02d8git et al., 2021), who resorted to increasingly popular methodologies for data collection, namely crowdsourcing and gamification. The scholars developed a Game-with-a-purpose (GW AP) named Dodiom to elicit idiom examples and ratings from non-expert labelers reached via Facebook, Linkedin and word-of-mouth. For instance, given a MWE, Dodiom asks the non-expert user to create a sentence in which the MWE is used idiomatically or not and to provide a label related to one of the two uses (literal or idiomatic).\\n\\nOur aim was to assess by means of a subsequent annotation task of the collected textual samples if the data obtained from non-experts (the crowd) in a cost-effective and time-saving way were valuable resources that could effectively be used as training and testing data for language understanding, in general, and idiom identification systems, as well as language learning material, or samples for lexicographic studies. In this paper, we describe the state of the art in idiom corpora development and gamification in Section 2. We then present the corpus collection in 3.1. In 3.2 we provide a description of the guidelines adopted for the data annotation task. The steps for evaluating the annotated data along with the results are shown in 4. Finally, we provide conclusions and a discussion on possible future work in 5.\\n\\n2. Related Work\\nGames-with-a-purpose have come to play a central role in the development of linguistic resources for NLP (Chamberlain et al., 2013).\\nGamification turned out to be an effective tool for the collection of language resources in a cost-effective way, proving on numerous occasions that even non-experts can successfully provide valuable scientific data. Snow et al. (2008) demonstrated the effectiveness of using crowdsourcing for a variety of natural language annotation tasks. In the case of crowdsourcing, the main strategy for achieving good quality in labeling is to aggregate results from many users to approximate the judgments of a single expert (Snow et al., 2008). Below, we briefly outline the main resources created using GW APs; we will then focus on reporting the methods used to evaluate aggregated data in the creation of Multiword expressions (MWEs) linguistic resources by means of crowdsourcing.\\nGW APs have been popularized in the NLP field by early initiatives such as 1001 Paraphrases (Chklovski, 2005), Phrase Detectives (Chamberlain et al., 2008) and Dr. Detective (Dumitrache et al., 2013). One of the first GW APs created and used to collect linguistic data, more specifically, corpora, was 1001 Paraphrases (Chklovski, 2005). In this game, participants were asked to produce paraphrases of expressions. In Phrase Detectives (Chamberlain et al., 2008) participants annotate anaphoric coreference among phrases.\"}"}
{"id": "lrec-2022-1-446", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Idioms, that are at the core of this research project, are seen as a subcategory of multiword expressions (MWEs) which have been subject to many initiatives in recent years such as the Parseme EU COST Action (Savary et al., 2015; Savary et al., 2017), the MWE-LEX workshop series (Markantonatou et al., 2020) and the ACL special interest group SIGLEX-MWE (Cook et al., 2021). Traditional methods of collecting and annotating MWEs generally rely on using textual samples taken from large text corpora, with an effort by the team of researchers in manual annotation of multiword phenomena (Schneider et al., 2014; Savary et al., 2018). However, the scarcity of MWEs (especially idioms) in texts has presented obstacles to corpus-based studies and NLP systems addressing these specific linguistic phenomena (Losnegaard et al., 2016). Crowdsourcing MWE collections and annotations seems to provide a valuable alternative for the development of large-scale corpora of MWEs by leveraging the insights of the participants (Kato et al., 2018; Fort et al., 2018; Fort et al., 2020; Haagsma et al., 2020). RigorMortis (Fort et al., 2018; Fort et al., 2020) gamifies the MWEs collection and annotation processes in French corpora. In this context, gamification was deployed for MWE annotation with the aim of assessing the reliability of non-experts contributions in an MWEs annotation exercise at token level compared to a reference annotation. Taking into account the complexity in identifying types of MWEs (Schneider et al., 2014), (Fort et al., 2020) underline that the crowdsourcing approach can be considered as a valid choice to develop new annotated resources for MWEs identification.\\n\\nKato et al. (2018) combine automatic annotations and crowdsourcing for verbal MWEs including some idioms setting the annotation task as a multiword sense disambiguation problem. Participants from English-speaking countries were chosen on the basis of some requirements such as contributors with high accuracy on the CrowdFlower platform and contributors with a success rate higher than 70% in answering test questions. This procedure shows that annotators agree in approximately 67% on the same sense of verbal MWEs. Magpie (Haagsma et al., 2020) also showed the suitability of crowdsourcing for large-scale annotation of a variety of idiomatic expressions for English. Specifically, after picking up a set of idioms from three electronic dictionaries and extracting all forms of idiom types from the British National Corpus, they asked a crowd to annotate the data. The Magpie corpus was formed by aggregating annotations from participants along with a confidence score. This annotation procedure showed in Magpie confirmed that crowdsourcing is suitable, but both the participants and the procedure need to be carefully selected in order to gather reliable results. In this paper we evaluate and assess the quality of the data collected via the Dodiom gamification platform, a GW AP developed in the framework of EnetCollect, and in particular we analyse if non-expert labelers can provide reliable natural language annotations specifically aimed at idiom corpora construction.\\n\\n3. Dataset\\n\\nIn this section, we describe the idiomatic dataset collected through the Dodiom game. Specifically, we present the data collection process related to the submissions performed by players along with related features associated with Dodiom data. Then, we describe the data annotation process carried out to evaluate the quality of the data collected through the gamified crowdsourcing approach adopted in the Dodiom experiment: we present the guidelines adopted and the annotation process performed by the annotators.\\n\\n3.1. Data Collection\\n\\nWe first provide a short outline of the Dodiom project ideation, participants, and data collection. Subsequently, we describe the annotation process and the results concerning the quality of the collected data. Dodiom originates as a collaboration project between the NLP Research Group from the Department of Artificial Intelligence and Data Engineering of Istanbul University and the UNIOR NLP Research Group from the Department of Literary, Linguistic and Comparative Studies of the University of Naples L'Orientale.\\n\\nThe aim of the Dodiom project is the implementation of a gamified crowdsourcing approach for idiom corpora construction, where the crowd is actively taking a role in creating and annotating the language resource and rating annotations (Eryi\u02d8git et al., 2021). The Dodiom game has the major aim of collecting valuable usage samples for idioms which contain words that may also commonly be used in their literal meanings within a sentence, and for this reason make it difficult both for NLP systems and for language students to make sense of it. As an example, the idiom \\\\textit{gettare la spugna} (to throw the sponge) may have a literal reading and an idiomatic one, namely \\\\textit{to throw in the towel}, or \\\\textit{ammainare le vele} may mean \\\\textit{to furl the sails} or \\\\textit{to surrender}, depending on the context. The game concerned two languages, namely Turkish and English.\\n\\nEuropean Network for Combining Language Learning with Crowdsourcing Techniques. EnetCollect had its main focus on combining the well-established domain of Language Learning with recent and successful crowdsourcing approaches. Official website available here: https://enetcollect.eurac.edu/.\\n\\nFor a more detailed description of i) the gamified crowdsourcing approach used for collecting language learning materials for idiomatic expressions and ii) the design of the Dodiom messaging bot, an asynchronous multiplayer game for native speakers who compete with each other while providing idiomatic and nonidiomatic usage examples and rating other players' entries refer to (Eryi\u02d8git et al., 2021).\"}"}
{"id": "lrec-2022-1-446", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Italian, even though it was designed with localization in mind, so as to collect idiom samples in multiple languages.\\n\\nThe Dodiom game, deployed between October and December 2020, aims at collecting idiomatic and non-idiomatic samples for specific idioms, which the players are required to submit when joining the game. Crowd-rating is also included in the game structure, as players are asked to express a positive or negative opinion upon other players' submissions ('likes' and 'dislikes' respectively). Improper use of the platform, as well as vulgar language, may also be reported by players (reports being later reviewed by moderators). For the Italian language, the overall Dodiom dataset includes a total amount of 6,730 samples, split into two sub-datasets: i) with-reward containing 5,286 samples, obtained during a session of the game where some monetary rewards were given to the best player of each day ii) without-reward containing 1,444 sentences.\\n\\n3.2. Data Annotation\\n\\nSince our goal is to evaluate the linguistic resources created and obtained through the use of the Dodiom game, we designed an annotation scheme aimed at estimating the reliability of the linguistic data collected. Namely, we devised a two-step annotation task in order to confirm whether the data provided by the players who joined the game are actually eligible and profitable for research and teaching purposes.\\n\\nIn the first step, we set up a list of guidelines along with 12 parameters which were employed to assess the sentences submitted by the players. Guidelines and parameters were set to aid the annotation exercise. Their purpose was to guarantee a consistent approach amongst annotators and to resolve ambiguous cases.\\n\\nFor eleven parameters, we selected a binary evaluation system for each of the categories: 0, assigned as a default value to all examples provided by the crowd (that is, when they do not match the category under examination), whereas value 1 was assigned whenever the example was deemed to match the category. Instead, for a single parameter, Quality, we opted for using a scale associated with three values: 0, 1 and 2 which will be later further explained.\\n\\nIn order to have a clearer insight about what expert linguists evaluate in the collected crowdsourced samples, we hereby list the parameters taken into account.\\n\\n- **Wrong Category**: the sentence provided by the player is not correctly classified by the player, for instance it is classified by the user as having an idiomatic use while it is non-idiomatic.\\n- **Undecidable**: no sufficient context to decide if it is an idiomatic or non-idiomatic usage.\\n- **Low Context**: when no or only poor context has been provided.\\n- **Vulgar**: the proposed example is perceived as vulgar or offensive.\\n- **Incorrect Grammar - Word Order**: the grammar or the word order of the provided example is incorrect.\\n- **Incorrect Spelling**: the example contains spelling mistakes.\\n- **Meaningless**: the submitted sentence does not make sense to an average speaker.\\n- **Negative Sentiment**: the example arouses negative feelings.\\n- **Restricted Readers**: jokes related to the game itself. Some of the examples submitted were strictly related to the game itself and would therefore be unintelligible for an average reader. An instance for this was \\\"A volte non so che esempi scrivere su dodiom, brancolo nel buio\\\" (Sometimes I don't know what examples to write on dodiom, I'm fumbling about in the dark)\\n- **Not-Idiomatic & Not-Literal**: the players did not provide an example for the meaning required by the game but for one of the other possible meanings of the same idiom.\\n- **Quality (0-2)**: 0 - No/poor quality; 1 - Good quality (good examples for machines and human learning purposes); 2 - Excellent quality (very good examples which can be included in Dictionaries and Language learning resources).\\n\\nThe annotation scheme described above was applied to two random subsets of the Italian Dodiom dataset (10% out of the total collected samples). Specifically, 575 sentences from the with-reward dataset and 154 from the without-reward dataset. Subsequently, both subsets were labeled by three expert annotators with background in linguistics: A1, A2 and A3. They were tasked with labeling the same 729 sentences, each operating separately. In Figure 2 we provide some annotated sentences according to the twelve parameters and extracted from the with-reward subset.\\n\\n4 M.A. students in Linguistics from the Department of Literary, Linguistic and Comparative Studies at University of Naples \\\"L'Orientale\\\"\"}"}
{"id": "lrec-2022-1-446", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Data Evaluation\\n\\nIn this section, we first describe the results obtained from the Inter-Annotator Agreement (IAA) between the three linguistic experts with respect to their judgments about the submissions of Dodiom players following the parameters described in section 3.2. Subsequently, we will present the results obtained comparing the three expert linguists' judgments about quality and category on the common subsets of 729 sentences by the Dodiom players.\\n\\n4.1. Inter-Annotator Agreement\\n\\nOnce the annotation process was completed, we compared the judgments expressed by the three expert linguists according to the twelve parameters chosen. In the following section, we first show the results obtained by measuring the agreement among annotators using the Average Pairwise Percentage Agreement and Krippendorf's Alpha of the three annotators against the common with-reward subset according to the defined parameters, then we show also the results obtained for the common subset without-reward.\\n\\nConcerning the IAA calculated on the subset with-reward, the agreement observed between the three linguistic experts and calculated with the average pairwise agreement was higher than 95% for the first eleven parameters, while a lower average was recorded for Quality with an average agreement of 53%. A plausible reason for the low agreement in the Quality criterion is related to the subjective nature encoded in the labels used to rate user submissions by experts. Furthermore, for the Quality parameter, the average of the agreement and the IAA shown in Table 1 do not take into account the ordinal nature of the labels. Consequently, an IAA was calculated that takes into account the closeness between the labels affixed by the annotators, i.e. the disagreement between 0 and 2 or 0 and 1 are penalized, while the disagreement between 1 and 2 is not penalized. We consider the agreement as the closeness between the two labels (1 and 2) that suggest good or excellent quality.\\n\\nIn addition, a lower reliability was recorded for Not-idiomatic parameter with an alpha value of 0.48. In Table 1, we show the results of the IAA calculated with Krippendorf's Alpha for all annotators and Pairwise Percentage Agreements for each pair of annotators.\\n\\nInstead, concerning the IAA calculated on the subset without-reward, in Table 2 we show the results obtained using Pairwise Percentage Agreements for each pair of annotators.\\n\\nIn contrast, to measure the reliability of the three annotators' judgments we use Krippendorf's Alpha (Antoine et al., 2014; Zapf et al., 2016). In this annotation exercise using without-reward subset, the reliability between annotators is high enough.\"}"}
{"id": "lrec-2022-1-446", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Linguistic Experts and the Crowd\\n\\nIn this section, we provide an insight on the results obtained by comparing the judgments made by Dodiom users regarding Idiom/Not-Idiom and Quality with the judgments made by the three linguistic experts. Regarding the with-reward subset, linguistic experts annotated on average 14% of sentences as being incorrectly assigned by users to the idiomatic category (336 sentences). In contrast, they annotated on average 1.3% of sentences marked by users as being incorrectly assigned by users to the non-idiomatic category (239 sentences). In Table 3, we provide the results regarding the corrections on the Category. With category corrections, we are referring to the number of times that expert linguists labeled the sentences of Dodiom users as 1 in the Wrong Category parameter. While in Table 4, we show the number of corrections made on the subset without-reward.\\n\\nTables 3 and 4 show that the interventions of the linguistic experts in correcting the category of idioms were very few. This shows that the crowd is able to create and use sentences containing idioms, as well as to recognize non-idiomatic uses. In addition, the judgment left by the crowd about the category of the sentence submitted or proposed by the Dodiom game are not far from the judgment of an expert linguist.\\n\\nIn Table 5, we show the results regarding the Quality judgments about idiomatic sentences by linguistic experts compared with those by Dodiom users. The results refer to the complete subset extracted from the Dodiom corpus. In fact, we show both the quality judgments about the with-reward subset and the without-reward subset.\\n\\n5. Conclusions\\n\\nWe demonstrate the effectiveness of gamification for the collection of valuable data related to idiom corpora construction. The guidelines we proposed to annotate the crowdsourced corpus proved that players have provided valuable results, which confirms the great advantage of deploying such strategies for the acquisition of precious language resources which, otherwise, could have taken much longer and would have been extremely expensive. Given these results, we decided to run the game again and have the crowd annotate a new idiom corpus focusing on idioms that turn out to be ambiguous, i.e. that have both literal and idiomatic readings, extracted from the online Italian dictionary compiled under the direction of linguist Tullio De Mauro.\"}"}
{"id": "lrec-2022-1-446", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our thanks go also to the Italian annotators Adriana Casasso and Giovanna Carandente, for their precious contribution. Autorship contribution is as follows: Giuseppina Morza is author of Sections 1 and 2; Johanna Monti is author of Section 3; Raffaele Manna of Section 4. Abstract and conclusions are in common.\\n\\n7. Bibliographical References\\n\\nAntoine, J.-Y., Villaneau, J., and Lefeuvre, A. (2014). Weighted krippendorff's alpha is a more reliable metrics for multi-coders ordinal annotations: experimental studies on emotion, opinion and coreference annotation. In EACL 2014, pages 10\u2013p.\\n\\nChamberlain, J., Poesio, M., Kruschwitz, U., et al. (2008). Phrase detectives: A web-based collaborative annotation game. In Proceedings of the International Conference on Semantic Systems (I-Semantics' 08), pages 42\u201349.\\n\\nChamberlain, J., Fort, K., Kruschwitz, U., Lafourcade, M., and Poesio, M. (2013). Using games to create language resources: Successes and limitations of the approach. In The People's Web Meets NLP, pages 3\u201344. Springer.\\n\\nChklovski, T. (2005). 1001 paraphrases: Incenting responsible contributions in collecting paraphrases from volunteers. In AAAI Spring Symposium: Knowledge Collection from Volunteer Contributors, pages 16\u201320.\\n\\nProceedings of the 17th Workshop on Multiword Expressions (MWE 2021), Online, August. Association for Computational Linguistics.\\n\\nDumitrache, A., Aroyo, L., Welty, C., Sips, R.-J., and Levas, A. (2013). \u201cdr. detective\u201d: combining gamification techniques and crowdsourcing to create a gold standard in medical text. In Proceedings of the 1st International Workshop on Crowdsourcing the Semantic Web (CrowdSem 2013), 12th International Semantic Web Conference. Citeseer.\\n\\nEryigit, G., Sentas, A., and Monti, J. (2021). Gami\ufb01ed crowdsourcing for idiom corpora construction. arXiv preprint, arXiv:2102.00881.\\n\\nFort, K., Guillaume, B., and Stern, V. (2014). ZOMBILINGO: eating heads to perform dependency syntax annotation (ZOMBILINGO: manger des t\u00eates pour annoter en syntaxe de d\u00b4ependances) [in French]. In Proceedings of TALN 2014 (Volume 3: System Demonstrations), pages 15\u201316, Marseille, France, July. Association pour le Traitement Automatique des Langues.\\n\\nFort, K., Guillaume, B., Constant, M., Lefebvre, N., and Pilatte, Y.-A. (2018). \u201cfingers in the nose\u201d: Evaluating speakers' identification of multi-word expressions using a slightly gamified crowdsourcing platform. In Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), pages 207\u2013213.\\n\\nFort, K., Guillaume, B., Pilatte, Y.-A., Constant, M., and Lefebvre, N. (2020). Rigor mortis: Annotating mwes with a gamified platform. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4395\u20134401.\\n\\nGuillaume, B., Fort, K., and Lefebvre, N. (2016). Crowdsourcing complex language resources: Playing to annotate dependency syntax. In International Conference on Computational Linguistics (COLING).\\n\\nHaagsma, H., Bos, J., and Nissim, M. (2020). MAGPIE: A large corpus of potentially idiomatic expressions. In Proceedings of the 12th Language Resources and Evaluation Conference, pages 279\u2013287, Marseille, France, May. European Language Resources Association.\\n\\nKato, A., Shindo, H., and Matsumoto, Y. (2018). Construction of large-scale english verbal multiword expression annotated corpus. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018).\\n\\nLosnegaard, G. S., Sangati, F., Escartin, C. P., Savary, A., Bargmann, S., and Monti, J. (2016). PARSEME survey on MWE resources. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 2299\u20132306, Portoro\u017e, Slovenia, May. European Language Resources Association (ELRA).\\n\\nStella Markantonatou, et al., editors. (2020). Proceedings of the Joint Workshop on Multiword Expressions and Electronic Lexicons, online, December. Association for Computational Linguistics.\\n\\nSavary, A., Sailer, M., Parmentier, Y., Rosner, M., Ros\u00e9n, V., Przepiorkowski, A., Krstev, C., Vincze, V., Wojtowicz, B., Losnegaard, G. S., et al. (2015). Parseme\u2013parsing and multiword expressions within a european multilingual network. In 7th Language & Technology Conference: Human Language Technologies as a Challenge for Computer Science and Linguistics (LTC 2015).\\n\\nSavary, A., Ramisch, C., Cordeiro, S. R., Sangati, F., Vincze, V., Qasemi Zadeh, B., Candito, M., Cap, F., Giouli, V., Stoyanova, I., et al. (2017). The parseme shared task on automatic identification of verbal multiword expressions. In The 13th Workshop on Multiword Expression at EACL, pages 31\u201347.\\n\\nAgata Savary, et al., editors. (2018). Proceedings of the Joint Workshop on Linguistic Annotation, Multiword Expressions and Constructions (LAW-MWE-CxG-2018), Santa Fe, New Mexico, USA, August. Association for Computational Linguistics.\\n\\nSchneider, N., Onuffer, S., Kazour, N., Danchik, E., Mordowanec, M. T., Conrad, H., and A Smith, N. (2014). Comprehensive annotation of multiword expressions in a social web corpus.\"}"}
{"id": "lrec-2022-1-446", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Snow, R., O\u2019connor, B., Jurafsky, D., and Ng, A. Y. (2008). Cheap and fast\u2013but is it good? evaluating non-expert annotations for natural language tasks. In Proceedings of the 2008 conference on empirical methods in natural language processing, pages 254\u2013263.\\n\\nZapf, A., Castell, S., Morawietz, L., and Karch, A. (2016). Measuring inter-rater reliability for nominal data\u2013which coefficients and confidence intervals are appropriate? BMC medical research methodology, 16(1):1\u201310.\"}"}
