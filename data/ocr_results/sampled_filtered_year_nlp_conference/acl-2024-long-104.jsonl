{"id": "acl-2024-long-104", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The performance of AFaCTA with close- and open-source models. We report the average Cohen's Kappa with human experts for agreement, and the accuracy scores are in percentage. We also report the portion of perfectly consistent annotations reported by each model in percentage, which can be found in the consistency column.\\n\\n### Table 4\\n\\n| Model          | Agreement | Accuracy | Consistency |\\n|----------------|-----------|----------|-------------|\\n| zephyr-7b-\u03b2    | 0.205     | 66.18    | 0.49        |\\n| llama-2-13b-chat | 0.306    | 56.74    | 0.00        |\\n| GPT-3.5        | 0.510     | 76.74    | 43.38       |\\n| GPT-4          | 0.615     | 86.27    | 48.78       |\\n\\nTable 5: The performance of each AFaCTA steps. Similar to Table 3, we report the average Cohen's Kappa with human experts for agreement, and the accuracy scores are in percentage.\\n\\n### Table 5\\n\\n| Model          | Agreement | Accuracy | Accuracy | Accuracy |\\n|----------------|-----------|----------|----------|----------|\\n| GPT-3.5        | 0.458     | 73.16    | 0.452    | 78.06    |\\n| GPT-4          | 0.633     | 85.54    | 0.437    | 79.90    |\\n\\nFigure 5: We notice that in Figure 2, GPT-3.5's accuracy on the perfectly consistent set does not seem to converge with 11 voters. So we extend the number of CoTs to 19, observing that the accuracy converges to 84.1%.\\n\\nFigure 6: Self-consistency CoT experiments on CheckThat!-2021-dev. Same metrics are reported as Figure 2.\\n\\nExperiments on Social Media Domain\\nWe compare AFaCTA's annotation performance with human experts on the re-annotated CheckThat!-2021 development set. We have chosen this small set of social media data due to the limitation of the annotation budget. Similar observations as PoliClaim test can be drawn. GPT-4 AFaCTA outperforms experts on perfectly consistent samples and underperforms on inconsistent samples. GPT-3.5 also achieves a moderate agreement with human experts on perfectly consistent samples. Error analysis shows that GPT-3.5's error concentrates on false negatives, similar to its behavior in the political speech domain (see Table 12).\\n\\nWe also conduct the self-consistency CoT experiments on CheckThat!-2021-dev to verify the importance of a diversified source of self-consistency. The results are shown in Figure 6. It can be observed that the level of self-consistency calibrates accuracy, and the 3 predefined reasoning paths outperform automatically generated ones. One discrepancy is that self-consistency CoT slightly outperforms GPT-3.5 AFaCTA when sampling more than 7 reasoning paths. We attribute this to GPT-3.5's heavier hallucinations on Twitter domain (see Table 12 where it fails to identify apparent factual information). Therefore, complicated reasoning paths like AFaCTA Step 3 might be challenging in many cases.\\n\\nImportantly, due to the annotation budget, our experimental dataset on the social media domain is limited. We leave the extensive analysis of this domain to future work.\\n\\nK Fine-tuning Settings\\nFor all RoBERTa and DistilBERT fine-tuning experiments, we keep all settings the same except for the training data. All models are fine-tuned for 5 epochs with a batch size of 64. We do not fine-tune on the test set.\\n\\nNote: The table and figures are provided in a natural text format, with the same content as the original, ensuring that the text is readable and understandable in its current form.\"}"}
{"id": "acl-2024-long-104", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: AFaCTA's performance on our re-annotated CheckThat!-2021-dev. Similar rows, columns, and scores are reported as Table 3.\\n\\nconduct checkpoint selection. For other hyperparameters, we keep the default setting of hugging-face TrainingArgument: a learning rate of 5e-5, a max_grad_norm of 1, no warm-up and weight decay, etc. We use the huggingface checkpoints of \\\"roberta-base\\\" and \\\"distilbert-base-uncased\\\". All experiments are conducted on a node with 4 32G V100 GPUs. It takes roughly 0.1 GPU hour to train a classifier. In this work, we always use Sci-kit Learn for score computing.\\n\\nL Statistical Significance Test\\nWe conduct a statistical significance test to show that different training set combinations of PoliClaim gold, PoliClaim silver, and PoliClaim bronze lead to statistically significant differences in fine-tuning claim detectors. We first conduct a Student-t test for each training combination based on the results of three random seeds and then aggregate p-values using Fisher's method. For example, to compare \\\"only PoliClaim gold\\\" vs. only \\\"PoliClaim silver\\\", we use the following formula:\\n\\n\\\\[ p_{x00} = \\\\text{Student-t}(\\\\{\\\\text{Acc}_{x00g}\\\\}, \\\\{\\\\text{Acc}_{x00s}\\\\}) \\\\]  \\n\\n\\\\[ p_{agg} = \\\\text{Fisher}(p_{100}, p_{200}, ..., p_{2000}) \\\\]\\n\\nwhere \\\\( r \\\\) denotes random seeds 42, 43, and 44; \\\\( p_{x00} \\\\) denotes the p-value of the \\\\( x00 \\\\) step; and \\\\( p_{agg} \\\\) denotes the aggregated p-value. The aggregated p-values of all comparisons are shown in Table 7. It can be seen that all observations in Section 5.5 and Appendix M are statistically significant. Scipy's implementations for Student-t test and Fisher's Method are used.\\n\\nWe do not conduct statistical tests on experiments of Section 5.1 as obtaining independent samples of human / GPT-4 annotation can be very costly, and OpenAI API does not support random seeds at the moment of experimenting.\\n\\nM Further Fine-tuning Experiments\\nThis section provides more supplementary results of the experiments in Section 5.5.\\n\\n| Dataset Combination | p-value (Student-t) | p-value (Fisher) |\\n|---------------------|--------------------|-----------------|\\n| Only S < Only G      | 5.54e-3            | 8.89e-5         |\\n| Only S < Only G      | 2.39e-36           | 5.79e-51        |\\n| Only S < Only G      | 1.88e-20           | 6.03e-29        |\\n| 500 G + B < 500 G + S| 1.50e-28           | 1.82e-30        |\\n| 1000 G + B < 1000 G + S | 8.13e-13   | 3.30e-8         |\\n| 1500 G + B < 1500 G + S | 2.19e-16   | 1.69e-15        |\\n| All G + B < All G + S | 3.68e-9       | 1.36e-13        |\\n\\nTable 7: Statistical significance of performance difference with different train sets. G, S, and B denotes PoliClaim gold, PoliClaim silver, and PoliClaim bronze correspondingly. By \u2217 and \u2217\u2217, we denote a p-value smaller than 0.01 and 0.001, respectively.\\n\\nFigure 7: The performance of fine-tuned DistilBERT on PoliClaim test when gradually adding training data of different quality. Same scores are reported as Figure 3.\\n\\nM.1 Only Golden, Silver, or Bronze\\nWe gradually increase the size of golden, silver, and bronze training data to fine-tune DistilBERT. The results are shown in Figure 7. The same observations can be drawn from Figure 3: perfectly consistent (silver) data achieve a similar growing trend as manually supervised (golden) data, while accuracy grows slower when adding (bronze) inconsistent data.\\n\\nM.2 Augmenting Gold Data with Silver/Bronze Data\\nWe conduct the data augmentation experiments in Section 5.5 on both RoBERTa (Figure 8) and DistilBERT (Figure 9) with a different number of PoliClaim gold data (500, 1000, 1500, and 1936). Similar conclusions as Section 5.5 can be drawn: perfectly consistent (silver) data are better at augmenting DistilBERT.\"}"}
{"id": "acl-2024-long-104", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 8: The RoBERTa performance of augmenting a limited number of PoliClaim gold data. An augmented version of Figure 4 with 1000 and 1500 Gold data experiments added.\\n\\nIn all experiments, the marginal benefit of adding data decreases quicker on DistilBERT than on RoBERTa, as expected. However, we suspect adding more high-quality annotated and diversified data might boost weaker models to outperform stronger models, though the marginal accuracy gain is low. We leave this exploration to future work.\\n\\nN Error Analyses\\n\\nWe conduct a thorough analysis on GPT-4 and GPT-3.5 AFaCTA. Errors on PoliClaim test can be found in Table 8, Table 9, and Table 10. Errors on CheckThat!-2021-dev can be found in Table 11 and Table 12.\\n\\nIn both domains, we observe that GPT-4 is good at disentangling factual information from speeches or tweets. But it also leads to false positive errors due to over-sensitivity towards factual information. It also makes negative errors due to the lack of full context of the statements. In general, GPT-4 only makes mistakes on confusing samples that lie between factual and non-factual claims.\\n\\nGPT-3.5's errors concentrate on false negatives. It regularly hallucinates about personal experience and quotations which are explicitly defined in the prompts. It is very conservative in identifying anything as verifiable fact arguing there not enough \\\"specific details\\\" to determine verifiability. However, many facts are already specific enough for verification (see row 2 of Table 9). Sometimes, it also fails to identify facts entangled with opinions (see row 1 of Table 10 and row 1 of Table 12).\"}"}
{"id": "acl-2024-long-104", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: The DistilBERT performance of augmenting a limited number of PoliClaim gold data. The same scores are reported as Figure 8.\\n\\nFigure 10: The performance of combining different amount of PoliClaim gold and PoliClaim test.\\n\\n| Error Type | Example Statement | Error Reason |\\n|------------|-------------------|--------------|\\n| False Positive | over-sensitive to granular, unspecific, or not-explicitly-presented facts | recognizing 'members of the legislature did some thing' as fact, which is too vague. |\\n| False Negative | not enough context | \u201cit\u201d here refers to the return of pension fund, which is in a far context. But AFaCTA only considers a one-sentence context. |\\n| Error 1 | I just want to thank you, thank you members of the legislature for all you did these past two years to keep us safe. | location names are recognized as facts. |\\n| Error 2 | It\u2019s true from the Flatirons to Fishers Peak to Pikes Peak and beyond. | people's appearance at the event is recognized as a fact, which is not explicitly presented. |\\n| Error 3 | Sheriff Pelle, fire fighters, and emergency responders, please stand so we can thank you for the life saving work that you do every day. | identify the speaker's back and addressing legislature as facts. |\\n| Error 4 | I\u2019m glad to be back at the capitol addressing the legislature in person, and I thank you for the invitation to speak to you tonight. | Error reason: it claims a fact that this year is an election year, but the model comprehends this as a hypothetical condition, due to its lack of context that 2022 is the election year for Alaska. |\\n| Error 5 | It\u2019s the result of great investment decisions, policies, violation, and direction. | |\\n| Error 6 | Alaskans won\u2019t accept that we can\u2019t get anything done because it\u2019s an election year. | |\"}"}
{"id": "acl-2024-long-104", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Error Type GPT-3.5 Errors and Explanations\\n\\nFalse Positive:\\nover-sensitive to\\nunspecific facts\\n\\nError 1: The fresh air is not a given as a fact, which is unspecific and leans towards unverifiable opinion.\\n\\nTable 9: The only false positive error and the major type of false negative errors made by GPT-3.5 AFaCTA on PoliClaim test.\"}"}
{"id": "acl-2024-long-104", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"False Negative:\\nunderstand facts as opinions or fail to identify facts entangled with opinions\\n\\nError 23:\\nThey plan their lives around hunting season, or fishing season; construction season, or tourism season. But not election season.\\n\\nError reason: the model misunderstands it as the speaker's opinion. But people's lifestyles and priorities can be verified with related surveys or studies.\\n\\nError 24:\\nA future where a dynamic, multi-modal transportation system meets the needs of our growing population.\\n\\nError reason: the model fails to identify \\\"our growing population\\\" as a fact.\\n\\nError 25:\\nWhen I was elected Governor, I knew that I would be remembered not for who I was, where I came from, or even what I said at events like this, but for what I did to make a meaningful, measurable, positive impact on the lives of Alaskans.\\n\\nError reason: the model fails to identify that \\\"the speaker is elected as the governor\\\" is a fact.\\n\\nError 26:\\nBut over time, we've learned we can't solve big problems like climate situationally, with short-term thinking.\\n\\nError reason: the model fails to identify the causality claim about short-term thinking and big problems.\\n\\nError 27:\\nBut at a time, when we've been heating and burning, one thing we can not do is repeat the mistakes of the past by embracing polluters.\\n\\nError reason: the model fails to recognize the fact of embracing polluters in the past.\\n\\nFalse Negative:\\nhallucinate about personal experience and citation\\n\\nError 28:\\nAt times, her school work and distance from her home state made her wonder if she should give up her Miss Alaska title.\\n\\nError reason: the model argues the personal experience is subjective.\\n\\nError 29:\\n\\\"A lot of people,\\\" she said, \\\"don't recognize that their low points are going to propel them to their future.\\n\\nError reason: subjective personal experience.\\n\\nError 30:\\nI agree with former Governor Jay Hammond that the government should never take more from the Permanent Fund than is distributed to the people of Alaska.\\n\\nError reason: fail to detect the citation.\\n\\nError 31:\\nShe is in a healthy marriage and is reconnecting with her children.\\n\\nError reason: consider personal experience as unverifiable.\\n\\nError 32:\\n\\\"Dad,\\\" Catherine said, \\\"Alaska has so much to offer.\\\"\\n\\nError reason: fail to detect the citation.\\n\\nError 33:\\nStill, she found the strength to take down the shooter, ending his violent killing spree and saving many precious lives.\\n\\nError reason: consider personal experience as unverifiable.\\n\\nFalse Negative:\\nhallucinate about rhetoric\\n\\nError 34:\\nThey're wondering how we've come to a place where the PFD is nothing more than what's left over after government takes the lion's share.\\n\\nError reason: fail to understand the metaphor.\"}"}
{"id": "acl-2024-long-104", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Error Type GPT-3.5 Errors and Explanations\\n\\nFalse Negative:\\nfail to identify\\nfacts entangled\\nwith opinions\\n\\nError reason: although have subjective interpretations, the facts that \\\"markets are gonna historically crash\\\" and the quotation of health care officials' speech.\\n\\nFalse Negative:\\nfail to understand\\ncomprehend\\n\\nError reason: this tweet claims the content of the link, which is verifiable.\\n\\nError 1:\\nReason: fail to identify \\\"the growing coronavirus threat\\\".\\n\\nError 2:\\nReason: fail to identify \\\"elected Donald Trump will end the US economy\\\".\\n\\nError 3:\\nReason: fail to identify the correlation between the infected persons' race and their suffers.\\n\\nError 4:\\nReason: fail to identify \\\"the Trump Crash and the death of the US economy.\\\"\\n\\nError 5:\\nReason: fail to identify \\\"the correlation between the infected persons' race and their suffers.\\\"\\n\\nError 6:\\nReason: this tweet claims the content of the link, which is verifiable.\\n\\nError 7:\\nReason: this tweet claims that the link contains a public safety announcement fighting COVID, which is verifiable.\\n\\nError 8:\\nReason: this tweet claims that the link contains a public safety announcement fighting COVID, which is verifiable.\\n\\nError 9:\\nReason: this tweet claims the content of the link, which is verifiable.\\n\\nError 10:\\nReason: this tweet claims the content of the link, which is verifiable.\\n\\nError 11:\\nReason: this tweet claims the content of the link, which is verifiable.\\n\\nError 12:\\nTable 12: All errors made by GPT-3.5 AFaCTA on CheckThat!-2021-dev.\"}"}
{"id": "acl-2024-long-104", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AFaCTA: Assisting the Annotation of Factual Claim Detection with Reliable LLM Annotators\\n\\nJingwei Ni1, Minjing Shi1, Dominik Stammbach1, Mrinmaya Sachan1, Elliott Ash1, Markus Leippold2, 3\\n\\n1ETH Z\u00fcrich\\n2University of Z\u00fcrich\\n3Swiss Finance Institute (SFI)\\n\\n{jingni, msachan, ashe}@ethz.ch, shimin@student.ethz.ch, markus.leippold@bf.uzh.ch\\n\\nAbstract\\n\\nWith the rise of generative AI, automated fact-checking methods to combat misinformation are becoming more and more important. However, factual claim detection, the first step in a fact-checking pipeline, suffers from two key issues that limit its scalability and generalizability: (1) inconsistency in definitions of the task and what a claim is, and (2) the high cost of manual annotation. To address (1), we review the definitions in related work and propose a unifying definition of factual claims that focuses on verifiability. To address (2), we introduce AFaCTA (Automatic Fact Claim Detection Annotator), a novel framework that assists in the annotation of factual claims with the help of large language models (LLMs). AFaCTA calibrates its annotation confidence with consistency along three predefined reasoning paths. Extensive evaluation and experiments in the domain of political speech reveal that AFaCTA can efficiently assist experts in annotating factual claims and training high-quality classifiers, and can work with or without expert supervision. Our analyses also result in PoliClaim, a comprehensive claim detection dataset spanning diverse political topics.\"}"}
{"id": "acl-2024-long-104", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Type Examples and Explanations\\n\\nFacts\\n\\nExample 1: We are tackling other needed projects to increase capacity like six-lane I-10 in West Mobile from Theodore to Irvington.\\n\\nFact part: The sentence presents a clear and explicit fact about a project.\\nOpinion part: the project's necessity is a subjective judgment.\\n\\nExample 2: We are thankful that we haven't suffered any loss of life, and it's always heartening to see and hear stories of Alaskans pitching in to help each other.\\n\\nFact part: no people die in the storm (according to contexts).\\n\\nExample 3: I thank the legislature for standing with my administration and the people of Alaska by funding this effort.\\n\\nFact part: they fund the effort of resource development (according to contexts).\\n\\nVerifiable but NOT check-worthy\\n\\nExample 1: Democrats and the Media need to stop using the #Coronavirus to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring people!\\n\\nThis tweet is labeled as check-worthy by CheckThat!-2021 (Nakov et al., 2021) since it is a polarized political opinion. However, the Democrats' and Media's intention is subjectively interpreted and cannot be verified by objective evidence.\\n\\nExample 2: Trump's preference for well-done steaks topped with ketchup.\\n\\nThis is an unverifiable personal preference. However, it is politicized and used to criticize political figures, thus making it checkworthy.\\n\\nCheckworthy but NOT verifiable\\n\\nExample 1: Democrats and the Media need to stop using the #Coronavirus to politicize things and scare people. It's irresponsible. This is not the time to try and gain political points or headlines from scaring people!\\n\\nThis tweet is labeled as check-worthy by CheckThat!-2021 since it is a polarized political opinion. However, the Democrats' and Media's intention is subjectively interpreted and cannot be verified by objective evidence.\\n\\nExample 2: Trump's preference for well-done steaks topped with ketchup.\\n\\nThis is an unverifiable personal preference. However, it is politicized and used to criticize political figures, thus making it checkworthy.\\n\\nVerifiable but (maybe) NOT check-worthy\\n\\nExample 1: Italy's Prime Minister Giuseppe Conte has announced that the whole of the country is being put on lockdown in an attempt to contain the #coronavirus outbreak.\\n\\nThis tweet with verifiable fact is labeled as NOT checkworthy by CheckThat!-2021.\\n\\nExample 2: Zee News: Petrol price reduced by Rs 2.69; CNN: Petrol price reduced by Rs 2.69; BBC: Petrol price reduced by Rs 2.69; NDTV: China is sending Corona Virus to the world via mails and WhatApp.\\n\\nThis tweet cites news with verifiable facts. But it is labeled NOT checkworthy by CheckThat!-2021.\\n\\nContext of Claims\\n\\nExample: ...Those with schizophrenia spectrum and psychosis disorders, many self-medicating with drugs or alcohol addictions. That's precisely what our containment resolution grants and our new CARE Court seek to address. Getting people off the streets, out of tents, and into housing and treatment is essential to making our streets safe for everyone, but public safety certainly isn't just about homelessness...\\n\\nThis claim defines the duty of CARE but is not self-contained. It is hard to determine its verifiability without the full semantic information in context.\\n\\nTable 1: Examples that are not well-defined according to definitions in related work, illustrating the definition of factual claim detection is hard and controversial. Example claims are highlighted in yellow. Explanations are written in italics.\\n\\nsan et al., 2015), COVID-19 tweets (Alam et al., 2021a), biomedical (W\u00fchrl and Klinger, 2021) and environmental claims (Stammbach et al., 2023a). This potentially limits models' ability to generalize to future topics. However, manually annotating datasets with new topics is too expensive. In light of this, we propose AFaCTA, a multi-step reasoning framework that leverages LLMs to assist in claim annotation, making annotation more scalable and generalizable while rigorously following our factual claim definition.\\n\\nIn fact-checking, it is essential to have high annotation accuracy. However, LLM annotators are far from perfect (Ziems et al., 2023; Pangakis et al., 2023). Thus, to ensure the reliability of LLM annotations, AFaCTA calibrates the correctness of the annotations based on the consistency of different paths. Our evaluation shows that AFaCTA outperforms experts by a large margin when all reasoning paths achieve perfect consistency but fails to achieve expert-level performance on inconsistent samples. Nevertheless, we argue that AFaCTA can be an efficient tool in assisting factual claim annotation: perfectly consistent samples can be labeled automatically by the tool, which roughly saves 50% of expert time (see GPT-4-AFaCTA's perfect consistency rate in Table 3). However, inconsistent ones may need expert supervision.\\n\\nUsing AFaCTA, we annotate PoliClaim, a high-quality claim detection dataset covering U.S. political speeches across 25 years, spanning various political topics. We split the 2022 speeches as the test set and the 1998 to 2021 speeches as the training set to imitate the real-world use case where a model learns from the past and predicts future claims. We evaluate hundreds of classifiers trained on various data combinations, finding that AFaCTA's annotated data with perfect consistency can be a strong substitute for data annotated by human experts. In summary, our contributions include:\\n\\n1. We review the regular misconceptions and confounders in claim definition, proposing a claim definition for fact-checking focusing on verifiability.\\n2. We propose AFaCTA, an LLM-based framework that assists factual claim annotation and ensures its reliability by calibrating annotation quality with consistency along different reasoning paths.\\n3. We annotate PoliClaim, a high-quality factual claim detection dataset covering political speeches of 25 years and various topics.\\n\\n2 Claim Definition for Fact-checking\\n\\nIn this section, we first provide an overview of the discrepancies in claim definitions in prior work.\"}"}
{"id": "acl-2024-long-104", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Then, we propose our definition of a factual claim with respect to existing discrepancies.\\n\\n2.1 Discrepancies in Prior Work\\n\\nClaim conceptions:\\n\\nThe term \\\"claim detection\\\" is used not only in fact-checking but also in other areas of research, for example, argument mining (Boland et al., 2022). However, this term refers to different concepts in different research areas. In fact-checking, claim detection aims at identifying objective information in statements, which can be ruled factually wrong or correct according to evidence (Thorne et al., 2018; Arslan et al., 2020; Gangi Reddy et al., 2022), and unverifiable subjective statements are usually not considered as factual claims. In contrast, in argument mining, claim detection aims at identifying the core argument or point of view referring to what is being argued about (Habernal and Gurevych, 2017). Therefore, both objective and subjective information can be identified as claims depending on their role in the discourse (Daxenberger et al., 2017; Chakrabarty et al., 2019). The intermixing of such concepts has led to dataset misuse issues in research: for instance, Gupta et al. (2021) annotate a claim detection dataset for fact-checking COVID-19 tweets. However, the dataset is jointly trained and evaluated with claim detection datasets for argument mining (Peldszus and Stede, 2015; Stab and Gurevych, 2017, inter alia), which potentially harms the soundness of the results.\\n\\nDiscrepancies in task definitions:\\n\\nSome prior work defines factual claim detection as identifying check-worthy claims (Arslan et al., 2020; Nakov et al., 2021, 2022; Stammbach et al., 2023b) while others aim at distinguishing factual claims and non-claims (Konstantinovskiy et al., 2020; Gupta et al., 2021). Alam et al. (2021a) and Arslan et al. (2020) have both check-worthiness and claim vs non-claim labels. However, Konstantinovskiy et al. (2020) posits that the definition of check-worthiness is subjective, depending on an annotator's knowledge or political stance about a topic. For example, the statement \\\"human-induced climate change is an immediate and severe threat\\\" might be deemed self-evident by climate scientists but as check-worthy by others who are skeptical of climate models or prioritize economic growth. Some might argue that claims like this, which are subject to disagreement regarding their importance, are check-worthy due to their controversial nature. However, it requires background knowledge outside the claim itself to determine the controversy. This could involve factors such as who made the claim and why it is controversial, making the task impossible to solve at the sentence level.\\n\\nCheck-worthiness labels also suffer from another serious problem of future prediction. Training a model detecting past check-worthy claims (e.g., about COVID-19) may fail to detect check-worthiness in future claims whose sociopolitical context and controversy are unknown.\\n\\nBlurry boundaries between factual claims and non-claims:\\n\\nIn related work, personal opinions are usually defined as non-factual claims (Arslan et al., 2020; Alam et al., 2021a). However, many opinions are explicitly based on verifiable facts, lying between the definition of factual claims and non-factual claims. For example: \\\"Hydroxychloroquine cures COVID.\\\" is a verifiable factual claim. But \\\"I believe Hydroxychloroquine cures COVID.\\\" becomes a personal opinion based on a verifiable fact. Alam et al. (2021a) excludes all opinions from factual claims, which is not a good practice. A false claim can be harmful in political speeches and social media, no matter if it is enclosed by \\\"I believe\\\" or not. Gupta et al. (2021) defines 'opinions with societal implications as factual claims\", where societal implications is again an ambiguous definition.\\n\\nThe first row of Table 1 showcases the prevalent entanglement of subjective and objective information. To the best of our knowledge, no previous work in factual claim detection discusses the intersection of opinions and facts and how to delineate facts from opinions.\\n\\nContext Unavailable:\\n\\nRelated work focusing on sentence-level factual claim detection in political speech fails to discuss that sometimes sentences are not self-contained (Arslan et al., 2020; Barr\u00f3n-Cede\u00f1o et al., 2023). However, resolving the co-references is essential for semantic understanding. The last row of Table 1 shows such an example.\\n\\n2.2 Our Definition of Factual Claims\\n\\nTo avoid claim misconceptions, we always use \\\"factual claim\\\" or \\\"claim detection for fact-checking\\\" to specify our focus on fact-checking rather than argument mining. We define facts focusing on verifiability following Arslan et al. (2020) and Alam et al. (2021a):\"}"}
{"id": "acl-2024-long-104", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1: is there any verifiable information?\\n\\nStep 2: Fact-extraction Chain of Thoughts\\n\\nStep 3: Reasoning with Debate\\n\\nStep 3.1: <statement> contains some verifiable information because \u2026\\n\\nStep 3.2: <statement> contains no verifiable information because \u2026\\n\\nStatement Sources\\n\\nStep 3.3: Which argument (verifiable vs. unverifiable) do you lean towards?\\n\\nThink Step - by - Step:\\n- analyze objective/subjective parts\\n- extract factual part\\n- verifiability reasoning\\n- determine verifiability\\n- fact category\\n\\nSelf-Consistency\\n\\nDirect answer without deep thinking\\n\\nLLM Prompting\\n\\nFigure 1: AFaCTA Pipeline. All steps that need LLM prompting are annotated with the brain icon. Besides the target statement, a short context (if available) is also provided to help the model understand the statement.\\n\\nbe objectively verified as true or false based on empirical evidence or reality. To have a clear and objective task definition, we follow Konstantinovskiy et al. (2020) to focus on verifiability (factual vs. not factual claim) instead of check-worthiness (check-worthy vs. not check-worthy). Whether a sentence contains a verifiable fact or not depends only on its content (and sometimes on a little context surrounding it to clarify key statements), regardless of political or social contexts not captured by the text itself. This differs from many related works that annotate political opinions without verifiable facts as check-worthy and verifiable facts as not check-worthy. Examples of differences in checkworthiness and verifiability are showcased in rows two and three of Table 1.\\n\\nControversial political opinions and interpretations are usually considered check-worthy due to their potential societal implications. However, they are often open to debate and can hardly be verified against certain evidence. Therefore, we argue that checkworthiness and verifiability are perpendicular dimensions of factual claim detection. In this work, we focus on verifiability for the scalability of data annotation and transferability to easy-to-deploy smaller models.\\n\\nTo address the opinion-with-fact problem that is overlooked by prior work, we define opinions and factual claims as:\\n\\n**Opinion:** An opinion is a judgment based on facts, an attempt to draw a reasonable conclusion from factual evidence. While the underlying facts can be verified, the derived opinion remains subjective and is not universally verifiable.\\n\\n**Factual claim:** A factual claim is a statement that explicitly presents some verifiable facts. Statements with subjective components like opinions can also be factual claims if they explicitly present objectively verifiable facts.\\n\\n**How to define verifiability?** The verifiability of information is not trivial to define because many assertions can be interpreted either subjectively or objectively. For instance, \u201cMIT is one of the best universities in the world\u201d can be either expressing the speaker\u2019s subjective feeling about MIT, which is not verifiable, or it can be asserting a verifiable fact, which can be checked with evidence like university rankings and public survey results. For clarity, we define a statement as verifiable if it provides enough specific information to guide fact-checkers in verification. Therefore, the above MIT claim is verifiable. Generally, we observe that a statement is verifiable when it provides specific details for evidence search. For example, \u201cMIT is a good university\u201d is less verifiable than \u201cMIT is one of the best universities according to the QS ranking\u201d.\\n\\n3 AFaCTA\\n\\nThis section introduces AFaCTA for assisting factual claim annotation. AFaCTA consists of three prompting steps and an aggregation step (illustrated in Figure 1), inspired by Kahneman (2011) and our claim definitions. The prompts can be found in Appendix C.\\n\\n**Step 1: Direct Classification.** We ask LLMs to answer whether a statement contains verifiable information without any chain of thought (CoT, Wang et al., 2023). This step corresponds to a human expert\u2019s fast decision-making at first sight of a statement without deep thinking.\\n\\n**Step 2: Fact-Extraction CoT.** We instruct LLMs to conduct step-by-step reasoning over a statement: firstly, analyze the objective and subjective information covered; secondly, extract the factual part;\"}"}
{"id": "acl-2024-long-104", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Sample and Claim indicate the numbers of samples and positive samples. Supervision indicates the portion of the labels with human supervision. Split indicates if the dataset is used for training or test.\\n\\nThirdly, reason why it is verifiable or unverifiable; and finally, determine whether the factual part is verifiable. This step aims at identifying verifiable facts entangled with subjective opinions (row 1 of Table 1). The prompt and an illustrative example of this step can be found in Appendix C.3.\\n\\nStep 3: Reasoning with Debate. We note that the verifiability of many statements depends on their interpretation. Ambiguity between verifiable and unverifiable statements often arises from a lack of specificity, as shown in the examples in Appendix A. Imitating a critical thinking process, we first prompt LLMs to argue that the statement contains some (or does not contain any) verifiable information. Then we pass the debating arguments to another LLM call to judge which aspect it leans towards. To address the position bias of LLM-as-a-judge (Zheng et al., 2023), we prompt the final judging step twice, each time with the positions of the verifiable and unverifiable arguments swapped. The prompts and an illustrative example of this step can be found in Appendix C.4.\\n\\nFinal Step: Results Aggregation. We aggregate the results of three steps through majority voting. Labels from steps 1 and 2 each contribute one vote, while two position-swapped labels from step 3 contribute 0.5 votes apiece (3 votes in total). Samples with more than 1.5 votes are classified as positive samples (factual claims), and others as negative samples. See Appendix D for a discussion on tie-breaking. Ideally, if all steps have perfect consistency (0 or 3 votes), the annotation accuracy should be high.\\n\\n4 PoliClaim Dataset\\n\\nWe obtain a large political speech data from Picard and Stammbach (2022), which mainly consists of State of the State (SOTS) speeches (already cleaned and split into sentences). These speeches are governors' major public addresses of the year, thus including meaningful political topics. We randomly sample two speeches from each year, from 1998 to 2021, as training data and four speeches from 2022 as test data.\\n\\nThis design has two considerations: (1) We aim to replicate the real-world scenario where models are trained on previous claims (e.g., from 1998 to 2021) and used to predict future claims on potentially unseen topics (e.g., in 2022). (2) The test set will be used to evaluate the annotation performance of AFaCTA, and the 2022 speeches are likely unseen by June LLM check-points we use to better replicate the future-claim-detection scenario.\\n\\nThe PoliClaim test set (PoliClaim test) was annotated by two human experts, who had no access to AFaCTA's output when annotating. The experts achieved a substantial Cohen's Kappa of 0.69 in independent annotation before the discussion. Then, they had meetings to resolve disagreements and develop gold labels. Disagreements were mainly caused by ambiguous verifiability, see Appendix A for disagreement resolving. Our annotation guideline, an instantiation of our factual claim definition, can be found in Appendix B.\\n\\nTo test AFaCTA's annotation performance on different domains, we re-annotate the development set of CheckThat!-2021 (Nakov et al., 2021), which originally contained check-worthiness labels of COVID-19 tweets, following the same annotation process (Cohen's Kappa 0.58). Due to budget limitations, our explorations and annotations mainly focused on the domain of political speech. We leave the extensive study on the social media domain (and other potential domains for factual claim detection) to future work.\\n\\nAfter verifying the performance of AFaCTA using the test sets (see more in Section 5.1), we annotated the training set with the tool's assistance, imitating its expected use case of assisting annotation. The perfectly consistent samples were labeled directly with GPT-4 AFaCTA, while the inconsistent samples were left for human annotation. We randomly sampled 8 speeches and manually re-labeled the inconsistent annotations from AFaCTA, leading to PoliClaim gold where all annotations are labeled with perfect consistency or human supervision. The perfectly consistent samples in the rest...\"}"}
{"id": "acl-2024-long-104", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the speeches fall into PoliClaim silver while the inconsistent samples fall into PoliClaim bronze. The statistics of datasets can be found in Table 2.\\n\\n5 Experiments\\nSince AFaCTA is an LLM-agnostic prompting framework, we test both GPT-3.5 (Ouyang et al., 2021) and GPT-4 (OpenAI, 2023) as the backbone LLM. We also test open-sourced LLMs which does not work well due to high position bias in Step 3 (see Appendix F). Detailed settings are in Appendix G to ensure reproducibility.\\n\\n5.1 AFaCTA Annotation Performance\\nIt is unlikely for LLMs to produce expert-level annotation on all samples $S$. Therefore, AFaCTA (with LLM $M$) calibrates its performance with self-consistency, dividing $S$ into two subsets: $S^M_{con}$ with perfect consistency across all steps (0 or 3 votes) and $S^M_{inc}$ with inconsistency among some steps (0.5 to 2.5 votes). We use two criteria to compare AFaCTA with human experts: (1) Accuracy: AFaCTA's accuracy vs. experts' average accuracy, both are computed against gold labels; (2) Agreement (Cohen's Kappa): AFaCTA's average agreement to experts vs. agreement between experts. Both metrics should be compared on $S$, $S^M_{con}$, and $S^M_{inc}$ to evaluate AFaCTA's reliability on entire, perfectly consistent, and inconsistent samples. See Appendix E for formulas and implementations of all metrics.\\n\\nThe results are presented in Table 3. On the full test set $S$, even GPT-4 AFaCTA underperforms the average performance of human experts on both accuracy and agreement. However, if we only consider the subset where AFaCTA has perfect consistency ($S^M_{con}$), GPT-4 outperforms human experts by a large margin on accuracy (98.49% > 94.85%) and achieves better agreement with experts (0.833 > 0.743). On the contrary, LLMs achieve worse annotation performance than human experts on inconsistent subsets ($S^M_{inc}$). Comparable inter-human agreement is achieved on both subsets, but the accuracy and agreement on $S^M_{con}$ are higher, indicating that $S^M_{con}$ is slightly less challenging than $S^M_{inc}$.\\n\\n**Takeaway**: With AFaCTA's self-consistency calibration, auto-annotation of perfectly consistent samples can be reliably adopted to reduce manual effort (also see Section 5.5). In the case of PoliClaim test, only 51.22% needs further supervision, while 48.78% of manual effort is saved with GPT-4-AFaCTA.\\n\\n---\\n\\n**Figure 2**: Left figure: accuracy vs. self-consistency levels achieved by 11 CoT calls. Self-consistency level $x$ means there are $x$ CoTs that agree on the label and $(11-x)$ disagree. Solid and dashed lines denote the performance of LLMs and random guesses on subsets of different self-consistency correspondingly. Right figure: accuracy on the subset where all $x$ CoTs achieve agreement vs. number of sampled CoTs $x$. Note that the subset of perfect consistency is getting narrower and narrower when sampling more CoTs.\"}"}
{"id": "acl-2024-long-104", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Agreement Accuracy | Agreement Accuracy | Agreement Accuracy |\\n|--------------------|--------------------|--------------------|\\n| GPT-3.5            | GPT-4              | Experts            |\\n| 0.510              | 0.615              | 0.690              |\\n| 0.767              | 0.862              | 0.777              |\\n| 0.475              | 0.277              | 0.746              |\\n| 0.754              | 0.833              | 0.743              |\\n| 0.604              | 0.490              | 0.636              |\\n| 0.806              | 0.640              | 0.629              |\\n\\nTable 3: AFaCTA's performance on PoliClaim test. \\\"S\\\", \\\"SM con\\\", and \\\"SM inc\\\" report scores on the full test set, perfectly consistent samples, and inconsistent samples correspondingly. The percentages (%) of \\\"SM con\\\" and \\\"SM inc\\\" samples are also reported in column titles. The Experts row reports inter-human agreement and average human annotation accuracy against gold labels.\\n\\nGPT-3.5 (-4) rows report AFaCTA's average agreement to both experts, and its accuracy score against gold labels. \\\"\u2020\\\" and \\\"\u2021\\\" denote GPT-3.5 and GPT-4 reported \\\"SM con\\\"/\\\"SM inc\\\" correspondingly (i.e., $M = \\\\text{GPT-3.5} / -4$).\\n\\nWe generate 11 CoTs (more details in Appendix I) for both GPT-3.5 and GPT-4 and then compute accuracy scores for different self-consistency levels. The results are illustrated in the left figure of Figure 2. We observe that self-consistency level, to some degree, calibrates accuracy: a higher self-consistency level generally indicates higher accuracy, and vice versa. However, self-consistency CoT underperforms AFaCTA on the perfectly consistent subset (84.18% < 98.49%) while the former samples 11 CoT reasoning paths, and the latter relies on only 3 predefined reasoning paths. One possible explanation is that the predefined paths encourage critical thinking and reasoning from different angles, making the achieved self-consistency more comprehensive. We also observe that AFaCTA and self-consistency CoT achieve perfect consistency on 48.78% and 58.09% of the data, respectively, indicating that the perfect-consistency in AFaCTA is only slightly harder to achieve than in self-consistency CoT.\\n\\nFurthermore, we find that the accuracy on perfectly consistent samples grows with the number of CoT voters (see the right figure of Figure 2). This is intuitive as more consistent outputs indicate more confident predictions. However, the marginal benefit of adding more CoTs drops significantly: the accuracy of GPT-4 tends to converge to 85%. Since the accuracy of GPT-3.5 seems to grow linearly up to 11 CoTs, we further extend it to 19 CoTs and observe convergence to 84.1% (see Figure 5), which is still much lower than GPT-3.5 AFaCTA's 90.4%.\\n\\nTakeaway: Auto-annotations with more self-consistency (especially the perfectly consistent ones) tend to be more accurate. However, the source of self-consistency needs to be diversified and well-defined to scale up annotation performance efficiently. In this case, we show that predefined reasoning paths with expertise outperform those automatically sampled by LLMs.\\n\\n5.4 Domain Agnostic AFaCTA\\nThe reasoning logic of AFaCTA is not restricted to the political speech domain. To verify its performance on the social media domain, we conduct the analyses in Section 5.1 and Section 5.3 again on the CheckThat!-2021 (Nakov et al., 2021) development set. Experiment results are similar to those on PoliClaim test (see Appendix J). Therefore, AFaCTA may assist factual claim annotation in various domains.\\n\\n5.5 AFaCTA Delivers Useful Annotations\\nTo explore whether AFaCTA's annotation can replace or augment manual annotation in training classifiers, we train hundreds of classifiers with different combinations of PoliClaim gold (AFaCTA annotations + Human Supervision), PoliClaim silver (AFaCTA perfectly consistent annotations), and PoliClaim bronze (AFaCTA inconsistent annotations).\"}"}
{"id": "acl-2024-long-104", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: The performance of augmenting a limited number of PoliClaim gold data (left figure: all 1936 samples, right figure: 500 samples) with extra data from PoliClaim silver and PoliClaim bronze. Experiments of augmenting 1000 and 1500 PoliClaim gold samples can be found in Appendix M. \u201c- -\u201d denotes the performance without augmentation.\\n\\nUsing only gold, silver, or bronze data: We first gradually increase the number of training data points (by 100 per step) of the same quality. Results are shown in Figure 3. We observe the same phenomenon as previous work (Stammbach et al., 2023b) where the marginal accuracy gain drops while adding more data. The PoliClaim gold and PoliClaim silver curves roughly follow the same growing trend, approaching GPT-4\u2019s aggregated performance. This indicates that the perfectly consistent annotations (silver) from AFaCTA can strongly substitute for manually annotated data. The PoliClaim gold curve is slightly higher, showing that learning from human-supervised hard samples (inconsistent annotations of AFaCTA) is beneficial. The PoliClaim bronze curve is much lower, showing that the noisy, inconsistent annotations harm the classifier training.\\n\\nAugmenting training with auto-annotated data: When the manual annotation budget is limited, can we augment the dataset with automatic annotation? In Figure 4, we gradually augment the PoliClaim gold data with automatically annotated ones (100 per step). It can be observed that: (1) The performance increases more with PoliClaim silver data augmentation, showing that the data quality is important in data augmentation. (2) Compared to augmenting the full PoliClaim gold dataset, augmentation results in more improvement when there are only 500 PoliClaim gold data. Therefore, high-quality automatic annotation is more helpful when the manual annotation budget is limited. (3) Combining gold and silver data leads to classifiers that outperform aggregated GPT-4 reasoning, demonstrating that extending training data with LLM annotation is a promising approach to achieving better performance. One of the best RoBERTa checkpoints trained on all PoliClaim gold and PoliClaim silver is available on HuggingFace.\\n\\n4 This section presents RoBERTa (Liu et al., 2019) results. Appendix M presents similar DistilBERT (Sanh et al., 2019) results as side findings. Detailed fine-tuning settings are in Appendix K.\\n\\n6 Related Work\\n\\nClaim Detection: The term \u201cclaim detection\u201d has different definitions in various research fields (Boland et al., 2022). Even inside the field of fact-checking, its exact definition depends on the domain (Alam et al., 2021b; Stammbach et al., 2023b) or task objective (Arslan et al., 2020; Konstantinovskiy et al., 2020; Gangi Reddy et al., 2022) and is somewhat arbitrary. In this work, we propose a definition focusing on one important dimension of factual claims \u2013 verifiability, to minimize the conceptual uncertainty. Another important dimension of factual claims is check-worthiness (Arslan et al., 2020; Nakov et al., 2021, 2022; Barr\u00f3n-Cede\u00f1o et al., 2023), whose definition is more arbitrary (Konstantinovskiy et al., 2020).\\n\\nAutomatic Annotation: Automatic data annotation using LLM is both promising (Pangakis et al., 2023) and necessary (Veselovsky et al., 2023). Early work observes that LLMs\u2019 annotation performance highly depends on tasks: LLMs outperform human annotators on some tasks (Gilardi et al., 2023; Zhu et al., 2023; T\u00f6rnberg, 2023) but fails to achieve human-level performance on others (Ziems et al., 2023; Reiss, 2023). Therefore, we argue that...\"}"}
{"id": "acl-2024-long-104", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a detailed task-specific study about LLM annotation reliability is essential.\\nPangakis et al. (2023) recommend evaluating LLMs' annotation against a small subset that is not in the LLMs' training corpus and annotated by subject matter experts. We follow these suggestions in this work. Concurrent studies also explore self-consistency (Pangakis et al., 2023) and CoT (He et al., 2023) to improve the performance and reliability of LLM annotation. However, they do not compare predefined reasoning paths with automatically sampled CoTs.\\n\\n7 Discussions\\n7.1 Check-Worthiness\\nThe objective of factual claim detection is to prioritize claims that are both verifiable and check-worthy, maximizing the use of potentially limited fact-checking resources. However, in this project, we focus on verifiability without exploiting the other important aspect: checkworthiness. Constantinovskiy et al. (2020) argue that the definition of check-worthiness is subjective. However, it is possible to define a claim's checkworthiness according to its context. For example, is the claimer an influential person or media? Is the topic controversial? There has already been work that takes some contextual information (e.g., claimer, topic, etc.) into account (Gangi Reddy et al., 2022). Future work may explore deterministic and efficient ways to define and annotate checkworthiness leveraging rich contextual information.\\n\\n7.2 Only GPT-4 Is Reliable\\nWe find that only GPT-4-AFaCTA outperforms human experts on perfectly consistent samples. GPT-3.5 achieves promising results but tends to produce false negative errors. Although GPT-4 is much cheaper than human supervision, it is close-sourced and is comparatively more expensive than other LLMs. Future work may study how to use open-sourced models to produce high-quality annotations. Specifically, future work may explore (1) training the model to better understand the annotation guideline; (2) leveraging internal certainties like output logits; and (3) extending the spectrum of self-consistency levels with cheaper inference.\\n\\n8 Conclusion\\nWe propose AFaCTA, which leverages LLMs to assist in the annotation of factual claim detection. It ensures reliability by calibrating annotation quality through consistency. AFaCTA's consistent annotation proves effective for training and data augmentation even without human supervision.\\n\\nLimitations\\nAFaCTA Prompt. The design of AFaCTA prompts is inspired by the fast and slow thinking patterns (Kahneman, 2011) and prior knowledge of factual claim definition. However, we do not explore other techniques (e.g., few-shot prompting, in-context learning, and putting whole annotation guidelines in context etc.) to improve AFaCTA performance further, for two reasons: (1) the current AFaCTA's performance is good enough to show the potential of assisting claim detection annotation with LLMs; and (2) we annotated thousands of sentences with GPT-4-AFaCTA, which is very expensive. Extending the current prompts with more in-context information is not affordable for us. Besides, AFaCTA step 2 and 3 cost (approximately) 6.5x and 8.5x more tokens than step 1. Although step 2 and 3 bring self-consistency calibration and performance gain through aggregation, the marginal benefit of API cost is far from perfect.\\n\\nSocial Media and Other Domains. In this work, we only conduct extensive experiments and analyses on the political speech domain, only exploring the social media domain with a small dataset (due to the definition discrepancy, we cannot evaluate our methods with prior datasets). We believe a comprehensive study on one domain can provide deeper insights, and the conclusions might be transferable to other domains. Therefore, we do not split our budget across various domains. Future work may consider extending the large-scale analyses to other domains that need fact-checking.\\n\\nLimited Expert Annotators. We only evaluate AFaCTA's annotation performance against two experts, which may lead to potential bias. We fail to hire more expert annotators mainly because expert annotation is extremely expensive, and it is hard to find more experts with good knowledge about factual claim definitions. As compensation, we release all expert annotations and detailed error analyses where the potential bias can be analyzed. Besides, adding unsupervised LLM-annotated data continuously improves the accuracy on PoliClaim test, demonstrating that our human labeling on PoliClaim test has very limited bias.\"}"}
{"id": "acl-2024-long-104", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethics Statement\\n\\nIn this work, all human annotators are officially hired and have full knowledge of the context and utility of the collected data. We adhered strictly to ethical guidelines, respecting the dignity, rights, safety, and well-being of all participants. There are no data privacy issues or bias against certain demographics with regard to the annotated data. Both original SOTS data (Picard and Stammbach, 2022) and CheckThat!-2021 (Nakov et al., 2021) datasets are widely used for NLP and other research. Our annotated datasets will also be publicly available for research purpose.\\n\\nAcknowledgements\\n\\nThis paper has received funding from the Swiss National Science Foundation (SNSF) under the project 'How sustainable is sustainable finance? Impact evaluation and automated greenwashing detection' (Grant Agreement No. 100018_207800). It is also funded by grant from Hasler Stiftung for the Research Program Responsible AI with the project \\\"Scientific Claim Verification.\\\"\\n\\nReferences\\n\\nFiroj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, and Preslav Nakov. 2021a. Fighting the COVID-19 infodemic: Modeling the perspective of journalists, fact-checkers, social media platforms, policy makers, and the society. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 611\u2013649, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nFiroj Alam, Shaden Shaar, Fahim Dalvi, Hassan Sajjad, Alex Nikolov, Hamdy Mubarak, Giovanni Da San Martino, Ahmed Abdelali, Nadir Durrani, Kareem Darwish, Abdulaziz Al-Homaid, Wajdi Zaghouani, Tommaso Caselli, Gijs Danoe, Friso Stolk, Britt Bruntink, and Preslav Nakov. 2021b. Fighting the COVID-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 611\u2013649, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nFatma Arslan, Naeemul Hassan, Chengkai Li, and Mark Tremayne. 2020. A Benchmark Dataset of Checkworthy Factual Claims. Proceedings of the International AAAI Conference on Web and Social Media, 14:821\u2013829.\\n\\nAlberto Barr\u00f3n-Cede\u00f1o, Firoj Alam, Andrea Galassi, Giovanni Da San Martino, Preslav Nakov, Tamer Elsayed, Dilshod Azizov, Tommaso Caselli, Gullal S. Cheema, Fatima Haouari, Maram Hasanain, Mucahid Kutlu, Chengkai Li, Federico Ruggeri, Julia Maria Stru\u00df, and Wajdi Zaghouani. 2023. Overview of the clef\u20132023 checkthat! lab on checkworthiness, subjectivity, political bias, factuality, and authority of news articles and their source. In Experimental IR Meets Multilinguality, Multimodality, and Interaction: 14th International Conference of the CLEF Association, CLEF 2023, Thessaloniki, Greece, September 18\u201321, 2023, Proceedings, page 251\u2013275, Berlin, Heidelberg. Springer-Verlag.\\n\\nKatarina Boland, Pavlos Fafalios, Andon Tchechmediev, Stefan Dietze, and Konstantin Todorov. 2022. Beyond facts \u2013 a survey and conceptualisation of claims in online discourse analysis. Semantic Web, 13(5):793\u2013827.\\n\\nTuhin Chakrabarty, Christopher Hidey, and Kathy McKeeown. 2019. IMHO fine-tuning improves claim detection. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 558\u2013563, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nJohannes Daxenberger, Steffen Eger, Ivan Habernal, Christian Stab, and Iryna Gurevych. 2017. What is the essence of a claim? cross-domain claim identification. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2055\u20132066, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nRevanth Gangi Reddy, Sai Chetan Chinthakindi, Zhenhailong Wang, Yi Fung, Kathryn Conger, Ahmed ELsayed, Martha Palmer, Preslav Nakov, Eduard Hovy, Kevin Small, and Heng Ji. 2022. NewsClaims: A New Benchmark for Claim Detection from News with Attribute Knowledge. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 6002\u20136018, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nFabrizio Gilardi, Meysam Alizadeh, and Ma\u00ebl Kubli. 2023. ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks. ArXiv:2303.15056 [cs].\\n\\nShreya Gupta, Parantak Singh, Megha Sundriyal, Md. Shad Akhtar, and Tanmoy Chakraborty. 2021. LESA: Linguistic Encapsulation and Semantic Amalgamation Based Generalised Claim Detection from Online Content. In Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume, pages 3178\u20133188, Online. Association for Computational Linguistics.\\n\\nIvan Habernal and Iryna Gurevych. 2017. Argumentation mining in user-generated web discourse. Computational Linguistics, 43(1):125\u2013179.\"}"}
{"id": "acl-2024-long-104", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-104", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In political speeches and social media, not all statements are necessarily grounded with enough specific information and are undoubtedly verifiable. Many statements are a mixture of specificity and vagueness, which makes verifiability hard to define. The specificity required for verification may vary based on the topic. But generally, the more specific information a fact contains, the more verifiable it is. For example, a vague statement like \\\"Birmingham is small\\\" tends to be a not verifiable opinion since it lacks specificity (e.g., the standard of \\\"being small\\\"). In contrast, \\\"Birmingham is small in terms of population compared to London\\\" offers a clearer path for verification by comparing the population sizes of both cities. Such ambiguity in verifiability results in different expert annotations. To resolve disagreement and obtain gold labels, we have the experts debate \\\"whether a statement provides enough specific information to guide fact-checkers in verification\\\" to achieve agreement.\\n\\nIn the following list, we showcase some examples with vague verifiability. We rely on our experts' critical thinking and common sense to determine their verifiability.\\n\\nE1. \\\"I promised that our roads would be the envy of the nation.\\\"\\nAnalysis: \\\"envy of the nation\\\" seems to be an unverifiable subjective expression. However, this is a part of the speaker's pledge about improving infrastructure and can be verified by comparing the roads with those in other states.\\n\\nE2. \\\"Evil acts against innocent people in the places where we once ran errands or recreated have also made us feel less safe.\\\"\\nAnalysis: the speaker claims the existance of evil acts which seems verifiable. However, no specific details are mentioned and different people may interpret or define \\\"evil act\\\" differently. Therefore, it is hard to verify.\\n\\nE3. \\\"In my budget proposals, we will fully fund our rainy-day accounts.\\\"\\nAnalysis: the \\\"rainy-day account.\\\" seems to be an unspecific metaphor which is hard to verify. However, we know from the context that the speaker claims to fund emergency cases (i.e., rainy days). Therefore, it tends to be verifiable.\"}"}
{"id": "acl-2024-long-104", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ensuring society provides a hand up when people need help. Analysis: it seems that the speaker is pledging a helpful society. However, nothing specific is mentioned, making this claim hard to verify.\\n\\nFolks, no doubt, the last couple of years have been especially trying for our medical professionals. Analysis: at the first glance, the medical professionals' personal feeling seems subjective and not verifiable. However, as COVID is a public event, this can be verified by checking data related to the workload, stress levels, and overall conditions of medical professionals.\\n\\nAuthoritarian and illiberal impulses aren't just rising overseas, they've been echoing here at home for some time. Analysis: it claims the arising of authoritarian and illiberal impulses. However, no specific events or details are mentioned thus different people may interpret those things differently, making it hard to verify.\\n\\nWe are finally going to fix the darn roads. Analysis: \\\"darn roads\\\" is a subjective expression. However, the speaker's pledge of improving (at least some) roads is verifiable.\\n\\nI'll call this nonsense what it is, and that is an un-American, outrageous breach of our federal law. Analysis: the speaker interprets the COVID vaccination plan as \\\"an un-American, outrageous breach of federal law\\\", which seems verifiable by checking laws. However, this is a controversial issue where different people may have different interpretations of the laws. And importantly, no specific legal provisions are mentioned. Therefore, it leans towards unverifiable opinion.\\n\\nWe make all our experts' annotations publicly available. Challenging samples can be found by locating disagreements. Though we tried our best to make the annotation accurate, errors may still occur due to their challenging nature. We encourage future work to improve our definitions to resolve the existing vagueness.\"}"}
{"id": "acl-2024-long-104", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is verifiable. Otherwise, it is not verifiable. For example:\\n\\nE1. \\\"Birmingham is small.\\\" is not verifiable because it lacks any specific information for determining veracity. It leans more toward subjective opinion.\\n\\nE2. \\\"Birmingham is small, compared to London\\\" is more verifiable than E1. A fact-checker can retrieve the city size, population size, ... etc., of London and Birmingham to compare them. However, what to compare to prove Birmingham's \\\"small\\\" is not specific enough.\\n\\nE3. \\\"Birmingham is small in population size, compared to London\\\" is more verifiable than E1 and E2. A fact-checker now knows it is exactly the population size to be compared.\\n\\nWhen does an opinion explicitly present a fact?\\n\\nMany opinions are more or less based on some factual information. However, some facts are explicitly presented by the speakers, while others are not. Explicit presentation means the fact is directly entailed by the opinion without extrapolation:\\n\\nE1. \\\"The pizza is delicious.\\\" This opinion seems to be based on the fact that \\\"pizza is a kind of food\\\". However, this fact is not explicitly presented.\\n\\nE2. \\\"I first want to thank my wife of 34 years, First Lady Rose Dunleavy.\\\" The name of the speaker's wife and their year of marriage are explicitly presented.\\n\\nAlong with these guidelines, definitions in Section 2 are also presented to the annotators.\\n\\nB.2 Annotation Questions\\n\\nQ1. Does the target statement explicitly present any verifiable factual information?\\n\\n\u2022 A - Yes, the statement contains factual information with enough specific details that a fact-checker knows how to verify it. E.g., Birmingham is small in population compared to London.\\n\\n\u2022 B - Maybe, the statement seems to contain some factual information. However, there are certain ambiguities (e.g., lack of specificity) making it hard to determine the verifiability. E.g., Birmingham is small compared to London. (lack of details about what standard Birmingham is small)\\n\\n\u2022 C - No, the statement contains no verifiable factual information. Even if there is some, it is clearly unverifiable. E.g., Birmingham is small.\\n\\nIf your answer to Q1 is B - Maybe, then please answer Q2 below:\\n\\nQ2. Do you think this statement needs fact-checking of any degree? In other words, does it lean more to checkable facts or subjective opinions?\\n\\n\u2022 A - Yes, it leans more to facts that need checking.\\n\\n\u2022 B - No, it leans more toward subjective opinion and does not need a fact-check.\\n\\nSamples labeled with A and B/A are positive samples, while those with C and B/B are negative samples.\\n\\nC AFaCTA Prompts\\n\\nFollowing are the prompts of AFaCTA. In all prompts, we always include the previous and next sentence of the target statement if the context is available. \\\"{sentence}\\\", and \\\"{context}\\\" are variables to be substituted with the target sentence and its contexts correspondingly. When annotating Twitter data, we simply change \\\"political speech\\\" to \\\"Twitter\\\" and remove the specifications about contexts (see exact prompts in our code base).\\n\\nC.1 System Prompt\\n\\nYou are an AI assistant who helps fact-checkers to identify fact-like information in statements.\\n\\nC.2 Step 1: Direct Classification\\n\\nGiven the <context> of the following <sentence> from a political speech, does it contain any objective information?\\n\\n<context>: \\\"...{ context }...\\\"\\n\\n<sentence>: \\\"{ sentence }\\\"\\n\\nAnswer with Yes or No only.\\n\\nC.3 Step 2: Fact-Extraction CoT\\n\\nIn this prompt, we use the categorical definition for facts in Konstantinovskiy et al. (2020), removing the final category of \\\"other statements you think are claims\\\" to reduce uncertainty.\\n\\nStatements in political speech are usually based on facts to draw reasonable conclusions.\\n\\nCategories of fact:\\n\\nC1. Mentioning somebody (including the speaker) did or is doing something specific and objective.\\n\\nC2. Quoting quantities, statistics, and data.\\n\\nC3. Claiming a correlation or causation.\"}"}
{"id": "acl-2024-long-104", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C4. Assertion of existing laws or rules of operation.\\n\\nC5. Pledging a specific future plan or making specific predictions about future.\\n\\nPlease first analyze the objective and subjective information that the following statement (from a political speech) covers. Then extract the fact that the statement is based on. Then carefully reason about if the extracted fact is objectively verifiable. Finally answer if the fact falls into the above categories (C1 to C5) or not (C0).\\n\\nContext for statement to help you understand it better: \\\"{ context }\\\"\\n\\n{ statement }\\n\\nFormat your answer in JSON with the following keys in order:\\n\\n```json\\n{\\n  \\\"ANALYSIS\\\": \\\"What are the objective and subjective information that {statement} covers?\\\",\\n  \\\"FACT_PART\\\": \\\"The extracted fact.\\\",\\n  \\\"VERIFIABLE_REASON\\\": \\\"Detailed reason about the extracted fact's verifiability. Note that a fact lacks important details or can be interpreted differently is not objectively verifiable. Future plans/pledge (C5) that are specific and clear can be verifiable. Citing others' words is verifiable and falls into C1.\\\",\\n  \\\"VERIFIABILITY\\\": \\\"A boolean value indicates the verifiability.\\\"\\n}\\n```\\n\\nC.4 Step 3: Reasoning with Debate\\n\\nArgument for objectivity:\\nConcisely argue that the following sentence from a political speech does contain some objective information.\\n\\nContext of sentence in the speech: \\\"{ context }\\\"\\n\\n{ sentence }\\n\\nFollowing is an example argument from GPT-4 that there is some objective information in the target statement:\\n\\nTarget statement: \\\"We are so thankful that we haven't suffered any loss of life, and it's always heartening to see and hear stories of Alaskans pitching in to help each other.\\\"\\n\\nGPT-4's reply: The sentence \\\"The storms have required state disaster declarations covering almost half of our boroughs and the communities they include\\\" does contain objective information. It provides specific details about the impact of the storms, stating that they have led to state disaster declarations in nearly half of the boroughs in the state. This is a factual claim that can be verified by checking the official records of disaster declarations.\\n\\nArgument for subjectivity:\\nConcisely argue that the following sentence from a political speech does not contain any objective information.\\n\\nContext of sentence in the speech: \\\"{ context }\\\"\\n\\n{ sentence }\\n\\nFollowing is an example argument from GPT-4 that there is NO objective information in the target statement:\\n\\nTarget statement: \\\"We are so thankful that we haven't suffered any loss of life, and it's always heartening to see and hear stories of Alaskans pitching in to help each other.\\\"\\n\\nGPT-4's reply: The sentence \\\"The storms have required state disaster declarations covering almost half of our boroughs and the communities they include\\\" does not contain objective information because it lacks specific details. It does not provide the exact number or names of the boroughs affected, the specific nature of the disaster declarations, or the precise extent of the damage. Without these details, the statement remains vague and subjective.\\n\\nFinal judgment:\\nTwo AI assistants are debating about whether the following sentence (from a political speech) contains any objectively verifiable information.\\nContext of sentence in the speech: \\\"{ context }\\\"\\n\\n{ sentence }\\n\\nAssistant A's View: \\\"{ assistant_a }\\n\\nAssistant B's View: \\\"{ assistant_b }\\n\\nBased on the above, does {sentence} contain any objectively verifiable information? Which perspective do you align with more closely? Please reply with \\\"Lean towards A\\\", or \\\"Lean towards B\\\" only.\\n\\nFollowing is an example of GPT-4's judge given arguments of different perspectives:\\n\\nTarget statement: \\\"We are so thankful that we haven't suffered any loss of life, and it's always heartening to see and hear stories of Alaskans pitching in to help each other.\\\"\\n\\nGPT-4's reply: Lean towards A. Here the objective argument is assigned to assistant A.\"}"}
{"id": "acl-2024-long-104", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for two reasons: (1) tie-break does not influence AFaCTA's performance as it does not affect the perfectly consistent subset (i.e., the subset annotated with best confidence and may not need further human supervision). (2) Samples with 1.5 votes are the most confusing ones reported by AFaCTA, which require human supervision the most. They should not be dogmatically classified in practice. We only do it here to compute AFaCTA's annotation performance on the test sets.\\n\\nE Details of Evaluation Metrics\\n\\nThis section presents the formulas of metrics used in Section 5. For conciseness, only formulas on perfectly consistent samples $S_M^{con}$ are showcased. Similar formulas are applied for inconsistent samples $S_M^{inc}$ and all samples $S_M$.\\n\\nAverage accuracy of human expert on perfectly consistent samples $S_M^{con}$ is calculated as:\\n\\n$$\\\\text{Acc}_{H^{con}} = \\\\frac{1}{2} \\\\sum_{h \\\\in \\\\{h_1, h_2\\\\}} \\\\text{acc\\\\_score}(G^{con}, P_{h^{con}})$$  \\n\\nwhere $G^{con}$ and $P_{h^{con}}$ denote the gold labels and human-annotated labels of samples where AFaCTA achieves perfect self-consistency; and $h_1$ and $h_2$ denotes two human experts.\\n\\nAccuracy of AFaCTA against gold label on $S_M^{con}$ is calculated as:\\n\\n$$\\\\text{Acc}_{M^{con}} = \\\\text{acc\\\\_score}(G^{con}, P^{M^{con}})$$\\n\\nwhere $P^{M^{con}}$ denotes AFaCTA's prediction on perfectly consistent samples.\\n\\nAgreement (Cohen's Kappa) between human annotators on $S_M^{con}$ is calculated as:\\n\\n$$\\\\text{Kappa}_{H^{con}} = \\\\text{cohen\\\\_kappa}(P_{h_1^{con}}, P_{h_2^{con}})$$\\n\\nAverage Cohen's Kappa between AFaCTA and two human annotators on $S_M^{con}$ is calculated as:\\n\\n$$\\\\text{Acc}_{M^{con}} = \\\\frac{1}{2} \\\\sum_{h \\\\in \\\\{h_1, h_2\\\\}} \\\\text{cohen\\\\_kappa}(P_{h^{con}}, P^{M^{con}})$$\\n\\nWe use Sci-Kit Learn's accuracy and Cohen's Kappa implementations to calculate all metrics.\\n\\nF AFaCTA with Open-sourced LLMs\\n\\nWe tried AFaCTA framework on two popular open-sourced LLMs: Llama-2-chat-13b (Touvron et al., 2023) and zephyr-7b-beta (Tunstall et al., 2023). Results are presented in Table 4. For both models, we use the official checkpoints on huggingface and conduct greedy decoding when inference. We observe that both models suffer from heavy position bias in AFaCTA step 3: when putting arguments for verifiable and unverifiable to different positions, llama-2-chat-13b and zephyr-7b-beta predict inconsistently in 99% and 97% cases correspondingly. Therefore, there are seldom annotations with perfect consistency, and the consistency-based annotation strategy of AFaCTA does not help.\\n\\nWe also observe that zephyr-7b-beta achieves better performance than GPT-3.5 on CheckThat!2021-dev, showing the potential of using open-sourced LLMs as annotators. In future work, we will explore fine-tuning open-sourced LLMs to mitigate the position bias problem and improve annotation quality.\\n\\nG Hyperparameter Settings\\n\\nFor OpenAI models, we always use gpt-3.5-turbo-0613 and gpt-4-0613. We use a temperature of 0, and top-p of 1 for all experiments except the self-consistency CoT (Wang et al., 2023) experiments where we use a temperature of 0.7. We make all LLM generations publicly available. We always use a random seed of 42 if not specified. For open-sourced LLM inference, we use greedy sampling, a top p of 1, and a maximum generation length of 3072.\\n\\nH Performance of Each AFaCTA Step\\n\\nWe compute the annotation performance of each AFaCTA reasoning step. For Step 3, we average the scores of labels 3.1 and 3.2 (see Figure 1). The results are presented in Table 5. It can be observed that Step 1, though simple, achieves promising performance. It outperforms other steps by a wide margin with GPT-4.\\n\\nI Self-Consistency CoT\\n\\nWe use the following prompt to generate Self-consistency CoT. It keeps most of the prompt template of AFaCTA Step 1 to make them comparable. We use a temperature of 0.7 to sample different CoTs.\\n\\n```\\ngiven the <context> of the following <sentence> from a political speech, does it contain any objective information?\\n<context>: \\\"...\\n<sentence>: \\\"=\\\"\\n```\\n\\nWe use the following prompt to generate Self-consistency CoT. It keeps most of the prompt template of AFaCTA Step 1 to make them comparable. We use a temperature of 0.7 to sample different CoTs.\\n\\n```\\ngiven the <context> of the following <sentence> from a political speech, does it contain any objective information?\\n<context>: \\\"...\\n<sentence>: \\\"=\\n```\"}"}
