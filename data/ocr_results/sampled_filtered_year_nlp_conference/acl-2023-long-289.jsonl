{"id": "acl-2023-long-289", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SpongeBob SquarePants - Steppin On the Beach\\n\\nOrigin Entry: SpongeBob SquarePants (SpongeBob SquarePants is a long-running American television series created by Stephen Hillenburg, airing on Nickelodeon. Learn more on KYM.)\\n\\nSource: Reddit\\n\\nTags: spongebob, squarepants, dday, history, normandy, landings, neverforget, steppin on the beach, world war ii, france, today in history, nickelodeon, cartoon, reddit, dankmemes\\n\\nAbout the Uploader: username, Sr. Researcher & Scrapbooker & Media Chauffeur\\n\\nThe Normandy landings were the landings and associated airborne operations on Tuesday, 6 June 1944 of the Allied invasion of Normandy in Operation Overlord during World War II. Often referred to as D-Day, it was the largest seaborne invasion in history. The operation began the liberation of France and laid the foundations of the Allied victory on the Western Front.\\n\\nTable 8: Comparison of the contextual insights obtained from KYM (knowyourmeme.com, top) and the one generated by MIME (bottom) for a sample meme. Text blacked-out (X) is for obscuring the user's identity; Emboldened sentences in blue indicate ground-truth evidences and the highlighted sentences indicate model prediction.\\n\\nComparison, we consider a sample meme (c.f. Table 8) from our test set, which also happens to be available on KYM. This meme is about a soldier (portrayed via character SpongeBob) stepping onto the beach on June 6th, 1944, which is an implicit reference to the D-Day landings during World War II. We present our comparative analysis in the following subsections.\\n\\nC.1 MIME\\n\\nSince Wikipedia articles are supposed to document in-depth factual details related to events, people, places, etc., one can expect the information obtained to be exhaustive, which is what MIME aims to leverage. MIME achieves this by establishing a cross-modal evidence-level association between memes and a supplementary context document. While there are different levels of details (with varying relatedness) present within Wikipedia documents, there are one or more sentences that distinctly complement the meme's intended message. In this case, the excerpts emboldened and highlighted contribute to building the meme's rationale, as depicted in Table 8. The key advantages to using an approach like MIME can be enlisted as follows:\\n\\n\u2022 Information derived can facilitate comprehensive assimilation of the meme's intended message.\\n\u2022 MIME does not rely on manually archived details and meta-data. Instead, it presumes the availability of a related context, which can be easily mined from the web.\\n\u2022 Finally, MIME can optimally detect accurate contextual evidence about a meme without presenting information that might not be useful.\\n\\nAlthough MIME in its current stage has limitations, it would require active fine-tuning and optimization towards regulating its cross-modal associativity, towards modeling memetic contextualization.\\n\\nC.2 KYM\\n\\nOn the other hand, as can be observed from Table 8, KYM divulges the details like (a) total views, (b) time of upload, (c) origin details, (d) source, (e) relevant tags and (e) uploader details. Most of this information could be considered as meta-data, w.r.t. the meme (template). Such multimedia information captures the details related to its digital archival. The following factors characterize such information:\\n\\n\u2022 The origin information about a meme is likely to be one of the most critical information, as it presents details regarding the inception of a particular meme, which is often imperative to establish the underlying perspective conveyed within a meme.\\n\u2022 Although tags aggregate a comprehensive set of related entities, it can also include some irrelevant information.\\n\u2022 Other available meta-data like no. of views, date uploaded, etc., could be beneficial w.r.t. detecting meme's harmfulness or virality over social media, but not as much towards divulging meme's intended message.\\n\\nInformation provided by KYM may or may not be sufficient to comprehend the actual meme's intended message, as it significantly relies on human intervention towards curating such data and is therefore always bound to be limited. Still, information like the origin details and related tags can facilitate establishing the mappings across layers of abstraction that memes typically require.\"}"}
{"id": "acl-2023-long-289", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-289", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nNo response.\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nPage 7, Section 6 Experimental Results; Appendix A\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nPage 7, Section 6 Experimental Results; Appendix A\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nPage 4, Subsection 3.3 Annotation Process\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nTable 2: Prescribed guidelines for MCC's annotation.; Appendix B, Figure 5: A Screenshot of the Annotation Tool.\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nPage 4, Subsection 3.3 Annotation Process; Page 10: Ethics statement Annotation\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nPage 10, Ethics statement, Terms and Conditions for data usage.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nPage 10, Ethics statement, Data Collection.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nPage 4, Subsection 3.3 Annotation Process\"}"}
{"id": "acl-2023-long-289", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Memes are a powerful tool for communication over social media. Their affinity for evolving across politics, history, and sociocultural phenomena makes them an ideal communication vehicle. To comprehend the subtle message conveyed within a meme, one must understand the background that facilitates its holistic assimilation. Besides digital archiving of memes and their metadata by a few websites like knowyourmeme.com, currently, there is no efficient way to deduce a meme's context dynamically. In this work, we propose a novel task, \\\\textit{MEMEX} \u2013 given a meme and a related document, the aim is to mine the context that succinctly explains the background of the meme.\\n\\nAt first, we develop \\\\textit{MCC} (Meme Context Corpus), a novel dataset for \\\\textit{MEMEX}. Further, to benchmark \\\\textit{MCC}, we propose \\\\textit{MIME} (MultImodal Meme Explainer), a multimodal neural framework that uses common sense enriched meme representation and a layered approach to capture the cross-modal semantic dependencies between the meme and the context. \\\\textit{MIME} surpasses several unimodal and multimodal systems and yields an absolute improvement of $\\\\approx 4\\\\%$ F1-score over the best baseline. Lastly, we conduct detailed analyses of \\\\textit{MIME}'s performance, highlighting the aspects that could lead to optimal modeling of cross-modal contextual associations.\\n\\n\\\\section{Introduction}\\nSocial media has become a mainstream communication medium for the masses, redefining how we interact within society. The information shared on social media has diverse forms, like text, audio, and visual messages, or their combinations thereof. A meme is a typical example of such social media artifact that is usually disseminated with the flair of sarcasm or humor. While memes facilitate convenient means for propagating complex social, cultural, or political ideas via visual-linguistic semiotics, they often abstract away the contextual details that would typically be necessary for the uninformed. Such contextual knowledge is critical for human understanding and computational analysis alike. We aim to address this requirement by contemplating solutions that facilitate the automated derivation of contextual evidence towards making memes more accessible. To this end, we formulate a novel task \u2013 \\\\textit{MEMEX}, which, given a meme and a related context, aims to detect the sentences from within the context that can potentially explain the meme. Table 1 visually explains \\\\textit{MEMEX}.\\n\\nMemes often camouflage their intended meaning, suggesting \\\\textit{MEMEX}'s utility for a broader set of multimodal applications having visual-linguistic dissociation. Other use cases include context retrieval for various art forms, news images, abstract graphics for digital media marketing, etc. Table 1 primarily showcases a meme's figure (left) and an excerpt from the related context (right). This meme is about the revenge killing of an Ottoman Sultan, by the Janissaries (infantry units), in reaction to their disbanding, by the Sultan. The first line conveys the supporting evidence for the meme from the related context, emboldened and highlighted in Table 1. The aim is to model the required cross-modal association that facilitates the detection of such supporting pieces of evidence.\"}"}
{"id": "acl-2023-long-289", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The recent surge in the dissemination of memes has led to an evolving body of studies on meme analysis in which the primary focus has been on tasks, such as emotion analysis (Sharma et al., 2020), visual-semantic role labeling (Sharma et al., 2022c), detection of phenomena like sarcasm, hate-speech (Kiela et al., 2020), trolling (Hegde et al., 2021) and harmfulness (Pramanick et al., 2021; Sharma et al., 2022b). These studies indicate that off-the-shelf multi-modal models, which perform well on several traditional visual-linguistic tasks, struggle when applied to memes (Kiela et al., 2020; Baltru\u0161aitis et al., 2017; Sharma et al., 2022b). The primary reason behind this is the contextual dependency of memes for their accurate assimilation and analysis. Websites like knowyourmeme.com (KYM) facilitate important yet restricted information.\\n\\n**MEMEX** requires the model to learn the cross-modal analogies shared by the contextual evidence and the meme at various levels of information abstraction, towards detecting the crucial explanatory evidence. The critical challenge is to represent the abstraction granularity aptly. Therefore, we formulate MEMEX as an \u201cevidence detection\u201d task, which can help deduce pieces of contextual evidence that help bridge the abstraction gap. However, besides including image and text modality, there is a critical need to inject contextual signals that compensate for the constraints due to the visual-linguistic grounding offered by conventional approaches.\\n\\nEven with how effective and convenient memes are to design and disseminate over social media strategically, they are often hard to understand or are easily misinterpreted by the uninitiated, typically without the proper context. Thereby suggesting the importance of addressing a task like MEMEX. Governments or organizations involved in content moderation over social media platforms could use such a utility, underlining the convenience that such a context deduction solution would bring about in assimilating harmful memes and thereby adjudicating their social implications in emergencies like elections or a pandemic.\\n\\nMotivated by this, we first curate MCC, a new dataset that captures various memes and related contextual documents. We also systematically experiment with various multimodal solutions to address MEMEX, which culminates into a novel framework named MIME (MultImodal Meme Explanier). Our model primarily addresses the challenges posed by the knowledge gap and multimodal abstraction and delivers optimal detection of contextual evidence for a given pair of memes and related contexts. In doing so, MIME surpasses several competitive and conventional baselines.\\n\\nTo summarize, we make the following main contributions:\\n\\n- **A novel task**, MEMEX, aimed to identify explanatory evidence for memes from their related contexts.\\n- **A novel dataset**, MCC, containing 3400 memes and related context, along with gold-standard human annotated evidence sentence-subset.\\n- **A novel method**, MIME that uses common sense-enriched meme representation to identify evidence from the given context.\\n- **Empirical analysis** establishing MIME\u2019s superiority over various unimodal and multimodal baselines, adapted for the MEMEX task.\\n\\n**Related Work**\\n\\nThis section briefly discusses relevant studies on meme analysis that primarily attempt to capture a meme\u2019s affective aspects, such as hostility and emotions. Besides these, we also review other popular tasks to suitably position our work alongside different related research dimensions being explored.\\n\\n**Meme Analysis:** Several shared tasks have been organized lately, a recent one on detecting heroes, villains, and victims from memes (Sharma et al., 2022c), which was later followed-up via an external knowledge based approach in (Sharma et al., 2023) and further extended towards generating explanations in (Sharma et al., 2022a). Other similar initiatives include troll meme classification (Suryawanshi and Chakravarthi, 2021) and meme-emotion analysis via their sentiments, types and intensity prediction (Sharma et al., 2020). Notably, hateful meme detection was introduced by Kiela et al. (2020) and later followed-up by Zhou et al. (2021). Significant interest was garnered as a result of these, wherein various models were developed. A few efforts included fine-tuning Visual BERT (Li et al., 2019), and UNITER (Chen 2023).\"}"}
{"id": "acl-2023-long-289", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2020), along with using Detectron-based representations (Velioglu and Rose, 2020; Lippe et al., 2020) for hateful meme detection. On the other hand, there were systematic efforts involving unified and dual-stream encoders using Transformers (Muennighoff, 2020; Vaswani et al., 2017b), ViLBERT, VLP, UNITER (Sandulescu, 2020; Lu et al., 2019; Zhou et al., 2020; Chen et al., 2020), and LXMERT (Tan and Bansal, 2019) for dual-stream ensembling. Besides these, other tasks addressed anti-semitism (Chandra et al., 2021), propaganda techniques (Dimitrov et al., 2021), harmfulness (Pramanick et al., 2021), and harmful targets in memes (Sharma et al., 2022b).\\n\\nVisual Question Answering (VQA): Early prominent work on VQA with a framework encouraging open-ended questions and candidate answers was done by Antol et al. (2015). Since then, there have been multiple variations observed. Antol et al. (2015) classified the answers by jointly representing images and questions. Others followed by examining cross-modal interactions via attention types not restricted to co/soft/hard-attention mechanisms (Lu et al., 2016; Anderson et al., 2018; Malinowski et al., 2018), effectively learning the explicit correlations between question tokens and localised image regions. Notably, there was a series of attempts toward incorporating common-sense reasoning (Zellers et al., 2019; Wu et al., 2016, 2017; Marino et al., 2019). Many of these studies also leveraged information from external knowledge bases for addressing VQA tasks. General models like UpDn (Anderson et al., 2018) and LXMERT (Tan and Bansal, 2019) explicitly leverage non-linear transformations and Transformers for the VQA task, while others like LMH (Clark et al., 2019) and SSL (Zhu et al., 2021) addressed the critical language priors constraining the VQA performances, albeit with marginal enhancements.\\n\\nCross-modal Association: Due to an increased influx of multimodal data, the cross-modal association has recently received much attention. For cross-modal retrieval and vision-language pre-training, accurate measurement of cross-modal similarity is imperative. Traditional techniques primarily used concatenation of modalities, followed by self-attention towards learning cross-modal alignments (Wang et al., 2016). Following the object-centric approaches, Zeng et al. (2021) and Li et al. (2020) proposed a multi-grained alignment approach, which captures the relation between visual concepts of multiple objects while simultaneously aligning them with text and additional meta-data. On the other hand, several methods also learned alignments between coarse-grained features of images and texts while disregarding object detection in their approaches (Huang et al., 2020; Kim et al., 2021). Later approaches attempted diverse methodologies, including cross-modal semantic learning from visuals and contrastive loss formulations (Yuan et al., 2021; Jia et al., 2021; Radford et al., 2021).\\n\\nDespite a comprehensive coverage of cross-modal and meme-related applications in general, there are still several fine-grained aspects of memes like memetic contextualization that are yet to be studied. Here, we attempt to address one such novel task, M3EMEX.\\n\\n3. MCC: Meme Context Corpus\\n\\nDue to the scarcity of publicly-available large-scale datasets that capture memes and their contextual information, we build a new dataset, MCC (Meme Context Corpus). The overall dataset curation was conducted in three stages: (i) meme collection, (ii) content document curation, and (iii) dataset annotation. These stages are detailed in the remaining section.\\n\\n3.1 Meme Collection\\n\\nIn this work, we primarily focus on political and historical, English language memes. The reason for such a choice is the higher presence of online memes based on these topics. This is complemented by the availability of systematic and detailed information documented over well-curated digital archives. In addition to these categories, we also extend our search-space to some other themes pertaining to movies, geo-politics and entertainment as well. For scraping the meme images, we mainly leverage Google Images and Reddit, for their extensive search functionality and diverse multimedia presence.\\n\\n3.2 Context Document Curation\\n\\nWe curate contextual corpus corresponding to the memes collected in the first step. This context typically constitutes pieces of evidence for the meme's background, towards which we consider...\"}"}
{"id": "acl-2023-long-289", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotation Guidelines\\n\\n1. Meme and the associated context should be understood before annotation.\\n2. Meme's semantics must steer the annotation.\\n3. Self-contained, minimal units of information can constitute evidence.\\n4. Valid evidence may or may not occur contiguously.\\n5. Cases not supported by a contextual document should be searched on other established sources.\\n6. Ambiguous cases should be skipped.\\n\\nTable 2: Prescribed guidelines for MCC's annotation.\\n\\n(a) Context size distribution\\n(b) Evidence size distribution\\n\\nFigure 1: Distribution of # tokens (n) in MCC for: (a) related contexts (n \\\\(\\\\geq 14,349\\\\)) and (b) context evidences (n \\\\(\\\\geq 5,312\\\\)) (outliers > 125, not depicted).\\n\\nWikipedia (Wiki) as a primary source. We use a Python-based wrapper API to obtain text from Wikipedia pages. For example, for Trump, we crawl his Wiki page. For the scenarios wherein sufficient details are not available on a page, we look for fine-grained Wiki topics or related non-Wiki news articles. For several other topics, we explore community-based discussion forums and question-answering websites like Quora or other general-purpose websites.\\n\\n3.3 Annotation Process\\nTowards curating MCC, we employed two annotators, one male and the other female (both Indian origin), aged between 24 to 35 yrs, who were duly paid for their services, as per Indian standards. Moreover, both were professional lexicographers and social media savvy, well versed in the urban social media vernacular. A set of prescribed guidelines for the annotation task, as shown in Table 2, were shared with the annotators. Once the annotators were sure that they understood the meme's background, they were asked to identify the sentences in the context document that succinctly provided the background for the meme. We call these sentences \\\"evidence sentences\\\" as they facilitate (sub-)optimal evidences that constitute likely background information. The annotation quality was assessed using Cohen's Kappa, after an initial dry-run and the final annotation. The first stage divulged a moderate agreement score of 0.55, followed by several rounds of discussions, leading to a substantial agreement score of 0.72.\\n\\n3.4 Dataset Description\\nThe topic-wise distribution of the memes reflects their corresponding availability on the web. Consequently, MCC proportionately includes History (38.59%), Entertainment (15.44%), Joe Biden (12.17%), Barack Obama (9.29%), Coronavirus (7.80%), Donald Trump (6.61%), Hillary Clinton (6.33%), US Elections (1.78%), Elon Musk (1.05%) and Brexit (0.95%). Since the contextual document-size corresponding to the memes was significantly large (on average, each document consists of 250 sentences), we ensured tractability within the experimental setup by limiting the scope of the meme's related context to a subset of the entire document. Upon analyzing the token distribution for the ground-truth pieces of evidence, we observe the maximum token length of 312 (c.f. Fig. 1b for the evidence token distribution). Therefore, we set the maximum context length threshold to 512 tokens. This leads to the consideration of an average of \\\\(\\\\pi\\\\) 128 tokens and a maximum of over 350 tokens (spanning 2-3 paragraphs) within contextual documents (c.f. Fig. 1a for the context token distribution). This corresponds to a maximum of 10 sentences per contextual document.\\n\\nWe split the dataset into 80:10:10 ratio for train/validation/test sets, resulting in 3003 memes in the train set and 200 memes each in validation and test sets. Moreover, we ensure proportionate distributions among the train, val and test sets. Each sample in MCC consists of a meme image, the context document, OCR-extracted meme's text, and a set of ground truth evidence sentences.\\n\\n4 Methodology\\nIn this section, we describe our proposed model, MIME. It takes a meme (an image with overlaid text) and a related context as inputs and outputs a sequence of labels indicating whether the context's constituting evidence sentences, either in part or collectively, explain the given meme or not.\\n\\nAdditional details are included in Appendix B.\"}"}
{"id": "acl-2023-long-289", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: The architecture of our proposed model, MIME. We obtain external knowledge-enriched multimodal meme representation using Knowledge-enriched Meme Encoder (KME). We make use of a Meme-Aware Transformer (MAT) and a Meme-Aware LSTM layer (MA-LSTM) to incorporate the meme information while processing the context smoothly.\\n\\nAs depicted in Fig. 2, MIME consists of a text encoder to encode the context and a multimodal encoder to encode the meme (image and text). To address the complex abstraction requirements, we design a Knowledge-enriched Meme Encoder (KME) that augments the joint multimodal representation of the meme with external common-sense knowledge via a gating mechanism. On the other hand, we use a pre-trained BERT model to encode the sentences from the candidate context.\\n\\nWe then set up a Meme-Aware Transformer (MAT) to integrate meme-based information into the context representation for designing a multi-layered contextual-enrichment pipeline. Next, we design a Meme-Aware LSTM (MA-LSTM) that sequentially processes the context representations conditioned upon the meme-based representation. Lastly, we concatenate the last hidden context representations from MA-LSTM and the meme representation and use this jointly-contextualized meme representation for evidence detection. Below, we describe each component of MIME in detail.\\n\\nContext Representation:\\nGiven a related context, \\\\( C \\\\) consisting of sentences \\\\([c_1, c_2, ..., c_n]\\\\), we encode each sentence in \\\\( C \\\\) individually using a pre-trained BERT encoder, and the pooled output corresponding to the \\\\([CLS]\\\\) token is used as the context representation. Finally, we concatenate the individual sentence representation to get a unified context representation \\\\( H_c \\\\), with a total of \\\\( n \\\\) sentences.\\n\\nKnowledge-enriched Meme Encoder:\\nSince memes encapsulate the complex interplay of linguistic elements in a contextualized setting, it is necessary to facilitate a primary understanding of linguistic abstraction besides factual knowledge. In our scenario, the required contextual mapping is implicitly facilitated across the contents of the meme and context documents. Therefore, to supplement the feature integration with the required common-sense knowledge, we employ ConceptNet (Speer et al., 2017): a semantic network designed to help machines comprehend the meanings and semantic relations of the words and specific facts people use. Using a pre-trained GCN, trained using ConceptNet, we aim to incorporate the semantic characteristics by extracting the averaged GCN-computed representations corresponding to the meme's text. In this way, the representations obtained are common-sense-enriched and are further integrated with the rest of the proposed solution.\\n\\nTo incorporate external knowledge, we use ConceptNet (Speer et al., 2017) knowledge graph (KG) as a source of external commonsense knowledge. To take full advantage of the KG and at the same time to avoid the query computation cost, we use the last layer from a pre-trained graph convolutional network (GCN), trained over ConceptNet (Malaviya et al., 2020). We first encode meme \\\\( M \\\\) by passing the meme image \\\\( M_i \\\\) and the meme text \\\\( M_t \\\\) to an empirically extracted representation using Google Vision's OCR API.\"}"}
{"id": "acl-2023-long-289", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cally designated pre-trained MMBT model (Kiela et al., 2019), to obtain a multimodal representation of the meme \\\\(H_m^2 R_d\\\\). Next, to get the external knowledge representation, we obtain the GCN node representation corresponding to the words in the meme text \\\\(M_t\\\\). This is followed by average-pooling these embeddings to obtain the unified knowledge representation \\\\(H_k^2 R_d\\\\).\\n\\nTo learn a knowledge-enriched meme representation \\\\(\\\\hat{H}_m\\\\), we design a Gated Multimodal Fusion (GMF) block. As part of this, we employ a meme gate \\\\((g_m)\\\\) and the knowledge gate \\\\((g_k)\\\\) to modulate and fuse the corresponding representations.\\n\\n\\\\[\\ng_m = (H_m + H_k)W_m + b_m \\\\quad (1)\\ng_k = (H_m + H_k)W_k + b_k\\n\\\\]\\n\\nHere, \\\\(W_m\\\\) and \\\\(W_k\\\\) are trainable parameters.\\n\\nMeme-Aware Transformer: A conventional Transformer encoder (Vaswani et al., 2017a) uses self-attention, which facilitates the learning of the inter-token contextual semantics. However, it does not consider any additional contextual information helpful in generating the query, key, and value representations. Inspired by the context-aware self-attention proposed by Yang et al. (2019), in which the authors proposed several ways to incorporate global, deep, and deep-global contexts while computing self-attention over embedded textual tokens, we propose a meme-aware multi-headed attention (MHA). This facilitates the integration of multi-modal meme information while computing the self-attention over context representations. We call the resulting encoder a meme-aware Transformer (MAT) encoder, which is aimed at computing the cross-modal affinity for \\\\(H_c\\\\), conditioned upon the knowledge-enriched meme representation \\\\(\\\\hat{H}_m\\\\).\\n\\nConventional self-attention uses query, key, and value vectors from the same modality. In contrast, as part of meme-aware MHA, we first generate the key and the value vectors conditioned upon the meme information and then use these vectors via conventional multi-headed attention-based aggregation. We elaborate on the process below.\\n\\nGiven the context representation \\\\(H_c\\\\), we first calculate the conventional query, key, and value vectors \\\\(Q, K, V\\\\) respectively as given below:\\n\\n\\\\[\\nQKV = H_c[W_QW_KW_V] \\\\quad (2)\\n\\\\]\\n\\nHere, \\\\(n\\\\) is the maximum sequence length, \\\\(d\\\\) is the embedding dimension, and \\\\(W_Q, W_K, W_V\\\\) are learnable parameters.\\n\\nWe then generate new key and value vectors \\\\(\\\\hat{K}, \\\\hat{V}\\\\), respectively, which are conditioned on the meme representation \\\\(\\\\hat{H}_m\\\\) (broadcasted corresponding to the context size). We use a gating parameter \\\\(2^R_n\\\\times 1\\\\) to regulate the memetic and contextual interaction. Here, \\\\(U_k\\\\) and \\\\(U_v\\\\) constitute learnable parameters.\\n\\n\\\\[\\n\\\\hat{K}, \\\\hat{V} = (1 - k, v)K, V + k, v(\\\\hat{H}_mU_kU_v) \\\\quad (3)\\n\\\\]\\n\\nWe learn the parameters \\\\(k\\\\) and \\\\(v\\\\) using a sigmoid based gating mechanism instead of treating them as hyperparameters as follows:\\n\\n\\\\[\\nk, v = \\\\frac{1}{1 + e^{-W_k1W_v1 + \\\\hat{H}_mU_kU_vW_k2W_v2}} \\\\quad (4)\\n\\\\]\\n\\nHere, \\\\(W_k1, W_v1, W_k2\\\\) and \\\\(W_v2\\\\) are learnable parameters.\\n\\nFinally, we use the query vector \\\\(Q\\\\) against \\\\(\\\\hat{K}\\\\) and \\\\(\\\\hat{V}\\\\), conditioned on the meme information in a conventional scaled dot-product-based attention. This is extrapolated via multi-headed attention to materialize the Meme-Aware Transformer (MAT) encoder, which yields meme-aware context representations \\\\(H_c/m\\\\).\\n\\nMeme-Aware LSTM: Prior studies have indicated that including a recurrent neural network such as an LSTM with a Transformer encoder like BERT is advantageous. Rather than directly using a standard LSTM in MIME, we aim to incorporate the meme information into sequential recurrence-based learning. Towards this objective, we introduce Meme-Aware LSTM (MA-LSTM) in MIME. MA-LSTM is a recurrent neural network inspired by (Xu et al., 2021) that can incorporate the meme representation \\\\(\\\\hat{H}_m\\\\) while computing cells and hidden states. The gating mechanism in MA-LSTM allows it to assess how much information it needs to consider from the hidden states of the enriched context and meme representations, \\\\(H_c/m\\\\) and \\\\(\\\\hat{H}_m\\\\), respectively.\\n\\nFig. 2 shows the architecture of MA-LSTM. We elaborate on the working of the MA-LSTM cell below. It takes as input the previous cell states \\\\(c_t\\\\), previous hidden representation \\\\(h_t\\\\), current cell input \\\\(H_c\\\\), and an additional meme representation \\\\(\\\\hat{H}_m\\\\). Besides the conventional steps involved for the computation of input, forget, output and gate values w.r.t the input \\\\(H_c\\\\), the input and the gate...\"}"}
{"id": "acl-2023-long-289", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"values are also computed w.r.t the additional input \\\\( \\\\hat{H}_m \\\\). The final cell state and the hidden state outputs are obtained as follows:\\n\\n\\\\[\\n    c_t = f_t c_{t-1} + i_t \\\\hat{c}_t + p_t \\\\hat{s}_t\\n\\\\]\\n\\n\\\\[\\n    h_t = \\\\text{tanh}(c_t)\\n\\\\]\\n\\nThe hidden states from each time step are then concatenated to produce the unified context representation \\\\( \\\\hat{H}_{c/m} \\\\).\\n\\nPrediction and Training Objective: Finally, we concatenate \\\\( \\\\hat{H}_m \\\\) and \\\\( \\\\hat{H}_{c/m} \\\\) to obtain a joint context-meme representation, which we then pass through a feed-forward layer to obtain the final classification. The model outputs the likelihood of a sentence being valid evidence for a given meme. We use the cross-entropy loss to optimize our model.\\n\\n5 Baseline Models\\n\\nWe experiment with various unimodal and multimodal encoders for systematically encoding memes and context representations to establish comparative baselines. The details are presented below.\\n\\nUnimodal Baselines:\\n\\n- BERT (Devlin et al., 2019): To obtain text-based unimodal meme representation.\\n- ViT (Dosovitskiy et al., 2021): Pre-trained on ImageNet to obtain image-based unimodal meme representation.\\n\\nMultimodal Baselines:\\n\\n- Early-fusion: To obtain a concatenated multimodal meme representation, using BERT and ViT model.\\n- MMBT (Kiela et al., 2019): For leveraging projections of pre-trained image features to text tokens to encode via multimodal bi-transformer.\\n- CLIP (Radford et al., 2021): To obtain multimodal representations from memes using CLIP image and text encoders, whereas CLIP text encoder for context representation.\\n- BAN (Kim et al., 2018): To obtain a joint representation using low-rank bilinear pooling while leveraging the dependencies among two groups of input channels.\\n- VisualBERT (Li et al., 2019): To obtain multimodal pooled representations for memes, using a Transformer-based visual-linguistic model.\\n\\n6 Experimental Results\\n\\nThis section presents the results (averaged over five independent runs) on our thematically diversified test-set and performs a comparison, followed by qualitative and error analysis. For comparison, we use the following standard metrics \u2013 accuracy (Acc.), macro averaged F1, precision (Prec.), recall (Rec.), and exact match (E-M) score.\\n\\n| Model   | Acc.  | F1    | Prec. | Rec.  | E-M   |\\n|---------|-------|-------|-------|-------|-------|\\n| UM      | 0.638 | 0.764 | 0.768 | 0.798 | 0.485 |\\n| ViT     | 0.587 | 0.698 | 0.711 | 0.720 | 0.450 |\\n| MME      | 0.646 | 0.772 | 0.787 | 0.798 | 0.495 |\\n| CLIP     | 0.592 | 0.709 | 0.732 | 0.747 | 0.460 |\\n| BAN      | 0.638 | 0.752 | 0.767 | 0.772 | 0.475 |\\n| V-BERT   | 0.641 | 0.765 | 0.773 | 0.783 | 0.490 |\\n| MMBT\u2020    | 0.650 | 0.772 | 0.790 | 0.805 | 0.505 |\\n| MIME?    | 0.703 | 0.812 | 0.833 | 0.828 | 0.585 |\\n\\nThe last row shows the absolute improvement of MIME over MMBT (the best baseline). E-F: Early Fusion and V-BERT: VisualBERT.\\n\\nTo compute the scores corresponding to the partial match scenarios, we compute the precision/recall/F1 separately for each case before averaging across the test set. Additionally, as observed in (Beskow et al., 2020), we perform some basic image-editing operations like adjusting contrast, tint, temperature, shadowing, and highlight, on meme images in MCC for (i) optimal OCR extraction of meme text, and (ii) noise-resistant feature learning from images.\\n\\nMeme-evidence Detection (MEM): As part of performance analysis, we observe from Table 3 that unimodal systems, in general, perform with mediocrity, with the Bert-based model yielding a relatively better F1 score of 0.7641, as compared to the worst score of 0.6985 by ViT-based model. It can be reasoned that textual cues would be significantly pivotal in modeling association when the target modality is also text-based. On the contrary, purely image-based conditioning would not be sufficient for deriving fine-grained correlations for accurately detecting correct evidence. Also, the lower precision, as against the higher recall scores, suggests the inherent noise being additionally modeled.\\n\\nOn the other hand, multimodal models either strongly compete or outperform unimodal ones, with CLIP being an exception. With an impressive F1 score of 0.7725, MMBT fares optimally compared to the other comparative multimodal baselines. This is followed by the early-fusion-based approach and VisualBERT, with 0.7721 and 0.7658 F1 scores, respectively.\"}"}
{"id": "acl-2023-long-289", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"John Paul Jones was a Scottish-American naval captain who was the United States' first well-known naval commander in the American Revolutionary War. He made many friends among U.S political elites, as well as enemies (who accused him of piracy).\\n\\nHis actions in British waters during the Revolution earned him an international reputation which persists to this day.\\n\\n---\\n\\nTable 4: Evidence detection from MMBT (top) and MIME (bottom) for a sample meme. The emboldened sentences in blue indicate ground-truth evidences and the highlighted sentences indicate model prediction.\\n\\nNetwork) performs better than early-fusion and CLIP, but falls short by a 1-2% F1 score. Models like MMBT and VisualBERT leverage pre-trained unimodal encoders like BERT and ResNet and project a systematic joint-modeling scheme for multiple modalities. Although this has proven to be beneficial towards addressing tasks that leverage visual-linguistic grounding, especially when pre-trained using large-scaled datasets like MSCOCO (VisualBERT), their limitations can be ascertained from Table 3, wherein MIME yields absolute improvements of 5.34%, 3.97%, 4.26%, 2.31% and 8.00% in accuracy, F1 score, precision, recall, and exact match scores, respectively, over the best baseline, MMBT. This suggests potential improvement that a systematic and optimal contextualization-based approach like MIME can offer.\\n\\nAnalysing Detected Evidences:\\n\\nWe analyze the detected evidence by contrasting MIME's prediction quality with MMBT's. The meme depicted in Table 4 does not explicitly convey much information and only mentions two entities, \\\"John Paul Jones\\\" and \\\"The British Isles\\\". The MMBT baseline predicts the first sentence as an explanation, which contains the word \\\"John Paul Jones\\\", whereas MIME correctly predicts the last sentence that explains the meme. Observing the plausible multimodal analogy that might have led MIME to detect the relevant evidence in this case correctly is interesting. In general, we observe that the evidence predicted by MMBT does not fully explain the meme, whereas those predicted by MIME are often more fitting.\\n\\nAblation Study:\\n\\nMIME's key modules are Knowledge-enriched Meme Encoder (KME), Meme-Aware Transformer (MAT) encoder, and Meme-Aware LSTM (MA-LSTM). The incremental assessment of these components, over MMBT as a base model, can be observed from Table 5.\\n\\nAdding external knowledge-based cues along with the MMBT representation via KME leads to an enhancement of 0.98%-2.91% and 5% across the first four metrics and the exact match, respectively. Similar enhancements are observed with MAT and MA-LSTM, with increments of 0.91-2.25% and 0.06-2.25%, respectively. Therefore, it can be reasonably inferred that KME, MAT, and MA-LSTM distinctly contribute towards establishing the efficacy of MIME.\\n\\nOn removing MA-LSTM, we notice a distinct performance drop by 2.47% across all five metrics. Dropping MAT from MIME downgrades the performance by 1.67-5.38% for the first four metrics and by 7.5% for the exact match score. Finally, we examine the influence via replacement by employing a standard Transformer-based encoder instead of MAT and a BiLSTM layer instead of MA-LSTM in MIME. The former results in a drop of 1.45-3.28% across all five metrics. Whereas, the drop for the latter is observed to be 0.21%-2.00%. This suggests the utility of systematic memetic contextualization while addressing MIME.\\n\\nError Analysis:\\n\\nHere, we analyze different types of errors incurred by the model. As observed from the first example in Table 6, ground-truth evidence contain abstract concepts like power dynamics and morality, along with various novel facts, which induce non-triviality. On the other hand, the second example depicts a partial prediction, wherein the extra excerpt detected by the MIME is likely due to the inductive biases based on concepts of presidential race, Jimmy Carter and visual description of the peanut statue. Finally, the model just mapped...\"}"}
{"id": "acl-2023-long-289", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Heart of Darkness (1899) is a novella by Polish-English novelist Joseph Conrad. It tells the story of Charles Marlow, a sailor who takes on an assignment from a Belgian trading company as a ferry-boat captain in the African interior. The novel is widely regarded as a critique of European colonial rule in Africa, whilst also examining the themes of power dynamics and morality. Although Conrad does not name the river where the narrative takes place, at the time of writing the Congo Free State, the location of the large and economically important Congo River, was a private colony of Belgium's King Leopold II.\\n\\nThe Jimmy Carter Peanut Statue is a monument located in Plains, Georgia, United States. Built in 1976, the roadside attraction depicts a large peanut with a toothy grin, and was built to support Jimmy Carter during the 1976 United States presidential election. The statue was commissioned by the Indiana Democratic Party during the 1976 United States presidential election as a form of support for Democratic candidate Jimmy Carter's campaign through that state. The statue, a 13-foot (4.0 m) peanut, references Carter's previous career as a peanut farmer. On February 26, 1815, Napoleon managed to sneak past his guards and somehow escape from Elba, slip past in transposition by a British ship, and return to France. Immediately, people and troops began to rally to the returned Emperor. French police forces were sent to arrest him, but upon arriving in his presence, they kneeled before him. Triumphantly, Napoleon returned to Paris on March 20, 1815. Paris welcomed him with celebration, and Louis XVIII, the new king, fled to Belgium. With Louis only just gone, Napoleon moved back into the Tuileries. The period known as the Hundred Days had begun.\\n\\nTable 6: Prediction errors from MIME on three test-set samples. The emboldened sentences in blue indicate ground-truth evidences and the highlighted sentences indicate model prediction. Its prediction based on the embedded meme text, e.g., #3, while partly oblivious to the meme's visuals. Overall, MIME obtains an exact match for 58.50% of the test-set cases. At the same time, it cannot predict any explanation for 12.5% cases. The model obtains partial matches for about 14% of the cases, and for the remaining 14%, the model makes wrong predictions. Discussion: As part of this study, we examine MIME's efficacy over other variants when the constituting components are considered both incrementally and decrementally (c.f Table 5). Notably, we observe that adding external common sense knowledge-based signals, and attending over the meme while processing the context evidence sentences using MAT and MA-LSTM modules, distinctly increases the performance. These components are empirically observed and demonstrated to induce performance enhancement and establish their efficacy proving their respective hypotheses of augmenting the representation learning with common sense-based multimodal feature enrichment, self-attention-based multimodal Transformer encoding of the pieces of evidence, and finally, sequence modeling of the derived multimodal Transformer representations, modeling their temporal entailment embedded in their contextual arrangement. To further delineate the scope of this study, it does not aim to deduce/derive every possible contextual evidence that can comprehensively contextualize a given meme; instead, it is to derive the evidence pieces, given closely related raw information (which can be conveniently obtained by directed query searches), that can help provide that necessary contextual impetus towards adjudicating various memetic phenomenon (like hate, offense, etc.). The fact that such a pipeline is not constrained by a particular topic, domain, and information source makes it reasonably scalable.\"}"}
{"id": "acl-2023-long-289", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Insufficient textual cues pose challenges for MIME, for learning the required contextual associativity.\\n\\nPotentially spurious pieces of evidence being picked up due to the lexical biasing within the related context.\\n\\nEthics and Broader Impact\\n\\nReproducibility.\\n\\nWe present detailed hyperparameter configurations in Appendix A and Table 7. The source code and MCC dataset are publicly shared at https://github.com/LCS2-IIITD/MEMEX_Meme_Evidence.git.\\n\\nData Collection.\\n\\nThe data collection protocol was duly approved by an ethics review board.\\n\\nUser Privacy.\\n\\nThe information depicted/used does not include any personal information.\\n\\nTerms and Conditions for data usage.\\n\\nWe performed basic image editing (c.f. Section 6) on the meme images downloaded from the Internet and used for our research. This ensures non-usage of the artwork/content in its original form. Moreover, we already included details of the subreddits and keywords used to collect meme content and the sources used for obtaining contextual document information as part of Appendix B.1, Section 3.2 and Figure 4d. Since the our dataset (MCC) contains material collected from various web-based sources in the public domain, the copyright and privacy guidelines applied are as specified by these corresponding sources, a few of them as follows:\\n\\n- Wikipedia: Text of Creative Commons Attribution-ShareAlike 3.0.\\n- Quora: License and Permission to Use Your Content, Section 3(c).\\n- Reddit Privacy Policy: Personal information usage and protection.\\n- Reddit Content Policy.\\n\\nFuture adaptations or continuation of this work would be required to adhere to the policies prescribed herein.\\n\\nAnnotation.\\n\\nThe annotation was conducted by NLP researchers or linguists in India, who were fairly treated and duly compensated. We conducted several discussion sessions to ensure that all annotators understood the annotation requirements for MEMEX.\\n\\nBiases.\\n\\nAny biases found in the dataset are unintentional, and we do not intend to cause harm to any group or individual. We acknowledge that memes can be subjective, and thus it is inevitable that there would be biases in our gold-labeled data or the label distribution. This is addressed by working on a dataset created using a diverse set of topics and following a well-defined annotation scheme, which explicitly characterizes meme-evidence association.\\n\\nMisuse Potential.\\n\\nThe possibility of being able to deduce relevant contextual, fact-oriented evidence, might facilitate miscreants to modulate the expression of harm against a social entity, and convey the intended message within a meme in an implicit manner. This could be aimed at fooling the regulatory moderators, who could potentially be utilizing a solution like the one proposed to contextualize memes, as such intelligently designed memes might not derive suitable contextual evidence easily. As a consequence, the miscreants could end up successfully hindering the overall moderation process. Additionally, our dataset can be potentially used for ill-intended purposes, such as biased targeting of individuals/communities/organizations, etc., that may or may not be related to demographics and other information within the text. Intervention via human moderation would be required to ensure this does not occur.\\n\\nIntended Use.\\n\\nWe curated MCC solely for research purposes, in-line with the associated usage policies prescribed by various sources/platforms. This applies in its entirety to its further usage as well. We will distribute the dataset for research purposes only, without a license for commercial use. We believe that it represents a valuable resource when used appropriately.\\n\\nEnvironmental Impact.\\n\\nFinally, large-scale models require a lot of computations, which contribute to global warming (Strubell et al., 2019). However, in our case, we do not train such models from scratch; instead, we fine-tune them on a relatively small dataset.\"}"}
{"id": "acl-2023-long-289", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgments\\nThe work was supported by Wipro research grant.\\n\\nReferences\\nPeter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. 2018. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086.\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2015. VQA: Visual Question Answering. In International Conference on Computer Vision (ICCV).\\nTadas Baltru\u0161aitis, Chaitanya Ahuja, and Louis-Philippe Morency. 2017. Multimodal machine learning: A survey and taxonomy. arXiv preprint arXiv:1705.09406.\\nDavid M. Beskow, Sumeet Kumar, and Kathleen M. Carley. 2020. The evolution of political memes: Detecting and characterizing internet memes with multi-modal deep learning. Information Processing & Management, 57(2):102170.\\nMohit Chandra, Dheeraj Pailla, Himanshu Bhatia, Aadilmehdi Sanchawala, Manish Gupta, Manish Shrivastava, and Ponnurangam Kumaraguru. 2021. \u201csubverting the jewtocracy\u201d: Online antisemitism detection using multimodal deep learning. In Proceedings of the 13th ACM Web Science Conference 2021, WebSci \u201921, page 148\u2013157, New York, NY, USA. Association for Computing Machinery.\\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. 2020. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer.\\nChristopher Clark, Mark Yatskar, and Luke Zettlemoyer. 2019. Don\u2019t take the easy way out: Ensemble based methods for avoiding known dataset biases. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 4069\u20134082, Hong Kong, China. Association for Computational Linguistics.\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\nDimitar Dimitrov, Bishr Bin Ali, Shaden Shaar, Firoj Alam, Fabrizio Silvestri, Hamed Firooz, Preslav Nakov, and Giovanni Da San Martino. 2021. Detecting propaganda techniques in memes. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6603\u20136617, Online. Association for Computational Linguistics.\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.\\nMaarten Grootendorst. 2022. Bertopic: Neural topic modeling with a class-based tf-idf procedure. arXiv preprint arXiv:2203.05794.\\nSiddhanth U Hegde et al. 2021. Do images really do the talking? Analysing the significance of images in Tamil troll meme classification. arXiv:2108.03886.\\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dongmei Fu, and Jianlong Fu. 2020. Pixel-bert: Aligning image pixels with text by deep multi-modal transformers. arXiv preprint arXiv:2004.00849.\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. 2021. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR.\\nDouwe Kiela, Suvrat Bhooshan, Hamed Firooz, and Davide Testuggine. 2019. Supervised multimodal bitransformers for classifying images and text. arXiv preprint arXiv:1909.02950.\\nDouwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia, and Davide Testuggine. 2020. The hateful memes challenge: Detecting hate speech in multimodal memes. Advances in Neural Information Processing Systems, 33.\\nJin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. 2018. Bilinear attention networks. In Proceedings of the 32nd International Conference on Neural Information Processing Systems, NIPS\u201918, page 1571\u20131581, Red Hook, NY, USA. Curran Associates Inc.\\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR.\"}"}
{"id": "acl-2023-long-289", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-289", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shardul Suryawanshi and Bharathi Raja Chakravarthi. 2021. Findings of the shared task on troll meme classification in Tamil. In Proceedings of the First Workshop on Speech and Language Technologies for Dravidian Languages, pages 126\u2013132, Kyiv. Association for Computational Linguistics.\\n\\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning cross-modality encoder representations from transformers. arXiv preprint arXiv:1908.07490.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017a. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017b. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008.\\n\\nRiza Velioglu and Jewgeni Rose. 2020. Detecting hate speech in memes using multimodal deep learning approaches: Prize-winning solution to hateful memes challenge. arXiv preprint arXiv:2012.12975.\\n\\nKaiye Wang, Qiyue Yin, Wei Wang, Shu Wu, and Liang Wang. 2016. A comprehensive survey on cross-modal retrieval. CoRR, abs/1607.06215.\\n\\nQi Wu, Chunhua Shen, Peng Wang, Anthony Dick, and Anton Van Den Hengel. 2017. Image captioning and visual question answering based on attributes and external knowledge. IEEE transactions on pattern analysis and machine intelligence, 40(6):1367\u20131381.\\n\\nQi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton Van Den Hengel. 2016. Ask me anything: Free-form visual question answering based on knowledge from external sources. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4622\u20134630.\\n\\nLu Xu, Zhanming Jie, Wei Lu, and Lidong Bing. 2021. Better feature integration for named entity recognition. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3457\u20133469, Online. Association for Computational Linguistics.\\n\\nBaosong Yang, Jian Li, Derek Wong, Lidia Chao, Xing Wang, and Zhaopeng Tu. 2019. Context-aware self-attention networks. Proceedings of the AAAI Conference on Artificial Intelligence, 33:387\u2013394.\\n\\nLu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. 2021. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6720\u20136731.\\n\\nYan Zeng, Xinsong Zhang, and Hang Li. 2021. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276.\\n\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason Corso, and Jianfeng Gao. 2020. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34 (07), pages 13041\u201313049.\\n\\nYi Zhou, Zhenhao Chen, and Huiyuan Yang. 2021. Multimodal learning for hateful memes detection. In 2021 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 1\u20136. IEEE.\"}"}
{"id": "acl-2023-long-289", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Hyper-parameters and per-batch inference run-time for each model.\\n\\n| Model       | BS | EP | # Param (M) | Runtime (s) |\\n|-------------|----|----|-------------|-------------|\\n| UM          | Bert | 16 | 20          | 110 0.66    |\\n| ViT         | 86  | 0.64 |\\n| MM          | Early Fusion | 196 | 0.62 |\\n| CLIP        | 152 | 0.73 |\\n| BAN         | 200 | 0.75 |\\n| VisualBERT  | 247 | 0.78 |\\n| MMBT        | 279 | 0.99 |\\n| MIME        | 303 | 0.72 |\\n\\nA Implementation Details and Hyperparameter values\\n\\nWe train all the models using Pytorch on an NVIDIA Tesla V100 GPU with 32 GB dedicated memory, CUDA-11.2, and cuDNN-8.1.1 installed. For the unimodal models, we import all the pre-trained weights from the torchvision.models subpackage of the PyTorch framework. We randomly initialize the remaining weights using a zero-mean Gaussian distribution with a standard deviation of 0.02.\\n\\nWe primarily perform manual fine-tuning, over five independent runs, towards establishing an optimal configuration of the hyper-parameters involved. Finally, we train all models we experiment with using the Adam optimizer and a binary cross entropy loss as the objective function.\\n\\nB Additional details about MCC\\n\\nB.1 Meme Collection\\n\\nWe use carefully constructed search queries for every category to obtain relevant memes from the Google Images search engine. Towards searching variants for the topics related to Joe Biden, some search queries used were \u201cJoe Biden Political Memes\u201d, \u201cJoe Biden Sexual Allegation Memes\u201d, \u201cJoe Biden Gaffe Memes\u201d, \u201cJoe Biden Ukraine Memes\u201d, among others; for memes related to Hillary Clinton, we had \u201cHillary Clinton Email Memes\u201d, \u201cHillary Clinton Bill Clinton Memes\u201d, \u201cHillary Clinton US Election Memes\u201d, \u201cHillary Clinton President Memes\u201d, etc. For crawling and downloading these images, we use Selenium, a Python framework for web browser automation. Additionally, for certain categories, we also crawl memes off Reddit. Specifically, we focus on r/CoronavirusMemes, r/PoliticalHumor, r/PresidentialRace subreddits. Instead of using the Python Reddit API Wrapper (PRAW), we use the Pushshift API, which has no limit on the number of memes crawled. We crawl all memes for coronavirus from 1st November 2019 to 9th March 2021. For Biden, Trump, etc., we crawl memes from the other two subreddits and use a set of search queries, a subset of the overall queries we utilized. After scraping all possible memes, we perform de-duplication using dupeGuru, a cross-platform GUI tool to find duplicate files in a specified directory. This eliminates intra- and inter-category overlaps. We then remove any meme which is either unimodal, i.e., memes having only images (c.f. Fig. 3(c)), or text-only blocks (c.f. Fig. 3(a)). Additionally, to ensure further tractability of our setup, we manually filter out code-mixed (c.f. Fig. 3(b)) and code-switched memes and memes in languages other than English. Annotating multilingual memes can be a natural extension of our work. We further segregate memes that have cartoons/animations (c.f. Fig. 3(d)). We also filter out memes with poor image quality, low resolution, etc.\\n\\nB.2 Context Document Curation\\n\\nThere might be scenarios where: (a) a Wiki document about the topic being reflected in the meme might not exist, or (b) a valid topic-based Wiki document does not exist, or a valid topic-based Wiki document does not exist.\"}"}
{"id": "acl-2023-long-289", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additionally, the word cloud depicted in Fig. 4c suggests that most memes are about 'politics' or 'history,' for which memes are present. Moreover, the annotators were male, while the other was female, and their ages ranged from 24 to 35. Additionally, the annotators were both professional lexicographers, related by smaller proportions for the alternatives.\\n\\nIt can be observed from Fig. 3 that the status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5. There is a screenshot of the annotation tool. Abbr. MCC, the text document in which the explanations are returned by the OCR pipeline. The other two OCR text (the annotators can correct and edit the text boxes: the first interactive text box is for the first part of the annotation is displayed at the top. It shows a status, the meme is displayed. There are three saving, the status is updated to 'updated'. Below the shot of the platform is given in Fig. 5."}
{"id": "acl-2023-long-289", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Top-20 prominent topics representing themes of the memetic content in MCC US politicians, history, and elections. Also, context length distribution, as depicted in Fig. 4a, suggests an almost normally distributed context length (in chars), with very few contexts having lengths lesser than $\\\\pi \\\\times 100$ and more than $\\\\pi \\\\times 800$ chars. Whereas, Fig. 4b depicts evidence length distribution, according to which most pieces of evidence contain fewer than 400 characters. This corroborates the brevity of the annotated pieces of evidence from diverse contexts.\\n\\nB.5 Thematic Analysis from Meme Text\\n\\nWe perform thematic analysis of the memetic content, using just the text embedded within memes. We took the OCR extracted meme's text and project top-20 topics using BERTopic (Grootendorst, 2022), a neural topic modeling approach with a class-based TF-IDF procedure. We depict 0-based topic indexes and thematic keywords as 0\u2013History, 1\u2013Covid-19, 2\u2013Politics, 3\u2013War with Japan, etc., in Fig. 6. These topics are collectively referenced and described via the most likely keywords appearing for that particular topic. This depiction also highlights how generalizable our proposed approach is in optimally detecting accurate evidence from various topics within a given related context. Besides different high-level topics, MCC also captures the diversity of the sub-topics. Although, except for a few topics like Topics: 15 and 18, reasonably diverse memes can be found in MCC.\\n\\nC Comparing contexts from KYM and MIME\\n\\nHere, we compare the insights available on knowyourmeme.com (also referred to by KYM) and the ones generated by our proposed modeling framework MIME, about a particular meme.\"}"}
