{"id": "emnlp-2024-main-702", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here are a few examples.\\n\\nWe filter out repositories/tasks when (1) the model decides to skip (e.g. if no indication of model or dataset), or (2) the provided script selected by the model cannot be found in the repository, or is not a Python or bash file.\\n\\nB.2 Tasks Evaluation (Script-Executed metric)\\n\\nWe use a simple heuristic to determine if a script was run successfully: we check if the script was executed without any exceptions being raised (according to printed output), and if it was executed for at least $s$ seconds. We use time limit to make sure we avoid any quick failures that did not raise an exception, such as messages about missing arguments. Based on the gold expert solutions, we find that $s = 10$ is a good trade-off to distinguish unsuccessful short runs from successful ones. Importantly, this evaluation metric is an approximation, and can surely be incorrect or even manipulated by agents that are aware of it. Yet as we show in \u00a74.3, we found it to match the landmarks evaluation in 90% of the cases, hopefully making it useful for development, and as a basis for potentially creating even larger sets of tasks for development and training purposes.\\n\\nC. Prompts and Interaction History\\n\\nC.1 ReAct-SUPER\\n\\nWe use the following prompt for ReAct-SUPER.\\n\\nReAct-SUPER Prompt\\n\\nInteract in a jupyter notebook to solve the request. Output a thought and an action.\\n\\nThought can reason about the current situation, and action allows you to interact with the Jupyter notebook. There are three types of actions:\\n\\n1. execute, which executes a notebook cell with the command under the \\\"content\\\" field. This could be a bash command (beginning with !), a python command, or magic command (beginning with %).\\n\\n2. edit, which allows you to replace existing lines in a file with new lines. To replace lines, you will need to provide the lines as they appear before the replacement, and the lines to replace them with.\\n\\n3. submit, which submits the final answer (or null if there's no answer to submit), formatted as JSON (make sure to use double quotes for keys and values).\\n\\nAlways output exactly one \\\"thought\\\" and one \\\"action\\\", and encapsulate the content with <content>...</content> tags.\\n\\nI will provide the cell output (observation) for you.\\n\\nTips:\\n\\n- When a github repository is given, you should start by cloning it. You can read the readme file and browse the repo for installation and usage instructions.\\n\\n- Avoid using conda, instead use pip for package management.\\n\\n- Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n\\n  You should interrupt if: (1) you are prompted with an interactive message (e.g., \\\"Do you want to continue? [Y/n]\\\"), In that case, you can bypass it by executing echo \\\"y\\\" | command, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs.\\n\\n  You should continue to wait if the current execution is correct and proceeding as expected.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Query: Evaluate EleutherAI/gpt-j-6B on hellaswag. What's the score?\\n\\nThought: First, I need to clone the GitHub repository EleutherAI/lm-evaluation-harness.\\n\\nAction: execute\\n```\\n!git clone https://github.com/EleutherAI/lm-evaluation-harness.git\\n```\\n\\nObservation:\\n```\\nCloning into 'lm-evaluation-harness'...\\n```\\n\\nThought: The repository has been cloned. Next, I should check the README file for installation and usage instructions to proceed with evaluating EleutherAI/gpt-j-6B on hellaswag.\\n\\n\u21aa\\n\\nAction: execute\\n```\\n!cat lm-evaluation-harness/README.md\\n```\\n\\nObservation:\\n```\\n# Language Model Evaluation Harness ...\\n```\\n\\nThought: The README provides detailed instructions for installing the `lm-eval` package and evaluating models like GPT-J-6B on tasks like hellaswag. I need to install the package and its dependencies first.\\n\\n\u21aa\\n\\n\u21aa\\n\\n\u21aa\\n\\n\u21aa\\n\\nAction: execute\\n```\\n!pip install -e lm-evaluation-harness/\\n```\\n\\nObservation:\\n```\\nObtaining file:///home/nora/lm-evaluation-harness\\nInstalling build dependencies ...\\n```\\n\\nThought: The `lm-eval` package and its dependencies have been installed successfully. Now, I can proceed with evaluating the EleutherAI/gpt-j-6B model on the hellaswag task as per the instructions in the README.\\n\\n\u21aa\\n\\n\u21aa\\n\\n\u21aa\\n\\n\u21aa\\n\\nAction: execute\\n```\\n!lm_eval --model hf --model_args pretrained=EleutherAI/gpt-j-6B --tasks hellaswag --device cuda:0 --batch_size 8\\n```\\n\\nObservation:\\n```\\n{\\n  \\\"hellaswag\\\": {\\n    \\\"acc\\\": 0.24603658536585363,\\n    \\\"acc_norm\\\": 0.34436059706446426,\\n  }, ...\\n}\\n```\\n\\nThought: The `lm-eval` was executed successfully, providing the metrics I needed. The `acc` metric is 0.2460. The `acc_norm` metric is 0.3444.\\n\\n\u21aa\\n\\nAction: submit\\n```\\n{\\\"acc\\\": 0.2460, \\\"acc_norm\\\": 0.3444}\\n```\\n\\nQuery: use pandas to read the file 'data.csv' and display the first 2 rows.\\n\\nThought: First, I need to read the file 'data.csv' using the pandas library.\\n\\nAction: execute\\n```\\nimport pandas as pd\\npd.read_csv('data.csv').head()\\n```\\n\\nObservation:\\n```\\nname\\n0 Alice\\n1 Bob\\n2 Charlie\\n```\"}"}
{"id": "emnlp-2024-main-702", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thought: The file \u2018data.csv\u2019 has been read successfully and the first 2 rows have been displayed.\\n\\nAction: submit\\n\\nQuery: Change the model name in the \u2018config.json\u2019 file from \u2018roberta-base\u2019 to \u2018roberta-large\u2019.\\n\\nThought: First, I need to read the \u2018config.json\u2019 file to find the location where I can replace the model name.\\n\\n\u21aa\\n\\nAction: execute\\n\\n<content>\\n!cat config.json\\n</content>\\n\\nObservation:\\n```\\n{\\n  \\\"model\\\": \\\"roberta-base\\\",\\n  \\\"batch_size\\\": 8,\\n  \\\"epochs\\\": 10\\n}\\n```\\n\\nThought: The file has been read successfully. Now, I can replace the model name in the \u2018config.json\u2019 file.\\n\\n\u21aa\\n\\nAction: edit\\n\\n<content>\\nconfig.json\\n<<<BEFORE_EDIT>>>\\n{\\n  \\\"model\\\": \\\"roberta-base\\\",\\n  \\\"batch_size\\\": 8,\\n  \\\"epochs\\\": 10\\n<<<AFTER_EDIT>>>\\n{\\n  \\\"model\\\": \\\"roberta-large\\\",\\n  \\\"batch_size\\\": 8,\\n</content>\\n\\nObservation: \\\"editing file config.json\\\"\\n\\nThought: The model name has been replaced in the \u2018config.json\u2019 file.\\n\\nAction: submit\\n\\nThe demonstrations used in this prompt were not taken from any of the SUPER sets, and were designed specifically to show the basic usage of the actions that are available to the agent.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C.2 Reflection Agent Prompt\\n\\nWe use the following prompt to generate reflections (Reflexion agent), without any demonstrations.\\n\\nReflexion Agent Prompt\\n\\nYou will be given the history of a past experience in which you were placed in an environment and given a task to complete.\\n\\n\u21aa\\n\\nYou were unsuccessful in completing the task. Do not summarize your environment, but rather think about the strategy and path you took to attempt to complete the task.\\n\\n\u21aa\\n\\nDevise a concise, new plan of action that accounts for your mistake with reference to specific actions that you should have taken.\\n\\n\u21aa\\n\\nFor example, if you tried A and B but forgot C, then devise a plan to achieve C with environment-specific actions. If you wasted too much time on A, then devise a plan for more easily and directly achieving A.\\n\\n\u21aa\\n\\nYou will need this later when you are solving the same task. Give your plan after \\\"Plan\\\" and end it with [END].\\n\\nC.3 SWE-Agent Prompt\\n\\nWe take the original SWE-Agent prompt and adjust it to instruct the agent to complete the research task in our environment rather than fixing GitHub issues, while making sure the tips and information are as similar as possible to the ReAct-SUPER prompt for fair comparison. We include the exact same three demonstrations provided in the other agents, adjusted for SWE-Agent tools, in a similar format to the original implementation.\\n\\nSWE-Agent System Prompt\\n\\nSETTING: You are an autonomous programmer, and you're working directly in the command line with a special Jupyter notebook interface.\\n\\n\u21aa\\n\\nThe special Jupyter notebook interface consists of a file editor that shows you {WINDOW} lines of a file at a time.\\n\\n\u21aa\\n\\nYou can execute commands in the notebook using:\\n\\n1. Bash commands: Commands starting with !.\\n2. Python commands: Standard Python code.\\n3. Magic commands: Commands starting with %, e.g., %cd <path>.\\n\\nAdditionally, you can also use the following commands to help you navigate and edit files.\\n\\nCOMMANDS: {command_docs}\\n\\nPlease note that THE EDIT COMMAND REQUIRES PROPER INDENTATION. If you'd like to add the line 'print(x)' you must fully write that out, with all those spaces before the code! Indentation is important and code that is not indented correctly will fail and require fixing before it can be run.\\n\\n\u21aa\\n\\nRESPONSE FORMAT:\\n\\nYour shell prompt is formatted as follows:\\n\\n(Open file: <path>)\\n(Current directory: <cwd>)\\n\\nIn [ ] You need to format your output using two fields: discussion and command. Your output should always include _one_ discussion and _one_ command field EXACTLY as in the following example:\\n\\n ipAddress\\n\\nDISCUSSION\\n\\nFirst I'll start by using ls to see what files are in the current directory. Then maybe we can look at some relevant files to see what they look like.\\n\\n```\\n!ls -a\\n```\\n\\nYou should only include a *SINGLE* command in the command section and then wait for a response from the shell before continuing with more discussion and commands. Everything you include in the DISCUSSION section will be saved for future reference.\\n\\n\u21aa\\n\\nIf you'd like to issue two commands at once, PLEASE DO NOT DO THAT! Please instead first submit just the first command, and then after receiving a response you'll be able to issue the second command.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You're free to use any other bash commands you want (e.g. find, grep, cat, ls, cd) in addition to the special commands listed above.\\n\\nSWE-Agent Instance Prompt\\n\\nWe're currently solving the following task in the repository.\\n\\nTASK: {query}\\n\\nINSTRUCTIONS:\\n\\nNow, you're going to execute this task on your own. You can use any bash commands or the special interface commands to help you. Edit all the files you need to and run any checks or tests that you want.\\n\\nRemember, YOU CAN ONLY ENTER ONE COMMAND AT A TIME. You should always wait for feedback after every command.\\n\\nWhen you obtain the final answer for the requested TASK, you can submit it by running the `submit` command.\\n\\nNOTE ABOUT THE EDIT COMMAND: Indentation really matters! When editing a file, make sure to insert appropriate indentation before each line!\\n\\nIMPORTANT TIPS:\\n\\n1. When a github repository is given, you should start by cloning it. You can read the readme file and browse the installation and usage instructions. Then you need to set up the Python environment and install necessary packages before you run any scripts in the repo. Avoid using conda, instead use pip for package management.\\n\\n2. If you run a command and it doesn't work, try running a different command. A command that did not work once will not work the second time unless you modify it!\\n\\n3. If you open a file and need to get to an area around a specific line that is not in the first 100 lines, don't just use the scroll_down command multiple times. Instead, use the goto 583 command. It's much quicker.\\n\\n4. Always make sure to look at the currently open file and the current working directory (which appears right after the currently open file). The currently open file might be in a different directory than the working directory! Note that some commands, such as `create`, open files, so they might change the current open file.\\n\\n5. When editing files, it is easy to accidentally specify a wrong line number or to write code with incorrect indentation. Always check the code after you issue an edit to make sure that it reflects what you wanted to accomplish. If it didn't, issue another command to fix it.\\n\\n6. Each execution has a 5-minute timeout. After this period, you can decide whether to interrupt the kernel or continue waiting.\\n\\nYou should interrupt if: (1) you are prompted with an interactive message (e.g., \u201cDo you want to continue? [Y/n]\u201d). In that case, you can bypass it by executing `echo \u201cy\u201d | command`, or (2) the execution is taking longer than expected because the wrong task is being executed, such as a different model or incorrect number of epochs. You should continue to wait if the current execution is correct and proceeding as expected.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Where are the exact lines to be replaced, and are the lines to replace them with. To provide the exact file contents that should be replaced, agents typically need to view the existing contents of the file, for example by using the `cat` command, and then copy it verbatim as the lines to be replaced. Our environment then looks for the contents to be replaced in the file and replaces it with the new contents. The edit command requires the provided replaced lines to be (1) precisely copied, including correct whitespaces and indentations and (2) unique in the contents file, so that the edit command is not ambiguous. To help the agent with these requirements, we configure the edit command to provide specific feedback to the agent in case one of these conditions does not apply. Specifically, if the the lines to be replaced were not found as-is, but these lines do appear in the edited file without surrounding whitespaces or tabs, then the environment provides the following feedback:\\n\\nDid you mean to replace the following lines (notice leading/trailing whitespaces difference)?\\n\\nSolutions that accept line numbers instead of exact content performed worse in our early experiments.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 23, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-702", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"followed by an exact copy of these lines, including the spaces. If more than one instances of these lines were found, the following feedback is provided:\\n\\nFound multiple (\\\\([k]\\\\)) occurrences of the \\\\(<\\\\text{BEFORE\\\\_EDIT}>\\\\) lines. Add 1-3 lines before or after these lines to replace to disambiguate. \\n\\n\u21aa \\n\u2192 \\n\\nHere are the first two occurrences with additional context, did you mean one of these?\\n\\n\u21aa \\n\u2192 \\n\\nOccurrence 1: ...\\n\\nwith the first two occurrences of these lines.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nGiven that Large Language Models (LLMs) have made significant progress in writing code, can they now be used to autonomously reproduce results from research repositories? Such a capability would be a boon to the research community, helping researchers validate, understand, and extend prior work. To advance towards this goal, we introduce SUPER, the first benchmark designed to evaluate the capability of LLMs in setting up and executing tasks from research repositories. SUPER aims to capture the realistic challenges faced by researchers working with Machine Learning (ML) and Natural Language Processing (NLP) research repositories. Our benchmark comprises three distinct problem sets: 45 end-to-end problems with annotated expert solutions, 152 sub-problems derived from the expert set that focus on specific challenges (e.g., configuring a trainer), and 604 automatically generated problems for larger-scale development. We introduce various evaluation measures to assess both task success and progress, utilizing gold solutions when available or approximations otherwise. We show that state-of-the-art approaches struggle to solve these problems with the best model (GPT-4o) solving only 16.3% of the end-to-end set, and 46.1% of the scenarios. This illustrates the challenge of this task, and suggests that SUPER can serve as a valuable resource for the community to make and measure progress.\\n\\n1 Introduction\\nResearch and scientific discoveries often rely on the reproducibility of experiments conducted by other researchers and the ease with which scientists can build upon each other's work. In the context of empirical ML and NLP research, it is often crucial for researchers to be able to execute and reproduce diverse research experiments from open-source repositories, whether to verify existing results or to test them under new conditions.\\n\\nIn practice, even when research code is available, running code from arbitrary repositories is often non-trivial and time-consuming (Samuel and Mietchen, 2022; Storks et al., 2023). Experimentation frequently requires substantial effort to set up and execute them: installing the environment, making non-trivial configuration changes, resolving outdated package dependencies, fixing bugs, and determining the correct execution commands, among other tasks. All of this requires a considerable understanding of the documentation and repository code, knowledge about fixing issues (e.g., CUDA errors), as well as the ability to modify the\\n\\nSUPER: Evaluating Agents on Setting Up and Executing Tasks\\nfrom Research Repositories\\nBen Bogin1,2 Kejuan Yang2 ... and repository code, knowledge about fixing issues (e.g., CUDA errors), as well as the ability to modify the\"}"}
{"id": "emnlp-2024-main-702", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"code appropriately. These steps are especially time-\\nconsuming for research repositories \\\"in-the-wild\\\",\\nas support and documentation may not be available.\\n\\nIn this work, we ask: Can LLMs automate the\\nset up and execution of tasks in research reposito-\\nries?\\n\\nConsider the research task in Fig. 1 where\\nthe agent is asked to use a research code repository\\nto train a model with a new optimizer, and evaluate\\nits performance on a custom dataset. A successful\\nagent would need to set up the experiment by in-\\nstalling dependencies, downloading the provided\\ndata, and making code changes to load it (first three\\ncells in the figure), then execute the training script\\nwhile responding to unexpected issues such as an\\nincompatible dependency (fourth and fifth cell),\\nand finally report the result metrics (last cell).\\n\\nWhile LLM-based agents have recently been\\nused to produce execution commands from popular\\nresearch repositories (Liu et al., 2023b), execute\\npopular ML repositories (Huang et al., 2024), or re-\\nsolve repository issues (Jimenez et al., 2024), no ex-\\nisting benchmark evaluates agents on the common\\nproblem faced by many researchers: both\\nsetting up\\nand\\nexecuting\\nexperiments using research reposito-\\nries in-the-wild, i.e., less popular repositories that\\nare not typically well-documented or maintained,\\nwhich make experiments harder to configure and\\nexecute. As a recent study shows (Storks et al.,\\n2023), both novice and advanced researchers find\\nthe challenge of \\\"setting up the code base\\\" to be\\nthe most difficult part of reproducing experiments.\\n\\nTo encourage research on this problem, we in-\\ntroduce\\nSUPER\\n(\\nS\\netting\\nU\\np\\nand\\nE\\nxecuting\\ntasks\\nfrom\\nR\\nesearch\\nrepos-\\nitories), a benchmark focus-\\ning on lower-profile research repositories.\\n\\nSUPER consists of three distinct problem sets. The\\nExpert set contains 45 manually curated problems\\nsolved by experts. The\\nMasked\\nset includes 152\\nsub-problems derived from the expert set through\\nour proposed \\\"Code Masking\\\" mechanism, which\\nremoves parts of the expert-written code to cre-\\nate diverse sub-problems targeting challenges like\\ndependency installation, data configuration, hyper-\\nparameter setup, runtime error resolution, script\\nexecution and more. Finally, the\\nAuto\\nset contains\\n604 automatically generated tasks with an even\\nmore diverse set of repositories, offering potential\\nuse in development, fine-tuning, or training with\\nenvironment feedback.\\n\\nTo evaluate agents on the Expert and Masked\\nsets, for which we have gold solutions, we com-\\npare their answers (e.g., metrics to be reported) to\\nthe gold solutions. To allow for partial credit, we\\nalso measure the progress of the agents by check-\\ning if they reach specific 'landmark' states in their\\nsolutions, such as completing a training stage. For\\nthe automatically generated problems, for which\\nwe have no gold solutions, we simply check if a\\nkey script (e.g., the training or evaluation script)\\nwas run successfully without exceptions, which we\\nfound to be an effective approximate metric.\\n\\nWe evaluate both proprietary and open-source\\nLLMs on\\nSUPER\\nas the underlying models of strong\\nbaseline agents with access to file-editing tools. We\\nfind that agents struggle to correctly solve many\\nof the problems, with the strongest agent solving\\nonly 46.1% of the Masked sub-problems. These\\nagents are even further away from solving entire\\nresearch tasks, completing correctly only 16.3% of\\nthe end-to-end\\nExpert\\ntasks. Open-source models\\nsubstantially lag behind on both the sub-problems\\nand end-to-end tasks. Moreover, we find that the\\nranking of the agents and models on the Auto set\\nis mostly the same as it is on the curated sets, sug-\\ngest its potential usefulness for development.\\n\\nOur analysis of model trajectories reveals that\\nagents are better at resolving well-specified sub-\\nproblems, such as solving exceptions, bugs, and\\nother issues, than tasks requiring repository and file\\nexploration to understand code structure. These re-\\nsults underscore many of the core challenges facing\\nLLM-based experiment execution systems, which\\nour benchmark aims to help advance.\\n\\n2 Related Work\\nCoding benchmarks:\\nWhile early code bench-\\nmarks (Chen et al., 2021; Austin et al., 2021; Cas-\\nsano et al., 2022) mainly focused on synthesizing\\nsimple functions from descriptions, recent bench-\\nmarks have shifted to more complex competitive\\nprogramming problems (Li et al., 2022; Hendrycks\\net al., 2021; Jain et al., 2024) and evaluating pro-\\nficiency with popular data science libraries (Lai\\net al., 2023). Unlike these, we follow the recent\\ntrend on evaluating LLMs in more natural program-\\nming scenarios, such as programming with external\\ntools and APIs (Li et al., 2023; Shen et al., 2023;\\nWang et al., 2023b; Patil et al., 2023), code editing\\nand debugging (Cassano et al., 2023; Tian et al.,\\n2024; Haque et al., 2023), resolving GitHub issues\\n(Jimenez et al., 2024) and understanding and cod-\\ning within a repository context (Liu et al., 2023a;\\nDing et al., 2024; Zhang et al., 2023).\"}"}
{"id": "emnlp-2024-main-702", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In contrast to these works, SUPER focuses on the end-to-end task of setting up and executing research tasks in lower-profile repositories, presenting a unique set of challenges, with tasks that require repository comprehension and reasoning, editing multiple files, setting up the repository environment for execution while interactively running commands in the environment. Table 1 compares the four datasets most relevant to SUPER. ML-Bench (Liu et al., 2023b), specifically, its ML-Agent-Bench setup, evaluates LLMs' ability to execute tasks but focuses on popular code repositories rather than low profile, and does not evaluate based on correctness of outcome, i.e., whether resulting metrics are correct. MLAgentBench (Huang et al., 2024) evaluates agents ability to run ML experiments but focuses on optimizing single-script ML experiments rather than comprehending and setting up arbitrary repositories for experimentation.\\n\\nLLM Agents: Recent advancements in LLM-based agents have shown significant progress across various domains, including games (Wang et al., 2023a), web navigation (Yao et al., 2022; Zhou et al., 2023), human interaction simulation (Park et al., 2023), automating complex computer tasks (Xie et al., 2024), data science and machine learning (Guo et al., 2024; Hong et al., 2024; Liu et al., 2024; Yang et al., 2024b), open-ended discovery (Jansen et al., 2024), and coding (Wang et al., 2024; Yang et al., 2024a; OpenDevin Team, 2024). Our benchmark introduces an important new domain that encourages the development of LLM-based agents to assist researchers in their end to end research tasks with arbitrary repositories.\\n\\n3 Benchmark Construction\\nIn this section we describe the process of building the SUPER benchmark. The SUPER benchmark consists of 3 sets (see Table 2) serving different purposes. The Expert set (\u00a73.1) contains manually written problems, solved by experts. The Masked set (\u00a73.2) contains sub-problems extracted from the Expert set using the gold solution, which provide easier and more focused sub-problems. Fig. 2 provides a high-level overview of the construction pipeline of these two sets. Finally, the Auto set (\u00a73.3) contains automatically generated problems which can be used for development and improvement of agents.\\n\\nEnvironment Setup. Running research-oriented repositories often necessitates both being able to run system shell commands (e.g. to install dependencies and run scripts) and stateful Python commands. Previous work and environments typically support only one of these (e.g., only system shell commands (Jimenez et al., 2024) or only Python commands (Huang et al., 2024)). Instead, we build an environment that allows running both of these commands with a Jupyter notebook as engine. In this setup, each execution code is equivalent to running a notebook cell, which contains Python code and/or bash commands, and where state is reserved between cell executions (e.g., each cell can use any of the previously defined Python variables). The execution of each cell returns an observation string.\\n\\n3.1 Expert Set\\nWe construct the Expert set by (1) identifying a set of relevant code repositories from research papers and manually writing research-oriented tasks based on them and (2) asking human experts to provide end-to-end solutions for these tasks (\u00a73.1.1). We then use the expert solutions as the basis for outcome-based evaluation, where we compare the agent's answer to the gold answer, and a more lenient landmark-based evaluation that indicates...\"}"}
{"id": "emnlp-2024-main-702", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Write tasks from research repositories\\n2. Collect expert human solutions\\n3. Extract Masked Code Scenarios\\n\\nFigure 2: An overview of the construction pipeline for the Expert and Masked sets. The Expert set contains manually written tasks, along with expert solutions (Step 2). The Masked set contains problems extracted from the experts set (Step 3).\\n\\nProgress toward correctly solving the task, even if the solution is not entirely correct (\u00a73.1.2).\\n\\n3.1.1 Construction Tasks.\\n\\nWe create tasks motivated by the following two common settings: (1) reproducing numbers from research papers by running specific experiments, and (2) running modified experiments with different datasets, models, or configurations.\\n\\nWe start by collecting repositories from the \\\"Papers With Code\\\" (github.com/paperswithcode/paperswithcode-data) database, which contains research papers linked to their GitHub repositories, along with some additional metadata such as the modality of the datasets used. We only sample research papers with \\\"Text\\\" modalities and select repositories from 2021 or beyond.\\n\\nWe then manually review the sampled repositories and write tasks that involve running a single experiment that is mentioned either in the repository's \\\"readme\\\" file or under a script available in the repository, if such can be found. Whenever possible, we make the task more challenging by requiring the experiment to be run on a new dataset or model, other than the one described in the available documentation. In these cases, we select either datasets available on HuggingFace Hub (https://huggingface.co/datasets) or provide a Google Drive link where the dataset can be found. The challenge of running on a specific dataset varies in difficulty: it could involve only a single configuration line change if the dataset is already supported, or creating a new dataset reader, adjusting column names, etc.\\n\\nFor each task, we define (1) the target Github repository, (2) the task definition (e.g., \\\"train a model...\\\"), (3) the metrics or output to be reported (e.g., \\\"F1 metric on the validation set\\\"), along with a specific structure of how the answer should be formatted, and (4) implementation instructions (e.g. specific hyper-parameters). The implementation instructions are important for two reasons: first, to allow fair evaluation of submitted answers by making sure the task is not under-specified, such that two agents that correctly complete the task get the same results. Second, to minimize computational requirements, as described next.\\n\\nMinimizing Computational Requirements.\\n\\nTo make SUPER faster and cheaper to run, we ensure tasks are executable without reliance on GPU machines and that they do not require more than 10 minutes of compute (e.g., for training models or installing packages) on basic compute instances (see \u00a74 for compute details). We therefore create tasks that require minimal compute by only asking to train and evaluate small models (e.g., gpt2-small), and by adding implementation instructions to the task, such as \\\"only load the first 10 examples of the dataset\\\" or \\\"run a single epoch\\\". Note that these restrictions do not make the task any easier for the agent. In fact, they often add additional challenges that agents need to solve (e.g., configuring hyper-parameters, finding where data is loaded to limit its loading to the first 10 samples, being able to run experiments that were designed for GPU on a CPU, etc.).\\n\\nExpert Annotation.\\n\\nWe use Upwork (https://www.upwork.com/) to find and hire experts that have experience with running ML and NLP experiments. We filter the initial list of applications by testing them on a pilot task which we solved ourselves, to make sure they are able to correctly execute a task and effectively solve issues by comparing their solution and results to ours. We instruct workers to execute their solutions on Google Colab, allowing us to collect the solutions in a consistent notebook-like environment.\\n\\nWe ask the experts to submit their (1) solution notebook, (2) answers (i.e. metrics to be reported), the specific git commit hash of the repository that they have used, and the final version list of all dependencies that were installed throughout the notebook. In addition, we instruct them to use default parameters whenever possible, and to report any decision that they had to make but was not specified in the original task (e.g., the selection 2 The git hash and dependencies ensure that these solutions can be reproduced in the future even as repository and package versions change.)\"}"}
{"id": "emnlp-2024-main-702", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of a delimiter token or specific hyper-parameters when no default values are provided. We add these decisions to the task description or implementation instructions to ensure that any agent solving the same task would have all the necessary information needed to get the same results.\\n\\nFinally, we manually review their solutions, making sure that (1) the solution correctly follows the task, (2) it can be executed in our environment, (3) all unspecified decisions have been recorded and (4) re-running the experiment multiple times yields the same results (up to a $10^{-2}$ error). If needed, we ask the workers to make corrections, or manually fix issues ourselves. Solutions that we could not execute on our Jupyter environment, such as solutions that had to modify the installed Python version were discarded. We provide cost details and guidelines in Appendix D.\\n\\n3.1.2 Evaluation\\n\\nAccuracy Evaluation. As described in \u00a73.1.1, experts provide us a deterministic solution for each task, which we then execute in our environment to get the gold answer, allowing us to evaluate agents based on their outcome. Answers consist of several values (e.g., numbers for metrics, string for model predictions). We define the accuracy metric as the portion of correctly answered values: where the predicted answer precisely matches the gold one (up to a $10^{-2}$ error). Unlike reference based evaluation used in cloze tests and various prior coding benchmarks (Liu et al., 2023a,b), outcome-based evaluation allows for alternate valid solutions.\\n\\nLandmark-Based Evaluation. Sometimes an indication of whether the model was precisely correct may be too strict, \u201cpunishing\u201d models that make progress but don\u2019t reach the end. E.g., an agent that loads the data but doesn\u2019t train would have the same accuracy as an agent that fails at the start. To measure progress towards the final goal, we use the gold task notebooks to identify landmark outputs; outputs from the environments that act as \u201cevidence\u201d that a particular step was run successfully. E.g., the explicit output string \u201c*** training completed ***\u201d in Figure 1 or the string \u201cLoading data... 100%\u201d implying successful data loading.\\n\\nImportantly, a perfect landmark score does not entail a perfect accuracy score, as landmarks only indicate that some action was performed, but it was not necessarily correct (e.g., a training script run successfully but with wrong hyper-parameters could be counted as success). Similarly, albeit unlikely by design, a model could correctly solve the task but not hit all of the landmarks (e.g., if it uses an alternate approach or guesses a solution) and have a lower landmark score. For each gold solution we manually extract 2-6 landmark outputs patterns. The landmarks metric evaluates the percentage of these patterns that appear in the outputs of any of the cells executed by the agent.\\n\\n3.2 Masked Coding sub-problems Extraction\\n\\nSolving end-to-end execution tasks is often long and complex, involving multiple non-trivial steps. As such, evaluating agents on the entire task provides a sparse signal of success, where agents have to complete numerous steps correctly to succeed, making it harder to \u201chill-climb\u201d results. Instead, we want to evaluate models in a more fine-grained way that will allow us to get success signals for any incremental progress towards the task. To this end, we propose to focus on a specific sub-problem from the task solution at a time, leveraging the expert solutions from the expert set.\\n\\nWe turn to an approach loosely inspired by cloze tests (Taylor, 1953) and masked language models (MLM; Devlin et al., 2019): given a gold solution for a task we remove (mask) some part of it (e.g., code that solves a dependency installation issue), and manually define the sub-problem such that an agent only needs to solve this narrower aspect of the overall task, as captured by the removed cells. While the ideal goal is for the agents to complete tasks in an end-to-end manner, extracting masked sub-problems allows us to evaluate agents on a broad range of technical challenges often encountered when working with research repositories.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"while performing a finer-grained analysis of their performance. In addition, this setup aligns well with the usage of interactive code assistants (e.g., CoPilot and Colab\u2019s AI-powered coding), where agents assist users that have already written or even executed some code, and can specify what issue or problem remains to be solved.\\n\\nMasked Coding Sub-Problems.\\n\\nEach masked sub-problem includes (1) a textual description of the remaining tasks or issues to resolve (e.g., \u201cfix the runtime error to complete training\u201d when cells fixing the error were removed), and (2) a code pre-fix (e.g., unmasked cells from the original notebook). The code prefix is pre-executed in the environment, and agents must then write and execute their own code to solve the sub-problem.\\n\\nExtraction Procedure.\\n\\nWe manually extract masked coding sub-problems by first identifying specific cells (not necessarily consecutive) in the gold solution that focus on a particular aspect. For example, cells related to loading and modifying the dataset can be grouped into a data configuration block (shown in pink in Fig. 3). To create sub-problems, we first mask a block (e.g., the orange Issue solving block). Next, we identify cells that don\u2019t depend on the masked block and define a remaining goal (such as making the training run given the issue). These cells are pre-executed, and the agent\u2019s task is to complete the remaining goal. We use the masked block and goal cell to define the sub-problem, e.g., if the code to handle execution on CPU has been masked, the sub-problem definition would be \u201cI have already executed some of the required steps. Now, you should make the necessary changes to make sure the code runs on a CPU. Your goal is to successfully run \u2018train.py\u2019.\u201d We choose cells with clearly identifiable success indicators as goals, e.g., successfully running \u2018train.py\u2019 would produce metrics on completion.\\n\\nEvaluation.\\n\\nSince each sub-problem has a clearly defined goal, extracted from the original task, we can use the same outputs and landmarks as in the expert set, and similarly evaluate accuracy and landmarks (\u00a73.1.2). We evaluate sub-problems with the same metrics defined in \u00a73.1.2.\\n\\n3.3 Automatically Generated Tasks\\n\\nThe Expert and Masked sets provide validated problems and reproducible solutions, allowing for more accurate evaluation of agents. However, creating expert tasks is both time-consuming and costly, and the limited number of tasks hinders their use for agent improvements, such as fine-tuning models based on trajectories and environment feedback (e.g., Chen et al., 2023; Song et al., 2024; Yin et al., 2024). To address this, we automatically generate tasks using an LLM (namely, GPT-4).\\n\\n3.3.1 Construction\\n\\nGeneration involves two steps: (1) filtering suitable repositories, and (2) generating tasks based on the readmes of these repositories.\\n\\nThe first step filters repositories from \u201cpaperswithcode\u201d (\u00a73.1.1), aiming to retain those more likely suitable for task generation. Many randomly sampled repositories are unsuitable due to issues like not supporting the running of any experiments, requiring GPU hardware, or dependencies on external LLM APIs (e.g., OpenAI or Anthropic). To avoid such cases, we apply a combination of heuristic and LLM-based filtering. Details on the filtering process and LLM prompts are provided in Appendix B.\\n\\nFor the filtered repositories, we prompt the LLM with each repository\u2019s README and instruct it to generate an experiment-running task. This includes specifying the goal, dataset, model, and script (e.g., \u201cRun probability-based prompt selection on the SST-2 dataset using opt-125m with run_prompt_selection.py\u201d). Further details on this process are provided in Appendix B. To assess quality, we sampled 100 tasks, finding 81% to be feasible. The main issues among the remainder were missing resources (e.g., dead links, missing code) or conceptually flawed tasks, such as a task asking to use a discriminative model for a generative task.\\n\\nDifference from Expert Set.\\n\\nThe Auto tasks differ from those in the Expert set in several key ways: Expert tasks may require training or inference on datasets not explicitly supported, sometimes with different formats and columns, whereas Auto tasks prioritize installing dependencies and starting experiments. Additionally, the Expert set includes only tasks that were successfully run in our Jupyter environment, while Auto tasks may involve more complex setups, such as requiring changes to the Python version. Finally, Auto tasks sometimes require navigating web pages mentioned in the repository\u2019s README to download datasets.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3.2 Evaluation\\n\\nWithout expert solutions, we cannot evaluate based on outcomes or landmarks. Instead, we use a simple heuristic metric, termed *Script-Executed*, to ensure the model sets up and executes the experiment without unresolved issues: we check if the script the agent was asked to run executes without exceptions for a minimum duration (see Appendix B.2 for details). The minimum duration ensures that the script was successful and didn't just fail silently.\\n\\nWhile this method does not guarantee perfect evaluation, we find it surprisingly effective, as we show in our analysis in \u00a74.3.\\n\\n3.4 SUPER Benchmark\\n\\nThe Expert set consists of 45 collected tasks, where each problem is paired with a gold output for outcome-based evaluation and a small set of landmarks for our softer evaluation metric (an average of 3 landmarks per problem).\\n\\nTo estimate the lines of code (LOC) required for a task, we count the non-comment lines in the gold solution and for editing cells, the number of changed lines. On average, each solution involved 44.3 LOC and 14.4 cells, highlighting the difficulty of these tasks due to potentially long agent trajectories. Consequently, the ability to solve these tasks provides an important signal on the performance of agents in handling long and complex tasks.\\n\\nThe Masked set contains 152 masked coding sub-problems derived from the 45 expert tasks. Like the Expert set, each sub-problem is paired with a gold output and landmarks. Table 3 shows the distribution of the extracted sub-problems across various categories, along with a representative solution for each category and the average lines of code (LOC) that were executed in the gold solution. Finally, the gold LOC may be significantly lower than what agents require, as experts can use their IDE or browser to read files, while agents must browse files via explicit commands. Additionally, experts may omit cells with failed attempts.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1 Baselines\\n\\nIn this section, we describe the three baseline agents that we evaluate on SUPER. ReAct (Yao et al., 2023) is a baseline agent that iteratively prompts the LLM to generate both an action and a natural language \u201cthought,\u201d using the interaction history as context. At each step, the action (a bash or Python command) is executed in the environment, and a \\\\text{<thought, action, observation>} tuple is added to the history. This process continues until the agent submits an answer or reaches token or compute limits.\\n\\nA challenge in running experiments is that output observations can be extremely long (e.g., training scripts or dependency installations producing 10k-40k tokens). Since ReAct agents accumulate history at each step, token usage increases rapidly. Given that agents operate within a fixed token or cost budget, this can lead to task failures. We thus apply truncation strategies for all our agents to mitigate this issue. See Appendix C for details.\\n\\nReAct-SUPER. The ability to execute Python and bash commands, in theory, allows agents to perform any necessary task. However, these actions are still limited compared to humans who can use IDEs to browse and edit files. In our early experiments, we indeed found that agents struggle to edit files (e.g., change configs) using just bash. To address this challenge, we introduce an additional edit action, similar in spirit to the Agent-Computer Interfaces from SWE-Agent (Yang et al., 2024a). The edit command takes three parameters: the file name, the content to be replaced, and the new content. If the exact content isn\u2019t found (e.g., due to missing whitespaces), the tool provides fixing suggestions. See App. E for more details.\\n\\nSWE-Agent (Yang et al., 2024a) is a ReAct-based agent, originally designed to solve GitHub issues. Like ReAct-SUPER, it provides agents with tools that allow editing of files, but also tools for reading files, scrolling through their contents and more (see original paper for details). We implement SWE-Agent in our environment and adapt its prompts to handle the execution of research tasks.\\n\\nReflecting Agents. To explore whether agents can improve by reflecting on failures, we evaluate Agent Model Acc. Landm. SWE-Agent GPT-4o 16.3 \u00b1 2.1 36.8 \u00b1 2.3\\nReact GPT-4o 12.2 \u00b1 1.0 33.6 \u00b1 0.9\\nReact-Super GPT-4o 14.4 \u00b1 2.2 42.6 \u00b1 2.9\\nSWE-Agent GPT-4o mini 3.3 16.1\\nReact-Super GPT-4o mini 5.6 20.6\\nSWE-Agent Llama 3.1 70B 5.6 4.8\\nReact-Super Llama 3.1 70B 6.1 9.6\\nSWE-Agent Mixtral 8x22B 1.1 0.0\\nReact-Super Mixtral 3.3 3.7\\n\\nTable 4: Results on Expert, with GPT-4o numbers averaged across 3 seeds.\\n\\n4.2 Results\\n\\nExpert Set. Table 4 presents results for the Expert set, averaged across three seeds for the most performant LLM (decoding temperature is 0.2). The low accuracies (12.2-16.3) suggest that current agents cannot yet perform this task well. However, the higher landmarks metric shows that agents often make partial progress, suggesting they could still assist in setting up repositories.\\n\\nMasked Set. Table 6 presents results on the Masked set, demonstrating that SWE-agent correctly solves a significant portion (46.1%) of the challenges, but that most sub-problems are still unsolved. The higher landmarks evaluation score (74.9%) suggests that agents often make progress towards solving the sub-problems, even if some of the steps might not necessarily be correct.\\n\\nWe find that SWE-agent performs better than ReAct-SUPER with GPT-4o as the LLM, but slightly worse with all weaker models, suggesting that weaker models were less effectively in leveraging SWE-Agent tools. The open-source Mixtral and Llama reach significantly lower scores on both the Masked and Expert sets.\\n\\nAuto Set. We show in Table 5 results for the Auto tasks, where ranking of models and agents are mostly consistent with the ranking of the models on the Masked set, suggesting potential usefulness of this set for future development.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results on 250 of the Auto tasks.\\n\\n| Agent Model  | Landmark | Accuracy |\\n|--------------|----------|----------|\\n| SWE-Agent GPT-4o | 46.1     | 74.9     |\\n| React GPT-4o   | 37.0     | 65.7     |\\n| React-Super GPT-4o | 41.6   | 72.5     |\\n| SWE-Agent GPT-4o mini | 27.0   | 51.8     |\\n| React-Super GPT-4o mini | 31.5   | 58.3     |\\n| SWE-Agent Llama 3.1 70B | 9.5    | 26.6     |\\n| React-Super Llama 3.1 70B | 7.0    | 13.2     |\\n\\nTable 6: Results of our baselines on SUPER (Masked) with different underlying LLMs.\\n\\nAblations.\\n\\nComparing React with ReAct-SUPER shows that the editing function enables the agent to hit more landmarks (72.5% vs 65.7%) and produce more accurate answers (41.6% vs 37.0%). We find that without the editing command, the agent usually resorts to editing files with the sed command, which is designed for simple single-line edits.\\n\\nCan agents that reflect do better? We next evaluate if retrying after reflecting on failures can improve the performance of our baseline agents, with k = 3 retries. As shown in Table 7, the additional retries with reflections have a positive but minimal impact on the score. If models lack the inherent ability to resolve some of these issues, retrial with reflections are not likely to help.\\n\\n4.3 Error Analysis\\n\\nThe Masked set categorizes each problem, allowing us to break down performance of agents (Table 3). We find that the hardest categories for the agent are data (27%), configuration (38%) and goal (43% accuracy), whereas CPU, issues and dependencies are easier (73%, 61% and 54% respectively). These findings suggest that agents are better at solving sub-problems where there is a specific error message to be solved (such as CPU support errors, incompatible dependencies, or exceptions) than more open-ended problems such as configuring data loading for a custom dataset.\\n\\nSpecifically, in the latter case, we find that agents often skip reviewing repository code, tending to hallucinate script or function arguments (e.g., adding n_examples=10 when no such argument exists) or missing available script parameters and attempting to modify them unsuccessfully in the code. Moreover, once agents commit to an approach, they rarely reconsider it until failure.\\n\\nThese issues suggest that agents should be better designed to analyze repositories and consider multiple solution approaches. We provide all trajectories of ReAct-SUPER and SWE-Agent in our code repository.\\n\\nEffectiveness of Proxy Metric.\\n\\nWhile the Expert and Masked sets are evaluated using expert solutions, the Auto set lacks solutions, so we rely on the weaker Script-Executed proxy metric (\u00a73.3.2). To validate its reliability, we compared Script-Executed with the accuracy and landmark metrics using ReAct-SUPER trajectories on the Masked set. Script-Executed agreed with landmark (assuming a score of 1 when landmark > 0.5) in 90% of cases and with accuracy in 69%. We found two key cases of disagreement with the landmark metric: (1) when the script ran correctly to get the answer but encountered an exception afterward (e.g., while creating a figure), and (2) when the script ran just long enough to appear successful by the proxy metric but later failed due to misconfiguration or exceptions without reaching the answer.\\n\\n5 Conclusion\\n\\nWe introduce SUPER, a benchmark for evaluating LLM-based agents on tasks from low-profile research repositories. Our empirical results demonstrate the difficulty of the benchmark, even for leading LLMs like GPT-4o. SUPER highlights key challenges in building autonomous execution agents, such as repository reasoning and code editing, and we hope it will drive measurable progress in tackling this important problem.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nDataset Size. The dataset size of our benchmark, comprising 45 and 152 sub-problems, is smaller compared to some other benchmarks available for agent evaluation, which could potentially affect the statistical significance of performance evaluations. However, the use of smaller, high-quality benchmarks is not uncommon. For instance, benchmarks such as HUMAN (Chen et al., 2021), CLASS (Du et al., 2023), and BAMBOOGLE (Press et al., 2023) contain 164, 100, and 125 examples respectively, and are widely used for assessing model performance. In addition, recent work has suggested that reducing large datasets to as few as 100 examples does not diminish their effectiveness (Maia Polo et al., 2024). Moreover, smaller-sized datasets offer the advantage of being less expensive to operate, thus providing better accessibility for researchers with limited resources, particularly when running interactive agents in environments that generate long outputs. Finally, our provided Auto set with 604 problems offers problems proposed for development, which alleviates the risk of overfitting to the evaluation sets.\\n\\nProgramming Languages and Domains. We have only collected solutions written in Python, and our environment only supports that programming language. We focus mostly on text-based repositories. While the challenges associated with running these repositories likely overlap with other domains, increasing the diversity of the repository domains could be beneficial.\\n\\nEvaluation Based on External Resources. Running benchmarks in realistic environments often depend on external resources. In our case, agents rely on availability of resources such as GitHub, pip and datasets, which we cannot control across runs. While completely sand-boxed setups could have allowed for a more controlled evaluation, we opt for fidelity, similarly to e.g. benchmarks for web agents that rely on access to real websites (Mialon et al., 2024; He et al., 2024, inter alia).\\n\\nEthical Considerations\\n\\nWhile autonomous research execution agents could significantly enhance research advancements, there is a risk of over-reliance on these agents, which could lead to conclusions drawn based on incorrect implementations of agents, and careless actors not checking the agent\u2019s reproduction work carefully.\\n\\nAcknowledgments\\n\\nWe thank Ori Yoran for his valuable comments and suggestions, and the anonymous reviewers for their helpful feedback. We also thank the Upworker expert coders for their work on the solutions to the Expert set problems.\\n\\nReferences\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models. arXiv preprint arXiv:2108.07732.\\n\\nFederico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q Feldman, et al. 2022. MultiPL-E: A scalable and extensible approach to benchmarking neural code generation. arXiv preprint arXiv:2208.08227.\\n\\nFederico Cassano, Luisa Li, Akul Sethi, Noah Shinn, Abby Brennan-Jones, Anton Lozhkov, Carolyn Anderson, and Arjun Guha. 2023. Can it edit? evaluating the ability of large language models to follow code editing instructions. arXiv preprint arXiv:2312.12450.\\n\\nBaian Chen, Chang Shu, Ehsan Shareghi, Nigel Collier, Karthik Narasimhan, and Shunyu Yao. 2023. FireAct: Toward language agent fine-tuning. ArXiv, abs/2310.05915.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nYangruibo Ding, Zijian Wang, Wasi Ahmad, Hantian Ding, Ming Tan, Nihal Jain, Murali Krishna Ramanathan, Ramesh Nallapati, Parminder Bhatia, Dan Roth, et al. 2024. CrossCodeEval: A diverse and multilingual benchmark for cross-file code completion. Advances in Neural Information Processing Systems, 36.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-702", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OpenAI. 2023. GPT-4 technical report. Preprint, arXiv:2303.08774.\\n\\nOpenDevin Team. 2024. OpenDevin: An Open Platform for AI Software Developers as Generlist Agents. https://github.com/OpenDevin/OpenDevin. Accessed: 06/11/2024.\\n\\nJoon Sung Park, Joseph O'Brien, Carrie Jun Cai, Meredith Ringel Morris, Percy Liang, and Michael S Bernstein. 2023. Generative agents: Interactive simulacra of human behavior. In Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology, pages 1\u201322.\\n\\nShishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large language model connected with massive APIs. arXiv preprint arXiv:2305.15334.\\n\\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah Smith, and Mike Lewis. 2023. Measuring and narrowing the compositionality gap in language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 5687\u20135711, Singapore. Association for Computational Linguistics.\\n\\nSheeba Samuel and Daniel Mietchen. 2022. Computational reproducibility of jupyter notebooks from biomedical publications. GigaScience, 13.\\n\\nYongliang Shen, Kaitao Song, Xu Tan, Wenqi Zhang, Kan Ren, Siyu Yuan, Weiming Lu, Dongsheng Li, and Yueting Zhuang. 2023. TaskBench: Benchmarking large language models for task automation. arXiv preprint arXiv:2311.18760.\\n\\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning. In NeurIPS.\\n\\nYifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. 2024. Trial and error: Exploration-based trajectory optimization of LLM agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7584\u20137600, Bangkok, Thailand. Association for Computational Linguistics.\\n\\nShane Storks, Keunwoo Yu, Ziqiao Ma, and Joyce Chai. 2023. NLP reproducibility for all: Understanding experiences of beginners. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 10199\u201310219, Toronto, Canada. Association for Computational Linguistics.\\n\\nWilson L Taylor. 1953. \\\"cloze procedure\\\": A new tool for measuring readability. Journalism quarterly, 30(4):415\u2013433.\\n\\nRunchu Tian, Yining Ye, Yujia Qin, Xin Cong, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2024. DebugBench: Evaluating debugging capability of large language models. arXiv preprint arXiv:2401.04621.\\n\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Manhandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:2305.16291.\\n\\nXingyao Wang, Yangyi Chen, Lifan Yuan, Yizhe Zhang, Yunzhu Li, Hao Peng, and Heng Ji. 2024. Executable code actions elicit better llm agents. arXiv preprint arXiv:2402.01030.\\n\\nXingyao Wang, Zihan Wang, Jiateng Liu, Yangyi Chen, Lifan Yuan, Hao Peng, and Heng Ji. 2023b. Mint: Evaluating llms in multi-turn interaction with tools and language feedback. arXiv preprint arXiv:2309.10691.\\n\\nTianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, et al. 2024. OSWorld: Benchmarking multimodal agents for open-ended tasks in real computer environments. arXiv preprint arXiv:2404.07972.\\n\\nJohn Yang, Carlos E. Jimenez, Alexander Wettig, Kilian Lieret, Shunyu Yao, Karthik Narasimhan, and Ofir Press. 2024a. SWE-agent: Agent-computer interfaces enable automated software engineering. Preprint, arXiv:2405.15793.\\n\\nZhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, et al. 2024b. MatPlotAgent: Method and evaluation for llm-based agentic scientific data visualization. arXiv preprint arXiv:2402.11453.\\n\\nShunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. 2022. Webshop: Towards scalable real-world web interaction with grounded language agents. Advances in Neural Information Processing Systems, 35:20744\u201320757.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik R Narasimhan, and Yuan Cao. 2023. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations.\\n\\nDa Yin, Faeze Brahman, Abhilasha Ravichander, Khyathi Chandu, Kai-Wei Chang, Yejin Choi, and Bill Yuchen Lin. 2024. Agent lumos: Unified and modular training for open-source language agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 12380\u201312403, Bangkok, Thailand. Association for Computational Linguistics.\\n\\nFengji Zhang, Bei Chen, Yue Zhang, Jacky Keung, Jin Liu, Daoguang Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen. 2023. Repocoder: Repository-level code completion through iterative retrieval and generation. arXiv preprint arXiv:2303.12570.\\n\\nShuyan Zhou, Frank F Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-702", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8 shows information about the 45 source repositories used to create the Expert and Masked sets, including their name, original GitHub link and the number of stars on GitHub.\\n\\n| Task          | GitHub Stars |\\n|---------------|--------------|\\n| colbert       | 2826         |\\n| textbox       | 1069         |\\n| amrbart       | 94           |\\n| g-transformer | 43           |\\n| pie-perf      | 80           |\\n| safetybench   | 138          |\\n| discodisco    | 6            |\\n| acqsurvey     | 11           |\\n| curriculum_learning | 9  |\\n| spa           | 5            |\\n| mezo          | 1016         |\\n| mode-connectivity-plm | 7  |\\n| mbib          | 22           |\\n| quantifying-stereotypes-in-language | 1  |\\n| rah-kbqa      | 6            |\\n| dir-gnn       | 115          |\\n| unsupervisedhierarchicalsymbolicregression | 0  |\\n| conv_graph    | 0            |\\n| mera          | 55           |\\n| pira          | 5            |\\n| pet           | 1618         |\\n| transnormerllm | 221        |\\n| bert-lnl      | 9            |\\n| blockskim     | 6            |\\n| data-label-alignment | 3  |\\n| hype          | 13           |\\n| paraphrase-nli | 3           |\\n| powerfulpromptft | 71        |\\n| robust_prompt_classifier | 5  |\\n| align-to-distill | 4   |\\n| inbedder      | 20           |\\n| transpolymer  | 51           |\\n| memorizing-transformers-pytorch | 622 |\\n| multi3woz     | 14           |\\n| galore        | 1332         |\\n| amos          | 24           |\\n| glee          | 9            |\\n| parallel-context-windows | 98 |\\n| logme-nlp     | 5            |\\n| mixup-amp     | 4            |\\n| upet          | 2            |\\n| dpt           | 6            |\\n| team          | 22           |\\n| cet           | 18           |\\n| linkbert      | 411          |\\n\\nAdding to the information in Table 1, we show below the average and median star ratings for other comparable benchmarks (all star ratings are collected as of September 3rd, 2024). This was computed automatically from the GitHub API based on the repositories listed in Jimenez et al. (2024), Lai et al. (2023) and Liu et al. (2023b) (we group together both the train and test repositories mentioned in this table).\\n\\n| Dataset        | # Repositories | Stars (Median) |\\n|----------------|----------------|----------------|\\n| SWE-Bench      | 12             | 27,844 (12,557) |\\n| DS1000         | 8              | 55,227 (35,309) |\\n| MLBench        | 18             | 13,099 (9,632)  |\\n| SUPER (Expert) | 45             | 224 (14)        |\\n| SUPER (Auto)   | 604            | 96 (23)         |\\n\\nA.2 Expert Tasks\\n\\nIn Table 9 we present six examples for problems from the Expert set.\"}"}
{"id": "emnlp-2024-main-702", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Train a teacher model on the iwslt14-de-en dataset (available from the download script in the repo), then train a teacher model and evaluate it. Report the BLEU score on the test set for the student model as a json structured as follows: \\\"bleu\\\": 0.0 (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Once loaded/tokenized, keep only the first 10 rows of each set in the dataset.\\n2. Train for only one epoch and with batch_size=1.\\n3. To evaluate the teacher model, generate predictions on the test set using 'generate.py' with beam_size=5. Then take the evaluation score with the 'score.py' script, with '\u2013ignore-case \u2013sacrebleu'.\\n\\nGit repository: https://github.com/ncsoft/Align-to-Distill\\n\\nEvaluate the generations of my code improving model which are provided in https://drive.google.com/file/d/1izs1iF5cd_NAZsOaZvrrQF3NAsoP8lHf/view?usp=sharing (v1 vs v0). Once evaluated, report the result problem_id and input_acc for each problem of the dataset, as a json list of dictionaries structured as follows:\\n\\n```\\n[\\\"problem_id\\\": \\\", \\\"input_acc\\\": 0.0]\\n```\\n\\n(replace \\\"\\\" and 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Set \\\"num_trials\\\": 2 in the evaluation configuration file to reduce computation time.\\n2. Load only the first 10 rows of the dataset.\\n\\nGit repository: https://github.com/madaan/pie-perf\\n\\nEvaluate the performance of gpt2-small (openai-community/gpt2) on our own sentence classification dataset, available at https://drive.google.com/file/d/1WhDfjunl1VRxK6Vhakhm05C8IABSa1Qc/view?usp=drive_link, using the PCW method with 1 window. Report the accuracy as a json structured as follows: \\\"accuracy\\\": 0.0 (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Run for only 1 epoch.\\n2. Perform only 1 run of the experiment (n-runs).\\n3. label 0 stands for 'negative' and label 1 stands for 'positive'.\\n\\nGit repository: https://github.com/AI21Labs/Parallel-Context-Windows\\n\\nTrain and evaluate the bert-base-uncased model on the SST-2 dataset using the AMP method. Report the maximum test accuracy as a json structured as follows: \\\"max_test_acc\\\": 0.0 (replace 0.0 with the actual value).\\n\\nAdditional instructions:\\n1. Only load the first 10 examples of each (training/validation/test) set in the dataset.\\n2. Train only one batch.\\n\\nGit repository: https://github.com/pai-smallisallyourneed/mixup-amp\\n\\nUse the lm-evaluation-harness to evaluate ai-forever/rugpt3small_based_on_gpt2 on the chegeka test set. Report \\\"metric\\\" and \\\"metric_stderr\\\" as a json structured as follows: \\\"metric\\\": 0.0, \\\"metric_stderr\\\": 0.0 (replace 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Load only the first 10 rows of the dataset.\\n2. Use the following hyperparameters: batch_size=1, num_fewshot=4\\n\\nGit repository: https://github.com/ai-forever/MERA\\n\\nFine-tune the roberta-base model on the MRPC dataset using the GaLore optimizer with the 'run_glue.py' script. Report the accuracy and f1 score as a json structured as follows: \\\"accuracy\\\": 0.0, \\\"f1\\\": 0.0 (replace 0.0 with the actual values).\\n\\nAdditional instructions:\\n1. Only load the first 10 examples of each (training/validation/test) set in the dataset.\\n2. Run for only 1 epoch.\\n3. Use the following hyperparameters: '\u2013enable_galore \u2013lora_all_modules \u2013max_length 512 \u2013seed=1234 \u2013lora_r 4 \u2013galore_scale 4 \u2013per_device_train_batch_size 16 \u2013update_proj_gap 500 \u2013learning_rate 3e-5\"}"}
{"id": "emnlp-2024-main-702", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Can the repository be run on a CPU? It is acceptable if it runs slowly, as we will use small models and datasets. The repository should be rejected only if it relies on specific GPU acceleration methods such as LoRA adapters, if it aims to improve GPU utilization or otherwise relies on specific GPU features.\\n\\n3. Can the repository be used with any of the following model families: BERT, T5, RoBERTa, GPT-2, GPT-Neo, DeBERTa, DistilBERT, BART, Pythia, OPT?\\n\\n4. Does the README provide an example or instructions on how to run an executable Python or Bash file to start an experiment? If so, provide the name of the executable file. For example:\\n\\n```\\npython run_glue.py --model_name_or_path roberta-base --task_name mrpc --output_dir results/ft/roberta_base/mrpc\\n```\\n\\nReturn a json file in this format:\\n\\n```\\n{}\\n```\\n\\nWe then keep repositories where `q_supported_dataset_name`, `q_cpu`, `q_model_families` and `q_execute_example` are all predicted by the LLM to be true, resulting in 1006 repositories. Note that this filtering process is rather conservative; it is likely possible to get more high-quality tasks from repositories that were filtered out.\\n\\nStep 2: Generating Tasks.\\n\\nWe iterate the filtered repositories and prompt GPT-4o (same version) with the following text to generate the tasks.\\n\\nTasks Generation Prompt\\n\\nCreate an experiment-running task based on the provided README file of a research code repository. Output a json dictionary with the following structure:\\n\\n```\\n{\"thought\": \"...\", \"output\": {\"task\": \"...\", \"entrypoint\": \"...\"}}\n```\\n\\nInstructions:\\n\\n1. Choose a specific script (either Bash or Python) for which the README provides an example. If no example is found, skip the repository (return an empty dictionary). The \u201centrypoint\u201d field should specify the script file to be run without any arguments. Use a specific file path, even if the script is typically executed using a module (e.g., \u201ctrain/run.py\u201d, not \u201cpython -m train.run\u201d).\\n\\nEnsure that the script selected is for running experiments (e.g., evaluation or fine-tuning) and not a utility script (such as a server or data processing script).\\n\\n2. The task description should include the following:\\n\\n1. A statement that reflects the goal of the repository. For instance, use \u201cFine-tune a model with the question answering infused pre-training method\u201d rather than just \u201cfine-tune a model,\u201d or \u201cPre-train a reduced-scale model\u201d rather than just \u201cpre-train a model.\u201d If the repository lacks sufficient detail, you may keep this generic.\\n\\n2. The specific dataset to be used. If the repository doesn\u2019t specify supported datasets, skip the repository. For repositories mentioning a group of datasets (e.g., GLUE), choose a specific dataset (e.g., MRPC) instead of just mentioning the group. If the README indicates that the data is unavailable (e.g., \u201cdata will be uploaded soon\u201d), skip the repository.\\n\\n3. The Python script/Bash file/entry point to be run, which should match the \u201centrypoint\u201d field.\\n\\n4. The model to be used. To ensure the task is feasible with minimal compute, select a small or base model from one of the following families:\\n\\n   - bert, roberta, t5, gpt, opt, deberta, distilbert, bart, pythia.\\n\\n   Then, select the smallest model in the family, based on this mapping: bert: bert-base, t5: google-t5/t5-small, gpt: openai-community/gpt2, deberta: deberta-base, bart: bart-base, pythia: EleutherAI/pythia-70m, OPT: facebook/opt-125m\\n\\nIf the README or repository content does not explicitly mention the model family or model size, skip the repository.\"}"}
