{"id": "lrec-2024-main-1375", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Textual Coverage of Eventive Entries in Lexical Semantic Resources\\n\\nEva Fu\u010d\u00edkov\u00e1, Cristina Fern\u00e1ndez-Alcaina, Jan Haji\u010d, and Zde\u0148ka Ure\u0161ov\u00e1\\n\\nInstitute of Formal and Applied Linguistics\\nCharles University, Faculty of Mathematics and Physics, Computer Science School\\nMalostransk\u00e9 n\u00e1m. 25, Prague 1, Czech Republic\\n{fucikova,alcaina,hajic,uresova}@ufal.mff.cuni.cz\\n\\nAbstract\\nThis short paper focuses on the coverage of eventive entries of some well-known lexical semantic resources when applied to random running texts taken from the internet. In order to get the widest coverage, only verbs have been chosen for the comparison, to get as many resources as possible (even though some of the resources cover other parts of speech as well). While coverage gaps are often reported for manually created lexicons (which is the case of most semantically-oriented lexical ones), it was our aim to quantify these gaps, cross-lingually, on a new purely textual resource set produced by the HPL T Project from crawled internet data. Several English, German, Spanish and Czech lexical semantic resources have been selected for this experiment. We also describe the challenges related to the fact that these resources are (to a varying extent) semantically oriented, meaning that the texts have to be preprocessed to obtain lemmas (base forms) and some types of MWEs before the coverage can be reasonably evaluated, and thus the results are necessarily only approximate. The coverage of these resources, with some exclusions as described in the paper, range from 41.00% to 97.33%, confirming the need to expand at least some (even well-known) resources to cover the prevailing source of today's textual resources with regard to lexical units describing events or states (or possibly other eventive mentions).\\n\\nKeywords: language resource, lexical semantics, event types, ontology, text corpora, plain text, textual coverage\\n\\n1. Introduction\\nLexical semantic resources and ontologies, together with their syntactic counterparts, play an important role in today's NLP, even in the age of powerful, but often factually incorrect LLMs like ChatGPT or similar. Their (obvious) disadvantage is however that due to the fact that they are overwhelmingly manually curated, they are always more or less incomplete. We are thus interested in their coverage on running texts, i.e., measuring how many occurrences of words (tokens) in some text actually do appear in the lexical resource.\\n\\nIn order to make the comparison as broad as possible, we have only included verbs from the resources being compared. Polysemy has not been considered due to the absence of reliable (and comparable across languages and/or resources) word sense disambiguation tools capable of accommodating the diversity of the resources. While this approach introduces errors (by increasing coverage because of the inevitable inclusion of non-matching senses), we still believe that when comparing the resources on relative basis, the coverage figures are useful even if they cannot be taken as fully accurate in absolute terms.\\n\\nIn this paper, we do not cover lexical coverage, i.e., the percentage of types which appear in the lexicon, since even if it might be an interesting figure, it is not much relevant when processing data.\\n\\nThere are many papers describing methods and processes to increase coverage, both type-based and token-based, using various approaches, from manual (e.g., (Sio and Morgado da Costa, 2022)) to semi-automatic to fully automatic (with the expected increase in noise inversely proportionate to the manual effort put in), e.g., (Feely et al., 2012; G\u00e1bor et al., 2012; Samvelian et al., 2014; Nimb et al., 2021). Increased coverage can also be obtained indirectly via linking of resources where each of them covers different areas of the language, as in SemLink (Stowe et al., 2021), SynSemClass (Ure\u0161ov\u00e1 et al., 2020, 2022) or BelNet (Navigli and Ponzetto, 2010).\\n\\nHowever, we could not find comparable figures regarding the coverage of the existing resources on large texts, especially those taken from the internet, available in large quantities. This paper thus tries to fill this gap for languages that have several such lexical resources available.\\n\\nThe paper is structured as follows: Sect. 2 describes the data used (both the textual and lexical resources), Sect. 3 describes the data preprocessing necessary to match the lexical resources' entries to text tokens, and Sect. 4 tabulates and discusses the results. Finally, we conclude and draw future plans in Sect. 5. The data on which this paper builds and the full outputs are available for verification and reproducibility purposes at http://hdl.handle.net/11372/LRT-5444.\"}"}
{"id": "lrec-2024-main-1375", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. The Corpora Used\\n\\nFor this study, we have chosen data recently produced by the project called High Performance Language Technologies (HPL T), which aims at collecting large plain text data in 80+ languages and then high-performance computing to build powerful and efficient language and translation models.\\n\\nFor our purposes, we have used monolingual corpora formatted as JSONL files which are compiled from large web crawls provided by the Internet Archive project and CommonCrawl.\\n\\nThe HPL T project has released its first dataset in September 2023; this is the data we have used, even though a new (cleaner, but smaller) version of the HPL T data exists at the time of the final submission.\\n\\nFor each of the languages there is a list (\u201cmap\u201d) of files containing the data. We have chosen, for all our languages (English, Spanish, German, and Czech) one sample called (3.jsonl.zst).\\n\\nFrom each of these files, the first 125,000 entries have been kept, and the \u201ctext\u201d field extracted from each JSONL entry. Each such text string contains a complete document as downloaded and processed by the HPL T project to get a \u201cclean\u201d text.\\n\\nThese limits have been set to keep the sample text corpus for each language around 100 million tokens. The exact sizes of the samples are:\\n\\n| Language | Token count  |\\n|----------|-------------|\\n| English  | 104,408,596 |\\n| German   | 98,956,434  |\\n| Spanish  | 117,477,816 |\\n| Czech    | 101,075,477 |\\n\\n2.2. Lexical Resources Tested\\n\\nFor our coverage evaluation against the text corpora as described above, we have chosen the following lexical resources:\\n\\n- FrameNet (Baker et al., 1998) (English)\\n- WordNet (Fellbaum, 1998) (English)\\n- SynSemClass (Ure\u0161ov\u00e1 et al., 2023) (English, German, Spanish, and Czech)\\n- VerbNet (Kipper et al., 2006) (English)\\n- EngVallex (Cinkov\u00e1 et al., 2014) (English)\\n- PropBank (Palmer et al., 2005) (English)\\n- German Universal Propositions (Akbik et al., 2015) (German)\\n- E-V ALBU (Kubczak, 2014) (German)\\n- Spanish Verbal SenSem Lexicon (Fern\u00e1ndez et al., 2004) (Spanish)\\n- AnCora (Taul\u00e9 et al., 2008) (Spanish)\\n- PDT-Vallex (Ure\u0161ov\u00e1 et al., 2014) (Czech)\\n- VALLEX 4.0 (Lopatkov\u00e1 et al., 2020) (Czech).\\n\\nMost of these resources are semantic in nature, except for PropBank, EngVallex.\"}"}
{"id": "lrec-2024-main-1375", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You can look it up easily.\\n\\nFigure 1: Using a dependency parser output for identifying phrasal verbs - example: (look_up) in the sentence You can look it up easily.\\n\\nalso includes most of PropBank verbs), GUP, E-VALBU, AnCora, PDT-Vallex and VALLEX, which display mostly syntactic features like valency, even though they contain some semantic features as well:\\n\\n| Language | Lexicons | Lexicons |\\n|----------|----------|----------|\\n| English  | 4        | 2        |\\n| German   | 2        | 2        |\\n| Spanish  | 2        | 1        |\\n| Czech    | 2        | 2        |\\n\\n3. Data Preprocessing\\n\\nGiven the nature of lexical resources, especially those referring to eventive word senses (or meanings, as the semantic ones inevitably do), the plain texts cannot be used directly, since the various forms (especially for highly inflective languages like Spanish or Czech) do not match the lexical entries, which are typically verb lemmas or other base forms, often even in the form of a multiword expression (MWE), such as for reflexive verbs (de: sich verstellen, cs: \u0161\u00ed\u0159it se, etc.) or phrasal verbs (en: look up - see Fig. 1 for an example of using the output of the UD Pipe parser for identifying a phrasal particle (up) attached to a verb (look), using the compound:prt dependency relation). Text analysis has to be used to get the lemmas or base verb forms to match against the lexical units in the lexical resource entries. In addition, some words types have to be excluded due to their non-content nature, such as modal verbs\u2013these are normally not included as an entry in lexical semantic resources. This requires even deeper analysis that just getting the lemmas.\\n\\nWe have used the UD Pipe tool, capable of performing tagging, lemmatization and syntactic (dependency) analysis in order to find just those verbs for which we need to compute the coverage, and in the right base form, including MWEs.\\n\\nThe syntactic dependency analysis has been used, The UDPipe in version 2 is trained on the Universal Dependencies v2 (Nivre et al., 2020) datasets for more than 100 languages. We have used the 2.12 models (named <prefix>-ud-2.12-230717) as follows:\\n\\n- for Czech: prefix czech-pdt,\\n- for English: prefix english-ewt,\\n- for German: prefix german-gsd,\\n- for Spanish: prefix spanish-ancora.\\n\\nThe following attributes (columns in the CoNLL-U format) of the UD Pipe output have been used:\\n\\n- the LEMMA column to get the lemma or base form of a reflexive or particle,\\n- the UPOS column to search for the values of VERB, PRON, and ADP that signal the relevant words,\\n- the DEPREL column to find components of verbal MWEs (phrasal and reflexive),\\n- the FORM column to distinguish Czech reflexives se, si.\\n\\nBased on them, we have constructed a \\\"matching-ready\\\" form for each VERB token in the data. While the use of the LEMMA column is obvious, the additional information (especially the syntactic relation for compounds using a particle (compound:prt), which the analyzer recognized as being dependent of the VERB) allowed us to match also phrasal verbs (such as en: break away, look up), verbs with separated prefix in German (such as de: mitgehen) and reflexives (cs: sm\u00e1t se, de: sich vorstellen). The manual inspection of the lexical resources used enabled us then to correctly form the final matching lexical string (pro-noun/particle before/after the verb lemma, joined by space, comma or underscore).\\n\\nGiven that (a) the texts are relatively noisy in terms of various formatting problems, missing spaces etc., and (b) the UDPipe tool still makes some (albeit rare) mistakes even on correct verbs (typically in short or nonstandard contexts), we have computed \\\"maximum noise\\\" figures that show how much the coverage might be influenced (to the worse) by these (possibly) non-verbs. The figures are based on reliable, manually curated sources of lexicons or verbal lemma lists extracted from them. Examples of non-verbs are strings such as in en: 25build, de: Ursachen or only for the various types of MWE identification, however.\\n\\n24 https://ufal.mff.cuni.cz/udpipe/2\\n\\n25 The following attributes (columns in the CoNLL-U format) of the UD Pipe output have been used:\\n\\n- the LEMMA column to get the lemma or base form of a reflexive or particle,\\n- the UPOS column to search for the values of VERB, PRON, and ADP that signal the relevant words,\\n- the DEPREL column to find components of verbal MWEs (phrasal and reflexive),\\n- the FORM column to distinguish Czech reflexives se, si.\\n\\nBased on them, we have constructed a \\\"matching-ready\\\" form for each VERB token in the data. While the use of the LEMMA column is obvious, the additional information (especially the syntactic relation for compounds using a particle (compound:prt), which the analyzer recognized as being dependent of the VERB) allowed us to match also phrasal verbs (such as en: break away, look up), verbs with separated prefix in German (such as de: mitgehen) and reflexives (cs: sm\u00e1t se, de: sich vorstellen). The manual inspection of the lexical resources used enabled us then to correctly form the final matching lexical string (pro-noun/particle before/after the verb lemma, joined by space, comma or underscore).\\n\\nGiven that (a) the texts are relatively noisy in terms of various formatting problems, missing spaces etc., and (b) the UDPipe tool still makes some (albeit rare) mistakes even on correct verbs (typically in short or nonstandard contexts), we have computed \\\"maximum noise\\\" figures that show how much the coverage might be influenced (to the worse) by these (possibly) non-verbs. The figures are based on reliable, manually curated sources of lexicons or verbal lemma lists extracted from them. Examples of non-verbs are strings such as in en: 25build, de: Ursachen or only for the various types of MWE identification, however.\"}"}
{"id": "lrec-2024-main-1375", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Verb Occurrences Possible noise Attested Language lemmas in corpus (lemmas) (tokens) lemmas tokens\\n\\nEnglish 34,526 9,197,397 80.57% 2.78% 6,710 8,941,473\\n\\nGerman 56,443 6,244,389 95.40% 18.00% 2596 5,120,287\\n\\nSpanish 57,481 8,843,767 95.35% 13.54% 2675 7,646,483\\n\\nCzech 37,156 5,462,633 79.81% 7.96% 7501 5,027,974\\n\\nTable 1: Filtering out corpus noise\\n\\nAt implant\u00e1t, wrongly analyzed or \\\"guessed\\\" by UDPipe to be VERBs, the statistics on this \\\"maximal\\\" noise are summarized in Table 1: the noise in terms of lemmas is very high, but the token counts are influenced much less.\\n\\nLanguage\\nVerbs excluded Percent\\n\\nEnglish be can could have may make must will would 5.44%\\n\\nGerman d\u00fcrfen haben k\u00f6nnen m\u00f6gen m\u00fcssen sein sollen wollen 3.90%\\n\\nSpanish deber poder querer saber ser soler 1.56%\\n\\nCzech by\u0107 d\u011blat lze muset moci m\u00edt sm\u011bt 10.91%\\n\\nTable 2: Modal (and other) verbs excluded, in %\\n\\nIn addition, modals, and copulas have been excluded (Table 2). These either do not possibly represent verbs that would be expected to have a separate entry in lexical semantic resources, or are ambiguous enough not to be included in these resources. Given that the texts cannot be (as of yet) analyzed fully semantically for a better matching, they have been excluded, too.\\n\\n4. Results\\n\\nThe resulting coverage on the final set of lemmas tested for coverage is presented in Table 3, with the \\\"winners\\\" in each language in bold. The basis for the coverage percentages (last column) is still the original number of verb occurrences in the texts used, i.e., the third column as seen in Table 1, minus the excluded modals and other such verbs, as seen in Table 2.\\n\\nThe resources are ordered from the \\\"most semantic\\\" ones (FrameNet, WordNet, SynSemClass) to the \\\"least semantic,\\\" such as the valency lexicons used for the Spanish and Czech treebank annotation. With the exception of German, Table 1 serves only as an indication of noise in the data, but the possibly problematic verbs have not been excluded from the coverage computation.\\n\\nE-VA-BU valency lexicon, the more syntactically-oriented lexicons show higher coverage (with PropBank showing its maturity with the highest coverage of all the lexicons), while among the semantic ones, WordNet wins for English (and overall), but has poor coverage for Czech. However, WordNet\u2014except for the hierarchical relations among its synsets\u2014does not offer additional semantic (or even syntactic) information for annotation or other applications, as opposed to FrameNet(s), VerbNet or SynSemClass. From these richly annotated semantic/syntactic resources, SynSemClass for English (and to a certain extent, for Czech) offers the best coverage, followed very closely by VerbNet.\\n\\nWhile keeping the original verb occurrences counts despite the noise in the data, as presented in Table 1, lowers the coverage due to possibly dubious verbs being counted, the filtering of modals and copulas (Table 2), on the other hand, inevitably increases the coverage. However, we deem it fair to do so, as it is not expected that these verbs would have an entry in semantic lexical resources (WordNet is an exception, but for comparison purposes, it has been simply treated the same way).\\n\\nThe controversial point might be the exclusion of verbs like to be, to have or to do, since they do have, depending on context or use, its own semantic \\\"content\\\" meaning (e.g., existential to be) and are (or should) be covered in resources like FrameNet, VerbNet or SynSemClass. However, even with the UDPipe analysis, it would be difficult to distinguish, e.g., the many senses of to have and its counterparts in the other languages. We thus hope that by excluding them, the coverage will be closer to the actual one than by not excluding them. This has been done uniformly across all the resources, with the aim of minimizing its influence on the differences among the resources when comparing them.\\n\\n5. Conclusions and Future Work\\n\\nAs described and presented in our paper, we have tried to quantify the coverage of widely used lexical resources, mainly those reflecting semantics, on recent internet texts. The results vary widely, with some of the most popular resources (WordNet, FrameNet, WordNet) showing poor coverage for some languages. However, with the exception of German, Table 1 serves only as an indication of noise in the data, but the possibly problematic verbs have not been excluded from the coverage computation.\\n\\nE-VA-BU valency lexicon, the more syntactically-oriented lexicons show higher coverage (with PropBank showing its maturity with the highest coverage of all the lexicons), while among the semantic ones, WordNet wins for English (and overall), but has poor coverage for Czech. However, WordNet\u2014except for the hierarchical relations among its synsets\u2014does not offer additional semantic (or even syntactic) information for annotation or other applications, as opposed to FrameNet(s), VerbNet or SynSemClass. From these richly annotated semantic/syntactic resources, SynSemClass for English (and to a certain extent, for Czech) offers the best coverage, followed very closely by VerbNet.\\n\\nWhile keeping the original verb occurrences counts despite the noise in the data, as presented in Table 1, lowers the coverage due to possibly dubious verbs being counted, the filtering of modals and copulas (Table 2), on the other hand, inevitably increases the coverage. However, we deem it fair to do so, as it is not expected that these verbs would have an entry in semantic lexical resources (WordNet is an exception, but for comparison purposes, it has been simply treated the same way).\\n\\nThe controversial point might be the exclusion of verbs like to be, to have or to do, since they do have, depending on context or use, its own semantic \\\"content\\\" meaning (e.g., existential to be) and are (or should) be covered in resources like FrameNet, VerbNet or SynSemClass. However, even with the UDPipe analysis, it would be difficult to distinguish, e.g., the many senses of to have and its counterparts in the other languages. We thus hope that by excluding them, the coverage will be closer to the actual one than by not excluding them. This has been done uniformly across all the resources, with the aim of minimizing its influence on the differences among the resources when comparing them.\"}"}
{"id": "lrec-2024-main-1375", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| lexical resource | language | coverage (tokens) | coverage (percent) |\\n|------------------|----------|-------------------|--------------------|\\n| FrameNet         | English  | 7,464,343         | 85.82%             |\\n|                  | German   | 2,460,251         | 41.00%             |\\n| WordNet          | English  | 8,465,366         | 97.33%             |\\n|                  | Czech    | 2,586,432         | 53.15%             |\\n| SynSemClass      | English  | 7,959,432         | 91.51%             |\\n|                  | German   | 3,339,962         | 55.66%             |\\n|                  | Spanish  | 6,627,769         | 76.13%             |\\n|                  | Czech    | 4,125,642         | 84.77%             |\\n| VerbNet          | English  | 7,657,626         | 88.04%             |\\n| SenSem           | Spanish  | 4,732,521         | 54.36%             |\\n| PropBank         | English  | 8,433,779         | 96.97%             |\\n| PropBank         | German   | 8,275,981         | 95.15%             |\\n| PropBank         | Czech    | 3,235,501         | 53.92%             |\\n| GUP              | German   | 4,729,042         | 78.81%             |\\n| AnCora           | Spanish  | 7,508,545         | 86.25%             |\\n| PDT-Vallex       | Czech    | 4,374,973         | 89.90%             |\\n| Vallex           | Czech    | 4,239,811         | 87.12%             |\\n\\nTable 3: Coverage of all the lexical resources used (FrameNet and PropBank/EngVallex, all for English) and some others showing relatively high (albeit not perfect, as expected) coverage. For the non-English languages, the situation is substantially worse \u2013 with exceptions, such as AnCora for Spanish and PDT-Vallex and SynSemClass for Czech.\\n\\nThe matching algorithm can still be substantially improved. The (syntactic) UDPipe parser can still provide more information than we have been able to use, such as proper distinction between auxiliary and modal verbs and the content-bearing ones, etc. Of course a good semantic parser would be the ultimate solution to use, alleviating the need for approximations and exclusions \u2013 provided that the parser would be trained on an annotation matching the lexical resources (which by itself is a non-trivial task to do for 17 resources), which differ in the treatment of reflexive particles, phrasal verbs, MWEs in general, treatment of light verbs, and in the semantic labeling schemas.\\n\\nIn the future, we also plan to extend the set of lexical resources for which coverage is computed, and redo those for which (if and when) new versions become available. If interest persists, we will publish a \u201cdashboard\u201d where further figures on coverage on these and possibly additional resources will be posted.\\n\\nAs is becoming common practice, we have packaged and published the data on which this paper builds as well as its full outputs, to allow for verification.\\n\\n6. Acknowledgements\\n\\nThe work described herein has been supported by the Grant Agency of the Czech Republic under the EXPRO program as project \u201cLUSyD\u201d (project No. GX20-16819X), by the Ministry of Education, Youth and Sports under the Inter-Excellence programme, project \u201cUniversal Meaning Representation\u201d (project No. LUAUS23283). In addition, we acknowledge the use of data resulting from the European Union\u2019s Horizon Europe research and innovation programme under grant agreement No. 101070350 and from UK Research and Innovation (UKRI) under the UK government\u2019s Horizon Europe funding guarantee, grant number 10052546. It has also used resources hosted, i.a., by the LINDA T/CLARIAH-CZ Research Infrastructure (projects LM2018101 and LM2023062, supported by the Ministry of Education, Youth and Sports of the Czech Republic). We would also like to thank our team of annotators on the SynSemClass ontology work under the LUSyD project, and also to the authors of all the other 16 lexical resources used, without whom this project and paper could not exist.\\n\\n7. Bibliographical References\\n\\nAlan Akbik, Laura Chiticariu, Marina Danilevsky, Yunyao Li, Shivakumar Vaithyanathan, and Huaiyu Zhu. 2015. Generating high quality proposition Banks for multilingual semantic role labeling. In Proceedings of the 53rd Annual Meeting of the Association for Computational\"}"}
{"id": "lrec-2024-main-1375", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 397\u2013407, Beijing, China. Association for Computational Linguistics.\\n\\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet Project. In Proceedings of the 36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics - Volume 1, ACL '98, pages 86\u201390, Stroudsburg, PA, USA. Association for Computational Linguistics.\\n\\nWes Feely, Claire Bonial, and Martha Palmer. 2012. Evaluating the coverage of VerbNet. In Joint 8th ISO - ACL SIGSEM Workshop on Interoperable Semantic Annotation, Pisa, Italy, October 3-5, 2012.\\n\\nChristiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Language, Speech, and Communication. MIT Press, Cambridge, MA.\\n\\nA. Fern\u00e1ndez, G. V\u00e1zquez, and I. Castell\u00f3n. 2004. Sensem: base de datos verbal del espa\u00f1ol. In IX Ibero-American Workshop on Artificial Intelligence, pages 155\u2013163. IBERAMIA. Puebla de los \u00c1ngeles, Mexico, ISBN: 968-863-786-6.\\n\\nKata G\u00e1bor, Marianna Apidianaki, Beno\u00eet Sagot, and \u00c9ric Villemonte de La Clergerie. 2012. Boosting the coverage of a semantic lexicon by automatically extracted event nominalizations. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 1466\u20131473, Istanbul, Turkey. European Language Resources Association (ELRA).\\n\\nKarin Kipper, Anna Korhonen, Neville Ryant, and Martha Palmer. 2006. Extending VerbNet with novel verb classes. In Proceedings of LREC, volume 2006.\\n\\nGeorge A. Miller. 1995. WordNet: A Lexical Database for English. Commun. ACM, 38(11):39\u201341.\\n\\nR. Navigli and Simone Paolo Ponzetto. 2010. BeloNet: Building a very large multilingual semantic network. In ACL.\\n\\nSanni Nimb, Bolette Pedersen, and Sussi Olsen. 2021. DanNet2: Extending the coverage of adjectives in DanNet based on thesaurus data (project presentation). In Proceedings of the 11th Global Wordnet Conference, pages 267\u2013272, University of South Africa (UNISA). Global Wordnet Association.\\n\\nJoakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Haji\u010d, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4034\u20134043, Marseille, France. European Language Resources Association.\\n\\nKarel Pala, Tom\u00e1\u0161 \u010capek, Barbora Zaj\u00ed\u010dkov\u00e1, Dita Bart\u016f\u0161kov\u00e1, Kate\u0159ina Kulkov\u00e1, Petra Hoffmannov\u00e1, Eduard Bej\u010dek, Pavel Stra\u0148\u00e1k, and Jan Haji\u010d. 2011. Czech WordNet 1.9 PDT. LINDA T/CLARIAH-CZ digital library at the Institute of Formal and Applied Linguistics (\u00daFAL), Faculty of Mathematics and Physics, Charles University.\\n\\nMartha Palmer, Daniel Gildea, and Paul Kingsbury. 2005. The Proposition Bank: An annotated corpus of semantic roles. Computational Linguistics, 31(1):71\u2013106.\\n\\nPollet Samvelian, Pegah Faghiri, and Sarra El Ayari. 2014. Extending the coverage of a MWE database for Persian CPs exploiting valency alternations. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 4023\u20134026, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nUt Seong Sio and Lu\u00eds Morgado da Costa. 2022. Enriching linguistic representation in the Cantonese WordNet and building the new Cantonese Wordnet corpus. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 70\u201378, Marseille, France. European Language Resources Association.\\n\\nKevin Stowe, Jenette Preciado, Kathryn Conger, Susan Windisch Brown, Ghazaleh Kazeminejad, James Gung, and Martha Palmer. 2021. SemLink 2.0: Chasing lexical resources. In Proceedings of the 14th International Conference on Computational Semantics (IWCS), pages 222\u2013227, Groningen, The Netherlands (online). Association for Computational Linguistics.\\n\\nMariona Taul\u00e9, M. Ant\u00f2nia Mart\u00ed, and Marta Recasens. 2008. AnCora: Multilevel annotated corpora for Catalan and Spanish. In Proceedings of the Sixth International Conference on Language Resources and Evaluation (LREC'08), Marrakech, Morocco. European Language Resources Association (ELRA).\\n\\nZde\u0148ka Ure\u0161ov\u00e1, Eva Fu\u010d\u00edkov\u00e1, Eva Haji\u010dov\u00e1, and Jan Haji\u010d. 2020. SynSemClass linked lexicon:\"}"}
{"id": "lrec-2024-main-1375", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
