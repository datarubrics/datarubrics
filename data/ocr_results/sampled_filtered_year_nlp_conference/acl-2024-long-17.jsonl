{"id": "acl-2024-long-17", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nLarge language models hold significant potential for integrating various data types, such as text documents and database records, for advanced analytics. However, blending text and numerical data presents substantial challenges. LLMs need to process and cross-reference entities and numbers, handle data inconsistencies and redundancies, and develop planning capabilities such as building a working memory for managing complex data queries. In this paper, we introduce four novel tasks centered around sports data analytics to evaluate the numerical reasoning and information fusion capabilities of LLMs. These tasks involve providing LLMs with detailed, play-by-play sports game descriptions, then challenging them with adversarial scenarios such as new game rules, longer durations, scrambled narratives, and analyzing key statistics in game summaries. We conduct extensive experiments on NBA and NFL games to assess the performance of LLMs on these tasks. Our benchmark, SportsMetrics, introduces a new mechanism for assessing LLMs' numerical reasoning and fusion skills.\\n\\n1 Introduction\\nLarge language models (LLMs) are more powerful than ever. OpenAI's GPT-4 Turbo (2023) features a 128k context window, allowing it to process over 300 pages of text in a single prompt. Claude v2.1 (2023) steps it up with a 200k token window, equivalent to roughly 150,000 words or more than 500 pages. Mistral AI (2023) has created a sparse mixture of experts model capable of processing up to 32k tokens. The developments suggest language models can now engage with vast amounts of text content and data, opening doors to exciting new applications in various domains.\\n\\nOne of the most promising uses of LLMs is in handling a combination of unstructured texts and structured data. For example, determining if a patient can be discharged from the hospital may involve reviewing doctor notes, radiology and pathology reports, lab results, and other records that blend text and numerical data (Adams et al., 2021; Bardhan et al., 2022; Cai et al., 2022, 2023; Veen et al., 2023; Ben Abacha et al., 2023); LLM Assistants for online shopping need to process product catalogs, sales transactions, and customer queries (Brynjolfsson et al., 2023; Loten, 2023). Yet, summarizing key details from a mix of unstructured and structured sources remains a considerable challenge. An LLM must navigate text descriptions, link entities, aggregate numbers, handle discrepancies, and beyond.\\n\\nInformation fusion focuses on synthesizing information from multiple textual sources to derive meaningful conclusions (Barzilay et al., 1999). Current approaches involve summarizing multiple text documents, providing concise answers to user queries, and integrating summarization with natural language inference to deduce information (Bhaskar et al., 2023; Caciularu et al., 2023; Sprague et al., 2023).\"}"}
{"id": "acl-2024-long-17", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chicago Bulls\\nPoints:                       95\\nRebounds:                49\\nAssists:                     \u2026\\n\\nDetroit Pistons\\nPoints:                       95\\nRebounds:                49\\nAssists:                     \u2026\\n\\nKey Stats (Before)\\n\\nFigure 2: (TOP LEFT) We examine the impact of changing game rules on final scores. For basketball, scoring events such as free throws, three-pointers, field goals, vary from 1 to 3 points. We ask LLMs to maintain these scoring events but under a new rule where each is worth only 1 point. (BOTTOM LEFT) We randomly swapped player team affiliations in the table without altering the game's play-by-play records. (RIGHT) LLMs are provided with detailed play-by-play descriptions of a sports game and player team affiliations. Their job is to use this information to update key game statistics in a JSON format.\\n\\n2022; Bostrom et al., 2022). The output is often a short text summary, the quality of which is difficult to evaluate (Deutsch et al., 2021). In contrast, our approach emphasizes the numerical aspect of information fusion (Geva et al., 2020; Zhu et al., 2021; Zhao et al., 2023; Reddy et al., 2024). We enable the LLM to navigate through lengthy texts, gather crucial statistics, and develop a working memory to manage complex data queries.\\n\\nWe introduce SportsMetrics, a benchmark designed to assess LLMs' abilities in numerical reasoning and data fusion. This benchmark provides LLMs with detailed, play-by-play descriptions of sports games, including timestamps, player actions, and team affiliations, as illustrated in Figure 1. We focus on four novel tasks to evaluate LLMs in adversarial scenarios: (a) adapting to new game rules, (b) handling lengthy game descriptions, (c) managing scrambled game narratives, and (d) analyzing critical statistics in game summaries. E.g., an LLM might be asked to complete a basketball game recap by inserting missing key statistics, which requires the development of a working memory for game stats and reasoning skills.\\n\\nOur SportsMetrics benchmark presents three main benefits. First, it leverages sports data, including team-player affiliations and play-by-play details; they are dynamic narratives that LLMs cannot easily memorize. Second, it allows us to evaluate LLMs' ability to track key statistics such as team points, assists, blocks, steals, and more, while also offering an overall game efficiency score for direct LLM comparison. Lastly, its use of widely understood sports terminology makes it more accessible to researchers than specialized medical language, making it an ideal benchmarking tool. While our current focus is on English, SportsMetrics also holds promise for multilingual applications.\\n\\n2 Related Work\\nThere is a growing need for a benchmark to evaluate LLMs' information fusion capabilities, which offers clear, quantitative scores for comparing various LLMs. For example, Chatbot Arena (Zheng et al., 2023) utilizes Elo ratings (Boubdir et al., 2023), MT-Bench comprises of 80 multi-turn questions, and MMLU focuses on a model's multitask accuracy across 57 tasks (Hendrycks et al., 2021). Multi-document summarization offers a promising benchmark (Lebanoff et al., 2021; Huang et al., 2021; Wang et al., 2022; Xu et al., 2023). However, developing a summary scoring system poses challenges due to variables such as summary length, content coverage, and faithfulness (Cao et al., 2022; Liu et al., 2023c; Krishna et al., 2023; Hu et al., 2023; Li et al., 2023; Xu et al., 2024; Joseph et al., 2024). Sports data, which combines static knowledge with player dynamics, presents an untapped opportunity for benchmarking LLMs.\\n\\nCombining information from a blend of textual and numerical records poses a significant challenge. In traditional multi-document summarization, the system creates a concise summary from a set of topically related documents. Giorgi et al. (Giorgi et al., 2023) show that this task remains difficult in an \u201copen-domain\u201d setting, where the document set is generated by a retriever and may include irrelevant information. With the growing popularity of...\"}"}
{"id": "acl-2024-long-17", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The NBA Game Score, developed by former ESPN writer John Hollinger, provides a rough measure of a player's productivity in a basketball game. It takes into account both positive contributions (such as points, rebounds, and assists) and negative ones (such as missed shots and turnovers). It's a useful tool for quickly comparing players' performances.\\n\\nIn 1979, the NCAA created the Passing Efficiency formula with specific scaling factors to ensure an average passer would have a rating of exactly 100 for yards-per-attempt and completion percentage. The factors 330 (3.3 times touchdown percentage) and 200 (2.0 times interception percentage) were selected so that they would balance each other out for an average player. While the NCAA and NFL formulas are essentially similar, the NFL's use of \\\"caps\\\" makes its formula a bit more complex to calculate.\\n\\n---\\n\\n**Game Score**\\n\\n\\\\[\\n\\\\text{Game Score} = (\\\\text{Points}) + 0.4 \\\\times (\\\\text{Field Goals Made}) - 0.7 \\\\times (\\\\text{Field Goals Attempted}) - 0.4 \\\\times (\\\\text{Free Throws Attempted} - \\\\text{Free Throws Made}) + 0.7 \\\\times (\\\\text{Offensive Rebounds}) + 0.3 \\\\times (\\\\text{Defensive Rebounds}) + (\\\\text{Steals}) + 0.7 \\\\times (\\\\text{Assists}) + 0.7 \\\\times (\\\\text{Blocks}) - 0.4 \\\\times (\\\\text{Personal Fouls}) - (\\\\text{Turnovers})\\n\\\\]\\n\\n**Passing Efficiency**\\n\\n\\\\[\\n\\\\text{Passing Efficiency} = \\\\frac{8.4 \\\\times (\\\\text{Yards}) + 330 \\\\times (\\\\text{Touchdowns}) - 200 \\\\times (\\\\text{Interceptions}) + 100 \\\\times (\\\\text{Completions})}{\\\\text{Attempts}}\\n\\\\]\\n\\nSources:\\n- https://www.nbastuffer.com/analytics101/game-score\\n- https://stassen.com/football/pass-eff/\"}"}
{"id": "acl-2024-long-17", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are a helpful assistant tasked with analyzing sports games. You have been given play-by-play breakdowns of a basketball game between two teams.\\n\\nThe \\\"Time\\\" column shows the exact time on the game clock when each play took place. The game clock counts down, so this column displays times in a descending order.\\n\\nThe \\\"Play\\\" column describes the action that happened at the respective times. It provides details of specific plays, movements, and outcomes on the court.\\n\\nTeam players are listed in two rows, each row representing one of the two basketball teams involved in the game.\\n\\nYour task is to fill in the missing key statistics from a basketball game recap. Each missing statistic is marked with '___'.\\n\\nGame Recap:\\n```\\nJalen Duren had ___ points and ___ rebounds as the Detroit Pistons overcame a career-high ___ points from Zach LaVine to beat the Chicago Bulls ___-___ on Saturday night.\\n```\\n\\nFirst, create an internal memory as a JSON object. Initially, this JSON object only has placeholders for team points, like this:\\n\\n```\\nInitial Memory: {\\n  \\\"Chicago Bulls\\\": {\\\"points\\\": null},\\n  \\\"Detroit Pistons\\\": {\\\"points\\\": null}\\n}\\n```\\n\\nNext, add the necessary key game or player statistics to the working memory to complete the missing information. These statistics might include categories such as 'field goals made', 'field goals attempted', 'free throws made', 'free throws attempted', 'rebounds', 'assists', 'blocks', 'steals', 'points' and others. This JSON object will later be populated with relevant data that will be used to fill in the blanks.\\n\\n```\\nWorking Memory Example: {\\n  \\\"Jalen Duren\\\": {\\\"points\\\": null, \\\"rebounds\\\": null},\\n  \\\"Zach LaVine\\\": {\\\"points\\\": null},\\n  \\\"Chicago Bulls\\\": {\\\"points\\\": null},\\n  \\\"Detroit Pistons\\\": {\\\"points\\\": null}\\n}\\n```\\n\\nNow, you will be given a new game recap. Your goal is to create a working memory as a JSON object, which can be used to fill in the missing key statistics in the Game Recap.\\n\\nGame Recap:\\n```\\nFranz Wagner scored 24 of his ___ points in the second half, Paolo Banchero added ___ points, and the Orlando Magic overcame Nikola Jokic's triple-double Wednesday night to record their fifth straight victory, ___-___ over the Denver Nuggets.\\n```\\n\\nInitially, you are given a JSON object where all values are set to null. Based on the provided play-by-play breakdown and team-player data, you will update these key statistics in JSON format.\\n\\n```\\n{\"Jalen Duren\": {\"points\": null, \"rebounds\": null}, \"Zach LaVine\": {\"points\": null}, \"Chicago Bulls\": {\"points\": null}, \"Detroit Pistons\": {\"points\": null}}\\n```\\n\\nYour task is to complete the missing key statistics from a basketball game recap. You'll fill in the blanks using only information from the working memory, which is represented as a JSON object containing the essential game or player statistics. Here's an example:\\n\\n```\\nWorking Memory Example: {\\n  \\\"Jalen Duren\\\": {\\\"points\\\": 23, \\\"rebounds\\\": 15},\\n  \\\"Zach LaVine\\\": {\\\"points\\\": 51},\\n  \\\"Chicago Bulls\\\": {\\\"points\\\": 102},\\n  \\\"Detroit Pistons\\\": {\\\"points\\\": 102}\\n}\\n```\\n\\nGame Recap:\\n```\\nJalen Duren had ___ points and ___ rebounds as the Detroit Pistons overcame a career-high ___ points from Zach LaVine to beat the Chicago Bulls ___-___ on Saturday night.\\n```\\n\\nOutput:\\n```\\nJalen Duren had 23 points and 15 rebounds as the Detroit Pistons overcame a career-high 51 points from Zach LaVine to beat the Chicago Bulls 118-102 on Saturday night.\\n```\"}"}
{"id": "acl-2024-long-17", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: LLMs used in this study. Prices are per 1,000 tokens. Llama-2 and Mistral-7B are free and open-source.\\n\\nTherefore, LLMs need to adjust to changing rules. Xie et al. (2023) highlight the importance of knowing when to trust a model's own knowledge. Meng et al. (2023) explored finetuning LLMs to alter specific knowledge, but such changes are often irreversible. Here, we propose two tasks to evaluate LLMs' abilities in adapting to new game rules.\\n\\nNew Scoring Rules\\nWe examine the impact of changing game rules on final scores. For basketball, scoring events such as free throws, three-pointers, field goals, vary from 1 to 3 points. We ask LLMs to maintain these scoring events but under a new rule where each action is worth only 1 point. This contradicts LLMs' existing knowledge, challenging them to recalibrate game scores accordingly. Ground-truth scores under this rule are obtained by counting the total number of scoring actions to determine each team's total points.\\n\\nPlayer Swapping\\nWe randomly swapped player team affiliations in the table without changing the game's play-by-play records, as illustrated in Figure 2. Ground-truth team scores for this task are calculated by summing individual player scores under their new affiliations. This task allows us to vary the degree of conflict between the model's existing knowledge and the provided evidence. Swapping more players increases the task's difficulty.\\n\\n3.3 Robustness Against Noise\\nShuffling Play-by-Plays\\nWe present an adversarial challenge where we shuffle basketball game play-by-play descriptions and then ask LLMs to track the total points of each team. We choose basketball because adjacent actions in this context do not show strong causal relationships. Changing the sequence of scoring actions does not affect the teams' total points. We anticipate that long-context LLMs will produce consistent or similar final game scores. To avoid confusing the model, we maintain the original order of timestamps.\\n\\nWe can also adjust the frequency of scoring plays...\"}"}
{"id": "acl-2024-long-17", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Average absolute difference between model predictions and the actual scores on NBA data for tracking a team\u2019s total points (\\\\(\\\\text{Points}\\\\)) and all key game statistics (\\\\(\\\\text{GScore}\\\\)). Moreover, we evaluate LLMs\u2019 performance in three adversarial scenarios: \\\\(\\\\Delta\\\\text{NewRule}\\\\), \\\\(\\\\Delta\\\\text{Swap}\\\\) and \\\\(\\\\Delta\\\\text{Shuffle}\\\\).\\n\\nPlayer statistics. During a self-reflection phase, the LLM evaluates if its JSON memory can accurately complete the missing statistics for the given game recap. If it can, it responds with this memory; if not, it further refines the memory structure. Finally, using the detailed play-by-play and team-player data, the LLM updates the key statistics in the JSON format, then uses this information to fill in the blanks in the game summary. Figure 5 illustrates various LLM attempts building a memory.\\n\\nOur task is inspired by several studies on LLM planning. Unlike LLM+P which uses the Planning Domain Definition Language (PDDL) for problem-solving (Liu et al., 2023a), we simplify the process by requiring only a valid JSON object for working memory. Relevant studies such as Reflexion (Shinn et al., 2023), ReAct (Yao et al., 2023b), and Tree-of-thought (Yao et al., 2023a) have also influenced our approach. Sumers et al. (2023) have developed a framework for integrating planning into LLM agents. Prior studies have focused on ALFWorld\u2019s interactive TextWorld environments. Our method focuses on sports, which involves masking key statistics in game recaps by sports journalists, then converting them into task data points for LLMs. We assess LLMs by their accuracy in filling in missing key statistics from game summaries.\\n\\n4 Experiments\\n\\nWe evaluate various LLMs in our SportsMetrics benchmark. These models are listed in Table 1 and split into two categories: long-context LLMs, capable of processing over 16k tokens, and standard LLMs, handling 4k to 8k tokens. Our evaluation focuses on their ability to accurately track a team\u2019s total points (\\\\(\\\\text{Points}\\\\)) and all key game statistics (\\\\(\\\\text{GScore}\\\\)). We measure the average absolute difference (deviation) between the models\u2019 predictions and the actual box scores, denoted as \\\\(\\\\Delta\\\\text{Points}\\\\) and \\\\(\\\\Delta\\\\text{GScore}\\\\), respectively.\\n\\nOur dataset comprises 28,492 NBA games and 5,867 NFL games spanning two decades from 2002 to 2023, available through ESPN\u2019s archives. We randomly selected 100 games from each sport for our test set. On average, NBA games contain 466 plays and NFL games 173 plays. An average NBA game includes 6,229 tokens, while an NFL game has 6,166 tokens, with maximum lengths reaching 7,322 and 7,659 tokens, respectively.\\n\\nLLMs\u2019 ability to integrate information is tested under three adversarial scenarios: (a) \u2018\\\\(\\\\text{NewRule}\\\\)\u2019, which assigns every scoring action just one point, regardless of the move, (b) \u2018\\\\(\\\\text{Swap}\\\\)\u2019 which randomly selects two players from each team to swap their affiliations in the team-player table, (c) \u2018\\\\(\\\\text{Shuffle}\\\\)\u2019, which duplicates any non-scoring action with a 20% chance (\\\\(p=0.2\\\\)) before shuffling the play-by-plays. We assess LLMs\u2019 performance in these scenarios and report the deviation of predicted team points from actual scores as \\\\(\\\\Delta\\\\text{NewRule}\\\\), \\\\(\\\\Delta\\\\text{Swap}\\\\) and \\\\(\\\\Delta\\\\text{Shuffle}\\\\).\\n\\nIn Table 2, we present our findings from the NBA section of our dataset. With \\\\(\\\\Delta\\\\) representing the gap between predictions and actual scores, smaller values are preferable. We find that long-context LLMs significantly outperform standard LLMs across all tasks. GPT-3.5-Turbo-1106 leads in performance in every task except for \\\\(\\\\Delta\\\\text{GScore}\\\\), where Gemini-Pro has a slight edge. Long-context models have been released recently in late 2023. These results demonstrate their remarkable ability in identifying relevant actions from game play-by-plays, attributing \\\\(\\\\Delta\\\\text{GScore}\\\\) consistently shows higher values compared to \\\\(\\\\Delta\\\\text{Points}\\\\) because it goes beyond counting a team\u2019s points. It offers a full game analysis by requiring the LLM to consolidate key statistics such as points, rebounds, steals, assists and more into an overall score. Considering only team points is insufficient, especially in sports like soccer where scoring is rare. When necessary, we can convert GameScore to points by zeroing out other stats.\"}"}
{"id": "acl-2024-long-17", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Discrepancies between model predictions and actual scores on NFL stats, including yards (\\\\(\\\\Delta\\\\)Yards), attempts (\\\\(\\\\Delta\\\\)ATT), completions (\\\\(\\\\Delta\\\\)COMP), touchdowns (\\\\(\\\\Delta\\\\)TD), interceptions (\\\\(\\\\Delta\\\\)INT) and passing efficiency (\\\\(\\\\Delta\\\\)PE).\\n\\n| Model                                    | \\\\(\\\\Delta\\\\)Yards | \\\\(\\\\Delta\\\\)ATT | \\\\(\\\\Delta\\\\)COMP | \\\\(\\\\Delta\\\\)TD | \\\\(\\\\Delta\\\\)INT | \\\\(\\\\Delta\\\\)PE |\\n|-------------------------------------------|------------------|---------------|----------------|-------------|--------------|-------------|\\n| Long-Context GPT-4-1106-Preview           | 34.77            | 4.44          | 2.96           | 0.17        | 0.13         | 14.33       |\\n| (16k+ Tokens) Claude-2.1                  | 52.53            | 5.43          | 3.75           | 0.29        | 0.22         | 17.53       |\\n| GPT-3.5-Turbo-1106                        | 64.87            | 7.80          | 4.73           | 0.49        | 0.30         | 18.43       |\\n| Gemini-Pro                                | 85.14            | 12.68         | 6.87           | 0.83        | 0.52         | 26.17       |\\n| Standard GPT-3.5-0613                     | 105.68           | 24.11         | 15.80          | 1.09        |              | 89.56       |\\n| (4k to 8k Tokens) Llama-2-13B-Chat        | 244.48           | 22.37         | 19.66          | 1.47        | 1.03         | 191.76      |\\n| Mistral-7B-Instruct                       | 119.31           | 17.64         | 9.05           | 1.23        | 0.69         | 202.07      |\\n\\nFigure 6: We organize games based on the length (number of tokens) of their play-by-play descriptions, with the x-axis showing the games and the y-axis the deviation scores from various LLMs, where lower scores indicate better performance. GPT-3.5-Turbo-1106 and Gemini-Pro stand out here, maintaining nearly flat curves.\\n\\nIn Table 3, we present NFL data findings. American football's play-by-plays have demonstrated a sequential nature, we cannot apply tests like New Rule, Swap, or Shuffle as with basketball games. Instead, we measure how model predictions deviate from actual scores on key game statistics, including yards (\\\\(\\\\Delta\\\\)Yards), attempts (\\\\(\\\\Delta\\\\)ATT), completions (\\\\(\\\\Delta\\\\)COMP), touchdowns (\\\\(\\\\Delta\\\\)TD), and interceptions (\\\\(\\\\Delta\\\\)INT). We also combine them into Passing Efficiency (\\\\(\\\\Delta\\\\)PE) for a holistic game analysis. Our results suggest that long-context LLMs greatly surpass standard models, with GPT-4-1106-Preview taking the lead, followed by Claude-2.1 and GPT-3.5-Turbo-1106.\\n\\nParticularly, passing yards are vital in the NFL games, often leading to scoring opportunities like touchdowns and field goals. On average, NFL teams average 200 to 250 passing yards per game. We find that the top model, GPT-4-1106-Preview, exhibits a 34.77-yard discrepancy in passing yards prediction, while the open-source Llama-2-13B-Chat lags significantly in comparison. This highlights the difficulty of tracking passing yards, a task even more challenging than summarizing basketball points, with most models struggling to accurately aggregate such data.\"}"}
{"id": "acl-2024-long-17", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Probability and Game Score Deviation\\n\\n| Probability | Game Score Deviation |\\n|-------------|----------------------|\\n| 0.5         | 1                    |\\n| 0.2         | 2                    |\\n| 0.0         | 3                    |\\n| 0.0         | 4                    |\\n\\n### Number of Swapped Players vs. Game Score Deviation\\n\\n| Number of Swapped Players | Game Score Deviation |\\n|---------------------------|----------------------|\\n| 10                        | 1                    |\\n| 20                        | 2                    |\\n| 30                        | 3                    |\\n| 40                        | 4                    |\\n| 50                        | 5                    |\\n\\n### Number of Replaced Players vs. Game Score Deviation\\n\\n| Number of Replaced Players | Game Score Deviation |\\n|---------------------------|----------------------|\\n| 50                        | 1                    |\\n| 100                       | 2                    |\\n| 150                       | 3                    |\\n| 200                       | 4                    |\\n| 250                       | 5                    |\\n\\n### Figure 7: LETF\\n\\nWe adjust the difficulty of identifying scoring events by either removing or duplicating non-scoring events. Moreover, we randomly swapped $n$ players' affiliations in the team-player table and replaced $n$ players' names with science fiction characters, all without changing the play-by-play texts.\\n\\n### Figure 8: Accuracy of Various LLMs\\n\\nWe assess the accuracy of various LLMs in completing missing key statistics from basketball game recaps. Claude-2.1 shows strong performance, while Mistral-7B-Instruct achieves the highest accuracy among standard LLMs. Our results suggest that the difference in performance between GPT-3.5-Turbo-1106 and GPT-4 across basketball and football games stems from the scoring frequency in each sport. Basketball's frequent scoring presents a challenge for GPT-4-1106-Preview to track all actions, while football, with less frequent scoring, is somewhat easier for the model to track. GPT-4-1106-Preview is optimized for handling extremely long contexts and is less accurate in tracking frequent scoring. This distinct characteristic accounts for the varied performance of both models.\\n\\nIn Figure 7, we test LLMs' robustness against adversarial conditions. In the left subfigure, we vary the difficulty of identifying scoring events by either dropping or duplicating non-scoring events. E.g., at probability $p = -0.5$, we eliminate any non-scoring event with a 50% chance; at $p = 0.2$, we duplicate any non-scoring event with a 20% chance, before shuffling the entire game description. The y-axis measures the deviation from the actual box score, with smaller values indicating better model performance. We observe that GPT-3.5-Turbo-1106 and Gemini-Pro perform the best, whose curves are quite flat, indicating their robustness to a varying level of noise in the play-by-plays. Overall, LLMs perform well when non-scoring events are removed, yet their performance drops as more non-scoring events are added, akin to searching for a needle in a larger haystack.\\n\\nFurther, we randomly swapped $n$ players' affiliations in the team-player table and replaced $n$ players' names with science fiction characters, all without changing the play-by-play texts. Our findings are shown in the middle and right subfigures. We find that Claude-2.1, Gemini-Pro, and GPT-3.5-Turbo-1106 are the top performers. Interestingly, renaming players significantly decreases all models' performance. This suggests LLMs may use familiar basketball player names from their pre-training to guess team scores, rather than analyzing the actual play-by-plays. GPT-4-1106-Preview is the least adaptable to these adversarial conditions among the long-context LLMs. We also observe a notable performance disparity exists between open-source and proprietary LLMs.\\n\\nWe assess the accuracy of various LLMs in completing missing key statistics from basketball game recaps. The types of missing data include a player's total points, team scores, assists, rebounds, and other stats. An LLM must understand the recap's context to precisely estimate the missing statistic. To do this, LLMs create a JSON object as its working memory. They then calculate the needed statistics using play-by-play and team-player data and use this memory object to fill in the blanks.\\n\\nFigure 8 presents the results of this task. Claude-2.1 shows strong performance, while Mistral-7B-Instruct achieves the highest accuracy among standard LLMs. Our results suggest that the difference in performance between GPT-3.5-Turbo-1106 and GPT-4 across basketball and football games stems from the scoring frequency in each sport. Basketball's frequent scoring presents a challenge for GPT-4-1106-Preview to track all actions, while football, with less frequent scoring, is somewhat easier for the model to track. GPT-4-1106-Preview is optimized for handling extremely long contexts and is less accurate in tracking frequent scoring. This distinct characteristic accounts for the varied performance of both models.\"}"}
{"id": "acl-2024-long-17", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1 shows strong performance, while Mistral-7B-Instruct achieves the highest accuracy among standard LLMs. This task requires that LLMs possess strong instruction-following capabilities to build an effective working memory. Figure 5 provides sample working memories from various LLMs. Although complex structures are possible, they increase the risk of errors when populating values. Models such as GPT-4-1106-Preview and Llama-2-13B-Chat face difficulties in creating a working memory. They hallucinate field values or fail to accurately fill fields with aggregated values from play-by-play data. By contrast, Claude-2.1's memory structure is the best in terms of efficiency, focusing on essential game statistics. Our task crucially evaluates LLMs' memory management skills when handling complex data queries.\\n\\n5 Conclusion\\n\\nWe introduce SportsMetrics, a novel benchmark designed to evaluate LLMs in sports data analytics. It assesses LLMs' numerical reasoning and fusion abilities through challenges such as new game rules, lengthy descriptions, scrambled narratives, and key stats analysis in game summaries. SportsMetrics highlights LLMs' potential in fields such as multiplayer gaming and collaborative workspaces.\\n\\n6 Limitations\\n\\nOur research focuses on NBA and NFL games, which are major sports with rich datasets. We are interested in exploring the generalizability of our findings to other sports. For example, soccer and cricket have distinct play styles and rules, which might challenge LLMs in unique ways. Our study has explored multiple adversarial scenarios, such as new game rules and scrambled game narratives. Such drastic changes might be uncommon in real-world conditions, and the models' ability to handle these scenarios might not translate to improved performance in other analytical tasks. Finally, our scoring system's effectiveness in assessing LLMs' numerical reasoning capabilities in different contexts, such as multiplayer online gaming or collaborative workspaces, remains to be validated. This study explores LLMs' potential in sports analytics. It is important to recognize these limitations when applying our findings to broader contexts.\\n\\nAcknowledgements\\n\\nWe are grateful to the reviewers for their insightful feedback, which has helped enhance the quality of our paper. This research has been partially supported by the NSF CAREER award, #2303655.\\n\\nReferences\\n\\nGriffin Adams, Emily Alsentzer, Mert Ketenci, Jason Zucker, and No\u00e9mie Elhadad. 2021. What's in a summary? laying the groundwork for advances in hospital-course summarization. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4794\u20134811, Online. Association for Computational Linguistics.\\n\\nAida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa: Towards interpretable math word problem solving with operation-based formalisms.\\n\\nRohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Slav Petrov, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, and Emily Pitler. 2023. Gemini: A family of highly capable multimodal models.\\n\\nAnthropic. 2023. Introducing Claude 2.1. https://www.anthropic.com/index/claude-2-1. Accessed on: Nov 21, 2023.\\n\\nJayetri Bardhan, Anthony Colas, Kirk Roberts, and Daisy Zhe Wang. 2022. DrugEHRQA: A question answering dataset on structured and unstructured electronic health records for medicine related queries. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pages 1083\u20131097, Marseille, France. European Language Resources Association.\\n\\nRegina Barzilay, Kathleen R. McKeown, and Michael Elhadad. 1999. Information fusion in the context of multi-document summarization. In Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics, pages 550\u2013557, College Park, Maryland, USA. Association for Computational Linguistics.\\n\\nAsma Ben Abacha, Wen-wai Yim, Yadan Fan, and Thomas Lin. 2023. An empirical study of clinical note generation from doctor-patient encounters. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2291\u20132302, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nAdithya Bhaskar, Alex Fabbri, and Greg Durrett. 2023. Prompted opinion summarization with GPT-3.5. In Findings of the Association for Computational Linguistics: ACL 2023, pages 9282\u20139300, Toronto, Canada. Association for Computational Linguistics.\\n\\nKaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction.\"}"}
{"id": "acl-2024-long-17", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"through search over statement compositions. In Find-\\nings of the Association for Computational Linguistics:\\nEMNLP 2022, pages 4871\u20134883, Abu Dhabi, United\\nArab Emirates. Association for Computational Lin-\\nguistics.\\n\\nMeriem Boubdir, Edward Kim, Beyza Ermis, Sara\\nHooker, and Marzieh Fadaee. 2023. Elo uncovered:\\nRobustness and best practices in language model eval-\\nuation.\\n\\nErik Brynjolfsson, Danielle Li, and Lindsey Raymond.\\n2023. Generative ai at work.\\n\\nAvi Caciularu, Matthew Peters, Jacob Goldberger, Ido\\nDagan, and Arman Cohan. 2023. Peek across:\\nImproving multi-document modeling via cross-\\ndocument question-answering. In Proceedings of the\\n61st Annual Meeting of the Association for Compu-\\ntational Linguistics (Volume 1: Long Papers), pages\\n1970\u20131989, Toronto, Canada. Association for Com-\\nputational Linguistics.\\n\\nPengshan Cai, Fei Liu, Adarsha Bajracharya, Joe Sills,\\nAlok Kapoor, Weisong Liu, Dan Berlowitz, David\\nLevy, Richeek Pradhan, and Hong Yu. 2022. Genera-\\ntion of patient after-visit summaries to support physi-\\ncians. In Proceedings of the 29th International Con-\\nference on Computational Linguistics, pages 6234\u2013\\n6247, Gyeongju, Republic of Korea. International\\nCommittee on Computational Linguistics.\\n\\nPengshan Cai, Zonghai Yao, Fei Liu, Dakuo Wang,\\nMeghan Reilly, Huixue Zhou, Lingxi Li, Yi Cao,\\nAlok Kapoor, Adarsha Bajracharya, Dan Berlowitz,\\nand Hong Yu. 2023. Paniniqa: Enhancing patient\\neducation through interactive question answering.\\n\\nMeng Cao, Yue Dong, and Jackie Cheung. 2022. Hal-\\n lucrated but factual! inspecting the factuality of\\nhallucinations in abstractive summarization. In Pro-\\ncedings of the 60th Annual Meeting of the Associa-\\ntion for Computational Linguistics (Volume 1: Long\\nPapers), pages 3340\u20133354, Dublin, Ireland. Associa-\\ntion for Computational Linguistics.\\n\\nZhiyu Chen, Wenhu Chen, Charese Smiley, Sameena\\nShah, Iana Borova, Dylan Langdon, Reema Moussa,\\nMatt Beane, Ting-Hao Huang, Bryan Routledge, and\\nWilliam Yang Wang. 2022. Finqa: A dataset of nu-\\nmerical reasoning over financial data.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\\nNakano, Christopher Hesse, and John Schulman.\\n2021. Training verifiers to solve math word prob-\\nlems.\\n\\nDaniel Deutsch, Rotem Dror, and Dan Roth. 2021. A\\nstatistical analysis of summarization evaluation met-\\nrics using resampling methods. Transactions of the\\nAssociation for Computational Linguistics, 9:1132\u2013\\n1146.\\n\\nAmosse Edouard, Elena Cabrio, Sara Tonelli, and Nhan\\nLe-Thanh. 2017. You\u2019ll never tweet alone: Building\\nsports match timelines from microblog posts. In Pro-\\ncedings of the International Conference Recent\\nAdvances in Natural Language Processing, RANLP\\n2017, pages 214\u2013221, Varna, Bulgaria. INCOMA\\nLtd.\\n\\nGregory Furman, Edan Toledo, Jonathan Shock, and Jan\\nBuys. 2022. A sequence modelling approach to ques-\\ntion answering in text-based games. In Proceedings\\nof the 3rd Wordplay: When Language Meets Games\\nWorkshop (Wordplay 2022), pages 44\u201358, Seattle,\\nUnited States. Association for Computational Lin-\\nguistics.\\n\\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\\nInjecting numerical reasoning skills into language\\nmodels. In Proceedings of the 58th Annual Meet-\\ning of the Association for Computational Linguistics,\\npages 946\u2013958, Online. Association for Computa-\\ntional Linguistics.\\n\\nJohn Giorgi, Luca Soldaini, Bo Wang, Gary Bader, Kyle\\nLo, Lucy Wang, and Arman Cohan. 2023. Open do-\\nmain multi-document summarization: A comprehen-\\nsive study of model brittleness under retrieval. In Find-\\nings of the Association for Computational Lin-\\nguistics: EMNLP 2023, pages 8177\u20138199, Singapore.\\nAssociation for Computational Linguistics.\\n\\nYi Gu, Shunyu Yao, Chuang Gan, Josh Tenenbaum, and\\nMo Yu. 2022. Revisiting the roles of \u201ctext\u201d in text\\ngames. In Findings of the Association for Computa-\\ntional Linguistics: EMNLP 2022, pages 6867\u20136876,\\nAbu Dhabi, United Arab Emirates. Association for\\nComputational Linguistics.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou,\\nMantas Mazeika, Dawn Song, and Jacob Steinhardt.\\n2021. Measuring massive multitask language under-\\nstanding.\\n\\nYebowen Hu, Kaiqiang Song, Sangwoo Cho, Xiaoyang\\nWang, Hassan Foroosh, and Fei Liu. 2023. Decipher-\\nPref: Analyzing influential factors in human prefer-\\ntence judgments via GPT-4. In Proceedings of the\\n2023 Conference on Empirical Methods in Natural\\nLanguage Processing, pages 8344\u20138357, Singapore.\\nAssociation for Computational Linguistics.\\n\\nKuan-Hao Huang, Chen Li, and Kai-Wei Chang. 2020.\\nGenerating sports news from live commentary: A\\nChinese dataset for sports game summarization. In\\nProceedings of the 1st Conference of the Asia-Pacific\\nChapter of the Association for Computational Lin-\\nguistics and the 10th International Joint Conference\\non Natural Language Processing, pages 609\u2013615,\\nSuzhou, China. Association for Computational Lin-\\nguistics.\\n\\nLuyang Huang, Shuyang Cao, Nikolaus Parulian, Heng\\nJi, and Lu Wang. 2021. Efficient attentions for long\\ndocument summarization. In Proceedings of the 2021\\nConference of the North American Chapter of the\\n\"}"}
{"id": "acl-2024-long-17", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sebastian Antony Joseph, Lily Chen, Jan Trienes, Han-nah Louisa G\u00f6ke, Monika Coers, Wei Xu, Byron C Wallace, and Junyi Jessy Li. 2024. Factpico: Factu-\\n\\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\\n\\nOsman Doruk Kicikoglu, Richard Bartle, Jon Chamber-\\n\\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. LongEval: Guidelines for human evaluation of\\n\\nFran\u00e7ois Lareau, Mark Dras, and Robert Dale. 2011. Detecting interesting event sequences for sports re-\\n\\nLogan Lebanoff, Bingqing Wang, Zhe Feng, and Fei Liu. 2021. Modeling endorsement for multi-document\\n\\nSha Li, Chi Han, Pengfei Yu, Carl Edwards, Manling Li, Xingyao Wang, Yi Fung, Charles Yu, Joel Tetreault, Eduard Hovy, and Heng Ji. 2023. Defining a new\\n\\nBo Liu, Yuqian Jiang, Xiaohan Zhang, Qiang Liu, Shiqi Zhang, Joydeep Biswas, and Peter Stone. 2023a. Llm+p: Empowering large language models with\\n\\nLinqing Liu, Patrick Lewis, Sebastian Riedel, and Pontus Stenetorp. 2022. Challenges in generalization in open domain question answering. In\\n\\nNelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paran-jape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language\\n\\nYixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Co-\\n\\nYixin Liu, Avi Singh, C. Daniel Freeman, John D. Co-Reyes, and Peter J. Liu. 2023d. Improving large language model fine-tuning for solving math prob-\\n\\nAngus Loten. 2023. Wendy's, Google train next-generation order taker: an AI Chatbot. https://www.wsj.com/articles/wendys-google-\\n\\nPan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. 2023. Dynamic prompt learning via policy gradient for semi-structured mathematical reasoning.\\n\\nStephanie M. Lukin, editor. 2020. Workshop on Games and Natural Language Processing. European Language Resources Association, Marseille, France.\\n\\nKevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. 2023. Mass-editing memory in a transformer.\\n\\nJack Merullo, Luke Yeh, Abram Handler, Alvin Gris-\\n\\ncorn II, Brendan O'Connor, and Mohit Iyyer. 2019. Investigating sports commentator bias within a large corpus of American football broadcasts. In\\n\\nMistralAI. 2023. Mixtral of experts. https://mistral.ai/news/mixtral-of-experts/. Accessed on: December 11, 2023.\\n\\nOpenAI. 2023. New models and developer products announced at DevDay. https://openai.com/blog/new-models-and-developer-products-announced-at-devday. Accessed on: November 6, 2023.\\n\\nArkil Patel, Satwik Bhattamishra, and Navin Goyal. 2021. Are nlp models really able to solve simple math word problems?\\n\\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019. Data-to-text generation with entity modeling. In\\n\\nhttp://www.associationforcomputationallinguistics.org\"}"}
{"id": "acl-2024-long-17", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Varshini Reddy, Rik Koncel-Kedziorski, Viet Dac Lai, Michael Krumdick, Charles Lovering, and Chris Tanner. 2024. Docfinqa: A long-context financial reasoning dataset.\\n\\nNoah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. 2023. Reflexion: Language agents with verbal reinforcement learning.\\n\\nZayne Sprague, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2022. Natural language deduction with incomplete information. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8230\u20138258, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nTheodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. 2023. Cognitive architectures for language agents.\\n\\nChris van der Lee, Emiel Krahmer, and Sander Wubben. 2017. PASS: A Dutch data-to-text system for soccer, targeted towards specific audiences. In Proceedings of the 10th International Conference on Natural Language Generation, pages 95\u2013104, Santiago de Compostela, Spain. Association for Computational Linguistics.\\n\\nDave Van Veen, Cara Van Uden, Louis Blankeimeier, Jean-Benoit Delbrouck, Asad Aali, Christian Bluethgen, Anuj Pareek, Malgorzata Polacin, Eduardo Pontes Reis, Anna Seehofnerova, Nidhi Rohatgi, Poonam Hosamani, William Collins, Neera Ahuja, Curtis P. Langlotz, Jason Hom, Sergios Gahtidis, John Pauly, and Akshay S. Chaudhari. 2023. Clinical text summarization: Adapting large language models can outperform human experts.\\n\\nAlex Wang, Richard Yuanzhe Pang, Angelica Chen, Jason Phang, and Samuel R. Bowman. 2022. SQuALITY: Building a long-document summarization dataset the hard way. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1139\u20131156, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nSam Wiseman, Stuart Shieber, and Alexander Rush. 2017. Challenges in data-to-document generation. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2253\u20132263, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nJian Xie, Kai Zhang, Jiangjie Chen, Renze Lou, and Yu Su. 2023. Adaptive chameleon or stubborn sloth: Revealing the behavior of large language models in knowledge conflicts.\\n\\nLiyan Xu, Zhenlin Su, Mo Yu, Jin Xu, Jinho D. Choi, Jie Zhou, and Fei Liu. 2024. Identifying factual inconsistency in summaries: Towards effective utilization of large language model.\\n\\nRuochen Xu, Song Wang, Yang Liu, Shuohang Wang, Yichong Xu, Dan Iter, Pengcheng He, Chenguang Zhu, and Michael Zeng. 2023. LMGQS: A large-scale dataset for query-focused summarization. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14764\u201314776, Singapore. Association for Computational Linguistics.\\n\\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023a. Tree of thoughts: Deliberate problem solving with large language models.\\n\\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. 2023b. React: Synergizing reasoning and acting in language models.\\n\\nJianmin Zhang, Jin-ge Yao, and Xiaojun Wan. 2016. Towards constructing sports news from live text commentary. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1361\u20131371, Berlin, Germany. Association for Computational Linguistics.\\n\\nYilun Zhao, Yitao Long, Hongjun Liu, Linyong Nan, Lyuhao Chen, Ryo Kamoi, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023. Docmath-eval: Evaluating numerical reasoning capabilities of llms in understanding long documents with tabular data.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena.\\n\\nFengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. Tat-qa: A question answering benchmark on a hybrid of tabular and textual content in finance.\"}"}
