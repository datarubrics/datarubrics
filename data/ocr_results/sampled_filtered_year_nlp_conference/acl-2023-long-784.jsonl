{"id": "acl-2023-long-784", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nFormulating selective information needs results in queries that implicitly specify set operations, such as intersection, union, and difference. For instance, one might search for \\\"shorebirds that are not sandpipers\\\" or \\\"science-fiction films shot in England\\\". To study the ability of retrieval systems to meet such information needs, we construct QUEST, a dataset of 3357 natural language queries with implicit set operations, that map to a set of entities corresponding to Wikipedia documents. The dataset challenges models to match multiple constraints mentioned in queries with corresponding evidence in documents and correctly perform various set operations. The dataset is constructed semi-automatically using Wikipedia category names. Queries are automatically composed from individual categories, then paraphrased and further validated for naturalness and fluency by crowdworkers. Crowdworkers also assess the relevance of entities based on their documents and highlight attribution of query constraints to spans of document text. We analyze several modern retrieval systems, finding that they often struggle on such queries. Queries involving negation and conjunction are particularly challenging and systems are further challenged with combinations of these operations.\\n\\n1 Introduction\\n\\nPeople often express their information needs with multiple preferences or constraints. Queries corresponding to such needs typically implicitly express set operations such as intersection, difference, and union. For example, a movie-goer might be looking for a science-fiction film from the 90s which does not feature aliens and a reader might be interested in a historical fiction novel set in France.\\n\\nBirds of Venezuelan Andes \u2014 Birds of Colombia\\nBirds found in the Venezuelan Andes but not in Colombia\\n\ud83d\uddf9 Fluent\\n\ud83d\uddf9 Natural\\n\ud83d\uddf9 Complete Evidence\\n\ud83d\uddf9 Definitely Relevant\\n\ud83d\uddf9 Partial Evidence\\n\ud83d\uddf9 Likely Relevant\\n(1) (3) (4) Relevance & Attribution Labeling\\n(2) Paraphrasing\\n\\nFigure 1: The dataset construction process for QUEST. First, (1) we sample Wikipedia category names and find their corresponding set of relevant entities. (2) Then, we compose a query with set operations and have this query paraphrased by crowdworkers. (3) These queries are then validated for fluency and naturalness. (4) Finally, crowdworkers mark the entities' relevance by highlighting attributable spans in their documents.\\n\\na botanist attempting to identify a species based on their recollection might search for shrubs that are evergreen and found in Panama. Further, if the set of entities that satisfy the constraints is relatively small, a reader may like to see and explore an exhaustive list of these entities. In addition, to verify and trust a system's recommendations, users benefit from being shown evidence from trusted sources (Lamm et al., 2021).\\n\\nAddressing such queries has been primarily studied in the context of question answering with structured knowledge bases (KBs), where query constraints are grounded to predefined predicates and symbolically executed. However, KBs can be incomplete and expensive to curate and maintain. Meanwhile, advances in information retrieval may enable developing systems that can address such queries without relying on structured KBs, by...\"}"}
{"id": "acl-2023-long-784", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our dataset construction process is outlined in Figure 2). While ensuring that the paraphrased queries are (At least 90% of the time based on our filtering?), systems that can make precise inferences based on relevance labels based on the entity documents, and systems that can match query constraints with corresponding evidence in documents and handle set operations implicitly specified by the query (see operations), our error analysis reveals that the conjunctive constraints in a query prove to be especially challenging for models effective at performing retrieval for queries with multiple conditions. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). These benchmarks require retrieval of a set of entities that exist as nodes in a graph structured data source at inference time, to simulate text-based retrieval. Wikipedia categories represent a broad set of natural language descriptions of entity properties and often correspond to selective trusted sources. This semi-structured data source at inference time, as a building block for our dataset construction approach, but do not allow access to this semi-structured data. To simulate text-based retrieval, we sample this semi-structured data source at inference time, to simulate text-based retrieval. This is the natural language queries from four domains, that are mapped to relatively comprehensive sets of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases as well as open-domain QA and retrieval over these bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below.\\n\\nRelated Work\\n\\nPrevious work in question answering and information retrieval has focused on QA over knowledge bases (Berant et al., 2016; Keysers et al., 2020; Gu et al., 2021, inter alia). These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the development of trusted sources. These relate to our work below. These benchmarks provide a comprehensive set of entities or documents. We highlight how categorization membership allows us to construct relatively sophisticated reasoning to determine relevance, representing annotation to the top few results of a baseline information retrieval system. Several datasets have been proposed for question answering over knowledge bases (Berant et al., 2016; Gu et al., 2019, inter alia). Also, such datasets do not focus on this comprehensive evaluation of retrieval systems. Performing well on this dataset requires systems to handle both set operations (acting on collections of entities). We evaluate several retrieval benchmarks such as MSMarco (Raffel et al., 2018) and Natural Questions (Kwiatkowski et al., 2021). Such annotation could aid the"}
{"id": "acl-2023-long-784", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Questions are optionally supplemented with logical forms. Lan et al. (2021) provide a comprehensive survey of complex KBQA datasets. Previous work has simultaneously noted that large curated KBs are incomplete (Watanabe et al., 2017). Notably, KBQA systems operate over a constrained answer schema, which limits the types of queries they can handle. Further, these schema are expensive to construct and maintain. For this reason, our work focuses on a setting where we do not assume access to a KB. We note that KBQA datasets have also been adapted to settings where a KB is incomplete or unavailable (Watanabe et al., 2017; Sun et al., 2019). This was done by either removing some subset of the data from the KB or ignoring the KB entirely. A key difference from these datasets is also that we do not focus on multi-hop reasoning over multiple documents. Instead, the relevance of an entity can be determined solely based on its document.\\n\\nOpen-Domain QA and Retrieval\\n\\nMany open-domain QA benchmarks, which consider QA over unstructured text corpora, have been proposed in prior work. Some of these, such as TREC (Craswell et al., 2020), MSMarco (Nguyen et al., 2016) and Natural Questions (Kwiatkowski et al., 2019) are constructed using \u201cfound data\u201d, using real user queries on search engines. Thakur et al. (2021) present a benchmark where they consider many such existing datasets. Datasets such as HotpotQA (Yang et al., 2018), and MultiRC (Khashabi et al., 2018) have focused on multi-hop question answering. Other work has explored e-commerce datasets (for example, (Kong et al., 2022)), but these have not been released publicly. Notably, the focus of these datasets differs from ours as we focus on queries that contain implicit set operations over exhaustive answer sets. Such queries are not well represented in existing datasets because they occur in the tail of the query distributions considered.\\n\\nMulti-Answer Retrieval\\n\\nRelated work (Min et al., 2021; Amouyal et al., 2022) also studies the problem of multi-answer retrieval, where systems are required to predict multiple distinct answers for a query. Min et al. (2021) adapt existing datasets (for example, WebQuestionsSP (Yih et al., 2016)) to study this setting and propose a new metric, MRecall@K, to evaluate exhaustive recall of multiple answers. We also consider the problem of multi-answer set retrieval, but consider queries that implicitly contain set constraints. In concurrent work, RomQA (Zhong et al., 2022) proposes an open-domain QA dataset, focusing on combinations of constraints extracted from Wiki-data. RomQA shares our motivation to enable answering queries with multiple constraints, which have possibly large answer sets. To make attribution to evidence feasible without human annotation, RomQA focuses on questions whose component constraints can be verified from single entity-linked sentences from Wikipedia abstracts, annotated with relations automatically through distant supervision, with high precision but possibly low recall (T-Rex corpus). In QUEST, we broaden the scope of query-evidence matching operations by allowing for attribution through more global, document-level inference. To make human annotation for attribution feasible, we limit the answer set size and the evidence for an answer to a single document.\\n\\n3 Dataset Generation\\n\\nQUEST consists of 3357 queries paired with up to 20 corresponding entities. Each entity has an associated document derived from its Wikipedia page. The dataset is divided into 1307 queries for training, 323 for validation, and 1727 for testing. The task for a system is to return the correct set of entities for a given query. Additionally, as the collection contains 325,505 entities, the task requires retrieval systems that can scale efficiently. We do not allow systems to access additional information outside of the text descriptions of entities at inference time. Category labels are omitted from all entity documents.\\n\\n3.1 Atomic Queries\\n\\nThe base atomic queries (i.e., queries without any introduced set operations) in our dataset are derived from Wikipedia category names. These are hand-curated natural language labels assigned to groups of related documents in Wikipedia. Category assignments to documents allow us to automatically determine the set of answer entities for queries with high precision and relatively high recall. We compute transitive closures of all relevant categories to determine their answer sets. However, repurposing these categories for constructing queries poses challenges: 1) lack of evidence or relations in an accompanying knowledge base.\\n\\n2 We use the Wikipedia version from 06/01/2022.\\n\\n3 Note that these category labels can sometimes be conjunctive themselves, potentially increasing complexity.\"}"}
{"id": "acl-2023-long-784", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Templates used for construction of queries with set operations and examples from the four domains considered, along with the count of examples per each domain and template.\\n\\n| Domain | Template | Example | Num. Queries |\\n|--------|----------|---------|--------------|\\n| Films  | $A \\\\cap B$ | Italian crime films set in the 1970's | 143 |\\n| Books  | $A \\\\cup B$ | 2010s drama films shot in Cleveland | 124 |\\n| Animals| $A \\\\cap B$ | Neogene mammals of Africa that are odd-toed ungulates | 111 |\\n| Plants | $A \\\\setminus B$ | Plants from the Northwestern US that can't be found in Canada | 61 |\\n\\n---\\n\\nWe select four domains to represent some diversity in queries: films, books, animals and plants. Focusing on four rather than all possible domains enables higher quality control. The former two model a general search scenario, while the latter two model a scientific search scenario.\\n\\n3.2 Introducing set operations\\n\\nTo construct queries with set operations, we define templates that represent plausible combinations of atomic queries. Denoting atomic queries as $A$, $B$ and $C$, our templates and corresponding examples from different domains are listed in Table 1. Templates were constructed by composing three basic set operations (intersection, union and difference). They were chosen to ensure unambiguous interpretations of resulting queries by omitting those combinations of set operations that are non-associative.\\n\\nBelow we describe the logic behind sampling atomic queries (i.e., $A$, $B$, $C$) for composing complex queries, with different set operations. In all cases, we ensure that answer sets contain between 2-20 entities so that crowdsourcing relevance judgments is feasible. We sample 200 queries per template and domain, for a total of 4200 initial queries.\\n\\nThe dataset is split into train + validation (80-20 split) and testing equally. In each of these sets, we sampled an equal number of queries per template.\\n\\n**Intersection.** The intersection operation for a template $A \\\\cap B$ is particularly interesting and potentially challenging when both $A$ and $B$ have large answer sets but their intersection is small. We require the minimum answer set sizes of each $A$ and $B$ to be fairly large (>50 entities), while their intersection to be small (2-20 entities).\\n\\n**Difference.** Similar to intersection, we require the answer sets for both $A$ and $B$ to be substantial (>50 entities), but also place maximum size constraints on both $A$ (<200 entities) and $B$ (<10000 entities) as very large categories tend to suffer from recall issues in Wikipedia. We also limit the intersection of $A$ and $B$ (see reasoning in Appendix B).\\n\\n**Union.** For the union operation, we require both $A$ and $B$ to be well-represented through the entities in the answer set for their union $A \\\\cup B$. Hence, we require both $A$ and $B$ to have at least 3 entities. Further, we require their intersection to be non-zero but less than 1/3rd of their union. This is so that $A$ and $B$ are somewhat related queries.\"}"}
{"id": "acl-2023-long-784", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For all other templates that contain compositions of the above set operations, we apply the same constraints recursively. For example, for $A \\\\cap B \\\\setminus C$, we sample atomic queries $A$ and $B$ for the intersection operation, then sample $C$ based on the relationship between $A \\\\cap B$ and $C$.\\n\\n3.3 Annotation Tasks\\n\\nAutomatically generating queries based on templates results in queries that are not always fluent and coherent. Further, entities mapped to a query may not actually be relevant and don't always have attributable evidence for judging their relevance. We conduct crowdsourcing to tackle these issues. The annotation tasks aim at ensuring that 1) queries are fluent, unambiguous and contain diverse natural language logical connectives, (2) entities are verified as being relevant or non-relevant and (3) relevance judgements are attributed to document text for each relevant entity. Crowdsourcing is performed in three stages, described below. More annotation details and the annotation interfaces can be found in Appendix C.\\n\\n3.3.1 Paraphrasing\\n\\nCrowdworkers were asked to paraphrase a templatically generated query so that the paraphrased query is fluent, expresses all constraints in the original query, and clearly describes what a user could be looking for. This annotation was done by one worker per query.\\n\\n3.3.2 Validation\\n\\nThis stage is aimed at validating the queries we obtain from the paraphrasing stage. Crowdworkers were given queries from the first stage and asked to label whether the query is 1) fluent, 2) equivalent to the original templatic query in meaning, and 3) rate its naturalness (how likely it is to be issued by a real user). This annotation was done by 3 workers per query. We excluded those queries which were rated as not fluent, unnatural or having a different meaning than the original query, based on a majority vote. Based on the validation, we removed around 11% of the queries from stage 1.\\n\\n3.3.3 Relevance Labeling\\n\\nNext, crowdworkers were asked to provide relevance judgements for the automatically determined answer sets of queries. Specifically, they were given a query and associated entities/documents, and asked to label their relevance on a scale of 0-3 (definitely not relevant, likely not relevant, likely relevant, definitely relevant). They were asked to ensure that relevance should mostly be inferred from the document, but they could use some background knowledge and do minimal research. We also asked them to provide attributions for document relevance. Specifically, we ask them to first label whether the document provides sufficient evidence for the relevance of the entity (complete/partial/no). Then, for different phrases in the query (determined by the annotator), we ask them to mark sentence(s) in the document that indicate its relevance. The attribution annotation is broadly inspired by Rashkin et al. (2021). For negated constraints, we ask annotators to mark attributable sentences if they provide counter-evidence. Since this annotation was time-intensive, we collected these annotations for two domains (films and books). We found that relevance labeling was especially difficult for the plants and animals domains, as they required more specialized scientific knowledge. In our pilot study prior to larger scale data collection, we collected 3 relevance ratings from different annotators for 905 query and document pairs from the films domain. In 61.4% of cases, all 3 raters judged the document to be \u201cDefinitely relevant\u201d or \u201cLikely relevant\u201d or all 3 raters judged the document to be \u201cDefinitely not relevant\u201d or \u201cLikely not relevant\u201d. The Fleiss\u2019 kappa metric on this data was found to be $K=0.43$. We excluded all entities which were marked as likely or definitely not relevant to a query based on the document text from its answer set. Around 23.7% of query-document pairs from stage 2 were excluded.\"}"}
{"id": "acl-2023-long-784", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: We compare several systems consisting of a retriever for efficiently selecting a set of candidates from the document corpus and a document relevance classifier for determining the final predicted document set.\\n\\n3.4 Dataset Statistics\\nBasic dataset statistics are reported in Table 2. The dataset contains more entities from the films domain, because this domain is more populated in Wikipedia. The average length of queries is 8.6 words and the average document length is 452 words. Documents from the films and books domains are longer on average, as they often contain plots and storylines. Around \\\\( \\\\sim 69\\\\% \\\\) of entities have complete evidence and \\\\( \\\\sim 30\\\\% \\\\) have partial evidence. Evidence was labeled as partial when not all phrases in the query had explicit evidence in the document (i.e., they may require background knowledge or reasoning). There are on average 33.2 words attributed for each entity with the maximum attribution text span ranging up to length 1837 words. Finally, the average answer set size is 10.5 entities.\\n\\n3.5 Additional Training Examples\\nBeyond the annotated data, we generated additional synthetic examples for training. We found including such examples improved model performance, and we include these examples for the experiments in \u00a74. To generate these examples, we sample 5000 atomic queries from all domains, ensuring that they do not already appear as sub-queries in any of the queries in Q\\\\textsc{uest} and use their corresponding entities in Wikipedia as their relevant entity set.\\n\\n4 Experimental Setup\\nWe evaluate modern retrieval systems to establish baseline performances. We also perform extensive error analysis to understand patterns of model errors and the quality of the labels in Q\\\\textsc{uest}.\\n\\n4.1 Task Definition\\nWe consider a corpus, \\\\( E \\\\), that contains entities across all domains in the dataset. Each entity is accompanied with a document based on its Wikipedia page. An example in our dataset consists of a query, \\\\( x \\\\), and an annotated set of relevant entities, \\\\( y \\\\subset E \\\\). As described in \u00a73, for all examples \\\\( |y| < 20 \\\\). Our task is to develop a system that, given \\\\( E \\\\) and a query \\\\( x \\\\), predicts a set of relevant entities, \\\\( \\\\hat{y} \\\\subset E \\\\).\\n\\n4.2 Evaluation\\nOur primary evaluation metric is average \\\\( F_1 \\\\), which averages per-example \\\\( F_1 \\\\) scores. We compute \\\\( F_1 \\\\) for each example by comparing the predicted set of entities, \\\\( \\\\hat{y} \\\\), with the annotated set, \\\\( y \\\\).\\n\\n4.3 Baseline Systems\\nWe evaluated several combinations of retrievers and classifiers, as shown in Figure 3. For the retriever component, we consider a sparse BM25 retriever (Robertson et al., 2009) and a dense dual encoder retriever (denoted DE). Following Ni et al. (2022), we initialize our dual encoder from a T5 (Raffel et al., 2020) encoder and train with an in-batch sampled softmax loss (Henderson et al., 2017). Once we have a candidate set, we need to determine a set of relevant entities. To classify relevance of each candidate document for the given query, we consider a cross-attention model which consists of a T5 encoder and decoder. We train the cross-attention classifier using a binary cross-entropy loss with negative examples based on non-relevant documents in top 1,000 documents retrieved by BM25 and random non-relevant documents (similarly to Nogueira and Cho (2019)). As cross-attention classification for a large number of candidates is computationally expensive, we restrict BM25 and the dual encoder to retrieve 100 candidates which are then considered by the cross-attention classifier. As our T5-based dual encoder can only efficiently accommodate up to 512 tokens, scores from BM25 and dual encoders trained with a softmax loss are not normalized to provide relevance probabilities for documents. We found that naively applying a global threshold to these scores to produce answer sets did not perform as well as using a classifier trained with a binary cross-entropy loss to predict document relevance.\"}"}
{"id": "acl-2023-long-784", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Average Precision, Recall, and F1 of baseline systems evaluated on the test dataset.\\n\\n| Retriever | Classifier | Avg. Precision | Avg. Recall | Avg. F1  |\\n|-----------|------------|----------------|-------------|---------|\\n| BM25      | T5-Base    | 0.168          | 0.160       | 0.141   |\\n| BM25      | T5-Large   | 0.178          | 0.168       | 0.150   |\\n| T5-Large  | T5-Base    | 0.153          | 0.354       | 0.176   |\\n| T5-Large  | T5-Large   | 0.165          | 0.368       | 0.192   |\\n\\nTable 4: Average Recall and MRecall of various retrievers.\\n\\n| Retriever | Avg. Recall@20 | Avg. Recall@50 | Avg. Recall@100 | Avg. Recall@1000 | MRecall@20 | MRecall@50 | MRecall@100 | MRecall@1000 |\\n|-----------|----------------|----------------|-----------------|------------------|------------|------------|-------------|-------------|\\n| BM25      | 0.104          | 0.153          | 0.197           | 0.395            | 0.020      | 0.030      | 0.037       | 0.087       |\\n| T5-Base   | 0.255          | 0.372          | 0.455           | 0.726            | 0.045      | 0.088      | 0.127       | 0.360       |\\n| T5-Large  | 0.265          | 0.386          | 0.476           | 0.757            | 0.047      | 0.100      | 0.142       | 0.408       |\\n\\nwe truncate document text. We discuss the impact of this and alternatives in \u00a75. Further, since T5 was pre-trained on Wikipedia, we investigate the impact of memorization in Appendix D. Additional details and hyperparameter settings are in Appendix A.\\n\\n4.4 Manual Error Annotation\\n\\nFor the best overall system, we sampled errors and manually annotated 1145 query-document pairs from the validation set. For the retriever, we sampled relevant documents not included in the top-100 candidate set and non-relevant documents ranked higher than relevant ones. For the classifier, we sampled false positive and false negative errors made in the top-100 candidate set. This annotation process included judgements of document relevance (to assess agreement with the annotations in the dataset) and whether the document (and the truncated version considered by the dual encoder or classifier) contained sufficient evidence to reasonably determine relevance. We also annotated relevance for each constraint within a query. We discuss these results in \u00a75.\\n\\n5 Results and Analysis\\n\\nWe report the performance of our baseline systems on the test set in Table 3. In this section, we summarize the key findings from our analysis of these results and the error annotation described in \u00a74.4.\\n\\nDual encoders outperform BM25. As shown in Table 3, the best overall system uses a T5-Large Dual Encoder instead of BM25 for retrieval. The performance difference is even more significant when comparing recall of Dual Encoders and BM25 directly. We report average recall (average per-example recall of the full set of relevant documents) and MRecall (Min et al., 2021) (the percentage of examples where the candidate set contains all relevant documents), over various candidate set sizes in Table 4.\\n\\nRetrieval and classification are both challenging. As we consider only the top-100 candidates from the retriever, the retriever's recall@100 sets an upper bound on the recall of the overall system. Recall@100 is only 0.476 for the T5-Large Dual Encoder, and the overall recall is further reduced by the T5-Large classifier to 0.368, despite achieving only 0.165 precision. This suggests that there is room for improvement from both stages to improve overall scores. As performance improves for larger T5 sizes for both retrieval and classification, further model scaling could be beneficial.\\n\\nModels struggle with intersection and difference. We also analyzed results across different templates and domains, as shown in Table 5. Different constraints lead to varying distributions over answer set sizes and the atomic categories used. Therefore, it can be difficult to interpret differences in F1 scores across templates. Nevertheless, we found the queries with set union have the highest average F1 scores. Queries with set intersection have the lowest average F1 scores, and queries with set difference also appear to be challenging.\\n\\nTo analyze why queries with conjunction and negation are challenging, we labeled the relevance of individual query constraints (\u00a74.4), where a system incorrectly judges relevance of a non-relevant document. The results are summarized in Table 6. For a majority of false positive errors involving intersection, at least one constraint is satisfied. This could be interpreted as models incorrectly treating\"}"}
{"id": "acl-2023-long-784", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"intersection as union when determining relevance. Similarly, for a majority of examples with set difference, the negated constraint is not satisfied. This suggests that the systems are not sufficiently sensitive to negations.\\n\\n| Constraints | Retriever | Classifier |\\n|-------------|-----------|------------|\\n| A \u2229 B       | 63.5      | 83.3       |\\n| A \u2229 B \u2229 C   | 56.5      | 73.2       |\\n| A \\\\ B       | 80.3      | 81.0       |\\n| A \u2229 B \\\\ C   | 47.6      | 95.5       |\\n\\nTable 5: F1 of our strongest baseline (T5-Large DE + T5-Large Classifier) across templates and domains. There is significant headroom to improve both precision and recall. As part of our manual error analysis (\u00a74.4), we made our own judgements of relevance and measured agreement with the relevance annotations in QUEST. As this analysis focused on cases where our best system disagreed with the relevance labels in the dataset, we would expect agreement on these cases to be significantly lower than on randomly selected query-document pairs in the dataset. Therefore, it provides a focused way to judge the headroom and annotation quality of the dataset.\\n\\nFor false negative errors, we judged 91.1% of the entities to be relevant for the films and books domains, and 81.4% for plants and animals. Notably, we collected relevance labels for the films and books domains and removed some entities based on these labels, as described in \u00a73, which likely explains the higher agreement for false negatives from these domains. This indicates significant headroom for improving recall as defined by QUEST, especially for the domains where we collected relevance labels.\\n\\nFor false positive errors, we judged 28.8% of the entities to be relevant, showing a larger disagreement with the relevance labels in the dataset. This is primarily due to entities not included in the entity sets derived from the Wikipedia category taxonomy (97.7%), rather than entities removed due to relevance labeling. This is a difficult issue to fully resolve, as it is not feasible to exhaustively label relevance for all entities to correct for recall issues in the Wikipedia category taxonomy. Future work can use pooling to continually grow the set of relevant documents (Sparck Jones and Van Rijsbergen, 1975). Despite this, our analysis suggests there is significant headroom for improving precision, as we judged a large majority of the false positive predictions to be non-relevant.\\n\\nTruncating document text usually provides sufficient context. In our experiments, we truncate document text to 512 tokens for the dual encoder, and 384 tokens for the classifier to allow for the document and query to be concatenated. Based on our error analysis (\u00a74.4), out of the documents with sufficient evidence to judge relevance, evidence occurred in this truncated context 93.2% of the time for the dual encoder, and 96.1% of the time for the classifier. This may explain the relative success of this simple baseline for handling long documents.\\n\\nWe also evaluated alternative strategies but these performed worse in preliminary experiments. Future work can evaluate efficient transformer variants (Guo et al., 2022; Beltagy et al., 2020).\\n\\nConclusion\\n\\nWe present QUEST, a new benchmark of queries which contain implicit set operations with corresponding sets of relevant entity documents. Our experiments indicate that such queries present a...\"}"}
{"id": "acl-2023-long-784", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"challenge for modern retrieval systems. Future work could consider approaches that have better inductive biases for handling set operations in natural language expressions (for example, Vilnis et al. (2018)). The attributions in QUEST can be leveraged for building systems that can provide fine-grained attributions at inference time. The potential of pretrained generative LMs and multi-evidence aggregation methods to answer set-seeking selective queries, while providing attribution to sources, can also be investigated.\\n\\nLimitations\\n\\nNaturalness. Since our dataset relies on the Wikipedia category names and semi-automatically generated compositions, it does not represent an unbiased sample from a natural distribution of real search queries that contain implicit set operations. Further, we limit attention to non-ambiguous queries and do not address the additional challenges that arise due to ambiguity in real search scenarios. However, the queries in our dataset were judged to plausibly correspond to real user search needs and system improvements measured on QUEST should correlate with improvements on at least a fraction of natural search engine queries with set operations.\\n\\nRecall. We also note that because Wikipedia categories have imperfect recall of all relevant entities (that contain sufficient evidence in their documents), systems may be incorrectly penalised for predicted relevant entities assessed as false positive. We quantify this in section 5. We have also limited the trusted source for an entity to its Wikipedia document but entities with insufficient textual evidence in their documents may still be relevant. Ideally, multiple trusted sources could be taken into account and evidence could be aggregated to make relevance decisions. RomQA (Zhong et al., 2022) takes a step in this latter direction although the evidence attribution is not manually verified.\\n\\nAnswer Set Sizes. To ensure that relevance labels are correct and verifiable, we seek the help of crowdworkers. However, this meant that we needed to restrict the answer set sizes to 20 for the queries in our dataset, to make annotation feasible. On one hand, this is realistic for a search scenario because users may only be interested in a limited set of results. On the other hand, our dataset does not model a scenario where the answer set sizes are much larger.\\n\\nAcknowledgements\\n\\nWe would like to thank Isabel Kraus-Liang, Mahesh Maddinala, Andrew Smith, Daphne Domansi, and all the annotators for their work. We would also like to thank Mark Yatskar, Dan Roth, Zhuyun Dai, Jianmo Ni, William Cohen, Andrew McCallum, Shib Sankar Dasgupta and Nicholas Fitzgerald for useful discussions.\\n\\nReferences\\n\\nSamuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022. Qampari:: An open-domain question answering benchmark for questions with many answers from multiple paragraphs. ArXiv, abs/2205.12665.\\n\\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020. Longformer: The long-document transformer. arXiv:2004.05150.\\n\\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. 2013. Semantic parsing on Freebase from question-answer pairs. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1533\u20131544, Seattle, Washington, USA. Association for Computational Linguistics.\\n\\nNick Craswell, Bhaskar Mitra, Emine Yilmaz, Daniel Campos, and Ellen M Voorhees. 2020. Overview of the TREC 2019 deep learning track. arXiv preprint arXiv:2003.07820.\\n\\nZhuyun Dai and Jamie Callan. 2019. Deeper text understanding for ir with contextual neural language modeling. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 985\u2013988.\\n\\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy Liang, Xifeng Yan, and Yu Su. 2021. Beyond iid: three levels of generalization for question answering on knowledge bases. In Proceedings of the Web Conference 2021, pages 3477\u20133488.\\n\\nMandy Guo, Joshua Ainslie, David Uthus, Santiago Ontanon, Jianmo Ni, Yun-Hsuan Sung, and Yinfei Yang. 2022. LongT5: Efficient text-to-text transformer for long sequences. In Findings of the Association for Computational Linguistics: NAACL 2022, pages 724\u2013736, Seattle, United States. Association for Computational Linguistics.\\n\\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-Hsuan Sung, L\u00e1szl\u00f3 Luk\u00e1cs, Ruiqi Guo, Sanjiv Kumar, Balint Miklos, and Ray Kurzweil. 2017. Efficient natural language response suggestion for smart reply. arXiv preprint arXiv:1705.00652.\"}"}
{"id": "acl-2023-long-784", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-784", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yusuke Watanabe, Bhuwan Dhingra, and Ruslan Salakhutdinov. 2017. Question answering from unstructured text by retrieval and comprehension. arXiv preprint arXiv:1703.08885.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\\n\\nWen-tau Yih, Matthew Richardson, Chris Meek, Ming-Wei Chang, and Jina Suh. 2016. The value of semantic parse labeling for knowledge base question answering. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 201\u2013206, Berlin, Germany. Association for Computational Linguistics.\\n\\nVictor Zhong, Weijia Shi, Wen-tau Yih, and Luke Zettlemoyer. 2022. RoMQA: A benchmark for robust, multi-evidence, multi-answer question answering. arXiv preprint arXiv:2210.14353.\"}"}
{"id": "acl-2023-long-784", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Experiment Details and Hyperparameters\\n\\nAll models were fine-tuned starting from T5 1.1 checkpoints. We fine-tune T5 models on 32 Cloud TPU v3 cores. Fine-tuning takes less than 8 hours for all models.\\n\\nDual Encoder. We used the t5x_retrieval library for implementing dual encoder models. We tuned some parameters based on results on the validation set. Relevant hyperparameters for training the dual encoder are:\\n\\n- Learning Rate: 1e-3\\n- Warmup Steps: 1500\\n- Finetuning Steps: 15000\\n- Batch Size: 512\\n- Max Query Length: 64\\n- Max Candidate Length: 512\\n\\nClassifier. For negative examples, we sampled 250 random non-relevant documents and sampled 250 non-relevant documents from the top-1000 documents retrieved by BM25. We also replicated each positive example 50 times. We found an approximately even number of positive and negative examples lead to better performance than training with a large class imbalance. We found a combination of random negatives and negatives from BM25 performed better than using only either individual type of negative examples. Additionally, selecting negative examples from BM25 performed better than selecting negative examples from the T5-Large dual encoder.\\n\\nFor the T5 input we concatenated the query and truncated document text. The T5 output is the string \\\"relevant\\\" or \\\"not relevant\\\". To classify document relevance at inference time, we applied a threshold to the probability assigned to the \\\"relevant\\\" label, which we tuned on the validation set. When classifying BM25 candidates we used a threshold of 0.9 and when classifying the dual encoder candidates we used a threshold of 0.95.\\n\\nOther relevant hyperparameters for training the classifier are:\\n\\n- Learning Rate: 1e-3\\n- Warmup Steps: 1000\\n- Finetuning Steps: 10000\\n- Batch Size: 1024\\n- Max Source Length: 512\\n- Max Target Length: 16\\n\\nB Set Difference and Recall\\n\\nNotation and Assumptions\\n\\nLet us assume we have two sets derived from the Wikipedia category graph, \\\\( \\\\hat{A} \\\\) and \\\\( \\\\hat{B} \\\\). The Wikipedia category graph can be missing some relevant entities, such that \\\\( \\\\hat{A} \\\\subset A \\\\) and \\\\( \\\\hat{B} \\\\subset B \\\\), where \\\\( A \\\\) and \\\\( B \\\\) are interpreted as the hypothetical sets containing all relevant entities. We quantify the degree of missing entities by denoting recall as \\\\( r_A \\\\) and \\\\( r_B \\\\), such that \\\\( |\\\\hat{A}| = r_A \\\\times |A| \\\\) and \\\\( |\\\\hat{B}| = r_B \\\\times |B| \\\\). We quantify the fraction of elements in \\\\( A \\\\) that are also in \\\\( B \\\\) as \\\\( r_{\\\\cap} \\\\), such that \\\\( |A \\\\cap B| = r_{\\\\cap} \\\\times |A| \\\\).\\n\\nFor simplicity, we also assume that the overlap between \\\\( \\\\hat{A} \\\\) and \\\\( \\\\hat{B} \\\\) is such that \\\\( |\\\\hat{A} \\\\cap B| = r_A \\\\times |A \\\\cap B| \\\\) and \\\\( |\\\\hat{A} \\\\cap \\\\hat{B}| = r_A \\\\times r_B \\\\times |A \\\\cap B| \\\\).\\n\\nDerivation\\n\\nWhat is the recall (\\\\( r \\\\)) and precision (\\\\( p \\\\)) of \\\\( \\\\hat{A} \\\\setminus \\\\hat{B} \\\\) relative to \\\\( A \\\\setminus B \\\\) as a function of \\\\( r_A \\\\), \\\\( r_B \\\\), and \\\\( r_{\\\\cap} \\\\)?\\n\\nFirst, we derive this function for recall:\\n\\n\\\\[\\n\\\\begin{align*}\\n  r_A &= \\\\frac{|(A \\\\setminus B) \\\\cap (\\\\hat{A} \\\\setminus \\\\hat{B})|}{|A \\\\setminus B|} \\\\\\\\\\n  &= \\\\frac{|\\\\hat{A} \\\\cap B| - r_{\\\\cap} \\\\times |A|}{|A| - (r_{\\\\cap} \\\\times |A|)} \\\\\\\\\\n  &= r_A \\\\left(1 - \\\\frac{r_{\\\\cap} \\\\times |A|}{|A|}\\right) \\\\\\\\\\n  &= r_A \\\\left(1 - \\\\frac{r_{\\\\cap}}{1}\\right) \\\\\\\\\\n  &= r_A \\\\times (1 - r_{\\\\cap}) \\\\times |A| \\\\\\\\\\n  &= r_A \\\\times (1 - r_{\\\\cap})\\n\\\\end{align*}\\n\\\\]\\n\\nAnd for precision:\\n\\n\\\\[\\n\\\\begin{align*}\\n  p &= \\\\frac{|(A \\\\setminus B) \\\\cap (\\\\hat{A} \\\\setminus \\\\hat{B})|}{|\\\\hat{A} \\\\setminus \\\\hat{B}|} \\\\\\\\\\n  &= \\\\frac{|\\\\hat{A} \\\\cap B| - r_{\\\\cap} \\\\times |A|}{|\\\\hat{A}| - (r_{\\\\cap} \\\\times |A|)} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - (r_{\\\\cap} \\\\times |A|)} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|} \\\\\\\\\\n  &= \\\\frac{r_A \\\\times |A| - r_{\\\\cap} \\\\times |A|}{|A| - r_A \\\\times |A|}\\n\\\\end{align*}\\n\\\\]\"}"}
{"id": "acl-2023-long-784", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ p = |( \\\\hat{A} \\\\setminus B)| - |( \\\\hat{A} \\\\setminus \\\\hat{B})| \\\\]\\n\\\\[ p = |\\\\hat{A}| - |\\\\hat{A} \\\\cap B| - |\\\\hat{A}| - |\\\\hat{A} \\\\cap \\\\hat{B}| \\\\]\\n\\\\[ p = r_A^*|A| - r_A^*r \\\\cap^*|A| \\\\]\\n\\\\[ p = r_A^*|A| - r_A^*r_B^*r \\\\cap^*|A| \\\\]\\n\\\\[ p = (1 - r \\\\cap) \\\\}\\n\\\\]\\n\\nDiscussion\\nWhile recall is simply equal to \\\\( r_A \\\\), precision is a more complicated function of \\\\( r_B \\\\) and \\\\( r \\\\cap \\\\), and can be very low for large values of \\\\( r \\\\cap \\\\). Intuitively, if subtracting \\\\( \\\\hat{B} \\\\) from \\\\( \\\\hat{A} \\\\) removes most of \\\\( \\\\hat{A} \\\\), then the precision of the resulting set will be dominated by the relevant entities missing from \\\\( \\\\hat{B} \\\\). This motivates limiting the intersection of the two sets used to construct queries involving set intersection. For example, if \\\\( r_B = 0.95 \\\\), then with \\\\( r \\\\cap < 0.8 \\\\), we can ensure \\\\( p > 0.83 \\\\).\\n\\nC Annotation Details\\nThe annotation tasks in QUEST were carried out by participants who were paid contractors. They are based in Austin, TX and either have a bachelor\u2019s degree (55%) or equivalent work experience (45%). They were paid by the hour for their work and were recruited from a vendor who screened them for knowledge of US English. They were informed of how their work would be used and could opt out. They received a standard contracted wage, which complies with living wage laws in their country of employment. The annotation interfaces presented to the annotators are shown in Figures 4, 5 and 6.\\n\\nD Impact of Memorization of Pre-training Data\\nSince the T5 checkpoints we use to initialize our models were pre-trained on the C4 corpus (which includes Wikipedia), we investigate whether these models have memorized aspects of the Wikipedia category graph. We compare recall of the T5-based dual encoder model for Wikipedia documents that were created prior to the pre-training date of the T5 checkpoint compared with documents that were added after pre-training. We report these in Table 7, along with the recalls for the same sets of documents with a BM25 retriever, for a baseline.\\n\\n| Retriever | Before | After |\\n|-----------|--------|-------|\\n| BM25      | 0.183  | 0.050 |\\n| T5-Large DE | 0.466 | 0.171 |\\n\\nTable 7: Average recall@100 on the subsets of documents created before vs after T5 pre-training. We note that the ratio of scores between the documents added before pre-training to documents added after pre-training is similar for both systems, which suggests factors other than memorization may explain the difference. For example, the documents created before vs. after the pre-training date have average lengths of 759.7 vs. 441.2 words, respectively.\"}"}
{"id": "acl-2023-long-784", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":null}"}
{"id": "acl-2023-long-784", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\n\\nNot applicable. We did not identify any risks.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\n\\nLeft blank.\\n\\nB Did you use or create scientific artifacts?\\n\\nWe will release the code and our dataset publicly.\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n\\nAppendix A\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\nYes, we will use an MIT license to release our dataset.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nAppendix A\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-784", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 4 and Appendix A\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nAppendix A\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix A\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nAppendix C\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nAppendix C\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nAppendix C\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nAppendix C\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nLeft blank.\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nAppendix C\"}"}
