{"id": "emnlp-2022-main-628", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CN-AutoMIC: Distilling Chinese Commonsense Knowledge from Pretrained Language Models\\n\\nChenhao Wang\u00b9,\u00b2, Jiachun Li\u00b9,\u00b2, Yubo Chen\u00b9,\u00b2, Kang Liu\u00b9,\u00b2,\u00b3 and Jun Zhao\u00b9,\u00b2\\n\\n\u00b9National Laboratory of Pattern Recognition, Institute of Automation, CAS, Beijing, China\\n\u00b2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\\n\u00b3Beijing Academy of Artificial Intelligence, Beijing, China\\n\\n{chenhao.wang,jiachun.li,yubo.chen,kliu,jzhao}@nlpr.ia.ac.cn\\n\\nAbstract\\n\\nCommonsense knowledge graphs (CKGs) are increasingly applied in various natural language processing tasks. However, most existing CKGs are limited to English, which hinders related research in non-English languages. Meanwhile, directly generating commonsense knowledge from pretrained language models has recently received attention, yet it has not been explored in non-English languages. In this paper, we propose a large-scale Chinese CKG generated from multilingual PLMs, named as CN-AutoMIC, aiming to fill the research gap of non-English CKGs. To improve the efficiency, we propose generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we develop cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping iteration process to reuse generated results. Finally, we conduct detailed analyses on CN-AutoMIC from different aspects. Empirical results show the proposed CKG has high quality and diversity, surpassing the direct translation version of similar English CKGs. We also find some interesting deficiency patterns and differences between relations, which reveal pending problems in commonsense knowledge generation. We share the resources and related models for further study.\\n\\n1 Introduction\\n\\nCompiling large-scale commonsense knowledge resources is a long-term goal of the AI community. Recent efforts focus on constructing commonsense knowledge graphs (CKGs) via manually compiling (Speer et al., 2017; Sap et al., 2019; Mostafazadeh et al., 2020) or automatic extraction (Tandon et al., 2014; Romero et al., 2019; Zhang et al., 2020; Nguyen et al., 2021). These projects have shown benefits for a wide range of downstream tasks (Lin et al., 2019; Tian et al., 2020; Ammanabrolu et al., 2021).\\n\\nHowever, most CKG projects are limited to English, which hinders further research in other languages. To go beyond such an English-centric trend in commonsense knowledge research, there are some challenging issues. First, directly translating English CKGs is not enough. For example, (PersonX is a little girl, xWant, to ask Christmas presents) is a triple from a CKG crowdsourced by English users (Sap et al., 2019), but it is not common in non-Christian cultures. In fact, such resources reflect only the commonsense perspectives of English-speaking communities. The translation will omit the cultural differences in other languages, and even implicitly exacerbate the English-centric bias (Mehrabi et al., 2021). Therefore, when creating CKGs for new languages, it is better to rely on native speakers and corpora. Second, current common practices in English CKG construction, i.e. manually compiling and automatic extraction, are difficult to generalize. For manually compiling, creating human-authored CKGs for each new language is very expensive. For automatic extraction, current pipelines rely on English-specific hand-craft patterns or language processing tools, which are not available to other languages. Therefore, when creating CKGs for new languages, the cost and availability of construction scheme should also be concerned.\\n\\nRecent work reveals pretrained language models (PLMs) can be a new source to generate commonsense knowledge (Bosselut et al., 2019; Nguyen and Razniewski, 2022). The latest research indicates the CKG built from huge PLMs (e.g. GPT-3) can surpass the crowdsourced ones in quantity and quality (West et al., 2021), and only a small amount of human-authored data are required for prompting and filtering. Interestingly, we find this way could be the ideal start point to construct CKGs for new languages, since PLMs can be trained on the corpora of target languages, and the generation process is low-cost and independent of language.\"}"}
{"id": "emnlp-2022-main-628", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"specific tools. However, up to now, work in this thread has not extended to non-English languages. The main challenge of this paradigm is that the generation quality and diversity are often conflicting and difficult to control. To sample diverse results, the generation should be run extensive times, and a large number of generated results are invalid and need to be filtered out. For new languages, as there is often no comparable PLM to GPT-3 (Brown et al., 2020) in English, the generated results will be even noisier. Therefore, we need to reduce unnecessary generation to increase the efficiency and take measures to ensure the quality.\\n\\nIn this paper, we propose a framework to distill commonsense knowledge from multilingual pre-trained language models. To increase the generation efficiency, we propose a generate-by-category strategy to reduce invalid generation. To ensure the filtering quality, we propose cascaded filters to discard low-quality results. To further increase the diversity and density, we introduce a bootstrapping process. Based on the framework, we propose a large-scale Chinese commonsense knowledge graph, \\\\textbf{CN-AutoMIC} (Automatically Obtained Machine Commonsense). To the best of our knowledge, it is the first non-English CKG built from pretrained language models. The empirical results show the proposed CKG has high quality and diversity. We also discuss some interesting deficiencies that need further solutions. We summarize the contribution as follows.\\n\\n\u2022 We propose a framework to distill commonsense knowledge from multilingual PLMs, incorporating several novel strategies to improve the generation efficiency and quality.\\n\u2022 We propose the first large-scale Chinese commonsense knowledge graph built with large-sized PLMs, \\\\textbf{CN-AutoMIC}. Its high-quality subset contains 1.1M triples with an accuracy of 87.2%. Besides the CKG, we also release small-sized commonsense models trained on it, named as \\\\textbf{CN-COMET}.\\n\u2022 We conduct comprehensive evaluation and analysis for \\\\textbf{CN-AutoMIC} and \\\\textbf{CN-COMET}. Besides verifying the quality and diversity, we also get some valuable observations about the deficiencies of generation. Considering generating commonsense knowledge with PLMs is still not fully explored, our findings can provide more insights into this topic.\\n\\n\\\\begin{figure}[h]\\n\\\\centering\\n\\\\includegraphics[width=\\\\textwidth]{figure1.png}\\n\\\\caption{The construction demonstration of \\\\textbf{CN-AutoMIC}. The dashed nodes the relations are generated and filtered with pretrained language models.}\\n\\\\end{figure}\\n\\n2 Related Work\\n\\n2.1 Commonsense Knowledge Graphs\\n\\nAfter some pioneers of strict logic formalization (Lenat et al., 1990), most recent commonsense knowledge resources, also known as commonsense knowledge graphs, represent commonsense knowledge in loosely-structured (head, relation, tail) triples, where the head and tail can be concepts or situations described in free-form phrases, and the relation can be various commonsense dimensions. Some of such resources are constructed by manually compiling or crowdsourcing (Speer et al., 2017; Sap et al., 2019; Hwang et al., 2021; Mostafazadeh et al., 2020). The others are mainly mined from corpora with human-crafted patterns or language processing tools (Tandon et al., 2014; Romero et al., 2019; Zhang et al., 2020; Fang et al., 2021; Nguyen et al., 2021).\\n\\nUnfortunately, most of these projects are limited in English. Among the mainstream CKGs, ConceptNet (Speer et al., 2017) is the only multilingual one. It supports 10 core languages and more common languages. However, most of its non-English parts are taxonomic or lexical knowledge. The rest parts are limited in quantity and coverage. A remarkable recent work of Chinese commonsense knowledge resources is \\\\textbf{C$_3$KG} (Li et al., 2022). It is based on the translation of ATOMIC (Sap et al., 2019; Hwang et al., 2021), which may fail to capture the cultural differences. Therefore, our work can be a strong complement to them.\"}"}
{"id": "emnlp-2022-main-628", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Extracting Knowledge from PLMs\\n\\nSince PLMs have seen a great number of documents and latently learned associations among concepts, there are extensive efforts to probe or extract relational knowledge from PLMs (Petroni et al., 2019; Sung et al., 2021; AlKhamissi et al., 2022). As for commonsense knowledge, some earlier work has tried to automatically complete CKGs by fine-tuning PLMs (Bosselut et al., 2019; Guo et al., 2020; Hwang et al., 2021), which still needs a large number of existing triples as training data. Recent research demonstrates that through natural language prompting, PLMs can adapt to generate commonsense knowledge under the few-shot setting (Da et al., 2021), or directly act as triple scorers without training (Davison et al., 2019). A significant progress is made by West et al. (2021). They use GPT-3 to generate a CKG from scratch. During the construction, only a small amount of human-authored data are required for in-context prompting generation and filtering results. They show the resulting CKG surpasses human-authored ATOMIC in quantity, quality, and diversity. Compared with West et al. (2021), our work proposes more improvement in generation and filtering and brings more comprehensive analysis for the paradigm from a non-English perspective.\\n\\n3 Construction of CN-AutoMIC\\n\\nFor clarity, in this section, we first show the overview of the construction task, then describe the construction process of CN-AutoMIC in detail.\\n\\n3.1 Overview\\n\\nWe expect to obtain commonsense knowledge represented in \\\\((\\\\text{head}, \\\\text{relation}, \\\\text{tail})\\\\) triples via prompting generation. The demonstration of construction is shown in Figure 1. Similar to the construction workflow of crowdsourced CKGs, we hope to first collect head items (Section 3.2) and then collect tail items according to several pre-defined relations (Section 3.3). We limit the head items to the description of eventualities (events, activities and states) (Bach, 1986), such as \\\"\u67d0\u4ebaX\u79bb\u5f00\u5bb6 (PersonX leaves home)\\\". Following West et al. (2021), we consider seven relation types about eventualities from ATOMIC, which are listed in Table 2. Since the raw generated results are mixed with noise and degeneration, we introduce human supervision to train filter models to distinguish high-quality results (Section 3.4). To reuse the generated tails and increase the density, we propose a bootstrapping iteration process (Section 3.5).\\n\\n3.2 Generating Head Items\\n\\nWe start from a minor size of head item seeds, using them as examples to prompt PLMs to generate more head items in the same format. Notably, although previous work (West et al., 2021) treats all head items without distinction and collects knowledge about them for all predefined relations, we argue that it is necessary to further subdivide head items into different categories, because some heads and relations are in conflict and they cannot produce valid results. For example, we cannot infer the intent of X \\\\((x\\\\text{Intent})\\\\) in \\\"\u67d0\u4ebaX\u53d7\u5230\u653b\u51fb (PersonX is attacked)\\\" because he is passively involved in it rather than intentionally causes it. Therefore, we divide the head items in three categories \\\\((\\\\text{voluntary occurrences}, \\\\text{involuntary occurrences} \\\\text{and states})\\\\) and match them with different relations, as illustrated in Table 1. Note that these categories are not strict, but they can hopefully reduce invalid generation. Then, we collect head item seeds for the three categories. We mainly sample the high-quality head items from ATOMIC, manually categorizing and translating them. We intentionally discard the instances that are English-specific or rare in Chinese context, such as \\\"PersonX has a baby shower\\\". In total, we collect 100, 50, and 45 seeds for voluntary occurrences, involuntary occurrences and states, respectively. Through pilot studies, we empirically choose mT5-XXL (13B parameters) (Xue et al., 2021) for generation. It is one of the biggest publicly available multilingual PLMs, covering 101 languages, but still 13x smaller than GPT-3 used in West et al. (2021). During the generation, we use the prompt shown in Figure 2. For each generation cycle, we sample 10 examples from the seeds to construct the prompt, and use nucleus sampling with \\\\(p = 0.9\\\\) to decode 100 results. The generation cycles for different head categories are independently conducted. After generating 600K raw results (2,000 cycles for each category), we discard 30% results of high negative log-likelihood and merge the duplicates in the remaining part, resulting in 100K head items.\\n\\n\\\\[\\\\text{Note that mT5 has an additional multilingual advantage. It can conduct text-infilling generation for any position the special token <extra_id_0> appears. This is helpful for some languages (e.g. SOV languages) which cannot guarantee that the content to be generate is at the end of the prompt.}\\\\]\"}"}
{"id": "emnlp-2022-main-628", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Three categories of the head items:\\n\\n| Category       | Examples                                      |\\n|----------------|-----------------------------------------------|\\n| Voluntary      | \u67d0\u4ebaX\u79df\u623f\u5b50;\u67d0\u4ebaX\u5b66\u5f00\u8f66;\u67d0\u4ebaX\u5938\u8d5e\u67d0\u4ebaY |\\n| Involuntary    | \u67d0\u4ebaX\u53d7\u53d7\u5230\u653b\u51fb;\u67d0\u4ebaX\u7761\u8fc7\u8fc7\u5934;\u67d0\u4ebaX\u6536\u5230\u67d0\u4ebaY\u7684\u6765\u4fe1 |\\n| States         | \u67d0\u4ebaX\u5f88\u75b2\u60eb;\u67d0\u4ebaX\u5934\u6655;\u67d0\u4ebaX\u8ba4\u8bc6\u67d0\u4ebaY |\\n\\n### Table 2: The relations and their verbalizing templates.\\n\\nThe words in brackets are placeholders and will be replaced according to the head and tail.\\n\\n| Relation Description / Verbalizing Templates | Example  |\\n|---------------------------------------------|----------|\\n| xWant {head}, in this case, {X} wants {tail} | (1. PersonX buys a book;) |\\n| xReact {head}, in this case, {X} feels {tail} | (10. PersonX plays basketball with PersonY;) |\\n| xEffect {head}, in this case, {X} is {tail} | (11. <extra_id_0>) |\\n| xAttr {head}, in this case, {X} is {tail} | (Please write down the intent of the person, Example: ) |\\n| xNeed {head}, in this case, {X} needed {tail} | (9. Zhang calls the police, Zhang's intent is <extra_id_0>;) |\\n| xIntent {head}, in this case, {X}'s intent in it is {tail} | (Please write down the intent of the person, Example: ) |\\n| HinderedBy {head}, in this case, can be hindered by {tail} | (Please write down the intent of the person, Example: ) |\\n\\n### 3.3 Generating Triples\\n\\nTo obtain complete triples, we further generate tail items according to different relations. Similar to Section 3.2, we use example triples for prompting generation. To make effective use of the capabilities of PLMs, we need to convert triples into natural language sentences. For this sake, we use the templates in Table 2 to verbalize triples, and further replace \\\"\u67d0\u4ebaX(PersonX)\\\" placeholders with random Chinese names.\\n\\nWe continue to use mT5-XXL model for generation. With the verbalized example triples, we construct the prompt as shown in Figure 3. For each relation, we use 8 example triples, which are sampled from ATOMIC and manually translated into Chinese. The prompt is used to generate 10 tail items for each (head, relation) pair with nucleus sampling ($p = 0.7$). As said before, each head item is only paired with the valid relations according to its category, so that the invalid generation results are reduced. After converting names back to placeholders and removing duplicated triples, this step produces 5.2M raw triples in total.\\n\\n### 3.4 Filtering Results\\n\\nAnnotation\\n\\nTo train filters that can distinguish high-quality triples, we randomly sample 4000 instances from the raw generated triples and ask three native Chinese speakers to annotate them. We intentionally give three questions for each triple. The annotators should first rate the head and tail alone to indicate whether they are acceptable. If not, the options of rejecting include syntax errors, abnormal or impossible situations (e.g. \\\"\u67d0\u4ebaX\u5728\u5929\u4e0a\u6e38\u6cf3(PersonX swims in the sky)\\\") and other faults (e.g. containing real names rather than name placeholders).\"}"}
{"id": "emnlp-2022-main-628", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Heads Tails Triples\\n\\nTable 3: Annotator's acceptance for sampled raw triples.\\n\\n|        | 0.0 | 0.2 | 0.4 | 0.6 | 0.8 | 1.0 |\\n|--------|-----|-----|-----|-----|-----|-----|\\n| Recall |     |     |     |     |     |     |\\n|        | 0.6 | 0.7 | 0.8 | 0.9 | 1.0 |     |\\n| Precision Head |     |     |     |     |     |     |\\n|         |     |     |     |     |     |     |\\n| Tail    |     |     |     |     |     |     |\\n| Triple  |     |     |     |     |     |     |\\n| Triple* |     |     |     |     |     |     |\\n\\nFigure 4: The precision-recall curves of filter models.\\n\\n\\\"Head\\\", \\\"Tail\\\", \\\"Triple\\\" are the performances of head classifier, tail classifier and triple classifier respectively. \\\"Triple*\\\" is the performance of triple classifier on a subset, where the head and tail are already acceptable. If annotators have accepted the head and tail, they should rate the triple as a whole, with options for acceptability: \\\"always/often\\\", \\\"sometimes/likely\\\", \\\"far-fetched/never\\\", or \\\"invalid\\\". The former two are considered \\\"accepted\\\". The latter two are \\\"rejected\\\". Table 3 shows the acceptance rate. For the overall results, we find the fleiss's $\\\\kappa$ (Fleiss, 1971) is 0.439, which indicates moderate agreement.\\n\\nTraining\\n\\nWe use 80% of the annotated data for training, and the remaining parts for validating and testing. We train binary classifier models to predict whether the input is acceptable. Considering the acceptance differences in the annotation results, we train single-use classifiers for heads, tails, and triples, respectively. We empirically choose a Chinese version of ELECTRA (Clark et al., 2020) as the underlying model and fine-tune it for the tasks. We set learning rate to 5e-5 and batch size to 128 by grid search, and use early stopping to maximize the average precision (AP) on the validation data. We also tried other multilingual or Chinese PLMs of similar size, but the ELECTRA-based models obtain the best AP.\\n\\nCascaded Filtering\\n\\nWe report the precision-recall curves of the trained models in Figure 4, including all three classifiers. We also report the performance of triple classifier on \\\"clean data\\\", where the head and tail in the triple are acceptable (Triple*). From the curves, we can find head and tail classifiers can reach very high precision at almost all recall value. And triple classifier perform better on \\\"clean data\\\". It indicates that we can achieve better performance by cascaded filtering, i.e., applying the head and tail classifier first, and then using the triple classifier. We also find it is useful to set different thresholds for different relation types. Finally, we set the thresholds for head and tail classifier to ensure precision > 0.98 on the test data. We empirically search three groups of thresholds for the triple classifier, based on precision = 0.9, 0.8, 0.75 on each relation. We use these thresholds to get three subsets with different sizes and denote them as high/mid/low subsets.\\n\\n3.5 Bootstrapping Iteration\\n\\nWe note that although the generated tail items have different formats from the head items, many of them can be converted into head items with simple templates, as shown in Table 4. After the above steps, many of the tail items have never appeared as head items. To further increase the diversity and density, we use the high-frequency tail items from the mid subset to generate more triples. We repeat the generating and filtering process described in Section 3.3 and Section 3.4, using the trained filters. Such bootstrapping iteration can be done several times, though we only conduct it once in this work. Finally, the resulting triples from different iterations are merged. We denote the merged sets as CN-AutoMIC high/mid/low respectively.\\n\\n4 Evaluation and Analysis\\n\\nIn this section, we evaluate and analyze the resources in three parts. First, we comprehensively evaluate CN-AutoMIC in size, quality and diversity. In this step, we also evaluate the commonsense model (CN-COMET) trained on it. Second, we conduct specific analyses for different constructions...\"}"}
{"id": "emnlp-2022-main-628", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"We evaluate the quality of CN-AutoMIC by sampling 1000 triples from ATOMIC-zh and CN-AutoMIC and conducting a human evaluation. The annotation setting is similar to Section 3.4, but only the triples need to be rated. We keep the annotation results by majority vote and report the average acceptance for the triples.\\n\\n**Overall Statistics**\\n\\nThe overall statistics of CN-AutoMIC are shown in Table 5. From the results, we find:\\n\\n1. Compared with the existing translated CKG, CN-AutoMIC contains a larger size of triples with better quality, as well as more unique head items. Interestingly, even the raw generated triples have better average acceptance than the translated CKG. We speculate that is because the translated CKG has a lot of syntax and translation errors.\\n\\n2. After filtering, the human acceptance reaches up to 87.2 from 47.6, indicating the effect of the filtering process. As a trade-off, the diversity of tail items is decreased.\\n\\n3. The English ATOMIC10x is generated by GPT-3 and has better basic quality. After filtering, it can reach very high acceptance, surpassing human-authored ATOMIC2020 by 10 percent. By contrast, CN-AutoMIC struggles on reaching high acceptance, which shows the difficulty of generating non-English commonsense knowledge.\\n\\n**Relation-level Results**\\n\\nWe show the relation-level results of CN-AutoMIC in Table 6. We can find the acceptance on most of the relations is between 80 to 90 percent. However, the number of triples varies significantly. That is because we use different filter thresholds for different relations to achieve the same quality level. To some extent, the change of triple amounts reflects how much commonsense knowledge of a specific relation type exists in the PLM. According to the results, mT5 seems to be better at \\\\( x_{\\\\text{Effect}} \\\\) than \\\\( x_{\\\\text{HinderedBy}} \\\\).\\n\\n---\\n\\n**Table 6: Relation-level results of CN-AutoMIC**\\n\\n| Relation   | Unique Heads | Unique Tails | Triples | Acceptance |\\n|------------|--------------|--------------|---------|------------|\\n| xWant      | 23,673       | 179,861      | 84.8    |\\n| xReact     | 1,985        | 145,431      | 95.4    |\\n| xEffect    | 91,808       | 463,298      | 86.5    |\\n| xAttr      | 1,660        | 28,973       | 88.9    |\\n| xNeed      | 45,221       | 209,525      | 81.2    |\\n| xIntent    | 20,575       | 79,012       | 88.1    |\\n| xHinderedBy| 8,868        | 34,740       | 85.0    |\"}"}
{"id": "emnlp-2022-main-628", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: The performance of commonsense knowledge model trained on different CKGs.\\n\\n| Category         | Voluntary Occurrences | Involuntary Occurrences | States  |\\n|------------------|-----------------------|-------------------------|---------|\\n|                  | Precision             |                         |         |\\n|                  | 84%                   | 71%                     | 76%     |\\n\\nTable 8: The category precision of generated head items.\\n\\nComparison, we evaluate all these models on a held-out set of ATOMIC-zh, and manually check the results. During the evaluation, we remove the instances that have unreadable head items. The results are shown in Table 7. The model trained on CN-AutoMIC high achieves the best performance, indicating the high-quality CKG can make the small-sized model infer better commonsense knowledge. Nevertheless, the best performance is still not satisfactory. The quality of training data might be still not good enough. And the translation noise in test data could also exacerbate the difficulty.\\n\\n4.2 Analyzing the Construction Steps\\n\\nIn this section, we analyze the effect of some intermediate steps.\\n\\nCategory Precision\\n\\nAs described in Section 3.2, head items are generated with three categories of seeds, so we can reduce invalid generation according to the categories. However, the categories of generated head items are not guaranteed to be what they are generated from. Therefore, we manually check 100 generated head items for each category, and report the precision in Table 8. The results indicate that most of the head items belong to the category they are generated from. Based on this, we avoid nearly 10% invalid generation according to the category.\\n\\nEffect of Cascaded Filtering\\n\\nWe validate the effect of cascaded filtering by temporarily removing the head and tail classifiers during constructing CN-AutoMIC high. That adds 47K new triples in total. We sample 500 triples from them and conduct manually checking. About 43.2% of them are bad triples. According to the estimation, removing head and tail classifiers can make the overall acceptance drops by 1.9%.\\n\\nEffect of Bootstrapping Iteration\\n\\nIn Table 9, we report the changes of high subset after one iteration. From the results, we find the quantities of unique heads, tails, and triples have substantially increased. Also, we find the retaining rate (i.e. the proportion of triples that are retained by the filters) also increases. We conjecture that it is because the filtered triples before iteration have high-quality tail items. Converting them to head items and conducting generation can get better performance.\\n\\n4.3 Case Study\\n\\nCulture-Specific Knowledge\\n\\nSince mT5 has been trained on Chinese corpora, it may generate commonsense knowledge specific to Chinese context. We find some explicitly culture-specific knowledge triples from the high subset and list them in Table 10. These examples involve festivals, traditional practices, games, and apps that are familiar to Chinese people but not popular in English communities. Therefore they cannot be found in current English CKGs. In contrast, CN-AutoMIC can capture such commonsense knowledge in Chinese perspectives to some extent.\\n\\nDeficiency Patterns\\n\\nWe show some regular generation mistakes in Table 11. We note two interesting error types:\\n\\n1. Some head items cannot pair with some relations. Taking them as input will always result in errors. For example, \u201cPersonX kills himself\u201d cannot pair with \\\\( x \\\\text{Want}(after that, X \\\\text{wants}) \\\\), because he will lose consciousness and cannot want to do anything. Though we have set three head categories for such problems, there are still some intractable cases. The fundamental reason is that PLMs are unable to \u201creject\u201d inappropriate input. Further research is needed to avoid such\"}"}
{"id": "emnlp-2022-main-628", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PersonX spends the Mid-Autumn Festival with PersonY \u2192 xNeed \u2192 make moon cakes\\n\\nPractice PersonX is in postpartum confinement \u2192 xIntent \u2192 take good care of (her) health\\n\\nPersonX takes a day off \u2192 xWant \u2192 play mahjong\\n\\nPersonX watches videos \u2192 xNeed \u2192 open the Youku app.\\n\\nTable 10: The cases of generated commonsense knowledge triples that are specific to Chinese context.\\n\\nMid-Autumn Festival is a Chinese traditional festival.\\nPostpartum confinement (or lying-in) is a traditional custom for new mothers.\\nMahjong is a game popular among Chinese.\\nYouku is a website similar to Youtube/Netflix.\\n\\nConflict with the Relation\\nPersonX kills himself \u2192 xWant (after that x wants) \u2192 Attend the funeral\\nPersonX is knocked unconscious \u2192 xReact (after that x feels) \u2192 excited\\n\\nNegative Expression or Unfavorable Situation\\nPersonX has no house \u2192 xNeed (before that x needs) \u2192 earn money\\nPersonX's hand is aching \u2192 HinderedBy \u2192 PersonX doesn't have painkillers\\n\\nTable 11: The error cases of generated commonsense knowledge triples.\\n\\n(2) For negative expressions or unfavorable situations, the model often performs badly generation for some relations. For example, in \u201cPersonX has no house; before that, X needs\u201d, the model is trying to generate the methods to avoid the trouble (such as \u201cearning money\u201d), rather than the reason that makes X get in the trouble. This might be due to the ambiguity in the natural language prompts.\\n\\n5 Discussion\\nWhat is the upper-bound size of the generation?\\nThe PLMs can conduct ever-lasting generation, but it seems there is a soft upper bound. The results generated later are easy to repeat previous results. Therefore, the cost of novel results would gradually increase and eventually become unaffordable. In this work, we have generated tens of millions of triples. It has still not reached the limit, but similar or repeated content has appeared in large numbers. For example, during generating head phrases, we observe that the proportion of non-repetitive results keeps descending (Figure 5).\\n\\nIf PLMs have already learned commonsense knowledge, why is it necessary to extract the knowledge from it?\\nFirst, according to the results in this paper, the quality of direct generated results is still poor. It indicates that even for a model with 13B+ parameters, the learned knowledge is still rough and full of mistakes. A distilling process can distinguish the clean knowledge and benefit smaller applicable models. Besides, recent work shows that even though explicitly training PLMs with knowledge, there is no guarantee that they can actually use such knowledge in target tasks (Wang et al., 2021). Therefore, we can exploit explicit symbolic knowledge as auxiliary information.\"}"}
{"id": "emnlp-2022-main-628", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion\\n\\nConsidering the dilemma of lacking non-English commonsense knowledge resources, in this paper, we propose CN-AutoMIC, the first Chinese commonsense knowledge graph that is totally generated by pretrained language models. During the construction, we use prompting generation to obtain head and tail items, as well as introduce categorized generation, cascaded filtering and bootstrap iteration to improve the quantity, quality and diversity. Through human evaluation, the resource is shown to have better quality than directly translated resources from English language. We discuss the culture-related phenomena and common deficiency patterns in the generated knowledge graph.\\n\\nAlthough our work is limited to Chinese, the basic framework and methods can be used to populate CKGs in more languages.\\n\\nLimitations\\n\\nThe main limitations of this work include:\\n\\n(1) We require large-scale pretrained models. The generation performance is strongly dependent on the size of models. We use 4 RTX-A6000 GPUs for running the T5-XXL model.\\n\\n(2) Due to the lack of large-sized PLMs, the quantity and quality of this generated Chinese CKG still fall behind similar English resources.\\n\\n(3) We still cannot interpret the behavior of large PLMs. The specific source of the generated commonsense knowledge is hard to locate, and there are potential ethical risks since the results are not completely checked.\\n\\n(4) We still require extra human labor when applying the method to each new language. Although basically our methods and underlying models (mT5) can generalize for other languages, we still need human-crafted prompts and a minor size of annotations for training filter models.\\n\\nAcknowledgements\\n\\nThis work is supported by the National Key Research and Development Program of China (No. 2020AAA0106400), and the National Natural Science Foundation of China (No. 61922085, 61976211, 62176257). This work is also supported by the Strategic Priority Research Program of Chinese Academy of Sciences (Grant No. XDA27020200), the Youth Innovation Promotion Association CAS, and Yunnan Provincial Major Science and Technology Special Plan Projects (No.202202AD080004).\\n\\nReferences\\n\\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona Diab, and Marjan Ghazvininejad. 2022. A review on language models as knowledge bases. arXiv preprint arXiv:2204.06031.\\n\\nPrithviraj Ammanabrolu, Wesley Cheung, William Broniec, and Mark O Riedl. 2021. Automated storytelling via causal, commonsense plot ordering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 5859\u20135867.\\n\\nEmmon Bach. 1986. The algebra of events. Linguistics and philosophy, pages 5\u201316.\\n\\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli Celikyilmaz, and Yejin Choi. 2019. COMET: Commonsense transformers for automatic knowledge graph construction. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4762\u20134779, Florence, Italy. Association for Computational Linguistics.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs]. ArXiv: 2005.14165.\\n\\nKevin Clark, Minh-Thang Luong, Quoc V. Le, and Christopher D. Manning. 2020. Electra: Pre-training text encoders as discriminators rather than generators. In International Conference on Learning Representations.\\n\\nJeff Da, Ronan Le Bras, Ximing Lu, Yejin Choi, and Antoine Bosselut. 2021. Analyzing commonsense emergence in few-shot knowledge models. In 3rd Conference on Automated Knowledge Base Construction.\\n\\nJoe Davison, Joshua Feldman, and Alexander Rush. 2019. Commonsense knowledge mining from pretrained models. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1173\u20131178, Hong Kong, China. Association for Computational Linguistics.\\n\\nTianqing Fang, Hongming Zhang, Weiqi Wang, Yangqiu Song, and Bin He. 2021. DISCOS: Bridging the gap between discourse knowledge and commonsense knowledge. In Proceedings of the Web Conference 2021. ACM.\"}"}
{"id": "emnlp-2022-main-628", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-628", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this study, we mainly have two kinds of annotation tasks, annotating training data for filters and human evaluation for results. Both of them require the workers to review a bunch of triples and answer questions. We show the annotation page in Figure 6. There are three questions for the workers:\\n\\n1. Whether the head is acceptable.\\n2. Whether the tail is acceptable.\\n3. Whether the triple is acceptable as a whole.\\n\\nFor question (1) and (2), we provide one option for acceptance and four options for rejection: abnormal expressions (syntax errors), violation of commonsense (impossible situations), unusable format (incomplete generation or containing names of real identities), and mismatch to the relation.\\n\\nFor question (3), there are four options about whether the triple is acceptable (plausible): \u201calways/often\u201d, \u201csometimes/likely\u201d, \u201cfar-fetched/never\u201d, and \u201cinvalid\u201d.\\n\\nBefore annotation, each worker are shown with a instruction, which contains background knowledge of the annotation task and examples for each answer options.\\n\\nDuring the pilot studies, we also compared several alternative text generation backbones besides mT5, including a multilingual autoregressive model XGLM (Winata et al., 2021) and two Chinese-specific models, Pangu-Alpha (Zeng et al., 2021) and CPM2 (Zhang et al., 2021). We construct a small set of test prompts to make the models complete some commonsense knowledge triples. For each model, We use its biggest publicly available checkpoint to generate 100 results for each prompt (directly sampling without hyper-parameter search).\\n\\nWe show the results and samples in Table 12 and Table 13. All these backbones can generate some plausible results. However, the XGLM (7.5B) model often generate `<unk>` tokens and CPM2 (11B) model sometimes give degenerate or irrelevant long results. In general, Pangu-Alpha (13B) and mT5-XXL (13B) have better generation quality. Based on comprehensive consideration of efficiency and feasibility, we conduct full experiments and analyses with mT5-XXL in this paper.\"}"}
{"id": "emnlp-2022-main-628", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model         | Pilot Tested | Triple Acceptance |\\n|--------------|--------------|-------------------|\\n| mT5-XXL (13B) | Yes          | 0.58              |\\n| XGLM (7.5B)  | Yes          | 0.42              |\\n| Pangu-Alpha (13B) | Yes    | 0.61              |\\n| CPM2 (11B)   | Yes          | 0.46              |\"}"}
{"id": "emnlp-2022-main-628", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"X falls in love with Y. As a result, X wants to...\\n\\n- become Y's wife\\n- marry Y (as the husband)\\n- watch TV\\n- chase\\n- get married\\n\\n- keep in touch with Y\\n- chase Y\\n- go to surgery\\n- have a love affair with Y\\n- confess love to Y\\n\\n- break up two\\n- own Y\\n- get married\\n- chase Y\\n- making new friends\\n\\n- chase back Y\\n- Y marry Z\\n- beg her\\n- chase Y back\\n- marry Y (as the wife)\\n\\nX buys book. Before that, X needs to...\\n\\n- borrow books\\n- money\\n- book\\n- in the bookstore\\n- read books\\n- have money\\n\\n- buy a book\\n- spend money\\n- own a book\\n- read a book\\n- take the book home\\n\\nWhen X start a war, people think X is...\\n\\n- ferocious\\n- selfish\\n- violent\\n- barbaric\\n- courageous\\n\\n- strong\\n- smart\\n- brave\\n- timid\\n\\n- rude\\n- selfish\\n- Brave and resourceful\\n- arising to struggle\\n- reckless\\n\\n- loss\\n- ferocious\\n- brutal\\n- (meaningless expression)\\n- extreme\\n\\nX reviews (lessons). For that, X feels...\\n\\n- tired\\n- stress\\n- nervous\\n- boring\\n\\n- tired\\n- relaxed\\n- energetic\\n- nervous\\n\\n- good grades\\n- meaningless\\n- headache\\n- too much study pressure\\n- relaxed\\n\\n- lonely\\n- nervous\\n- different from the past\\n- too much pressure\"}"}
