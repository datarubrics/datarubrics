{"id": "acl-2024-long-832", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"M4LE: A Multi-ability Multi-range Multi-task Multi-domain Long-context Evaluation Benchmark for Large Language Models\\n\\nWai-Chung Kwan, Xingshan Zeng, Yufei Wang, Yusen Sun, Liangyou Li, Yuxin Jiang, Lifeng Shang, Qun Liu, Kam-Fai Wong\\n\\nAbstract\\nManaging long sequences has become an important and necessary feature for large language models (LLMs). However, assessing their ability to handle long contexts remains a challenge. This paper introduces M4LE, a Multi-ability, Multi-range, Multi-task, Multi-domain benchmark for Long-context Evaluation. It encompasses 36 NLP datasets, covering 11 types of tasks and 12 domains, providing a comprehensive test bed. To address the lack of tasks featuring naturally long sequences, we propose an automatic approach to convert short-sequence tasks into long-sequence scenarios. These scenarios evaluate LLMs\u2019 long-context understanding across five key abilities: understanding of single or multiple relevant spans in long contexts based on explicit or semantic hints, and global context understanding. This automatic approach allows us to create instances evenly distributed from 1k to 8k input length.\\n\\n1 Introduction\\nLarge language models (LLMs) are gaining traction in addressing diverse NLP challenges. LLMs, mostly transformer-based models (Vaswani et al., 2017), are trained on a large amount of data with numerous parameters (Ouyang et al., 2022; Touvron et al., 2023b). These models have demonstrated impressive capabilities across a wide range of tasks (Brown et al., 2020; Schick et al., 2023; Shen et al., 2023; Bang et al., 2023). As LLMs continue to evolve, their ability to handle long-sequence tasks, such as extracting specific information from or summarizing lengthy documents, has become an important and competitive feature (Du et al., 2022; Chiang et al., 2023; Li et al., 2023). Therefore, a comprehensive, fair, and objective benchmark to evaluate the long-sequence capabilities of models is necessary for the progress of LLMs.\\n\\nDespite numerous efforts to develop benchmarks for assessing the knowledge or reasoning ability of LLMs (Hendrycks et al., 2021; Suzgun et al., 2022; Huang et al., 2023), comprehensive evaluation of their long-context understanding ability has received limited attention. Recent concurrent works, such as L-Eval (An et al., 2023) and LongBench (Bai et al., 2023), primarily rely on existing long-sequence NLP datasets which usually limit the task diversity and flexibility in conducting length-control experiments. They lack an objective and comprehensive understanding of the model\u2019s capability across different dimensions of long sequences.\\n\\nIn this study, we aim to maximize the diversity of constructed tasks and analyze the long-context capabilities of LLMs from a user\u2019s practical perspective. We discovered that when processing instructions based on long sequences, the essential components for task completion can be classified as single-span, multiple-span, or global, based on relevance. Building on this and considering how to locate the relevant information, we categorize long-context understanding into five distinct abilities and introduce an automated method to transform short-sequence tasks into a comprehensive long-sequence scenario encompassing all these capacities.\"}"}
{"id": "acl-2024-long-832", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"CHAPTER I On a hill by the Mississippi \u2026 For months no male emerged from the mass. \u2026\\n\\nCHAPTER II It was a frail and blue and lonely Carol who.\\n\\nCHAPTER X \u2026said Kennicott, as he unpacked his suit-case.\\n\\nSummarize CHAPTER I.\\n\\nSummarize the first and the last chapters.\\n\\nSummarize the chapters about Carol as well as Kennicott.\\n\\nSummarize the whole article.\"}"}
{"id": "acl-2024-long-832", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison with other long context benchmarks.\\n\\n|          | SCROLLS | ZeroSCROLLS | L-Eval | LongBench | M4LE |\\n|----------|---------|-------------|--------|-----------|------|\\n| #Tasks   | 3       | 4           | 4      | 6         | 11   |\\n| #Datasets | 7       | 10          | 18     | 21        | 36   |\\n| #Domains | 7       | 9           | 10     | 10        | 12   |\\n| Languages| en en en en, zh en, zh | | | | |\\n| Ranges   | \u00d7 \u00d7 \u00d7 \u00d7 | \u2713           |        | \u00d7         |      |\\n| Abilities| \u00d7 \u00d7 \u00d7 \u00d7 | \u2713           |        | \u00d7         |      |\\n\\nPerformance drop in this scope can only be observed on competent models. A more effective fine-tuning approach deserves exploration, as current methods show no significant improvement over simple Neural Tangent Kernel (NTK) aware scaling methods. We also observe that language differences and the positioning of relevant information impact the long-context understanding capabilities.\\n\\n2 Related Work\\n\\n2.1 Long-Context Modelling for LLMs\\n\\nTo address length extrapolation challenges in LLMs beyond the training context window, several methodologies have emerged. Position embeddings such as Alibi (Press et al., 2022) and XPos (Sun et al., 2023) have been developed. Alibi employs an exponential decay on the attention matrix to mitigate out-of-distribution positions' influence, while XPos introduces a block-wise causal attention mask. While these techniques require integration during training, alternative approaches enhance existing RoPE-based LLMs (Su et al., 2021), notably LLaMA (Touvron et al., 2023a), LLaMA 2 (Touvron et al., 2023b), and PaLM (Chowdhery et al., 2022). Concurrently, kaiokendev (2023) and Chen et al. (2023) propose extending context length by modifying RoPE through Position Interpolation and subsequent limited data fine-tuning. Another line of research introduces fine-tuning free approaches (bloc97, 2023; emozilla, 2023; Peng et al., 2023), including NTK-aware and dynamic NTK interpolations.\\n\\n2.2 Existing Evaluation Benchmarks for LLMs\\n\\nAs LLMs have demonstrated superior performance in a wide range of NLP tasks, comprehensively and effectively evaluating their ability becomes increasingly critical. Many of the research efforts focus on developing benchmarks for specific knowledge types (Hendrycks et al., 2021; Zhong et al., 2023) and specific task families (Chen et al., 2021; Cobbe et al., 2021). For more details, we refer readers to the recent LLMs evaluation survey (Chang et al., 2023; Wang et al., 2023). Several preliminary studies have begun to assess the model capability on long context input. Long Range Areana (Tay et al., 2020) verifies the capability of transformer-based models to handle various long sequence inputs, such as languages, vision tokens, and symbols. SCROLLS (Shaham et al., 2022) simply collects a set of naturally long NLP benchmarks covering multiple tasks and domains. Recently, ZeroSCROLLS (Shaham et al., 2023), L-Eval (An et al., 2023) and LongBench (Bai et al., 2023) are proposed to evaluate long text modelling capability of LLMs. However, these benchmarks are mainly compiled from a set of existing long NLP benchmarks, thereby suffering from data diversity (i.e., limited evaluation patterns) and data leakage (i.e., LLMs potentially already using these benchmarks for pre-training or alignment). In contrast, M4LE not only constructs evaluation instances from various tasks, domains, and length ranges but also covers three types of attention spans, offering a comprehensive evaluation of LLMs' long text capability.\\n\\n3 M4LE\\n\\nThis section outlines the M4LE benchmark's rationale, design principles, data sources, and task construction methodologies. M4LE is designed to comprehensively evaluate large language models' (LLMs) abilities in understanding long contexts. It covers a wide range of tasks, domains, and context lengths, ensuring a thorough assessment of LLMs' competencies in this crucial area.\\n\\n3.1 Design Principle\\n\\nEach sample in M4LE is a tuple of \\\\langle Task description, Context, Instruction, Response \\\\rangle. To follow the instructions, LLMs must identify relevant information within a lengthy context. This information can be a single text segment (single-span), multiple...\"}"}
{"id": "acl-2024-long-832", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"text segments (multiple-span), or the entire context (global). The models locate these segments either through direct hints (explicit) or inferred meaning (semantic). We categorize the understanding ability into five distinct types: explicit single-span, semantic single-span, explicit multiple-span, semantic multiple-span, and global context understanding (Figure 1). This classification helps in assessing the models' comprehension capabilities.\\n\\nTo ensure a comprehensive evaluation, we prioritize task diversity in two aspects:\\n\\n\u2022 Data Source: We select widely-used Chinese and English datasets in NLP which cover a variety of representative task types (e.g., QA, Summarization) and domains (e.g., News, Wiki, Web). In addition, we introduce tasks that integrate multiple task types, like Classification and Retrieval. These newly integrated tasks help measure LLMs' ability to solve more complex tasks.\\n\\n\u2022 Length Range: It is important to reveal how LLMs perform on various lengths of contexts. In our benchmark, we evenly divide samples into buckets according to their context lengths. In addition, in order to alleviate the effects of the location of relevant parts in context (Liu et al., 2023), we intentionally construct instances with the relevant paragraphs uniformly distributed in the input context.\\n\\nBy focusing on these five core abilities and maximizing task diversity, M4LE offers a comprehensive assessment of LLMs' long-context understanding capabilities.\\n\\n3.2 Data Collection\\n\\nWe collect established datasets, both in English and Chinese, to cover a broad range of tasks and domains. We not only select datasets featuring long inputs, but also include datasets with shorter inputs for our customized construction, and at the same time, enriching the domain variety. The short-context datasets can be adapted to longer contexts using our designed process, which will be introduced in the next subsection. Below we describe the datasets selected in the benchmark briefly.\\n\\nQuestion-Answering (QA): We include TriviaQA (Joshi et al., 2017), a single-document QA dataset based on web snippets and Wikipedia, with documents extended to 12k words. Additionally, NQ-Open (Lee et al., 2019), HotpotQA (Yang et al., 2018), and DRCD (Shao et al., 2019) are included, all of which are based on Wikipedia articles. We further collect NewsQA (Trischler et al., 2017) and DuoRC (Saha et al., 2018), both in English and constructed from news articles and movie plots. We also add C3 (Sun et al., 2021), a Chinese dataset comprising textbook questions.\\n\\nClassification: We incorporate BIGPATENT (Sharma et al., 2019) which includes long patent documents, and MNDS News (Petukhova and Fachada, 2023) in English and THUCNews (Hu et al., 2019) in Chinese which would be further processed for different abilities. We also utilize a sentiment classification dataset collected from e-commerce platforms (SophonPlus, 2013).\\n\\nSummarization: For English, we include Arxiv, Pubmed (Cohan et al., 2018), BIGPATENT (Sharma et al., 2019), and Booksum (Kryscinski et al., 2022), where the corresponding domains span across academic, medical, patent documents and books. We also introduce shorter summarization datasets enabling extension, such as CNNNews (See et al., 2017) and MNDS News, featuring news articles, and Wikihow (Koupaee and Wang, 2018). For Chinese, we incorporate CNewsum (Wang et al., 2021), CLTS+ (Liu et al., 2022), and News2016 (Xu, 2019), all constructed from long news articles. The LCSTS (Hu et al., 2015) dataset contains shorter news articles, while CEPSUM (Li et al., 2020) comprises product descriptions from e-commerce platforms. We also use NCLS (Zhu et al., 2019) to establish a bilingual task that generates a Chinese summary for a specific English news article.\\n\\nNatural Language Inference (NLI): We construct two tasks using English and Chinese Wikipedia articles from WikiText-103 (Merity et al., 2016) and Wiki2019zh (Xu, 2019), respectively.\\n\\nTranslation: Three translation datasets are included, depending on sentence-level translation alignments to form long contexts, including Tedtalks (Qi et al., 2018), OpenSubtitles (Lison and Tiedemann, 2016), and News commentary (Tiedemann, 2012).\\n\\nRetrieval: Lastly, we construct two retrieval tasks from the same datasets used for the NLI task for both languages. Since M4LE comprises numerous tasks combined with retrieval capability, we do not construct additional standalone retrieval datasets.\"}"}
{"id": "acl-2024-long-832", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A commercial rocket company has announced plans to go where no other has gone before. Elon Musk, founder and CEO of SpaceX, has revealed that the company is developing the Falcon Heavy rocket, capable of carrying more cargo and astronauts into space than any other rocket. This marks a significant milestone in the space exploration industry, as SpaceX aims to revolutionize space travel with its advanced technology.\\n\\nQatar plans to build a new spaceport on the Al Bidda Peninsula, near Doha, to support its space program and to establish itself as a global leader in space technology. The spaceport will be equipped with state-of-the-art facilities, including launch pads, control centers, and training facilities for astronauts. This investment underscores Qatar's commitment to advancing its space program and to exploring the possibilities of space exploration.\\n\\nThings a 7-year-old boy expects Mom to put in his backpack: A peanut butter sandwich. Things he doesn't get: a flare gun. These everyday items reveal the perspective of a child, highlighting the importance of understanding different age groups and their needs. The boy's expectations are simple yet essential, demonstrating the value of considering diverse perspectives in everyday situations.\\n\\nNew York police say a Queens mom put a gun in her son's school backpack. This incident highlights the significance of alertness and vigilance in ensuring safety and security. The police are reminding parents and school authorities to be cautious and proactive in preventing such incidents from occurring.\\n\\n3.3 Task Construction\\n\\nThis subsection details the dataset construction process of the evaluation benchmark. We construct test instances with diverse length ranges by transforming instances from collected datasets. Figure 2 illustrates the construction process. To construct a test instance for a specific task with a target length range, we first sample instances from a single source dataset. These original instances contain context, such as an article, a talk transcript, or several text segments. We then concatenate their context paragraphs into a single sequence as \u201cContext\u201d, marking each paragraph with an explicit identifier at the beginning for indexing. The value of N is determined by dividing the median context length by the target length. For each task, we manually craft a description and make sure LLaMA2-7B-Chat can understand it through preliminary testing with a few examples. We further provide instructions to guide the model to locate relevant information within the context using paragraph identifiers for explicit tasks and semantic hints for semantic tasks. This approach extends existing datasets with short contexts to accommodate arbitrary context lengths. Table 2 provides an overview of the constructed datasets in M4LE. Appendix A provides the detailed statistics of the datasets used.\"}"}
{"id": "acl-2024-long-832", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"stance, in a question-answering task, the model might be asked to answer a question based on paragraph II. This approach has been used to construct ten unique datasets covering a wide range of task types and domains for the ability. Consequently, the task types are a fusion of retrieval and their original task, such as classification, which is labeled as \\\"CLS + RET\\\".\\n\\nSemantic Single-Span Understanding. Analogous to explicit single-span understanding, the instructions for the tasks long to this ability instruct models to complete tasks based on a designated paragraph. Rather than using explicit identifiers, we provide hints about the paragraph, and models are tasked with retrieving it based on semantic information. For example, in a translation task, the model might be prompted to translate a paragraph associated with sports. Tasks within this ability are designed to introduce increased complexity and challenges since semantic-level retrieval necessitates the model to understand all paragraphs to pinpoint the right one. We have constructed nine distinct datasets aligned with this ability.\\n\\nExplicit Multiple-Span Understanding. We add further complexities to the tasks within this ability. Specifically, models are tasked with handling assignments related to multiple, disjoint paragraphs within the lengthy input context. This could necessitate addressing several original instances, for example, summarizing the first and the third paragraph. Despite these complexities, the instructions for this ability continue to utilize explicit hints. We have constructed four distinct datasets to align with this ability.\\n\\nSemantic Multiple-Span Understanding. We replace the explicit hints in explicit multiple-span understanding with semantic ones, resulting in the instructions for tasks in this scope. We've developed three distinct datasets of high complexity in line with this. Within this ability, we've incorporated counting tasks (labeled as \\\"CNT\\\"), which demand the counting of relevant paragraphs. Such tasks pose a challenge since counting is not an innate function of language models.\\n\\nGlobal Context Understanding. Finally, we present tasks in global context understanding, which is a special case within our construction process. When the original instances have sufficiently extensive context, such that the target length range $K$ can be attained with $N = 1$, we directly employ them for the associated tasks, indicating that the entire context is relevant to the task completion, and global understanding is required. Within this category, we have included ten different datasets.\\n\\n3.4 Models\\n\\nWe introduce the five families of LLMs evaluated in this study, comprising a total of 11 models.\\n\\nLLaMA 2: It is a family of LLMs that support a maximum 4k input length (Touvron et al., 2023b). These models use rotary positional embeddings (RoPE) (Su et al., 2021). LLaMA 2 has 7B, 13B and 70B variant. We focus on its 7B and 13B models in this section. We also include their aligned versions: LLaMA2-7B-Chat and LLaMA2-13B-Chat.\\n\\nVicuna: We employ Vicuna-7B-v1.5-16K and Vicuna-13B-v1.5-16K (Chiang et al., 2023), fine-tuned based on the LLaMA2 models with 125k conversational data, collected from ShareGPT with context length up to 16K tokens using linear positional interpolation (Chen et al., 2023).\\n\\nLongChat: We leverage LongChat-7B-v1.5-32K and LongChat-13B-16K (Li et al., 2023), fine-tuned on 80K and 18K conversations respectively, with context lengths up to 32K and 16K tokens, respectively. They utilize linear positional interpolation.\\n\\nChatGLM2: ChatGLM2-6B and ChatGLM2-6B-32K are based on the GLM (Du et al., 2022) models. Similar to LLaMA2, ChatGLM2 leverage RoPE. Both models are further refined on 8K and 32K input data, respectively, using linear positional interpolation.\\n\\nGPT-3.5-Turbo: It is a closed-source language model developed based on InstructGPT (Ouyang et al., 2022). Analogous to LLaMA 2, it is fine-tuned with instruction data and refined by RLHF. We use the GPT-3.5-Turbo-16K variant, which supports a 16K context length.\\n\\n3.5 Inference Details\\n\\nApart from the tuples introduced in Section 3.1, we also employ a concise and short in-context example, from the same dataset, to demonstrate the desired output format. Several full examples used in this work can be found in Appendix E. The main goal of M4LE is to evaluate the performance variations across different context length buckets and abilities. We did not perform extensive prompt engineering 3\\n\\n3We use the GPT-3.5-Turbo-16K-0613 api from https://cuhk-api-dev1-apim1.developer.azure-api.net.\"}"}
{"id": "acl-2024-long-832", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for each task to obtain the optimal performance. Instead, we focus on analyzing performance changes of particular LLMs with longer input contexts. Since LLaMA 2 models were trained on data within 4k tokens, we used dynamic NTK-aware RoPE scaling (emozilla, 2023; Peng et al., 2023) for context longer than 4k. We used 16 floating points precision during inference. To facilitate fair comparisons across various tasks with different metrics, we normalized the raw performance score \\\\( r(M, l) \\\\) (i.e., the performance of LLM \\\\( M \\\\) at context length \\\\( l \\\\)) as follows:\\n\\n\\\\[\\n\\\\hat{r}(M, l) = \\\\frac{r(M, l)}{r(GPT-3.5-Turbo-16K, 1000)}\\n\\\\]\\n\\n\\\\( \\\\hat{r}(M, l) \\\\) provides a measure of how other models perform relative to GPT-3.5-Turbo-16K in the length range bucket of 0-1000 tokens, and how their performance deteriorates with longer input.\\n\\n3.6 Results\\n\\nFigure 3 illustrates the changes in normalized average scores for various evaluated models as context lengths extend, and Figure 4 depicts their ability in the context length range of 0-1000, 1000-4000, and 4000-8000 (the full results for each task can be found in Appendix C). Based on the figures, several key observations emerge:\\n\\nThe performance of all models significantly deteriorates with increasing context lengths. This trend is expected, given that a longer context might necessitate more sophisticated modeling capabilities. It suggests that these LLMs struggle with understanding extensive context. The performance gap between ChatGPT and most open-source models widens as context length increases. This is largely because open-source models tend to exhibit a steeper decline, particularly when the context length exceeds 4k. For example, Vicuna-13B-v1.5-16K achieves competitive performance, compared to GPT-3.5-Turbo-16K, in the 0-4K length range, but its performance drops significantly after that. A notable exception is ChatGLM2-6B-32k which achieves similar performance when testing on 6K and 8K instances and is only surpassed by GPT-3.5-Turbo-16K on 8K instances.\\n\\nFine-tuning with additional long context data does not offer a significant advantage over simply NTK scaling for understanding long contexts. Both Vicuna and LongChat models are claimed to support long context as they are directly fine-tuned with longer context data. However, their performance still drops quickly when context length exceeds 4k, with no additional advantage compared to LLaMA2 models, which are trained only on 4k data and merely equipped with NTK scaling method when context length exceeds 4k. This suggests that existing long-context fine-tuning methods contribute minimally to improving long context understanding and a more efficient and effective way to enhance long context understanding ability is needed.\\n\\nMultiple-span understanding is more difficult, and semantic retrieval is even harder for competent models. There is a significant drop in performance on tasks requiring multiple-span attention as context lengthens. This is expected since attending to multiple positions is naturally harder than a single position, and it might require additional ability to distinguish and determine compared to global understanding. Surprisingly, semantic retrieval is only more challenging for GPT-3.5-Turbo-16K, the most competent model in the experiment. We hypothesize that this is because explicit retrieval, looking for relative information by an identifier, is an unnatural task for less competent and generalized LLM. On the contrary, semantic retrieval is more similar to tasks like QA that these models experienced during instruction fine-tuning.\\n\\n3.7 Ablation Study\\n\\nWe perform further analysis to understand how models behave in different languages and locations of the supporting document.\\n\\nImpact of language differences on long-context understanding. Tasks in different languages may have distinct ability requirements due to the nature of languages and the effects of tokenization. While most models presented in this study are primarily trained on English data, we aim to assess the influence of language differences on the results. In Figure 5, we compare the performance of the top-performing models (namely ChatGPT, ChatGLM2, Vicuna, and LongChat) in both Chinese and English tasks to determine if their long-context understanding abilities differ across languages. We observe a comparable decline in performance for both GPT-3.5-Turbo-16K and ChatGLM2-6B-32K across the two languages. However, the Vicuna and LongChat models exhibit a more pronounced performance drop in Chinese. This suggests...\"}"}
{"id": "acl-2024-long-832", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The normalized scores of various models in different context lengths (left), accompanied by the slopes of the corresponding best-fit lines (right). The performance of all models deteriorates with increasing context length.\\n\\nFigure 4: The comparison of abilities of various models in three context length ranges, respectively. It shows that multi-span understanding is more difficult in general. While semantic retrieval appears to be intuitively more challenging, our findings indicate that it is only more demanding for competent models such as GPT-3.5-Turbo-16K at longer lengths.\\n\\nLost-in-the-middle exists in other NLP long sequence tasks. Recently, Liu et al. (2023) find that LLMs tend to ignore the information in the middle of long input context for the task of question-answering and retrieval. In this section, following the setup in Liu et al. (2023), we conduct a comprehensive experiment to study the impact of positions of the supporting paragraphs within the context based on our proposed M4LE benchmark.\\n\\nSpecifically, we generate additional instances from the tasks in M4LE, each containing an identical input but with the supporting paragraph placed at different locations. We employ four datasets for question-answering and summarization, and two datasets for retrieval tasks. The setup details are in Appendix B.\\n\\nThe average score for each relative position of the supporting document across the three tasks is presented in Figure 6, demonstrating that models typically perform better when the supporting document is positioned either at the beginning or the end of the context, a finding consistent with Liu et al. (2023). Consequently, this suggests that the tendency for LLM to ignore information in the middle of the context is ubiquitous across various languages, models, and tasks. This also shows the potential of M4LE in discovering interesting and unique LLMs behavior in the long context scenario.\"}"}
{"id": "acl-2024-long-832", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: The normalized performance of the models fine-tuned in longer data for English and Chinese tasks, re-\\nspectively. While GPT-3.5-Turbo-16K and ChatGLM2-6B-32K exhibit a similar trend in the decline of performance\\nin both languages, other models demonstrate a more pronounced performance drop in Chinese tasks with increasing\\ncontext lengths.\\n\\nFigure 6: The performance of various models across three tasks, with the supporting document located at different\\nrelative positions. It shows higher performance is often obtained when the supporting document is positioned either\\nat the beginning or the end, consistent with Liu et al. (2023).\\n\\nConclusion\\nIn this paper, we propose M4LE for LLMs assessing their capability of long-context understanding.\\n\\nLimitations\\nDue to computational constraints, our experiments are restricted to smaller open-source models and\\nlengths of up to 8K. Nevertheless, our method can create instances of arbitrary length (the released\\nbenchmark will include instances up to 32,000\\nwords) and the analyses in this paper reveal mean-\\ningful observations concerning long-context un-\\nderstanding capabilities. Additionally, our study\\nfocuses on English and Chinese, the two most\\ncommonly used languages. We suggest future re-\\nsearch to apply our methodology to construct long\\ninstances in additional languages.\\n\\nAcknowledgements\\nThis research work is partially supported by CUHK\\ndirect grant No. 4055209 and CUHK Knowledge\\nTransfer Project Fund No. KPF23GWP20.\\n\\nReferences\\nChenxin An, Shansan Gong, Ming Zhong, Mukai\\nLi, Jun Zhang, Lingpeng Kong, and Xipeng Qiu.\\n2023. L-eval: Instituting standardized evaluation\\nfor long context language models. arXiv preprint\\narXiv:2307.11088.\"}"}
{"id": "acl-2024-long-832", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, Yuxiao Dong, Jie Tang, and Juanzi Li. 2023. LongBench: A Bilingual, Multitask Benchmark for Long Context Understanding.\\n\\nEnejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multitask, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.\\n\\nbloc97. 2023. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901.\\n\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Kaijie Zhu, Hao Chen, Linyi Yang, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, et al. 2023. A survey on evaluation of large language models. arXiv preprint arXiv:2307.03109.\\n\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\\n\\nShouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. 2023. Extending context window of large language models via positional interpolation. arXiv preprint arXiv:2306.15595.\\n\\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. Palm: Scaling language modeling with pathways.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\\n\\nArman Cohan, Franck Dernoncourt, Doo Soon Kim, Trung Bui, Seokhwan Kim, Walter Chang, and Nazli Goharian. 2018. A Discourse-Aware Attention Model for Abstractive Summarization of Long Documents. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 615\u2013621, New Orleans, Louisiana. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-832", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-832", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Timo Schick, Jane Dwivedi-Yu, Roberto Dess\u00ec, Roberta Raileanu, Maria Lomeli, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Language models can teach themselves to use tools. arXiv preprint arXiv:2302.04761.\\n\\nAbigail See, Peter J. Liu, and Christopher D. Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083, Vancouver, Canada. Association for Computational Linguistics.\\n\\nUri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. 2023. Zeroscrolls: A zero-shot benchmark for long text understanding. arXiv preprint arXiv:2305.14196.\\n\\nUri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS: Standardized CompaRison Over Long Language Sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 12007\u201312021, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nChih Chieh Shao, Trois Liu, Yuting Lai, Yiying Tseng, and Sam Tsai. 2019. DRCD: A Chinese Machine Reading Comprehension Dataset.\\n\\nEva Sharma, Chen Li, and Lu Wang. 2019. BIG-PATENT: A large-scale dataset for abstractive and coherent summarization. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2204\u20132213, Florence, Italy. Association for Computational Linguistics.\\n\\nYongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. Hugginggpt: Solving ai tasks with chatgpt and its friends in huggingface. arXiv preprint arXiv:2303.17580.\\n\\nSophonPlus. 2013. Chinesenlpcorpus. https://github.com/SophonPlus/ChineseNlpCorpus.\\n\\nJianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. 2021. RoFormer: Enhanced Transformer with Rotary Position Embedding. arXiv.org.\\n\\nSimeng Sun, Kalpesh Krishna, Andrew Mattarella-Micke, and Mohit Iyyer. 2021. Do Long-Range Language Models Actually Use Long-Range Context? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 807\u2013822, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nYutao Sun, Li Dong, Barun Patra, Shuming Ma, Shaozhan Huang, Alon Benhaim, Vishrav Chaudhary, Xia Song, and Furu Wei. 2023. A length-extrapolatable transformer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14590\u201314604, Toronto, Canada. Association for Computational Linguistics.\\n\\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261.\\n\\nYi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao, Liu Yang, Sebastian Ruder, and Donald Metzler. 2020. Long Range Arena: A Benchmark for Efficient Transformers. In International Conference on Learning Representations.\\n\\nJ\u00f6rg Tiedemann. 2012. Parallel Data, Tools and Interfaces in OPUS. In Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12), pages 2214\u20132218, Istanbul, Turkey. European Language Resources Association (ELRA).\\n\\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023a. Llama: Open and efficient foundation language models.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodr\u00edguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023b. Llama 2: Open Foundation and Fine-Tuned Chat Models.\\n\\nAdam Trischler, Tong Wang, Xingdi Yuan, Justin Harris, Alessandro Sordoni, Philip Bachman, and Kaheer Suleman. 2017. NewsQA: A Machine Comprehension Dataset. In Proceedings of the 2nd Workshop on Representation Learning for NLP, pages 191\u2013200, Vancouver, Canada. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-832", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nDanqing Wang, Jiaze Chen, Xianze Wu, Hao Zhou, and Lei Li. 2021. CNewSum: A Large-scale Chinese News Summarization Dataset with Human-annotated Adequacy and Deducibility Level.\\n\\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966.\\n\\nBright Xu. 2019. Nlp chinese corpus: Large scale chinese corpus for nlp.\\n\\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: A Dataset for Diverse, Explainable Multi-hop Question Answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2369\u20132380, Brussels, Belgium. Association for Computational Linguistics.\\n\\nWanjun Zhong, Ruixiang Cui, Yiduo Guo, Yaobo Liang, Shuai Lu, Yanlin Wang, Amin Saied, Weizhu Chen, and Nan Duan. 2023. Agieval: A human-centric benchmark for evaluating foundation models. arXiv preprint arXiv:2304.06364.\\n\\nJunnan Zhu, Qian Wang, Yining Wang, Yu Zhou, Jiajun Zhang, Shaonan Wang, and Chengqing Zong. 2019. NCLS: Neural Cross-Lingual Summarization. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3054\u20133064, Hong Kong, China. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-832", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section describes the datasets used in M4LE. Table 2 provides an overview of the constructed datasets.\\n\\nA.1 MNDS News\\nMNDS News (Petukhova and Fachada, 2023) is an English hierarchical news category classification dataset comprising 10,917 news articles from 260 sources. We only use the 17 first-level categories as the labels for this study. For multiple retrieval tasks, we randomly sample a class label that appears in the instance.\\n\\nA.2 THUCNews\\nTHUCNews (Hu et al., 2019) is a Chinese classification dataset containing 74 million news articles from Sina, with each article belonging to one of the ten categories. We filter out the articles with the number of words less than 20. The multiple retrieval task is built similarly to MNDS News.\\n\\nA.3 MARC\\nMARC (Keung et al., 2020) is a dataset for the bilingual (English and Chinese) setting. It contains multilingual Amazon reviews with star ratings from 1 to 5, where 5 is the best. We use 1-star and 5-star reviews for negative and positive reviews respectively, and ask models to return all positive reviews.\\n\\nA.4 Online Shopping\\nOnline Shopping (SophonPlus, 2013) is a Chinese sentiment dataset containing 60K product reviews from Chinese e-commerce platforms. Each review is marked as positive or negative.\\n\\nA.5 BIGPATENT\\nBIGPATENT (Sharma et al., 2019) consists of 1.3 million records of U.S. BIGPATENT documents across nine technological areas. The abstract of the document is used as the golden document summary.\\n\\nA.6 CEPSUM\\nCEPSUM (Li et al., 2020) is a dataset containing product descriptions and summary pairs collected from a popular Chinese e-commerce platform. We removed instances with less than 60 words in the product description.\\n\\nA.7 CNNNews\\nCNNNews (See et al., 2017) contains English online news articles from CNN, where each of it is paired with a multi-sentence summary. We preprocess the data using the script from See et al. (2017) and select the instances with at least 30 words in the article.\\n\\nA.8 LCSTS\\nLCSTS (Hu et al., 2015) is a Chinese summarization dataset consisting of over 2 million posts and short summary pairs collected from the Chinese microblogging website Sina Weibo. We use part two of the data, which consists of 10,666 (text, summary) pairs with a human-labeled score to indicate the relevance between the post and the summary. The score ranges from 1 to 5, where 5 indicates the most relevant. We select only the samples with a score of 5 in the relevance score.\\n\\nA.9 NCLS\\nNCLS (Zhu et al., 2019) is a cross-lingual summarization dataset consisting of pairs of articles in one language and summaries in another language (Chinese or English), constructed from the CNNNews and LCSTS datasets.\\n\\nA.10 WikiHow\\nWikiHow (Koupaee and Wang, 2018) comprises 230,000 English articles that describe a procedural task along with corresponding summaries. Each article has a title that starts with \\\"How to\\\". The procedures described in the article are separated into multiple steps, where each step corresponds to a paragraph. Each paragraph has a short summary. These summaries are concatenated to form the summary of the article. We remove instances with articles that have less than 60 words.\\n\\nA.11 News2016\\nNews2016 (Xu, 2019), encompassing over 2 million Chinese news articles. Each article contains a title and keywords. The title is used as the golden summary of the news article. We remove instances with the number of words less than 200 and more than 800.\\n\\nA.12 Arxiv\\nArxiv (Cohan et al., 2018) consists of 215K academic papers from arXiv.org. The abstracts of the papers are used as the golden summary.\"}"}
{"id": "acl-2024-long-832", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Ability | Dataset | Task Type | Language | Domain | Metric | Ave. Len. |\\n|---------|---------|-----------|----------|--------|--------|-----------|\\n| Explicit | MNDS News | CLS + RET | En | News | Acc | 3805 |\\n| Explicit | THUCNews | CLS + RET | Zh | News | Acc | 3650 |\\n| Explicit | NewsQA | QA + RET | En | News | Acc | 3679 |\\n| Explicit | C3 | QA + RET | Zh | Textbook | Acc | 3797 |\\n| Explicit | WoW | RET | En | Wiki | Acc | 3434 |\\n| Explicit | DRCD | RET | Zh | Wiki | Acc | 3617 |\\n| Explicit | CNNNews | SUM + RET | En | News | Rouge-L | 3754 |\\n| Explicit | CEPSUM | SUM + RET | Zh | E-Commerce | Rouge-L | 4003 |\\n| Explicit | LCSTS | SUM + RET | Zh | News | Rouge-L | 4102 |\\n| Explicit | NCLS | SUM + RET | En,Zh | News | Rouge-L | 3470 |\\n| Explicit | | | | | | |\\n| Explicit | MNDS News | CLS | En | News | F1 | 3772 |\\n| Explicit | THUCNews | CLS | Zh | News | F1 | 3721 |\\n| Explicit | MARC | CLS | En,Zh | E-Commerce | F1 | 3543 |\\n| Explicit | Online Shopping | CLS | Zh | E-Commerce | F1 | 3714 |\\n| Semantic | WikiText-103 | NLI | En | Wiki | Acc | 3278 |\\n| Semantic | Wiki2019zh | NLI | Zh | Wiki | Acc | 3723 |\\n| Semantic | DuoRC | QA | En | Movie | Acc | 3572 |\\n| Semantic | NQ-Open | QA | En | Wiki | Acc | 3128 |\\n| Semantic | DuReader | QA | Zh | Web | Rouge-L | 3261 |\\n| Semantic | DRCD | QA | Zh | Wiki | Acc | 3300 |\\n| Semantic | WikiHow | SUM | En | WikiHow | Rouge-L | 3514 |\\n| Semantic | News2016 | SUM | Zh | News | Rouge-L | 3785 |\\n| Semantic | TedTalks | TRAN | En,Zh | TedTalks | BLEU | 2956 |\\n| Semantic | | | | | | |\\n| Semantic | MNDS News | CLS + CNT | En | News | Acc | 3791 |\\n| Semantic | THUCNews | CLS + CNT | Zh | News | Acc | 3699 |\\n| Semantic | HotpotQA | QA | En | Wiki | Acc | 1060 |\\n| Global | BIGPATENT | CLS | En | Patent | Acc | 3407 |\\n| Global | TriviaQA | QA | En | Web | Acc | 3329 |\\n| Global | Arixv | SUM | En | Academic | Rouge-L | 3748 |\\n| Global | BIGPATENT | SUM | En | Patent | Rouge-L | 3293 |\\n| Global | Pubmed | SUM | En | Medical | Rouge-L | 3678 |\\n| Global | Booksum | SUM | En | Book | Rouge-L | 2643 |\\n| Global | CNewsum | SUM | Zh | News | Rouge-L | 1883 |\\n| Global | CLTS+ | SUM | Zh | News | Rouge-L | 3158 |\\n| Global | OpenSubtitles | TRAN | En,Zh | Movie | BLEU | 2048 |\\n| Global | News Commentary | TRAN | En,Zh | News | BLEU | 3585 |\"}"}
{"id": "acl-2024-long-832", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A.13 Booksum\\n\\nBooksum (Kryscinski et al., 2022), which includes 405 English books including plays, short stories, and novels with human-written summaries for each chapter. We combine the consecutive chapters and the corresponding summaries to construct instances for any context length range.\\n\\nA.14 CNewsum\\n\\nCNewsum (Wang et al., 2021) contains 304,307 Chinese news articles from different press publishers with human-written summaries.\\n\\nA.15 CLTS+\\n\\nCLTS+ (Liu et al., 2022) is an improved Chinese new articles summarization dataset based on CLTS (Liu et al., 2020). CLTS contains more than 180,000 Chinese long articles with human-written summaries. CLTS+ utilizes back translation to enhance the abstractiveness of the summaries.\\n\\nA.16 NewsQA\\n\\nNewsQA (Trischler et al., 2017) is an English QA dataset based on 12,744 news articles from CNN. Crowdsourced workers are recruited to generate 119,633 questions and answers.\\n\\nA.17 C3\\n\\nC3 (Sun et al., 2021) is a Chinese textbook-based machine comprehension dataset. The questions are multiple-choice questions collected from exams for second-language Chinese learners.\\n\\nA.18 DuoRC\\n\\nDuoRC (Saha et al., 2018) is an English question-answer dataset based on 7680 movie plots collected from IMDb and Wikipedia. Crowdsourced workers are hired to create 186,089 unique question-answer pairs.\\n\\nA.19 NQ-Open\\n\\nNaturalQuestions-Open (NQ-Open) (Lee et al., 2019) is an open-domain question-answering dataset based on Wikipedia documents. The questions are collected from Google Search queries. We directly use the processed version from Liu et al. (2023).\\n\\nA.20 DuReader\\n\\nDuReader (He et al., 2018) is an open-domain Chinese machine reading comprehension dataset, consisting of 200K questions collected from Baidu Search.\\n\\nB Experiment Details for Lost-In-The-Middle\\n\\nFor the experiment in Figure 6, which explores the effects of the positions of the relevant paragraphs, we additionally construct the following instances:\\n\\nIn the QA task, 100 instances, each comprising 20 paragraphs, are generated from NQ-Open and DuoRC for English, and from DRCD and C3 for Chinese. Similarly, for the summarization task, we generate 100 instances each from WikiHow and CNNNews for English and News2016, and LCSTS for Chinese. For the retrieval task, we formulate 200 instances each using WoW for English and DRCD for Chinese. The supporting paragraph will be evenly placed at different locations.\\n\\nC Main Results\\n\\nWe report the results used for plotting Figure 3.\\n\\nD Task Results\\n\\nWe show the results of each task in Table 7 to 45.\\n\\nE Prompts\\n\\nIn this section, we describe the prompts used in MLE. The prompt begins with the task definition, followed by the in-context example and the testing instance. Below we show the prompt examples used for each of the five abilities. Other tasks\u2019 prompts are constructed similarly.\"}"}
{"id": "acl-2024-long-832", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                      | Explicit | Single | Semantic | Single | Explicit | Multiple | Semantic | Multiple Global |\\n|---------------------------|----------|--------|----------|--------|----------|----------|----------|---------------|\\n| LLaMA2-7B                 | 0.39     | 0.33   | 0.25     | 0.13   | 0.07     | 0.39     | 0.33     | 0.25          |\\n| LLaMA2-7B-Chat            | 0.41     | 0.34   | 0.25     | 0.13   | 0.11     | 0.41     | 0.34     | 0.25          |\\n| LLaMA2-13B                | 0.44     | 0.38   | 0.28     | 0.22   | 0.19     | 0.44     | 0.38     | 0.28          |\\n| LLaMA2-13B-Chat           | 0.44     | 0.37   | 0.28     | 0.23   | 0.21     | 0.44     | 0.37     | 0.28          |\\n| ChatGLM2-6B               | 0.40     | 0.33   | 0.25     | 0.21   | 0.17     | 0.40     | 0.33     | 0.25          |\\n| ChatGLM2-6B-32K           | 0.39     | 0.32   | 0.30     | 0.27   | 0.26     | 0.39     | 0.32     | 0.30          |\\n| LongChat-7B-v1.5-32K      | 0.41     | 0.37   | 0.33     | 0.27   | 0.24     | 0.41     | 0.37     | 0.33          |\\n| LongChat-13B-16K          | 0.41     | 0.37   | 0.33     | 0.25   | 0.21     | 0.41     | 0.37     | 0.33          |\\n| Vicuna-7B-v1.5-16K        | 0.43     | 0.39   | 0.32     | 0.26   | 0.14     | 0.43     | 0.39     | 0.32          |\\n| Vicuna-13B-v1.5-16K       | 0.48     | 0.45   | 0.40     | 0.33   | 0.23     | 0.48     | 0.45     | 0.40          |\\n| GPT-3.5-Turbo-16K         | 0.50     | 0.47   | 0.42     | 0.39   | 0.36     | 0.50     | 0.47     | 0.42          |\\n\\nTable 3: The average normalized performance of different models in various lengths.\\n\\n| Model                      | Explicit | Single | Semantic | Single | Explicit | Multiple | Semantic | Multiple Global |\\n|---------------------------|----------|--------|----------|--------|----------|----------|----------|---------------|\\n| LLaMA2-7B                 | 0.39     | 0.38   | 0.41     | 0.49   | 0.37     | 0.41     | 0.38     | 0.41          |\\n| LLaMA2-7B-Chat            | 0.43     | 0.43   | 0.39     | 0.43   | 0.39     | 0.43     | 0.43     | 0.39          |\\n| LLaMA2-13B                | 0.44     | 0.44   | 0.46     | 0.49   | 0.44     | 0.44     | 0.44     | 0.44          |\\n| LLaMA2-13B-Chat           | 0.44     | 0.45   | 0.44     | 0.48   | 0.42     | 0.44     | 0.45     | 0.44          |\\n| ChatGLM2-6B               | 0.43     | 0.45   | 0.27     | 0.47   | 0.40     | 0.43     | 0.45     | 0.27          |\\n| ChatGLM2-6B-32K           | 0.45     | 0.44   | 0.29     | 0.38   | 0.36     | 0.45     | 0.44     | 0.29          |\\n| LongChat-7B-v1.5-32K      | 0.42     | 0.42   | 0.38     | 0.41   | 0.42     | 0.42     | 0.42     | 0.38          |\\n| LongChat-13B-16K          | 0.40     | 0.40   | 0.42     | 0.47   | 0.40     | 0.40     | 0.40     | 0.42          |\\n| Vicuna-7B-v1.5-16K        | 0.42     | 0.44   | 0.37     | 0.46   | 0.46     | 0.42     | 0.44     | 0.37          |\\n| Vicuna-13B-v1.5-16K       | 0.47     | 0.49   | 0.48     | 0.51   | 0.48     | 0.47     | 0.49     | 0.48          |\\n| GPT-3.5-Turbo-16K         | 0.50     | 0.50   | 0.50     | 0.50   | 0.50     | 0.50     | 0.50     | 0.50          |\\n\\nTable 4: Performance comparison of various models in different abilities over the 0-1000 tokens.\\n\\n| Model                      | Explicit | Single | Semantic | Single | Explicit | Multiple | Semantic | Multiple Global |\\n|---------------------------|----------|--------|----------|--------|----------|----------|----------|---------------|\\n| LLaMA2-7B                 | 0.31     | 0.33   | 0.19     | 0.35   | 0.28     | 0.31     | 0.31     | 0.19          |\\n| LLaMA2-7B-Chat            | 0.33     | 0.33   | 0.25     | 0.35   | 0.27     | 0.33     | 0.33     | 0.25          |\\n| LLaMA2-13B                | 0.34     | 0.35   | 0.28     | 0.32   | 0.32     | 0.34     | 0.34     | 0.28          |\\n| LLaMA2-13B-Chat           | 0.38     | 0.33   | 0.28     | 0.36   | 0.29     | 0.38     | 0.33     | 0.28          |\\n| ChatGLM2-6B               | 0.39     | 0.32   | 0.15     | 0.32   | 0.26     | 0.39     | 0.32     | 0.15          |\\n| ChatGLM2-6B-32K           | 0.43     | 0.36   | 0.15     | 0.33   | 0.28     | 0.43     | 0.36     | 0.15          |\\n| LongChat-7B-v1.5-32K      | 0.42     | 0.36   | 0.28     | 0.33   | 0.33     | 0.42     | 0.36     | 0.28          |\\n| LongChat-13B-16K          | 0.41     | 0.35   | 0.27     | 0.36   | 0.32     | 0.41     | 0.35     | 0.27          |\\n| Vicuna-7B-v1.5-16K        | 0.42     | 0.37   | 0.19     | 0.34   | 0.35     | 0.42     | 0.37     | 0.19          |\\n| Vicuna-13B-v1.5-16K       | 0.48     | 0.42   | 0.34     | 0.42   | 0.41     | 0.48     | 0.42     | 0.34          |\\n| GPT-3.5-Turbo-16K         | 0.48     | 0.43   | 0.43     | 0.35   | 0.46     | 0.48     | 0.43     | 0.43          |\\n\\nTable 5: Performance comparison of various models in different abilities over the 2000-4000 tokens.\\n\\n| Model                      | Explicit | Single | Semantic | Single | Explicit | Multiple | Semantic | Multiple Global |\\n|---------------------------|----------|--------|----------|--------|----------|----------|----------|---------------|\\n| LLaMA2-7B                 | 0.13     | 0.20   | 0.06     | 0.21   | 0.16     | 0.13     | 0.13     | 0.06          |\\n| LLaMA2-7B-Chat            | 0.13     | 0.15   | 0.04     | 0.22   | 0.11     | 0.13     | 0.13     | 0.04          |\\n| LLaMA2-13B                | 0.16     | 0.25   | 0.07     | 0.18   | 0.15     | 0.16     | 0.16     | 0.07          |\\n| LLaMA2-13B-Chat           | 0.16     | 0.26   | 0.05     | 0.20   | 0.17     | 0.16     | 0.16     | 0.05          |\\n| ChatGLM2-6B               | 0.24     | 0.24   | 0.04     | 0.16   | 0.18     | 0.24     | 0.24     | 0.04          |\\n| ChatGLM2-6B-32K           | 0.40     | 0.31   | 0.06     | 0.22   | 0.23     | 0.40     | 0.31     | 0.06          |\\n| LongChat-7B-v1.5-32K      | 0.30     | 0.24   | 0.09     | 0.17   | 0.22     | 0.30     | 0.24     | 0.09          |\\n| LongChat-13B-16K          | 0.23     | 0.24   | 0.06     | 0.21   | 0.23     | 0.23     | 0.23     | 0.06          |\\n| Vicuna-7B-v1.5-16K        | 0.24     | 0.23   | 0.05     | 0.13   | 0.22     | 0.24     | 0.23     | 0.05          |\\n| Vicuna-13B-v1.5-16K       | 0.33     | 0.22   | 0.10     | 0.18   | 0.23     | 0.33     | 0.22     | 0.10          |\\n| GPT-3.5-Turbo-16K         | 0.43     | 0.37   | 0.29     | 0.20   | 0.39     | 0.43     | 0.37     | 0.29          |\\n\\nTable 6: Performance comparison of various models in different abilities over the 4000-8000 tokens.\"}"}
{"id": "acl-2024-long-832", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 54.00 | 50.75 | 34.48 | 32.37 | 23.08 |\\n| LLaMA2-7B-Chat        | 64.50 | 62.19 | 40.89 | 18.84 | 16.83 |\\n| LLaMA2-13B            | 58.00 | 55.22 | 42.36 | 31.40 | 24.37 |\\n| LLaMA2-13B-Chat       | 64.00 | 62.19 | 44.83 | 36.23 | 25.32 |\\n| ChatGLM2-6B           | 49.00 | 37.81 | 31.53 | 23.67 | 16.83 |\\n| ChatGLM2-6B-32K      | 46.50 | 46.27 | 36.95 | 28.99 | 35.10 |\\n| LongChat-7B-v1.5-32K | 59.50 | 57.21 | 49.75 | 47.34 | 37.50 |\\n| LongChat-13B-16K     | 59.00 | 52.74 | 49.75 | 48.31 | 24.39 |\\n| Vicuna-7B-v1.5-16K   | 61.00 | 59.70 | 50.74 | 44.93 | 31.73 |\\n| Vicuna-13B-v1.5-16K  | 65.00 | 59.20 | 54.19 | 51.21 | 24.39 |\\n| GPT-3.5-Turbo-16K    | 62.00 | 59.70 | 55.17 | 51.69 | 46.63 |\\n\\nTable 7: NQ-Open (QA)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 78.00 | 71.00 | 45.00 | 47.26 | 33.50 |\\n| LLaMA2-7B-Chat        | 83.00 | 76.00 | 43.00 | 43.28 | 34.52 |\\n| LLaMA2-13B            | 82.00 | 81.00 | 74.00 | 50.40 | 42.70 |\\n| LLaMA2-13B-Chat       | 88.00 | 83.00 | 77.50 | 51.84 | 45.32 |\\n| ChatGLM2-6B           | 79.00 | 74.00 | 67.50 | 56.22 | 41.00 |\\n| ChatGLM2-6B-32K      | 81.50 | 74.50 | 69.50 | 72.14 | 67.00 |\\n| LongChat-7B-v1.5-32K | 81.00 | 77.50 | 70.50 | 77.61 | 72.00 |\\n| LongChat-13B-16K     | 66.00 | 60.00 | 51.50 | 54.73 | 47.45 |\\n| Vicuna-7B-v1.5-16K   | 85.00 | 84.50 | 80.50 | 83.58 | 73.50 |\\n| Vicuna-13B-v1.5-16K  | 88.50 | 91.50 | 84.50 | 82.59 | 74.32 |\\n| GPT-3.5-Turbo-16K    | 89.00 | 90.50 | 85.50 | 86.57 | 79.50 |\\n\\nTable 8: DRCD (QA)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 98.50 | 95.52 | 64.00 | 35.91 | 23.12 |\\n| LLaMA2-7B-Chat        | 97.50 | 99.50 | 82.50 | 46.38 | 32.02 |\\n| LLaMA2-13B            | 98.50 | 99.50 | 54.00 | 26.57 | 18.00 |\\n| LLaMA2-13B-Chat       | 97.50 | 99.00 | 84.00 | 58.45 | 48.92 |\\n| ChatGLM2-6B           | 97.00 | 93.03 | 65.00 | 32.37 | 15.87 |\\n| ChatGLM2-6B-32K      | 93.50 | 91.54 | 86.50 | 74.88 | 54.33 |\\n| LongChat-7B-v1.5-32K | 98.50 | 99.50 | 97.50 | 97.10 | 76.92 |\\n| LongChat-13B-16K     | 98.00 | 99.00 | 94.00 | 90.82 | 74.93 |\\n| Vicuna-7B-v1.5-16K   | 98.50 | 99.50 | 94.50 | 91.79 | 64.42 |\\n| Vicuna-13B-v1.5-16K  | 98.50 | 99.00 | 98.50 | 92.27 | 26.92 |\\n| GPT-3.5-Turbo-16K    | 98.50 | 98.51 | 97.50 | 90.82 | 87.98 |\\n\\nTable 9: WoW (RET)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 11.62 | 12.96 | 11.72 | 8.46 | 3.57 |\\n| LLaMA2-7B-Chat        | 14.19 | 14.68 | 16.79 | 8.40 | 4.59 |\\n| LLaMA2-13B            | 13.51 | 13.24 | 12.34 | 9.38 | 5.86 |\\n| LLaMA2-13B-Chat       | 13.47 | 13.56 | 13.96 | 11.46 | 5.93 |\\n| ChatGLM2-6B           | 12.88 | 13.22 | 12.63 | 10.32 | 6.81 |\\n| ChatGLM2-6B-32K      | 13.71 | 14.28 | 14.24 | 12.39 | 8.00 |\\n| LongChat-7B-v1.5-32K | 14.14 | 14.80 | 14.39 | 10.81 | 8.11 |\\n| LongChat-13B-16K     | 11.94 | 13.42 | 13.48 | 8.75 | 7.15 |\\n| Vicuna-7B-v1.5-16K   | 15.14 | 15.35 | 15.29 | 11.63 | 6.47 |\\n| Vicuna-13B-v1.5-16K  | 14.28 | 14.81 | 14.07 | 8.37 | 6.92 |\\n| GPT-3.5-Turbo-16K    | 18.00 | 16.98 | 15.65 | 12.18 | 10.86 |\\n\\nTable 10: DRCD (RET)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 87.50 | 88.50 | 84.00 | 73.00 | 65.00 |\\n| LLaMA2-7B-Chat        | 86.00 | 86.50 | 76.00 | 64.00 | 63.50 |\\n| LLaMA2-13B            | 90.50 | 92.00 | 82.00 | 75.50 | 61.00 |\\n| LLaMA2-13B-Chat       | 90.50 | 89.00 | 80.50 | 73.00 | 66.00 |\\n| ChatGLM2-6B           | 78.50 | 66.00 | 52.00 | 54.00 | 32.50 |\\n| ChatGLM2-6B-32K      | 77.50 | 76.00 | 61.50 | 58.50 | 45.50 |\\n| LongChat-7B-v1.5-32K | 87.50 | 84.50 | 80.00 | 75.50 | 68.50 |\\n| LongChat-13B-16K     | 85.00 | 86.50 | 75.00 | 75.50 | 50.00 |\\n| Vicuna-7B-v1.5-16K   | 91.00 | 87.50 | 84.50 | 78.50 | 56.50 |\\n| Vicuna-13B-v1.5-16K  | 88.50 | 85.00 | 80.00 | 77.00 | 50.00 |\\n| GPT-3.5-Turbo-16K    | 89.50 | 83.00 | 82.00 | 77.00 | 73.50 |\\n\\nTable 11: Booksum (SUM)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 47.50 | 36.50 | 36.50 | 36.50 | 36.50 |\\n| LLaMA2-7B-Chat        | 44.50 | 42.00 | 42.00 | 42.00 | 42.00 |\\n| LLaMA2-13B            | 52.50 | 39.50 | 39.50 | 39.50 | 39.50 |\\n| LLaMA2-13B-Chat       | 51.50 | 41.00 | 41.00 | 41.00 | 41.00 |\\n| ChatGLM2-6B           | 43.50 | 31.50 | 31.50 | 31.50 | 31.50 |\\n| ChatGLM2-6B-32K      | 41.50 | 35.00 | 35.00 | 35.00 | 35.00 |\\n| LongChat-7B-v1.5-32K | 49.50 | 40.50 | 40.50 | 40.50 | 40.50 |\\n| LongChat-13B-16K     | 55.00 | 43.50 | 43.50 | 43.50 | 43.50 |\\n| Vicuna-7B-v1.5-16K   | 50.00 | 44.50 | 44.50 | 44.50 | 44.50 |\\n| Vicuna-13B-v1.5-16K  | 56.00 | 52.00 | 52.00 | 52.00 | 52.00 |\\n| GPT-3.5-Turbo-16K    | 55.00 | 41.50 | 41.50 | 41.50 | 41.50 |\\n\\nTable 12: TriviaQA (QA)\\n\\n| Model                  | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B             | 47.50 | 36.50 | 36.50 | 36.50 | 36.50 |\\n| LLaMA2-7B-Chat        | 44.50 | 42.00 | 42.00 | 42.00 | 42.00 |\\n| LLaMA2-13B            | 52.50 | 39.50 | 39.50 | 39.50 | 39.50 |\\n| LLaMA2-13B-Chat       | 51.50 | 41.00 | 41.00 | 41.00 | 41.00 |\\n| ChatGLM2-6B           | 43.50 | 31.50 | 31.50 | 31.50 | 31.50 |\\n| ChatGLM2-6B-32K      | 41.50 | 35.00 | 35.00 | 35.00 | 35.00 |\\n| LongChat-7B-v1.5-32K | 49.50 | 40.50 | 40.50 | 40.50 | 40.50 |\\n| LongChat-13B-16K     | 55.00 | 43.50 | 43.50 | 43.50 | 43.50 |\\n| Vicuna-7B-v1.5-16K   | 50.00 | 44.50 | 44.50 | 44.50 |"}
{"id": "acl-2024-long-832", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Type       | 1k  | 2k  | 4k  | 6k  | 8k  |\\n|-----------------|-----|-----|-----|-----|-----|\\n| LLaMA2-7B       | 24.17 | 23.81 | 25.28 | 19.44 | 14.66 |\\n| LLaMA2-7B-Chat  | 29.89 | 26.48 | 24.41 | 14.14 | 13.02 |\\n| LLaMA2-13B      | 30.95 | 32.29 | 21.61 | 16.36 | 13.32 |\\n| LLaMA2-13B-Chat | 25.05 | 21.74 | 20.69 | 12.94 | 11.92 |\\n| ChatGLM2-6B     | 28.45 | 25.07 | 20.27 | 19.86 | 19.71 |\\n| ChatGLM2-6B-32K| 19.25 | 18.86 | 20.35 | 15.16 | 13.04 |\\n| LongChat-7B-v1.5-32K| 27.57 | 28.78 | 26.30 | 18.98 | 23.14 |\\n| LongChat-13B-16K| 24.77 | 26.33 | 24.47 | 23.34 | 28.07 |\\n| Vicuna-7B-v1.5-16K| 32.52 | 31.99 | 26.03 | 21.18 | 20.79 |\\n| Vicuna-13B-v1.5-16K| 33.41 | 31.40 | 26.63 | 14.40 | 12.54 |\\n| GPT-3.5-Turbo-16K| 28.65 | 23.13 | 19.25 | 16.97 | 17.36 |\\n\\nTable 15: BIGPATENT (SUM)\"}"}
{"id": "acl-2024-long-832", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 24.00  | 22.00  | 1.49   | 0.50   | 0.49   |\\n| LLaMA2-7B-Chat| 39.00  | 30.00  | 0.50   | 0.50   | 0.00   |\\n| LLaMA2-13B    | 47.50  | 22.50  | 0.50   | 4.00   | 0.00   |\\n| LLaMA2-13B-Chat| 66.00 | 5.00   | 0.00   | 0.00   | 0.00   |\\n| ChatGLM2-6B   | 47.00  | 15.50  | 2.49   | 8.00   | 0.00   |\\n| ChatGLM2-6B-32K| 51.50 | 25.00  | 6.97   | 5.00   | 1.96   |\\n| LongChat-7B-v1.5-32K| 47.00 | 15.00  | 1.49   | 2.50   | 0.98   |\\n| LongChat-13B-16K| 21.50 | 23.50  | 1.00   | 5.00   | 0.00   |\\n| Vicuna-7B-v1.5-16K| 37.50 | 4.50   | 0.00   | 0.50   | 0.00   |\\n| Vicuna-13B-v1.5-16K| 75.00 | 26.00  | 3.48   | 0.00   | 0.00   |\\n| GPT-3.5-Turbo-16K| 77.50 | 58.00  | 4.98   | 12.50  | 4.41   |\\n\\nTable 23: Wiki2019zh (NLI)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 57.44  | 33.21  | 13.73  | 6.94   | 6.45   |\\n| LLaMA2-7B-Chat| 31.62  | 18.03  | 17.74  | 9.19   | 5.43   |\\n| LLaMA2-13B    | 54.87  | 35.51  | 19.43  | 2.58   | 1.12   |\\n| LLaMA2-13B-Chat| 59.10 | 45.35  | 22.91  | 7.89   | 3.13   |\\n| ChatGLM2-6B   | 45.35  | 34.06  | 9.15   | 8.68   | 5.87   |\\n| ChatGLM2-6B-32K| 20.92 | 10.02  | 17.49  | 15.33  | 12.09  |\\n| LongChat-7B-v1.5-32K| 48.47 | 43.76  | 32.78  | 25.66  | 21.02  |\\n| LongChat-13B-16K| 55.77 | 50.73  | 37.16  | 26.45  | 23.00  |\\n| Vicuna-7B-v1.5-16K| 52.23 | 43.40  | 30.19  | 18.55  | 10.60  |\\n| Vicuna-13B-v1.5-16K| 61.13 | 54.82  | 43.19  | 33.21  | 21.38  |\\n| GPT-3.5-Turbo-16K| 73.07 | 63.61  | 48.60  | 39.22  | 22.59  |\\n\\nTable 24: MNDS News (CLS, Explicit Multiple)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 60.00  | 36.32  | 17.16  | 16.18  | 10.29  |\\n| LLaMA2-7B-Chat| 30.00  | 27.86  | 22.55  | 19.12  | 12.00  |\\n| LLaMA2-13B    | 50.50  | 20.90  | 16.18  | 16.18  | 11.72  |\\n| LLaMA2-13B-Chat| 43.50 | 43.78  | 26.96  | 28.89  | 19.57  |\\n| ChatGLM2-6B   | 47.50  | 34.33  | 17.65  | 15.20  | 15.69  |\\n| ChatGLM2-6B-32K| 14.00  | 32.84  | 16.18  | 15.20  | 19.61  |\\n| LongChat-7B-v1.5-32K| 32.50 | 18.41  | 23.04  | 24.02  | 12.25  |\\n| LongChat-13B-16K| 50.50 | 41.79  | 21.08  | 22.55  | 37.50  |\\n| Vicuna-7B-v1.5-16K| 39.50 | 31.84  | 25.98  | 20.10  | 10.78  |\\n| Vicuna-13B-v1.5-16K| 55.00 | 47.76  | 26.96  | 13.24  | 10.30  |\\n| GPT-3.5-Turbo-16K| 54.50 | 39.80  | 17.65  | 19.61  | 12.25  |\\n\\nTable 25: MNDS News (CLS, Semantic Multiple)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 29.00  | 19.92  | 12.00  | 6.35   | 2.19   |\\n| LLaMA2-7B-Chat| 38.05  | 29.21  | 19.89  | 8.34   | 0.02   |\\n| LLaMA2-13B    | 47.18  | 43.22  | 16.05  | 2.65   | 0.00   |\\n| LLaMA2-13B-Chat| 48.73 | 42.74  | 25.36  | 4.91   | 0.00   |\\n| ChatGLM2-6B   | 20.88  | 7.60   | 4.67   | 2.46   | 2.55   |\\n| ChatGLM2-6B-32K| 20.54  | 8.85   | 6.01   | 0.22   | 0.00   |\\n| LongChat-7B-v1.5-32K| 34.88 | 30.98  | 26.39  | 6.88   | 0.00   |\\n| LongChat-13B-16K| 51.43 | 44.99  | 30.75  | 7.94   | 0.00   |\\n| Vicuna-7B-v1.5-16K| 33.63 | 29.48  | 6.49   | 0.23   | 0.00   |\\n| Vicuna-13B-v1.5-16K| 66.40 | 45.69  | 32.44  | 21.28  | 11.65  |\\n| GPT-3.5-Turbo-16K| 65.58 | 49.92  | 33.37  | 23.50  | 14.25  |\\n\\nTable 26: MARC (CLS)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 31.60  | 21.02  | 22.52  | 17.92  | 15.95  |\\n| LLaMA2-7B-Chat| 32.01  | 27.26  | 18.19  | 15.48  | 11.87  |\\n| LLaMA2-13B    | 40.79  | 33.70  | 27.80  | 16.87  | 12.38  |\\n| LLaMA2-13B-Chat| 31.89 | 25.69  | 22.84  | 18.72  | 11.76  |\\n| ChatGLM2-6B   | 31.44  | 22.57  | 20.92  | 17.84  | 15.68  |\\n| ChatGLM2-6B-32K| 37.68  | 30.31  | 29.33  | 22.77  | 24.71  |\\n| LongChat-7B-v1.5-32K| 30.79 | 28.92  | 23.22  | 15.25  | 9.19   |\\n| LongChat-13B-16K| 26.88  | 24.92  | 23.17  | 14.93  | 12.08  |\\n| Vicuna-7B-v1.5-16K| 32.74 | 29.45  | 25.10  | 16.76  | 11.08  |\\n| Vicuna-13B-v1.5-16K| 35.06 | 32.61  | 31.64  | 23.05  | 19.37  |\\n| GPT-3.5-Turbo-16K| 32.28 | 29.77  | 25.12  | 23.19  | 23.04  |\\n\\nTable 27: DuReader (QA)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 23.80  | 6.10   | 0.72   | 0.09   | 0.05   |\\n| LLaMA2-7B-Chat| 26.39  | 17.88  | 11.14  | 4.67   | 0.00   |\\n| LLaMA2-13B    | 43.50  | 22.64  | 10.20  | 2.85   | 0.00   |\\n| LLaMA2-13B-Chat| 32.73 | 23.59  | 14.12  | 3.59   | 0.00   |\\n| ChatGLM2-6B   | 1.69   | 0.37   | 0.57   | 0.00   | 0.00   |\\n| ChatGLM2-6B-32K| 10.22  | 3.87   | 0.89   | 0.00   | 0.00   |\\n| LongChat-7B-v1.5-32K| 28.13 | 19.17  | 10.14  | 4.72   | 0.00   |\\n| LongChat-13B-16K| 27.78  | 16.21  | 3.11   | 1.28   | 0.00   |\\n| Vicuna-7B-v1.5-16K| 19.58  | 6.93   | 0.20   | 0.10   | 0.43   |\\n| Vicuna-13B-v1.5-16K| 40.92  | 27.95  | 7.15   | 4.18   | 3.76   |\\n| GPT-3.5-Turbo-16K| 34.84  | 31.15  | 19.03  | 14.29  | 10.23  |\\n\\nTable 28: Online Shopping (CLS)\\n\\n| Model          | 1k     | 2k     | 4k     | 6k     | 8k     |\\n|---------------|--------|--------|--------|--------|--------|\\n| LLaMA2-7B     | 67.17  | 33.62  | 20.27  | 7.54   | 4.00   |\\n| LLaMA2-7B-Chat| 64.12  | 31.26  | 14.43  | 1.29   | 0.00   |\\n| LLaMA2-13B    | 58.83  | 34.57  | 16.17  | 4.71   | 0.00   |\\n| LLaMA2-13B-Chat| 49.83  | 19.02  | 3.03   | 0.37   | 0.00   |\\n| ChatGLM2-6B   | 51.08  | 36.49  | 25.11  | 10.41  | 2.07   |\\n| ChatGLM"}
{"id": "acl-2024-long-832", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                | THUCNews (CLS, Explicit Single) | MNDS News (CLS, Explicit Single) | CNewsum (SUM) | CLTS+ (SUM) | CEPSUM (SUM) | CNNNews (SUM) | News2016 (SUM) |\\n|---------------------|---------------------------------|----------------------------------|--------------|------------|-------------|---------------|----------------|\\n|                     | 1k 2k 4k 6k 8k                  | 1k 2k 4k 6k                      | 1k 2k 4k 6k 8k| 1k 2k 4k 6k 8k| 1k 2k 4k 6k 8k| 1k 2k 4k 6k 8k| 1k 2k 4k 6k 8k|\\n| LLaMA2-7B           | 18.50 14.00 5.00 4.48 3.50      | 17.67 12.48 9.66 3.04            | 37.12 26.96 24.15 10.31 8.68 | 20.99 20.96 16.51 9.00 8.88 | 18.75 15.32 13.38 11.23 9.84 | 5.00 10.15 11.64 7.03 4.20 | 22.34 18.79 9.45 8.31 4.36 |\\n| LLaMA2-7B-Chat      | 30.50 22.50 10.50 11.44 3.50   | 22.57 12.09 11.03 4.18           | 36.83 31.13 12.40 11.31 7.94 | 20.58 19.72 16.87 10.08 7.75 | 16.69 9.00 3.98 2.12 3.23 | 10.04 7.44 3.49 2.13 2.88 | 19.78 20.01 11.21 9.41 5.39 |\\n| LLaMA2-13B          | 33.50 35.50 8.50 7.46 2.00      | 18.69 13.45 10.59 5.72           | 33.86 28.09 20.15 12.96 9.20 | 21.30 20.92 14.27 7.71 4.00 | 17.71 15.68 7.67 5.06 5.31 | 11.75 9.14 10.58 8.88 7.06 | 20.62 16.49 5.05 3.26 4.31 |\\n| LLaMA2-13B-Chat     | 35.50 36.50 15.00 10.95 5.50   | 23.09 15.51 11.46 9.70           | 34.12 26.76 23.76 17.05 10.34 | 21.22 19.83 17.50 8.50 3.83 | 9.90 9.37 5.14 4.48 3.12 | 9.84 6.27 8.39 8.34 5.12 | 9.84 9.37 5.14 4.48 3.12 |\\n| ChatGLM2-6B         | 17.00 17.50 6.00 3.48 3.00     | 28.61 14.23 10.56 9.45           | 37.26 23.70 10.97 8.89 10.06 | 28.13 18.41 11.73 7.54 | 25.08 18.96 14.35 14.14 10.39 | 13.91 13.99 15.63 12.42 14.85 | 10.84 18.96 14.35 14.14 10.39 |\\n| ChatGLM2-6B-32K     | 26.00 29.50 22.00 19.40 22.00  | 28.13 18.41 11.73 7.54           | 38.11 34.49 32.31 29.36 26.12 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n| LongChat-7B-v1.5-32K| 29.00 31.00 20.50 23.88 17.00 | 21.11 14.99 11.63 7.21           | 39.25 32.58 26.72 23.24 19.26 | 19.61 12.55 10.20 10.57 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n| LongChat-13B-16K    | 32.00 34.00 31.00 15.47 11.00  | 19.61 12.55 10.20 10.57          | 37.34 32.63 26.10 23.62 19.00 | 19.61 12.55 10.20 10.57 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n| Vicuna-7B-v1.5-16K  | 30.00 27.50 21.50 17.41 15.00 | 17.09 14.54 12.07 20.21          | 30.00 27.50 21.50 17.41 15.00 | 17.09 14.54 12.07 20.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n| Vicuna-13B-v1.5-16K | 40.50 38.50 34.50 20.40 16.50 | 20.76 15.95 13.31 11.92          | 40.50 38.50 34.50 20.40 16.50 | 20.76 15.95 13.31 11.92 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n| GPT-3.5-Turbo-16K   | 41.50 41.50 33.00 26.37 17.50  | 28.32 18.11 14.85 13.74          | 41.50 41.50 33.00 26.37 17.50 | 28.32 18.11 14.85 13.74 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 | 21.11 14.99 11.63 7.21 |\\n\\nTable 31: THUCNews (CLS, Explicit Single)\\nTable 32: MNDS News (CLS, Explicit Single)\\nTable 33: CNewsum (SUM)\\nTable 34: CLTS+ (SUM)\\nTable 35: CEPSUM (SUM)\\nTable 36: CNNNews (SUM)\\nTable 37: News2016 (SUM)\\nTable 38: LCSTS (SUM)\"}"}
{"id": "acl-2024-long-832", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 31.50 | 28.00 | 23.88 | 16.00 | 5.45  |\\n| LLaMA2-7B-Chat  | 35.50 | 30.50 | 19.90 | 14.50 | 8.98  |\\n| LLaMA2-13B      | 37.50 | 34.00 | 29.85 | 7.50  | 5.00  |\\n| LLaMA2-13B-Chat | 44.50 | 42.50 | 34.33 | 16.83 | 13.21 |\\n| ChatGLM2-6B     | 71.00 | 66.50 | 61.19 | 58.00 | 53.43 |\\n| ChatGLM2-6B-32K| 72.50 | 70.50 | 63.18 | 65.00 | 68.14 |\\n| LongChat-7B-v1.5-32K | 30.00 | 30.00 | 25.87 | 26.50 | 10.26 |\\n| LongChat-13B-16K| 23.00 | 29.00 | 24.38 | 30.00 | 18.96 |\\n| Vicuna-7B-v1.5-16K | 34.50 | 27.50 | 26.87 | 21.00 | 12.82 |\\n| Vicuna-13B-v1.5-16K | 56.00 | 49.50 | 52.74 | 50.00 | 30.98 |\\n| GPT-3.5-Turbo-16K| 85.00 | 84.00 | 81.09 | 76.00 | 74.02 |\\n\\nTable 39: C3 (QA)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 2.50  | 1.00  | 0.00  | 1.99  | 4.50  |\\n| LLaMA2-7B-Chat  | 7.50  | 2.00  | 0.50  | 2.49  | 7.50  |\\n| LLaMA2-13B      | 6.00  | 3.50  | 2.00  | 1.00  | 0.00  |\\n| LLaMA2-13B-Chat | 7.50  | 7.50  | 4.50  | 3.30  | 0.00  |\\n| ChatGLM2-6B     | 8.50  | 7.00  | 8.00  | 5.47  | 4.00  |\\n| ChatGLM2-6B-32K| 9.50  | 8.00  | 9.00  | 6.97  | 8.00  |\\n| LongChat-7B-v1.5-32K | 13.50 | 16.00 | 15.50 | 14.93 | 4.84  |\\n| LongChat-13B-16K| 8.50  | 7.50  | 16.00 | 11.44 | 7.50  |\\n| Vicuna-7B-v1.5-16K | 7.50  | 11.00 | 8.00  | 6.97  | 1.61  |\\n| Vicuna-13B-v1.5-16K | 11.00 | 19.00 | 24.50 | 14.93 | 4.00  |\\n| GPT-3.5-Turbo-16K| 18.00 | 16.00 | 13.00 | 14.43 | 18.50 |\\n\\nTable 40: NewsQA (QA)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 38.00 | 31.00 | 26.50 | 19.00 | 10.50 |\\n| LLaMA2-7B-Chat  | 41.00 | 37.00 | 34.00 | 24.00 | 10.00 |\\n| LLaMA2-13B      | 41.00 | 36.00 | 29.00 | 24.00 | 12.50 |\\n| LLaMA2-13B-Chat | 42.50 | 42.50 | 34.50 | 30.50 | 18.00 |\\n| ChatGLM2-6B     | 35.50 | 27.00 | 12.00 | 12.00 | 14.00 |\\n| ChatGLM2-6B-32K| 31.50 | 32.50 | 29.50 | 27.00 | 27.50 |\\n| LongChat-7B-v1.5-32K | 43.50 | 42.50 | 37.50 | 33.00 | 16.50 |\\n| LongChat-13B-16K| 43.00 | 37.00 | 35.50 | 32.00 | 17.50 |\\n| Vicuna-7B-v1.5-16K | 42.00 | 40.50 | 35.00 | 31.00 | 20.00 |\\n| Vicuna-13B-v1.5-16K | 39.50 | 38.00 | 36.00 | 9.50  | 11.50 |\\n| GPT-3.5-Turbo-16K| 39.50 | 36.50 | 32.50 | 31.00 | 32.50 |\\n\\nTable 41: Duorc (QA)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 10.27 | 6.66  | 2.20  | 2.01  | 0.69  |\\n| LLaMA2-7B-Chat  | 8.83  | 5.13  | 1.37  | 1.13  | 0.40  |\\n| LLaMA2-13B      | 20.99 | 12.85 | 2.92  | 1.78  | 0.72  |\\n| LLaMA2-13B-Chat | 15.93 | 9.24  | 3.64  | 2.58  | 1.32  |\\n| ChatGLM2-6B     | 12.85 | 7.61  | 0.28  | 0.69  | 0.38  |\\n| ChatGLM2-6B-32K| 13.44 | 5.05  | 3.60  | 3.37  | 3.22  |\\n| LongChat-7B-v1.5-32K | 14.10 | 10.97 | 8.00  | 6.39  | 4.78  |\\n| LongChat-13B-16K| 10.40 | 8.85  | 5.13  | 4.54  | 3.24  |\\n| Vicuna-7B-v1.5-16K | 19.88 | 20.31 | 8.61  | 7.74  | 3.17  |\\n| Vicuna-13B-v1.5-16K | 27.31 | 22.04 | 13.88 | 9.82  | 5.13  |\\n| GPT-3.5-Turbo-16K| 33.30 | 28.38 | 24.33 | 23.94 | 18.48 |\\n\\nTable 42: News Commentary en2zh (TRAN)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 13.28 | 7.42  | 0.89  | 0.22  | 0.01  |\\n| LLaMA2-7B-Chat  | 8.16  | 4.01  | 0.50  | 0.32  | 0.09  |\\n| LLaMA2-13B      | 20.28 | 13.89 | 2.43  | 1.38  | 0.34  |\\n| LLaMA2-13B-Chat | 8.83  | 7.19  | 2.53  | 1.56  | 0.58  |\\n| ChatGLM2-6B     | 6.80  | 7.51  | 0.16  | 0.04  | 0.02  |\\n| ChatGLM2-6B-32K| 5.55  | 7.32  | 1.14  | 2.26  | 2.21  |\\n| LongChat-7B-v1.5-32K | 15.01 | 9.61  | 7.31  | 2.91  | 3.08  |\\n| LongChat-13B-16K| 12.82 | 9.55  | 4.18  | 2.30  | 1.13  |\\n| Vicuna-7B-v1.5-16K | 17.64 | 15.14 | 10.58 | 6.76  | 2.35  |\\n| Vicuna-13B-v1.5-16K | 20.17 | 17.43 | 12.88 | 11.32 | 7.35  |\\n| GPT-3.5-Turbo-16K| 26.23 | 22.22 | 17.99 | 15.94 | 13.12 |\\n\\nTable 43: News Commentary zh2en (TRAN)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 9.30  | 6.21  | 1.01  | 0.91  | 1.05  |\\n| LLaMA2-7B-Chat  | 15.20 | 9.40  | 3.05  | 2.17  | 0.88  |\\n| LLaMA2-13B      | 14.58 | 10.47 | 2.71  | 3.00  | 2.14  |\\n| LLaMA2-13B-Chat | 13.94 | 10.78 | 2.16  | 3.09  | 2.32  |\\n| ChatGLM2-6B     | 14.86 | 0.98  | 0.07  | 0.02  | 0.00  |\\n| ChatGLM2-6B-32K| 13.67 | 5.19  | 1.84  | 1.17  | 1.18  |\\n| LongChat-7B-v1.5-32K | 20.43 | 9.78  | 4.23  | 2.93  | 3.03  |\\n| LongChat-13B-16K| 6.43  | 5.50  | 2.91  | 2.06  | 2.83  |\\n| Vicuna-7B-v1.5-16K | 23.75 | 11.36 | 5.93  | 2.01  | 3.23  |\\n| Vicuna-13B-v1.5-16K | 22.52 | 20.22 | 9.77  | 4.03  | 3.12  |\\n| GPT-3.5-Turbo-16K| 25.84 | 22.48 | 13.99 | 9.84  | 9.39  |\\n\\nTable 44: Tedtalks en2zh (TRAN)\\n\\n| Model Type       | 1k    | 2k    | 4k    | 6k    | 8k    |\\n|------------------|-------|-------|-------|-------|-------|\\n| LLaMA2-7B       | 13.82 | 5.32  | 0.25  | 0.00  | 0.00  |\\n| LLaMA2-7B-Chat  | 17.49 | 5.26  | 1.99  | 0.93  | 0.00  |\\n| LLaMA2-13B      | 19.94 | 5.55  | 1.75  | 0.00  | 0.00  |\\n| LLaMA2-13B-Chat | 17.37 | 5.74  | 2.64  | 0.00  | 0.00  |\\n| ChatGLM2-6B     | 13.22 | 4.26  | 1.03  | 0.19  | 0.05  |\\n| ChatGLM2-6B-32K| 9.72  | 2.91  | 1.53  | 1.17  | 1.18  |\\n| LongChat-7B-v1.5-32K | 12.06 | 2.01  | 0.43  | 0.09  | 0.00  |\\n| LongChat-13B-16K| 14.78 | 2.05  | 0.99  | 1.11  | 0.82  |\\n| Vicuna-7B-v1.5-16K | 20.46 | 5.97  | 1.97  | 2.83  | 1.32  |\\"}
{"id": "acl-2024-long-832", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are given multiple news articles below. Each of them belongs to one of the following categories:\\n1. crime, law and justice\\n2. arts, culture, entertainment and media\\n3. economy, business and finance\\n4. disaster, accident and emergency incident\\n5. environment\\n6 education\\n7. health\\n8. human interest\\n9. lifestyle and leisure\\n10. politics\\n11. labour\\n12. religion and belief\\n13. science and technology\\n14. society\\n15. sport\\n16. conflict, war and peace\\n17. weather\\n\\nYou will be asked to return the category of a news article I specified at the end.\\n\\nArticle AD3258: Rarely do the worlds of art and science intersect, but they did with famed Dutch artist Escher. Even if you do not recognize his name, it is likely you have seen his work without knowing it. One of the largest collections of his work is now on display in the US.\\n\\nArticle D55E47: On Sunday, NBC's Meet The Press will air an interview with President Donald Trump, conducted by the network's political director, Chuck Todd...\\n\\nArticle 5675E9: The full extent of the ferry disaster in the Iraqi city of Mosul is becoming clearer...\\n\\nQuestion: What is the category of article AD3258?\\nAnswer: arts, culture, entertainment and media\\n\\nArticle 11BD15: Read the full article by Catherine Frompovitch at NaturalBlaze\\n\\nAbstract\\nThe human cytochrome P450 (CYP) superfamily comprises 57 genes. These genes code for enzymes that can have a role in: metabolism of drugs, foreign chemicals, arachidonic acid and eicosanoids; cholesterol metabolism and bile-acid biosynthesis; steroid synthesis and metabolism; vitamin D(3) synthesis and metabolism; retinoic acid hydroxylation;...\\n\\nArticle 92FF60: Undoubtedly, this latest flooding crisis in Iran reveals the highly vicious nature of the current U.S. Administration with regards to the application of collective punishment of a target nation...\\n\\n...\"}"}
{"id": "acl-2024-long-832", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Article 1: It's style that will leave you looking classy and feminine. This is a twist on the classic messier and more mermaid-like. This more obscure braid requires skill, but results in an interesting look. It's cute, classic, and easy to do. It looks bit medieval and very eye-catching. It's ideal for weddings or other elegant occasions.\\n\\nArticle 2: It's a green app that contains a white phone icon inside a white text bubble. It's at the top-center of the screen. Select the chat with the attachment you wish to download. Select the attachment you wish to download. It's in the upper-right corner of the screen. The attachment has been saved to your Android device.\\n\\nQuestion: Summarize the article related to \\\"How to Style Very Long Hair\\\" using a few instructive sentences.\\n\\nSummary: Do a French braid. Make an intricate fishtail braid. Try a Dutch braid. Do a triple braid. Make a crazy braid. Do a cascading waterfall braid.\\n\\nArticle 1: Make sure that the shoe is the appropriate length and width for your child's foot. If a shoe squeeze a child's foot too much, it can cause the child to have foot conditions such as blisters and calluses. Remember that your child's foot will grow at a rapid pace and that he may need to be fitted every few months for a new size. So, if your child takes off his shoe and you notice that there are red marks on your child's foot, it may be time to take your child in for a new fitting and buy him a new shoe...\\n\\nArticle 2: Learning and studying shouldn't be stressful. Being stressed out can actually make it harder to learn and remember things. Think about the reasons why you're stressed out and try to resolve those reasons (remove them from your life). For example, if you get stressed out about assignments because you leave them to the last minute to finish, create yourself a study schedule. Build enough time into the study schedule so that you finish your assignments well enough in advance of the due dates to eliminate any of the stress you were feeling. If the grades you're receiving aren't that great it can be easy to let negativity take over.\\n\\nQuestion: Summarize the article related to \\\"How to Fit Your Kid for Shoes\\\" using a few instructive sentences.\\n\\nSummary: Make sure that the shoe is the appropriate length and width for your child's foot. If a shoe squeeze a child's foot too much, it can cause the child to have foot conditions such as blisters and calluses. Remember that your child's foot will grow at a rapid pace and that he may need to be fitted every few months for a new size. So, if your child takes off his shoe and you notice that there are red marks on your child's foot, it may be time to take your child in for a new fitting and buy him a new shoe...\\n\\nThe article id is 0A1A04. Rarely do the worlds of art and science intersect, but they did with famed Dutch artist Escher. Even if you do not recognize his name, it is likely you have seen his work without knowing it. One of the largest collections of his work is now on display in the US.\\n\\nThe article id is 95A4BF. On Sunday, NBC's Meet The Press will air an interview with President Donald Trump, conducted by the network's political director, Chuck Todd. While Todd's interviews with 2020 Democratic contenders have consisted largely of challenges from the left interspersed with the odd softball, Trump is unlikely to receive the same friendly treatment.\\n\\nThe article id is A7D6BE. The full extent of the ferry disaster in the Iraqi city of Mosul is becoming clearer. Civil Defence says the number of dead is now at least 120, while 100 people are still missing. Iraq's Prime Minister Adel Abdul Mahdi is formally requesting a local governor be sacked over the incident.\\n\\nQuestion: Provide me the article id of all the news articles related to 'arts, culture, entertainment and media.'\\n\\nAnswer: 0A1A04, 95A4BF.\"}"}
{"id": "acl-2024-long-832", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pratia is a genus of flowering plants in the family Campanulaceae, native to Asia, Australia and New Zealand.\\n\\nSutherlandia is a genus of flowering plants in the family Fabaceae.\\n\\nQuestion: Are Sutherlandia and Pratia in the same family?\\n\\nAnswer: no.\\n\\nThe Stresa Festival Orchestra is a formation composed by young and talented musicians, coming from renewed European orchestras, calling by Gianandrea Noseda to perform every year some original production for the Stresa Festival. The debut of the Orchestra, on 26 August 2003 with Mozart' \u201cDon Giovanni\u201d, began the project of the concert performances of different operas: \u201cCos\u00ec fan tutte\u201d (2004), \u201cLe nozze di Figaro\u201d (2005), \u201cThe magic flute\u201d (2006), \u201cLa clemenza di Tito\u201d (2007), ...\\n\\nThe Metropolitan City of Messina (Italian: \u201cCitt\u00e0 metropolitana di Messina\u201d) is a metropolitan city in Sicily, Italy. Its capital is the city of Messina. It replaced the Province of Messina and comprises the city of Messina and other 107 municipalities (\u201ccomuni\u201d). According to Eurostat the FUA of the metropolitan area of Messina has in 2014 277,584 inhabitants.\\n\\nPompei is a city and \u201ccomune\u201d in the Metropolitan City of Naples in Italy, home of the ancient Roman ruins part of the UNESCO World Heritage Sites.\\n\\nBanca di Credito Popolare S.C.p.A (BCP) is an Italian cooperative bank based in Torre del Greco, in Metropolitan City of Naples, Campania. Most of the revenue of the bank came from the Metropolitan City of Naples, which the bank had 44 branches in the metropolitan city.\\n\\nQuestion: What Metropolitan City was Massimo Giordano born in?\\n\\nAnswer:\"}"}
