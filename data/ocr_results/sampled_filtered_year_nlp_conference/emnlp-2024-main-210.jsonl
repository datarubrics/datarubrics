{"id": "emnlp-2024-main-210", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Criteria Description | Description |\\n|----------------------|-------------|\\n| Factuality           | Whether the information provided in the response is accurate, based on reliable facts and data. |\\n| User Satisfaction    | Whether the response meets the user's question and needs, and provides a comprehensive and appropriate answer to the question. |\\n| Logical Coherence    | Whether the response maintains overall consistency and logical coherence between different sections, avoiding self-contradiction. |\\n| Richness             | Whether the response includes rich info, depth, context, diversity, detailed explanations, and examples to meet user needs and provide a comprehensive understanding. |\\n| Creativity           | Whether the response is innovative or unique, providing novel insights or solutions. |\\n| Fairness and Responsibility | Whether the advice or information provided in the response is feasible, carries a certain degree of responsibility and considers potential risks and consequences. |\\n| Completeness         | Whether the response provides sufficient information and details to meet the user's needs, and whether it avoids omitting important aspects. |\\n| Clarity              | Whether the response is clear and understandable, and whether it uses concise language and structure so that the user can easily understand it. |\\n| Engagement           | Whether the answer is interesting and attractive, helps users relax, and provides high-quality emotional value or entertainment value, etc. |\\n| Appropriateness      | Content is suitable for all users and avoids inappropriate or offensive content. |\"}"}
{"id": "emnlp-2024-main-210", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Intent-aware Criteria\\n\\nYou are asked to assess the quality of an AI assistant's answer to a user's question as an impartial judge. Since the type of answer you are evaluating is Solve Professional Problem, you need to evaluate the answer in the following 5 criteria:\\n\\n1. Factuality: Whether the information provided is accurate and based on reliable facts and data.\\n2. User Satisfaction: Whether the response meets the user's question and needs and provides a comprehensive and appropriate answer to the question.\\n3. Clarity: Whether the response is clear and understandable, and whether it uses concise language and structure so that the user can easily understand it.\\n4. Logical Coherence: Whether the response maintains overall consistency and logical coherence between different sections, avoiding self-contradiction.\\n5. Completeness: Whether the response provides sufficient information and details to meet the user's needs, and whether it avoids omitting important aspects.\\n\\nNote that a longer answer is not always better, the answer that is concise and meets the above requirements is the best.\\n\\nChain of thought reasoning steps:\\n\\nWe will provide you with the user's question, an 8-score reference answer, and answers from the AI assistant that needs your assessment. When starting your evaluation, you need to follow the reasoning steps below:\\n\\n1. Compare the AI assistant's answer with the reference answer, point out any shortcomings in the AI assistant's answer, and explain further.\\n2. Evaluate the AI assistant's answer in terms of the different criteria, giving each criterion a score from 1 to 10 after the evaluation of each.\\n3. Finally, combine the evaluations from each criterion and give the AI assistant's answer a composite score of 1 to 10.\\n4. Your scoring needs to be as rigorous as possible and adhere to the following scoring rules: in general, the higher the quality of the model's answers, the higher the score. The two most important criteria are factual correctness and fulfillment of user needs, and the scores for these two dimensions dominate the final composite score.\\n\\nScoring Standards:\\n\\nWhen the model answer has irrelevance to the question, or intrinsically factually incorrect, or generates harmful content, the total score should be 1 to 2;\\n\\nWhen the model answer has no serious errors and is largely harmless, but is of low quality and does not meet user requirements, the total score must be 3 to 4;\\n\\nWhen the model answer basically meets the user's needs but performs poorly on some criteria and is of medium quality, the total score can be 5 to 6;\\n\\nWhen the quality of the model response is similar to the reference answer and performs well in all criteria, the total score should be 7 to 8;\\n\\nA score of 9 to 10 can only be achieved if the model significantly exceeds the quality of the reference answer, adequately addresses the user's question and all the needs, and is close to a perfect score on all criteria. As an example, the reference answer would receive a score of 8.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You need to evaluate and explain before you score. Your explanation of each criterion needs to be followed by the scoring. After that, at the end of your answer, return all of your scores in the following dictionary format, including the curly brackets, and make sure that your scores are integers:\\n\\n{'Dimension 1': scoring, 'Dimension 2': scoring, ... , 'Final Score': Score}, e.g. {'Factuality': 9, 'User Satisfaction': 6, ... , 'Final Score': 7}.\\n\\nTable 11: English Instruction for evaluating Solve Professional Problem intent questions. Part 2.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 20, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u8bf7\u4f60\u4ee5\u516c\u6b63\u7684\u8bc4\u5224\u8005\u7684\u8eab\u4efd\uff0c\u8bc4\u4f30\u4e00\u4e2aAI\u52a9\u624b\u5bf9\u4e8e\u7528\u6237\u63d0\u95ee\u7684\u56de\u7b54\u7684\u8d28\u91cf\u3002\u7531\u4e8e\u60a8\u8bc4\u4f30\u7684\u56de\u7b54\u7c7b\u578b\u662f\u3010\u89e3\u51b3\u4e13\u4e1a\u95ee\u9898\u3011\uff0c\u56e0\u6b64\u9700\u8981\u4ece\u4e0b\u9762\u76845\u4e2a\u7ef4\u5ea6\u5bf9\u56de\u7b54\u8fdb\u884c\u8bc4\u4f30\u3002\\n\\n1. \u5b8c\u6574\u6027 (Completeness)\\n2. \u51c6\u786e\u6027 (Factuality)\\n3. \u6613\u8bfb\u6027 (Clarity)\\n4. \u7406\u89e3\u6b65\u9aa4 (Chain of reasoning)\\n5. \u7528\u6237\u6ee1\u610f\u5ea6 (User Satisfaction)\\n\\n\u8bc4\u5206\u6807\u51c6\uff08\u6ee1\u520610\u5206\uff09\\n- 1\u5206\uff1a\u4e0d\u76f8\u5173\u6216\u4e0d\u9002\u7528\\n- 2\u5206\uff1a\u4e0e\u95ee\u9898\u76f8\u5173\u4f46\u7f3a\u4e4f\u8be6\u7ec6\u89e3\u91ca\\n- 3\u5206\uff1a\u90e8\u5206\u95ee\u9898\u6ca1\u6709\u89e3\u7b54\\n- 4\u5206\uff1a\u89e3\u7b54\u90e8\u5206\u6b63\u786e\\n- 5\u5206\uff1a\u89e3\u7b54\u57fa\u672c\u6b63\u786e\u4f46\u5b58\u5728\u4fe1\u606f\u9057\u6f0f\\n- 6\u5206\uff1a\u89e3\u7b54\u57fa\u672c\u6b63\u786e\u4e14\u4fe1\u606f\u5b8c\u6574\\n- 7\u5206\uff1a\u89e3\u7b54\u51c6\u786e\u4e14\u4fe1\u606f\u5168\u9762\\n- 8\u5206\uff1a\u89e3\u7b54\u51c6\u786e\u4e14\u4fe1\u606f\u6e05\u6670\\n- 9\u5206\uff1a\u89e3\u7b54\u51c6\u786e\u4e14\u4fe1\u606f\u8be6\u7ec6\\n- 10\u5206\uff1a\u89e3\u7b54\u51c6\u786e\u4e14\u4fe1\u606f\u8be6\u5c3d\\n\\n\u8bf7\u6839\u636e\u5b9e\u9645\u60c5\u51b5\u5bf9\u6bcf\u4e2a\u7ef4\u5ea6\u8fdb\u884c\u8bc4\u5206\uff0c\u5e76\u603b\u7ed3\u4f60\u7684\u603b\u4f53\u8bc4\u4ef7\u3002\"}"}
{"id": "emnlp-2024-main-210", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 13: Bi-lingual Benchmark Results. Part 1.\\n\\n| Model            | Factual QA | Solve Problem | Text Assistant | English | Chinese |\\n|------------------|------------|---------------|----------------|---------|---------|\\n| GPT-4            | 8.55       | 8.47          | 8.42           | 8.21    | 8.11    |\\n| Qwen-max         | 8.30       | 7.44          | 8.14           | 7.44    | 8.38    |\\n| Claude-3-opus    | 8.05       | 7.64          | 8.12           | 7.54    | 7.62    |\\n| GLM-4            | 8.08       | 7.09          | 7.93           | 7.49    | 7.76    |\\n| Moonshot-v1-8k   | 7.67       | 7.56          | 8.00           | 7.22    | 7.57    |\\n| ERNIE-Bot-4      | 7.22       | 7.15          | 7.96           | 7.47    | 7.03    |\\n| GPT-3.5-turbo    | 7.45       | 6.55          | 7.07           | 6.48    | 7.35    |\\n| Baichuan2-Turbo  | 7.33       | 6.67          | 6.68           | 6.57    | 6.57    |\\n| Deepseek-chat    | 6.75       | 6.07          | 6.86           | 6.70    | 6.78    |\\n| Spark-3.5        | 6.43       | 6.79          | 6.65           | 7.03    | 7.08    |\\n\\n### Table 14: Bi-lingual Benchmark Results. Part 2.\\n\\n| Model            | Factual QA | Solve Problem | Text Assistant | English | Chinese |\\n|------------------|------------|---------------|----------------|---------|---------|\\n| GPT-4            | 7.48       | 7.94          | 8.50           | 7.70    | 7.43    |\\n| GLM-4            | 7.45       | 7.14          | 8.07           | 6.93    | 7.50    |\\n| Qwen-max         | 7.59       | 7.21          | 7.64           | 6.99    | 7.88    |\\n| Moonshot-v1-8k   | 7.18       | 7.06          | 6.71           | 7.11    | 7.38    |\\n| Claude-3-opus    | 6.86       | 7.04          | 7.21           | 7.06    | 7.13    |\\n| GPT-3.5-turbo    | 6.64       | 6.29          | 6.79           | 6.13    | 7.38    |\\n| Deepseek-chat    | 6.55       | 5.98          | 7.00           | 5.27    | 7.13    |\\n| ERNIE-Bot-4      | 6.95       | 7.13          | 6.64           | 7.30    | 6.25    |\\n| Baichuan2-Turbo  | 6.27       | 6.37          | 6.64           | 6.00    | 7.00    |\\n| Spark-3.5        | 6.43       | 6.79          | 6.65           | 7.03    | 7.08    |\"}"}
{"id": "emnlp-2024-main-210", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Service                     | Score |\\n|----------------------------|-------|\\n| GPT-4                      | 8.63  |\\n| GPT-3.5                    | 6.95  |\\n| Search Engine (Google Top-1) | 5.14  |\\n| RAG (GPT-4 + Google Top-5 page) | 6.47  |\\n\\nTable 15: Comparison between LLM services and Search Engine in 57 Factual QA cases from the URS dataset.\\n\\n| Service                     | Score |\\n|----------------------------|-------|\\n| GPT-4                      | 9.00  |\\n| GPT-3.5                    | 8.50  |\\n| Translator (Google)        | 7.00  |\\n\\nTable 16: Comparative Evaluation of LLM Services and Translator in 8 translation-related Text Assistant Scenarios from the URS dataset.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 23, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u5982\u4f55\u770b\u5f85AI\u5c06\u5bfc\u81f4\u5927\u90e8\u5206\u4eba\u7684\u89c2\u70b9\\n\\nAI\u7684\u53d1\u5c55\u786e\u5b9e\u53ef\u80fd\u5bf9\u4e00\u4e9b\u884c\u4e1a\u4ea7\u751f\u5f71\u54cd\uff0c\u5bfc\u81f4\u4e00\u90e8\u5206\u5de5\u4f5c\u7684\u673a\u5668\u53d6\u4ee3\u3002\u4f46\u8fd9\u5e76\u4e0d\u610f\u5473\u7740\u4f1a\u5bfc\u81f4\u5927\u90e8\u5206\u4eba\u7684\u5931\u4e1a\uff0c\u4e3b\u8981\u6709\u4ee5\u4e0b\u51e0\u70b9\u539f\u56e0\u3002\\n\\n1. AI\u4e5f\u4f1a\u521b\u9020\u65b0\u7684\u673a\u4f1a\u3002\u5c3d\u7ba1AI\u53ef\u80fd\u66ff\u4ee3\u4e00\u4e9b\u4eba\u529b\u5bc6\u96c6\u578b\u7684\u5de5\u4f5c\uff0c\u4f46\u540c\u65f6\u4e5f\u53ef\u80fd\u521b\u9020\u51fa\u65b0\u7684\u5de5\u4f5c\u5c97\u4f4d\u3002\u4f8b\u5982\uff0c\u73b0\u5728\u5f88\u591a\u516c\u53f8\u5df2\u7ecf\u5f00\u59cb\u96c7\u4f63\u4e0eAI\u76f8\u5173\u7684\u6280\u672f\u4eba\u5458\uff0c\u5982\u6570\u636e\u79d1\u5b66\u5bb6\u3001\u673a\u5668\u5b66\u4e60\u5de5\u7a0b\u5e08\u7b49\u3002\\n\\n2. AI\u53ef\u80fd\u63d0\u9ad8\u4eba\u4eec\u7684\u5de5\u4f5c\u6548\u7387\uff0c\u8ba9\u4eba\u4eec\u6709\u66f4\u591a\u7684\u65f6\u95f4\u6295\u5165\u5230\u9700\u8981\u4eba\u7c7b\u7279\u8d28\uff08\u5982\u521b\u610f\u3001\u540c\u7406\u5fc3\uff09\u7684\u5de5\u4f5c\u4e2d\u3002\\n\\n3. \u793e\u4f1a\u548c\u653f\u5e9c\u4e5f\u53ef\u4ee5\u91c7\u53d6\u4e00\u4e9b\u653f\u7b56\uff0c\u5982\u6559\u80b2\u6539\u9769\u3001\u804c\u4e1a\u57f9\u8bad\u3001\u7a0e\u6536\u8c03\u6574\u7b49\uff0c\u5e2e\u52a9\u4eba\u4eec\u9002\u5e94\u65b0\u7684\u5c31\u4e1a\u5f62\u5f0f\u3002\\n\\n\u603b\u7684\u6765\u8bf4\uff0cAI\u7684\u53d1\u5c55\u53ef\u80fd\u4f1a\u6539\u53d8\u5c31\u4e1a\u5e02\u573a\u7684\u9762\u8c8c\uff0c\u4f46\u4e0d\u4e00\u5b9a\u4f1a\u5bfc\u81f4\u5927\u90e8\u5206\u4eba\u7684\u5931\u4e1a\u3002\u6211\u4eec\u9700\u8981\u79ef\u6781\u5e94\u5bf9\uff0c\u6301\u7eed\u5b66\u4e60\uff0c\u63d0\u5347\u81ea\u5df1\u7684\u7ade\u4e89\u529b\u3002\\n\\n\u5173\u4e8eAI\u53ef\u80fd\u5bfc\u81f4\u5927\u89c4\u6a21\u5931\u4e1a\u7684\u89c2\u70b9\uff0c\u6211\u7684\u770b\u6cd5\u662f\uff1a\\n\\n1. AI\u6280\u672f\u7684\u53d1\u5c55\u548c\u5e94\u7528\u786e\u5b9e\u4f1a\u5728\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u6539\u53d8\u5c31\u4e1a\u5e02\u573a\uff0c\u4e00\u4e9b\u5de5\u4f5c\u53ef\u80fd\u88abAI\u53d6\u4ee3\uff0c\u5c24\u5176\u662f\u90a3\u4e9b\u9ad8\u5ea6\u91cd\u590d\u3001\u7a0b\u5f0f\u5316\u7684\u4f5c\\n\\n2. \u5386\u53f2\u6765\u770b\uff0c\u6bcf\u4e00\u6b21\u6280\u672f\u9769\u547d\u90fd\u4f34\u968f\u7740\u90e8\u5206\u884c\u4e1a\u7684\u6d88\u5931\u548c\u65b0\u884c\u4e1a\u7684\u51fa\u73b0\u3002\u6bd4\u5982\u5de5\u4e1a\u9769\u547d\u5bfc\u81f4\u624b\u5de5\u4e1a\u8005\u5931\u4e1a\uff0c\u4f46\u521b\u9020\u4e86\u66f4\u591a\u5de5\u4eba\u7684\u804c\u4f4d\u3002\\n\\n3. \u56e0\u6b64\uff0cAI\u5e26\u6765\u7684\u5e94\u662f\u5c31\u4e1a\u7ed3\u6784\u7684\u8f6c\u578b\u548c\u5347\u7ea7\uff0c\u800c\u4e0d\u662f\u5355\u7eaf\u7684\u5931\u4e1a\u3002\\n\\n4. \u4e3a\u4e86\u9002\u5e94AI\u65f6\u4ee3\uff0c\u6559\u80b2\u548c\u804c\u4e1a\u57f9\u8bad\u9700\u8981\u4e0e\u65f6\u4ff1\u8fdb\u3002\u5b66\u6821\u5e94\u52a0\u5f3aSTEM\u6559\u80b2\uff0c\u57f9\u517b\u5b66\u751f\u7684\u79d1\u6280\u7d20\u517b\uff1b\u4f01\u4e1a\u548c\u653f\u5e9c\u5e94\u79ef\u6781\u5c55\\n\\n5. \u653f\u5e9c\u5728AI\u65f6\u4ee3\u626e\u6f14\u7740\u81f3\u5173\u91cd\u8981\u7684\u89d2\u8272\u3002\u4e00\u65b9\u9762\u8981\u5927\u529b\u53d1\u5c55AI\u4ea7\u4e1a\uff0c\u62a2\u5360\u6280\u672f\u5236\u9ad8\u70b9\uff1b\u53e6\u4e00\u65b9\u9762\u8981\u5b8c\u5584\u793e\u4f1a\u4fdd\u969c\u4f53\u7cfb\uff0c\u4e3a\u5c31\u4e1a\u8f6c\u578b\u63d0\u4f9b\u5b89\u5168\u7f51\uff0c\u5c3d\u91cf\u964d\u4f4e\u9635\u75db\u3002\\n\\n6. AI\u7684\u6700\u7ec8\u76ee\u6807\u5e94\u8be5\u662f\u7528\u6280\u672f\u9020\u798f\u4eba\u7c7b\uff0c\u800c\u4e0d\u662f\u53d6\u4ee3\u4eba\u7c7b\u3002\u5728\u5c31\u4e1a\u95ee\u9898\u4e0a\uff0c\u6211\u4eec\u8981\u52aa\u529b\u5b9e\u73b0\u4eba\u673a\u534f\u4f5c\uff0c\u53d1\u6325\u4eba\u5de5\u667a\u80fd\u548c\u4eba\u7c7b\u667a\u6167\u5404\u81ea\u7684\u4f18\u52bf\uff0c\u521b\u9020\u66f4\u52a0\u667a\u80fd\u548c\u4eba\u6027\u5316\u7684\u5de5\\n\\n\u603b\u4e4b\uff0c\u5bf9AI\u53ef\u80fd\u5e26\u6765\u7684\u5931\u4e1a\u98ce\u9669\uff0c\u6211\u4eec\u9700\u8981\u4fdd\u6301\u4e50\u89c2\u548c\u5f00\u653e\u7684\u5fc3\u6001\uff0c\u672a\u96e8\u7ef8\u7f2a\uff0c\u5728\u987a\u5e94\u6280\u672f\u53d1\u5c55\u8d8b\u52bf\u7684\u540c\u65f6\uff0c\u52aa\u529b\u521b\u9020\u4e00\u4e2a\u66f4\u52a0\u5305\u5bb9\u548c\u53ef\u6301\u7eed\u7684\u672a\u6765\u3002\u8fd9\u9700\u8981\u5168\u793e\u4f1a\u5171\u540c\u52aa\u529b\u3002\"}"}
{"id": "emnlp-2024-main-210", "page_num": 24, "content": "{\"primary_language\":\"zh\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u6709\\n\u6ca1\\n\u6709\\n\u4e00\u4e9b\\n\u80fd\\n3d\\n\u6253\\n\u5370\\n\u7684\\n\u70b9\\n\u5b50\\n\\n1. \u4e2a\u6027\u5316\u9996\u9970: \u4f7f\u75283D\u6253\u5370\u6280\u672f\u53ef\u4ee5\u6253\u9020\u51fa\u4e00\u4e9b\u72ec\u4e00\u65e0\u4e8c\u7684\u9996\u9970\u6b3e\u5f0f,\u5982\u6212\u6307\u3001\u9879\u94fe\u3001\u8033\u73af\u7b49\u3002\\n2. \u624b\u673a\u58f3: \u53ef\u4ee5\u6309\u7167\u624b\u673a\u7684\u5177\u4f53\u578b\u53f7\u548c\u4e2a\u4eba\u559c\u597d\u8bbe\u8ba1\u7279\u8272\u624b\u673a\u58f3\u3002\\n3. \u5bb6\u5c45\u88c5\u9970\u54c1: \u5982\u70db\u53f0\u3001\u76f8\u6846\u3001\u82b1\u76c6\u7b49\u3002\\n4. \u52a8\u624b\u73a9\u5177: \u4f8b\u5982\u76ca\u667a\u62fc\u56fe,\u4e5f\u53ef\u4ee5\u8bbe\u8ba1\u7279\u8272\u7684\u68cb\u76d8\u6e38\u620f\u3002\\n5. \u529e\u516c\u5ba4\u5de5\u5177: \u5982\u7b14\u7b52\u3001\u540d\u7247\u67b6\u7b49\u3002\\n6. \u53a8\u623f\u7528\u5177: \u4f8b\u5982\u997c\u5e72\u6a21\u5177,\u4e5f\u53ef\u4ee5\u8bbe\u8ba1\u72ec\u7279\u7684\u9910\u5177,\u5982\u559d\u5496\u5561\u7684\u676f\u5b50\u3002\\n7. \u4e2a\u6027\u5316\u5de5\u7f69: \u5728\u75ab\u60c5\u671f\u95f4,\u53ef\u4ee5\u8bbe\u8ba1\u4e00\u4e9b\u6709\u8da3\u76843D\u53e3\u7f69\u3002\\n8. \u6700\u5c0f\u5316\u5bb6\u5177\u6a21\u578b: \u8fd9\u5bf9\u5ba4\u5185\u8bbe\u8ba1\u5e08\u6765\u8bf4\u975e\u5e38\u6709\u7528,\u4ed6\u4eec\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u623f\u95f4\u7684\u7cbe\u786e\u6a21\u578b\u3002\\n9. \u81ea\u5b9a\u4e49\u5243\u987b\u5200\u628a\u624b: \u901a\u8fc73D\u6253\u5370,\u4f60\u53ef\u4ee5\u8f7b\u677e\u83b7\u5f97\u7b26\u5408\u4f60\u4e2a\u4eba\u559c\u597d\u548c\u63e1\u6301\u65b9\u5f0f\u7684\u5243\u987b\u5200\u628a\u624b\u3002\\n10. \u76ae\u80a4\u6a21\u578b: \u5bf9\u76ae\u80a4\u79d1\u533b\u751f,\u53ef\u4ee5\u6253\u5370\u51fa\u4e00\u4e9b\u76ae\u80a4\u75be\u75c5\u7684\u4e09\u7ef4\u6a21\u578b,\u6709\u52a9\u4e8e\u4ed6\u4eec\u89e3\u91ca\u75be\u75c5\u7684\u5177\u4f53\u60c5\u51b5\u3002\\n11. 3D\u6253\u5370\u7684\u978b\u5b50: \u901a\u8fc7\u7cbe\u786e\u6d4b\u91cf\u811a\u7684\u5c3a\u5bf8,\u53ef\u4ee5\u521b\u5efa\u5b8c\u5168\u5339\u914d\u811a\u578b\u7684\u978b\u5b50\u3002\\n\\n\u8981\u6ce8\u7684\u662f,\u6709\u4e9b\u4ea7\u54c1\u5982\u98df\u54c1\u3001\u53e3\u7f69\u7b49\u9700\u8981\u8003\u8651\u6750\u6599\u7684\u5b89\u5168\u6027\u548c\u8212\u9002\u6027\u3002\u57283D\u6253\u5370\u4efb\u4f55\u7269\u54c1\u65f6,\u4e00\u5b9a\u8981\u6ce8\u610f\u6750\u6599\u7684\u9009\u62e9\u548c\u52b3\u52a8\u5b89\u5168\u3002\"}"}
{"id": "emnlp-2024-main-210", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u65e5\u5e38\u8865\u5145\u5242\u7684\u63a8\u8350\u6444\u5165\u91cf\u662f\u591a\u5c11\uff1f\\n\\n\u63a8\u8350\u7684\u65e5\u5e38\u8865\u5145\u5242\u6444\u5165\u91cf\u56e0\u5e74\u9f84\u3001\u6027\u522b\u3001\u5065\u5eb7\u72b6\u51b5\u548c\u7279\u5b9a\u9700\u6c42\u800c\u5f02\u3002\\n\\n\u4ee5\u4e0b\u662f\u51e0\u79cd\u5e38\u89c1\u8865\u5145\u5242\u7684\u5927\u81f4\u6307\u5bfc\u91cf\uff1a\\n\\n1. **\u7ef4\u751f\u7d20D**\uff1a\u6210\u4eba\u6bcf\u65e5\u63a8\u8350\u6444\u5165\u91cf\u4e3a400-800 IU\uff08\u56fd\u9645\u5355\u4f4d\uff09\uff0c\u4f46\u8bb8\u591a\u4e13\u5bb6\u63a8\u8350\u6bcf\u65e5\u6444\u51651000-2000 IU\u4ee5\u4f18\u5316\u8840\u6db2\u4e2d\u7684\u7ef4\u751f\u7d20D\u6c34\u5e73\u3002\\n\\n2. **Omega-3\u8102\u80aa\u9178**\uff08\u5982\u9c7c\u6cb9\uff09\uff1a\u5bf9\u5fc3\u810f\u5065\u5eb7\uff0c\u63a8\u8350\u6bcf\u65e5\u6444\u5165EPA\u548cDHA\u7684\u603b\u91cf\u4e3a250-500mg\u3002\\n\\n3. **\u7ef4\u751f\u7d20C**\uff1a\u6210\u4eba\u63a8\u8350\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a\u7537\u602790mg\uff0c\u5973\u602775mg\u3002\\n\\n4. **\u7ef4\u751f\u7d20B12**\uff1a\u6210\u4eba\u63a8\u8350\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a2.4\u03bcg\u3002\\n\\n5. **\u9499**\uff1a\u6210\u4eba\u5efa\u8bae\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a1000mg\uff0c50\u5c81\u4ee5\u4e0a\u5973\u6027\u548c70\u5c81\u4ee5\u4e0a\u7537\u6027\u5efa\u8bae\u6bcf\u65e5\u6444\u5165\u91cf\u589e\u81f31200mg\u3002\\n\\n6. **\u94c1**\uff1a\u6210\u4eba\u7537\u6027\u63a8\u8350\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a8mg\uff0c\u6210\u4eba\u5973\u6027\u5efa\u8bae18mg\uff08\u6839\u636e\u6708\u7ecf\u5468\u671f\u53ef\u80fd\u9700\u8981\u8c03\u6574\uff09\u3002\\n\\n7. **\u9541**\uff1a\u6210\u4eba\u7537\u6027\u5efa\u8bae\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a\u7537\u6027400-420mg\uff0c\u5973\u6027\u4e3a310-320mg\u3002\\n\\n8. **\u950c**\uff1a\u6210\u4eba\u7537\u6027\u63a8\u8350\u6bcf\u65e5\u6444\u5165\u91cf\u4e3a11mg\uff0c\u6210\u4eba\u5973\u6027\u63a8\u8350\u6444\u5165\u91cf\u4e3a8mg\u3002\\n\\n\u91cd\u8981\u63d0\u793a\uff1a\u8fd9\u4e9b\u6570\u91cf\u662f\u5927\u81f4\u6307\u5bfc\uff0c\u4e2a\u4eba\u9700\u6c42\u53ef\u80fd\u6709\u6240\u4e0d\u540c\u3002\u670d\u7528\u4efb\u4f55\u8865\u5145\u5242\u524d\uff0c\u5efa\u8bae\u54a8\u8be2\u533b\u751f\u6216\u8425\u517b\u4e13\u5bb6\uff0c\u4ee5\u83b7\u5f97\u4e2a\u6027\u5316\u7684\u5efa\u8bae\u5e76\u907f\u514d\u8fc7\u91cf\u6444\u5165\u3002\u8fc7\u91cf\u6444\u5165\u67d0\u4e9b\u8425\u517b\u7d20\u53ef\u80fd\u5bf9\u5065\u5eb7\u4ea7\u751f\u8d1f\u9762\u5f71\u54cd\u3002\"}"}
{"id": "emnlp-2024-main-210", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A User-Centric Multi-Intent Benchmark for Evaluating Large Language Models\\n\\nJiayin Wang1, Fengran Mo2, Weizhi Ma3\u2217, Peijie Sun1, Min Zhang1\u2217, Jian-Yun Nie2\\n\\n1Department of Computer Science and Technology, Tsinghua University, Beijing, China.\\n2Universit\u00e9 de Montr\u00e9al, Qu\u00e9bec, Canada.\\n3Institute for AI Industry Research, Tsinghua University, Beijing, China.\\n\\nJiayinWangTHU@gmail.com, mawz@tsinghua.edu.cn, z-m@tsinghua.edu.cn\\n\\nAbstract\\n\\nLarge language models (LLMs) are essential tools that users employ across various scenarios, so evaluating their performance and guiding users in selecting the suitable service is important. Although many benchmarks exist, they mainly focus on specific predefined model abilities, such as world knowledge, reasoning, etc. Based on these ability scores, it is hard for users to determine which LLM best suits their particular needs. To address these issues, we propose to evaluate LLMs from a user-centric perspective and design this benchmark to measure their efficacy in satisfying user needs under distinct intents. Firstly, we collect 1,846 real-world use cases from a user study with 712 participants from 23 countries. This first-hand data helps us understand actual user intents and needs in LLM interactions, forming the User Reported Scenarios (URS) dataset, which is categorized with six types of user intents. Secondly, based on this authentic dataset, we benchmark 10 LLM services with GPT-4-as-Judge. Thirdly, we show that benchmark scores align well with human preference in both real-world experience and pair-wise annotations, achieving Pearson correlations of 0.95 and 0.94, respectively. This alignment confirms that the URS dataset and our evaluation method establish an effective user-centric benchmark. The dataset and code are publicly available.\\n\\n1 Introduction\\n\\nLarge language models (LLMs) are rapidly developing and gradually changing the way people interact with computing systems and permeating diverse facets of work and daily life (Wang et al., 2023a). Although LLMs show powerful capacities for completing various tasks (Chang et al., 2023), it is essential to understand how they satisfy users\u2019 actual intents and needs (Elangovan et al., 2024). According to related studies, many attempts have been made to evaluate LLMs, which may be divided into following two groups.\\n\\nThe first group of work (Hendrycks et al., 2020; Clark et al., 2018; Zellers et al., 2019) considers LLMs as task-completion models and mainly employs close-domain assessments to measure their various task-solving abilities, such as world knowledge, mathematics ability, coding skill, reasoning, planning, agent task, etc. (Chang et al., 2023). However, the existing attempts focused on predefined tasks, without considering the roles that humans would like LLMs to play in real-world scenarios. For example, while a task was evaluated, it was unclear if the task was important to humans and frequently required by users in real applications.\\n\\nThe second group evaluates whether LLMs\u2019 responses align with human preferences (Wang et al., 2023b). Table 1 summarizes the user-related LLM evaluation benchmarks. Their evaluation datasets include synthetic data (Li et al., 2023b), human-written data (Zheng et al., 2024), or selected user logs from certain LLM services (Lin et al., 2024). We can observe that these benchmarks may be limited in terms of data quality, quantity, or focus on...\"}"}
{"id": "emnlp-2024-main-210", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison between user-related LLM benchmarks with our URS benchmark. The compared studies are AlpacaEval (Li et al., 2023b), MT Bench-101 (Bai et al., 2024), TencentLLMEval (Xie et al., 2023), MT Bench (Zheng et al., 2024), AlignBench (Liu et al., 2023a) and WildBench (Lin et al., 2024).\\n\\nBesides, the categorization in current benchmarks is mainly focused on single-step tasks (Zhao et al., 2023; Liu et al., 2023a). However, end users might be more interested in knowing the ability of LLMs to satisfy their intended needs, rather than LLMs' performance on predefined individual tasks. In particular, a user intent might involve multiple model abilities with a series of tasks. For example, when a user is asking for advice about travel plans, this might involve model abilities of planning, personalization, and calling APIs. Consequently, the categorization of model-side abilities complicates the assessment of LLM performance in user-side scenarios, making it challenging for users to select the suitable service tailored to their specific needs.\\n\\nThis paper addresses the above issues in LLM evaluations by developing a user-centric benchmark to examine whether LLMs' behaviors satisfy user needs in real-world scenarios. As shown in Figure 1, our work is different with the existing ability-focused benchmarks and highlighted with the three unique characteristics:\\n\\nUser-centric. We benchmark LLMs from the user perspective in both dataset construction and evaluation designs. Our constructed User Reported Scenarios (URS) benchmark encompasses 1,846 authentic interactions (1,014 in English and 832 in Chinese) across 15 diverse LLM services, contributed by 712 users from 23 countries, each vetted through third-party manual quality checks. This dataset, reflecting the real-world usage of LLMs, is publicly available for research with user consent. Based on this first-hand data, we design evaluations to measure LLMs' efficacy in satisfying real user needs. The benchmark results align well with human preference in both real-world user experience across intents and pairwise comparison between LLMs. This user-centric approach promotes deeper collaboration between LLMs and humans as their capabilities continue to improve.\\n\\nIntent-driven. Different from the task-specific evaluations, our benchmark is categorized by user self-selected intents, including factual question answering, professional problem solving, text assisting, asking for advice, seeking creativity, and leisure. Such categorization provides the potential utilization of LLMs by real users and is validated through the user study. Based on the divided intents, the users who lack specific knowledge about the model capabilities can simplify their choice of proper service. For researchers and developers, this categorization helps to provide a more precise and targeted evaluation of how well LLMs meet user needs in diverse scenarios as one simple user intent may demand a blend of model abilities to address a series of tasks (Bolotova et al., 2022).\\n\\nMulti-cultural. Our data is contributed by users from 23 countries in Asia, Europe, North America, Oceania, South America, and Africa. Their cases cover multiple cultural backgrounds, such as nations' traditional festivals, local points of interest, and pop culture across the globe. This brings diversity in LLM evaluations, which is not included in simple translations of English-dominant content (Hershcovich et al., 2022; Huang et al., 2024b).\\n\\nIn this study, our contributions are threefold: (1) We collect 1,846 authentic cases from 712 global users to form the multi-intent, multi-cultural URS scenario dataset. (2) Based on this URS dataset, we propose an intent-aware evaluation method to benchmark the efficacy of 10 LLM services in satisfying user needs in diverse real-world scenarios.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Experiments demonstrate that benchmark results align closely with human preferences, as evidenced by Pearson correlations of 0.95 and 0.94 with real-world user experiences and pairwise annotations. These results validate that our automated evaluation method and URS dataset establish a new and effective user-centric benchmark.\\n\\n2 Related Work\\n\\n2.1 LLM Benchmark\\n\\nThe existing benchmarks are mainly divided into two categories as illustrated in Figure 1. They are model ability-focused and user-centric.\\n\\nAbility-focused benchmarks. Considering the broad capabilities of Large Language Models (LLMs), the evaluations span a diverse range of domains, typically categorized into Knowledge: MMLU (Hendrycks et al., 2020), CMMLU (Li et al., 2023a), and C-Eval (Huang et al., 2024c); Mathematical abilities: GSM8k (Cobbe et al., 2021), MATH (Hendrycks et al., 2021), and OlympiadBench (He et al., 2024a); Coding skills: HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021), or SWEBench (Jimenez et al., 2023); Reasoning: BBH (Suzgun et al., 2022; bench authors, 2023), ARC (Clark et al., 2018), and HeliaSwag (Zellers et al., 2019); Agent tasks: Agent-Bench (Liu et al., 2023b) and ToolBench (Qin et al., 2023). Each category aims to measure specific abilities expected from LLMs.\\n\\nUser-Centric benchmarks. Unlike benchmarks that focus on model capabilities, a few emphasize user experience with chat-based LLMs. Table 1 details related user-centric benchmarks. However, we notice several problems with these benchmarks. AlpacaEval (Li et al., 2023b), MTBench (Zheng et al., 2024), and MT-Bench-101 (Bai et al., 2024) are limited in data quality or quantity. TencentLLMEval (Xie et al., 2023) used tasks that were designed by hand and written by 50 annotators, an approach that may lead to deviation from real-world scenarios. AlignBench (Liu et al., 2023a) and WildBench (Lin et al., 2024) are constructed based on real-world use cases from logs of one specific LLM, which may introduce bias since the users may have access limitations to certain LLMs and use LLM services selectively depending on their needs. Furthermore, the existing user-centric benchmarks mainly do not consider multilingual and multicultural aspects, which are covered in our dataset.\\n\\n2.2 Evaluation Methods\\n\\nBesides dataset constructions, evaluation methods are crucial in developing benchmarks. To simplify evaluation processes, many studies utilize multiple-choice questions as criterion (Hendrycks et al., 2020; Li et al., 2023a; Huang et al., 2024c). Others employ post-processing of generated content and apply rules or deterministic tests to assess performance (Cobbe et al., 2021; Chen et al., 2021). With the recent advancements in LLM techniques, an increasing number of approaches use a more capable AI model to evaluate answers (He et al., 2024b; Li et al., 2023b), enriching benchmark types by allowing a more diverse format of test questions. AlpacaEval (Li et al., 2023b), AlignBench (Liu et al., 2023a) adopt GPT-4 as evaluator. PRE (Chu et al., 2024) shows that GPT-4 performs well in pairwise evaluations and Huang et al. (2024a) shows that GPT4 outperforms fine-tuned judge models in generalizability and fairness. In our benchmark, we adopt previous settings for open-domain evaluation, using LLM-based pairwise evaluation.\\n\\n2.3 User Intent\\n\\nFrom a user-centric view, we should first confirm user intent and evaluate service responses correspondingly. Related ideas try to understand user intent in interacting with LLMs. Shah et al. (2023) provides a user intent taxonomy based on New Bing, which includes information retrieval, problem solving, learning, content creation, and leisure. Bodonhelyi et al. (2024) details user intent with ChatGPT with informational, problem solving, creative, educational, personal interaction, technical and professional, transactional, ethical, and philosophical intents. Inspired by these studies, we design a user intent taxonomy accordingly, which is further verified by 712 user study participants.\\n\\n3 User Reported Scenarios Dataset\\n\\n3.1 User Intent Taxonomy\\n\\nUser intents represent specific needs or goals when interacting with a service and users may anticipate different types of feedback under different intents (Bolotova et al., 2022). For example, for factual intents, users may expect brief and accurate answers; for brainstorming needs, users tend to expect rich and innovative answers. By considering different user intents, we can assess LLMs\u2019 fine-grain efficacy in diverse scenarios. Therefore, we first define the taxonomy of user intent based on user study participants.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: IP Distribution of the 712 participants.\\n\\n3.2 Dataset Development\\n\\nTo better understand LLMs' applications in real-world scenarios, we conducted a user study to collect participants' recent interactions with LLM services, following data collection strategies of related studies (Feldhus et al., 2023; Bolotova et al., 2022).\\n\\nTable 2: User Intent Distribution in the URS dataset.\\n\\n| Intent                  | English | Chinese | Total |\\n|-------------------------|---------|---------|-------|\\n| Factual QA              | 374     | 199     | 573   |\\n| Solve Problem           | 180     | 348     | 528   |\\n| Text Assistant          | 81      | 43      | 124   |\\n| Ask for Advice          | 175     | 94      | 269   |\\n| Seek Creativity         | 121     | 72      | 193   |\\n| Leisure                 | 83      | 76      | 159   |\\n\\n3.2.1 User Study\\n\\nParticipants in the study were requested to provide five to ten real cases with LLMs they have used. Each reported case included the following three components: the LLM service they queried, complete conversation, and their usage intent (either selected from the predefined list or filled in by themselves). The detailed questionnaire is attached in Appendix A.\\n\\nAdditionally, the study investigate users' general experience with LLMs, including their satisfaction across different intents, which is used to validate benchmark alignment with human feedback in Section 4.5. We also record the anonymous, voluntarily reported demographic information in the questionnaire to examine whether this survey involves diverse groups of participants.\\n\\nTo capture multi-cultural scenarios, we offer the questionnaire in both English and Chinese and spread it through Prolific, X, and WeChat Moments platforms for global enrollment. Crowdworkers are paid according to Prolific recommendations (9\u00a3/hr * average 10 mins = 1.5\u00a3).\\n\\n3.2.2 Construction\\n\\nIn the initial phase of dataset construction, we check the diversity of our participants. Figure 2 shows the distribution of automatically recorded IP for English and Chinese questionnaire respondents. The feedback comes from 712 participants across 23 countries, showing the diversity in distributions. Detailed profiles, including demographic information (age and occupation) and LLM usage experience, are provided in Appendix B.1.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Example English and Chinese Cases under each Intent. The notation \u201cEN\u201d represents that the case is in English. \u201cCN\u201d means the cases are reported in Chinese and we attach their English translation in italics. \u201c......\u201d represents the text behind is omitted. Note that due to space limitations, we present cases with shorter length in the above table. The average length of each question is 29.65 tokens in the URS dataset.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Evaluation Procedure. For each evaluation instance, the evaluator is provided with the user intent, five intent-aware criteria, chain-of-thought reasoning steps, scoring standards for each two-point segment, addition with the question, an 8-score reference answer for this question, and the test LLM output for evaluation. Then, a parser will extract the final score from the evaluator's detailed rating content to form the benchmark.\\n\\nTime is also an important factor influencing LLM evaluations. Since the user study was conducted in February 2024, the collected cases in this dataset took places before this time.\\n\\nTable 3 showcases exemplary instances from the dataset. Real-world scenarios are often brief in articulation but complex in resolutions and span an expansive range of situations. Notably, beyond linguistic differences, the cases in this dataset also encompass a variety of cultural backgrounds. These include traditional events, such as Major Snow and the Spring Festival in the lunar calendar as well as popular entertainment information that are globally or locally recognized, such as the Marvel Cinematic Universe and Cantonese songs. These multicultural contexts could enhance the benchmarking of LLMs, addressing more diverse needs of the global user base.\\n\\n4 URS Benchmark\\n\\n4.1 Evaluation Framework\\n\\nWe aim to design an evaluation framework that can autonomously, expeditiously, and precisely determine the performance of any LLMs. The automation is made possible by using a strong model (e.g., GPT-4) acting as the evaluator, whose effectiveness is validated in previous work for open-domain evaluations (Chang et al., 2023; Sottana et al., 2023; Liu et al., 2023c) and further examined in Section 4.5. Aiming for high speed and precision, we implement a direct pair-wise scoring approach, providing a fixed reference answer for each question, in contrast to point-wise rating and Elo rating methods, as has been demonstrated to be effective in the previous study (Li et al., 2023b; Liu et al., 2023a). As shown in Figure 3, we categorize the evaluation instruction into 4 parts, including intent-aware criteria, chain-of-thought reasoning steps, scoring standards, and reference materials, which are described below (The detailed instructions are provided in Appendix C.2):\\n\\nIntent-aware criteria. During evaluations, we inform the evaluator about the user intent for each question and provide five specific evaluation criteria tailored to that intent. This approach is designed to help evaluators accurately capture user needs across various intent types and provide precise scorings. Detailed illustrations of these criteria and the correspondence relationship between user intents and evaluation criteria are provided in Appendix C.1.\\n\\nChain-of-thought reasoning steps. To fully leverage the reasoning capabilities of evaluators, the instruction provides four steps before assigning final ratings. These steps include contrasting the reference answer with the test response, scoring each criterion on a 1-10 scale, reassessing whether the response meets user needs, and integrating these assessments to determine the final score.\\n\\nScoring standards. To ensure accurate and differentiated scoring, we provide standards for each scoring segment. Each 2-point increment constitutes a distinct segment. The answer furnished by us is an 8-score reference. Besides, a note is provided to indicate that longer responses are not necessarily better to limit the potential length bias.\\n\\nReference materials. The instruction provided for the evaluator includes the question given by real users, their intent for asking, a fixed reference answer for this question (provided by a strong LLM, which is GPT-4 in this paper and checked manually), and the response generated by the test LLM for evaluation.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: URS Benchmark Results. For each intent and the overall scenarios, we mark the three best-performing LLM services, with the first marked '*', the second bolded, and the third underlined.\\n\\n4.2 Experimental Settings\\nAmong the top-used LLM services reported in the previous user study, we test all the LLMs with available APIs, including Baichuan2-Turbo (Yang et al., 2023), Claude-3-opus (Anthropic, 2024), Deepseek-chat (Bi et al., 2024), ERNIE-Bot-4 (Baidu), GLM-4 (Du et al., 2021), GPT-3.5-turbo (Achiam et al., 2023), GPT-4o (Achiam et al., 2023), Moonshot-v1-8k (MoonshotAI), Qwen-max (Bai et al., 2023), Spark-3.5 (iFLYTEK). We do not adjust the default temperature setting of LLMs based on query intent. This helps us to model the efficacy of LLM services in different real-world scenarios without user intent information. We benchmark the above LLMs on 1024 human-examined cases in the URS dataset.\\n\\nIn the following sections, we conduct extensive experiments to answer four research questions:\\n\\n(RQ1) What is the performance of different LLMs in the user-centric evaluation perspective?\\n(RQ2) Are the evaluation results stable across different LLMs as evaluators?\\n(RQ3) Do the benchmark results align with real user perceptions?\\n(RQ4) What multi-cultural features do we observe in this benchmark?\\n\\n4.3 Overall Results (RQ1)\\nThe overall benchmark results are shown in Table 4. We have the following observations:\\n\\n(1) GPT-4 yields an average score of 8.15 when its previously generated responses are used as 8-score references in the scoring instruction. This difference indicates an acceptable level of variance from the established standard. The benchmark stability across different evaluators is further discussed in the next section.\\n\\n(2) There is a noticeable stratification in scores across different models. Apart from GPT-4, the leading group comprises Claude-3 and Qwen-max, followed by a secondary tier including Moonshot-v1, GLM-4, and ERNIE-Bot-4. The performances of the third tier are comparable to that of GPT-3.5.\\n\\n(3) In examining efficacy across diverse user intents, GPT-4 generally demonstrates superior performance, except for the Text Assistant category, where Qwen-max prevails. Furthermore, a comparative analysis of LLMs across different user intents reveals a pronounced proficiency in objective contexts, including Solve Problem and Factual QA. This is probably because subjective scenarios require more diverse competencies such as personalization, creativity and humor, where current LLMs are relatively weak.\\n\\nSeparated results of English and Chinese scenarios are presented in Appendix D.1. Scoring examples are provided in Appendix D.2.\\n\\n4.4 Cross Validation between GPT-4 and Claude-3 (RQ2)\\nIn the overall benchmark result, we adopt GPT-4's direct output to the question as a reference for score 8, and use GPT-4 as the evaluator base for final scoring, as detailed in Section 4.1. Although this approach is widely adopted, it may cause favor to responses similar to GPT-4. To counteract this bias, we re-evaluated the performance of the top-2 LLMs, GPT-4o, and Claude-3-opus, employing...\"}"}
{"id": "emnlp-2024-main-210", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Comparison between GPT-4 and Claude-3. \u201cA Eva B Ans\u201d indicates the evaluation setting, where \u2018A\u2019 denotes the evaluator LLM and \u2018B\u2019 represents the source LLM used to generate 8-point reference answers. These results show that GPT-4 slightly outperforms Claude-3.\\n\\nFigure 5: Benchmark Score and User Reported Satisfaction Correlate Well across Intents. \u201cBenchmark Score\u201d is averaged under different intents. \u201cUser Reported Satisfaction\u201d is the average satisfaction level reported in the user study. Intents are ranked by user satisfaction.\\n\\nCross-validation techniques. We run 5-fold evaluation settings on randomly selected 200 cases, using GPT and Claude as the reference generator and base evaluator respectively, and the third performed LLM, Qwen, as both reference generation and evaluation for a third-party evaluation.\\n\\nAs illustrated in Figure 4, when GPT-generated responses are used as references, GPT-4 consistently achieves higher scores no matter the evaluator models. When Claude-generated responses are adopted in evaluations, the results are comparable. In third-party assessments, GPT-4 also maintained a superior ranking over Claude-3. This order is consistent with the overall benchmark results, indicating that the potential bias towards GPT does not affect the benchmark ranking.\\n\\n4.5 Alignment with Human Experience (RQ3)\\n\\nTo further evaluate the efficacy of LLM-as-Judge design in this benchmark, we analyzed benchmark accuracy through the lens of real-world human experience, examining both intent-wise user satisfaction and pairwise LLM-wise comparisons.\\n\\nFor intent-wise evaluation, user satisfaction levels were collected in the user study detailed in Section 3.2.1, involving 420 global participants who rated their satisfaction on a five-point scale for each intent while using LLM services. A high Pearson correlation coefficient of 0.95 between the average benchmark score and user-reported satisfaction across different intents, as shown in Figure 5, confirms the alignment. Notably, subjective scenarios such as Ask for Advice, Seek Creativity and Leisure received lower ratings than more subjective ones in both benchmark results and user feedback. This indicates the current LLM services still need improvements in satisfying real-world user needs.\\n\\nIn addition to the above alignment analysis based on large-scale user studies, we further conducted pair-wise annotations according to methods in Chatbot Arena (Chiang et al., 2024) to check the consistency between the designed automatic evaluation and human preferences. Due to space limitations, the detailed approach is provided in Appendix D.3.\\n\\nBased on 400 paired comparisons assessed by 5 human annotators, we computed Bradley-Terry coefficients (Bradley and Terry, 1952) for each LLM. As shown in Figure 6, the ranking of LLMs according to these coefficients aligns with the benchmark rankings with Pearson r=0.94, further affirming the effectiveness of evaluation designs.\\n\\n4.6 Analysis of Multi-cultural Features (RQ4)\\n\\nWe further analyzed the multi-cultural features in the benchmark. Based on the location of question providers, we divide the dataset to observe the performance differences of LLMs when answering questions from users with different cultural backgrounds. Table 5 presents the results of the top five countries with the highest number of survey participants. Among the 10 tested LLMs, 6 perform...\"}"}
{"id": "emnlp-2024-main-210", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model                  | UK  | China | Canada | USA  | Australia | Others |\\n|------------------------|-----|-------|--------|------|-----------|--------|\\n| GPT-4o                 | 8.68| 8.31  | 8.36   | 9.13 | 8.22      | 8.09   |\\n| Claude-3-opus          | 7.05| 7.54  | 7.64   | 8.13 | 7.00      | 7.49   |\\n| Qwen-max               | 7.84| 8.72  | 8.62   | 8.13 | 8.88      | 7.30   |\\n| Moonshot-v1-8k         | 7.58| 7.96  | 7.45   | 8.63 | 7.22      | 7.18   |\\n| GLM-4                  | 7.55| 7.61  | 7.86   | 8.04 | 7.44      | 7.26   |\\n| ERNIE-Bot-4            | 6.74| 8.32  | 7.41   | 8.13 | 6.67      | 7.22   |\\n| Spark-3.5              | 7.03| 7.92  | 7.41   | 7.29 | 6.56      | 6.75   |\\n| Baichuan2-Turbo        | 6.42| 7.00  | 6.50   | 7.04 | 5.78      | 6.53   |\\n| GPT-3.5-turbo          | 6.74| 6.33  | 7.14   | 7.79 | 6.67      | 6.48   |\\n| Deepseek-chat          | 6.50| 6.96  | 7.23   | 7.54 | 6.11      | 6.17   |\\n\\nTable 5: Multi-Cultural Analysis: we group the benchmark questions by user locations. For each LLM (line-wise), we bold the best performing result across countries. Results show that LLMs generally perform better on questions raised by USA users, reflecting the need for further attention to multi-cultural issues.\\n\\nThis reveals a potential cultural bias in LLM performance and evaluation, reflecting further emphasis on multi-cultural evaluations and improvements.\\n\\n5 Conclusion\\n\\nBy focusing on real-world cases, user intent categorization, and the intent-aware evaluation method, we establish a benchmark of high alignment with human preference, endeavoring to advance the user-centric evaluation of LLMs. We hope the insights gained from this research will not only help improve the performance of LLMs but also foster a deeper understanding of how these services can be effectively integrated into people's lives to enhance productivity, creativity, and overall welfare.\\n\\nLimitations\\n\\nWhile evaluating LLMs from the user-centric perspective offers considerable advantages, there are certain limitations in our URS benchmark.\\n\\n**Evaluation Method:** Our approach aligns with prevailing practices in open-ended question evaluation, where GPT-4 is employed as the base evaluator. This could potentially lead to a bias toward responses resembling GPT's style, thereby influencing the neutrality of the evaluation. We analyzed through cross-validation that this approach does not affect the top 2 LLM orders. Besides, analysis shows that benchmark score aligns with real-world user experience and human preferences in pairwise annotations. These further tested the validity of our method. In addition, the user study and human pairwise annotations could be further scale up to enhance persuasiveness.\\n\\nBesides, we notice it is very important to evaluate LLM services' performances in multi-turn conversational settings, as there are a few cases where the LLMs do not answer the user question and ask to provide more detailed information about user intents. These may be judged unsuccessful in a single-turn assessment, but the service might be capable of meeting this user need.\\n\\nNote that this benchmark is not holistic, as we mainly measure LLMs' efficacy in satisfying real-world user needs. User satisfaction is indeed crucial, while it represents one aspect of LLM performance. Other critical facets include effectiveness, efficiency, ease of use, and error tolerance. In this study, our primary focus is on user-centric evaluation of LLMs, which we have validated through strong alignments with human preferences to address its effectiveness.\\n\\n**Data Distribution:** The data for our study was mainly sourced from a global researcher recruitment platform, which does not ensure a random selection of all participants using LLM services. Consequently, the demographic distribution of our dataset may not accurately reflect that of typical LLM users or the global population. While as an independent third party unaffiliated with specific LLM services, and given our non-selective approach to data collection, our methodology incorporates greater diversity and offers a closer alignment with real-world usage scenarios compared to datasets that are human-designed, model synthetic, or selected by non-third parties or single-sourced.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ethical Considerations\\n\\nIn the user study, participants were informed that their reported conversations would be publicly released for research purposes. The notification is detailed in Appendix A. At the same time, they voluntarily chose whether to provide anonymized basic information, such as age group and profession.\\n\\nAdditionally, the survey design included an anchor question requiring participants to select specific options; responses that failed to adhere to these instructions were discarded as malicious feedback.\\n\\nThe questionnaire was provided in English and Chinese (Mandarin), the top 2 widely spoken languages, and distributed via the global recruitment platform. In this process, we did not filter any attributes as this could actively introduce bias. Following data collection, a third party conducted manual reviews to ensure the data's quality, harmless-ness, and the absence of any personal information.\\n\\nThis dataset could be used for research purposes to understand user intents and real-world needs, evaluate conversational systems' responses, and train for better alignment with human preference.\\n\\nNote that this URS dataset uses an Apache License.\\n\\nAcknowledgement\\n\\nThis work is supported by the Natural Science Foundation of China (Grant No. U21B2026, 62372260) and Quan Cheng Laboratory (Grant No. QCLZD202301).\\n\\nReferences\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nAI Anthropic. 2024. The claude 3 model family: Opus, sonnet, haiku. Claude-3 Model Card.\\n\\nJacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, and Charles Sutton. 2021. Program synthesis with large language models.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the AI2 reasoning challenge. arXiv preprint arXiv:1803.05457.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark othersChen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\\n\\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie Tang. 2021. Glm: General language model pretraining with autoregressive blank infilling. arXiv preprint arXiv:2103.10360.\\n\\nAparna Elangovan, Ling Liu, Lei Xu, Sravan Bodapati, and Dan Roth. 2024. Considers-the-human evaluation framework: Rethinking human evaluation for generative large language models. ACL 2024.\\n\\nNils Feldhus, Qianli Wang, Tatiana Anikina, Sahil Chopra, Cennet Oguz, and Sebastian M\u00f6ller. 2023. Interrolang: Exploring NLP models and datasets through dialogue-based explanations. arXiv preprint arXiv:2310.05592.\\n\\nChaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yuxiang Zhang, et al. 2024a. Olympiadbench: A challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008.\\n\\nChaoqun He, Renjie Luo, Shengding Hu, Yuanqian Zhao, Jie Zhou, Hanghao Wu, Jiajie Zhang, Xu Han, Zhiyuan Liu, and Maosong Sun. 2024b. Ultraeval: A lightweight platform for flexible and comprehensive evaluation for LLMs. ACL 2024 demo paper.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\\n\\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874.\\n\\nDaniel Hershcovich, Stella Frank, Heather Lent, Miryam de Lhoneux, Mostafa Abdou, Stephanie Brandl, Emanuele Bugliarello, Laura Cabello Piqueras, Ilias Chalkidis, et al. 2022. Challenges and strategies in cross-cultural NLP. ACL 2022.\\n\\nHui Huang, Yingqi Qu, Jing Liu, Muyun Yang, and Tiejun Zhao. 2024a. An empirical study of LLM-as-a-judge for LLM evaluation: Fine-tuned judge models are task-specific classifiers. arXiv preprint arXiv:2403.02839.\\n\\nKaiyu Huang, Fengran Mo, Hongliang Li, You Li, Yuanchi Zhang, Weijian Yi, Yulong Mao, Jinchen Liu, Yuzhuang Xu, Jinan Xu, et al. 2024b. A survey on large language models with multilingualism: Recent advances and new frontiers. arXiv preprint arXiv:2405.10936.\\n\\nYuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. 2024c. C-eval: A multi-level multi-discipline Chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36.\\n\\niFLYTEK. Spark.\\n\\nCarlos E Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2023. Swe-bench: Can language models resolve real-world GitHub issues? arXiv preprint arXiv:2310.06770.\\n\\nHaonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. 2023a. Cmmlu: Measuring massive multitask language understanding in Chinese. arXiv preprint arXiv:2306.09212.\\n\\nXuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. AlpacaEval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval.\\n\\nBill Yuchen Lin, Khyathi Chandu, Faeze Brahman, Yuntian Deng, Abhilasha Ravichander, Valentina Pyatkin, et al. 2024. Wildbench: Benchmarking LLMs with challenging tasks from real users in the wild.\\n\\nXiao Liu, Xuanyu Lei, Shengyuan Wang, Yue Huang, Zhuoer Feng, Bosi Wen, Jiale Cheng, Pei Ke, Yifan Xu, Weng Lam Tam, et al. 2023a. Alignbench: Benchmarking Chinese alignment of large language models. arXiv preprint arXiv:2311.18743.\\n\\nXiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, et al. 2023b. Agentbench: Evaluating LLMs as agents. arXiv preprint arXiv:2308.03688.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023c. Gpteval: NLG evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634.\\n\\nMoonshotAI. Moonshot.\\n\\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Runchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. Toolllm: Facilitating large language models to master 16000+ real-world APIs.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chirag Shah, Ryen W White, Reid Andersen, Georg Buscher, Scott Counts, Sarkar Snigdha Sarathi Das, Ali Montazer, Sathish Manivannan, Jennifer Neville, Xiaochuan Ni, et al. 2023. Using large language models to generate, validate, and apply user intent taxonomies. arXiv preprint arXiv:2309.13063.\\n\\nAndrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation metrics in the era of gpt-4: reliably evaluating large language models on sequence to sequence tasks. arXiv preprint arXiv:2310.13800.\\n\\nMirac Suzgun, Nathan Scales, Nathanael Sch\u00e4rli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them.\\n\\nBryan Wang, Gang Li, et al. 2023a. Enabling conversational interaction with mobile UI using large language models. In Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems.\\n\\nYufei Wang, Wanjun Zhong, Liangyou Li, Fei Mi, Xingshan Zeng, Wenyong Huang, Lifeng Shang, Xin Jiang, and Qun Liu. 2023b. Aligning large language models with human: A survey. arXiv preprint arXiv:2307.12966.\\n\\nShuyi Xie, Wenlin Yao, Yong Dai, Shaobo Wang, Donlin Zhou, Lifeng Jin, Xinhua Feng, Pengzhi Wei, Yujie Lin, Zhichao Hu, et al. 2023. Tenentllmeval: a hierarchical evaluation of real-world capabilities for human-aligned llms. arXiv preprint arXiv:2311.05374.\\n\\nAiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang, Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong Yan, et al. 2023. Baichuan 2: Open large-scale language models. arXiv preprint arXiv:2309.10305.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence. arXiv preprint arXiv:1905.07830.\\n\\nWenting Zhao, Xiang Ren, Jack Hessel, Claire Cardie, Yejin Choi, and Yuntian Deng. 2023. (inthe) wildchat: 570k chatgpt interaction logs in the wild. In The Twelfth International Conference on Learning Representations.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\n\\nA User Study\\n\\nAt the start of the user survey, we have this notification:\\n\\nYour dialog reported in this questionnaire may be publicly released for research purposes, so please avoid including personal information, and we will also conduct a secondary check, thank you!\\n\\nSubsequently, participants are asked to provide five to ten distinct real use cases with LLM services. Each use case should comprehensively cover the following aspects:\\n\\n1. LLM Service Utilized:\\n   Which of the following large language models did you ask the question to?\\n\\n2. Conversation Content:\\n   Your question and the system's output. It is recommended to directly share your conversation link!\\n\\n   If you fill it out manually, please respectively follow the format below.\\n\\n   **Question:** xxx\\n   **Answer:** xxx\\n\\n3. Usage Intent:\\n   Your intent behind this question is\\n\\nParticipants are also asked to voluntarily report the following demographic information anonymously:\\n\\n1. Age Group: To understand the age distribution of LLM users.\\n2. Profession: To gauge the professional background and its influence on LLM usage.\\n3. IP Location: To capture the geographical distribution of participants.\\n\\nGeneral usage experience with LLMs:\\n\\n1. LLM used:\\n   Large Language Models you have used\\n\\n2. Usage Frequency:\\n   How often do you use LLMs\\n\\n3. Satisfaction with LLMs:\\n   Your general satisfaction with LLM services across each intent\\n\\nB URS Dataset Information\\n\\nB.1 Participants\\n\\nFigure 7 and Figure 8 report the demographic information and the LLM services usage pattern of the user study participants, which demonstrates the diversity of the benchmark contributors.\\n\\n- Figure 7: Occupation distribution of survey participants.\\n- Figure 8: Number of Participants reported having used each LLM.\\n\\nB.2 Distribution of LLM usage\\n\\nTable 6 presents the distribution of LLM usage from the collected user study. ChatGPT is the most popular one in both language backgrounds, while the others show some differences.\\n\\nC Evaluation\\n\\nC.1 Intent-aware Criteria\\n\\nThe definition of intent-aware criteria is shown in Table 9. We evaluate each defined user intent with five selected criteria as described in Table 7.\\n\\nC.2 Instruction\\n\\nHere is the template for each evaluation case.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Distribution on Top 5 Source LLMs. In the URS dataset, most conversations are reported with their source LLMs. Notably, for Chinese feedback, Character.AI, Claude, Mistral, and Qwen are tied for 5th place.\\n\\nInstruction Template\\n\\nYou are asked to assess the quality of an AI assistant\u2019s answer to a user\u2019s question as an impartial judge. Since the type of answer you are evaluating is [user_intent], you need to evaluate the answer in the following 5 criteria:\\n\\n1. [criteria_dict[user_intent][0]]\\n2. [criteria_dict[user_intent][1]]\\n3. [criteria_dict[user_intent][2]]\\n4. [criteria_dict[user_intent][3]]\\n5. [criteria_dict[user_intent][4]]\\n\\nWe will provide you with the user\u2019s question, an 8-score reference answer, and answers from the AI assistant that needs your assessment. When starting your evaluation, you need to follow the reasoning steps below:\\n\\n1. Compare the AI assistant\u2019s answer with the reference answer, point out any shortcomings in the AI assistant\u2019s answer, and explain further.\\n2. Evaluate the AI assistant\u2019s answer in terms of the different criteria, giving each criterion a score from 1 to 10 after the evaluation of each.\\n3. Finally, combine the evaluations from each criterion and give the AI assistant\u2019s answer a composite score of 1 to 10.\\n\\n4. Your scoring needs to be as rigorous as possible and adhere to the following scoring rules: in general, the higher the quality of the model\u2019s answers, the higher the score.\\n\\nWhen the model answer has irrelevance to the question, or intrinsically factually incorrect, or generates harmful content, the total score should be 1 to 2;\\n\\nWhen the model answer has no serious errors and is largely harmless, but is of low quality and does not meet user requirements, the total score must be 3 to 4;\\n\\nWhen the model answer basically meets the user\u2019s needs but performs poorly on some criteria and is of medium quality, the total score can be 5 to 6;\\n\\nWhen the quality of the model response is similar to the reference answer and performs well in all criteria, the total score should be 7 to 8;\\n\\nA score of 9 to 10 can only be achieved if the model significantly exceeds the quality of the reference answer, adequately addresses the user\u2019s question and all the needs, and is close to a perfect score on all criteria. As an example, the reference answer would receive a score of 8.\\n\\nDo not let the length of the answer affect your score. Longer answers are not necessarily better. Answers that are concise and meet the above requirements are good.\\n\\nYou need to evaluate and explain before you score. Your explanation of each criterion needs to be followed by the scoring. After that, at the end of your answer, return all of your scores in the following dictionary format, including the curly brackets, and make sure that your scores are integers:\\n\\n{'Dimension 1': score, 'Dimension 2': score, ... , 'Final Score': score}, e.g. {'{criteria_dict[user_intent][0]}': 9, '{criteria_dict[user_intent][1]}': 6, ... , 'Final Score': 7}.\\n\\nUser Question: question\\n\\n[Start of Reference Answer]\\n{reference_ans}\\n\\n[End of reference answer]\\n\\n[start of AI assistant\u2019s answer]\\n{test_model_output}\\n\\n[end of AI assistant\u2019s answer]\\n\\nThe instructions for evaluation with four parts are shown in Table 10, 11 and Table 12 in English and Chinese, respectively. We use these designed instructions to guide the evaluator to output the evaluation results for benchmarking.\\n\\nD. Benchmark Results\\n\\nD.1 Bi-lingual Results\\n\\nTable 13 and Table 14 show the English and Chinese\u2019s separated benchmark results across different intent.\\n\\nD.2 Scoring Examples\\n\\nFor subjective intents, we list the example cases as shown in Table 17, 18 and 19.\\n\\nD.3 Alignment with Human Annotation\\n\\nWe aim to further investigate whether the benchmarked LLM order aligns with human preferences by conducting annotation experiments. Following the method outlined in Chatbot Arena (Chiang et al., 2024), we enlisted five human annotators (three males and two females) to evaluate paired questions as shown on the UI in Figure 9. We randomly selected 400 questions from the 1024-case benchmark and randomly paired two LLM outputs for each question. Annotators, blind to which LLMs generated the responses, were tasked with choosing their preferred answer from the two provided, or indicating if the choice was equal or\"}"}
{"id": "emnlp-2024-main-210", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Evaluation Criteria under Different User Intents.\\n\\n| Intents          | Criteria                      |\\n|------------------|-------------------------------|\\n| Intents          | Factual QA Factuality, User Satisfaction, Clarity, Completeness, Logical Coherence |\\n| Solve Professional Problem | Factuality, User Satisfaction, Clarity, Logical Coherence, Completeness |\\n| Text Assistant   | Clarity, User Satisfaction, Logical Coherence, Factuality, Creativity |\\n| Ask for Advice   | User Satisfaction, Factuality, Fairness and Responsibility, Creativity, Richness |\\n| Seek Creativity  | User Satisfaction, Logical Coherence, Creativity, Richness, Factuality |\\n| Leisure         | User Satisfaction, Engagement, Appropriateness, Creativity, Factuality |\\n\\nTable 8: Addition Ability. \u201cMax L\u201d represents the longest sequence of consecutive additions attainable at a minimum accuracy rate of 80%. \u201cAccuracy\u201d denotes the success ratio in 90 trials covering the length of 2 to 9 with 10 test cases of each length. Outcomes are presented for LLMs with an accuracy of 50% and above.\\n\\nOf the 400 paired comparisons, 50 were marked \u201ccannot be determined.\u201d Utilizing the remaining annotations, we calculated the Bradley-Terry (BT) scores to establish LLM rankings based on pairwise competition outcomes. As shown in Figure 6, the resulting LLM order aligns with the benchmark rankings, and the BT score demonstrates a Pearson correlation of 0.94 with the benchmark score. The evaluation data is anonymously available.\\n\\nD.4 Comparison with Different Evaluation Method\\n\\nFigure 10 shows cross-validation evaluation results. We change the model to generate reference answers and the model to serve as evaluators respectively. Results show while using GPT4 output as the reference answer for 8 scores, the order between the three models remains unchanged no matter the evaluation model. While using Qwen or GLM\u2019s output as both reference generator and evaluator, there exists a strong incline towards the model itself.\\n\\nWe also check the rating bias of different LLMs as evaluators. Rating bias is defined in Equation 1.\\n\\n\\\\[\\n\\\\text{Rating Bias}(A) = \\\\frac{\\\\sum B \\\\text{Rating} - \\\\text{Rating} B}{r} - r \\\\sum B, r = 8\\n\\\\]\\n\\nwhere \\\\(\\\\text{Rating} A - \\\\text{Rating} B\\\\) representing using LLM A as an evaluator and the output of LLM B as a reference answer to evaluate the performance of LLM B. \\\\(r\\\\) is the score for reference, in our cases, it is equal to 8. Note that in the experiments, reference answer and output awaiting score are two-time outputs of the same model, so if model B is stable and model A can score accurately, the Rating Bias should be \\\\(r\\\\). We measure the Rating Bias to measure the calibrability of the evaluator model. As shown in the results, compared to Qwen-max and GLM-4, GPT-4 performs more accurately in this cross-validation experiments.\"}"}
{"id": "emnlp-2024-main-210", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Analysis of Evaluation Methodologies: \u201cA-B\u201d indicates the evaluation setting, where \u2018A\u2019 denotes the evaluator LLM and \u2018B\u2019 represents the source LLM used to generate 8-point reference answers. Rating bias is defined in Equation 1. Specifically, the rating bias for evaluator A corresponds to the discrepancy between the reference score, 8, and the benchmark score of Model B under the assessment setting of A-B.\\n\\nD.5 Case Study\\n\\nIn our benchmark, we incorporate a diverse range of scenarios, extending beyond commonplace applications to include highly specialized use cases. A notable example involves assessing the models\u2019 proficiency in executing sequential summations without relying on external tools. This particular computational skill often manifests spontaneously and poses a challenge for targeted training. Consequently, evaluating a model\u2019s capability to accurately perform a series of two-digit additions offers a rapid and effective indicator of its computational prowess.\\n\\nOur examination entails the concatenation of two-digit numbers ranging from 2 to 20 in sequence, with each magnitude category being subjected to 10 randomly generated test instances. Table 8 delineates the performances of the top LLMs, showing their ability to achieve the longest chain of consecutive additions while maintaining a minimum accuracy threshold of 80% and the accuracy of adding from 2 to 10 numbers. Models not listed achieved an accuracy lower than 50%.\\n\\nIt is worth noting that the top 4 LLMs with maximum length >5 are the ones with larger parameters out of the total 10 LLMs. This might indicate that the ability of computing has correlations with the scaling of model size.\\n\\nD.6 Scenario Study\\n\\nSince we evaluate LLMs as collaborative tools for users, we juxtapose their performance with traditional assistants like search engines and translators. This comparison can assist and guide users in choosing the most effective tool for their specific intents. To this end, we evaluate LLM performance compared to Google Search Engine for Factual QA intent questions and Google Translator for translation queries in Text Assistant scenarios.\\n\\nSearch Engine.\\n\\nWe first retrieve search results for 57 random questions within the Factual QA intent and devise two methods to form answers: 1) Direct search results: the content of the top-1 web page, and 2) Retrieval-augmented generation: GPT-4 summarized answer based solely on information from the top-5 pages. Note that this approach enhances the search performances, not the LLMs\u2019. The results are reported in Table 15. Our analysis shows that direct search results do not outperform GPT-3.5. This limitation can be attributed to the fact that web pages might not provide straightforward answers to user queries, requiring users to navigate through multiple pages to piece together a response. The retrieval-augmented generation approach, which employs GPT-4 to create summarized answers based on search results without incorporating its inherent knowledge, also underperforms LLMs. These observations underscore LLMs\u2019 efficacy in factual QA scenarios, likely due to their ability to respond directly to user queries.\\n\\nTranslation.\\n\\nTo simulate the text assistant scenarios where users have several options of choosing translators or LLM services for their translation needs, we select the translation cases in our URS dataset for evaluation. Results are shown in Table 16, where we find LLM services outperform the translation tool. This may be because users need specific styles or modifications of the original context, while traditional translators are hard to accomplish.\"}"}
