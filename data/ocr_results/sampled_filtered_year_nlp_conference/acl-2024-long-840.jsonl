{"id": "acl-2024-long-840", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Israel Ministry of Justice. 2022. Opinion: Uses of copyrighted materials for machine learning. Accessed: 2024-02-15.\\n\\nYacine Jernite, Huu Nguyen, Stella Biderman, Anna Rogers, Maraim Masoud, Valentin Danchev, Samson Tan, Alexandra Sasha Luccioni, Nishant Subramani, G\u00e9rard Dupont, Jesse Dodge, Kyle Lo, Zeerak Talat, Isaac Johnson, Dragomir R. Radev, Somaieh Nikpoor, Jorg Frohberg, Aaron Gokaslan, Peter Henderson, Rishi Bommasani, and Margaret Mitchell. 2022. Data governance in the age of large-scale data-driven language technology. Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency.\\n\\nAlbert Qiaochu Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, L'elio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timoth\u00e9e Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv, abs/2310.06825.\\n\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, Matthijs Douze, H\u00e9rve J\u00e9gou, and Tomas Mikolov. 2016a. Fasttext.zip: Compressing text classification models. arXiv preprint arXiv:1612.03651.\\n\\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016b. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759.\\n\\nNikhil Kandpal, Haikang Deng, Adam Roberts, Eric Wallace, and Colin Raffel. 2023. Large language models struggle to learn long-tail knowledge. In Proceedings of the 40th International Conference on Machine Learning, volume 202 of Proceedings of Machine Learning Research, pages 15696\u201315707. PMLR.\\n\\nRodney Kinney, Chloe Anastasiades, Russell Authur, Iz Beltagy, Jonathan Bragg, Alexandra Buraczynski, Isabel Cachola, Stefan Candra, Yoganand Chandrasekhar, Arman Cohan, Miles Crawford, Doug Downey, Jason Dunkelberger, Oren Etzioni, Rob Evans, Sergey Feldman, Joseph Gorney, David Graham, Fangzhou Hu, Regan Huff, Daniel King, Sebastian Kohlmeier, Bailey Kuehl, Michael Langan, Daniel Lin, Haokun Liu, Kyle Lo, Jaron Lochner, Kelsey MacMillan, Tyler Murray, Chris Newell, Smita Rao, Shaurya Rohatgi, Paul Sayre, Zejiang Shen, Amanpreet Singh, Luca Soldaini, Shivashankar Subramanian, Amber Tanaka, Alex D. Wade, Linda Wagner, Lucy Lu Wang, Chris Wilhelm, Caroline Wu, Jiangjiang Yang, Angele Zamarron, Madeleine Van Zuylen, and Daniel S. Weld. 2023. The Semantic Scholar Open Data Platform. arXiv preprint arXiv:2301.10140.\\n\\nJohn Kirk and Gerald Nelson. 2018. The international corpus of english project: A progress report. World Englishes.\\n\\nKate Knibbs. 2023. The battle over books could change AI forever.\\n\\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The Stack: 3 TB of permissively licensed source code. arXiv preprint arXiv:2211.15533.\\n\\nHema Swetha Koppula, Krishna P. Leela, Amit Agarwal, Krishna Prasad Chitrapura, Sachin Garg, and Amit Sasturkar. 2010. Learning url patterns for webpage de-duplication. In Proceedings of the Third ACM International Conference on Web Search and Data Mining, WSDM '10, page 381\u2013390, New York, NY, USA. Association for Computing Machinery.\\n\\nTaku Kudo. 2018. Subword regularization: Improving neural network translation models with multiple subword candidates. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 66\u201375, Melbourne, Australia. Association for Computational Linguistics.\\n\\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 66\u201371, Brussels, Belgium. Association for Computational Linguistics.\\n\\nHugo Laurenccon, Lucile Saulnier, Thomas Wang, Christopher Akiki, Albert Villanova del Moral, Teven Le Scao, Leandro von Werra, Chenghao Mou, Eduardo Gonz\u00e1lez Ponferrada, Huu Nguyen, Jorg Frohberg, Mario vSavsko, Quentin Lhoest, Angelina McMillan-Major, G\u00e9rard Dupont, Stella Rose Biderman, Anna Rogers, Loubna Ben Allal, Francesco De Toni, Giada Pistilli, Olivier Nguyen, Somaieh Nikpoor, Maraim Masoud, Pierre Colombo, Javier de la Rosa, Paulo Villegas, Tristan Thrush, S. Longpre, Sebastian Nagel, Leon Weber, Manuel Sevilla Mu\u00f1oz, Jian Zhu, Daniel Alexander van Strien, Zaid Alyafeai, Khalid Almubarak, Minh Chien Vu, Itziar Gonzalez-Dios, Aitor Soroa Etxabe, Kyle Lo, Manan Dey, Pedro Ortiz Suarez, Aaron Gokaslan, Shamik Bose, David Ifeoluwa Adelani, Long Phan, Hieu Trung Tran, Ian Yu, Suhas Pai, Jenny Chim, Violette Lepercq, Suzana Ilic, Margaret Mitchell, Sasha Luccioni, and Yacine Jernite. 2023. The bigscience roots corpus: A 1.6tb composite multilingual dataset. ArXiv, abs/2303.03915.\\n\\nTeven Le Scao, Thomas Wang, Daniel Hesslow, Stas Bekman, M Saiful Bari, Stella Biderman, Hady Elsahehar, Niklas Muennighoff, Jason Phang, Ofir Press, Colin Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika, Jaesung Tae, Zheng Xin Yong, Julien Lainay, and Iz Beltagy. 2022. What language model to train if you have one million GPU hours? In Findings of the Association for Computational Linguistics: EMNLP 2022, pages 765\u2013782, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nKatherine Lee, A. Feder Cooper, and James Grimmelmann. 2024. Talkin' 'Bout AI Generation: Copyright\"}"}
{"id": "acl-2024-long-840", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating training data makes language models better. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8424\u20138445, Dublin, Ireland. Association for Computational Linguistics.\\n\\nColin Leong, Joshua Nemecek, Jacob Mansdorfer, Anna Filighera, Abraham Owodunni, and Daniel Whitenack. 2022. Bloom library: Multimodal datasets in 300+ languages for a variety of downstream tasks. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 8608\u20138621, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nHector J. Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Proceedings of the Thirteenth International Conference on Principles of Knowledge Representation and Reasoning, KR\u201912, page 552\u2013561. AAAI Press.\\n\\nQuentin Lhoest, Albert Villanova del Moral, Patrick von Platen, Thomas Wolf, Mario \u0160a\u0161ko, Yacine Jernite, Abhishek Thakur, Lewis Tunstall, Suraj Patil, Mariama Drame, Julien Chaumond, Julien Plu, Joe Davison, Simon Brandeis, Victor Sanh, Teven Le Scao, Kevin Canwen Xu, Nicolas Patry, Steven Liu, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Nathan Raw, Sylvain Lesage, Anton Lozhkov, Matthew Carrigan, Th\u00e9o Matussi\u00e8re, Lean dro von Werra, Lysandre Debut, Stas Bekman, and Cl\u00e9ment Delangue. 2021. Datasets: A Community Library for Natural Language Processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175\u2013184. Association for Computational Linguistics.\\n\\nHanlin Li, Nicholas Vincent, Yacine Jernite, Nick Merrill, Jesse Josua Benjamin, and Alek Tarkowski. 2023a. Can licensing mitigate the negative implications of commercial web scraping? In Companion Publication of the 2023 Conference on Computer Supported Cooperative Work and Social Computing, CSCW \u201923 Companion, page 553\u2013555, New York, NY, USA. Association for Computing Machinery.\\n\\nRaymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, Joel Lamy-Poirier, Jo\u00e3o Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, Jason Stillerman, Siva Sankalp Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, Nourhan Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, Maxim Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, Alexander Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Mu\u00f1oz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, and Harm de Vries. 2023b. Starcoder: may the source be with you! ArXiv, abs/2305.06161.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Manadar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. ArXiv, abs/1907.11692.\\n\\nZhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. 2023. Llm360: Towards fully transparent open-source llms. LLM360 Team. 2024. Llm360 k2-65b: Scaling up open and transparent language models.\\n\\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kinney, and Daniel Weld. 2020. S2ORC: The semantic scholar open research corpus. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4969\u20134983, Online. Association for Computational Linguistics.\\n\\nS. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, and Daphne Ippolito. 2023. A pretrainer\u2019s guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. ArXiv, abs/2305.13169.\\n\\nAlexandra Luccioni and Joseph Viviano. 2021. What\u2019s in the box? an analysis of undesirable content in the Common Crawl corpus. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers), pages 182\u2013189, Online. Association for Computational Linguistics.\\n\\nJeffrey MacKie-Mason and Haipeng Li. 2023. Re: Notice of inquiry (\u201cnoi\u201d) and request for comments, artificial intelligence and copyright, docket no. 2023-6. https://www.regulations.gov/comment/COLC-2023-0006-8194. Posted by the U.S. Copyright Office.\\n\\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384\u20131403, Abu\"}"}
{"id": "acl-2024-long-840", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nInbal Magar and Roy Schwartz. 2022. Data contamination: From memorization to exploitation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 157\u2013165, Dublin, Ireland. Association for Computational Linguistics.\\n\\nIan Magnusson, Akshita Bhagia, Valentin Hofmann, Luca Soldaini, Ananya Harsh Jha, Oyvind Tafjord, Dustin Schwenk, Evan Pete Walsh, Yanai Elazar, Kyle Lo, Dirk Groeneveld, Iz Beltagy, Hannaneh Hajishirzi, Noah A Smith, Kyle Richardson, and Jesse Dodge. 2023. Paloma: A benchmark for evaluating language model fit. arXiv [cs.CL].\\n\\nMitchell Marcus, Grace Kim, Mary Ann Marcinkiewicz, Robert MacIntyre, Ann Bies, Mark Ferguson, Karen Katz, and Britta Schasberger. 1994. The Penn Treebank: Annotating predicate argument structure. In Human Language Technology: Proceedings of a Workshop held at Plainsboro, New Jersey, March 8-11, 1994.\\n\\nMarco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. 2014. A SICK cure for the evaluation of compositional distributional semantic models. In Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC'14), pages 216\u2013223, Reykjavik, Iceland. European Language Resources Association (ELRA).\\n\\nTodor Markov, Chong Zhang, Sandhini Agarwal, Florentine Eloundou Nekoul, Theodore Lee, Steven Adler, Angela Jiang, and Lilian Weng. 2023. A holistic approach to undesired content detection in the real world. In Proceedings of the Thirty-Seventh AAAI Conference on Artificial Intelligence and Thirty-Fifth Conference on Innovative Applications of Artificial Intelligence and Thirteenth Symposium on Educational Advances in Artificial Intelligence, AAAI'23/IAAI'23/EAAI'23. AAAI Press.\\n\\nMarc Marone and Benjamin Van Durme. 2023. Data portraits: Recording foundation model training data. ArXiv, abs/2303.03919.\\n\\nSrdjan Matic, Costas Iordanou, Georgios Smaragdakis, and Nikolaos Laoutaris. 2020. Identifying sensitive urls at web-scale. Proceedings of the ACM Internet Measurement Conference.\\n\\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. 2016. Pointer Sentinel Mixture Models. arXiv preprint arXiv:1609.07843.\\n\\nMicrosoft. 2018. Presidio - data protection and de-identification sdk.\\n\\nMicrosoft. 2019. Blingfire: A lightning fast Finite State machine and REgular expression manipulation library. https://github.com/microsoft/BlingFire.\\n\\nTodor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can a suit of armor conduct electricity? a new dataset for open book question answering. arXiv [cs.CL].\\n\\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. 2022. MetaICL: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2791\u20132809, Seattle, United States. Association for Computational Linguistics.\\n\\nNiklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. 2023a. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124.\\n\\nNiklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. 2023b. Scaling data-constrained language models. arXiv preprint arXiv:2305.16264.\\n\\nRoberto Navigli, Simone Conia, and Bj\u00f6rn Ross. 2023. Biases in large language models: Origins, inventory, and discussion. J. Data and Information Quality, 15(2).\\n\\nHelen Ngo, Cooper Raterink, Jo\u00e3o G M Ara\u00fajo, Ivan Zhang, Carol Chen, Adrien Morisot, and Nicholas Frosst. 2021. Mitigating harm in language models with conditional-likelihood filtration.\\n\\nOfir Press, Noah A Smith, and Mike Lewis. 2021. Train short, test long: Attention with linear biases enables input length extrapolation.\\n\\nOpen Data Commons. 2010. Open Data Commons Attribution License (ODC-By) v1.0. https://opendatacommons.org/licenses/by/1-0/. Announcement. [accessed August 2023].\\n\\nOpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.\\n\\nAntonis Papasavva, Savvas Zannettou, Emiliano De Cristofaro, Gianluca Stringhini, and Jeremy Blackburn. 2020. Raiders of the lost kek: 3.5 years of augmented 4chan posts from the politically incorrect board. 14th International AAAI Conference On Web And Social Media (ICWSM), 2020.\\n\\nGuilherme Penedo, Hynek Kydl\u00edck, Loubna Ben Allal, Anton Lozhkov, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. FineWeb: decanting the web for the finest text data at scale. https://huggingface.co/spaces/HuggingFaceFW/blogpost-fineweb-v1.\\n\\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra-Aim\u00e9e Cojocaru, Alessandro Capelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refined-web dataset for falcon llm: Outperforming curated corpora with web data, and web data only. ArXiv, abs/2306.01116.\"}"}
{"id": "acl-2024-long-840", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-840", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nguyen, Lucile Saulnier, Samson Tan, Pedro Ortiz Suarez, Victor Sanh, Hugo Laurenccon, Yacine Jeremite, Julien Launay, Margaret Mitchell, Colin Rafaelle, Aaron Gokaslan, Adi Simhi, Aitor Soroa Etxabe, Alham Fikri Aji, Amit Alfassy, Anna Rogers, Ariel Kreisberg Nitzav, Canwen Xu, Chenghao Mou, Chris C. Emezue, Christopher Klamm, Colin Leong, Daniel Alexander van Strien, David Ifeoluwa Adelani, Dragomir R. Radev, Eduardo Gonzalez Ponferrada, Efrat Levkovizh, Ethan Kim, Eyal Bar Natan, Francesco De Toni, Gerard Dupont, German Kruszewski, Giada Pistilli, Hady ElSahar, Hamza Benyamina, Hieu Trung Tran, Ian Yu, Idris Abdulmumin, Isaac Johnson, Itziar Gonzalez-Dios, Javier de la Rosa, Jenny Chim, Jesse Dodge, Jian Zhu, Jonathan Chang, Jorg Frohberg, Josephine L. Tobing, Joydeep Bhattacharjee, Khalid Almubarak, Kimbo Chen, Kyle Lo, Leandro von Werra, Leon Weber, Long Phan, Loubna Ben Allal, Ludovic Tanguy, Manan Dey, Manuel Romero Munoz, Maraim Masoud, Maria Grandury, Mario vsavsko, Max Huang, Maximin Coavoux, Mayank Singh, Mike Tian-Jian Jiang, Minh Chien Vu, Mohammad Ali Jauhar, Mustafa Ghaleb, Nishant Subramani, Nora Kassner, Nurulaqilla Khamis, Olivier Nguyen, Omar Espejel, Ona de Gibert, Paulo Villegas, Peter Hendersson, Pierre Colombo, Priscilla A. Amuok, Quentin Lhoest, Rheza Harliman, Rishi Bommasani, Roberto Lopez, Rui Ribeiro, Salomey Osei, Sampo Pyysalo, Sebastian Nagel, Shamik Bose, Shamsuddeen Hassan Muhammad, Shanya Sharma, S. Longpre, Somaieh Nikpoor, S. Silberberg, Suhas Pai, Sydney Zink, Tiago Timponi Torrent, Timo Schick, Tristan Thrush, Valentin Danchev, Vassilina Nikoulina, Veronika Laippala, Violette Lepercq, Vrinda Prabhu, Zaid Alyafeai, Zeerak Talat, Arun Raja, Benjamin Heinzerling, Chenglei Si, Elizabeth Salesky, Sabrina J. Mielke, Wilson Y. Lee, Abheesht Sharma, Andrea Santilli, Antoine Chaffin, Arnaud Stiegler, Debajyoti Datta, Eliza Szczechla, Gunjan Chhablani, Han Wang, Harshit Pandey, Hendrik Strobelt, Jason Alan Fries, Jos Rozen, Leo Gao, Lintang Sutawika, M Saiful Bari, Maged S. Al-shaibani, Matteo Manica, Nihal V. Nayak, Ryan Teehan, Samuel Albanie, Sheng Shen, Srulik Ben-David, Stephen H. Bach, Taewoon Kim, Tali Bers, Thibault Fevry, Trishala Neeraj, Urbanish Thakker, Vikas Raunak, Xiang Tang, Zheng Xin Yong, Zhiqing Sun, Shaked Brody, Y. Uri, Hadar Tojarieh, Adam Roberts, Hyung Won Chung, Jaesung Tae, Jason Phang, Ofir Press, Conglong Li, Deepak Narayanan, Hatim Bourfoune, Jared Casper, Jeff Rasley, Max Ryabinin, Mayank Mishra, Minjia Zhang, Mohammad Shoeybi, Myriam Peyrounette, Nicolas Patry, Nouamane Tazi, Omar Sanseviero, Patrick von Platen, Pierre Cornette, Pierre Francqois Lavall\u00e9e, R\u00e9mi Lacroix, Samyam Rajbhandari, Sanchit Gandhi, Shaden Smith, St\u00e9phane Requena, Suraj Patil, Tim Dettmers, Ahmed Baruwa, Amanpreet Singh, Anastasia Cheveleva, Anne-Laure Ligozat, Arjun Subramonian, Aureli N\u2019ev\u2019eol, Charles Lovering, Daniel H Garrette, Deepak R. Tunuguntla, Ehud Reiter, Ekaterina Taktasheva, Ekaterina Voloshina, Eli Bogdanov, Genta Indra Winata, Hailey Schoelkopf, Jan-Christoph Kalo, Jekaterina Novikova, Jessica Zosa Forde, Xiangru Tang, Jungo Kasai, Ken Kawamura, Liam Hazan, Marine Carpuat, Miruna Clinciu, Najoung Kim, Newton Cheng, Oleg Serikov, Omer Antverg, Oskar van der Wal, Rui Zhang, Ruochen Zhang, Sebastian Gehrmann, Shachar Mirkin, S. Osher Pais, Tatiana Shavrina, Thomas Scialom, Tian Yun, Tomasz Limisiewicz, Verena Rieser, Vitaly Protasov, Vladislav Mikhailov, Yada Pruksachatkun, Yonatan Belinkov, Zachary Bamberger, Zdenvek Kasner, Alice Rueda, Amanda Pestana, Amir Feizpour, Ammar Khan, Amy Faranak, Ananda Santa Rosa Santos, Anthony Hevia, Antigona Unldreaj, Arash Aghagol, Arezoo Abdollahi, Aycha Tammour, Azadeh HajiHosseini, Bahareh Behroozi, Benjamin Olusola Ajibade, Bharat Kumar Saxena, Carlos Munoz Ferrandis, Danish Contractor, David M. Lansky, Davis David, Douwe Kiela, Duong Anh Nguyen, Edward Tan, Emily Baylor, Ezinwanne Ozoani, Fatim Tahirah Mirza, Frankline Ononiwu, Habib Rezanejad, H.A. Jones, Indrani Bhattacharya, Irene Solaiman, Irina Sedenko, Isar Nejadgholi, Jan Passmore, Joshua Seltzer, Julio Bonis Sanz, Karen Fort, L\u00edvia Macedo Dutra, Mairon Samagaio, Maraim Elbadri, Marigot Mieskes, Marissa Gerchick, Martha Akinlolu, Michael McKenna, Mike Qiu, M. K. K. Ghauri, Mykola Burynok, Nafis Abrar, Nazneen Rajani, Nour Elkott, Nourhan Fahmy, Olanrewaju Samuel, Ran An, R. P. Kromann, Ryan Hao, Samira Alizadeh, Sarmad Shubber, Silas L. Wang, Sourav Roy, Sylvain Viguier, Thanh-Cong Le, Tobi Oyebade, Trieu Nguyen Hai Le, Yoyo Yang, Zachary Kyle Nguyen, Abhinav Ramesh Kashyap, A. Palasciano, Alison Callahan, Anima Shukla, Antonio Miranda-Escalada, Ayush Kumar Singh, Benjamin Beilharz, Bo Wang, Caio Matheus Fonseca de Brito, Chenxi Zhou, Chirag Jain, Chuxin Xu, Clementine Fourrier, Daniel Leon Perinan, Daniel Molano, Dian Yu, Enrique Manjavacas, Fabio Barth, Florian Fuhrimann, Gabriel Altay, Giyaseddin Bayrak, Gully Burns, Helena U. Vrabec, Iman I.B. Bello, Isha Dash, Ji Soo Kang, John Giorgi, Jonas Golde, Jose David Posada, Karthi Sivaraman, Lokesh Bulchandani, Lu Liu, Luisa Shinzato, Madeleine Hahn de Bykhovetz, Maiko Takeuchi, Marc P\u00e0mies, Maria Andrea Castillo, Mariganna Nezhurina, Mario Sanger, Matthias Samwald, Michael Cullan, Michael Weinberg, M. Wolf, Mina Mihaljcic, Minna Liu, Moritz Freidank, Myung-sun Kang, Natasha Seelam, Nathan Dahlberg, Nicholas Michio Broad, Nikolaus Muellner, Pascale Fung, Patricia Haller, R. Chandrasekhar, Renata Eisenberg, Robert Martin, Rodrigo L. Canalli, Rosaline Su, Ruisi Su, Samuel Cahyawijaya, Samuele Garda, Shlok S Deshmukh, Shubhanshu Mishra, Sid Kiblawi, Simon Ott, Sinee Sang-aroonsiri, Srishti Kumar, Stefan Schweter, Sushil Pratap Bharati, T. A. Laud, Th\u2019eo Gigant, Tomoya Kainuma, Wojciech Kusa, Yanis Labrak, Yashasvi Bajaj, Y. Venkatraman, Yifan Xu, Ying Xu, Yu Xu, Zhee Xao Tan, Zhongli Xie, Zifan Ye, Mathilde Bras, Younes Belkada, and Thomas Wolf. 2022. Bloom: A 176b-parameter open-access multilingual language model. ArXiv, abs/2211.05100.\"}"}
{"id": "acl-2024-long-840", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-840", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Beyond the imitation game: Quantifying and extrapolating the capabilities of language models.\\n\\nTransactions on Machine Learning Research\\n\\nDetecting personal information in training corpora: an analysis. In Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 208\u2013220, Toronto, Canada. Association for Computational Linguistics.\\n\\nJapan goes all in: Copyright doesn\u2019t apply to AI training. Accessed: 2024-2-15.\\n\\nLaMDA: Language models for dialog applications. arXiv [cs.CL].\\n\\nD4: Improving llm pretraining via document de-duplication and diversification. ArXiv, abs/2308.12284.\\n\\nRedpajama-data-1t. Together Computer. 2023a.\\nRedpajama-data-v2. Together Computer. 2023b.\\nRedpajama-incite-base-3b-v1. Together Computer. 2023c.\\n\\nZyda: A 1.3t dataset for open language modeling.\\n\\nLlama: Open and efficient foundation language models. ArXiv, abs/2302.13971.\\n\\nLlama 2: Open foundation and fine-tuned chat models. ArXiv, abs/2307.09288.\\n\\nDirections in abusive language training data, a systematic review: Garbage in, garbage out. PloS one, 15(12):e0243300.\"}"}
{"id": "acl-2024-long-840", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eric Wallace, Tony Zhao, Shi Feng, and Sameer Singh. 2021. Concealed data poisoning attacks on NLP models. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 139\u2013150, Online. Association for Computational Linguistics.\\n\\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019. Superglue: A stickier benchmark for general-purpose language understanding systems. Advances in neural information processing systems, 32.\\n\\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. GLUE: A multi-task benchmark and analysis platform for natural language understanding. In Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 353\u2013355, Brussels, Belgium. Association for Computational Linguistics.\\n\\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://stability.ai/news/introducing-stable-lm-2.\\n\\nJohannes Welbl, Amelia Glaese, Jonathan Uesato, Sumanth Dathathri, John Mellor, Lisa Anne Hendricks, Kirsty Anderson, Pushmeet Kohli, Ben Coppin, and Po-Sen Huang. 2021. Challenges in detoxifying language models. In Findings of the Association for Computational Linguistics: EMNLP 2021, pages 2447\u20132469, Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nJohannes Welbl, Nelson F Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. arXiv [cs.HC].\\n\\nTim Weninger, Xihao Avi Zhu, and Jiawei Han. 2013. An exploration of discussion threads in social news sites. In Proceedings of the 2013 IEEE/ACM International Conference on Advances in Social Networks Analysis and Mining, New York, NY, USA. ACM.\\n\\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\u00e1n, Armand Joulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4003\u20134012, Marseille, France. European Language Resources Association.\\n\\nJason Weston, Antoine Bordes, Sumit Chopra, and Tomas Mikolov. 2015. Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv: Artificial Intelligence.\\n\\nAlbert Xu, Eshaan Pathak, Eric Wallace, Suchin Gururangan, Maarten Sap, and Dan Klein. 2021. Detoxifying language models risks marginalizing minority voices. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2390\u20132397, Online. Association for Computational Linguistics.\\n\\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2020. mT5: A massively multilingual pre-trained text-to-text transformer. arXiv [cs.CL].\\n\\nShuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. 2023. Rethinking benchmark and contamination for language models with rephrased samples. ArXiv, abs/2311.04850.\\n\\nYelp. 2013. Detect secrets. https://github.com/Yelp/detect-secrets. V1.4.0.\\n\\nSavvas Zannettou, Barry Bradlyn, Emiliano De Cristofaro, Haewoon Kwak, Michael Sirivianos, Gianluca Stringini, and Jeremy Blackburn. 2018. What is gab: A bastion of free speech or an alt-right echo chamber. In Companion Proceedings of the The Web Conference 2018, WWW '18, page 1007\u20131014, Republic and Canton of Geneva, CHE. International World Wide Web Conferences Steering Committee.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics.\\n\\nGe Zhang, Scott Qu, Jiaheng Liu, Chenchen Zhang, Chenghua Lin, Chou Leuang Yu, Danny Pan, Esther Cheng, Jie Liu, Qunshu Lin, Raven Yuan, Tuney Zheng, Wei Pang, Xinrun Du, Yiming Liang, Yinghao Ma, Yizhi Li, Ziyang Ma, Bill Lin, Emmanouil Benetos, Huan Yang, Junting Zhou, Kaijing Ma, Minghao Liu, Morry Niu, Noah Wang, Quehry Que, Ruibo Liu, Sine Liu, Shawn Guo, Soren Gao, Wangchunshu Zhou, Xinyue Zhang, Yizhi Zhou, Yubo Wang, Yuelin Bai, Yuhan Zhang, Yuxiang Zhang, Zenith Wang, Zhenzhu Yang, Zijian Zhao, Jiajun Zhang, Wanli Ouyang, Wenhao Huang, and Wenhu Chen. 2024. Map-neo: Highly capable and transparent bilingual large language model series. arXiv preprint arXiv:2405.19327.\\n\\nHao Zhang. 2022. Language model decomposition: Quantifying the dependency and correlation of language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2508\u20132517, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nTerry Yue Zhuo, Armel Zebaze, Nitchakarn Suppatitarachai, Leandro von Werra, Harm de Vries, Qian Liu, and Niklas Muennighoff. 2024. Astraios: Parameter-efficient instruction tuning code large language models. arXiv preprint arXiv:2401.00788.\\n\\nAcknowledgements\\n\\nDolma would not have been possible without the support of many individuals and institutions. The experimental components of this work were made possible through...\"}"}
{"id": "acl-2024-long-840", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a partnership with AMD and CSC, enabling use of\\nthe LUMI supercomputer. We thank Jonathan Frankle,\\nCody Blakeney, Matthew Leavitt and Daniel King and\\nthe rest of the MosaicML team for sharing findings from\\nexperiments on preliminary versions of our data. We\\nthank Vitaliy Chiley for messaging us on Twitter with\\na suggestion for resolving a random number generator\\nbug that was affecting our data shuffling. We thank Er-\\nfan Al-Hossami, Shayne Longpre, and Gregory Yauney\\nfor sharing findings from their own large-scale pretrain-\\ning data experiments. We thank Ce Zhang and Maurice\\nWeber of Together AI for thoughtful discussion on open\\ndatasets and data distribution format. We thank Stella\\nBiderman and Aviya Skowron for discussions around\\ndata licensing and data processing framework. We thank\\nour teammates at AI2 Nicole DeCario, Matt Latzke, Dar-\\nrrell Plessas, Kelsey MacMillan, Carissa Schoenick, Sam\\nSkjonsberg, and Michael Schmitz for their help with the\\nwebsite, design, internal and external communications,\\nbudgeting, and other activities that supported smooth\\nprogress on this project. Finally, we also express grati-\\ntude for the helpful discussions and feedback from our\\nteammates at AI2 and close collaborators, including\\nPrithviraj (Raj) Ammanabrolu, Maria Antoniak, Chris\\nCallison-Burch, Peter Clark, Pradeep Dasigi, Nicole De-\\ncario, Doug Downey, Ali Farhadi, Suchin Gururangan,\\nSydney Levine, Maarten Sap, Ludwig Schmidt, Will\\nSmith, Yulia Tsvetkov, and Daniel S. Weld.\\n\\nB Author Contributions\\n\\nDolma would not be possible without the help of our\\nmany teammates and collaborators. Weekly project\\nmeetings, messaging apps and documentation were\\naccessible for anyone at AI2. Major decisions about\\nDolma were often made in these channels, with excep-\\ntion for certain topics (e.g., legal, funding). While many\\nwere involved in the Dolma effort (see Acknowledge-\\nments \u00a7A), the authors of this paper were those who\\nowned and delivered a critical piece of the puzzle. We\\ndetail their contributions below (authors in alphabetical\\norder):\\n\\nContributors to\\ndata acquisition and source-specific\\ndata processing\\ninclude Akshita Bhagia, Dirk Groen-\\neveld, Rodney Kinney, Kyle Lo, Dustin Schwenk, and\\nLuca Soldaini. Everyone contributed to literature review\\non available sources and best practices and decisions\\naround sources to pursue. Akshita Bhagia, Rodney Kin-\\nney, Dustin Schwenk, and Luca Soldaini handled the\\nbulk of data acquisition and processing and ablation\\nexperiments with 1B models for source-specific design\\ndecisions. Kyle Lo and Luca Soldaini handled discus-\\nsions with legal to inform our choice of sources.\\n\\nContributors to\\ninfrastructure and tooling\\ninclude\\nRussell Authur, Dirk Groeneveld, Rodney Kinney, Kyle\\nLo, and Luca Soldaini. Rodney Kinney, Kyle Lo, and\\nLuca Soldaini designed and implemented the shared\\ntoolkit used for processing our corpus at scale. Dirk\\nGroeneveld wrote the Bloom filter for deduplication\\nand decontamination. Russell Authur wrote a toolkit for\\nacquisition and storage of Common Crawl data.\\n\\nContributors to\\nsource-agnostic data processing\\nin-\\nclude Khyathi Chandu, Yanai Elazar, Rodney Kinney,\\nKyle Lo, Xinxi Lyu, Ian Magnusson, Aakanksha Naik,\\nAbhilasha Ravichander, Zejiang Shen, and Luca Sol-\\ndaini. Khyathi Chandu, and Aakanksha Naik developed\\nthe toxic text filter. Kyle Lo, and Xinxi Lyu helped eval-\\nuate it. Luca Soldaini developed the language filtering\\napproach. Rodney Kinney, Zejiang Shen, and Luca Sol-\\ndaini developed the \u201cquality\u201d filter. Yanai Elazar identi-\\nfied repeating\\nn\\n-gram sequences. Abhilasha Ravichan-\\nder, Kyle Lo, and Luca Soldaini developed the PII filter.\\n\\nJesse Dodge and Ian Magnusson developed the evalua-\\ntion set decontamination approach.\\n\\nContributors to\\nablation experiments\\ninclude Iz Belt-\\nagy, Akshita Bhagia, Jesse Dodge, Dirk Groeneveld,\\nRodney Kinney, Kyle Lo, Ian Magnusson, Matthew\\nPeters, Kyle Richardson, Dustin Schwenk, Luca Sol-\\ndaini, Nishant Subramani, Oyvind Tafjord, and Pete\\nWalsh. This work included designing and prioritizing\\nexperiments given compute constraints, implementing\\nand running the 1B model experiments, and interpret-\\ning results. In particular, Oyvind Tafjord\u2019s work on the\\nevaluation toolkit and Pete Walsh\u2019s work on the model\\nimplementation were critical.\\n\\nContributors to\\nposthoc experiments and analysis\\non the final Dolma artifacts. Ben Bogin led the probing\\nexperiments on 1B model weights to assess impact of\\ndiffering code mixtures with support from Kyle Lo and\\nNiklas Muennighoff. Yanai Elazar ran the data analysis\\ntool to summarize and document Dolma\u2019s composition.\\nValentin Hofmann led the tokenization fertility analysis\\nwith support from Kyle Lo. Ananya Harsh Jha and Ian\\nMagnusson performed experiments training and evalu-\\nat ing baseline 1B models on other open datasets with\\nsupport from Luca Soldaini. Sachin Kumar and Jacob\\nMorrison performed analysis of systematic issues in\\nour choice of language identification and toxicity classi-\\nfiers with support from Kyle Lo. Niklas Muennighoff\\nled analysis of correlation between different filters em-\\nployed on Common Crawl data with support from Kyle\\nLo and Luca Soldaini.\\n\\nContributors to\\nlicensing and release policy\\ninclude\\nDavid Atkinson, Jesse Dodge, Jennifer Dumas, Nathan\\nLambert, Kyle Lo, Crystal Nam, and Luca Soldaini.\\nDavid Atkinson, Jesse Dodge, Jennifer Dumas, and\\nCrystal Nam led the bulk of this, including research into\\ndata licenses, risk-level determination for pretraining\\ndata, and defining the release policy. Kyle Lo and Luca\\nSoldaini provided feedback throughout this process and\\nhandled technical details needed for the release. Nathan\\nLambert provided feedback on release process and han-\\ndled the actual release strategy, particularly around ex-\\nternal communication.\\n\\nAll of the contributors above helped with\\ndocumentation and writing\\nof their respective components. In\\nparticular, Li Lucy provided an extensive literature re-\\nview of language models, open corpora and pretraining\"}"}
{"id": "acl-2024-long-840", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"corpus creation practices. Emma Strubell gave valuable feedback on our manuscript. Nathan Lambert helped with feedback on the blog post and other forms of external-facing communication about Dolma. Hannaneh Hajishirzi, Noah Smith, and Luke Zettle-moyer advised on the project, including broad strategy, writing, recruiting and providing resources. As OLMo project leads, Iz Beltagy, Jesse Dodge, and Dirk Groeneveld helped with visibility and coordination with other critical OLMo project workstreams. Notably, we credit Noah Smith for coming up with the name Dolma. Finally, Kyle Lo and Luca Soldaini led the overall Dolma project and were involved in all aspects, including project management, planning and design, discussions with legal and ethics committees, data and compute partnerships, infrastructure, tooling, implementation, experiments, writing/documentation, etc.\\n\\nC (Lack of) details about pretraining data curation for both open and closed language models\\n\\nWe provide a high-level overview of the pretraining data curation practices (or lack of reporting therof) of the largest, most performant language models (in no particular order) to illustrate the need for clear documentation and transparency around dataset curation.\\n\\nC.1 PaLM 2 (Anil et al., 2023)\\n\\nAnil et al. (2023) provides limited information on pretraining data used for PaLM 2; we summarize what we could from gather from their manuscript's Sections 3 and D1:\\n\\n1. Corpus size\\n   Unreported other than it's larger than what was used to train PaLM (Chowdhery et al., 2022)\\n\\n2. Data provenance\\n   Unreported other than they use web documents, books, code, mathematics, and conversational data.\\n\\n3. PII\\n   Reported as performed filtering, but without further details.\\n\\n4. Toxicity\\n   Toxic text identified using Perspective API but lacking details needed for reproduction (i.e., text unit, threshold). No details on removal. They did report tackling toxicity through the use of control tokens, but do not provide enough details on this method.\\n\\n5. Language ID\\n   Reports the most frequent languages included as well as their frequencies. Lack of details needed for reproduction (i.e., text unit, tools used, threshold).\\n\\n6. Quality\\n   Reported as performed filtering, but without further details.\\n\\n7. Deduplication\\n   Reported as performed filtering, but without further details.\\n\\n8. Decontamination\\n   N/A.\\n\\n9. Other\\n   Anil et al. (2023) report aggregated statistics of how often certain demographic identities are represented (or not) in the data. Such statistics include identities (e.g., American) or English pronouns. These were identified using tools such as KnowYourData or those available on Google-Cloud, but the manuscript lacks specifics necessary for reproduction.\\n\\nC.2 GPT-4 (OpenAI, 2023)\\n\\nOpenAI (2023) provides limited information on pretraining data used for GPT-4; we summarize what we could from gather from their manuscript's Section 2, Appendix C and D, footnotes 5, 6, 10 and 27, and Sections 1.1 and 3.1 in the System Card:\\n\\n1. Corpus size\\n   N/A\\n\\n2. Data provenance\\n   N/A aside from reporting that (1) data was sourced from both the Internet as well as third-party providers, (2) data was sourced mainly before September 2021 with trace amounts of more recent data, and (3) they included GSM-8K (Cobbe et al., 2021) as a tiny fraction of the total pretraining mix.\\n\\n3. PII\\n   N/A.\\n\\n4. Toxicity\\n   Removed documents that violate their usage policies from pretraining, including \u201cerotic content,\u201d using a combination of lexicon-based heuristics and bespoke classifiers following Markov et al. (2023).\\n\\n5. Language ID\\n   N/A aside from reporting that the majority of pretraining data is in English.\\n\\n6. Quality\\n   N/A.\\n\\n7. Deduplication\\n   N/A.\\n\\n8. Decontamination\\n   No discussion of decontamination procedures, but instead reported post-hoc statistics measuring extent of contamination on professional and academic exams, as well as several academic benchmarks. Method for identifying contamination based on exact substring match (after removing whitespaces) of a test example against a pretraining data example. They reported some contamination with BIG-Bench (Srivastava et al., 2023).\\n\\n9. Other\\n   There are myriad works performing \u201cdata archeology\u201d on GPT-4 that is, attempting to glean information about the pretraining data used in GPT-4 through probes for memorization. For example, Chang et al. (2023) show GPT-4 can generate sequences from copyrighted books. We do not attempt to survey all of these investigative works.\"}"}
{"id": "acl-2024-long-840", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Unfortunately, we know next to nothing about the pre-training data used for Claude.\\n\\n| Corpus size         | 2T tokens. |\\n|---------------------|------------|\\n| Data provenance     | N/A        |\\n| PII                 | Excluded data from certain websites known to contain high volumes of PII, though what these sites are was not disclosed. |\\n| Toxicity            | Not explicitly discussed, but appears to not have performed toxicity filtering, opting instead to handle toxic text generation in a later training stage. They do report results from a post hoc analysis in which they used a HateBERT classifier finetuned on ToxiGen to score each document line (and averaged to produce a document-level score). |\\n| Language ID         | Not stated, but they provide a post hoc analysis of the pretraining dataset using FastText Language ID with a 0.5 threshold for detected language. We assume this is likely the same protocol they used for pretraining data curation as it is also seen in the CCNet library, which was used for Llama. |\\n| Quality             | N/A.       |\\n| Deduplication       | N/A.       |\\n| Decontamination     | They provide extensive reporting on their deduplication method, which relies on a modified version of the ngram deduplication tool from Lee et al. |\\n| Other               | Reported upsampling certain sources, but without further details. They also report a similar analysis as in PaLM 2 on aggregate statistics about demographic identities and English pronouns. |\\n\\nTouvron et al. (2023a) provides some information on pretraining data used for training LLaMA; we summarize what we could gather from their manuscript's Section 2.1.\\n\\n| Corpus size         | 1.4T tokens. |\\n|---------------------|-------------|\\n| Data provenance     | LLaMA used data with known provenance, including five shards of CommonCrawl between 2017 and 2020, C4, GitHub code from Google BigQuery public datasets (restricted to Apache, BSD and MIT licenses), Wikipedia dumps from June to August 2022, Project Gutenberg books, Books3 from The Pile, LaTeX files from arXiv, and StackExchange pages. |\\n| PII                 | N/A.       |\\n| Toxicity            | Not explicitly discussed. Reports evaluation on the RealToxicityPrompts benchmark. |\\n| Language ID         | Reports use of the CCNet library, which employs FastText classifiers to remove non-English text (below a 0.5 threshold). No additional language ID reported for C4, GitHub, Books, arXiv, and StackExchange sets. For Wikipedia, reported restriction of pages to those using Latin or Cyrillic scripts: bg, ca, cs, da, de, en, es, fr, hr, hu, it, nl, pl, pt, ro, ru, sl, sr, sv, uk. |\\n| Quality             | Reports use of the CCNet library to remove low-quality content from CommonCrawl; CCNet uses KenLM, an n-gram language model to score perplexity of text as a measure of similarity to Wikipedia text. They do not report their chosen threshold for filtering. They also report use of a linear model trained to classify pages as Wikipedia Reference-like or not. They also report light heuristic filtering of boilerplate content for GitHub and Wikipedia subsets. |\\n| Deduplication       | Reports use of the CCNet library to identify duplicated lines for Common Crawl texts, file-level exact match deduplication for GitHub code, and deduplicating books with over 90% for Gutenberg and Books3 subsets. |\\n| Decontamination     | N/A.       |\\n| Mixture             | The manuscript reports a mixture of 67% CommonCrawl, 15% C4, 4.5% GitHub, 4.5% Wikipedia, 4.5% Books, 2.5% arXiv, and 2.0% StackExchange. Model training was a single epoch over this mixture except for an upsampling of Wikipedia and Books (2 epochs). |\"}"}
{"id": "acl-2024-long-840", "page_num": 28, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al., 2020), and the Pushshift Reddit Dataset (Baumgartner et al., 2020) as processed by (Roller et al., 2021). They made several notable changes to these sources:\\n\\n1. \\\\textbf{RoBERTa}. Reports updated the CC-News collection up to September 2021.\\n\\n2. \\\\textbf{Pile}. Reports restricted to the following collections: CommonCrawl, DM Mathematics, Project Gutenberg, HackerNews, OpenSubtitles, OpenWebText2, USPTO and Wikipedia. (Zhang, 2022) report omission of other Pile subsets due to gradient norm spikes at the 1B model scale.\\n\\n3. \\\\textbf{Pushshift Reddit}. Reports restricted to only the longest chain of comments in each thread; an operation that reportedly reduced the dataset by 66%.\\n\\nAlso describes: (1) \\\\textbf{deduplication} using MinHashLSH (Rajaraman and Ullman, 2011) with a Jaccard similarity threshold of 0.95, and (2) \\\\textbf{language ID} filtering to English-only text, though they do not describe the method used.\\n\\nThey do not discuss whether they do (or do not) perform any processing for \\\\textbf{PII}, \\\\textbf{toxicity}, \\\\textbf{quality}, or \\\\textbf{decontamination}.\\n\\n\\\\section*{D Experimental Setup}\\n\\n\\\\subsection*{D.1 Ablation Setup}\\n\\nFor all data ablations described in this section, we train a 1B parameter model on up to 150B tokens. We follow model architecture and training from OLMo (Groeneweld et al., 2024); we summarize key details here, but direct the reader to the manuscript for further details.\\n\\nEach model is an decoder-only transformer model with 16 layers, 16 attention heads, and 2048 dimensional-ity. We use ALiBi positional embeddings (Ofir Press et al., 2021), SwiGLU activation (Shazeer, 2020), and mixed precision; model context size is set to 2048 tokens. We use EleutherAI\u2019s GPT NeoX tokenizer (Black et al., 2022). The model is trained using the LionW optimizer (Chen et al., 2023a) with $1 \\\\times 10^{-4}$ peak learning rate, warm-up of 2000 steps, cosine decay, and $1 \\\\times 10^{-2}$ weight decay. Batch size was set to 1024. While we set our max number of steps to 95k (which is approximately 200B tokens), we conclude our experiments at 150B tokens.\\n\\nWe use 64 AMD Instinct MI250X accelerators. Each MI250X accelerator contains two logical nodes; therefore, from the point of view of our training code, our experiments ran on 128 compute units grouped in 16 nodes. Per each logical unit, we use a micro-batch size of 8. We implement our experiments using the anonymized codebase.\\n\\n\\\\subsection*{D.2 Perplexity Evaluation Suite}\\n\\nFor data ablations, we keep track of language model perplexity using Paloma (Magnusson et al., 2023). Datasets included:\\n\\n- \\\\textbf{C4} (Raffel et al., 2020; Dodge et al., 2021): Standard contemporary LM pretraining corpus automatically filtered from the April 2019 Common Crawl scrape.\\n- \\\\textbf{mC4} (Xue et al., 2020); English subset: the English language portion of a pretraining corpus automatically filtered from 71 Common Crawl scrapes.\\n- \\\\textbf{Pile} (Gao et al., 2020), validation set: widely-used language modeling pretraining corpus; contains documents curated from multiple sources including several non-web sources.\\n- \\\\textbf{WikiText 103} (Merity et al., 2016): a standard collection of verified \u201cGood\u201d and \u201cFeatured\u201d articles on Wikipedia.\\n- \\\\textbf{Penn Tree Bank} (Marcus et al., 1994): widely-used NLP corpus derived from Wall Street Journal articles.\\n- \\\\textbf{M2D2} (Reid et al., 2022), S2ORC subset: papers from Semantic Scholar (Lo et al., 2020) grouped by hierarchical academic field categories.\\n- \\\\textbf{M2D2} (Reid et al., 2022), Wiki subset: Wikipedia articles grouped by hierarchical categories in the Wikipedia ontology.\\n- \\\\textbf{C4 100 domains} (Chronopoulou et al., 2022): balanced samples of the top 100 domains in C4.\\n- \\\\textbf{Gab} (Zannettou et al., 2018): data from 2016-2018 from an alt-right, free-speech-oriented social media platform that has been shown to contain more hate speech than mainstream platforms.\\n- \\\\textbf{ICE} (Greenbaum, 1991): English from around the world curated by local experts, with subsets for Canada, East Africa, Hong Kong, India, Ireland, Jamaica, Philippines, Singapore, and the USA.\\n- \\\\textbf{Twitter AAE} (Blodgett et al., 2016): balanced sets of tweets labeled as African American or white-aligned English.\\n- \\\\textbf{Manosphere} (Ribeiro et al., 2021): sample of 9 forums where a set of related masculinist ideologies developed over the past decade.\\n- \\\\textbf{4chan} (Papasavva et al., 2020): data from 2016-2019 politics subsection of an anonymity-focused forum found shown to contain high rates of toxic content.\\n\\nWe also curated held-out sets from other open language model corpora to augment Paloma:\\n\\n- \\\\textbf{Dolma} (this work), uniform sample: A sample 8,358 documents from the Dolma corpus across all of its subsets (13 from books, 1,642 from Common Crawl web pages, 4,545 Reddit submissions, 450 scientific articles, 1,708 Wikipedia and Wikibooks entries).\\n- \\\\textbf{RedPajama v1} (Together Computer, 2023b): 1 trillion tokens replication of the LLaMA 1 (Touvron et al., 2023a) pretraining corpus.\"}"}
{"id": "acl-2024-long-840", "page_num": 29, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Falcon RefinedWeb (Penedo et al., 2023): A corpus of English sampled from all Common Crawl scrapes until June 2023, more aggressively filtered and deduplicated than C4 and mC4-en.\\n\\nDolma 100 Subreddits (this work): Balanced samples of the top 100 subreddits by number of posts, sourced from the Dolma Reddit subset.\\n\\nDolma 100 Programming Languages (this work): Balanced samples of the top 100 programming languages by number of tokens, sourced from the Dolma Stack subset.\\n\\nD.3 Downstream Evaluation Suite\\nWe primarily base our data ablation decisions on the performance of models on this evaluation suite:\\n\\n\u2022 AI2 Reasoning Challenge (Clark et al., 2018): A science question-answering dataset broken into easy and challenge subsets. Only the easy subset was used in online evaluations. The challenge subset was, however, included in offline evaluations.\\n\\n\u2022 BoolQ (Clark et al., 2019): A reading comprehension dataset consisting of naturally occurring yes/no boolean questions and background contexts.\\n\\n\u2022 HellaSwag (Zellers et al., 2019): A multiple-choice question-answering dataset that tests situational understanding and commonsense.\\n\\n\u2022 OpenBookQA (Mihaylov et al., 2018): A multiple-choice question-answering dataset modeled on open-book science exams.\\n\\n\u2022 Physical Interaction: Question Answering (PIQA) (Bisk et al., 2019): A multiple-choice question-answering dataset that focuses on physical commonsense and naive physics.\\n\\n\u2022 SciQ (Welbl et al., 2017): A crowdsourced multiple-choice question-answering dataset consisting of everyday questions about physics, chemistry and biology, among other areas of science.\\n\\n\u2022 WinoGrande (Sakaguchi et al., 2019): A dataset of pronoun resolution problems involving various forms of commonsense. Modeled after the Winograd challenge from Levesque et al. (2012).\\n\\nD.4 Training Setup for OLMo-1B\\nFor OLMo-1B, we follow the experimental setup outlined for dataset ablation experiments in Appendix D, with the following differences:\\n\\n\u2022 We set the max number of steps to 739,328 (which is roughly 3.1T tokens).\\n\\n\u2022 We double the batch size to 2048 and do so by scaling up to 256 compute units (double what we used for data ablations).\\n\\n\u2022 Due to instabilities we found in the LionW optimizer, we switched to using AdamW.\\n\\nE Construction of Conversational Threads in Forums Data\\nContent comes from Reddit's data API in two separate but linked forms: submissions and comments. Submissions are either \u201clink posts\u201d to external content (e.g. news articles, blogs, or even multimedia content) or \u201cself posts\u201d (submissions written by the poster meant to initiate a discussion thread on a topic). Comments are user replies to either the initiating post (top level comments) or to another user\u2019s comment. Posts, top-level comments, and replies to comments form a nested conversational thread with a submission post at its root and comments branching out into multiple possible dialogue trees.\\n\\nThe tree-like structure of Reddit threads allows for multiple possible data formats depending on how the various components of a thread are combined. We investigate three formats for their potential as LM pretraining data:\\n\\n\u2022 Atomic content. This simple format treats all comments and submissions as independent documents without any structure or connection to the thread they appear in.\\n\\n\u2022 Partial threads. This format assembles comments from the same thread into a structured, multi-round dialogue between users. Submissions are left as separate documents. Assembled dialogues are limited to a maximum parent depth, and the resulting documents are only snippets of their originating thread (which are spread across several documents).\\n\\n\u2022 Full threads. This complex format combines a given submission and all of its child comments into a single document encompassing an entire thread. Code-like indentation is used to indicate the depth of a comment in the thread\u2019s hierarchy.\\n\\nWe experimentally evaluated these strategies for assembling documents in Figure 4. We found that, for language modeling purposes, treating comments and submissions as atomic units leads to better downstream performance compared to partial and full threads. We hypothesize that the more complex formatting required to handle dialogues might introduce undesirable content for language modeling, such as short and repeated comments. We leave the study of better formatting for forum content for language modeling to future work.\\n\\nF Tokenization Analysis\\nThe first step of processing text with LMs is tokenization, i.e., mapping the text to a sequence of tokens with corresponding input embeddings (Sennrich et al., 2016; Kudo, 2018; Kudo and Richardson, 2018). Recently, there has been a growing interest in the question of how well LM tokenizers fit different data sources (e.g., data in different languages; Ahia et al., 2023; Petrov et al., 2023) Inspired by this emerging line of work,\"}"}
{"id": "acl-2024-long-840", "page_num": 30, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Tokenization analysis. Tokens with small IDs, which have a high count in the tokenizer training data, also tend to have a high count in Dolma (a). The Stack has a substantially higher fertility compared to the other data sources (b), which can be explained by the higher relative frequency of whitespace characters such as \\\\n and \\\\t (c). See text for more details.\\n\\nWe conduct an explorative analysis of the GPTNeoX tokenizer (Black et al., 2022) applied to Dolma, which provides a first picture of how challenging the different data sources comprised by Dolma are for current LM tokenizers.\\n\\nWe start by taking a global look at the tokenizer\u2019s fit to Dolma. Out of the 50,280 tokens in the tokenizer vocabulary, 50,057 are present in the tokenized text of Dolma. In other words, 223 tokens are never used, amounting to roughly 0.4% of the tokenizer vocabulary. The 223 tokens mostly consist of combinations of whitespace characters (e.g., \\\\n \\\\n, two newline characters followed by two blank space characters). Note that when training an LM with the examined tokenizer on Dolma, the input embeddings corresponding to these tokens would not be updated. In terms of the count distribution of tokens, we find that tokens with smaller IDs tend to have higher counts in Dolma (see Figure 6a), which is also reflected by a strong Spearman\u2019s correlation between (i) the ranking of tokens based on their counts in Dolma and (ii) the token IDs ($r = 0.638$, $p < 0.001$). Given how the tokenizer was trained (Sennrich et al., 2016; Black et al., 2022), smaller IDs correspond to byte pairs merged earlier and hence tokens occurring more frequently in the tokenizer training data. Overall, these results suggest a good fit of the GPTNeoX tokenizer to Dolma.\\n\\nDoes the tokenizer fit all data sources included in Dolma equally well? To examine this question, we analyze fertility, which is defined as the average number of tokens per word generated by a tokenizer (Acs, 2019; Scao et al., 2022), in our case measured on a specific data source. We find that fertility is similar for most data sources, ranging between 1.15 (conversational forum subset) and 1.28 (books subset), with the exception of the code subset, which has a substantially higher fertility of 2.45 (see Figure 6b). This means that the costs of processing the code subset \u2014 be they computational or financial in nature (Petrov et al., 2023) \u2014 are more than twice as high compared to the other data sources.\\n\\nWhat causes this discrepancy? We find that in the code subset (which mostly contains code), words are often preceded by whitespace characters other than a blank space (e.g., newline, tab, return). Crucially, while a blank space before a word is tokenized as part of that word (e.g., I love you $\\\\rightarrow$ \u201cI\u201d, \u201c love\u201d, \u201c you\u201d), other whitespace characters yield separate tokens (e.g., I love you $\\\\rightarrow$ \u201cI\u201d, \u201c\\\\t\u201d, \u201c love\u201d, \u201c\\\\t\u201d, \u201c you\u201d). This can also be seen by plotting the relative frequency of tokens representing whitespace characters by data source, which is one order of magnitude higher for The Stack compared to most other data sources (see Figure 6c). When training LMs on The Stack (or code more generally), it thus might be advisable to add special tokens to the tokenizer (e.g., \u201c\\\\n if\u201d; Hong et al., 2021). It is important to notice that this observation applies to most tokenizers in use today (e.g., the tokenizer used by GPT-4), which tend to lack tokens such as \u201c\\\\n if\u201d.\\n\\nG Auditing our Language Filter\\n\\nTo analyze the impact of the FastText language identification classifier, we ran an external audit on the International Corpus of English (ICE) (Kirk and Nelson, 2018), a dataset containing spoken and written English from nine countries around the world. We ran our language ID tool on all documents in the ICE dataset to estimate how many documents from each region would have been erroneously filtered. The ground truth in this analysis is that every document is in English, and should be classified as such. Interestingly, we found that at our fairly permissive threshold (keeping documents with at least a 0.5 score for English) correctly identified all English-language documents in ICE each as English, no matter the region it was from.\\n\\nH Details on Toxicity Filters\\n\\nImplementation. To remove toxic content from Dolma, we used the Jigsaw Toxic Comments dataset (Cjadams et al., 2017), which contains forum comments tagged with (multilabel) categories \u201ctoxic\u201d, \u201csevere toxic\u201d, \u201cthreat\u201d, \u201cinsult\u201d, \u201cobscene\u201d, \u201cidentity\u201d, \u201csexually explicit\u201d, \u201charassment\u201d, \u201cpolitical\u201d, \u201cracist\u201d, \u201cstereotyping\u201d, \u201cother hate\u201d, \u201csextortion\u201d, \u201cthreatening\u201d, \u201cetc.\u201d, \u201cabusive name calling\u201d, \u201csexually oriented\u201d, \u201cfighting\u201d, \u201cthreatening us\u201d, \u201cthreatening others\u201d, \u201cother threat\u201d, \u201csexual content\u201d, \u201cmedical advice\u201d, \u201cchain letter\u201d, \u201cflame war\u201d, \u201cother flame\u201d, \u201cspam\u201d, \u201cother spam\u201d, \u201c\u7684\u653f\u6cbb\u201d, \u201c\u5176\u4ed6\u4ec7\u6068\u201d, \u201c\u5176\u4ed6\u5a01\u80c1\u201d, \u201c\u5176\u4ed6\u6027\u5185\u5bb9\u201d, \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\u201d, \u201c\u5176\u4ed6\u6076\u610f\u201d\uff0c\u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\u201d\uff0c\u201c\u5176\u4ed6\u94fe\u4fe1\u201d, \u201c\u5176\u4ed6\u653b\u51fb\u201d, \u201c\u5176\u4ed6\u6027\u6697\u793a\u201d, \u201c\u5176\u4ed6\u8272\u60c5\u201d, \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\u201d, \u201c\u5176\u4ed6\u5a01\u80c1\u201d, \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\u201d, \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\u201d, \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\u201d, \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6\u5a01\u80c1\", \u201c\u5176\u4ed6\u5783\u573e\u90ae\u4ef6\", \u201c\u5176\u4ed6\u94fe\u4fe1\", \u201c\u5176\u4ed6\u653b\u51fb\", \u201c\u5176\u4ed6\u6027\u6697\u793a\", \u201c\u5176\u4ed6\u8272\u60c5\", \u201c\u5176\u4ed6\u533b\u7597\u5efa\u8bae\", \u201c\u5176\u4ed6 threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201cother threat\", \u201c"}
{"id": "acl-2024-long-840", "page_num": 31, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Percentage of English-language documents in the International Corpus of English (ICE) (Kirk and Nelson, 2018) that would be misidentified as non-English as a result of thresholding the FastText classifier's predicted English score. We find a majority of English documents in ICE remain identified as English even with a threshold of 0.90.\\n\\nand/or \\\"identity hate\\\" alongside unlabeled comments, to train two FastText classifiers\u2014a binary \\\"hate\\\" detector and a binary \\\"NSFW\\\" detector:\\n\\n1. For our \\\"hate\\\" detector, we group all unlabeled comments and \\\"obscene\\\"-only comments as negatives and leave remaining comments as positives.\\n2. For our \\\"NSFW\\\" detector, we take all comments tagged as \\\"obscene\\\" as positives and leave other remaining comments as negatives. It is important to note this detector only filters toxic content that mentions sexual or obscene topics, not sexual content in general.\\n\\nFigure 8: Distribution of Reddit comments labeled as toxic by English variation.\\n\\nAnalysis of resulting classifier.\\nTo measure dialectal biases in the FastText toxicity classifier, we analyze its proclivity to predict English variations spoken in different countries as toxic. Starting with the unfiltered Reddit corpus, we create a dataset of comments from location-based subreddits, filtering for country-specific subreddits with more than 50K comments. This dataset serves as a crude proxy for different dialects of English, assuming most commenters live in the respective locations and speak the variation. We further assume the fraction of actually toxic comments in each of these subreddits to be roughly the same. We compute the toxicity score for each comment in this dataset using the FastText classifier and report the percentage of comments marked as toxic against different classifier thresholds in Figure 8.\\n\\nFor all thresholds, for any two locations, we find <5% difference in the fraction of comments marked as toxic suggesting little to no bias. Further, we plot the distribution of toxicity scores for comments in each subreddit and find that scores assigned to the comments often fall at the extremes (close to 0 or close to 1), suggesting that any reasonable threshold (lying between 0.1 to 0.9) to predict toxicity will lead to similar outcomes.\\n\\nI Details on PII Filters\\n\\nFilter implementation. The Common Crawl, C4, Reddit, and GitHub subsets used the same regular expressions for identifying PII. We refer the reader to our GitHub for exact implementations of our regular expressions for each of the PII types\u2014email address, phone number, and IP address. Once spans are tagged, we employ different processing strategies based on their density on each document:\\n\\n\u2022 5 or fewer PII spans detected: we replace all spans on a page with special tokens |EMAIL_ADDRESS| | , |PHONE_NUMBER| | , and |IP_ADDRESS| | for email addresses, phone numbers, and IP addresses respectively.\\n\\nIn total, we find that 0.02% of documents in the 25 Common Crawl snapshots match this filter.\\n\\n\u2022 6 or more PII spans detected: we remove any document that contains 6 or more matching PII spans. We use this approach because pages containing abundant phone numbers and email addresses are likely to pose a greater risk of disclosing other PII classes. 0.001% of documents in the 25 Common Crawl snapshots match this filter.\\n\\nJ Do quality and content filters have similar effects?\\nIn order to further understand how filters described in \u00a7 5.2, \u00a7 5.3, and \u00a7 5.4 interact with each other, we perform a correlation analysis on a subset of documents sampled from our pipeline. The correlation among the documents flagged for removal by our Common Crawl filters is depicted in Figure 9. Overall, we find that correlations are generally low, thus our filters select fairly different documents and are not redundant.\\n\\n17 reddit.com/r/LocationReddits/wiki/index\\n\\n18 When training models on Dolma, we add these special tokens to the tokenizer vocabulary.\"}"}
{"id": "acl-2024-long-840", "page_num": 32, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: Pearson Correlation of various Dolma filters on the High, Medium, and Low buckets of our Common Crawl data, computed over 24M, 20M, and 43M documents, respectively. The filters are Gopher=Gopher rules from Rae et al. (2021), Dedup.=Deduplication, PII=Personally Identifiable Information, Hate=Toxicity and Decont.=Decontamination. Calculated at the document-level: two filters contribute to positive correlation when any span in a document is tagged by both filters. We find our various filters remove different documents and are not redundant.\\n\\nThere is some positive correlation between our PII (Personal Identifiable Information) filters and filters removing hate speech. This is likely because hate speech is often directed at people. The Gopher filtering rules correlate negatively with our deduplication, especially for the high-perplexity tail part of our data. This is due to the Gopher rules removing many high-perplexity documents such as random strings, which are not caught by deduplication due to their randomness. As these random strings likely do not contribute to a better understanding of language, it is important to filter them out and thus rely on filters beyond deduplication.\\n\\nK Dolma data distribution figures using WIMBD\\n\\nWe use the tool from Elazar et al. (2023) to inspect the final data composition in Figure 10. In particular, we analyze web domain, year, and language distributions. We note that Dolma contains documents from a broad set of internet domains, mostly from 2020, 2022, and 2021. The most common internet domains in Dolma, per token, are patents.google.com, followed by www.nature.com and www.frontiersin.org. In fact, similar to other corpora reported in Elazar et al. (2023), 63.6% of Dolma's web documents are from '.com' sites (followed then by '.org' and '.co.uk' sites). Finally, as all language identification tools are imperfect, we summarize what languages are remaining post English-only filtering: We find the most common language after English is not well identified ('un') with 0.86% of the documents, followed by 0.06% of the documents identified as Chinese.\\n\\nL Test Set Contamination in Dolma\\n\\nDecontamination for perplexity evaluation. Using the paragraph deduplication tools described in \u00a75.4, we mark any paragraph in Dolma as contaminated if (i) it is longer than 13 Unicode-segmented tokens and (ii) it appears in any of the documents in Paloma. To train OLMo-1B, we remove any document with at least one paragraph marked as contaminated. This approach, while prone to false positives, has a negligible impact on the final removal rate (\u22640.001% characters in Dolma contaminated, \u22640.02% of documents removed), and reduces likelihood of false negatives.\\n\\nDecontamination of downstream tasks. Using WIMBD (Elazar et al., 2023), we analyze test set contamination in Dolma. We find contamination of entire datasets from popular benchmarks like GLUE (Wang et al., 2018) and SuperGLUE (Wang et al., 2019), and evaluation datasets like SNLI (Bowman et al., 2015b) and the Winograd Schema Challenge (Levesque et al., 2012). Further analysis reveals that many of these sets are contaminated in our code subset, as public repositories in GitHub often contains copies of these datasets. We report the top contaminated datasets in Figure 11. Results indicate that portion of datasets in Prompt-source appear in Dolma. Six datasets are completely contaminated (100%): the Winograd Schema Challenge (Levesque et al., 2012), Sick (Marelli et al., 2014), AX from GLUE (Wang et al., 2018), SemEval (specifically, Task 1 from 2014), COPA from SuperGLUE (Roemmele et al., 2011), and AX (the diagnostic task) from SuperGLUE (Wang et al., 2019). In addition, other datasets are mostly contaminated, with over 90% of their test sets appearing in Dolma documents: OpenAI HumanEval (Chen et al., 2021), WIC from SuperGLUE (Pilehvar and Camacho-Collados, 2019), ESNLI (Camburu et al., 2018), and SNLI (Bowman et al., 2015a).\\n\\nWe note that the contaminated datasets have been excluded from the downstream tasks we use for model evaluation (c.r.f. Appendix D).\"}"}
{"id": "acl-2024-long-840", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nInformation about pretraining corpora used to train the current best-performing language models is seldom discussed: commercial models rarely detail their data, and even open models are often released without accompanying training data or recipes to reproduce them. As a result, it is challenging to conduct and advance scientific research on language modeling, such as understanding how training data impacts model capabilities and limitations. To facilitate scientific research on language model pretraining, we curate and release Dolma, a three-trillion-token English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. We extensively document Dolma, including its design principles, details about its construction, and a summary of its contents. We present analyses and experimental results on intermediate states of Dolma to share what we have learned about important data curation practices. Finally, we open-source our data curation toolkit to enable reproduction of our work as well as support further research in large-scale data curation.\\n\\n1 Introduction\\n\\nLanguage models are now central to tackling myriad natural language processing tasks, including few-shot learning, summarization, question answering, and more. Increasingly, the most powerful language models are built by a few organizations who withhold most model development details (Anthropic, 2023; OpenAI, 2023; Anil et al., 2023; Gemini Team et al., 2023). In particular, the composition of language model pretraining data is often vaguely described, even in cases where the model itself is released for public use, such as Llama 2 (Touvron et al., 2023b). This hinders understanding of the effects of pretraining corpus composition on model capabilities and limitations, with impacts on scientific progress as well as on the public who interfaces with these models.\\n\\nOur aim is to increase participation in scientific research of language models through open corpora:\\n\\n\u2022 Data transparency helps developers and users of applications that rely on language models to make more informed decisions (Gebru et al., 2021). For example, models have shown to perform better on tasks that are more similar to their pretraining data (Razeghi et al., 2022; Kandpal et al., 2023), or social biases in models' pretraining data may necessitate additional consideration when using them (Feng et al., 2023; Navigli et al., 2023; Seshadri et al., 2023).\\n\\n\u2022 Open pretraining data is necessary to analyze how\"}"}
{"id": "acl-2024-long-840", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The Dolma corpus at-a-glance. It consists of three trillion tokens sampled from a diverse set of domains; sourced from approximately 200 TB of raw text before curation down to an 11 TB dataset. It has been extensively cleaned for language model pretraining use. Tokens calculated using the LLaMA tokenizer.\\n\\nIts composition influences model behavior, allowing those training models to interrogate and improve current data practices (Longpre et al., 2023; Gao, 2021; Elazar et al., 2023). Examples of this research include memorization (Carlini et al., 2022; Chang et al., 2023), deduplication (Lee et al., 2022), adversarial attacks (Wallace et al., 2021), benchmark contamination (Magar and Schwartz, 2022), and training data attribution (Hammoudeh and Lowd, 2022; Grosse et al., 2023).\\n\\nTo support broader participation and inquiry in these lines of research, we present Data for Open Language Models' Appetite (Dolma), an open corpus of three trillion tokens designed to support language model pretraining research. We source much of our data from sources similar to those present in past work, including a mix of web text from Common Crawl, scientific research from Semantic Scholar, code from GitHub, public domain books, social media posts from Reddit, and encyclopedic materials from Wikipedia. Compared to other publicly-available pretraining corpora, Dolma offers a larger pool of tokens at comparable quality while maintaining diverse data composition. In summary, our contributions are two-fold:\\n\\n1. We release the Dolma Corpus, a diverse, multi-source collection of 3T tokens across over 4B documents acquired from 6 different data sources that are (i) commonly seen in large-scale language model pretraining and (ii) made accessible to the general public. Table 1 provides a high-level overview of the amount of data from each source.\\n\\n2. We open source the Dolma Toolkit, a high-performance, portable tool designed to efficiently curate large datasets for language model pretraining. Through this toolkit, practitioners can not only produce our dataset, but also study and improve data curation practices.\\n\\nRelated Work\\n\\nClosed data curation practices in language model pretraining research. Pretraining data practices for language model research have grown increasingly closed, both with respect to access to data as well as documentation of key details about the data itself or its curation practices that would enable reproduction efforts or further scientific study. Proprietary models (e.g., GPT-4, OpenAI, 2023; PaLM 2, Anil et al., 2023; Claude, Anthropic, 2023) disclose little to no information (not even corpus size, or data provenance), and do not share data artifacts. Despite increasing access to powerful open models, few are released alongside their training data; exceptions include T5 on C4 (Raffel et al., 2020), BLOOM (Leong et al., 2022) on ROOTS (Piktus et al., 2023), GPT-J (Wang and Komatsuzaki, 2021), GPT-NeoX (Black et al., 2022), Pythia (Biderman et al., 2023) on Pile (Gao et al., 2020), and INCITE (Together Computer, 2023c) on RedPajama v1 (Together Computer, 2023a). The most powerful open models (e.g., Llama 2 (Touvron et al., 2023b), Mistral (Jiang et al., 2023), Yi (Bai et al., 2023), Qwen (01.AI, 2023)) do not share their data nor provide sufficient details for reproduction. Among large-scale language model pretraining efforts, the ones accompanied with transparent data curation documentation include LLaMA (Touvron et al., 2023a) (released model, unreleased data), Gopher (Rae et al., 2021) (unreleased model and data), and Falcon (Almazrouei et al., 2023) (released model, released partial data). Appendix \u00a7C further illustrates the many unknowns of data curation practices of open and closed models, as well as recent trends away from open data practices that have motivated our work.\\n\\nOpen corpora for language model pretraining. We recognize prior efforts to curate, document, and release open corpora to support language model pretraining.\"}"}
{"id": "acl-2024-long-840", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"research. However, limitations in these prior corpora have motivated us to curate a new dataset:\\n\\n\u2022 C4 (Raffel et al., 2020) (175B tokens) and Pile (Gao et al., 2020) (387B tokens) are high-quality datasets with demonstrated use in training language models, but are unfortunately limited in scale. ROOTS (Pikitis et al., 2023) is large (\u2248400B tokens) but given its multilingual focus, its English-only portion is only 30% of the dataset and thus contributes too few tokens to train English-only models. We recognize that scale and English-only concentration do not imply a \u201chigher-quality\u201d dataset; rather, certain threads of research necessitate these foci, motivating our new corpus (see \u00a73).\\n\\n\u2022 While Falcon (Almazrouei et al., 2023) (580B tokens) and RedPajama v2 (Together Computer, 2023b) (30T tokens) meet our scale criterion, they are entirely derived from Common Crawl web pages, and thus lack source diversity commonly targeted when curating data for the largest language models (e.g., scientific papers, code). We also note that RedPajama v2 is only lightly-curated, distributing content output by CCNet (Wenzek et al., 2020) mostly as-is, thus placing the onus on model developers to decide their own filtering before training.\\n\\n\u2022 RedPajama v1 (Together Computer, 2023a) (\u22481.2T tokens) is most similar to our effort and a source of inspiration when designing Dolma. While RedPajama v1 was a specific reproduction of the LLaMA (Touvron et al., 2023a) training data, we have a broader reproduction target which required diving into data sources that RedPajama v1 did not pursue, including larger collections of scientific papers and social media forums like Reddit (see \u00a73). Further, recent work has identified data quality issues suggesting significant additional cleanup of RedPajama v1 is recommended before costly language model training (Soboleva et al., 2023; Elazar et al., 2023).\\n\\nWhile this manuscript was under review, several other open corpora for language modeling have been released, including FineWeb (Penedo et al., 2024), Zyda (Tokpanov et al., 2024), and the datasets used to train LLM360 Amber (Liu et al., 2023), LLM360 K2 (LLM360 Team, 2024), and MAP-Neo (Zhang et al., 2024) models.\\n\\n3 Data Design Goals\\n\\nWe present the design goals of Dolma and discuss how these goals guided our decision-making during data curation. In sharing these, we hope to inform users of Dolma\u2019s strengths and limitations while also reinforcing practice around such disclosures in dataset curation research (see curation rationales in Bender and Friedman (2018) and motivation questions in Gebru et al. (2021)).\\n\\nBe consistent with prior language model pretraining recipes.\\n\\nBy matching data sources and methods used to create other language modeling corpora, to the extent they are known, we enable the broader research community to use our artifacts to study (and scrutinize) language models being developed today, even those developed behind closed doors. In this reproduction effort, we follow established practices to the extent they are known. Notably, this also means scoping Dolma to English-only text to better leverage known curation practices and maximize generalizability of scientific work on Dolma to existing language models.\\n\\nWhen in doubt, make evidence-backed decisions.\\n\\nStill, there remain myriad data curation decisions for which there is no single clear recipe from prior work, both when best practice isn\u2019t known as well as when implementations differ in subtle ways. In such cases, we prioritize decisions that maximize performance of language models trained on Dolma over a diverse suite of tasks and datasets (see \u00a74.2).\\n\\nLarge scale data to train large models.\\n\\nHoffmann et al. (2022) suggested that one can train compute-optimal models by maintaining a fixed ratio between language model size (in parameters) and a minimum number of training tokens. Recent works that follow these \u201cscaling laws,\u201d such as Llama 2, show that there is still room for performance improvement by increasing the number of training tokens. We aim for a sufficiently large corpus\u20142\u20133T tokens\u2014to allow further study of the relationship between model and dataset size.\\n\\nMake necessary adjustments to preserve openness.\\n\\nA core tenet of our work is openness, which we define to mean (i) sharing the data itself and (ii) documenting the process to curate it. This requirement means we occasionally must deviate from known recipes due to additional practical, legal or ethical considerations that arise when pursuing dataset research in the open. For example, despite their use in training language models like LLaMA, we avoid sources like Books3 (Gao et al., 2020) which are the center of ongoing legal cases around AI use of copyrighted materials (Knibbs, 2023). Similarly, despite the lack of discussion around the removal of personally identifiable information in prior recipes, we perform this filtering to mitigate risks associated with data release (Subramani et al., 2023).\\n\\n4 Data Curation Methodology\\n\\n4.1 The Dolma Toolkit\\n\\nPretraining data curation requires defining complex pipelines that transform raw data from multiple sources into a single collection of cleaned, plain text documents (Wenzek et al., 2020; Almazrouei et al., 2023). To curate Dolma, we create and open-source a high-performance toolkit to facilitate efficient processing on...\"}"}
{"id": "acl-2024-long-840", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"hundreds of terabytes of text content. Our toolkit uni-\\nfies common dataset curation steps into \u201cfiltering\u201d and\\n\u201cmixing\u201d operations:\\n\\n/filter\\nWe unify common data transformations\\nlike language, quality or content filters into a single im-\\nplementation. Given a configuration\u2014a text unit (e.g.,\\ndocument, paragraph, 3 sentence, etc.), a scoring method\\n(e.g., linear classifier, language model perplexity, reg-\\nular expression matches), and a removal policy (e.g.,\\ndelete, replace with string)\u2014our toolkit parallelizes fil-\\ntering operations by identifying and removing undesir-\\nable text at massive scale. For Dolma, we use these to\\nfilter non-English, \u201clow quality\u201d or unnatural,\\ntoxicity, and PII at the document and sub-document levels.\\nIn internal tests to replicate C4 recipe, our toolkit per-\\nformed filtering at a rate of 122 CPU hours per TB; for\\nreference, processing the full \u201craw\u201d Dolma files totaling\\n200 TB on a c6a.48xlarge instance with 192 vCPUs\\nwould take 5 days.\\n\\n/mixing\\nWe unify common cross-file operations,\\nlike up/down-sampling, deduplication and decontami-\\nnation, into a single Rust module that \u201cmixes\u201d content\\nacross files into a smaller set of files. For example, we\\ncan achieve up-sampling by repeatedly reading the same\\nfile paths when mixing. We also implement a Bloom\\nfilter (Bloom, 1970) compatible with our mixer which\\nenables linear-time probabilistic detection of duplicates.\\nWe can repurpose this for test set decontamination by\\nfirst seeding the Bloom filter with test examples, then\\nflagging any detected duplicates when mixing the pre-\\ntraining data.\\n\\n4.2 Data Ablations\\nTo help us make informed decisions, we conduct\\ndata ablations in which we train language models on a\\ndataset following a specific data curation decision, or\\nintervention, and evaluate the resulting model\u2019s perfor-\\nmance on a range of test datasets against a\\nbaseline\\ndataset. By comparing intervention and baseline results\\nwhile controlling for model architecture and training,\\nwe can isolate the impact of specific dataset curation\\ndecisions on downstream models.\\n\\n3 We define a paragraph to be a span of text ending in a\\nnewline UTF-8 character \u201c\\n\u201d.\\n\\n4 The term \u201cquality filter,\u201d while widely used in literature,\\ndoes not appropriately describe the outcome of filtering a\\ndataset. Quality might be perceived as a comment on the\\ninformativeness, comprehensiveness, or other characteristics\\nvalued by humans. However, the filters used in Dolma and\\nother language models efforts select text according to criteria\\nthat are inherently ideological (Gururangan et al., 2022).\\n\\n5 Similar to \u201cquality\u201d, there is no single definition for \u201ctoxi-\\ncity\u201d. Rather, specific definitions vary depending on task (Vid-\\ngen and Derczynski, 2020) and dataset curators\u2019 social iden-\\ntities (Santy et al., 2023); annotators\u2019 beliefs also influence\\ntoxic language detection (Sap et al., 2021). Predicting toxi-\\ncity remains challenging (Welbl et al., 2021; Markov et al.,\\n2023), especially as existing methods have been shown to\\ndiscriminate against minoritized groups (Xu et al., 2021).\\n\\nModel training.\\nWe conduct data ablations using\\na 1.2 billion parameter decoder-only model from the\\nOLMo family of open language models (Groeneveld\\net al., 2024). This is in line with similar model sizes\\nthat have been used for ablations in prior work (Le Scao\\net al., 2022). As training such models to completion\\nis prohibitively expensive, especially when one must\\nperform these experiments for each significant data cu-\\nration decision, we only train these models up to 150\\nbillion tokens before terminating them early. Further\\ndetails of our training setup in Appendix D.1.\\n\\nTasks and test datasets.\\nTo select our evaluation tasks\\nand datasets, we prioritize those that\\n(i) have been used\\nin prior language model pretraining evaluation,\\n(ii) cap-\\nture a diverse range of language model knowledge and\\ncapabilities, and\\n(iii) for which we can avoid test set\\ncontamination (Dodge et al., 2021; Yang et al., 2023).\\nWe arrive at\\n8 datasets\\nin our evaluation suite (full\\ndetails in Appendix \u00a7D) that have been used in prior\\nlanguage modeling research (e.g., LLaMA, Llama 2,\\netc.) and capture a range of capabilities (e.g., question\\nanswering, commonsense reasoning, etc.). Full test set\\ncontamination analysis validating our dataset choices in\\nAppendix \u00a7L.\\n\\nEvaluation.\\nWe perform evaluation of our data ab-\\nlation models using zero-shot in-context prompting,\\ncasting every task as (ranked) text classification, fol-\\nlowing in-context prompt truncation from Min et al.\\n(2022), prompts from PromptSource (Bach et al., 2022),\\nand using an in-house evaluation harness similar to the\\nEleuther harness (Gao et al., 2023).\\n\\n5/5/5 Curating Dolma-Web\\nIn this section, we describe the web subset of Dolma,\\nwhich consists of 2.28T tokens derived from\\nCommon Crawl,\\na collection of over 250 billion pages that were\\ncrawled since 2007. Common Crawl is organized in\\nsnapshots, each corresponding to a full crawl over its\\nseed URLs; as of Feb 2024, there are 97 snapshots. We\\nused 25 snapshots between\\n2020-05\\nto\\n2023-06.\\n\\n5.1 Acquisition &\\nLanguage Filtering\\nOur web pipeline leverages CCNet (Wenzek et al., 2020)\\nto perform language filtering and initial content dedupli-\\ncation. CCNet has been used to develop other language\\nmodel datasets like that for LLaMA, RedPajama v1,\\nRedPajama v2. CCNet processes each web page with\\na FastText (Joulin et al., 2016a) language ID model\\nto determine the primary language for each document; we\\nkeep all pages with English document score greater than\\nor equal to 0.5 (removed 61.7% of the data, by byte size).\\n\\n6 commoncrawl.org\\n7 To minimize storage and compute costs, we only acquired\\nenough shards of Common Crawl to meet our target 2-3T\\ntoken corpus size, assuming at least a 10x reduction from the\\nsum of all data cleaning efforts, including CCNet (\u00a73).\\n8 fasttext.cc/docs/en/language-identification\\n\\n5/5/5 Curating Dolma-Web\\nIn this section, we describe the web subset of Dolma,\\nwhich consists of 2.28T tokens derived from\\nCommon Crawl,\\na collection of over 250 billion pages that were\\ncrawled since 2007. Common Crawl is organized in\\nsnapshots, each corresponding to a full crawl over its\\nseed URLs; as of Feb 2024, there are 97 snapshots. We\\nused 25 snapshots between\\n2020-05\\nto\\n2023-06.\\n\\n5.1 Acquisition &\\nLanguage Filtering\\nOur web pipeline leverages CCNet (Wenzek et al., 2020)\\nto perform language filtering and initial content dedupli-\\ncation. CCNet has been used to develop other language\\nmodel datasets like that for LLaMA, RedPajama v1,\\nRedPajama v2. CCNet processes each web page with\\na FastText (Joulin et al., 2016a) language ID model\\nto determine the primary language for each document; we\\nkeep all pages with English document score greater than\\nor equal to 0.5 (removed 61.7% of the data, by byte size).\\n\\n6 commoncrawl.org\\n7 To minimize storage and compute costs, we only acquired\\nenough shards of Common Crawl to meet our target 2-3T\\ntoken corpus size, assuming at least a 10x reduction from the\\nsum of all data cleaning efforts, including CCNet (\u00a73).\\n8 fasttext.cc/docs/en/language-identification\"}"}
{"id": "acl-2024-long-840", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Further, CCNet identifies and removes very common paragraphs by grouping shards in each snapshot into small sets and removing duplicated paragraphs in each. This step removed approximately 70% of paragraphs, primarily consisting of headers and navigation elements. Overall, CCNet pipeline filters out 84.2% of the content in Common Crawl, from 175.1 TB to 27.7 TB. More details are provided in our Datasheet \u00a7N.\\n\\n5.2 Quality Filtering\\n\\nWeb crawled data requires significant cleanup before language model training; undesirable content ranges from artifacts introduced by HTML to plain text conversion (e.g., page headers, ill-formatted text) to pages lacking \u201cprose-like\u201d content (e.g., boilerplate text, short segments). Per arguments posed in Rae et al. (2021) and Almazrouei et al. (2023) against model-based quality filters, we approach quality filtering by combining heuristics introduced by Gopher and C4. Specifically, we keep all the Gopher rules (Gopher All) and keep a single heuristic from C4 designed to remove paragraphs that do not end in punctuation (C4 NoPunc), as opposed to adopting the full set of C4 rules (C4 All). Implementation details of all filtering rules are provided in our Datasheet \u00a7N.\\n\\n![Figure 1](image1.png)\\n\\nWe find a positive effect of web data quality filters on 1.2B model performance, evaluated across training iterations, over a no-filtering baseline. We only show results on HellaSwag here; all figures for other evaluation datasets are in the Appendix \u00a7O. Ablation results shown in \u00a71 validate our filtering strategy: we find that C4 NoPunc on its own outperforms both C4 All as well as Gopher All on both perplexity and downstream tasks. Finally, combining Gopher All + C4 NoPunc offers the best performance. In all, Gopher All tagged 15.23% of UTF-8 characters for removal, while C4 NoPunc tagged 22.73% of characters for removal.\\n\\n5.3 Content Filtering\\n\\nFiltering Toxic Content\\n\\nData sampled from the web often contains harmful or toxic content (Matic et al., 2020; Luccioni and Viviano, 2021; Birhane et al., 2023a,b). Such content is often filtered to minimize the likelihood that downstream language models are prone to toxic content generation (Anil et al., 2023; Rae et al., 2021; Thoppilan et al., 2022; Hoffmann et al., 2022; Longpre et al., 2023). To remove this content from Dolma, we train our own FastText classifiers on the Jigsaw Toxic Comments (cjadams et al., 2017) dataset, producing two models that identify \\\"hate\\\" and \\\"NSFW\\\" content, respectively. See Appendix \u00a7H for implementation details. We run these classifiers on Common Crawl sentences and remove any sentence scored above a set threshold.\\n\\n![Figure 2](image2.png)\\n\\nWe find a positive effect of web data content filters on 1.2B model performance, evaluated across training iterations, over a no-filtering baseline. We only show results on HellaSwag here; all figures for other evaluation datasets are in the Appendix \u00a7O. To understand filter thresholding effects on Dolma, we conduct a data ablation choosing two very different thresholds for these content filters (\u00a72). We find the \\\"High Threshold\\\" ($\\\\tau = 0.4$) removes less content (5.5\u20137.3%), but generally yields lower downstream performance than the \\\"Low Threshold\\\" ($\\\\tau = 0.0004$) which removes more content (29.1\u201334.9%).\\n\\nWeighing the tradeoff between dataset scale (\u201cHigh\u201d) and performance maximization (\u201cLow\u201d), we adopt the more permissive \\\"High\\\" threshold to ensure we meet our minimum token count requirement. The cause of this was surprising: Our quality, content, and deduplication filters overlap very little in which texts they remove.\\n\\n9 Using BlingFire sentence splitter (Microsoft, 2019).\\n\\n10 Manual inspection of the distribution of sentence scores revealed a bi-modal distribution with peaks near 0.0 and 1.0 (e.g., Figure 8). As such, we chose \\\"Low\\\" to remove even slightly toxic data (>0.0), and \\\"High\\\" to limit our max data removal amount to preserve our target dataset scale.\"}"}
{"id": "acl-2024-long-840", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"resulting in a compounded filtering effect when combining them. In future versions of Dolma, we will start with more shards of Common Crawl and adopt stricter filter thresholds.\\n\\nFiltering Personally Identifiable Information Data sampled from the web can also leak personally identifiable information (PII) of users (Luccioni and Viviano, 2021; Subramani et al., 2023). Traces of PII are abundant in large-scale datasets (Elazar et al., 2023), and language models have also been shown to reproduce PII at inference time (Carlini et al., 2022; Chen et al., 2023b). Dolma's size makes it impractical to use model-based PII detectors like Presidio (Microsoft, 2018); instead, we rely on carefully-crafted regular expressions that sacrifice some accuracy for significant speed-up. Following Subramani et al. (2023), we focus on three kinds of PII that are detectable with high precision: email addresses, IP addresses and phone numbers. For documents with 5 or fewer PII spans, we replace the span with a special token (e.g., EMAIL_ADDRESS); this affects 0.02% of documents. Otherwise, we remove entire documents with higher density of PII spans; this affects 0.001% of documents. In data ablation experiments, we find that execution details around PII (e.g., removal versus special token replacement) had no effect on model performance, which is expected given the tiny percentage of affected data. See Appendix \u00a7I for implementation details; all figures for results on evaluation suite are in the Appendix \u00a7O.\\n\\n5.4 Deduplication\\n\\nDeduplication of pretraining data has been shown to be effective for improving token efficiency during model training (Lee et al., 2022; Abbas et al., 2023; Tirumala et al., 2023); as such, it has become common practice among pretraining data recipes. In Dolma, we perform three stages of deduplication:\\n\\n(i) Exact URL dedup filters 53.2% of documents.\\n(ii) Exact document dedup filters 14.9% of URL-deduped documents, including empty documents.\\n(iii) Exact paragraph dedup filters 18.7% of paragraphs from the URL-deduped documents, including empty paragraphs.\\n\\nThis multi-stage approach is designed to increase efficiency: Stage (i) is commonly used first thanks to its computational efficiency (Agarwal et al., 2009; Koppula et al., 2010; Penedo et al., 2023). Stages (i) and (ii) are designed to remove copies of the same item, such as re-crawls of the same URL and identical pages with multiple URLs (e.g., same news article in multiple online newspapers). Performing these early before any content or quality filtering greatly reduces the number of documents to process. In contrast, Stage (iii) removes common boilerplate content (e.g., the byline under all articles by the same author); as paragraph removal risks disrupting content analysis, we perform it last. We perform all three stages using the Bloom filter in \u00a74.1.\\n\\n5.5 Putting It All Together\\n\\nTo summarize, the Dolma web pipeline transforms the output of CCNet through URL and document-level deduplication, then quality and content filtering, and finally paragraph-level deduplication.\\n\\n![Figure 3: We find a positive compounding effect on 1.2B model performance, evaluated across training iterations, when stacking quality filtering, content filtering and paragraph-level deduplication, over a no-filtering baseline. We show results on HellaSwag here; all figures for other evaluation datasets are in the Appendix \u00a7O.](image)\\n\\nWe show the positive compounding effect of all stages of our web pipeline on downstream model performance, as assessed through our data ablations \u00a74.2. We present summary statistics in Appendix \u00a7K.\\n\\n6 Curating Dolma-Code\\n\\nIn this section, we describe the code subset of Dolma, which consists of 411B tokens derived from GitHub.\\n\\n6.1 Acquisition & Filter Language Filtering\\n\\nLike prior work in code language models (e.g., StarCoder (Li et al., 2023b)), we also acquire code through the Stack (Kocetkov et al., 2022), a deduplicated but otherwise unfiltered collection of permissively-licensed GitHub repositories. The raw version of this dataset was collected in March 2023. We filter data-heavy files with extensions such as JSON and CSV.\\n\\n6.2 Quality Filtering\\n\\nWe apply heuristics derived from the code subset of RedPajama v1 and StarCoder. RedPajama v1 uses rules to remove repetitive file preambles, such as license statements and documents with excessively long lines or mostly numerical content. Overall, RedPajama v1 removes files that are mostly data or generated through templates. To select high-quality code snippets, we also use rules from the StarCoder pipeline; these heuristics filter GitHub repositories with no to few stars, files with too few or too many comments, and HTML files with low code-to-text ratio. Implementation details of all filtering rules are provided in our Datasheet \u00a7N.\\n\\nWhen conducting data ablations, we find that, compared to RedPajama v1 rules alone, RedPajama v1 and...\"}"}
{"id": "acl-2024-long-840", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"StarCoder rules combined lead to lower perplexity on code datasets (e.g., HumanEval; Chen et al., 2021) and improved performance on datasets in our evaluation suite. Therefore, we chose to use this combination of the two filtering rules for this Dolma code subset.\\n\\n6.3 Content Filtering\\n\\nWe apply the same heuristics to filter and mask PII used in the web subset (\u00a75). Additionally, we filter any documents containing code secrets and software-specific personal information by running the detect-secrets library (Yelp, 2013) and removing any documents with a match.\\n\\n6.4 Deduplication\\n\\nWe started from the already-deduplicated version of the Stack, which used the pipeline first introduced by Allal et al. (2023), which uses MinHash (Broder, 2002) and Locally Sensitive Hashing to find similar documents.\\n\\n7. Curating Dolma-Social\\n\\nIn this section, we describe the social media subset of Dolma, which consists of 80B tokens derived from Reddit data.\\n\\n7.1 Acquisition & Filter\\n\\nWe derive this subset from 378M posts from December 2005 until March 2023 obtained through Pushshift (Baumgartner et al., 2020). We include both submissions\u2014initial message in conversations on Reddit\u2014and comments\u2014replies to messages. The tree-like structure of Reddit threads allows for multiple possible data formats depending on how the various components of a thread are linearized for language model pretraining. To better inform this transformation, we conduct a data ablation over several approaches:\\n\\n1. Atomic Content. Treats all comments and submissions as independent documents.\\n2. Partial Threads. Comments from the same thread combined into a multi-round dialogue between users. Submissions as separate documents.\\n3. Full Threads. Combines submissions with all child comments into one document.\\n\\nSee Appendix \u00a7E for implementation details. From results in Figure 4, we see treating submissions and comments as independent documents (Atomic Content) leads to better performance on our evaluation suite. We hypothesize that artificial formatting introduced when combining thread elements negatively impacts language model training; we leave further investigation to future work. Finally, we filter non-English content using the approach from \u00a75.1.\\n\\nAll figures for results on evaluation suite in Appendix \u00a7O.\\n\\n7.2 Quality Filtering\\n\\nLike web crawled data, social media posts also require significant cleanup before language model training. We repurpose the pipeline introduced by Henderson et al. (2019) to filter submissions and comments. We remove comments shorter than 500 characters, and submissions shorter than 400 characters.\\n\\nWe also remove documents over 40,000 characters. We remove comments with fewer than 3 votes, as lower scores are more likely for comments that are deeply nested in a conversational thread (Weninger et al., 2013) or content that is more likely to result in emotionally-charged discourse (Davis and Graham, 2021). Votes have been used as a signal in constructing the WebText (Radford et al., 2019) and OpenWebText (Peterson, 2020) corpora. We discard documents that have been deleted by their authors, removed by moderators, or labeled by their authors as \u201cover 18\u201d.\\n\\nWe exclude any document originated from a set 26,123 banned or NSFW subreddits.\\n\\n7.3 Content Filtering\\n\\nWe apply the same content filtering in \u00a75.3, except due to the short length of many Reddit documents, instead of masking PII, we fully remove the document.\\n\\n7.4 Deduplication\\n\\nWe employ the same strategy used in the web pipeline (\u00a75.4). Since submissions and comments are shorter than web documents, we only deduplicate at a\"}"}
{"id": "acl-2024-long-840", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This strategy is useful to reduce the incidence of \\\"copypasta\\\" (identical text repeated across comments and subreddits for comedic effect) and other repetitive information.\\n\\nIn this section, we briefly summarize additional high-quality sources that were used to derive Dolma. More details on collection and processing in Datasheet \u00a7N.\\n\\nC4 for Curated Web Content\\n\\nSimilar to data recipes for LLaMA and Llama 2, we supplement our web subset with C4 (Raffel et al., 2020). We further refine this data by reprocessing it through our full web pipeline (excluding URL deduplication) (\u00a75) which removed additional content, including more low-quality and duplicated texts, and performed PII masking.\\n\\nSemantic Scholar for Academic Literature\\n\\nThe P2S2o dataset (Soldaini and Lo, 2023) is a collection of approximately 40 million open-access academic papers that have been cleaned, filtered, deduplicated, and formatted for pretraining language models. It is derived from the Semantic Scholar Open Research Corpus (S2ORC; Lo et al., 2020). As this dataset has been created for language modeling purposes, we use it as-is.\\n\\nProject Gutenberg for Books\\n\\nProject Gutenberg is a repository of over 70 thousand public domain books. We collected Project Gutenberg's archive in April 2023. We use English language books, which we filter using the same approach described in \u00a75.1. We deduplicate this dataset based on book title exact match.\\n\\nWikipedia and Wikibooks for Encyclopedic Content\\n\\nThis dataset was derived by March 2023 Wikimedia dumps. We use the \\\"English\\\" and \\\"Simple\\\" editions of Wikipedia and Wikibooks as base for the Encyclopedic subset of Dolma. Sources were processed using WikiExtractor (Attardi, 2023). We remove any document with 25 or fewer UTF-8-segmented words, as we found shorter pages to either be the result of short, templated pages (e.g., pages containing only a few words and an information box) or XML parsing errors. By design, this dataset does not contain duplicated documents.\\n\\nTraining a Language Model on Dolma\\n\\nAs a final validation step of the Dolma pipeline, we train, evaluate and release a decoder-only, autoregressive language model which we call OLMo-1B. We present zero-shot experimental results of OLMo-1B on a range of downstream tasks demonstrating comparable quality to other released language models of comparable size.\\n\\nTable 2: Comparison of OLMo-1B and other similarly-sized language models on our evaluation suite.\\n\\n| Task       | StableLM (1.6B) | Pythia (1.1B) | TinyLlama (1.1B) | OLMo-1B (1.2B) |\\n|------------|-----------------|---------------|------------------|---------------|\\n| ARC-E      | 63.7            | 50.2          | 53.2             | 58.1          |\\n| ARC-C      | 43.8            | 33.1          | 34.8             | 34.5          |\\n| BoolQ      | 76.6            | 61.8          | 64.6             | 60.7          |\\n| HellaSwag  | 68.2            | 44.7          | 58.7             | 62.5          |\\n| OpenBookQA | 45.8            | 37.8          | 43.6             | 46.4          |\\n| PIQA       | 74.0            | 69.1          | 71.1             | 73.7          |\\n| SciQ       | 94.7            | 86.0          | 90.5             | 88.1          |\\n| WinoGrande | 64.9            | 53.3          | 58.9             | 58.9          |\\n| Average    | 66.5            | 54.5          | 59.4             | 60.3          |\\n\\nWe note that, while all models share a roughly comparable number of parameters, only TinyLlama was trained on roughly the same number of tokens as OLMo-1B. Pythia was trained on nearly 10 times fewer tokens and StableLM was trained on 2 trillion tokens for two epochs (data composition not shared). Nevertheless, we find that OLMo-1B performs better on average than the most comparable model, TinyLlama, outperforming it in 4 out of 8 tasks from our evaluation suite \u00a74.2. Though zero-shot evaluations of such tasks are often challenging for smaller 1B models, we see that performance across all tasks and models is above naive random performance.\\n\\nMeasuring Domain Fit\\n\\nIn \u00a73, we motivated our decision in curating Dolma to cover a diverse set of sources. In this section, we use OLMo-1B to assess Dolma's distribution of documents leads to pretrained language models that fit well to diverse textual domains, compared to training on other open corpora. To represent diverse domains, we use Paloma (Magnusson et al., 2023), a stratified collection of hundreds of fine-grained textual sources; thus, training on more diverse datasets should result in models with lower overall perplexity on Paloma. We repeat our data ablation methodology, training 1.2B models on 150B token samples from C4, mC4 (English-only) (Xue et al., 2020), RedPajama v1, RefinedWeb (Almazrouei et al., 2023), Pile, and Dolma.\\n\\nFrom the results in Figure 5, we observe the following: (1) The model trained on Pile performs well as it is comprised of many diverse sources, despite its overall smaller scale. (2) Larger multi-source datasets like Dolma and, to a lesser extent, RedPajama v1 yield models with similar coverage of diverse domains to Pile. (3) Finally, training on single-source corpora like C4, mC4 (English-only), and RefinedWeb leads to models with poor fit to diverse domains as indicated by higher average perplexity.\"}"}
{"id": "acl-2024-long-840", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: 1.2B parameter language models trained on 150B tokens from Dolma and other open corpora, evaluated across training iterations on perplexity over diverse domains in Paloma (Magnusson et al., 2023).\\n\\nIntuitively, the model trained on the Pile is well-fit to such data as that pretraining corpus is mostly sourced from similar smaller, hand-picked sources. But as we wish to scale the total number of tokens in a corpus, the challenge becomes how to integrate more available web data without losing sample efficiency on diverse evaluations such as Paloma. In this case, we see that OLMo-1B nearly matches the perplexity curve of the Pile model despite a much larger fraction of web data included.\\n\\nConclusion\\n\\nIn this manuscript, we introduce Dolma, a three trillion token English corpus for language model pretraining. The Dolma corpus is comprised of a diverse set of content, including web documents, scientific papers, code, public-domain books, social media, and encyclopedic materials. Building off a list of explicit desiderata, we document our data curation pipelines, providing experimental results that support our decisions. We freely release Dolma and open-source all tools we used to curate this dataset as part of the OLMo project (Groeneveld et al., 2024). Since the time of writing, we have made improvements to Dolma and have continued to make releases; for example, our follow-on release of Dolma v.1.7 yields significant performance improvement on downstream tasks, holding the model constant.\\n\\nWe hope this line of work can promote transparency, reproducibility, and further research in the field of language modeling, as well as address the current gap in the availability of pretraining data of commercial and open language models. We release Dolma under ODC-By and our toolkit under Apache 2.0.\\n\\nLimitations\\n\\nEnglish-only corpus. Dolma was curated to contain English data. As tools for language identification may have false negatives, Dolma might contain a small percentage of non-English data. Traces of non-English data are unlikely to lead to any meaningful downstream performance on non-English tasks for any model trained on Dolma. Thus, Dolma reinforces the expectation of English being the \u201cdefault\u201d language for NLP.\\n\\nRepresentativeness of sources in Dolma. As mentioned in \u00a73, it is impossible to curate a corpus that is representative of all language model data curation practices. Further, many open and closed language models are trained on content that cannot be acquired or redistributed, and thus could not be included in Dolma.\\n\\nSingle model configuration for ablations. The experimental setup we use to validate our data curation pipeline only covers a subset of model types used to create language models. For example, while many language models are in the 7 billion to 70 billion parameters range, we train 1 billion parameter models; further, we did not investigate the use of any alternative architectures to dense auto-regressive transformer models. This choice was dictated by the need to efficiently iterate over many possible configurations, but it might result in design decisions that are not relevant at larger model sizes. We expect downstream model developers to scrutinize Dolma before using it to train their language models, similar to the process we sketch in \u00a79.\\n\\nLimited tasks in evaluation suite. As detailed in \u00a74.2, we select tasks that have been used to evaluate previous base language models, and that are not present in our training data (i.e., Dolma is not contaminated against them). As such, we can only assess a subset of tasks language models are routinely used for. For example, the effect of adding code to pretraining data cannot be fully measured until models are able to generate executable code; such capability is typically observed only after models are finetuned to follow instructions (Munnighoff et al., 2023a; Zhuo et al., 2024).\\n\\nManual inspection and evaluation of Dolma is infeasible. Given the corpus size, it is impossible to fully inspect Dolma to assess its content. While tools like WIMBD (Elazar et al., 2023) and Data Portraits (Marone and Durme, 2023) aid programmatic inspection of subsets of data, they cannot provide an assessment of all documents in a corpus. As such, we cannot fully describe the properties of Dolma in terms of data distribution, content quality, and potential harms due to the inclusion or exclusion of particular content.\\n\\nEthical Considerations\\n\\nMinimize risk of harm to individuals during data curation. Curating a pretraining corpus may introduce risk to individuals, either by facilitating access during data curation. Curating a pretraining corpus may introduce risk to individuals, either by facilitating access during data curation.\"}"}
{"id": "acl-2024-long-840", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to information that is present in the corpus, or by enabling training of harmful models that disclose personal information (Carlini et al., 2020) or produce toxic content (Gehman et al., 2020; Ngo et al., 2021). To minimize these risks while meeting our stated goals, we engaged with legal and ethics experts early in the project and evaluated data design decisions based on their feedback on a case-by-case basis. Broadly, we follow accepted practices when available (e.g., masking of certain personal identifiable information), and take a measured approach when diverging opinions exist in the literature (e.g., most effective approach to identify and remove toxic content). Further, we will provide tools to request data removal.\\n\\nWe believe in compromising on desired research artifact properties like model reproducibility, performance, and extensibility in cases of significant harm to individuals.\\n\\nBesides a risk-based approach, alternative frameworks for considering the ethical implications of language model data have also been proposed. Data stewardship (Jernite et al., 2022) seeks to create a framework to collect and reflect explicit interests of data owners. Data trusts (Chan et al., 2023) or data licensing (Li et al., 2023a) can also enable explicit consent in sharing data for AI training. As no current state-of-the-art model is trained on data collected through these frameworks, these approaches would limit the representativeness goal stated in \u00a73. As these principles are adopted, we will consider them for future versions of Dolma.\\n\\nCopyright and fair use considerations.\\n\\nAt the time of writing, the landscape governing applicability of copyright law and fair use doctrine (also known as \u201cfair dealing\u201d) and language models is largely undetermined (Cooper et al., 2023; Lee et al., 2024). In the United States, legal scholars and practitioners have suggested that training models on copyright content might constitute fair use (Balasubramaniam et al., 2023; MacKie-Mason and Li, 2023; Henderson et al., 2023), while also recognizing limitations of existing doctrine in this application (Farhadi et al., 2023). Further, legal assessments regarding the use of copyrighted data in language models vary widely depending on jurisdiction: in early 2024, Israel (Israel Ministry of Justice, 2022) and Japan (Technomancers.ai, 2023) allow copyrighted content to be used for AI training data, although the latter is currently re-considering this framework. While most datasets we used were curated with copyright and licensing in mind (e.g., open access papers in peS2o (Soldaini and Lo, 2023), open source repositories in the Stack (Kocetkov et al., 2022)) or were already permissively licensed (e.g., Wikipedia is released under a Creative Commons license), we recognize that large web crawls may also contain copyrighted material. Yet, given current tools, it's not possible to reliably or scalably detect copyrighted materials in a corpus of this size. Our decision to curate and distribute Dolma factors in several considerations, including that all our data sources were publicly available and already being used in large-scale language model pretraining (both open and closed). We recognize that the legal landscape of AI is changing rapidly, especially as it pertains to use of copyrighted materials for training models.\\n\\nReferences\\n\\nAmro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari S. Morcos. 2023. Semdedup: Data-efficient learning at web-scale through semantic deduplication. ArXiv, abs/2303.09540.\\n\\nJudit Acs. 2019. Exploring BERT\u2019s Vocabulary.\\n\\nAmit Agarwal, Hema Swetha Koppula, Krishna P. Leela, Krishna Prasad Chitrapura, Sachin Garg, Pavan Kumar GM, Chittaranjan Haty, Anirban Roy, and Amit Sasturkar. 2009. Url normalization for de-duplication of web pages. In Proceedings of the 18th ACM Conference on Information and Knowledge Management, CIKM \u201909, page 1987\u20131990, New York, NY, USA. Association for Computing Machinery.\\n\\nOrevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, and Yulia Tsvetkov. 2023. Do all languages cost the same? tokenization in the era of commercial language models.\\n\\nLoubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Munoz Ferrandis, Niklas Muennighoff, Mayank Mishra, Alex Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, Joel Lamy Poirier, Hailey Schoelkopf, Sergey Troshin, Dmitry Abulkhanov, Manuel Romero, Michael Lappert, Francesco De Toni, Bernardo Garc\u00eda del R\u00edo, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, Ian Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, David Lansky, Huu Nguyen, Danish Contractor, Luis Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, Sean Hughes, Daniel Fried, Arjun Guha, Harm de Vries, and Leandro von Werra. 2023. SantaCoder: don\u2019t reach for the stars! arXiv [cs.SE].\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, Merouane Debbah, Etienne Goffinet, Daniel Heslow, Julien Launay, Quentin Malartic, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. Falcon-40B: an open large language model. TII UAE.\\n\\nAngelescu, Radu. 2013. GutenbergPy. https://github.com/raduangelescu/gutenbergpy. Version 0.3.5 [accessed August 2023].\\n\\nRohan Anil, Andrew M. Dai, Orhan Firat, Melvin Johnson, Dmitry Lepikhin, Alexandre Tachard Pas sos, Siamak Shakeri, Emanuel Taropa, Paige Bailey,\"}"}
{"id": "acl-2024-long-840", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Cr\u00e9py, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D\u00edaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier Garc\u00eda, Sebastian Gehrmann, Lucas Gonz\u00e1lez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. ArXiv, abs/2305.10403.\\n\\nAnthropic. 2023. Introducing Claude. https://www.anthropic.com/index/introducing-claude. Accessed: 2024-02-15.\\n\\nGiuseppe Attardi. 2023. Wikiextractor. https://github.com/attardi/wikiextractor/tree/8f1b434a80608e1e313d38d263ed7c79c9ee75a9. Accessed: 2024-02-15.\\n\\nTom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni. 2022. ReFinED: An efficient zero-shot-capable approach to end-to-end entity linking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pages 209\u2013220, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.\\n\\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93\u2013104, Dublin, Ireland. Association for Computational Linguistics.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. ArXiv, abs/2309.16609.\\n\\nGowri Saini Balasubramaniam, Sara Rachel Benson, Anita Say Chan, Keith Jacobs, Karen V. Jenkins, Smirity Kaushik, Jiaqi Ma, Madelyn Rose Sanfilippo, Eryclis Rodrigues Bezerra Silva, Emmy Tither, Michael Twidale, Ted E. Underwood, Yaman Yu, and Kyrie Zhou. 2023. Comment on docket document (colc-2023-0006-0001): Copyright and artificial intelligence (AI). https://www.regulations.gov/comment/COLC-2023-0006-8998. Posted by the U.S. Copyright Office. See attached file(s).\\n\\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. arXiv [cs.SI].\\n\\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604.\\n\\nStella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373.\\n\\nAbeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti, and Alexandra Sasha Luccioni. 2023a. Into the laions den: Investigating hate in multimodal datasets. ArXiv, abs/2311.03449.\\n\\nAbeba Birhane, Vinay Uday Prabhu, Sanghyun Han, and Vishnu Naresh Boddeti. 2023b. On hate scaling laws for data-swamps. ArXiv, abs/2306.13141.\\n\\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about physical commonsense in natural language. arXiv [cs.CL].\\n\\nSid Black, Stella Rose Biderman, Eric Hallahan, Quentin G. Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason R. Z. Chen, Eric Chu, J. Clark, Laurent El Shafey, Yanping Huang, Kathleen S. Meier-Hellstern, Gaurav Mishra, Erica Moreira, Mark Omernick, Kevin Robinson, Sebastian Ruder, Yi Tay, Kefan Xiao, Yuanzhong Xu, Yujing Zhang, Gustavo Hernandez Abrego, Junwhan Ahn, Jacob Austin, Paul Barham, Jan A. Botha, James Bradbury, Siddhartha Brahma, Kevin Michael Brooks, Michele Catasta, Yongzhou Cheng, Colin Cherry, Christopher A. Choquette-Choo, Aakanksha Chowdhery, C Cr\u00e9py, Shachi Dave, Mostafa Dehghani, Sunipa Dev, Jacob Devlin, M. C. D\u00edaz, Nan Du, Ethan Dyer, Vladimir Feinberg, Fan Feng, Vlad Fienber, Markus Freitag, Xavier Garc\u00eda, Sebastian Gehrmann, Lucas Gonz\u00e1lez, Guy Gur-Ari, Steven Hand, Hadi Hashemi, Le Hou, Joshua Howland, An Ren Hu, Jeffrey Hui, Jeremy Hurwitz, Michael Isard, Abe Ittycheriah, Matthew Jagielski, Wen Hao Jia, Kathleen Kenealy, Maxim Krikun, Sneha Kudugunta, Chang Lan, Katherine Lee, Benjamin Lee, Eric Li, Mu-Li Li, Wei Li, Yaguang Li, Jun Yu Li, Hyeontaek Lim, Han Lin, Zhong-Zhong Liu, Frederick Liu, Marcello Maggioni, Aroma Mahendru, Joshua Maynez, Vedant Misra, Maysam Moussalem, Zachary Nado, John Nham, Eric Ni, Andrew Nystrom, Alicia Parrish, Marie Pellat, Martin Polacek, Alex Polozov, Reiner Pope, Siyuan Qiao, Emily Reif, Bryan Richter, Parker Riley, Alexandra Ros, Aurko Roy, Brennan Saeta, Rajkumar Samuel, Renee Marie Shelby, Ambrose Slone, Daniel Smilkov, David R. So, Daniela Sohn, Simon Tokumine, Dasha Valter, Vijay Vasudevan, Kiran Vodrahalli, Xuezhi Wang, Pidong Wang, Zirui Wang, Tao Wang, John Wieting, Yuhuai Wu, Ke Xu, Yunhan Xu, Lin Wu Xue, Pengcheng Yin, Jiahui Yu, Qiaoling Zhang, Steven Zheng, Ce Zheng, Wei Zhou, Denny Zhou, Slav Petrov, and Yonghui Wu. 2023. Palm 2 technical report. ArXiv, abs/2305.10403.\\n\\nAnthropic. 2023. Introducing Claude. https://www.anthropic.com/index/introducing-claude. Accessed: 2024-02-15.\\n\\nGiuseppe Attardi. 2023. Wikiextractor. https://github.com/attardi/wikiextractor/tree/8f1b434a80608e1e313d38d263ed7c79c9ee75a9. Accessed: 2024-02-15.\\n\\nTom Ayoola, Shubhi Tyagi, Joseph Fisher, Christos Christodoulopoulos, and Andrea Pierleoni. 2022. ReFinED: An efficient zero-shot-capable approach to end-to-end entity linking. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Track, pages 209\u2013220, Hybrid: Seattle, Washington + Online. Association for Computational Linguistics.\\n\\nStephen Bach, Victor Sanh, Zheng Xin Yong, Albert Webson, Colin Raffel, Nihal V. Nayak, Abheesht Sharma, Taewoon Kim, M. Saiful Bari, Thibault Fevry, Zaid Alyafeai, Manan Dey, Andrea Santilli, Zhiqing Sun, Srulik Ben-david, Canwen Xu, Gunjan Chhablani, Han Wang, Jason Fries, Maged Alshaibani, Shanya Sharma, Urmish Thakker, Khalid Almubarak, Xiangru Tang, Dragomir Radev, Mike Tian-jian Jiang, and Alexander Rush. 2022. PromptSource: An integrated development environment and repository for natural language prompts. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 93\u2013104, Dublin, Ireland. Association for Computational Linguistics.\\n\\nJinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen technical report. ArXiv, abs/2309.16609.\\n\\nGowri Saini Balasubramaniam, Sara Rachel Benson, Anita Say Chan, Keith Jacobs, Karen V. Jenkins, Smirity Kaushik, Jiaqi Ma, Madelyn Rose Sanfilippo, Eryclis Rodrigues Bezerra Silva, Emmy Tither, Michael Twidale, Ted E. Underwood, Yaman Yu, and Kyrie Zhou. 2023. Comment on docket document (colc-2023-0006-0001): Copyright and artificial intelligence (AI). https://www.regulations.gov/comment/COLC-2023-0006-8998. Posted by the U.S. Copyright Office. See attached file(s).\\n\\nJason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. arXiv [cs.SI].\\n\\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating system bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587\u2013604.\\n\\nStella Rose Biderman, Hailey Schoelkopf, Quentin G. Anthony, Herbie Bradley, Kyle O'Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar van der Wal. 2023. Pythia: A suite for analyzing large language models across training and scaling. ArXiv, abs/2304.01373.\\n\\nAbeba Birhane, Vinay Prabhu, Sang Han, Vishnu Naresh Boddeti, and Alexandra Sasha Luccioni. 2023a. Into the laions den: Investigating hate in multimodal datasets. ArXiv, abs/2311.03449.\\n\\nAbeba Birhane, Vinay Uday Prabhu, Sanghyun Han, and Vishnu Naresh Boddeti. 2023b. On hate scaling laws for data-swamps. ArXiv, abs/2306.13141.\\n\\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2019. PIQA: Reasoning about physical commonsense in natural language. arXiv [cs.CL].\\n\\nSid Black, Stella Rose Biderman, Eric Hallahan, Quentin G. Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason R.\"}"}
{"id": "acl-2024-long-840", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phang, Michael Martin Pieler, USVSN Sai Prashanth, Shivanshu Purohit, Laria Reynolds, Jonathan Tow, Benqi Wang, and Samuel Weinbach. 2022. Gpt-neox-20b: An open-source autoregressive language model. ArXiv, abs/2204.06745.\\n\\nSu Lin Blodgett, Lisa Green, and Brendan O'Connor. 2016. Demographic dialectal variation in social media: A case study of African-American English. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1119\u20131130, Austin, Texas. Association for Computational Linguistics.\\n\\nBurton H Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Communications of the ACM, 13(7):422\u2013426.\\n\\nSamuel Bowman, Gabor Angeli, Christopher Potts, and Christopher D Manning. 2015a. A large annotated corpus for learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 632\u2013642. Association for Computational Linguistics.\\n\\nA Z Broder. 2002. On the resemblance and containment of documents. In Proceedings. Compression and Complexity of SEQUENCES 1997 (Cat. No.97TB100171), pages 21\u201329. IEEE Comput. Soc.\\n\\nOana-Maria Camburu, Tim Rockt\u00e4schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances in Neural Information Processing Systems, 31.\\n\\nNicholas Carlini, Daphne Ippolito, Matthew Jagielski, Katherine Lee, Florian Tramer, and Chiyuan Zhang. 2022. Quantifying memorization across neural language models. arXiv [cs.LG].\\n\\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-Voss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, Alina Oprea, and Colin Raffel. 2020. Extracting training data from large language models. arXiv [cs.CR].\\n\\nTommaso Caselli, Valerio Basile, Jelena Mitrovi\u0107, and Michael Granitzer. 2021. HateBERT: Retraining BERT for abusive language detection in English. In Proceedings of the 5th Workshop on Online Abuse and Harms (WOAH 2021), pages 17\u201325, Online. Association for Computational Linguistics.\\n\\nAlan Chan, Herbie Bradley, and Nitarshan Rajkumar. 2023. Reclaiming the digital commons: A public data trust for training data. In Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, AIES \u201923, page 855\u2013868, New York, NY, USA. Association for Computing Machinery.\\n\\nKent K. Chang, Mackenzie Cramer, Sandeep Soni, and David Bamman. 2023. Speak, memory: An archaeology of books known to chatgpt/gpt-4. ArXiv, abs/2305.00118.\"}"}
{"id": "acl-2024-long-840", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-840", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-840", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-840", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Srinivasan, Claudia van der Salm, Andreas Fidje-land, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-Pluci\u00b4nska, David Bridson, Dario de Ce-sare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Ivo Penchev, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Glober-gson, Adam Kurzrok, Lynette Webb, Sahil Dua, Dong Li, Preethi Lahoti, Surya Bhupatiraju, Dan Hurt, Ha-roon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Taylan Bilal, Evgenii Eltyshhev, Daniel Balle, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Ja-son Gelman, Yang Xu, George Polovets, Ji Liu, Hong-long Cai, Warren Chen, Xianghai Sheng, Emily Xue, Sherjil Ozair, Adams Yu, Christof Angermueller, Xi-aowei Li, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gu-rumurthy, Mark Goldenson, Parashar Shah, M K Blake, Hongkun Yu, Anthony Urbanowicz, Jenni-maria Palomaki, Chrisantha Fernando, Kevin Brooks, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebas-tian Ruder, Morgan Redshaw, Jinhyuk Lee, Komal Jalan, Dinghua Li, Ginger Perng, Blake Hechtman, Parker Schuh, Milad Nasr, Mia Chen, Kieran Milan, Vladimir Mikulik, Trevor Strohman, Juliana Franco, Tim Green, Demis Hassabis, Koray Kavukcuoglu, Jeffrey Dean, and Oriol Vinyals. 2023. Gemini: A family of highly capable multimodal models. arXiv [cs.CL].\\n\\nSidney Greenbaum. 1991. Ice: The international corpus of english. English Today, 7(4):3\u20137.\\n\\nDirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bha-gia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Du-mas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muen-nighoff, Aakanksha Naik, Crystal Nam, Matthew E Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Sol-daini, Noah A Smith, and Hannaneh Hajishirzi. 2024. OLMo: Accelerating the science of language models. arXiv [cs.CL].\\n\\nRoger Baker Grosse, Juhan Bae, Cem Anil, Nelson Elhage, Alex Tamkin, Amirhossein Tajdini, Benoit Steiner, Dustin Li, Esin Durmus, Ethan Perez, Evan Hubinger, Kamil.e Lukovsiut.e, Karina Nguyen, Nicholas Joseph, Sam McCandlish, Jared Kaplan, and Sam Bowman. 2023. Studying large language model generalization with influence functions. Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah A. Smith. 2022. Whose language counts as high quality? measuring language ideologies in text data selection. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2562\u20132580, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nZayd Hammoudeh and Daniel Lowd. 2022. Training data influence analysis and estimation: A survey. ArXiv, abs/2212.04612.\\n\\nThomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. 2022. ToxiGen: A large-scale machine-generated dataset for adversarial and implicit hate speech detection. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3309\u20133326, Dublin, Ireland. Association for Computational Linguistics.\\n\\nKenneth Heafield. 2011. KenLM: Faster and smaller language model queries. In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 187\u2013197, Edinburgh, Scotland. Association for Computational Linguistics.\\n\\nMatthew Henderson, Pawe\u0142 Budzianowski, I\u00f1igo Casanueva, Sam Coope, Daniela Gerz, Girish Kuma-r, Nikola Mrk\u0161i\u00b4c, Georgios Spithourakis, Pei-Hao Su, Ivan Vulic, and Tsung-Hsien Wen. 2019. A repository of conversational datasets. In Proceedings of the Workshop on NLP for Conversational AI. Data available at github.com/PolyAI-LDN/conversational-datasets.\\n\\nPeter Henderson, Xuechen Li, Dan Jurafsky, Tatsunori Hashimoto, Mark A. Lemley, and Percy Liang. 2023. Foundation models and fair use. ArXiv, abs/2303.15715.\\n\\nJordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Si-monyan, Erich Elsen, Jack W. Rae, Oriol Vinyals, and L. Sifre. 2022. Training compute-optimal large language models. ArXiv, abs/2203.15556.\\n\\nJimin Hong, TaeHee Kim, Hyesu Lim, and Jaegul Choo. 2021. A V ocaDo: Strategy for adapting vocabulary to downstream domain. In Proceedings of the 2021 Conference on Empirical Methods in Natural Lan-guage Processing, pages 4692\u20134700, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-840", "page_num": 33, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Frequencies over different document metadata as computed using the WIMBD tool from Elazar et al. (2023). In subfigure (c), \\\\textit{un} denotes documents whose language could not be identified; \\\\textit{long} indicates documents that are too long to be processed with the tool\u2019s language ID module.\\n\\nFigure 11: Contamination percentages of datasets from PromptSource (Bach et al., 2022).\\n\\nM Strategies for Subsets Mixing and Upsampling with Dolma\\nLike the pretraining corpora of nearly every large-scale language model, Dolma is a multi-source dataset. Training on Dolma thus requires a mixing strategy that determines how much data from each source to include, and potentially which sources to upsample. Like other multi-source corpora (e.g., ROOTS (Laurenccon et al., 2023), the Pile (Gao et al., 2020), RedPajama v1 (Together Computer, 2023a)), Dolma does not prescribe a single mixing strategy. We refer the reader to Rae et al. (2021) for an example of how one might programmatically search over mixing configurations to maximize performance. Here, we perform mixing experiments as an opportunity to answer some research questions about how different data sources interact. We use the same ablation setup described in \u00a74.\\n\\nHow much code is important for pretraining? It is common practice for language models to be pretrained on some amount of code, even if code generation is not the intended task. Some research has suggested that mixing code into training over plain text documents improves performance on reasoning tasks (Madaan et al., 2022). We investigate whether this observation holds for models trained on Dolma, and if so, how much code is needed? We create three mixtures from the C4 and Stack subsets containing 0%, 5% and 15% of code data. On each, we train a 1B model. We evaluate these models on three different reasoning tasks: bAbI (Weston et al., 2015), WebNLG (Gardent et al., 2017) and GSM8k (Cobbe et al., 2021). For the first two tasks, we follow the experimental setup of Muennighoff et al. (2023b) and evaluate each model in an ICL setup with a changing number of demonstrations (0-5) across 5 random seeds. Muennighoff et al. (2023b) show that adding code to pre-training data improves ICL performance on bAbI and WebNLG and they suggest that code improves long-range state-tracking capabilities. Our experiments, as shown in Table 3, corroborate these findings: while the C4-only model fails on all bAbI tasks, adding code improves performance, with a similar trend for WebNLG. On the more difficult GSM8k benchmark, all models failed to get any correct answer in an ICL setup, and even when fine-tuning the models on the entire training set. However, we find that by fine-tuning on program-aided output, where questions are solved by writing Python snippets as described in (Gao et al., 2022), code models outperform the C4-only model. These results show that models pre-trained on code can leverage code generation to answer challenging reasoning tasks even when the original task does not directly involve code.\\n\\nEvaluating mixing strategies for pretraining on Dolma\\nWhile Dolma does not prescribe a specific source mixture, we analyze some commonly used strategies...\"}"}
{"id": "acl-2024-long-840", "page_num": 34, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performance of three models pre-trained with increasing amounts of code on three datasets, across 5 random seeds. We measure exact match for bAbI and GSM8K, and Rouge-2 for WebNLG.\\n\\nWe show results of mixtures in Figure 12. Overall, we observe that the different mixtures have an effect on the ability of resulting models to capture specific subdomains. All mixtures show similar perplexity scores on pages sampled from 100 domains from C4 (Figure 12, left), indicating their general effectiveness at modeling web documents. On the other hand, we note how models struggle to model specialized domains unless they are exposed to them. As an example, a model trained on the Web-only mix struggles to represent data in the code domain (Figure 12, center, HumanEval). Finally, we use results on the S2ORC subset of M2D2, which consists of academic papers, to illustrate how different data mixtures affect perplexity. As is the case with code, Web-only model exhibits higher perplexity due to domain mismatch. On the other hand, models trained on Reference+ and Gopher-like mixes achieve lower perplexity than the model trained on the Na\u00efve mix, due to more in-domain content. However, we note that, despite significant differences in the amount of academic papers between Reference+ and Gopher-like (4.9% vs 24.2%), they achieve nearly identical results, suggesting that even a relatively small percentage of in-domain data is sufficient to achieve good domain fit.\\n\\nFollowing the template by Gebru et al. (2021), we provide a Datasheet for Dolma.\\n\\nN.1 Motivation for Dataset Creation\\n\\nWhy was the dataset created?\\n\\nDolma was created with the primary purpose of training OLMo autoregressive language model. It is a mixture of documents from multiple data sources. Documents have been transformed using a combination of rule-based and statistical tools to extract textual content, remove layout information, and filter for English content.\\n\\nDolma contains data sourced from different domains. In particular, it contains a mixture of text obtained from a web scrape, scientific content extracted from academic PDFs and its associated metadata, code over a variety of programming languages, reference material from Wikipedia and Wikibooks, as well as public domain books from Project Gutenberg.\\n\\nWhat (other) tasks could the dataset be used for?\\n\\nWe expect this dataset to be useful to train other language models, either in its current form or through further filtering and combining it with other datasets. Beside language model training, this dataset could be used to study interaction between pretraining corpora and models trained on them. For example, one could study provenance of generations from the model, or perform further corpus analysis.\\n\\nSpecific subset of Dolma could be used to train domain-specific models. For example, the code subset could be used to train an AI programming assistant. Are there obvious tasks for which it should not be used?\\n\\nDue to the myriad transformations applied to the original source materials to derive our dataset, we believe it is ill-suited as a replacement for users seeking to directly consume the original content. We refer users of our dataset to our license and terms on the Hugging Face Hub huggingface.co/datasets/allenai/dolma which detail any use restrictions.\\n\\nHas the dataset been used for any tasks already?\\n\\nThe OLMo (Groeneveld et al., 2024) model family is trained on this dataset.\\n\\nIf so, where are the results so others can compare?\\n\\nExperimental results are detailed in this paper and in the OLMo (Groeneveld et al., 2024) manuscript.\\n\\nWho funded the creation of the dataset?\\n\\nAll individuals who are responsible for this dataset are employed by the Allen Institute for AI. Similarly, computing resources are provided by AI2.\\n\\nIf there is an associated grant, provide the grant number.\\n\\nCompute for the OLMo project is provided by AMD and CSC, using GPUs on the LUMI supercomputer.\"}"}
{"id": "acl-2024-long-840", "page_num": 35, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mix Name Description Sampling Proportion\\n\\nNa\u00efve Sample each source in Table 1 equally.\\n/gl\u2322be Web 100%\\n/code Code 100%\\n\u1f516 /graduati\u2322 \u2359 Ref. 100%\\n\u3125E Books 100%\\n\\n/gl\u2322be Web 83.5%\\n/code Code 13.8%\\n\u1f516 /graduati\u2322 \u2359 Ref. 2.5%\\n\u3125E Books 0.2%\\n\\nWeb Only Similar to Ayoola et al. (2022), we test a mixture that only uses web data.\\n/gl\u2322be Web 100%\\n/code Code 0%\\n\u1f516 /graduati\u2322 \u2359 Ref. 0%\\n\u3125E Books 0%\\n\\n/gl\u2322be Web 100%\\n/code Code 0%\\n\u1f516 /graduati\u2322 \u2359 Ref. 0%\\n\u3125E Books 0%\\n\\nReference+ It is common practice to upsample knowledge-intensive documents when composing training mixtures. In our case, we upsample the PeS2o papers, Wikipedia, Wikibooks, and Gutenberg books subsets by 2x.\\n/gl\u2322be Web 100%\\n/code Code 100%\\n\u1f516 /graduati\u2322 \u2359 Ref. 200%\\n\u3125E Books 200%\\n\\n/gl\u2322be Web 81.2%\\n/code Code 13.5%\\n\u1f516 /graduati\u2322 \u2359 Ref. 4.9%\\n\u3125E Books 0.4%\\n\\nGopher-like Following Rae et al. (2021), we create a mix that is heavily biased towards reference material. As we do not have access to the same sources, an exact replication of their mix is not possible.\\n/gl\u2322be Web 17%\\n/code Code 8%\\n\u1f516 /graduati\u2322 \u2359 Ref. 200%\\n\u3125E Books 200%\\n\\n/gl\u2322be Web 68.4%\\n/code Code 5.4%\\n\u1f516 /graduati\u2322 \u2359 Ref. 24.2%\\n\u3125E Books 2.0%\\n\\nTable 4: Overview of the mixtures and their composition.\\n\\nHumanEval\\n\\nC4 (100 Domains)\\n\\nTotal Tokens\\n\\nPerplexity\\n\\nFigure 12: 1B model ablations for different proportions of Dolma data. All mixtures perform similarly on web data (left), while excluding code increases perplexity on code datasets (center). Finally, increasing reference material by up-sampling papers and Wikipedia yields lower perplexity on S2ORC (right). Overall, source distribution is linked to downstream capabilities; thus, Dolma users should sample subsets according to their needs.\\n\\nMetadata for subsets of Dolma could be used to reconstruct relationships between items:\\n\\n\u2022 Common Crawl. Each document uses the URL of the web page from which it was extracted as its identifier; therefore, it can be used to identify relationships between documents.\\n\\n\u2022 C4. The URL of each web page from which documents were extracted is included as metadata; therefore, it can be used to identify relationships between documents.\\n\\n\u2022 Reddit. The originating subreddits and thread ids of documents are included in the metadata.\\n\\n\u2022 Semantic Scholar. The id of each document is the Semantic Scholar Corpus ID of its corresponding manuscript. Metadata for each manuscript can be obtained using the Semantic Scholar APIs (Kinney et al., 2023).\\n\\n\u2022 GitHub. The name of the GitHub repository each document belongs to is included as metadata.\\n\\n\u2022 Project Gutenberg. The title of each book is included as the first line of each document.\\n\\n\u2022 Wikipedia, Wikibooks. For both, metadata includes the URL corresponding to the page content was extracted from. Structure and connections between documents can be recovered through the URL.\\n\\nHow many instances of each type are there? Summary statistics are reported in Table 1.\\n\\nWhat data does each instance consist of? \\\"Raw\\\" data (e.g., unprocessed text or images)? Features/attributes?\\n\\nFor each source, raw data is not available directly but could be recovered using source-specific methods:\\n\\n\u2022 Common Crawl. We obtain data from common crawl snapshots from 2020-05 to 2023-06. WARC files from Common Crawl can be intersected with Dolma ids to recover original HTML files.\\n\\n\u2022 C4. We obtained this corpus from the Hugging Face\"}"}
{"id": "acl-2024-long-840", "page_num": 36, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hub\\n\\n22. In turn, documents in C4 have been derived from a Common Crawl snapshot for 04/2019. URLs in C4 can be used to recover HTML files.\\n\\n- Reddit. The complete set of monthly data dumps used in this work are no longer distributed by Pushshift, however they can still be obtained through torrents and some public web archives.\\n- Semantic Scholar. peS2o is derived from S2ORC (Lo et al., 2020). Original parsed documents can be obtained from extracting documents in S2ORC that share the same ID with peS2o. Further, metadata in S2ORC can be used to obtain original PDF.\\n- GitHub. The filename and repository name, both available in metadata, can be used to recover original file contents.\\n- Project Gutenberg. The title of each book is the first line of each document.\\n- Wikipedia, Wikibooks. For both, metadata includes the URL corresponding to the page content was extracted from. Structure and connections between documents can be recovered through the URL.\\n\\nIs there a label/target associated with instances? If the instances are related to people, are subpopulations identified (e.g., by age, gender, etc.) and what is their distribution?\\n\\nThere are no labels associated with instances. Many text instances were likely created by people or groups of people, but in the vast majority of cases authorship information is unavailable let alone subpopulation metadata. We leave aggregation and reporting of these statistics to future work.\\n\\nIs everything included or does the data rely on external resources? (e.g., websites, tweets, datasets) If external resources, a) are there guarantees that they will exist, and remain constant, over time; b) is there an official archival version. Are there licenses, fees or rights associated with any of the data?\\n\\nThe data are derived from the web and the original resources may not persist over time. However, each source represents an archival snapshot of that data that should remain fixed and available:\\n\\n- Common Crawl. The Common Crawl data is available on Amazon S3 as part of the Amazon Web Services' Open Data Sponsorship program and can be freely downloaded. We followed Common Crawl terms of use.\\n- C4. This corpus can be obtained from from the Hugging Face Hub and is released under ODC-By 1.0 (Open Data Commons, 2010).\\n- Reddit. Pushshift no longer distributes this dataset due to changes to the Reddit API's terms. Unofficial copies of the data might be available through torrents and some public web archives. Pushshift data dumps inherit the Terms of use of the Reddit API at the time of their collection (March 2023).\\n- Semantic Scholar. peS2o is derived from S2ORC (Lo et al., 2020). S2ORC is released through the Semantic Scholar Public API under ODC-By 1.0 (Open Data Commons, 2010).\\n- GitHub. The corpus is available on the Hugging Face Hub and consists of code released under a variety of permissive licenses. More details including terms of use for hosting or sharing the corpus are provided in the datacard at the link above.\\n- Project Gutenberg. Project Gutenberg consists of books that are not protected under U.S. copyright law. The corpus is available at gutenberg.org.\\n- Wikipedia, Wikibooks. Wikimedia data dumps are freely available and released under CC BY-SA 4.0 license (Creative Commons, 2013).\\n\\nAre there recommended data splits or evaluation measures? (e.g., training, development, testing; accuracy/AUC)\\n\\nNo. See current manuscript Section \u00a74.2.\\n\\nWhat experiments were initially run on this dataset? Have a summary of those results and, if available, provide the link to a paper with more information here.\\n\\nSee current manuscript Section \u00a74.2 for description of data ablation methodology, and remainder of paper for full set of experiments. Every experimental result is available through links provided in the manuscript.\\n\\nN.3 Data Collection Process\\n\\nHow was the data collected? (e.g., hardware apparatus/sensor, manual human curation, software program, software interface/API; how were these constructs/measures/methods validated?)\\n\\nData acquisition for each subset was performed as follows:\\n\\n- Common Crawl. snapshots were downloaded from Common Crawl\u2019s official S3 bucket using the cc_net pipeline (Wenzek et al., 2020). Data was obtained between March 17th and March 27th, 2023.\"}"}
{"id": "acl-2024-long-840", "page_num": 37, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We clone C4 from the Hugging Face Hub using Git with the Git-LFS extension. Repository cloned on May 24th, 2023.\\n\\nReddit. Reddit was acquired in the form of monthly data dumps of comments and submissions collected and distributed by the Pushshift project. We used the complete set of 422 publicly available dumps (208 comments, 214 submissions) spanning a period from 06/2005\u201303/2023. The majority of Dumps were acquired in March, 2023 with the last dumps downloaded in May of 2023.\\n\\nSemantic Scholar. We clone pes2o from the Hugging Face Hub using Git with the Git-LFS extension. We use pes2o V2. Repository cloned on June 30th, 2023.\\n\\nGitHub. We clone The Stack (deduplicated) from the Hugging Face Hub using Git with the Git-LFS extension. Repository cloned on May 28th, 2023.\\n\\nProject Gutenberg. Data was downloaded directly from gutenberg.org. We used GutenbergPy (Angelescu, Radu, 2013) to extract books. Website accessed on April 3rd, 2023.\\n\\nWikipedia, Wikibooks. Dumps were downloaded from Wikimedia's website. We use the dump from March 20th, 2023.\\n\\nWho was involved in the data collection process? (e.g., students, crowdworkers) How were they compensated? (e.g., how much were crowdworkers paid?)\\n\\nData was collected and postprocessed by full-time employees at the Allen Institute for AI. No instances in this dataset are manually annotated.\\n\\nOver what time-frame was the data collected? Does the collection time-frame match the creation time-frame?\\n\\nPlease see list above.\\n\\nHow was the data associated with each instance acquired? Was the data directly observable (e.g., raw text, movie ratings), reported by subjects (e.g., survey responses), or indirectly inferred/derived from other data (e.g., part of speech tags; model-based guesses for age or language)? If the latter two, were they validated/verified and if so how?\\n\\nAny metadata associated with each instance was obtained directly from each source.\\n\\nDoes the dataset contain all possible instances? Or is it, for instance, a sample (not necessarily random) from a larger set of instances? If the dataset is a sample, then what is the population? What was the sampling strategy (e.g., deterministic, probabilistic with specific sampling probabilities)? Is the sample representative of the larger set (e.g., geographic coverage)? If not, why not (e.g., to cover a more diverse range of instances)? How does this affect possible uses?\\n\\nSampling for each subset was performed as follows:\\n\\n- **Common Crawl.** Common Crawl is not a representative sample of the web. Summary statistics about Common Crawl are reported through the cc-crawl-statistics (Common Crawl, 2016) project, available at commoncrawl.github.io/cc-crawl-statistics. Dolma uses Common Crawl snapshots from 2020-05 to 2023-06.\\n\\n- **C4.** We use C4 in its entirety.\\n\\n- **Reddit.** We use all available Reddit content from 06/2005\u201303/2023.\\n\\n- **GitHub.** We use The Stack (deduplicated) in its entirety.\\n\\n- **Semantic Scholar.** We use pes2o V2 in its entirety.\\n\\n- **Project Gutenberg.** We process all Gutenberg books.\\n\\n- **Wikipedia, Wikibooks.** We use the English and Simple subset of Wikipedia and Wikibooks in their entirety.\\n\\nIs there information missing from the dataset and why? (this does not include intentionally dropped instances; it might include, e.g., redacted text, withheld documents) Is this data missing because it was unavailable?\\n\\nCommon Crawl is the only source we did not use in its entirety. We use only about a quarter of all snapshots available. This amount was deemed sufficient for the goal of the Dolma project. We decided to use the 24 most recent Common Crawl snapshots at the time.\\n\\nAre there any known errors, sources of noise, or redundancies in the data?\\n\\nNot that we are aware of, although a negligible portion of Common Crawl data could have been lost due to network issues with S3 storage. When accessing Common Crawl, we implemented retry mechanisms, but copy could have failed due to exceeding the retry limits.\\n\\nN.4 Data Preprocessing\\n\\nWhat preprocessing/cleaning was done? (e.g., discretization or bucketing, tokenization, part-of-speech tagging, SIFT feature extraction, removal of instances, processing of missing values, etc.)\\n\\nCommon Crawl snapshots follow naming convention xxxx-yy, where xxxx is the year the snapshot was finalized, and yy is the week, ranging from 01 to 52.\"}"}
{"id": "acl-2024-long-840", "page_num": 38, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All data sources are filtered using FastText language identification models (Joulin et al., 2016a,b) with an English threshold of 0.5.\\n\\nFor the Common Crawl and C4 subsets, we use the following filters that substantially modify the original data. Note that data might be tagged for removal by one or more filter.\\n\\n- Only Common Crawl, as part of their distribution pipeline: Linearize all HTML into plain text files (WET files generation);\\n- Only Common Crawl, as part of CCNet pipeline: We remove frequently occurring paragraph in Common Crawl by identifying repeated paragraphs on small subsets of each snapshots. This step gets rid of headers that are shared across many pages, such as navigational headers. Removal is operationalized as follows: given \\\\(1, \\\\ldots, n, \\\\ldots, N\\\\) shards each snapshot is comprised to, group shards in sets \\\\(S = \\\\{n-k, n\\\\}\\\\); then, remove exact duplicates of paragraphs in \\\\(S\\\\). Paragraphs are defined as newline-separated slices of documents, and compared using their SHA1. We choose \\\\(k\\\\) such that each set is at most 20GB. (approximately 70% of paragraph removed);\\n- Only Common Crawl, deduplication by URL: We deduplicate pages by URL (53% of duplicates removed);\\n- Language identification: remove all documents with an English score lower than 0.5, as determined by FastText language identification models (Joulin et al., 2016a,b) (removed 61.69% of web pages by size);\\n- Quality filter: Remove documents with more than half of their line not ending in \\\",\\\", \\\",?\\\", \\\",!\\\", or \\\",\\\". (22.73% of characters tagged for removal);\\n- Quality filter: Remove any document that does not pass any of the Gopher rules (Rae et al., 2021) (15.23% of characters tagged for removal).\\n\\nThese are a slight modification of the original CCNet pipeline, where \\\\(k\\\\) is chose so that each set is 2% of snapshot. We chose to use a fixed shard size, rather an a percentage of the corpus, because fixed size is more predictable in terms of resource usage, leading to less-error prone code. Conceptually it's equivalent to putting a threshold on the absolute probability of a paragraph occurring. The term \\\"quality filter\\\", while widely used in literature, does not appropriately describe the outcome of filtering a dataset. Quality might be perceived as a comment on the informativeness, comprehensiveness, or other characteristics valued by humans. However, the filters used in Dolma and other language models efforts select text according to criteria that are inherently ideological (Gururangan et al., 2022).\\n\\nWe use allenai/gpt-neox-olmo-dolma-v1\\_5 to obtain tokens.\"}"}
{"id": "acl-2024-long-840", "page_num": 39, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Quality filter: Remove comments and submissions shorter than 500 characters in length.\\n\u2022 Quality filter: Remove user comments with fewer than three upvotes (Reddit users vote on the quality of submissions and comments).\\n\u2022 Content filter: Remove comments and submissions from banned, toxic, or NSFW subreddits.\\n\u2022 Content filter: Remove sentences that get ranked as toxic or as hatespeech by a FastText classifier (score above 0.4).\\n\u2022 Content filter: Mask Personal Identifiable Information (PII) using regular expressions that identify emails, phone numbers, and IP addresses.\\n\u2022 Deduplication: We deduplicate comments and submissions (jointly) at a paragraph level using a Bloom filter.\\n\\nFor the code subset derived from The Stack (deduplicated), we use the following filters:\\n\u2022 Language filter: Removed files associated with the following programming languages:\\n  \u2013 Data or numerical content: csv, json, json5, jsonld, jsoniq, svg\\n  \u2013 Assembly code: assembly\\n\u2022 Quality filter: Removed copyright statements in code files from document preamble.\\n\u2022 Quality filter: Removed documents matching any of the RedPajama v1 (Together Computer, 2023a) code filters (41.49% of data tagged for removal):\\n  \u2013 Maximum line length > 1000 characters.\\n  \u2013 Average line length > 100 characters.\\n  \u2013 Proportion of alpha-numeric characters < 0.25.\\n  \u2013 Ratio of alphabetical characters to number of tokens < 1.5.\\n\u2022 Quality filter: Removed documents matching any of the following Starcoder filters (Li et al., 2023b):\\n  \u2013 Contains XML template code.\\n  \u2013 HTML code-to-text ratio <= 0.2.\\n  \u2013 Java, Javascript, Python code-to-comment ratio <= 0.01 or > 0.8.\\n\u2022 Content filter: Mask Personal Identifiable Information (PII) using regular expressions that identify emails, phone numbers, and IP addresses; pages containing 6 or more PIIs are completely removed from the corpus.\\n\\nThe Common Crawl, C4, Reddit, and Code subsets used the same regular expressions for identifying PII:\\n\\n39 Code license and provenance is still tracked in metadata.\\n\\n40 Tokens counted using whitespace tokenizer.\\n\\nEmail addresses:\\n\\n\\\\[ . \\\\s@,?!;:)(\\\\]*\\\\(\\\\[\\\\]\\\\s@,?!;:)(\\\\]?)\\\\[ . \\\\s@,?!;:)(\\\\]?\\n\\nIP addresses:\\n\\n\\\\s+\\\\(?\\\\d{3}\\\\)?\\\\[-. ]\\\\*(-\\\\d{3}\\\\][-. ]\\\\(?\\\\d{4}\\\\)\\n\\nPhone numbers:\\n\\n(?:25[0-5]|2[0-4]\\\\[0-9|[01]\\\\[0-9]{1,2})\\\\.){3}(?:25[0-5]|2[0-4]\\\\[0-9|[01]\\\\[0-9]{1,2})\\n\\nFor the Wikipedia and Wikibooks subsets, we remove pages that contain fewer than 25 UTF-8 words.\\n\\nFor the Gutenberg subset:\\n\u2022 Language identification: for each paragraph (defined as newline-separated spans of text), we use FastText to perform language identification. Then, we compute the average language score by averaging the score for all passages. If a document has a language score lower than 0.5, it is discarded;\\n\u2022 Quality filter: we remove pages that contain fewer than 25 UTF-8 words;\\n\u2022 Quality filter: Remove any document that contains a token or sequence of tokens repeating over 100 times.\\n\\nFor the Semantic Scholar subset, we remove any document that contains a token or sequence of tokens repeating over 100 times.\\n\\nFor Dolma versions 1.0 and 1.5, we perform decontamination for all subsets of Dolma. In particular, we remove paragraphs that are shared with documents in the Paloma evaluation suite (Magnusson et al., 2023). Overall, only 0.003% of our dataset is removed due to contamination with this evaluation set. Dolma version 1.6 is not decontaminated.\\n\\nWas the \u201craw\u201d data saved in addition to the preprocessed/cleaned data? (e.g., to support unanticipated future uses)\\n\\nRaw data is available for all subsets except Common Crawl. Due to space constrains, we only keep linearized version of Common Crawl snapshots, filtered by Language ID as described above.\\n\\nRaw data is not available for download outside the Allen Institute for AI. Interested individuals may contact authors of this manuscript if they require access to raw data.\\n\\nIs the preprocessing software available?\\n\\nYes, all preprocessing software is available on GitHub at github.com/allenai/dolma and on PyPI.\\n\\nDoes this dataset collection/processing procedure achieve the motivation for creating the dataset stated in the first section of this datasheet?\\n\\nYes, it does.\"}"}
{"id": "acl-2024-long-840", "page_num": 40, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"N.5 Dataset Distribution\\n\\nHow is the dataset distributed? (e.g., website, API, etc.; does the data have a DOI; is it archived redundantly?)\\n\\nDolma is distributed via the Hugging Face Hub, which offers access via the datasets (Lhoest et al., 2021) Python package, direct download, and Git using the Git-LFS extension. Additionally, a copy is stored on the cloud storage of the Allen Institute for AI.\\n\\nWhen will the dataset be released/first distributed? (Is there a canonical paper/reference for this dataset?)\\n\\nThe dataset is available now. This manuscript serves as a reference for the dataset.\\n\\nWhat license (if any) is it distributed under? Are there any copyrights on the data?\\n\\nInformation about the license associated with Dolma are available on its release page on the Hugging Face Hub: huggingface.co/datasets/allenai/dolma.\\n\\nAre there any fees or access/export restrictions?\\n\\nThe dataset is distributed for free. Users should verify any restrictions on its release page on the Hugging Face Hub: huggingface.co/datasets/allenai/dolma.\\n\\nN.6 Dataset Maintenance\\n\\nWho is supporting/hosting/maintaining the dataset?\\n\\nThe Allen Institute for AI maintains the dataset. For support questions, users are invited to open an issue on GitHub or on the community tab of dataset page (the former being preferred over the latter). Any other inquiry should be sent to ai2-info@allenai.org.\\n\\nWill the dataset be updated? How often and by whom? How will updates/revisions be documented and communicated (e.g., mailing list, GitHub)? Is there an erratum?\\n\\nDataset will be uploaded on a need-to basis by main- tainers at the Allen Institute for AI. Newer version of the dataset will be labeled accordingly. The latest version of the dataset, as well as a changelog, will be made available starting from the first revision.\\n\\nIf the dataset becomes obsolete how will this be communicated? Is there a repository to link to any/all papers/systems that use this dataset?\\n\\nUsers should keep track of the version of the dataset in use. Information about latest version of Dolma are available on its release page on the Hugging Face Hub: huggingface.co/datasets/allenai/dolma. Dolma users should cite this manuscript when using this data.\\n\\nIf others want to extend/augment/build on this dataset, is there a mechanism for them to do so? If so, is there a process for tracking/assessing the quality of those contributions. What is the process for communicating/distributing these contributions to users?\\n\\nCreation and distribution of derivatives is described above. In case contributors want to flow their improvement back to future Dolma releases, they should contact corresponding authors of this manuscript.\\n\\nN.7 Legal & Ethical Considerations\\n\\nIf the dataset relates to people (e.g., their attributes) or was generated by people, were they informed about the data collection? (e.g., datasets that collect writing, photos, interactions, transactions, etc.)\\n\\nSubsets of Dolma derived from web data are likely created by people or groups of people, however author- ship information is often unavailable. Authors were not directly informed about the data collection. For encyclopedic and web content, logs of web servers will contain records of spiders ran by Common Crawl. For academic content, the pes2o subset (Sol- daini and Lo, 2023) is derived from manuscripts that are licensed for permissive distribution by their authors. Reddit content was acquired through a public API adherent to terms of service; individual authors of Reddit posts were not contacted directly. Finally, the Allen Institute for AI did not contact Project Gutenberg.\\n\\nIf it relates to other ethically protected subjects, have appropriate obligations been met? (e.g., medical data might include information collected from ani- mals)\\n\\nDue to the nature of and size of Dolma, it is impossi- ble to determine which obligations, if any, are appropri- ate.\\n\\nIf it relates to people, were there any ethical review applications/reviews/approvals? (e.g. Institutional Review Board applications) If it relates to people, were they told what the dataset would be used for and did they consent? What community norms exist for data collected from human communications? If consent was obtained, how? Were the people provided with any mechanism to revoke their consent in the future or for certain uses?\\n\\nThe Dolma project includes Ethics committee com- prised of internal and external members to the Allen Institute for AI. Plans for the creation of Dolma were reviewed with the committee, and we incorporated their recommendations. Following practices established in similar efforts, no consent was collected from individuals who might be represented in the dataset. We make available a form for individuals who wish to be removed from the dataset. forms.gle/q4BNUUxUxKwKkfdT6\"}"}
{"id": "acl-2024-long-840", "page_num": 41, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"If it relates to people, could this dataset expose people to harm or legal action? (e.g., financial, social, or otherwise) What was done to mitigate or reduce the potential for harm?\\n\\nDolma contains text instances that have been derived from web pages Common Crawl crawled from the web. Content might contain sensitive information including personal information, or financial information users of the web chose to put publicly online. This data is taken only from public places, so the same data is or has been accessible via browsing the web. We have measured a variety of types of personal information, and built tools specifically to remove some types of sensitive information, and through our license we restrict what users can do with this data.\\n\\nWe recommend individuals to submit a request through our form if they wish their information to be removed.\\n\\nIf it relates to people, does it unfairly advantage or disadvantage a particular social group? In what ways? How was this mitigated?\\n\\nDolma is not a representative sample of none of its sources. It might underrepresent or overrepresent some communities on the internet; further, papers in the Pile subset are skewed towards STEM disciplines; books in the Gutenberg library are mostly from the public domain (at the time of publication, books published before 1927); finally, the English and Simple subset of Wikipedia and Wikibooks might be biased towards events and people from the global north.\\n\\nWe did not attempt to alter distribution of social groups in Dolma. Large-scale interventions to correct societal biases in large datasets remain challenging, and are left to future work.\\n\\nIf it relates to people, were they provided with privacy guarantees? If so, what guarantees and how are these ensured?\\n\\nThis dataset contains text that was derived from web pages scraped by Common Crawl from the web. For much of that data it's not possible identify the authors. In many instances, creators purposely choose to post anonymously online, so aiming to infer authorship can be ethically fraught. We provide access to our data, and encourage any creators that would likely to have data from or about them removed to reach out.\\n\\nDoes the dataset comply with the EU General Data Protection Regulation (GDPR)? Does it comply with any other standards, such as the US Equal Employment Opportunity Act?\\n\\nWe created this dataset in aggregate, not separately identifying any individual's content or information. We took reasonable steps to remove types of personal information that were possible to reliably detect. We restrict who has access to the data, and we release this under a license that prohibits uses that might be deemed discriminatory. We also provide an avenue for any person to contact us to have text from or about them removed from our corpus.\\n\\nDoes the dataset contain information that might be considered sensitive or confidential? (e.g., personally identifying information) Does the dataset contain information that might be considered inappropriate or offensive?\\n\\nThis dataset contains text that was derived from web pages scraped by Common Crawl from the web. Therefore, it can contain text posted on public websites by creators on the internet. If an author publicly posted personal information or offensive content, it could be included in this dataset. We took reasonable steps to remove types of personal information that were possible to reliably detect. We also removed documents that contained sentences that were classified as being toxic.\\n\\n---\\n\\n| Total Tokens | Perplexity |\\n|-------------|------------|\\n| 20B         | 20         |\\n| 40B         | 30         |\\n| 60B         | 10         |\\n| 80B         | 20         |\\n| 100B        | 120B       |\\n| 140B        | 8          |\\n\\nFigure 13: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), WikiText 103 (Merity et al., 2016), and Pile (Gao et al., 2020) (Val).\"}"}
{"id": "acl-2024-long-840", "page_num": 42, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 14: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 100 dom (Chronopoulou et al., 2022), Penn Tree Bank (Marcus et al., 1994), and Gab (Zannettou et al., 2018)\\n\\nFigure 15: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991), M2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016)\\n\\nFigure 16: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al., 2021)\\n\\nFigure 17: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020) (English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021)\"}"}
{"id": "acl-2024-long-840", "page_num": 43, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 18: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and Winogrande (Sakaguchi et al., 2019)\\n\\nFigure 19: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\\n\\nFigure 20: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 44, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 21: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\\n\\nFigure 22: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\\n\\nFigure 23: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\\n\\nFigure 24: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\"}"}
{"id": "acl-2024-long-840", "page_num": 45, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 25: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and Winogrande (Sakaguchi et al., 2019).\\n\\nFigure 26: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019).\\n\\nFigure 27: Training Cross Entropy.\"}"}
{"id": "acl-2024-long-840", "page_num": 46, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Subset       | Total Tokens | Perplexity |\\n|--------------|--------------|------------|\\n| 4chan        |              |            |\\n| C4 (100 Dom) |              |            |\\n| GAB          |              |            |\\n| ICE          |              |            |\\n| M2D2 (Wiki)  |              |            |\\n| TwitterAEE   |              |            |\\n| Manosphere   |              |            |\\n| mC4 (English)|              |            |\\n| M2D2 (S2ORC) |              |            |\\n| C4           |              |            |\\n\\nFigure 28: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), C4 (100 Domains), Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), M2D2 (Reid et al., 2022) (Wiki), and Twitter AEE (Blodgett et al., 2016).\\n\\nFigure 29: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991), M2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016).\\n\\nFigure 30: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al., 2021).\\n\\nFigure 31: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020) (English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021).\"}"}
{"id": "acl-2024-long-840", "page_num": 47, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Downstream Tasks\\n\\n**OpenBookQA** (Mihaylov et al., 2018)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | OpenBookQA |\\n|-------------------------------|----------------|----------|------------|\\n| 0.25                          | 0.3            | 0.35     | 0.3        |\\n\\n**ARC-E (Easy)** (Clark et al., 2018)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | ARC-E (Easy) |\\n|-------------------------------|----------------|----------|--------------|\\n| 0.4                           | 0.45           | 0.5      | 0.5          |\\n\\n**Winogrande** (Sakaguchi et al., 2019)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Winogrande |\\n|-------------------------------|----------------|----------|------------|\\n| 0.5                           | 0.52           | 0.54     | 0.54       |\\n\\n**SciQ** (Welbl et al., 2017)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | SciQ |\\n|-------------------------------|----------------|----------|------|\\n| 0.6                           | 0.7            | 0.8      | 0.8  |\\n\\n**HellaSwag** (Zellers et al., 2019)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | HellaSwag |\\n|-------------------------------|----------------|----------|-----------|\\n| 0.3                           | 0.35           | 0.4      | 0.45      |\\n\\n**PIQA** (Bisk et al., 2019)\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | PIQA |\\n|-------------------------------|----------------|----------|------|\\n| 0.6                           | 0.65           | 0.7      | 0.7  |\\n\\n### Training Cross Entropy\\n\\n**Train**\\n\\n| PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Train |\\n|-------------------------------|----------------|----------|-------|\\n| 3                             | 4              | 5        | 6     |\\n\\n**PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Train |\\n|-------------------------------|----------------|----------|-------|\\n| 6                             | 7              | 8        | 9     |\\n\\n**PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Train |\\n|-------------------------------|----------------|----------|-------|\\n| 3                             | 4              | 5        | 6     |\\n\\n**PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Train |\\n|-------------------------------|----------------|----------|-------|\\n| 3                             | 4              | 5        | 6     |\\n\\n**PII Remove (>=5) + Mask (<5) | PII Remove All | Baseline | Train |\\n|-------------------------------|----------------|----------|-------|\\n| 3                             | 4              | 5        | 6     |\\n\\nFigure 34: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 48, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 35: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\\n\\nFigure 36: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\\n\\nFigure 37: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\\n\\nFigure 38: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\"}"}
{"id": "acl-2024-long-840", "page_num": 49, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 39: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2019)\\n\\nFigure 40: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\\n\\nFigure 41: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 50, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 42: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022).\\n\\nFigure 43: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021).\\n\\nFigure 44: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki).\\n\\nFigure 45: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC).\"}"}
{"id": "acl-2024-long-840", "page_num": 51, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Quality Filters | OpenBookQA | ARC-Easy | Winogrande | SciQ | HellaSwag | PIQA |\\n|----------------|-----------|----------|------------|------|-----------|------|\\n| Total Tokens   | Accuracy  | Total Tokens   | Accuracy | Total Tokens   | Accuracy | Total Tokens   | Accuracy | Total Tokens   | Accuracy | Total Tokens   | Accuracy |\\n| 0.25           | 50B       | 100B      | 0.4       | 0.45  | 0.5       | 0.52  | 0.54       | 0.56   | 0.6       | 0.7     | 0.8      |\\n| Baseline       |           |           |           |       |           |       |           |        |           |         |         |\\n| Quality Filters|           |           |           |       |           |       |           |        |           |         |         |\\n| Quality Filters + Dedup |   |           |           |       |           |       |           |        |           |         |         |\\n| Quality Filters + Dedup + Content Filters |   |           |           |       |           |       |           |        |           |         |         |\\n\\nFigure 46: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2019)\\n\\n| Train | Total Tokens | Cross Entropy |\\n|-------|--------------|---------------|\\n| Total Tokens | 0.25 | 0.3 | 0.35 | Baseline | Quality Filters | Quality Filters + Dedup | Quality Filters + Dedup + Content Filters |\\n| Train | Total Tokens | Cross Entropy |\\n|-------|--------------|---------------|\\n| 15775 |              |               |\\n\\nFigure 48: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 52, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset          | 4chan | Pile | C4 (100 Domains) | Manosphere | GAB | ICE | M2D2 (Wiki) | TwitterAEE | mC4 (English) | M2D2 (S2ORC) |\\n|------------------|-------|------|------------------|------------|-----|-----|------------|------------|--------------|--------------|\\n| Paloma (Magnusson et al., 2023) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) | ![Perplexity](#) |\\n\\n**Figure 49:** Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 Dom (Chronopoulou et al., 2022)\\n\\n**Figure 50:** Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\\n\\n**Figure 51:** Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\\n\\n**Figure 52:** Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\"}"}
{"id": "acl-2024-long-840", "page_num": 53, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task                | Filters  | Baseline | Accuracy |\\n|---------------------|----------|----------|----------|\\n| **Total Tokens**    |          |          |          |\\n| **OpenBookQA**      |          |          |          |\\n| **ARC-Easy**        |          |          |          |\\n| **Winogrande**      |          |          |          |\\n| **SciQ**            |          |          |          |\\n| **HellaSwag**       |          |          |          |\\n| **PIQA**            |          |          |          |\\n\\n**Figure 53**: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and Winogrande (Sakaguchi et al., 2019).\"}"}
{"id": "acl-2024-long-840", "page_num": 54, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Total Tokens | Perplexity |\\n|--------------|------------|\\n| 0            | 10B        |\\n| 20           | 30         |\\n| Dolma (RPJ rules) | Dolma (RPJ rules & StarCoder rules) |\\n| Pile         | Total Tokens |\\n| 4chan        | Perplexity  |\\n| C4 (100 Domains) | Dolma (RPJ rules) |\\n| Manosphere   | Dolma (RPJ rules) |\\n| Gab          | Dolma (RPJ rules) |\\n| ICE          | Dolma (RPJ rules) |\\n| M2D2 (Wiki)  | Dolma (RPJ rules) |\\n| TwitterAEE   | Dolma (RPJ rules) |\\n| mC4 (English) | Dolma (RPJ rules) |\\n| M2D2 (S2ORC) | Dolma (RPJ rules) |\\n\\nFigure 56: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\\n\\nFigure 57: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\\n\\nFigure 58: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\\n\\nFigure 59: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\"}"}
{"id": "acl-2024-long-840", "page_num": 55, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 60: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-Easy (Clark et al., 2018), and Winogrande (Sakaguchi et al., 2019)\\n\\nFigure 61: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\\n\\nFigure 62: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 56, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Subset          | Total Tokens | Perplexity |\\n|----------------|--------------|------------|\\n| 4chan          | 100          | 0          |\\n| Pile           | 100          | 9          |\\n| C4 (100 Domains)| 100          | 2          |\\n| Manosphere     | 100          | 3          |\\n| Gab            | 100          | 4          |\\n| ICE            | 100          | 5          |\\n| M2D2 (Wiki)    | 100          | 6          |\\n| TwitterAEE     | 100          | 7          |\\n| mC4 (English)  | 100          | 8          |\\n| M2D2 (S2ORC)   | 100          | 9          |\\n\\n**Figure 63**: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Pile (Gao et al., 2020) (Val), and C4 100 dom (Chronopoulou et al., 2022)\\n\\n**Figure 64**: Perplexity results on Paloma (Magnusson et al., 2023); subsets C4 (Raffel et al., 2020; Dodge et al., 2021) and Manosphere (Ribeiro et al., 2021)\\n\\n**Figure 65**: Perplexity results on Paloma (Magnusson et al., 2023); subsets Gab (Zannettou et al., 2018), ICE (Greenbaum, 1991), and M2D2 (Reid et al., 2022) (Wiki)\\n\\n**Figure 66**: Perplexity results on Paloma (Magnusson et al., 2023); subsets Twitter AAE (Blodgett et al., 2016), mC4 (Xue et al., 2020) (English), and M2D2 (Reid et al., 2022) (S2ORC)\"}"}
{"id": "acl-2024-long-840", "page_num": 57, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task               | Total Tokens | Accuracy  |\\n|--------------------|--------------|-----------|\\n| OpenBookQA         |              |           |\\n| ARC-Easy           |              |           |\\n| Winogrande         |              |           |\\n| SciQ               |              |           |\\n| HellaSwag          |              |           |\\n| PIQA               |              |           |\\n\\nFigure 67: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2019)\\n\\nFigure 68: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\"}"}
{"id": "acl-2024-long-840", "page_num": 58, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Pipeline | Total Tokens | Perplexity |\\n|----------|--------------|------------|\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content | Atomic Content |\\n| 0 20B 40B 60B | Partial Threads, Dedup | Partial Threads, Dedup |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Complete Threads | Complete Threads |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads |\\n| 0 20B 40B 60B | Atomic Content, Dedup, PII, Toxic | Atomic Content, Dedup, PII, Toxic |\\n| 0 20B 40B 60B | Partial Threads | Partial Threads"}
{"id": "acl-2024-long-840", "page_num": 59, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task          | Partial Threads, Dedup | Complete Threads |\\n|--------------|------------------------|------------------|\\n| OpenBookQA   | 0.24                   | 0.26             |\\n| ARC-Easy     | 0.35                   | 0.4              |\\n| Winogrande   | 0.5                    | 0.52             |\\n| SciQ         | 0.5                    | 0.6              |\\n| HellaSwag    | 0.3                    | 0.35             |\\n| PIQA         | 0.6                    | 0.65             |\\n\\nFigure 73: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2019)\\n\\n| Task          | Partial Threads, Dedup | Complete Threads |\\n|--------------|------------------------|------------------|\\n| Train        | 0.24                   | 0.26             |\\n| Cross Entropy|                        |                  |\\n\\nFigure 75: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 60, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Subset       | Total Tokens | Perplexity |\\n|--------------|--------------|------------|\\n| 4chan        | 20B          | 30         |\\n| Pile         | 40B          | 50         |\\n| C4 (100 Dom) | 60B          | 70         |\\n| Manosphere   | 80           |            |\\n| GAB          |              |            |\\n| ICE          |              |            |\\n| M2D2 (Wiki)  |              |            |\\n| TwitterAEE   |              |            |\\n| mC4 (English)|              |            |\\n| M2D2 (S2ORC) |              |            |\"}"}
{"id": "acl-2024-long-840", "page_num": 61, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset          | Zero-Shot | No Filtering | PII + NSFW + Hate Filter | NSFW + Hate Filter | OpenBookQA | Total Tokens | Accuracy |\\n|------------------|-----------|--------------|--------------------------|-------------------|------------|--------------|----------|\\n| 20B              | 0.26      | 0.28         | 0.3                      | 0.32              |            |              |          |\\n| 40B              |           |              |                          |                   |            |              |          |\\n| 60B              |           |              |                          |                   |            |              |          |\\n| ARC-Easy         | 0.35      | 0.4          | 0.45                     | 0.5               |            |              |          |\\n| 20B              | 0.5       | 0.52         | 0.54                     | 0.56              |            |              |          |\\n| Winogrande       | 0.5       | 0.6          | 0.7                      | 0.8               |            |              |          |\\n| SciQ             | 0.3       | 0.35         | 0.4                      | 0.45              |            |              |          |\\n| 20B              | 0.5       | 0.6          | 0.7                      | 0.8               |            |              |          |\\n| HellaSwag        | 0.3       | 0.35         | 0.4                      | 0.45              |            |              |          |\\n| 20B              | 0.6       | 0.65         | 0.7                      |                   |            |              |          |\\n| PIQA             | 0.6       | 0.65         | 0.7                      |                   |            |              |          |\\n\\nFigure 80: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-Easy (Clark et al., 2018), and Winogrande (Sakaguchi et al., 2019)\\n\\nFigure 81: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\\n\\nFigure 82: Training Cross Entropy\"}"}
{"id": "acl-2024-long-840", "page_num": 62, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 83: Perplexity results on Paloma (Magnusson et al., 2023); subsets 4chan (Papasavva et al., 2020), Dolma Reddit Subset, and Dolma Papers Subset.\\n\\nFigure 84: Perplexity results on Paloma (Magnusson et al., 2023); subsets ICE (Greenbaum, 1991), M2D2 (Reid et al., 2022) (Wiki), and Twitter AAE (Blodgett et al., 2016).\\n\\nFigure 85: Perplexity results on Paloma (Magnusson et al., 2023); subsets Manosphere (Ribeiro et al., 2021).\\n\\nFigure 86: Perplexity results on Paloma (Magnusson et al., 2023); subsets mC4 (Xue et al., 2020) (English), M2D2 (Reid et al., 2022) (S2ORC), and C4 (Raffel et al., 2020; Dodge et al., 2021).\"}"}
{"id": "acl-2024-long-840", "page_num": 63, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 87: Perplexity results on Paloma (Magnusson et al., 2023); subsets Penn Tree Bank (Marcus et al., 1994), Dolma Wikipedia Subset, and Gab (Zannettou et al., 2018)\\n\\nFigure 88: Perplexity results on Paloma (Magnusson et al., 2023); subsets Pile (Gao et al., 2020) (Val), Dolma Books Subset, and C4 100 dom (Chronopoulou et al., 2022)\\n\\nFigure 89: Perplexity results on Paloma (Magnusson et al., 2023); subsets WikiText 103 (Merity et al., 2016), Dolma Code Subset, and Dolma Web Subset\\n\\nFigure 90: Results downstream tasks OpenBookQA (Mihaylov et al., 2018), ARC-E (Clark et al., 2018), and WinoGrande (Sakaguchi et al., 2019)\"}"}
{"id": "acl-2024-long-840", "page_num": 64, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 91: Results downstream tasks SciQ (Welbl et al., 2017), HellaSwag (Zellers et al., 2019), and PIQA (Bisk et al., 2019)\\n\\nFigure 92: Training Cross Entropy\"}"}
