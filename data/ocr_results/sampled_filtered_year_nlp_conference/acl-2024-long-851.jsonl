{"id": "acl-2024-long-851", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MMToM-QA: Multimodal Theory of Mind Question Answering\\nChuanyang Jin1 Yutong Wu2 Jing Cao3 Jiannan Xiang4 Yen-Ling Kuo5 Zhiting Hu4 Tomer D. Ullman2 Antonio Torralba3 Joshua B. Tenenbaum3 Tianmin Shu6\\n\\nAbstract\\nTheory of Mind (ToM), the ability to understand people's mental states, is an essential ingredient for developing machines with human-level social intelligence. Recent machine learning models, particularly large language models, seem to show some aspects of ToM understanding. However, existing ToM benchmarks use unimodal datasets \u2013 either video or text. Human ToM, on the other hand, is more than video or text understanding. People can flexibly reason about another person's mind based on conceptual representations (e.g., goals, beliefs, plans) extracted from any available data. To address this, we introduce a multimodal Theory of Mind question answering (MMToM-QA) benchmark. MMToM-QA comprehensively evaluates machine ToM both on multimodal data and on different kinds of unimodal data about a person's activity in a household environment. To engineer multimodal ToM capacity, we propose a novel method, BIP-ALM (Bayesian Inverse Planning Accelerated by Language Models). BIP-ALM extracts unified representations from multimodal data and utilizes language models for scalable Bayesian inverse planning. We conducted a systematic comparison of human performance, BIP-ALM, and state-of-the-art models, including GPT-4. The experiments demonstrate that large language models and large multimodal models still lack robust ToM capacity. BIP-ALM, on the other hand, shows promising results, by leveraging the power of both model-based mental inference and language models.\\n\\n1 Introduction\\nTheory of Mind (ToM) is the cognitive ability to ascribe hidden mental states (e.g. goals, beliefs, and desires) to other individuals based on their observed behavior. A hallmark of human social intelligence, ToM serves as the foundation for a wide range of social interactions and a pillar of commonsense reasoning (Lake et al., 2017). Systems designed to safely and productively interact with humans in an open-ended manner, such as assistive robots (e.g., Dautenhahn, 2007; Hadfield-Menell et al., 2016; Patel and Chernova, 2022; Puig et al., 2023), AI teachers (e.g., Wang et al., 2021), and autonomous vehicles (e.g., Chandra et al., 2020), would greatly benefit from incorporating ToM reasoning capabilities. The recent advancements in machine learning, especially in the realm of Large Language Models (LLMs), have spurred increased interest in assessing these models' aptitude for ToM reasoning (e.g., Rabinowitz et al., 2018; Kosinski, 2023; Sap et al., 2019, 2022; Ullman, 2023; Shapira et al., 2023; Shu et al., 2021; Moghaddam and Honey, 2023; Nematzadeh et al., 2018; Gandhi et al., 2021, 2023; Kim et al., 2023). Many of these assessments use either text-based or video-based benchmarks inspired by classic ToM experiments in the cognitive science literature (Wimmer and Perner, 1983).\\n\\nWhile the recent ToM benchmarks provide well-designed, cognitively informed tools, they share several notable limitations. One such limitation is the dependence on massive training data, which raises the concern that these models work by finding data patterns in a way that deviates from human-like ToM reasoning (e.g., Ullman, 2023; Sap et al., 2022; Shapira et al., 2023). This paper focuses on a different but related limitation: These benchmarks rely on unimodal data, either in the form of videos (e.g., Gandhi et al., 2021), or textual descriptions of actions and environments (e.g., Kosinski, 2023; Sap et al., 2022; Gandhi et al., 2023). But ToM reasoning goes beyond merely text comprehension or video understanding. It is about forming a causal model of another person's mind, which connects mental variables to possible actions (Baker et al., 2009; Saxe, 2012; Jara-Ettinger et al., 2016; Jara-Ettinger, 2019). Such a model can infer mental\"}"}
{"id": "acl-2024-long-851", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What's inside the apartment:\\n\\n\u2026 The kitchen is equipped with a microwave, eight cabinets, \u2026 Inside the microwave, there is a cupcake. There is a wine glass and an apple on one of the kitchen tables. There are water glasses, a bottle of wine, a condiment bottle, and a bag of chips inside the cabinets.\\n\\nActions taken by Emily:\\n\\nEmily is initially in the bathroom. She then walks to the kitchen, goes to the sixth cabinet, opens it, subsequently closes it, and then goes towards the fourth cabinet.\\n\\nWhich one of the following statements is more likely to be true?\\n\\n(a) Emily has been trying to get a cupcake.               (b) Emily has been trying to get a wine glass.\\n\\nFigure 1:\\n\\nSketch of the MMToM-QA benchmark. Each question is associated with a video stream (representative frames highlighting key moments are shown above for illustration) and text input (illustrative text above is shortened for brevity). In the example video, Emily can see the wine glass on one of the kitchen tables (1st frame) and passes by it without picking it up (2nd frame). At the end of the clip (3rd frame), it appears that she could be walking towards the cabinets on the left side of the room; or she might want to check if a goal object is inside the microwave. The text indicates that there are no cupcakes in the cabinets, but there is a cupcake inside the microwave. To confidently choose the correct answer, a model must fuse relevant information from both the video and the text.\\n\\nBy examining multimodal ToM reasoning, we can both gain insight into the computational models that underlie human ToM and offer a stronger test for current ML models, particularly LLMs.\\n\\nTo systematically evaluate the ability of ML models to infer mental states from multimodal data, we developed a novel Multimodal Theory of Mind Question Answering benchmark (MMToM-QA). As shown in Figure 1, the benchmark includes as input both videos and text describing the activity of a person in a household environment. The benchmark also includes questions associated with different points in each of the videos. The questions refer to the mental states (goals and beliefs) of the person described by the video or text. Each question has two possible options, neither surely true nor surely false, but with one option significantly more likely to be true given the observations. Some questions can be adequately answered based on a single modality, but some questions require fusing information from both modalities (e.g. understanding the woman's goal in Figure 1). We validated our benchmark through human experiments, showing that people are adept at answering the questions in the benchmark and providing a human baseline.\\n\\nWe propose a novel multimodal ToM model, Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM). As illustrated in Figure 3, BIP-ALM first extracts symbolic representations about the physical scene and the actions of the person from both video and text inputs. Using these symbolic representations, BIP-ALM then extends Bayesian inverse planning (BIP) (Baker et al., 2017), a cognitively grounded ToM method originally designed for visual data, to reason about the multimodal data. To accelerate the inference in real-world scenarios such as household activities in our benchmark, BIP-ALM uses a language model (LM) finetuned on human activity data to evaluate the likelihood of hypotheses about the person's belief and goal. By doing so, it takes advantage of the robustness of Bayesian inverse planning, as well as the scalability and open-endedness of LMs.\\n\\nWe compared the performance of BIP-ALM and several state-of-the-art models for text QA or multimodal QA, including GPT-4(V). We found that existing models, however impressive in other QA benchmarks, make large and systematic errors in our benchmark, and fail to match human performance. In contrast, BIP-ALM significantly outperforms these models.\\n\\nIn sum, our main contributions include (1) the first benchmark for multimodal ToM, (2) a novel ToM reasoning method, BIP-ALM, that combines Bayesian inverse planning and LMs to conduct multimodal ToM, (2) a novel ToM reasoning method, BIP-ALM, that combines Bayesian inverse planning and LMs to conduct\"}"}
{"id": "acl-2024-long-851", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"robust yet efficient ToM inference based on multimodal data, and (3) a systematic comparison of different ML models and human ToM.\\n\\n2 Related Work\\n\\nTheory of Mind Benchmarks. Existing ToM benchmarks are based on either videos or text. Visual-based benchmarks (e.g., Gandhi et al., 2021; Shu et al., 2021; Netanyahu et al., 2021) typically use animations of goal-directed agents to evaluate different concepts in ToM. Text-based QA benchmarks (e.g., Le et al., 2019; Shapira et al., 2023; Hewitt and Cohen, 2021; Gandhi et al., 2023; Kim et al., 2023; He et al., 2023) adapt or extend a classic false belief test, the Sally-Anne test (Wimmer and Perner, 1983). Questions in these benchmarks ask a model to select the true hypothesis about a person's knowledge and belief based on a given premise. Triangle COPA (Gordon, 2016) asks questions about the mental states of agents based on text descriptions of abstract shapes acting like social agents. Moreover, there are QA benchmarks (e.g., Zadeh et al., 2019; Sap et al., 2019) that do not specifically test ToM, but evaluate social intelligence in general. Lastly, there have also been multi-agent challenges (e.g., Sclar et al., 2022; Puig et al., 2020, 2023) evaluating ToM as part of the tasks. Unlike existing benchmarks, our MMToM-QA evaluates machine ToM on multimodal data to test both goal and belief inference. We also go beyond simple visual and textual stimuli and evaluate ToM using long everyday activities in complex environments. Critically, ToM QAs ask questions about people's mental states. This is fundamentally different from VQAs (e.g., Antol et al., 2015; Zellers et al., 2019) which do not require mental state inference. Table 2 in Appendix A provides a detailed comparison.\\n\\nMultimodal Question Answering. There have been several multimodal QA benchmarks developed in recent years (e.g., Talmor et al., 2021; Singh et al., 2021; Sanders et al., 2023; Fu et al., 2023; Li et al., 2023). These benchmarks focus on the ability of models to detect and retrieve relevant information from multimodal inputs (e.g., images, videos, text, tables) to answer factual questions. However, there have not been multimodal QA benchmarks for ToM, an ability fundamentally different from the kind of multimodal information retrieval tested in the existing benchmarks.\\n\\nMachine Theory of Mind. There have been two main approaches to engineering machine ToM: end-to-end methods such as Theory of Mind neural networks (e.g., Rabinowitz et al., 2018), and model-based methods such as Bayesian inverse planning (e.g., Baker et al., 2017; Shu et al., 2021). Both types of approaches focus mostly on unimodal data and simple domains. Recent studies have suggested that machine ToM may also emerge in LLMs such as GPT-4 (Kosinski, 2023; Bubeck et al., 2023). However, more systematic evaluations have shown that apparent ToM capacities in LLMs are not yet as robust as humans (Sap et al., 2022; Shapira et al., 2023; Sclar et al., 2023), and often fail to pass trivial variants of common tests (Ullman, 2023). Our BIP-ALM model builds on the strengths of these different methods. It extends Bayesian inverse planning to fuse multimodal data and conduct inference in complex scenarios with the use of language models.\\n\\n3 MMToM-QA Benchmark\\n\\n3.1 Overview\\n\\nOur benchmark consists of 134 videos of a person looking for daily objects in household environments, in line with cognitive studies examining mental attributions to agents navigating an environment (e.g. Baker et al., 2017). On average, each video has 1,462 frames, depicting 36 human actions. Based on these videos, we constructed 600 questions about a person's goals and beliefs. Each question is paired with a clip of the full activity in a video (as RGB-D frames), as well as a text description of the scene and the actions taken by the person in that clip. All questions have two choices. The questions are categorized into seven types (see Figure 2), evaluating belief inference and goal inference in rich and diverse situations. Each belief inference type has 100 questions, totaling 300 belief questions; each goal inference type has 75 questions, totaling 300 goal questions. We provide another set of synthetic human behavior data in household environments for model training. This training set includes 1,000 procedurally synthesized videos with ground-truth annotations of the scene, objects, goals, and beliefs.\\n\\n3.2 Question Types\\n\\nQuestions fall into two categories \u2013 belief inference and goal inference. There are several types within each category (Figure 2), evaluating different aspects of multimodal ToM reasoning. Unlike...\"}"}
{"id": "acl-2024-long-851", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scene: \u2026 Inside the bridge, you'll find a bottle of wine\u2026\\n\\nActions: \u2026 Finally, she moves towards the fridge, preparing to open it.\\n\\nQuestion:\\nIf Elizabeth has been trying to get a bottle of wine, which one of the following statements is more likely to be true?\\n\\n(a) Elizabeth thinks that there is a bottle of wine inside the fridge.\\n(b) Elizabeth thinks that there isn't any bottle of wine inside the fridge.\\n\\nType 1.1: True belief, short-term.\\nThis type refers to a person about to open a container. The question focuses on an object the person has not seen so far and treats it as a hypothetical goal object. The question is then whether the person believes the object is in the container. This type examines true belief; that is, the inference that the person believes the goal is in the container is more consistent with the current action, and it is also the same as the actual world state.\\n\\nType 1.2: False belief, short-term.\\nThis type is similar to Type 1.1, but differs in a significant way: the hypothetical goal object is not inside the container that the person is about to open, so the person has a false belief. In this case, the correct answer should still be that it is more likely that the person thinks there is such a goal object inside the container, given the current action, even though they would not find what they want there in reality.\\n\\nType 1.3: Belief tracking, long-term.\\nIn the scenarios this type refers to, the person passes by a container but does not check it. After a while, they have still not found a goal object, but are also not going back to check the container they passed by. This suggests that they do not think the goal object is inside that container. This question tests whether a model can use the long-term observation of past actions to make judgments consistent with history, not just the most recent action.\\n\\nType 2.1: Goal inference given true belief.\\nThis question targets a person's unknown goal. In the scenarios that this type refers to, the person walks towards a container, where there is a hypothetical goal object that the person has not observed so far. The person, on the other hand, has observed the other hypothetical goal object but has not picked it up in the past. The correct inference is that it is more likely that the person wants the goal object that they have not seen so far and thinks it is inside the container (true belief). This type tests whether a model can infer the goal given a true belief.\\n\\nType 2.2: Goal inference given false belief.\\nThis type is similar to Type 2.1, but the person has a hypothetical false belief. In these scenarios, a specific object is inside a container that the person is walking towards. However, the question states that this person thinks that there is no such object inside the container (false belief). So, the most likely explanation is that the person's goal is a different object, which they think might be inside the container.\\n\\nType 2.3: Goal inference given updated belief.\\nUnlike Type 2.1 and 2.2, in a Type 2.3 question, the person's belief changes during the process, and the model needs to update its inference accordingly.\"}"}
{"id": "acl-2024-long-851", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"video does not end with the person walking towards a container. Instead, the person opens the container and then closes it without taking an object from it. The correct inference is that the person's goal is an item not yet seen rather than anything inside the container. To correctly answer this type of question, a model has to infer how a person may update their belief and change their plan (e.g., closing the container without picking anything from it) to pursue the goal accordingly.\\n\\nType 2.4: Goal inference given future actions.\\n\\nQuestions in Type 2.1, 2.2, and 2.3 ask about goals that are consistent with the belief and the latest action. In contrast, in Type 2.4, a model needs to consider possible future actions as well. Specifically, one of the hypothetical goal objects is an unobserved object at a location that is still far away from the person and is not directly related to the latest action (which is walking to a nearby container). But, in these scenarios, the person is on a path to potentially reach the location of that object. For instance, in the Type 2.4 example illustrated in Figure 2, a person is walking towards the right side of the room, and so they might want to search through all possible locations on the right side, including the dishwasher. This gives rise to the correct answer, dish bowl, which is located inside the dishwasher. As such, Type 2.4 requires a model to reason about the spatial relationships (the locations of objects and the person as well as the person's heading direction) in a visual scene and predict possible future actions for a goal.\\n\\n3.3 Procedural Generation\\n\\nWe designed a procedural generation method for creating the benchmark. First, we procedurally synthesized a large set of videos in a realistic household embodied simulator, VirtualHome-Social (Puig et al., 2020). As Puig et al. (2020) have demonstrated, such procedural video generation can create synthetic human activities that are human-like and well-annotated (including ground-truth states, goals, and beliefs). It also alleviates the concerns of cost and privacy that come with real-world human activity video collection. At each step in a video, we sampled a question type and two opposing hypotheses based on the definition of the type. Finally, we generated the text description and the question based on the ground-truth state, actions, and the sampled hypotheses using GPT-4 to create the text input for the question. Using the same procedural generation, we synthesize the videos in the training set. We provide more details in Appendix B.7.\\n\\n3.4 Evaluation Protocol\\n\\nWe can evaluate a model in three conditions: (1) Multimodal QA in which both the video and text inputs are present, (2) Text QA with only the text input, and (3) Video QA with only the video input. We evaluated all models in a zero-shot manner, which is the standard setting in recent literature on ToM QA evaluation (Shapira et al., 2023). Crucially, we do not provide any example QAs during training. We expect a model to learn how a person updates their mental state and acts accordingly in a physical environment from the human behavior data in the training set, and generalize the learned knowledge to answer the questions at test time.\\n\\n4 The BIP-ALM Model\\n\\nTo infer the mental state of a person based on video and text inputs, we propose a novel machine Theory of Mind method, Bayesian Inverse Planning Accelerated by Language Models (BIP-ALM), which builds on Bayesian inverse planning (BIP) (Baker et al., 2017). Prior works have shown that BIP can reverse engineer human ToM reasoning in simple domains. BIP-ALM extends BIP by (1) building unified representations about a scene, a person's actions, and the mental state hypotheses from multi-modal inputs, and (2) finetuning a language model to efficiently conduct inverse symbolic planning, based on unified symbolic representations.\\n\\nFigure 3 provides an overview of the method. We first extract symbolic representations of the states and actions from both the video and the text. We then align and fuse representations extracted from different modalities, to form a unified representation of the event and the physical scene. This unified representation allows us to use a principled method to infer a person's mental state based on inputs from any given modality. We then use an inverse symbolic planner to compare the two hypotheses extracted from the question and produce the answer. We introduce each module below and provide more details in Appendix C.\\n\\n4.1 Unified Symbolic Representations\\n\\nVisual Perception.\\n\\nOur visual perception module processes visual data and transforms it into symbolic representations. For each frame, we adopt the method in Blukis et al. (2022) to create a voxel map and construct a scene graph.\"}"}
{"id": "acl-2024-long-851", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Overview of our model, BIP-ALM. For visual, linguistic, and fused information, we show examples of the symbolic representations of states (s\u2081,t), actions (a\u2081,t), and the two hypotheses about the person's goal (g\u2081 and g\u2082) and belief (b\u2081t\u2081 and b\u2082t\u2082) for a question asked at time step t.\\n\\nText Parsing. We use GPT-4 to parse text, to extract symbolic representations of the initial state as well as subsequent actions. GPT-4 first parses the text into three components \u2013 the description of the environment state, the human actions, and the question. Each component is further translated into symbolic representation by GPT-4. For state, we generate predicates such as In(apple, fridge). For action, we generate action commands such as walk towards kitchen. Finally, we translate the question into two hypotheses about the goal and the belief. For each hypothesis, the goal is represented by the goal object (e.g., apple), and the belief is represented by a predicate (e.g., In(apple, fridge)), or its negation (\u00acIn(apple, fridge)), indicating the hypothetical location of the object.\\n\\nFusion. The fusion module aligns and integrates information from different input streams. Specifically, we transform the scene graphs from the video input to a set of predicates (similar to those extracted from the text), which describe the spatial relationships between entities and the status of objects. We first form the symbolic representation of the initial state by combining predicates from the video and the text. We then align the actions parsed from the text with the actions detected from the video, and truncate the video frames into several intervals, each corresponding to an action. We term each interval, a time step t. Starting from the initial state, we update the state predicates from the previous step with the new predicates obtained from the video frame corresponding to the current step. By doing so, we can construct a symbolic state sequence and a symbolic action sequence, as illustrated in Figure 3. In addition, we form two hypotheses based on the hypothetical goals and beliefs parsed from the question.\\n\\n4.2 Inverse Symbolic Planner\\n\\nWe formulate an agent's behavior as a forward generative model using a Partially Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998), defined by the tuple \u27e8S,A,T,G,R,\u03a9,\u03b3\u27e9. s\u209c \u2208 S and a\u209c \u2208 A are the state and the action at time t. T(s\u209c|s,a\u209c) are the state transition probabilities. g \u2208 G is a goal, which defines the reward of the agent r\u209c = R(s\u209c,a\u209c,g). o\u209c \u2208 \u03a9 is the agent's observation at t derived following the observation function, o\u209c = O(s\u209c). Finally, \u03b3 \u2208 (0,1] is a discount factor.\\n\\nThe belief of an agent is modeled as a probability distribution over the state b(s). In this work, we factorize the belief of the full state into beliefs about the possible locations of individual objects. Conditioned on both the goal and the belief, a rational agent will take actions based on the optimal policy \u03c0(a\u209c|g,b\u209c) to maximize its return \u2211\u221e\u209c=0\u03b3\u209cr\u209c.\\n\\nGiven this forward generative model, we can conduct inverse inference about the agent's goal and belief. By assuming a deterministic state transition, we can jointly infer the goal and belief of an agent given observed states and actions as follows:\\n\\nP(g,b\u209c|s\u2081:t,a\u2081:t\u22121) \u221d \u220f\u03c4=1\u03c0(a\u209c|g,b\u209c)P(b\u209c|b\u209c\u22121,s\u209c) \u00b7 P(b\u2080)P(g) (1)\\n\\nGiven two hypotheses about goal and belief, H\u2081 = \u27e8g\u2081,b\u2081t\u27e9 and H\u2082 = \u27e8g\u2082,b\u2082t\u27e9, we can evaluate\\n\\n\u221a\\nRGB-D Video\\nVisual Perception Module\\nIn(apple, kitchen cabinet)In(cupcake, fridge)Closed(fridge)Closed(kitchen cabinet)\u2026\"}"}
{"id": "acl-2024-long-851", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which one is more likely to be true as\\n\\\\[ P(g_1, b_{t_1} | s_{t_1}, a_{t_1}) \\\\]\\n\\\\[ P(g_2, b_{t_2} | s_{t_1}, a_{t_1}) = \\\\pi(a_{t_1} | g_1, b_{t_1}) \\\\cdot \\\\prod_{\\\\tau=1}^{t-1} \\\\pi(a_{\\\\tau} | g_1, \\\\hat{b}_{\\\\tau}) \\\\]\\n\\\\[ \\\\pi(a_{\\\\tau} | g_2, \\\\hat{b}_{\\\\tau}) \\\\cdot \\\\prod_{\\\\tau=1}^{t-1} \\\\pi(a_{\\\\tau} | g_2, \\\\hat{b}_{\\\\tau}) \\\\],\\n(2)\\n\\nwhere \\\\( \\\\hat{b}_{\\\\tau} \\\\) is the estimated belief at a past step \\\\( \\\\tau < t \\\\).\\n\\nSince the hypothetical belief in the question is about the belief at the current step, we estimate the belief in the past steps to form a full belief hypothesis. In this work, we assume a uniform distribution for the initial belief. We represent the agent's observation as the subset of the state predicates that the agent can observe, and update the agent's belief accordingly. This gives us an estimated agent belief \\\\( \\\\hat{b}_{\\\\tau} = \\\\hat{b}_{\\\\tau}(s_{\\\\tau}) \\\\) at each past step.\\n\\nBased on Eqn. (2), we need to evaluate (1) the likelihood of the last action \\\\( a_t \\\\) given the hypothetical belief and goal \\\\( \\\\pi(a_t | g, b_t) \\\\), (2) the probability of a hypothetical belief at the last step \\\\( P(b_t | \\\\hat{b}_{t-1}, s_t) \\\\), and (3) the likelihood of all past actions given the hypothetical goal and the estimated belief prior to the current step \\\\( \\\\prod_{\\\\tau=1}^{t-1} \\\\pi(a_{\\\\tau} | g, \\\\hat{b}_{\\\\tau}) \\\\).\\n\\nThe computational bottleneck here is the policy. Conventional methods rely on planning or reinforcement learning to acquire such a policy. Inspired by the recent use of LLMs for decision-making (Huang et al., 2022; Li et al., 2022), we adopt a language model to amortize the policy. In particular, we symbolically represent the belief at each step as a list of possible locations of the corresponding goal object. We then prompt a language model with the symbolic representations of the state \\\\( s_t \\\\), goal \\\\( g \\\\), and estimated belief \\\\( \\\\hat{b}_t \\\\), and generate the likelihood of the observed action \\\\( a_t \\\\) based on the output logits. Figure 5 in Appendix B.2 illustrates how the likelihood estimation works in a qualitative example. We can finetune the language model on the ground-truth state, belief, goal, and action sequences in the training dataset.\\n\\n5 Experiments\\n5.1 Baselines\\n\\nHuman baseline. We conducted a human experiment to validate our questions, and to evaluate human performance. Participants (N=180) were recruited online via Prolific. We randomly sampled 120 questions (20% of all questions) from all types.\\n\\nLarge language models (LLMs). We evaluate LLMs on the text-only version of MMToM-QA, including GPT-4 (OpenAI, 2023), GPT-3.5, GPT-J (6B) (Wang and Komatsuzaki, 2021), and LLaMA 2 (7B) (Touvron et al., 2023). We also evaluate SimToM (Wilf et al., 2023) and SymbolicToM (Sclar et al., 2023), two recent approaches that improve ToM in LLMs through better prompting. We apply them to GPT-4 and create two baselines: SimToM w/ GPT-4 and SymbolicToM w/ GPT-4. For all LLMs, we prompt them with the text input.\\n\\nLarge multimodal models (LMMs). We evaluate GPT-4V (OpenAI, 2023), InstructBLIP (Dai et al., 2023), Video-LLaMA 2 (Zhang et al., 2023), and LLaVA (Liu et al., 2023) in both multimodal and video-only conditions. For all LMMs, we uniformly sample a few frames from each video following prior works (Dai et al., 2023). We use the largest versions for all LMMs. We finetuned GPT-J (6B) and LLaMA 2 (7B) and created two BIP-ALM models: BIP-ALM w/ GPT-J and BIP-ALM w/ LLaMA 2.\\n\\n5.2 Results\\nWe summarize the main results in Figure 4. On the multimodal version, humans achieve 93% accuracy averaging across question types. For each tested question, the majority of the participants chose the correct answer, validating our question designs. When only given unimodal data, human performance drops in accuracy overall, with video-only questions being harder to answer than text-only. While most questions suffer, the performance of text-based reasoning on the true-belief/false-belief...\"}"}
{"id": "acl-2024-long-851", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Human and model performance in each type. Only GPT-4(V) and SymbolicToM baselines perform well in at least one type in multimodal (MM) and text-only conditions. We provide more quantitative and qualitative results in Appendix B.\\n\\nThe baselines show performance that is close to random guessing across all types in all conditions, except GPT-4(V) and SymbolicToM w/ GPT-4 in multimodal and text-only conditions, as shown in Table 1. GPT-4(V) reaches human-level accuracy on Type 1.1 and shows competitive performance on Type 1.3. However, it also makes systematic mistakes in harder questions that involve false beliefs (Type 1.2). This suggests that GPT-4(V) can understand the true world state from the text but confuses belief with the true world state. GPT-4(V) also struggles with goal inference. Its accuracy on Type 2.3 is particularly low. We hypothesize that this is because it mistakenly thinks that the goal has to be one of the objects inside the container the person opens and fails to recognize that the person updates the belief after checking inside the container. SymbolicToM can improve GPT-4\u2019s performance on a few types by removing irrelevant text, but it still struggles with harder types.\\n\\nOur BIP-ALM models outperform all baselines by a large margin. Even without finetuning, as the ablated study in Appendix B.1 shows, our model with small pretrained LMs can already achieve better results than using much larger pretrained LMs (e.g., GPT-4) alone. BIP-ALM also can flexibly conduct ToM reasoning with any unimodal or multimodal data thanks to the unified representations.\\n\\nAs reported in Appendix B.1, we examined the effect of few-shot or chain-of-thought prompting (Table 4) and model sizes (Table 5). We did not find any setting that can consistently improve a baseline\u2019s performance in all types. We also fine-tuned Video-LLaMA 2 (13B) on our training set for video instruction tasks following Zhang et al. (2023). As Table 6 shows, the finetuned model performs moderately better in a few simpler question types (e.g., Type 1.1), but its overall performance is still not better than chance, unlike our method.\\n\\nGeneralization evaluation. We created an additional test set, the human test set, for generalization evaluation. It has 40 videos and 120 questions. To generate the videos in this set, we used 2 new apartments unseen in the training set and the main test set. We recruited 3 participants who had no prior exposure to the system to control the avatar to reach assigned goals via the human interface so that we could collect real human belief updates and human actions. We then used the same method to generate the questions. We report the model performance on this human test set in Table 7 (Appendix B.1). It shows that our method can generalize to both real...\"}"}
{"id": "acl-2024-long-851", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We presented MMToM-QA, the first multimodal benchmark for machine ToM. We conducted a systematic evaluation of human performance, state-of-the-art methods, and our BIP-ALM model. We summarize the key findings as follows.\\n\\n**How does each modality contribute to ToM?**\\n\\nFrom a video, a model gets the dynamic state change as well as what objects the agent is walking towards and is passing by at a given step. A model needs this information to determine the agent's expected action plans given its mental state. Because of the partial observations caused by the limited camera view and occlusion, the text provides additional state information that is sometimes unavailable in the video. A model requires information about the true world state to determine an agent's observation and whether it has a false belief. This is illustrated in Figure 6 in Appendix B.2.\\n\\n**Do LLMs and LMMs understand ToM?**\\n\\nGPT-4 and GPT-4V excelled on questions that only require retrieving information about the true world state. However, they still cannot reason about the mental state of a person and track the change in the mental state over time. We found more specifically that they have poor judgment on goals, which was not evaluated in existing text-based benchmarks.\\n\\n**What are the successes and failures of BIP-ALM?**\\n\\nInstead of directly mapping the multimodal inputs to beliefs and goals, BIP-ALM conducts model-based inference by imaging possible actions given a hypothetical mental state and state context via language models. This results in better performance in the main test set and enables the method to generalize to real human behavior in unseen environments. We also observed several failures. First, BIP-ALM cannot imagine missing state information from videos. Second, it uses symbolic state representations and only conducts symbolic planning. However, in Type 2.4, one needs to imagine the future path of a person given continuous spatial information such as the current path, facing direction, and the locations of potential goal objects. Finally, the action likelihood estimated by the LMs sometimes could be inaccurate due to LMs' occasional failures in planning.\\n\\n**Limitations and future work**\\n\\nFirst, MMToM-QA only includes videos of people looking for objects in household environments. In the future, we would like to extend this to more diverse scenarios. Second, we intend to incorporate additional ToM concepts such as desires, emotions, and constraints in a future version of the benchmark. Finally, we intend to enrich the representations in BIP-ALM with broader relations and predicates, extending its reach even further to more complex scenes and human behaviors. This could be potentially achieved by finetuning larger LMs in broader datasets that are collected in simulators or crowdsourced (text descriptions of real-world human behaviors).\\n\\n**Ethics Statement**\\n\\nThe ability to understand humans' mental states is a crucial foundation for building human-centered AI systems that can safely and cooperatively interact with humans. Benchmarking state-of-the-art machine learning models' Theory of Mind capacity can provide us insights into whether current machine learning models can indeed adequately understand humans. We believe that our benchmark is a significant contribution towards this effort. Both our MMToM-QA benchmark and the BIP-ALM model are built on prior studies in cognitive science and thus are grounded in the cognitive theories of human behaviors. Such cognitively grounded benchmarks and models can help develop AI systems that are more aligned with humans' social cognition. While we do not foresee any potential harm from our work, we recognize the need to ensure the diversity and fairness of our benchmark. We have made our best effort to increase the diversity of the avatars used to generate the videos. We have also validated our benchmark in a human experiment. We welcome feedback and suggestions from the community to further improve our benchmark.\\n\\n**Acknowledgements**\\n\\nThis work was supported by the DARPA Machine Common Sense program and a grant from Lockheed Martin.\\n\\n**References**\\n\\nStanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. 2015. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433.\\n\\nChris L Baker, Julian Jara-Ettinger, Rebecca Saxe, and Joshua B Tenenbaum. 2017. Rational quantitative...\"}"}
{"id": "acl-2024-long-851", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chris L Baker, Rebecca Saxe, and Joshua B Tenenbaum. 2009. Action understanding as inverse planning. Cognition, 113(3):329\u2013349.\\n\\nValts Blukis, Chris Paxton, Dieter Fox, Animesh Garg, and Yoav Artzi. 2022. A persistent spatial semantic representation for high-level natural language instruction execution. In Conference on Robot Learning, pages 706\u2013717. PMLR.\\n\\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\\n\\nRohan Chandra, Aniket Bera, and Dinesh Manocha. 2020. Stylepredict: Machine theory of mind for human driver behavior from trajectories. arXiv preprint arXiv:2011.04816.\\n\\nWenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. 2023. Instructblip: Towards general-purpose vision-language models with instruction tuning.\\n\\nKerstin Dautenhahn. 2007. Socially intelligent robots: dimensions of human\u2013robot interaction. Philosophical transactions of the royal society B: Biological sciences, 362(1480):679\u2013704.\\n\\nChaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, et al. 2023. Mme: A comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394.\\n\\nKanishk Gandhi, Jan-Philipp Fr\u00e4nken, Tobias Gerstenberg, and Noah D Goodman. 2023. Understanding social reasoning in language models with language models. arXiv preprint arXiv:2306.15448.\\n\\nKanishk Gandhi, Gala Stojnic, Brenden M Lake, and Moira R Dillon. 2021. Baby intuitions benchmark (bib): Discerning the goals, preferences, and actions of others. Advances in Neural Information Processing Systems, 34:9963\u20139976.\\n\\nAndrew S. Gordon. 2016. Commonsense interpretation of triangle behavior. In AAAI Conference on Artificial Intelligence.\\n\\nDylan Hadfield-Menell, Stuart J Russell, Pieter Abbeel, and Anca Dragan. 2016. Cooperative inverse reinforcement learning. In Advances in neural information processing systems.\\n\\nYinghui He, Yufan Wu, Yilin Jia, Rada Mihalcea, Yulong Chen, and Naihao Deng. 2023. Hi-tom: A benchmark for evaluating higher-order theory of mind reasoning in large language models. arXiv preprint arXiv:2310.16755.\\n\\nJohn Hewitt and Michael Cohen. 2021. Exploring roberta\u2019s theory of mind through textual entailment.\\n\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685.\\n\\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. 2022. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International Conference on Machine Learning, pages 9118\u20139147. PMLR.\\n\\nJulian Jara-Ettinger. 2019. Theory of mind as inverse reinforcement learning. Current Opinion in Behavioral Sciences, 29:105\u2013110.\\n\\nJulian Jara-Ettinger, Hyowon Gweon, Laura E Schulz, and Joshua B Tenenbaum. 2016. The na\u00efve utility calculus: Computational principles underlying commonsense psychology. Trends in cognitive sciences, 20(8):589\u2013604.\\n\\nLeslie Pack Kaelbling, Michael L Littman, and Anthony R Cassandra. 1998. Planning and acting in partially observable stochastic domains. Artificial intelligence, 101(1-2):99\u2013134.\\n\\nHyunwoo Kim, Melanie Sclar, Xuhui Zhou, Ronan Le Bras, Gunhee Kim, Yejin Choi, and Maarten Sap. 2023. Fantom: A benchmark for stress-testing machine theory of mind in interactions. arXiv preprint arXiv:2310.15421.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:22199\u201322213.\\n\\nMichal Kosinski. 2023. Theory of mind may have spontaneously emerged in large language models. arXiv preprint arXiv:2302.02083.\\n\\nBrenden M Lake, Tomer D Ullman, Joshua B Tenenbaum, and Samuel J Gershman. 2017. Building machines that learn and think like people. Behavioral and brain sciences, 40:e253.\\n\\nMatt Le, Y-Lan Boureau, and Maximilian Nickel. 2019. Revisiting the evaluation of theory of mind through question answering. In Conference on Empirical Methods in Natural Language Processing.\\n\\nLei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, et al. 2023. M3it: A large-scale dataset towards multi-modal multilingual instruction tuning. arXiv preprint arXiv:2306.04387.\\n\\nShuang Li, Xavier Puig, Chris Paxton, Yilun Du, Clinton Wang, Linxi Fan, Tao Chen, De-An Huang, Ekin Aky\u00fcrek, Anima Anandkumar, et al. 2022. Pre-trained language models for interactive decision-making. Advances in Neural Information Processing Systems, 35:31199\u201331212.\"}"}
{"id": "acl-2024-long-851", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual instruction tuning. arXiv preprint arXiv:2304.08485.\\n\\nShima Rahimi Moghaddam and Christopher J Honey. 2023. Boosting theory-of-mind performance in large language models via prompting. arXiv preprint arXiv:2304.11490.\\n\\nAida Nematzadeh, Kaylee Burns, Erin Grant, Alison Gopnik, and Thomas L Griffiths. 2018. Evaluating theory of mind in question answering. arXiv preprint arXiv:1808.09352.\\n\\nAviv Netanyahu, Tianmin Shu, Boris Katz, Andrei Barbu, and Joshua B Tenenbaum. 2021. Phase: Physically-grounded abstract social events for machine social perception. In Proceedings of the aaai conference on artificial intelligence, volume 35, pages 845\u2013853.\\n\\nOpenAI. 2023. Gpt-4 technical report. ArXiv, abs/2303.08774.\\n\\nMaithili Patel and Sonia Chernova. 2022. Proactive robot assistance via spatio-temporal object modeling. arXiv preprint arXiv:2211.15501.\\n\\nXavier Puig, Tianmin Shu, Shuang Li, Zilin Wang, Yuan-Hong Liao, Joshua B Tenenbaum, Sanja Fidler, and Antonio Torralba. 2020. Watch-and-help: A challenge for social perception and human-ai collaboration. arXiv preprint arXiv:2010.09890.\\n\\nXavier Puig, Tianmin Shu, Joshua B Tenenbaum, and Antonio Torralba. 2023. Nopa: Neurally-guided online probabilistic assistance for building socially intelligent home assistants. arXiv preprint arXiv:2301.05223.\\n\\nNeil Rabinowitz, Frank Perbet, Francis Song, Chiyuan Zhang, SM Ali Eslami, and Matthew Botvinick. 2018. Machine theory of mind. In International conference on machine learning, pages 4218\u20134227. PMLR.\\n\\nKate Sanders, David Etter, Reno Kriz, and Benjamin Van Durme. 2023. Multivent: Multilingual videos of events with aligned natural text. arXiv preprint arXiv:2307.03153.\\n\\nMaarten Sap, Ronan LeBras, Daniel Fried, and Yejin Choi. 2022. Neural theory-of-mind? on the limits of social intelligence in large lms. arXiv preprint arXiv:2210.13312.\\n\\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan LeBras, and Yejin Choi. 2019. Socialiqa: Commonsense reasoning about social interactions. arXiv preprint arXiv:1904.09728.\\n\\nRebecca Saxe. 2012. The happiness of the fish: Evidence for a common theory of one's own and others' actions. In Handbook of Imagination and Mental Simulation, pages 257\u2013309. Psychology Press.\\n\\nMelanie Sclar, Sachin Kumar, Peter West, Alane Suhr, Yejin Choi, and Yulia Tsvetkov. 2023. Minding language models' (lack of) theory of mind: A plug-and-play multi-character belief tracker. arXiv preprint arXiv:2307.03980.\\n\\nMelanie Sclar, Graham Neubig, and Yonatan Bisk. 2022. Symmetric machine theory of mind. In International Conference on Machine Learning, pages 19450\u201319466. PMLR.\\n\\nNatalie Shapira, Mosh Levy, Seyed Hossein Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, and Vered Shwartz. 2023. Clever hans or neural theory of mind? stress testing social reasoning in large language models. arXiv preprint arXiv:2305.14763.\\n\\nTianmin Shu, Abhishek Bhandwaldar, Chuang Gan, Kevin Smith, Shari Liu, Dan Gutfreund, Elizabeth Spelke, Joshua Tenenbaum, and Tomer Ullman. 2021. Agent: A benchmark for core psychological reasoning. In International Conference on Machine Learning, pages 9614\u20139625. PMLR.\\n\\nHrituraj Singh, Anshul Nasery, Denil Mehta, Aishwarya Agarwal, Jatin Lamba, and Balaji Vasan Srinivasan. 2021. Mimoqa: Multimodal input multimodal output question answering. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5317\u20135332.\\n\\nAlon Talmor, Ori Yoran, Amnon Catav, Dan Lahav, Yizhong Wang, Akari Asai, Gabriel Ilharco, Hanhnane Hajishirzi, and Jonathan Berant. 2021. Multimodalqa: Complex question answering over text, tables and images. arXiv preprint arXiv:2104.06039.\\n\\nHugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288.\\n\\nTomer Ullman. 2023. Large language models fail on trivial alterations to theory-of-mind tasks. arXiv preprint arXiv:2302.08399.\\n\\nBen Wang and Aran Komatsuzaki. 2021. GPT-J-6B: A 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/mesh-transformer-jax.\\n\\nQiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021. Towards mutual theory of mind in human-ai interaction: How language reflects what students perceive about a virtual teaching assistant. In Proceedings of the 2021 CHI conference on human factors in computing systems, pages 1\u201314.\\n\\nAlex Wilf, Sihyun Shawn Lee, Paul Pu Liang, and Louis-Philippe Morency. 2023. Think twice: Perspective-taking improves large language models' theory-of-mind capabilities. arXiv preprint arXiv:2311.10227, arXiv:2311.10227v1.\"}"}
{"id": "acl-2024-long-851", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Comparison of Theory of Mind Benchmarks\\n\\nWe provide a comparison of Theory of Mind benchmarks in Table 2, summarizing the evaluated ToM concepts, the size of the test set, available modalities of the inputs, the generation method, and the evaluation for each benchmark. From the table, we can see that our benchmark is the only one that provides multimodal inputs. Additionally, commonly used text-based ToM QA benchmarks do not evaluate goal inference, whereas questions in our benchmark ask about both goals and beliefs.\\n\\nB Benchmark Details\\n\\nB.1 More Quantitative Results\\n\\nWe conducted an ablated study to show the effect of finetuning language models for BIP-ALM. As Table 3 shows, finetuning GPT-J and LLaMA 2 significantly boosts the performance of BIP-ALM. It is also interesting to see that even before finetuning, BIP-ALM with pretrained GPT-J or LLaMA can already outperform much larger models including GPT-4. This further demonstrates the advantage of conducting inverse symbolic planning for multi-modal ToM reasoning.\\n\\nWe evaluated open-sourced models (LLaMA 2, InstructBLIP, and Video-LLaMA 2) with different model sizes (Table 5 in Appendix B.1). All baselines performed no better than chance, regardless of the model sizes. As shown in Table 4 in Appendix B.1, we found no meaningful improvement for almost all baselines after using different few-shot or chain-of-thought Kojima et al. (2022) prompting. Only GPT-4 in the text-only condition has an improvement in simple types (e.g., Type 1.3) with few-shot prompting. The accuracies for LLaMA (w/ 1-shot) and LLaMA (w/ 2-shot) are notably low. The model often fails to generate options a or b, as it faces difficulties in processing prompts of this length. We finetuned Video-LLaMA 2 (13B) on our training set following Zhang et al. (2023). As Table 6 shows, the finetuned model performs moderately better in a few simpler question types (e.g., Type 1.1), but its overall performance is still not better than chance.\\n\\nWe further evaluated the generalization of models on the human test set with real human behaviors in unseen environments (Table 7).\\n\\nB.2 Qualitative Results\\n\\nFigure 5 demonstrates how the inverse symbolic planner enabled by the language model in our BIP-ALM model can estimate the likelihood of a given hypothesis. In Figure 5A, given the goal of getting a water glass, our model reasons that Elizabeth is more likely to open the microwave if she believes that there is a water glass inside the microwave, even though there is not any water glass inside the microwave according to the true world state. By imagining reasonable actions conditioned on hypothesized mental states, our model can successfully infer that Elizabeth has a false belief. In contrast, GPT-4 selects (b) as the more likely option, failing to recognize the false belief. Figure 5B depicts how the likelihood of a hypothesis changes after the model observes different actions. Specifically, in this case, the model first thinks that it is more likely that Karen is going to open the oven if the goal is to get a plate because there is a plate inside the oven. However, if the goal is truly to get a plate, at the next step, Karen should not close the oven but pick up the plate instead. If the goal is not to get a plate but a salmon, on the other hand, then Karen should close the oven and continue to look for salmon in other places. Therefore, the fact that Karen closes the oven without picking up the plate suggests that it is unlikely that her goal is to get a plate and that it is still quite possible that she wants to get a salmon instead. The action likelihood ratios estimated by our model at these two steps reflect this reasoning. Consequently, our model answers the question correctly.\\n\\nFigure 6 illustrates how BIP-ALM may form different state information from different modalities...\"}"}
{"id": "acl-2024-long-851", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: A comparison of Theory of Mind benchmarks.\\n\\n| Benchmark            | Concepts          | Test Size | Modality | Generation | Evaluation |\\n|----------------------|-------------------|-----------|----------|------------|------------|\\n| ToMi (Le et al., 2019) | False beliefs     | 400 Text  | Multiple choice | Q&A        |            |\\n| epistemic reasoning (Hewitt and Cohen, 2021) | Knowledge, beliefs | 2,000 Text Templates | True or false | judgement |            |\\n| Adv-CSFB (Kosinski, 2023) | False beliefs | 183 Text | Hand-designed | Multiple choice |            |\\n| BigToM (Gandhi et al., 2023) | Beliefs | 5,000 Text | Procedural generation | Question Answering |            |\\n| FANToM (Kim et al., 2023) | Facts and Beliefs | 4,807 Text | Procedural generation | Question Answering |            |\\n| Triangle COPA (Gordon, 2016) | Social interaction | 100 Text | Hand-designed | Multiple choice | Q&A        |\\n| Hi-ToM (He et al., 2023) | Higher-order beliefs | 600 Text | Procedural generation | (Multiple choice) Q&A |            |\\n| BIB (Gandhi et al., 2021) | Goal preferences, efficient actions, instrumental actions | 5,000 Video | Procedural generation | Surprise rating |            |\\n| AGENT (Shu et al., 2021) | Goal preferences, efficient actions, unobserved constraints, cost-reward trade-off | 960 Video | Procedural generation | Surprise rating |            |\\n| PHASE (Netanyahu et al., 2021) | Goals, relationships | 100 Video | Procedural generation | Multiple choice | recognition |\\n| MMToM-QA (Our benchmark) | Beliefs, goals | 600 Text and video | Procedural generation | Multiple choice | Q&A        |\"}"}
{"id": "acl-2024-long-851", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Method**\\n\\n**Goal Inference**\\n\\n**All**\\n\\n| 1.1 | 1.2 | 1.3 | All |\\n|---|---|---|---|\\n| MM |\\n\\n**Belief Inference**\\n\\n| Ours GPT-J (w/o FT) | 84.0 | 63.0 | 92.0 | 79.7 |\\n|---|---|---|---|---|\\n| Ours GPT-J | 90.0 | 69.0 | 86.0 | 81.7 |\\n| Ours LLaMA 2 (w/o FT) | 56.0 | 46.0 | 96.0 | 66.0 |\\n| Ours LLaMA 2 | 88.0 | 68.0 | 85.0 | 80.3 |\\n\\n| Ours GPT-J (w/o FT) | 57.0 | 36.0 | 77.0 | 56.7 |\\n|---|---|---|---|---|\\n| Ours GPT-J | 63.0 | 57.0 | 72.0 | 64.0 |\\n| Ours LLaMA 2 (w/o FT) | 51.0 | 33.0 | 75.0 | 53.0 |\\n| Ours LLaMA 2 | 69.0 | 63.0 | 60.0 | 64.0 |\\n\\n**Text**\\n\\n| Ours GPT-J (w/o FT) | 76.0 | 61.0 | 90.0 | 75.7 |\\n|---|---|---|---|---|\\n| Ours GPT-J | 88.0 | 69.0 | 88.0 | 81.7 |\\n| Ours LLaMA 2 (w/o FT) | 66.0 | 53.0 | 98.0 | 72.3 |\\n| Ours LLaMA 2 | 89.0 | 68.0 | 90.0 | 82.3 |\\n\\n**Video**\\n\\n| Ours GPT-J (w/o FT) | 57.0 | 36.0 | 77.0 | 56.7 |\\n|---|---|---|---|---|\\n| Ours GPT-J | 63.0 | 57.0 | 72.0 | 64.0 |\\n| Ours LLaMA 2 (w/o FT) | 51.0 | 33.0 | 75.0 | 53.0 |\\n| Ours LLaMA 2 | 69.0 | 63.0 | 60.0 | 64.0 |\\n\\n**Table 3: Results of the ablated study**\\n\\n\u201cw/o FT\u201d indicates ablated models in which our model uses pretrained language models without finetuning. \u201cMM\u201d represents the multimodal condition.\\n\\n---\\n\\n**Scene:** \u2026 The microwave contains two cupcakes and a plate\u2026\\n\\n**Actions:** \u2026 She is on the verge of opening the microwave.\\n\\n**Question:** If Elizabeth has been trying to get a water glass, which one of the following statements is more likely to be true?\\n\\n(a) Elizabeth thinks that there is a water glass inside the microwave.\\n\\n(b) Elizabeth thinks that there isn\u2019t any water glass inside the microwave.\\n\\n**Figure 5:** Examples of how BIP-ALM evaluates the likelihood of different hypotheses via the action likelihood estimation from the language model. The results here are based on BIP-ALM with finetuned LLaMA 2. The green option in each example is the correct answer and BIP-ALM selects the correct answers in both cases. The blue panels show the likelihood ratio estimated by the language model at a certain step for each example, explaining how BIP-ALM can come to the correct conclusions by conducting inverse planning via a language model. \\n\\n(a) It is more likely for Elizabeth to open the microwave if she believes that there is a water glass inside the microwave and that she wants to get a water glass, even though there is not any water glass inside the microwave (i.e., she has a false belief).\\n\\n(b) The likelihood of hypothesis will change after the model observes more actions.\\n\\n**B.3 Discussion on SimToM and SymbolicToM**\\n\\nThe two recent approaches evaluated in our benchmark, SimToM (Wilf et al., 2023) and SymbolicToM (Sclar et al., 2023) have previously shown promising results in existing text-based ToM QA benchmarks. However, our experimental results demonstrate that there is still a large gap between their performance and the human performance on our MMToM-QA benchmark. We provide more discussions on these two recent approaches below.\\n\\n**SimToM** utilizes the concept of perspective-taking to filter context based on what the character in question knows before answering a question about their mental state. This approach marginally enhances the performance of GPT-4 on MMToM-QA.\\n\\n**SymbolicToM** (Sclar et al., 2023) constructs symbolic graphical representations of each character\u2019s belief states, retrieves relevant sentences from the graph, and feeds them into an LLM to answer a given ToM question. As questions in MMToM-QA include only one person and exclude high-order beliefs, the method automatically extracts all sentences containing the relevant objects in the questions. This approach of filtering out useful information enhances the performance of LLMs (e.g., GPT-4).\"}"}
{"id": "acl-2024-long-851", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4: Results of baselines with few-shot or chain-of-thought (CoT) prompting\\n\\n| Method                      | Belief Inference | Goal Inference | All     |\\n|-----------------------------|------------------|----------------|---------|\\n| InstructBLIP (w/ 1-shot)    | 42.0             | 49.0           | 53.0    |\\n| InstructBLIP (w/ 2-shot)    | 31.0             | 26.0           | 55.0    |\\n| InstructBLIP (w/ CoT)       | 68.0             | 66.0           | 39.0    |\\n| Video-LLaMA 2 (w/ 1-shot)   | 45.0             | 45.0           | 37.0    |\\n| Video-LLaMA 2 (w/ 2-shot)   | 57.0             | 42.0           | 36.0    |\\n| Video-LLaMA 2 (w/ CoT)      | 30.0             | 13.0           | 43.5    |\\n| LLaVA (w/ 1-shot)           | 6.0              | 4.0            | 2.0     |\\n| LLaVA (w/ 2-shot)           | 3.0              | 3.0            | 2.0     |\\n| LLaVA (w/ CoT)              | 45.0             | 20.0           | 61.0    |\\n| GPT-4V (w/ 1-shot)          | 91.0             | 10.0           | 81.0    |\\n| GPT-4V (w/ 2-shot)          | 85.0             | 7.0            | 54.0    |\\n| GPT-4V (w/ CoT)             | 95.0             | 16.0           | 54.0    |\\n| GPT-4 with captions (w/ 1-shot) | 94.0         | 22.0           | 55.0    |\\n| GPT-4 with captions (w/ 2-shot) | 95.0         | 14.0           | 89.0    |\\n| GPT-4 with captions (w/ CoT) | 59.0             | 49.0           | 78.0    |\\n| GPT-4                        | 97.0             | 12.0           | 77.0    |\\n| GPT-4 (w/ 1-shot)           | 99.0             | 40.0           | 86.0    |\\n| GPT-4 (w/ 2-shot)           | 99.0             | 39.0           | 96.0    |\\n| GPT-4 (w/ CoT)              | 97.0             | 13.0           | 82.0    |\\n| GPT-3.5                     | 81.0             | 11.0           | 39.0    |\\n| GPT-3.5 (w/ 1-shot)         | 100              | 25.0           | 13.0    |\\n| GPT-3.5 (w/ 2-shot)         | 100              | 16.0           | 16.0    |\\n| GPT-J                       | 56.0             | 53.0           | 38.0    |\\n| GPT-J (w/ 1-shot)           | 44.0             | 47.0           | 54.0    |\\n| GPT-J (w/ 2-shot)           | 44.0             | 47.0           | 54.0    |\\n| GPT-J (w/ CoT)              | 59.0             | 58.0           | 41.0    |\\n| LLaMA 2 (w/ 1-shot)         | 66.0             | 65.0           | 31.0    |\\n| LLaMA 2 (w/ 2-shot)         | 48.0             | 51.0           | 50.0    |\\n| LLaMA 2 (w/ CoT)            | 56.0             | 53.0           | 46.0    |\\n\\nOn the other hand, the performance increase of SymbolicToM on MMToM-QA is not as significant as on the ToMi benchmark. The ToMi benchmark only has simple scenarios with a few objects and locations, and very short action sequences. In contrast, MMToM-QA demands the inference of mental states from long human activities in complex environments. Given the observations, the inference in MMToM-QA also has a varying degree of uncertainty. Therefore, in contrast to achieving...\"}"}
{"id": "acl-2024-long-851", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results of baselines with different model sizes.\\n\\n| Method          | Belief Inference | Goal Inference | All       |\\n|-----------------|------------------|----------------|-----------|\\n|                 | 1.1 1.2 1.3     | 2.1 2.2 2.3 2.4 |\\n| MM              |                 |                |           |\\n| InstructBLIP (7B) | 49.0 53.0 62.0 | 52.0 48.0 50.7 | 50.8      |\\n| InstructBLIP (13B) | 62.0 52.0 32.0 | 46.7 29.3 42.7 | 46.7      |\\n| Video-LLaMA 2 (7B) | 21.9 12.5 25.0 | 21.4 10.5 17.7 | 18.2      |\\n| Video-LLaMA 2 (13B) | 36.0 38.0 52.0 | 36.0 41.3 30.7 | 40.2      |\\n| LLaMA 2 (7B)    | 64.0 55.0 50.0 | 49.3 48.0 41.3 | 50.3      |\\n| LLaMA 2 (13B)    | 44.0 47.0 54.0 | 48.0 37.3 49.3 | 48.5      |\\n\\nTable 6: Effect of finetuning baseline models on our training set. We compare the finetuned Video-LLaMA 2, i.e., Video-LLaMA 2 (FT), and the pretrained Video-LLaMA 2. Note that the baselines here are multimodal models. So they are evaluated in the multimodal (MM) condition and the video-only condition but not in the text-only condition.\\n\\n| Method          | Belief Inference | Goal Inference | All       |\\n|-----------------|------------------|----------------|-----------|\\n|                 | 1.1 1.2 1.3     | 2.1 2.2 2.3 2.4 |\\n| MM              |                 |                |           |\\n| Video-LLaMA 2 (FT) | 61.0 51.0 42.0 | 41.3 44.0 45.3 | 47.5      |\\n| Video-LLaMA 2   | 24.0 32.0 67.0  | 45.3 50.7 56.0 | 50.2      |\\n| Video-LLaMA 2 (FT) | 44.0 58.0 44.0 | 60.0 41.3 56.0 | 50.2      |\\n\\nTable 7: Generalization results on the human test set. All models are the same as the ones evaluated in Table 1. 100% accuracy on the ToMi benchmark, SymbolicToM with GPT-4 has an overall accuracy of 63% on MMToM-QA. This is lower than the accuracy of BIP-ALM which uses much smaller language models. In addition, unlike humans and our BIP-ALM model, SimToM and SymbolicToM cannot answer multimodal or video-only questions, since they are purely text-based methods.\\n\\nB.4 More Details About the Human Experiment\\nEach question in each condition (multimodal, text-only, or video-only) was answered by 5 participants. 180 participants were recruited via Prolific (mean age = 29.7; 65 female). They were paid $12 per hour. The study was approved by an institutional review board. All data collected through Prolific have been anonymized. The instructions and conditions for the human experiment were explained in detail, and participants were asked to fill out a consent form before starting the experiment.\"}"}
{"id": "acl-2024-long-851", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the apartment: The kitchen is equipped with a microwave, eight cabinets, a sink, and a refrigerator. Inside the microwave, there is a cupcake.\"}"}
{"id": "acl-2024-long-851", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Types of data provided in our benchmark.\\n\\nFigure 8: Context length of MMToM-QA.\\n\\nApartment & initial state\\nGoal: get a plate\\n\\nVirtualHome-Social Simulator\\nPlanner Observation Action\\n\\nState: In(agent, bedroom) In(plate, cabinet) Closed(cabinet) \u2026\\nAction: Walk towards kitchen\\n\\nVideo frames\\nGround-truth state and action\\n\\nState: In(agent, kitchen) In(plate, cabinet) Closed(cabinet) \u2026\\nAction: Walk towards kitchen cabinet\\n\\nHypotheses after t = 1\\nGoal: water glass, Belief: [kitchen table, cabinet, \u2026]\\nGoal: cupcake, Belief: [fridge, microwave, \u2026]\\nGoal: apple, Belief: [fridge, kitchen table\u2026]\\nGoal: plate, Belief: [kitchen table, kitchen cabinet, \u2026]\\n\\nHypotheses after t = 2\\nGoal: water glass, Belief: [kitchen table, cabinet, \u2026]\\nGoal: cupcake, Belief: [fridge, microwave, \u2026]\\nGoal: apple, Belief: [fridge, kitchen table\u2026]\\nGoal: plate, Belief: [kitchen table, kitchen cabinet, \u2026]\\n\\nIdeal Observer\\n\\nStep 1: Video Synthesis\\nStep 2: Question Sampling\\nStep 3: Text Generation\\n\\nH1: Goal: plate, Belief: IN(Plate, Cabinet)\\nH2: Goal: plate, Belief: NOT IN(Plate, Cabinet)\\n\\nGPT-4\\nState context\\nAction context\\nQuestion and two options\\n( for a question at t = 2)\"}"}
{"id": "acl-2024-long-851", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It also has access to the planner of the agent and will evaluate each remaining goal and belief hypothesis pair by simulating actions conditioned on the hypothesis. If the simulated actions are consistent with the actual actions taken by the agent, we then further eliminate that hypothesis. Finally, to generate a question of a certain type at a given step, we sample two hypotheses that fit the design of the question type, with one from the set of possible hypotheses produced by the ideal observer model and the other being a hypothesis ruled out by the model.\\n\\nAt any given moment in a video, we ensure that our benchmark poses at most one question. To achieve this, we randomly select one question from the pool of possible questions at every step. We further sample a subset of the remaining questions to achieve a balanced distribution of question types.\\n\\n### B.8 Utilizing GPT-4 for Enhanced Text Generation\\n\\nWe first translate the symbolic representations of state and action descriptions into natural language using simple templates. We then use GPT-4 to enhance the phrasing and diversify the expression. The prompts are provided as follows.\\n\\n#### Improving state descriptions\\n\\nWe are describing where things are in an apartment. Please improve the grammar of the description without changing the meaning. Please use only one line break to separate descriptions about each room.\\n\\nOriginal: There is a kitchen and a living room. 4 kitchencabinets are inside the kitchen. There is nothing inside the 1st kitchencabinet from left to right. There is nothing inside the 3rd kitchencabinet from left to right. There is nothing inside the 2nd kitchencabinet from left to right. A waterglass and a wineglass and 2 dishbowls are inside the 4th kitchencabinet from left to right.\\n\\nImproved: There is a kitchen and a living room. The kitchen has four cabinets. The first, second, and third cabinets, from left to right, are empty. The fourth cabinet, from the left, contains a water glass, a wine glass and two dish bowls.\\n\\n#### Improving action descriptions\\n\\nPlease improve the following descriptions about a person's actions without changing the meaning.\\n\\nOriginal: [name] is in the kitchen, walktowards stove.\\n\\nImproved: [name] is in the kitchen. [name] walks towards the stove.\\n\\nOriginal: [name] is in the livingroom, walktowards kitchen, walktowards 1st kitchencabinet, open 1st kitchencabinet, close 1st kitchencabinet.\\n\\nImproved: [name] is in the living room. [name] walks to the kitchen, approaches the first cabinet, opens it, and then closes it.\\n\\n#### C BIP-ALM Implementation Details\\n\\n### C.1 Visual Perception\\n\\nWe adopt Blukis et al. (2022) to obtain a voxel map for each frame. Specifically, we first get the instance segmentation map from the RGB image. Combined with the depth map and the camera data, we create a 3D point cloud, with each point representing the pixel on the instance segmentation map. We group the 3D point cloud into a voxel map and then estimate the 3D bounding boxes of the objects. We can also estimate the 3D human pose in each frame. In the current experiments, we use ground-truth instance segmentation maps and ground-truth 3D human poses. In future work, we plan to also evaluate our model on segmentation and pose estimation results acquired from off-the-shelf computer vision models.\\n\\nUsing the 3D bounding boxes and the 3D human pose, we construct a scene graph. Each node in the graph is either an object or a person. For nodes representing containers, we indicate whether they are open or closed (which in our scenarios can be detected from the change in the sizes of their bounding boxes). There are two types of edges in a graph \u2013 (1) inside edges indicating containment and (2) close edges indicating proximity. Note that our BIP-ALM model is not restricted to these relationships and can be applied to broader scenarios with more types of spatial relationships.\"}"}
{"id": "acl-2024-long-851", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Utilizing GPT-4, we parse the provided question to extract representations about the state context, action context, question, and the two options. In this subsection, we detail all the prompts used.\\n\\nTo extract representations regarding the state context, we use the following prompt:\\n\\n**State Extraction**\\nPlease extract the description of the rooms and where things are in an apartment, found after the phrase \\\"What's inside the apartment\\\" and before the description of a person's actions. Keep the line breaks.\\n\\n**Input:** {question}\\n\\n**Extracted:**\\n\\n**State Parsing**\\nPlease parse the following description about where things are in an apartment. Each sentence should follow the pattern '[something] is/are in/on the [somewhere].' Use a '.' to separate the sentences, and keep the original line breaks.\\n\\nOriginal: The living room contains a sofa, a desk, a cabinet, and a coffee table, and the cabinet holds chips, a wine glass, and an apple.\\n\\nParsed: A sofa, a desk, a cabinet and a coffee table are in the living room. Chips, a wine glass and an apple are in the cabinet.\\n\\nOriginal: The kitchen has an oven, a microwave, and four cabinets. The oven contains a salmon, the microwave holds a cupcake, the third cabinet from the left has a wine glass, the fourth cabinet is empty. The first and second kitchen cabinets each holds a plate.\\n\\nParsed: An oven and a microwave and 4 kitchen cabinets are in the kitchen. A salmon is in the oven. A cupcake is in the microwave. A wine glass is in the 3rd kitchen cabinet. Nothing is in the 4th kitchen cabinet. A plate is in the 1st kitchen cabinet. A plate is in the 2nd kitchen cabinet.\\n\\nTo parse the human actions, we employ the following prompt:\\n\\n**Action Extraction**\\nPlease extract the exact description of a person's actions (starting from the initial location), found after the phrase \\\"[someone]'s action\\\" and before the question. Please do not include the question, choices, or the answer.\\n\\n**Input:** {question}\\n\\n**Extracted:**\\n\\n**Action Parsing**\\nPlease parse the description of a person's actions. Use a '.' to separate each action, and remove all occurrences of the word 'and' in the description.\\n\\nOriginal: Jennifer is in the bedroom. She proceeds to the kitchen and strides towards the oven, preparing to open it.\\n\\nParsed: In the bedroom. walk to kitchen. walk towards oven. about to open oven.\\n\\nOriginal: Mark is in the bathroom. He then walks to the kitchen. He sequentially approaches the oven, the second, and third kitchen cabinets, opening and closing each one in turn.\\n\\nParsed: In the bedroom. walk to kitchen. walk towards oven. open oven. close oven. walk towards 2nd kitchen cabinet. open 2nd kitchen cabinet. close 2nd kitchen cabinet. open 3rd kitchen cabinet.\\n\\nTo parse and analyze the question, we prompt GPT-4 to determine if a question falls under the \\\"Belief Inference\\\" or \\\"Goal Inference\\\" category, and extract all the hypothetical beliefs, hypothetical goals, and conditions in the question:\"}"}
{"id": "acl-2024-long-851", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Question Parsing:\\nPlease determine the type of inference for the input question: either \u201cBelief Inference\u201d, which inquires about a person\u2019s belief regarding an object, or \u201cGoal Inference\u201d, which seeks to understand a person\u2019s objective.\\n\\nIf a question falls under the \u201cBelief Inference\u201d, please identify the [object] and the [container] that the object may or may not be inside in choices (a) and (b).\\n\\nIf a question falls under the \u201cGoal Inference\u201d, please identify the two possible objects that the person is looking for in choices (a) and (b). If the input contains a statement indicating that someone believes there isn\u2019t an [object] inside a [container], please also identify both the [object] and the [container] mentioned. Otherwise, return \u2018NaN.\u2019\\n\\nInput: ... (detailed descriptions about states and actions) ... If Elizabeth has been trying to get a plate, which one of the following statements is more likely to be true? (a) Elizabeth thinks that there is a plate inside the fridge. (b) Elizabeth thinks that there isn\u2019t any plate inside the fridge.\\nOutput: Belief Inference. plate, fridge.\\n\\nInput: ... (detailed descriptions about states and actions) ... If Jennifer has been trying to get a plate, which one of the following statements is more likely to be true? (a) Jennifer thinks that there is a salmon inside the oven. (b) Jennifer thinks that there isn\u2019t any salmon inside the oven.\\nOutput: Belief Inference. salmon, oven.\\n\\nInput: ... (detailed descriptions about states and actions) ... Which one of the following statements is more likely to be true? (a) Mark has been trying to get a plate. (b) Mark has been trying to get a cupcake.\\nOutput: Goal Inference. plate, cupcake.\\n\\nNaN.\\n\\nInput: ... (detailed descriptions about states and actions) ... If Mary thinks there isn\u2019t an apple inside the microwave, which one of the following statements is more likely to be true? (a) Mary has been trying to get an apple. (b) Mary has been trying to get a bottle of wine.\\nOutput: Goal Inference. apple, wine. apple, microwave.\\n\\nInput: {question}\\nOutput: C.3 Representation Fusion\\n\\nFigure 10 illustrates how BIP-ALM fuse complementary information extracted from the video and the text to form unified symbolic representations. In particular, the orange predicates can only be extracted from the video and the blue predicates can only be extracted from the text. After merging predicates extracted from both modalities, we can then construct a full state sequence.\\n\\nWhen fusing information from different inputs, we may encounter conflicting predicates. In the case of conflict, we only keep the predicates from the text and remove the contradictory predicates from the video. This is because the predicates from the video are more likely to be erroneous due to noisy visual perception. However, more broadly speaking, such conflict-resolving mechanisms can be calibrated by the reliability of different modalities in a given domain.\\n\\nC.4 Belief Representation and Update\\nA person\u2019s belief consists of beliefs about the locations of individual objects as illustrated in Figure 11. At each step, we estimate the person\u2019s observation and update the estimated belief accordingly \\\\( \\\\hat{b}_t \\\\). Then for each step, we construct a symbolic representation of the belief about each object as a list of possible locations of the object that have non-zero probabilities in the belief.\\n\\nNote that for representing the negation of a predicate, we can simply remove the location from the list of possible locations in the symbolic belief representation.\\n\\nC.5 Prompt for Language Models in Inverse Symbolic Planner\\nAs introduced in the main paper, we employ a language model (either GPT-J or LLaMA 2) to amortize the policy and estimate the likelihood of the\"}"}
{"id": "acl-2024-long-851", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Closed(fridge)\\nClosed(kitchen cabinet)\\n\\nFrom Video\\nClose(agent, fridge)\\nClosed(fridge)\\nClosed(kitchen cabinet)\\n\\nFigure 10: Illustration of fusing multimodal representations.\\n\\nBelief about apple at t+1\\nBelief about plate at t+1\\n\\nFigure 11: Illustration of estimated belief update.\\n\\nC.6 Training Details\\nIn the experiments, we finetuned GPT-J (6B) and LLaMA 2 (7B) for our BIP-ALM method. Adhering to our evaluation protocol, we employed the ground-truth state, belief, goal, and action from 1,000 training videos, excluding the use of any example QAs. The models were trained using the state, belief, and goal as inputs at specific timestamps, with the aim of predicting the corresponding action. This approach enhanced the Inverse Symbolic Planner's ability to estimate the likelihood of human actions at a specific moment conditioned on the hypothesis about the goal and belief. The training data comprised 20,000 samples, consistent with the input and output of the Inverse Symbolic Planner defined in Appendix C.5.\\n\\nFor finetuning, we incorporated the Low-Rank Adapters (LoRA) method (Hu et al., 2021).\"}"}
{"id": "acl-2024-long-851", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"training process leveraged the AdamW optimizer, with a set learning of $5 \\\\times 10^{-5}$ and a batch size of 4. We trained both models for 5 epochs, which took about 20 GPU hours on a single A100 GPU.\\n\\nD Implementation Details of Baselines\\n\\nIn all baselines, we use gpt-4-0613 for GPT-4 and text-davinci-003 for GPT-3.5. We sample a few frames from each video for LMMs, following the standard settings. In particular, we sample 16 frames, 30 frames, 6 frames, and 8 frames from each video for InstructBLIP, Video-LLaMA 2, LLaV A, and GPT-4V respectively. We use Vicu\u00f1a-13B for InstructBLIP and LLaMA-2-13B-Chat for Video-LLaMA 2.\\n\\nWe followed Zhang et al. (2023) to create a video instruction dataset using our training data to finetune Video-LLaMA 2. In particular, based on our training videos and the ground-truth annotations, we generated 7,088 training examples for describing either the environment or the actions of the person in a given video clip. For each video clip, we sampled 8 frames and used the ground-truth scene graph at the last frame or the action sequence during the whole clip to generate the ground-truth descriptions for the training data. We then finetuned the Video-LLaMA 2 (13B) model on our training data on 2 A100 GPUs.\\n\\nE Full Version of the Example Questions in Figure 2\\n\\nE.1 Type 1.1 Example\\n\\nThe video input: [Link to video]\\n\\nThe text input:\\n\\nWhat's inside the apartment: The apartment consists of a bedroom, kitchen, living room, and bathroom.\\n\\nIn the bedroom, there is a coffee table and a desk, with a remote control resting on the coffee table.\\n\\nThe kitchen is equipped with four cabinets, a fridge, a kitchen table, a microwave, and an oven. The first and third cabinets, from left to right, are empty, while the second cabinet houses a condiment bottle. The fourth cabinet contains a water glass. Inside the fridge, you'll find a bottle of wine, a dish bowl, and two plates. The microwave holds a cupcake, and the oven contains a salmon, two cupcakes, and a plate.\\n\\nThe living room features a cabinet, a sofa, a coffee table, and a desk. The cabinet is filled with two plates, a bottle of wine, two wine glasses, a condiment bottle, a water glass, a bag of chips, and an apple. A remote control, a wine glass, and a book are placed on the coffee table.\\n\\nLastly, the bathroom has a cabinet, which is currently empty.\\n\\nActions taken by Elizabeth: Elizabeth is initially in the bathroom. She then proceeds to the kitchen and heads towards the oven. After opening and closing the oven, she moves to the second kitchen cabinet, opens it, and then shuts it. She repeats this action with the third and first kitchen cabinets. Subsequently, she walks towards the fourth kitchen cabinet, opens it, and then closes it. Finally, she moves towards the fridge, preparing to open it.\\n\\nQuestion: If Elizabeth has been trying to get a bottle of wine, which one of the following statements is more likely to be true?\\n\\n(a) Elizabeth thinks that there is a bottle of wine inside the fridge.\\n(b) Elizabeth thinks that there isn\u2019t any bottle of wine inside the fridge.\\n\\nCorrect answer: a.\\n\\nE.2 Type 1.2 Example\\n\\nThe video input: [Link to video]\\n\\nThe text input:\\n\\nWhat's inside the apartment: The apartment consists of a bedroom, kitchen, living room, and bathroom.\\n\\nIn the bedroom, there is a coffee table and a desk, with three wine glasses and a dish bowl placed on the coffee table.\\n\\nThe kitchen is equipped with four cabinets, a fridge, a kitchen table, a microwave, and...\"}"}
{"id": "acl-2024-long-851", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The first, second, and fourth cabinets, from left to right, contain a dish bowl each, while the third cabinet houses a plate. The fridge contains two apples, a dish bowl, and a salmon. The microwave holds two cupcakes, and there is a salmon in the oven.\\n\\nThe living room features a cabinet, a sofa, a coffee table, and a desk. The cabinet is filled with a plate, a bag of chips, a water glass, a remote control, a bottle of wine, and a condiment bottle.\\n\\nLastly, the bathroom has a cabinet, which is currently empty.\\n\\nActions taken by Jennifer:\\nJennifer is situated in the living room. She heads towards the cabinet and is about to open it.\\n\\nQuestion:\\nIf Jennifer has been trying to get a cupcake, which one of the following statements is more likely to be true?\\n\\n(a) Jennifer thinks that there isn't any cupcake inside the cabinet.\\n(b) Jennifer thinks that there is a cupcake inside the cabinet.\\n\\nCorrect answer: b.\\n\\nE.3 Type 1.3 Example\\nThe video input: https://youtu.be/lN810N3KdjM.\\n\\nThe text input:\\nWhat's inside the apartment:\\nThe apartment consists of a bedroom, kitchen, living room, and bathroom. In the bedroom, there is a sofa with a book on it and a cabinet containing a remote control, a wine glass, two dish bowls, a bottle of wine, and a condiment bottle. The kitchen is equipped with a fridge, sofa, dishwasher, eight cabinets, an oven, a microwave, and a kitchen table. The fridge contains an apple, three plates, and a bottle of wine, while a bag of chips rests on the sofa. Inside the dishwasher, there is a plate, a water glass, and a wine glass. The first to the seventh cabinets, from left to right, are all empty. However, the eighth cabinet houses a wine glass. The oven contains a salmon, the microwave is empty, and the kitchen table is adorned with a plate, a wine glass, two apples, two books, and a cupcake.\\n\\nThe living room features a sofa with a water glass and a book on it, and a desk. Lastly, the bathroom has a cabinet, which is currently empty.\\n\\nActions taken by Charles:\\nCharles is in the kitchen. He walks to the seventh kitchen cabinet, opens and closes it. He repeats the same action with the sixth kitchen cabinet. Subsequently, he moves towards the dishwasher.\\n\\nQuestion:\\nIf Charles has been trying to get a salmon, which one of the following statements is more likely to be true?\\n\\n(a) Charles thinks that there is a salmon inside the fridge.\\n(b) Charles thinks that there isn't any salmon inside the fridge.\\n\\nCorrect answer: b.\\n\\nE.4 Type 2.1 Example\\nThe video input: https://youtu.be/NsOPbJWPn1c.\\n\\nThe text input:\\nWhat's inside the apartment:\\nThe apartment consists of a bedroom, a bathroom, a living room, and a kitchen. In the bedroom, there is a coffee table with a plate on it. The bathroom houses a cabinet, which is currently empty. The living room is furnished with a cabinet, a coffee table, a sofa, and a desk. The cabinet is filled with two apples, a condiment bottle, three wine glasses, two water glasses, a cupcake, two bags of chips, a remote control, and a bottle of wine. Both a water glass and a wine glass are placed on the coffee table.\"}"}
{"id": "acl-2024-long-851", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The kitchen is equipped with a fridge, an oven, a kitchen table, and a microwave. Inside the fridge, there are two apples. The oven contains a salmon. Meanwhile, the microwave houses a salmon and two cupcakes.\\n\\nActions taken by James:\\nJames is in the kitchen. He strides towards the stove, opens it, and then shuts it. He then opens the fridge, closes it, opens the microwave, and closes it as well. Finally, he walks towards the living room and approaches the cabinet.\\n\\nQuestion:\\nWhich one of the following statements is more likely to be true?\\n(a) James has been trying to get a bottle of wine.\\n(b) James has been trying to get an apple.\\n\\nCorrect answer: a.\\n\\nE.5 Type 2.2 Example\\nThe video input: https://youtu.be/Fn6s47ZtxMQ.\\nThe text input:\\nWhat's inside the apartment:\\nThe apartment consists of a bedroom, bathroom, living room, and kitchen.\\n\\nIn the bedroom, there is a sofa, a cabinet, a desk, and a coffee table. A book rests on the sofa. The cabinet contains a remote control, three cupcakes, a wine glass, an apple, and a bag of chips. The coffee table holds two books and a dish bowl.\\n\\nThe bathroom houses a single cabinet, which is currently empty.\\n\\nThe living room is furnished with a sofa, a desk, and a coffee table. A dish bowl and a book are placed on the sofa, while a plate sits on the coffee table.\\n\\nThe kitchen is equipped with a dishwasher, an oven, a kitchen table, eight cabinets, a microwave, and a fridge. Inside the dishwasher, there is a dish bowl. The oven contains a salmon and a plate. The second kitchen cabinet from the left holds an apple, while the fourth and fifth cabinets contain two dish bowls and another apple respectively. There is a water glass inside the seventh cabinet. The first, third, sixth, and eighth cabinets are empty. The microwave contains a condiment bottle.\\n\\nThe fridge stores two cupcakes, a dish bowl, a plate, and a bottle of wine.\\n\\nActions taken by Mark:\\nMark is in the kitchen. He then advances towards the seventh kitchen cabinet.\\n\\nQuestion:\\nIf Mark thinks there isn't a water glass inside the 7th kitchen cabinet, which one of the following statements is more likely to be true?\\n(a) Mark has been trying to get a water glass.\\n(b) Mark has been trying to get a cupcake.\\n\\nCorrect answer: b.\\n\\nE.6 Type 2.3 Example\\nThe video input: https://youtu.be/IUJW6Zv0EWA.\\nThe text input:\\nWhat's inside the apartment:\\nThe apartment consists of a bedroom, bathroom, living room, and kitchen.\\n\\nIn the bedroom, there is a cabinet and a sofa. The cabinet contains a condiment bottle, an apple, two wine glasses, and a plate. The sofa holds three books.\\n\\nThe bathroom features a cabinet, which is currently empty.\\n\\nThe living room is furnished with a desk and a sofa, with a book resting on the sofa.\\n\\nThe kitchen is equipped with eight cabinets, a sofa, an oven, a fridge, a kitchen table, a microwave, and a dishwasher. The first kitchen cabinet, from left to right, contains a bag of chips. The second and fourth cabinets are empty. The third cabinet houses a wine glass and a dish bowl. The seventh cabinet stores two plates. The fifth, sixth, and eighth cabinets are empty. The oven contains a cupcake. The fridge holds a plate and a dish bowl. The kitchen table...\"}"}
{"id": "acl-2024-long-851", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"is adorned with an apple, a bottle of wine, a plate, and a water glass. The microwave contains a condiment bottle and a salmon. Lastly, the dishwasher has a water glass inside.\\n\\nActions taken by Mary:\\n\\nMary is situated in the living room. She proceeds towards the kitchen and heads to the second kitchen cabinet. She opens it, then promptly closes it. She then opens the fourth kitchen cabinet and closes it as well. Following this, she opens the dishwasher and closes it. She then moves towards the sixth kitchen cabinet, opens it, and closes it. She repeats this action with the seventh kitchen cabinet. Finally, she walks towards the first kitchen cabinet, opens it, and then closes it.\\n\\nQuestion:\\n\\nWhich one of the following statements is more likely to be true?\\n\\n(a) Mary has been trying to get a bag of chips.\\n(b) Mary has been trying to get a condiment bottle.\\n\\nCorrect answer: b.\\n\\nE.7 Type 2.4 Example\\n\\nThe video input:\\n\\nhttps://youtu.be/Y4H_9cXR5mw\\n\\nThe text input:\\n\\nWhat's inside the apartment:\\nThe apartment consists of a bedroom, bathroom, living room, and kitchen.\\n\\nIn the bedroom, there is a sofa, a cabinet, a desk, and a coffee table. A book rests on the sofa. The cabinet houses an apple, a wine glass, two books, and two cupcakes. The coffee table holds a book, a water glass, a wine glass, and a remote control.\\n\\nThe bathroom contains a single cabinet, which is currently empty.\\n\\nThe living room is furnished with a sofa, a coffee table, and a desk. A water glass sits on the sofa, and a remote control is on the coffee table.\\n\\nThe kitchen is equipped with eight cabinets, a microwave, a fridge, a dishwasher, a kitchen table, and an oven. The fourth and seventh cabinets from the left, as well as the eighth, are empty. The microwave contains a salmon, a cupcake, and a condiment bottle. The fridge is stocked with two bottles of wine, a cupcake, an apple, and two dish bowls. The dishwasher holds a dish bowl, a wine glass, and a plate. The second cabinet from the left contains a water glass. The first cabinet from the left holds a bag of chips and a wine glass. The fifth cabinet has an apple, and the third cabinet contains a condiment bottle. The sixth cabinet is empty. Lastly, there is a salmon in the oven.\\n\\nActions taken by William:\\n\\nWilliam is situated in the kitchen. He advances towards the first kitchen cabinet, opens it, and then shuts it. Finally, he moves towards the fifth kitchen cabinet.\\n\\nQuestion:\\n\\nWhich one of the following statements is more likely to be true?\\n\\n(a) William has been trying to get a wine glass.\\n(b) William has been trying to get a dish bowl.\\n\\nCorrect answer: b.\"}"}
