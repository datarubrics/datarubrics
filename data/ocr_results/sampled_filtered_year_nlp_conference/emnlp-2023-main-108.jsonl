{"id": "emnlp-2023-main-108", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Generation-based Deductive Method for Math Word Problems\\n\\nYuxuan Hu1,2, Jing Zhang1,3\u2217, Haoyang Li1,3, Cuiping Li1,3, ..., Hong Chen1,3\\n\\n1 School of Information, Renmin University of China, Beijing, China\\n2 Key Laboratory of Data Engineering and Knowledge Engineering, MOE, China\\n3 Engineering Research Center of Database and Business Intelligence, MOE, China\\n\\n{huyuxuan1999,zhang-jing,lihaoyang.cs,licuiping,chong}@ruc.edu.cn\\n\\nAbstract\\n\\nMath word problems (MWP) involving advanced operators such as linear equation solver cannot be easily tackled by earlier MWP methods, because the existing generation methods suffer from repeated sub-expression generation and deductive methods are restricted to dealing with binary operations. This paper proposes a new multivariate directed acyclic graph (mDAG) as an alternative to the generation methods' binary expression tree or the deductive methods' binary directed acyclic graph. Then to produce the topological ordering of mDAG, we propose a generation-based deductive (GeDe) model, which equips a generation model with a re-encoder to keep the deductive property but avoid the expensive enumeration of the deductive methods. GeDe performs well on math problems with many operators on the widely used benchmarks as well as solving multivariate operators on our own CMWPA benchmark. Our code is available at https://github.com/hyx1999/GeDe\\n\\n1 Introduction\\n\\nSolving Math Word Problems (MWPs) is the task of answering natural language problems that require mathematical reasoning ability (Bobrow, 1964). To achieve such a skill, researchers have proposed a variety of MWP solvers, each of which seeks to produce a specific logic form that can be used to calculate the answer to the problem. Deductive methods and generation-based methods are typically the two main approaches used to solve MWPs. Inspired by advances in machine translation, some generation-based methods directly adopt a sequence-to-sequence (seq2seq) model to generate the sequence of the math expression according to the problem (Wang et al., 2017). To further capture the structure of the math expression, some sequence-to-tree (seq2tree) methods (Xie and Sun, 2019) adopt a tree decoder to generate the binary expression tree, where each node denotes an operator or a quantity. These generation-based methods, however, suffer from a fatal flaw in that they require repeated generation of the same sub-expression (or sub-tree), which makes them inefficient. For example, in Figure 1 (a), the sub-expression $(94 - 35 \\\\times 2) \u00f7 (4 - 2)$ is generated four times. Humans, on the other hand, can represent repeated sub-expressions with an intermediate quantity that can be naturally reused in the following computation process.\\n\\nDeductive approaches (Cao et al., 2021; Jie et al., 2022) are suggested to address the aforementioned reuse issue. Specifically, deductive methods convert the math expression into a binary Directed Acyclic Graph (bDAG), where each node represents an operation that consists of a binary operator and two input quantities. The calculation result of an operation is represented by a new intermediate quantity. Then, these methods need to generate a topological ordering, i.e., an operation sequence, of the bDAG. By doing this, subsequent operations can easily reuse the previously generated intermediate quantities. As shown in Figure 1 (b), quantity $q_3$ represents the sub-expression $(94 - 2 \\\\times 35) \u00f7 (4 - 2)$, which is then reused by two subsequent operations denoted by quantity $q_4$ and $q_8$. When the operation sequence is inferred, these operations are computed consecutively to produce the final answer. Beyond the ability to reuse the intermediate quantity, deductive methods are more interpretable because the step-by-step generation of operations helps people understand how the reasoning works. To generate the operation at each reasoning step, existing deductive methods follow an \u201cenumerate-then-classify\u201d procedure. To be more precise, they create a collection of candidate operations by listing every possible combination of the quantities and operators, and then they use a classifier to choose the operation that has the highest probability, which can be viewed as a greedy search strategy.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pre-order traversal of the above binary expression tree:\\n\\n\\\\[ \\\\frac{94 - 2 \\\\times 35}{4 - 2} \\\\times \\\\left( \\\\frac{94 - 2 \\\\times 35}{4 - 2} \\\\right) + (35 - 1 \\\\times (\\\\frac{94 - 2 \\\\times 35}{4 - 2})) \\\\div 1 \\\\times (35 - 1 \\\\times (\\\\frac{94 - 2 \\\\times 35}{4 - 2})) \\\\div 1 \\\\]\\n\\nOne topological ordering of the above bDAG:\\n\\n\\\\[ q_0, q_1 = \\\\text{linear equation solver} (1, 1, 4, 2, 35, 94) \\\\]\\n\\nOne topological ordering of the above mDAG:\\n\\n\\\\[ q_0, q_1 = q_0 \\\\times q_1 \\\\]\\n\\nMathematical Expression:\\n\\n\\\\[ \\\\left( \\\\frac{94 - 2 \\\\times 35}{4 - 2} \\\\right) \\\\times \\\\left( \\\\frac{94 - 2 \\\\times 35}{4 - 2} \\\\right) + (35 - 1 \\\\times (\\\\frac{94 - 2 \\\\times 35}{4 - 2})) \\\\div 1 \\\\times (35 - 1 \\\\times (\\\\frac{94 - 2 \\\\times 35}{4 - 2})) \\\\div 1 \\\\]\\n\\nQuestion:\\n\\nThere are several chickens and rabbits in a cage. Inside, we observe 94 feet and 35 heads. A chicken has 1 head and 2 feet. A rabbit has 1 head and 4 feet. The number of rabbits and chickens are denoted by \\\\( x \\\\) and \\\\( y \\\\), respectively. Tell me the value of \\\\( x \\\\times x + y \\\\times y \\\\).\\n\\nAnswer:\\n\\n673\"}"}
{"id": "emnlp-2023-main-108", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2 Related Work\\n\\n2.1 Math Word Problem\\n\\nEarly efforts to solve MWPs use rule-based approaches, which are only able to address a limited number of MWP scenarios (Kushman et al., 2014; Liguda and Pfeiffer, 2012; Roy and Roth, 2018). Deep learning models, on the other hand, are better capable of addressing a wider range of MWPs. The first seq2seq model for MWPs is proposed by Wang et al. (2017). This model employs RNN to encode the problem and produce mathematical expressions. To enhance the seq2seq model, additional techniques have been developed, including reinforcement learning (Huang et al., 2018), template-based methods (Wang et al., 2019), and group attention mechanisms (Li et al., 2019). Seq2tree, a tree structure decoder, is developed by Xie and Sun (2019). It replaces the original sequence decoder and greatly outperforms seq2seq models in terms of performance. KA-S2T (Wu et al., 2020) and MWP-BERT (Liang et al., 2022) inject commonsense knowledge and quantities' properties to improve model performance. In order to encode the relationships between quantities in MWPs, Graph2tree (Li et al., 2020; Zhang et al., 2020) encodes the input problem using graph neural networks.\\n\\nIn addition to the generation models with seq2seq, seq2tree, or graph2tree structures, other efforts use deductive methods to solve MWPs step by step rather than directly generating the entire expression. Cao et al. (2021) represent the calculation process by bDAG and extract the bDAG structure by aggregating quantities and sub-expressions iteratively. Jie et al. (2022) view the task as a complex relation extraction problem and predict the relation of two quantities gradually. Compared with generation methods, deductive methods can easily employ the intermediate values to avoid repetitive generation. We expand the deductive methods to handle more complex advanced operators.\\n\\n2.2 Large-scale Pre-trained Language Model\\n\\nIn-context few-shot learning or even zero-shot learning based on large-scale pre-trained language models, such as GPT-3 (Brown et al., 2020), PaLM (Chowdhery et al., 2022), and OPT (Zhang et al., 2022), has been thoroughly studied for multiple tasks, including math word problem solving (Cobbe et al., 2021; Wang et al., 2022; Wei et al., 2022). This tuning-free methods have achieved promising performance, and their success mainly relies on the reasoning power of large-scale PLMs. However, the reasoning power is extremely expensive due to the large number of parameters, massive pre-training data, carefully designed pre-training objectives, and huge overhead of computational resources. In contrast, we investigate fine-tuning the small models.\\n\\n3 Problem Definition\\n\\nThe goal of MWP is to generate a specific logic form that can be executed to answer the problem $P = \\\\{p_1, p_2, \\\\ldots, p_n\\\\}$ which consists of $n$ tokens and $m$ quantity tokens $Q = \\\\{q_1, q_2, \\\\ldots, q_m\\\\}$. Some commonsense constants, such as $\\\\pi$ and $e$, may not explicitly appear in the problem; thus, we additionally add them to the quantity set $Q$.\\n\\nIn this paper, we define the multivariate directed acyclic graph (mDAG) as our target logic form, which describes the process of solving MWPs. The nodes of mDAG denote operations that consist of an operator and multiple quantities, and the edges represent the dependency between nodes. Our goal is to generate an operation sequence $O = (o_1, o_2, \\\\ldots, o_{|O|})$ which can be obtained from the topological ordering of mDAG. $|O|$ is the number of operations. The $t$-th operation is a sequence of tokens $o_t = (a_{t1}, a_{t2}, \\\\ldots, a_{t|o_t|})$ with each token representing an operator or a quantity. Each operator is selected from the operator set $V$, which is predefined by the provided dataset. Each quantity is chosen from $Q$, which is initialized with the $m$ quantity tokens in $P$ and can gradually grow as the steps of reasoning progress. $|o_t|$ is the number of tokens of the $t$-th operation.\\n\\n4 Approach\\n\\n4.1 Overview\\n\\nIn general, the proposed GeDe method consists of two main components: the re-encoder and decoder. The former aims to jointly encode the problem and quantities, which can support the reuse of intermediate quantities. The latter is designed to generate an operation according to the output of the re-encoder. Since our target is an operation sequence, we need to perform multiple reasoning steps, with each step generating an operation. We illustrate the reasoning process in Figure 2. At each step...\"}"}
{"id": "emnlp-2023-main-108", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are several chickens and rabbits in a cage. Inside, we can observe $q!$ feet and $q\\\"$ heads. A chicken has $q#$ head and $q$\\\" feet. A rabbit has $q%$ head and $q&$ feet.\\n\\n**Reasoning step 1**\\n\\nProblem $P_r$\\n\\n**Reasoning step 2**\\n\\n$R_m$: $(R!, Q!): [q!, \\\\ldots, q%; q! \\\\times q! = \\\\text{linear equation solver}(q#, q! q$\\\\\\n\\n$P_r': \\\\text{Concat}(P'(o'), o')$\\n\\nNote: The specific quantities in the original problem $P$ are replaced by $q! \\\\sim q&$. $\\ldots$ update inputs\\n\\n$o!$: ...\\n\\nFigure 2: Illustration of iteratively generating the operation sequence by the proposed GeDe. At each reasoning step, GeDe re-encodes the input by adding new intermediate quantities and then generates a new operation. In the next reasoning step, we update the input sequence by adding new intermediate quantities generated in the previous step. The updated input sequence is fed into the re-encoder and the decoder to generate an operation. The generation process is equipped with a hierarchical beam search strategy to enable both token-level beam search within an operation and operation-level beam search in the whole operation sequence.\\n\\n**4.2 Re-Encoder**\\n\\nThis section delves into the re-encoder by explaining the input and the encoder respectively. Since we are only interested in the semantics of the quantities rather than their precise values, we first substitute each quantity in the original problem $P$ with a general special token, $[Q_{TT}i]$. This leaves $P_r$ devoid of any specific quantities. In order to obtain the encoder's input sequence, $P_t$ in, we concatenate $P_r$ with all intermediate quantities, where each quantity signifies its corresponding operation. We take the example in Figure 2 to explain the input. The given math problem contains six quantities, which are replaced by $[Q_{TT}0]$ to $[Q_{TT}5]$. At reasoning step $t$, we have already generated the following operation:\\n\\n$$[LES][Q_{TT}2][Q_{TT}4][Q_{TT}3][Q_{TT}5][Q_{TT}0][Q_{TT}1]$$\\n\\n(1)\\n\\nwhere LES stands for a multivariant operator of linear equation solver given the operands of a matrix made up of $[Q_{TT}2], [Q_{TT}3], [Q_{TT}4], [Q_{TT}5]$ and a vector made up of $[Q_{TT}0] and $[Q_{TT}1]$. In practice, the operation is represented by a sequence that expands the matrix and vector by row. Then we denote the outputs of this operation by two new quantities $[Q_{TT}6]$ and $[Q_{TT}7]$ and concatenate the sequence $[Q_{TT}6][Q_{TT}7][=][LES][Q_{TT}2][Q_{TT}4][\\\\ldots][Q_{TT}1]$ (2) with the original input $P_r$ to obtain $P_t$.\\n\\nWe instantiate the re-encoder $M_E$ by a PLM (e.g., BERT or GPT) to represent the input sequence and obtain the reasoning state, i.e., $R_t = M_E(P_t)$, (3) where $R_t \\\\in \\\\mathbb{R}^N \\\\times H$ represents the reasoning state at step $t$. $N$ denotes the length of the input sequence and $H$ denotes the hidden size. For the subsequent generation module, we extract the representation of each quantity from $R_t$ according to their positions in $P_t$ in:\\n\\n$$Q_t = \\\\{R_t[i] | i \\\\in I_q\\\\}$$\\n\\n(4)\\n\\nwhere $Q_t \\\\in \\\\mathbb{R}^M \\\\times H$, $M$ denotes the number of quantities, $I_q$ saves the indexes of all the quantities in $P_t$ in, and $R_t[i]$ denotes the $i$-th row of $R_t$.\\n\\nIn summary, the original input is re-encoded with the previously generated intermediate quantities at each reasoning step to update the reasoning state and record all intermediate quantities, which may be reused in the subsequent generation process.\\n\\n**4.3 Decoder**\\n\\nWe adopt a Gated Recurrent Unit (GRU) network (Chung et al., 2014) combined with the attention mechanism (Vaswani et al., 2017) as the decoder $M_D$. Following the majority of the earlier works (Liang et al., 2022; Tan et al., 2021; Xie and Sun, 2019), we choose GRU instead of transformer for a fair comparison. Although some works choose pre-trained transformer (Shen et al., 2021), their performance might not be improved due to the larger parameters but limited labeled data.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Operation Generation. The decoder aims to provide an operation $o_t = (a_{t1}, a_{t2}, ..., a_{t|o_t|})$ at each reasoning step $t$. To enable the auto-regressive generation, we insert a special beginning token $[BOS]$ before the first token $a_{t1}$ and add a special ending token ($[EOS]$ or $[EOO]$) after the last token $a_{t|o_t|}$ to re-create $o_t = (a_0, a_{t1}, a_{t2}, ..., a_{t|o_t|+1})$.\\n\\nWhile $[EOS]$ only signifies the termination of the current operation, $[EOO]$ stands for the final token of the complete operation sequence. The hidden state $h_t$ of each token $a_i$ can be obtained by $h_t = GRU(h_{t-1}, a_i)$ where $h_{t-1} \\\\in \\\\mathbb{R}^{1 \\\\times H}$ represents the hidden state of the previous step, $h_0$ is initialized from the hidden state of the $[CLS]$ token produced by the encoder, and $a_i \\\\in \\\\mathbb{R}^{1 \\\\times H}$ is the representation of the token $a_i$.\\n\\nNext, using $h_t$ as the query to attend to current reasoning state $R_t$, we obtain the attention-enhanced state $A_t = MHA(h_t, R_t)$, where $MHA$ denotes multi-head attention (Vaswani et al., 2017). Finally, we determine the likelihood of the output token by determining how well $A_t$ resembles the representation of quantities and operators, i.e.,\\n\\n$$p(a_t | o_{<t}, a_{<i}, P) = \\\\text{softmax}(A_t (V|Q_t)^T),$$\\n\\nwhere $o_{<t}$ represents $o_1, o_2, ..., o_{t-1}$ before reasoning step $t$, $a_{<i}$ represents $a_0, a_{t1}, a_{t2}, ..., a_{t-1}$ before the $i$-th token of step $t$, $|$ is the matrix concatenation operator, $V \\\\in \\\\mathbb{R}^{|V| \\\\times H}$ and $Q_t \\\\in \\\\mathbb{R}^{M \\\\times H}$ denote the representations of operators and $t$-th step's quantities respectively.\\n\\nWhen obtaining a new operation $o_t$, we can determine the number of new quantities by the operator in $o_t$ and record these new intermediate quantities for the subsequent reasoning steps. When $[EOS]$ has the highest probability, the decoding process of the current operation ends but a new operation generation starts instead. When $[EOO]$ has the highest probability, the entire decoding process is complete.\\n\\nTraining Objective. Given a problem $P$ and its ground truth operation sequence $O$, we maximize the probability of generating $O$ by $P$, i.e.,\\n\\n$$p(O | P) = \\\\prod_{t=1}^{T} p(o_t | o_{<t}, a_{<i}, P).$$\\n\\n4.4 Hierarchical Beam Search\\n\\nTo enhance the generation quality during inference, beam search is used in many generation tasks as a refined version of greedy search (Tillmann and Ney, 2003). However, using beam search in the deductive methods is difficult because the search space of the operation sequence is nested. In other words, we need to generate each operation based on tokens and generate the entire operation sequence based on operations. Therefore, previous deductive methods (Cao et al., 2021; Jie et al., 2022) only adopt the greedy search and leave the implementation of the beam search as further work. To address this challenge, we propose a hierarchical beam search strategy. Compared with the traditional beam search, the hierarchical beam search can control the generation process at two levels. Specifically, the hierarchical beam search consists an inner beam search and an outer beam search. The former is a standard beam search which seeks a series of tokens to form a candidate operation. The latter is designed to search a complete operation sequence. The beam score of the inner beam search purely relies on the probabilities of tokens predicted by the decoder. Suppose the $t$-th step generates $l$ tokens, the inner beam score $ibs_t$ is calculated as:\\n\\n$$ibs_t = \\\\log l \\\\prod_{i=1}^{l} p(a_i)^{1/l},$$\\n\\nwhere $p(a_i)$ is computed by Eq (5). We use the inner beam scores of generated operations to approximate the distribution of operations to support the outer beam search. The probability of the $t$-th operation $o_t$ can be calculated as the softmax score of its inner beam score, i.e.,\\n\\n$$p(o_t) = \\\\text{softmax}(\\\\exp(ibs_t)).$$\\n\\nSuppose the entire operation sequence contains $T$ operations, the outer beam score is computed as:\\n\\n$$obs_T = \\\\log(T \\\\prod_{t=1}^{T} p(o_t))^{1/T} = \\\\frac{1}{T} \\\\sum_{t=1}^{T} \\\\log p(o_t).$$\\n\\nAlgorithm 1 presents the hierarchical beam search algorithm. Each outer beam is denoted by the symbol $\\\\text{beam}$, which keeps track of both the current operation sequence and the beam score. The empty operation sequence and score of zero are used to construct the initial outer beam initially (line 1). Then, we iteratively expand outer beams until they are all finished, i.e., all the outer beams are terminated with $[EOO]$ (line 4-14). For\"}"}
{"id": "emnlp-2023-main-108", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1 Hierarchical Beam Search\\n\\nInput: Math World Problem $P$, Beam size $K$\\n\\nOutput: beams with Top-$K$ operation sequences\\n\\n1: $\\\\text{beams} \\\\leftarrow [\\\\text{InitialBeam}];$\\n2: while not all beams are over do\\n3: $\\\\text{beams}_n \\\\leftarrow [];$\\n4: for beam in $\\\\text{beams}$ do\\n5: if beam is over then\\n6: $\\\\text{beams}_n.$append($\\\\text{beam}$);\\n7: else\\n8: $\\\\text{ops} \\\\leftarrow \\\\text{InnerBeamSearch}(P, \\\\text{beam}, K);$ \\n9: for $\\\\text{op}$ in $\\\\text{ops}$ do\\n10: $\\\\text{beam}_{\\\\text{new}} \\\\leftarrow \\\\text{Extend}(\\\\text{beam}, \\\\text{op});$\\n11: $\\\\text{beams}_n.$append($\\\\text{beam}_{\\\\text{new}});$\\n12: end for\\n13: end if\\n14: end for\\n15: $\\\\text{beams} \\\\leftarrow \\\\text{GetTopK}(\\\\text{beams}_n, K);$ \\n16: end while\\n\\nAt each extensible outer beam, we search candidate operations $\\\\text{ops}$ using the inner beam search (line 8). The inner and the outer beam search share the same beam size $K$. Next, we extend outer beams with these candidate operations (line 9-12). At the end of each step, we only maintain the top-$K$ outer beams according to their scores computed by Eq. (9) (line 15). Finally, beams save the top-$K$ operation sequences. We discuss the complexity of GeDe in Appendix A.1.\\n\\n4.5 Decoding Constraint\\n\\nLogic forms need to obey clear grammatical rules. In order to guarantee the validity of the output, we provide two constraint strategies, one during and one after the decoding process. Inspired by PICARD (Scholak et al., 2021), an incremental grammar checker proposed for Text-to-SQL task, the constraint strategy during the decoding process is to filter out illegal beams at each decoding step in the inner beam search to prevent potential syntax errors in the generated operation. For example, when we detect that the current token generation step needs to generate an operator, we will reject all non-operators. Following (Jie et al., 2022), the after decoding constraint strategy eliminates candidate operations that are improbable to exist in real-world mathematical problems, such as \\\"$\\\\left[\\\\text{QTT}_i\\\\right] - \\\\left[\\\\text{QTT}_i\\\\right]\\\" \\\\text{ and } \\\"\\\\left[\\\\text{QTT}_i\\\\right] \\\\left[\\\\text{QTT}_i\\\\right].\\\"\\n\\n5 Experiments\\n\\nIn this section, we establish a dataset for multivariate advanced operators and show that the proposed GeDe is capable of doing these types of operations successfully. We also conduct experiments on four widely-adopted MWP datasets to show the effectiveness of our model on binary operations.\\n\\n5.1 Experimental Setup\\n\\nDatasets. We consider four MWP datasets including our created CMWPA and three widely-used existing MWP datasets: MAWPS (Koncel-Kedziorski et al., 2016), Math23k (Wang et al., 2017), MathQA (Amini et al., 2019), and SV AMP (Patel et al., 2021). We use CMWPA to verify the validity of multivariate operations. Following (Tan et al., 2021), we perform pre-processing to filter out unsolvable problems. In all the datasets, we take into account the basic binary operators addition ($+$), subtraction ($-$), multiplication ($\\\\times$), division ($\\\\div$), and exponentiation ($\\\\hat{\\\\text{}}$). For advanced operators used in the CMWPA dataset, we consider the linear equation solver, the quadratic function extremum solver, and the quadratic function integral solver. Appendix A.2 presents the statistics for each dataset.\\n\\nEvaluation Metric. Following previous work (Jie et al., 2022), we compare the predicted and the gold answer to calculate the accuracy as the evaluation metric. We parse out the operator and operands from the model predicted expression sequence and then use the corresponding operator executor to calculate the answers. We explain the details of the parsing and execution in Appendix A.3.\\n\\nImplementation Details. We adopt RoBERTa-base $^2$ (Liu et al., 2019) as our re-encoder for English datasets, and Chinese-RoBERTa-base $^3$ (Cui et al., 2020) for Chinese datasets. The purpose of using the Roberta model is to make a more fair comparison with previous work. We can also use unidirectional attention models (e.g., GPT). We use AdmaW to optimize the loss function with a learning rate of $2\\\\times10^{-5}$, a weight decay of $1\\\\times10^{-2}$, and a batch size of 8. During inference, the beam size $K$ is set to 4 by default. For CMWPA, Math23K, MathQA, and SV AMP we report accuracy on their test set. For MAWPS and Math23k, we follow previous works and also report 5-fold cross-validation performance. We conduct all experiments with a RTX 3090 (24G) GPU.\\n\\n$^2$https://huggingface.co/roberta-base\\n$^3$https://huggingface.co/hfl/chinese-roberta-wwm-ext\"}"}
{"id": "emnlp-2023-main-108", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Accuracy on three existing MWP datasets (%).\\n\\n| Model                  | MAWPS 5-fold | Math23k 5-fold | MathQA Test Set | SV AMP Test Set |\\n|------------------------|--------------|----------------|-----------------|-----------------|\\n| GroupAttn (Li et al., 2019) | 76.1         | 69.5           | 66.9            | 21.5            |\\n| mBERT+LSTM (Tan et al., 2021) | -            | 75.1           | -               | -               |\\n| RoBERTaGen (Lan et al., 2022) | 88.4         | -              | 76.6            | 30.3            |\\n| Generate&Rank (Shen et al., 2021) | 84.0         | 85.4           | 84.3            | -               |\\n| GTS (Xie and Sun, 2019) | 82.6         | 75.6           | 74.3            | 41.0            |\\n| Graph2Tree (Zhang et al., 2020) | 85.6         | 77.4           | 75.5            | 69.5            |\\n| HMS (Lin et al., 2021) | 80.3         | 76.1           | -               | -               |\\n| MultiE&D (Shen and Jin, 2020) | -            | 78.4           | 76.9            | -               |\\n| BERT-CL (Li et al., 2022) | -            | 82.4           | -               | -               |\\n| MWP-RoBERTa (Liang et al., 2022) | -            | 84.5           | 82.0            | 76.6            |\\n| DR RoBERTa-DR (Jie et al., 2022) | 92.0         | 85.1           | 83.0            | 78.6            |\\n| GeDe                   | 92.3         | 85.4           | 84.2            | 81.5            | 45.7            |\\n\\n5.2 Experiment on CMWPA\\n\\nThe existing MWP datasets only use basic binary operators as target logic form. Rewriting these logic forms to support advanced operators is expensive. Therefore, based on handcraft templates, we create a synthetic dataset named CMWPA (Complex Math Word Problems with Advanced operators).\\n\\nTo create the CMWPA dataset, we first define needed operators which include five binary operators (addition (+), subtraction (\u2212), multiplication (\u00d7), division (\u00f7), and exponentiation (\u02c6)), as well as three advanced operators, which can be used to solve linear equations (the [linear equation solver] operator), find the maximum value of quadratic functions (the [quadratic function extremum solver] operator), and find the definite integrals of quadratic functions (the [quadratic function integral solver] operator). For each operator, we write one or more templates to generate a text description and its operation. We only consider the quadratic function because the operations related to the quadratic function can be transformed to a series of binary operations for training the baseline model. The templates of CMWPA is described in Appendix A.4. In this dataset, for each problem, we provide two types of logic forms: multivariate operation sequence and binary operation sequence. An example is given in Appendix Table 5.\\n\\nWe conduct experiments on CMWPA to demonstrate that using advanced operators to solve complex MWPs is more effective than only using basic binary operators. Concretely, our proposed GeDe is applied to generate multivariate operation sequences. Then for fair comparison, we adopt GeDe to generate binary operation sequence.\\n\\n**Experiment Results.**\\n\\nTable 2 shows the accuracy and inference time on CMWPA, using mDAG as logic form.\\n\\n| Logic Form                | Accuracy | Inference Time |\\n|---------------------------|----------|----------------|\\n| BET                       | 32.0     | 600 ms/per sample |\\n| mDAG                      | 95.0     | 400 ms/per sample |\\n\\nTable 2: Accuracy (%) and time cost on CMWPA of GeDe with different annotation (BET: Binary Expression Tree, mDAG: Multivariate Directed Acyclic Graph).\\n\\nUsing advanced operators to solve MWPs can essentially reduce the learning difficulty and lead to improved both the accuracy and efficiency.\\n\\n5.3 Experiment on Existing MWP Datasets\\n\\nBaselines. The baselines can be broadly categorized into four groups, sequence-to-sequence (S2S), sequence-to-tree (S2T), graph-to-tree (G2T), and deductive-reasoning (DR), where the first three of these are all generation-based methods but are instantiated with different encoders or decoders. We select baselines having reported the performances on at least one of the three datasets.\\n\\nExperiment Results. We start by running tests on MAWPS and Math23k. As shown in Table 1, our model achieves promising performance on both the datasets compared to previous state-of-the-art (SOTA) methods. Given that MAWPS only has an average of 1.41 binary operations, the proposed model shows significant improvement.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GeDe only slightly improves 0.3% accuracy on MAWPS compared to earlier baselines. This is not enough to demonstrate the benefits of the proposed model. On Math23k, GeDe performs equally well as the earlier SOTA method Generate&Rank. However, Generate&Rank fine-tunes a mBART-large (Liu et al., 2020) model with 610M parameters. In contrast, GeDe only involves 126M parameters and thus reveals a better parameter-efficiency.\\n\\nWe further evaluate our method on MathQA, the most challenging MWP dataset with an average of 4.25 binary operations, and show results in Table 1. Our model greatly beats all baselines (+2.9%), which demonstrates the model\u2019s efficacy in handling complex MWPs. In summary, on three existing MWP datasets, the performances of GeDe are on par or better than those of the closest competitors.\\n\\nSV AMP is also a challenging dataset that is manually created to evaluate a model\u2019s robustness. On this dataset, GeDe achieves an accuracy of 45.7%, which can outperform the vast majority of baselines except the DR model.\\n\\nIn addition, we conduct experiments based on Roberta-large on the Math23k dataset. The model achieves an accuracy of 86.7% on the Math23K test set. Using Roberta-large improves the accuracy by 1.3% over using Roberta-base. This shows that using a larger PLM improves the performance of our method and outperforms the baseline Generate & Rank model on the Math23K test set.\\n\\nTo further highlight the advantages of the proposed GeDe, following (Jie et al., 2022), we provide a fine-grained analysis on MathQA based on various numbers of operations. To be more specific, we compare our model with the most powerful baseline RoBERTa-DR (Jie et al., 2022) and display the analysis results in Table 3. We observe that GeDe performs better on samples with 1, 3, and 4 operations, particularly on samples with at least 5 operations. This comparison indicates our model is more robust to problems requiring more reasoning steps, because the designed re-encoder can capture adequate interactions between the newly produced quantities and the original problem.\\n\\n### 5.4 Ablation Study\\n\\nIn this section, we take a thorough ablation study on MathQA dataset to verify the effectiveness of the re-encode and the hierarchical beam search strategies in the proposed GeDe.\\n\\n#### Effect of Re-encoder.\\n\\nThe proposed re-encoder in Section 4.2 can update both new quantities and reasoning state at each reasoning step. We investigate the two functions respectively.\\n\\nInstead of using dynamic quantity embeddings, we develop a variant model with static quantity embeddings. In other words, instead of having distinct embeddings updated based on various contexts in various math problems, \\\\[QT[i]\\\\] in various math problems is assigned a unified embedding that is updated globally. Note we keep re-encoding the original problem with the newly produced quantities at each step \\\\(t\\\\), but only the updated reasoning state \\\\(R_t\\\\) is leveraged. The comparison results in Table 4 show that without the dynamic quantity embeddings, the performance drops 1.2% on MathQA\u2019s test set. Since different MWPs\u2019 quantities reflect different semantics, it is preferable for them to be dynamically updated with their contexts.\\n\\nThen we completely remove the re-encoder and only allow the encoder to encode the original problem. Instead, we directly use the hidden state in the decoder\u2019s GRU network to represent the reasoning state. Table 4 shows that without the re-encoder, the performance drops 5.7%. In this variant model, although the quantities are dynamically updated according to various problems, the interactions between the quantities and the input problem are not fully exploited as the re-encoder does.\\n\\n#### Effect of Hierarchical Beam Search.\\n\\nPrevious deductive methods (Cao et al., 2021; Jie et al., 2022) generate the operation sequence based on hierarchical greedy search, and regard the implementation of beam search as a future challenge. We implement GeDe with hierarchical beam search.\\n\\n### Table 3: Fine-grained accuracy on MathQA (%).\\n\\n| Operations | RoBERTa-DR | GeDe |\\n|------------|------------|------|\\n| 1          | 77.4       | 78.0 |\\n| 2          | 83.5       | 81.8 |\\n| 3          | 83.4       | 85.1 |\\n| 4          | 81.7       | 84.0 |\\n| \u22655         | 71.4       | 77.5 |\\n| Overall    | 78.6       | 81.5 |\\n\\n### Table 4: Ablation study on MathQA (%).\\n\\n| Model variant | Accuracy |\\n|---------------|----------|\\n| GeDe          | 81.5     |\\n| - w/o dynamic quantity embeddings | 80.3 |\\n| - w/o re-encoder | 75.8 |\\n| - w/o hierarchical beam search | 81.0 |\\n\\nIn Section 4.2, the proposed re-encoder can update both new quantities and reasoning state at each reasoning step. We investigate the two functions respectively. Instead of using dynamic quantity embeddings, we develop a variant model with static quantity embeddings. In other words, instead of having distinct embeddings updated based on various contexts in various math problems, \\\\[QT[i]\\\\] in various math problems is assigned a unified embedding that is updated globally. Note we keep re-encoding the original problem with the newly produced quantities at each step \\\\(t\\\\), but only the updated reasoning state \\\\(R_t\\\\) is leveraged. The comparison results in Table 4 show that without the dynamic quantity embeddings, the performance drops 1.2% on MathQA\u2019s test set. Since different MWPs\u2019 quantities reflect different semantics, it is preferable for them to be dynamically updated with their contexts.\\n\\nThen we completely remove the re-encoder and only allow the encoder to encode the original problem. Instead, we directly use the hidden state in the decoder\u2019s GRU network to represent the reasoning state. Table 4 shows that without the re-encoder, the performance drops 5.7%. In this variant model, although the quantities are dynamically updated according to various problems, the interactions between the quantities and the input problem are not fully exploited as the re-encoder does.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ment hierarchical beam search in our GeDe to improve greedy search. We compare them, where the beam size is set to 1 to create a greedy search. As shown in Table 4, when the hierarchical beam search is disabled (beam size = 4) and replaced with the hierarchical greedy search (beam size = 1), the performance drops 0.5%. By observing the inner and outer beam scores in the generation process, for most of the samples, we find that the score of the first beam is significantly greater than that of the remaining beams, resulting in a relatively small gap between greedy and beam search. This problem, also referred to as neural networks\u2019 \u201cover-confidence\u201d, has been studied by some works (Miao et al., 2021; Wang et al., 2021). Such improvement is left in the further.\\n\\n7 Limitations\\nFrom the time complexity analysis in Appendix A.1, we can see that our model will face the efficiency issue when it needs to generate a long operation sequence. At the same time, the re-encode module needs to concatenate the problem description with generated operations, which may reach the input length limit of PLM. Therefore, our future work will study how to compress the input sequence during the generation process to address above issues.\\n\\n8 Ethics Statement\\nFor many years, public opinion has debated the pros and cons associated with artificial intelligence technology. One consensus is that advances in technology may be used in a variety of scenarios, leading to different influences. To provide an ethical analysis of this work and others on the same line, we will address three aspects: the possible positive or negative effects of our work, the impact of harmful information in datasets, and the equality and differences between different languages.\\n\\nFirst, the point of studying MWP is to explore the mathematical reasoning capabilities of artificial intelligence (Wei et al., 2022). However, the developed models may still be applied to harmful aspects, such as cheating in math exams. On the other hand, the presence of harmful information in the training data may lead the model to learn some implicit biases (Liang et al., 2021; Steed et al., 2022). In our experiments, for the three existing datasets, we exactly follow the experimental setup of previous works to pre-process and remove the potential harmful information. For our manually created dataset CMWPA, our templates also do not contain any harmful information. However, in the inference phase, our model cannot reject answers when the user provides malicious input. Therefore, we need to employ extra efforts to avoid this issue when the model is deployed online.\\n\\nFinally, we use both English and Chinese datasets in our experiments to respect linguistic equality and better take into account language differences. The experimental results validate the robustness of our model across languages. Nevertheless, English and Chinese are the two most popular languages, and we should make greater efforts to concentrate on and preserve the development of minor languages in the field of natural language processing (Zhang et al., 2021).\\n\\nAcknowledgments\\nThis work is supported by National Natural Science Foundation of China (62322214, 62072460, 62172424, 62276270); Beijing Natural Science Foundation (4212022); the Public Computing Cloud at Renmin University of China.\\n\\nReferences\\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1745.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-108", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-108", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Appendix\\n\\nA.1 Complexity Analysis\\n\\nConsider a math problem that has \\\\( n \\\\) words in the problem description and \\\\( |O| \\\\) operations in the solving process. A total of \\\\( \\\\kappa \\\\) words are needed to describe the \\\\( |O| \\\\) operations, i.e., \\\\( \\\\kappa = \\\\sum_{t=1}^{\\\\tau} |o_t| \\\\).\\n\\nGeDe needs to perform \\\\( |O| \\\\) operation re-encode steps and \\\\( \\\\kappa \\\\) token generation steps. For the \\\\( \\\\tau \\\\)-th re-encode step, its computational complexity is \\\\( O((n + \\\\sum_{t=1}^{\\\\tau-1} |o_t|)^2) \\\\). For generating the tokens in \\\\( o_\\\\tau \\\\), its computational complexity is \\\\( O(|o_\\\\tau| \\\\times (n + \\\\sum_{t=1}^{\\\\tau-1} |o_t|)) \\\\). Therefore, the overall time complexity is \\\\( \\\\sum_{\\\\tau=1}^{|O|} (n + \\\\sum_{t=1}^{\\\\tau-1} |o_t|)^2 + |o_\\\\tau| \\\\times (n + \\\\sum_{t=1}^{\\\\tau-1} |o_t|) < O(|O| \\\\times n^2 + \\\\kappa \\\\times (n + \\\\kappa)) \\\\).\\n\\nIf we use the unidirectional attention model as re-encoder, the complexity can be lowered to \\\\( O(n^2 + \\\\kappa \\\\times (n + \\\\kappa)) \\\\), which is the same as what the current seq2seq generation methods achieve. The additional time complexity is acceptable because \\\\( |O| \\\\) is typically not very large.\\n\\nA.2 Datasets Statistics\\n\\nThe statistics of datasets are presented in Table 6. CMWPA is a synthetic English dataset with 1000 training samples and 100 test and validation samples. MAWPS and MathQA are public English MWP datasets that contain 1.9K math problems and 20K math problems, respectively. Math23K is a public Chinese MWP dataset that contains 23K math problems. We use the average number of operations to assess the difficulty of a MWP dataset. As we can see, MAWPS is the simplest dataset because almost all problems require only one or two operations. MathQA is the most challenging dataset, requiring more operations and, hence, more steps in the reasoning process to obtain the answer. SVAMP is also a challenging dataset that is manually created to evaluate a model's robustness. They apply variations to the instances sampled from MAWPS. Such variations could include adding extra quantities, swapping the positions between noun phrases, etc.\\n\\nA.3 Parsing and Execution\\n\\nDue to the existence of higher-order operators, the way we calculate the answer is different from previous works. We implement the corresponding solving function using Python for each pre-defined operator, which is also included in our published code. During inference, for the generated operation sequence, we sequentially calculate the returned quantities for each operation. Naturally, the returned quantities of the last operation denote the answer to the problem. For a generated operation, we first parse out its operator and several operands. Then, we call the solving function corresponding to the operator to obtain the returned quantities.\\n\\nA.4 CMWPA Templates\\n\\nWe show the templates corresponding to the advanced operators as follows.\\n\\nTwo templates for the [linear equation solver] operator:\\n\\n- Text description: \\\\( q_0 \\\\times o_0 + q_1 \\\\times o_1 = q_4; q_2 \\\\times o_0 + q_3 \\\\times o_1 = q_5 \\\\).\\n\\n- Operation: [linear equation solver] \\\\( q_0 \\\\) \\\\( q_1 \\\\) \\\\( q_2 \\\\) \\\\( q_3 \\\\) \\\\( q_4 \\\\) \\\\( q_5 \\\\).\\n\\n- Text description: Determine \\\\( o_0 \\\\), \\\\( o_1 \\\\) as the result of inverse of matrix \\\\( \\\\begin{bmatrix} q_0 & q_1 \\\\\\\\ q_2 & q_3 \\\\end{bmatrix} \\\\) times vector \\\\( \\\\begin{bmatrix} q_4 \\\\\\\\ q_5 \\\\end{bmatrix} \\\\).\"}"}
{"id": "emnlp-2023-main-108", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One template for the quadratic function integral solver operator:\\n\\n- Text description: Determine $o_0$ as the definite integral of quadratic function $q_0 \\\\cdot x^2 + q_1 \\\\cdot x + q_2$ between the intervals $q_3$ and $q_4$.\\n\\nOne template for the quadratic function extremum solver operator:\\n\\n- Text description: Determine $o_0$ as the extremum value of quadratic function $q_0 \\\\cdot x^2 + q_1 \\\\cdot x + q_2$.\\n\\nBased on the templates, we generate a sample as follows. First, we randomly initialize a candidate set of quantities. Then, we randomly select a template and fill in slots by randomly selecting quantities from the quantity candidate set. We input the returned quantities of the operation into the candidate set and repeat the above process several times. In this way, a problem description and its operation sequence are generated. We also convert the operation sequence into a pre-order binary expression as another type of annotation for training the seq2seq baseline.\\n\\nA.5 CMWPA Example\\n\\nWe provide a sample of CMWPA in Table 5. This sample is initialized with 6 quantities and involves four types of operators: the subtraction operator, the linear equation solver operator, the quadratic function integral solver operator, and the quadratic function extremum solver operator. Two types of annotations are provided: the multivariant operation sequence and the pre-order binary expression (pre-order binary expression can be transformed into a binary operation sequence (bDAG) or a binary expression tree). For each operation in the multivariant operation sequence, we provide the operation, the input quantities, and the returned output quantities.\"}"}
{"id": "emnlp-2023-main-108", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem:\\nGiven \\\\( q_0 = 0.23 \\\\) . \\\\( q_1 = 0.43 \\\\) . \\\\( q_2 = 0.18 \\\\) . \\\\( q_3 = 0.26 \\\\) . \\\\( q_4 = 0.71 \\\\) . \\\\( q_5 = 0.85 \\\\) . Determine \\\\( q_6 \\\\) as the \\\\( q_4 \\\\) minus \\\\( q_5 \\\\) . Determine \\\\( q_7 \\\\) \\\\( q_8 \\\\) as the result of inverse of matrix \\\\( \\\\begin{bmatrix} q_4 & q_3 \\\\\\\\ q_2 & q_5 \\\\end{bmatrix} \\\\) times vector \\\\( \\\\begin{bmatrix} q_0 \\\\\\\\ q_6 \\\\end{bmatrix} \\\\) . Determine \\\\( q_9 \\\\) as the definite integral of quadratic function \\\\( q_6 \\\\times x^2 + q_7 \\\\times x + q_5 \\\\) between the intervals \\\\( q_1 \\\\) and \\\\( q_8 \\\\) . Determine \\\\( q_{10} \\\\) as the the extremum value of quadratic function \\\\( q_8 \\\\times x^2 + q_6 \\\\times x + q_9 \\\\) .\\n\\nMultivariant Operation Sequence:\\n\\n1. operation 1: \\\\([-, q_4, q_5]\\\\) returned quantities of operation 1: \\\\([q_6]\\\\)\\n2. operation 2: \\\\([\\\\text{linear equation solver}, q_4, q_3, q_2, q_5, q_0, q_6]\\\\) returned quantities of operation 2: \\\\([q_7, q_8]\\\\)\\n3. operation 3: \\\\([\\\\text{quadratic function integral solver}, q_1, q_8, q_6, q_7, q_5]\\\\) returned quantities of operation 3: \\\\([q_9]\\\\)\\n4. operation 4: \\\\([\\\\text{quadratic function extremum solver}, q_8, q_6, q_9]\\\\) returned quantities of operation 4: \\\\([q_{10}]\\\\)\\n\\nPre-order binary expression:\\n\\n\\\\[+ , * , / , - , * , q_4 , q_0 , * , q_2 , - , q_4 , q_5 , - , * , q_4 , q_5 , * , q_2 , , q_3 , \\\\hat{ }, * , c_3 , / , - , q_4 , q_5 , * , c_1 , / , - , * , q_4 , q_5 , - , * , q_4 , q_5 , - , * , q_4 , q_5 , , q_0 , * , q_2 , - , q_4 , q_5 , - , * , q_4 , q_5 , * , q_2 , q_3 , \\\\hat{ }, c_1 , + , * , - , q_4 , q_5 , * , c_1 , / , - , * , q_4 , q_5 , - , * , q_4 , q_5 , * , q_2 , q_3 , \\\\hat{ }, c_2 , / , - , * , q_4 , q_5 , , q_3 , + , * , / , - , q_4 , q_5 , \\\\hat{ }, c_2 , + , * , / , / , - , * , q_5 \\\\]\"}"}
