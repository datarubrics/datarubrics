{"id": "lrec-2024-main-639", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FinCorpus-DE10k: A Corpus for the German Financial Domain\\n\\nSerhii Hamotskyi, Nata Kozaeva, Christian H\u00e4nig\\nAnhalt University of Applied Sciences\\nBernburger Str. 55, 06366 K\u00f6then, Germany\\nserhii.hamotskyi@hs-anhalt.de, nata.kozaeva@student.hs-anhalt.de, christian.haenig@hs-anhalt.de\\n\\nAbstract\\nWe introduce a predominantly German corpus comprising 12.5k PDF documents sourced from the financial domain. The corresponding extracted textual data encompasses more than 165 million tokens derived predominantly from German, and to a lesser extent, bilingual documents. We provide detailed information about the document types included in the corpus, such as final terms, base prospectuses, annual reports, information materials, law documents, international financial reporting standards, and monthly reports from the Bundesbank, accompanied by comprehensive statistical analysis. To our knowledge, it is the first non-email German financial corpus available, and we hope it will fill this gap and foster further research in the financial domain both in the German language and in multilingual contexts.\\n\\nKeywords: Text Corpus, Financial Domain, German, Bilingual\\n\\n1. Introduction\\nThe study of financial language is pivotal for understanding the intricacies of global economics, legal frameworks, and business communications. In the pursuit of unraveling the complexities of financial discourse, the availability of diverse and comprehensive linguistic resources is paramount. In this context, we present a significant contribution to the field in the form of a German corpus, offering profound insights into the German financial domain. While large language models perform well on many tasks, there are scenarios where fine-tuning language models is beneficial compared to employing large language models. One of these scenarios is decreasing the model size to optimize runtime (Biesner et al., 2022). But the performance on specific tasks can benefit as well, e.g. in the clinical or financial domains (J\u00f8rgensen et al., 2023). Domain-specific language models achieve higher accuracy for sentiment analysis in the financial domain and English language (FinBERT (Araci, 2019) achieved a 15% improvement in accuracy) and token classification (Biesner et al., 2022).\\n\\nFinancial text is characterized by a unique vocabulary with implications including sentiment analysis, e.g. many words like liability / share / stock / bull having different connotations compared to the general language (Mishev et al., 2020). This phenomenon also exists for other languages than English, still, some languages lack the availability of language resources (e.g. German). Some studies suggest that other languages might benefit from domain-specific corpora and language models, e.g. H\u00e4nig et al. (2023) show for Named Entity Recognition in the financial domain that model accuracy benefits from domain-adjusted language models in a cross-language scenario.\\n\\nJ\u00f8rgensen et al. (2023) note the recent effort to produce monolingual financial BERTs to process financial text (while highlighting the need for, and importance of, multilingual financial datasets and models).\\n\\nWe present a corpus consisting of 12.5k financial documents as PDFs (mostly in German, with some documents bilingual - German and English) to stimulate the area of German NLP in the financial domain. To our knowledge, there's only one other corpus of German financial language - the email-based CODE ALLTAG (Krieg-Holz et al., 2016). Our corpus is composed of seven document types, organized in an intuitive directory structure accompanied by relevant metadata. Potential uses of our corpus include tasks in the field of Natural Language Processing like Language Model Fine-Tuning (for financial tasks), Document Understanding (e.g. document structure extraction) or OCR (parallel visual and textual data).\\n\\n2. Related Work\\nGerman is traditionally considered a high-resource language and a large amount of both general-purpose and specialized corpora exist and are publicly available. Surprisingly, there's a notable gap in the financial domain. CODE ALLTAG (Krieg-Holz et al., 2016) is a text corpus composed of German-language emails from Usenet groups, and it contains a \u201cFINANCE\u201d collection containing 174,375 emails and almost 2.5M sentences. To our knowledge, this collection is the only German financial corpus that is freely available.\\n\\nData from the Bundesanzeiger has been used in the literature for similar purposes, e.g. company name recognition (Loster et al., 2017) or training...\"}"}
{"id": "lrec-2024-main-639", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languagemodelsondata\u201dthatissimilartofinancial\\ntext\u201d (Biesner et al., 2022); none of these datasets\\nhave been made available.\\n\\nThe Bundesstelle f\u00fcr Open Data published two\\npython packages, deutschland\\n3 and\\nhandelsregister\\n4 that allow querying and downloading\\ndata from the\\nBundesanzeiger\\n5 and\\nHandelsregister\\n5 respectively, but no static financial corpus\\nexists.\\n\\nWhilenotbelongingtothefinancialdomain,legal\\ncorpora in German exist, most notably Open Legal\\nData\u2019s (Ostendorff et al., 2020) dataset of 100k\\nGerman court decisions and 444k citations\\n6.\\n\\nMost work done in the NLP financial commu-\\nnity is carried on in a monolingual English setting\\n(J\u00f8rgensen et al., 2023), but as the financial en-\\nvironment is multilingual (as evidenced, in part,\\nby the German+English prospectuses in our own\\ncorpus) the relevance of multilingual resources\\nincreases; see (J\u00f8rgensen et al., 2023) for an\\noverview of English, non-English and multilingual\\nfinancial datasets built for a specific downstream\\ntask, mostoftenNamedEntityRecognitionandtext\\nclassification.\\n\\n3. FinCorpus-DE10k Dataset\\n\\n3.1. Dataset summary\\n\\nThe corpus contains 12,235 PDF files of financial\\ndocuments (mostly security prospectuses) from\\nseven collections, most with less than 100 pages,\\nas well as the corresponding plaintext files for app-\\nprox. 10,500 of them. The documents are predom-\\ninantly (71%) in German, the remaining ones are\\nbilingual (German and English).\\n\\nThe basic statistics by collection can be seen in\\nTable 1.\\n\\nMetadata for the files is provided in\\n./metadat.csv\\n. It contains the following columns (with\\n4-9 empty if no text was extracted):\\n\\n  1. collection: the name of the collection the file\\n     belongs to\\n  2. pdf_only: True if the extracted text of the docu-\\n    ment is not included\\n  3. pdf_fn: the path to the PDF file\\n  4. txt_fn: the path to the.txt file with the extracted\\n    text if present\\n  5. num_pages: number of pages present in the\\n    PDF\\n  6. num_chars, num_tokens, num_sentences:\\n    number of characters, tokens and sentences,\\n    respectively\\n  7. token_len: mean token length in characters\\n  8. sentence_len: mean sentence length in tokens\\n  9. tokens_per_page: mean number of tokens per\\n    PDF page\\n  10. language: languages present in the file, either\\n      \u201cDE\u201d or \u201cEN,DE\u201d\\n  11. ISIN, country: only in Final Terms documents\\n\\nDue to the variety of sources included in the\\ndataset, the different sub-collections are released\\nunder different licenses. Unless stated other-\\nwise, the license is Creative Commons Attribution-\\nNonCommercial 4.0 International (CC BY-NC 4.0\\n7).\\n\\nThe \u201cBundesbank Monthly Reports\u201d and \u201cAnnual\\nReports\u201d collections are released under the CC\\nAttribution-NonCommercial-\\nNoDerivs\\n4.0 Interna-\\ntional license (CC BY-NC-ND 4.0\\n8).\\n\\n\u201cInformational\\nMaterials\u201d and \u201cIFRS\u201d don\u2019t have a specific license\\nattached. To ensure making the corpus as widely\\naccessible as possible, it\u2019s released in two version.\\nThe first, openly available, contains only the col-\\nllections releasable under Creative Commons li-\\ncenses\\n9. The second - \u201ccomplete\u201d\\n10- contains\\nall collections, including \u201cIFRS\u201d and \u201cInformational\\nMaterials\u201d, and will be made available on request.\\n\\nWe diligently adhered to the licensing terms to\\nthe best of our understanding and in good faith,\\nbut the responsibility for the use and compliance\\nwith the applicable law rests upon the final users.\\n\\nIn the event that documents included in any of\\nthe collections unbeknownst to us have different\\nlicenses than the one stated, those licenses shall\\ntake precedence. Although extensive efforts were\\nmade to identify and exclude such documents,\\nwe will promptly remove any documents from the\\ndataset if they are found to be infringing.\\n\\nThe code used for the generation and cleanup\\nof the corpus is available on GitHub\\n11.\\n\\n6\\nhttps://en.wikipedia.org/wiki/German_Commercial\\n_Register\\n5\\nhttp://openlegaldata.io/research/2019/02/19/court-\\ndecision-dataset.html\\n\\n7\\nhttps://creativecommons.org/licenses/by-nc/4.0/\\n8\\nhttps://creativecommons.org/licenses/by-nc-nd/4.0/\\n9\\nhttps://huggingface.co/datasets/anhaltai/fincorpus-\\nde-10k\\n10\\nhttps://huggingface.co/datasets/anhaltai/fincorpus-\\nde-10k-complete\\n11\\nhttps://github.com/AnhaltAI/fincorpus-de-10k-\\nscripts\"}"}
{"id": "lrec-2024-main-639", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Initial Collection and Normalization\\n\\n3.2.1. Collection\\n\\nThe core of the FinCorpus-DE10k dataset is composed of more than ten thousand securities prospectuses in PDF format. They were gathered by the German Central Bank (Deutsche Bundesbank) from various sources, such as from the websites of various financial institutions and regulatory bodies, as well as from publicly available databases, and are part of the Final terms and Base prospectuses collections. They were augmented by a number of separate collections described in details in their individual sections.\\n\\n3.2.2. Filtering and normalization\\n\\nFirstly, corrupted, password-protected or otherwise unfit PDF files were filtered out. The PDF format allows textual information in the form of PDF text elements, either added during creation or from e.g. OCR at a later step. We extracted this text layer with the PyMuPDF library, used it for language detection and statistics, and provide it in txt files as part of the dataset. It may not fully correspond to the PDF due to OCR and layout considerations, see Section 3.4 for a more comprehensive description of this step.\\n\\nWe used automatic language detection to find and filter out documents in languages other than German or German+English (including the removal of English-only documents), as described in Section 3.5.\\n\\nWe additionally dropped documents that were likely to state issues during further processing, using a number of manual thresholds and heuristics described in Section 3.6.\\n\\n3.3. Collections\\n\\n3.3.1. Final Terms Prospectuses\\n\\nThe collection contains 10,450 PDF files, 1 to 719 pages long, with a mean of 25 and a 75th percentile of 32 pages; 98% of files are under 100 pages.\\n\\nFiles from this and the Base prospectuses collections are financial prospectuses that provide terms and conditions of the issuance of financial securities. 95% of the filenames in this collection contain the ISIN (International Securities Identification Number) of the prospectus itself, e.g. \u201cDE000SLB8387.pdf\u201d. An ISIN is composed of three parts: the first two characters are an ISO 3166-1 alpha-2 country code, the next nine numbers are the National Securities Identifying Number (NSIN) that identifies the security, and a single numerical check digit. For securities cleared through Clearstream or Euroclear (which are worldwide) \u201cXS\u201d is used in place of the country code (R\u00f6man, 2017).\\n\\nThis allows us to filter them by emitting country: a breakdown can be seen on Table 2. We extracted text only from prospectuses from Germany and Austria.\\n\\n3.3.2. Base Prospectuses\\n\\nBase prospectuses contain information about the issuer, description of the security and the summary of the prospectus. This information can be provided as a single document or in three separate ones. The issuer description and the securities note must also include the risk factors spe-\"}"}
{"id": "lrec-2024-main-639", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The prospectuses can be referenced in the final terms documents. The structure, content, release procedure are regulated by Article 8 and 10 of REGULATION (EU) 2017/1129 (\\\"Prospectus Regulation\\\"). The prospectus approval process in Germany is regulated by the Federal Financial Supervisory Authority (BaFin). Informally, one can see them as the larger documents containing overall information needed for an investor, while the \\\"final terms\\\" documents are issued for each individual security and contain information distinguishing the security, including determining which information from the base prospectus is applicable to it.\\n\\nCompared to the Final terms collection, this collection contains fewer but longer documents; Final terms has 16.3 times more documents but only 2.15 times more tokens.\\n\\n3.3.3. Annual Reports\\nContain annual (in a few instances quarterly) reports from (mostly) the Bundesbank and other institutions, spanning the years 1995\u20132022. Annual reports of the Bundesbank generally provide information about economic and financial issues, monetary policy, risks of financial stability etc. Annual reports of publicly traded companies consist of standard sections with general corporate information, operating and financial highlights, financial statements, including the balance sheet, income statement, and cash flow statement, Auditor's report etc.\\n\\nThis collection contains a larger number of data visualizations and images.\\n\\n3.3.4. International Financial Reporting Standards (IFRS)\\nContains the EU International Financial Reporting Standards (IFRS) from the years 2017\u20132023. These documents describe standards as a set of accounting rules that facilitate understanding and the comparability of companies' financial statements across state boundaries to ensure corporate transparency.\\n\\nAll seven documents are extremely similar, each successive document containing an updated version of the previous one with new additions/deletions.\\n\\nWe want to raise awareness for this duplication within the dataset, because some studies suggest that duplicated data might be detrimental for language model training (Lee et al., 2022). Thorough research experiments need to follow to accurately estimate the impact of duplications in the training data for language model in case of small domain-specific corpora.\\n\\n3.3.5. Law\\nContains files with German laws in the financial and related domains, some in their English translations. The core regulations applicable to the financial sector in Germany are laid down in the Banking Act (KWG); the Securities Institutions Act (WpIG), the Securities Trading Act (WpHG) etc. as well as EU Directives implemented into German law.\\n\\nIt has the longest mean sentence length out of all other collections, likely a reflection of the subject matter as well as more uniform PDF files.\\n\\n3.3.6. Informational Materials\\nContains miscellaneous brochures and advertisements in the area of finance. They have a wide variety of fonts, photos, colors, and are mostly aimed at a more general audience. In contrast with the Law collection, it has the shortest average sentence length of the entire corpus.\\n\\n3.3.7. Bundesbank Monthly Reports\\nThis collection contains 838 monthly reports of the German Bundesbank from the years 1949\u20132022. No extracted text is provided from this collection, only the PDF documents. The text elements in the documents from the years 1961\u20131999 (incl.) were absent originally and were added by us.\\n\\nThe dataset is fascinating from a digital humanities standpoint, as a decades-long sequence of documents written in the same context for the same purpose (allowing e.g. to track the changes in the German financial language throughout the years, as well as conventions in the presentation of data etc.). Some of the reports, especially the older ones, are quite challenging from an OCR perspective (one of the reasons being a large amount of tables and graphs), leading to the quality of both the PDF text layer and the resulting the plain-text representation being one of the lowest of the entire dataset.\\n\\nWe decided to add it to the collection nevertheless, as the stated goal of our corpus is providing a PDF files corpus with German financial language, but without the extracted text, which would have been an outlier by most metrics (e.g. its average token length is almost half of the corpus average due to the high amount of OCR artifacts).\\n\\n3.4. Layout and Text Extraction\\nThe layout of the files isn't uniform and at times relatively complex, precluding the use of trivial layout parsing approaches.\\n\\nhttps://www.bundesbank.de/de/publikationen/berichte/monatsberichte\"}"}
{"id": "lrec-2024-main-639", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We\u2019ll describe in detail on Final terms and Base prospectuses, as they make up the major part of the corpus.\\n\\n3.4.1. Final terms and Base prospectuses\\n\\nThe Final terms and Base prospectuses contain, in roughly decreasing frequency of occurrences:\\n\\n- Columns\\n- Checkboxes\\n- Two languages in different configurations\\n- Table-like structures\\n\\nThe prospectuses were issued by different issuers, with some issuers being represented much more often than others.\\n\\nIssuers can use different programs to write (or (semi-)automatically generate) the PDF, and have different layout and design conventions. This has practical implications for potential parsing of these files (be it to extract plaintext suitable for model training or to analyze the prospectuses from a financial standpoint).\\n\\nAs an example, checkboxes are represented in different ways: as character using one of the standard UTF-8 symbols, as character from an embedded font represented with a UTF-8 code point from an Unicode Private Use Area, or as image. All of these can be used in the same document, sometimes - on the same page; sometimes a different strategy is used for checked and unchecked checkboxes (see Figure 3). Detecting/analyzing checkboxes is crucial for automatic evaluation of prospectuses (H\u00e4nig et al., 2023), where only valid statements should be considered and invalid statements must be ignored for the eventual analysis.\\n\\nSimilarly, different layouts (esp. columns) lead to difficulties in extracting textual flow consistently from all the documents.\\n\\nWe provide the text we extracted from the documents as plaintext, but it\u2019s meant as an approximation and has not been manually checked or corrected. The heterogeneous layout structure of the documents precludes easy text flow extraction. Various tools can use different algorithms to deal with columns, text blocks, tables, and lead to different results. Our choice in that matter is best treated as only one possibility, not necessarily the best one (but fitting for our goal of doing language detection and calculating statistics on document level).\\n\\n3.4.2. Other collections\\n\\nAnnual and monthly reports contain a very high amount of tables and graphs, but in most ways the points from the last subsection apply.\\n\\n3.4.3. Manual quality assessment estimation\\n\\nTo evaluate the efficacy of the Optical Character Recognition (OCR) process in addressing known complications within the refined version of the dataset, a methodological examination involving manual spot checks was conducted on a minimal subset of the dataset, comprising 35 documents, with an allocation of six documents per collection, by a duo of annotators. Each document was graded on a scale from 1 to 5 (worst to best), leading to a mean of 4.55, considerably higher than our initial expectations. The primary complications identified were associated with inadequate column parsing, exemplified by the merging of text from distinct columns into a singular plaintext format, and issues pertaining to hyphenation.\\n\\nGiven the constrained sample size, the assessment yields a low-confidence approximation of the OCR process\u2019s performance. Nonetheless, it successfully corroborates the existence of two prevalent systematic challenges within the dataset, specifically related to hyphenation and columnar structuring.\\n\\n3.5. Languages\\n\\nMost (around 71%) of the text corpus is in German, the remaining documents are bilingual (German and English). The PDF corpus has roughly the same ratios but with lower confidence intervals. While we made an effort to filter out documents in other languages or language combinations, an negligible number of such documents could be present in the corpus.\\n\\nIn the cases where the documents are bilingual, usually it\u2019s either a translation (either in two columns or each section/item translated sequentially), or a header block with general info or disclaimers followed by the actual content in another language.\\n\\nWe used the automatic language identification library lingua-py and didn\u2019t manually verify each of the documents included in the final collection. In particular, we have reason to believe the share of bilingual documents is an overestimation.\\n\\nIn Figure 2 we plotted the distribution of languages in the dataset, reflecting the fraction of each language in each document. The box plot labeled \u201cEN_DE\u201d corresponds to the combined percentage of English and German languages, while the remaining portion, which is \\\\( \\\\left( 1 - P(EN_{-}DE) \\\\right) \\\\), represents all the other languages (labeled as \u201cOTHERS\u201d), or the sum of the percentages of Portuguese, Dutch, French, Nynorsk, Spanish, and Italian (the languages spoken in countries...\"}"}
{"id": "lrec-2024-main-639", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Subsequent to the exclusion of documents containing only English, a graphical representation was constructed to display the three most prevalent languages within each document, organized according to the frequency of occurrence rather than linguistic designation: label 0 signifies the predominant language within the document, label 1 the secondary language, and label 2 the tertiary language. The data reveals two prominent clusters: the majority of documents are monolingual, identified as German (label 0), while a subset exhibits bilingual characteristics, with two languages present in the 40-60% interval (labels 0 & 1). Contrary to our initial assumption that trilingual documents might be observed, languages appearing after the two primary ones represent an insubstantial fraction (label 2), which is typically considered as extraneous noise.\\n\\nFigure 2: The initial language distribution in the corpus. (EN,DE\\\" is the sum of the English and German percentages in each document, \\\"OTHER\\\" is the sum of all the others ($P(OTHER) = 1 - P(EN,DE)$)).\\n\\nFigure 3: Example of heterogeneous encoding of a checked checkbox in one of the documents.\\n\\nWe encountered the following failure modes, sometimes in different pages of the same document:\\n\\n- Encoding issues: when extracting text from the PDF, reading and saving it as UTF-8, some or all of the characters of a document were saved as UTF \\\"REPLACEMENT CHARACTER\\\" U+FFFD (whose function is replacing a character that is unknown or unrepresentable in UTF-8).\\n- Incomplete text elements in the PDF file: only some of the text in the PDF is machine readable. Scenarios we've seen include:\\n  - scanned PDF files on which new text is added in a PDF editor, with the new text being readable and the surrounding text being treated as picture...\"}"}
{"id": "lrec-2024-main-639", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u2022 Bad OCR: The text elements in the PDF were the result of an OCR process resulting in almost unreadable text.\\n\u2022 The text elements in the PDF are wrong, either when extracting text or copy+pasting it from an PDF reader. The resulting characters aren't OCR artifacts, they are completely unrelated to the visual ones.\\n\\nThese problems are distinct but were often present in similar (kinds of) documents. We didn't have the goal to fix them, just to recognize with enough recall to filter them out from the dataset.\\n\\nOne option would have been to OCR each file and just use the resulting text, but this could have introduced OCR errors in perfectly machine-readable documents. Comparing the results of the own OCR to the extracted text could point at suspicious documents, a comparison that could be done through string similarity metrics. To detect partial OCR scenarios, comparing the text lengths or the text boundaries on each page could be enough.\\n\\nWe found many such documents among those with unexpected language detection results, and later saw that it's an extremely valuable signal for wrong or partially extracted text: language identification can be challenging in itself, but if the text is small disjoint chunk of text, or mostly numbers and punctuation, the chances of incorrect identifications increase.\\n\\nFiltering out documents on the basis of language identification results achieves more than one goal and has a high tolerance for errors: removing a document with potentially wrong extracted text is good, but if the extracted text is correct and it's the document itself that's short or atypical, its removal increases the quality of the corpus just as well.\\n\\nAt the end, we found that the following heuristics work for our use case:\\n\u2022 Sum of English and German text is less than 50% of the document.\\n\u2022 The entire document contains less than 6 sentences.\\n\u2022 The average number of tokens per page is less than 100.\\n\u2022 More than 5% of the document contains UTF-8 replacement characters.\\n\\nLastly, for the Final terms collection, we left only the prospectuses from Germany and Austria, which decreased the number of language detection issues as well as narrowed the list down to documents more likely to be in the two language combinations we needed.\\n\\nAll the documents which were discarded during the process were discarded only from the text corpus but not from the PDF one. Only documents containing exclusively English text were removed completely.\\n\\nAt the end, all this removed 17% of Base prospectuses, 12% of Final terms, 6% of Brochures, 2% of Laws and 1% of Annual reports.\\n\\n3.7. Availability and Reproducibility\\n\\nThe corpus has been uploaded to Huggingface Hub at https://huggingface.co/datasets/anhaltai/fincorpus-de-10k. The complete version of the dataset (see Section 3.1 on the distinction) will be provided via email application.\\n\\nThe code used to preprocess and analyse the dataset is available at https://github.com/AnhaltAI/fincorpus-de-10k-scripts.\\n\\nA SemVer-based versioning scheme will be used, with this version being 1.0.0. We want to preserve the option to update the dataset without breaking any research results that depend on it, with possible changes including the detected language of the document, the addition or removal of documents (e.g. due to licensing issues), and the addition of improved text of the documents. All changes between versions will noted in a changelog.\\n\\n4. Discussion\\n4.1. Comparison with CodE AlltagXL\\n\\nContrasting with the only other German financial corpus available, CodE AlltagXL's (Krieg-Holz et al., 2016) \\\"FINANCE\\\" collection, one noted difference is that our corpus is built from published material, as opposed to emails.\\n\\nIn the two extremes of language use the authors discuss, on one hand language from high-end performers, conforming to the formal rules of the language (books, manuals, articles, technical papers), and on the other hand - dialogue-oriented, mostly informal and colloquial language from users of different backgrounds (social media, chats, blogs), with the CodE AlltagXL corpus being a mixture of both, our corpus strongly and consistently leans towards the formal side.\"}"}
{"id": "lrec-2024-main-639", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some collections contain documents one can consider formulaic, the medium of securities prospectuses especially allows relatively little freedom and variety in the language used. A deeper comparison of language use in both corpora would be an interesting avenue for further research.\\n\\n4.2. Limitations\\n\\nAs mentioned above, despite the large number of documents, our corpus may have less variety than an user-generated one, though we believe both are equally useful facets of the language used in the financial domain.\\n\\nOne of the important limitations of the corpus is hyphenation on line breaks (especially in documents with multiple narrow columns). We did not handle it explicitly and left it as-is, leading to some words broken into two parts, separated by a hyphen and a newline symbol. One negative impact of this is that this could have inflated the number of tokens in the dataset statistics (see Table 1), but not significantly \u2014 the issue is present only in some line endings in some of the documents. The extent to which this impacts the utility of our dataset is debatable. The training of LMs using modern approaches should be stable to the variance introduced by this effect, as they employ their own normalization and sub-word tokenization and therefore rely less on word and line-breaks.\\n\\nThe PDF-first nature of our corpus, along with the complex layouts found in our files, presents a challenge not found in the CodE Alltag (Krieg-Holz et al., 2016) digital-first corpus; parsing complex layouts into a \u2018natural\u2019 text flow is far from being a solved problem. We provide the source PDF files to allow as much freedom as possible in the matter, and in the hope that better approaches will be available in the future.\\n\\nThe bilingual nature of the corpus is, too, affected by this. In some documents the translations are given separately, in some - in parallel columns, in some - each item is given in two translations, one immediately following the other. The use of automatic language detection is as important as layout parsing for the automated plaintext extraction in such PDF files, especially for purposes like building parallel corpora.\\n\\n5. Conclusions & Future Work\\n\\nThis paper introduces a novel German financial corpus, addressing a significant gap in existing language resources for the financial domain. Currently, only one other German financial corpus is available, specifically focusing on emails. Notably, our corpus includes bilingual documents in English and German, reflecting the growing multilingual nature of the financial sector. Detailed information about the languages present in each document, alongside other metadata, is provided.\\n\\nDespite the availability of tools like Python packages provided by the German government to download articles from sources such as Bundesanzeiger and Handelsregister, there is an absence of published datasets for building German financial text corpora. This stands in contrast to other domains, such as law, where corpora are readily accessible, and to financial corpora in other languages, particularly English. Surprisingly, even in multilingual financial datasets (as outlined by J\u00f8rgensen et al. (2023)), German remains absent.\\n\\nWe hope that our contribution will facilitate the development of similar resources and streamline research efforts built upon them. By providing this corpus, we aim to ease the path for future research endeavors in the multilingual financial domain, fostering a more comprehensive understanding of linguistic patterns within this critical sector.\\n\\n6. Bibliographical References\\n\\nDogu Araci. 2019. FinBERT: Financial sentiment analysis with pre-trained language models. CoRR, abs/1908.10063. ArXiv: 1908.10063 tex.bibsource: dblp computer science bibliography, https://dblp.org tex.biburl: https://dblp.org/rec/journals/corr/abs-1908-10063.bib tex.timestamp: Thu, 29 Aug 2019 16:32:34 +0200.\\n\\nDavid Biesner, Rajkumar Ramamurthy, Robin Stenzel, Max L\u00fcbbering, Lars Hillebrand, Anna Ladi, Maren Pielka, R\u00fcdiger Loitz, Christian Bauchhage, and Rafet Sifa. 2022. Anonymization of German financial documents using neural network-based language models with contextual word representations. International Journal of Data Science and Analytics, 13(2):151\u2013161.\\n\\nChristian H\u00e4nig, Markus Schl\u00f6sser, Serhii Hamotskyi, Gent Zambaku, and Janek Blankenburg. 2023. NLP-based Decision Support System for Examination of Eligibility Criteria from Securities Prospectuses at the German Central Bank. In Proceedings of AAAI23 Bridge 8: AI for Financial Institutions, Washington, D. C., USA.\\n\\nRasmus J\u00f8rgensen, Oliver Brandt, Mareike Hartmann, Xiang Dai, Christian Igel, and Desmond Elliott. 2023. MultiFin: A Dataset for Multilingual Financial NLP. In Findings of the Association for Computational Linguistics: EACL 2023, pages 894\u2013909, Dubrovnik, Croatia. Association for Computational Linguistics.\"}"}
{"id": "lrec-2024-main-639", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ulrike Krieg-Holz, Christian Schuschnig, Franz Matthies, Benjamin Redling, and Udo Hahn. 2016. CodE alltag: A German-Language E-Mail corpus. In Proceedings of the tenth international conference on language resources and evaluation (LREC\u201916), pages 2543\u20132550, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nKatherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas Carlini. 2022. Deduplicating Training Data Makes Language Models Better. ArXiv:2107.06499 [cs].\\n\\nMichael Loster, Zhe Zuo, Felix Naumann, Oliver Maspfuhl, and Dirk Thomas. 2017. Improving company recognition from unstructured text by using dictionaries. In EDBT, pages 610\u2013619.\\n\\nKostadin Mishev, Ana Gjorgjevikj, Irena Vodeniska, Lubomir T. Chitkushev, and Dimitar Trajanov. 2020. Evaluation of Sentiment Analysis in Finance: From Lexicons to Transformers. IEEE Access, 8:131662\u2013131682.\\n\\nMalte Ostendorff, Till Blume, and Saskia Ostendorff. 2020. Towards an Open Platform for Legal Information. In Proceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, pages 385\u2013388. ArXiv:2005.13342 [cs].\\n\\nJan R. M. R\u00f6man. 2017. Trading Financial Instruments. In Analytical Finance: Volume I, pages 1\u201320. Springer International Publishing, Cham.\\n\\nLanguage Resource References\\n\\nKrieg-Holz, Ulrike and Schuschnig, Christian and Matthies, Franz and Redling, Benjamin and Hahn, Udo. 2016. CodE Alltag: A German-Language E-Mail Corpus. European Language Resources Association (ELRA).\"}"}
