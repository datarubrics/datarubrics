{"id": "emnlp-2024-main-456", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Summarization Method | Model Name                     | Attr | Rela | Even | Pers | Avg. |\\n|----------------------|--------------------------------|------|------|------|------|------|\\n|                      | Mistral-7B-Instruct-v0.2       | 87.25| 87.95| 84.88| 85.97| 86.51|\\n|                      | Mixtral-8x7B-MoE              | 86.98| 85.37| 75.89| 77.21| 81.36|\\n|                      | vicuna-7b-v1.5-16k            | 87.61| 87.81| 84.89| 85.56| 86.47|\\n|                      | vicuna-13b-v1.5-16k           | 87.62| 87.67| 84.91| 82.92| 85.78|\\n|                      | Qwen1.5-7B-Chat               | 86.80| 87.26| 84.73| 86.98| 86.44|\\n|                      | Qwen1.5-14B-Chat              | 87.28| 87.51| 84.88| 87.43| 86.78|\\n|                      | Qwen1.5-72B-Chat              | 88.05| 88.78| 85.48| 87.80| 87.53|\\n|                      | GPT-3.5-Turbo                 | 87.86| 88.05| 84.90| 87.13| 86.98|\\n|                      | GPT-4-Turbo                   | 87.91| 88.27| 85.32| 87.33| 87.21|\\n\\n| Incremental Updating | Mistral-7B-Instruct-v0.2       | 87.95| 88.46| 83.95| 87.48| 86.96|\\n|                      | Mixtral-8x7B-MoE              | 87.58| 87.63| 84.74| 85.96| 86.47|\\n|                      | vicuna-7b-v1.5-16k            | 86.14| 86.59| 84.51| 85.42| 85.66|\\n|                      | vicuna-13b-v1.5-16k           | 87.20| 87.50| 85.10| 86.08| 86.47|\\n|                      | Qwen1.5-7B-Chat               | 87.22| 87.82| 84.94| 87.22| 86.80|\\n|                      | Qwen1.5-14B-Chat              | 87.83| 88.05| 85.42| 87.36| 87.16|\\n|                      | Qwen1.5-72B-Chat              | 88.42| 89.10| 86.37| 88.01| 87.97|\\n|                      | GPT-3.5-Turbo                 | 88.74| 88.74| 85.39| 87.86| 87.68|\\n|                      | GPT-4-Turbo                   | 88.38| 88.63| 85.58| 87.73| 87.58|\\n\\n| Hierarchical Merging | Mistral-7B-Instruct-v0.2       | 87.95| 88.46| 83.95| 87.48| 86.96|\\n|                      | Mixtral-8x7B-MoE              | 87.58| 87.63| 84.74| 85.96| 86.47|\\n|                      | vicuna-7b-v1.5-16k            | 86.14| 86.59| 84.51| 85.42| 85.66|\\n|                      | vicuna-13b-v1.5-16k           | 87.20| 87.50| 85.10| 86.08| 86.47|\\n|                      | Qwen1.5-7B-Chat               | 87.22| 87.82| 84.94| 87.22| 86.80|\\n|                      | Qwen1.5-14B-Chat              | 87.83| 88.05| 85.42| 87.36| 87.16|\\n|                      | Qwen1.5-72B-Chat              | 88.42| 89.10| 86.37| 88.01| 87.97|\\n|                      | GPT-3.5-Turbo                 | 88.74| 88.74| 85.39| 87.86| 87.68|\\n|                      | GPT-4-Turbo                   | 88.38| 88.63| 85.58| 87.73| 87.58|\\n\\n| Sum-in-One-Go        | GPT-4-Turbo                   | 89.18| 89.81| 86.83| 88.45| 88.56|\\n|                      | Claude3-Sonnet                | 89.38| 89.70| 87.08| 88.19| 88.59|\\n\\n| Incremental          | GPT-4-Turbo                   | 88.26| 88.48| 85.43| 87.26| 87.36|\\n| Hierarchical         | GPT-4-Turbo                   | 88.61| 88.86| 85.68| 87.79| 87.73|\\n\\nTable 11: Metric BERTScore of different LLMs performance on character profiling.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Margot Davies is a determined and skilled female reporter with...\"}"}
{"id": "emnlp-2024-main-456", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I: Init\\n\\n/* Data */\\n\\nBelow is the beginning part of a story:\\n- - -\\n{}\\n- - -\\n\\n/* Task prompt */\\nWe are going over segments of a story sequentially to gradually update one comprehensive summary of the character {}.\\n\\nWrite a summary for the excerpt provided above, make sure to include vital information related to gender, skills, talents, objectives, background, relationships, key events, and personality of this character. You must briefly introduce characters, places, and other major elements if they are being mentioned for the first time in the summary. The story may feature non-linear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the summary so it presents a consistent and chronological narrative. Despite this step-by-step process of updating the summary, you need to create a summary that seems as though it is written in one go. The summary must be within {} words and could include multiple paragraphs.\\n\\n/* Output Format */\\n\\nOutput your summary into four specific sections, ...\\n\\nII: Update\\n\\n/* Data */\\n\\nBelow is a segment from a story:\\n- - -\\n{}\\n- - -\\n\\nBelow is a summary of the character {} of the story up until this point:\\n- - -\\n{}\\n- - -\\n\\n/* Task prompt */\\nWe are going over segments of a story sequentially to gradually update one comprehensive summary of the character {}.\\n\\nYou are required to update the summary to incorporate any new vital information in the current excerpt. This information may relate to gender, skills, talents, objectives, background, relationships, key events, and personality of this character. You must briefly introduce characters, places, and other major elements if they are being mentioned for the first time in the summary. The story may feature non-linear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the summary so it presents a consistent and chronological narrative. Despite this step-by-step process of updating the summary, you need to create a summary that seems as though it is written in one go. The updated summary must be within {} words and could include multiple paragraphs.\\n\\n/* Output Format */\\n\\nOutput your summary into four specific sections, ...\\n\\nIII: Compress\\n\\n/* Data */\\n\\nBelow is a segment from a story:\\n- - -\\n{}\\n- - -\\n\\n/* Task prompt */\\nCurrently, this summary contains {} words. Your task is to condense it to less than {} words. The condensed summary should remain clear, overarching, and fluid while being brief. Whenever feasible, maintain details about gender, skills, talents, objectives, background, relationships, key events, and personality about this character - but express these elements more succinctly. Make sure to provide a brief introduction to characters, places, and other major components during their first mention in the condensed summary. Remove insignificant details that do not add much to the character portrayal. The story may feature non-linear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the summary so it presents a consistent and chronological narrative.\\n\\n/* Output Format */\\n\\nOutput your summary into four specific sections, ...\\n\\nCondensed summary (to be within {} words):\\n\\nTable 13: Prompt templates for incremental updating.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I: Init\\n\\n/* Data */\\nBelow is a part of a story:\\n- - -\\n{}\\n- - -\\n/* Task prompt */\\nWe are creating one comprehensive summary for the character {} by recursively merging summaries of its chunks. Now,\\nwrite a summary for the excerpt provided above, make sure to include vital information related to gender, skills, talents,\\nobjectives, background, relationships, key events, and personality of this character. You must briefly introduce characters,\\nplaces, and other major elements if they are being mentioned for the first time in the summary. The story may feature non-\\nlinear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the\\nsummary so it presents a consistent and chronological narrative. Despite this recursive merging process, you need to create\\na summary that seems as though it is written in one go. The summary must be within {} words and could include multiple\\nparagraphs.\\n\\n/* Output Format */\\nOutput your summary into four specific sections, ...\\n\\nII: Merge\\n\\n/* Data */\\nBelow are several summaries of the character {} from consecutive parts of a story:\\n- - -\\n{}\\n- - -\\n/* Task prompt */\\nWe are creating one comprehensive summary for the character {} by recursively merging summaries of its chunks. Now,\\nmerge the given summaries into one single summary, make sure to include vital information related to gender, skills, talents,\\nobjectives, background, relationships, key events, and personality of this character. You must briefly introduce characters,\\nplaces, and other major elements if they are being mentioned for the first time in the summary. The story may feature non-\\nlinear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you should organize the\\nsummary so it presents a consistent and chronological narrative. Despite this recursive merging process, you need to create\\na summary that seems as though it is written in one go. The summary must be within {} words and could include multiple\\nparagraphs.\\n\\n/* Output Format */\\nOutput your summary into four specific sections, ...\\n\\nIII: Merge Context\\n\\n/* Data */\\nBelow is a summary of the context about the character {} preceding some parts of a story:\\n- - -\\n{}\\n- - -\\nBelow are several summaries of the character {} from consecutive parts of the story:\\n- - -\\n{}\\n- - -\\n/* Task prompt */\\nWe are creating one comprehensive summary of the character {} by recursively merging summaries of its chunks. Now,\\nmerge the preceding context and the summaries into one single summary, make sure to include vital information related to\\ngender, skills, talents, objectives, background, relationships, key events, and personality of this character. You must briefly\\nintroduce characters, places, and other major elements if they are being mentioned for the first time in the summary. The\\nstory may feature non-linear narratives, flashbacks, switches between alternate worlds or viewpoints, etc. Therefore, you\\nshould organize the summary so it presents a consistent and chronological narrative. Despite this recursive merging process,\\nyou need to create a summary that seems as though it is written in one go. The summary must be within {} words and could\\ninclude multiple paragraphs.\\n\\n/* Output Format */\\nOutput your summary into four specific sections, ...\\n\\nSummary:\"}"}
{"id": "emnlp-2024-main-456", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I: Consistency Score\\n\\n/* Task prompt */\\nYou are a character extraction performance comparison assistant. You will be given the golden information about character {}'s dimension in a novel. You will then be given the summarized information about character {} extracted by a model from the origin novel.\\n\\nYour task is to rate the summarized information on one metric.\\n\\nPlease make sure you read and understand these instructions carefully.\\n\\nEvaluation Criteria:\\nConsistency (1-5) - the factual alignment between the golden and the summarized information. A score of 1 indicates significant discrepancies, while a score of 5 signifies a high level of factual consistency.\\n\\nEvaluation Steps:\\n1. Read the golden information carefully and identify the main facts and details it presents.\\n2. Read the summarized information and compare it to the golden information. Check if the summary contains any factual errors or lacks necessary foundational facts. If the summarized one includes information not mentioned in the golden information, please ignore it, as the summary is extracted from the original book and may contain more extraneous information.\\n3. Assign a score for consistency based on the Evaluation Criteria and explain the reason. Your output should be structured as the following schema: `{{\"score\": int // A score range from 1 to 5, \"reason\": string // The reason of evaluation result}}`\\n\\n/* Data */\\nGolden information:\\n{}\\n\\nSummarized information:\\n{}\\n\\n/* Output Format */\\nEvaluation Form (Please output the result in JSON format. Do not output anything except for the evaluation result. All output must be in JSON format and follow the schema specified above.):\\n- Consistency:\\n  \\n  ```json\\n  { \\\"score\\\": 3, \\\"reason\\\": \\\"The summarized information is partially consistent with the golden information, ... \\\" }\\n  ```\\n\\nTable 15: Prompt templates for factual consistency examination. Generated texts by GPT-4 are highlighted.\\n\\n8035\"}"}
{"id": "emnlp-2024-main-456", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I: Normal\\n\\n/* Task prompt */\\nYou are a helpful assistant proficient in analyzing the motivation for the character's decision in novels. You will be given the profile about character {} in a novel. Your task is to choose the most accurate primary motivation for the character's decision according to the character's profile. You also need to provide reasons, the reasons should be related to the character's basic attributes, experiences, relationships, or personality, of this character.\\n\\nYour output should be structured as the following schema:\\n\\n{{\\\"Choice\\\": str // \\\"A\\\"/\\\"B\\\"/\\\"C\\\"/\\\"D\\\", \\\"Reason\\\": string // The reason of the choice}}\\n\\n/* Data */\\nCharacter Profile:\\nname: {}\\nSummary of this character: {}\\nQuestion: {}\\n\\n/* Output Format */\\nOutput (All output must be in JSON format and follow the schema specified above.):\\n\\n{\\n  \\\"Choice\\\": \\\"A\\\",\\n  \\\"Reason\\\": \\\"Margot's primary motivation for ...\\\"\\n}\\n\\nII: Ablate All Dimensions\\n\\n/* Task prompt */\\nYou are a helpful assistant proficient in analyzing the motivation for the character's decision in novels. Your task is to choose the most accurate primary motivation for the character's decision according to the character's profile. Since you are not given the character analysis, you are supposed to choose the most reasonable motivation based on the provided information in the question.\\n\\nYour output should be structured as the following schema:\\n\\n{{\\\"Choice\\\": str // \\\"A\\\"/\\\"B\\\"/\\\"C\\\"/\\\"D\\\", \\\"Reason\\\": string // The reason of the choice}}\\n\\n/* Data */\\nCharacter Profile:\\nname: {}\\nQuestion: {}\\n\\n/* Output Format */\\nOutput (All output must be in JSON format and follow the schema specified above.):\\n\\n{\\n  \\\"Choice\\\": \\\"A\\\",\\n  \\\"Reason\\\": \\\"Given the lack of specific information about Margot, ...\\\"\\n}\\n\\nTable 16: Prompt templates for motivation recognition. Generated texts by GPT-4 are highlighted.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating Character Understanding of Large Language Models via Character Profiling from Fictional Works\\n\\nXinfeng Yuan\u2661, Siyu Yuan\u2661\u2217, Yuhan Cui\u2660\u2217, Tianhe Lin\u2661, Xintao Wang\u2660, Rui Xu\u2660, Jiangjie Chen\u2660, Deqing Yang\u2661\u2020\\n\\nSchool of Data Science, Fudan University\\nShanghai Key Laboratory of Data Science, School of Computer Science, Fudan University\\n{xfyuan23, syyuan21, xtwang21, ruixu21}@m.fudan.edu.cn\\n{yhcui20, thlin20, jjchen19, yangdeqing}@fudan.edu.cn\\n\\nAbstract\\n\\nLarge language models (LLMs) have demonstrated impressive performance and spurred numerous AI applications, in which role-playing agents (RPAs) are particularly popular, especially for fictional characters. The prerequisite for these RPAs lies in the capability of LLMs to understand characters from fictional works. Previous efforts have evaluated this capability via basic classification tasks or characteristic imitation, failing to capture the nuanced character understanding with LLMs. In this paper, we propose evaluating LLMs' character understanding capability via the character profiling task, i.e., summarizing character profiles from corresponding materials, a widely adopted yet understudied practice for RPA development. Specifically, we construct the CROSS dataset from literature experts and assess the generated profiles by comparing them with ground truth references and evaluating their applicability in downstream tasks. Our experiments, which cover various summarization methods and LLMs, have yielded promising results. These results strongly validate the character understanding capability of LLMs.\\n\\nResources are available at https://github.com/Joanna0123/character_profiling.\\n\\n1 Introduction\\n\\nThe recent progress in large language models (LLMs) (OpenAI, 2023; Anthropic, 2024) has catalyzed numerous AI applications, among which role-playing agents (RPAs) have attracted a wide range of audiences. RPAs are interactive AI systems that simulate various personas for applications, including chatbots of fictional characters (Wang et al., 2023c), AI none player characters in video games (Wang et al., 2023a), and digital replicas of real humans (Gao et al., 2023a). In practice, LLMs are generally prompted with character profiles to role-play fictional characters (Wang et al., 2023b; Zhao et al., 2023), and these profiles are typically generated through the automatic summarization of corresponding literature using advanced LLMs (Wang et al., 2023c; Li et al., 2023a).\\n\\nPrevious efforts have studied LLMs' capabilities of understanding characters from fictional works. The research on character understanding mainly concentrates on basic classification tasks, such as character prediction (Brahman et al., 2021; Yu et al., 2022; Li et al., 2023b) and personality prediction (Yu et al., 2023), which aims at recognizing characters or predicting their traits from given contexts correspondingly. Recently, the research focus has shifted to character role-playing, primarily focusing on the imitation of characteristics such as knowledge (Tang et al., 2024; Shen et al., 2023) and linguistic style (Zhou et al., 2023; Wang et al., 2023c). Hence, these tasks fail to capture the nuanced character understanding of LLMs.\\n\\nIn this paper, we systematically evaluate LLMs' capability on the character profiling task, i.e., summarizing profiles for characters from fictional works. For research, character profiling is indeed the first task to explore the depth of LLMs' character understanding via generation. This is more challenging than previous classification tasks, contributing to a more nuanced comprehension of how LLMs understand the character. In practice, the character profiles generated by LLMs have been widely adopted for RPA development (Wang et al., 2023c; Li et al., 2023a; Xu et al., 2024), and have the potential to facilitate human understanding of characters, but their effectiveness remains significantly understudied. Our work in this paper aims to evaluate LLMs' performance on character profiling, of which the challenges mainly include the absence of high-quality datasets and evaluation protocols.\\n\\nTo address these challenges, we construct the CROSS (Character Profiles from SuperSummary) dataset for character profiling, and propose two tasks to evaluate the generated profiles. The\"}"}
{"id": "emnlp-2024-main-456", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Profile of Harry Potter\\nAttributes: Harry Potter is an English half-blood wizard, and one of the most famous wizards of modern times. When he was eleven years old, he was sent to live with his Aunt and Uncle. He spent much of his childhood being abused by his family, and only found solace in reading the books he received for his birthday. At the age of eleven, he learned that he was a wizard and was accepted to attend Hogwarts School of Witchcraft and Wizardry. When he arrived, he found his best friends were Ron Weasley and Hermione Granger. In his fourth year at Hogwarts, Harry won the Triwizard Tournament and the Goblet of Fire, which caused much speculation among the students. Harry is a brave, loyal, and selfless person. He possesses an instinctual, intuitive intelligence. He is fond of Quidditch and flying on his broomstick. Harry is ultimately a Harry Potter - Harry Potter.\\n\\nScenarios\\nTowards the end of the book, Harry decides to keep the Elder Wand's true allegiance a secret.\\n\\nQuestion: Why does Harry decide to keep the Elder Wand's allegiance a secret?\\nA. He plans to use its power for personal gain.\\nB. To prevent future attempts by others to claim its power and ensure it cannot be used for evil again.\\nC. Because he does not fully understand how the wand's allegiance works.\\nD. He fears others will see him as weak if they know the wand's allegiance can change.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paper, we explore the capabilities of LLMs in generating character profiles for RPAs construction.\\n\\nMotivation Analysis & Character Understanding\\n\\nMotivation is a fundamental concept, which is shaped by personality traits and the immediate surroundings (Young, 1961; Atkinson, 1964; Kleinginna Jr and Kleinginna, 1981). In narrative texts, the motivation of a character can reveal their inner traits and their relationship with the external world. Thus, understanding the motivation of characters strongly aligns with the LLMs' ability to comprehend characters. Previous studies typically propose benchmarks in character identification (Chen and Choi, 2016; Brahman et al., 2021; Sang et al., 2022; Yu et al., 2022), situated personality prediction (Yu et al., 2023), question answering (Ko\u02c7cisk`y et al., 2018). Despite these efforts, prior research has not focused on assessing a character's motivation based on character profiles. To bridge this gap, we propose the motivation recognition task. This task aims to directly evaluate whether LLMs can grasp a character's essence by identifying the motivations behind each decision within a story.\\n\\n3 Character Profiling Framework\\n\\n3.1 Task Formulation\\n\\nCharacter profiling aims to generate profiles for fictional characters from corresponding literature. Given the input character name $N$ and the original content $B$ of a fictional work, the LLM should output the character profile $P$ which covers the core information about the character. Specifically, in this paper, $P = (P_{attributes}, P_{relationships}, P_{events}, P_{personality})$ is structured in four dimensions, as detailed in Section 3.2. An example of a character profile is presented in Figure 1.\\n\\n3.2 Character Profile Dimensions\\n\\nFor a character, the profile should be highly complex and multi-faceted, embodying diverse information. Drawing inspiration from previous studies and current developments in persona products (Zhao et al., 2023; Baichuan, 2023), we define four main profile dimensions for LLMs to summarize, which are commonly examined in literary studies (Yu et al., 2023; Zhao et al., 2024; Shen et al., 2023). Please refer to Appendix A for a further comparison.\\n\\nAttributes\\n\\nThe basic attributes of a character encompass gender, skills, talents, objectives, and background.\\n\\nRelationships\\n\\nA character's interpersonal relationships are a vital aspect of their profile, which are intimately connected to the character's experiences and personality. Moreover, these relationships can serve as a foundation for constructing fictional character relationship diagrams.\\n\\nEvents\\n\\nEvents cover the experiences that characters have been part of or impacted by, marking a critical profile dimension. Due to the complexity of certain narratives, such as alternating timelines and showcasing events from diverse worlds or different perspectives, we require the model to rearrange events and order them chronologically.\\n\\nPersonality\\n\\nPersonality refers to the lasting set of characteristics and behaviors that form an individual's unique way of adapting to life (American Psychological Association, 2018). While well-rounded characters often exhibit complex personalities, their personalities can be analyzed through their actions, choices, and interactions with others.\\n\\n3.3 Summarization Methods\\n\\nBook-length texts often comprise over 100,000 tokens, surpassing the context window limitations of many current LLMs. As a result, the primary framework for long context processing involves segmenting books into manageable segments for LLMs, followed by subsequent comprehensive processing. As illustrated in Figure 2a and Figure 2b, we inherit two methods for book summarization (Chang et al., 2023), i.e., hierarchical merging and incremental updating. Additionally, for models that can handle long context windows, we explore the method of summarizing in one go, as shown in Figure 2c.\\n\\nHierarchical Merging\\n\\nThe hierarchical merging approach (Wu et al., 2021) employs a simple, zero-shot prompt technique. It begins by summarizing information from segments within a book, generating the summaries at level 1. Then, several summaries are combined to establish the initial context at level 2. Subsequently, it merges the following summaries with context iteratively. The merging process continues at the next level until a final summary is generated.\\n\\nIncremental Updating\\n\\nOne major issue with hierarchical methods lies in constructing summaries...\"}"}
{"id": "emnlp-2024-main-456", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"at level 1. As shown in Figure 2a, the provided text only contains novel content from the current segment without any background information from earlier segments. Thus, this absence of context may increase the risk of misinterpreting information in later segments.\\n\\nIn response, Chang et al. (2023) introduces incremental updating. This method leverages background information from the preceding text to enhance summary quality. The process of incremental updating consists of three phases: First, it starts by summarizing the book's opening segment. Then, this summary is refined and updated by incorporating details from the following segments recursively. Throughout this process, to ensure conciseness and relevance, the summary is periodically condensed to comply with a set pre-defined maximum length. By following these steps, the method seeks to promote a more integrated and coherent comprehension of the entire text.\\n\\nRecent developments in LLMs have introduced models capable of processing over 100,000 tokens. For example, GPT-4-Turbo (OpenAI, 2023) supports a context window of up to 128,000 tokens. This advancement enables us to explore a method for inputting the full content of a book into the model in one step. For this investigation, we select books from our dataset that contain fewer than 120,000 tokens.\\n\\n4 Evaluation Protocol\\n\\n4.1 Evaluation Tasks\\n\\nIntrinsic Evaluation: Factual Consistency Examination (FCE)\\n\\nTo generate character profiles...\\n\\nIn this paper, unless otherwise specified, we adopt the version of GPT-4-Turbo-0125 throughout.\\n\\nTable 1: A toy example of MR task. A complete set of data includes character name, character profile, scenario, question, options, correct answer, and reason. The reasoning model is GPT-4-Turbo-0409.\\n\\n| Scenario       | Question                                      | Options                                                                 |\\n|----------------|-----------------------------------------------|-------------------------------------------------------------------------|\\n| Nora decides not to pursue a long-distance relationship with Charlie after the summer ends. | Why does Nora make the decision to part ways with Charlie?               | A. Because she fears long-distance relationships are doomed to fail.     |\\n|                |                                               | B. Because she believes she needs to focus on her personal growth and independence. |\\n|                |                                               | C. Because she feels their goals and aspirations are no longer aligned.   |\\n|                |                                               | D. Because she worries that their frequent arguments are harming her well-being. |\\n\\nModel Reasoning Output\\n\\nChoice: \\\"A\\\", Reason: Nora's decision is primarily motivated by her fear that long-distance relationships are doomed to fail. Given her history of being dumped and her protective nature due to her family's past, Nora is likely cautious about entering a relationship that has inherent challenges and uncertainties...\"}"}
{"id": "emnlp-2024-main-456", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Extrinsic Evaluation: Motivation Recognition (MR) As shown in Table 1, to thoroughly evaluate whether the summarized profiles enhance models' understanding of a character's essence, we introduce a Motivation Recognition task for downstream evaluation. This task investigates if the character profiles generated by the model effectively aid in comprehending the characters, particularly in recognizing the motivations behind their decisions.\\n\\nGiven the input $X = (N, P, D, Q, A)$, which includes the character name $N$, the character profile $P$ defined by four dimensions, the character's decision $D$, a question $Q$ about the motivations behind the decision, and a set of potential answer $A = \\\\{a_i\\\\}_{i=1}^{4}$ for $Q$, the LLMs should determine the answer $Y$ from $A$ that correctly reflects the character's motivation. Details of MR dataset construction are provided in Section 4.3.\\n\\n4.2 Evaluation Metrics\\n\\nMetric for FCE: Consistency Score\\n\\nAs demonstrated in a previous study (Goyal et al., 2022), current reference-based automatic metrics like ROUGE metric (Lin, 2004) exhibit a significantly low correlation with human judgment for summaries generated by GPT-3. Therefore, we adopt the evaluation method used in recent research (Liu et al., 2023; Gao et al., 2023b; Li et al., 2024), utilizing an LLM as an evaluator to improve alignment with human perception and reduce cost. Specifically, we introduce Consistency Score, which is the degree of factual consistency between the reference profiles and the summaries generated by LLMs, evaluated by Llama-3-70B. We ask Llama-3-70B to assign a score on a scale from 1 to 5, reflecting the accuracy of the summaries in capturing the essential factual details. A higher score indicates a closer match to the factual content.\\n\\nTo evaluate the quality of the LLM evaluation, we randomly select 50 samples for human evaluation. We calculate the Pearson Correlation Coefficient (Cohen et al., 2009) between the consistency score result of human annotators and Llama-3-70B. The coefficient value of $0.752$ with the $p$-value $= 4.3e^{-12} < 0.05$ suggests that these two sets of results have a significant correlation. This validates that the evaluation capabilities of Llama-3-70B for this task are comparable to those of humans.\\n\\nMetric for MR: Accuracy\\n\\nMultiple-choice questions can be easily evaluated by examining the choice of models. We define $\\\\text{Acc}$ as the accuracy across the entire question dataset.\\n\\n4.3 CROSS Dataset Construction\\n\\nBook Dataset To reduce the confounding effect of book memorization on the results, we select 126 high-quality novels published in 2022 and 2023. In fact, as shown in Appendix B.2, we find that there is no significant correlation between the year of publication and the consistency score for works from the past ten years. For each novel, we concentrate solely on its main character. We manually remove sections not pertinent to the novel's original content, such as prefaces, acknowledgements, and author introductions. Additionally, we select 47 books within CROSS containing fewer than 120,000 tokens for the summarizing-in-one-go method.\\n\\nGolden Character Profile Extraction\\n\\nThe golden character profiles are gathered from the SuperSummary website, known for its high-quality plot summaries and character analyses conducted by literary experts. With permission from the site, we utilize their book summaries, chapter summaries, and character analyses. The original character analyses from SuperSummary lack a standardized format and predefined profile dimensions. Therefore, we utilize GPT-4 to reorganize the original summaries.\\n\\nGiven the original plot summaries and character analyses, we require the model to reorganize character profiles across four main dimensions while ensuring no critical details are overlooked. To guarantee the quality of the reorganized profiles, two annotators evaluate whether the reorganized profiles adequately retained the essential information from the original text. The assessment reveals that all results exhibit a high level of informational integrity and consistency, confirming the credibility of the reorganized profiles.\\n\\nMR Dataset Construction\\n\\nUsing resources from the SuperSummary website, we develop motivation questions.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Results of different LLMs performance on character profiling and motivation recognition.\\n\\n| Method                  | Model                                        | Consistency Score | MR Acc. | Attr | Rela | Even | Pers | Avg. |\\n|-------------------------|----------------------------------------------|-------------------|---------|------|------|------|------|------|\\n| **CROSS (Full dataset)**| Mistral-7B-Instruct-v0.2                      | 2.75              | 2.20    | 1.88 | 3.89 | 2.68 | 48.31|\\n|                         | Mixtral-8x7B-MoE                             | 2.75              | 2.58    | 2.28 | 4.02 | 2.91 | 52.13|\\n|                         | vicuna-7b-v1.5-16k                           | 2.44              | 1.72    | 1.45 | 3.17 | 2.20 | 42.70|\\n|                         | vicuna-13b-v1.5-16k                          | 2.79              | 2.22    | 1.76 | 3.56 | 2.58 | 46.29|\\n|                         | Qwen1.5-7B-Chat                              | 2.35              | 1.98    | 1.58 | 3.75 | 2.42 | 44.49|\\n|                         | Qwen1.5-14B-Chat                             | 2.39              | 2.18    | 1.41 | 3.74 | 2.43 | 47.42|\\n|                         | Qwen1.5-72B-Chat                             | 3.33              | 2.71    | 2.45 | 4.08 | 3.14 | 52.36|\\n|                         | GPT-3.5-Turbo                                | 3.49              | 2.57    | 1.95 | 3.95 | 2.99 | 49.44|\\n|                         | GPT-4-Turbo                                  | 3.72              | 3.24    | 3.58 | 3.87 | 3.60 | 57.75|\\n| **Incremental Updating**| Mistral-7B-Instruct-v0.2                      | 3.07              | 2.20    | 1.98 | 3.83 | 2.77 | 50.56|\\n|                         | Mixtral-8x7B-MoE                             | 3.17              | 2.59    | 2.03 | 3.93 | 2.93 | 48.09|\\n|                         | vicuna-7b-v1.5-16k                           | 2.40              | 1.77    | 1.40 | 3.08 | 2.16 | 44.94|\\n|                         | vicuna-13b-v1.5-16k                          | 2.91              | 2.12    | 1.54 | 3.27 | 2.46 | 45.39|\\n|                         | Qwen1.5-7B-Chat                              | 3.05              | 2.37    | 1.88 | 3.83 | 2.78 | 44.04|\\n|                         | Qwen1.5-14B-Chat                             | 3.29              | 2.70    | 2.21 | 4.04 | 3.06 | 47.42|\\n|                         | Qwen1.5-72B-Chat                             | 3.67              | 2.97    | 2.98 | 4.21 | 3.46 | 54.61|\\n|                         | GPT-3.5-Turbo                                | 3.29              | 2.87    | 2.17 | 3.90 | 3.06 | 51.69|\\n|                         | GPT-4-Turbo                                  | 3.81              | 3.48    | 3.36 | 4.23 | 3.72 | 53.71|\\n\\n### Table 3: Results of Motivation Recognition ablation study. The reasoning model by default is GPT-4-Turbo-0409.\\n\\n| Ablation Dimension | ref. Reasoned by Ablation Dimension Acc. | Std. % | ref. Reasoned by Ablation Dimension Acc. |\\n|-------------------|-----------------------------------------|--------|----------------------------------------|\\n|                   | Generated Profile (GPT-4-Turbo + incremental updating) | 57.75  | 0.32                                    |\\n|                   | Reference Profile in CROSS               | -      | -                                       |\\n\\nThe process involves four main steps: First, we utilize GPT-4 to generate several motivation recognition multiple-choice questions (MCQs) and manually select the top 10 examples. Second, we identify a primary character from each of the 126 books and formulate questions related to them. Given the character's name, chapter summaries from the SuperSummary, and the 10 examples, GPT-4 is instructed to generate a set of motivation recognition multiple-choice questions. Each question is designed to include a decision made by the character within a specific scenario, offering four options, the correct option, and justifications for why each option is correct or incorrect. Through this process, GPT-4 generates a total of 641 questions for the 126 characters. Moreover, we find that some questions can be easily answered using common-sense knowledge or grammatical structure. Thus, given a question and the correct answer, we ask GPT-4 to provide three likely motivations behind the decision in the question that differ from the correct answer. These options, meant to confuse, are similar to the correct answer in sentence structure. We replace the incorrect options generated in the previous step with these three motivations. To maintain the quality of MR questions, two annotators are assigned to filter them, with Fleiss's $\\\\kappa = 0.91$ (Fleiss et al., 1981). According to the annotation results, 445 out of the 641 questions meet the established criteria, guaranteeing the quality of the questions.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benjamin's relationships are complex and multi-faceted. He is married to Mildred, a woman of delicate health and refined tastes. Rask marries Helen Brevoort, a woman from an old-money New York family with a similarly reserved personality.\\n\\nBenjamin's life takes a dramatic turn when he saves his grandson, Waldo, during an unexpected home birth. Benjamin's role as a caregiver extends beyond his family when he helps deliver Waldo Shenkman, his neighbor's son.\\n\\nBobby Western's relationships are complex, featuring camaraderie with colleagues like Oiler and Red, a controversial bond with his sister, and deep connections with figures such as Heaven, Asher, Granellen.\\n\\nBobby's most significant relationships are with his sister Alicia, who suffers from schizophrenia and eventually dies by suicide, and his father, a renowned physicist.\\n\\nAvery continues her work, focusing on helping clients like Marissa and Matthew Bishop navigate their marital issues. Avery encounters various challenges, including dealing with Skylar's unexpected visit. Matthew is orchestrating these events as part of a revenge plot against Marissa and her affair partner, Skip, whom Avery briefly dated. It's orchestrated by a pharmaceutical company, Acelia, seeking retribution against Avery for whistleblowing.\\n\\nIn the wake of Mildred's death, Benjamin's life takes a turn towards solitude and reflection. He begins to work on his autobiography with the help of Ida Partenza. Returning to New York, Rask realizes his wife's death has little impact on his life. He continues investing but never replicates his earlier success, returning to the solitary, dispassionate life.\\n\\nMillie's history with Enzo and her relationship with Brock add complexity as she aids Wendy in escaping Douglas's control, accidentally killing Douglas in the process. Millie ends up shooting a man she believes to be Douglas during a violent altercation, only to discover later that the man was actually Russell Simmons.\\n\\nAva is introspective, self-aware, and morally driven, with a strong desire for acceptance. She's empathetic but guarded, resourceful in adversity, and adept at navigating complex social situations. Ava is adept at manipulating situations to her advantage, portraying herself as vulnerable to deceive others while secretly harboring a willingness to commit fraud to achieve her goals.\\n\\nJune Hayward is introspective, ambitious, and somewhat cynical. She navigates her literary career with determination and vulnerability, showing resilience in the face of criticism and a deep appreciation for her moments of success. June Hayward is characterized by her intense jealousy, ambition, and insecurity. She is manipulative, willing to betray close relationships and ethical boundaries to achieve literary success.\\n\\nTable 4: A case study on common errors generated by models in the character profiling task.\\n\\nExperiment Settings\\nModels for Summarization\\nFor the incremental and hierarchical methods, we experiment with the following LLMs: Mistral-7B-Instruct-v0.2 (Jiang et al., 2023), Mixtral-8x7B-MoE (Jiang et al., 2024), Qwen1.5-7B-Chat, Qwen1.5-14B-Chat, Qwen1.5-72B-Chat (Bai et al., 2023), vicuna-7b-v1.5-16k, vicuna-13b-v1.5-16k (Zheng et al., 2024), GPT-3.5-Turbo-0125 and GPT-4-Turbo-0125. We set the chunk size to 3000 tokens for both methods. We require that the complete profile generated by the model contain no more than 1200 words. For the summarizing-in-one-go method, we experiment with the GPT-4-Turbo-0125 and Claude-3-Sonnet (Anthropic, 2024). For all these models, we adopt the original model and official instruction formats. The temperatures of all these models are set to 0 in our experiments.\\n\\nMR Task Setting\\nWe assess the quality of profiles summarized under different models and methods through the accuracy rate on MR tasks. We uniformly employ GPT-4-Turbo-0409 as the reasoning model for this specific task. Furthermore, we study human performance in the MR task supported by reference profile in CROSS dataset. We employ two human annotators to answer all the questions and calculate the average accuracy and standard deviation.\\n\\nDimension Ablation Study\\nTo further explore the impact of different dimensions of character information on the MR task, we conduct an analysis through ablation experiments as shown in Table 3, using character profiles summarized via the incremental method with GPT-4. Each experiment is repeated three times, and we report the average and...\"}"}
{"id": "emnlp-2024-main-456", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the experiments, we wish to answer two research questions:\\n\\nRQ1) Can LLMs generate character profiles from fictional works precisely?\\n\\nRQ2) Can LLMs recognize the character's motivation for a specific decision based on the character profile?\\n\\n6.1 Can LLMs generate character profiles from fictional works precisely?\\n\\nExperiment result in Table 2 shows that:\\n\\n1) LLMs generally exhibit promising performance in generating character profiles from fiction. Among all models, GPT-4 consistently outperforms other models across various methods, exhibiting the advanced capability of LLMs to accurately summarize character profiles.\\n\\n2) Despite GPT-4, larger and more complex LLMs, such as Qwen1.5-72B-Chat, tend to achieve higher consistency scores.\\n\\n3) There are variations in model performance across different dimensions. For example, LLMs typically achieve higher consistency scores in capturing personality but are less effective at summarizing event-related information.\\n\\nSummarization Method Comparison\\n\\nWe compare the outcomes of the incremental and hierarchical methods across the full CORSS. For 47 books containing fewer than 120,000 tokens in CORSS, we include the summarizing-in-one-go method. The results in Table 2 show that the summarizing-in-one-go method achieves the highest consistency scores in all dimensions, surpassing methods that process content in segments. We believe this success stems from processing the entire content of a book at once, which maintains the narrative's coherence and minimizes information loss. Additionally, since character details are unevenly distributed throughout fiction, summarizing the text in one step allows the model to focus more effectively on the essential elements of the narrative.\\n\\nThe incremental updating method, while slightly lagging in average consistency, performs better in events than hierarchical summarizing. This performance can be attributed to its iterative updating nature, which allows the model to refine and update its understanding as more information becomes available or as errors are corrected in subsequent passes. This finding aligns with those reported by Chang et al. (2023), which indicate that book summaries generated by the incremental method surpass those produced by the hierarchical method in terms of detail.\\n\\nError Analysis\\n\\nWe conduct a case study to further investigate why LLMs fail to generate the correct character profile. We define five types of errors, i.e., 1) Character Misidentification, which occurs when characters are mistaken for one another, leading to confusion about their actions or roles. 2) Relationship Misidentification, an error where the type of relationship between characters is inaccurately represented. 3) Omission of Key Information, a common error where the significant relationships or events are overlooked while less important information is described in excessive detail. 4) Event Misinterpretation, events are incorrectly interpreted, or earlier interpretations are not adequately revised in light of subsequent revelations. 5) Character Misinterpretation, where the motives or traits of a character are incorrectly summarized, resulting in a cognitive bias in the understanding of a character's overall image.\\n\\nAs shown in Table 4, a key finding is that the model often becomes confused and generates illusions when faced with complex narrative structures. For example, in the book \\\"Trust\\\", the character Benjamin Rask is a figure in the novel \\\"Bonds\\\" which is part of \\\"Trust\\\". The prototype for Rask is another character, Andrew Bevel, from \\\"Trust\\\". Due to frequent shifts in narrative perspective, the model confuses Rask with Bevel, mistakenly attributing Bevel's traits to Rask. The errors are shown in the first examples of Character Misidentification and Event Misinterpretation. Another example occurs in \\\"The Housemaid's Secret\\\", where the model fails to understand the plot twist, which results in an incorrect final summary. This error is shown in the second case of Event Misinterpretation.\\n\\n6.2 Can LLMs recognize the character's motivation for a specific decision?\\n\\nOverall Performance\\n\\nAs shown in Table 2, profiles generated by GPT-4 through incremental method enable the model to achieve the highest accuracy (57.75%), which is slightly lower than that of the reference profiles (63.07%) shown in Table 3, indicating the effectiveness of the generated profiles in enhancing character comprehension. Additionally, based on the human annotators' results (72.58%), GPT-4 still shows a performance gap compared to humans in this task.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Moreover, a strong positive correlation is observed between the consistency scores and the MR accuracy of the profiles summarized by the model. This finding supports the validity of character profiling, suggesting that accurate character profiles help models better understand the motivations behind a character's behavior.\\n\\nAmong the three summarization methods, profiles from hierarchical merging exhibit relatively low accuracy on the MR task. It is also found that despite high scores in other dimensions, the consistency score for the events obtained through the hierarchical method is relatively low. This indirectly suggests that the quality of events has a more significant influence on the MR task.\\n\\nAblation Study on MR\\n\\nAs Table 3 demonstrates, the results of the ablation experiments reveal that each of the four dimensions within the profile contributes to the downstream task. Among these, the dimension of the event is the most critical. Excluding this dimension alone leads to a notable decrease in accuracy (\u22129.21%). The rationale behind this is that events contain substantial plot-related information, which assists the model in grasping the background knowledge pertinent to the characters' decision-making processes. Additionally, events integrate elements from the other dimensions, offering a holistic depiction of character personas. However, omitting the other dimensions has a less pronounced impact. We also observe that reducing the amount of information in the profile correlates with greater variance in experimental outcomes, suggesting that the model becomes less stable as it processes less detailed profiles.\\n\\n7 Conclusion\\n\\nWe introduce the first task for assessing the character profiling ability of large language models (LLMs), using a dataset of 126 character profiles from novels. Our evaluation, which includes the Factual Consistency Examination and Motivation Recognition, reveals that LLMs generally perform well. However, even the most advanced models occasionally generate hallucinations and errors, particularly with complex narratives, highlighting the need for further improvement.\\n\\nLimitations\\n\\nIn this paper, we only explore four common dimensions for character profiles, thus leaving other potential dimensions unexplored. This limitation suggests that future work could expand the scope to include a wider range of dimensions and investigate their effects on downstream tasks.\\n\\nAnother limitation of our work stems from potential biases in the evaluation process. Despite selecting highly contemporaneous data to prevent data leakage, it is still possible that some models might have been trained on these specific books. Besides, the evaluation metrics used in this paper rely on the evaluator LLMs, potentially compromising the accuracy of the results due to errors inherent in these models, which could result in a biased estimation of profile consistency. Moreover, while we test the three most popular summarization methods, we acknowledge that there is potential for improvement in the design of these methods to maximize the character profiling capabilities of LLMs.\\n\\nEthics Statement\\n\\nUse of Human Annotations\\n\\nOur institution recruits annotators to implement the annotations of motivation recognition dataset construction. We ensure the privacy rights of the annotators are respected during the annotation process. The annotators receive compensation exceeding the local minimum wage and have consented to the use of motivation recognition data processed by them for research purposes. Appendix D provides further details on the annotations.\\n\\nRisks\\n\\nThe CRoss dataset in our experiment is sourced from publicly available sources. However, we cannot guarantee that they are devoid of socially harmful or toxic language. Furthermore, evaluating the data quality of the motivation recognition dataset is based on common sense, which can vary among individuals from diverse backgrounds. We use ChatGPT (OpenAI, 2022) to correct grammatical errors in this paper.\\n\\nAcknowledgements\\n\\nWe thank the anonymous reviewers for their valuable suggestions. This work was supported by the Chinese NSF Major Research Plan (No.92270121).\\n\\nReferences\\n\\nAI@Meta. 2024. Llama 3 model card.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anthropic. 2024. The Claude 3 model family: Opus, sonnet, haiku.\\n\\nBaichuan. 2023. Baichuan-npc.\\n\\nFaeze Brahman, Meng Huang, Oyvind Tafjord, Chao Zhao, Mrinmaya Sachan, and Snigdha Chaturvedi. 2021. \u201cLet your characters tell their story\u201d: A dataset for character-centric narrative understanding. arXiv preprint arXiv:2109.05438.\\n\\nYapei Chang, Kyle Lo, Tanya Goyal, and Mohit Iyyer. 2023. Bookscore: A systematic exploration of book-length summarization in the era of llms. arXiv preprint arXiv:2310.00785.\\n\\nNuo Chen, Yan Wang, Haiyun Jiang, Deng Cai, Yuhan Li, Ziyang Chen, Longyue Wang, and Jia Li. 2023. Large language models meet Harry Potter: A dataset for aligning dialogue agents with characters. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8506\u20138520.\\n\\nYu-Hsin Chen and Jinho D Choi. 2016. Character identification on multiparty conversation: Identifying mentions of characters in tv shows. In Proceedings of the 17th annual meeting of the special interest group on discourse and dialogue, pages 90\u2013100.\\n\\nIsrael Cohen, Yiteng Huang, Jingdong Chen, Jacob Benesty, Jacob Benesty, Jingdong Chen, Yiteng Huang, and Israel Cohen. 2009. Pearson correlation coefficient. Noise reduction in speech processing, pages 1\u20134.\\n\\nJoseph L Fleiss, Bruce Levin, Myunghee Cho Paik, et al. 1981. The measurement of interrater agreement. Statistical methods for rates and proportions, 2(212\u2013236):22\u201323.\\n\\nJingsheng Gao, Yixin Lian, Ziyi Zhou, Yuzhuo Fu, and Baoyuan Wang. 2023a. LiveChat: A large-scale personalized dialogue dataset automatically constructed from live streaming. arXiv preprint arXiv:2306.08401.\\n\\nMingqi Gao, Jie Ruan, Renliang Sun, Xunjian Yin, Shiping Yang, and Xiaojun Wan. 2023b. Human-like summarization evaluation with ChatGpt. arXiv preprint arXiv:2304.02554.\\n\\nTanya Goyal, Junyi Jessy Li, and Greg Durrett. 2022. News summarization and evaluation in the era of GPT-3. arXiv preprint arXiv:2209.12356.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Giullaume Lample, Lucile Saulnier, et al. 2023. Mistral 7b. arXiv preprint arXiv:2310.06825.\\n\\nAlbert Q Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. 2024. Mixtral of Experts. arXiv preprint arXiv:2401.04088.\\n\\nPaul R Kleinginna Jr and Anne M Kleinginna. 1981. A categorized list of motivation definitions, with a suggestion for a consensual definition. Motivation and emotion, 5(3):263\u2013291.\\n\\nTom\u00e1\u0161 Ko\u010disk\u00fd, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, G\u00e1bor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317\u2013328.\\n\\nCheng Li, Ziang Leng, Chenxi Yan, Junyi Shen, Hao Wang, Weishi Mi, Yaying Fei, Xiaoyang Feng, Song Yan, HaoSheng Wang, et al. 2023a. Chatharuhi: Reviving anime character in reality via large language model. arXiv preprint arXiv:2308.09597.\\n\\nDawei Li, Hengyuan Zhang, Yanran Li, and Shiping Yang. 2023b. Multi-level contrastive learning for script-based character understanding. arXiv preprint arXiv:2310.13231.\\n\\nShuang Li, Jiangjie Chen, Siyu Yuan, Xinyi Wu, Hao Yang, Shimin Tao, and Yanghua Xiao. 2024. Translate meanings, not just words: Idiomkb's role in optimizing idiomatic translation with language models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pages 18554\u201318563.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. Gpteval: Nlg evaluation using GPT-4 with better human alignment. arXiv preprint arXiv:2303.16634.\\n\\nOpenAI. 2022. Chatgpt.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nYisi Sang, Xiangyang Mou, Mo Yu, Shunyu Yao, Jing Li, and Jeffrey Stanton. 2022. Tvshowguess: Character comprehension in stories as speaker guessing. arXiv preprint arXiv:2204.07721.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yunfan Shao, Linyang Li, Junqi Dai, and Xipeng Qiu. 2023. Character-llm: A trainable agent for role-playing. arXiv preprint arXiv:2310.10158.\\n\\nTianhao Shen, Sun Li, and Deyi Xiong. 2023. Roleeval: A bilingual role evaluation benchmark for large language models. arXiv preprint arXiv:2312.16132.\\n\\nCharles Spearman. 1961. The proof and measurement of association between two things.\\n\\nDominik Stammbach, Maria Antoniak, and Elliott Ash. 2022. Heroes, villains, and victims, and gpt-3: Automated extraction of character roles without training data. arXiv preprint arXiv:2205.07557.\\n\\nYihong Tang, Jiao Ou, Che Liu, Fuzheng Zhang, Di Zhang, and Kun Gai. 2024. Enhancing role-playing systems through aggressive queries: Evaluation and improvement. arXiv preprint arXiv:2402.10618.\\n\\nQuan Tu, Shilong Fan, Zihang Tian, and Rui Yan. 2024. Charactereval: A chinese benchmark for role-playing conversational agent evaluation. arXiv preprint arXiv:2401.01275.\\n\\nGuanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Man-dlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. 2023a. Voyager: An open-ended embodied agent with large language models. arXiv preprint arXiv:Arxiv-2305.16291.\\n\\nXintao Wang, Yunze Xiao, Jentse Huang, Siyu Yuan, Rui Xu, Haoran Guo, Quan Tu, Yaying Fei, Ziang Leng, Wei Wang, Jiangjie Chen, Cheng Li, and Yanghua Xiao. 2023b. Incharacter: Evaluating personality fidelity in role-playing agents through psychological interviews. arXiv preprint arXiv:2310.17976.\\n\\nZekun Moore Wang, Zhongyuan Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Man Zhang, et al. 2023c. Rolellm: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. arXiv preprint arXiv:2310.00746.\\n\\nJeff Wu, Long Ouyang, Daniel M Ziegler, Nisan Stiennon, Ryan Lowe, Jan Leike, and Paul Christiano. 2021. Recursively summarizing books with human feedback. arXiv preprint arXiv:2109.10862.\\n\\nRui Xu, Xintao Wang, Jiangjie Chen, Siyu Yuan, Xinfeng Yuan, Jiaqing Liang, Zulong Chen, Xiaoqing Dong, and Yanghua Xiao. 2024. Character is des-tiny: Can large language models simulate persona-driven decisions in role-playing. arXiv preprint arXiv:2404.12138.\\n\\nPaul Thomas Young. 1961. Motivation and emotion: A survey of the determinants of human and animal activity.\\n\\nMo Yu, Jiangnan Li, Shunyu Yao, Wenjie Pang, Xi-aochen Zhou, Zhou Xiao, Fandong Meng, and Jie Zhou. 2023. Personality understanding of fictional characters during book reading. arXiv preprint arXiv:2305.10156.\\n\\nMo Yu, Yisi Sang, Kangsheng Pu, Zekai Wei, Han Wang, Jing Li, Yue Yu, and Jie Zhou. 2022. Few-shot character understanding in movies as an assessment to meta-learning of theory-of-mind. arXiv preprint arXiv:2211.04684.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nRuncong Zhao, Wenjia Zhang, Jiazheng Li, Lixing Zhu, Yanran Li, Yulan He, and Lin Gui. 2023. Narratplay: Interactive narrative understanding. arXiv preprint arXiv:2310.01459.\\n\\nRuncong Zhao, Qinglin Zhu, Hainiu Xu, Jiazheng Li, Yuxiang Zhou, Yulan He, and Lin Gui. 2024. Large language models fall short: Understanding complex relationships in detective narratives. arXiv preprint arXiv:2402.11051.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.\\n\\nJinfeng Zhou, Zhuang Chen, Dazhen Wan, Bosi Wen, Yi Song, Jifan Yu, Yongkang Huang, Libiao Peng, Jiaming Yang, Xiyao Xiao, et al. 2023. Characterglm: Customizing chinese conversational ai characters with large language models. arXiv preprint arXiv:2311.16832.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Profiling v.s. Other Character Centric\\nSummaries\\n\\nA.1 Dimension\\n\\nSome previous work also summarizes certain information about characters from books or scripts. However, these studies on character understanding tend to focus on one specific aspect of the character, such as role (Stammbach et al., 2022), relationship (Zhao et al., 2024), personality (Yu et al., 2023), mental states (Yu et al., 2022). Furthermore, although these studies offer valuable insights into character understanding, their specific focus may not capture the multifaceted nature of character. Although recent RPA works have managed to summarize character information in a multi-dimensional approach, such as attributes, appearance, relationship, storyline (Zhao et al., 2023; Li et al., 2023a; Wang et al., 2023c), there is a lack of systematic assessment of the quality of these summaries.\\n\\nA.2 Evaluation\\n\\nOur evaluation framework of character profiling covers a wide range of information related to characters, facilitating a multidimensional understanding of characters. Although we explicitly request models to consider four dimensions, many dimensions of information are included in our framework or can be easily derived from the summarized profile. Specifically, in contrast to key mental states (Yu et al., 2022), our framework inherently encompasses critical mental information. For example, a character's objectives, part of their attributes profile defined in Section 3.2, reflect their desires and intentions. Key emotions and beliefs are often revealed through their reactions and behaviors in the profile of the events. Our work also includes factual details, like relationships with other characters and interactions with the external world.\\n\\nA.3 Application\\n\\nWe believe that the extensive character summary can provide necessary and valuable information for various downstream applications, e.g. chatbots of fictional characters (Chen et al., 2023), interactive narratives (Zhao et al., 2023), and study guides for human readers. However, since we prompt the model to limit the total word count of the entire profile, some dimensions may be more concise and not as detailed as summaries that focus solely on that dimension.\\n\\nB CROSS Dataset\\n\\nB.1 Dataset Construction\\n\\nWe select 126 books to construct our dataset. For each book, we collect the book's epub format and transform it into TXT format, and then process the texts into chunks of content with the required chunk size. All 126 books are fictional novels with an average token count of 134412. Among these books, 47 books are less than 120k tokens in length, and the average token count of these books is 101885. To minimize the potential for data leakage, we exclusively restrict our book selection to those published within the years 2022 and 2023. Additionally, we ensure that the selected books are either not sequels or, if they are sequels, can be regarded as independent works. Our selection focuses on works of fiction, specifically excluding biographical novels and other works based on real historical figures.\\n\\nFor the evaluation of our work, we obtain permission from the developer of the SuperSummary website to use the summaries and character analyses of these books written by experts. All book summaries and character analyses are used for academic research purposes. We release only the reorganized profiles, not the original summaries to protect the developers' copyrights.\\n\\nB.2 Integrity Verification of CROSS dataset\\n\\nTo confirm that our datasets and task genuinely evaluate understanding capability rather than simply testing the recovery of LLM training data, we design experiments to demonstrate that data leakage is not a significant concern with our dataset. In the following four datasets, all profiles are generated by GPT-4 through the incremental updating method.\\n\\nFor Publication Years\\n\\nWe count the number of books in CROSS dataset published in the years 2022 and 2023, and their average consistency scores are shown in Table 5. The average consistency scores of the books from 2022 and 2023 are very close (3.60 vs 3.62).\\n\\nFor Different Sales Volumes\\n\\nWe collect the collections tag in SuperSummary for books in CROSS dataset. The results of books in the \\\"New York Times Best Sellers\\\" collection and those that are not are shown in Table 6. These two consistency scores are also very close (3.58 v.s. 3.62). The two experiments above demonstrate that within CROSS dataset...\"}"}
{"id": "emnlp-2024-main-456", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dataset, publication year and sales volume do not significantly affect task performance. Furthermore, we collect a small set of highly canonized texts for comparison with our dataset.\\n\\nFor Classic Works in the 20th Century\\n\\nWe gather the top 10 books (excluding series) from the \\\"Best Books of the 20th Century\\\" list on the goodreads website. This collection includes well-known classics like \\\"The Little Prince\\\", \\\"1984\\\", and \\\"One Hundred Years of Solitude\\\". The results of this set are shown in Table 7, where the consistency score is much higher than that of CROSS dataset. This finding suggests that our selection of data effectively reduces the impact of data leakage compared with choosing classic works. We believe that high performance is due to the accumulation of time. The training set contains a large number of related corpora, such as Wikipedia entries, literary analyses, fan creations, and other related content, which deepen the model's understanding of these books and characters.\\n\\nFor Books Over Last Ten Years\\n\\nIn order to test the impact of publication year on task performance in more recent books, we collect books from the \\\"Best Books in {year}\\\" list on the goodreads website. We gather five books published in each of the last ten years, making a total of fifty books. The results of the average consistency score over different years are shown in Figure 3, and the detailed scores on different dimensions are shown in Table 8. We conduct a Spearman Rank Correlation Coefficient Test (Spearman, 1961) on this set. The coefficient of $-0.037$ with $p$-value of $0.799 (> 0.05)$ indicates no significant correlation between the year of publication and the average consistency score over these 50 samples. This result suggests that even though the texts of books from a few years ago may well have been trained by the model, there are not enough related corpora to allow the model to perform well on this task solely based on memory.\\n\\nBased on the above analysis, we reasonably speculate that, at least for the next few years, our dataset will remain effective for updated LLMs. Moreover, this work does not only focus on the dataset itself but, importantly, on a feasible framework designed to continually update and expand this dataset. Furthermore, we will keep updating the dataset and evaluating the performance of new LLMs in our future works.\\n\\nC Detailed Information of Summarization Method\\n\\nGiven a book $B$ with length $L$, for chunk-based method, we split $B$ into independent chunk $c_1, c_2, \\\\ldots, c_{\\\\lceil L/C \\\\rceil}$ with chunk size $C = 3000$. We fix the context window $W = 8096$ and the maximum summary length $M = 1200$.\\n\\nC.1 Incremental Updating\\n\\nThe progress of incremental updating is listed as follows:\\n\\n\u2022 Step 1: Given the first chunk $c_1$, the model outputs the initial summary $s_1$.\\n\\n\u2022 Step 2: Given the chunk content $c_2$, and the summary $s_1$, the model outputs summary $s_2$ which contains content of the first two chunk.\\n\\n\u2022 The summary is iteratively updated within the next chunk through step 2 until the final summary $s_{\\\\lceil L/C \\\\rceil}$ is obtained.\\n\\n\u2022 If the summary exceeds $M$ in these steps, the model is required to compress the summary into the required length.\\n\\nC.2 Hierarchical Merging\\n\\nThe progress of hierarchical merging is listed as follows:\\n\\n\u2022 Step 1: Given the chunks $c_1, c_2, \\\\ldots, c_{\\\\lceil L/C \\\\rceil}$, the model outputs the level 1 summaries for each chunk.\\n\\n\u2022 Step 2: Merge as many consecutive level 1 summaries as possible with the limit that the total length of the summaries and the prompt is less than 8027.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Results of character profiling on 10 books in \u201cBest Books of the 20th Century\u201d list in goodreads.\\n\\nTable 8: Results of character profiling on books in \u201cBest Books in {#the year}\u201d list in goodreads.\\n\\nthan $W$. Given these summaries, the model outputs the first level 2 summary, which serves as the context for the next merging.\\n\\n\u2022 Step 3: Merge as many remaining level 1 summaries as possible with the limit that the total length of these summaries and the prompt and the context is less than $W$. Given this input, the model outputs the next level 2 summary, which also serves as the context for the next merging. This process is iteratively conducted within the remaining summaries.\\n\\n\u2022 Merge the level 2 summaries by repeating steps 2 and 3 until a final summary is obtained.\\n\\nC.3 Summarizing in One Go\\nWe first ensure the total length of the selected book and the summarizing prompt is less than the context window limit of GPT-4 and Claude-3-Sonnet. Given the whole content of the book, the model outputs the final summary at once.\\n\\nD Manual Annotation\\nWe invite four native English-speaking college students as human annotators for manual evaluation in our work. These annotators receive compensation exceeding the local minimum wage. Two annotators also have consented to the use of motivation recognition data filtered by them for research purposes.\\n\\nD.1 Reference Profile Examination\\nTo examine the correctness of the character profile reorganized by GPT-4 from the original book summary and character analysis, we employ two annotators to check the consistency between the reorganized profile and the original content. The annotators are given the origin plot summary, character analysis, and reorganized character profile. Then they are required to determine whether the reorganized profile is consistent with the original information. The two annotators\u2019 result shows that the profiles of all samples in CORSS dataset do not contain plot inconsistencies and misjudgments of the character\u2019s traits. This result indicates that the profile\u2019s quality meets the standard of a golden profile.\\n\\nD.2 Manual Evaluation of FCE\\nIn order to examine the quality of Llama-3-70B evaluator result, we sample 50 pieces in our dataset and invite two annotators to evaluate the generated profile in consistency score. We provide the annotators and Llama-3-70B with the same scoring prompt. For the metric consistency score, the Pearson Correlation Coefficient between the average human result and Llama-3-70B scoring is 0.752 with $p$-value $= 4.3e^{-12}$. The $p$-value < 0.05 demonstrates that these two sets of results have a significant correlation. The coefficient result indicates that the Llama-3-70B evaluation ability is comparable with human annotators on the assessing character profile.\\n\\nD.3 Motivation Recognition MCQs filtering\\nTo ensure the quality of the MR question dataset, we employ two annotators for conducting manual\"}"}
{"id": "emnlp-2024-main-456", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"filtering. The annotators are provided with reference character profiles, generated questions, and the following criteria:\\n\\n- The decision must be made by the selected character. Each question must feature a decision and the scenario, with the focus character as the decision-maker.\\n- Questions should ask directly or indirectly about the character's motivation for making the decision. Each question must directly or indirectly inquire about the character's motivation for making their decision, avoiding irrelevant information.\\n- The decision must be meaningful within the story context. The decision in the question must contribute meaningfully to the storyline. It should reflect a conscious choice by the character that holds importance in the narrative, rather than representing a mundane or routine decision.\\n- Leaking questions is prohibited. Scenarios and questions must not include the motivation behind the characters' decisions.\\n\\nWe require the annotators to determine if the question meets the criteria. By filtering the dataset, we finally get 445 high-quality motivation recognition multiple-choice questions with Fleiss's $\\\\kappa = 0.91$. We also adjust the arrangement of the options to ensure a fair distribution of correct answers.\\n\\nD.4 Human Performance in Motivation Recognition\\n\\nWe employ two annotators to study the human performance in the MR task. The annotators are provided with the complete information in the MR dataset in CROSS except for the correct answer and the reason. They are instructed to choose an option for each question.\\n\\nE Traditional Metrics on Generated Profiles\\n\\nIn our evaluation protocol, traditional metrics for text summarization like ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), and BERTScore (Zhang et al., 2019) are not used because they have been shown to be unreliable for measuring summary quality of GPT-3 generated summaries compared to human evaluations (Goyal et al., 2022). However, to provide a comprehensive perspective, we present the results of these three traditional metrics in Table 10 and Table 11 for reference. To ensure the fairness and accuracy of evaluations using lexical metrics, preventing the language styles of different models from influencing the results, we use the profile extracted from original SuperSummary data by Llama-3-70B rather than GPT-4 as reference profile in this section. The results of traditional metrics are generally consistent with the conclusions of the consistency score, showing similar patterns:\\n\\n1) Larger and more complex LLMs perform better;\\n2) The performance on the event dimension is often the lowest;\\n3) For different summarization methods, the average results show that summarizing in one go performs better than hierarchical summarizing, which in turn performs better than incremental updating. It is also shown that the relative performance of models varies widely across different metrics. We believe this happens because each metric focuses on different aspects. ROUGE-L and BLEU mostly measure lexical matching, while BERTScore emphasizes semantic similarity, and the consistency score checks factual accuracy. Since profiles generated by different models can be semantically or factually consistent but differ in wording and syntax, their performance may vary depending on the metric used.\\n\\nF Prompts\\n\\nFor summarization, we mainly adopt the prompt structure from Chang et al. (2023).\\n\\nF.1 Summarizing in One Go\\n\\nIn our experiment, we have found that the long-context capabilities of Claude-3-Sonnet are limited. Consequently, the model occasionally forgets the instructions and generates a simplistic summary.\"}"}
{"id": "emnlp-2024-main-456", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Incremental Updating\\n\\n| Model                                | ROUGE-L % | BLEU % | Attr | Rela | Even | Pers | Avg. | Attr | Rela | Even | Pers | Avg. |\\n|--------------------------------------|-----------|--------|------|------|------|------|------|------|------|------|------|------|\\n| Mistral-7B-Instruct-v0.2             | 23.89     | 27.04  | 22.16| 18.70| 22.94| 5.41 | 5.88 | 3.32 | 2.76 | 4.34 |\\n| Mixtral-8x7B-MoE                     | 25.01     | 27.94  | 20.55| 19.26| 23.19| 5.57 | 6.02 | 3.10 | 3.28 | 4.49 |\\n| Vicuna-7b-v1.5-16k                   | 30.53     | 30.46  | 21.69| 24.04| 26.68| 8.48 | 7.24 | 2.65 | 4.32 | 5.67 |\\n| Vicuna-13b-v1.5-16k                  | 30.60     | 29.58  | 22.94| 21.75| 26.22| 8.56 | 6.41 | 3.60 | 3.28 | 5.46 |\\n| Qwen1.5-7B-Chat                      | 22.04     | 23.04  | 19.10| 20.83| 21.25| 3.66 | 3.79 | 2.11 | 3.58 | 4.32 |\\n| Qwen1.5-14B-Chat                     | 23.05     | 22.03  | 18.13| 21.01| 21.05| 4.53 | 4.12 | 1.48 | 3.18 | 3.33 |\\n| Qwen1.5-72B-Chat                     | 26.94     | 29.08  | 21.68| 23.32| 25.26| 6.29 | 7.50 | 2.76 | 3.90 | 5.11 |\\n| GPT-3.5-Turbo                        | 29.35     | 27.71  | 20.44| 21.09| 24.65| 7.46 | 6.55 | 2.97 | 3.29 | 5.07 |\\n| GPT-4-Turbo                          | 28.61     | 27.56  | 20.62| 21.98| 24.69| 8.00 | 6.67 | 3.10 | 3.17 | 5.23 |\\n\\n## Hierarchical Summarizing\\n\\n| Model                                | ROUGE-L % | BLEU % | Attr | Rela | Even | Pers | Avg. | Attr | Rela | Even | Pers | Avg. |\\n|--------------------------------------|-----------|--------|------|------|------|------|------|------|------|------|------|------|\\n| Mistral-7B-Instruct-v0.2             | 27.94     | 29.23  | 22.64| 23.12| 25.73| 9.07 | 8.57 | 3.81 | 5.51 | 6.74 |\\n| Mixtral-8x7B-MoE                     | 26.91     | 30.79  | 24.41| 21.79| 25.97| 6.55 | 8.14 | 4.28 | 3.74 | 5.68 |\\n| Vicuna-7b-v1.5-16k                   | 27.87     | 29.31  | 21.47| 23.47| 25.53| 6.20 | 5.85 | 2.10 | 3.98 | 4.53 |\\n| Vicuna-13b-v1.5-16k                  | 28.56     | 29.34  | 22.12| 22.08| 25.53| 7.38 | 6.02 | 3.66 | 3.23 | 4.82 |\\n| Qwen1.5-7B-Chat                      | 25.19     | 26.44  | 19.29| 22.52| 23.36| 5.00 | 5.41 | 2.15 | 4.05 | 4.15 |\\n| Qwen1.5-14B-Chat                     | 24.15     | 24.12  | 19.07| 21.08| 22.11| 4.72 | 5.34 | 1.54 | 2.89 | 3.62 |\\n| Qwen1.5-72B-Chat                     | 29.46     | 31.26  | 25.17| 24.60| 27.62| 8.61 | 8.84 | 4.66 | 5.14 | 6.81 |\\n| GPT-3.5-Turbo                        | 32.05     | 29.98  | 21.58| 24.46| 27.02| 9.33 | 7.56 | 3.06 | 4.17 | 6.03 |\\n| GPT-4-Turbo                          | 28.88     | 26.01  | 21.85| 23.03| 24.94| 8.42 | 7.06 | 3.59 | 3.45 | 5.63 |\\n\\n## Factual Consistency Examination\\n\\nFor evaluation, we mainly adopt the prompt structure from Liu et al. (2023). The prompt template is shown in Table 15.\\n\\n## Motivation Recognition\\n\\nThe prompt template of MR task is shown in Table 16.\"}"}
