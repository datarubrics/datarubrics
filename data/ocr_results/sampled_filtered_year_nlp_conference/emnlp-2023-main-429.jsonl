{"id": "emnlp-2023-main-429", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the instruction and then answer the question using A or B.\\n\\n[Example1]\\nInstruction: (x, works for, y) indicates that y works for x.\\nQuestion: (?, works for, anthony fauci)\\nA: Find an entity that works for anthony fauci.\\nB: Find an entity that anthony fauci is employed by.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\\n\\n[Example2]\\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\\nQuestion: (?, bigger than, elephant)\\nA: Find an entity that is smaller than elephant.\\nB: Find an entity that is bigger than elephant.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: A\\n\\n[Example3]\\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\\nQuestion: (?, in the south of, china)\\nA: Find an entity that is in the north of china.\\nB: Find an entity that is in the south of china.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: A\\n\\n[Example4]\\nInstruction: (x, has part, y) indicates that y has a part called x.\\nQuestion: (?, has part, solingen)\\nA: Find an entity that has a part called solingen.\\nB: Find an entity that solingen contains.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-429", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the instruction and then answer the question using A or B.\\n\\n[Example1]\\nInstruction: (x, works for, y) indicates that y works for x.\\nQuestion: (?, works for, anthony fauci)\\nA: Find an entity that works for anthony fauci.\\nB: Find an entity that anthony fauci is employed by.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\\n\\n[Example2]\\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\\nQuestion: (?, bigger than, elephant)\\nA: Find an entity that is smaller than elephant.\\nB: Find an entity that is bigger than elephant.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: A\\n\\n[Example3]\\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\\nQuestion: (?, in the south of, china)\\nA: Find an entity that is in the north of china.\\nB: Find an entity that is in the south of china.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: A\\n\\n[Example4]\\nInstruction: (x, teach, y) indicates that y teaches x.\\nQuestion: (?, teach, andy bramante)\\nA: Find a person that teaches andy bramante.\\nB: Find a person that is the student of andy bramante.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\\n\\n[Example5]\\nInstruction: (x, interviewed, y) indicates that y interviewed x.\\nQuestion: (?, interviewed, biden)\\nA: Find a person that biden conducted an interviewed with.\\nB: Find a person that interviewed biden.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: A\\n\\n[Example6]\\nInstruction: (x, successor, y) indicates that y is the successor of x.\\nQuestion: (?, successor, barack obama)\\nA: Find a person that is the successor of barack obama.\\nB: Find a person that is the predecessor of barack obama.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\nQuestion: (?, has part, solingen)\\nA: Find an entity that has a part called solingen.\\nB: Find an entity that solingen contains.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the instruction and then answer the question using A or B.\\n\\n[Example 1]\\n\\nInstruction: (x, works for, y) indicates that y works for x.\\n\\nQuestion: (?, works for, anthony fauci)\\n\\nA: Find an entity that is employed by anthony fauci.\\nB: Find an entity that anthony fauci works for.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\\n\\n[Example 2]\\n\\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\\n\\nQuestion: (?, bigger than, elephant)\\n\\nA: Find an entity so that elephant is bigger than it.\\nB: Find an entity so that elephant is smaller than it.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: A\\n\\n[Example 3]\\n\\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\\n\\nQuestion: (?, in the south of, china)\\n\\nA: Find an entity so that china is in the south of it.\\nB: Find an entity so that china is in the north of it.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: A\\n\\n[Example 4]\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\n\\nQuestion: (?, has part, solingen)\\n\\nA: Find an entity that has a part called solingen.\\nB: Find an entity that solingen contains.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the instruction and then answer the question using A or B. Note that in this task, if the relation is defined in a converse manner, unlike the conventional definition, you should carefully choose the answer. Your answer should be in JSON format with the following keys: thought, answer.\\n\\n[Example 1]\\nInstruction: (x, works for, y) indicates that y works for x.\\nQuestion: (?, works for, anthony fauci)\\nA: Find an entity that is employed by anthony fauci.\\nB: Find an entity that anthony fauci works for.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: {'thought': \"Let's think step by step. Firstly, the question is asking for x. Then, the instruction indicates y works for x. According to the question, y is anthony fauci, and therefore anthony fauci works for x, the answer is B.\", 'answer': 'B'}\\n\\n[Example 2]\\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\\nQuestion: (?, bigger than, elephant)\\nA: Find an entity so that elephant is bigger than it.\\nB: Find an entity so that elephant is smaller than it.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: {'thought': \"Let's think step by step. Firstly, the question is asking for x. Then, the instruction indicates y is bigger than x. According to the question, y is elephant, and therefore elephant is bigger than x, the answer is A.\", 'answer': 'A'}\\n\\n[Example 3]\\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\\nQuestion: (?, in the south of, china)\\nA: Find an entity so that china is in the south of it.\\nB: Find an entity so that china is in the north of it.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\nAnswer: {'thought': \"Let's think step by step. Firstly, the question is asking for x. Then, the instruction indicates y is in the south of x. According to the question, y is china, and therefore china is in the south of x, the answer is A.\", 'answer': 'A'}\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\nQuestion: (?, has part, solingen)\\nA: Find an entity that has a part called solingen.\\nB: Find an entity that solingen contains.\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct? Look out for the ORDER of the entities in the instruction!\\nAnswer: Expected Answer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Instruction: (x, works for, y) indicates that y works for x.\\n\\nQuestion: (?, works for, anthony fauci)\\n\\nA: Find an entity that is employed by anthony fauci.\\nB: Find an entity that anthony fauci works for.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\\n\\nInstruction: (x, bigger than, y) indicates that y is bigger than x.\\n\\nQuestion: (?, bigger than, elephant)\\n\\nA: Find an entity so that elephant is bigger than it.\\nB: Find an entity so that elephant is smaller than it.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: A\\n\\nInstruction: (x, in the south of, y) indicates that y is in the south of x.\\n\\nQuestion: (?, in the south of, china)\\n\\nA: Find an entity so that china is in the south of it.\\nB: Find an entity so that china is in the north of it.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: A\\n\\nInstruction: (x, teach, y) indicates that y teaches x.\\n\\nQuestion: (?, teach, andy bramante)\\n\\nA: Find a person that andy bramante is the student of.\\nB: Find a person that andy bramante teaches.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\\n\\nInstruction: (x, interviewed, y) indicates that y interviewed x.\\n\\nQuestion: (?, interviewed, biden)\\n\\nA: Find a person that biden interviewed.\\nB: Find a person that conducted an interview with biden.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: A\\n\\nInstruction: (x, successor, y) indicates that y is the successor of x.\\n\\nQuestion: (?, successor, barack obama)\\n\\nA: Find a person that barack obama is the predecessor of.\\nB: Find a person that barack obama is the successor of.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\n\\nQuestion: (?, has part, solingen)\\n\\nA: Find an entity that has a part called solingen.\\nB: Find an entity that solingen contains.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Investigation of LLMs\u2019 Inefficacy in Understanding Converse Relations\\n\\nChengwen Qi, Bowen Li, Binyuan Hui, Bailin Wang, Jinyang Li, Jinwang Wu, Yuanjun Laili\\n\\n1 Beihang University\\n2 Shanghai AI Laboratory\\n3 3B Group\\n4 The University of Hong Kong\\n5 MIT\\n\\nchengwen_qi@buaa.edu.cn, libowen.ne@gmail.com\\nhuybery@gmail.com, lailiyuanjun@buaa.edu.cn\\n\\nhttps://github.com/3B-Group/ConvRe\\n\\nAbstract\\n\\nLarge Language Models (LLMs) have achieved remarkable success in many formal language oriented tasks, such as structural data-to-text and semantic parsing. However, current benchmarks mostly follow the data distribution of the pre-training data of LLMs. Therefore, a natural question arises: do LLMs really understand the structured semantics of formal languages? In this paper, we investigate this problem on a special case, converse binary relations. We introduce a new benchmark ConvRe focusing on converse relations, which contains 17 relations and 1240 triples extracted from popular knowledge graph completion datasets. Our ConvRe features two tasks, Re2Text and Text2Re, which are formulated as multiple-choice question answering to evaluate LLMs\u2019 ability to determine the matching between relations and associated text. For the evaluation protocol, apart from different prompting methods, we further introduce variants to the test text and few-shot example text. We conduct experiments on three popular LLM families and have observed various scaling trends. The results suggest that LLMs often resort to shortcut learning and still face challenges on our proposed benchmark.\\n\\n1 Introduction\\n\\nLarge Language Models (LLMs) have demonstrated impressive empirical results on various NLP tasks (Bubeck et al., 2023; OpenAI, 2023; Anthropic, 2023), including formal language-oriented tasks such as structural data-to-text (Xiang et al., 2022) and semantic parsing (Chen et al., 2021; Li et al., 2023a), which require sophisticated comprehension and production of structured language content. Despite these promising advances, a critical concern remains largely unexplored: do these LLMs genuinely understand the nuanced semantics of formal languages, or are they merely exploiting statistical patterns inherent in their pre-training data? If such shortcuts exist, it implies that LLMs may struggle to generalize to novel and unique formal language definitions, potentially hindering the robustness and scalability of practical applications.\\n\\nIn this work, we delve into this question by focusing on a specific aspect of formal language understanding: the comprehension of converse relations. As shown in Figure 1, the converse relation redefines the semantic relation between entities while keeping the surface form of the triple unchanged. For instance, the triple (x, has part, y) should be interpreted as \u201cx has a part called y\u201d in the normal relation (Codd, 1983), while \u201cy has a part called x\u201d in converse form. Notably, LLMs are largely unfamiliar with converse relations, as the data they learn in pre-training mostly comprises normal relations. It\u2019s imperative for LLMs to accurately understand and utilize these converse relations, i.e., truly following instructions rather than recalling memorized patterns (shortcuts) about normal relations, as it significantly impacts the semantic coherence of their output.\\n\\nTo systematically evaluate the competence of LLMs in recognizing and processing converse relations, we introduce a novel benchmark, ConvRe. This benchmark draws upon 17 diverse relations and 1240 triples derived from prominent knowledge graph completion datasets. ConvRe introduces two primary tasks, Re2Text and Text2Re, formatted as multiple-choice question answering tests. These tasks challenge LLMs to correctly match relations (Re) with their corresponding natural language text (Text).\\n\\nDuring empirical evaluation, we add various prompting methods and introduce variants to the text. More specifically, we manually craft examples of different types in the few-shot prompting, creating a more challenging testbed for these models. Our findings, based on thorough experiments,\"}"}
{"id": "emnlp-2023-main-429", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Could LLMs understand converse relations?\\n\\nFigure 1: Illustration of converse relation comprehension by LLMs. This diagram highlights the unique challenges converse relations present for LLMs, potentially leading to diverse scaling trends.\\n\\nTable 1: The definition of normal and converse relation. Examples are provided below the notations. A triple can be defined to represent the normal relation $R$ or the converse relation $R^\\\\top$. Each relation is associated with a pairing of natural language text, which can further be paraphrased.\\n\\n\\\\begin{tabular}{|c|c|}\\n\\\\hline\\nNormal Relation & Converse Relation \\\\\\\\\\n\\\\hline\\n$x$ has a part called $y$. & $y$ has a part called $x$. \\\\\\\\\\n\\\\hline\\n\\\\end{tabular}\\n\\nIn this section, we will introduce the motivation, task formulation and design choice of our ConvRe benchmark as well as the details surrounding data collection.\\n\\n2.1 Motivation\\n\\nThe recent surge in the performance of LLMs in understanding formal language, including tasks such as semantic parsing or data2text, can potentially be misleading. Traditional evaluation benchmarks used in such tasks often reflect statistical patterns similar to those found in the pre-training data of LLMs. We posit that this could lead LLMs to take a shortcut as described in Geirhos et al. (2020), thereby inflating the understanding of formal language semantics. Instead of comprehensively grasping the semantics, the LLMs might simply be learning the statistical tendencies present in their training data. To this end, we propose a new benchmark that uses normal and converse relations to examine the true semantic comprehension capabilities of LLMs.\\n\\n2.2 Normal and Converse Relation\\n\\nFormally, a binary relation $R$ over sets $X$ and $Y$ is a set of ordered pairs $(x, y)$ consisting of elements $x \\\\in X$ and $y \\\\in Y$ (Codd, 1983). Usually, a normal relation $R$ is represented as $R = \\\\{ (x, R, y) \\\\iff xRy \\\\}$, where $R$ is the specific relation phrase. Normal relations usually appear in the knowledge graph, along with a pair of\\n\\nThere are some terminologies, such as spurious correlation and superficial cues/bias/artifacts, that are similar to the term shortcut used in this paper. We provide supplementary explanations of these terms in Appendix A for better clarity.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The definition of normal and converse relation. Examples are provided below the notations. A triple can be defined to represent the normal relation $R$ or the converse relation $R^\\\\top$. Each relation is associated a pairing natural language representation, which can further be paraphrased.\\n\\n### Re2Text Task\\n\\nRead the instruction and then answer the question using A or B.\\n\\n**Instruction:** $(x, \\\\text{has part}, y)$ indicates that $y$ has a part called $x$.\\n\\n**Question:** (?, has part, hilt)\\n\\n**A:** Find an entity that has a part called hilt.\\n\\n**B:** Find an entity that hilt contains.\\n\\n**Answer:** A\\n\\n### Figure 2\\n\\nExamples of Re2Text and Text2Re tasks on converse relation. We additionally paraphrase the natural language representations (answer candidates for Re2Text, question for Text2Re) to make them differ from the sentences in the Instruction.\\n\\nFigure 2: The Re2Text task converts relation into semantically equivalent natural language text. Given that LLMs mostly encounter normal relations during pre-training, deciphering converse relations poses a significant challenge. LLMs tend to exploit textual similarity shortcuts for prediction, which can mislead the model's performance as it bypasses genuine comprehension. In the regular scenario (top), two shortcuts lead the model towards divergent answers, where the incorrect answer (A) will not be overly preferred. In the hard scenario (bottom), the text for the correct response (B) is modified, transforming two shortcuts into a single one. This solitary shortcut is more likely to misdirect the model towards the incorrect answer (A), highlighting the pitfalls of shortcuts learning.\\n\\n---\\n\\n**2.4 Text Variants**\\n\\nTest Variants in Zero-shot Prompting\\n\\nGeirhos et al. (2020) highlighted a phenomenon in deep learning known as **shortcut learning**. These are decision rules that achieve high performance on standard benchmarks but fail to generalize under more challenging testing conditions such as real-world scenarios. This issue is particularly significant in language processing tasks, where a language model may show an ability to reason that is learned from the training data, but its performance can plummet drastically\u2014sometimes to levels equivalent to random guessing\u2014when superficial correlations are removed from the dataset (Niven and Kao, 2019).\\n\\nTo assess how extensively current LLMs leverage shortcut learning for the evaluation tasks we have designed, we introduce variants to the text in both our tasks. In the Re2Text task, we paraphrase one answer candidate, while in the Text2Re task, we paraphrase the question. Specifically, we modify the key predicate and restructure the sentence (as illustrated in figure 3). We note that the subtle variations on the test text bring different effects.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We note that the subtle variations on the test text will be evidenced by the empirical results in our experiments (see Section 4.2). Examples on the test variants as well as intuitive explanations on the similar spirit as the complexity based prompting (Fu et al., 2022), where correlations are removed from the dataset (Niven et al., 2019). To assess how extensively current LLMs leverage plausible relations, we will employ the same configurations for the test text in the hard test setting corresponds to the unaltered test text (for the Text2Re task), while the altered examples are labeled as hard test setting aligns with the unaltered test text, then the unaltered examples are labeled as easy tests.\\n\\nExample Variants in Few-shot Prompting\\n\\nTo convert the question into a semantically equivalent triple query, which choice is correct?\\n\\nA: (?, has part, hilt)\\n\\nB: (hilt, has part, ?)\\n\\nQuestion: Find an entity that possesses a specific component named hilt.\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\n\\nTo make our tasks more comprehensive, and thus cover the entire spectrum of directness, plausible relations must satisfy two requirements:\\n\\n1. The involved subject and object are changeable, implying that x and y can be swapped.\\n\\n2. The relation is asymmetric, meaning that the order of x and y affects the meaning of the relation.\\n\\nAn example of such a relation is\\n\\n\\\\[ R = \\\\{ (\\\\text{parent}, \\\\text{child}), (\\\\text{child}, \\\\text{parent}) \\\\} \\\\]\\n\\nAn example of such a relation is\\n\\n\\\\[ R = \\\\{ (\\\\text{language}, \\\\text{native}), (\\\\text{native}, \\\\text{language}) \\\\} \\\\]\\n\\nA language associates a person with a language. A language cannot logically be the subject of a language; therefore, it is asymmetric.\\n\\nConversely, if the relation is symmetric, such an association sociates a person with a language. A language and its native language are not the same entity, and their relationship should be a head or a tail, as the both are semantically equivalent.\\n\\nIn terms of determining whether a given entity is a head or a tail, in the case of symmetric relations, it is not possible to determine whether a given entity is a head or a tail, as the both are semantically equivalent.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Zero-shot prompts.\\n\\n| ID | Prompting Method | Shot | Relation | Hint | Examples |\\n|----|------------------|------|----------|------|----------|\\n| 1  | normal-re, normal-text | 0 | N |  |         |\\n| 2  | normal-re, altered-text | 0 | N |  |         |\\n| 3  | converse-re, normal-text (Text2Re) | 0 | C |  |         |\\n| 4  | converse-re, altered-text (Re2Text) | 0 | C |  |         |\\n| 5  | converse-re, normal-text, hint | 0 | C | \u2713 |         |\\n| 6  | converse-re, altered-text, hint | 0 | C | \u2713 \u2713 |         |\\n\\nTable 3: Few-shot prompts.\\n\\n| ID | Prompting Method | Shot | Relation | Examples |\\n|----|------------------|------|----------|----------|\\n| 7  | 3-shot, hard-hard | 3 | C | hard hard |\\n| 8  | 3-shot, hard-hard, hint-cot (w/ CoT) | 3 | C | hard hard |\\n| 9  | 6-shot, hard-hard | 6 | C | hard hard |\\n| 10 | 3-shot, regular-hard | 3 | C | regular hard |\\n| 11 | 3-shot, regular-hard, hint-cot (w/ CoT) | 3 | C | regular hard |\\n| 12 | 6-shot, regular-hard | 6 | C | regular hard |\\n\\nTable 4: Text variants on test and example sides for few-shot prompting.\\n\\n| Text2Re | Re2Text |\\n|---------|---------|\\n| hard-hard | \u2713 \u2713 |\\n| regular-hard | \u2713 |\\n\\n Examples are provided in two options, regular and hard.\\n\\nversely, if the relation is symmetric, such as neighboring country, it would be meaningless to determine whether a given entity should be a head or a tail, as the both are semantically equivalent.\\n\\n\u2022 The involved subject and object are interchangeable. That is, the relation $R$ and its converse counterpart $R^\\\\top$ should be semantically plausible, though not equivalent. An example of a relation we would avoid under this criterion is native language, which associates a person with a language. A language cannot logically be the subject of native language, thereby disqualifying this relation. Relations of this sort could allow LLMs to rely on shortcut learning to solve tasks. For instance, in the case of native language, the entity's type inadvertently reveals the answer so that the LLMs may exploit this leaked information.\\n\\nWe manually select 17 relations from six widely used knowledge graph datasets: WN18RR (Dettmers et al., 2018), FB15K-237 (Toutanova and Chen, 2015), Wikidata5M (only transductive settings) (Wang et al., 2021), NELL-ONE (Xiong et al., 2018), ICEWS14 (Garc\u00eda-Dur\u00e1n et al., 2018), ConceptNet5 (Speer et al., 2017). For each relation, we randomly sample 80 triples from corresponding datasets and manually remove the triples that are not suitable for our task. Finally, we get 1240 triples in our benchmark, detailed breakdown of the number of triples for each relation can be found in Appendix B.\\n\\n3 Experiment Setup\\n\\n3.1 Model and Metric\\n\\nWe evaluated three LLM families on our ConvRe benchmark: OpenAI GPT-3 (Brown et al., 2020), Anthropic Claude (Anthropic, 2023), and Google Flan-T5 (Chung et al., 2022) (model details in Appendix C). Since we do not have enough credits for the OpenAI APIs, we evaluate OpenAI GPT-4 on a subset of our benchmark for few-shot experiments.\\n\\nWe use the classification accuracy as our metric. The subset is constructed by randomly sampling 20 triples for each relation from the full set. In the case where the subset is not sufficient, we increase the number of samples. The number of samples per relation in our study is given in Table 5.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Prompting Methods\\n\\nAs depicted in Zhang et al. (2023), different prompting methods can have a considerable impact on the scaling trends of language models. To account for this in our study, we utilize diverse prompting methods. Generally, we have zero-shot and few-shot prompting, each tailored with specific design elements. Detailed illustrations are provided in Table 2, 3 and 4. While we previously discussed these from a motivation point of view, this subsection offers a closer look at the implementation specifics.\\n\\nZero-shot\\n\\nWe assess both normal and converse relations mainly on the zero-shot setting, where each setting is coupled with regular and altered test text (refer to the text variations in Section 2.4). For the converse relation evaluation, we additionally equip the prompt with hint (Kojima et al., 2022). An illustration of the hint used in our experiment is shown in Figure 4.\\n\\nFew-shot\\n\\nIn this setting, we only apply the hard settings, as documented in Table 3. The corresponding zero-shot tests (ID 3# for Text2Re and ID 4# for Re2Text, detailed in Table 2) are employed as baselines. The arrangements for the example variants are thoroughly detailed in Table 4. Within each group, we have three distinct sub-settings: 3-shot, 3-shot with hint & Chain-of-Thought (CoT, Wei et al. 2022b), and 6-shot.\\n\\nnumber of triples for a particular relation is less than 20, we include all of them. Ultimately, the subset comprises a total of 328 triples. We run GPT-4 on both full set and subset in zero-shot settings. Results show that the subset can reflect the model's performance. Details can be found in Appendix D.\\n\\n4 Results\\n\\nIn this section, we demonstrate the results of different LLM families on ConvRe benchmark and provide an in-depth analysis. More results on chat models can be found in Appendix E.\\n\\n4.1 Converse Relation\\n\\nOur first experiment, conducted in the zero-shot setting, involves both normal and converse relations across all model families. As shown in Figure 5, the performance on converse relations, within the scope of unaltered test text, is consistently inferior to that on normal relations across all tested models and tasks. More specifically, we note a roughly positive scaling trend for normal relations and an inverse scaling trend for converse relations, despite some outliers. The state-of-the-art LLM, GPT-4, underperforms compared to smaller models, with its performance falling significantly below random-guess levels. We conjecture that larger models have stronger priors, causing them to rely more heavily on memorized patterns from training data, which can conflict with the given task.\\n\\n4.2 Text Variants\\n\\nAs introduced in Section 2.4, we are curious about LLMs' behaviours against text variants on the test and the few-shot examples. Our initial focus is the zero-shot setting (Figure 5). For normal relations, test variants cause a noticeable performance drop. It means that if a given answer candidate fits the superficial pattern stated in the instruction, models are more likely to select it although it could be incorrect. This suggests that LLMs tend to take shortcut learning even within conventional problem settings. For converse relations, variants on the test text harm the performance on Re2Text while enhance it on Text2Re.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Zero-shot results on ConvRe. Each experimental setting has been indexed with a unique ID that can be referred to in Table 2. Sub-figures in the same row share the same figure legend, so we only display it once in the leftmost sub-figure to save space. The table version of the results can be found in Appendix I.\\n\\nFigure 6: Few-shot results on ConvRe. Each experimental setting has been indexed with a unique ID that can be referred to in Table 3. Sub-figures in the same row share the same figure legend, so we only display it once in the leftmost sub-figure to save space. Detailed settings on the text variants can be found in Table 4. For GPT-4, we only test it on a subset of our benchmark. Due to Flan-T5's weak ability to follow CoT instructions, we do not report the results of Flan-T5 with hint and CoT prompting.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"These findings lend strong support to our previous hypothesis presented in Section 2.4. In the few-shot setting, the zero-shot baselines for both tasks are set to be hard (see Table 3 and 4). Generally, hard examples outperform standard examples (hard-hard vs. regular-hard) on average across different models on the two tasks. This can be attributed to the fact that hard examples align more consistently with the hard tests and effectively help models in avoiding bias and shortcut learning.\\n\\n4.3 Shot Number\\nExamples, particularly an increased number of examples, are expected to outperform zero-shot prompting. However, we do not consistently observe improvements across different models and tasks. Notably, GPT models demonstrate the most consistent improvements, indicating superior in-context learning abilities among these models. Interestingly, when using few-shot examples, the models mostly exhibit inverse scaling or inverted U-shaped scaling, which suggests that our benchmark presents a challenge for the current LLMs.\\n\\n4.4 Hint and CoT\\nThe zero-shot experiments in Figure 5 indicate that the use of hints in prompts typically yields improvements for GPT and Flan-T5 models. However, Claude-1 stands out as an exception, appearing to be negatively affected by the hint. In the few-shot experiments, employing hints and the Chain-of-Thought (CoT) approach substantially boosts performance, particularly for larger models. GPT models exhibit positive scaling and U-shaped scaling on the Re2Text task. However, for the Text2Re task, we still observe inverted U-shaped scaling for GPT models and inverse scaling for Claude models. This indicates that LLMs still struggle on this task even with strong prompting methods. We also find that Flan-T5 cannot properly follow CoT instructions, so we do not report the results of Flan-T5 with hint and CoT prompting.\\n\\n5 Related Work\\nStudies on LLMs have shown positive scaling trends, whereby larger models generally perform better on downstream tasks (Brown et al., 2020; Rae et al., 2021; Chowdhery et al., 2022; Srivastava et al., 2022; Liang et al., 2022). However, researchers showed that model performance scaling can deviate from naive expectations. Srivastava et al. (2022) showed slower and less smooth trends, and that social biases sometimes scale inversely with model size, a finding that is echoed in Parrish et al. (2022). TruthfulQA (Lin et al., 2022) demonstrated that while larger models can provide more informative answers, they tend to be less truthful. McKenzie et al. (2023) introduced the inverse scaling challenge and collected tasks that are highly atypical but still easily understandable by a human. Wei et al. (2022a) uncovered the U-shaped scaling trend by expanding the model scope for evaluation. Zhang et al. (2023) proposed NeQA and showed that this task exhibit inverse, U-shaped, or positive scaling with different prompt methods or model families. Miceli-Barone et al. (2023) showed that LLMs fail to correctly generate Python code when default identifiers are swapped.\\n\\nRecent research has highlighted the issue of inflated performance in LLMs. Geirhos et al. (2020) coined the term shortcut learning, revealing models' reliance on superficial cues. Tu et al. (2020) studied the model's robustness to spurious correlations, which refers to the prediction rules that work for the majority examples but do not hold in general. Li et al. (2023b) found that LLMs tend to rely on shallow matching rather than understanding mathematical concepts. Bender et al. (2021) highlighted the importance of understanding the mechanism by which LLMs achieved state-of-the-art performance. Perez et al. (2021) showed that LLMs' few-shot ability is often overestimated due to the use of large held-out sets. Ji et al. (2023) surveyed the hallucination problem in language generation, highlighting the issue of factually incorrect output. Liu et al. (2023) identified attention glitches in Transformers, indicating a failure in capturing robust reasoning. Berglund et al. (2023) introduced the term reverse curse, showing that LLMs trained on 'A is B' fails to learn the reverse relationship 'B is A'.\\n\\n6 Concolusion\\nIn this paper, we present an investigation into LLMs' understanding of structured semantics, specifically focusing on converse binary relations. By introducing a novel benchmark, ConvRe, we offer a systematic and comprehensive evaluation suite to observe the performance of LLMs across diverse settings and prompting methods. We have carried out a detailed experimental study and observed various scaling trends that shed light on the\"}"}
{"id": "emnlp-2023-main-429", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"capabilities and limitations of LLMs. Our findings suggest that LLMs often resort to shortcut learning and still face considerable challenges on our proposed benchmark, even when strong prompting techniques are employed. Our work underscores the importance of developing evaluation methodologies to improve the understanding of LLMs and their performance across various tasks.\\n\\nLimitations\\nThis paper proposes a new benchmark ConvRe to evaluate the competence of LLMs in recognizing and processing converse relations. Due to the limitation of the budget, we have evaluated three representative LLM families on the subset of our benchmark for some settings. We note that the LLM APIs may change over time. Although we have set the sampling temperature to 0, we cannot fully guarantee the reproducibility of our results. Another potential limitation is the prompting methods used in this work. To automatically evaluate the model's performance, we have followed the previous studies and formatted the tasks as multiple-choice question answering tests. This setting may affect the performance of smaller models. Finally, due to the unknown data sources and pretraining methods used for proprietary models (e.g., Claude and GPT), it's difficult to arrive at a clear and comprehensive understanding of the behaviors exhibited by LLMs on our benchmark.\\n\\nEthics Statement\\nOur work proposes a new benchmark to help reveal the real capability of LLMs in formal language oriented tasks. The triples in our benchmark are all extracted from publicly available and widely used knowledge graph dataset. We show that LLMs have taken shortcut learning in these tasks and their performance could be inflated. These findings may help users have a better understanding of LLMs and avoid the potential risks.\\n\\nAcknowledgements\\nThis work is supported by the National Key Research and Development Program of China (Grant No. 2021YFB3300400) and National Natural Science Foundation of China (Grant No. 62173017).\\n\\nReferences\\nAnthropic. 2023. Introducing Claude.\\nEmily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of stochastic parrots: Can language models be too big? In Proceedings of the 2021 ACM conference on fairness, accountability, and transparency, pages 610\u2013623.\\nLukas Berglund, Meg Tong, Max Kaufmann, Mikita Balesni, Asa Cooper Stickland, Tomasz Korbak, and Owain Evans. 2023. The reversal curse: Llms trained on \\\"a is b\\\" fail to learn \\\"b is a\\\". arXiv preprint arXiv:2309.12288.\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual.\\nS\u00e9bastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. 2023. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712.\\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. 2021. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374.\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. 2022. Palm: Scaling language modeling with pathways. ArXiv preprint abs/2204.02311.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suzgun, Xinyun Chen, Aakanksha Chowdhery, Alex Castro-Ros, Marie Pellat, Kevin Robinson, Dasha Valter, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Zhao, Yanping Huang, Andrew Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. 2022. Scaling instruction-finetuned language models.\\nEdgar Frank Codd. 1983. A relational model of data for large shared data banks. Communications of the ACM, 26(1):64\u201369.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-429", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As described in Tu et al. (2020), spurious correlation refers to the prediction rules that work for the majority examples but do not hold in general. Superficial cues/biases/artifacts can be treated as unintended correlations between input and output in existing datasets, which are often introduced during data collection or human annotation (Bender et al., 2021; Le Bras et al., 2020; Niven and Kao, 2019). The shortcut used in this paper refers to decision rules that perform well on standard benchmarks but fail to transfer to more challenging testing conditions, such as real-world scenarios (Geirhos et al., 2020). While these terms may have nuanced differences, their essence converges to the idea that models might exploit unintended patterns in datasets, particularly those evident in the majority of examples. This can harm their ability to generalize in open-world scenarios. In this paper, we have introduced textual variance in our benchmark to serve as adversarial test sets and incorporated the counterfactual assumption to assess the real task-level generalization capabilities of LLMs.\\n\\n### B Benchmark Details\\n\\nTo meet the second condition for relations in Sec 2.5, we merge the relation `mother of person` from NELL-ONE dataset with the relation `father` from Wikidata5M to create a new relation called `parent of`. In this way, there are 17 relations in total, and the detailed number of triples for each relation is shown in Table 5. The source knowledge graphs these relations come from cover a wide range of domains, such as socio-political and commonsense, which can ensure the diverseity of our dataset.\\n\\n### C Model Family Details\\n\\n#### C.1 OpenAI GPT\\n\\nThe models we use in our experiments are mainly GPT-3 models (text-ada-001, text-babbage-001 and text-curie-001), GPT-3.5 models (text-davinci-003 and gpt-3.5-turbo) and GPT-4. GPT-3 models can understand and generate natural language. These models were superseded by the more powerful GPT-3.5 generation models. Among the GPT-3.5 models, gpt-3.5-turbo has been optimized for chat but also works well for traditional completion tasks. The version of gpt-3.5-turbo we use in our experiments is gpt-3.5-turbo-0301. GPT-4 is a large multimodal model that can solve difficult problems with greater accuracy than any of the models in OpenAI GPT family, and the version we use for our experiments is gpt-4-0314.\\n\\n#### C.2 Anthropic Claude\\n\\nClaude is capable of a wide variety of conversational and text processing tasks, it can help with use cases including summarization, search, creative and collaborative writing. Claude comes with two different sizes: claude-1 and claude-instant-1. claude-1 is the largest model in Claude family and ideal for a wide range of complex tasks. claude-instant-1 is a smaller model with far lower latency. Both of the models are provided with many different sub-versions. Among them, claude-1.3 and claude-instant-1.1 are used for our experiments.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: The details of the relations in our ConvRe benchmark\\n\\nC.3 Google Flan-T5\\n\\nFlan-T5 is an enhanced version of T5 that has been finetuned in a mixture of tasks. Unlike the OpenAI GPT model, Flan-T5 is an encoder-decoder model. There are five models with different sizes in Flan-T5 family: Flan-T5-Small, Flan-T5-Base, Flan-T5-Large, Flan-T5-XL and Flan-T5-XXL. All five models are used in our experiments.\\n\\nD Subset Results\\n\\nTo verify that the constructed subset can unbiasedly reflect the performance of GPT-4 model, we compare the performance of GPT-4 model on both benchmark dataset and subset. The results are shown in Table 6. The performance of the GPT-4 model shows minimal differences between the complete set and the subset, confirming the validity of the subset.\\n\\n|           | Re2Text | Text2Re |\\n|-----------|---------|---------|\\n| complete set (1240) | 0.987 0.935 0.227 0.164 | 0.936 0.953 0.171 0.144 |\\n| subset (328) | 0.997 0.942 0.192 0.155 | 0.942 0.951 0.171 0.165 |\\n\\nTable 6: The comparison results of GPT-4 model on the complete set and subset under zero shot settings.\\n\\nE Chat Model Performance\\n\\nAs chat models usually have a better ability to follow instructions, they may demonstrate a different scaling trend on our benchmark. Therefore, we independently evaluate and compare the two chat model families (i.e. OpenAI GPT and Anthropic Claude) on our benchmark. As GPT-4 is also optimized for chat, we include it for analysis as well. The performances of the two families are shown in Figure 7.\\n\\nIn Re2Text task, it can be observed that few-shot with Chain-of-Thought can significantly improve the performance of GPT models. The accuracy of GPT-4 demonstrates a remarkable improvement, soaring from below 0.2 in the zero-shot setting to surpassing 0.9 in the Few-shot+Hint+CoT setting. Chain-of-Thought is also helpful in improving the performance of Claude-1.\\n\\nIn Text2Re task, GPT models exhibit a distinct and consistent inverse scaling trend in both zero-shot and few-shot settings when the relation is reversed. However, the scaling trend of Claude models is more intricate. Specifically, in zero-shot settings, Claude models demonstrate a positive scaling trend in the majority of settings. In few-shot settings, on the contrary, an inverse scaling trend is exhibited by Claude models.\\n\\nF Model Behaviors\\n\\nThis section introduces the behaviors of different models that we observed during the experiments. Under zero-shot settings, Claude and Flan-T5 can...\"}"}
{"id": "emnlp-2023-main-429", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Zero-shot and Few-shot results of chat models on ConvRe. Each experimental setting has been indexed with a unique ID that can be referred in Table 2. Sub-figures in the same row share the central figure legend.\\n\\nHowever, text-ada-001 and text-babbage-001 fail in most cases, they tend to repeat our question or instruction. In our experiments, if these two models didn\u2019t give a clear answer, we will treat the choice with higher log probability in the first token as their answers. In few-shot settings, nearly all models except Flan-T5 conform to the expected answer format. The generated thoughts of Flan-T5 are usually shorter than the examples, and the format of its answer seldom aligns with the expected format.\\n\\nG Neutral Relation Results\\nIn this section, we explore the impact of neutral relations on ConvRe benchmark. Specifically, we change the relation text to a more neutral name: relation R, and then run the experiments on the subset mentioned in Appendix D. The results are shown in Table 7.\\n\\nIt can be observed that altering symbols to adopt more neutral names generally shows various effects on the models. The performance of most models in prompt 3# and 4# (the challenging setup) is still around 50% or even worse. However, for Claude models, considerable improvements on converse relations (prompt 3#, 4#, 7#, and 8#) can be observed in the Text2Re task, along with the performance drop on normal relations (prompt 1# and 2#). In conclusion, altering relation text to more neutral forms may help alleviate problems in understanding converse relations, but it carries the risk of harming the performance in normal relations.\\n\\nH Analysis of the Impact of Different Entity Pairs\\nWe firstly extract all the triples with relation hypernym and run five different models on them within the hard setting (prompt 4#) of Re2Text task. Then the overlap percentages of the wrongly answered triples across the models are calculated. The results are shown in Table 8. The diverse accuracies and low overlap percentages of incorrectly answered entity pairs indicate that different entity pairs for the same relation indeed lead to different results on different models.\\n\\nI The Table Version of the Results\\nWe provide our entire experimental results in Table 9 for better clarity.\\n\\nJ Prompt Examples\\nFigure 8 to Figure 19 demonstrate the 12 kinds of prompts used in Re2Text tasks.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model          | Prompt 4# | Prompt 7# | Prompt 8# | Prompt 1# | Prompt 2# | Prompt 3# |\\n|---------------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| text-ada-001 | 0.494 (-0.012) | 0.500 (+0.003) | 0.509 (-0.003) | 0.515 (+0.015) | 0.494 (-0.040) | 0.509 (-0.003) |\\n| text-babbage-001 | 0.518 (+0.051) | 0.537 (+0.074) | 0.537 (-0.015) | 0.527 (+0.012) | 0.500 (-0.009) | 0.466 (-0.019) |\\n| text-curie-001 | 0.527 (+0.006) | 0.463 (-0.031) | 0.448 (-0.046) | 0.439 (-0.037) | 0.500 (-0.009) | 0.500 (-0.009) |\\n| text-davinci-003 | 0.857 (-0.006) | 0.567 (-0.134) | 0.659 (+0.074) | 0.259 (+0.027) | 0.101 (-0.054) | 0.774 (+0.094) |\\n| gpt-3.5-turbo | 0.765 (-0.073) | 0.616 (+0.134) | 0.384 (-0.211) | 0.229 (+0.083) | 0.439 (+0.110) | 0.716 (+0.253) |\\n| gpt-4 | 0.985 (-0.003) | 0.918 (-0.027) | 0.561 (+0.335) | 0.439 (+0.268) | 0.317 (+0.088) | 0.784 (-0.146) |\\n| claude-1 | 0.905 (+0.003) | 0.777 (-0.031) | 0.732 (+0.198) | 0.537 (+0.165) | 0.335 (-0.055) | 0.848 (+0.031) |\\n| claude-instant-1 | 0.762 (+0.073) | 0.613 (-0.152) | 0.485 (+0.113) | 0.384 (-0.122) | 0.558 (-0.104) | 0.777 (-0.144) |\\n| flan-t5-small | - | - | - | - | - | - |\\n| flan-t5-base | 0.796 (-0.042) | 0.329 (+0.061) | 0.665 (-0.039) | 0.201 (+0.049) | 0.488 (+0.049) | - |\\n| flan-t5-large | 0.634 (-0.061) | 0.430 (-0.012) | 0.558 (+0.003) | 0.378 (+0.094) | 0.253 (+0.146) | - |\\n| flan-t5-xl | 0.875 (-0.046) | 0.546 (-0.201) | 0.518 (+0.216) | 0.210 (+0.131) | 0.183 (+0.088) | - |\\n| flan-t5-xxl | 0.738 (-0.070) | 0.591 (-0.095) | 0.476 (+0.104) | 0.290 (+0.064) | 0.180 (+0.034) | - |\\n\\nTable 7: The performance of LLMs on ConvRe benchmark after altering relation text to relation R. The number in the parentheses represents the difference between the neutral relation naming and normal naming under the same setup on the same subset. We do not report the results of Flan-T5 as it struggles to follow the Chain-of-Thought instructions.\\n\\n| Model          | Prompt 4# | Prompt 7# | Prompt 8# | Prompt 1# | Prompt 2# | Prompt 3# |\\n|---------------|-----------|-----------|-----------|-----------|-----------|-----------|\\n| gpt-3.5-turbo | 77.50% | 71.25% | 100.00% | 83.75% | 47.50% |\\n| gpt-4 | - | - | - | - | - |\\n| claude-1 | 11.25% | 0.00% | 1.25% | 17.50% |\\n| claude-instant-1 | 0.00% | 0.00% | - | 0.00% | 0.00% |\\n| flan-t5-xxl | 17.50% | 25.00% | - | - | - |\\n\\nTable 8: The accuracy of five different models on relation hypernym in the hard setting (prompt 4#) of Re2Text task. The bottom part shows the overlap percentage of incorrectly answered entity pairs between these models.\"}"}
{"id": "emnlp-2023-main-429", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: The table of our entire experiments.\\n\\nRead the instruction and then answer the question using A or B.\\n\\nInstruction: (x, has part, y) indicates that x has a part called y.\\n\\nQuestion: (?, has part, solingen)\\n\\nA: Find an entity that solingen contains.\\nB: Find an entity that has a part called solingen.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer:\\n\\nExpected Answer: B\"}"}
{"id": "emnlp-2023-main-429", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Read the instruction and then answer the question using A or B.\\n\\nInstruction: (x, has part, y) indicates that y has a part called x.\\n\\nQuestion: (?, has part, solingen)\\n\\nA: Find an entity that possesses a specific component named solingen.\\nB: Find an entity that is a part of solingen.\\n\\nTo convert the question into a semantically equivalent natural language sentence, which choice is correct?\\n\\nAnswer: Expected Answer: B\"}"}
