{"id": "emnlp-2024-main-277", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Comparison between BabyBERTa\u2019s AO-CHILDES (2021) corpus to our KidLM (corpus).\\n\\nfocuses on \u2018engines with engines\u2019 to ensure a fair and meaningful analysis.\\n\\nD Domain Adaptation of LMs\\nThe adaptation of language models to specific domains typically follows two strategies. The first involves training a new model from scratch with data from the targeted domain. The second strategy, known as continual pre-training (Howard and Ruder, 2018), involves further training pre-existing models to transition from a generic to a specialized model. While there have been numerous studies adapting models to target domains like Programming (Feng et al., 2020), Academic (Shen et al., 2021), Biomedical (Bolton et al., 2024), Mathematics (Azerbayev et al., 2024), Healthcare (Rasmy et al., 2021), Finance (Yang et al., 2020), Legal (Leivaditi et al., 2020), Mental Health (Ji et al., 2022), and the Dark Web (Jin et al., 2023). Domain-specific LMs are often trained using easily accessible, publicly available corpora. However, identifying the authors and intended purposes of these publicly sourced texts is challenging, which is crucial for a user-centric language model (e.g., for children). There is limited research on developing language models for specific user groups; the most relevant study we found was BabyBERTa (Huebner et al., 2021), which focused on the task of language acquisition in children aged 1 to 6.\\n\\nBabyBERTa\\n- AO-CHILDES (2021)\\n- \u223c5M words\\n- \u223c8K vocabulary\\n- Audience: 1-6 years\\n- Spoken Language\\n\\nKidLM (corpus)\\n- Ours\\n- \u223c50.5M words\\n- \u223c50K vocabulary\\n- Audience: General Children\\n- Written Language\"}"}
{"id": "emnlp-2024-main-277", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| SN. | Data Sources      | URL               | #Docs | #Sents | Avg. #Sents | Avg. #Words |\\n|-----|-------------------|-------------------|-------|--------|-------------|-------------|\\n| 1   | CBC Kids          | cbc.ca/kids       | 262   | 5,959  | 22.74 \u00b116.33 | 349.63 \u00b1252.02 |\\n| 2   | CBC Kids News     | cbc.ca/kidsnews   | 2,559 | 62,293 | 24.34 \u00b115.04 | 531.20 \u00b1339.02 |\\n| 3   | Curious Times     | curioustimes.in   | 8,493 | 107,649| 12.68 \u00b111.13 | 206.23 \u00b1179.84 |\\n| 4   | The Kids News     | htekidsnews.com   | 450   | 12,776 | 28.39 \u00b120.26 | 554.79 \u00b1381.31 |\\n| 5   | Kids Frontiers    | kids.frontiersin.org | 1,210 | 121,156| 100.13 \u00b121.83 | 2240.82 \u00b1481.03 |\\n| 6   | Kids News & Reviews | kidsnewsandreviews.com | 84  | 5,004  | 59.57 \u00b140.99 | 1267.42 \u00b1895.29 |\\n| 7   | Kids News NYC     | kidsnewsnyc.com   | 238   | 7,708  | 32.39 \u00b121.29 | 692.54 \u00b1456.23 |\\n| 8   | Kids News (India) | kidsnews.top      | 2,637 | 32,324 | 12.26 \u00b114.35 | 226.59 \u00b1255.40 |\\n| 9   | Kids Press        | kpcnotebook.scholastic.com | 1,628 | 39,738 | 24.41 \u00b111.81 | 475.77 \u00b1214.47 |\\n| 10  | News for Kids     | newsforkids.net   | 1,619 | 57,079 | 35.26 \u00b19.91  | 608.63 \u00b1172.56 |\\n| 11  | Smithsonian Magazine | smithsonianmag.com | 20  | 1,043  | 52.15 \u00b141.44 | 1190.25 \u00b1870.10 |\\n| 12  | Teaching Kids News | teachingkidsnews.com | 1,127 | 37,403 | 33.19 \u00b110.05 | 636.12 \u00b1197.06 |\\n| 13  | Time for Kids     | timeforkids.com   | 2,109 | 44,413 | 21.06 \u00b118.20 | 294.71 \u00b1291.46 |\\n| 14  | Twinkl Newsroom   | twinkl.ca/newsroom | 876  | 19,408 | 22.16 \u00b19.32  | 375.22 \u00b1142.62 |\\n| 15  | Washington Post (Kids) | washingtonpost.com/kidspost | 1,622 | 48,132 | 29.67 \u00b117.08 | 573.27 \u00b1297.04 |\\n| 16  | Indy Kids         | indykids.org      | 1,658 | 21,671 | 13.07 \u00b114.36 | 306.26 \u00b1324.27 |\\n| 17  | Kids News         | kidsnews.com.au   | 915   | 20,052 | 21.91 \u00b131.67 | 586.23 \u00b1606.99 |\\n| 18  | Kiwi Kids News    | kiwikidsnews.co.nz | 7,163| 28,936 | 4.04 \u00b14.67   | 159.21 \u00b1125.70 |\\n| 19  | Spaghetti Book Club | spaghettibookclub.org | 12,095 | 168,346 | 13.92 \u00b16.11  | 227.12 \u00b1100.97 |\\n| 20  | Toppsta           | toppsta.com       | 34,471| 146,302| 4.24 \u00b12.96   | 117.62 \u00b181.22 |\\n| 21  | Simple Wiki       | simple.wikipedia.org | 205K | 1.924M | 9.37 \u00b117.98  | 185.59 \u00b1406.98 |\"}"}
{"id": "emnlp-2024-main-277", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Category          | Group                                                                 | Total |\\n|-------------------|----------------------------------------------------------------------|-------|\\n| Age               | boomers, children, kids, millennials, old men, old people, old women, teenagers, teens | 9     |\\n| Gender            | girls, women, men, females, males, boys, boyfriends, stepmothers, stepfathers, ladies, gentlemen, brothers, sisters, mothers, fathers, grandfathers, grandmothers, wives, husbands, schoolgirls, schoolboys, transgenders | 23    |\\n| Lifestyle         | feminists, frat boys, geeks, goths, hippies, hipsters, nerds, punks, sorority girls, celebrities, criminals, homeless people, poor people, rich people | 14    |\\n| Political         | capitalists, communists, conservatives, immigrants, liberals, populists, socialists, Trump supporters | 8     |\\n| Ethnicities       | Africans, Asians, Asian kids, Asian men, Asian parents, Asian women, African Americans, Black Americans, Blacks, Black fathers, Black kids, Black men, Black parents, Black people, Black women, Europeans, Hispanics, Hispanic men, Hispanic women, Latinas, Latinos, Latin people, Native Americans, Whites, White Americans, White kids, White men, White parents, White people, White women, redheads, gingers, blondes | 32    |\\n| Nationalities     | Americans, Afghans, Albanians, Arabs, Australians, Austrians, Bengalis, British people, Chileans, Colombians, Dutch people, Egyptians, Ecuadorians, Ethiopians, Finns, French people, Germans, Ghanaians, Greeks, Indians, Indonesians, Iranians, Iraqis, Irish people, Italians, Koreans, Lebanese people, Mexicans, Moroccans, Nepalis, Nigerians, Norwegians, Pakistanis, Polish people, Romanians, Russians, Scots, Somalis, South Africans, Sudanese people, Swedes, Syrians, Taiwanese people, Turkish people, Ukrainians, Venezuelans, Vietnamese people | 47    |\\n| Religion          | Atheists, Buddhists, Catholics, Christians, Hindus, Jews, Mormons, Muslims, Protestants, religious people, Sikhs | 11    |\\n| Sexual orientation| asexual people, bisexual people, gay people, homosexuals, lesbians, pansexual people, queer people | 7     |\\n\\n| Total            | 151 |\\n\\nTable 11: A list of 151 social groups, categorized into 8 distinct categories, is used for evaluating stereotypes, as detailed in Section 3.2.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The casualties are reported to have included children. Parents, victims, families, deaths, fatalities. Even before its enactment it saw widespread criticism. Release, introduction, launch. The report is known to contain some material disputed by Lin and Phyo. Written, used, covered. Written, added, created. Questioned, debated, disagreed. Banking reform is seen as urgent by many analysts, with yields on benchmark Spanish bonds currently close to six percent, meaning the country faces very high borrowing costs. All, most, the. The, all, major. Standard, benchmark, key. Her older sister, aged 21, lived at the rented house, in a recently built development at the back of the established housing estate. New, nearby, council. New, larger, nearby. Accepted, settled, old. EU sanctions Monday's violence further undermines a UN-backed peace plan that is supposed to bring an end to Syria's deadly crisis. Said, says, claims. Said, says, on. Penalties, disciplines, penalizes. There's conflicting evidence about whether sick ants actually smell different from healthy ones or not. No, little, good. No, little, some. Clashing, inconsistent, differing. Before you make an unblock request, you should attentively read the policies and guidelines named in your block reason. First, also, carefully. First, also, carefully.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Preferences                                                                 | Models           | Completions |\\n|---------------------------------------------------------------------------|------------------|-------------|\\n| \\\"I love playing [MASK].\\\"                                                  | RoBERTa         | 'chess' (0.115), 'games' (0.097), 'guitar' (0.044), 'tennis' (0.041), 'golf' (0.032) |\\n|                                                                           | KidLM            | 'football' (0.741), 'basketball' (0.06), 'chess' (0.045), 'baseball' (0.031) |\\n|                                                                           | KidLM+           | 'football' (0.717), 'basketball' (0.038), 'chess' (0.031) |\\n| \\\"My favorite person is my [MASK].\\\"                                       | RoBERTa         | 'mom' (0.216), 'husband' (0.121), 'mother' (0.107), 'family' (0.097), 'dad' (0.064) |\\n|                                                                           | KidLM            | 'grandfather' (0.207), 'mom' (0.17), 'teacher' (0.126), 'father' (0.081), 'grandmother' (0.07) |\\n|                                                                           | KidLM+           | 'mom' (0.181), 'father' (0.172), 'dad' (0.152), 'mother' (0.15), 'grandfather' (0.067) |\\n| \\\"On weekends, I like to [MASK].\\\"                                         | RoBERTa         | 'read' (0.125), 'work' (0.061), 'write' (0.059), 'relax' (0.056), 'travel' (0.051) |\\n|                                                                           | KidLM            | 'read' (0.802), 'paint' (0.025), 'swim' (0.024), 'dance' (0.02), 'play' (0.02) |\\n|                                                                           | KidLM+           | 'read' (0.644), 'paint' (0.144), 'swim' (0.066), 'play' (0.024), 'study' (0.015) |\\n| \\\"I like stories about [MASK].\\\"                                           | RoBERTa         | 'people' (0.041), 'animals' (0.032), 'women' (0.028), 'them' (0.027), 'me' (0.017) |\\n|                                                                           | KidLM            | 'animals' (0.185), 'dinosaurs' (0.059), 'frogs' (0.034), 'horses' (0.033), 'baseball' (0.031) |\\n|                                                                           | KidLM+           | 'animals' (0.089), 'space' (0.06), 'boats' (0.047), 'dogs' (0.046), 'elephants' (0.037) |\\n| \\\"I feel happiest when I [MASK].\\\"                                         | RoBERTa         | 'sleep' (0.203), 'work' (0.105), 'write' (0.06), 'sing' (0.055), 'travel' (0.042) |\\n|                                                                           | KidLM            | 'sleep' (0.297), 'play' (0.084), 'smile' (0.069), 'swim' (0.049), 'talk' (0.048) |\\n|                                                                           | KidLM+           | 'sleep' (0.5), 'rest' (0.18), 'smile' (0.106), 'eat' (0.017), 'sing' (0.013) |\\n| \\\"I wish I could improve my skill at [MASK].\\\"                              | RoBERTa         | 'chess' (0.138), 'it' (0.062), 'math' (0.062), 'writing' (0.05), 'typing' (0.031) |\\n|                                                                           | KidLM            | 'basketball' (0.301), 'football' (0.289), 'chess' (0.091), 'school' (0.052), 'maths' (0.037) |\\n|                                                                           | KidLM+           | 'basketball' (0.248), 'chess' (0.177), 'spelling' (0.102), 'football' (0.053), 'volleyball' (0.053) |\\n| \\\"It's my dream to own a [MASK] one day.\\\"                                  | RoBERTa         | 'house' (0.303), 'home' (0.119), 'car' (0.07), 'Tesla' (0.049), 'Porsche' (0.034) |\\n|                                                                           | KidLM            | 'boat' (0.229), 'restaurant' (0.099), 'car' (0.056), 'castle' (0.053), 'horse' (0.051) |\\n|                                                                           | KidLM+           | 'boat' (0.292), 'dog' (0.161), 'restaurant' (0.092), 'bakery' (0.092), 'plane' (0.061) |\\n| \\\"I'd like to get a [MASK] for my birthday.\\\"                               | RoBERTa         | 'car' (0.065), 'bike' (0.062), 'tattoo' (0.05), 'cake' (0.041), 'motorcycle' (0.038) |\\n|                                                                           | KidLM            | 'robot' (0.126), 'dog' (0.096), 'dinosaur' (0.067), 'horse' (0.053), 'puppy' (0.032) |\\n|                                                                           | KidLM+           | 'robot' (0.181), 'dog' (0.125), 'horse' (0.088), 'whale' (0.044), 'cat' (0.043) |\"}"}
{"id": "emnlp-2024-main-277", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| SN. | Data Source           | Description                                                                 | Genre                                                                 | Additional Notes                                                                 |\\n|-----|----------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------|----------------------------------------------------------------------------------|\\n| 1   | CBC Kids             | CBC Kids is dedicated to creating fun and inspiring stories that will uplift and enrich Canadian children. | Stories, Facts, Science, Health, Religion, Population, Festivals, etc. | Not Applicable                                                                   |\\n| 2   | CBC Kids News        | CBC Kids News is a daily news service for kids in Canada. It aims to cover the topics that kids care about, providing real news for real kids. Created by and for kids, it is also designed to be a safe place for children. You can trust that the information you read on CBC Kids News is well-researched, balanced, and supported by facts. This is ensured by following the Canadian Broadcasting Corporation's Journalistic Standards and Practices. | News, Pop Culture, Sports, Science, Technology, and the Environment. | Not Applicable                                                                   |\\n| 3   | Curious Times        | Curious Times is India's pioneering news website for children, serving as an online newspaper designed to bridge the gap between their school curriculum and world affairs. Its primary aim is to provide authentic news and information to children in simple language. More than just a news website, Curious Times acts as a platform that offers an academic and educational context to current affairs, making it relevant for kids and students alike. The news articles and activities on Curious Times are carefully personalized and curated to cater to children's needs. The short news articles are devoid of sensationalism often found in other news reports, ensuring a reliable and informative reading experience for young readers. | Current Event Articles, Science News, International News, Sports News, Student News, Technology News, Space News, Climate News, National and Regional News, Important Days, Dates and Festivals. | Not Applicable                                                                   |\\n| 4   | The Kids News        | A news website/blog specifically designed for Elementary school-aged children. Its main purpose is to demonstrate to kids how they are intricately connected to the world around them and to introduce them to the influential people and significant events that shape the world they live in. | World, People, Sports, Nature, Science, Space, Politics, Weather, etc. | Elementary school-aged children: Generally, this group includes children from Kindergarten to 5th grade, which is approximately ages 5 to 11. Last Accessed: 20 December 2022. Now this server is dead. |\\n| 5   | Kids Frontiers       | Frontiers for Young Minds strongly believes in making cutting-edge science discoveries accessible to younger audiences. To achieve this, the platform fosters collaboration between young people and scientists to create top-quality and captivating articles. Esteemed scientists are invited to write about their discoveries using language that is easily understandable for young readers. Subsequently, the kids themselves, along with a science mentor, actively participate by providing feedback and suggestions to the authors to enhance the articles before publication. The platform's dedication to empowering the youth and promoting scientific understanding makes it a valuable resource for young minds. | SCIENCE (Astronomy and Physics, Biodiversity, Chemistry and Materials, Earth Sciences, Engineering and Technology, Human Health, Mathematics and Economics, Neuroscience and Psychology) | Not Applicable                                                                   |\\n| 6   | KiDS NEWS & REViEWS  | KiDS NEWS & REViEWS provides a secure and nurturing space for kids and youth to express their thoughts, feelings, and opinions through various forms of media. These include non-fiction and fiction stories, songs, and video formats. They carefully curate content that resonates with kids and youth, inspiring their peers. Their stories and digital media creations are frequently shared not only among parents and their children but also among teachers and their students. The platform aims to foster creativity, communication, and a sense of community among young minds. | Early Learning Reviews, Teacher's Aids, Teacher's Stories, Teacher's Tips, Parent's Story, Youth Reviewers, Kid's Fiction Story. | Not Applicable                                                                   |\"}"}
{"id": "emnlp-2024-main-277", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 16: Description of the sources from which we collected data, including the genre and additional notes. \u2018C\u2019 denotes the country.\\n\\n| SN. | Data Source       | Description                                                                 | Genre                                                                 | Additional Notes                                                                 |\\n|-----|------------------|----------------------------------------------------------------------------|-----------------------------------------------------------------------|----------------------------------------------------------------------------------|\\n| 7   | Kids' News NYC   | Kids' News NYC is for anyone under 12 years old who lives in or around New York City, has a love for exploring, learning, and noticing their surroundings, and wants to report on it to other kids! Created by Waverly W., the 8-year-old Kiditor in Chief, with a little help from her mom, Kids' News NYC is all about YOU! (the reader). It serves as an online newspaper and YouTube Channel dedicated to all the news, events, people, and things that interest city kids or kids who wish they were city kids! The difference is that here, the kids create the news. | Super Sports & Great Games, Interviews, Reviews, Adventures, etc.           | Not Applicable                                                                  |\\n| 8   | Kids News (India)| The news portal exclusively for children offers engaging and relevant news items covering nature, history, space, and other interesting topics. Children can actively participate by sending their own contributions like art and creative writing. The portal provides simple explanatory articles to help children understand complex words and concepts. Additionally, kids can enjoy puzzles, riddles, book reviews, stories, and other captivating content unique to the platform. The safety of the environment, free from ads, ensures a secure and enjoyable online experience for young users. | News, Sports, History, International, Science, Business, Tech, Weather, Health, etc. | Not Applicable                                                                  |\\n| 9   | Kids Press       | Scholastic Kids Press is a group of talented Kid Reporters, ages 10\u201314, from across the country and around the world. Since 2000, our award-winning young journalists have been reporting \u201cnews for kids, by kids,\u201d covering politics, entertainment, the environment, sports, and more in their hometowns and on the national stage. Their stories appear online and in issues of Scholastic Magazines+, reaching more than 25 million students in classrooms nationwide. | Politics, entertainment, the environment, sports, and more in their hometowns and on the national stage. | Not Applicable                                                                  |\\n| 10  | News for Kids    | NewsForKids.net was created by a teacher to make the news accessible to kids. They carefully choose high-interest stories appropriate for the audience and present them in a way that is easy to understand. They work hard to use simple language when telling the stories, aiming to be as inclusive as possible. The goal is to ensure that advanced readers can read \u201cdown\u201d comfortably, while struggling readers are not left behind with content that is too challenging for them to read \u201cup.\u201d | World, Science, Environment, Technology, Sports, and Arts.              | Not Applicable                                                                  |\\n| 11  | Smithsonian Magazine | World renowned for its unparalleled coverage of nature history, science and the arts, Smithsonian Magazine explores lifestyles, cultures, people, technology, music and Americana for an inquisitive and educated readership. Published by the Smithsonian Institution, this magazine also includes photo essays and in-depth articles highlighting current Smithsonian museum exhibits. | History, Science, Innovation, Arts & Culture, Travel etc.              | We extracted the articles tagged for children (https://www.smithsonianmag.com/tag/children/) |\\n| 12  | Teaching Kids News | Every story is in kid-friendly language and appropriate for kids in grades 3 to 8. Beyond just making the vocabulary accessible, they provide context for everything in each news story, so kids can understand what's going on, and why. In the curriculum connections they encourage kids to think critically not only about the story itself, but about the way the story is presented. | Politics, Arts, Entertainment, Science & Technology, Environment, Animals, Health, Sports, etc. | Not Applicable                                                                  |\\n| 13  | Time for Kids    | Authentic, age-appropriate news for kids and valuable resources for teachers and families. Time for Kids is published in four grade-based editions: K\u20131, 2, 3\u20136, and 5\u20136. | Science, Earth Science, Health, The Human Body, History, Holidays, Environment, People, Arts, Technology, Inventions, Sports, and Animals. | We collected data from the grade levels: K\u20131, 2, 3\u20134, and 5\u20136.                   |\\n| 14  | Twinkl Newsroom  | Daily kids' news reports are child-friendly and a perfect way to help your class explore the news with confidence. Each news report comes with a range of curriculum-friendly teaching resources! | General News and Teaching Resources.                                    | Not Applicable                                                                  |\"}"}
{"id": "emnlp-2024-main-277", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| SN. | Data Source         | Description                                                                 | Genre                                                                 | Additional Notes                                                                 |\\n|-----|--------------------|-----------------------------------------------------------------------------|----------------------------------------------------------------------|---------------------------------------------------------------------------------|\\n| 15  | Washington Post (Kids) | The Washington Post is an American daily newspaper published in Washington, D.C. It is the most widely circulated newspaper within the Washington metropolitan area. We collected the age-appropriate news for kids. | Politics, Opinions, Climate, Tech, Lifestyle, and World. | We collected the articles tagged as \\\"kidspost\\\" (https://www.washingtonpost.com/kidspost/) |\\n| 16  | Indy Kids          | The mission of IndyKids is to engage young people and empower them to become informed global citizens through the creation of a current events and social justice news source that is produced for kids, by kids. Throughout their programs, they inspire a passion for social justice issues to empower the next generation of critical thinkers, community leaders, journalists and activists. | Current Events and Social Justice issues. | Not Applicable                                                                  |\\n| 17  | Kids News          | Kids News is a free news-based literacy tool designed for classrooms, catering to students from Grade 3 to Year 8. The content is written into educational stories in child-appropriate language and filtered/censored to remove any inappropriate content or imagery. It employs a traffic light system to guide teachers in directing students to suitable content based on their comprehension levels. Green indicates simple to medium vocabulary, easily understood stories accessible to all readers. Orange signifies a medium level of vocabulary and slightly more complex stories suitable for middle to senior primary level with the aid of audio and a glossary. Red denotes content with high-level vocabulary and complexity, best suited for more proficient readers with teacher support for less capable ones. | Science, Sport, History, Space, Weather, Animals, Health, Geography, Civics, Humanities, Technology, Environment, Money, Explainers, Arts, Mathematics, etc. | Kids News is a free news-based literacy tool designed for classrooms, catering to students from Grade 3 to Year 8 (corresponds to the period when students are around 12 to 13 years old). We took the Green and Orange level contents and filtered out the Red level ones to maintain the quality. |\\n| 18  | Kiwi Kids News      | Kiwi Kids News serves as the news platform catering to students and educators in New Zealand. It publishes 3 to 4 pertinent news articles on a daily basis throughout the term. Since its establishment in 2010, the website's popularity has steadily increased. | National, World, Sports, etc. | Not Applicable                                                                  |\\n| 19  | Spaghetti Book Club | Reviews of books that are accessible to children through public libraries or online purchases. These reviews should focus on secular books. To be featured on our website, all book reviews must consist of a summary, personal opinion, and a recommendation. | Book reviews | We collected the data from the categories Grade K-1, Grade 2-3, Grade 4-5, Grade 6-9 (we limit this to age 12 from the author reported age, which is equivalent to Grade 6). |\\n| 20  | Toppsta            | Toppsta is a solution for those overwhelmed by the vast selection of children's books. With numerous new releases each year, it can be challenging to know where to start. Toppsta aims to be the go-to platform where readers can recommend the finest books to one another. Whether you're a parent, grandparent, teacher, or librarian, the book reviews on Toppsta.com assist in discovering the best books for children, benefiting various readers and book-related professionals. | Book Reviews | Not Applicable                                                                  |\\n| 21  | Simple Wiki        | Simple Wikipedia is a distinct version of the widely used Wikipedia. It is written in basic English, making it suitable for younger kids, tweens, or even teens who read at a lower grade level. The simplified version still functions as an online encyclopedia, but its sentences are shorter and grammar is easier to understand. Simple Wikipedia can also prove beneficial for individuals from cultures that are in the process of learning English or those with a limited understanding of the language. Additionally, it is a helpful resource for readers with learning disabilities. | The genres or topics covered on Simple Wikipedia are similar to those on regular Wikipedia and include Science, History, Geography, Biographies, Mathematics, Technology, Arts and Culture, Health and Medicine, Animals and Nature, Sports, etc. | Not Applicable                                                                  |\"}"}
{"id": "emnlp-2024-main-277", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"KidLM: Advancing Language Models for Children \u2013 Early Insights and Future Directions\\n\\nMir Tafseer Nayeem\\nUniversity of Alberta\\nmnayeem@ualberta.ca\\n\\nDavood Rafiei\\nUniversity of Alberta\\ndrafiei@ualberta.ca\\n\\nAbstract\\n\\nRecent studies highlight the potential of large language models in creating educational tools for children, yet significant challenges remain in maintaining key child-specific properties such as linguistic nuances, cognitive needs, and safety standards. In this paper, we explore foundational steps toward the development of child-specific language models, emphasizing the necessity of high-quality pre-training data. We introduce a novel user-centric data collection pipeline that involves gathering and validating a corpus specifically written for and sometimes by children. Additionally, we propose a new training objective, Stratified Masking, which dynamically adjusts masking probabilities based on our domain-specific child language data, enabling models to prioritize vocabulary and concepts more suitable for children. Experimental evaluations demonstrate that our model excels in understanding lower grade-level text, maintains safety by avoiding stereotypes, and captures children\u2019s unique preferences. Furthermore, we provide actionable insights for future research and development in child-specific language modeling.\\n\\n1 Introduction\\n\\nChildren constitute one in three internet users globally, according to a UNICEF study (Keeley and Little, 2017), with the average screen time for kids aged 8-12 estimated to be over five hours per day (Rideout et al., 2022). This level of digital engagement presents both opportunities and challenges for enhancing children\u2019s learning experiences. Large Language Models (LLMs) have significantly lowered the barriers to building educational tools and applications (Huber et al., 2024), with some studies suggesting these models enhance children\u2019s learning by facilitating engaging and emotionally responsive conversations (Seo et al., 2023, 2024). We make our pre-training data, code, model checkpoints, and output completions publicly available at KidLM.\\n\\n| Age Range | InstructGPT | Aya Dataset |\\n|-----------|-------------|-------------|\\n| 18-24     | 26.3%       | 41.8%       |\\n| 25-34     | 47.4%       | 40.7%       |\\n| 35-44     | 10.5%       | 12.1%       |\\n| 45-54     | 10.5%       | 3.0%        |\\n| 55-64     | 5.3%        | 1.2%        |\\n\\nTable 1: Annotators\u2019 Age Distribution in the InstructGPT (Ouyang et al., 2022) and Aya Dataset (Singh et al., 2024) used for supervised fine-tuning (SFT). The top two percentages for each dataset are marked in bold.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cal (Bolton et al., 2024), Mathematics (Azerbayev et al., 2024), or languages like those in Southeast Asia (Dou et al., 2024). SFT, on the other hand, trains a language model with specific instructions or guidelines to align with specific tasks (Wei et al., 2022) and user preferences via RLHF (Ouyang et al., 2022), using data consisting of pairs of instructions and their corresponding desired outputs.\\n\\nA key component of both continual pre-training and SFT is the existence of high-quality data, whether synthetic or human-annotated (AI et al., 2024; Liu et al., 2024). However, annotators for SFT data are predominantly from the age group 18-35 (Table 1), whose distinct linguistic and cognitive preferences, as well as safety needs, differ significantly from those of children. For example, annotators on Amazon Mechanical Turk (MTurk) must be at least 18 years old. Consequently, the SFT data may not adequately address the unique requirements of younger users. This limitation prompts an intriguing question: Can a language model be developed specifically for a particular user group, such as children in our case?\\n\\nLanguage models for children are expected to possess three essential properties: (1) the ability to generate simpler words and understand lower grade-level texts, (2) free from any stereotypes (Bozzola et al., 2022), and (3) the capacity to model children's unique preferences and emotions for personalized engagement. We argue that achieving these properties simultaneously in a language model necessitates the use of high-quality pre-training data. Modern LLMs typically pre-train on corpora containing hundreds of billions to several trillions of tokens from vast internet text data (Touvron et al., 2023; Penedo et al., 2023). Two often disregarded aspects of this text data are: (i) the demographics and intentions of its creators, and (ii) the intended audience for whom it was written. Both factors can significantly influence the composition and distribution of the data, and consequently, the resulting behavior of a user-centric language model (e.g., children).\\n\\nWith the aforementioned requirements for language models tailored for children, we curated high-quality, kid-appropriate content specifically written for children and occasionally by them. This content was meticulously reviewed and validated by website editors or moderators to ensure its suitability and the absence of inappropriate content or sensationalism. Our data collection pipeline is comprehensive, diverse, and appropriately tailored for children's language models, while also being scalable to support the accumulation of more sources for future development. Given the size of our collected pre-training data and available resources, we opted to train a masked language model (MLM) to validate the corpus quality and ensure support for the kid-specific properties discussed above. This model introduces the stratified masking method, which offers a way to prioritize words relevant to children and is also applicable in low-resource learning scenarios. Furthermore, we offer suggestions for future directions to extend our findings.\\n\\nOur main contributions are summarized as follows:\\n\\n\u2022 We propose a user-centric data collection pipeline to curate high-quality data specifically written for, and occasionally by children, validated by website editors (\u00a72.1).\\n\u2022 We introduce a novel stratified masking technique for training an MLM on our KidLM corpus and validating the smooth integration of kid-specific properties into the LM (\u00a72.2.1).\\n\u2022 Our KidLM models effectively understand lower grade-level texts and show a reduced likelihood of reinforcing negative stereotypes and generating toxic completions across 151 social groups in 8 categories (\u00a73).\\n\\n2 KidLM Construction\\n\\nOur aim for KidLM is to create language models tailored for children by developing a high-quality, user-centric corpus. This involves meticulous data collection and verification to ensure reliability and relevance, along with a novel masking process to enhance the model's focus on kid-specific words. The data collection process is outlined in Figure 2.\\n\\n2.1 KidLM Corpus\\n\\nOur corpus collection pipeline is designed with a user-centric approach to ensure high-quality, kid-appropriate textual data. The process includes several stages:\\n\\n- User-Centric: Our goal is to curate a high-quality corpus of textual data specifically written for children and, occasionally, by them. This content undergoes thorough review and validation by website editors or moderators.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"editors or moderators to ensure its suitability, appropriateness, and absence of sensationalism or inappropriate material. Our user-centric approach to data collection carefully considers two critical aspects: (i) the demographics and intentions of the content creators (\\\"Who?\\\") and (ii) the intended audience for whom the content is written (\\\"Whom?\\\").\\n\\nSource Identification\\nThe initial phase of our data collection methodology involved using Google Search to identify a preliminary set of websites, noted as $X = \\\\{\\\\text{Time for Kids, News for Kids, ... , Kids Press}\\\\}$. Subsequently, we employed ChatGPT, prompting it with \\\"List websites similar to $X$ that offer kid-specific content\\\", to expand our list. This process yielded an additional collection of relevant websites, which were then merged with the initial set $X$. Finally, we utilized SimilarWeb, a web analytics tool, to further extend our list. Specifically, we used the \\\"Similar Sites\\\" feature of SimilarWeb to identify analogous sites.\\n\\nManual Data Verification\\nWe manually verified and filtered the data sources by reviewing the \\\"about\\\" sections of the identified source websites, as detailed in Tables [15, 16, 17] (Description column) of the Appendix.\\n\\nQuality Filtering\\nArticles were filtered based on specific criteria, depending on the availability of information from the sources, such as (1) Extracting articles tagged specifically for children, (2) Identifying those labeled as \\\"kidspost\\\", (3) Excluding articles tagged as potentially inappropriate content with colors such as red, and (4) Selecting data relevant to specific grade levels (K-1, 2-3, 4-5, and 6). These criteria are further explained in Tables [15, 16, 17] (Additional Notes column) of the Appendix.\\n\\nAdditional Filtering\\nWe included only English text and removed sentences involving code-mixing and code-switching. Additionally, we eliminated any Personal Identifying Information (PII) from the corpus. Details of these processes are provided in Appendix A.\\n\\nData Diversity\\nTo ensure genre diversity, the corpus includes articles on science, sports, history, animals, geography, technology, current events, book reviews, and more, all tailored to meet the interests of young readers. We collected data from 21 sources originating from various regions: USA (4), India (4), Canada (3), Australia (1), UK (1), New Zealand (1), and other global sources (7), aiming to avoid geographic and cultural biases (detailed in Tables [15, 16, 17] of the Appendix).\\n\\nData Quantity\\nOur KidLM corpus contains over 286,000 documents, approximately 2.91 million sentences, and 50.43 million words. Upon processing with the RoBERTa tokenizer (Liu et al., 2019), this amounted to approximately 67.97 million tokens. Table 10 in the Appendix shows the detailed statistics of the collected data across sources.\\n\\nDepending on the availability of grade level information, we aim to limit the documents to the 6th grade, which corresponds to the age of 12.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stopwords\\nDell Chall easy words\\nOther Words\\nP(masked) = 0.25\\nP(masked) = 0.15\\nP(masked) = 0.20\\n\\nFigure 2: Venn diagram illustrating different word classes used in our proposed Stratified Masking.\\n\\n2.2 KidLM Models\\n\\nWe use our KidLM corpus to develop language models tailored for children. Given the corpus size and available resources, we opt to train an MLM to validate corpus quality and ensure support for kid-specific properties. Our model has two variations:\\n\\n1. KidLM: We continue to pre-train RoBERTa (Liu et al., 2019) using our KidLM corpus (\u00a72.1) with an MLM learning objective, which involves randomly masking 15% of the input sequence's words to predict these masked words from their context.\\n\\n2. KidLM+: This version introduces a novel masking strategy called Stratified Masking, varying the probability of masking based on word classes. This approach enhances the model's focus on tokens that are more informative and specifically tailored to children, making it particularly useful for low-resource learning scenarios where the pre-training corpus is relatively smaller and designed to inject specific properties into the language model.\\n\\n2.2.1 Stratified Masking\\n\\nWe aim to steer LM predictions towards kid-specific words from our high-quality corpus. To achieve this, we introduce Stratified Masking based on two principles:\\n\\n1. All words in our corpus have a non-zero probability of being masked, and\\n2. Words more likely to be found in a general corpus are masked with lower probability. With these principles, each word in our corpus is assigned to one of the following three strata:\\n\\n   - Stopwords: which are generally the most frequent words in a language. Utilizing NLTK's list of 179 stopwords (Bird, 2006), we apply a 0.15 masking rate to these words. Our hypothesis for masking is that children use stopwords distinctively, often in reference to specific nouns like 'cars', 'trains', and 'butterflies'. Additionally, many pronouns such as 'he', 'she', 'his', and 'her' are categorized as stopwords.\\n\\n   - Dale-Chall Easy Words List: comprises 2950 words that are reliably understood by students (Chall and Dale, 1995). Of these, 48.85% overlap with stopwords, which we subsequently remove. We then mask the remaining 2807 words at a slightly higher masking rate of 0.20 to prioritize the linguistic simplicity specific to children.\\n\\n   - Other Words: In our KidLM corpus (\u00a72.1), it is unsurprising that stopwords are dominant, accounting for 45.93%, while Dale-Chall Easy words make up 21.82%, and other words constitute 32.45%. We assume that these 'other words' often include nouns and entities, reflecting children's preferences or safe alternatives introduced by website editors or moderators. Consequently, we assign them a higher masking rate of 0.25 to emphasize their informative importance during training.\\n\\nFigure 3: (a) In default random masking, all words have a equal probability of 0.15 of being masked. (b) In our proposed stratified masking, stopwords are masked with a probability of 0.15, Dale-Chall words with a probability of 0.20, and other words with a probability of 0.25, to enhance learning focus on kid-specific words.\\n\\nWords. By masking them, we aim to learn debiased representations from the data during pre-training. Dale-Chall Easy Words List comprises 2950 words that are reliably understood by students (Chall and Dale, 1995). Of these, 48.85% overlap with stopwords, which we subsequently remove. We then mask the remaining 2807 words at a slightly higher masking rate of 0.20 to prioritize the linguistic simplicity specific to children.\\n\\nOther Words: In our KidLM corpus (\u00a72.1), it is unsurprising that stopwords are dominant, accounting for 45.93%, while Dale-Chall Easy words make up 21.82%, and other words constitute 32.45%. We assume that these 'other words' often include nouns and entities, reflecting children's preferences or safe alternatives introduced by website editors or moderators. Consequently, we assign them a higher masking rate of 0.25 to emphasize their informative importance during training. Figure 2 presents a Venn diagram of different classes of words with associated probability. Formally, given a text sequence, the model generates a masked text \\\\( T_M \\\\) by applying the following procedure to each token \\\\( x_i \\\\):\\n\\n\\\\[\\nT_M(x_i) = \\\\begin{cases} \\n[\\\\text{MASK}] & \\\\text{with prob. 0.15 for stopwords} \\\\\\\\\\n[\\\\text{MASK}] & \\\\text{with prob. 0.20 for DC easy words} \\\\\\\\\\n[\\\\text{MASK}] & \\\\text{with prob. 0.25 otherwise}\\n\\\\end{cases}\\n\\\\]\\n\\nThe model is then trained to minimize the loss:\\n\\n\\\\[\\nL_{MLM} = -\\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\log p(x_i|T_M; \\\\theta)\\n\\\\]\\n\\nwhere \\\\( \\\\theta \\\\) is the parameters of the model. We utilized the pre-trained checkpoint of the RoBERTa.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We evaluate our KidLM models based on the following two criteria:\\n\\n(1) How well does KidLM understand lower grade-level texts (\u00a73.1)?\\n\\n(2) How robust is KidLM in maintaining safety standards by avoiding the generation of stereotypes (\u00a73.2)?\\n\\nWe compared our model with base LMs to ensure a fair and consistent comparison, highlighting the impact of our high-quality pre-training data.\\n\\n3.1 Evaluating on Grade-Level Texts\\n\\nOur objective is to compare various language models against our KidLM models. We employ Perplexity (PPL) as an evaluation metric, which measures the uncertainty of a language model when predicting the next word in a sequence (Radford et al., 2019; Salazar et al., 2020). A lower perplexity score indicates that the model is more confident and accurate in its predictions, suggesting a better understanding of the language and context (Bengio et al., 2000). To assess this, we use texts across different lower grade-levels, allowing us to measure how well each model handles the linguistic, syntactic, and semantic simplicity of texts. The holdout Newsela Corpus (Xu et al., 2015) is used for this purpose. We randomly selected 40 documents for each of the lower grade-levels, such as 2nd, 3rd, and 4th grades, and segmented these documents into sentences to compute sentence-level perplexity scores (for holdout test data statistics, refer to Table 2).\\n\\nResults & Analysis\\n\\nAs shown in Table 3, general-purpose LLMs demonstrate decreasing perplexity as grade levels increase, indicating less uncertainty in predicting relatively more complex texts. At the 2nd grade level, perplexity values are highest across all these LLMs, highlighting the difficulty in comprehending simpler texts. The Llama family models show that more training data doesn't always improve performance with simpler texts. For example, Llama 2, trained on 2 trillion tokens, and Llama 3, trained on 15 trillion tokens, illustrate this point, suggesting a need for more user-centered training data. In contrast, our models, KidLM and KidLM+, show a reversing trend with generally less uncertainty in predicting lower grade levels and consistently less uncertainty across all grade levels, demonstrating their effectiveness in understanding simpler language. Further, we present a qualitative analysis of our model outputs in generating simpler words within a given context (\u00a74).\\n\\n3.2 Evaluating Stereotype\\n\\nOur evaluation involves querying a range of pre-trained LMs, LLMs, and debiased LMs, using a structured prompt: \\\"why are {group} so ...\\\". While Baker and Potts (2013) utilized prompts such as 'why do {group}', 'how do {group}', and 'what do {group}' to effectively elicit stereotypes, our prompt is specifically designed to extract reasons associated with a group's characteristics, focusing on adjectives. The placeholder \\\"{group}\\\" is replaced with terms from a broad range of social groups. For selecting these groups, we referred to lists from (Choenni et al., 2021) and the StereoSet (Nadeem et al., 2021), which are commonly used in assessing stereotypes in LMs. Following (Leidinger and Rogers, 2023), we merged and reorganized these sources to create a comprehensive list of social groups used in our evaluations.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our evaluation encompasses a wider range of social groups for thoroughly analyzing stereotypes in LMs. Masked LMs (MLMs) and Debiased LMs For MLMs, such as RoBERTa, KidLM (ours), and KidLM+ (ours), we prompt the models with \\\"why are {group} so [MASK]\\\" to predict completions for the [MASK] position. We also compare our models with two debiased models where debiasing occurs during the pre-training stage, (1) Auto Debias (Guo et al., 2022), and (2) Context Debias (Kaneko and Bollegala, 2021). Since these models are debiased MLMs, we employed the same prompt settings designed for MLMs.\\n\\nCausal Language Models We compared our models with several open-sourced causal language models such as GPT-2 (base and large) (Radford et al., 2019), Llama 2 (7B and 13B) (Touvron et al., 2023), Llama 3, and Mistral 7B (Jiang et al., 2023). We employed the prompt \\\"why are {group} so\\\" as a context to generate words that reflect stereotypical reasoning or biases. This method offers insights into potential biases embedded within their training data and predictive algorithms. We configured the generator to produce a maximum of one token by setting the parameter max_new_tokens = 1. We filtered out the completions that were duplicates, non-words, less than three characters, grammatically incorrect, or non-adjectives (e.g., \\\"so often\\\", \\\"so sure\\\", \\\"so far\\\", \\\"so much\\\", \\\"so into\\\", \\\"so so\\\", etc). We opted not to compare our models with closed-source models, as detailed in Appendix C.\\n\\nEvaluation Method To thoroughly analyze the stereotypical biases present in LMs, we sampled multiple completions (n=5) for each LM for each group, ranking them according to the probability of completions. Our analysis includes a total of 151 groups, divided into 8 categories. Social bias or stereotypes in language models manifest as prejudiced outputs that associate target concepts to demographic groups (Gallegos et al., 2023). To evaluate these stereotypes, we analyze sentiment and toxicity scores of model completions, a common method in assessing stereotypes in language generation (Blodgett et al., 2020; Nadeem et al., 2021; Aky\u00fcrek et al., 2022; Deshpande et al., 2023; Liang et al., 2023). Toxicity refers to offensive, harmful, or discriminatory language (Kiritchenko et al., 2021), while sentiment reflects human perceptions, attitudes, and emotions (Ekman and Davidson, 1994). Notably, content from humans may display more pronounced stereotyping, as observed through negative sentiments or increased toxicity (Liu, 2024).\"}"}
{"id": "emnlp-2024-main-277", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Models\\n\\nOutputs / Labels\\n\\nHuman [killing, fighting, butchery]\\nKidLM [refugees, celebrations, rebels]\\n\\n\u201c...hasn\u2019t stopped the bloodshed.\u201d KidLM+ [villagers, goats, fun]\\n\\nHuman [decays, breaks down, dissolves]\\nKidLM [converts, turns, changes]\\n\\n\u201cIt decomposes to arsenic trioxide, elemental arsenic and iodine when heated at 200\u00b0C.\u201d KidLM+ [turns, converts, changes]\\n\\nHuman [bosses, leaders, instigators]\\nKidLM [prisoners, women, suspects]\\n\\n\u201c...ringleaders have been captured and sent to other facilities.\u201d KidLM+ [tigers, dogs, mice]\\n\\nTable 5: Lexical simplification probing comparison with our KidLM models to human labels.\\n\\nSentiment & Toxicity Scores\\n\\nTo quantify sentiment, we utilized SiEBERT (Hartmann et al., 2023), a language model fine-tuned for sentiment classification, chosen for its extensive training across diverse English datasets, including tweets and reviews. For toxicity assessment, we utilize the Toxicity Scorer (Leong et al., 2023), a fine-tuned DeBERTa-v3-large model (He et al., 2023) that offers superior estimation accuracy and higher throughput compared to the Perspective API.\\n\\nBoth sentiment and toxicity are measured on a scale from 0 to 100, with higher scores reflecting more positive sentiment and reduced toxicity, allowing a more fine-grained analysis.\\n\\nResults\\n\\nTable 4 presents average sentiment and toxicity scores for various models, including PLMs, LLMs, and debiased models. KidLM, fine-tuned on our corpus with standard (random) masking, outperforms typical PLMs in sentiment scores and shows a reduced tendency for reinforcing negative stereotypes. Its performance in toxicity scores indicates an ability to minimize toxic completions, even with less positive sentiments. KidLM+ excels in both sentiment and toxicity, benefiting from our Stratified Masking technique. Mistral 7B, with its emphasis on high-quality pre-training data (Jiang et al., 2023), emerges as a close contender in sentiment, underscoring the significance of data quality.\\n\\nSample outputs in Table 13 of the Appendix.\\n\\nAnalysis\\n\\nIn this section, we provide a qualitative analysis of our model outputs in two key settings. First, we assess the preferred lexical simplification within context compared to human labels. Second, we design probe tests categorized into diverse types (Table 6) to analyze the models\u2019 ability to capture and reflect children\u2019s unique preferences, emotions, and wishes. These analyses aim to highlight the impact of our corpus and the effectiveness of our stratified masking procedure in generating contextually preferred responses for children.\\n\\nTo structure the analysis, we employ the \u201ccloze test\u201d (Taylor, 1953) to design queries, where certain words in a query are masked, and the model\u2019s task is to predict or fill in these blanks. Formally, let $Q = \\\\{q_1, q_2, ..., q_k\\\\}$ represent a set of probe queries, where each query $q_i$ is a sentence with one or more masked positions. Each query can be represented as:\\n\\n$q_i = \\\\{w_1, w_2, ..., [\\\\text{MASK}], ..., w_N\\\\}$ (2)\\n\\nwhere $w_j$ is a word or a token in the query, [MASK] represents the masked position(s), and $N$ is the total number of words in the sentence. A LM, $M$, is employed to predict plausible words for each masked position. For each masked position in query $q_i$, the model outputs a probability distribution over a predefined vocabulary $V$. This probability distribution is denoted by $P(v|q_i; M)$, representing the probability of a vocabulary word $v \\\\in V$ being a plausible completion at the masked position in $q_i$. The objective is to identify the top $K$ most likely words from $V$, this set of words is represented as $\\\\text{Top}_K(q_i)$ and is defined as:\\n\\n$\\\\text{Top}_K(q_i) = \\\\arg\\\\max_{v \\\\in V} P(v|q_i; M)$ (3)\\n\\nLexical Simplification involves replacing a word in context with a simpler alternatives (Paetzold and Specia, 2016). To analyze the ability of our KidLM models to generate simpler words within a given context, we utilized the TSAR-EN dataset (\u0160tajner et al., 2022), annotated by MTurk annotators who are required to be at least 18 years old. For each sentence, we selected the annotated complex word (highlighted in bold in Table 5), replaced it with [MASK], and then probe LMs to generate words for the masked position and rank them according to their output probability. Table 5 compares the sample outputs generated by our models to human labels. While human annotators, influenced by their age (over 18), tend to list simpler synonyms of the known complex word, our KidLM+ model excels in generating simpler, preferred, and stereotype-free completions. This behavior can be attributed to our proposed stratified masking procedure. More...\"}"}
{"id": "emnlp-2024-main-277", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Preference Probing\\n\\nTable 6: Output completions grouped by types, providing qualitative insights into model behaviors. Detailed comparisons and additional sample outputs can be found in the Appendix (Table 12).\\n\\nPreference Probing involves creating a set of probe queries and using language models to predict preferences for these queries (Appendix [Table 7]). By generating completions with associated probabilities, we examine the model\u2019s confidence in each preferred completion. We compare the outputs of our models with those of RoBERTa, which was initially trained with BooksCorpus (Zhu et al., 2015) and English Wikipedia and then we use this model to continue pre-train with our KidLM corpus to develop KidLM models.\\n\\nIn Table 6, we present sample outputs comparing KidLM and KidLM+ models against RoBERTa through diverse probe tests. Under Preferences, KidLM and KidLM+ demonstrated a strong ability to generate child-friendly completions. KidLM+ suggested \u2018chicken\u2019, \u2018spaghetti\u2019, and \u2018noodles\u2019 with high confidence, reflecting common preferences among children. This contrasted with RoBERTa, which suggested more adult-oriented foods like \u2018pizza\u2019, \u2018sushi\u2019, and \u2018seafood\u2019. For Emotions and Feelings, KidLM models showed a nuanced understanding of common childhood fears. KidLM+ generated \u2018spiders\u2019 and \u2018everything\u2019 with high probabilities, aligning closely with typical childhood fears, while RoBERTa generated less specific completions like \u2018death\u2019 and \u2018him\u2019.\\n\\nIn the Wishes and Desires category, KidLM models accurately reflected typical children\u2019s wishes. KidLM+ offered \u2018chocolate\u2019 and \u2018cake\u2019 with high confidence, capturing common birthday desires among kids. In contrast, RoBERTa suggested more general or abstract terms. The higher confidence observed in the KidLM+ model can be attributed to our stratified masking approach (additional sample outputs can be found in Appendix (Table 14)).\\n\\nWe qualitatively analyze and interpret the model\u2019s preferred completions, but a critical question remains: how can we evaluate this with actual human feedback? In next section, we discuss future directions involving human-centered evaluations.\\n\\n5 Discussion and Future Directions\\n\\nPre-training Data\\n\\nDecoder-only LLMs operate on a causal language modeling objective, learning to predict the next token based on the sequence of previous tokens (Touvron et al., 2023; Penedo et al., 2023). Consequently, they may require significantly more pre-training data compared to our current KidLM corpus. On a positive note, our user-centric data collection pipeline is not only comprehensive but also extensible, allowing continuous integration of new sources to expand our corpus. Additionally, quality filtering and controlled repetition of available data, as shown in recent studies (Muenighoff et al., 2023), can significantly enhance the performance of LLMs in data-constrained settings.\\n\\nAlignment to Children\\n\\nBase LLMs pre-trained with unsupervised text corpora are typically inadequate as open-domain conversational assistants. Fine-tuning is essential, but using existing SFT data can compromise the kid-specific properties developed during pre-training stage (Table 1). Furthermore, MTurk is unsuitable for collecting such data due to age demographic restrictions. Recent studies demonstrate that a small set of examples (e.g., 1,000) can achieve significant alignment performance (Zhou et al., 2023). Another study highlights that base LLMs and their alignment-tuned versions perform nearly identically (Lin et al., 2024), with base LLMs achieving effective conversational alignment purely through in-context learning.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing (ICL). These studies support our hypothesis that high-quality, user-centered pre-training data is essential for developing kid-specific LMs.\\n\\nHuman-Centered Evaluation\\n\\nCurrent LLM evaluation methods focus on developing datasets and benchmarks (Liang et al., 2023; Chang et al., 2024) but often fail to address the 'sociotechnical gap' (Weidinger et al., 2023). Assessing models in isolated 'lab settings' limits the incorporation of human factors (Ibrahim et al., 2024). Human-Computer Interaction (HCI) offers diverse metrics to meet the evaluation needs of different stakeholders (Damacharla et al., 2018). Interdisciplinary research between HCI and NLP is essential for responsible, human-centered evaluation and auditing of LLMs (Xiao et al., 2024). As a potential research direction, we suggest an evaluation framework that integrates insights from both fields. This process may involve various stakeholders at different stages: (1) Pre-deployment (e.g., educators, psychologists, parents), and (2) Post-deployment (e.g., children, parents, educators).\\n\\nRelated Work\\n\\nChildren and Language Technology\\n\\nPrior studies from the HCI community have explored how technology can support children in learning and sharing their emotions (Santos et al., 2020; J. Ryu et al., 2021), as well as enhancing parents' awareness of their children's emotional well-being (Peppling et al., 2020). These studies demonstrated that chatbots and tangible artifacts can accurately detect children's emotions and promote emotional regulation. However, they often overlook children's perceptions and preferences regarding emotional communication (Seo et al., 2024b) and are limited by the technical constraints of rule-based chatbots (Seo et al., 2024a). LLMs have simplified the development of educational tools and applications (Huber et al., 2024). Research suggests these models can enhance children's learning through engaging, emotionally responsive interactions (Seo et al., 2024b) and support visual programming (Chen et al., 2024). However, significant risks include bias and toxicity from unvetted datasets (Deshpande et al., 2023), insufficient contextual appropriateness (Seo et al., 2024a,b), and difficulty in maintaining lexical simplicity suitable for young users (Valentini et al., 2023). These challenges highlight the need for child-specific LMs with built-in safety, contextual relevance, and simplicity.\\n\\nMasking Strategies & Rates\\n\\nEntityBERT (Lin et al., 2021) employs a masking strategy that targets \\\"entities\\\" identified by a domain-specific pre-trained named entity recognizer (NER) model. Similarly, Salient Span Masking (Guu et al., 2020) uses an NER model to mask entities for open-domain QA tasks. Both methods rely on a domain-specific NER, and their masking strategy is consistent across any applied domain. In contrast, Selective Masking (Gu et al., 2020) tailors token masking during continued pre-training based on data and labels from the downstream task. Meanwhile, Difference Masking (Wilf et al., 2023) automatically selects tokens for masking by identifying unique anchor words in the target domain data, distinguished from the general domain using a TF-IDF-like scoring function. Wettig et al. (2023) found that a 15% masking rate is not universally optimal for MLMs, suggesting that larger models should adopt a higher rate when pre-training from scratch. Moreover, Yang et al. (2023) introduced time-variant masking, adjusting the masking rate at different training stages to enhance pre-training efficiency. Our method, on the other hand, groups words into classes or strata, with our novel Stratified Masking adjusting masking probabilities based on the strata to which they belong. This enhances the model's focus on tokens that are more informative and specifically tailored to children, facilitating the smoother integration of kid-specific properties into the language model. Unlike other methods, our approach does not depend on any external models, task-specific signals, custom vocabulary, or a fixed masking rate for all tokens. The works related to domain adaptation of LMs are in Appendix D.\\n\\nConclusion\\n\\nIn this paper, we take the important first steps toward designing child-specific language models to make NLP systems more accessible to children. We curated a high-quality pre-training corpus using our proposed user-centric data collection pipeline and introduced novel Stratified Masking to enhance the model's focus on tokens that are more informative and specifically tailored to children. Experimental evaluations demonstrate that our model effectively understands lower grade-level text, maintains safety standards by avoiding the generation of stereotypes, and captures children's unique preferences. Furthermore, based on our insights, we offer suggestions for future research and development.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nResource Constraints\\n\\nRecognizing the importance of this vulnerable population, we took a step back to carefully consider their unique needs and began our work from the ground up, starting with the data. Given the size of our pre-training data, we opted to train an MLM to validate the corpus quality and ensure the integration of kid-specific properties into the language model. Additionally, developing KidLM in resource-constrained academic settings prompted us to propose Stratified Masking, a novel training objective for data-efficient, user-centric language modeling. Our approach aligns with recent research that emphasizes the importance of curating pre-training data to derive meaningful insights for future developments and to optimize models in resource-constrained settings (Lucy et al., 2024). Our insights and observations pave the way for future research and development. We hope that our efforts will inspire the community to advance this work, guided by our future directions.\\n\\nDiscussions on Stratified Masking\\n\\nWe assigned masking rates of 0.15 to stopwords, 0.20 to Dale-Chall easy words, and 0.25 to other words, focusing on more informative and kid-specific vocabulary. This approach led to a masking ratio of stopwords:Dale-Chall words:other words = 0.15:0.20:0.25, increasing in increments of 0.05. We recognize that alternative ratios, such as 0.15:0.25:0.35 with increments of 0.10, are also feasible. However, due to limited computational resources and the extensive training required, we were unable to experiment with finding the optimal masking ratios.\\n\\nOther Harm Categories\\n\\nAlthough our model demonstrates a reduced likelihood of reinforcing negative stereotypes and generating toxic completions across 151 social groups in 8 categories, we were unable to explore other harm categories such as hate speech, sexual content, and violent crimes from the MLCommons taxonomy of hazards. We encourage future work to investigate these additional harm categories to provide a more comprehensive assessment of language model safety.\\n\\nGrade Level and Content Criteria\\n\\nOur primary goal was to collect textual content specifically written for children or by children. By \\\"children,\\\" we refer to general children's text with linguistic, syntactic, and semantic simplicity. Depending on the availability of grade level information, we aim to limit the documents to the 6th grade, which corresponds to the age of 12 in the elementary school division. However, we cannot guarantee that all content meets our criteria when such information is not directly available. These criteria are explained in Appendix Tables [15, 16, 17] (Additional Notes column).\\n\\nLanguage Specificity\\n\\nOur research and the development of KidLM are exclusively centered on the English language. This means its use and effectiveness might not be the same for other languages.\\n\\nEthics Statement\\n\\nData Crawling\\n\\nWe took ethical consideration into account when scraping data from the sources listed in Tables [15, 16, 17]. The data we have collected is intended exclusively for non-commercial research purposes. We conducted our web scraping activities at a reasonable rate, with no intention of causing a Distributed Denial of Service (DDoS) attack. Additionally, we read the instructions listed in robots.txt of each website to ensure we were able to crawl the desired content as per the Robots Exclusion Protocol (REP) standards.\\n\\nMitigating Risks in Content and Model Use\\n\\nWe made significant efforts to minimize offensive content in the pre-training data by deliberately crawling sites where such content is minimal. Furthermore, following a manual review of the auto-completion stereotype task's outputs, it seems unlikely that the KidLM+ model produces illicit content when given appropriate context. Nevertheless, we cannot provide an absolute guarantee that no such content is present. Therefore, we strongly recommend exercising caution when using the KidLM and KidLM+ models.\\n\\nCarbon Footprint\\n\\nTo minimize environmental impact, we limited our continual training to the RoBERTa base model using our corpus, thus reducing the carbon footprint associated with training larger models. Both the KidLM and KidLM+ models were trained on a single RTX 3090 GPU for a total of 168 hours, resulting in an estimated carbon emission of only 25.4kg.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We thank all the anonymous reviewers and the meta-reviewer for their valuable feedback and constructive suggestions for improving this work. This research is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC). Additionally, Mir Tafseer Nayeem is supported by a Huawei PhD Fellowship.\\n\\nReferences\\n\\n01. AI, : Alex Young, Bei Chen, Chao Li, Chen-gen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, Kaidong Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.ai. Preprint, arXiv:2403.04652.\\n\\nAfra Feyza Aky\u00fcrek, Muhammed Yusuf Kocyigit, Sejin Paik, and Derry Tanti Wijaya. 2022. Challenges in measuring bias via open-ended language generation. In Proceedings of the 4th Workshop on Gender Bias in Natural Language Processing (GeBNLP), pages 76\u201376, Seattle, Washington. Association for Computational Linguistics.\\n\\nZhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen Marcus McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, and Sean Welleck. 2024. Llemma: An open language model for mathematics. In The Twelfth International Conference on Learning Representations.\\n\\nPaul Baker and Amanda Potts. 2013. 'why do white people have thin lips?' google and the perpetuation of stereotypes via auto-complete search forms. Critical Discourse Studies, 10(2):187\u2013204.\\n\\nYoshua Bengio, R\u00e9jean Ducharme, and Pascal Vincent. 2000. A neural probabilistic language model. In Advances in Neural Information Processing Systems, volume 13. MIT Press.\\n\\nSteven Bird. 2006. NLTK: The Natural Language Toolkit. In Proceedings of the COLING/ACL 2006 Interactive Presentation Sessions, pages 69\u201372, Sydney, Australia. Association for Computational Linguistics.\\n\\nSu Lin Blodgett, Solon Barocas, Hal Daum\u00e9 III, and Hanna Wallach. 2020. Language (technology) is power: A critical survey of \\\"bias\\\" in NLP. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5454\u20135476, Online. Association for Computational Linguistics.\\n\\nElliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, and Christopher D. Manning. 2024. Biomedlm: A 2.7b parameter language model trained on biomedical text. Preprint, arXiv:2403.18421.\\n\\nElena Bozzola, Giulia Spina, Rino Agostiniani, Sarah Barni, Rocco Russo, Elena Scarpato, Antonio Di Mauro, Antonella Vita Di Stefano, Cinthia Caruso, Giovanni Corsello, and Annamaria Staiano. 2022. The use of social media in children and adolescents: Scoping review on the potential risks. International Journal of Environmental Research and Public Health, 19(16).\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nJeanne Sternlicht Chall and Edgar Dale. 1995. Readability Revisited: The New Dale-Chall Readability Formula. Brookline Books.\\n\\nYupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024. A survey on evaluation of large language models. ACM Trans. Intell. Syst. Technol., 15(3).\\n\\nLiuqing Chen, Shuhong Xiao, Yunnong Chen, Yaxuan Song, Ruoyu Wu, and Lingyun Sun. 2024. Chatscratch: An ai-augmented system toward autonomous visual programming learning for children aged 6-12. In Proceedings of the CHI Conference on Human Factors in Computing Systems, CHI '24, New York, NY, USA. Association for Computing Machinery.\\n\\nRochelle Choenni, Ekaterina Shutova, and Robert van Rooij. 2021. Stepmothers are mean and academics are pretentious: What do pretrained language models learn about you? In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1477\u20131491, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nPraveen Damacharla, Ahmad Y. Javaid, Jennie J. Gallimore, and Vijay K. Devabhaktuni. 2018. Common metrics to benchmark human-machine teams (hmt): A review. IEEE Access, 6:38637\u201338655.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-277", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-277", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-277", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodr\u00edguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.\\n\\nMaria Valentini, Jennifer Weber, Jesus Salcido, T\u00e9a Wright, Eliana Colunga, and Katharina von der Wense. 2023. On the automatic generation and simplification of children\u2019s stories. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3588\u20133598, Singapore. Association for Computational Linguistics.\\n\\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V Le. 2022. Finetuned language models are zero-shot learners. In International Conference on Learning Representations.\\n\\nLaura Weidinger, Maribeth Rauh, Nahema Marchal, Arianna Manzini, Lisa Anne Hendricks, Juan Mateos-Garcia, Stevie Bergman, Jackie Kay, Conor Griffin, Ben Bariach, Iason Gabriel, Verena Rieser, and William Isaac. 2023. Sociotechnical safety evaluation of generative AI systems. Preprint, arXiv:2310.11986.\\n\\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and Danqi Chen. 2023. Should you mask 15% in masked language modeling? In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 2985\u20133000, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nAlex Wilf, Syeda Akter, Leena Mathur, Paul Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, and Louis-Philippe Morency. 2023. Difference-masking: Choosing what to mask in continued pretraining. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 13222\u201313234, Singapore. Association for Computational Linguistics.\\n\\nZiang Xiao, Wesley Hanwen Deng, Michelle S. Lam, Motahhare Eslami, Juho Kim, Mina Lee, and Q. Vera Liao. 2024. Human-centered evaluation and auditing of language models. In Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems, CHI EA \u201924, New York, NY, USA. Association for Computing Machinery.\\n\\nWei Xu, Chris Callison-Burch, and Courtney Napoles. 2015. Problems in current text simplification research: New data can help. Transactions of the Association for Computational Linguistics, 3:283\u2013297.\\n\\nDongjie Yang, Zhuosheng Zhang, and Hai Zhao. 2023. Learning better masking for better language model pre-training. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7255\u20137267, Toronto, Canada. Association for Computational Linguistics.\\n\\nY. Yang, Mark Christopher Siy UY, and Allen Huang. 2020. Finbert: A pretrained language model for financial communications. Preprint, arXiv:2006.08097.\\n\\nChunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, LILI YU, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. Lima: Less is more for alignment. In Advances in Neural Information Processing Systems, volume 36, pages 55006\u201355021. Curran Associates, Inc.\\n\\nY. Zhu, R. Kiros, R. Zemel, R. Urtasun, A. Torralba, and S. Fidler. 2015. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In 2015 IEEE International Conference on Computer Vision (ICCV), pages 19\u201327, Los Alamitos, CA, USA. IEEE Computer Society.\\n\\nSanja \u0160tajner, Daniel Ferr\u00e9s, Matthew Shardlow, Kai North, Marcos Zampieri, and Horacio Saggion. 2022. Lexical simplification benchmarks for English, Portuguese, and Spanish. Frontiers in Artificial Intelligence, 5.\"}"}
{"id": "emnlp-2024-main-277", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Supplementary Material: Appendices\\n\\nA Data Preprocessing\\n\\nWe removed URLs and HTML markups, including only textual content while excluding lists, tables, and headers, as well as sentences containing code-switching (TAY, 1989). In linguistics, code-switching (a.k.a., language alternation) occurs when a speaker alternates between two or more languages (or language varieties) from one sentence to another. Code-Switching is intersentential and inspired by social and psychological motivations. We only took the sentences written in English and considered any other language as code-switching. We used the spacy-langdetect module to identify languages. While doing this, we noticed the presence of words from multiple languages within a single sentence, a phenomenon widely known as code-mixing (Mabule, 2015), when the speaker mixes various linguistic units from different languages in a single utterance or sentence. To address this problem, we used the confidence scores from the language detection model and only kept sentences with scores greater than or equal to 0.9.\\n\\nProtection of Privacy\\n\\nWe deliberately chose not to collect specific information, such as author names (whether they are children or reporters) and the publication dates of articles. Additionally, we preprocess the data to remove any personal contact details, including email addresses, phone numbers, and Twitter handles, by applying simple regular expressions to the pre-training corpus, following (Nayeem and Rafiei, 2023). As a result, our dataset minimizes the presence of Personal Identifying Information (PII). This decision highlights our commitment to prioritizing user privacy.\\n\\nB Training & Hyperparameters\\n\\nWe trained our model on a single RTX 3090 GPU with 24GB of memory. The AdamW optimizer (Loshchilov and Hutter, 2019) was employed with a learning rate of $5 \\\\times 10^{-5}$. We utilized the pre-trained checkpoint of the RoBERTa (Liu et al., 2019) base model and its pre-trained tokenizer, avoiding the use of any custom vocabulary. To facilitate larger batch sizes, we implemented gradient accumulation.\\n\\nDetailed hyperparameter settings are presented in Table 8.\\n\\n| Hyperparameter | KidLM | KidLM+ |\\n|---------------|-------|--------|\\n| Learning Rate | $5 \\\\times 10^{-5}$ | $5 \\\\times 10^{-5}$ |\\n| Batch Size    | 64    | 64     |\\n| Sequence Length | 128   | 128    |\\n| Learning Rate Schedule | Cosine | Cosine |\\n| Optimizer     | AdamW | AdamW  |\\n| Warmup Proportion | 10%   | 10%    |\\n| Training Epochs | 200   | 200    |\\n| Architecture  | RoBERTa (base) | RoBERTa (base) |\\n\\nTable 7: Our probe query templates designed for qualitatively measure preference autocompletion categorized into diverse groups such as Preferences, Emotions and Feelings, and Wishes and Desires.\\n\\nTable 8: Our KidLM models hyperparameter settings.\\n\\nC Closed-Source Models\\n\\nWe chose not to compare our models with closed-source models accessed through APIs, such as Claude-2, ChatGPT (gpt-3.5-turbo-0613), and GPT-4 (OpenAI, 2023a). These APIs likely incorporate complex engineering solutions, potentially involving multiple models chained together, making them fundamentally different and not directly comparable to standalone models. For instance, OpenAI has implemented a content moderation filter for their language models, which evaluates the outputs based on criteria such as hate, self-harm, sexual content, and violence (Markov et al., 2023; OpenAI, 2023b). To draw an analogy, while a model is akin to an engine, an API is more comparable to a car. Therefore, our comparison...\"}"}
