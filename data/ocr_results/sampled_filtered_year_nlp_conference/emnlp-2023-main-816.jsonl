{"id": "emnlp-2023-main-816", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nTable of contents (ToC) extraction centres on structuring documents in a hierarchical manner. In this paper, we propose a new dataset, ESGDoc, comprising 1,093 ESG annual reports from 563 companies spanning from 2001 to 2022. These reports pose significant challenges due to their diverse structures and extensive length. To address these challenges, we propose a new framework for ToC extraction, consisting of three steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). This construction-modelling-modification (CMM) process offers several benefits. It eliminates the need for pairwise modelling of section headings as in previous approaches, making document segmentation practically feasible. By incorporating structured information, each section heading can leverage both local and long-distance context relevant to itself. Experimental results show that our approach outperforms the previous state-of-the-art baseline with a fraction of running time. Our framework proves its scalability by effectively handling documents of any length.\\n\\n1 Introduction\\nA considerable amount of research has been proposed to comprehend documents (Xu et al., 2019; Zhang et al., 2021; Xu et al., 2021a,b; Peng et al., 2022; Li et al., 2022; Gu et al., 2022; Shen et al., 2022; Lee et al., 2022, 2023), which typically involves the classification of different parts of a document such as title, caption, table, footer, and so on. However, such prevailing classification often centres on a document\u2019s local layout structure, sidelining a holistic comprehension of its content and organisation. While traditional summarisation offers a concise representation of a document\u2019s content, a Table of Contents (ToC) presents a structured and hierarchical summary. This structural organisation in a ToC provides a comprehensive pathway for pinpointing specific information. For example, when seeking information about a company\u2019s carbon dioxide emissions, a ToC enables a systematic navigation through the information hierarchy. In contrast, conventional summarisation might only provide a vague indication of such information, requiring sifting through the entire document for precise detail.\\n\\nSeveral datasets have been proposed to facilitate the research in document understanding (Zhong et al., 2019b; Li et al., 2020; Pfitzmann et al., 2022). Most of these studies lack a structured construction of documents and primarily focus on well-structured scientific papers. A dataset called HierDoc (Hierarchical academic Document) (Hu et al., 2022) was introduced to facilitate the development of methods for extracting the table of contents (ToC) from documents. This dataset was compiled from scientific papers downloaded from arXiv, which are typically short and well-structured. The hierarchical structure can often be inferred directly from the headings themselves. For example, the heading \\\"1. Introduction\\\" can be easily identified as a first-level heading based on the section numbering. Moreover, due to the relatively short length of scientific papers, it is feasible to process the entire document as a whole. Hu et al. (2022) proposed the multimodal tree decoder (MTD) for ToC extraction from HierDoc. MTD first utilises text, visual, and layout information to encode text blocks identified by a PDF parser; then classifies all text blocks into two categories, headings and non-headings; and finally predicts the relationship of each pair of headings, facilitating the parsing of these headings into a tree structure representing ToC.\\n\\n2https://arxiv.org/\"}"}
{"id": "emnlp-2023-main-816", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Five examples of ESG reports, with the left three presented in portrait orientation and the right-most two in landscape orientation. They show a wide range of diverse structures. It is common to observe the absence of section numbering.\\n\\nHowever, understanding long documents such as ESG (Environmental, Social, and Governance) annual reports poses significant challenges compared to commonly used scientific papers. First, ESG reports tend to be extensive, often exceeding 100 pages, which is uncommon for scientific papers. Second, while scientific papers generally adhere to a standard structure that includes abstract, introduction, methods, results, discussion, and conclusion sections, ESG reports exhibit more diverse structures with a wide range of font types and sizes. Third, ESG reports often include visual elements such as charts, graphs, tables, and infographics to present data and key findings in a visually appealing manner, which adds complexity to the document parsing process. Some example ESG reports are illustrated in Figure 1.\\n\\nIn this paper, we develop a new dataset, ESGDoc, collected from public ESG annual reports from 563 companies spanning from 2001 to 2022 for the task of ToC extraction. The existing approach, MTD (Hu et al., 2022), faces difficulties when dealing with challenges presented in ESGDoc. MTD models relationships of every possible heading pairs and thus requires the processing of the entire document simultaneously, making it impractical for lengthy documents. As will be discussed in our experiments section, MTD run into out-of-memory issue when processing some lengthy documents in ESGDoc. Moreover, MTD only uses Gated Recurrent Unit (GRU) (Cho et al., 2014) to capture the context of a section heading, lacking long-distance interaction, particularly for high-level headings that may be tens of pages apart.\\n\\nIn order to overcome the challenges presented in ESGDoc, we propose a new scalable framework, consisting of three main steps: (1) Constructing an initial tree of text blocks based on reading order and font sizes; (2) Modelling each tree node (or text block) independently by considering its contextual information captured in node-centric subtree; (3) Modifying the original tree by taking appropriate action on each tree node (Keep, Delete, or Move). Our method is named as CMM (Construction-Modelling-Modification). This approach allows higher-level headings to focus on capturing high-level and long-distance information, while lower-level headings focus more on local information. Additionally, CMM also models each heading independently, removing the need for modelling pairwise relationships among headings and enabling more effective document segmentation. Here, we can divide documents based on the tree structure instead of relying on page divisions. This ensures that each segment maintains both local and long-distance relationships, preserving the long-distance connections that would be lost if division were based on page boundaries. As CMM does not require the processing of a document as a whole, it can be easily scaled to deal with lengthy documents. Experimental results show that our approach outperforms the previous state-of-the-art baseline with only a fraction of running time, verifying the scalability of our model as it is applicable to documents of any length. Our main contributions are summarised as follows:\\n\\n\u2022 We introduce a new dataset, ESGDoc, comprising 1,093 ESG annual reports specifically designed for table of contents extraction.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a novel framework that processes documents in a construction-modelling-modification manner, allowing for the decoupling of each heading, preserving both local and long-distance relationships, and incorporating structured information.\\n\\nWe present a novel graph-based method for document segmentation and modelling, enabling the retention of both local and long-distance information within each segment.\\n\\nRelated Work\\n\\nDatasets\\n\\nMany datasets have been proposed for document understanding. PubLayNet dataset (Zhong et al., 2019b) is a large-scale dataset collected from PubMed Central Open Access, which uses scientific papers in PDF and XML versions for automatic document layout identification and annotation. Article-regions dataset (Soto and Yoo, 2019) offers more consistent and tight-fitting annotations. DocBank dataset (Li et al., 2020) leverages the latex source files and font colour to automatically annotate a vast number of scientific papers from arXiv. DocLayNet dataset (Pfitzmann et al., 2022) extends the scope from scientific papers to other types of documents. However, these datasets primarily contain annotations of the type and bounding box of each text, such as title, caption, table, and figure, but lack structured information of documents.\\n\\nApproaches for Document Understanding\\n\\nIn terms of methods for document understanding, a common approach is the fusion of text, visual, and layout features (Xu et al., 2019; Zhang et al., 2021; Xu et al., 2021a,b; Peng et al., 2022; Li et al., 2022), where visual features represent images of texts and the document, and layout features comprise bounding box positions of texts. Some methods also introduced additional features. For instance, XY-LayoutLM (Gu et al., 2022) incorporates the reading order, VILA (Shen et al., 2022) utilises visual layout group, FormNet (Lee et al., 2022, 2023) employs graph learning and contrastive learning. The aforementioned methods focus on classifying individual parts of the document rather than understanding the structure of the entire document.\\n\\nTable of Contents (ToC) Extraction\\n\\nIn addition to document understanding, some work has been conducted on the extraction of ToC. Early methods primarily relied on manually designed rules to extract the structure of documents (Namboodiri and Jain, 2007; Doucet et al., 2011). Tuarob et al. (2015) designed some features and use Random Forest (Breiman, 2001) and Support Vector Machine (Bishop and Nasrabadi, 2006) to predict section headings. Mysore Gopinath et al. (2018) propose a system for section titles separation. MTD (Hu et al., 2022) represents a more recent approach, fusing text, visual, and layout information to detect section headings from scientific papers in the HierDoc (Hu et al., 2022) dataset. It also uses GRU (Cho et al., 2014) and attention mechanism to classify the relationships between headings, generating the tree of ToC. While MTD performs well on HierDoc, it requires modelling all headings in the entire document simultaneously, which is impractical for long documents. To address this limitation, we propose a new framework that decouples the relationships of headings for ToC extraction and introduces more structural information by utilising font size and reading order, offering a more practical solution for long documents.\\n\\nDataset Construction\\n\\nTo tackle the more challenging task of ToC extraction from complex ESG reports, we construct a new dataset, ESGDoc, from ResponsibilityReports.com. Initially, we have downloaded 10,639 reports in the PDF format. However, only less than 2,000 reports have ToC in their original reports. To facilitate the development of an automated method for ToC extraction from ESG reports, we selectively retrain reports that already possess a ToC. The existing ToC serves as the reference label for each ESG report, while the report with the ToC removed is used for training our framework specifically designed for ToC extraction.\\n\\nOur final dataset comprises 1,093 publicly available ESG annual reports, sourced from 563 distinct companies, and spans the period from 2001 to 2022. The reports vary in length, ranging from 4 pages to 521 pages, with an average of 72 pages. In contrast, HierDoc (Hu et al., 2022) has a total of 650 scientific papers, which have an average of 19 pages in length. We randomly partitioned the dataset into a training set with 765 reports, a development set with 110 reports, and a test set with 218 reports.\\n\\nText content from ESG reports was extracted using PyMuPDF in a format referred to as \\\"block.\\\" A block, defined as a text object in the PDF standard.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"standard, which is usually a paragraph, can encompass multiple lines of text. We assume that the text within a text object is coherent and should be interpreted as a cohesive unit. Each block comprises the following elements: text content, font, size, colour, position, and id. The id is a unique identifier assigned to each block to distinguish blocks that contain identical text content. The position refers to the position of the block within a page in the ESG PDF report, represented by four coordinates that denote the top-left and bottom-right points of the block bounding box. Other elements, such as font, size, and colour, provide additional information about the text.\\n\\n4 Methodology\\n\\nWe propose a framework for ToC extraction based on the following assumptions:\\n\\nAssumption 1\\nHumans typically read documents in a left-to-right, top-to-bottom order, and a higher-level heading is read before its corresponding subheading and body text.\\n\\nAssumption 2\\nIn a table of contents, the font size of a higher-level heading is no smaller than that of a lower-level heading or body text.\\n\\nAssumption 3\\nIn a table of contents, headings of the same hierarchical level share the same font size.\\n\\nIn our task, a document is defined as a set of blocks. To replicate the reading order of humans, we reorder the blocks from the top-left to the bottom-right of the document. We employ the XY-cut algorithm (Ha et al., 1995) to sort the blocks. The sorted blocks are denoted as \\\\( \\\\{x_i\\\\}_{n_b=1}^{n_b} \\\\), where \\\\( x_{<i} \\\\) precedes \\\\( x_i \\\\) and \\\\( x_{>i} \\\\) follows \\\\( x_i \\\\). Here, \\\\( n_b \\\\) represents the total number of blocks. For each block \\\\( x_i \\\\), we define \\\\( s_i \\\\) as its size.\\n\\nProblem Setup\\n\\nGiven a list of blocks, ToC extraction aims to generate a tree structure representing the table of contents, where each node corresponds to a specific block \\\\( x \\\\). We introduce a pseudo root node \\\\( r \\\\) as the root of the ToC tree.\\n\\nWe propose to initially construct a full tree containing all the blocks, where the hierarchical relation between blocks is simply determined by their respective font sizes. Specifically, when two blocks, \\\\( x_i \\\\) and \\\\( x_j \\\\), are read in sequence, if they are close to each other and their font sizes \\\\( s_i > s_j \\\\), then \\\\( x_j \\\\) becomes a child of \\\\( x_i \\\\). We then modify the tree by removing or rearranging nodes as necessary. Essentially, for a node (i.e., a block) \\\\( x_i \\\\), we need to learn a function which determines the operation ('Keep', 'Delete', or 'Move') to be performed on the node. In order to enable document segmentation and capture the contextual information relating to the node, our approach involves extracting a subtree encompassing its neighbourhood including the parent, children and siblings, within a range of \\\\( n_d \\\\) hops. Subsequently, we use Graph Attention Networks (GATs) (Brody et al., 2021; Velickovic et al., 2017) to update the node information within the subtree. An overview of our proposed framework is illustrated in Figure 2.\\n\\nBefore delving into the detail of our proposed framework, we first define some notations relating to node operations.\\n\\n\\\\[ PA(x_i) \\\\] as the parent node of node \\\\( x \\\\), \\\\[ PR(x_i) \\\\] as the preceding sibling node of \\\\( x \\\\), \\\\[ SU(x_i) \\\\] as the subsequent sibling node of \\\\( x \\\\), \\\\[ PRS(x_i) \\\\] as all the preceding sibling nodes of \\\\( x \\\\).\\n\\n4.1 Tree Construction\\n\\nWe first construct a complete tree \\\\( T \\\\), consisting of all identified blocks using PyMuPDF, based on reading order and font sizes. For each node \\\\( x_i \\\\), we find a node in its previous nodes \\\\( x_j \\\\in \\\\{x_{<i}\\\\} \\\\) that is closest to \\\\( x_i \\\\) and \\\\( s_j > s_i \\\\). Then \\\\( x_i \\\\) becomes a child of \\\\( x_j \\\\). A detailed algorithm is in Appendix A.\\n\\nFollowing the principles outlined in Assumptions 1, 2 and 3, this approach assumes that the ToC is contained within the tree structure, as shown in the top-left portion of Figure 2. The subsequent steps of our model involve modifying this tree \\\\( T \\\\) to generate the ToC.\\n\\n4.2 Tree Modelling\\n\\nIn this section, for a given tree node, we aim to learn a function which takes the node representation as input and generates the appropriate operation for the node. In what follows, we first describe how we encode the contextual information of a node, and then present how to learn node representations.\\n\\nNode-Centric Subtree Extraction\\n\\nTo effectively encode the contextual information of a tree node and to avoid processing the whole document in one go, we propose to extract a node-centric subtree, \\\\( t_i \\\\), a tree consisting of neighbourhood nodes, including \\\\( PR(x_i) \\\\) and \\\\( SU(x_i) \\\\), of node \\\\( x_i \\\\), extracted via Breadth First Search (BFS) on \\\\( x_i \\\\) with a depth \\\\( n_d \\\\). Here, \\\\( n_d \\\\) is a hyper-parameter. The neighbourhood nodes consist of the parent node,\"}"}
{"id": "emnlp-2023-main-816", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of CMM. Initially, blocks across multiple pages in a document shown in top-left, are reordered into a sequence based on reading order (Top-right). A full tree consisting of all text blocks is constructed based on reading order and font size (Center-left). Subsequently, for each node, a node-centric subtree is extracted and modelled by a graph neural network (GNN). In the subtree shown in center-right, the first-level heading '1.' can access both long-distance relationships with other first-level headings '2.', '3.', '4.', and local relationships with heading '1.1', '1.2', and bodies. In contrast, in the subtree shown in bottom-right, the second-level heading '3.2.' concentrates more on local information but also has access to some global information. When modelling with GNN, each node is connected with its neighbourhood nodes, including parent, children and siblings. After the tree modelling phase, the node-level operations (Keep, Delete, and Move) are predicted. The original tree is then modified according to the node-level operations, resulting in the final Table of Contents (Bottom-left). The nodes numbered and coloured in this figure are for illustrative purposes; some body nodes are omitted for brevity. During inference, the model is unaware of whether a node is a heading or non-heading.\\n\\nNode Encoding\\n\\nBefore discussing how to update node representations in a subtree, we first encode each node (or block) $x_i$ into a vector representation. We employ a text encoder, which is a pre-trained language model, to encode the text content of $x_i$. We also utilise additional features from $x_i$ defined as $f_i$. These features include: (1) pdf page number; (2) font and font size; (3) colour as RGB; (4) the number of text lines and length; and (6) the position of the bounding box of the block, represented by the coordinates of the top-left and bottom-right points. The representation of $x_i$ is derived from text encoder and $f_i$ with a Multilayer Perceptron (MLP) as follows:\\n\\n$$b_i = MLP([TextEncoder(x_i), f_i])$$\\n\\nwhere $b_i$ denotes the hidden representation of block $x_i$ and $[,]$ denotes concatenation.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Node Representation Update in a Subtree\\n\\nWe transform each node-centric subtree \\\\( t_i \\\\) to a graph \\\\( G = (V, E) \\\\), where the nodes \\\\( V = \\\\{ x_j \\\\in t_i \\\\} \\\\), and the embedding of each node \\\\( x_j \\\\) is assigned as \\\\( v_j \\\\).\\n\\nFor each node \\\\( x_j \\\\), there are three types of edges, from its parent, from its children, and from its siblings. The edges between the parent/siblings and \\\\( x_i \\\\) may span across multiple pages, as headings can be widely separated. Such edges can provide long-distance relationship information. On the other hand, the edges from children to \\\\( x_i \\\\) provide localised information about the heading. Thus, \\\\( x_i \\\\) benefits from learning from both long-distance and local relationships.\\n\\nWe employ Graph Attention Networks (GAT) with \\\\( n \\\\) layers for graph learning, enabling each \\\\( x_i \\\\) to focus on other nodes that are more relevant to itself. GAT also uses edge embeddings. In our model, we define edge features \\\\( f_{j,i} \\\\) for edge \\\\( e_{j,i} \\\\) as follows: (1) the edge type (parent, children, or siblings); (2) size difference \\\\( s_j - s_i \\\\); (3) whether \\\\( x_j \\\\) and \\\\( x_i \\\\) have the same font or colour; (4) page difference; (5) position difference as the differences of the coordinates of the top-left and bottom-right points of the corresponding block bounding boxes.\\n\\nWith nodes \\\\( V = \\\\{ x_i \\\\in t_i \\\\} \\\\), node embeddings \\\\( \\\\{ v_i \\\\} \\\\), edge \\\\( E = \\\\{ e_{j,i} \\\\} \\\\), and edge embeddings \\\\( f_{j,i} \\\\), the graph learning is performed as follows:\\n\\n\\\\[\\n\\\\{ h_i \\\\}_|t_i| = GAT(V, E) (3)\\n\\\\]\\n\\nwhere \\\\( \\\\{ h_i \\\\}_|t_i| \\\\) are the hidden representations of nodes \\\\( \\\\{ x_i \\\\}_|t_i| \\\\) in the node-centric subtree \\\\( t_i \\\\). In practice, multiple node-centric subtrees can be merged and represented simultaneously in GPU to accelerate training and inference.\\n\\n4.3 Tree Modification\\n\\nIn this section, we discuss how the model predicts and executes modifications to the tree. We define three types of operations for each node:\\n\\n1. Delete: This node is predicted as not a heading and will be deleted from the tree.\\n2. Move: This node is predicted as a low-level heading that is a sibling of a high-level heading due to having the same font size in rare cases. The node \\\\( 3.2.1 \\\\) in Figure 2 is an example. This node will be relocated to be a child as its preceding sibling as non-heading nodes have already been deleted.\\n3. Keep: This node is predicted as a heading and does not require any operations.\\n\\nWe define three scores \\\\( o_{kp,i}, o_{de,i} \\\\) and \\\\( o_{mv,i} \\\\) to represent the likelihood that the node \\\\( x_i \\\\) should be kept, deleted or moved. These scores are computed as follows:\\n\\n\\\\[\\no_{kp,i} = W_{kp} h_i + b_{kp}\\n\\\\]\\n\\n\\\\[\\no_{de,i} = W_{de} h_i + b_{de}\\n\\\\]\\n\\n\\\\[\\no_{mv,i} = W_{mv} \\\\text{POOL}(\\\\text{PRS}(h_i)) + b_{mv}\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( \\\\text{POOL}(\\\\text{PRS}(h_i)) \\\\) denotes a max pooling layer on the representations of preceding siblings of \\\\( x_i \\\\); \\\\( .,. \\\\) denotes the concatenation; \\\\( W_{kp}, W_{de}, W_{mv}, b_{kp}, b_{de}, \\\\) and \\\\( b_{mv} \\\\) are learnable parameters.\\n\\nThe score of Keep and Delete is inferred from the node directly, as \\\\( h_i \\\\) has gathered neighbourhood information with both long-distance and local relationships. The score of Move is inferred from the node and its preceding siblings so that the node can compare itself with its preceding siblings to decide whether it is a sub-heading of preceding siblings.\\n\\nThe probabilities of the node operations are computed with the softmax function as follows:\\n\\n\\\\[\\np_.i = \\\\frac{e^{o_{kp,i}} + e^{o_{de,i}} + e^{o_{mv,i}}}{e^{o_{kp,i}} + e^{o_{de,i}} + e^{o_{mv,i}}}\\n\\\\]\\n\\n(5)\\n\\nwhere \\\\( .,. \\\\) could be \\\\( ., kp \\\\), \\\\( ., de \\\\) or \\\\( ., mv \\\\). The final operation for node \\\\( x_i \\\\) is determined as follows:\\n\\n\\\\[\\n\\\\hat{y}_i = \\\\arg\\\\max_p p_i\\n\\\\]\\n\\n(6)\\n\\nwhere each \\\\( \\\\hat{y}_i \\\\) could be Keep, Delete, or Move.\\n\\nFor each node-centric subtree \\\\( t_i \\\\), the model only predicts the operation \\\\( \\\\hat{y}_i \\\\) for node \\\\( x_i \\\\) and ignores other nodes. With all \\\\( \\\\{ \\\\hat{y}_i \\\\}_n = 1 \\\\) predicted for nodes \\\\( \\\\{ x_i \\\\}_n = 1 \\\\), the original tree \\\\( T \\\\) will be modified as shown in Algorithm 1, where node deletion is performed first, followed by node relocation. We assume that all non-heading nodes have already been deleted during the deletion step. Each node is then checked following the reading order whether it should be moved. Therefore, for a node to be moved, we can simply set its preceding sibling as its parent node.\\n\\nThe modified tree \\\\( T' \\\\) represents the final inference output of our method, which is a ToC.\\n\\n4.4 Inference and Training\\n\\nFor training the model, we define the ground truth label \\\\( y_i \\\\) of operation for each node \\\\( x_i \\\\). If a node \\\\( x_i \\\\) is not a heading, then its label is \\\\( y_i = \\\\text{Delete} \\\\). If a node \\\\( x_i \\\\) is a heading, and there is a higher-level\"}"}
{"id": "emnlp-2023-main-816", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Tree Modification\\n\\nInput: A tree \\\\( T \\\\), node operations \\\\( \\\\{ \\\\hat{y}_i \\\\}^n_{b_i=1} \\\\)\\n\\n\\\\( X[de] \\\\leftarrow \\\\{ x_i \\\\in T \\\\mid \\\\hat{y}_i = 'delete' \\\\} \\\\)\\n\\n\\\\( T' = \\\\{ x_i \\\\in T \\\\setminus X[de] \\\\} \\\\)\\n\\nReconstruct the tree \\\\( T' \\\\) with reading order and font size.\\n\\nforeach \\\\( x_i \\\\in T' \\\\) do\\n\\nif \\\\( \\\\hat{y}_i = 'Move' \\\\) then\\n\\n\\\\( PA(x_i) \\\\leftarrow PR(x_i) \\\\)/n /* Set the parent of \\\\( x_i \\\\) as its preceding sibling */\\n\\nend\\n\\nend\\n\\nOutput: The modified tree \\\\( T' \\\\)\\n\\nheading in its preceding nodes, then the label is \\\\( y_i = 'Move' \\\\). Otherwise, the label is \\\\( y_i = 'Keep' \\\\).\\n\\nThe loss is the cross entropy between \\\\( \\\\hat{y}_i \\\\) and \\\\( y_i \\\\).\\n\\n5 Experiments\\n\\n5.1 Experimental Setup\\n\\nBaselines\\nWe use MTD (Hu et al., 2022) as our baseline, which utilises multimodal information from images, text, and layout. The MTD consists of two steps: firstly, classifying and selecting headings from documents with a pre-trained language model, and secondly, modelling via GRU (Cho et al., 2014) and decoding heading relations into a hierarchical tree.\\n\\nDataset\\nWe evaluate CMM on the following ToC extraction datasets:\\n\\n(1) ESGDoc dataset consists of 1,093 ESG annual report documents, with 765, 110, and 218 in the train, development, and test sets, respectively. In our experiments, MTD encounters out-of-memory issues when processing some long documents in ESGDoc as it needs to model the entire document as a whole. Therefore, we curated a sub-dataset, denoted as ESGDoc(Partial), which consists of documents from ESGDoc that are less than 50 pages in length. This sub-dataset contains 274, 40, and 78 documents in the train, development, and test sets, respectively.\\n\\n(2) HierDoc dataset (Hu et al., 2022) contains 650 scientific papers with 350, 300 in the train, and test sets, respectively. Given that the extracted text from HierDoc does not include font size, we extract font size from PDF directly using PyMuPDF.\\n\\nEvaluation Metrics\\nWe evaluate our method in two aspects: heading detection (HD) and the tree of ToC. HD is evaluated using the F1-score, which measures the effectiveness of our method in identifying headings from the document, which primarily relates to construction and modelling steps, as it does not measure the hierarchical structure of ToC. For ToC, we use tree-edit-distance similarity (TEDS) (Zhong et al., 2019a; Hu et al., 2022), which compares the similarity between two trees based on their sizes and the tree-edit-distance (Pawlak and Augsten, 2016) between them:\\n\\n\\\\[ \\\\text{TEDS}(T_p, T_g) = 1 - \\\\frac{\\\\text{TreeEditDist}(T_p, T_g)}{\\\\max(|T_p|, |T_g|)} \\\\] (7)\\n\\nFor each document, a TEDS is computed between the predicted tree \\\\( T_p \\\\) and the ground-truth tree \\\\( T_g \\\\). The final TEDS is the average of the TEDSs of all documents.\\n\\nImplementation Detail\\nWe use RoBERTa-base (Liu et al., 2019) as the text encoder model. We set the BFS depth \\\\( n_d = 2 \\\\), and the hidden size of \\\\( b, v, h \\\\) to 128. Our model is trained on a NVIDIA A100 80G GPU using the Adam optimizer (Kingma and Ba, 2015) with a batch size 32. We use a learning rate of \\\\( 1 \\\\times 10^{-5} \\\\) for pretrained parameters, and a learning rate of \\\\( 1 \\\\times 10^{-3} \\\\) for randomly initialised parameters. In some instances, the font size may be automatically adjusted slightly depending on the volume of text to ensure that texts that have varying fonts do not share the same font sizes. Texts with very small sizes are automatically deleted during modification.\\n\\n5.2 Assumption Violation Statistics\\n\\n| Dataset    | A1 | A2 | A3 | Any |\\n|------------|----|----|----|-----|\\n| HierDoc    | 0.0| 0.5| 4.1| 4.6 |\\n| ESGDoc     | 0.8| 1.7| 8.7| 10.8|\\n\\nTable 1: The percentage (%) that each assumption is violated. Any denotes the percentage that the heading violates at least one assumption.\\n\\nOur method is based on Assumption 1, 2, and 3. However, these assumptions do not always hold. This section presents statistics on the percentage of headings that violate these assumptions by automatically examining consecutive blocks along the sorted blocks \\\\( \\\\{ x_i \\\\}^{n_{b_i}=1} \\\\) with their labels. As shown in Table 1, there are 4.6% and 10.8% of headings that contravene these assumptions in HierDoc and ESGDoc, respectively. Our current method is unable...\"}"}
{"id": "emnlp-2023-main-816", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to process these non-compliant headings. Despite these limitations, our method still achieves good performance, as will be detailed in Section 5.3.\\n\\n5.3 Overall Results\\n\\nTable 2 presents the overall TEDS results on HierDoc and ESGDoc. Both models demonstrate good performance on HierDoc with CMM slightly outperforming MTD. However, we observe significant performance drop on ESGDoc, indicating the challenge of processing complex ESG reports compared to scientific papers. MTD exhibits a notably low TEDS score in ESGDoc (Full) due to the out-of-memory issue it encountered when processing certain lengthy documents. To address this, we exclude documents longer than 50 pages, resulting in MTD achieving a TEDS score of 26.9% on ESGDoc (Partial). Nevertheless, our approach CMM outperforms MTD by a substantial margin. The HD F1-score of our method outperforming MTD by 12.8% on ESGDoc (Partial) also demonstrates the effectiveness of construction and modelling steps.\\n\\nDue to the violation of assumptions as discussed in Section 5.2, the improvement of our model over MTD in TOC is less pronounced compared to HD.\\n\\n|                  | HierDoc | ESGDoc (Full) | ESGDoc (Partial) |\\n|------------------|---------|---------------|------------------|\\n| Training Time    | MTD     | CMM (Ours)    | MTD              |\\n| Ratio            | 2420.6x | 1.0x          | 513.5            |\\n| Inference Time   | MTD     | CMM (Ours)    | MTD              |\\n| Ratio            | 4.2x    | 1.0x          | 1.3x             |\\n\\nTable 3: The GPU training and inference time in minutes for MTD and CMM on HierDoc and ESGDoc (Partial). MTD consumes 4.6x and 2.1x more time for training and 4.2x and 1.3x more time for inference on HierDoc and ESGDoc, respectively. Different from MTD, CMM does not need to model all possible pairs of headings. Instead, it only predicts whether a node should be deleted or relocated, thereby reducing the computational time.\\n\\nCompared to MTD, our model exhibits higher efficiency on HierDoc compared to ESGDoc. This could be attributed to the larger number of edges in the graphs constructed from node-centric subtrees in our method for ESGDoc. ESG annual reports often contain numerous small text blocks, such as \\\"$5,300m\\\", \\\"14,000\\\", and \\\"3,947 jobs\\\", as illustrated in the first example of Figure 1. Our method treats these individual texts as separate nodes in both the trees and graphs, leading to a significant increase in the number of edges in ESGDoc compared to HierDoc.\\n\\n5.4 Run-Time Comparison\\n\\n5.5 Ablation Study\\n\\nTable 4 illustrates how different components in CMM contribute to performance:\\n\\n| Component | Performance |\\n|-----------|-------------|\\n| w/ page-based division | CMM divides the document into subtrees based on the tree structure. We substitute the tree-based division with a page-based one. Initially, the document is divided using a window of 6 pages with a 2-page overlap. All other steps remain unchanged, including the modelling of node-centric subtrees. The choice of the...\"}"}
{"id": "emnlp-2023-main-816", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Ablation Study of CMM on HierDoc and ESGDoc with heading detection (HD) and ToC results reported in F1-score and TEDS (%), respectively.\\n\\nPage number for division is made to keep a similar GPU memory consumption. This results in a performance drop of 0.3% on HierDoc and 3.1% on ESGDoc. The page-based division impedes long-distance interaction, resulting in a lack of connection between high-level headings.\\n\\nWe exclude the GRU in Eq. (2) and directly set $v_i = b_i$. The results show a performance drop of 0.4% on HierDoc and 2.7% on ESGDoc.\\n\\nWe exclude the GNN in Eq. (3) and directly set $h_i = v_i$. This results in a more significant performance drop of 0.7% on HierDoc and 8.3% on ESGDoc. With GNN, each heading can gather information from other long-distance and local body nodes effectively and simultaneously.\\n\\nAs shown in Table 4, there is a larger performance drop on ESGDoc compared to HierDoc. This can be attributed to the same reason outlined in Section 5.3: inferring hierarchical relationships from headings themselves is easier in HierDoc than in ESGDoc. Therefore, the removal of components that introduce hierarchical relationships does not significantly harm the performance on HierDoc.\\n\\nFigure 3 demonstrates how performance varies with different values of $n_d$, the depth of neighbour during BFS for constructing node-centric subtrees. Due to the nature of the data, there is limited improvement observed on HierDoc as $n_d$ increases. Notably, there is a substantial increase in TEDS from $n_d = 1$ to $n_d = 2$, but the improvement becomes negligible when $n_d > 2$. Therefore, we select $n_d = 2$ considering the trade-off between performance and efficiency.\\n\\nThe primary factors contributing to the negligible improvement $n_d > 2$ may include: (1) A significant portion of documents exhibit a linear structure. To illustrate, when $n_d = 2$, it corresponds to a hierarchical arrangement featuring primary headings, secondary headings, and main body content in a three-tier configuration. (2) The constructed initial tree inherently positions related headings in close proximity according to their semantic relationships, without regard for their relative page placement. As a consequence, a heading's most relevant contextual information predominantly emerges from its immediate neighbours within the tree. For example, when examining heading 1.2., information from heading 1. ($n_d = 1$) offers a comprehensive overview of the encompassing chapter. Simultaneously, heading 2. ($n_d = 2$) can provide supplementary insights, such as affirming that heading 1.2. is nested within the domain of heading 1., rather than heading 2. However, delving into deeper levels may become redundant. For example, a heading like 2.2. ($n_d = 3$), situated more distantly in the semantic space, would not notably enhance the understanding of heading 1.1.\\n\\nSome case studies illustrating the ToC extraction results of CMM on ESGDoc are presented in Appendix C.\\n\\n6 Conclusion and Future Work\\n\\nIn this paper, we have constructed a new dataset, ESGDoc, and proposed a novel framework, CMM, for table of contents extraction. Our pipeline, consisting of tree construction, node-centric subtree modelling, and tree modification stages, effectively addresses the challenges posed by the diverse structures and lengthy nature of documents in ESGDoc.\\n\\nThe methodology of representing a document as an initial full tree, and subsequently predicting node operations for tree modification, and further leveraging the tree structure for document segmentation, can provide valuable insights for other document analysis tasks.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThis work was funded by the UK Engineering and Physical Sciences Research Council (grant no. EP/T017112/1, EP/T017112/2, EP/V048597/1).\\n\\nYH is supported by a Turing AI Fellowship funded by the UK Research and Innovation (grant no. EP/V020579/1, EP/V020579/2).\\n\\nLimitations\\n\\nOur method exhibits two primary limitations. Firstly, it relies on the extraction of font size. For documents in photographed or scanned forms, an additional step is required to obtain the font size before applying our method. However, with the prevailing trend of storing documents in electronic formats, this limitation is expected to diminish in significance.\\n\\nSecondly, our method is grounded in Assumptions 1, 2, and 3. As discussed in Section 5.2, our current method encounters difficulties in scenarios where these assumptions are not met. Some examples of such assumption violations are provided in Appendix B. However, it is worth noting that these assumption violations primarily impact the modification step in our construction-modelling-modification approach. If we focus solely on the construction and modelling steps, our method still outperforms MTD in heading detection. Therefore, future efforts to enhance the modification step, which is susceptible to assumption violations, could hold promise for improving the overall performance of our approach.\\n\\nEthics Statement\\n\\nThe ESG annual reports in ESGDoc are independently published by the companies and are publicly accessible. ResponsibilityReports.com compiles these ESG annual reports, which are also accessible directly on the respective companies' websites. There are also other websites such as CSRWIRE and sustainability-reports.com serve as repositories for these reports. Because these reports are publicly available, the use of such data for research purposes is not anticipated to present any ethical concerns.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-816", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2 describes how to build an initial full tree from a document.\\n\\nAlgorithm 2: Building a Tree based on Reading Order and Font Sizes\\n\\nInput: Root node \\\\( r \\\\), all blocks \\\\( \\\\{x_i\\\\}_{n_i=1} \\\\) with their corresponding sizes \\\\( \\\\{s_i\\\\}_{n_i=1} \\\\).\\n\\nfor \\\\( i = 1 \\\\) to \\\\( n_b \\\\) do\\n  \\\\( j \\\\leftarrow i - 1 \\\\)\\n  while \\\\( j > 0 \\\\) do\\n    if \\\\( j = 0 \\\\) then\\n      \\\\( \\\\text{PA}(x_i) \\\\leftarrow r \\\\) /* Set \\\\( r \\\\) as the parent node of \\\\( x_i \\\\)*/\\n      break\\n    else if \\\\( s_j > s_i \\\\) then\\n      \\\\( \\\\text{PA}(x_i) \\\\leftarrow x_j \\\\) /* Set \\\\( x_j \\\\) as the parent node of \\\\( x_i \\\\)*/\\n      break\\n    end\\n    \\\\( j \\\\leftarrow j - 1 \\\\)\\n  end\\nend\\n\\nOutput: a tree \\\\( T \\\\) with root node \\\\( r \\\\).\\n\\nB Assumption Violation Examples\\n\\nFor Assumption 1, upon manually inspecting some samples, we found that errors in these particular cases were linked to the errors in the XY-cut algorithm (Ha et al., 1995), resulting in an incorrect arrangement of text blocks.\\n\\nFigure A1 presents two examples where Assumption 2 is not satisfied. In the first example, the term \\\"The way we work\\\" serves as the parent-heading of \\\"Corporate governance\\\" which is a sub-heading, but featuring a smaller font size. Despite the smaller font size of \\\"The way we work\\\", it is clearly delineated from the sub-headings below by two green horizontal lines. However, our method focuses solely on text, neglecting visual cues such as these lines.\\n\\nIn the second example, \\\"COMMUNITY OUTREACH\\\" is a sub-heading under \\\"SOCIAL RESPONSIBILITY\\\", but has a larger font size, as this page emphasises community achievements.\\n\\nFigure A2 presents two instances where Assumption 3 is violated. In the first example, \\\"indirect economic impacts\\\" and \\\"Transmission System Investments\\\" are headings situated at the same hierarchical level but have distinct font sizes. This dissimilarity could potentially lead to confusion for human readers, questioning whether these two headings should be placed within the same hierarchical level.\\n\\nIn the second example, \\\"Sustainability Fund purchases\\\" and \\\"Spend by solution type\\\" are also headings at the same level, with subtly different font sizes, 11 and 10, respectively. While this difference may go unnoticed by humans, it does impact the performance of our method.\\n\\nC Case Study\\n\\nFigure A3 and Figure A4 illustrate a favourable scenario and an unfavorable one for CMM within the context of ESGDoc. The favourable case in Figure A3 demonstrates the capability of our model to handle lengthy documents, where it generates a high-quality tree structure.\\n\\nConversely, Figure A4 represents a challenging scenario where our model encounters difficulties across multiple nodes. Figure A5 further elaborates on this issue, showcasing four example pages of the unfavorable case illustrated in Figure A4.\\n\\nOn the top-left page, CMM incorrectly retain the non-heading \\\"ABOUT BRANDYWINE\\\". This is a challenging case as \\\"ABOUT BRANDYWINE\\\" is prominently displayed in a large font at the top-left corner of the page, making it difficult to identify as a non-heading. A similar situation occurs on the top-right page, where CMM incorrectly keeps the non-heading \\\"ENVIRONMENTAL PROGRESS\\\". In this instance, \\\"ENVIRONMENTAL PROGRESS\\\" is enlarged to emphasise the company's achievements.\\n\\nFor the bottom two pages in Figure A5, CMM might encounter confusion between headings with coloured lead-in sentences. In the bottom-left page, \\\"MANAGING CLIMATE RISK\\\" functions as a heading and follows a pattern similar to other lead-in sentences such as \\\"GOVERNANCE\\\" and \\\"STRATEGY AND RISK MANAGEMENT\\\". They typically begin with a large, colored sentence followed by a paragraph. The bottom-right page presents a similar challenge. \\\"OUR TENANTS\\\" and \\\"VALUED PARTNERSHIPS\\\" share a similar pattern, with the former being a heading, whereas the latter not. The determination of \\\"MANAGING CLIMATE RISK\\\" and \\\"OUR EMPLOYEES\\\" as headings is primarily based on their position and other visual clues.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure A1: Two examples in ESGDoc where Assumption 2 is violated, one in portrait and the other in landscape orientation.\\n\\nFigure A2: Two examples in ESGDoc where Assumption 3 is violated, one in portrait and the other in landscape orientation.\\n\\nHowever, it is worth noting that CMM does not use visual information, making it difficult for the model to handle such scenarios. Future work could explore the integration of visual information to enhance the model's performance in handling these situations.\"}"}
{"id": "emnlp-2023-main-816", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-816", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A MESSAGE FROM OUR CEO\\n\\nAN ONGOING COMMITMENT\\n\\nESG LEADERSHIP AND RECOGNITION\\n\\nBUILDING CERTIFICATIONS\\n\\nUN SUSTAINABLE DEVELOPMENT GOALS\\n\\nMANAGING CLIMATE RISK\\n\\nENERGY EFFICIENCY\\n\\nWASTE REDUCTION\\n\\nWATER MANAGEMENT\\n\\nPRIORITIZING HEALTH AND WELLBEING\\n\\nOUR TENANTS\\n\\nQUALITY\\n\\nOUR EMPLOYEES\\n\\nESG GOVERNANCE\\n\\nEXECUTIVE TEAM\\n\\nSASB DISCLOSURES\\n\\nCOMMUNITY IMPACT\\n\\nDISCLOSURES\\n\\nFigure A4: An unfavorable case for CMM on ESGDoc. Blocks highlighted in red represent incorrect predictions.\\n\\nFigure A5: Four example pages of the unfavorable case highlighted in Figure A4. CMM preserves non-headings in the top two pages, while deleting headings in the bottom two pages.\"}"}
