{"id": "acl-2023-long-310", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nDiffusion models are a milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text\u2013image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce attribution maps, we upscale and aggregate cross-attention maps in the denoising module, naming our method DAAM. We validate it by testing its segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. On two generated datasets, we attain a competitive 58.8\u201364.8 mIoU on noun segmentation and fair to good mean opinion scores (3.4\u20134.2) on generalized attribution. Then, we apply DAAM to study the role of syntax in the pixel space across head\u2013dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.\\n\\n1 Introduction\\nDiffusion models trained on billions of captioned images represent state-of-the-art text-to-image generation (Yang et al., 2022), with some achieving photorealism, such as Google's Imagen (Saharia et al., 2022) and OpenAI's DALL-E 2 (Ramesh et al., 2022). However, despite their quality and popularity, the dynamics of their image synthesis remain undercharacterized. Citing ethics, corporations have restricted the general public from using the models and their weights, preventing effective analysis. To overcome this barrier, Stability AI recently open-sourced Stable Diffusion (Rombach et al., 2022), a 1.1 billion-parameter latent diffusion model pretrained and fine-tuned on the LAION 5-billion image dataset (Schuhmann et al., 2022).\\n\\nWe probe Stable Diffusion to provide insight into large diffusion models. Focusing on text\u2013image attribution, our central question is, \\\"How does an input word influence parts of a generated image?\\\" To this, we propose to produce 2D attribution maps for each word by combining cross-attention maps in the model. A related work in prompt-guided editing from Hertz et al. (2022) conjectures that per-head cross attention relates words to areas in Imagen-generated images, but they fall short of constructing global per-word attribution maps. We name our method diffusion attentive attribution maps, or \\\"DAAM;\\\" see Figure 1 for an example.\\n\\nTo evaluate the veracity of DAAM, we apply it to a semantic segmentation task (Lin et al., 2014) on generated imagery, comparing DAAM maps with annotation. We attain a 58.8\u201364.8 mean intersection over union (mIoU) score competitive with unsupervised segmentation models, described in Section 3.1. We further bolster these results using a generalized study covering all parts of speech (all in Penn Treebank; Marcinkiewicz, 1994), such as adjectives and verbs. Through human annotation, we show that the mean opinion score (MOS) is above fair to good (3.4\u20134.2) on interpretable words.\\n\\nNext, we study how relationships in the syntactic space of prompts relate to those in the pixel space across head\u2013dependent heat map interaction patterns for ten common dependency relations. We show that, for some relations, the head map consistently subsumes the dependent, while the opposite is true for others. Finally, we study several semantic phenomena, focusing on feature entanglement; we find that the presence of cohyponyms worsens generation quality by 9%, and descriptive adjectives attend too broadly. We are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future research. Our code is at https://github.com/castorini/daam.\"}"}
{"id": "acl-2023-long-310", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of images. We assess head\u2013dependent DAAM interactions across ten common syntactic relations (enhanced Universal Dependencies; Schuster and Manning, 2016), finding that, for some, the heat map of the dependent strongly subsumes the head\u2019s, while the opposite is true for others. For others, such as coreferent word pairs, the words\u2019 maps greatly overlap, indicating coreferent understanding during generation. We assign intuition to our observations; for example, we observe that the maps of verbs contain their subjects, suggesting that verbs strongly contextualize the generation of both the subjects and their surroundings.\\n\\nFinally, we form hypotheses to further our syntactic findings, studying semantic phenomena using DAAM, particularly those affecting image quality. In Section 5.1, we demonstrate that, in constructed prompts with two distinct nouns, cohyponyms have worse quality (9% worse than non-cohyponyms), e.g., \u201ca giraffe and a zebra\u201d generates a giraffe or a zebra, but not both. Cohyponym status and generation incorrectness each increases the amount of heat map overlap, advancing DAAM\u2019s utility toward improving diffusion models. We also show in Section 5.2 that descriptive adjectives attend too broadly across the image, far beyond their nouns. If we fix the scene layout (Hertz et al., 2022) and vary only the adjective, the entire image changes, not just the noun. These two phenomena suggest feature entanglement, where objects are entangled with both the scene and other objects.\\n\\nIn summary, our contributions are as follows:\\n\\n(1) we propose and evaluate an attribution method, novel within the context of interpreting diffusion models, measuring which parts of the generated image the words influence most;\\n\\n(2) we provide new insight into how syntactic relationships map to generated pixels, finding evidence for directional imbalance in head\u2013dependent DAAM map overlap, alongside visual intuition (and counterintuition) in the behaviors of nominals, modifiers, and function words; and\\n\\n(3) we shine light on failure cases in diffusion models, showing that descriptive adjectival modifiers and cohyponyms result in entangled features and DAAM maps.\\n\\n2 Our Approach\\n\\n2.1 Preliminaries\\n\\nLatent diffusion models (Rombach et al., 2022) are a class of denoising generative models that are trained to synthesize high-fidelity images from random noise through a gradual denoising process, optionally conditioned on text. They generally comprise three components: a deep language model like CLIP (Radford et al., 2021) for producing word embeddings; a variational autoencoder (VAE; Kingma and Welling, 2013) which encodes and decodes latent vectors for images; and a time-conditional U-Net (Ronneberger et al., 2015) for gradually denoising latent vectors. To generate an image, we initialize the latent vectors to random noise, feed in a text prompt, then iteratively denoise the latent vectors with the U-Net and decode the final vector into an image with the VAE.\\n\\nFormally, given an image, the VAE encodes it as a latent vector \\\\( \\\\ell_0 \\\\in \\\\mathbb{R}^d \\\\). Define a forward \u201cnoise injecting\u201d Markov chain \\\\( p(\\\\ell_t | \\\\ell_{t-1}) := N(\\\\ell_t; \\\\sqrt{1-\\\\alpha_t} \\\\ell_0, \\\\alpha_t I) \\\\) where \\\\( \\\\{\\\\alpha_t\\\\}_{t=1}^T \\\\) is defined following a schedule so that \\\\( p(\\\\ell_T) \\\\) is approximately zero-mean isotropic. The corresponding denoising reverse chain is then parameterized as \\\\( p(\\\\ell_{t-1} | \\\\ell_t) := N(\\\\ell_{t-1}; 1/\\\\sqrt{1-\\\\alpha_t}(\\\\ell_t + \\\\alpha_t \\\\epsilon_{\\\\theta}(\\\\ell_t, t)), \\\\alpha_t I) \\\\), for some denoising network \\\\( \\\\epsilon_{\\\\theta}(\\\\ell, t) \\\\) with parameters \\\\( \\\\theta \\\\). Intuitively, the forward process iteratively adds noise to some signal at a fixed rate, while the reverse process, using a neural network, removes noise until recovering the signal. To train the network, given caption\u2013image pairs, we optimize \\\\( \\\\min_{\\\\theta} \\\\sum_{t=1}^T \\\\zeta_t \\\\mathbb{E}_{p(\\\\ell_t | \\\\ell_0)} \\\\| \\\\epsilon_{\\\\theta}(\\\\ell_t, t) - \\\\nabla \\\\ell_t \\\\log p(\\\\ell_t | \\\\ell_0) \\\\|^2 \\\\) ,\\n\\nwhere \\\\( \\\\{\\\\zeta_t\\\\}_{t=1}^T \\\\) are constants computed as \\\\( \\\\zeta_t := 1 - \\\\prod_{j=1}^{t} (1 - \\\\alpha_j) \\\\). The objective is a reweighted form of the evidence lower bound for score matching (Song et al., 2021). To generate a latent vector, we initialize \\\\( \\\\hat{\\\\ell}_T \\\\) as Gaussian noise and iterate\\n\\n\\\\[ \\\\hat{\\\\ell}_t = \\\\frac{1}{\\\\sqrt{1-\\\\alpha_t}}(\\\\hat{\\\\ell}_t + \\\\alpha_t \\\\epsilon_{\\\\theta}(\\\\hat{\\\\ell}_t, t)) + \\\\sqrt{\\\\alpha_t} z_t. \\\\]\\n\\nIn practice, we apply various optimizations to improve the convergence of the above step, like modeling the reverse process as an ODE (Song et al., 2021), but this definition suffices for us. We can additionally condition the latent vectors on text and pass word embeddings \\\\( X := [x_1; \\\\ldots; x_l W] \\\\) to \\\\( \\\\epsilon_{\\\\theta}(\\\\ell, t; X) \\\\). Finally, the VAE decodes the denoised latent \\\\( \\\\hat{\\\\ell}_0 \\\\) to an image. For this paper, we use the publicly available weights of the state-of-the-art, 1.1 billion-parameter Stable Diffusion 2.0 model (Rombach et al., 2022), trained on 5 billion caption\u2013image pairs (Schuhmann et al., 2022) and implemented in HuggingFace\u2019s Diffusers library (von Platen et al., 2022).\"}"}
{"id": "acl-2023-long-310", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Diffusion Attentive Attribution Maps\\n\\nGiven a large-scale latent diffusion model for text-to-image synthesis, which parts of an image does each word influence most? One way to achieve this would be attribution approaches, which are mainly perturbation- and gradient-based (Alvarez-Melis and Jaakkola, 2018; Selvaraju et al., 2017), where saliency maps are constructed either from the first derivative of the output with respect to the input, or from input perturbation to see how the output changes. Unfortunately, gradient methods prove intractable due to needing a backpropagation pass for every pixel for all $T$ time steps, and even minor perturbations result in significantly different images in our pilot experiments.\\n\\nInstead, we use ideas from natural language processing, where word attention was found to indicate lexical attribution (Clark et al., 2019), as well as the spatial layout of Imagen\u2019s images (Hertz et al., 2022). In diffusion models, attention mechanisms cross-contextualize text embeddings with coordinate-aware latent representations (Rombach et al., 2022) of the image, outputting scores for each token\u2013image patch pair. Attention scores lend themselves readily to interpretation since they are already normalized in $[0, 1]$. Thus, for pixelwise attribution, we propose to aggregate these scores over the spatiotemporal dimensions and interpolate them across the image.\\n\\nWe turn our attention to the denoising network $\\\\epsilon_{\\\\theta}(\\\\ell, t; X)$ responsible for the synthesis. While the subnetwork can take any form, U-Nets remain the popular choice (Ronneberger et al., 2015) due to their strong image segmentation ability. They consist of a series of downsampling convolutional blocks, each of which preserves some local context, followed by upsampling deconvolutional blocks, which restore the original input size to the output. Specifically, given a 2D latent $\\\\ell_t \\\\in \\\\mathbb{R}^{w \\\\times h}$, the downsampling blocks output a series of vectors $\\\\{h_{\\\\downarrow}^i, t\\\\}_{i=1}^K$, where $h_{\\\\downarrow}^i, t \\\\in \\\\mathbb{R}^{\\\\lceil wc_i \\\\rceil \\\\times \\\\lceil hc_i \\\\rceil}$ for some $c > 1$. The upsampling blocks then iteratively upscale $h_{\\\\downarrow}^i, t$ to $\\\\{h_{\\\\uparrow}^i, t\\\\}_{0}^{K-1} \\\\in \\\\mathbb{R}^{\\\\lceil wc_i \\\\rceil \\\\times \\\\lceil hc_i \\\\rceil}$.\\n\\nTo condition these representations on word embeddings, Rombach et al. (2022) use multi-headed cross-attention layers (Vaswani et al., 2017)\\n\\n$$h_{\\\\downarrow}^i, t := F_i(t, \\\\hat{h}_{\\\\downarrow}^i, t, X) \\\\cdot (W_i v X),$$\\n\\n$$F_i(t, \\\\hat{h}_{\\\\downarrow}^i, t, X) := \\\\text{softmax}\\\\left(\\\\frac{W_i q \\\\hat{h}_{\\\\downarrow}^i, t (W_i k X)^T}{\\\\sqrt{d}}\\\\right),$$\\n\\nwhere $F_i$ is an $I \\\\times B \\\\times D \\\\times C$ attention array and $W_k, W_q, W_v$ are projection matrices with attention heads. The same mechanism applies when upsampling $h_{\\\\uparrow}^i$. For brevity, we denote the respective attention score arrays as $F_i(t)$ and $F_i(t)$, and we implicitly broadcast matrix multiplications as per NumPy convention (Harris et al., 2020).\\n\\nSpatiotemporal aggregation. $F_i(t)[x, y, \\\\ell, k]$ is normalized to $[0, 1]$ and connects the $k$th word to the intermediate coordinate $(x, y)$ for the $i$th downsampling block and $\\\\ell$th head. Due to the fully convolutional nature of U-Net (and the VAE), the intermediate coordinates locally map to a surrounding affected square area in the final image, the scores thus relating each word to that image patch. However, different layers produce heat maps with varying scales, deepest ones being the coarsest (e.g., $h_{\\\\downarrow}^K, t$ and $h_{\\\\uparrow}^{K-1}, t$), requiring spatial normalization to create a single heat map. To do this, we upscale all intermediate attention score arrays to the original image size using bicubic interpolation, then sum them over the heads, layers, and time steps:\\n\\n$$D_R^k[x, y] := \\\\sum_{i, j, \\\\ell} \\\\tilde{F}_i(t)[x, y] + \\\\tilde{F}_i(t)[x, y],$$\\n\\nwhere $k$ is the $k$th word and $\\\\tilde{F}_i(t)[x, y]$ is shorthand for $F_i(t)[x, y]$ bicubically upscaled to fixed size $(w, h)$. Since $D_R^k$ is positive and scale normalized (summing normalized values preserves linear scale), we can visualize it as a soft heat map, with higher values having greater attribution. To generate a hard, binary heat map (either a pixel is influenced or not), we can threshold $D_R^k$ as $D_I^\\\\tau_k[x, y] := \\\\mathbb{I}(D_R^k[x, y] \\\\geq \\\\tau_{\\\\max} \\\\sum_{i, j} D_R^k[i, j])$,\\n\\nwhere $\\\\mathbb{I}(\\\\cdot)$ is the indicator function and $\\\\tau \\\\in [0, 1]$.\\n\\nSee Figure 2 for an illustration of DAAM. 1We show that aggregating across all time steps and layers is indeed necessary in Section A.1.\"}"}
{"id": "acl-2023-long-310", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"# Method COCO-Gen Unreal-Gen\\n\\n| Method                      | COCO-Gen mIoU | Unreal-Gen mIoU |\\n|-----------------------------|---------------|-----------------|\\n| Supervised Methods          |               |                 |\\n| 1 Mask R-CNN (ResNet-101)   | 80.4          | 84.0            |\\n| 2 QueryInst (ResNet-101-FPN)| 81.2          | 83.6            |\\n| 3 Mask2Former (Swin-S)      | 82.0          | 85.0            |\\n| 4 CLIPSeg                   | 74.2          | 79.0            |\\n| Unsupervised Methods        |               |                 |\\n| 5 Whole image mask          | 21.7          | 24.8            |\\n| 6 PiCIE + H                 | 31.7          | 35.9            |\\n| 7 STEGO (DINO ViT-B)        | 42.0          | 38.2            |\\n| 8 Our DAAM-0.3              | 62.7          | 64.7            |\\n| 9 Our DAAM-0.4              | 62.8          | 64.8            |\\n| 10 Our DAAM-0.5             | 59.6          | 60.0            |\\n\\nTable 1: Mean IoU of semantic segmentation methods on our synthesized datasets. Best in each section bolded.\\n\\n## 3 Attribution Analyses\\n\\n### 3.1 Object Attribution\\n\\nQuantitative evaluation of our method is challenging, but we can attempt to draw upon existing annotated datasets and methods to see how well our method aligns. A popular visuosemantic task is image segmentation, where areas (i.e., segmentation masks) are given a semantically meaningful label, commonly nouns. If DAAM is accurate, then our attention maps should arguably align with the image segmentation labels for these tasks\u2014despite not having been trained to perform this task.\\n\\n**Setup.**\\n\\nWe ran Stable Diffusion 2.0-base using 30 inference steps per image with the DPM (Lu et al., 2022) solver\u2014see Appendix A.1. We then synthesized one set of images using the validation set of the COCO image captions dataset (Lin et al., 2014), representing realistic prompts, and another set by randomly swapping nouns in the same set (holding the vocabulary fixed), representing unrealism. The purpose of the second set was to see how well the model generalized to uncanny prompts, whose composition was unlikely to have been encountered at training time. We named the two sets \u201cCOCO-Gen\u201d and \u201cUnreal-Gen,\u201d each with 500 prompt\u2013image pairs. For ground truth, we extracted all countable nouns from the prompts, then hand-segmented each present noun in the image.\\n\\nTo compute binary DAAM segmentation masks, we used Eqn. 7 with thresholds $\\\\tau \\\\in \\\\{0.3, 0.4, 0.5\\\\}$, for each noun in the ground truth. We refer to these methods as DAAM-$\\\\langle \\\\tau \\\\rangle$, e.g., DAAM-0.3.\\n\\nFor supervised baselines, we evaluated semantic segmentation models trained explicitly on COCO, like Mask R-CNN (He et al., 2017) with a ResNet-101 backbone (He et al., 2016), QueryInst (Fang et al., 2021) with ResNet-101-FPN (Lin et al., 2017), and Mask2Former (Cheng et al., 2022) with Swin-S (Liu et al., 2021), all implemented in MMDetection (Chen et al., 2019), as well as the open-vocabulary CLIPSeg (L\u00fcddecke and Ecker, 2022) trained on the PhraseCut dataset (Wu et al., 2020). We note that CLIPSeg\u2019s setup resembles ours since the image captions are assumed given as well. However, theirs is supervised since they train their model on segmentation labels as well. Our unsupervised baselines consisted of the state-of-the-art STEGO (Hamilton et al., 2021) and PiCIE + H (Cho et al., 2021). As is standard (Lin et al., 2014), we evaluated all approaches using the mean intersection over union (mIoU) over the prediction\u2013truth mask pairs. We denote mIoU$_{80}$ when restricted to the 80 COCO classes that the supervised baselines were trained on and mIoU$_{\\\\infty}$ as the mIoU without the class restriction; see Sec. B for details.\\n\\n**Results.**\\n\\nWe present results in Table 1. The COCO-supervised models (rows 1\u20133) are constrained to COCO\u2019s 80 classes (e.g., \u201ccat,\u201d \u201ccake\u201d), while DAAM (rows 5\u20137) is open vocabulary; thus, DAAM outperforms them by 22\u201328 points in mIoU$_{\\\\infty}$ and underperforms by 20 points in mIoU$_{80}$. CLIPSeg (row 4), an open-vocabulary model trained on semantic segmentation datasets, achieves the best of both worlds in mIoU$_{80}$ and mIoU$_{\\\\infty}$, with the highest mIoU$_{\\\\infty}$ overall and high mIoU$_{80}$. However, its restriction to nouns precludes it from generalized segmentation (e.g., verbs). DAAM largely outperforms both unsupervised baselines (rows 6\u20137) by a margin of 6\u201327 points (except for STEGO on COCO-Gen mIoU$_{\\\\infty}$, where it\u2019s similar), likely because we assume the prompts to be provided. Similar findings hold on the unrealistic Unreal-Gen set, showing that DAAM is resilient to nonsensical texts, confirming that DAAM works when Stable Diffusion has to generalize in composition.\\n\\nAs for $\\\\tau$, 0.4 works best on all splits, though it isn\u2019t too sensitive, varying by 0.1\u20135 points in mIoU. We also show that all layers and time steps contribute to DAAM\u2019s segmentation quality in Section A.1. Overall, DAAM forms a strong baseline of 58.1\u201364.8 mIoU$_{80}$. As our goal is to prove sanity, not state of the art, we conclude that DAAM is sane for noun attribution, which we extend to all parts of speech in the next section.\"}"}
{"id": "acl-2023-long-310", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Generalized Attribution\\n\\nWe extend our veracity analyses beyond nouns to all parts of speech, such as adjectives and verbs, to show that DAAM is more generally applicable.\\n\\nA high-quality, reliable analysis requires human annotation; hence, we ask human raters to evaluate the attribution quality of DAAM maps, using a five-point Likert scale.\\n\\nThis setup generalizes that of the last section because words in general are not visually separable, which prevents effective segmentation annotation. For example, in the prompt \u201cpeople running,\u201d it is unclear where to visually segment \u201crunning.\u201d Is it just the knees and feet of the runners, or is it also the swinging arms? On the contrary, if annotators are instead given the proposed heat maps for \u201crunning,\u201d they can make a judgement on how well the maps reflect the word.\\n\\nSetup.\\n\\nTo construct our word\u2013image dataset, we first randomly sampled 200 words from each of the 14 most common part-of-speech tags in COCO, extracted with spaCy, for a total of 2,800 unique word\u2013prompt pairs. Next, we generated images alongside DAAM maps for all pairs, varying the random seed each time. To gather human judge-ments, we built our annotation interface in Amazon MTurk, a crowdsourcing platform. We presented the generated image, the heat map, and the prompt with the target word in red, beside a question asking expert workers to rate how well the highlighting reflects the word. They then selected a rating among one of \u201cbad,\u201d \u201cpoor,\u201d \u201cfair,\u201d \u201cgood,\u201d and \u201cexcellent\u201d, as well as an option to declare the image itself as too poor or the word too abstract to interpret. For quality control, we removed annotators failing attention tests. For further robustness, we assigned three unique raters to each example. We provide further details on the user interface and annotation process in the appendix section A.2.\\n\\nResults.\\n\\nOur examples were judged by a total of fifty raters, none producing more than 18% of the total number of annotations. We filtered out all word\u2013image pairs deemed too abstract (e.g., \u201cthe\u201d), when any one of the three assigned raters selected that option. This resulted in six interpretable part-of-speech tags with enough judgements; see the appendix for detailed statistics. To compute the final score of each word\u2013image pair, we took the median of the three raters' opinions.\\n\\nWe plot our results in Figure 3. In the top sub-plot, we show that DAAM maps for adjectives, verbs, nouns, and proper nouns attain close to or slightly above \u201cgood,\u201d whereas the ones for numerals and adverbs are closer to \u201cfair.\u201d This agrees with the generated examples in Figure 4, where numerals (see the giraffes' edges) and adverbs (feet and ground motion blur) are less intuitively highlighted than adjectives (blue part of teapot), verbs (fists and legs in running form), and nouns. Nevertheless, the proportion of ratings falling between fair and excellent are above 80% for numerals and adverbs and 90% for the rest\u2014see the bottom of Figure 3. We thus conclude that DAAM produces plausible maps for each interpretable part of speech.\\n\\nOne anticipated criticism is that different heat maps may explain the same word, making a qualitative comparison less meaningful. In Figure 4, \u201cquickly\u201d could conceivably explain \u201crunning\u201d too. We concede to this, but our motivation is not to compare quality but rather to demonstrate plausibility. Without these experiments, the DAAM maps for words like \u201crunning\u201d and \u201cblue\u201d could very well have been meaningless blotches.\"}"}
{"id": "acl-2023-long-310", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Head\u2013dependent DAAM map overlap statistics across the ten most common relations in COCO. Bolded are the dominant maps, where the absolute difference $\\\\Delta$ between mIoD and mIoH exceeds 10 points. All bolded numbers are statistically significant ($p<0.01$), with the Holm\u2013Bonferroni correction applied.\\n\\n| Relation   | mIoD | mIoH | $\\\\Delta$ | mIoU |\\n|------------|------|------|----------|------|\\n| Unrelated pairs | 65.1 | 66.1 | 1.0      | 47.5 |\\n| All head\u2013dependent pairs | 62.3 | 62.0 | 0.3      | 43.4 |\\n| compound    | 71.3 | 71.5 | 0.2      | 51.1 |\\n| punct       | 68.2 | 70.0 | 1.8      | 49.5 |\\n| nconj:and   | 58.0 | 56.1 | 1.9      | 38.2 |\\n| det         | 54.8 | 52.2 | 2.6      | 35.0 |\\n| case        | 51.7 | 58.1 | 6.4      | 36.9 |\\n| acl         | 67.4 | 79.3 | 12.0     | 55.4 |\\n| nsubj       | 76.4 | 63.9 | 12.0     | 52.2 |\\n| amod        | 62.4 | 77.6 | 15.2     | 51.1 |\\n| nmod:of     | 73.5 | 57.9 | 16.0     | 47.5 |\\n| obj         | 75.6 | 46.3 | 29.0     | 55.4 |\\n\\nCoreferent word pairs exhibit the highest overlap out of all relations (66.6 mIoU), indicating coreference resolution.\"}"}
{"id": "acl-2023-long-310", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5 Visuosemantic Analyses\\n\\n5.1 Cohyponym Entanglement\\n\\nTo further study the large overlap found in Section 4, we hypothesize that semantically similar words in a prompt have worse generation quality, where only one of the words is generated in the image, not all.\\n\\nSetup. To test our hypothesis, we used WordNet (Miller, 1995) to construct a hierarchical ontology expressing semantic fields over COCO's 80 visual objects, of which 28 have at least one other cohyponym across 16 distinct hypernyms (as listed in the appendix). Next, we used the prompt template, \\\"a(n) <noun> and a(n) <noun>,\\\" depicting two distinct things, to generate our dataset. Using our ontology, we randomly sampled two cohyponyms 50% of the time and two non-cohyponyms other times, producing 1,000 prompts from the template (e.g., \\\"a giraffe and a zebra,\\\" \\\"a cake and a bus\\\"). We generated an image for each prompt, then asked three unique annotators per image to select which objects were present, given the 28 words. We manually verified the image\u2013label pairs, rejecting and republishing incorrect ones. Finally, we marked the overall label for each image as the top two most commonly picked nouns, ties broken by submission order. We considered generations correct if both words in the prompt were present in the image. For more setup details, see the appendix.\\n\\nResults. Overall, the non-cohyponym set attains a generation accuracy of 61% and the cohyponym set 52%, statistically significant at the 99% level according to the exact test, supporting our hypothesis. To see if DAAM assists in explaining these effects, we compute binarized DAAM maps ($\\\\tau = 0.4$, the best value from Sec. 3.1) for both words and quantify the amount of overlap with IoU. We find that the $m\\\\text{IoU}$ for cohyponyms and non-cohyponyms are 46.7 and 22.9, suggesting entangled attention and composition. In the top of Figure 6, we further group the $m\\\\text{IoU}$ by cohyponym status and correctness, finding that incorrectness and cohyponymy independently increase the overlap. In the bottom subplot, we show that the amount of overlap ($m\\\\text{IoU}$) differentiates correctness, with the low, mid, and high cutoff points set at $\\\\leq 0.4, 0.4\u20130.6,$ and $\\\\geq 0.6,$ following statistics in Section 4. We observe accuracy to be much better on pairs with low overlap (71.7\u201377.5%) than those with high overlap (9.8\u201336%). We present some example generations and maps in Figure 7, which supports our results.\\n\\n5.2 Adjectival Entanglement\\n\\nWe examine prompts where a noun's modifying adjective attends too broadly across the image. We start with an initial seed prompt of the form, \\\"a <adj> <noun> <verb phrase>,\\\" then vary the adjective to see how the image changes. If there is no entanglement, then the background should...\"}"}
{"id": "acl-2023-long-310", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9: A DAAM map and generated images for \u201ca <adj> car driving down the streets,\u201d above images of the cropped background, oversaturated for visualization. To remove scene layout as a confounder, we fix all cross-attention maps to those of the seed prompt, which Hertz et al. (2022) show to equalize layout.\\n\\nOur first case is, \u201ca {rusty, metallic, wooden} shovel sitting in a clean shed,\u201d \u201crusty\u201d being the seed adjective. As shown in Figure 8, the DAAM map for \u201crusty\u201d attends broadly, and the background for \u201crusty\u201d is surely not clean. When we change the adjective to \u201cmetallic\u201d and \u201cwooden,\u201d the shed changes along with it, becoming grey and wooden, indicating entanglement. Similar observations apply to our second case, \u201ca {bumpy, smooth, spiky} ball rolling down a hill,\u201d where \u201cbumpy\u201d produces rugged ground, \u201csmooth\u201d flatter ground, and \u201cspiky\u201d blades of grass.\\n\\nIn our third case, we study color adjectives using \u201ca {blue, green, red} car driving down the streets,\u201d presented in Figure 9. We discover the same phenomena, with the difference that these prompts lead to quantifiable notions of adjectival entanglement. For, say, \u201cgreen,\u201d we can conceivably measure the amount of additional green hue in the background, with the car cropped out\u2014see bottom row. A caveat is that entanglement is not necessarily unwanted; for instance, rusty shovels likely belong in rusted areas. It strongly depends on the use case of the model.\\n\\nRelated Work and Future Directions\\n\\nThe primary area of this work is in understanding neural networks from the perspective of computational linguistics, with the goal of better informing future research. A large body of relevant papers exists, where researchers apply textual perturbation (Wallace et al., 2019), attention visualization (Vig, 2019; Kovaleva et al., 2019; Shimaoka et al., 2016), and information bottlenecks (Jiang et al., 2020) to relate important input tokens to the outputs of large language models. Others explicitly test for linguistic constructs within models, such as probing vision transformers for verb understanding (Hendricks and Nematzadeh, 2021) and examining visual grounding in image-to-text transformers (Ilinykh and Dobnik, 2022). Our distinction is that we carry out an attributive analysis in the space of generative diffusion models, as the pixel output relates to syntax and semantics. As a future extension, we plan to assess the unsupervised parsing ability of Stable Diffusion with syntactic\u2013geometric probes, similar to Hewitt and Manning\u2019s (2019) work in BERT.\\n\\nThe intersection of text-to-image generation and natural language processing is substantial. In the context of enhancing diffusion models with prompt engineering, Hertz et al. (2022) apply cross-attention maps for the purpose of precision-editing generated images using text, and Woolf (2022) proposes negative prompts for removing undesirable, scene-wide attributes. Related as well are works for generative adversarial networks, where Karras et al. (2019) and Materzy\u0144ska et al. (2022) disentangle various features such as style and spelling. Along this vein, our work exposes more entanglement in cohyponyms and adjectives. A future line of work is how to disentangle such concepts and improve generative quality.\\n\\nLast but not least are semantic segmentation works in computer vision. Generally, researchers start with a backbone encoder, attach decoders, and then optimize the model in its entirety end-to-end on a segmentation dataset (Cheng et al., 2022), unless the context is unsupervised, in which case one uses contrastive objectives and clustering (Cho et al., 2021; Hamilton et al., 2021). Toward this, DAAM could potentially provide encoder features in a segmentation pipeline, where its strong raw baseline numbers suggest the presence of valuable latent representations in Stable Diffusion.\\n\\nConclusions\\n\\nIn this paper, we study visuolinguistic phenomena in diffusion models by interpreting word\u2013pixel cross-attention maps. We prove the correctness of our attribution method, DAAM, through a quantitative semantic segmentation task and a qualitative generalized attribution study. We apply DAAM to assess how syntactic relations translate to visual interactions, finding that certain maps of heads inappropriately subsume their dependents\u2019. We use these findings to form hypotheses about feature entanglement, showing that cohyponyms are jumbled and adjectives attend too broadly.\"}"}
{"id": "acl-2023-long-310", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nOur analysis has both methodological and technical limitations. While dependency parsers are the most robust semanto-syntactic tools available to us, we are limited both by the quality of the parser's output and its paradigm. All automated tools make errors, and while our work uses short and simple phrases that are comparatively easy for these tools to handle, it is possible that even systematic errors could seep into the analysis. It is also possible that other semanto-syntactic tools would highlight different phenomena and improve (or worsen) the quality of the analysis.\\n\\nDue to the dataset used, which we picked for quantitative comparison to prior art, there is an inherent bias towards concrete concepts, as they are derived from image captions. We are therefore limited in the understanding of how our method applies to more abstract concepts (say, \\\"love\\\" and \\\"dignity\\\"), potentially warranting further study.\\n\\nThere are also concerns about the internal validity of attention maps as an interpretability tool. For example, Serrano and Smith (2019) argue, \\\"[In many cases,] gradient-based rankings of attention weights better predict [models'] effects than their magnitudes.\\\" However, for the analysis of diffusion models, gradient methods are intractable because a backpropagation pass is required for every pixel for all time steps, as stated in Section 2.2. Therefore, attention scores remain the most feasible method.\\n\\nLastly, we have consciously limited ourselves to purely making analytical observations regarding attribution and entanglement. This has arguably allowed us to cover a very wide range of phenomena and make a large number of observations, but this choice naturally limits us to not providing a method to resolve the issues we have observed with existing models, which is something we have left (and described in Section 6) as future work.\\n\\nAcknowledgments\\n\\nResources used in preparing this research were provided, in part, by the Province of Ontario, the Government of Canada through CIFAR, companies sponsoring the Vector Institute, and the HuggingFace team. In particular, we would like to thank Aleksandra (Ola) Piktus, who helped us get a community grant for our public demonstration on HuggingFace spaces.\\n\\nReferences\\n\\nDavid Alvarez-Melis and Tommi S. Jaakkola. 2018. On the robustness of interpretability methods. arXiv:1806.08049.\\n\\nKai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. 2019. MMDetection: Open MMLab detection toolbox and benchmark. arXiv:1906.07155.\\n\\nBowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. 2022. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nJang Hyun Cho, Utkarsh Mall, Kavita Bala, and Bharath Hariharan. 2021. PiCIE: Unsupervised semantic segmentation using invariance and equivariance in clustering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERT's attention. In Proceedings of BlackboxNLP.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\n\\nYuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu. 2021. Instances as queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\\n\\nMark Hamilton, Zhoutong Zhang, Bharath Hariharan, Noah Snavely, and William T. Freeman. 2021. Unsupervised semantic segmentation by distilling feature correspondences. In International Conference on Learning Representations.\\n\\nCharles R. Harris, K. Jarrod Millman, St\u00e9fan J. Van Der Walt, Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith, et al. 2020. Array programming with NumPy. Nature.\\n\\nDavid J. Hauser and Norbert Schwarz. 2016. Attentive turkers: MTurk participants perform better on online attention checks than do subject pool participants. Behavior research methods.\\n\\nKaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. 2017. Mask R-CNN. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\"}"}
{"id": "acl-2023-long-310", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nLisa Anne Hendricks and Aida Nematzadeh. 2021. Probing image-language transformers for verb understanding. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021.\\n\\nAmir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2022. Prompt-to-prompt image editing with cross attention control. arXiv:2208.01626.\\n\\nJohn Hewitt and Christopher D. Manning. 2019. A structural probe for finding syntax in word representations. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers).\\n\\nNikolai Ilinykh and Simon Dobnik. 2022. Attention as grounding: Exploring textual and cross-modal attention on entities and relations in language-and-vision transformer. In Findings of the Association for Computational Linguistics: ACL 2022.\\n\\nOlga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP).\\n\\nTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. 2017. Feature pyramid networks for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In European Conference on Computer Vision.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. 2021. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision.\\n\\nCheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. 2022. DPM-solver: A fast ODE solver for diffusion probabilistic model sampling in around 10 steps. arXiv:2206.00927.\\n\\nTimo L\u00fcddecke and Alexander Ecker. 2022. Image segmentation using text and image prompts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nChristopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Rose Finkel, Steven Bethard, and David McClosky. 2014. The Stanford CoreNLP natural language processing toolkit. In Proceedings of 52nd annual meeting of the Association for Computational Linguistics: System Demonstrations.\\n\\nMary Ann Marcinkiewicz. 1994. Building a large annotated corpus of English: The Penn treebank. Using Large Corpora.\\n\\nJoanna Materzy\u0144ska, Antonio Torralba, and David Bau. 2022. Disentangling visual and written concepts in CLIP. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nGeorge A. Miller. 1995. WordNet: a lexical database for English. Communications of the ACM.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Santosy, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. 2022. Hierarchical text-conditional image generation with CLIP latents. arXiv:2204.06125.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, et al. 2022. Photo-realistic text-to-image diffusion models with deep language understanding. arXiv:2205.11487.\\n\\nChristoph Schuhmann, Romain Beaumont, Cade W. Goodwin, Ross Wightman, Theo Coombes, et al. 2022. LAION-5B: An open large-scale dataset for training next generation image-text models.\"}"}
{"id": "acl-2023-long-310", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-310", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For all images, we ran the Stable Diffusion 2.0 base model (512 by 512 pixels) with 30 inference steps, the default 7.5 classifier guidance score, and the state-of-the-art DPM solver. We automatically filtered out all offensive images, against which the 2.0 model has both training-time and after-inference protection. We also steered clear of offensive prompts, which were absent to start with in COCO. Our computational environment consisted of PyTorch 1.11.0 and CUDA 11.4, running on Titan RTX and A6000 graphics cards. Our spaCy model was en_core_web_md.\\n\\nTo draw the ground-truth segmentation masks, we used the object selection tool, the quick selection tool, and the brush from Adobe Photoshop CC 2022 to fill in a black mask for each area corresponding to a present noun. We then exported each mask (without the background image) as a binary PNG mask and attached it to the relevant noun\u2014see Figure 10 for some examples.\\n\\nTwo trained annotators worked on the total set of 1000 image\u2013prompt pairs, with one completing 180 on each dataset and the other 320 on each.\\n\\nWe conducted ablation studies to see if summing across all time steps and layers, as in Eqn. 6, is necessary. We searched both sides of the summation: for one study, we restricted DAAM to $j \\\\leq j^*$, as $j^* = 1 \\\\rightarrow T$; for its dual study, we constrained $j \\\\geq j^*$.\\n\\nWe applied the same methods to layer resolution, 16x16, 32x32, 64x64. We present our results in Fig. 11, which suggests that all time steps and layers contribute positively to segmentation quality.\\n\\nWe also ran COCO-Gen experiments with pixel accuracy and dice score metrics, two less common ones in the segmentation literature. In terms of pixel accuracy, CLIPSeg attained 90%, DAAM-0.4 90%, and Mask2former 85%, which largely agrees with our mIoU$_\\\\infty$ findings. We conjecture that DAAM and Mask2former improve against CLIPSeg because pixel accuracy penalizes outliers less. For dice score, CLIPSeg achieved 72, DAAM-0.4 68, and Mask2former 30, which also agrees with our mIoU$_\\\\infty$ results. We conclude that, in addition to mIoU, both metrics support the use of DAAM.\\n\\nWe designed our annotation UIs for Amazon MTurk, a popular crowdsourcing platform, where we submitted jobs requiring three unique annotators at the master level to complete each task. We presented the UI pictured in Figure 12, asking them to rate the relevance of the red word to the highlighted area in the image. If the image was too poor or if the word was missing, they could also choose options 6 and 7.\\n\\nTo filter out low-quality or inattentive annotators, we randomly asked workers to interpret punctuation, such as periods. Since these tokens are self-evidently too abstract and missing in the image, we removed workers who didn't select one of those two options. However, we found overall attention to be high, having a reject rate of less than 2% of the tasks, consistent with Hauser and Schwarz\u2019s (2016) findings that MTurk users outperform subject pool participants. We show response statistics in Figure 13, where adpositions, coordinating conjunctions, participles, punctuation, and articles have high non-interpretable rates.\"}"}
{"id": "acl-2023-long-310", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We briefly conducted additional experiments within Section 3.2 using CLIPSeg to compare its attribution ability to DAAM. We find that DAAM significantly (p < 0.02; unpaired t-test) outperforms CLIPSeg on verbs, proper nouns, and adverbs, because CLIPSeg was unable to produce viable maps. No significant differences were noted on nouns and adjectives, which CLIPSeg can segment. Overall, DAAM outperforms CLIPSeg by 0.9 MOS points (3.4 vs 2.5). We conclude that, while CLIPSeg is plausible for some parts-of-speech, such as nouns, it is implausible for others.\\n\\nB Supplements for Syntactic Analyses\\n\\nMeasures of overlap.\\n\\nWe use three measures of overlap to characterize head\u2013dependent map interactions: mean intersection over union (mIoU), intersection over the dependent (mIoD), and intersection over the head (mIoH). When mIoU is high, the maps overlap greatly; when mIoD is high but mIoH is low, the head map occupies more of the dependent than the dependent does the head; when the opposite is true, the dependent occupies more.\\n\\n\\\\[\\n\\\\begin{align*}\\n&m_{\\\\text{IoU}} = \\\\frac{\\\\sum_{(x,y)} D_{\\\\tau(i_1)}[x,y] \\\\land D_{\\\\tau(i_2)}[x,y]}{\\\\sum_{(x,y)} D_{\\\\tau(i_1)}[x,y] \\\\lor D_{\\\\tau(i_2)}[x,y]}, \\\\\\\\\\n&m_{\\\\text{IoD}} = \\\\frac{\\\\sum_{(x,y)} D_{\\\\tau(i_1)}[x,y] \\\\land D_{\\\\tau(i_2)}[x,y]}{\\\\sum_{(x,y)} D_{\\\\tau(i_1)}[x,y]}, \\\\\\\\\\n&m_{\\\\text{IoH}} = \\\\frac{\\\\sum_{(x,y)} D_{\\\\tau(i_1)}[x,y] \\\\land D_{\\\\tau(i_2)}[x,y]}{\\\\sum_{(x,y)} D_{\\\\tau(i_2)}[x,y]}\\n\\\\end{align*}\\n\\\\]\\n\\nWe visually present our mIoD and mIoH statistics in Figure 14. To compute mIoU, we compute mIoU (Eqn. 8) without restricting ourselves to the typical 80 COCO classes. For mIoU, we only look at objects with one of those labels.\"}"}
{"id": "acl-2023-long-310", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C Supplements for Semantic Analyses\\n\\nSemantic relation ontology. We present our relation ontology below, continued on the next page:\\n\\n- **[ROOT]**\\n- **[BAG]**\\n  - backpack\\n  - handbag\\n  - suitcase\\n- **[FOOD]**\\n  - **[BAKED GOODS]**\\n    - cake\\n    - donut\\n  - **[DISH]**\\n    - hot dog\\n    - pizza\\n    - sandwich\\n- **[FRUIT]**\\n  - apple\\n  - banana\\n  - orange\\n- **[ELECTRICAL DEVICE]**\\n  - **[APPLIANCE]**\\n    - oven\\n    - refrigerator\\n    - toaster\\n  - **[MONITOR DEVICE]**\\n    - cell phone\\n    - laptop\\n    - tv\\n- **[FURNITURE]**\\n  - bench\\n  - chair\\n  - couch\\n- **[MAMMAL]**\\n  - **[FARM ANIMAL]**\\n    - cow\\n    - horse\\n    - sheep\\n  - **[PETS]**\\n    - cat\\n    - dog\\n  - **[WILD ANIMAL]**\\n    - bear\\n    - elephant\\n    - giraffe\\n    - zebra\\n\\n- **[ROOT]**\\n  - **[KITCHENWARE]**\\n    - **[CUTLERY]**\\n      - fork\\n      - knife\\n      - spoon\\n    - **[VESSEL]**\\n      - bowl\\n      - cup\\n  - **[SPORTS]**\\n    - skateboard\\n    - snowboard\\n    - surfboard\\n  - **[VEHICLE]**\\n    - **[AUTOMOBILE]**\\n      - bus\\n      - car\\n      - truck\\n    - **[BIKE]**\\n      - bicycle\\n      - motorcycle\\n\\nFigure 15: The annotation UI for cohyponym entanglement, asking annotators to pick the present objects.\\n\\n**Cohyponym annotation process.** Similar to the generalized attribution annotation process, we designed our UIs for Amazon MTurk. We submitted a job requiring three unique annotators at the master level to complete each task. We presented to them the UI shown in Figure 15. We manually verified each response, removing workers whose quality was consistently poor. This included workers who didn't include all objects generated. Overall, the worker quality was exceptional, with a reject rate below 2%. Out of a pool of 30 workers, no single worker annotated more than 16% of the examples.\"}"}
{"id": "acl-2023-long-310", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For every submission:\\n\\nA1. Did you describe the limitations of your work?\\n\\nA2. Did you discuss any potential risks of your work?\\n\\nA3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n\\nA4. Have you used AI writing assistants when working on this paper?\\n\\nDid you use or create scientific artifacts?\\n\\nB1. Did you cite the creators of artifacts you used?\\n\\nB2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n\\nB3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n\\nB4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\n\\nB5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n\\nB6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nDid you run computational experiments?\\n\\nC1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-310", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nError bars in Sec 4, significance tests throughout 3-5.\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nAppendix A.1\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nAppendix A.2\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nAppendix A.2\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nIt wasn't necessary due to the simplicity of the task.\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nIt was determined exempt following research ethics board approval procedure: https://uwaterloo.ca/research/sites/ca.research/files/uploads/files/research_or_quality_assurance_decision_tree.pdf\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nIt wasn't available.\"}"}
