{"id": "acl-2024-long-799", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Latxa: An Open Language Model and Evaluation Suite for Basque\\n\\nJulen Etxaniz* Oscar Sainz* Naiara Perez* Itziar Aldabe German Rigau\\n\\nEneko Agirre Aitor Ormazabal Mikel Artetxe Aitor Soroa\\n\\nHiTZ Center - Ixa, University of the Basque Country UPV/EHU\\n\\n{julen.etxaniz,a.soroa}@ehu.eus\\n\\nAbstract\\n\\nWe introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses.\\n\\n1 Introduction\\n\\nMotivated by their increasing training cost and commercial interest, the development of Large Language Models (LLMs) has been led by close initiatives like GPT (OpenAI et al., 2023), Claude (Wu et al., 2023) and Gemini (Team, 2023). In recent times, a more open ecosystem has emerged following the release of various competitive models like Llama 2 (Touvron et al., 2023) and Mistral (Jiang et al., 2024). However, despite early efforts to build open multilingual models (Lin et al., 2022; Scao et al., 2023), the most competitive ones are notoriously English-centric. As shown in Table 1, all these open models perform poorly in low-resource languages like Basque, with most results marginally surpassing random chance.\\n\\nIn this work, we present Latxa, an open family of LLMs for Basque that substantially outperforms all these previous models. Basque is an agglutinative language written in Latin script and with no known relatives, although a significant part of the vocabulary is shared with contact languages like Spanish and French. Basque is the 52th language in Common Crawl, with 0.035% of the total content \u2013 for reference, English is the 1st language with 46% of the content and Spanish is the 5th with 4.6%. Our work builds on various open resources and models that we further expand to Basque, highlighting the importance of an open ecosystem for the development of language technology for low-resource languages. In particular, our models are based on Llama 2, which we continue training in Basque using a new corpus with 4.3M documents from 4 existing and 3 new sources. In addition, we release 4 diverse and challenging multiple-choice benchmarks comprising a total of 23,282 questions, covering language proficiency, reading comprehension, trivia questions, and public examinations.\\n\\nAs shown in Table 1, Latxa performs substantially better than all existing open models, with the 70B variant outperforming the previous best open model (Yi 34B) by 18.95 points in average. In addition, it also outperforms the Llama 2 model it is based on by 25.18 points, and it is also superior to GPT-3.5 Turbo in all datasets we evaluate on. Interestingly, our best model also outperforms GPT-4 Turbo in language proficiency exams (EusProf), despite lagging behind in reading comprehension and knowledge-intensive tasks. This suggests that the capabilities that an LLM exhibits in a given language are not determined by its linguistic competence in this particular language, opening the doors to further improvements in low-resource LLMs as\"}"}
{"id": "acl-2024-long-799", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"This paper makes the following contributions:\\n\\n1. We release a high-quality corpus for Basque, comprising 4.3M documents and 4.2B tokens. The corpus combines the EusCrawl v1.1, Egunkaria, Booktegi, Wikipedia, CulturaX, Colossal OSCAR and HPLT v1 datasets (the first 3 being new), which we carefully deduplicate and filter.\\n\\n2. We release the Latxa family of Basque LLMs, comprising 3 models with 7B, 13B and 70B parameters.\\n\\n3. We release 4 new multiple-choice benchmarks for Basque: EusProficiency (official language proficiency exams), EusReading (reading comprehension), EusTrivia (trivia questions from 5 knowledge areas), and EusExams (public examinations).\\n\\n4. We present extensive experiments comparing Latxa to previous open and closed models.\\n\\n5. We show that it is possible to train significantly stronger LLMs for low-resource languages building on the existing ecosystem of open models and resources. In a similar spirit to other open LLMs, such as Pythia (Biderman et al., 2023), LLM360 (Liu et al., 2023) and OLMO (Groeneveld et al., 2024), we release all the necessary data, code, weights and documentation to run and evaluate our models, facilitating similar efforts for other low-resource languages.\\n\\n### Training Corpora\\n\\nOur training corpus combines various existing datasets, as well as some new ones that we release with this work. We have prioritized quality over quantity when constructing our corpus, prioritizing high-quality data sources and applying a thorough deduplication and filtering process. We next describe our data sources in \u00a72.1, followed by our preprocessing pipeline in \u00a72.2. Table 2 summarizes the statistics of the resulting dataset.\\n\\n#### 2.1 Data Sources\\n\\n- **EusCrawl v1.1.** The original version of EusCrawl (Artetxe et al., 2022) was built using ad-hoc scrapers to extract text from 33 newswire websites, resulting in higher quality documents compared to general-purpose approaches. In this work, we release an updated version of EusCrawl, including new content up to November 2023. This increases the number of unique documents from 1.38 to 1.94 millions, for a total of 384 million words.\\n\\n- **Egunkaria.** Euskaldunon Egunkaria was a daily newspaper written fully in the Basque language. The corpus includes approximately 176k news articles, editorials, and various types of reviews from the years 2001 to 2006, totalling 39 million words.\\n\\n- **Booktegi.** The Booktegi platform hosts free content in Basque, such as books, interviews, and audio materials. The corpus comprises approximately 3 million words from 166 EPUB books covering essays, fiction, and poetry.\\n\\n- **Wikipedia.** We download and process a Basque Wikipedia dump, obtaining nearly 550k documents and more than 54 million words. The plain text has been extracted with WikiExtractor.\\n\\n### Table 1: Main results.\\n\\nThe best results in each compute class are in bold. Best overall results are underlined.\\n\\n| Model      | 7B        | 13B       | 70B       |\\n|------------|-----------|-----------|-----------|\\n| GPT-3.5 Turbo | n/a       | \u2013         | 57.33     |\\n| GPT-4 Turbo  | n/a       | \u2013         | 90.67     |\\n| XGLM 7B      | 57.71     | 23.88     | 41.47     |\\n| BLOOM 7B     | 57.18     | 27.00     | 40.17     |\\n| Mistral 7B   | 51.09     | 38.89     | 39.22     |\\n| Llama 2 7B   | 50.43     | 26.22     | 38.20     |\\n| Latxa 7B     | 65.45     | 37.33     | 52.56     |\\n| mGPT 13B     | 55.39     | 25.00     | 37.56     |\\n| Llama 2 13B  | 50.63     | 32.00     | 38.98     |\\n| Latxa 13B    | 66.51     | 53.89     | 53.36     |\\n| Yi 34B       | 52.22     | 54.56     | 43.90     |\\n| Llama 2 70B  | 51.62     | 33.56     | 42.55     |\\n| Latxa 70B    | 70.55     | 71.67     | 59.74     |\\n\\nTable 2: Summary of dataset statistics.\\n\\n- Total number of unique documents: 4,300,000\\n- Total number of tokens: 4,200,000,000\\n- Number of documents: 4,300,000\\n- Number of tokens: 4,200,000,000\\n\\n2 The 20231101 dump corresponding to November 2023.\\n3 https://github.com/attardi/wikiextractor\"}"}
{"id": "acl-2024-long-799", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Data sources and statistics at each preprocessing stage. \\\"Toks\\\" are Llama 2 tokens.\\n\\nCulturaX. CulturaX (Nguyen et al., 2023) is a large multilingual dataset resulting from the combination and processing of mC4 (Xue et al., 2021) and the four OSCAR releases 2019, 21.09, 22.01, and 23.01 (Ortiz Su\u00e1rez et al., 2019). These corpora originate, in turn, from 66 Common Crawl (CC) snapshots spanning from 2015 to 2022. Basque content constitutes 0.02% of CulturaX, encompassing nearly 1.60 million documents and 622 million words.\\n\\nColossal OSCAR. The largest release of the OSCAR project (Ortiz Su\u00e1rez et al., 2019) to date, Colossal OSCAR 1.0, is based on 10 CC snapshots. Here, we use the two snapshots not covered by CulturaX, namely, 06-07-22 and 05-06-23. Additionally, we have had access to an OSCAR-processed CC snapshot from April 2023. In total, we have obtained almost 650k documents in Basque from these datasets, totalling 282 million words.\\n\\nHPLT v1. The High Performance Language Technologies project (HPLT; Aulamo et al., 2023) compiled another massive, multilingual dataset from the Internet Archive and CC. In this work, we use the first release, which contains 2.29 million documents (1.55 billion words) in Basque. It must be noted that, unlike the aforementioned web-based sources, the HPLT dataset was released without any deduplication or filtering. Consequently, our preprocessing approach has been particularly aggressive with this dataset (see \u00a72.2).\\n\\n2.2 Preprocessing\\n\\nWe used the Dolma toolkit (Soldaini et al., 2024) and Corpus Cleaner v2 (CCv2; Palomar-Giner et al., 2024) to normalize, deduplicate and filter the datasets. Since the majority of our data sources were intentionally selected, organized, and/or curated by their respective authors, our main focus has been on removing outliers and cross-dataset duplicates. This process is briefly outlined below, with further details available in Appendix A. The final size of the processed corpus is shown in Table 2. In total, it amounts to 1.22B words and 4.17B Llama 2 tokens. Each dataset is shuffled and then split separately into testing (1%), development (1%) and training (98%) example sets.\\n\\nNormalization. CCv2 is first used to fix document encoding and whitespace normalization.\\n\\nDeduplication. Cross-dataset document repetitions are identified and removed at both the URL and document content levels. Specifically, we conduct near-deduplication with Bloom filters (Bloom, 1970) as implemented in Dolma. To maximise corpus quality, we prioritized content from well-curated sources (Wikipedia, EusCrawl, Egunkaria and Booktegi) then from massive but comparatively cleaner sources (CulturaX and Colossal OSCAR) over HPLT (see Figure 2(a) in Appendix A). The latter undergoes additional deduplication at the paragraph level.\\n\\nFiltering. Documents unlikely to contain quality content are identified and removed in two stages. First, a combination of heuristics from Gopher (Rae et al., 2022) and C4 (Raffel et al., 2020), with adaptations tailored to the Basque language is applied (e.g., regarding average word length). We also perform language identification with CLD2 through Dolma, which has predominantly impacted HPLT, with approximately one-third of this corpus being discarded at this stage. Finally, the corpora are processed with CCv2, which assigns an aggregated quality score per document based on a comprehensive set of taggers. Once again, HPLT has been affected most, resulting in a further 25% reduction in document counts for this dataset.\"}"}
{"id": "acl-2024-long-799", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We train 7B, 13B and 70B models following a continued pretraining approach. To that end, we use Llama 2 as the base model (Touvron et al., 2023), and continue training it using the corpus described in \u00a72. To mitigate catastrophic forgetting from the original model, we also include English data in the continued pretraining stage. For that purpose, we use 500k random documents from The Pile (Gao et al., 2020), totaling 0.9B tokens.\\n\\n3.1 Pretraining Details\\nThe training of Latxa has been conducted using the GPT-Neox (Andonian et al., 2023) library. As infrastructure, we have leveraged the CINECA HPC Leonardo computing cluster located in Italy, which is powered by 3,456 nodes each containing 4x custom A100 64GB GPUs. The models are trained for 10k steps with a sequence length of 4,096 tokens and an effective batch size of 1M tokens, resulting in a total of 10B tokens. We use a cosine learning rate schedule, with a warm-up of 500 steps and decaying down to 3% of the peak learning rate. We set up the peak learning rate to be $1 \\\\times 10^{-4}$.\\n\\nAll other hyperparameters follow Touvron et al. (2023). Figure 1 shows the validation perplexity during training.\\n\\n3.2 Carbon Emissions\\nPretraining LLMs requires compute-expensive experiments, carrying a significant carbon footprint. The 7B, 13B and 70B models were trained on 32, 64 and 256 GPUs, respectively. We report the compute hours and power consumption involved in our experiments in Table 3. The carbon emitted was estimated using a GPU power consumption of 440 W and a carbon efficiency of 0.297 kg/kWh (carbon efficiency on Italy on February 9, 2024, according to ElectricityMaps).\\n\\n| Size (GPU Hours) | Time Carbon Emitted |\\n|-----------------|--------------------|\\n| 7B 952.53h      | 124.47kg CO\u2082 eq   |\\n| 13B 2,518.0h    | 329.06kg CO\u2082 eq   |\\n| 70B 30,266.0h   | 3,955.17kg CO\u2082 eq |\\n| Total 33,636.5h | 4,408.7kg CO\u2082 eq  |\\n\\nTable 3: Carbon footprint of training different models\\n\\n4 New Evaluation Datasets\\nTo overcome the scarcity of Basque benchmarks that are suitable for evaluating base language models, we collect new evaluation data from various online games and tests. We have decided to take this approach instead of translating existing datasets to avoid translation artifacts (Artetxe et al., 2020). Most importantly, this allows to have localized datasets that test the models' knowledge about topics that are most relevant for Basque speakers. These tasks cover language proficiency (EusProficiency), reading comprehension (EusReading), trivia questions (EusTrivia), and exams of advanced professional level (EusExams). All the datasets consist of multiple-choice questions, making them suitable for few-shot learning akin to MMLU (Hendrycks et al., 2021) in English. We next describe each dataset in more detail, while Table 4 summarizes their statistics. For examples of each task, see Table 9 in Appendix C.\\n\\nEusProficiency.\\nEusProficiency comprises 5,169 exercises on different topics from past EGA exams, the official C1-level certificate of proficiency in Basque. We have collected the atarikoa exercises from EGA exams through the years 1998 to 2008. Atarikoa is the first qualifying test of EGA, which measures different aspects of language competency, such as reading comprehension, grammar, vocabulary, spelling, and writing. Each test generally has 85 multiple-choice questions, with 4 choices and a single correct answer. Currently, there is no comparable dataset available, nor could one be obtained by translating existing analogous datasets from other languages.\\n\\n5https://www.electricitymaps.com/\"}"}
{"id": "acl-2024-long-799", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EusReading consists of 352 reading comprehension exercises (irakurmena) sourced from the same set of past EGA exams. Each test generally has 10 multiple-choice questions, with 4 choices and a single correct answer. These exercises are more challenging than Belebele (Bandra et al., 2023) due to the complexity and length of the input texts (see Table 4). As a result, EusReading is useful to measure long context understanding of models.\\n\\nEusTrivia consists of 1,715 trivia questions from multiple online sources. 56.3% of the questions are elementary level (grades 3-6), while the rest are considered challenging. A significant portion of the questions focuses specifically on the Basque Country, its language, and culture. Each multiple-choice question contains two, three or four choices (3.84 on average) and a single correct answer. Five areas of knowledge are covered:\\n\\n- **Humanities and Natural Sciences** (27.8%): This category encompasses questions about history, geography, biology, ecology and other social and natural sciences.\\n- **Leisure and Art** (24.5%): This category includes questions on sports and athletes, performative and plastic arts and artists, architecture, cultural events, and related topics.\\n- **Music** (16.0%): Here are grouped all the questions about music and musicians, both classical and contemporary.\\n- **Language and Literature** (17.1%): This category is concerned with all kinds of literature productions and writers, as well as metalinguistic questions (e.g., definitions, synonyms, and word usage).\\n- **Mathematics and ICT** (14.5%): This category covers mathematical problems and questions about ICT, as well as questions about people known for their contributions to these fields of knowledge.\\n\\nEusExams is a collection of tests designed to prepare individuals for Public Service examinations conducted by several Basque institutions, including the public health system Osakidetza, the Basque Government, the City Councils of Bilbao and Gasteiz, and the University of the Basque Country (UPV/EHU). Within each of these groups, there are different exams for public positions, such as administrative and assistant roles. Each multiple-choice question contains 2 to 4 choices and one correct answer. The dataset is mostly parallel with 16k questions in Basque and 18k in Spanish, from which we only consider the Basque subset. It could be said to be similar to MMLU's professional-level questions, but focusing on knowledge relevant to the Basque community, with questions related to local public services and law.\\n\\n### Table 4: Evaluation dataset statistics\\n\\n| Dataset       | Number of Examples | Average Input Length (Chars) | Average Output Length (Chars) | Number of Choices |\\n|---------------|--------------------|------------------------------|------------------------------|-------------------|\\n| EusReading    | 352                | 5,340                        | 2-4                          | 67                |\\n| EusTrivia     | 1,715              | 55                           | 2-4                          | 14                |\\n| EusExams      | 16,774             | 115                          | 4                            | 63                |\\n| XStoryCloze   | 1,511              | 202                          | 2                            | 44                |\\n| Belebele      | 900                | 584                          | 4                            | 28                |\\n| BEC           | 1,302              | 97                           | 3                            |                   |\\n| BHTCv1        | 1,854              | 265                          | 12                           |                   |\\n| Korref        | 587                | 275                          | 2                            |                   |\\n| QNLI eu       | 238                | 158                          | 2                            |                   |\\n| VaxxStance    | 312                | 209                          | 3                            |                   |\\n| WiC eu        | 1,400              | 375                          | 2                            |                   |\\n\\nBasqueGLUE tasks do not have output length because they are classification tasks.\"}"}
{"id": "acl-2024-long-799", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"cent models, but are trained on fewer tokens and exhibit generally weaker performance. Finally, we tested the latest GPT-3.5 Turbo (gpt-3.5-turbo-0125) and GPT-4 Turbo (gpt-4-1106-preview) for BasqueGLUE tasks and gpt-4-0125-preview for the rest, as they are the leading commercial models for Basque.\\n\\n5.2 Evaluation Datasets\\nIn addition to the new evaluation datasets introduced in \u00a74, the models have been evaluated on the following benchmarks:\\n\\n- Belebele (Bandarkar et al., 2023): A multiple-choice reading comprehension dataset spanning 122 language variants.\\n- XStoryCloze (Lin et al., 2022): A professionally translated version of the StoryCloze dataset (Mostafazadeh et al., 2017) to 10 non-English languages. StoryCloze is a common-sense reasoning dataset that consists in choosing the correct ending to a four-sentence story.\\n- BasqueGLUE (Urbizu et al., 2022): A collection of 6 NLU datasets for Basque: sentiment analysis (BEC), stance detection (VaxxS-tance), topic classification (BTHCv2), coreference detection (EpecKorrefBin), question-answering NLI (QNLI eu), and word-in-context (WiC eu).\\n\\nCollectively, these datasets allow us to evaluate the performance of the models on a wide range of competences including world knowledge, linguistic knowledge, reading comprehension, and common sense reasoning.\\n\\nFollowing previous work (Brown et al., 2020; Touvron et al., 2023), we check for n-gram overlaps between these evaluation datasets and Latxa\u2019s training corpus, and find no evidence of wholesale or annotation contamination (Chowdhery et al., 2023; Sainz et al., 2023). Further information on our contamination study can be consulted in Appendix B.\\n\\n5.3 Evaluation Framework\\nThe models are evaluated using the LM Evaluation Harness library (Biderman et al., 2024) by Eleuther AI. To that end, we have implemented the new evaluation datasets following similar multiple-choice datasets that are already included in the library, such as Belebele. The specific prompts and examples for each task are reported in Table 9 in Appendix C. BasqueGLUE has also been implemented as a generative evaluation dataset (see Table 10).\\n\\nWe use 5 in-context examples for all tasks but two: following common practice, XStoryCloze is evaluated in a 0-shot setting, and EusReading is evaluated in a 1-shot fashion, as more examples would not fit into the context of most models. In all cases, we compute the log probabilities of all candidates, and pick the one with the highest score as the models\u2019 final answer.\\n\\nFor GPT models, we have implemented the evaluation using the OpenAI API. We have kept the evaluation as similar as possible to allow a fair comparison with our models. As getting log probabilities of candidate XStoryCloze continuations from the API is not possible, we have decided not to evaluate GPT in that task. For few-shot tasks, we use the same prompts and provide few-shot examples as user and assistant messages. In addition, we use a system prompt in English to specify the set of candidate answers per task (see Appendix C).\\n\\n6 Results\\nWe report our main results in Table 1, while Table 5 and Table 6 report fine-grained results on the different subsets of EusTrivia and BasqueGLUE, respectively. In what follows, we summarize our main findings:\\n\\nEffectiveness of continued pretraining. Latxa obtains the best results in each compute class, outperforming all previous open models by a large margin. As the only exception, Mistral 7B is better.\"}"}
{"id": "acl-2024-long-799", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: BasqueGLUE results by task. *VaxxStance is measured in terms of macro-average F1-score of the categories IN FAVOUR and AGAINST.\\n\\n\u2020 BERTeus and ElhBERTeu are fine-tuned encoders.\\n\\nthan Latxa 7B on Belebele and EusReading, but Latxa 7B wins in all the other 5 datasets. Our best model obtains 61.08 points on average, outperforming the previous best open model by 18.95 points. In addition, it outperforms the Llama 2 model it is based on by 25.18 points, confirming the effectiveness of continued pretraining to build language models for low-resource languages.\\n\\nOpen vs. closed models. With the exception of EusProficiency, the best results are obtained by GPT-4 Turbo, a closed commercial system. The difference between GPT-4 Turbo and the previous best open model (Yi) is abysmal. For instance, GPT-4 Turbo is 30.55 points better than Yi on EusTrivia, while the latter is only 16.02 points better than random chance. This can partly be attributed to previous open initiatives being primarily English-centric. While Latxa substantially narrows this gap, we believe that future research on open models should pay more attention to low-resource languages.\\n\\nEnglish-centric vs. multilingual models. The 3 multilingual models we evaluate (XGLM, BLOOM and mGPT) do better than all English-centric models on XStoryCloze, which requires linguistic competence in Basque and is evaluated in a zero-shot fashion. However, English-centric open models do generally better on other tasks, presumably due to their better in-context learning capabilities and general knowledge captured. Consistent with our previous point, this suggests that existing open models are either English-centric and struggle in low-resource languages like Basque, or are multilingual and significantly lag behind the English-centric models in language-agnostic capabilities.\\n\\nImpact of scale. We find that larger Latxa models obtain substantially better results: the 70B model is 10.99 points better than the 13B model on average, which is 9.14 points better than the 7B model. This transcends conventional scaling laws, which establish that, when pretraining models from scratch in low-resource scenarios, the performance is bottlenecked by the amount of training data rather than the model size (Kaplan et al., 2020). However, our results show a different picture for continued pretraining, where bigger and stronger base models result in better performance despite the limited pretraining data in the target language. This suggests that even better results could be obtained through continued pretraining as stronger English-centric models become available, which is encouraging for low-resource languages.\\n\\nGeneral vs. language-specific knowledge. We find evidence that Latxa is particularly strong in tasks that test for proficiency in the Basque language. In particular, Latxa obtains the best results on Basque language proficiency exams (EusProficiency), despite lagging behind GPT-4 Turbo in the rest of the tasks. Similarly, Latxa outperforms...\"}"}
{"id": "acl-2024-long-799", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GPT-4 Turbo on the Language & Literature subset of EusTrivia, even if GPT-4 Turbo is superior in the rest of the categories (Table 5). This suggests that Latxa is more proficient than GPT-4 Turbo in Basque, but the latter does better in most tasks due to its stronger general capabilities. Another evidence of this is that Latxa is particularly weak in the Maths & ICT subset of EusTrivia, where it even lags behind Yi, while GPT-4 Turbo is particularly strong in this category. This is likely because most problems in this category can be understood with basic knowledge of Basque, but solving them may require more complex mathematical reasoning. This suggests that the general capabilities of language models are highly language-agnostic which, in line with our previous finding, suggests that stronger English-centric models can lead to stronger models for low-resource languages by using the same continued pretraining recipe.\\n\\nClassical NLP tasks. Table 6 reports fine-grained results on BasqueGLUE, which comprises various classical NLP tasks like topic classification and coreference detection. In addition to our usual set of decoder-only models evaluated in a few-shot fashion, we report results for BERTeus (Agerri et al., 2020) and ElhBERTeu (Urbizu et al., 2023), which are encoder-only models that were fine-tuned specifically on these tasks. The best results are obtained by these specialized encoder-only models, which shows that the traditional pretraining/fine-tuning paradigm with BERT-style models is still competitive for classical NLP tasks. The only exception is EpecKorrefBin, where both GPT-4 Turbo and Latxa 70B perform substantially better than the fine-tuned encoder-only models. In future work, we would like to explore fine-tuning Latxa and other decoder-only models on these tasks.\\n\\n7 Related Work\\n\\nThe amount of documents per language in Common Crawl is helpful to organize the literature. There is no agreed-upon definition of low-resource language, so we set five arbitrary buckets of languages following a logarithmic distribution: high bucket with just English (rank 1, 46% of latest crawl), high-medium languages around Spanish (rank 5, 4.6%), medium around Danish (rank 24, 0.46%), low resource around Nepalese (rank 46, 0.044 CC-MAIN-2024-22 crawl at https://commoncrawl.github.io/cc-crawl-statistics/plots/languages), and very-low around Somali (rank 81, 0.0046%). Basque would be low-resource (rank 52, 0.035%). While there were some early efforts to build open multilingual language models like XGLM (Lin et al., 2022), BLOOM (Scao et al., 2023), mGPT (Shliazhko et al., 2024) and the translation oriented MADLAD-400 (Kudugunta et al., 2023), their performance significantly lags behind more recent English-centric models like Llama 2 (Touvron et al., 2023) or Mistral (Jiang et al., 2023). Concurrent to our work, \u00dcst\u00fcn et al. (2024) present Aya, a fine-tuned mT5 encoder-decoder (Xue et al., 2021), which has been instruction-tuned and supports 101 languages.\\n\\nIn the case of high-medium-resource languages, different teams have focused on building models from scratch. Ekgren et al. (2022) built a Swedish 20B model which was favorably evaluated on perplexity, followed by (Ekgren et al., 2024), which builds a 40B model for five Nordic languages, including low-resource Danish and Faroese, English and code. It was evaluated on perplexity for the three richest languages. Faysse et al. (2024) present a French-English bilingual model trained with the same number of tokens for each language, although the amount of text for French (376M documents) is half the English text. The authors stress the fact that the tokenizer should not be biased towards any of the two languages. The results on French (and English) LLM evaluation benchmarks show that their largest model (1.2B parameters) underperforms both 3B Llama 2 and Mistral models.\\n\\nRegarding medium-resource, Luukkonen et al. (2023) focus on Finnish. Using a 19B word corpus they trained several models from scratch, ranging from 186M to 13B parameters. As an alternative, they also continued pretraining a multilingual model, the 176B BLOOM. The evaluation on FIN-bench, a version of Big-Bench (Srivastava et al., 2023), shows the 13B model underperforms the 7B model, while the continued pretraining model obtains the best results by a large margin. In follow-up work, Luukkonen et al. (2024) train a 40B bilingual model from scratch with a larger Finnish corpus, and obtain the best results in FIN-bench. They do not compare the results to commercial models. The results suggest that monolingual models trained from scratch saturate at relatively small sizes, and multilingual or continued pretraining would be the best option for these languages.\\n\\nFocusing on very-low-resource languages,\"}"}
{"id": "acl-2024-long-799", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yong et al. (2023) compare different approaches to extend the 176B BLOOM model from 46 languages to new languages unseen at training, including Guarani (rank 116, 0.0006%) and seven larger languages for which they sample 100K documents at most. Model sizes range from 0.56B to 7.1B parameters. They report good results for adapters on zero-shot benchmarks. In concurrent work, Lin et al. (2024) present MaLA-500, a continued pretrained model using LORA based on Llama 2 that supports 500 languages. MaLA seems to improve Llama 2 on low to very-low resource languages, but degrades in some medium languages already covered in Llama 2. Evaluation is based on a variant of perplexity and text classification.\\n\\nPrevious work on low-resource languages has been done in the context of multilingual models (see above). In all cases, evaluation is based on perplexity and/or some readily available datasets, and does not include native benchmarks designed to evaluate base models.\\n\\n8 Conclusion and Future Work\\n\\nIn this work, we present a new open framework for the development and evaluation of LLMs in Basque. The framework includes Latxa, a set of state-of-the-art generative LLMs that have been built by continuing to pretrain the Llama 2 7B, 13B and 70B models in Basque. The pretraining dataset is the largest public dataset available to date and includes data from carefully curated sources, as well as content derived from automatically filtered versions of Common Crawl. After preprocessing and deduplication, the released corpora comprise 1.22B words and 4.17B tokens. We also present 4 new evaluation datasets, collectively the largest evaluation benchmark for Basque that allows assessing the knowledge of the models about the Basque language and culture.\\n\\nThe Latxa models outperform all previous open models and GPT-3.5 Turbo, but they still lag behind GPT-4 Turbo in most benchmarks. Interestingly, Latxa 70B outperforms GPT-4 Turbo on EusProficiencia and Language & Literature EusTrivia questions, suggesting that the capabilities of LLMs in a particular language are not determined by their linguistic competence in this language. This, along with the effectiveness of scale, suggests that applying the exact same continued pretraining recipe could lead to better Basque models as stronger English-only models become available.\\n\\nIn the future, we plan to extend the training dataset by gathering quality content from diverse Basque sources such as publishers or media, as well as building evaluation datasets to assess aspects such as truthfulness or hallucinations. We also plan to further tune Latxa to follow instructions, which should improve the overall capabilities of our models.\\n\\nLimitations\\n\\nTo alleviate the potentially disturbing or harmful content, Latxa has been trained on carefully selected and processed data which comes mainly from local media, national/regional newspapers, encyclopedias and blogs. Still, the model is based on Llama 2 models and can potentially carry the same biases, risks and limitations.\\n\\nLatxa models are pretrained LLMs without any task-specific or instruction fine-tuning. The model was not fine-tuned to follow instructions or to work as a chat assistant, therefore, this kind of usage is not tested nor recommended. That is, the model can either be prompted to perform a specific task or further fine-tuned for specific use cases.\\n\\nAcknowledgements\\n\\nThis work has been partially supported by the Basque Government (Research group funding IT-1805-22 and IKER-GAITU project), the Spanish Ministry for Digital Transformation and of Civil Service, and the EU-funded NextGenerationEU Recovery, Transformation and Resilience Plan (ILENIA project, 2022/TL22/0021533). The models were trained on the Leonardo supercomputer at CINECA under the EuroHPC Joint Undertaking, project EHPC-EXT-2023E01-013. Julen Etxaniz and Oscar Sainz hold a PhD grant from the Basque Government (PRE_2023_2_0060 and PRE_2023_2_0137, respectively).\\n\\nReferences\\n\\nRodrigo Agerri, I\u00f1aki San Vicente, Jon Ander Campos, Ander Barrena, Xabier Saralegi, Aitor Soroa, and Eneko Agirre. 2020. Give your text representation models some love: the case for Basque. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 4781\u20134788, Marseille, France. European Language Resources Association.\"}"}
{"id": "acl-2024-long-799", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yu, Peng Liu, Qiang Liu, Shawn Yue, Senbin Yang, Shiming Yang, Tao Yu, Wen Xie, Wenhao Huang, Xiaohui Hu, Xiaoyi Ren, Xinyao Niu, Pengcheng Nie, Yuchi Xu, Yudong Liu, Yue Wang, Yuxuan Cai, Zhenyu Gu, Zhiyuan Liu, and Zonghong Dai. 2024. Yi: Open foundation models by 01.AI. arXiv preprint arXiv:2403.04652.\\n\\nAlex Andonian, Quentin Anthony, Stella Biderman, Sid Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh Levy-Kramer, Connor Leahy, Lucas Nestler, Kip Parker, Michael Pieler, Jason Phang, Shivanshu Purohit, Hailey Schoelkopf, Dashiell Stander, Tri Songz, Curt Tigges, Benjamin Th\u00e9rien, Phil Wang, and Samuel Weinbach. 2023. GPT-NeoX: Large scale autoregressive language modeling in PyTorch.\\n\\nMikel Artetxe, Itziar Aldabe, Rodrigo Agerri, Olatz Perez-de Vi\u00f1aspre, and Aitor Soroa. 2022. Does corpus quality really matter for low-resource languages? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7383\u20137390, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nMikel Artetxe, Gorka Labaka, and Eneko Agirre. 2020. Translation artifacts in cross-lingual transfer learning. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7674\u20137684, Online. Association for Computational Linguistics.\\n\\nMikko Aulamo, Nikolay Bogoychev, Shaoxiong Ji, Graeme Nail, Gema Ram\u00edrez-S\u00e1nchez, J\u00f6rg Tiedemann, Jelmer van der Linde, and Jaume Zaragoza. 2023. HPLT: High performance language technologies. In Proceedings of the 24th Annual Conference of the European Association for Machine Translation, pages 517\u2013518, Tampere, Finland. European Association for Machine Translation.\\n\\nLucas Bandarkar, Davis Liang, Benjamin Muller, Mikel Artetxe, Satya Narayan Shukla, Donald Husa, Naman Goyal, Abhinandan Krishnan, Luke Zettlemoyer, and Madian Khabsa. 2023. The Belebele benchmark: a parallel reading comprehension dataset in 122 language variants. arXiv preprint arXiv:2308.16884.\\n\\nStella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O\u2019Brien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. 2023. Pythia: A suite for analyzing large language models across training and scaling. In International Conference on Machine Learning, pages 2397\u20132430. PMLR.\\n\\nStella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi, Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPofi, Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa Jaiswal, Wilson Y. Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata, Fran\u00e7ois Yvon, and Andy Zou. 2024. Lessons from the trenches on reproducible evaluation of language models. arXiv preprint arXiv:2405.14782.\\n\\nBurton H. Bloom. 1970. Space/time trade-offs in hash coding with allowable errors. Commun. ACM, 13(7):422\u2013426.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 1877\u20131901. Curran Associates, Inc.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2023. PaLM: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1\u2013113.\"}"}
{"id": "acl-2024-long-799", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-799", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-799", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. 2021. mT5: A massively multilingual pre-trained text-to-text transformer. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 483\u2013498, Online. Association for Computational Linguistics.\\n\\nZheng Xin Yong, Hailey Schoelkopf, Niklas Muenighoff, Alham Fikri Aji, David Ifeoluwa Adelani, Khalid Almubarak, M Saiful Bari, Lintang Sutawika, Jungo Kasai, Ahmed Baruwa, Genta Winata, Stella Biderman, Edward Raff, Dragomir Radev, and Vasilina Nikoulina. 2023. BLOOM+1: Adding language support to BLOOM for zero-shot prompting. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 11682\u201311703, Toronto, Canada. Association for Computational Linguistics.\\n\\nAhmet \u00dcst\u00fcn, Viraat Aryabumi, Zheng-Xin Yong, Wei-Yin Ko, Daniel D\u2019souza, Gbemileke Onilude, Neel Bhandari, Shivalika Singh, Hui-Lee Ooi, Amr Kayid, Freddie Vargus, Phil Blunsom, Shayne Longpre, Niklas Muennighoff, Marzieh Fadaee, Julia Kreutzer, and Sara Hooker. 2024. Aya model: An instruction finetuned open-access multilingual language model. arXiv preprint arXiv:2402.07827.\"}"}
{"id": "acl-2024-long-799", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Data Preprocessing Details\\n\\nThe mix of raw documents \u20147.39 million documents, 2.96 billion words\u2014 was deduplicated with Dolma (Soldaini et al., 2024) to discard both intra- and cross-dataset document repetitions. The number of words discarded per dataset at this stage is illustrated in Figure 2(a), where datasets are shown in order of preference (Egunkaria and Booktegi are omitted due to their size). In the case of EusCrawl v1.1, the duplicate documents arise mainly from an overlap between EusCrawl v1 and the updated content. For Colossal OSCAR, more than 60% of the documents are already present in the preferred datasets (i.e., EusCrawl, Egunkaria, Booktegi, EuWiki or CulturaX). HPLT was further deduplicated at the paragraph level due to its quality. After deduplication, the pretraining corpus amounts to 5.80 million documents and 1.45 billion words.\\n\\nThe deduplicated corpus was further filtered based on document-level features. Specifically, we applied Dolma\u2019s implementation of a set of heuristics from Gopher (Rae et al., 2022) and C4 (Raffel et al., 2020), and the Corpus Cleaner v2 (CCv2; Palomar-Giner et al., 2024). The document-level features are as follows:\\n\\n- eu: percentage of the text that is written in Basque according to CLD2\\n- # words: number of words\\n- word len: mean word length\\n- bullet: fraction of lines starting with '*' or '-'\\n- ellipsis: fraction of lines ending with '. . . '\\n- lorem ipsum: whether it occurs in the text\\n- brackets: whether '{' occurs in the text\\n- symbols: '#' and '. . . ' to word ratio\\n- alpha: fraction of words with at least one alphabetic character\\n- CCv2: aggregated score given by CCv2\\n\\nThe threshold applied to each feature and its impact on the datasets is shown in Table 7. We adapted Dolma\u2019s default thresholds regarding document and word length to better fit our data and the Basque language based on observed distributions of our corpora (see examples in Figure 2). Note that the filters are not mutually exclusive, that is, the same document might be flagged for removal by several filters.\\n\\nAs a result of this process, the least curated corpora, HPLT and Colossal OSCAR, were further reduced by 61% and 5% respectively, in terms of words. The 22.37% EuWiki documents flagged as being too short correspond to Wikipedia\u2019s redirect pages, which ultimately did not affect the word count significantly. The final size of the pretraining corpus is 4.30 million documents and 1.22 billion words, which equates to 4.17B Llama 2 tokens.\\n\\nB Dataset Contamination\\n\\nFollowing previous work on data contamination (Brown et al., 2020; Touvron et al., 2023), we check for token n-gram overlaps between test items and training data. To that end, we index training documents in Elasticsearch, applying the standard tokenizer to lowercase text, and removing stopwords (built-in Basque stopwords and all auxiliary verbs). Given that test items vary greatly in length from one benchmark to another, we avoid establishing an arbitrary n-gram length threshold above which to consider a test item contaminated. Instead, we report statistics based on the longest n-grams matched, spanning our search from n-grams equal to each item\u2019s total length to just one word. For each n-gram size $n$ we only considered test items of equal or bigger size when assessing contamination. Results are summarized in Table 8, which reports the contamination percentages ($\\\\text{cont} \\\\ %$) across each benchmark with respect to a specific quartile of test questions or context lengths ($n$). Higher contamination values are to be expected at lower quartiles, as shorter n-grams (typically 1 to 5 words) tend to involve frequent word combinations and are thus more likely to overlap with the training data. We observe that contamination percentages tend to decrease to near zero after the first quartile, with the following exceptions.\\n\\nNotably, QNLI has substantial overlap even at higher n-gram lengths. This is explained by the fact that QNLI contexts were taken from Wikipedia. In line with the analysis of Chowdhery et al. (2023), we do not consider these items to be contaminated, as the questions and answers have not been found alongside the contexts in the training data.\\n\\nIn the case of EusProficiency, it must be noted it comprises particularly short test items, the median length of a question being 4 words. Upon manual analysis, we did not observe any annotation contamination.\\n\\nAs for EusExams, this evaluation benchmark consists of Public Service examinations and thus contains many references to national and regional laws, directives and plans, services, administration offices, etc. Such mentions can also be found on the\"}"}
{"id": "acl-2024-long-799", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 7: Percentage of documents dropped by each filter.\\n\\n| Filter   | EuWiki | EusCrawl | CulturaX | OSCAR | HPLT |\\n|----------|--------|----------|----------|-------|------|\\n| < 0.5    | 0.0B   | 0.5B     | 1.0B     | 1.5B  |      |\\n| < 4      | 0.00   | 0.04     | 3.61     | 0.13  | 22.37|\\n| < 3      | 0.00   | 0.04     | 0.66     | 0.02  | 0.87 |\\n| > 12     | 0.00   | 1.12     | 0.88     | 0.36  | 1.26 |\\n| < 0.8    | 0.11   | 1.21     | 23.93    | 19.14 | 4.19 |\\n| > 0.1    | 0.09   | 0.05     | 0.47     | 0.23  | 0.02 |\\n| > 0.3    | 0.74   | 0.09     | 0.25     | 3.47  | 0.00 |\\n| > 0.9    | 0.03   | 0.01     | 0.01     | 0.17  | 0.00 |\\n| lorem ipsum | 0.00 | 0.00     | 0.02     | 0.31  | 0.00 |\\n| brackets  | 0.43   | 0.07     | 0.73     | 16.30 | 0.23 |\\n| CCv2      | < 0.55 | 0.04     | 0.56     | 25.12 | 0.11 |\\n\\n(a) Cross-dataset document repetitions\\n\\n(b) # words: Words per document\\n\\n(c) alpha: Alphabetic characters to word length ratio\\n\\n(d) word len: Medium characters per word\\n\\nFigure 2: Basic corpus quality statistics before preprocessing.\"}"}
{"id": "acl-2024-long-799", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Task Examples and Prompts\\n\\nTables 9 and 10 contain, respectively, the prompt templates and examples of our new datasets (i.e., EusProficiency, EusReading, EusTrivia, and EusExams) and BasqueGLUE tasks. \\\"System\\\" refers to system prompts and applies only to GPT evaluations. Additionally, the number of shots and metrics used to measure the results are also specified per task.\\n\\nD Model Card\\n\\nWe report Latxa's model card in Table 11.\\n\\nE Detailed EusExams Results\\n\\nWe provide detailed results for EusExams by category in Table 12. Results are consistent across categories, our models outperform every model in the same size category by a large margin. Latxa 13B outperforms GPT-3.5 Turbo in most categories, but Latxa 70B is far from GPT-4 Turbo performance. This is expected as all categories in these exams require advanced knowledge. Health System is the most challenging category, followed by City Council. Public Office and University tests are easier for most models. For specific results of each test check Table 13.\"}"}
{"id": "acl-2024-long-799", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Dataset      | 25% | 50% | 75% | Max |\\n|--------------|-----|-----|-----|-----|\\n| Belebele     | 18  | 40  | 51  | 64  | 138 |\\n| XStory Cloze | 1   | 100 | 4   | 17.4|\\n| EusProficiency| 1  | 99.7| 3   | 34.1|\\n| EusReading   | 1   | 100 | 5   | 88.1|\\n| EusTrivia    | 1   | 100 | 4   | 7.1 |\\n| EusExams     | 1   | 96.6| 6   | 13.6|\\n| BEC          | 1   | 100 | 8   | 0.2 |\\n| BHTCv1       | 1   | 99.8| 19  | 2.8 |\\n| Korref       | 8   | 0.0 | 21  | 0.0 |\\n| QNLI         | 1   | 100 | 3   | 97.1|\\n| VaxxStance   | 2   | 99.4| 15  | 0.3 |\\n| WiC          | 2   | 100 | 12  | 2.6 |\\n\\nTable 8: Data contamination results for all our evaluation datasets. The table shows the contamination percentage (cont %) considering different n-gram sizes (n) that depend on the length of each dataset's items.\"}"}
{"id": "acl-2024-long-799", "page_num": 18, "content": "{\"primary_language\":\"es\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"System\\nRespond always with a single letter: A, B, C or D.\\n\\nExample\\n\\nQuestion: Upon entering a restaurant, to another diner: A. Good night! B. Enjoy! C. Bless you! D. Greetings! Answer: B\\n\\nPassage: Ernest Hemingway, without his knowledge, came for the last time to Bilbao, and in general to the Peninsula. And indeed, on the 24th, at Bilbao, the bull gave his favourite Ord\u00f3\u00f1ez a good goring. It was nothing, luckily. As can be seen from the photograph published in El Correo Espa\u00f1ol the next day, the bullfighter is there, alive, in bed, the moment the writer visits him. \\n\\nQuestion: On August 24, 1960 A. The bullfighter Ord\u00f3\u00f1ez, who appears next to C. Barrena of El Correo Espa\u00f1ol, was caught by a bull. B. Ernest Hemingway visited the bullfighter Ord\u00f3\u00f1ez, who had received a goring in the square of Bilbao. C. In the photo of El Correo, a bull catches the bullfighter Ord\u00f3\u00f1ez. D. Ernest Hemingway arrived in Bilbao for the first and last time. Answer: B\\n\\nQuestion: How many kilograms are there in a tonne? A. 10,000 kilos B. 1,000,000 kilos C. 1,000 kilos D. 100 kilos Answer: C\\n\\nQuestion: UPV/EHU'S LEGACY IS: A. The property owned by UPV/EHU. B. The rights and property owned by the UPV/EHU. C. The rights and property of the UPV/EHU in ownership, as well as any other property acquired or assigned to it in the future. D. The property of the UPV/EHU in ownership, as well as any other property acquired or assigned to it in the future. Answer: C\"}"}
{"id": "acl-2024-long-799", "page_num": 19, "content": "{\"primary_language\":\"es\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Example\\nTestua: Eta Euskal Herrian noizko @eajpnv ? #URL\\nGaldera: Nolako jarrera agertzen du aurreko testuak?\\nErantzuna: negatiboa\\nText: And in the Basque Country when @eajpnv ? #URL Question: What sentiment does the previous text convey? Answer: negative\\n\\nExample\\nTestua: 45 urtetik gorakoen txertaketa hasiko da igandean Israelen https://t.co/6opid1ULyd\\nGaldera: Nolako jarrera agertzen du aurreko testuak txertoei buruz?\\nErantzuna: neutrala\\nText: vaccination of over 45 years of age begins on Sunday in Israel https://t.co/6opid1ULyd Question: What stance does the previous text take on vaccines? Answer: neutral\\n\\nExample\\nTestua: Eusko Jaurlaritza ari da prestatzen eta EAEko ikastetxe guztietara zabaltzeko asmoa du.\\nGaldera: Zein da aurreko testuaren gaia?\\nErantzuna: Gizartea\\nText: The Basque Government is preparing it and intends to extend it to all schools in the Basque Country. Question: What is the subject of the previous text? Answer: Society\\n\\nExample\\nTestua: *Luis Uranga* harrituta azaldu da Portugalgo klubaren jokaerarekin. RICARDO SA PINTOK datorren denboraldian Lisboako Sportingen jokatu zuela pentsatzen genuen guztiok, baina une honetan Lisboarako bidea erabat zaildu zaio *Realeko aurrelariari*.\\nGaldera: Aurreko testuan, \\\"*Luis Uranga*\\\" eta \\\"*Realeko aurrelariari*\\\" gauza bera dira?\\nErantzuna: ez\\nQuestion: *Luis Uranga* was surprised by the way the Portuguese club acted. We all thought RICARDO SA PINTO would be playing next season at the Sporting of Lisbon, but right now the road to Lisbon has become very difficult for *the forward of La Real*. In the previous text, is \\\"*Luis Uranga*\\\" the same as \\\"*Realeko aurrelariari*\\\"? Answer: no\\n\\nExample\\nNortzuen lehen alaba izan zen Dua Lipa?\\nLiparen lehen hezkuntzako ikasketak musika klaseak eduki zituen, eta jotzen ikasi zuen lehen instrumentua biolontxeloa izan zen.\\nGaldera: aurreko galderari erantzuten al dio emandako testuak?\\nErantzuna: ez\\nWhose first daughter was Dua Lipa? Lipa's primary education included music lessons, and the first instrument she learned to play was the cello. Question: does the text given answer the previous question? Answer: No.\\n\\nExample\\n1. esaldia: beste alde batetik, irakasleek materiala prestatzeko dituzten aukera informatikoak ere gero eta ugariagoak dira;\\n2. esaldia: Unitate horretan konturatuko zinen bezala, materialak aldakorrak dira: batzuk lurrindu egiten dira berotzen direnean, beste batzuk apurtu edo eraldatu, edo aldaketa kimikoak jasan ditzakete.\\nGaldera: Aurreko bi esaldietan, \\\"material\\\" hitzak esanahi berdina du?\\nErantzuna: ez\\nSentence 1: on the other hand, the computer possibilities for teachers to prepare materials are increasing; Sentence 2: As you may have noticed in that unit, materials are changeable: some evaporate when heated, others brake or transform, or they may undergo chemical changes. Question: In the two previous sentences, does the word \\\"material\\\" have the same meaning? Answer: no\"}"}
{"id": "acl-2024-long-799", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Latxa comes in a range of parameter sizes: 7B, 13B, and 70B.\\n\\nModels input text only.\\n\\nModels generate text only.\\n\\nLatxa, similar to Llama 2, is an auto-regressive language model that uses an optimized transformer architecture.\\n\\nLatxa was trained between October 2023 and February 2024.\\n\\nThis is a static model trained on an offline dataset. Future versions of the model may include more updated data.\\n\\nLatxa is based on Llama 2 models, and therefore, inherits their license. It is a custom commercial license available at: https://ai.meta.com/resources/models-and-libraries/llama-downloads/\\n\\nLatxa models are intended to be used with Basque data; for any other language, the performance is not guaranteed. Latxa inherits the Llama 2 License which allows for commercial and research use. Latxa family models are pretrained LLMs without any task-specific or instruction fine-tuning. That is, the model can either be prompted to perform a specific task or further fine-tuned for specific use cases.\\n\\nThe model was not fine-tuned to follow instructions or to work as a chat assistant, therefore, this kind of usage is not tested nor recommended.\\n\\nThe training of Latxa was conducted using GPT-Neox library. As infrastructure, we leveraged the CINECA HPC Leonardo computing cluster located in Italy. At most, 256 custom A100 GPUs were used to train the models.\\n\\nPretraining utilized a cumulative 34.7K GPU hours of computation on hardware of type A100 64Gb (TDP 440W). Estimated total emissions were 4.53tCO$_2$eq.\\n\\nLatxa is trained on corpora from different sources. In general, quality was preferred over quantity, but content derived from automatically filtered versions of CommonCrawl was also included.\\n\\nAfter collecting the corpora, it was cleaned and deduplicated. Pretraining corpora includes: EusCrawl v1.1, Egunkaria, Booktegi, EuWiki, CulturaX, Colossal OSCAR, and, HLPT v1.\\n\\nThe pretraining data has a cutoff of November 2023.\\n\\nEvaluation\\n\\nSee Evaluation Data (Section 4), Experimental Setting (Section 5), and Results (Section 6)\\n\\nEthical Considerations and Limitations\\n\\nTo alleviate the potentially disturbing or harmful content, Latxa has been trained on carefully selected and processed data which comes mainly from local media, national/regional newspapers, encyclopedias and blogs. Still, the model is based on Llama 2 models and can potentially carry the same biases, risks and limitations.\"}"}
{"id": "acl-2024-long-799", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"|                             | Admin staff 2022 | Support staff 2022 | Admin assistant 2022 | General questions 2022 | Public Office University City Council Health System Average |\\n|-----------------------------|------------------|---------------------|-----------------------|-------------------------|-------------------------------------------------------------|\\n| GPT-3.5 Turbo               | 46.98            | 72.70               | 22.70                 | 22.99                   |                                                              |\\n| XGLM 7B                     | 23.82            | 23.38               | 24.82                 | 25.88                   |                                                              |\\n| BLOOM 7B                    | 24.08            | 24.02               | 25.56                 | 24.92                   |                                                              |\\n| Mistral 7B                  | 34.72            | 33.38               | 27.84                 | 28.37                   |                                                              |\\n| Llama 2 7B                  | 30.35            | 27.68               | 31.98                 | 28.26                   |                                                              |\\n| Latxa 7B                    | 37.22            | 37.29               | 35.24                 | 29.39                   |                                                              |\\n| mGPT 13B                    | 25.01            | 24.47               | 24.80                 | 28.09                   |                                                              |\\n| Llama 2 13B                 | 31.99            | 30.52               | 26.81                 | 26.52                   |                                                              |\\n| Latxa 13B                   | 48.78            | 50.35               | 45.52                 | 36.66                   |                                                              |\\n| Mixtral 8x7B                | 44.87            | 42.73               | 38.47                 | 35.54                   |                                                              |\\n| Yi 34B                      | 41.13            | 41.76               | 36.61                 | 36.18                   |                                                              |\\n| Llama 2 70B                 | 37.42            | 34.75               | 30.78                 | 27.94                   |                                                              |\\n| Latxa 70B                   | 58.16            | 56.26               | 50.04                 | 43.78                   |                                                              |\\n\\nTable 12: Detailed accuracy results over EusExams categories. Best results in each compute class are in bold. Best overall results are underlined.\\n\\nTable 13: Detailed results on EusExams tests and categories (in bold). Best results in each compute class are in bold. Best overall results are underlined.\"}"}
