{"id": "acl-2023-long-41", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This scene takes place in the following location: boardroom. Four birds are in an office. They're perched around a table. Birds don't have offices. The scene includes: Parrot, Speech repetition.\\n\\none of the following funny captions is most relevant to the scene:\\n\\nA) Just be glad he's not wearing his kilt today.\\nB) The founding fathers were clear. You must win by two.\\nC) She'll appreciate you're wearing protection.\\nD) We have to stop eating the seed money.\\nE) Can I interest you in opening an offshore account?\\n\\nthe funny caption that matches the scene is:\\n\\nIn this task, you will see a description of an uncanny situation. Then, you will see two jokes that were written about the situation. One of the jokes is better than the other one. Pick which of the two jokes is the one rated as funnier by people.\\n\\nThis scene takes place in the following location: a cave. A caveman is drawing a picture of an elephant on his cave wall. The elephant is standing by as a model. The elephant is friends with a man. The scene includes: Caveman, Mammoth, Cave painting.\\n\\nchoices:\\n\\nA) Trust me. One day your portrait will be used as the symbol of a political party even more primitive than we are.\\nB) So I've added the pointy trunk. Were there any other unique characteristics the mugger had that you remember?\\n\\nthe funnier is:\\n\\nFigure 9: Example GPT-3 zero-shot prompts for Matching (top) and Quality ranking (bottom) tasks. In-context prompts are similar, except 5 random labelled training examples are also provided in the prompt.\\n\\n|               | Accuracy (\u2191) | CrowdAcc (\u2191) | NY Acc (\u2191) | B-4 (\u2191) | Rouge-L (\u2191) | PPL (\u2193) |\\n|---------------|--------------|--------------|------------|---------|-------------|---------|\\n| Random        | 20.0         | 50.0         | 50.0       | -       | -           | -       |\\n| Caption Only  | 19.4         | 59.4         | 64.5       | 3.61    | 17.8        | 34.0    |\\n| text-ada-001  | 20.1         | 50.8         | 49.9       | 2.04    | 15.9        | 2367    |\\n| text-babbage-001 | 19.0     | 51.3         | 51.1       | 2.18    | 17.2        | 137     |\\n| text-curie-001 | 20.4         | 51.0         | 50.0       | 2.99    | 18.1        | 108     |\\n| text-davinci-001 | 35.6         | 54.4         | 53.8       | 3.79    | 19.5        | 151     |\\n| text-davinci-002 | 57.2         | 55.1         | 54.8       | 5.07    | 20.5        | 107     |\\n\\nTable 4: GPT-3 scaling experiment results, averaged over 5 cross-validation splits. In all cases, models are given access to the same sample of 5 in-context examples. Overall, text-davinci-002 performs best \u2014 this appears to be both because of scale (e.g., text-davinci-001 generally outperforms text-curie-001) and also because of training improvements in the updated 002 version of the model.\\n\\nForming Matching Instances. For each high quality caption, we create a matching instance that serves as the correct answer. Next, we randomly assign captions to mismatched contests to form negative, mismatched sets to serve as false options. While the assignment is random, we have two constraints: 1) we assign within cross-validation splits only, to ensure that training/validation/testing captions are disjoint; and 2) we construct the corpus with no answer-only biases by performing the negative assignment such that each answer appears exactly once as a correct answer and exactly 4 times as an incorrect answer in other instances.\\n\\nForming Quality ranking Instances. For each high quality caption, we aim to sample from the larger set of all submissions for the contest captions that are just \\\"okay.\\\" First, we note that 25 contests from early on in the contest's history were missing entries, so we are limited to sampling negatives for 679 contests. Next, because many entries are exact duplicates, we deduplicate on string matching, such that \\\"okay\\\" captions are not exact copies of 1) the identified high quality captions; and 2) any other sampled \\\"okay\\\" captions.\\n\\nNext, for later contests from Jain et al. (2020), we have estimated quality ratings based on crowd feedback for each entry already: in that case, we discard the top third and bottom third of captions according to mean crowd rating \u2014 the middle tertile form the \\\"okay\\\" set we sample from. But, for earlier contests, we do not have direct ratings: we only have access to New Yorker finalists and a large pool of entries. For those cases, we aim to eliminate captions that are clearly likely to be low quality. To accomplish this, we train a quality ranking model (conditioned just on the caption text, rather than any information about the contest) using crowdlabelled data from 253 contests provided by Jain et al. (2020). We sample a good/bad set by selecting from each contest the top and bottom 1000 entries according to their mean crowdsource score: the resulting dataset forms a corpus of 506K captions. We form two sets of labelled data based on the parity of the contest...\"}"}
{"id": "acl-2023-long-41", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this task, you will see a description of an uncanny situation. Then, you will see a joke that was written about the situation. Explain how the joke relates to the situation and why it is funny.\\n\\nThis scene takes place in the following location: a laboratory. A man in lab attire is sitting in a room with several infinite monkeys. One of them is in front of a typewriter. It's unusual to see a monkey operating a typewriter. The scene includes: Infinite monkey theorem, Mad scientist.\\n\\nCaption: Have you considered writing this story in the third monkey rather than the first monkey?\\n\\nExplanation of the caption: A play on the phrase \"going paperless.\" Instead of carrying around a bunch of folders, documents, and other paperwork, people can use digital files. It's a goal for many companies because it would save money and be more efficient. However, this company has a tiger on top of their filing cabinet. So instead of \"going paperless,\" this company is going \"tiger-full.\" The man is going to miss having a tiger in the office when they switch to digital files \u2014 presumably because it won't be as exciting.\"}"}
{"id": "acl-2023-long-41", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are CaptionContestGPT, an expert language model at understanding the famous New Yorker caption contest. You follow the contest each week, and understand what makes for a humorous caption for each cartoon. You are aware of the various theories of humor, and read/analyze the caption contest entries and winners each week.\\n\\nSome things to remember:\\n- You're well versed in the history of the New Yorker Caption contest, and the types of captions that are selected as finalists/winners vs. those that are not.\\n- You think step-by-step, but aren't overly verbose.\\n- You can express uncertainty in your thinking, but in the end, pick the single best answer in the requested format.\\n\\nI will describe a New Yorker cartoon to you. Then, I will give you 5 choices (labelled A-E) for captions. One of the captions was the winning caption for that cartoon, the other captions do not correspond to this cartoon. Your job is to first reason step-by-step about which answer might be correct, and, in the end, respond with \\\"Answer: X\\\" where X is either A, B, C, D, or E.\\n\\nSure, please describe the New Yorker cartoon, and provide me with the 5 caption choices.\"}"}
{"id": "acl-2023-long-41", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"number (i.e., even vs. odd). We train/validate two T5-Large models based on this split for the binary classification task. While the average validation accuracy we achieve is 65%, we achieve higher precision in identifying the \u201cbad\u201d label: precision-at-10 is 83, precision-at-20 is 77, precision-at-30 is 72. It appears to be harder to identify very good captions than very low rated ones: precision-at-10 is 77, precision-at-20 is 73, precision-at-30 is 70.\\n\\nUpon training these models, we perform inference on all captions in contests without crowd ratings, and discard the 25% of entries with the lowest predicted score. Entries with very low scores have some common characteristics, e.g., they don\u2019t have the gestalt of a New Yorker caption, they have many typos/formatting issues, they include the contact information of the submitter, etc. Examples of discarded captions (some are obfuscated for privacy reasons) are:\\n\\n- THEY COULDN\u2019T WAIT TO MARRY SO THEY CAME TO RECITE THEIR VOWS BETWEEN TAKES FROM \u201cPRIMITIVE LOVE LIFE\u201d\\n- You\u2019re hurting me, will we ever break up?\u201d (@technology)\\n- The stressed is so \u201cBad\u2019 in the world. \u201cyou or me\u201d did not see(BIG)/(FOOT)\\n- Too mammalian, needs reptile.\u201d [NAME], [STATE] [EMAIL]@gmail.com\\n\\nAfter identifying a set of captions that are not obviously bad, nor apparently among the top quality submissions, our second step is to deduplicate entries. Because submitted captions for each contest are frequently identical to other submissions or play off the same core joke concept, we perform the same SBERT+hierarchical clustering semantic deduplication step as we did for sampling the diverse high quality set (described above). Specifically, we extract SentenceBERT embeddings (Reimers and Gurevych, 2019) for each of the N entries, and then compute a hierarchical clustering of the embeddings into $\\\\frac{707}{N}$ clusters, sampling only a single representative from each cluster to form a less-redundant set. This removes 30% of the data with close neighbors in the final set: for example, for a contest depicting two monsters eating buildings in New York City, this step downsamples 100 \u201ctastes like chicken\u201d jokes (which end up in a single cluster) to a single exemplar.\\n\\nAfter filtering, for all contests, we are left with a (softly) deduplicated pool of candidate entries that are likely to be at least okay, but unlikely to be as good as the verifiably high quality entries. For each high quality entry, we sample an \u201cokay\u201d caption with: 1) similar estimated quality according to the text-only models; 2) similar length in words; 3) similar length in characters; 4) similar amount of punctuation; 5) a dissimilar SBERT embedding.\\n\\nExplanation corpus. After several attempts to solicit high-quality explanations from crowdworkers fell short, one of the authors of this paper decided to simply annotate a corpus of explanations themselves. For each contest, a high quality caption was sampled for annotation \u2014 this high quality caption was sampled arbitrarily from the set of New Yorker finalists if they were available, and, in the few cases where New Yorker finalists weren\u2019t available, from the set of high quality crowd captions. Of the 704 sampled explanations, the author reported understanding 651 of them, and wrote an explanation for each. This was a substantial effort: the resulting corpus has a mean of 60 words of explanation per cartoon, and the total length, 39.3K words, is comparable in length to a novella.\\n\\nD Graphical version of matching and ranking results. In Figure 12, we use vertically-stacked bars to illustrate the difference between zero-shot (small dots), five-shot (vertical stripes), and fine-tuned (solid) versions of various models. Human results are set off by dark green lines.\\n\\nFigure 12: Graphical version of the matching results given in Table 2. The scatter-plot in Figure 13 uses the same graphical conventions to display the quality-ranking results. Recall our caveat that crowd accuracy may be more statistically reliable, in the sense that crowd selectors, whose tastes underlie the y-axis results, vastly outnumber New Yorker...\"}"}
{"id": "acl-2023-long-41", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Results for the explanation task using automatically computed metrics. Results are averages over 5 cross-validation splits. Underlined results are the best model in the From Pixels (FP) setting, where at test time, models only have access to the cartoon images. Bold results are best in the From Description (FD) setting, where at test time, models have access to human-authored descriptions of the cartoons. GPT-3.5 and GPT-4\u2019s API does not provide log probabilities, so we cannot compute perplexity for those models.\\n\\nEditors, whose tastes underlie the x-axis results.\\n\\nFigure 13: Graphical version of the ranking results given in Table 2.\\n\\nE Automatic evaluation of explanations\\n\\nFor completeness, we provide the results for automatically-calculated explanation-evaluation metrics in Table 5. (Log probabilities are unavailable for GPT-3.5/GPT-4 so we cannot report perplexity for them.) However, we believe that the human evaluations reported in the main body of the text are better quality measures.\\n\\nF Machine explanations that were preferred over human ones\\n\\nGPT-4\\n\\nIn 8/130 cases, for our human vs. GPT-4 5-shot experiments, the machine generation was preferred to the human reference by 3/3 annotators. In Figure 14 we conduct a close reading of these 8 instances to understand where the human references fell short. In all cases, both were topical, but, for a handful of cases, the machine generation is arguably better because it\u2019s more succinct, or offers a more meaningful detail.\\n\\nGPT-3\\n\\nWe also include a close reading of several instances where a majority of annotators preferred GPT-3 annotations vs. our human ones. This occurred in 16/131 cases for our human vs. GPT-3 experiments: in 15 cases, 2/3 annotators preferred the machine generation, and in 1 case, 3/3 annotators preferred the machine generation. We present a few examples of these cases with comments in Figure 15. Similar to GPT-4, most commonly, both the explanations were reasonable, in one case, the human-written explanation missed a pop culture reference to \u201cThe Attack of the 50 Foot Woman\u201d (1958) but GPT-3 mentions it. In six cases, we disagree with annotator consensus: the machine caption makes some correct references, but we believe it (sometimes subtly) misses the point.\\n\\nG Aiding humor generation with system-assisted brainstorming\\n\\nOur main experiments focus on three tasks that probe machine capacity for matching, ranking, and explaining caption contest entries. But, given prior interest in generating caption entries, to provide a pointer towards more creative use cases, we developed a curated prompt that re-frames the corpus in a manner that enables the production of cartoon descriptions, concept brainstorming, and, ultimately, caption generation. In contrast to our existing annotations, we are inspired by Wallace (2022)\u2019s framing, where free associations are used to connect-the-dots from cartoons to concepts to captions. So, for 9 finalist captions, we reverse-engineer a plausible brainstorming process that could have given rise to the caption by listing several potential associations from the scene, and, from that list, selecting the 1-3 that underpin the basis for the winning joke. In the end, our prompt consists of 9 examples formatted as:\\n\\n1. cartoon description (from our annotations)\\n2. uncanny description (from our annotations)\\n3. list of entities in scene (from our annotations)\\n4. list of concepts from the cartoon that could be used for a caption (hand-designed for the\"}"}
{"id": "acl-2023-long-41", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A person in a diver suit is out-of-place at a cocktail party.\\n\\nCaption: This suit looked way better in the store.\\n\\nAgree: The human annotation assumes the person didn't notice; the machine explanation is more on-point/coherent.\\n\\nSometimes, clothes you try on in a store look better when you are deciding to buy them or not. Here, the idea is taken to the extreme when the person buys the diving suit bought the \u201csuit\u201d without even noticing it was a diving suit (and looks out of place for the party).\\n\\nWe all experience situations where a garment looks great in the store, but when we wear it out in a public setting, we feel out of place. The situation creates comedic tension because it's an extreme example of being underdressed or out of place.\\n\\nA man is on a cell phone in mid-air despite being thrown off a bucking horse. He is smiling.\\n\\nCaption: Hi Hon. Can you meet me at the hospital in about ten minutes?\\n\\nSomewhat agree: The human explanation says \u201cfriend\u201d instead of spouse/partner, but the machine authored explanation could also be improved (\u201cmismatch\u201d is less relevant).\\n\\nThe man on the phone is pre-planning his own hospital visit. This is unusual, because one would think that he would be rushing to the hospital instead of, in a surprisingly carefree and cheerful manner, telling his friend to meet him at the hospital.\\n\\nThe man in the suit is attempting to ride a bucking horse, which implies that he is not experienced in horse riding or else the horse is trained extremely well.\\n\\nA man in an office building is planting seedlings in his cubicle.\\n\\nCaption: The third quarter is going to be delicious.\\n\\nMostly agree: Although the machine explanation refers to documents that are not explicitly evident, the human explanation assumes a less likely interpretation of \u201cdelicious\u201d.\\n\\nA play on the term \u201cdelicious\u201d \u2014 in a usual office setting, if third quarter was going to be delicious, it would be about dark enjoyment of office politics, but the literal interpretation is about a wholesome vegetable feast.\\n\\nIn the business world, people often discuss quarters in terms of profits and financial performance. The joke here is that the absurdity of treating financial documents as plants and expecting positive results from this unorthodox method.\\n\\nA woman is walking in town, but her ruff collar is comically large.\\n\\nCaption: It deployed when her carriage rear-ended an ox cart.\\n\\nMostly agree: Both are good, but the machine explanation has a more specific conclusion.\\n\\nThe woman's oversized ruff collar is juxtaposed against the modern invention of an air bag. Air bags deploy during car accidents, but the literal interpretation is about a wholesome vegetable feast.\\n\\nThe joke is a play on modern-day car safety features, like airbags, that deploy in an accident to protect passengers. In the business world, people talk about quarters in terms of profits and financial performance.\\n\\nA wolf trying on a sheepskin as if it were a costume, looks in a mirror as a butler looks on, holding various other outfits.\\n\\nCaption: I'll take this and the granny dress.\\n\\nMostly agree: The machine explanation is more specific.\\n\\nA reference to the wolf from Little Red Riding Hood. A \u201cwolf in sheep's clothing,\u201d as depicted, is an idiom meaning a person who is not what they seem. In this case, the wolf is trying on a sheepskin as a costume and asking for a \u201cgranny dress.\u201d\\n\\nThe same (also nefarious) wolf as in Little Red Riding Hood: who kills a grandmother and wears her clothes as a disguise.\\n\\nA flying saucer trails a man and a woman driving down the road.\\n\\nCaption: When they said, \u201cTake us to your leader.\u201d I don\u2019t think they meant your mother\u2019s house.\\n\\nSlightly disagree: Both take the joke in slightly different ways, but the human explanation seems more plausible.\\n\\nThis is a joke about in-laws. Pop culture depictions of aliens often have them requesting that the first humans they make contact with take them to their leader. The joke is that the man is thinking about how unusual it is that the giant spaceship is tailing them.\\n\\nWhen people think of aliens asking to be taken to our leaders, they usually mean they want to meet important authority figures. People often think of their mothers as being very dictatorial, even if they aren't technically their leader.\\n\\nA man looks in a bathroom mirror as he gets ready, but he looks like Abraham Lincoln in the mirror.\\n\\nCaption: On second thought let\u2019s skip the play.\\n\\nSlightly disagree: The human one expresses the same idea more succinctly.\\n\\nA joke about Abraham Lincoln's assassination, which took place at a play. This man is getting ready to go to a play and has a premonition of being assassinated, due to this premonition, he suggests to his partner that they not go to the play after all.\\n\\nThe joke plays on two ideas. First, it suggests that the man's reflection, which appears to be Abraham Lincoln, might be the wolf from Little Red Riding Hood. Second, it suggests that the man might be getting ready to be assassinated, due to this premonition, he suggests to his partner that they not go to the play.\\n\\nA man driving talks to a woman in the passenger seat. In the backseat is a cramped giant whale.\\n\\nCaption: He wants to know if you can move your seat up.\\n\\nBoth are good: both explanations express the same idea in a reasonable way.\\n\\nMoving a front seat up is an everyday request from people sitting in the backseat of cars if they don't have enough room. The man's request is unusual because it's how unusual it is that the whale is in the backseat and the benign-ness of the request is what makes it funny.\"}"}
{"id": "acl-2023-long-41", "page_num": 23, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our opinion re: annotator decision\\n\\nHuman-authored explanation\\n\\nTwo teachers walking through a hallway of an elementary school while the children walk on the ceilings and walls.\\n\\nCaption: Back in my day, kids had a little respect for the law of gravity.\\n\\nDisagree:\\n\\nthe machine caption starts off well, but concludes with a number of non-sequiturs. Perhaps the annotators only skim the later parts of a long explanation.\\n\\nNormally \u201crespecting laws\u201d are human-written laws \u2014 but here, the kids have somehow opted out of respecting physical laws of gravity, which means they can walk on walls/ceilings.\\n\\nThe kids in this school are violating the laws of physics by walking on the walls and ceiling. The teachers is ... whether or not kids respect gravity probably depends on the teacher\u2019s training, not whether they were a kid in the past.\\n\\nA doctor is performing a physical examination on Superman.\\n\\nCaption: They think now kryptonite is actually good for you, in small doses.\\n\\nDisagree:\\n\\nBoth are okay, but the human-authored explanation is more specific in referencing the appropriate cultural phenomenon, and the machine-generated one hallucinates that kryptonite is medicinally helpful.\\n\\nA joke about modern medical findings sometimes being counter-intuitive: some studies claim to show that things ... parallel to the other studies, and funny to think that scientists are studying kryptonite\u2019s potential positive impacts.\\n\\nKryptonite is a material that causes superman to become weak; however, the doctor is suggesting that kryptonite is ... helpful medicinally, it\u2019s difficult to explain how small doses of something that kills superman could possibly be helpful.\\n\\n3 people are going to therapy in a 3-tiered bunk-bed.\\n\\nCaption: And how do you feel about how she feels about how he feels?\\n\\nDisagree:\\n\\nThe machine caption omits the oddity of having three patients involved in what appears to be couples therapy. Nor is there wordplay involved.\\n\\nA wordplay joke about feelings \u2014 a therapist might ask a question like: \u201chow do you feel about what happened with your partner\u2019s feelings?; this is a very complicated/deeper concern that requires a lot of reflection.\\n\\nA board meeting is happening in an office, but the employees are birds.\\n\\nCaption: We have to stop eating the seed money.\\n\\nAgree:\\n\\nthe machine explanation is probably better.\\n\\nSeed money is cash provided to a startup company (like the ones the ... but, the play on words implies that the birds spent the seed money literally on seeds to eat, because birds eat seeds.\\n\\nA play on the phrase \u201cseed money\u201d \u2014 in the investment world, \u201cseed money\u201d refers to the initial capital that is ... to stop eating bird seed because it\u2019s causing them not to be able to afford investments like a real office/chair/table.\\n\\nA stack of cars with drivers in them. The driver on the topmost car is leaning out of the window yelling to the drivers below.\\n\\nCaption: I hope this guy Knievel knows what he\u2019s doing.\\n\\nBoth have pros/cons:\\n\\nThe machine is more specific in referencing crashing and the implication for the drivers, but makes up \u201cjumping the stack\u201d meaning crashing.\\n\\nA reference to Evil Knievel, a stuntman who jumps motorcycles over cars. This pile of cars is going to be an obstacle ... the stuntman coordinating this both knows how to jump over and also how to get the drivers out of this precarious situation.\\n\\nA reference to Evel Knievel, a famous stuntman that jumped cars and other objects. This is a joke about him jumping ... it\u2019s a reference to Evel Knievel \u201cjumping the stack\u201d, i.e., crashing, which would be a bad outcome for the drivers.\\n\\nA person wearing boxing gloves is attempting to play a piano on a stage.\\n\\nCaption: I could have been a conductor.\\n\\nBoth bad:\\n\\nthe human caption gives a better mechanical explanation, but both miss that the caption is a play on a quote from \u201cOn the Waterfront\u201d (1954), \u201cI coulda been a contender.\u201d\\n\\nIt would be mechanically difficult to play a piano with boxing gloves because the keys are too small. if this person ... reason that they should be a conductor \u2014 it would be easier for them to hold the conductor batons vs. play piano keys.\\n\\nA joke about Muhammad Ali\u2019s skill as a boxer, which sometimes takes a metaphorical meaning when referring to his ... which requires a high level of skill. It\u2019s funny in part because it\u2019s not clear why he would have not been a boxer.\\n\\nA father and a child are standing in an office building looking out the window onto a giant woman straddling a nearby building.\\n\\nCaption: Son, we finally attracted the fifty-foot-and-over demographic!\\n\\nBoth bad:\\n\\nThe human-authored explanation misses the direct reference to the movie \u201cAttack of the 50-Foot-Woman\u201d (1958), and the machine focuses on non-sequiturs like age 18/viewers/etc.\\n\\nA play on the phrase \u201cfifty and over demographic\u201d \u2014 this phrase would usually refer to the group of people who are ... the age 50+ demographic, they have attracted the height-of-fifty-foot+ demographic, of which this giant woman is a member.\\n\\nA play on the term \u201cfifty-foot-and-over\u201d and \u201cover 18\u201d \u2014 in the media, advertisers sometimes say that they want to ... \u2014 the company is trying to attract the titular Attack of the 50-Foot-Woman (who is, indeed, over 50-feet-tall).\"}"}
{"id": "acl-2023-long-41", "page_num": 24, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"First, you will see a description of a scene from a New Yorker cartoon that has an unusual and/or funny element. Our goal is to find examples that can be used to train a language model to write captions for any given cartoon.\\n\\n**Example 1:**\\n\\n**Description of Scene:** A group of people are playing croquet. One of the players is a very large chicken.\\n\\n**Unusual Part of Scene:** Chickens are not usually intelligent enough to play croquet and they are not usually that big. However, this particular chicken is both free-range and playing a game, which is quite unusual.\\n\\n**Funniest Caption:** I'm not sure this is what they meant by free-range.\\n\\n**Explanation of Caption:** The caption is a play on the term \u201cfree-range chicken,\u201d which is a type of chicken that is allowed to roam freely outside. The caption is funny because it combines this concept with the idea of a chicken playing a game, which is unexpected.\\n\\n**Example 2:**\\n\\n**Description of Scene:** A person is sitting in their living room, but is looking towards the door as an entire circus, complete with all performers, is walking through the door.\\n\\n**Unusual Part of Scene:** It is unlikely and disruptive for an entire set of circus acts to be intruding on a quiet living room.\\n\\n**Potential Concepts to Form a Funny Caption Based On:** unannounced visitors, salespeople, clowns, big top\\n\\n**Funniest Caption:** I'm never buying a timeshare again.\\n\\n**Explanation of Caption:** The caption is a metaphor for an unannounced group of salespeople who are likely to be disruptive and intrusive.\\n\\n---\\n\\nFigure 16: A portion of a 2,407 token prompt that re-formulates various annotations within our corpus in a format conducive for creative collaborations with a language model. The full prompt is available here. Generating line-by-line from this prompt could help to facilitate brainstorming for unusual cartoon situations (first 4 lines), concepts about real or generated contests that could serve as a basis for a humorous caption (line 5), and, captions themselves (lines 6-8). As a demonstration, we present an unconditional sample, in which the model describes a garden party where a chicken is playing croquet (cherry picked from 3 outputs; temperature=.8, top p=.9, frequency penalty=.2, presence penalty=.05), and also, a conditional sample, given a basic description of Contest #818's scene, which ran in mid-September 2022 (cherry picked from 5 outputs; same sampling parameters): the caption is arguably funny, but the explanation is not correct.\"}"}
{"id": "acl-2023-long-41", "page_num": 25, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. a selected set of 1-3 ideas (selected from (4))\\n6. caption (a finalist)\\n7. explanation of the caption (from our annotations)\\n\\nA portion of our prompt is given in Figure 16, along with an unconditional generation (where the cartoon concept and caption are generated) and a conditional generation. Within 5 samples, GPT-3 invents a scene where a large chicken is playing croquet in a yard, and the caption: \u201cI\u2019m not sure this is what they meant by free range.\u201d Also, when conditioned on a basic description of a real contest which depicts a large group of circus performers intruding on an unsuspecting person in their living room (Contest #818), it generates \u201cI\u2019m never buying a timeshare again.\u201d Looking forward, we expect the matching/quality ranking models could be used in conjunction with this prompt to automatically filter for scene-specific generations with style similar to previous finalists.\\n\\nRelated work beyond peer reviewed AI venues\\n\\nOutside of peer-reviewed NLP venues, several projects have used computational techniques to analyze the contest, usually with the goal of generating AI-assisted entries:\\n\\n\u2022 The Pudding: Mishkin et al. (2022) collaborated with GPT-3 (Brown et al., 2020) to generate entries.\\n\u2022 coolposts: Wilson (2019) used topic models to condition an RNN caption generator.\\n\u2022 LILY Lab @ Yale\u2019s Spring 2017 projects include a number of caption contest efforts, including work by Prince, Friedman, Zucker, Anbarasu, and Dohrn.\\n\u2022 The Verge: Zelenko and Bi (2015) trained a Markov language model on previous winning entries.\\n\\nSome of our favorite New Yorker cartoons\\n\\nWe list our favorite captions below. The corresponding images can be seen by clicking on the cartoonist/author names.\\n\\nYC: \u201cThe doctor said it might help me quit.\u201d\\n\u2014 Vince Conitzer/Jeffrey Adam Katzenstein\\n\\nJD: \u201cYou are so smart. You look amazing. You inspire me. [Complimentary bread].\u201d\\n\u2014 Seth Fleishman\\n\\nJMH: \u201cThanks, I\u2019ll write that down.\u201d\\n\u2014 Victoria Roberts\\n\\nJDH: \u201cThey\u2019re from Earth. I wonder if they know Dan.\u201d\\n\u2014 Benjamin Schwartz\\n\\nLL: \u201cI want to be feared as a tyrant, loved as a father, and revered as a god, but I also want them to think I\u2019m funny.\u201d\\n\u2014 Zachary Kanin\\n\\nAM: \u201cI can\u2019t believe I\u2019d been carrying them in my mouth.\u201d\\n\u2014 Amy Hwang\\n\\nRZ: \u201cWell, there\u2019s your problem.\u201d\\n\u2014 Edward Koren\"}"}
{"id": "acl-2023-long-41", "page_num": 26, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ACL 2023 Responsible NLP Checklist\\n\\nA For every submission:\\n\\n- A1. Did you describe the limitations of your work?\\n- A2. Did you discuss any potential risks of your work?\\n- A3. Do the abstract and introduction summarize the paper\u2019s main claims?\\n- A4. Have you used AI writing assistants when working on this paper?\\n\\nB Did you use or create scientific artifacts?\\n\\n- B1. Did you cite the creators of artifacts you used?\\n- B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\n- B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\n- B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymous it?\\n- B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n- B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n\\nC Did you run computational experiments?\\n\\n- C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-41", "page_num": 27, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nSection 3 and Appendix B\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nSection 3\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nSection 3\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nSection 2\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nAppendix A\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nSection 2, Appendix A\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nSection 2, Appendix A\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nAppendix A\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nWe don't know many specifics, other than country of IP: which we discuss in appendix A.\"}"}
{"id": "acl-2023-long-41", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nLarge neural networks can now generate jokes, but do they really \u201cunderstand\u201d humor? We challenge AI models with three tasks derived from the New Yorker Cartoon Caption Contest: matching a joke to a cartoon, identifying a winning caption, and explaining why a winning caption is funny. These tasks encapsulate progressively more sophisticated aspects of \u201cunderstanding\u201d a cartoon; key elements are the complex, often surprising relationships between images and captions and the frequent inclusion of indirect and playful allusions to human experience and culture. We investigate both multimodal and language-only models: the former are challenged with the cartoon images directly, while the latter are given multifaceted descriptions of the visual scene to simulate human-level visual understanding. We find that both types of models struggle at all three tasks. For example, our best multimodal models fall 30 accuracy points behind human performance on the matching task, and, even when provided ground-truth visual scene descriptors, human-authored explanations are preferred head-to-head over the best machine-authored ones (few-shot GPT-4) in more than 2/3 of cases. We release models, code, leaderboard, and corpus, which includes newly-gathered annotations describing the image\u2019s locations/entities, what\u2019s unusual in the scene, and an explanation of the joke.\\n\\n1 Introduction\\n\\nHumor can be dissected, as a frog can, but the thing dies in the process and the innards are discouraging to any but the pure scientific mind.\\n\\n\u2013 White, E. B. (1941)\\n\\nEach week, The New Yorker publishes a un-captioned cartoon image, inviting readers to submit their funniest English-language caption for it. Editors choose three finalists from sometimes thousands of submissions; then, readers vote to pick the final winner. We develop a suite of three progressively harder tasks built around this contest to test how well AI models \u201cunderstand\u201d humor across vision and language: 1) matching jokes to cartoons, 2) identifying a winning caption, and 3) generating an explanation of why an image/caption combination is funny.\\n\\nThese tasks are difficult because the connection between a winning caption and image can be quite subtle, and the caption can make playful allusions to human experience, culture, and imagination. Consider the image and winning caption \u201cCan you please pass the cow?\u201d in Figure 1. Unlike literal image captions such as in MSCOCO (Lin et al., 2014), here, the caption\u2019s relation to the image is indirect: the size of the mugs must first be recognized as unusual, and then, the caption invokes\\n\\nThe (relatable) experience of \u201cnot getting\u201d a New Yorker cartoon often results from inability to identify the image/text relationship.\"}"}
{"id": "acl-2023-long-41", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You both know Jane \u2013 vs \u2013\\nAccounting meet archives.\\n\\nPublicly, we are still saying there are no side effects\\nI\u2019ll admit he may look ugly, but his resume is beautiful.\\n\\nFigure 2: Instances of our three tasks.\\nMatching requires models to select the finalist caption for the given cartoon from among distractors that were finalists, but for other contests. Quality ranking requires models to differentiate a finalist from a non-finalist, both written for the given cartoon. Explanation requires models to generate free-text explanations of how a high-quality caption relates to the cartoon. Cartoons by Robert Mankoff and Mick Stevens.\\n\\nAn association between a large mug and a large amount of cream/milk \u2014 perhaps a whole cow\u2019s worth. Further, matching a caption to an image is not sufficient: non-finalist entries (e.g., \u201c...Insomniacs Anonymous\u201d in Figure 1) also match the image, but something else makes one seem funnier than the other. Finally, even if a model can accurately identify winning submissions, we would like it to also be able to explain why a particular highly rated/relevant caption is funny.\\n\\nWe cover our three tasks in two settings: in the from pixels setting, models are given access only to the cartoon images at test time, and must perform computer vision; in the from description setting, we allow models access to a newly-collected, human-authored corpus of cartoon descriptions, thus simulating access to a human-level computer-vision system \u2014 or, alternately, facilitating benchmarking of models that don\u2019t have a built-in image-processing component. The annotations we collect and release are rich and multifaceted: they describe the image overall and its locations and entities, what\u2019s unusual about the image, and an explanation of the joke. We view this effort as a significant contribution of our work.\\n\\nOur results reveal a gap between AI and human-level humor \u201cunderstanding.\u201d In the from pixels setting, our best multimodal model (fine-tuned CLIP ViT-L/14 (Radford et al., 2021)) achieves 62% accuracy on a 5-way multiple choice task, but humans achieve 94% in the same setting. Even with significant manual annotation of the cartoons in the from description setting (and despite significant improvements in language modeling performance since this work\u2019s submission), large language models still fall short: human explanations are still preferred in more than two-thirds of cases compared to our best explanation model, 5-shot GPT-4.\\n\\nWe release our challenging NLP/vision benchmarks, annotations, models, leaderboard, and code at https://capcon.dev/. Beyond AI research, we also hope that our work will spur progress in human-AI collaboration tools for cartoonists, contest entrants, and beyond (see Appendix G for AI-generated captions).\\n\\nDatasets and Task Setups\\nOur corpus compiles 14 years of weekly New Yorker caption contests. Each contest consists of: (1) a captionless cartoon; (2) that week\u2019s entries; (3) the three finalists, selected by New Yorker editors; and (4) for some contests, quality estimates for each submission collected via crowdsourcing. The corpus was constructed from two sources. The first is Jain et al. (2020), from which we obtain roughly 250 contests (mean/median 6.1K/5.7K unique captions per contest; 1.5M total), starting from #508. Crowd ratings in this corpus are gathered.\\n\\n2 GPT-3 (Brown et al., 2020) was the most performant in Jan. 2023 when this work was submitted, but we have since updated our results.\\n3 Our data may contain offensive jokes. We manually removed a handful of cases we observed to target specific protected classes. We do not endorse the jokes in the corpus, but rather, view them as interesting objects of study.\\n4 We regret that The New Yorker does not currently have an alliterative-paragraph contest.\\n5 We manually corrected some errors in the corpus.\"}"}
{"id": "acl-2023-long-41", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Basic size statistics for our three tasks. We extend Shahaf et al. (2015); Radev et al. (2016); Jain et al. (2020) by (a) proposing matching, quality ranking, and explanation tasks; (b) providing new, dense annotations for each cartoon (see Figure 3); (c) authoring a set of 651 joke explanations.\\n\\nThe second corpus, due to Shahaf et al. (2015); Radev et al. (2016) and derived from contests #1\u2013#507, includes 2M unique captions (mean/median 5.2K/5.0K per contest), but no crowd ratings. We remove by hand 55 contests whose images' resolutions are too low, and identify 80 low resolution (but usable) cases, taking special care when annotating this set (\u00a72.2).\\n\\n2.1 Task Setups\\n\\nWe pose three tasks. Matching and explanation are novel, whereas quality ranking extends the formulations introduced in Shahaf et al. (2015); Radev et al. (2016).\\n\\nMatching. Can a model recognize when a caption is appropriate for a given cartoon? Five choices are given, only one of which truly corresponds. For the example in Figure 1, we supply the following possibilities:\\n\\n(a) O.K. I'm at the window. To the right? Your right or my right?\\n(b) I'd kill for some cream cheese.\\n(c) Bob just came directly from work.\\n(d) Can you please pass the cow?\\n(e) They only allow one carry-on.\\n\\nThe correct caption is a finalist for the cartoon. Negative choices are randomly selected finalists from other contests, and as a result, are great captions for some other contest's image.\\n\\nIn some cases, matching depicted objects to their textual references may suffice, but in other cases, the relationship is more indirect. For example, Figure 2 (top) contains a subtle reference to Jane Goodall, thus requiring external knowledge; Figure 2 (bottom) relies on a stereotype of pharmaceutical companies being untrustworthy, hence requiring reasoning beyond the literal text.\\n\\nQuality ranking. Can a model identify highly rated captions? For each finalist, we sample for comparison a caption that was not selected as a finalist, and ask models to identify which one (the real one or the distractor) was rated as higher quality. As preprocessing, we run one round of text-only filtering to discard submissions that are easily identifiable as low quality, and also perform semantic deduplication; more details in Appendix C.\\n\\nHere is the end result for Figure 1:\\n\\n(a) Can you please pass the cow?\\n(b) Welcome to Insomniacs Anonymous.\\n\\nWhich caption a particular individual prefers can be a matter of personal taste; but there is a general preference among our human annotators for the true finalist (see \u00a73).\\n\\nExplanation. Can a model generate as good an explanation as a human for why a caption-and-image combination is funny? Free-form explanations of why captions are funny/appropriate for their corresponding image were written by an author of this paper.\\n\\nThe rough annotation guidance was: \u201cIn a few sentences, explain the joke as if to a friend who doesn\u2019t \u2018get it\u2019 yet.\u201d Starting from a random finalist for each contest, after filtering out cases where the author did not understand the joke, a corpus of 651 human-created joke explanations to serve as comparison points was formed (mean/median 60/59 words, 39.3K total). We consider a model to succeed at this task if human judges, presented with (unlabeled) pairs of author/machine-generated explanations, do not show a preference for the author-generated ones.\\n\\nEvaluation metrics. For matching and quality ranking, we evaluate using accuracy. For quality ranking, we report $NYAcc$ \u2014 the average accuracy over instances where the finalist was an official New Yorker finalist \u2014 and $CrowdAcc$, where the\"}"}
{"id": "acl-2023-long-41", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A safe is on the exam table in a doctor's office. The doctor is listening to the safe with a stethoscope.\\n\\nA doctor is in her office and she is using her stethoscope on a patient. The patient in this case is just a large metal safe that is sitting on a chair.\\n\\nThe mouse is wearing a jetpack to cheat at the maze. Mice are not that intelligent.\\n\\nA rat has learned to build and operate a miniature jet pack.\\n\\nTwo scientists are observing a rat making its way through a maze to find the cheese. The rat is operating a jet pack to skip the maze and go straight to the reward at the end.\\n\\nA random sample of annotations is shown in Figure 3. We used Amazon Mechanical Turk, and paid crowdworkers a minimum of $15/hr. Low-resolution images involved special treatment: 1) we offered additional pay to crowdworkers; and 2) at least one of the annotations is conducted by an author of this work using the same HIT interface.\\n\\n3 Experiments\\n\\nWe split the 704 cartoons into 5 cross-validation splits such that entire contests are held out at test time. Task construction details are in Appendix C; modeling details (e.g., hyperparameter sweeps, task formatting) are in Appendix B.\\n\\nFrom Pixels (FP) Models\\n\\nWe explore two vision+language models. CLIP. We fine-tune CLIP ViT-L/14@366px (Radford et al., 2021) (428M parameters), which consists of a text Transformer (Vaswani et al., 2017) and a vision Transformer (Dosovitskiy et al., 2021) pretrained to align images/captions in the WebImageText corpus (400M pairs). For multiple choice, we use InfoNCE (Oord et al., 2018) to encourage the cosine similarity of the cartoon/correct answer to be higher than the incorrect ones. For zero-shot classification, we use the prompt a new yorker cartoon with...\"}"}
{"id": "acl-2023-long-41", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a man is playing with a cat in the kitchen.\\na cat is sitting on a counter and smoking a cigarette.\\na kitchen smoking,\"}"}
{"id": "acl-2023-long-41", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: Prediction results for the matching and quality ranking tasks: averages over 5 cross-validation splits.\\n\\n| Model Description | Matching Accuracy | Quality Ranking Accuracy | New Yorker Editor Selection | Crowd Selection |\\n|-------------------|-------------------|--------------------------|-----------------------------|----------------|\\n| Caption Only (T5-11B) | 19.4 | 59.4 | 64.5 | |\\n| FP CLIP ViT-L/14@336px (finetuned) | 62.3 | 57.0 | 66.9 | |\\n| OFA-Huge \u2192 T5-Large | 45.2 | 59.1 | 64.3 | |\\n| OFA-Huge \u2192 T5-11B | 51.8 | 60.3 | 65.0 | |\\n| T5-Large | 59.6 | 61.8 | 64.8 | |\\n| T5-11B | 70.8 | 62.3 | 65.6 | |\\n| GPT3-175B (finetuned) | 75.1 | 64.8 | 69.8 | |\\n| GPT-3.5 (5-shot) | 63.8 | 55.6 | 55.2 | |\\n| GPT-4 (5-shot) | 84.5 | 73.3 | 68.2 | |\\n| Human Estimate From Pixels (FP) | 94.0 | 83.7 | 64.6 | |\\n\\n- Underlined results are the best model in the From Pixels (FP) setting, where at test time, models only have access to the cartoon images.\\n- Bold results are best in the From Description (FD) setting, where at test time, models have access to human-authored descriptions of the cartoons.\\n\\nAppendix D presents these results visually.\\n\\nRight: sample predictions by CLIP (finetuned), GPT-4 (5-shot), and the caption-only baseline over a matching/ranking instance.\\n\\nCartoon by Joe Dator.\\n\\n3.1 Matching and quality ranking results\\n\\nTable 2 contains the results. Among the description models, GPT-4 (5-shot) generally performs best, e.g., achieving 84.5% accuracy on matching. It (and fine-tuned GPT-3) also perform better at predicting New Yorker editor selections than our three humans (column NY Acc: GPT-3 69.8 vs. Human estimate, 64.6), but underperform at predicting crowd selections (Crowd Acc column: GPT-4 73.3 vs. 83.7).\\n\\nWe also see that our From Pixels models leave significant headroom compared to the human performance estimates.\\n\\nOther observations include: 1) both From Pixels and From Description models mostly outperform the Caption Only baseline (even for smaller model sizes), suggesting that the models are truly using feature interactions between cartoons/captions to improve their predictive accuracy; 2) fine-tuning CLIP tends to do best for matching in the From Pixels setting, but OFA+T5-11B is competitive for quality ranking (and supports generation, see \u00a73.2); and 3) the performance difference between T5 vs. OFA exemplifies the effect of suboptimal visual recognition when shifting from the From Pixels setting to the From Description setting.\\n\\nFinally, while performance drops are incurred universally for zero-shot models, pointing towards the utility of the new annotated corpus we are releasing (\u00a72.2), GPT-4\u2019s zero-shot chain-of-thought incurs a smaller performance drop compared to other zero-shot models; see \u00a7B.7 for a sample chain-of-thought.\\n\\n3.2 Human evaluation of explanation.\\n\\nWe gather judgments from 3 crowd-workers per test instance by asking them which of a pair of explanations they prefer, and take a majority vote to determine a winner. Results and annotator agreement are in Table 3, and samples of GPT-3, GPT-4, and human joke explanations are in Figure 5. Our evaluations address seven questions:\\n\\n- Q1: Do models utilize the image context of the caption to generate better explanations? Test: T5-11B vs. Caption-only T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n- Q2: Is computer vision a bottleneck for top-quality explanation generation? Test: T5-11B (in the FD setting) vs. OFA \u2192 T5-11B. Answer: Yes. Compared to the same model trained with access to human written descriptions available at test time.\\n- Q3: Do models improve their explanations by learning to interpret human-authored captions? Test: Caption-only T5-11B vs. T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n- Q4: Are there key subtopics that are not well expressed in the image? Test: T5-11B vs. Caption-only T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n- Q5: Do models improve their explanations by learning to interpret human-authored captions? Test: Caption-only T5-11B vs. T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n- Q6: Do models improve their explanations by learning to interpret human-authored captions? Test: Caption-only T5-11B vs. T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n- Q7: Do models improve their explanations by learning to interpret human-authored captions? Test: Caption-only T5-11B vs. T5-11B. Answer: Yes. Compared to the same model trained with no access to image information, the model with image information wins in 84.7% of cases.\\n\\nWe also see that our From Pixels models leave significant headroom compared to the human performance estimates. Other observations include: 1) both From Pixels and From Description models mostly outperform the Caption Only baseline (even for smaller model sizes), suggesting that the models are truly using feature interactions between cartoons/captions to improve their predictive accuracy; 2) fine-tuning CLIP tends to do best for matching in the From Pixels setting, but OFA+T5-11B is competitive for quality ranking (and supports generation, see \u00a73.2); and 3) the performance difference between T5 vs. OFA exemplifies the effect of suboptimal visual recognition when shifting from the From Pixels setting to the From Description setting.\\n\\nFinally, while performance drops are incurred universally for zero-shot models, pointing towards the utility of the new annotated corpus we are releasing (\u00a72.2), GPT-4\u2019s zero-shot chain-of-thought incurs a smaller performance drop compared to other zero-shot models; see \u00a7B.7 for a sample chain-of-thought.\"}"}
{"id": "acl-2023-long-41", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In baseball, a \\\"closer\\\" is a relief pitcher who is responsible for finishing the game and effectively \\\"closing\\\" the opponent's chances of winning. The presence of the Grim Reaper, who is the personification of death, implies that when he plays as the \\\"closer,\\\" it's the ultimate form of closing as no one can escape death. The spectators are making a comment on how having the Grim Reaper play as a closer is an extreme and unbeatable choice.\\n\\nA closer in baseball is a type of pitcher who is good at finishing games, i.e., playing in later innings. But here, death is literally playing baseball as represented by the grim reaper. The joke is funny because it frames death as a strong \\\"closer\\\" --- death is also quite effective at finishing lives.\\n\\nThe barber's chair has been launched through the ceiling because of the spring in place of it, creating an unexpected and destructive event. The customer's calm statement \\\"He'll be back\\\" is funny because it contrasts with the dramatic situation, suggesting they are used to bizarre occurrences at this barbershop, or that the barber is going to just come back unfazed.\\n\\nUsually, if a customer leaves an establishment, it is because they are dissatisfied with the service; employees will sometimes say \\\"they'll be back\\\" in confidence that the customer will realize they are in the wrong and that they need the service. But here, the customer has exited the barber shop involuntarily by being launched by the spring into the ceiling; the other barber rightly says that \\\"he'll be back\\\", but it's for gravitational reasons, i.e., they will literally be back when gravity causes them to fall back into the barber shop from above.\\n\\nThe giant pen suggests that the characters in the scene are being drawn or controlled by someone else, much like comic strip characters. The caption, \\\"My life's a joke,\\\" is a play on the idea that their existence is a humorous creation someone else has devised, and they have no real control over their lives or the situations they find themselves in.\\n\\nA play on the word \\\"closer\\\" --- in baseball, a \\\"closer\\\" is a starting pitcher who is used in the last few innings of a game. But, here, the closer is the grim reaper, and so this is a reference to both a literal closer in baseball and a figurative closer in life.\"}"}
{"id": "acl-2023-long-41", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"better on automatic evaluation metrics for explanation like BLEU-4 and Rouge-L (see Appendix E), which suggest that the earlier family of may fit the surface features of the generation task more effectively, e.g., 5-shot GPT-3 achieves 5.07 BLEU-4 compared to 4.99 for 5-shot GPT-4. This suggests that mirroring the surface form of our explanation corpus is not sufficient to generate the highest quality explanations.\\n\\nQ7: Does our best model, GPT-4, explain jokes as well as humans?\\n\\nTest: Human vs. Few-shot GPT-4.\\n\\nAnswer: No. Human-written explanations are preferred by annotators in 68% of pairwise cases.\\n\\nWe qualitatively examine the 39/130 cases where the human reference receives 3/3 annotator votes. In these cases, the machine-generated explanations usually incorrectly interpret the image, e.g., in one case, a caption jokes about two cavepeople in a hole looking at a caveman in a cave with the caption \u201cPersonally, I\u2019m not a big fan of modern architecture.\u201d; GPT-4 incorrectly interprets the hole as \u201cmodern architecture\u201d instead of the cave. We also examine the 8/130 cases where the GPT-4 produced caption was unanimously preferred: a close reading of these cases is provided in Appendix F. In 3 of these 8 cases, the human explanations, while on the right track, had slight inaccuracies, and in the remaining 5 cases, the human and machine explanations both express the same idea, but with different styles (GPT-4\u2019s sometimes arguably being more formal, detailed, or fluent).\\n\\n3.3 Error Analysis for Matching\\n\\nWe conduct an error analysis of a performant from pixels model (CLIP ViT-L/14@336px fine-tuned), and a performant from description model (GPT3-175B finetuned). We concatenate the test set predictions over the 5 cross validation splits, and ask:\\n\\nQ8: Are some contests more difficult than others?\\n\\nAnswer: Yes. Details: We conduct a $\\\\chi^2$ test by forming a contest-by-correctness (704-by-2) contingency table, aggregating over the 3-6 matching instances for each contest, and find that errors are clustered according to contest ($p<.05$ for both CLIP and GPT-3). There\u2019s a moderate Spearman $\\\\rho = .28, p \\\\ll .001$ between the per-contest accuracy between the models, but (as a null hypothesis) only a slight correlation between contest date and difficulty for either ($\\\\rho = .07/-.08, p = .08/-.05$). When the models\u2019 predictions agree, they are correct 87% of the time. When GPT-3 is wrong, CLIP is right only 38% of the time; under the null hypothesis that their errors are uncorrelated, CLIP\u2019s accuracy would be 62% ($p \\\\ll .001$ errors are uncorrelated, permutation test). However, when we attempt to identify consistent factors that predict contest difficulty using various visual/linguistic predictors, we find hard vs. easy difficult to predict a priori; our best classifiers perform only slightly above random. We will distribute the hard vs. easy contest lists as a resource for future work.\\n\\n4 Related Work\\n\\nHumor. Raskin (1979) and Attardo (2008) highlight three \u201cgreat families\u201d of theories of the roots of humor: 1) hostility, claims of superiority over someone or something (Gruner, 1978; Billig, 2005); 2) release of a constraint (Freud, 1905; Fry, 1963; Mindess, 1971) and 3) incongruity, (sometimes \u201cincongruity-resolution\u201d; Mulder and Nijholt, 2002) the introduction (and subsequent resolution) of generally incompatible contexts (Schopenhauer, 1818; Shultz, 1976). Shahaf et al. (2015) note that most New Yorker caption contest cartoons involve incongruous situations.\\n\\nNLP + The Caption Contest. King et al. (2013), Shahaf et al. (2015), and Radev et al. (2016) analyze 5, 16, and 50 New Yorker Caption Contests, respectively. Best-performing features for identifying the funniest among a set of caption choices include: perplexity, match to image setting and uncanniness description, readability, proper nouns (Shahaf et al., 2015), overlap with WordNet\u2019s (Fellbaum, 1998) \u201cperson\u201d and \u201crelative\u201d synsets, lexical centrality among submissions (Radev et al., 2016, inspired by Mihalcea and Pulman (2009)), and sentiment (both papers). Our \u201clocation\u201d and \u201cuncanny description\u201d annotations are direct analogs of the \u201ccontext\u201d and \u201canomaly\u201d tags of Shahaf et al. (2015), and our data incorporates that generously released by the previous researchers. Our extensions are (a) the addition of two novel tasks; (b) using new data/resources/models to curate ranking pairs (see assigned as negative choices (2646-by-2 table, $p=.92/.79$ for GPT3/CLIP).\"}"}
{"id": "acl-2023-long-41", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u00a72); and (c) evaluating two distinct audience preferences: New Yorker editors vs. \\\"the crowd\\\". Appendix H highlights efforts beyond the scope of peer reviewed AI venues, e.g., blog posts.\\n\\nMeasuring preferences over captions. While humor is ultimately subjective, work on the contest has studied modeling average preferences of raters. Tanczos et al. (2017) design quality ranking algorithms for the caption contest, framed as identifying the best \\\"arm\\\" in a multi-armed bandit setting; their crowdsourcing system NEXT (Jamieson et al., 2015) is used by The New Yorker. It does not directly use the content of the cartoons/contests. The result is Jain et al. (2020)'s continuously updated corpus, from which we draw some of our data.\\n\\nMultimodal and computational humor. Chandrasekaran et al. (2016) explore humor recognition in images, and Castro et al. (2019); Hasan et al. (2019); Patro et al. (2021); Hasan et al. (2021) explore laughter prediction in TED-talks/sitcoms. Tsakona (2009); Fallianda et al. (2018) study political cartoons. Chakrabarty et al. (2022) recently proposed a version of NLI for figurative language, which can be humorous. Some work has tried to detect whether a sentence is humorous or not (Blinov et al., 2019; Annamoradnejad and Zoghi, 2020). More difficult to evaluate (Valitutti, 2011) are setups where the goal is to automatically generate humorous content in various contexts (Binsted and Ritchie, 1994; Stock and Strapparava, 2003; Mihalcea and Strapparava, 2005, 2006; Wang and Wen, 2015; Chandrasekaran et al., 2018; Yoshida et al., 2018; Sundaram, 2018; Shimomoto et al., 2019); a survey is provided by Amin and Burghardt (2020).\\n\\nExplaining humor. In the taxonomy of Tan (2022), joke explanations are most related to proximal mechanisms: \\\"This type of explanation attempts to provide the mechanism behind the predicted label, i.e., how to infer the label from the text\\\", or efficient cause a la Aristotle (Lombrozo, 2006). Chowdhery et al. (2022) undertake a qualitative exploration of (non-visual) joke explanations.\\n\\n5 Conclusion\\n\\nWe demonstrate that today's vision and language models still cannot recognize caption relevance, evaluate (at least in the sense of reproducing crowdsourced rankings), or explain The New Yorker Caption Contest as effectively as humans can. However, the partial capacity of today's AI is still substantial, and may be sufficient for models to serve as creative collaborators, e.g., as brainstorming assistants for humorists/cartoonists. Specifically: 1) our matching/quality ranking models could help entrants receive quantitative feedback on the relevance/predicted quality of their submissions, and 2) the annotated corpus+explanations we introduce could be repurposed for generation (we explore generation of novel cartoons/captions in Appendix G). Finally, a promising avenue for future work focused on generating humorous captions (c.f. our focus of humor \\\"understanding\\\" benchmarks) would be to operationalize the feedback provided by our matching/ranking models in an reinforcement learning from human feedback (RLHF) loop.\\n\\nA last remark.\\n\\nWe cannot claim to know whether the human-machine 'humor understanding gap' will be closed sooner or later. But we encourage other researchers to have as much fun with the topic as we did!\\n\\n6 Limitations\\n\\nThe New Yorker Cartoon Caption Contest represents a narrow slice of humor, deriving from a particular language, region, history, culture, style, and set of conventions. Hence, the results of this study do not represent or cover all types of humor. Our framing of the quality ranking task could be interpreted as seemingly prescriptive (i.e., that joke A is \\\"objectively\\\" better than joke B), but New Yorker editorial selections should not be taken as ground truth for funniness; disagreement about what is funny is expected and valid. Our tasks operationalize the prediction of only average preferences (rather than individual ones), and these preferences may include a partiality or bias towards items that conform to the characteristics of prior contest winners or published New Yorker cartoons. Finally, the explanations in our annotated corpus were largely written by a single author of this paper. While a larger pool of the crowdworkers judged these explanations to be of higher quality in comparison to machine generations, future work would be well-suited to compare the person-to-person variance in explaining why particular jokes are funny.\\n\\n16Or never. Is never good for you?\"}"}
{"id": "acl-2023-long-41", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nWe thank the cartoonists and contest entrants for their wonderful efforts! We additionally thank our crowd annotators for their diligent work, Lisa Watkins for contributing to the human performance estimates, and the anonymous reviewers for their constructive comments. This work was funded in part by DARPA MCS through NIWC Pacific (N66001-19-2-4031), the Allen Institute for AI, and a Google Focused Research Award. Jack Hessel conducted initial work while at Cornell University. Ana Marasovi\u0107 conducted this work while at The Allen Institute for AI. Rowan Zellers conducted this work while at University of Washington.\\n\\nReferences\\n\\nMiriam Amin and Manuel Burghardt. 2020. A survey on approaches to computational humor generation. In The 4th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature.\\n\\nIssa Annamoradnejad and Gohar Zoghi. 2020. BERT: Using BERT sentence embedding for humor detection. arXiv preprint arXiv:2004.12765.\\n\\nSalvatore Attardo. 2008. A primer for the linguistics of humor. The primer of humor research, 8:101\u201355.\\n\\nMichael Billig. 2005. Laughter and ridicule: Towards a social critique of humour. Sage.\\n\\nKim Binsted and Graeme Ritchie. 1994. An implemented model of punning riddles. In AAAI.\\n\\nVladislav Blinov, Valeria Bolotova-Baranova, and Pavel Braslavski. 2019. Large dataset and language model fun-tuning for humor recognition. In ACL.\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners. NeurIPS.\\n\\nSantiago Castro, Devamanyu Hazarika, Ver\u00f3nica P\u00e9rez-Rosas, Roger Zimmermann, Rada Mihalcea, and Soujanya Poria. 2019. Towards multimodal sarcasm detection (an obviously perfect paper). In ACL.\\n\\nTuhin Chakrabarty, Arkadiy Saakyan, Debanjan Ghosh, and Smaranda Muresan. 2022. FLUTE: figurative language understanding and textual explanations. In EMNLP.\\n\\nArjun Chandrasekaran, Devi Parikh, and Mohit Bansal. 2018. Punny captions: Witty wordplay in image descriptions. In NAACL.\\n\\nArjun Chandrasekaran, Ashwin K. Vijayakumar, Stanislaw Antol, Mohit Bansal, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. 2016. We are humor beings: Understanding and predicting visual humor. In CVPR.\\n\\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fidus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. 2022. PaLM: Scaling language modeling with pathways. arXiv preprint arXiv:2204.02311.\\n\\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR.\\n\\nFallianda, Rani Yuni Astiti, and Zulvy Alivia Hanim. 2018. Analyzing humor in newspaper comic strips using verbal-visual analysis. Lingua Cultura, 12(4):383\u2013388.\\n\\nChristiane Fellbaum. 1998. WordNet: An Electronic Lexical Database. Bradford Books.\\n\\nSigmund Freud. 1905. Jokes and their Relation to the Unconscious, volume 8 of The Standard Edition of the Complete Psychological Works of Sigmund Freud. Hogarth, London.\\n\\nWilliam F. Fry. 1963. Sweet madness: A study of humor. Pacific Books, Palo Alto.\\n\\nCharles R. Gruner. 1978. Understanding laughter: The workings of wit & humor. Nelson-Hall, Chicago.\\n\\nKilem Gwet. 2014. Handbook of Inter-Rater reliability: The Definitive Guide to Measuring the Extent of Agreement Among Raters, 4th edition edition. Advanced Analytics, LLC.\"}"}
{"id": "acl-2023-long-41", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Md Kamrul Hasan, Sangwu Lee, Wasifur Rahman, Amir Zadeh, Rada Mihalcea, Louis-Philippe Morency, and Ehsan Hoque. 2021. Humor knowledge enriched transformer for understanding multimodal humor. In AAAI.\\n\\nMd Kamrul Hasan, Wasifur Rahman, AmirAli Bagher Zadeh, Jianyuan Zhong, Md Iftekhar Tanveer, Louis-Philippe Morency, and Mohammed (Ehsan) Hoque. 2019. UR-FUNNY: a multimodal language dataset for understanding humor. In EMNLP.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020. The curious case of neural text degeneration. In ICLR.\\n\\nLalit Jain, Kevin Jamieson, Robert Mankoff, Robert Nowak, and Scott Sievert. 2020. The New Yorker cartoon caption contest dataset.\\n\\nKevin G. Jamieson, Lalit Jain, Chris Fernandez, Nicholas J. Glattard, and Rob Nowak. 2015. NEXT: A system for real-world development, evaluation, and application of active learning. In NeurIPS.\\n\\nBen King, Rahul Jha, Dragomir Radev, and Robert Mankoff. 2013. Random walk factoid annotation for collective discourse. In ACL.\\n\\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2022. Large language models are zero-shot reasoners. In NeurIPS.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text summarization branches out.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. 2014. Microsoft COCO: Common objects in context. In ECCV.\\n\\nTania Lombrozo. 2006. The structure and function of explanations. Trends in Cognitive Sciences, 10(10):464\u2013470.\\n\\nAna Marasovi\u0107, Iz Beltagy, Doug Downey, and Matthew E. Peters. 2022. Few-shot self-rationalization with natural language prompts. In Findings of NAACL.\\n\\nRada Mihalcea and Stephen Pulman. 2009. Characterizing humour: An exploration of features in humorous texts. In Proceedings of the 8th International Conference on Computational Linguistics and Intelligent Text Processing, page 337\u2013347, Berlin, Heidelberg. Springer-Verlag.\\n\\nRada Mihalcea and Carlo Strapparava. 2005. Making computers laugh: Investigations in automatic humor recognition. In EMNLP.\\n\\nRada Mihalcea and Carlo Strapparava. 2006. Technologies that make you smile: Adding humor to text-based applications. IEEE Intelligent Systems, 21(5):33\u201339.\\n\\nHarvey Mindess. 1971. Laughter and Liberation. Nash.\\n\\nPamela Mishkin, Matt Daniels, Russell Goldenberg, Ilia Blinderman, and James Yu. 2022. The pudding caption contest experiments. https://pudding.cool/projects/caption-contest/. Accessed: 2022-04-01.\\n\\nMatthijs P. Mulder and Antinus Nijholt. 2002. Humour research: State of the art. Centre for Telematics and Information Technology, University of Twente.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. 2018. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748.\\n\\nOpenAI. 2023. Gpt-4 technical report.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In ACL.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. 2019. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS.\\n\\nBadri N. Patro, Mayank Lunayach, Deepankar Srivastava, Sarvesh, Hunar Singh, and Vinay P. Namboodiri. 2021. Multimodal humor dataset: Predicting laughter tracks for sitcoms. In WACV.\\n\\nAdam Poliak, Jason Naradowsky, Aparajita Haldar, Rachel Rudinger, and Benjamin Van Durme. 2018. Hypothesis only baselines in natural language inference. In *SEM.\\n\\nMatt Post. 2018. A call for clarity in reporting BLEU scores. In WMT.\\n\\nDragomir Radev, Amanda Stent, Joel Tetreault, Aashish Pappu, Aikaterini Iliakopoulou, Agustin Chanfreau, Paloma de Juan, Jordi Vallmitjana, Alejandro Jaimes, Rahul Jha, and Robert Mankoff. 2016. Humor in collective discourse: Unsupervised funniness detection in the New Yorker cartoon caption contest. In LREC.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning transferable visual models from natural language supervision. In ICML.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR.\"}"}
{"id": "acl-2023-long-41", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Victor Raskin. 1979. Semantic mechanisms of humor. In Annual Meeting of the Berkeley Linguistics Society, volume 5, pages 325\u2013335.\\n\\nJeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. 2020. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In KDD.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In EMNLP.\\n\\nArthur Schopenhauer. 1818. The world as will and idea, volume 1.\\n\\nDafna Shahaf, Eric Horvitz, and Robert Mankoff. 2015. Inside jokes: Identifying humorous cartoon captions. In KDD.\\n\\nErica K Shimomoto, Lincon S Souza, Bernardo B Gatto, and Kazuhiro Fukui. 2019. News2meme: An automatic content generator from news based on word subspaces from text and image. In Conference on Machine Vision Applications.\\n\\nThomas R Shultz. 1976. A cognitive-developmental analysis of humour. Transaction Publishers.\\n\\nOliviero Stock and Carlo Strapparava. 2003. Getting serious about the development of computational humor. In IJCAI.\\n\\nRajesh Shanmuga Sundaram. 2018. Generation of Humorous Caption for Cartoon Images Using Deep Learning. Ph.D. thesis, Texas A&M University-Commerce.\\n\\nChenhao Tan. 2022. On the diversity and limits of human explanations. In NAACL.\\n\\nErvin Tanczos, Robert Nowak, and Bob Mankoff. 2017. A KL-LUCB algorithm for large-scale crowdsourcing. In NeurIPS.\\n\\nPaul Trichelair, Ali Emami, Adam Trischler, Kaheer Suleman, and Jackie Chi Kit Cheung. 2019. How reasonable are common-sense reasoning tasks: A case-study on the Winograd schema challenge and SW AG. In EMNLP.\\n\\nVilly Tsakona. 2009. Language and image interaction in cartoons: Towards a multimodal theory of humor. Journal of Pragmatics, 41(6):1171\u20131188.\\n\\nAlessandro Valitutti. 2011. How many jokes are really funny? In Human-Machine Interaction in Translation: Proceedings of the 8th International NLPCS Workshop.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. NeurIPS.\\n\\nDavid Wallace. 2022. Lecture notes for MIT 2.00b toy product design: Innovation and associations.\\n\\nPeng Wang, An Yang, Rui Men, Junyang Lin, Shuai Bai, Zhikang Li, Jianxin Ma, Chang Zhou, Jinygren Zhou, and Hongxia Yang. 2022. Unifying architectures, tasks, and modalities through a simple sequence-to-sequence learning framework. In ICML.\\n\\nWilliam Yang Wang and Miaomiao Wen. 2015. I can has cheezburger? a nonparanormal approach to combining textual and visual information for predicting and generating popular meme descriptions. In NAACL.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou. 2022. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS.\\n\\nWhite, E. B. 1941. Preface. In E. B. White and Katherine S. White, editors, A Subtreasury Of American Humor, page xvii. The original version of this quote appeared as a preview in The Saturday Review (1941), credited to both Whites. But, the quote appears in the preface to A Subtreasury (1941) with authorship solely credited to E.B.. We thus credited the quote itself to E.B., and credited both E.B. and K.S. as editors of the anthology in which it appears in non-preview form.\\n\\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta, Mark Riedl, and Yejin Choi. 2022. Reframing human-AI collaboration for generating free-text explanations. In NAACL.\\n\\nHannah Wilson. 2019. Project four - nobody knows you're a bot.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In EMNLP: System Demonstrations.\\n\\nKota Yoshida, Munetaka Minoguchi, Kenichiro Wani, Akio Nakamura, and Hirokatsu Kataoka. 2018. Neural joking machine: Humorous image captioning. In CVPR Language & Vision Workshop.\\n\\nMichael Zelenko and Frank Bi. 2015. On the internet, nobody knows you're a machine.\\n\\nAndy Zeng, Maria Attarian, Brian Ichter, Krzysztof Choromanski, Adrian Wong, Stefan Welker, Federico Tombari, Aveek Purohit, Michael Ryoo, Vikas Sindhwani, Johnny Lee, Vincent Vanhoucke, and Pete Florence. 2022. Socratic models: Composing zero-shot multimodal reasoning with language. arXiv preprint arXiv:2204.00598.\"}"}
{"id": "acl-2023-long-41", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Crowdworking Details\\n\\nWe use three Mechanical Turk interfaces to gather data. These are:\\n\\n1. Cartoon description (Figure 6). We ran this HIT 3 times per cartoon.\\n\\n2. Cartoon wikipedia links (Figure 7). We ran this HIT 2 times per cartoon.\\n\\n3. Pairwise explanations (Figure 8). We ran this HIT 2.7K times to facilitate the comparisons in \u00a73.2 Qualification+training rounds.\\n\\nTo ensure our set of crowdworkers were properly trained for the annotations, we ran two types of qualification rounds: one for the description/link HITs, and one for the pairwise explanation HITs.\\n\\nFor the description/link HITs, our qualification round was based off an earlier and more involved HIT that involved a joint setup where, for 3 cartoons, users described cartoons, highlighted image regions, explained jokes, etc. We allowed users from \\\\{AU, CA, NZ, GB, US\\\\} with 10K prior approved HITs and a minimum acceptance rate of 97% on their previous HITs to participate. Some of the cartoons and captions contain mature themes; we provided the recommended disclaimer for this and other HITs: \\\"WARNING: This HIT may contain adult content. Worker discretion is advised.\\\"\\n\\nWe manually graded the responses of 30 annotators in a qualification round, and qualified 21. Through a mix of the older, more involved HIT and the streamlined HIT in Figure 6, which is a pared-down version of the original HIT without captions, we gathered descriptions of the cartoons. We also gathered the locations/Wikipedia entity links from the qualified annotators. These annotations were gathered in mid-to-late 2021.\\n\\nAbout 9 months later, we conducted a second set of Mechanical Turk studies for pairwise judgment evaluations for explanation. A second qualification round was run, in which we asked annotators to rate the quality of several joke explanations which we manually selected to be good/bad across various desirable axes. We qualified 29 out of 51 annotators who attempted the HIT via manual inspection of their responses. This set of annotators were given access to the final pairwise-judgment HITs.\\n\\nCrowdworking studies of standard computer vision corpora (involving no personal disclosures) are not required by our IRB to be reviewed by them. While the authors of this work are not lawyers and this is not legal advice, this opinion is based on United States federal regulation 45 CFR 46, under which this study qualifies and as exempt. We hashed crowdworker IDs in the public release so annotations cannot be back-traced to individual workers.\\n\\nB Additional Experimental Details\\n\\nB.1 From Description details\\n\\nFor each cartoon, we have multiple annotations of each type, as detailed in \u00a72.2. During training, we utilize all location/description/uncanny description/sets of links, but at test time, we randomly sample a single set of these four annotation types such that inference requires only a single forward pass. For fair comparison, the randomly sampled description available at test time is held constant between all methods.\\n\\nMore detail about how we managed multiple annotations: because we have 2 locations $\\\\times$ 3 descriptions $\\\\times$ 3 uncanny descriptions $\\\\times$ 2 entity links, there are potentially 36 possible combinations we could use to form a from description instance for each cartoon. However: tuples are constructed at the annotator level to account for potential dependencies between annotation types: because descriptions/uncanny descriptions were collected in the same HIT, the uncanny description may reference entities from the description because they were authored at the same time by the same annotator in sequence. Similarly, the (locations, links) were collected in the same HIT. So, we instead consider all six possible tuples holding author constant between HITs, i.e., 3 (description, uncanny description) $\\\\times$ 2 (location, link). For test time, select a single random valid tuple of annotations for evaluation, which is fixed for all comparisons.\\n\\nB.2 CLIP\\n\\nFor fine-tuning results, we do linear warmup for 200 steps and conduct a small learning rate search on the validation set for each cross-validation split independently between \\\\{5$e^{-5}$, 1$e^{-5}$, 5$e^{-6}$\\\\}, keeping batch size fixed at 32. To keep the entire cartoon in the 336px square input, we resize and pad. At training time, we perform data augmentations on the image, including: random horizontal flipping, random color jittering, and random grayscaling.\"}"}
{"id": "acl-2023-long-41", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Instructions, example, and interface for the Cartoon Description HIT. We gather, but do not use, the final \u201cWhich question?\u201d annotation in our experiments.\"}"}
{"id": "acl-2023-long-41", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2023-long-41", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use validation-set early stopping on cross-entropy loss, and fine-tune OFA separately for each cross-validation split. After fine-tuning, we select the top-1 prediction according to beam search (n=5). We finetune OFA Huge with a learning rate of $5 \\\\times 10^{-5}$, which was determined via a small grid search over the first cross-validation split. We use label-adjusted smoothed cross entropy loss as implemented by the OFA authors with smoothing of 0.1. We train for a maximum of 7 epochs with a warmup ratio of 6%. For each image, we query for the four different types of annotations shown in Figure 3. To facilitate this, in addition to providing OFA with the image, we also provide it with a per-annotation-type prompt:\\n\\n1. for locations: \\\"Where does this take place?\\\"\\n2. for descriptions: \\\"Describe this image.\\\"\\n3. for uncanny: \\\"What's unusual about this image?\\\"\\n4. for entities: \\\"What entities are there?\\\"\\n\\nIn early experiments, instead of composing with a language model, we did attempt to fine-tune OFA directly for the explanation task. However, we found that the resulting perplexity (roughly 300) was significantly higher than for other fine-tuned models, with the errors difficult to diagnose.\\n\\nB.4 T5-Large/T5-11B.\\nFor T5-Large, we conduct a small, per-cross-validation split learning rate search between{$1 \\\\times 10^{-4}$, $1 \\\\times 10^{-5}$, $5 \\\\times 10^{-5}$} and keep batch size fixed at 64. For T5-11B we use a fixed learning rate of $1 \\\\times 10^{-5}$ and a batch size of 64.\\n\\nB.5 GPT-3 Zero Shot/In Context\\nWe use GPT-3's davinci-text-002 model for our main zero shot and in-context learning experiments. Examples of zero-shot prompts for all tasks are given in Figure 9. The in-context prompts are similar, except they contain 5 random samples from the training set. A full, randomly selected in-context prompt for the explanation generation task is given in Figure 10.\\n\\nB.6 GPT-3 Fine-tuning\\nWe use the OpenAI fine-tuning API to fine-tune davinci, a 175B parameter language model. While the precise details of how the API works are not currently available (e.g., which parameters are updated, or which version of davinci is used), we use the same cross-validation setup as for the other models so that the results are comparable. The total fine-tuning cost is approximately (3 tasks) \u00d7 (5 cross-val splits) \u00d7 (40 dollars per fine-tune) = 600 dollars.\\n\\nB.7 GPT 3.5/GPT-4 Details\\nBetween submitting this work and its acceptance, OpenAI released two new models, GPT-3.5 (sometimes called ChatGPT when accessed through the chat interface) and GPT-4; we updated our results to include these models. Figure 11 provides an example of a prompt/response in the new \\\"Chat\\\" API, which requires a more structured conversational prompt compared to the GPT-3 \\\"Completion\\\" API; this prompt includes a \\\"system\\\" prompt, which describes the desired behavior of the model, e.g., \\\"You are CaptionContestGPT...\\\" We sample with default hyperparameters in all cases. The cost of GPT 3.5 is an order of magnitude less than GPT-4. In total our GPT-4 queries cost on the order of $4K.\\n\\nC Task Construction Details\\nIdentification of High Quality Captions. For each contest, our first step is to identify a set of high quality captions; these are involved in construction of instances for all three tasks. For cases where we have access to the three official New Yorker finalists, all are automatically added to the high quality set. Next, for cases where we have crowd ratings, we consider the top 5 crowd ranked captions according to the mean score provided by Jain et al. (2020). From these top 5, we select 3 diverse candidates among these using a semantic deduplication method: specifically, we compute the SBERT (Reimers and Gurevych, 2019) vector for each candidate using paraphrase-MiniLM-L6-v2, compute a hierarchical clustering of the candidates, and sample a single candidate from each cluster \u2014 the result is a set of candidates that is representative of all clusters. In total, there are 2.7K high quality captions across 704 contests. Each contest either has 3 high quality captions (coming from the official New Yorker finalists or, if those aren't available, highly crowd-rated options), or 6 (if both official finalists and crowd rated are available).\"}"}
