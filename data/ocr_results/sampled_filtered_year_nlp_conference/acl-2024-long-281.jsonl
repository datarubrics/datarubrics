{"id": "acl-2024-long-281", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eliciting Better Multilingual Structured Reasoning from LLMs through Code\\n\\nBryan Li1\u2217, Tamer Alkhouli2\u2020, Daniele Bonadiman2, Nikolaos Pappas2, Saab Mansour2\\n\\n1University of Pennsylvania, bryanli@seas.upenn.edu\\n2/aws AI Labs, {alkhouli, dbonadim, nppappa, saabm}@amazon.com\\n\\nAbstract\\n\\nThe development of large language models (LLM) has shown progress on reasoning, though studies have largely considered either English or simple reasoning tasks. To address this, we introduce a multilingual structured reasoning and explanation dataset, termed xSTREET, that covers four tasks across six languages. xSTREET exposes a gap in base LLM performance between English and non-English reasoning tasks.\\n\\nWe then propose two methods to remedy this gap, building on the insight that LLMs trained on code are better reasoners. First, at training time, we augment a code dataset with multilingual comments using machine translation while keeping program code as-is. Second, at inference time, we bridge the gap between training and inference by employing a prompt structure that incorporates step-by-step code primitives to derive new facts and find a solution. Our methods show improved multilingual performance on xSTREET, most notably on the scientific commonsense reasoning subtask. Furthermore, the models show no regression on non-reasoning tasks, thus demonstrating our techniques maintain general-purpose abilities.\\n\\n1 Introduction\\n\\nThe ability to perform complex reasoning tasks is fundamental to human intelligence, where multiple steps of thought are required. Complex reasoning remains an open-problem for large language models (LLMs), despite some recent progress. Prior works consider complex reasoning tasks specified only in English. Such an English-centric perspective provides a limited assessment of the underlying reasoning capabilities of LLMs, given any specific language is largely a surface-form representation.\\n\\n\u2217Work done during an internship at Amazon\\n\u2020corresponding author\\n\\n1https://github.com/amazon-science/xstreet released under CC-BY-4.0.\\n\\nFigure 1: An overview of our methods to improve multilingual structured reasoning. First (top), we create the translated code comments (TCC) dataset, and use it in a fine-tuning setup. Second (bottom), we use the resulting LLM for inference on reasoning tasks. We find the most success with a code prompt format that bridges the representations between training and inference.\\n\\nThis motivates our first inquiry into the multilingual complex reasoning capabilities of LLMs.\\n\\nWe introduce the xSTREET reasoning and explanation dataset (as shown in Figure 2). xSTREET covers 4 tasks, and extends the English STREET benchmark (Ribeiro et al., 2022) to 5 additional diverse languages, inheriting the source\u2019s expert annotations and structured graphs for reasoning steps (7.8 average steps/answer). The tasks cover arithmetic, logic and science commonsense problems.\\n\\nWe perform machine translation for the training and development data splits, and also perform human post-editing to the test sets, to ensure a high quality multilingual benchmark. We use xSTREET to evaluate several LLMs, identifying the multilingual setting as significantly challenging.\\n\\nTo remedy the non-English reasoning gap, we turn to the widely accepted hypothesis that LLMs trained on code are better at reasoning than those trained only on text. This code and reasoning hypothesis...\"}"}
{"id": "acl-2024-long-281", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A white rabbit can hop 15 meters in one minute. The brown rabbit hops 12 meters per minute. What is the total distance the two rabbits will hop in 5 minutes?\\n\\nWhite rabbit + brown rabbit = 15 + 12 = 27\\n\\n5 minutes * 27 = 135 meters\\n\\nThe answer is 135.\"}"}
{"id": "acl-2024-long-281", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1 Code & Reasoning Hypothesis for LLMs\\nThis hypothesis arose from empirical evidence by several concurrent works. Suzgun et al. (2022) state, \u201cCodex, trained on both code and text data, shows better performance in following task instructions and exploiting algorithmic patterns based on the prompt exemplars.\u201d Liang et al. (2023) state, \u201cfor reasoning-intensive scenarios, we find that the code models, especially Codex davinci v2, consistently outperform the text models, even on synthetic reasoning scenarios posed in natural language.\u201d Hendy et al. (2023) state that \u201cWe hypothesize that the models acquire their reasoning capabilities through training on natural language multilingual data along with programming languages data\u201d. In summary, these works provide evidence that training LLMs on code serves as indirect supervision for complex reasoning tasks. One of our major goals is to explore the extent to which this hypothesis holds beyond English.\\n\\n2.2 Code Prompts for Complex Reasoning\\nReasoning tasks posed in natural language can be reformulated as code prompts. Using these code-like structures to interact with code-LLMs better aligns the representations seen at training time with those at inference time. Madaan et al. (2022) use few-shot prompting on the Codex LLM to convert tasks into Python graphs, deal with structured commonsense tasks. Zhang et al. (2023) proceed similarly, but for causal reasoning tasks. Chen et al. (2023) consider arithmetic reasoning tasks, and then execute the LLM-generated code on an external interpreter. The reformulation process from natural language specification to code prompts is an open-ended one, requiring manual annotation effort, creativity, and trial and error.\\n\\nWhile these works use code prompts for complex reasoning tasks with classification or numerical outputs, as we did, code prompts can also be applied to tasks with generative outputs, such as knowledge graph construction (Bi et al., 2023) and story understanding (Dong et al., 2023). To the best of our knowledge, our work is the first to use code prompts in multiple languages.\\n\\n2.3 Multilingual Reasoning for LLMs\\nThe MEGA benchmark (Ahuja et al., 2023) covers 70 languages and 16 tasks. MEGA considers only simple reasoning tasks, which, as discussed earlier, limits our understanding of how well LLMs can reason across languages.\\n\\nMGSM (Shi et al., 2022) is an arithmetic reasoning dataset in 10 languages, translated from GSM8K (Cobbe et al., 2021). They find that the chain-of-thought technique (CoT) (Wei et al., 2022b), by adding to the prompt few-shot examples of step-by-step reasoning, is also effective in the multilingual setting. Interestingly, they find that for non-English questions, English CoT outperforms native language CoT. They further emphasize the reasoning ability increases with model scale. Our xSTREET benchmark is a more comprehensive view of multilingual complex reasoning. xSTREET covers not only arithmetic, but adds logic and science tasks, has many more entries, and has ground-truth structured reasoning annotations.\\n\\n2.4 STREET Complex Reasoning Benchmark\\nThe STREET benchmark is a composite of several complex reasoning tasks (Ribeiro et al., 2022). The work adds expert human annotations for multi-premise, multi-step explanations. Each task\u2019s explanation is structured in a reasoning graph. Reasoning graphs, as shown in Figure 2, consist of nodes which contain statements, and edges that connect nodes.\\n\\nSource Tasks\\nThe tasks and answer formats are:\\n\u2022 ARC science commonsense questions (multiple-choice)\\n\u2022 GSM8k arithmetic word problems (number)\\n\u2022 AQUA_RAT arithmetic word problems (multiple-choice)\\n\u2022 AR_LSAT logic problems from a standardized test (multiple-choice)\\n\\nLinearized prompt format\\nWhile a reasoning graph is abstract, to interface with an LLM, Ribeiro et al. (2022) use linearized prompts. This represents a graph as a sequence of tokens, as shown in Figure 3. Statements are given a number index;\"}"}
{"id": "acl-2024-long-281", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"output statements (i.e., reasoning steps) include a trace of the nodes leading to the new statement. Problems with the linearized format arise in that it is task-specific, and that it diverges from LLM\u2019s training data distribution. While in-context learning can help the model pattern-match the output format, the underlying reasoning abilities of the LLM may not be properly elicited. Following Madaan et al. (2022), we argue that interfacing with a code-LLM through code prompts is a more \u201cintuitive\u201d way for the LLM to reason through a task, leading to our novel code prompts format in \u00a75.\\n\\n2.5 Source Code Dataset\\n\\nThe Stack is a 3.1 TB dataset of permissively licensed source code in 30 programming languages (Kocetkov et al., 2022). In this work, we utilize the official small subset, and consider only 3 popular programming languages: Java, JavaScript, Python (10k files each, 30k total).\\n\\n3 Multilingual Complex Reasoning Benchmark: xSTREET\\n\\nWe create the xSTREET dataset by translating STREET into 5 languages: Arabic (ar), Spanish (es), Russian (ru), Chinese (zh), and Japanese (ja). These languages have linguistic and script diversity; furthermore, they are the languages used in many online programming help websites. To create the xSTREET test split, we hire expert human translators for all 5 languages through an internal team (detailed in \u00a79). Translators are tasked with post-editing the machine translation of one sentence at a time; for context, they can refer to the entire STREET entry the sentence comes from. After receiving the translations, we re-use the reasoning graph edges, and replace English nodes with the translations to create xSTREET. This process is shown in Figure 2. We therefore extend the 914 English entries in STREET to 5484 examples in xSTREET (914 * 6 languages).\\n\\nTo create the xSTREET train and development splits, we use machine translation. We then asked native speakers to evaluate the quality of 10 random sampled translations of each language. Annotators gave feedback that, despite some errors, the translations were of reasonable enough quality to use for training purposes.\\n\\n| Dataset         | # entry | # sents/entry | avg # sents/entry |\\n|-----------------|---------|---------------|-------------------|\\n| ARC             | 340     | 4334          | 12.7              |\\n| AQUA RAT        | 254     | 3436          | 13.5              |\\n| AR LSAT         | 50      | 1158          | 23.2              |\\n| GSM8k           | 270     | 2255          | 8.4               |\\n| **Total**       | **914** | **11183**     | **12.2**          |\\n| **x6 languages**| **5484**| **67098**     |                   |\\n\\nTable 1: Statistics for the xSTREET test benchmark. Dataset statistics for the xSTREET test benchmark are given in Table 1.\\n\\n4 Code with Multilingual Comments as Indirect Supervision for Reasoning\\n\\nTaking the idea of using code for reasoning, and comments for multilinguality a step further, we address the question: can multilingual code serve as indirect supervision for multilingual reasoning? In other words, we investigate whether the code & reasoning hypothesis holds multilingually. We therefore propose a lightweight fine-tuning recipe, which consists of creating a multilingually commented code dataset, then fine-tuning on it, which serves as indirect supervision for downstream reasoning tasks.\\n\\n4.1 Translated Code Comments Dataset (TCC)\\n\\nThe first step of the recipe is creating a source code dataset with translated code comments, termed TCC. For each file from the source dataset, and for each target language, we perform the following. We parse the code to extract out comments, translate comments into the target language, then replace the original comments with translations. This is depicted in Appendix Figure 7.\\n\\nWe use two simple filters: for source code files that A) have >5 comments, and B) whose comments are over 50% in English. This filters 30k source code files down to 20k. After translating into 5 additional languages, TCC consists of 20k*6=120k files total. See Appendix Table 5 for dataset statistics.\\n\\n4.2 Train Time: fine-tuning on TCC\\n\\nIn the second step, we leverage low-rank adaptation (LoRA) (Hu et al., 2021) to finetune instruction-\"}"}
{"id": "acl-2024-long-281", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use two methods to preserve the original model's capabilities despite the additional finetuning. First is by using LoRA itself, as it keeps the original base model's parameters frozen and introduces only a few learned parameters. Secondly, we replay 100k examples from the base model's training data, xP3 (Muennighoff et al., 2022), in a multitask setup with the TCC LM task.\\n\\nThe recipe for a reasoning-enhanced LLM is now complete, and this is depicted in Figure 1.\\n\\n5 Multilingual Complex Reasoning as a Downstream Task\\n\\nWe hypothesize that structure, when applied to reasoning problems formulated in different languages, can abstract away some of the language-specific details, better surfacing the reasoning steps needed for a model. We thus propose the SIM (Select-and-infer multilingual comments) code prompts for complex reasoning tasks.\\n\\nSIM code prompts utilize several functions. We do not provide the API definitions, instead, we expect the model to learn to use them from the in-context examples. The functions are:\\n\\n- `select_facts(facts)`\\n- `infer_new_fact(selected)`\\n- `is_solved(fact, question)`\\n- `make_choice(fact, choices)`\\n- `facts.append(fact)`\\n\\n`select_facts` and `infer_new_fact` are loosely inspired by Selection-Inference (Creswell et al., 2023). A key difference, though, is that we use a single prompt, instead of iterative prompts. We therefore include `is_solved(fact, question)` as a signal for the LLM to stop generation.\\n\\nEach function is annotated with its return value in an inline code comment. This is inspired by prior work (Zhang et al., 2023).`infer_new_fact` has a string return value, i.e., the text of the new fact. We experiment with two versions of the return value of `select_facts`. The first, termed SIM-indexed, uses variables `facts[i]` to reference the facts array (similar to the indices used in linearized format). The second, termed SIM-text, directly uses each fact's text, dereferenced from `facts[i]`. We find that SIM-text works best for smaller models, while SIM-indexed does for larger ones, and hence apply this going forward.\\n\\nWe write a rule-based Python script that converts existing structured graph annotations to SIM code prompts. SIM prompts express the exact same information as the linearized format. This property is unlike code prompts for prior work, wherein the conversion is done through in-context learning with an LLM, which can introduce errors as discussed in \u00a72.2. The different prompting formats for LLMs are shown in Figure 3.\\n\\nMultilingual code prompts\\n\\nWe use multilingual input in SIM code prompts as follows. First, facts given in the question are listed in the language of the task in a list of strings. Second, new facts and selected facts are given as comment lines adjacent to the function calls. See Figure 3 for an example.\\n\\n6 Experimental Setup\\n\\nModels Used\\n\\nWe primarily study open-source models, which allows for application of both train and inference-time techniques. We use BLOOMZ (Muennighoff et al., 2022) as our base LLM. This model is instruction-finetuned on prompts in 46 natural languages and 10 programming languages. For our experiments, we consider the 7.1B-parameter BLOOMZ, as well as BLOOMZ-TCC which is further finetuned on TCC.\\n\\nFor inference-time only, we consider two larger LLMs. We use the instruction-finetuned version of Falcon (Almazrouei et al., 2023) (40B), another open-source LLM trained on text+code. Compared to BLOOMZ, Falcon is more performant on English tasks; however, it has limited multilingual abilities.\\n\\nWe also use GPT-3 (175B), a closed-source model that is popularly-used and powerful.\\n\\nPrompting setup\\n\\nWe use few-shot prompting, and random sample up to 5 exemplars from the train split (up to a model's context length). For each inference example, the same exemplars are used for all models and prompt types. We use greedy decoding, and task the model with generating up to 682 tokens. (max context length of BLOOMZ 2048 // 3).\"}"}
{"id": "acl-2024-long-281", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\\n\\nNatalia sold 48/2 = 24 clips in May. Natalia sold 48 + 24 = 72 clips altogether in April and May. The answer is 72.\"}"}
{"id": "acl-2024-long-281", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Results on ARC task of xSTREET, with BLOOMZ-based models. The random baseline is 25%. \u2018Avg\u2019 bars are across the 5 non-English languages. Linearized prompts use lines, while code prompts use dots.\\n\\n| Model      | XNLI  | XStory-Cloze | XQUAD |\\n|------------|-------|--------------|-------|\\n| BLOOMZ     | 45.5  | 72.4         | 80.5  |\\n| BLOOMZ-TCC | 45.6  | 71.8         | 80.4  |\\n\\nTable 2: Results for 3 non-complex multilingual reasoning tasks, averaged over all languages.\\n\\nAppendix \u00a7A.1.\\n\\n7.2 Results for Larger LLMs\\nAs code prompts are at inference time, they can be used to interface with any LLM. We report results for GPT-3 in Figure 5. We see as before that the multilingual setting poses additional challenges for reasoning, as English results are always higher than corresponding non-English tasks.\\n\\nFirst considering ARC, GPT-3 performs strongly in English for both formats, nearly solving the task. Comparing English to multilingual ARC, linearized suffers a sharp drop (93.2 \u2192 73.2), while code prompts remain robust (99.1 \u2192 94.2). This underscores the effectiveness of S\\\\textsuperscript{IM} prompts in disentangling the reasoning and multilingual components of the task.\\n\\nFor the other tasks, S\\\\textsuperscript{IM} always outperforms linearized format. Comparing relative gains, code prompts boost performance more in English than on multilingual settings. While still a very positive result, this differs from ARC as discussed above. To discuss why this is the case, we consider the dual effects of S\\\\textsuperscript{IM} code prompts, vs. linearized: the function calls capture the reasoning structure, while the multilingual comments capture the language understanding. Because the arithmetic and logical reasoning tasks are far more symbolic than the ARC commonsense reasoning task, multilingual language understanding is less effective.\\n\\n7.3 Non-Complex Reasoning Task Results\\nRecall that our fine-tuning recipe aims to improve reasoning of an LLM, while maintaining its natural language understanding (NLU) abilities. We show this is the case by reporting results on 3 multilingual tasks:\\n- XNLI: natural language inference\\n- XStoryCloze: given 4 sentences from a short story, choose between 2 possible completions\\n- XQUAD: extractive question answering\\n\\nTo query LLMs, we follow the specific prompting guidelines for each task from Ahuja et al. (2023). Table 2 shows that for all 3 tasks, the differences between BLOOMZ and BLOOMZ-TCC are statistically insignificant. Therefore, the mitigation strategies we used, LoRA and training data replay, have proved effective.\\n\\n7.4 Effect of Code Comments on Downstream Reasoning\\nThe code & reasoning hypothesis speaks to training on code improving LLM reasoning. However, an integral part of source code is comments, which have been underexplored by prior work. We study 2 ablation settings, with the same finetuning setup:\\n- T\\\\textsubscript{CC}-en: original source code files (i.e. English-only comments).\\n- T\\\\textsubscript{CC}-del: source code files without any comments (comments are deleted).\\n\\nWe evaluate the best prompt format (S\\\\textsuperscript{IM}) on the ARC subtask. Results are shown in Figure 6. We see that overall, finetuning on T\\\\textsubscript{CC} is the best configuration, then T\\\\textsubscript{CC}-en, and finally T\\\\textsubscript{CC}-del. These trends generally hold over the 6 languages.\\n\\nThis ablation study adds a new consideration to the code & reasoning hypothesis: that within code, Russian (ru) is an exception, where BLOOMZ-TCC\\\\textsubscript{-en} outperforms BLOOMZ-TCC\\\\textsubscript{-del}. We will investigate this further, but note this may be an artifact of the base LLM, BLOOM, not having tokenization for Cyrillic script.\"}"}
{"id": "acl-2024-long-281", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Results on GSM8k, AQUA_RAT, AR_LSAT tasks of STREET (left) and xSTREET (right), with GPT-3. For each task, the random baseline is shown with a dotted line. xSTREET results are averaged over 5 languages.\\n\\nFigure 6: Results on ARC subtask of xSTREET for the code ablation experiments, where BLOOMZ is finetuned on different datasets. BLOOMZ-T\\\\textsubscript{CC} uses our proposed multilingual code comment augmentation process, BLOOMZ-T\\\\textsubscript{CC}-en uses the source code files with English comments, and BLOOMZ-T\\\\textsubscript{CC}-del uses source code files with all comments deleted. As in Figure 4, we use SIM prompts with up to 5-shot examples, and show 'Avg' bars across the 5 non-English languages.\\n\\nEven the comments are influential in downstream reasoning performance. Furthermore, we see that the diversity of code comments introduced by our proposed data augmentation of T\\\\textsubscript{CC} further boosts performance in all languages, including English.\\n\\n8 Analysis\\nTo further understand wherein our techniques help, or fail to help, model reasoning, we perform some manual analysis. For brevity, we focus on 2 languages, en and ar, and 2 tasks, ARC and GSM8K. We first perform error analysis on BLOOMZ, then perform a case study for each task.\\n\\nWe further perform 3 additional experiments, which are detailed in Appendix \u00a7E. To highlight one interesting finding, we show that training on diverse code comments, such as from the multilingual T\\\\textsubscript{CC}, boosts xSTREET performance in all languages including English.\\n\\nError Analysis for BLOOMZ English\\nFor this task, and with base BLOOMZ, SIM achieves 61.5 ARC accuracy, while linearized achieves 35.3. Our manual analysis of outputs reveals that the performance discrepancy is largely due to poor instruction-following when using linearized vs. using SIM. Ribeiro et al. (2022) find that for linearized (and their model), 62% of generations fail to generate a parsable answer (i.e., reasoning graph is incomplete). Our findings concur, in that linearized has 66% (223/340) invalid generations. In contrast, SIM has only 19% invalid. BLOOMZ-T\\\\textsubscript{CC} with SIM further reduces invalid rate to 9%, and increases accuracy to 76.2. We observe that in cases where all formats output successfully, the reasoning graph and answers are nearly identical. The difference is that SIM prompts allows the model to generate a complete reasoning graph far more often. We reiterate that this behavior is a novel finding given BLOOMZ-T\\\\textsubscript{CC} was indirectly supervised on code, rather than directly on reasoning tasks. Further discussion is found in Appendix A.\\n\\nWe summarize this section with the following view: our techniques elicit better instruction-following.\"}"}
{"id": "acl-2024-long-281", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"following of the proscribed reasoning format from a base LLM, leading to improved benchmark performance. Within a reasoning step, the models are making similar decisions, but at the reasoning-graph level, our methods assist in harder cases.\\n\\n8.1 Case Study on GSM8k English\\n\\nWe perform a case study of one GSM8k problem, comparing 3 models (BLOOMZ, BLOOMZ-T CC, GPT-3) and 2 formats (linearized, SIM) in Appendix Table 3. We observe that only GPT-3 with SIM achieves the correct answer, and reasoning steps also concur with the gold completion. GPT-3 with linearized representation makes an erroneous first step, which propagates the error downwards. Both BLOOMZ models with linearized formatting only follow the output format, and the text statements are copied from the input instead of being new statements. BLOOMZ with SIM has repetitive output and does not output an answer. While BLOOMZ-T CC still outputs a wrong answer, it does perform 2 rounds of reasoning through selecting and inferring facts. So, we see that both interventions elicit better underlying reasoning abilities of LLMs.\\n\\n8.2 Case Study on ARC Arabic\\n\\nWe look at an Arabic example from ARC in Appendix Table 4. We observe that for the linearized format, the final answer is incorrect (A), given the model makes a wrong penultimate inference. The SIM format, meanwhile, allows GPT-3 to output the correct answer (D), given it makes a correct inference step (albeit 1 step less than the gold). In fact, directly prompting GPT-3 leads to a correct answer. This again highlights the importance of aligning the prompt format, which is code here, to the training format.\\n\\n9 Conclusion\\n\\nWe introduced xSTREET, a multilingual structured reasoning benchmark which covers 5 diverse languages, spans science commonsense, arithmetic and logical reasoning tasks, and includes high-quality intermediate reasoning steps. We found that current multilingual LLMs underperform in the non-English setting, then proposed two methods to remedy this, based on the popular hypothesis that LLMs trained on code are better reasoners. At training, we propose translating the comments of a source code dataset, to use as indirect supervision data for parameter-efficient fine-tuning. During inference, we leverage code structure to represent reasoning graphs. We perform extensive experimentation, and both of our methods better elicit underlying reasoning abilities of LLMs.\\n\\nOur work brings together two areas of challenge for LLMs \u2014 multilinguality, and complex reasoning. In particular, our fine-tuning recipe shows that the code & reasoning hypothesis can apply multilingually. We suspect that improvements can be amplified if multilingual comments are included at the pre-training, instead of the fine-tuning stage. We hope our findings underscore the key role that code should play in the development of LLMs with better reasoning capabilities across languages.\\n\\nLimitations\\n\\nOne limitation is that we were unable to apply our fine-tuning recipe to the stronger LLMs. \u201cStronger\u201d refers to two characteristics. First and unavoidably, we can only apply the method to weaker open-source models, as closed-source models are proprietary; nevertheless, we explored them with our inference-time SIM prompts approach, and this worked well. Second, we only were able to fine-tune a 7B parameter model due to our resource constraints, so it is to-be-determined the effectiveness of the recipe on 70B+ models.\\n\\nBetween the submission and publication of this work (February to August 2024), LLM development has been brisk, and several recently released \u223c7B LLMs have shown decent performance on arithmetic reasoning. In our work, we were limited to BLOOMZ-7B, which we saw was poor at math. For followup work, therefore, we are excited to try our finetuning approach on TCC while using these newer LLMs as base models.\\n\\nAnother limitation is for the xSTREET benchmark, we performed human translation on only the test set of the source STREET dataset. As we used machine translation for the train set, but also drew few-shot exemplars from these, the lower exemplar quality worsens performance compared to a gold standard exemplars. We also fine-tuned on machine-translated TCC.\\n\\nWhile we tried to be inclusive with the languages chosen, studying 6 languages from different families and using different scripts, we acknowledge that more community effort will need to go into expanding the study of multilingual complex reasoning to lower-resource languages. We further...\"}"}
{"id": "acl-2024-long-281", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"acknowledge the limits of the translation of English reasoning tasks and intermediate steps alone, in that reasoning processes may differ for speakers of different languages. So too may a multilingual LLM respond inconsistently to queries posted in different languages (Li et al., 2024), which warrants future studies into how this holds for the reasoning tasks studied in this work.\\n\\nFinally, in this work, we considered only the final answer accuracy for the tasks. The original STREET tasks from Ribeiro et al. (2022) included various graph similarity metrics used to consider the intermediate reasoning steps as well \u2013 a definite strength of their structured reasoning approach vs. unstructured approaches such as CoT. We did not do this consideration due to the difficulty of reimplementing the graph similarity metric calculation for the different languages, and leave this to follow up work. Furthermore, we note that the 7B LLM we used had overall poor graph similarity (near 0 for all metrics) using the original STREET evaluation scripts and dataset.\\n\\nData Statement\\nWe provide a data statement in adherence with the ACL code of conduct and recommendations laid out in Bender and Friedman (2018). Linguists working on the Machine Translation Post Editing project for the multilingual dataset into Arabic, Chinese, Japanese, Russian, and Spanish are in-country, native speakers. They all are certified translators with more than 5 years of full-time translation experience, according to the 17100 Translation ISO Standard. These linguists were hired through vendors and were remunerated above industry standard rates. Instructions were to post-edit machine translated output and included guidelines on what to localize (artist names, city names, metric conversions), format (capitalization, punctuation) and structure (sentence level breaks). The vendor project managers made sure the instructions were adhered to. The QA process consisted of content review based on the Multidimensional Quality Metric (MQM) model that allocates different weights to 5 error severities (0-none to 5-critical) in several error topics. Total sample reviewed was 5 (5k words) of the total (100k words) source word count.\\n\\nAcknowledgements\\nWe would like to thank Danilo Neves Ribeiro for his guidance on working with the STREET benchmark, and insightful conversations on how tackle our multilingual extension of complex reasoning. We thank several colleagues for providing annotations: Etsuko Ishii, Igor Shalyminov, Yuwei Zhang. We thank these people for discussion and feedback: Salvatore Romeo, Yi Zhang, Sam Davidson, and Sailik Sengupta.\\n\\nReferences\\nKabir Ahuja, Rishav Hada, Millicent Ochieng, Prachi Jain, Harshita Diddee, Samuel Maina, Tanuja Ganu, Sameer Segal, Maxamed Axmed, Kalika Bali, et al. 2023. Mega: Multilingual evaluation of generative ai. arXiv preprint arXiv:2303.12528.\\n\\nEbtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru, M\u00e9rouane Debbah, \u00c9tienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, Daniele Mazzotta, Badreddine Noune, Baptiste Pannier, and Guilherme Penedo. 2023. The falcon series of open language models.\\n\\nZhen Bi, Jing Chen, Yinuo Jiang, Feiyu Xiong, Wei Guo, Huajun Chen, and Ningyu Zhang. 2023. Codekgc: Code language model for generative knowledge graph construction. arXiv preprint arXiv:2304.09048.\\n\\nWenhu Chen, Xueguang Ma, Xinyi Wang, and William W Cohen. 2023. Program of thoughts prompting: Disentangling computation from reasoning for numerical reasoning tasks. Transactions on Machine Learning Research.\\n\\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168.\\n\\nAntonia Creswell, Murray Shanahan, and Irina Higgins. 2023. Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning. In The Eleventh International Conference on Learning Representations.\\n\\nYijiang Dong, Lara Martin, and Chris Callison-Burch. 2023. Corrpus: Code-based structured prompting for neurosymbolic story understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 13152\u201313168.\\n\\nAmr Hendy, Mohamed Gomaa Abdelrehim, Amr Sharaf, Vikas Raunak, Mohamed Gabr, Hitokazu Matsushita, Young Jin Kim, Mohamed Afify, and Hany Hassan Awadalla. 2023. How good are gpt models at machine translation? a comprehensive evaluation. ArXiv, abs/2302.09210.\"}"}
{"id": "acl-2024-long-281", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al. 2021. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations.\\n\\nDenis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Mu\u00f1oz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. 2022. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533.\\n\\nBryan Li, Samar Haider, and Chris Callison-Burch. 2024. This land is Your, My land: Evaluating geopolitical biases in language models.\\n\\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher D. Manning, Christopher R'e, Diana Acosta-Navas, Drew A. Hudson, E. Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang, Keshav Santhanam, Laurel J. Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan S. Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023. Holistic evaluation of language models. Annals of the New York Academy of Sciences, 1525:140 \u2013 146.\\n\\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang, and Graham Neubig. 2022. Language models of code are few-shot commonsense learners. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1384\u20131403, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika, Adam Roberts, Stella Biderman, Teven Le Scao, M Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey Schoelkopf, et al. 2022. Crosslingual generalization through multitask finetuning. arXiv preprint arXiv:2211.01786.\\n\\nAnsong Ni, Jeevana Priya Inala, Chenglong Wang, Oleksandr Polozov, Christopher Meek, Dragomir R. Radev, and Jianfeng Gao. 2022. Learning math reasoning from self-sampled correct and partially-correct solutions. In International Conference on Learning Representations.\\n\\nAjay Patel, Bryan Li, Mohammad Sadegh Rasooli, Noah Constant, Colin Raffel, and Chris Callison-Burch. 2022. Bidirectional language models are also few-shot learners. In The Eleventh International Conference on Learning Representations.\\n\\nDanilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Henghui Zhu, Rui Dong, Deguang Kong, Juliette Burger, Anjelica Ramos, William Yang Wang, George Karypis, et al. 2022. Street: A multi-task structured reasoning and explanation benchmark. In The Eleventh International Conference on Learning Representations.\\n\\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. 2022. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations.\\n\\nMirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed Huaihsin Chi, Denny Zhou, and Jason Wei. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics.\\n\\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, et al. 2022a. Emergent abilities of large language models. Transactions on Machine Learning Research.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022b. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems, 35:24824\u201324837.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pieroric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38\u201345, Online. Association for Computational Linguistics.\\n\\nZhewei Yao, Reza Yazdani Aminabadi, Olatunji Ruwase, Samyam Rajbhandari, Xiaoxia Wu, Amar Ahmad Awan, Jeff Rasley, Minjia Zhang, Conglong Li, Connor Holmes, Zhongzhu Zhou, Michael Wyatt, Molly Smith, Lev Kurilenko, Heyang Qin, Masahiro Tanaka, Shuai Che, Shuaiwen Leon Song, and Yuxiong He. 2023. DeepSpeed-Chat: Easy, Fast and Affordable RLHF Training of ChatGPT-like Models at All Scales. arXiv preprint arXiv:2308.01320.\\n\\nLi Zhang, Hainiu Xu, Yue Yang, Shuyan Zhou, Weiqiu You, Manni Arora, and Chris Callison-Burch. 2023. Causal reasoning of entities and events in procedural texts. In Findings of the Association for Computational Linguistics: EACL 2023, pages 415\u2013431, Dubrovnik, Croatia. Association for Computational Linguistics.\"}"}
{"id": "acl-2024-long-281", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Raymond and Samantha are cousins. Raymond was born 6 years before Samantha. Raymond had a son at the age of 23. If Samantha is now 31,\\n\\nhow many years ago was Raymond's son born?\\n\\nWhen Raymond's son was born Samantha was 23 - 6 = 17 years old.\\n\\nThus it has been 31 - 17 = 14 years since Raymond's son was born.\\n\\nThe answer is 14.\\n\\nRaymond's son was born 23 - 6 = 17 years ago.\\n\\nSamantha is 31, so Raymond's son was born 31 - 17 = 14 years ago.\\n\\nThe answer is 14.\\n\\nA Further Discussion\\n\\nWe find that regardless of answer correctness, BLOOMZ-based models often fail to generate new text, instead of copying text from the input. This again is likely due to the weaknesses of BLOOMZ, as this is not observed for GPT-3 with any format. Our use of both interventions, greatly reduces the incidence of this problem, which as we have discussed leads to BLOOMZ-TCC better eliciting the model's underlying reasoning abilities.\\n\\nA.1 BLOOM Results for GSM8K, AQUA_RAT, AR_LSAT\\n\\nThese results are shown in Appendix Figure 8. For all tasks, performance is around random chance. For GSM8K, random chance is 0, and the models fails to solve nearly any math problem. While all numbers are close and likely statistically insignificant, we see that BLOOMZ-TCC slightly underperforms base BLOOMZ, and linearized and code prompts perform similarly.\\n\\nOur hypothesis on why this happens, as discussed before, builds on the view that truly complex reasoning capabilities are emergent with LLM's model scale. The 7B BLOOMZ model used has no baseline ability for these 3 tasks (while it did for ARC), and therefore our interventions, which are indirect supervision on code, cannot help elicit better reasoning.\\n\\nWe discuss the two interventions separately. First, we study the effectiveness of code prompts on larger LLMs in \u00a77.2. Second, for the finetuning recipe, we draw some initial points in Appendix A, given our resource constraints on small LLMs. This suggests limitations to the code+reasoning hypothesis, which have not been adequately discussed in prior work. Indirectly supervising LLMs for reasoning by training on code is effective for specific types of reasoning, such as ARC's commonsense reasoning, and less so for math problems like GSM8K, though, intuitively, code probably does not help, given code rarely includes arithmetic.\\n\\nWith 192 GB vRAM, we could finetune at most 7B multilingual models (which have much larger vocabulary sizes and thus larger embeddings). We leave future work to use our recipe with larger models, such as by using 4-bit quantization.\"}"}
{"id": "acl-2024-long-281", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Code input:\\n\\n```python\\nfacts = ['\u0081\u00d2 '\u00cb@ \\t\u00e1\u00d3 \\t\u0090P\\nB@ \u00fa\u00cd@ \\n H. Q\u00af \\nB@ \u00f8 \\n \u00f0A\u0482\u00cb@ \u00d5\u00e6\\\" m.\u00cc'@ \u00f1 \u00eb Q \u00d2\u00ae\u00cb@',\\n'P \\tQm.\u00cc'@\u00f0 Y \u00d6\u00cf@ ... Case study for a ARC entry, in Arabic. The correct answer is 'D' (in green). Only GPT-3 withSIM prompts\\ngets it correct.\\n```\\n\\nCode output:\\n\\n```python\\nnew_fact = infer_new_fact(selected) # 'The answer is D'\\n```\\n\\nTable 1: Case study for a ARC entry, in Arabic. The correct answer is 'D' (in green). Only GPT-3 withSIM prompts\\ngets it correct.\\n\\n---\\n\\n17 As for the logical reasoning problems of AR_LSAT, we defer study to future work applying our finetuning recipe to larger LLMs.\\n\\nB Hyperparameters\\nFor the TCC finetuning recipe, we set maximum sequence length to 1024, set learning rate to 1e-5 (with 0.1 weight decay), do not use warm-up, and use cosine learning rate schedule. We trained for 2 epochs using a batch size of 3 and gradient accumulation over 20 steps. We set the LoRA layers dimension to 128. The implementation is done with the DeepSpeed-Chat framework (Yao et al., 2023) and the transformers library (Wolf et al., 2020).\\n\\nC Dataset Statistics\\nStatistics for TCC are shown in Appendix Table 5.\\n\\n17 Arithmetic reasoning can be improved by directly finetuning on math \u2013 Ni et al. (2022) achieve 19.5% on GSM8k on a 2.7B LLM with this approach.\\n\\nD Full Results\\nWe now report results of all experiments and settings studied in this work. Table 6 gives results for all models and prompt formats, on STREET and xSTREET (averaged across 5 languages). Table 7 gives per-language xSTREET results. We first discuss BLOOMZ and GPT-3, which were analyzed in the paper, then separately discuss Falcon.\"}"}
{"id": "acl-2024-long-281", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Machine translate each comment\\n1. Extract comments\\n3. Replace comments with translation, keep code as-is\\n\\nD.1 Direct Prompting\\nThese tables include the direct prompting strategy, in which the model is given the same input as linearized, but needs to generate only the answer without intermediate reasoning.\\n\\nFor ARC, we find that, surprisingly, direct outperforms linearized (all LLMs). This is likely because the ARC questions are relatively easy already, and directly solving them given the context as well is possible. As discussed earlier, linearized format is artificial and hard to follow, which causes many reasoning graphs without valid answers. In contrast, SIM prompts are captured in code, which the models have seen, and therefore SIM results outperform direct and linearized.\\n\\nFor GSM8k and AQUA_RAT (using GPT-3), direct prompting fails. Using intermediate reasoning, as found by many prior works, is essential for arithmetic problem-solving. Linearized boosts performance significantly, and SIM code even further.\\n\\nRegarding xSTREET, overall trends are relatively consistent with those discussed above.\\n\\nD.2 Results for Falcon\\nFalcon is an open-source LLM that is more performant than BLOOMZ, albeit English-centric. We chose the 40B instruction-finetuned variant, intermediate between BLOOMZ 7B and GPT-3 175B.\\n\\nFirst, we consider STREET results. For ARC, Falcon fails with direct prompts (34.7), but does much better with linearized (76.5) and SIM (81.5).\\n\\nFor GSM8k, now that the base model has some math ability with direct prompts (4.4), it can improve with linearized (28.9) and SIM (19.6). SIM underperforming linearized here is because of a context size issue.\\n\\nFalcon has a max context length of 2048 tokens, while GPT-3 and BLOOMZ can accept up to 4096. SIM code prompts use a lot of tokens for the code structure, and therefore, Falcon will run out of tokens quickly and therefore fail to generate a full reasoning graph in more cases than when using linearized. This is more so a limitation of Falcon than our work (recall that most prior work considers complex reasoning tasks with huge closed-source models).\\n\\nFor AQUA_RAT, Falcon performance is near random (20) for all 3 prompt formats; i.e., it is \u201ctoo hard\u201d. For AR_LSAT, Falcon is near random (20) for direct and linearized, but achieves 34.0 with SIM prompts.\\n\\nMultilingual performance\\nEven though Falcon is an English-centric LLM, we evaluate its performance on xSTREET. We see that linearized performs the best across all tasks, with code prompts behind, and direct even further behind. Again, we attribute this to Falcon\u2019s shorter context length of 2048 \u2013 which is especially non-optimal for 4 of 5 languages studied which do not use the Latin script. The Falcon tokenizer did not see these scripts, resulting in byte-level tokenization, which further uses up the budget.\\n\\nE Additional Experiments\\nWe perform 2 additional ablation experiments below.\\n\\nE.1 Finetuning on SIM code prompts\\nWe experiment with directly finetuning on SIM code prompts (all 6 languages), so as to have a\\n\\n18 BLOOMZ has 7B and 176B variants, but nothing in between.\"}"}
{"id": "acl-2024-long-281", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 8: Results on GSM8K, AQUA_RAT, AR_LSAT subtasks for BLOOMZ-based models, with (up-to) 5-shot prompts. Random baselines are indicated with dashed grey lines. For each task, we report results 1) for, and 2) averaged over the 5 languages.\\n\\n| Model                      | GSM8k | AQUA_RAT | AR_LSAT |\\n|----------------------------|-------|----------|---------|\\n| 54.7 2.6 24.8 20.0        |       |          |         |\\n| 41.7 1.7 21.1 20.8        |       |          |         |\\n| 70.3 3.3 20.9 24.0        |       |          |         |\\n| 49.0 1.8 21.9 23.2        |       |          |         |\\n| 34.7 8.5 24.8 26.0        |       |          |         |\\n| 32.0 6.1 24.6 24.0        |       |          |         |\\n| 97.6 4.4 21.7 22.0        |       |          |         |\\n| 90.7 1.6 25.6 24.4        |       |          |         |\\n| 61.5 0.7 23.6 26.0        |       |          |         |\\n| 52.6 1.5 23.3 24.0        |       |          |         |\\n| 76.2 1.9 21.7 28.0        |       |          |         |\\n| 61.1 1.3 25.4 24.8        |       |          |         |\\n| 81.5 19.6 24.4 34.0       |       |          |         |\\n| 43.9 4.6 23.5 26.8        |       |          |         |\\n| 99.1 45.2 37.8 32.0       |       |          |         |\\n| 94.2 26.9 32.8 24.8       |       |          |         |\\n| 61.9 34.8 40.2 19.0       |       |          |         |\\n| - - - -                    |       |          |         |\\n\\nTable 6: Full results (STREET and xSTREET average) for all models, tasks, prompt formats, and languages. Rows are grouped by prompt type and model, while columns are grouped by the subtask and the language.\\n\\n* We include the reported results of Ribeiro et al. (2022) for STREET. This is for reference, as we cannot reproduce their exact prompts and examples chosen (and so cannot run on xSTREET).\\n\\n| Model                      | zh   | ar   | ja   | ru   |\\n|----------------------------|------|------|------|------|\\n| 46.5 41.2 57.4 33.5 30.0   |      |      |      |      |\\n| 2.2 1.9 1.1 1.1 2.2        |      |      |      |      |\\n| 19.7 24.8 21.3 24.0 15.7   |      |      |      |      |\\n| 22.0 24.0 16.0 24.0 18.0   |      |      |      |      |\\n| 63.2 44.7 68.5 31.2 37.6   |      |      |      |      |\\n| 1.9 1.5 2.2 1.5 1.9        |      |      |      |      |\\n| 22.0 25.6 19.7 22.0 20.1   |      |      |      |      |\\n| 18.0 28.0 26.0 24.0 20.0   |      |      |      |      |\\n| 34.7 32.4 30.3 31.8 30.9   |      |      |      |      |\\n| 10.0 6.7 3.3 4.8 5.6       |      |      |      |      |\\n| 24.8 24.0 25.2 24.8 24.0   |      |      |      |      |\\n| 26.0 22.0 24.0 26.0 22.0   |      |      |      |      |\\n| 97.4 95.0 81.8 90.6 88.5   |      |      |      |      |\\n| 3.0 1.5 0.0 1.5 1.9        |      |      |      |      |\\n| 25.2 24.8 25.6 26.0 26.4   |      |      |      |      |\\n| 35.0 37.1 35.3 29.1 30.9   |      |      |      |      |\\n| 2.2 1.1 0.4 0.4 0.4        |      |      |      |      |\\n| 22.0 26.8 23.6 24.0 25.6   |      |      |      |      |\\n| 24.0 26.0 24.0 20.0 26.0   |      |      |      |      |\\n| 57.9 67.6 58.8 45.6 32.9   |      |      |      |      |\\n| 0.7 1.5 2.2 2.2 0.7        |      |      |      |      |\\n| 21.3 25.6 23.6 22.4 23.6   |      |      |      |      |\\n| 26.0 30.0 30.0 12.0 22.0   |      |      |      |      |\\n| 70.3 74.4 65.0 54.7 40.9   |      |      |      |      |\\n| 0.4 1.5 1.5 2.2 0.7        |      |      |      |      |\\n| 27.2 24.0 28.3 22.8 24.8   |      |      |      |      |\\n| 26.0 34.0 16.0 30.0 18.0   |      |      |      |      |\\n| 57.1 55.3 30.3 44.1 32.6   |      |      |      |      |\\n| 8.1 8.1 1.9 2.6 2.2        |      |      |      |      |\\n| 26.0 24.4 23.2 20.5 23.2   |      |      |      |      |\\n| 96.5 96.5 90.3 94.1 93.5   |      |      |      |      |\\n| 37.4 28.1 18.5 21.9 28.5   |      |      |      |      |\\n| 36.6 32.7 31.1 32.7 31.1   |      |      |      |      |\"}"}
{"id": "acl-2024-long-281", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model that can perform the SIM-formatted reasoning without in-context examples. We use the same hyperparameter and configuration from \u00a74.2, again using LoRA to fine-tune a subset of the 7B model\u2019s parameters, but omitting the data replay to maximize performance. We train one model for all tasks, and all 6 languages. This differentiates our SIM finetuned model from the linearized finetuned model of Ribeiro et al. (2022), which finetunes a separate T5 (0.8B) model for each task with full finetuning.\\n\\nAs multilingual trends remain similar, we will just discuss English results (STREET). Using BLOOMZ-TCC or BLOOMZ as the base model does not make a difference. The SIM finetuned models achieves 85.9 (vs. 76.2) on ARC; the other 3 tasks are still near random chance. We suspect that performing full finetuning instead of LoRA should overcome this, and omit this experiment due to resource constraints.\\n\\nE.2 Improving the Code Comment Quality of TCC\\n\\nOur initial and used version of TCC, as described in the main text, simply took 30k source code files from The Stack, then filtered down to 20k files, using criteria of >5 comments, of which >50% of comments in English.\\n\\nTo validate the quality of the translated code files, we recruited human annotators who were proficient programmers, and native speakers of each language. While overall, translations were judged to be reasonable, the main feedback points were:\\n\\n\u2022 Some files had non-useful comments, such as long copyright statements in the header, or linting messages.\\n\\n\u2022 Some files had comments which were actually commented-out code (i.e. unused functions).\\n\\n\u2022 Terms related to programming, or referencing function or variable names in the code, were often mistranslated, if they should have been translated at all.\\n\\nWe leave the last point to future work, as we used an online MT API, and programming-specific MT is out of scope. For the other two, we tried to develop a version of TCC to specifically select files which have plenty of meaningful comments. We describe this filtering experiment below.\\n\\nWe now consider the entire Stack dataset, instead of just 30k from the official small subset. As the Stack totals 6 TB, we considered only the first 3 million examples (1m each for Java, Python, JavaScript). Our scripts performed the following steps in order:\\n\\n1. Delete copyrights, headers, linting comments from files.\\n2. Keep only those files with >1 standard deviation of number of comments:number of lines ratio. Note that 1 comment can span multiple lines.\\n3. Keep only those files with >5 comments.\\n\\nThis resulted in about 250K examples. We performed the code comment extraction and translation process, and termed the resulting dataset TCC-v2. We then applied the finetuning recipe as we did with the original TCC; furthermore, we keep the data size consistent as the original, using a 67k subset of TCC-v2. The resulting BLOOMZ-TCC-v2 models had similar downstream reasoning performance as BLOOMZ-TCC, and therefore we did not use it in the main text.\\n\\nWe hypothesize this experiment did not improve performance because, the program code plays a much bigger role in LLM's reasoning abilities than the comments.\\n\\n19https://huggingface.co/datasets/bigcode/the-stack-dedup\"}"}
