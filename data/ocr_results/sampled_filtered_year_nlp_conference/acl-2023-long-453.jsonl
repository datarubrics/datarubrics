{"id": "acl-2023-long-453", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MidMed: Towards Mixed-Type Dialogues for Medical Consultation\\nXiaoming Shi1\u2217, Zeming Liu2\u2217, Chuan Wang3, Haitao Leng4, Kui Xue1, Xiaofan Zhang1, Shaoting Zhang1\u2020\\n\\n1Shanghai Artificial Intelligence Laboratory, Shanghai, China\\n2Research Center for Social Computing and Information Retrieval, HIT, Harbin, China\\n3State Key Laboratory of Information Security, IIE, CAS, Beijing, China\\n4MMU KuaiShou Inc., Hangzhou, China\\n\\n{shixiaoming, xuekui, zhangxiaofan, shaotingzhang}@pjlab.org.cn\\nzmliu@ir.hit.edu.cn; wangchuan@iie.ac.cn; lenghaitao@kuaishou.com\\n\\nAbstract\\nMost medical dialogue systems assume that patients have clear goals (medicine querying, surgical operation querying, etc.) before medical consultation. However, in many real scenarios, due to the lack of medical knowledge, it is usually difficult for patients to determine clear goals with all necessary slots. In this paper, we identify this challenge as how to construct medical consultation dialogue systems to help patients clarify their goals. To mitigate this challenge, we propose a novel task and create a human-to-human mixed-type medical consultation dialogue corpus, termed MidMed, covering five dialogue types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded dialogue, QA, and chitchat. MidMed covers four departments (otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175 dialogues. Furthermore, we build baselines on MidMed and propose an instruction-guiding medical dialogue generation framework, termed InsMed, to address this task. Experimental results show the effectiveness of InsMed.\\n\\n1 Introduction\\nCurrent medical dialogue systems (Xu et al., 2019; Liao et al., 2020; Zeng et al., 2020; Liu et al., 2022a) mainly focus on diagnosis by obtaining symptoms and then making diagnosis automatically. These dialogue systems have shown significant potential and alluring technological value to simplify diagnostic procedures (Semigran et al., 2015). Previous works assume that patients have explicit goals (medicine querying, surgical operation querying, etc.), and perform in the way of task-oriented dialogue to accomplish patients' goals. However, explicit patient goals are usually unavailable in real-world scenarios. For example, a patient wants to consult about his itchy skin but lacks medical knowledge. Thus, it is difficult for the patient to decide which slots (e.g. medicine or a surgical operation) are needed. To figure out explicit patient goals, medical consultation services are needed, which provide advice of treatment, medicine, food, etc., as shown in Figure 1. However, those medical consultation services are underexplored in previous works.\\n\\nTo facilitate the study of medical consultation, we construct a new human-to-human mixed-type dialogue dataset for medical consultation (MidMed), covering five dialogue types: task-oriented dialogue for diagnosis, knowledge-grounded dialogue, QA, recommendation, and chitchat. MidMed is constructed by revising dialogues of MedDialog (a human-to-human medical diagnosis dialogue dataset) (Zeng et al., 2020). As shown in Figure 1, a patient queries about \\\"sweaty hands\\\", and has no explicit goal for medicine or a surgical operation. In the scenario, the doctor first collects the symptoms and makes a diagnosis. To help clarify the patient's goal, the doctor further recommends medicine and food, replies for foods to avoid, and gives emotional comfort. Through the consultation, the patient determines to apply \\\"dexamethasone cream\\\" and have more \\\"tomatoes\\\". Finally, MidMed is obtained, containing 8,175 dialogues and 98,000 utterances, with at least three dialogue types in each dialogue.\\n\\nTo promote research on medical consultation dialogue systems, we conduct benchmarking experiments on MidMed for end-to-end dialogue generation. Furthermore, to generate informative and relevant responses with dialogue topic sequences, inspired by Schick and Sch\u00fctze (2021); Wei et al. (2021), we present an instruction-guiding medical dialogue generation framework (InsMed) to handle mixed-type dialogues. InsMed is composed...\"}"}
{"id": "acl-2023-long-453", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Patient: \u624b\u811a\u591a\u6c57\u548c\u5e72\u71e5\u8715\u76ae\u3002\u590f\u5929\u624b\u4e0a\u4e0d\u6212\u6c57,\u8715\u76ae\u73b0\u8c61\u4e25\u91cd\u3002\\n(I have excessive sweating and dry peeling in hands and feet. In summer, skin peeling is serious and hands are sweaty.)\\n\\nDoctor: \u6709\u6ca1\u6709\u7619\u75d2\u7684\u611f\u89c9? (Do you feel itching?)\\n\\nPatient: \u6ca1\u6709\u7619\u75d2\u7684\u611f\u89c9\u3002 (I do not feel itching.)\\n\\nDoctor: \u521d\u6b65\u5e94\u8be5\u8003\u8651\u6c57\u75b1\u75b9,\u662f\u6e7f\u75b9\u7684\u4e00\u79cd\u8868\u73b0\u3002 (It considered to be sweat, which is a manifestation of eczema.)\\n\\nPatient: \u591a\u8c22\u533b\u751f\u3002 (Thanks.)\\n\\nDoctor: \u63a8\u8350\u8fdb\u884c\u836f\u7269\u6cbb\u7597\u3002\u7528\u6e29\u6c34\u6ce1\u624b\u534a\u5c0f\u65f6,\u7136\u540e\u6d82\u5730\u585e\u7c73\u677e\u4e73\u818f\u3002 (Drug therapy is recommended. Soak your hands in warm water for half an hour, then apply dexamethasone cream.)\\n\\nPatient: \u597d\u7684\u533b\u751f,\u6211\u4f1a\u7167\u505a\u7684\u3002 (I will do as you tell me.)\\n\\nDoctor: \u9664\u4e86\u836f\u7269\u6cbb\u7597,\u996e\u98df\u65b9\u9762\u4e5f\u9700\u8981\u6ce8\u610f\u3002\u63a8\u8350\u5403\u7315\u7334\u6843,\u5bf9\u6cbb\u7597\u6e7f\u75b9\u5177\u6709\u4e00\u5b9a\u7684\u4f5c\u7528\u3002 (Besides medication, diet is also important. I recommend you to have kiwi fruits, which benefits treating eczema.)\\n\\nPatient: \u4e0d\u592a\u559c\u6b22\u5403\u7315\u7334\u6843\u3002 (I do not like kiwi fruits.)\\n\\nDoctor: \u90a3\u53ef\u4ee5\u5403\u897f\u7ea2\u67ff,\u5bcc\u542b\u6709\u5927\u91cf\u7684\u7ef4\u751f\u7d20C,\u5bf9\u6cbb\u7597\u6e7f\u75b9\u4e5f\u6709\u5e2e\u52a9\u3002 (Alternatively, you can have tomatoes, which are rich in vitamin C. It is also helpful for treating eczema.)\\n\\nPatient: \u8c22\u8c22\u533b\u751f\u3002\u90a3\u6709\u4ec0\u4e48\u4e0d\u80fd\u5403\u7684\u5417? (Thanks. Is there anything I should not eat?)\\n\\nDoctor: \u6e7f\u75b9\u4e0d\u5efa\u8bae\u5403\u8f9b\u8fa3\u523a\u6fc0\u7684\u98df\u7269\u3001\u6cb9\u817b\u7684\u98df\u7269,\u4e0d\u5efa\u8bae\u996e\u9152\u3002 (You should avoid spicy food, greasy food, and alcohol.)\\n\\nPatient: \u611f\u8c22\u533b\u751f\u3002\u60a8\u7684\u5efa\u8bae\u592a\u5b9d\u8d35\u4e86! (Thanks. Your advice is valuable.)\\n\\nDoctor: \u60a8\u5ba2\u6c14\u4e86\u3002\u60a8\u7684\u75c5\u60c5\u4e0d\u4e25\u91cd,\u5e73\u5e38\u591a\u6ce8\u610f\u5f88\u5feb\u5c31\u80fd\u597d\u4e86\u3002 (You are welcome. Your condition is not serious. Best wishes.)\"}"}
{"id": "acl-2023-long-453", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Datasets Mixed-type Medical Dialogue Types\\n\\n| Dataset       | Relevant Skills | Type of Dialogue |\\n|---------------|-----------------|------------------|\\n| MZ (Wei et al., 2018) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| DX (Xu et al., 2019) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| CMDD (Lin et al., 2019) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| MedDG (Liu et al., 2022a) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| MedDialog (Zeng et al., 2020) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| DialoAMC (Chen et al., 2022) | \u2717 \u2713 | Task-oriented dialogue for diagnosis |\\n| DuRecDial (Liu et al., 2020) | \u2713 \u2717 | Rec., chitchat, QA, task-oriented dialogue |\\n| DodecaDialogue (Shuster et al., 2020) | \u2713 \u2717 | Know., chitchat, QA, empathetic dialogue, image chat |\\n| BlendedSkillTalk (Smith et al., 2020) | \u2713 \u2717 | Know., empathetic dialogue, chitchat |\\n| ACCENTOR (Sun et al., 2021) | \u2713 \u2717 | Chitchat, task-oriented dialogue |\\n| DuRecDial 2.0 (Liu et al., 2021) | \u2713 \u2717 | Rec., chitchat, QA, task-oriented dialogue |\\n| SalesBot (Chiu et al., 2022) | \u2713 \u2717 | Chitchat, task-oriented dialogue |\\n| DuClarifyDial (Liu et al., 2022b) | \u2713 \u2717 | Rec., know., chitchat, QA, task-oriented dialogue |\\n\\nTable 1: Comparison of MidMed with other datasets. \u201cknow.\u201d and \u201crec.\u201d stand for knowledge-grounded dialogue, and conversational recommendation, respectively.\\n\\n2.2 Mixed-type Dialogue Systems\\n\\nRecently, research on the mixed-type dialogue has increased significantly. These researches fall into two categories: (1) train an all-in-one conversation model by using multiple single-skill conversation datasets, such as persona-chat, task-oriented dialogue, to bind multiple dialogue skills (Madotto et al., 2020; Roller et al., 2021; Madotto et al., 2021); (2) collect mixed-type dialog datasets (Shuster et al., 2020; Smith et al., 2020; Liu et al., 2020; Sun et al., 2021; Liu et al., 2021; Chiu et al., 2022; Liu et al., 2022b) to train mixed-type dialog models. Those datasets are intended to mix different dialogue skills to meet specific needs, such as recommending movies and songs, and are unable to solve medical consultations. Compared with them, we collect a mixed-type dialogue corpus, MidMed, to facilitate the study of medical consultations.\\n\\n3 Dataset Collection\\n\\nIn this section, we describe the three steps for MidMed construction: (1) Selecting basic diagnosis dialogue data; (2) Constructing annotation guidance; (3) Collecting mixed-type dialogue by crowdsourcing.\\n\\n3.1 Selecting Basic Diagnosis Dialogue\\n\\nTo be close to real-world scenarios, MidMed is constructed based on real diagnosis dialogue dataset MedDialog (Zeng et al., 2020), which is collected from online medical community haodf.com. MedDialog dataset contains 3.4 million Chinese dialogues (consultations) between patients and doctors, covering 29 broad categories of specialties including internal medicine, pediatrics, dentistry, etc., and 172 fine-grained specialties including cardiology, neurology, gastroenterology, urology, etc.\\n\\nBasic Dialogue Selection. For MidMed construction, we recruit twenty medical students, who are experts in four departments, otorhinolaryngology, ophthalmology, skin, and the digestive system department. To ensure better data quality and construction efficiency, the dialogues only in these four departments are reserved. Besides, we observe that dialogues with few dialogue utterances are usually of poor quality. Thus, for high data quality and efficiency of data construction, only those conversations with more than four utterances are kept. After the above data processing, there are total 9,000 dialogues obtained.\\n\\nCoarse-grained Privacy Removing. Furthermore, for ethical concerns, specific regular expressions for coarse-grained filtering are employed to remove privacy. To delete patients' privacy, regular expressions, such as \u201c\u6211\u53eb... (My name is ...)\u201d, are designed to delete sentences containing name, gender, and region. Besides, regular expressions, such as \u201c\u9648\u533b\u751f\u60a8\u597d... (Hello, doctor Chen, ...)\u201d, are utilized to delete doctors' privacy.\\n\\n3.2 Constructing Annotation Guidance\\n\\nAnnotation guidance is designed to instruct annotators for data annotation, including target dialogue topic sequences and reference knowledge. Specifically, target topic sequences assign topics for each dialogue session. To support the annotation of each topic, reference knowledge is provided.\"}"}
{"id": "acl-2023-long-453", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2.1 Target Dialogue Topic Sequence\\n\\nDue to the complexity of the data annotation, it is of great difficulty to conduct data annotation with only high-level instructions. Inspired by the work of MultiWOZ (Budzianowski et al., 2018), we provide a target dialogue topic sequence for each dialogue construction. The dialogue topic sequences are employed to instruct annotators to annotate the content of specific topics. As shown in Figure 1, the target dialogue topic sequence is composed of dialogue topics, including Patient Self Report, Doctor Inquiry Additional, Doctor Recommend Medicine, etc. The whole dialogue topic sequences are shown in Figure 2. The combination of different topics ensures the diversity of dialogue topic sequences.\\n\\n3.2.2 Reference Knowledge\\n\\nThe knowledge graph stores large-scale knowledge in the form of easy-to-use triples, and it has various applications in all modules of the human-computer dialogue system (Tuan et al., 2019, 2022; Yang et al., 2020). Therefore, we incorporate knowledge graphs into medical consultation to provide more accurate interactive questions and answers. Specifically, we crawled a large number of web pages from some high-quality medical vertical websites such as 39.net and then obtained a large amount of triplet knowledge by using information extraction techniques such as entity extraction and relation extraction. By using these triples, a large-scale medical knowledge graph is constructed, whose entities include diseases, symptoms, drugs, foods, etc., and relationships include disease-drug relation, disease-food relation, etc.\\n\\nTo provide reference knowledge for dialogue annotation, we extract a knowledge graph subset for each dialogue. Specifically, diseases in the whole knowledge graph are mapped with the dialogue with exact string matching. The disease existing in the medical dialogues are employed as the head entity for select triples from the knowledge graph. Finally, we extract a knowledge graph subset, which covers four types of entities: disease, symptom, diet, and medicine, with a total of 229,570 triples.\\n\\n3.3 Collecting Mixed-type Dialogue\\n\\nFor data annotation, the trial annotation and the formal annotation are conducted, sequentially. First, the trial annotation aims to select an annotation team and make the annotation team get familiar with the guide. Second, the formal annotation is conducted for collecting the whole dataset.\\n\\n3.3.1 Trial Annotation\\n\\nTo ensure the high quality of dialogues, trial annotation is conducted. In the trial annotation stage, three crowdsourcing teams (about 20 annotators per team) are selected for trial annotation. There are mainly two advantages. (1) Trial annotation helps select a reliable annotation team. (2) The trial annotation helps the annotation team get familiar with the annotation task. Lastly, the team achieving the best performance in the trial annotation is selected for the formal annotation.\"}"}
{"id": "acl-2023-long-453", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3.2 Formal Annotation\\n\\nAfter the trial annotation, the formal annotation is conducted. In the formal annotation, to ensure data quality, the fine-grained privacy removing, skipping option, and quality audit and re-annotating mechanisms are employed. To ensure diversity, the mechanism of annotation without target dialogue topic sequences is applied.\\n\\nOverall Annotation. In the formal data annotation process, annotators are required to act as doctors and patients in turn. Annotators construct dialogues based on a given basic diagnosis dialogue, a target dialogue topic sequence, and reference knowledge. The annotation progress is conducted as follows. First, the annotator enters the chat interface to start chatting, and the \u201cpatient\u201d initiates the conversation. Second, annotators conduct a dialogue based on the dialogue topic sequence. It is important that the information utilized in the dialogue conforms to the reference knowledge. After successfully mentioning all target topics in sequence, the \u201cdoctor\u201d ends the conversation.\\n\\nFurthermore, we introduce the fine-grained privacy removing, the skipping option, quality audit and re-annotating to improve data quality, and introduce the annotation without target dialogue topic sequence mechanism to improve data diversity.\\n\\nFine-grained Privacy Removing. In the data annotation process, for better data quality, annotators are also required to delete privacy that cannot be covered by regular expressions, including gender, age, name, institution name, etc.\\n\\nSkipping Option. We observe that there are many basic diagnosis dialogues with low quality. These bad dialogues may lead to annotated dialogues of low quality. To alleviate the issue, a skip option is provided to annotators. Specifically, annotators can choose whether to annotate the given basic diagnosis dialogue or not to the quality of the given dialogue. If annotators choose \u201cSkip\u201d, they then skip the current dialogue directly and conduct the annotation of the next dialogue.\\n\\nTo ensure the option is not being overused, we review all the skipped conversations and select high-quality dialogues from the skipped conversations. Those high-quality dialogues are returned to the annotation process, and the rest low-quality dialogues are abandoned.\\n\\nQuality Audit and Re-annotating. To deal with low-quality samples, we introduce the quality audit and re-annotation mechanism. Specifically, we review all the annotated samples and pick out low-quality dialogues. These low-quality samples are returned to the annotation team for re-annotation.\\n\\nAnnotation without Target Dialogue Topic Sequence. Though the target dialogue topic sequences lead to good annotation quality, they usually lead to monotonous dialogue structures. To address the issue, annotators are also allowed to construct the dialogues without following the target dialogue topic sequences. This option enables annotators to construct more diverse and flexible dialogues based on the basic diagnosis dialogues. Meanwhile, to prevent this option from being abused, this option is required to be used for no more than ten percent of the whole annotation data.\\n\\n3.4 Dataset Analysis\\n\\nData statistics. Table 2 provides statistics of the MidMed. There are totally 8,175 dialogues with 11.79 utterances in each dialogue on average. The longest dialogue contains 46 utterances. Besides, there are 19.26 tokens in an utterance on average, indicating rich semantic information.\\n\\nTable 1 lists medical dialogue datasets (MZ (Wei et al., 2018), DX (Xu et al., 2019), CMDD (Lin et al., 2019), MedDG (Liu et al., 2022a), MedDialog (Zeng et al., 2020), Di- algoAMC (Chen et al., 2022) ) and mixed-type dialogue dataset (DuRecDial (Liu et al., 2020), DodecaDialogue (Shuster et al., 2020), Blended-SkillTalk (Smith et al., 2020), ACCENTOR (Sun et al., 2021), DuRecDial 2.0 (Liu et al., 2021), SalesBot (Chiu et al., 2022), DuClarifyDial (Liu et al., 2017)).\"}"}
{"id": "acl-2023-long-453", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MidMed is the first dialogue dataset for consultation, covering five types of dialogues.\\n\\nData quality. Following (Liu et al., 2020), for data quality evaluation, we employ human evaluations. Specifically, we assign \u201c1\u201d for dialogues coincident with annotation guidance, and \u201c0\u201d for the others. Then, we conduct a quality evaluation on 100 randomly sampled dialogues. Finally, an average score of \u201c0.90\u201d is achieved. The result indicates that the dialogues in the dataset are with high quality.\\n\\n4 Method\\n\\nDuring training, a dialogue, with a sequence of utterances between a patient and a doctor, is given. Then, the dialogue is processed into a set of samples \\\\( \\\\{ (s_i, t_i) \\\\} \\\\in D \\\\), where \\\\( t_i \\\\) is the \\\\( i \\\\)-th target doctor response, \\\\( s_i \\\\) is the concatenation of all former utterances before \\\\( t_i \\\\), and \\\\( D \\\\) is the training dataset. Dialogue generation is formulated as a sequence-to-sequence generation problem, which aims to generate \\\\( t_i \\\\) conditioned on \\\\( s_i \\\\).\\n\\nInsMed has three modules, dialogue topic selecting, reference knowledge selecting, and the instruction-guided generation module. The dialogue topic prediction and the reference knowledge selection module aim to obtain dialogue topics and reference knowledge, respectively. Then, for better generation performance, these two types of information are transformed into instructions in natural language. Finally, instructions are concatenated with context, as the input to generation models. Next, the above modules are introduced.\\n\\n4.1 Dialogue Topic Selection\\n\\nThe dialogue topic selection module is divided into two stages, the dialogue topic prediction, and the dialogue topic converting. The dialogue topic prediction aims to predict dialogue topics for the next utterance. Formally, this task is regarded as a multi-class classification problem. Specifically, the input of the prediction module is a dialogue context \\\\( s_i \\\\), and the output is the predicted dialogue topics. The classification process is formulated,\\n\\n\\\\[\\np_i = f(s_i),\\n\\\\]\\n\\nwhere \\\\( f \\\\) is the classification function BERT (Devlin et al., 2018) and \\\\( p_i \\\\in \\\\mathbb{R}^{\\\\mid C \\\\mid} \\\\) is the predicted probability value, \\\\( C \\\\) is the predefined category set. The dialogue topic \\\\( a_i \\\\) is selected as the predicted dialogue topic if the value of the dimension is the highest probability value in \\\\( p_i \\\\).\\n\\nThen, in the dialogue topic converting stage, \\\\( a_i \\\\) is converted into natural language with predefined templates, represented as \\\\( \\\\tilde{a}_i \\\\). For example, the predicted topic is Recommend Medicine, and the converted instruction is \\\"In the next utterance, the doctor will recommend medicine\\\".\\n\\n4.2 Reference Knowledge Selection\\n\\nThe reference knowledge selection module aims to obtain the reference knowledge for model generation, thus guiding models to generate more informative responses. The module is divided into two parts, knowledge retrieval, and reference knowledge converting. The knowledge retrieval module aims to retrieve reference knowledge from the whole knowledge graph for response generation. An exact string match is utilized for retrieval. Specifically, the diseases \\\\( d_{m=1} \\\\) in the whole knowledge graph are mapped with medical dialogues with exact string matching, where \\\\( m \\\\) is the number of diseases. The disease \\\\( d_i \\\\) existing in the medical dialogues are regarded as related diseases of the dialogues. Then, the reference knowledge is obtained by inquiry the knowledge graph with \\\\( d_i, e = \\\\{ \\\\text{head, relation, tail} \\\\in \\\\mathcal{KG}, \\\\text{head} = d_i, \\\\text{relation} = r \\\\} \\\\), where \\\\( r \\\\) is the slot in the predicted dialogue topic \\\\( a_k \\\\).\\n\\nFor example, if the dialogue topic is Doctor Recommend Medicine, \\\\( r \\\\) is Medicine, and \\\\( e = \\\\{ \\\\text{bonmopirocin ointment}, \\\\text{dexamethasone cream} \\\\} \\\\).\\n\\nThen, in the reference knowledge converting, \\\\( e \\\\) is converted into natural language with predefined templates, represented as \\\\( \\\\tilde{e}_i \\\\). As the example in Figure 3, the converted knowledge instruction is \\\"the recommended medicine is bonmopirocin ointment and dexamethasone cream\\\".\\n\\n4.3 Instruction-guiding Generation\\n\\nThe Instruction-guiding generation module aims to generate accurate and informative responses with instructions. The problem of response generation is formulated as a sequence-to-sequence task (Sutskever et al., 2014). The input to the generation model is the concatenation of the dialogue context \\\\( s_i \\\\), the predicted dialogue topic instruction \\\\( \\\\tilde{a}_i \\\\), and the reference knowledge \\\\( \\\\tilde{e}_i \\\\). The output is the doctor's response \\\\( t_i \\\\).\"}"}
{"id": "acl-2023-long-453", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the next utterance, the doctor recommends medicine. The recommended medicine is dexamethasone cream.\\n\\nPatient: I have excessive sweating and dry peeling of hands and feet...\\n\\nDoctor: Do you feel itching?\\n\\nPatient: Thanks!\"}"}
{"id": "acl-2023-long-453", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"irrelevant or logical contradictory to the given current goal and global context.\\n\\n- score 1 (fair): more than one-third responses irrelevant or logical contradictory to the given current goal and global context.\\n- score 2 (good): otherwise.\\n\\nInformativeness examines how much knowledge (goal topics and topic attributes) is provided in responses:\\n\\n- score 0 (bad): no knowledge is mentioned at all.\\n- score 1 (fair): only one knowledge triple is mentioned in the response.\\n- score 2 (good): more than one knowledge triple is mentioned in the response.\\n\\nHuman-likeness examines similarity between each generated response with corresponding human response from the perspectives of appropriateness, fluency, and proactivity:\\n\\n- score 0 (bad): not like human responses.\\n- score 1 (fair): like human responses, but some parts still have deficiencies.\\n- score 2 (good): otherwise.\\n\\n5.3 Baselines\\n\\nWe carefully select a few strong baselines for comparison. Specifically, two baselines for mixed-type dialogue generation (BST (Smith et al., 2020), MGCG (Liu et al., 2020)), a baseline for medical dialogue generation (VRbot (Li et al., 2021)), two common baselines for medical dialogue (Seq2Seq (Sutskever et al., 2014), DialoGPT (Zhang et al., 2020)), and a baseline for general dialogue generation (BART (Lewis et al., 2020)) are used in this experiment. Besides, the proposed model utilizes the same data as these baselines, with domain-specific knowledge.\\n\\n- BST (Smith et al., 2020) is a mixed-type dialogue model that can display many skills, and blend them in a seamless and engaging way.\\n- MGCG (Liu et al., 2020) consists of a goal-planning module and a goal-guided responding module. The goal-planning module conducts dialog management to control the dialog flow. The responding module generates responses for completing each goal.\\n- VRbot (Li et al., 2021) introduces both patient state and physician action as latent variables with categorical priors for explicit patient state tracking and physician policy learning, respectively. A variational Bayesian generative approach is utilized to approximate posterior distributions over patient states and physician actions.\\n- Seq2Seq (Sutskever et al., 2014) Sutskever et al. (2014) uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of fixed dimensionality, and then another LSTM to decode the target sequence from the vector.\\n- DialoGPT (Zhang et al., 2020) is a large, tunable neural conversational response generation model based on GPT. DialoGPT is trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017.\\n- BART (Lewis et al., 2020) is a denoising autoencoder for pretraining sequence-to-sequence models. It is composed of a BERT encoder (a bidirectional encoder) and a GPT decoder (a left-to-right decoder).\\n\\n5.4 Automatic Evaluation\\n\\nThe results on automatic evaluation metrics are shown in Table 3. InsMed is compared with the other five generation models on various evaluation metrics. The results show the following conclusions.\\n\\nFirst, BART (large) is much better than other baseline generation models. The reason may be that BART (large) is much more powerful than other generation models, with more parameters and more training data.\\n\\nSecond, InsMed achieves state-of-the-art performance on almost all metrics. This demonstrates that instructions help BART to generate more accurate responses.\\n\\n5.5 Human Evaluation\\n\\nTable 4 shows the human evaluation results on the test set of MidMed.\\n\\nFirst, comparing BART, InsMed with other baselines, the results demonstrate that pre-training on large-scale data improves relevance, informativeness, and human-likeness. The reason may be that pre-training on large-scale data provides a large amount of common language knowledge.\"}"}
{"id": "acl-2023-long-453", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Model           | ROUGE NIST-4 | BLEU | BLEU-1 | BLEU-2 | BLEU-3 | BLEU-4 | METEOR |\\n|-----------------|--------------|------|--------|--------|--------|--------|--------|\\n| BST (Smith et al., 2020) | 13.64        | 0.81 | 2.88   | 14.01  | 4.89   | 2.15   | 1.02   |\\n| MGCG (Liu et al., 2020)   | 14.37        | 0.98 | 3.36   | 15.88  | 5.39   | 2.61   | 1.06   |\\n| VRbot (Li et al., 2021)   | 23.01        | 1.41 | 4.84   | 22.67  | 8.07   | 3.55   | 1.31   |\\n| Seq2Seq (Sutskever et al., 2014) | 12.21        | 0.77 | 2.93   | 14.25  | 4.92   | 2.08   | 1.01   |\\n| DialoGPT (Zhang et al., 2020) | 19.58        | 1.14 | 4.62   | 17.64  | 5.97   | 2.84   | 1.53   |\\n| BART (Lewis et al., 2020) | 31.63        | 3.12 | 21.95  | 41.36  | 27.26  | 22.04  | 18.87  |\\n| InsMed (Ours)       | 40.59        | 3.30 | 23.13  | 42.61  | 28.46  | 23.00  | 19.73  |\\n| w/o Topic          | 34.99        | 3.17 | 22.41  | 42.03  | 27.97  | 22.60  | 19.26  |\\n| w/o KG             | 32.22        | 3.14 | 22.19  | 41.13  | 27.21  | 22.12  | 18.91  |\\n\\nTable 3: Automatic evaluation results of five baseline models and InsMed, on eight evaluation metrics. Values of ROUGE, BLEU, and METEOR are expressed as percentages (%).\\n\\nTable 4: Human evaluation results of six baseline models, InsMed, and Groundtruth, on three aspects, including relevance, informativeness, and human-likeness. Scores of \u201c0\u201d, \u201c1\u201d, and \u201c2\u201d are assigned to each dialogue, where \u201c0\u201d represents bad samples and \u201c2\u201d represents good samples. The average scores are reported.\\n\\nSecond, comparing InsMed with BART, the results show that InsMed performs better than BART, especially in relevance and informativeness. The reason may be that instructions in InsMed provide specific targets for generation, leading to a more relevant and informative response generation.\\n\\n5.6 Ablation Study\\n\\nTable 3 shows the ablation results, where \u201cw/o Topic\u201d means removing dialogue topic instructions from the InsMed and \u201cw/o KG\u201d means removing reference knowledge instructions from the InsMed. Results show that reducing any module of MidMed leads to poor results. This illustrates the effectiveness of each module of the InsMed.\\n\\n6 Conclusion\\n\\nThis work identified the challenge of helping patients clarify their goals through medical consultations. To address this challenge, this work proposed a novel task, medical consultation over mixed-type dialogue, and collected a new Chinese human-to-human mixed-type dialogue dataset, in which each session has rich variability of dialogue types with natural topic transitions. To facilitate further research, we conducted benchmarking experiments on MidMed for end-to-end dialogue generation and proposed an instruction-guiding medical dialogue generation framework InsMed. Experimental results show the effectiveness of InsMed. In the future, we will investigate the possibility of cross-departments (e.g. dermatology and endocrinology) medical consultation at low cost.\\n\\n7 Limitation\\n\\nInsMed is built based on the large-scale pre-training model BART, which requires high computing resources. Besides, the data currently only covers four departments, limiting the usage scenarios of the data.\\n\\n8 Ethical Statement\\n\\nWe make sure that MidMed is collected in a manner that is consistent with the terms of use of any sources and the intellectual property and privacy rights of the original authors of the texts. And crowd workers were treated fairly. This includes, but is not limited to, compensating them fairly, ensuring that they were able to give informed consent, and ensuring that they were voluntary participants who were aware of any risks of harm associated with their participation.\\n\\n9 Acknowledgements\\n\\nThanks for the insightful comments from reviewers. This work is supported by the Shanghai Artificial Intelligence Laboratory.\"}"}
{"id": "acl-2023-long-453", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\nAbhaya Agarwal and Alon Lavie. 2007. Meteor: An automatic metric for mt evaluation with high levels of correlation with human judgments. Proceedings of WMT-08.\\n\\nPawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramad-an, and Milica Ga\u0161i\u00b4c. 2018. MultiWOZ - a large-scale multi-domain Wizard-of-Oz dataset for task-oriented dialogue modelling. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 5016\u20135026, Brussels, Belgium. Association for Computational Linguistics.\\n\\nWei Chen, Zhiwei Li, Hongyi Fang, Qianyuan Yao, Cheng Zhong, Jianye Hao, Qi Zhang, Xuanjing Huang, Zhongyu Wei, et al. 2022. A benchmark for automatic medical consultation system: Frameworks, tasks and datasets. arXiv preprint arXiv:2204.08997.\\n\\nSsu Chiu, Maolin Li, Yen-Ting Lin, and Yun-Nung Chen. 2022. SalesBot: Transitioning from chit-chat to task-oriented dialogues. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6143\u20136158, Dublin, Ireland. Association for Computational Linguistics.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. of NAACL-HLT, page 4171\u20134186.\\n\\nGeorge Doddington. 2002. Automatic evaluation of machine translation quality using n-gram co-occurrence statistics. In Proceedings of the second international conference on Human Language Technology Research, pages 138\u2013145.\\n\\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A method for stochastic optimization. ICLR.\\n\\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7871\u20137880, Online. Association for Computational Linguistics.\\n\\nDongdong Li, Zhaochun Ren, Pengjie Ren, Zhumin Chen, Miao Fan, Jun Ma, and Maarten de Rijke. 2021. Semi-supervised variational reasoning for medical dialogue generation. In SIGIR.\\n\\nKangenbei Liao, Qianlong Liu, Zhongyu Wei, Baolin Peng, Qin Chen, Weijian Sun, and Xuanjing Huang. 2020. Task-oriented dialogue system for automatic disease diagnosis via hierarchical reinforcement learning. arXiv preprint arXiv:2004.14254.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nXinzhu Lin, Xiahui He, Qin Chen, Huaixiao Tou, Zhongyu Wei, and Ting Chen. 2019. Enhancing dialogue symptom diagnosis with global attention and symptom graph. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5033\u20135042.\\n\\nWenge Liu, Jianheng Tang, Yi Cheng, Wenjie Li, Yefeng Zheng, and Xiaodan Liang. 2022a. Meddg: an entity-centric medical consultation dataset for entity-aware medical dialogue generation. In CCF International Conference on Natural Language Processing and Chinese Computing, pages 447\u2013459. Springer.\\n\\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, and Wanxiang Che. 2021. DuRecDial 2.0: A bilingual parallel corpus for conversational recommendation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 4335\u20134347, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nZeming Liu, Haifeng Wang, Zheng-Yu Niu, Hua Wu, Wanxiang Che, and Ting Liu. 2020. Towards conversational recommendation over multi-type dialogs. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1036\u20131049, Online. Association for Computational Linguistics.\\n\\nZeming Liu, Jun Xu, Zeyang Lei, Haifeng Wang, Zheng-Yu Niu, and Hua Wu. 2022b. Where to go for the holidays: Towards mixed-type dialogs for clarification of user goals. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1024\u20131034, Dublin, Ireland. Association for Computational Linguistics.\\n\\nAndrea Madotto, Zhaojiang Lin, Yejin Bang, and Pascale Fung. 2021. The adapter-bot: All-in-one controllable conversational model. In AAAI.\\n\\nAndrea Madotto, Zhaojiang Lin, Chien-Sheng Wu, Jamin Shin, and Pascale Fung. 2020. Attention over parameters for dialogue systems.\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju, Mary Williamson, Yinhan Liu, Jing Xu, Myle Ott, Eric Michael Smith, Y-Lan Boureau, and et al. 2021.\"}"}
{"id": "acl-2023-long-453", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Recipes for building an open-domain chatbot.\\n\\nProceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume.\\n\\nTimo Schick and Hinrich Sch\u00fctze. 2021. Exploiting cloze questions for few shot text classification and natural language inference. Conference of the European Chapter of the Association for Computational Linguistics, pages 255\u2013269.\\n\\nHannah L. Semigran, Jeffrey A. Linder, Courtney Gidengil, and Ateev Mehrotra. 2015. Evaluation of symptom checkers for self diagnosis and triage: audit study. BMJ, 351:h3480.\\n\\nKurt Shuster, Da JU, Stephen Roller, Emily Dinan, Y-Lan Boureau, and Jason Weston. 2020. The dialogue dodecathlon: Open-domain knowledge and image grounded conversational agents. Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics.\\n\\nEric Michael Smith, Mary Williamson, Kurt Shuster, Jason Weston, and Y-Lan Boureau. 2020. Can you put it all together: Evaluating conversational agents' ability to blend skills. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2021\u20132030, Online. Association for Computational Linguistics.\\n\\nKai Sun, Seungwhan Moon, Paul A. Crook, Stephen Roller, Becka Silvert, Bing Liu, Zhiguang Wang, Honglei Liu, Eunjoon Cho, and Claire Cardie. 2021. Adding chit-chat to enhance task-oriented dialogues. In NAACL.\\n\\nIlya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems, 27.\\n\\nYi-Lin Tuan, Sajjad Beygi, Maryam Fazel-Zarandi, Qiaozi Gao, Alessandra Cervone, and William Yang Wang. 2022. Towards large-scale interpretable knowledge graph reasoning for dialogue systems. In Findings of the Association for Computational Linguistics: ACL 2022, pages 383\u2013395, Dublin, Ireland. Association for Computational Linguistics.\\n\\nYi-Lin Tuan, Yun-Nung Chen, and Hung-yi Lee. 2019. DyKgChat: Benchmarking dialogue generation grounding on dynamic knowledge graphs. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 1855\u20131865, Hong Kong, China. Association for Computational Linguistics.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008.\\n\\nZifeng Wang, Rui Wen, Xi Chen, Shilei Cao, Shao-Lun Huang, Buyue Qian, and Yefeng Zheng. 2021. Online disease diagnosis with inductive heterogeneous graph convolutional networks. In Proceedings of the Web Conference 2021, pages 3349\u20133358.\\n\\nJason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652.\\n\\nZhongyu Wei, Qianlong Liu, Baolin Peng, Huaixiao Tou, Ting Chen, Xuan-Jing Huang, Kam-Fai Wong, and Xiang Dai. 2018. Task-oriented dialogue system for automatic diagnosis. In Proc. of ACL, pages 201\u2013207.\\n\\nLin Xu, Qixian Zhou, Ke Gong, Xiaodan Liang, Jian-heng Tang, and Liang Lin. 2019. End-to-end knowledge-routed relational dialogue system for automatic diagnosis. In Proc. of AAAI.\\n\\nShiquan Yang, Rui Zhang, and Sarah Erfani. 2020. GraphDialog: Integrating graph knowledge into end-to-end task-oriented dialogue systems. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1878\u20131888, Online. Association for Computational Linguistics.\\n\\nGuangtao Zeng, Wenmian Yang, Zeqian Ju, Yue Yang, Sicheng Wang, Ruisi Zhang, Meng Zhou, Jiaqi Zeng, Xiangyu Dong, Ruoyu Zhang, Hongchao Fang, Penghui Zhu, Shu Chen, and Pengtao Xie. 2020. MedDialog: Large-scale medical dialogue datasets. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9241\u20139250, Online. Association for Computational Linguistics.\\n\\nYizhe Zhang, Michel Galley, Jianfeng Gao, Zhe Gan, Xiujun Li, Chris Brockett, and Bill Dolan. 2018. Generating informative and diverse conversational responses via adversarial information maximization. Advances in Neural Information Processing Systems, 31.\\n\\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao, Jingjing Liu, and Bill Dolan. 2020. DIALOGPT: Large-scale generative pre-training for conversational response generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 270\u2013278, Online. Association for Computational Linguistics.\\n\\n8155\"}"}
{"id": "acl-2023-long-453", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A For every submission:\\n\\n\u25a1 A1. Did you describe the limitations of your work?\\n\\n\u25a1 A2. Did you discuss any potential risks of your work?\\nNot applicable. Left blank.\\n\\n\u25a1 A3. Do the abstract and introduction summarize the paper's main claims?\\n\\n\u25a1 A4. Have you used AI writing assistants when working on this paper?\\nGrammarly\\n\\nB Did you use or create scientific artifacts?\\n\\n\u25a1 B1. Did you cite the creators of artifacts you used?\\n1 and 3\\n\\n\u25a1 B2. Did you discuss the license or terms for use and/or distribution of any artifacts?\\nNot applicable. Left blank.\\n\\n\u25a1 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided that it was specified? For the artifacts you create, do you specify intended use and whether that is compatible with the original access conditions (in particular, derivatives of data accessed for research purposes should not be used outside of research contexts)?\\nNot applicable. Left blank.\\n\\n\u25a1 B4. Did you discuss the steps taken to check whether the data that was collected/used contains any information that names or uniquely identifies individual people or offensive content, and the steps taken to protect/anonymize it?\\nNot applicable. Left blank.\\n\\n\u25a1 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and linguistic phenomena, demographic groups represented, etc.?\\n3\\n\\n\u25a1 B6. Did you report relevant statistics like the number of examples, details of train/test/dev splits, etc. for the data that you used/created? Even for commonly-used benchmark datasets, include the number of examples in train/validation/test splits, as these provide necessary context for a reader to understand experimental results. For example, small differences in accuracy on large test sets may be significant, while on small test sets they may not be.\\n3\\n\\nC Did you run computational experiments?\\n\\n\u25a1 C1. Did you report the number of parameters in the models used, the total computational budget (e.g., GPU hours), and computing infrastructure used?\\n5\\n\\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing assistance.\"}"}
{"id": "acl-2023-long-453", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"C2. Did you discuss the experimental setup, including hyperparameter search and best-found hyperparameter values?\\n\\nC3. Did you report descriptive statistics about your results (e.g., error bars around results, summary statistics from sets of experiments), and is it transparent whether you are reporting the max, mean, etc. or just a single run?\\n\\nC4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did you report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE, etc.)?\\n\\nD. Did you use human annotators (e.g., crowdworkers) or research with human participants?\\n\\nD1. Did you report the full text of instructions given to participants, including e.g., screenshots, disclaimers of any risks to participants or annotators, etc.?\\n\\nD2. Did you report information about how you recruited (e.g., crowdsourcing platform, students) and paid participants, and discuss if such payment is adequate given the participants' demographic (e.g., country of residence)?\\n\\nD3. Did you discuss whether and how consent was obtained from people whose data you're using/curating? For example, if you collected data via crowdsourcing, did your instructions to crowdworkers explain how the data would be used?\\n\\nD4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\\n\\nD5. Did you report the basic demographic and geographic characteristics of the annotator population that is the source of the data?\\n\\nNot applicable. Left blank.\"}"}
