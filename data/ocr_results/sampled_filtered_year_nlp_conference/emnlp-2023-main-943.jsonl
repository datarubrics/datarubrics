{"id": "emnlp-2023-main-943", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nContext-aware neural machine translation, a paradigm that involves leveraging information beyond sentence-level context to resolve intersentential discourse dependencies and improve document-level translation quality, has given rise to a number of recent techniques. However, despite well-reasoned intuitions, most context-aware translation models yield only modest improvements over sentence-level systems. In this work, we investigate and present several core challenges, relating to discourse phenomena, context usage, model architectures, and document-level evaluation, that impede progress within the field. To address these problems, we propose a more realistic setting for document-level translation, called paragraph-to-paragraph (PARA2PARA) translation, and collect a new dataset of Chinese-English novels to promote future research.\\n\\n1 Introduction\\n\\nNeural machine translation (NMT) has garnered considerable scientific interest and commercial success in recent years, with current state-of-the-art systems approaching or exceeding human quality for a few resource-rich languages when translating individual sentences (Wu et al., 2016; Hasan et al., 2018; Yang et al., 2020). Despite the strong empirical performance of such systems, the independence assumption that underlies sentence-level NMT raises several issues. Certain textual elements, such as coreference (Guillou and Hardmeier, 2016), lexical cohesion (Carpuat, 2009), or lexical disambiguation (Rios Gonzales et al., 2017) are impossible to correctly translate without access to linguistic cues that exist beyond the present sentence (Sim Smith, 2017). When evaluating documents rather than individual sentences, the adequacy and fluency of professional human translation continues to surpass that of MT systems (L\u00e4ubli et al., 2018), thus underscoring the need for incorporating long-range context.\\n\\nDespite some efforts to meaningfully exploit inter-sentential information, many context-aware (or interchangeably, document-level) NMT systems only show meager gains across sentence-level and document-level translation metrics (Tiedemann and Scherrer, 2017; Miculicich et al., 2018; M\u00fcller et al., 2018; Tu et al., 2018; Maruf et al., 2019; Lupo et al., 2022a,b; Wu et al., 2022). Performance improvements against sentence-level baselines on overall translation accuracy, pronoun resolution, or lexical cohesion become less pronounced when context-aware systems are trained on realistic, high-resourced settings (Lopes et al., 2020), casting doubt on the efficacy of such approaches.\\n\\nIn this paper, we conduct a thorough empirical analysis and present some key obstacles that hinder progress in this domain:\\n\\n1. Existing document-level corpora contain a sparse number of discourse phenomena that require inter-sentential context to be accurately translated.\\n2. Though context is necessary for pronoun resolution and named entity consistency, it is less helpful for tense and discourse markers.\\n3. The sentence-level Transformer baseline already performs up to par with concatenation-based NMT settings.\\n4. Advanced model architectures do not meaningfully improve document-level translation on existing document-level datasets.\\n5. Current metrics designed for document-level translation evaluation do not adequately measure document-level translation quality.\\n\\nThe above findings suggest that paragraph-to-paragraph (PARA2PARA) translation, wherein a...\"}"}
{"id": "emnlp-2023-main-943", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Document is translated at the granularity of paragraphs, may serve as a more suitable and realistic setting for document-level translation, which in practice is unencumbered by sentence-level alignments. To this end, we develop and release a new paragraph-aligned Chinese-English dataset, consisting of 10,545 parallel paragraphs harvested from 6 novels within the public domain, in order to spur future research.\\n\\n2 Background\\n\\nThe high-level objective of sentence-level machine translation is to model the sentence-level conditional probability $P(y|x)$, in which the source and target sentences $x = (x_1, ..., x_M)$, $y = (y_1, ..., y_N)$ are textual sequences of respective lengths $M$ and $N$. Under the dominant paradigm of neural machine translation (Sutskever et al., 2014), the conditional probability $P_{\\\\theta}(y|x)$ is typically decomposed into the following auto-regressive formulation (with $\\\\theta$ denoting parameterized weights):\\n\\n$$P_{\\\\theta}(y|x) = \\\\prod_{n=1}^{N} P_{\\\\theta}(y_n|x, y_{<n})$$  \\\\hspace{1cm} (1)\\n\\nEquation 1 implies that when predicting the target token $y_n$, the model could only access the current source sentence, $x$, as well as all previously translated tokens in the current target sentence, $y_{<n}$.\\n\\nTranslating sentences in a document in such an isolated fashion, without any extra-sentential information that lies beyond sentence boundaries, has been found to produce syntactically valid, but semantically inconsistent text (L\u00e4ubli et al., 2018). To remedy this, context-aware neural machine translation considers a document $D$ that entails a set of logically cohesive source sentences $X = \\\\{x_1, x_2, ..., x_d\\\\}$, and a parallel set of target sentences $Y = \\\\{y_1, y_2, ..., y_d\\\\}$. Under a left-to-right translation schema, the model computes the probability of translating the source sentence $x_i$ conditioned on the context $C_i$, wherein $0 \\\\leq i \\\\leq d$:\\n\\n$$P_{\\\\theta}(y_i|x_i, C_i) = \\\\prod_{j=1}^{J} P_{\\\\theta}(y_{ji}|x_{ji}, y_{<ji}, C_i)$$  \\\\hspace{1cm} (2)\\n\\nIn practice, there are multiple ways to formulate $C_i$. Passing in $C_i = \\\\{\\\\emptyset\\\\}$ reduces to the sentence-level case (1). Throughout this paper, we explore two concatenation-based setups first presented by Tiedemann and Scherrer (2017). The 1-2 setup prepends the preceding target sentence to the current target sentence ($C_i = \\\\{y_{i-1}\\\\}$), denoting sentence boundaries with a <SEP> token. The 2-2 setup incorporates additional context from the previous source sentence ($C_i = \\\\{x_{i-1}, y_{i-1}\\\\}$). The target context is integrated in the same manner as in one-to-two.\\n\\nIn order to investigate the importance of context after the current sentence, we also explore a 3-1 setting, wherein we introduce additional source-side context by concatenating the previous and subsequent sentences to the current one ($C_i = \\\\{x_{i-1}, x_{i+1}\\\\}$), and do not incorporate any target context.\\n\\n3 Related Work\\n\\n3.1 Model Architectures\\n\\nRecent progress in context-aware NMT generally falls along two lines: multi-encoder approaches and concatenation-based ones (Kim et al., 2019). Under the first taxonomy, additional sentences are encoded separately, such that the model learns an internal representation of context sentences independently from the current sentence. The integration of context and current sentences can occur either prior to being fed into the decoder (Maruf and Haffari, 2018; Voita et al., 2018; Miculicich et al., 2018; Zhang et al., 2018; Maruf et al., 2019), or within the decoder itself (Bawden et al., 2018; Cao and Xiong, 2018; Kuang and Xiong, 2018; Stojanovski and Fraser, 2018; Tu et al., 2018; Zhang et al., 2018). The effectiveness of these multi-encoder paradigms is subject to debate; in a standardized analysis, Li et al. (2020) finds that rather than effectively harnessing inter-sentential information, the context encoder functions more as a noise generator that provides richer self-training signals, since even the inclusion of random contextual input can yield substantial translation improvement. In addition, Sun et al. (2022) finds that BLEU-score improvements from context-aware approaches often diminish with larger training datasets or thorough baseline tuning.\\n\\nOn the other hand, concatenation-based NMT approaches are conceptually simpler and have been found to perform on par with or better than multi-encoder systems (Lopes et al., 2020; Ma et al., 2021). Under this paradigm, context sentences are appended to the current sentence, with special tokens to mark sentence boundaries, and the concatenated sequence is passed as input through the encoder-decoder architecture (Ma et al., 2020).\"}"}
{"id": "emnlp-2023-main-943", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2 Datasets\\n\\nUntil recently, the bulk of context-aware NMT research has focused on document-level, sentence-aligned parallel datasets. Most commonly used corpora, including IWSLT-17 (Cettolo et al., 2012), NewsCom (Tiedemann, 2012), Europarl (Koehn, 2005), and OpenSubtitles (Lison et al., 2018) are sourced from news articles or parliamentary proceedings. Such datasets often contain a high volume of sentences that is sufficient for training sentence-level NMT systems, yet the number of documents remains comparatively limited.\\n\\nIn an attempt to address the scarcity of document-level training data, recent works have developed datasets that are specifically tailored for context-aware NMT. Jiang et al. (2023) curated Bilingual Web Books (BWB), a document-level parallel corpus consisting of 9.6 million sentences and 196 thousand documents (chapters) sourced from English translations of Chinese web novels. Thai et al. (2022) introduced P\\\\textsc{AR}, a multilingual dataset of non-English novels from the public domain, which is aligned at the paragraph level based on both human and automatic translations. Using automatic sentence alignments, Al Ghussin et al. (2023) extracted parallel paragraphs from Paracrawl (Ba\u00f1\u00f3n et al., 2020), which consists of crawled webpages.\\n\\n3.3 Evaluation\\n\\nIn addition to metrics that evaluate sentence-level translation quality, e.g., BLEU (Papineni et al., 2002) and COMET (Rei et al., 2020), a number of automatic metrics designed specifically for document-level MT have been recently proposed. Jiang et al. (2022) introduced BlonDe, a document-level automatic metric that calculates the similarity-based F1 measure of discourse-related spans across four categories. Vernikos et al. (2022) show that pre-trained metrics, such as COMET, can be extended to incorporate context for document-level evaluation. To measure the influence of context usage in context-aware NMT models, Fernandes et al. (2021) proposed Context-aware Cross Mutual Information (CXMI), a language-agnostic indicator that draws from cross-mutual information.\\n\\nAnother approach to document-level MT evaluation focuses on hand-crafted contrastive evaluation sets to gauge the model's capacity for capturing inter-sentential discourse phenomena, including ContraPro (M\u00fcller et al., 2018) in English-to-German, Bawden (Bawden et al., 2018) in English-to-French, and Voita (Voita et al., 2019) in English-to-Russian translation. Though targeted, these test sets tend to be small, and are constricted to a particular language pair and discourse phenomenon.\\n\\n4 Challenges\\n\\nWe identify key obstacles that account for the lack of progress in this field, based on a careful empirical analysis over a range of language pairs, model architectures, concatenation schemas, and document-level phenomena.\\n\\n4.1 Discourse phenomena is sparse in surrounding context.\\n\\nContextual sparsity is a bottleneck to document-level neural machine translation that manifests in two forms (Lupo et al., 2022a). First, the majority of words within a sentence can be accurately translated without additional access to inter-sentential information; context poses as a weak training signal and its presence has not been found to substantially boost translation performance. Second, only a few words in neighboring sentences may actually contribute to the disambiguation of current tokens at translation time.\\n\\nWe investigate contextual sparsity via a fine-grained analysis on the BWB (Jiang et al., 2022) test set, which has been manually tagged with specific discourse-level phenomena. Specifically, we use it to probe NMT models' ability to exploit long-range context by analyzing the frequency of particular discourse phenomena that can only be resolved with context.\\n\\nFor the manual analysis, we randomly sample 200 discourse-annotated instances from the test set and ask bilingual annotators who are fluent in Chinese and English to identify and count instances that contain a particular context-dependent discourse phenomenon. Annotators are asked to discern if the following document-level discourse phenomena exist in each sentence pair:\\n\\n- **Pronoun Ellipsis:** The pronoun is dropped in Chinese, but must be included in the English translation.\\n\\n4.1 Discourse phenomena is sparse in surrounding context.\\n\\nContextual sparsity is a bottleneck to document-level neural machine translation that manifests in two forms (Lupo et al., 2022a). First, the majority of words within a sentence can be accurately translated without additional access to inter-sentential information; context poses as a weak training signal and its presence has not been found to substantially boost translation performance. Second, only a few words in neighboring sentences may actually contribute to the disambiguation of current tokens at translation time.\\n\\nWe investigate contextual sparsity via a fine-grained analysis on the BWB (Jiang et al., 2022) test set, which has been manually tagged with specific discourse-level phenomena. Specifically, we use it to probe NMT models' ability to exploit long-range context by analyzing the frequency of particular discourse phenomena that can only be resolved with context.\\n\\nFor the manual analysis, we randomly sample 200 discourse-annotated instances from the test set and ask bilingual annotators who are fluent in Chinese and English to identify and count instances that contain a particular context-dependent discourse phenomenon. Annotators are asked to discern if the following document-level discourse phenomena exist in each sentence pair:\\n\\n- **Pronoun Ellipsis:** The pronoun is dropped in Chinese, but must be included in the English translation.\\n\\n4.1 Discourse phenomena is sparse in surrounding context.\\n\\nContextual sparsity is a bottleneck to document-level neural machine translation that manifests in two forms (Lupo et al., 2022a). First, the majority of words within a sentence can be accurately translated without additional access to inter-sentential information; context poses as a weak training signal and its presence has not been found to substantially boost translation performance. Second, only a few words in neighboring sentences may actually contribute to the disambiguation of current tokens at translation time.\\n\\nWe investigate contextual sparsity via a fine-grained analysis on the BWB (Jiang et al., 2022) test set, which has been manually tagged with specific discourse-level phenomena. Specifically, we use it to probe NMT models' ability to exploit long-range context by analyzing the frequency of particular discourse phenomena that can only be resolved with context.\\n\\nFor the manual analysis, we randomly sample 200 discourse-annotated instances from the test set and ask bilingual annotators who are fluent in Chinese and English to identify and count instances that contain a particular context-dependent discourse phenomenon. Annotators are asked to discern if the following document-level discourse phenomena exist in each sentence pair:\\n\\n- **Pronoun Ellipsis:** The pronoun is dropped in Chinese, but must be included in the English translation.\\n\\n4.1 Discourse phenomena is sparse in surrounding context.\\n\\nContextual sparsity is a bottleneck to document-level neural machine translation that manifests in two forms (Lupo et al., 2022a). First, the majority of words within a sentence can be accurately translated without additional access to inter-sentential information; context poses as a weak training signal and its presence has not been found to substantially boost translation performance. Second, only a few words in neighboring sentences may actually contribute to the disambiguation of current tokens at translation time.\\n\\nWe investigate contextual sparsity via a fine-grained analysis on the BWB (Jiang et al., 2022) test set, which has been manually tagged with specific discourse-level phenomena. Specifically, we use it to probe NMT models' ability to exploit long-range context by analyzing the frequency of particular discourse phenomena that can only be resolved with context.\\n\\nFor the manual analysis, we randomly sample 200 discourse-annotated instances from the test set and ask bilingual annotators who are fluent in Chinese and English to identify and count instances that contain a particular context-dependent discourse phenomenon. Annotators are asked to discern if the following document-level discourse phenomena exist in each sentence pair:\\n\\n- **Pronoun Ellipsis:** The pronoun is dropped in Chinese, but must be included in the English translation.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lexical Cohesion: The same named entity must be translated consistently across the current sentence and context sentences.\\n\\nTense: Tense information that can be omitted in Chinese, and must be inferred based on context to be correctly translated in English.\\n\\nAmbiguity: Instances in which an ambiguous word or phrase in the current sentence requires context to be correctly translated.\\n\\nDiscourse Marker: A discourse marker, e.g., while, as long as, else, that is not explicit in Chinese, but must be pragmatically inferred and present in English.\\n\\nTable 1 indicates that lexical cohesion (83.2%) and pronoun ellipsis (53.8%) constitute the majority of discourse phenomena found in the 119 sentences that require inter-sentential signals for correct translation. In contrast, other categories\u2014tense (4.2%), ambiguity (9.2%) and discourse marker (16.8%)\u2014occur much less frequently.\\n\\nWe next examine how far the useful context tends to be from the cross-lingually ambiguous sentence. Taking $d$ as the sentence distance, the majority of discourse phenomena can be disambiguated based on the nearest context sentence ($d=1$). Specifically, the necessary information for tense, ambiguity, and discourse markers can almost always be found by $d=1$, whereas relevant context for pronoun ellipses and lexical cohesion tends to be more spread out. Hardly any useful information can be found in very distant context ($d>3$).\\n\\nA significant fraction (40.5%) of sentences in the sampled test set can be translated independently, i.e., without access to inter-sentential information. Correspondingly, we notice that many sentences across document-level data are not lengthy with discourse-level phenomena, but rather simple constructions. Figure 1 indicates that the majority of sentences are relatively short in BWB and IWSLT-17, ranging from 20-50 characters (Chinese) or 10-30 words (French and German).\\n\\n| Discourse Phenomena | Freq. | d=1 (%) | d=2 (%) | d=3 (%) | d>3 (%) |\\n|---------------------|-------|---------|---------|---------|---------|\\n| Ellp. Pronoun       | 64    | 76.6    | 12.5    | 7.8     | 3.1     |\\n| Lexical Cohesion    | 99    | 56.6    | 23.2    | 13.1    | 7.1     |\\n| Tense               | 5     | 100.0   | 0.0     | 0.0     | 0.0     |\\n| Ambiguity           | 11    | 90.1    | 9.9     | 0.0     | 0.0     |\\n| Discourse Marker    | 20    | 100.0   | 0.0     | 0.0     | 0.0     |\\n| No Context          | 81    | \u2013       | \u2013       | \u2013       | \u2013       |\\n\\nTable 1: Frequency of context-dependent discourse phenomena in a 200-count sample of the BWB test set, and the percentage of cases where relevant context can be found at distance $d=1$, $d=2$, $d=3$, $d>3$ sentences.\\n\\n4.2 Context does not help disambiguate certain discourse phenomena.\\n\\nAn implicit assumption in context-aware NMT is that the inclusion of the proper context would influence the model to leverage it to resolve any potential discourse ambiguities. To this end, we investigate different types of discourse phenomena on the BWB test set and show that this premise does not always hold; while pronoun resolution or named entity consistency is often better resolved with the incorporation of context, tense and discourse markers are relatively insensitive to context and yield meager improvement.\\n\\n4.2.1 Pronoun Resolution\\n\\nWe examine two types of pronoun translation: pronoun ellipsis and anaphoric resolution.\\n\\nPronoun ellipsis. As Chinese is a pro-drop language, pronouns can be freely omitted and are implicitly inferred from surrounding context. In contrast, grammatical and comprehensible translation into English requires that the pronoun be made explicit. To test concatenation-based NMT systems' ability to resolve Chinese-English pronoun ellipsis, we conduct inference on a subset of BWB that contains 519 instances of pronoun ellipsis.\\n\\nTable 2 indicates that the disambiguation of pronoun ellipsis is particularly responsive to context. Incorporating a single target-side context sentence (the 1-2 setting) improves the BlonDe F1-score from 55.88 to 63.91; adding another source-side context sentence (the 2-2 setting) marginally improves to 65.91. In this scenario, more source-side context may carry useful information, as the 3-1 setting performs the best overall on BlonDe (66.06).\\n\\nAnaphoric resolution. When translating to languages that contain grammatical gender, anaphoric pronouns form another instance of cross-lingual...\"}"}
{"id": "emnlp-2023-main-943", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: BLENDE evaluation of pronoun translation on the BWB test subset and accuracy for anaphoric pronoun resolution on CONTRA PRO and BAWDEN. The 3-1 setting requires the surrounding context sentences, and therefore cannot be applied to contrastive sets.\\n\\n| Setting | Person | Non-person | Person |\\n|---------|--------|------------|--------|\\n| 1-1     | 32.34  | 14.67      | 54.55  |\\n| 1-2     | 49.36  | 21.33      | 51.96  |\\n| 2-2     | 45.53  | 14.67      | 52.42  |\\n| 3-1     | 36.17  | 17.33      | 51.15  |\\n\\nTable 3: Named entity analysis for consistency and accuracy on relevant samples from the BWB test set. Ambiguity. For example, when translating into German, the English pronoun *it* can become either *es*, *sie*, or *er*, depending on the grammatical gender of its referent.\\n\\nThus, we also conducted experiments from English to German (En\u2192De) and French (En\u2192Fr), both grammatically gendered languages, and evaluated on the contrastive sets ControPro (M\u00fcller et al., 2018) and Bawden (Bawden et al., 2018), respectively. While Table 2 shows steady improvement for anaphoric resolution on ControPro, curiously, the 1-2 concatenation-based model exhibits a slight dip compared to its sentence-level counterpart on Bawden. We hypothesize that the small size (200 examples) of the Bawden dataset causes the significant variance in the results.\\n\\n4.2.2 Named Entities\\n\\nNamed entities\u2014real-world objects denoted with proper names\u2014are domain-specific and low-frequency, and thus tend to be absent from bilingual dictionaries (Modrzejewski et al., 2020). Their translations are often either inconsistent (e.g., different target translations for the same source phrase) or inaccurate (with regards to some target reference). In this section, we examine for named entity consistency and accuracy on the annotated BWB test set.\\n\\n| Type all contrast cause cond. conj. (a)syn. | Count | 1-1 | 1-2 | 2-2 | 3-1 |\\n|------------------------------------------|-------|-----|-----|-----|-----|\\n| Consistency (%)                          | 55.68 | 58.97 | 40.99 | 71.68 | 47.15 | 56.59 |\\n| Accuracy (%)                             | 55.39 | 57.05 | 37.12 | 70.80 | 52.03 | 57.51 |\\n| 1-1                                       | 54.99 | 57.05 | 37.12 | 70.80 | 51.21 | 56.79 |\\n| 3-1                                       | 53.57 | 59.97 | 37.12 | 65.48 | 43.90 | 54.46 |\\n\\nTable 4: Accuracy across discourse marker categories and concatenation settings on the BWB test set.\\n\\nConsistency. We extract 780 examples (705 person entities, 75 non-person entities) to construct a consistency test subset. Each instance includes a sentence with a named entity that is also mentioned in the preceding sentence. We then measure the frequency at which different context-aware translation models could consistently translate the entity across the two consecutive sentences.\\n\\nAccording to Table 3, this task proves to be challenging\u2014no system achieves above-random performance\u2014but the presence of context facilitates consistency as each context-aware setting performs better than the 1-1 baseline on person entities (32.34%). Adding target-side context (1-2 and 2-2 settings) appears strictly more helpful. By contrast, source-side context (3-1 setting) results in marginal performance gains relative to the baseline.\\n\\nAccuracy. To explore the frequency at which named entities are accurately translated, we next examine the 1734 person entities from the BWB test set. Surprisingly, the sentence-level model is better than context-aware models at correctly translating named entities, with the best accuracy of 54.55% (Table 3). While context is important for ensuring named entity consistency, these findings suggest that adding context may introduce additional noise and do not necessarily lead to more accurate translations. We hypothesize that the dependency on context might hurt the model's downstream performance when the NMT model tries to be consistent with the context translation, which results in a propagation of errors down the sequence.\\n\\nIn addition, when comparing all the results using the entity category in BLENDE across the three language pairs in Table 5 and Table 6, it becomes clear that additional context does not meaningfully increase the accuracy of named entity translation.\\n\\n4.2.3 Discourse Marker and Tense\\n\\nDiscourse markers. The omission of discourse markers (DM)\u2014particles that signal the type of coherence relation between two segments (Grote and...\"}"}
{"id": "emnlp-2023-main-943", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Automatic metric results on BWB (Zh \u2192 En) across different architectures (XFMR and MEGA) and concatenation settings (1-1, 1-2, 2-2, and 3-1). We report average and standard deviations across three runs.\\n\\nStede, 1998), e.g., so, because, for this reason\u2014requires context awareness when translating from morphologically poorer languages to morphologically richer ones. Following Jiang et al. (2022), we separate DMs into five categories: contrast, cause, condition, conjunction, and (a-)synchronous, and examine how different context-aware settings fare with each discourse relation.\\n\\nAs Table 4 shows, the sentence-level (1-1) baseline performs the best across discourse markers in aggregate, and across the cause and condition categories. The incorporation of context does not significantly improve the accuracy of discourse marker translation; interestingly, the 3-1 setting fares poorly, with the lowest performance across all categories except on contrast DMs.\\n\\nTense. Tense consistency is another extrasentential phenomenon that requires context for disambiguation, particularly when translating from an analytic source language (e.g., Chinese) to a synthetic target language (e.g., English), wherein tense must be made explicit.\\n\\nFrom experimental results on the BWB (Table 5) and IWSLT (Table 6) data, there is minimal variance across all translation settings in the BlonDe scores for tense and DM, suggesting that context is not particularly conducive for any language pair. Tense is generally consistently resolvable, with all models surpassing 70 on Zh \u2192 En.\\n\\nAs expected, translating from French\u2014a more synthetic language\u2014yields marginally higher BlonDe scores, at over 75. One reason that the BlonDe score for tense may be relatively inflexible across language pairs is that most sentences from the corpora generally adhere to a particular tense, such as past tense in literature, thus diminishing the necessity of context.\\n\\n4.2.4 Is source or target context more helpful? Fernandes et al. (2021) finds that concatenation-based context-aware NMT models lean on target context more than source context, and that incorporating more context sentences on either side often leads to diminishing returns in performance. However, according to Table 2-6, this is not universally the case; the effectiveness of target-side versus source-side context is largely dependent on the language pair. Though target-side context often helps with translation consistency, such as preserving grammatical formality across sentences, it does not necessarily guarantee a better translation quality than source-side context (e.g., the 3-1 setting performs best on pronoun translation for French and German according to Table 6, and pronoun ellipsis for Chinese in Table 2).\\n\\n4.3 The context-agnostic baseline performs comparably to context-aware settings. Experimental results across both the BWB (Table 5) and IWSLT-17 (Table 6) datasets demonstrate that a vanilla 1-1 baseline performs on par with, or even better than its context-aware counterparts on the sentence-level automatic metrics, BLEU and COMET. This suggests that, due to problems with common document-level datasets (e.g., relative lack of contextual signals) (\u00a74.1),\"}"}
{"id": "emnlp-2023-main-943", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Automatic metric results on IWSLT-17 (Fr $\\\\rightarrow$ En and De $\\\\rightarrow$ En), on different architectures (XFMR and MEGA) and concatenation settings (1-1 and 3-1). We report average and standard deviations across three runs.\\n\\nand the inability of sentence-level metrics to cap-\\nture document-level attributes, context-aware mod-\\nels do not exhibit a meaningful improvement over\\ncontext-agnostic models at the sentence level. In terms of document-level improvement, the\\nsentence-level baseline even outperforms context-\\naware models in select instances, such as when\\ntranslating named entities (53.17% on Zh, 65.11%\\non De). There are no notable differences in han-\\ndling tense and discourse markers across contex-\\ntual settings, which aligns with our observations\\nin \u00a74.2.3. These results demonstrate that on com-\\nmonly used datasets, context-aware models also do\\nnot significantly improve document-level transla-\\ntion over a sentence-level Transformer baseline.\\n\\n4.4 Advanced model architectures do not\\nmeaningfully improve performance.\\n\\nMotivated by the limitations of the self-attention\\nmechanism on long-range dependency model-\\ning (Tay et al., 2022), recent work has proposed\\nmore advanced architectures to better leverage con-\\ntextual signals into translation (Lupo et al., 2022b;\\nSun et al., 2022; Wu et al., 2022, 2023). The hy-\\npothesis is that as long-range sequence architec-\\ntures can effectively model longer context windows,\\nthey are better-equipped to handle the lengthier na-\\nture of document-level translation. To test this theory, we replace the Transformer\\n(XFMR) attention mechanism with a recently intro-\\nduced MEGA architecture (Ma et al., 2023), which\\novercomes several limitations of the Transformer\\non long-range sequence modeling.\\n\\nAs Table 6 shows, MEGA always performs better than XFMR across both the 1-1 and 3-1 settings on the sentence-\\nlevel metrics, BLEU and COMET. At the document\\nlevel, MEGA has the highest overall BlonDe F1-\\nscore when translating from both German (53.37 vs.\\n52.88) and French (49.22 vs. 48.23). While MEGA\\ntends to outscore XFMR on the pronoun and entity\\ncategories, there is no significant improvement, if\\nany for tense and discourse marker. Furthermore,\\nMEGA usually starts from a higher sentence-level\\nbaseline (except on pronoun resolution for Fr $\\\\rightarrow$\\nEn); when moving from the sentence-level to the con-\\ntextual 3-1 setting, MEGA does not show higher\\nrelative gains than XFMR.\\n\\nOne potential explanation as to why MEGA per-\\nforms better on automatic metrics is because it is a\\nstronger model and better at translation overall (Ma\\net al., 2023), rather than it being able to leverage\\ncontext in a more useful manner. The lack of im-\\nprovement in particular discourse categories does\\nnot necessarily indicate that existing context-aware\\nmodels are incapable of handling long-range dis-\\ncourse phenomena. Rather, it suggests that current\\ndata may not sufficiently capture the complexities\\nin such situations. As discussed, discourse phenom-\\nena are sparse; some of them could not be resolved\\neven with necessary context.\\n\\nThis finding aligns with similar work (Sun et al.,\\n2022; Post and Junczys-Dowmunt, 2023) which\\nalso propose that, on existing datasets and under\\ncurrent experimental settings that use sentence-\\nlevel alignments, the standard Transformer model\\nremains adequate for document-level translation.\\n\\n4.5 There is a need for an appropriate\\ndocument-level translation metric.\\n\\nThough BLEU and COMET are both widely used\\nfor sentence-level machine translation, they pri-\\nmarily focus on assessing sentence-level transla-\\ntion, while BLEU and COMET are commonly used for sentence-level machine translation. They primarily focus on assessing sentence-level translation, but do not capture document-level attributes. Context-aware models do not exhibit a meaningful improvement over context-agnostic models at the sentence level. In terms of document-level improvement, the sentence-level baseline can even outperform context-aware models in some cases, such as when translating named entities. Tense and discourse markers are handled similarly across contextual settings, aligning with observations in \u00a74.2.3.\\n\\nThese results demonstrate that on commonly used datasets, context-aware models do not significantly improve document-level translation over a sentence-level Transformer baseline.\\n\\n4.4 Advanced model architectures do not meaningfully improve performance.\\n\\nMotivated by the limitations of the self-attention mechanism on long-range dependency modeling (Tay et al., 2022), recent work has proposed more advanced architectures to leverage contextual signals into translation (Lupo et al., 2022b; Sun et al., 2022; Wu et al., 2022, 2023). The hypothesis is that long-range sequence architectures can effectively model longer context windows, making them better-equipped to handle the lengthier nature of document-level translation.\\n\\nTo test this theory, we replaced the Transformer (XFMR) attention mechanism with a recently introduced MEGA architecture (Ma et al., 2023), addressing limitations of the Transformer in long-range sequence modeling.\\n\\nAs Table 6 shows, MEGA consistently outperforms XFMR in both 1-1 and 3-1 settings for sentence-level metrics, BLEU and COMET. At the document level, MEGA has the highest overall BlonDe F1-score when translating from both German (53.37 vs. 52.88) and French (49.22 vs. 48.23). While MEGA outperforms XFMR in pronoun and entity categories, there is no significant improvement, if any, for tense and discourse markers. MEGA usually starts from a higher sentence-level baseline (except on pronoun resolution for Fr $\\\\rightarrow$ En), and when transitioning to the contextual 3-1 setting, MEGA does not show higher relative gains than XFMR.\\n\\nOne potential explanation for MEGA's better performance on automatic metrics is that it is a stronger model overall (Ma et al., 2023), rather than its ability to leverage context more effectively. The lack of improvement in specific discourse categories does not necessarily indicate that existing context-aware models are incapable of handling long-range discourse phenomena. Instead, it suggests that current data may not sufficiently capture the complexities in such situations. As discussed, discourse phenomena are sparse; some cannot be resolved even with necessary context.\\n\\nThis finding aligns with similar work (Sun et al., 2022; Post and Junczys-Dowmunt, 2023), which proposes that for existing datasets and current experimental settings using sentence-level alignments, the standard Transformer model remains adequate for document-level translation.\\n\\n4.5 There is a need for an appropriate document-level translation metric.\\n\\nThough BLEU and COMET are widely used for sentence-level machine translation, they primarily focus on assessing sentence-level translation. This limitation highlights the need for a more suitable metric that can effectively capture the complexities of document-level translation, especially considering the limitations of current metrics and models.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can never go back again, for the past is still too close. The things we have tried to forget would stir again and that sense of fear building up the blind unreasoning panic, now mercifully stilled\u2014might once again become a living companion.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We then benchmark the dataset under two experimental settings for Zh\u2192En translation: i). a standard closed-domain setup, in which both the training and testing data are sourced from the same novels; ii). a more challenging open-domain setup, wherein two novels are held and used as only the test set. We experiment with training a Transformer-based model on PARA2PARA data from scratch (NONE), as well as incorporating pre-trained baselines, in which the model is first trained on the sentence-level WMT17 Zh-En dataset (Bojar et al., 2017), before further fine-tuning on the PARA2PARA data, using the following backbone architectures:\\n\\n\u2022 XFMR Big (Vaswani et al., 2017), the Transformer-BIG.\\n\u2022 LIGHT CONV Big (Wu et al., 2019), which replaces the self-attention modules in the Transformer-BIG with fixed convolutions.\\n\u2022 MBART25 (Liu et al., 2020), which is pre-trained on 25 languages at the document level.\\n\\nTable 7 shows preliminary baseline results on BLEU, BlonDe, and COMET.\\n\\nIn the NONE setting, the Transformer\u2019s relatively low performance and incoherent output underscores the difficulty of training from scratch on the PARA2PARA corpus, due to two reasons\u2014the inherent difficulty of training on paragraph-level, longer-sequence data, and the limited dataset size (especially relative to that of sentence-level MT datasets). To disentangle the two factors, we report additional baselines that leverage pre-training to offset the issue of low-domain data; all of them exhibit a marked performance improvement over the NONE setting, attesting to the challenging constitution of paragraph-to-paragraph translation.\\n\\nOn the closed-domain setting, LIGHT CONV Big yields the highest score across all three metrics. Open-domain results are mixed: as expected, scores are lower across the board as this setting is challenging. XFMR Big has the best BLEU and discourse marker F1-score on BlonDe, although all pre-training baselines perform similarly. LIGHT-CONV Big performs the best on pronoun, entity, and tense on BlonDe and has the highest COMET score.\\n\\n6 Conclusion\\n\\nDespite machine-human parity at the sentence level, NMT still lags behind human translation on long collections of text, motivating the need for context-aware systems that leverage signals beyond the current sentence boundary. In this work, we highlight and discuss key obstacles that hinder momentum in context-aware NMT. We find that training signals that improve document-level discourse phenomena occur infrequently in surrounding context, and that most sentences can be accurately translated in isolation. Another challenge is that context benefits the resolution of some discourse phenomena over others. A context-agnostic Transformer baseline is already competitive against context-aware settings, and replacing the Transformer\u2019s self-attention mechanism with a more complex long-range mechanism does not significantly improve translation performance. We also note the need for a generalizable document-level translation metric. Finally, we make the case for paragraph-aligned translation, and release a new PARA2PARA dataset, alongside baseline results, to encourage further efforts in this direction.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Limitations\\n\\nSeveral limitations restrict the scope of this work. To begin, our choice of languages in this study\u2014English, Chinese, French, German\u2014is non-exhaustive, and it is possible that our findings would not generalize well to scenarios that involve low-resourced languages or distant language pairs. In particular, a significant portion of our investigation on discourse relations that necessitate context for proper disambiguation targets the Chinese-English BWB test set, which is the only public dataset that has been manually annotated with this type of information. Some of the discourse phenomena that we consider may not occur as frequently in other languages. While this work is a preliminary step that sheds light on the current nature of data that drives context-aware neural machine translation, future directions could entail extending similar analysis to other languages or discourse phenomena (e.g., the disambiguation of deixis when translating from Russian to English (Voita et al., 2019)).\\n\\nAnother restriction is that this work only examines concatenation-based architectures, which tend to be conceptually simple, effective, and hence subject to widespread adoption in recent years (Fernandes et al., 2021). While the purported advantages of multi-encoder NMT models are mixed (Li et al., 2020), for comprehensiveness, it would be insightful to examine whether they behave differently relative to concatenation-based systems under our experimental setup. Other potential avenues for exploration entail loss-based approaches to context-aware neural machine translation, such as context discounting (Lupo et al., 2022b) or contrastive learning-based schemas (Hwang et al., 2021).\\n\\nLastly, although the PARA2 dataset may pose as a more natural setting for context-aware translation, it is considerably smaller than other document-level datasets. Given that the small scale of training data is a prevalent issue in context-aware neural machine translation (Sun et al., 2022), future efforts could focus on expanding this dataset (as it is easier to source paragraph-aligned parallel translations in the wild than sentence-level ones) or moving beyond the literary domain.\\n\\nAcknowledgements\\n\\nWe thank the anonymous reviewers for their constructive feedback in improving this work. JH is supported by an NSF Graduate Research Fellowship. This research is supported in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via the HIATUS Program contract #2022-22072200006. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for governmental purposes notwithstanding any copyright annotation therein.\\n\\nReferences\\n\\nYusser Al Ghussin, Jingyi Zhang, and Josef van Genabith. 2023. Exploring paracrawl for document-level neural machine translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1304\u20131310, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nMarta Ba\u00f1\u00f3n, Pinzhen Chen, Barry Haddow, Kenneth Heafield, Hieu Hoang, Miquel Espl\u00e0-Gomis, Mikel L. Forcada, Amir Kamran, Faheem Kirefu, Philipp Koehn, Sergio Ortiz Rojas, Leopoldo Pla Sempere, Gema Ram\u00edrez-S\u00e1nchez, Elsa Sarr\u00edas, Marek Strelec, Brian Thompson, William Waites, Dion Wiggins, and Jaume Zaragoza. 2020. ParaCrawl: Web-scale acquisition of parallel corpora. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 4555\u20134567, Online. Association for Computational Linguistics.\\n\\nRachel Bawden, Rico Sennrich, Alexandra Birch, and Barry Haddow. 2018. Evaluating discourse phenomena in neural machine translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1304\u20131313, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nOndej Bojar, Rajen Chatterjee, Christian Federmann, Yvette Graham, Barry Haddow, Shujian Huang, Matthias Huck, Philipp Koehn, Qun Liu, Varvara Logacheva, Christof Monz, Matteo Negri, Matt Post, Raphael Rubino, Lucia Specia, and Marco Turchi. 2017. Findings of the 2017 conference on machine translation (wmt17). In Proceedings of the Second Conference on Machine Translation, Volume 2: Shared Task Papers, pages 169\u2013214, Copenhagen, Denmark. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\\n\\nQian Cao and Deyi Xiong. 2018. Encoding gated translation memory into neural machine translation. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 3042\u20133047, Brussels, Belgium. Association for Computational Linguistics.\\n\\nMarine Carpuat. 2009. One translation per discourse. In Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions (SEW-2009), pages 19\u201327, Boulder, Colorado. Association for Computational Linguistics.\\n\\nMauro Cettolo, Christian Girardi, and Marcello Federico. 2012. WIT3: Web inventory of transcribed and translated talks. In Proceedings of the 16th Annual conference of the European Association for Machine Translation, pages 261\u2013268, Trento, Italy. European Association for Machine Translation.\\n\\nPatrick Fernandes, Kayo Yin, Graham Neubig, and Andr\u00e9 F. T. Martins. 2021. Measuring and increasing context usage in context-aware machine translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6467\u20136478, Online. Association for Computational Linguistics.\\n\\nBrigitte Grote and Manfred Stede. 1998. Discourse marker choice in sentence planning. In Natural Language Generation, Niagara-on-the-Lake, Ontario, Canada. Association for Computational Linguistics.\\n\\nLiane Guillou and Christian Hardmeier. 2016. PROTEST: A test suite for evaluating pronouns in machine translation. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), pages 636\u2013643, Portoro\u017e, Slovenia. European Language Resources Association (ELRA).\\n\\nHany Hassan, Anthony Aue, Chang Chen, Vishal Chowdhary, Jonathan Clark, Christian Federmann, Xuedong Huang, Marcin Junczys-Dowmunt, William Lewis, Mu Li, Shujie Liu, Tie-Yan Liu, Renqian Luo, Arul Menezes, Tao Qin, Frank Seide, Xu Tan, Fei Tian, Lijun Wu, Shuangzhi Wu, Yingce Xia, Dongdong Zhang, Zhirui Zhang, and Ming Zhou. 2018. Achieving human parity on automatic Chinese to English news translation. CoRR, abs/1803.05567.\\n\\nJ. Stuart Hunter. 1986. The exponentially weighted moving average. In Journal of Quality Technology.\\n\\nYongkeun Hwang, Hyeongu Yun, and Kyomin Jung. 2021. Contrastive learning for context-aware neural machine translation using coreference information. In Proceedings of the Sixth Conference on Machine Translation, pages 1135\u20131144, Online. Association for Computational Linguistics.\\n\\nYuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Mrinmaya Sachan, and Ryan Cotterell. 2023. Discourse-centric evaluation of document-level machine translation with a new densely annotated parallel corpus of novels. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 7853\u20137872, Toronto, Canada. Association for Computational Linguistics.\\n\\nYuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dongdong Zhang, Jian Yang, Haoyang Huang, Rico Sennrich, Ryan Cotterell, Mrinmaya Sachan, and Ming Zhou. 2022. BlonDe: An automatic evaluation metric for document-level machine translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1550\u20131565, Seattle, United States. Association for Computational Linguistics.\\n\\nMarzena Karpinska and Mohit Iyyer. 2023. Large language models effectively leverage document-level context for literary translation, but critical errors persist. In WMT.\\n\\nYunsu Kim, Duc Thanh Tran, and Hermann Ney. 2019. When and why is document-level context useful in neural machine translation? In Proceedings of the Fourth Workshop on Discourse in Machine Translation (DiscoMT 2019), pages 24\u201334, Hong Kong, China. Association for Computational Linguistics.\\n\\nPhilipp Koehn. 2005. Europarl: A parallel corpus for statistical machine translation. In Proceedings of Machine Translation Summit X: Papers, pages 79\u201386, Phuket, Thailand.\\n\\nPhilipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, Brooke Cowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ond\u0159ej Bojar, Alexandra Constantin, and Evan Herbst. 2007. Moses: Open source toolkit for statistical machine translation. In Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions, pages 177\u2013180, Prague, Czech Republic. Association for Computational Linguistics.\\n\\nShaohui Kuang and Deyi Xiong. 2018. Fusing recency into neural machine translation with an inter-sentence gate model. In Proceedings of the 27th International Conference on Computational Linguistics, pages 607\u2013617, Santa Fe, New Mexico, USA. Association for Computational Linguistics.\\n\\nTaku Kudo and John Richardson. 2018. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. In Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-943", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2023-main-943", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix A Implementation Details\\n\\nA.1 Training\\nWe train all models on the fairseq framework (Ott et al., 2019). Following Vaswani et al. (2017); Fernandes et al. (2021), we use the Adam optimizer with $\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.98$, dropout set to 0.3, an inverse square root learning rate scheduler with an initial value of $10^{-4}$, and the warm-up step set to 4000. We run inference on the validation set and save the checkpoint with the best BLEU score. We compute all BLEU scores using the sacreBLEU toolkit (Post, 2018). Wherever possible, we report the average and standard deviation across three randomly seeded runs.\\n\\nA.2 Models\\nTransformer\\nThe Transformer (Vaswani et al., 2017) is an encoder-decoder architecture that relies on a self-attention mechanism, in which every position of a single sequence relates to one another in order to compute a representation of that sequence. An $n$-length output sequence of $d$-dimensional representations $Y \\\\in \\\\mathbb{R}^{n \\\\times d}$ can be computed from an input sequence of $d$-dimensional representations $X \\\\in \\\\mathbb{R}^{n \\\\times d}$ as follows:\\n\\n$$Y = \\\\text{Attn}(X) = f(QK^T\\\\tau(X))V(3)$$\\n\\n$Q$, $K$, and $V$ are sequences of queries, keys, and values, respectively, with learnable weights and biases. Here, $f(\\\\cdot)$ is an attention function, most commonly set to softmax, and $\\\\tau$ is a correspondent scaling term. We use the Transformer base version across all experiments, which consists of 6 encoder layers, 6 decoder layers, a model dimension of 512, and an FFN hidden dimension of 2048.\\n\\nMEGA\\nThe recently introduced MEGA (Moving Average Equipped Gated Attention) (Ma et al., 2023) architecture solves for two limitations of the traditional Transformer, which have long since resulted in sub-optimal performance on long-sequence tasks: a weak inductive bias, and a quadratic computational complexity. This mechanism applies a multi-dimensional, damped exponential moving average (EMA) to $12$The sacreBLEU signature is BLEU+case.mixed+lang.src-tgt+numrefs.1+smooth.exp+{test-set}+tok.13a.\"}"}
{"id": "emnlp-2023-main-943", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a single-head gated attention, in order to preserve inductive biases. MEGA serves as a drop-in replacement for the Transformer attention mechanism, and full details can be found in (Ma et al., 2023). MEGA is of comparable size to the Transformer, with 6 encoder and 6 decoder layers, a model dimension of 512, and an FFN hidden dimension of 1024, alongside an additional shared representation dimension (128), value sequence dimension (1024), and EMA dimension (16).\\n\\nIn total, the Transformer architecture is around 65M parameters; the MEGA architecture is around 67M parameters.\\n\\nA.3 Data\\nFor the En\u2194Fr and En\u2194De language pairs, we train on the IWSLT17 (Cettolo et al., 2012) datasets, which contain document-level transcriptions and translations culled from TED talks. The test sets from 2011-2014 are used for validation, and the 2015 test set is held for inference. For Zh\u2192En, we use the BWB (Jiang et al., 2023) dataset, which consists of Chinese webnovels.\\n\\nData for each language pair is encoded and vectorized with byte-pair encoding (Sennrich et al., 2016) using the SentencePiece (Kudo and Richardson, 2018) framework. We use a 32K joint vocabulary size for Zh\u2192En, and a 20K vocabulary size for the other language pairs.\\n\\nFull corpus statistics are in Table 8.\\n\\n| Dataset Lg. Pair | Train | Valid | Test |\\n|-----------------|-------|-------|------|\\n| BWB Zh\u2192En       | 9576566 | 2632  | 2618 |\\n| WMT17 Zh\u2192En     | 25134743 | 2002  | 2001 |\\n| IWSLT17 En\u2194Fr   | 232825  | 5819  | 1210 |\\n| IWSLT17 En\u2194De   | 206112  | 5431  | 1080 |\\n\\nTable 8: Sentence counts across parallel datasets.\\n\\nA.4 Evaluation\\nAnnotated BWB test set. Some manually annotated paragraphs from the BWB test set can be found in Table 9, which is used in the discourse phenomena analysis.\\n\\nDiscourse marker categories. Following (Jiang et al., 2022), we categorize discourse markers into the following:\\n\\n- **Contrast**: but, while, however, although, though, yet, whereas, in contrast, by comparison, conversely\\n- **Cause**: so, thus, hence, as a result, therefore, thereby, accordingly, consequently, for this reason\\n- **Condition**: if, as long as, provided that, assuming that, given that\\n- **Conjunction**: also, in addition, moreover, additionally, besides, else, plus, furthermore\\n- **(A)synchronous**: when, after, then, before, until, after, once, after, next\\n\\nContrastive set examples. Contrastive evaluation examples for anaphoric pronoun resolution are in Table 10. Following standard practice, the model is evaluated in a discriminative manner: rather than generating translated sequences, the model is provided with the previous sentence as context, and is asked to choose the current sentence with the correct pronoun from the incorrect ones.\\n\\nB PARA2P PARA Translation\\n\\nB.1 Data and Preprocessing\\nWe gather the Chinese and English versions of six novels within the public domain, which are freely available online (Table 11). Prior to the tokenization step, we normalize punctuation and segment Chinese sentences using the open-sourced Jieba package. English sentences are tokenized using the Moses toolkit (Koehn et al., 2007). We employ byte-pair encoding (Sennrich et al., 2016) for subword tokenization.\\n\\nIn the open-domain setting, *A Tale of Two Cities* and *Twenty Thousand Leagues Under the Seas* are withheld as the test set.\\n\\nB.2 Translation Examples\\nTranslation examples on the PARA2P PARA dataset are in Figure 3.\\n\\nB.3 LLM Evaluations\\nLarge Language Models (LLMs) (e.g., ChatGPT (OpenAI, 2022)) have recently accrued a great deal of mainstream and scientific interest, as they are found to maintain considerable fluency, consistency, and coherency across multiple NLP tasks, including document-level NMT (Wang et al., 2023)(concurrent work). To investigate how LLMs would fare on the PARA2P PARA dataset, we also obtain translations using GPT-3.5 (*gpt-3.5-turbo*), a commercial, black-box LLM. Table 12 shows GPT-3.5's performance alongside that of the three...\"}"}
{"id": "emnlp-2023-main-943", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qiao Lian clenched her fists and lowered her head. Actually, he was right. She was indeed an idiot, as only an idiot would believe that they could find true love online. She curled her lips and took a deep breath. \\n\\nQiao Lian: \u201cWhat happened?\u201d\\n\\nSong Cheng was extremely nervous and followed him. Shen Liangchuan walked forward, one step at a time, until he reached the front of the room. Wang Wenhao was currently ingratiating himself with a C-list celebrity. The celebrity asked, \u201cHey, I heard that you beat a paparazzi?\u201d\\n\\n\u201cYes, the paparazzi nowadays are so disgusting. I have wanted to teach them a lesson myself for some time now!\u201d\\n\\n\u201cAre not you afraid of becoming an enemy of them?\u201d\\n\\nTable 9: Annotated paragraphs from the BWB test set.\\n\\n| Context Sentence | Current Sentence |\\n|------------------|------------------|\\n| src There were spring nights. Through open windows it came in, dancing. |\\n| tgt Es gab Fr\u00dchlingsn\u00e4chte. Bei offenen Fenstern tanzt es herein. |\\n\\nTable 10: Examples from the ContraPro and Bawden contrastive evaluation sets. Highlighted pronouns in the current sentence require the preceding context sentence for proper disambiguation.\\n\\nB.4 Pre-trained baseline performance\\n\\nTo investigate how fine-tuning on the PARA2PARA dataset affects the baselines' performance, we evaluate the pre-trained baselines on the same test set without any training on the PARA2PARA corpus.\\n\\nAs Table 13 illustrates, all three baselines exhibit significantly worse performance across the board (\u2717), and improve after fine-tuning (\u2713).\"}"}
{"id": "emnlp-2023-main-943", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Table 11: Corpus information for the PARA 2 dataset. APL = average paragraph length in tokens. |\\n|---|---|---|---|---|\\n| Domain | Pre-training BLEU | BlonDe | COMET |\\n|---|---|---|---|\\n| Closed | | | |\\n| GPT-3.5 | 11.60 | 28.29 | 83.21 |\\n| XFMR Big | 16.00 | 35.36 | 79.16 |\\n| LIGHT CONV Big | 16.87 | 36.70 | 79.28 |\\n| MBART25 | 15.63 | 35.37 | 78.72 |\\n| Open | | | |\\n| GPT-3.5 | 11.90 | 27.86 | 86.02 |\\n| XFMR Big | 9.17 | 25.35 | 72.20 |\\n| LIGHT CONV Big | 8.60 | 25.48 | 72.50 |\\n| MBART25 | 7.97 | 22.41 | 72.24 |\\n\\n| Table 12: GPT-3.5 evaluations on the PARA 2 dataset. |\\n|---|---|---|---|---|---|---|\\n| Domain | Pre-training | Fine-tuning BLEU | BlonDe | COMET | |\\n|---|---|---|---|---|---|\\n| Closed | | | | | |\\n| XFMR Big | \u2717 | 7.30 | 25.42 | 67.41 |\\n| LIGHT CONV Big | \u2717 | 6.30 | 23.30 | 58.19 |\\n| MBART25 | \u2717 | 6.70 | 21.35 | 63.56 |\\n| Open | | | | | |\\n| XFMR Big | \u2717 | 4.70 | 21.22 | 59.81 |\\n| LIGHT CONV Big | \u2717 | 4.20 | 20.78 | 53.35 |\\n| MBART25 | \u2717 | 4.10 | 18.61 | 55.89 |\\n\\n| Table 13: Ablation study on the effect of fine-tuning on the Zh \u2192 En PARA 2 PARA dataset. \u2717 no fine-tuning; \u2713 denotes fine-tuning. Bold denotes best performance. |\\n|---|---|---|---|---|---|---|\\n| Domain | Pre-training | Fine-tuning BLEU | BlonDe | COMET | |\\n|---|---|---|---|---|---|\\n| Closed | | | | | |\\n| ONE | \u2713 | 1.37 | 0.06 | 8.44 | 0.39 | 47.28 | 2.18 | 18.73 | 6.49 |\\n| XFMR Big | \u2713 | 16.00 | 0.10 | 35.36 | 0.19 | 79.16 | 0.4 | 52.33 | 60.63 | 0.7339 |\\n| LIGHT CONV Big | \u2713 | 16.87 | 0.06 | 36.70 | 0.14 | 79.28 | 0.28 | 55.38 | 61.66 | 0.7409 |\\n| MBART25 | \u2713 | 15.63 | 0.25 | 35.37 | 0.20 | 78.72 | 0.27 | 54.04 | 60.59 | 0.7385 |\\n| Open | | | | | |\\n| ONE | \u2713 | 0.73 | 0.32 | 1.82 | 0.21 | 48.12 | 5.54 | 0.00 | 0.00 | 39.27 | 2.11 | 13.91 | 3.90 | 0.3587 | 0.02 |\\n| XFMR Big | \u2713 | 9.17 | 0.67 | 25.35 | 1.05 | 72.20 | 0.48 | 32.54 | 51.83 | 0.7003 |\\n| LIGHT CONV Big | \u2713 | 8.60 | 0.10 | 25.48 | 0.10 | 72.50 | 0.43 | 38.83 | 51.79 | 0.7027 |\\n| MBART25 | \u2713 | 7.97 | 0.06 | 22.41 | 0.71 | 72.24 | 0.61 | 20.07 | 50.52 | 0.7012 |\"}"}
{"id": "emnlp-2023-main-943", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I tried to think of an excuse. I knew he did not want to lunch with me. It was his form of courtesy. I should ruin his meal. I\\nMy face told me so clearly, but Captain Nemo didn\u2019t say anything. He asked me to follow him, just like a man who listens\\ndesperately to his own will. We went to the dining-room, and breakfast was already ready. \u201cProfessor Aronnax,\u201d the\\ncaptain told me, \u201cI ask you to eat well. Don\u2019t be rude. We dine and talk. Although I promise you to go for a walk in the\\nMy face told me this clearly, but Captain Nemo did not say, \u201cWhat, to ask me to follow him is like following him like a man\\nwho listens desperately to fate. We are in the dining room, and breakfast is there.\u201d Mr. Aronas, \u201csaid the captain, \u201cI\\nask you to have dinner, not to be invidious. We talk as we eat. I don\u2019t give you any\"}"}
