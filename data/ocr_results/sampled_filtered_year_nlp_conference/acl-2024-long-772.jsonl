{"id": "acl-2024-long-772", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Annotated Caption: Unleashing the Power: A Closer Look at McLaren's Latest Road-Going Marvel.\\n\\nGenerated Caption: First, we see a man standing in front of a blue car on display at a trade show. He is wearing a suit and is an older man. There is a person in the background with an umbrella. The man is wearing a black tie and the car is parked in front of a white wall. The car has a silver body and wheels, and the background is mostly white. Next, we see a man in a suit standing next to a silver blue car on display at a trade show. He is older and has a black tie. In the background, there are people with bags, and there is a white and yellow umbrella. The car has a red body and wheels, and there is a silver rim around the tires. The man is wearing a black suit, and the car is parked in front of a white wall. The background is mostly white. Then, we see a man in a suit standing next to a silver blue car at a trade show. He is an older man and has a black tie. In the background, there are people with bags, and there is a white and yellow umbrella. The car has a red body and wheels, and there is a silver rim around the tires. The man is wearing a black suit, and the car is parked in front of a white wall. The background is mostly white. Finally,\\n\\nFigure 23: Pathological example caption data generated by Video-LLaMA2-Instruct for video clip take from YouTube video _6st0uhrcBZU.\\n\\nAnnotated Caption: This video showcases a modded version of GTA San Andreas.\\n\\nGenerated Caption: First, a black and white picture of a car on a street is shown. Next, a black and white image of a city at night is displayed, with a car parked on the side of the road and a black and white sign that reads, \\\"stop\\\" in the background. Then, a black and white picture of a car on a street is shown again, followed by a black and white photo of a city at night with a car parked on the side of the road and a black and white sign that reads, \\\"stop\\\" in the background. A man wearing a black and white shirt and black and white shoes walks down a street, followed by a black and white picture of a city at night with a car parked on the side of the road and a black and white sign that reads, \\\"stop\\\" in the background. A man wearing a black and white shirt and black and white shoes walks down a street, followed by a black and white picture of a city at night with a car parked on the side of the road and a black and white sign that reads, \\\"stop\\\" in the background. The video then shows a black and white picture of a city at night with a car parked on the side of the road and a black and white sign that reads, \\\"stop\\\" in the background, followed by a black and white image of a city at night with a car parked on the side of the road. Figure 24: Pathological example caption data generated by Video-LLaMA2-Instruct for video clip take from YouTube video eoIJuYUA0UI.\"}"}
{"id": "acl-2024-long-772", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DeV An: Dense Video Annotation for Video-Language Models\\n\\nTingkai Liu1\u2217, Yunzhe Tao1, Haogeng Liu2,3, Qihang Fan2,3, Ding Zhou1, Huaibo Huang2, Ran He2, Hongxia Yang1\\n\\n1ByteDance, Inc.\\n2MAIS & CRIPAC, Institute of Automation, Chinese Academy of Sciences, China\\n3School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\\n\\nAbstract\\n\\nWe present a novel human annotated dataset for evaluating the ability for visual-language models to generate both short and long descriptions for real-world video clips, termed DeV An (Dense Video Annotation). The dataset contains 8.5K YouTube video clips of 20-60 seconds in duration and covers a wide range of topics and interests. Each video clip is independently annotated by 5 human annotators, producing both captions (1 sentence) and summaries (3-10 sentences). Given any video selected from the dataset and its corresponding ASR information, we evaluate visual-language models on either caption or summary generation that is grounded in both the visual and auditory content of the video. Additionally, models are also evaluated on caption- and summary-based retrieval tasks, where the summary-based retrieval task requires the identification of a target video given excerpts of a given summary. Given the novel nature of the paragraph-length video summarization task, we compared different existing evaluation metrics and their alignment with human preferences and found that model-based evaluation metrics provide more semantically-oriented and human-aligned evaluation. Finally, we benchmarked a wide range of current video-language models on DeV An, and we aim for DeV An to serve as a useful evaluation set in the age of large language models and complex multi-modal tasks. Code is available at https://github.com/TK-21st/DeVAn.\\n\\n1 Introduction\\n\\nWith billions of active users on video content platforms such as YouTube and TikTok, there has been an unprecedented need for automated complex video understanding. Classically, video understanding has focused on captioning and/or retrieval tasks on short videos with brief (sentence-long) captions. The concise nature of both the videos selected and captions labeled has partly been the result of model limitations, where detailed and nuanced multi-sentence video descriptions have not been possible with lightweight text decoders. With the recent leaps in large language models (LLMs), however, vision-language models (VLMs) now have the opportunity to tap into the immense natural language capabilities of models such as LLaMA (Touvron et al., 2023a,b) and ChatGPT (Ouyang et al., 2022; OpenAI, 2023). With tens to hundreds of billions of parameters, these LLMs are able to write entire essays with details and poise that mimic human to an unprecedented extent. With video conversational models such as ImageBind-LLM (Han et al., 2023), Video-LLaMA (Zhang et al., 2023), Video-ChatGPT (Maaz et al., 2023) and VideoChat (Li et al., 2023b) claiming to be able to generate detailed and fine-grained descriptions of video inputs, we believe the time is ripe for an evaluation benchmark that matches the capabilities of modern VLMs powered by LLMs.\\n\\nIn the current work, we focus on videos with multi-shot compositions containing diverse information streams such as dialogues, background music, and complex visual sequences. We developed DeV An, a novel task and dataset for dense long-form video descriptions. This new multi-modal dataset contains 8.5K video clips carefully selected from previously published YouTube-based video datasets (YouTube-8M (Abu-El-Haija et al., 2016) and YT-Temporal-1B (Zellers et al., 2022)) that integrate visual and auditory information. Over the span of 10 months, a team of 24 human annotators (college and graduate level students) created 5 short captions (1 sentence each) and 5 long summaries (3-10 sentences) for each video clip, resulting in a rich and comprehensive human-annotated dataset that serves as a robust ground truth for subsequent model training and evaluation (See Figure 1 for example).\\n\\nAs opposed to short video captions where N-\"}"}
{"id": "acl-2024-long-772", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kids' disappointment turns to joy as they receive rideable toys from their parents on Christmas Day.\\n\\nThis video records 2 kids and their Christmas presents. Their parents made a series of puzzles for them to find out the presents. First, they got 2 printed materials, and they needed to flip them over to get more information. Then the man with the camera gave them a hint to \\\"eat the cookies\\\", and following that they found their presents: 2 toy cars, one for the boy and one for the girl.\"}"}
{"id": "acl-2024-long-772", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"guistic components with additional trainable components, often a lightweight visual backbone. Effectively, such models take advantage of the natural language capabilities of LLMs by providing soft prompts encoded by a lightweight trainable multimodal adaptor. As LLMs are capable of both consuming and generating texts with hundreds if not thousands of words, models in this category are often capable to generate long and detailed video descriptions. We hope that our DeV An benchmark will contribute the continued advancements in these powerful video-language models.\\n\\nVideo-Language Datasets\\n\\nDatasets in this domain can be broadly categorized based on their domain specificity and downstream tasks. Refer to Table 1 for comparison. Under domain specificity, datasets like MSVD (Chen and Dolan), MSR-VTT (Xu et al., 2016), YouTube-8M (Abu-El-Haija et al., 2016), YT-Temporal-1B (Zellers et al., 2022), HD-Vila-100M (Xue et al., 2022) provide a panoramic view of diverse video content, fostering a comprehensive model understanding. In contrast, datasets such as How2 (Sanabria et al., 2018) YouCook2 (Zhou et al., 2017) and HowTo100M (Miech et al., 2019) predominantly focus on instructional content.\\n\\nIn terms of task orientation, open-domain datasets mentioned above are often focused on video-to-text generation and retrieval tasks. In contrast, datasets such as Kinetics-700 (Carreira et al., 2022), ActivityNet (Caba Heilbron et al., 2015) and ActivityNet Captions (Krishna et al., 2017) focus on more specialized downstream applications such as activity detection.\\n\\nAs our dataset is designed to primarily gauge the ability for models to accurately capture a balanced understanding of overall content and details in a given video, we focused on generation and retrieval tasks for open domain videos.\\n\\n3 DeV An Dataset\\n\\nIn this section, we describe the procedure with which DeV An was constructed and how generation and retrieval task performances are evaluated on DeV An.\\n\\n3.1 Evaluation Dataset\\n\\nThe dataset utilized in this study is an amalgamation of YouTube videos, which were sourced from two previously available large-scale video datasets: YouTube-8M and YT-Temporal-1B.\\n\\nThe selection of videos for human annotation was focused on English videos with high quality and diversity, and saw one significant evolution during the course of the annotation process. Refer to Figure 6 in Appendix A for examples of relevant metadata information used during the video selection process.\\n\\nFirst Phase: 2.3K Videos\\n\\nIn the first phase of the data curation process, videos are selected from YouTube-8M and YT-Temporal-1B datasets based solely video metadata with the following criteria:\\n\\n- Video title, description and subtitles (if applicable) must be primarily in English;\\n- Video must contain Chapter information, which is video keyframe information provided by video uploaders;\\n- Video clips, when segmented based on chapter information, should be between 20 to 60 seconds.\\n\\nWe find that of all videos in the YouTube-8M and YT-Temporal-1B datasets, roughly 1% satisfied our constraint. Based on the \\\"category\\\" metadata information of the videos, we uniformly sampled around 2.3K video segments were curated following this procedure, which form the first phase of our data annotation process. Note that the 100K training dataset mentioned later in this paper was curated in tandem with the first phase evaluation dataset, as such the distribution of our training dataset aligns best with this portion of the test set (see Section 3.4 for more information).\\n\\nSecond Phase: 6.2K Videos\\n\\nIn the second phase of the data annotation process, we adjust the criteria to favor videos for which visual-grounding is necessary for accurate annotation. In particular, previous selection criteria (most significantly, the requirement for \u201cChapter\u201d information) led to a bias towards News and Instruction type videos, for which speech information contents were dominant. As a result, human annotators often heavily favored, for example, the content of the News articles being broadcast over the actual visual setting of the broadcasting room. This has the undesired consequence that visual-language models with strong language capabilities but weaker visual grounding can potentially have better performance than the more visually-grounded counterparts. Additionally, to avoid videos that are montages of static images,\"}"}
{"id": "acl-2024-long-772", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Video-Language Datasets Comparison. Refer to Figure 7 in Appendix B for detailed information on DeV An.\\n\\nwe apply an additional frame-embedding based filter to select videos with high inter-frame variability. The video selection process is as follows.\\n\\n1. The audio content of each video is first processed by Whisper-Base (Radford et al., 2022) to generate automatic speech recognition (ASR) content, followed by an entropy computation, where only videos with entropy lower than 4.2 are kept.\\n\\n2. The visual content of each video is evaluated by uniformly sampling 8 frames and computing embedding of each frame using CLIP (Radford et al., 2021); $L_2$ distances between embeddings of neighboring frames are computed and averaged, where only videos with average inter-frame $L_2$ distance above 5.5 are kept.\\n\\n3. Video title, description and subtitles (if applicable) must be primarily in English.\\n\\n4. Instead of \u201cChapter\u201d information, videos are segmented using key-frames detected via TransNet (Soucek et al., 2019). Only segments that satisfy the 20-60 second duration requirement are kept.\\n\\nAs in the First Phase, the automatically filtered videos were again sampled uniformly based on the \u201ccategory\u201d metadata information. Selected video clips are filtered manually during the annotation process where annotators were provided the option to discard a given video if it is deemed of poor quality: non-English or does not contain sufficient information content for summarization in 3-10 sentences. We find that roughly 20% of automatically selected videos were filtered by human annotators.\\n\\nThe 10-months-long annotation process is divided into multiple rounds, with each round covering 500-1500 videos. After each round of annotation, 20% of videos are randomly selected for quality control independent of the original annotators. If systematic problems are detected in a batch, the entire batch is returned to annotators for revision before going through another round of quality control. In later rounds, as quality of annotation stabilized, the percentage of videos selected for independent quality control is adjusted downwards to a minimum of 7.5%. This process is repeated until the batch at question is deemed of satisfactory quality, and every batch went through at least one round of revision. Refer to Appendix F for more details on the annotation process.\\n\\nAs shown in Figure 2, the final 8.5K evaluation dataset contains videos covering a wide range of topics and interests. The statistics of the videos and annotations are shown in Table 1 and Figure 7 in Appendix B. For more qualitative examples of the dataset, refer to Appendix H.\"}"}
{"id": "acl-2024-long-772", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"N-gram-based metrics, following prior works such as MSR-VTT (Xu et al., 2016), we report commonly used metrics including BLEU (Papineni et al., 2001), ROUGE-L (Lin, 2004) and CIDEr (Vedantam et al., 2015) to gauge the quality of the model-generated captions. Evaluation of these metrics follow the implementation used in CLIP-Score (Hessel et al., 2021), where the Stanford CoreNLP's PTBTokenizer (Manning et al., 2014) is used for text pre-processing. These metrics have proven effective for evaluating the lexical overlap and syntactic structure in brief captions, which are relatively straightforward and independent of language models. In addition to N-gram-based metrics, we also report model-based metric, BLEURT (Sellam et al., 2020), which we found to have a better agreement to human preferences (see Section 3.3 for more details) especially for long-form video summarization tasks. As such, while we report a wide range of evaluation metrics, the model-based BLEURT metric serves as our primary method of evaluation for video-to-text generation tasks.\\n\\nText-to-Video Retrieval Task\\nFor evaluating the efficacy of models in the video retrieval tasks, we follow the classic retrieval accuracy metrics at different levels of granularity: Recall @1, @5, and @10. While this is sufficient for standard text-to-video retrieval tasks using one-sentence video descriptions, it is not directly applicable for retrieval via multi-sentence video summaries. For such task, we introduce a new evaluation methodology where the recall is evaluated using individual sentences from a given video summary. The overall summary-to-video retrieval performance is the averaged recall from each sampled sentences. Note that only sentences with more than 5 words were used to ensure that excerpts of video summaries contain sufficient information for retrieval. This task mimics the common scenario where viewers may desire to search for videos based on memories of partial information. We report Recall @1, @5 and @10 for both caption-to-video and summary-to-video retrieval tasks.\\n\\n3.3 Alignment of Evaluation Metrics to Human Preference\\nGiven the novel nature of long-form video summarization task, we sought to compare the alignment of different evaluation metrics to human preferences.\\n\\nTo start, we computed the Spearman Rank Correlation between different evaluation metrics on both captioning and summarization tasks as shown in Figure 8. Note that Spearman Rank Correlation was chosen over Pearson Correlation to emphasize pairwise consistency between evaluation metrics. Using one annotated response as prediction and all other responses as ground truths, we computed N-gram based (BLEU-4, ROUGE-L, CIDEr) and model-based (BLEURT) metrics for both captioning and summarization tasks. We observe that across all annotators and all videos, the correlation between N-gram-based and model-based metrics is 0.65 for captioning task, while the minimum correlation within N-gram metrics is 0.73. This result suggests that N-gram based metrics may offer more consistent evaluation for captioning task. In contrast, N-gram metric such as CIDEr appear poorly correlated with other metrics for long-form video summarization tasks.\\n\\nTo determine the most suitable metric for evaluating long-form summarization task, we intuited that the alignment of a metric to human preferences can be measured by the metric's ability to tightly cluster annotations of the same video created by different labelers. Formulating this intuition as a text-to-text retrieval problem, we compared recall performances of identifying the same annotations from different annotators using different evaluation metrics, and found that while CIDEr and BLEURT have similar recall performances for video captions, BLEURT significantly outperforms all N-gram based metrics for video summaries by over 14% (see Table 5 in Appendix C). This provides indirect support that the model-based BLEURT metric may be better aligned to human preferences especially for long-form video summaries. To further validate this result, we randomly selected 20 videos and manually ranked summaries from annotators 1 and 5 by their perceived quality and relatedness to the video content. We than compared this human labeled ranking result to the ranking created by aforementioned metrics, computed using label 1/5 as predictions and label 2/3/4 as ground truths. We observe a 56% alignment of human ranking to ranking by CIDEr score and a 67% alignment to that by BLEURT, reinforcing the previous finding that BLEURT is better aligned to human preferences.\\n\\nIt is worth noting that while the difference in human alignment between BLEURT and CIDEr...\"}"}
{"id": "acl-2024-long-772", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our training dataset contains captions and summaries for 100K ASR-rich video segments. Note that as opposed to test dataset in Figure 2, the ASR Number of Words normalized by video duration does not have a significant concentration around 0, indicating that all videos in the training dataset contains a significant amount of ASR information.\\n\\n3.4 Training Dataset\\nIn addition to the 8.5K evaluation dataset, we also prepared a training dataset with 100K video clips whose captions and summaries were generated using metadata information (e.g., title, description, category, ASR, etc.). The training dataset is comprised of 100,000 video segments, selected via the same procedure as in the first phase of evaluation dataset curation (see Section 3.1). Note due to time constraint, we were not able to collect another training set based on the selection criteria in the second phase of evaluation dataset development. Consequently, the training dataset is skewed towards ASR-rich videos (see Figure 3).\\n\\nSimilar to videos in the evaluation dataset, videos in the training dataset were segmented based on the video chapter information. A prompt template was then used to create queries for an LLM to generate 5 captions and summaries for each video (see Appendix D). To exercise control over the text generation process and maintain consistency, each prompt was prefixed with \\\"This video\\\". At the time of dataset creation, the gpt-3.5-turbo (Ouyang et al., 2022) model was observed to have significantly more issues related to hallucinations as compared to text-davinci-003 (Brown et al., 2020), thereby motivating the choice of the seemingly less powerful but more \\\"reliable\\\" text-davinci-003 for our specific requirements.\\n\\nIt is important to note that the main focus of the current work is on the zero-shot performances of current models on our DeV An evaluation dataset, and, as later discussed in Section 4.3, only the end-to-end VideoCoCa model was fine-tuned on the DeV An training dataset. This is due to the fact that our VideoCoCa model was initialized from the CoCa-ViT-L-14 ckpt (see Section 4.3 and Appendix E for more details), which was originally trained on COCO-styled short captions. Direct inference of such model for video summarization tasks resulted in catastrophically poor performance as it lacks the capability to be prompted via natural language to generate video descriptions of different lengths (see Appendix G). However, as the DeV An training dataset was created using only the metadata information of the videos, the training dataset differs significantly from the evaluation dataset in terms of category, audio content, description length, and vocabulary, thereby constituting a zero-shot evaluation of the VideoCoCa model.\\n\\n4 Experiments\\nIn this section, we describe both human performance and model performance on the DeV An evaluation set. To comprehensively evaluate the capabilities and limitations of various architectures for video captioning and retrieval, we consider two distinct types of models, each with its own set of advantages and drawbacks.\\n\\n4.1 Human Performance\\nTo establish a human performance benchmark, we use a strategy similar to that described in Section 3.3 where one human annotation is used as the \\\"prediction\\\", while the remaining four annotations are used as \\\"references\\\". The aggregated results across annotators for both captioning and retrieval are as follows:...\"}"}
{"id": "acl-2024-long-772", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Results for generation and retrieval tasks of DeV An evaluation dataset. For evaluation of human performance, annotation from each annotator is used as prediction and computed against ground truth results from all other 4 annotators. The overall metrics are then aggregated via Average and Minimum. Note that only results for VideoCoCa models are shown for retrieval tasks, since other VLMs tested in the current work do not support retrieval.\\n\\n| Task Comparison Metric | Metric across Annotators | Metric across Time |\\n|------------------------|--------------------------|--------------------|\\n| Caption                 | B4: BLEU-4, R: Rouge-L, C: CIDEr, B-RT: BLEURT | B4: BLEU-4, R: Rouge-L, C: CIDEr, B-RT: BLEURT |\\n| Summary                 | B4: BLEU-4, R: Rouge-L, C: CIDEr, B-RT: BLEURT | B4: BLEU-4, R: Rouge-L, C: CIDEr, B-RT: BLEURT |\\n\\nTable 3: Same annotator re-labeling summaries of the same video twice is equivalent to the same video summary labeled by different annotators. 150 randomly selected captions and summaries were relabeled by the same annotator at least 1 month apart from the original annotation. We observe that annotations by the same annotator 1 month apart have similar levels of discrepancy as compared to annotations by different annotators.\\n\\n4.2 Trainable Visual Encoder with Adaptor on Frozen LLMs\\n\\nRecently, many methods combining visual encoders with frozen LLMs have emerged. Pioneered by models including BLIP-2, recent additions to this category of models include ImageBind-LLM, Video-LLaMA2 and VideoChatGPT, which all involve projecting video encoding (visual and optionally auditory information) into soft tokens that serve as prefix for frozen LLMs. We performed zero-shot instruction-based evaluations of these models with default system prompts from demos in their corresponding repositories, and using instructions \\\"Describe this video in ONE sentence. \\\" for captioning task and \\\"Summarize this video in three to ten sentences. \\\" for summarization task.\\n\\nSurprisingly, as shown in Table 2, we found that models like Video-LLaMA2 have very poor video-to-text generation performances for both captioning and summarization when evaluated using N-gram-based metrics. Upon closer examination of the generated results, we realize that due to poor instruction-following (or a lack of prompt engineering), models with frozen LLMs generated responses with highly variable length as shown in Figure 4. Pathological examples of such cases with close to 0 CIDEr but high BLEURT scores are...\"}"}
{"id": "acl-2024-long-772", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Distribution of Number of Words in Captions and Summaries. Note that for legibility, only distributions for models without audio signals were shown. However, we found that the distributions of caption and summary lengths do not vary significantly with the introduction of audio signals.\\n\\nAs such, while the results generated are semantically related to the video and human annotations (as found both via human evaluation and BLEURT), they have very low performances in N-gram-based evaluations.\\n\\n4.3 End-to-End Foundation Model\\n\\nDespite recent success of VLMs with frozen LLMs, they often cannot support retrieval tasks due to the lack of a dedicated language encoder. To provide a baseline model that is able to achieve competitive results in both video-to-text generation and text-to-video retrieval tasks, we developed an end-to-end trainable foundation model based on the VideoCoCa architecture (Yan et al., 2023) that includes both a visual encoder and ASR encoder (see Figure 5 for model architecture). As no open-source VideoCoCa implementation was available, we created an implementation from scratch following the Attention Pooler type model described in the VideoCoca manuscript.\\n\\nFollowing CoCa (Yu et al., 2022) and Video-CoCa, the text encoder takes in Caption or Summary as input where a special [CLS] token is prefixed to all input sequences. The text encoder is evenly divided into unimodal (bottom) and multimodal parts, where the output encoding of the [CLS] token by the unimodal encoder is used to compute contrastive training objective against other modalities.\\n\\nOn the visual encoder side, 8 frames are uniformly sampled from each video and encoded in FFN and Causal Self-Attention layers. The output visual encoding of each frame is concatenated to form the overall representation of the visual information in an input video. The visual encoder output is further concatenated with BERT encoding of the ASR information. The encodings are then integrated and compressed by the \u201cAttention Pooler\u201d module with 256 query tokens. The first output token of Attention Pooler is used to compute contrastive loss against encoding of [CLS] token mentioned above.\\n\\nFollowing CoCa, the training objective is a combination of generation loss and contrastive loss. All parameters are initialized from the OpenCLIP (Ilharco et al., 2021) implementation of CoCa, except for the ASR encoder which was initialized from BERT-base (Devlin et al., 2019). Results of the video-to-text generation and text-to-video retrieval on DeV An can be found in Table 2. We note that the integration of audio information via ASR significantly improves both retrieval and generation performances of our model. This is consistent with the observation that roughly 25% of the evaluation dataset contains videos rich in speech content.\\n\\n5 Conclusion\\n\\nIn this paper, we introduce a new multi-modal dataset curated from a diverse range of YouTube videos, designed to gauge the capabilities of visual-language models in long-form video summarization tasks. Through a carefully orchestrated annotation process involving multiple human annotators, multiple rounds of video selection and quality control, we ensure that the dataset is comprised of...\"}"}
{"id": "acl-2024-long-772", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We show that while qualities of one-sentence captions can be accurately evaluated by N-gram, multi-sentence summaries require a more semantically aligned metric such as BLEURT. Using such metrics, we provide an extensive benchmarking of the current state-of-the-art video-to-text generation models. However, current visual-language models with frozen LLMs often do not support retrieval tasks due to lack of language encoders. To provide a baseline for text-to-video retrieval tasks, we finetuned VideoCoCa using our automatically generated training dataset, which achieved reasonable retrieval, but comparatively weaker generation performances.\\n\\nAs the field of video summarization and captioning continues to evolve, it is imperative that datasets and evaluation metrics keep pace. Our work aims to serve as a stepping stone in this direction, providing a balanced approach to video understanding.\\n\\n6 Limitations\\nWhile DeV An strives to become a comprehensive and objective evaluation benchmark for video-to-text generation and text-to-video retrieval tasks, certain limitations warrant consideration.\\n\\n- A limitation of the current work both in the creation of training dataset and in the benchmarking of existing models is that of prompt engineering. It has long been observed that LLMs can be highly sensitive to prompt design (Liu et al., 2021), which may contribute to the high variability in the length of the generated responses. While we tried to mitigate this problem by staying close to the default instructions in used in the corresponding works, we still observed problems of poor instruction following.\\n\\n- Additionally, despite an effort to compare different available metrics, neither CIDEr nor BLEURT achieved higher than 80% alignment to human preferences. While it is feasible to pose the problem in a multiple choice format to mitigate this problem, this would usually require finetuning models to following MCQ instructions. Alternatively, a Reward Model for video annotations may be trained based on human preferences (similar to finetuning BLEURT). However, this would require significantly more human annotations which was unfortunately infeasible. Due to time and resource constraints, these limitations were not addressed in the current work. However, we do expect performance gains of the evaluated models by addressing these limitations.\\n\\n7 Ethics Statement\\nThis research aims to provide an objective and comprehensive benchmark for video-to-text generation and text-to-video retrieval tasks from open domain videos. Although our research does not involve human subjects directly, it is important to acknowledge and discuss the broader ethical implications.\\n\\nData Bias and Fairness: Both training and testing videos in DeV An are selected from YouTube-8M and Youtube-Temporal-1B datasets, and follow their data curation and anonymization practices. While these datasets are widely used, we acknowledge that we cannot fully ascertain the extent to which they may contain discriminatory, biased, or sensitive material. Given that our finetuned VideoCoCa model inherits the biases present in our training datasets, there exists the risk of perpetuating or even amplifying existing societal biases. Despite the broad acceptance of these datasets, caution should be exercised.\\n\\nResponsible Usage: Like all open domain video datasets, it is imperative to implement safety mechanisms to ensure that models trained or evaluated on our dataset do not inadvertently produce outputs that could disclose sensitive or personal information.\\n\\nAI Assistant Usage: ChatGPT was used for grammatical correction in the current manuscript.\\n\\nReferences\\nSami Abu-El-Haija, Nisarg Kothari, Joonseok Lee, Paul Natsev, George Toderici, Balakrishnan Varadarajan, and Sudheendra Vijayanarasimhan. 2016. YouTube-8M: A Large-Scale Video Classification Benchmark. ArXiv:1609.08675 [cs].\\n\\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are few-shot learners.\"}"}
{"id": "acl-2024-long-772", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-772", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-772", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A YouTube Metadata information\\n\\nYouTube metadata information refers to all information related to an uploaded video except for the video and audio content. This includes information such as title, description, category, tags, asr, chapter, playlist, subtitles, etc. In figure 6, we highlight a few important metadata information that is relevant for video selection in our work.\\n\\nWe highlight \\\"Chapter\\\" information, which corresponds to keyframe information with human annotated segment subtitles. These chapter information naturally divide video into semantically meaningful chunks that we used to split longer videos into 20-60 second clips.\\n\\nB Detailed Statistics of DeV An Evaluation Dataset\\n\\nHere we provide detailed statistics on the evaluation dataset. In particular, in Figure 7, second column from left, we show that the ASR content, when normalized by duration of video, demonstrates a clear bimodal distribution, corresponding to videos with high and low ASR content selected during the two phases of annotation process. Additionally, we observe that a significant portion of video summaries are longer than 100 words, which is considerably longer than video annotations in previously available datasets.\\n\\nFigure 7: Distribution of video duration, ASR content, length of captions and length of summaries of DeV An dataset.\\n\\nThe Length of ASR normalized by duration of video clip indicates that our dataset covers videos ranging from no speech content to high speech content. Note that number of words are calculated by counting the number of white-space-separated character groups, and may contain punctuation.\\n\\nC Detailed Generation Metrics for each Annotator\\n\\nHere we provide metrics comparing each annotator's caption & summaries to all other annotators. Each row in Table 4 correspond to metric evaluated using the given annotator's caption/summary as prediction and all others' as ground truth. We computed both Average and Minimum values for each metric for reference, where the minimum value indicates the lower-bound of human performance.\\n\\n| Annotator | B4 R C B-RT B4 R C B-RT B4 R C B-RT | Avg | Min |\\n|-----------|-------------------------------------|-----|-----|\\n| 1         | 6.5 33.2 55.3 51.4 17.3 35.8 40.0 56.4 | 6.3 | 4.5 |\\n| 2         | 6.7 33.5 57.1 51.4 16.7 35.5 37.1 56.5 | 6.7 | 4.5 |\\n| 3         | 7.3 33.0 57.0 50.9 16.8 35.3 41.0 56.4 | 7.3 | 4.5 |\\n| 4         | 6.7 31.3 52.9 50.2 15.1 33.8 35.7 55.1 | 6.7 | 4.5 |\\n| 5         | 4.5 29.5 47.1 48.6 12.4 32.1 30.9 53.6 | 4.5 | 4.5 |\\n\\nTable 4: Human Performance of Video-to-Text Generation Task.\\n\\nAnnotation from each annotator is used as Prediction and computed against Ground Truth results from all other 4 annotators. The overall metrics are then aggregated via Average and Minimum.\\n\\nTo contrast the similarities between annotations of the same video across annotators to that between different annotations, we performed a text-to-text retrieval task. The motivation behind this experiment is that a good evaluation metric should produce high similarity between annotations created by different human labelers for the same video, thereby resulting in a high text-to-text retrieval performance. Guided by this intuition, we randomly selected 100 videos, and computed pairwise metrics between annotator 1's caption/summary to another annotator's caption/summary.\"}"}
{"id": "acl-2024-long-772", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"notations from all other annotators. We then perform a text-to-text retrieval task using each metric and report recall@1,5,10. As shown in Table 5, for the captioning task, CIDEr and BLEURT give similar recall performance. However, for the summarization task, BLEURT metric provides much higher recall than all other N-gram based metrics, providing an indirect support for the claim that the BLEURT metric is better aligned to human preferences.\\n\\nTable 5: Comparison of text-to-text retrieval performances across metrics.\\n\\n|          | R@1  | R@5  | R@10 |\\n|----------|------|------|------|\\n| BLEU-4   | 45%  | 66%  | 71%  |\\n| ROUGE-L  | 37%  | 59%  | 66%  |\\n| CIDEr    | 65%  | 80%  | 87%  |\\n| BLEURT   | 61%  | 87%  | 90%  |\\n\\nWe also computed Spearman Rank Correlation of all annotations from all annotators across evaluation metrics. As shown in Figure 8. Note that Spearman Rank Correlation was chosen to emphasize rank consistency between metrics and loosens the linearity assumption implicit in Pearson Correlation.\\n\\nSpearman Rank Correlation\\n\\n|          | BLEU-4 | ROUGE | CIDER | BLEURT |\\n|----------|--------|-------|-------|--------|\\n| BLEU-4   | 1      | 0.79  | 0.73  | 0.56   |\\n| ROUGE    | 0.79   | 1     | 0.78  | 0.63   |\\n| CIDER    | 0.73   | 0.78  | 1     | 0.65   |\\n| BLEURT   | 0.56   | 0.63  | 0.65  | 1      |\\n\\nFigure 8: Spearman Rank Correlation between evaluation metrics of human annotators. Spearman Rank Correlation is evaluated between all human performance values for each video clip across annotators. Spearman Rank Correlation is chosen over Pearson Correlation to emphasize the ranking consistency using different evaluation metrics.\\n\\nD Prompt of Training Data Generation\\n\\nThe following prompt template is used for generating video summaries in training set. Here cc refers to the video subtitles.\\n\\n```\\nsummary_template = \"\"\"Please write a summary in 3 to 10 sentences that accurately summarizes the video's content and captures its essence based on the following information.\\nTitle: {{ title }}|{{ chapter_title }}\\nCategory: {{ category }}\\nDescription: {{ description }}\\nClosed Captions: {{ cc }}\\nSUMMARY: This video\"\"\"\n```\\n\\nThe following prompt template is used for generation video captions in training set. Here cc refers to the video subtitles.\\n\\n```\\ncaption_template = \"\"\"Please write a 1-sentence caption that accurately summarizes the video's content and captures its essence based on the following information.\\nTitle: {{ title }}|{{ chapter_title }}\\nCategory: {{ category }}\\nDescription: {{ description }}\\nClosed Captions: {{ cc }}\\nONE-SENTENCE CAPTION: This video\"\"\"\n```\\n\\nE VideoCoCa Training Details\\n\\nVideoCoCa models are trained using 64 V100-32G GPUs with global batchsize of 256 (3 per card). Training 100K dataset for 5 epochs consumes roughly 5 hours.\\n\\nLearning rate of VideoCoCa follows Linear Warmup with Cosine decay pattern, with warmup LR of 1e-7 (500 warmup steps), peak LR of 1e-5 and a minimum LR of 1e-6.\\n\\nBoth VideoCoCa and VideoCoCa w/o ASR models are trained with 8 uniformly sampled input video frames for 5 epochs.\\n\\nF Details of Human Annotation\\n\\nAnnotation of DeV An occurred over a 10-months period, divided into multiple rounds, with each round covering 500-1500 videos. Prior to annotation, annotators were recruited and evaluated based on their performances on 200 held out videos. All accounted for, 24 human annotators (college and graduate level students) were recruited and performed their tasks on an online platform as shown in Figure 9. Annotators are required to fill in one caption and one summary for each video, or alternatively mark the video as invalid if it resembles a slideshow recording, is not in English, or does not...\"}"}
{"id": "acl-2024-long-772", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"have sufficient visual information to support a 3-10 sentence video description.\\n\\nAfter each round of annotation, 20% of videos are randomly selected for quality control independent of the original annotators. If systematic problems are detected in a batch of annotations, the entire batch is returned to annotators for revision before going through another round of quality control. In later rounds, as quality of annotation stabilized, the percentage of videos selected for independent quality control is adjusted downwards to a minimum of 7.5%. This process is repeated until the batch at question is deemed of satisfactory quality, and every batch went through at least one round of revision.\\n\\nFigure 9: Screenshot of annotation platform.\\n\\nFigure 9: Screenshot of annotation platform.\\n\\nG Performance of VideoCoCa model without finetuning\\n\\nIn Table 6, we show the performance of VideoCoCa model (CoCa-ViT-L-14) evaluated on the DeV An summarization task. Due to the lack of instruction following capabilities of VideoCoCa, it had a catastrophically low performance on the out-of-domain video summarization task without finetuning on video summarization dataset.\\n\\n| Architecture          | Training | BLEU-4 | ROUGE | CIDEr | BLEURT |\\n|-----------------------|----------|--------|-------|-------|--------|\\n| VideoCoCa DeV An Training Set | 2.9 | 16.4 | 3.3 | 23.9 |\\n| VideoCoCa No Training  | 7.5 | 7.5 | 0.2 | 11.0 |\\n\\nTable 6: Video summarization performances of VideoCoCa with and without training on DeV An training set.\\n\\nH Examples from DeV An Dataset\\n\\nIn this section, we provide more qualitative examples from DeV An dataset. Note that though each video has correspondingly 5 captions and summaries, only one caption and one summary is shown for brevity.\\n\\nVideo clip from zheszz4PLUw\\n\\nASR: Hey there, this is Derek Mithog with Voltaire Cycles Franchises and today I... takes away all of the guesswork and hassle by delivering the bike or trike fully assembled to the customer's door.\\n\\nCaption: Introducing Voltaire Cycles' hassle-free bike and trike delivery service.\\n\\nSummary: In this video, a man from Voltaire Cycles franchises is introducing their door-to-door, fully assembled delivery service for bikes and trikes. Many customers are hesitant to purchase a bike or trike online due to the assembly process involved. Most sites claim that the bike comes mostly assembled, but that still means a lot of work for the customer. Voltaire Cycles' delivery service takes away all of the guesswork and hassle by delivering the bike or trike fully assembled to the customer's door.\\n\\nFigure 10: Example DeV An data annotated from YouTube video zheszz4PLUw.\\n\\nVideo clip from -da7ZrCiupo\\n\\nASR: None.\\n\\nCaption: Videos from MTV that were popular in 2008 and 2009.\\n\\nSummary: In this video, a mobile phone is playing scenes from music videos that were popular on MTV in the years 2008 and 2009. This gives a nostalgic vibe, as those years were around the peak of MTV's music video era. The silver color of the phone is a characteristic detail, and it's interesting how technology has evolved since then.\\n\\nFigure 11: Example DeV An data annotated from YouTube video -da7ZrCiupo.\\n\\nI Pathological Examples Captions Generated from Video-Language Models\\n\\nWe noted that models like Video-LLaMA2-Instruct had very low CIDEr scores but high BLEURT scores. In this section, we show some pathological examples where model generated responses show close to 0 CIDEr score but very high BLEURT scores.\"}"}
{"id": "acl-2024-long-772", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Video clip from 0Cmr0lSWU44\\nASR: 47, empty cars. 40, dogs on a leash. 35, unserer Girlfriend is probably trying to... \\nCaption: A grey vintage car is showcased.\\nSummary: A man takes the wheel and showcases the car in its entirety. A scene unfolds in an underground garage, with a silver-colored sedan approaching slowly. The sedan features black front covers, headlights, and an exquisite exterior design.\\nClose-up shots accentuate the front, sides, and rear of the vehicle. As a final touch, smoke billows from the exhaust.\\n\\nFigure 12: Example DeV An data annotated from YouTube video 0Cmr0lSWU44.\\n\\nVideo clip from 6CLbKXgWIvU\\nASR: None.\\nCaption: People are dancing at a nightclub.\\nSummary: People are dancing and reveling at a nightclub. Women are dancing on stage, while people below either wave their hands to the music or dance along. The stage is illuminated with flashing lights, with a DJ spinning records. Everyone joyfully moves their bodies in rhythm with the music. The stage lights change colors. The poster of the last person appears.\\n\\nFigure 13: Example DeV An data annotated from YouTube video 6CLbKXgWIvU.\\n\\nVideo clip from 38jPK8lRlb0\\nASR: None.\\nCaption: Restarting a Samsung phone.\\nSummary: In this video, a hand is shown holding a Samsung cellphone. It takes a while to show the home screen. After several minutes, the phone finally displays the home screen wallpaper.\\n\\nFigure 14: Example DeV An data annotated from YouTube video 38jPK8lRlb0.\\n\\nVideo clip from cb_HvxX80sE\\nASR: What amplified the protests? the F.C.C. and the Senate. The remains of the city are... \\nCaption: Discover the Seaport Workboat: Remote-controlled sailing fun in the pool!\\nSummary: The video showcases images of a Seaport Workboat, a remote control boat, as it sails in a pool. The boat's design and features are on display, demonstrating its ability to navigate through water with ease. The visuals provide a clear insight into the boat's appearance and performance, offering viewers a glimpse of its functionality.\\n\\nFigure 15: Example DeV An data annotated from YouTube video cb_HvxX80sE.\\n\\nVideo clip from HVmx56OSjDE\\nASR: None.\\nCaption: A float parade is held at an amusement park.\\nSummary: This is a video of a float parade in an amusement park. It starts with a festooned vehicle with some actors called \\\"Dream Seekers\\\" next to it. We can see an actress wearing a red tiara with purple feathers behind her. The float looks like a fountain, and then all kinds of floats appear. The second float looks like a giant clock.\\n\\nFigure 16: Example DeV An data annotated from YouTube video HVmx56OSjDE.\\n\\nVideo clip from Hvo6APy2f5A\\nASR: You get used to the rhythm. See? It's kinda nice. May I present to you... the... \\nCaption: Innovative cat bell offers a solution for feline needs, but training manual not included.\\nSummary: In this video, the speaker introduces the \\\"cat bell\\\" as a solution to address a cat's needs such as hunger, thirst, or attention. The idea is to train the cat to hit the bell, but the product does not include training instructions. The speaker suggests that interested buyers can search for training methods online, but the product being sold is essentially just a bell.\\n\\nFigure 17: Example DeV An data annotated from YouTube video Hvo6APy2f5A.\"}"}
{"id": "acl-2024-long-772", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Under the radar sports media, the hottest sports videos in the nation. In the video, athletes are fighting on the field. The video begins with the logo of a sports report and scores. Many wonderful moments are shown in the video.\\n\\nWalleye population hasn't always been this abundant. Cormorants, a fish-eating bird species, have caused a decrease in the Walleye population of Oneida Lake. Approximately 20 years ago, there were only around 200,000 adult Walleye in the lake. During the late 1980s and early 1990s, the fishery suffered a significant decline due to the impact of cormorants. However, there was uncertainty about whether the Walleye population would recover to reach one million.\\n\\nThis is your Shipmate Action Figure Therapy's Drunk Sailor, taking a moment to rant about the Navy. He first wished the football team good luck in the coming games. After that, he listed a few reasons why Navy is better and Navy team will win the game against Army. He made fun of the Army over their losing 9 games in a row, and he made some sexual jokes on them.\\n\\nRandom videos of cats doing various activities were the story of the video. First, we see a little brown puppy sitting on a plate on the floor with food in its bowl. Next, we see a black puppy with a white dot on its face sitting on a table with food in its bowl. Then, we see a black and white puppy sitting on a plate on the floor with food in its bowl. The camera then cuts to a brown puppy sitting on a plate on the floor with food in its bowl. In the next frame, we see a black puppy with a white dot on its face sitting on a table with food in its bowl. After that, we see a brown puppy sitting on a table with food in its bowl. The following frame shows a black and white puppy sitting on a plate on the floor with food in its bowl. The camera then cuts to a brown puppy with a white dot on its face sitting on a table with food in its bowl. Finally, we see a little brown puppy sitting on a plate on the floor with food in its bowl. The video ends with a black puppy with a white dot on its face sitting on a table with food in its bowl.\\n\\nA sneak peek of a new office location with finishes and equipment coming in, featuring Dr. Tom. First, we see a man and a woman standing in a modern dental office with blue walls and a black-framed window. They are looking at a TV and a man is standing next to them. Then, we see a man and woman standing in a modern dental office with blue walls and a black-framed window. They are looking at a blue TV screen and a man is standing next to them. Next, we see a man and woman standing in a modern dental office with blue walls and a black-framed window. They are looking at a blue TV screen, and the man is standing next to them. Then, we see a man and woman standing in a modern dental office with blue walls and a black-framed window. They are looking at a blue TV screen, and the man is standing next to them. Finally, we see a man and woman standing in a modern dental office with blue walls and a black-framed window. They are looking at a blue TV screen, and the man is standing next to them. Overall, the video shows people enjoying themselves in a dental office and looking at TV screens.\"}"}
