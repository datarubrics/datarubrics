{"id": "lrec-2024-main-1243", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SaGE: Evaluating Moral Consistency in Large Language Models\\n\\nVamshi Krishna Bonagiri, Sreeram Vennam, Priyanshul Govil, Ponnurangam Kumaraguru, Manas Gaur\\n\\n1 International Institute of Information Technology Hyderabad (IIITH)\\n2 University of Maryland Baltimore County (UMBC)\\n\\nvamshi.b@research.iiit.ac.in, sreeram.vennam@students.iiit.ac.in, priyanshul.govil@research.iiit.ac.in, pk.guru@iiit.ac.in, manas@umbc.edu\\n\\nAbstract\\n\\nDespite recent advancements showcasing the impressive capabilities of Large Language Models (LLMs) in conversational systems, we show that even state-of-the-art LLMs are morally inconsistent in their generations, questioning their reliability (and trustworthiness in general). Prior works in LLM evaluation focus on developing ground-truth data to measure accuracy on specific tasks. However, for moral scenarios that often lack universally agreed-upon answers, consistency in model responses becomes crucial for their reliability. To address this issue, we propose an information-theoretic measure called Semantically Graph Entropy (SaGE), grounded in the concept of \u201cRules of Thumb\u201d (RoTs) to measure a model\u2019s moral consistency. RoTs are abstract principles learned by a model and can help explain their decision-making strategies effectively. To this extent, we construct the Moral Consistency Corpus (MCC), containing 50K moral questions, responses to them by LLMs, and the RoTs that these models followed. Furthermore, to illustrate the generalizability of SaGE, we use it to investigate LLM consistency on two popular datasets \u2013 TruthfulQA and HellaSwag. Our results reveal that task-accuracy and consistency are independent problems, and there is a dire need to investigate these issues further. Our Data and Code are publicly available at: https://github.com/vnnm404/SaGE\\n\\nKeywords: Large Language Models, Evaluation, Trustworthiness, Consistency, Reliability, Morality\\n\\n1. Introduction\\n\\n\\\"Not to care about being consistent in one\u2019s moral attitudes and feelings... would undermine one\u2019s credibility as a moral agent, not to mention as a trustworthy and responsible person; one\u2019s moral responses would be unpredictable and one\u2019s character unreliable\\\" \u2013 Campbell and Kumar (2012)\\n\\nAs Large Language Models (LLMs) continue to scale in performance, the proliferation of these AI systems in everyday use is inevitable (OpenAI, 2023; Lee, 2020). However, these systems are under-utilized due to concerns about their trustworthiness and reliability (Liu et al., 2023; Hu et al., 2021; Mayer et al., 1995). Consequently, the field of AI alignment has emerged to ensure that LLMs are calibrated to human values, morals, ethics, and social norms (Gabriel 2020; Ammanabrolu et al. 2022; Hadfield-Menell et al. 2019).\\n\\nOne of the key factors in ensuring alignment is morality \u2013 principles concerning the distinction between right and wrong, or good and bad behavior. Morality is an important factor in human decisions, and acts as a driving force behind the persuasiveness and polarization of human opinions. Since morals are shaped and maintained through mutual agreements, it\u2019s essential for AI systems to be signed with careful consideration of these values, ensuring alignment with human moral frameworks (Gauthier 1987, Hu et al. 2021).\\n\\nMoral Consistency is the ability to preserve non-contradictory moral values across different types of situations, and is often considered the hallmark of ethics (University; Arvanitis and Kalliris, 2019).\"}"}
{"id": "lrec-2024-main-1243", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, LLMs are known to yield inconsistent outputs even in semantically equivalent contexts (see Figure 1) (Jang and Lukasiewicz, 2023). This inconsistent behavior, if shown in moral scenarios, could lead LLMs to:\\n\\n1. Create confusion and uncertainty, hindering users' trust (Liu et al., 2023).\\n2. Corrupt users' moral beliefs, as shown by Kr\u00fcgel et al. (2023).\\n3. Behave in unexpected ways when deployed in the real world, leading to ethical and social risks (Weidinger et al., 2021).\\n\\nMoral consistency is widely acknowledged in psychology and ethics. However, its importance in the NLP community is yet to be established. Specifically, there is a lack of standardized methodologies and metrics to effectively assess moral consistency in LLMs, or morality in general (Chang et al., 2023).\\n\\nExisting research works in evaluating LLM alignment examine task-specific accuracies with human-labeled ground truth data in applications such as commonsense inference (Zellers et al., 2019), reasoning (Clark et al., 2018), multitasking (Ma et al., 2023), and truthful question-answering (Lin et al., 2022). However, ground truth data alone may not be good enough to evaluate LLMs (Gehrmann et al., 2023), especially on more subjective and complicated problems, such as morality and inconsistency. Thus, distinguishing between accuracy and challenges such as consistency becomes vital for crafting appropriate evaluation methodologies.\\n\\nTo address this research gap, we introduce a novel framework to measure the moral consistency of LLMs in semantically similar contexts. Our method encompasses the development of the Moral Consistency Corpus (MCC), extended from the existing \u201cMoral Integrity Corpus\u201d (MIC) (Ziems et al., 2022). Subsequently, we introduce Semantic Graph Entropy (SaGE), a novel information-theoretic metric grounded in the concept of Rules of Thumb (RoTs) to measure moral consistency in an LLM's responses. RoTs are basic conceptual units of morality that a model has learned during its training stage. Our approach consists of generating semantically equivalent scenarios and employing consistency checks to see if a target LLM follows the same RoT while responding to these scenarios (see Figure 2).\\n\\nOur framework is model-agnostic, does not require ground truth labels, and provides a reliable way to measure the consistency of a language model. We use SaGE to show that even state-of-the-art (SOTA) LLMs are morally inconsistent, questioning their reliability in the real world. Further, we generalize our method to measure consistency in other popular tasks like commonsense reasoning and truthful question-answering. Our experiments reveal that accuracy and consistency are not directly related, emphasizing the importance of understanding and improving LLMs in generating reliable responses. We also show that sampling methods do not improve consistency, and there is a need to craft better methods that can guide LLMs to provide consistent responses. Finally, we discuss one such method that can potentially improve LLM consistency.\\n\\n2. Related Work\\n\\n2.1. Morality in Language Models\\n\\nMoral decision-making is often grounded in foundational norms \u2013 don't lie, don't cheat, don't steal, etc. (Jin et al., 2022b). Prior works have attempted to teach such norms to AI models like Delphi (Jiang et al., 2022). Delphi was trained on a huge corpus of ethical judgments (Commonsense Norm Bank) and showed impressive results on its test data. However, when deployed in the real world, it was found to be inconsistent, illogical, and offensive (Talat et al., 2021). To help strengthen the morality in AI models, Forbes et al. (2020a) introduced the concept of RoTs \u2013 basic conceptual units of social norms and morality that can guide conversational agents to behave morally and pro-socially (Ziems et al. 2022, Kim et al. 2022). Subsequently, Jin et al. (2022a) proposed the MoralExceptQA challenge to teach LLMs about the exceptions within moral rules. We, however, believe that before delving into exceptions, it's crucial to ensure that LLMs are even able to follow the rules consistently.\\n\\nAs LLMs have grown in scale and capability, the spectrum of potential social risks they present has also broadened (Weidinger et al., 2021). This has led to an increasing number of works emphasizing the evaluation of these models to align with human morals. Pan et al. (2023) introduced the MACHIAVELLI benchmark to measure an LLM's tendency toward morality instead of maximizing reward. Kr\u00fcgel et al. (2023) qualitatively revealed that ChatGPT is morally inconsistent and is capable of corrupting users' moral judgments. Scherrer et al. (2023) show high levels of LLM inconsistency in moral scenarios by using them as survey respondents. However, these works require human intervention in curating datasets. Thus, they are limited by human perception and may not generalize well in the real world (Talat et al., 2021). Our work addresses this limitation by introducing an automated and generalizable approach which does not require additional human efforts, ensuring broader applicability.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: An illustration of our pipeline to evaluate moral consistency. Our five-step process includes (1) Generating quality paraphrases for each question, (2) Generating answers from the target LLM, (3) Generating RoTs for each Question-Answer pair, (4) Creating a semantic graph from the RoTs, and (5) Calculating the Semantic Graph Entropy (SaGE).\\n\\n2.2. Inconsistency in Language Models\\n\\nSemantic consistency is the ability to make consistent decisions in semantically equivalent contexts (Elazar et al., 2021). Mitchell et al. (2022) showed that neural models' internal beliefs are inconsistent across examples. Subsequently, Jang et al. (2022) expanded on these works by introducing multiple categories such as negational, symmetric, transitive, and additive consistency. While recent works have highlighted the improved capabilities of LLMs, they are still known to generate inconsistent outputs to semantically equivalent situations (Jang and Lukasiewicz, 2023). Similar to our work, these works attempted to evaluate and benchmark language models on consistency. However, they still rely on creating ground truth datasets.\\n\\nFluri et al. (2023) proposed using consistency checks as a measure to evaluate super-human scenarios (forecasting future events, making legal judgments, etc.) with no ground truth. Similarly, since moral scenarios often do not have answers which are universally agreed upon, evaluation based on ground truth becomes difficult, and may seem normative (Cialdini et al., 1991). Therefore, we propose a way to evaluate LLMs' moral consistency in a descriptive manner without defining ground truth labels. To the best of our knowledge, our work is a first in measuring moral consistency in LLMs using norms (RoTs).\\n\\n3. The Moral Consistency Corpus\\n\\nTo understand the level of moral consistency in LLMs, we develop the Moral Consistency Corpus (MCC), containing 50K moral questions, depicting 10K unique moral scenarios, and 50K \u00d7 11 answers given by 11 LLMs, along with the RoTs they used to answer these questions. MCC is constructed by selectively augmenting 10K questions from MIC through paraphrasing and using 11 LLMs (listed in Table 1) to generate answers for these questions. Finally, we generate RoTs followed by the LLMs to answer these questions. We choose MIC in our experiments due to its collection of moral questions. However, our approach can be generalized to any dataset, as shown in section 5.4.\\n\\n3.1. Generating Paraphrases and Responses\\n\\nAs we are quantifying moral consistency in semantically equivalent scenarios, our approach heavily relies on generating paraphrases. While paraphrase generation used to be a challenge in NLP (Zhou and Bhat, 2021), recent works have proven that instruct-tuned LLMs produce effective paraphrases (Kaneko et al., 2023).\\n\\nMIC is a corpus containing 38K moral prompt-reply pairs between humans and chatbots, along with human-annotated RoTs. We randomly sample 10K of these data points for our experiments.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and Okazaki, 2023). Many recent works have used paraphrasing for tasks such as data augmentation (Abaskohi et al., 2023), adversarial attacks (Agarwal et al., 2023; Morris et al., 2020), and improving natural language generation evaluation (Tang et al., 2023).\\n\\nInspired by these works, we use an LLM to generate five high-quality paraphrases for each question in the selected 10K questions. We used a Vicuna-13b model (Chiang et al., 2023) for the paraphrase generation, as our qualitative visual inspection revealed that it produced suitable paraphrases for our task. We use the following one-shot prompt to generate paraphrases from the Vicuna model.\\n\\nOur prompt: paraphrase generation\\n\\nInstruction: Your task is to generate multiple paraphrased sentences. Do not change the meaning of the text and be concise.\\n\\nSentence: example\\n\\nParaphrases: example paraphrases\\n\\nSentence: <question>\\n\\nParaphrases:\\n\\nTo ensure high quality (Bandel et al., 2022), we filter the paraphrases by selecting those that yielded a ParaScore (Shen et al., 2022) greater than 0.8. ParaScore is a metric that uses both lexical divergence and semantic similarity to ensure good-quality evaluation of paraphrases.\\n\\nThen, we generate answers for each paraphrased question using 11 different LLMs \u2013 OPT (125m, 1.3b, 2.7b, 6.7b, and 13b), LLama 2 (7b-chat-hf, 13b-chat-hf), Falcon (7b-instruct), Mistral (7b-instruct-v0.1), GPT-3.5 Turbo, and GPT-4. We chose these LLMs as they are considered SOTA due to their performance on popular benchmarks (Edward Beeching, 2023).\\n\\n3.2. Rules of Thumb (RoTs)\\n\\nForbes et al. (2020b) and Ziems et al. (2022) define RoTs as fundamental judgments about right or wrong behavior. We adapt this definition and redefine RoTs for the use of moral consistency measurement as abstract guidelines or principles inferred by a model from its training data, aiding in its judgment of right or wrong behavior. We propose using RoTs as explanations to represent better and evaluate a model's moral judgment. To this extent, we redefine moral consistency for this work as the ability to follow equivalent RoTs in semantically similar scenarios. Prior attempts by Kim et al. (2022) have shown that it is possible to generate RoTs by looking at the question-answer pairs. Inspired by these approaches, we generate RoTs for every question-answer pair in MCC using a few-shot approach (Brown et al., 2020). Following is our prompt template to generate RoTs, and the few shot examples are taken from MIC.\\n\\nOur prompt: RoT generation\\n\\nA Rule of Thumb (RoT) is a fundamental judgment about right and wrong behavior. An RoT should explain the basics of good and bad behavior, should contain a judgment (e.g. \u201cyou should\u201d) and an action (e.g. \u201cgive money to the poor\u201d) and make a general rule but still provide enough detail such that it is understandable even out of context.\\n\\nInstruction: Generate the rule of thumb for the task question-answer pair: be as concise as possible.\\n\\nQuestion: When is it worth it to fight a losing battle?\\n\\nAnswer: It is never worth it.\\n\\nRule of Thumb: Cowardice saves you from fights you cannot win.\\n\\nQuestion: <question>\\n\\nAnswer: <answer>\\n\\nRule of Thumb:\\n\\n4. Our Metric\\n\\nGiven a question $q$ and a generative language model $M$, the task of SaGE is to assess $M$'s consistency level while answering $q$. We first generate $n$ paraphrases of $q$, represented as $X(q) = \\\\{x_1, ..., x_n\\\\}$. Then, we generate model responses to each of these paraphrased questions $A(q) = \\\\{a_1, ..., a_n\\\\}$, followed by a set of RoTs obeyed while answering the respective questions $R(q) = \\\\{r_1, ..., r_n\\\\}$ (i.e., $(x_i, a_i) \\\\rightarrow r_i$). We then use semantic embeddings to represent the RoTs and construct a semantic graph for $q$. Finally, we calculate the graph entropy of the semantic graph constructed and scale the metric accordingly.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Preliminary: Graph Entropy\\n\\nGraph entropy is a measure used to determine the structural information content of graphs (Rashevsky, 1955). Graph entropy measures have been applied in diverse fields such as sociology (Lu et al. 2008, Butts 2001), chemistry, biology (Morrowitz 1955, Rashevsky 1955), and even linguistics (Abramov and Lokot, 2011; Goel et al., 2022).\\n\\nIn our work, we aim to quantify the consistency in a model's responses to paraphrased questions. We do so by analyzing the structural and semantic properties of the responses. We define graph entropy in this section and adapt it to our task in the later sections.\\n\\nWe start with the definition of Shannon's entropy (Shannon, 1948). Given a probability vector \\\\( p = (p_1, \\\\ldots, p_n) \\\\), with \\\\( 0 \\\\leq p_i \\\\leq 1 \\\\) and \\\\( \\\\sum_{i=1}^{n} p_i = 1 \\\\). The Shannon's entropy of \\\\( p \\\\) is defined as:\\n\\n\\\\[\\nH(p) = -\\\\sum_{i=1}^{n} p_i \\\\log(p_i) \\\\quad (1)\\n\\\\]\\n\\nFor a Graph \\\\( G = (V, E) \\\\), we consider the vertex probability defined by Dehmer and Mowshowitz (2011) as:\\n\\n\\\\[\\np(v_i) = f(v_i) \\\\prod_{j=1}^{\\\\left| V \\\\right|} f(v_j),\\n\\\\]\\n\\nwhere \\\\( f(v_i) \\\\) is an arbitrary information functional of \\\\( v_i \\\\). Thus, the graph entropy \\\\( I(G) \\\\) is defined as:\\n\\n\\\\[\\nI(G) = -\\\\sum_{i=1}^{n} p(v_i) \\\\log(p(v_i)) = -\\\\sum_{i=1}^{n} f(v_i) \\\\prod_{j=1}^{\\\\left| V \\\\right|} f(v_j) \\\\log(f(v_i)) \\\\prod_{j=1}^{\\\\left| V \\\\right|} f(v_j).\\n\\\\]\\n\\n4.2. Semantic Graphs\\n\\nTo assess the consistency in the RoTs, we first convert their textual representations \\\\( \\\\{r_1, \\\\ldots, r_n\\\\} \\\\) to their respective semantic embeddings \\\\( \\\\{s_1, \\\\ldots, s_n\\\\} \\\\).\\n\\nWe define a Semantic Graph \\\\( G_s = (V, E) \\\\) as a graph with semantic embeddings with vertices \\\\( V = \\\\{s_1, s_2, \\\\ldots, s_n\\\\} \\\\), and the edges as \\\\( E = \\\\{d(s_1, s_2), d(s_1, s_3), \\\\ldots, d(s_1, s_n), \\\\ldots, d(s_{n-1}, s_n)\\\\} \\\\), where \\\\( d(s_i, s_j) \\\\) represents the cosine distance between two semantic embeddings.\\n\\nWe utilize the approach of generating semantic representations of the input sequences by employing an SBERT DeBERTa model (Reimers and Gurevych, 2019; He et al., 2020), fine-tuned on Natural Language Inference (NLI) datasets (Williams et al., 2017). This model is selected due to its superior performance in creating sentence embeddings for comparison, as highlighted by Reimers and Gurevych (2019).\\n\\n4.3. Semantic Graph Entropy (SaGE)\\n\\nWe define SaGE as the graph entropy of our semantic graph \\\\( G_s \\\\). In order to calculate SaGE, we define the information functional \\\\( f(v_i) \\\\) for our use case as:\\n\\n\\\\[\\nf(v_i) = n \\\\sum_{j=1}^{\\\\left| V \\\\right|} \\\\text{sim}(v_i, v_j)\\n\\\\]\\n\\nwhere \\\\( \\\\text{sim}(v_i, v_j) \\\\) represents the semantic similarity (calculated using cosine similarity) between \\\\( v_i \\\\) and \\\\( v_j \\\\). In information theoretic terms, \\\\( f(v_i) \\\\) represents the amount of mutual information stored within the vertex \\\\( v_i \\\\). The underlying assumption is that semantically similar sequences hold more mutual information (Prior and Geffet, 2019). Substituting this in eq. 2, we get:\\n\\n\\\\[\\np(v_i) = \\\\prod_{j=1}^{\\\\left| V \\\\right|} \\\\text{sim}(v_i, v_j)\\n\\\\]\\n\\nFinally, the graph entropy \\\\( I(G_s) \\\\) is scaled by \\\\( \\\\lambda = \\\\prod_{i=1}^{\\\\left| V \\\\right|} \\\\prod_{j=1}^{\\\\left| V \\\\right|} \\\\text{sim}(v_i, v_j) / (n(n-1)) \\\\), to get:\\n\\n\\\\[\\nI(G_s) = \\\\lambda n \\\\sum_{i=1}^{n} p(v_i) \\\\log(p(v_i))\\n\\\\]\\n\\nA higher value of the graph entropy would indicate less consistency, as more randomness is associated with it. To make a higher value of SaGE indicate more consistency, we normalize the graph entropy and define SaGE as:\\n\\n\\\\[\\n\\\\text{SaGE}(G_s) = 1 - \\\\frac{I(G_s)}{\\\\log(n)}\\n\\\\]\\n\\n5. Experiments and Analysis\\n\\nWe show consistency as an intrinsic property of LLMs, independent of their hyperparameters or performance on popular benchmarks. We also explore the reliability of SaGE, and if consistency can be improved using naive methods. We lay out our investigation by answering the following questions:\\n\\n1. How morally consistent are current SOTA LLMs? (Section 5.1)\\n2. Is SaGE a reliable metric to quantify moral consistency? (Section 5.2)\\n3. Can consistency be controlled through sampling methods? (Section 5.3)\\n4. How does consistency correlate with accuracy in popular benchmarks? (Section 5.4)\\n5. Can we improve consistency with RoTs? (Section 5.5)\\n\\nWhile semantic graph entropy in itself can capture the structural properties of the graph, we multiply it with the average of sentence similarity, to capture the sentence similarity properties as well.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.1. Results on MCC\\n\\nFor a question q, given \\\\( n \\\\) paraphrases \\\\( X(q) = \\\\{x_1, ..., x_n\\\\} \\\\), with generated answers as \\\\( A(q) = \\\\{a_1, ..., a_n\\\\} \\\\), Elazar et al. (2021)'s measure of consistency is defined as:\\n\\n\\\\[\\n\\\\text{Cons}_{\\\\text{lex}}(q) = 2 \\\\frac{n}{n(n-1)} \\\\sum_{i,j=1, i \\\\neq j} \\\\text{sim}(a_i, a_j)\\n\\\\]\\n\\nHere, \\\\( \\\\text{sim}(x, y) \\\\) is replaced with lexical similarity metrics such as BLEU (Papineni et al., 2002) and ROUGE (Lin, 2004). Consequent works have replaced the lexical similarity metrics with semantic similarity metrics (Raj et al., 2022) for more reliability. Therefore, we replace \\\\( \\\\text{sim}(x, y) \\\\) with BERTScore to incorporate semantic similarity.\\n\\nTo quantify moral consistency in LLMs, we follow our pipeline on a subset of MIC to construct MCC. Then, we evaluate 11 LLMs on MCC using SaGE, along with the metrics mentioned above. Our approach relies on checking if these LLMs are consistent with their answers, when questions are paraphrased.\\n\\nTable 1 shows the LLMs' average scores on the MCC dataset. Of the SOTA LLMs we picked, the maximum observed SaGE score was 0.681, revealing that LLMs are inconsistent in moral scenarios. We notice that among the OPT models, there is an increase in consistency with the number of model parameters. However, this does not hold perfectly for the other groups of models, as GPT-3.5 Turbo shows a higher level of consistency compared to GPT-4. Since SOTA models do not perform well on our task, MCC can serve as a benchmark to assess moral consistency in future LLMs. Through this experiment, we highlight the issue of moral consistency in current LLMs, and call for the development of better models that are morally aligned and consistent.\\n\\n5.2. Human Evaluations\\n\\nTo assess the reliability of SaGE, we compare it with the metrics mentioned in section 5.1 with respect to human annotations. For human annotations, we qualitatively select 500 data points from MCC that contain questions which demand the LLM's moral opinions.\\n\\nMeasuring consistency with human judgments is not a trivial task. Therefore, similar to (Gururangan et al., 2018), we asked the annotators to look at pairwise answers from the dataset, and determine if they are semantically equivalent. To ensure the consistency of our annotations, we employed a three-rater system where 'Y' denoted agreement (semantic equivalence), 'N' indicated disagreement, and 'NA' represented uncertainty. We observed a Krippendorff's \\\\( \\\\alpha \\\\) score of 0.868, signifying high reliability among annotators.\\n\\nWe construct a mapping of: 'Y' \\\\( \\\\rightarrow 1 \\\\) and 'N' \\\\( \\\\rightarrow 0 \\\\) and calculate the entropy of this distribution, by converting it into a probability distribution for each question. Then, we measure its correlation with respect to SaGE and the other metrics chosen. Results displayed in Table 2 show that SaGE best correlates with human judgments for our task. Interestingly, the usage of RoTs show a significant increase in correlations, implying the relevance of RoTs in assessing moral consistency. The low correlations of BLEU and ROUGE indicate that lexical similarity is not a good measurement, reinforcing prior research (Kan\u00b4e et al., 2019). Meanwhile, semantic similarity measures such as BERTScore capture semantic information, showing an increase in correlations. However, SaGE accounts for structural properties as well as semantic similarity in the data, making it a better metric to assess consistency.\\n\\n5.3. Consistency and Temperature\\n\\nTemperature-based sampling is a common approach to sampling-based generation. It is used to alter the probability distribution of a model's output, with temperature as a parameter (Holtzman et al., 2019). During the decoding step, the probability mass function (PMF) of the model's vocabulary (with temperature \\\\( T \\\\)) is estimated as:\\n\\n\\\\[\\nP(r(v_k)) = \\\\frac{e^{l_k/T}}{P_i e^{l_i/T}} (8)\\n\\\\]\\n\\nwhere \\\\( v_k \\\\) is the \\\\( k \\\\)-th vocabulary token and \\\\( l_k \\\\) the corresponding logit. This would imply that when \\\\( T = 0 \\\\), the PMF becomes a Kronecker delta function, and the response becomes completely deterministic. Similarly, a larger \\\\( T \\\\) value would make the PMF more evenly distributed, increasing the randomness in generations.\\n\\nHowever, moral consistency is an intrinsic property of LLMs, whereas sampling methods represent extrinsic methods to generate text after an LLM processes the input. To show that moral consistency is not a function of temperature, we perform our consistency experiment on different temperature values. This is done in two settings: (1) The model is prompted with the same question 5 times, and (2) with 5 different paraphrases. We use the same 500 quality questions used in the previous section for this experiment.\\n\\nFigure 3 summarizes the results. While consistency decreases in the case of same questions, we see almost no change in consistency in the case of paraphrasing. This reveals that consistency in the real world (where paraphrased inputs are common) is not a function of temperature and is an intrinsic property of LLMs. This shows that sampling-based extrinsic methods are not a fix for consistency, and...\"}"}
{"id": "lrec-2024-main-1243", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Average consistency scores of 11 LLMs on MCC. The 'Ans' column represents the scores calculated on LLM answers, and the 'RoT' column represents scores calculated on the generated RoTs. Results show that none of the state-of-the-art LLMs cross a SaGE score of 0.681, indicating the inability of LLMs to be morally consistent. Some of the best-performing models in different categories are indicated in bold.\\n\\n\u2020: Results on a subset of MCC (10%) due to API limitations.\\n\\nFigure 3: Representation of the variation in ROUGE and SaGE scores across different temperatures. The dashed line depicts consistency trends without paraphrasing, and the solid line depicts consistency trends with paraphrases. The figure reveals that consistency is not dependent on temperature.\\n\\nTable 2: Pearson correlations of SaGE with the average of human annotations. SaGE shows significant improvement over the previous metrics. On top of that, the results show that using RoTs enhances the reliability of such metrics even further.\\n\\nTable 3: SaGE scores and accuracies on TruthfulQA and HellaSwag. No correlations are observed between the two (see Figure 4).\\n\\n5.4. Consistency and Accuracy\\nIn this section, we evaluate LLM consistency in tasks similar to moral reasoning with established benchmarks to understand if consistency can be studied through such datasets. We employ our pipeline on two popular benchmarks:\\n\\n1. TruthfulQA (Lin et al., 2022): A benchmark to measure whether a language model is truthful in generating answers to questions. It contains 817 questions that some humans would falsely answer due to false beliefs or misconceptions.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. HellaSwag (Zellers et al., 2019): A commonsense inference challenge dataset. HellaSwag contains over 39K contexts and 4 possible extensions each of the contexts, of which only one adheres to commonsense.\\n\\nThe major distinguishing factor of MCC from these datasets is that MCC does not have ground truth, while HellaSwag and TruthfulQA have ground truth to evaluate accuracies against. This presents us with an opportunity to see if a model that is accurate on a task, is also consistent on the same task.\\n\\nTable 3 and Figure 4 summarize the results. The results reveal that task accuracy and consistency are two different problems. It is important to note that a model that is truthful or can reason, should also be able to do so consistently. However, we show that SOTA LLMs fail to perform these tasks consistently, revealing a major pitfall in the evaluation strategies being employed in current systems (i.e., through ground truth data).\\n\\n5.5. Improving consistency\\n\\nIn order to explore possible strategies of improving consistency, we employ a naive method to see if LLMs even have the ability to behave consistently. We do this by prompting the LLM to follow specific RoTs while answering questions. These RoTs are human annotated, and are taken from the MIC corpus. Specifically, we use the prompt below to make LLMs follow specific RoTs while answering questions.\\n\\nOur prompt: RoT-based answer generation\\nInstruction: Answer the following question. Keep in mind this rule of thumb, <RoT>.\\n\\nQuestion: <question>\\nAnswer: \\n\\nWe sample a subset of HellaSwag (1K data points) for our experiments due to computing constraints. Table 4 summarizes the results of the experiment. We notice that there is a significant improvement (around 10%) when we ask the LLM to follow an RoT while answering a question. This indicates that LLMs can be taught to follow rules consistently. This methodology can be employed by knowledge-based systems to pick certain rules during inference, allowing the models to produce more consistent results. Our results indicate that there is a scope for improvement in LLM consistency, and SaGE can reliably measure such improvements.\\n\\n| Model          | BLEU | ROUGE | BERT Score |\\n|----------------|------|-------|------------|\\n| GPT-3.5        | 0.015| 0.151 | 0.529      |\\n| GPT-3.5 with RoT prompting | 0.018| 0.169 | 0.565      |\\n\\nTable 4: Average consistency scores before and after including RoTs to be followed in the prompt. The experiment reveals a clear increase in consistency levels after including RoT in the prompt. The experiment is carried out on 500 handpicked samples from MCC.\\n\\n6. Conclusion\\n\\nIn this work, we introduce a novel framework to evaluate the moral consistency of LLMs. We introduce SaGE, a new information-theoretic metric, grounded in the concept of RoTs to measure moral consistency in LLMs. Our approach mainly consists of generating high quality paraphrases of moral questions, and employing consistency checks to see if a target LLM follows the same RoT while answering these questions. We also introduce the MCC to measure the moral consistency in LLMs. We evaluate SOTA LLMs on our dataset, and show that they are morally inconsistent in their generations. Further, we show that inconsistency is an intrinsic property of LLMs, and cannot be solved with extrinsic methods such as temperature sampling. By employing SaGE on other popular tasks, we show that task based accuracy and consistency are independent problems, indicating an urgent need to investigate this problem further. Finally, we show that LLMs can be taught to be consistent by simply making them follow RoTs, hinting a scope of improvement in this domain. We invite future works to develop more consistent models and evaluate them on our dataset, or use our methodology to evaluate consistency on their own task, ultimately leading to LLMs that can produce morally consistent and ethically sound responses.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Ethical Considerations\\n\\nPrecautions taken during dataset construction.\\n\\nFirstly, as MCC is a direct extension of MIC (Ziems et al., 2022), we ensure that it adheres to the same moral principles as MIC, and followed similar ethical assumptions while constructing the dataset. While we understand that the generation of rules and norms can be seen as normative, we emphasize that our work only uses RoT to evaluate if a model is consistently following the same RoT, making it a completely descriptive approach. Therefore, we do not judge if any RoT is right or wrong, but simply use it to evaluate the consistency of a model\u2019s judgement.\\n\\nRisks from data release.\\n\\nMCC is an evaluation dataset specifically for research purposes only; it is advised against using this dataset for training models, as it may encompass rules that contravene the ethical principles of certain communities. It is very critical to note that the paraphrases and RoTs generated are not fully monitored by the authors or any humans, so it may contain unreliable, ethically questionable, or upsetting generations by LLMs. Therefore, to ensure awareness among the data users, we explicitly provide these details and warnings to users who seek to use our data. Furthermore, we emphasize that the RoTs generated are not intended to be universally binding, nor do they reflect a human\u2019s moral opinions. They do not constitute a comprehensive ethical framework but rather serve as a means to elucidate pre-existing biases in models.\\n\\nRisks in methodology.\\n\\nFor our human annotation experiments, we ensure that the annotators are fully aware of the potential for harmful or sensitive data, and allowed them to opt out at any point. Furthermore, we were constantly monitoring them to ensure a smooth annotation process, which did not make them uncomfortable in any form whatsoever.\\n\\nIn the section focused on improvements, we prompt the model on which rules to follow. While this can be considered a normative approach, we only perform this experiment to show that it is possible to increase consistency in current LLMs, and SaGE can measure it. We do not consider this an effective method to build morally aligned agents, but only a naive method to improve consistency. Our work focuses solely on improving moral consistency, as we consider the moral alignment with humans a subsequent problem.\\n\\nWe understand that moral consistency is a broader term in ethics, and the moral consistency of humans itself is often debated about (Paton, 1971; Marcus, 1980). However, we tackle a subproblem of it which is semantic consistency in moral scenarios, and argue that these inconsistencies would cause issues to the users and LLMs trust.\\n\\n8. Limitations\\n\\nOur experiments are limited to only 11 LLMs and 5 paraphrases due to GPU and compute constraints. However, we make sure to include most of the SOTA architectures in our experiments, along with models with different number of parameters (from the OPT family) to analyse consistency across such categories. Our methods also depend on many NLP tools such as SBERT for sentence embeddings, and Vicuna for paraphrase and RoT generation. Therefore, some of their limitations will carry over to our work. Despite that, we chose these tools due to their proven capabilities in the respective tasks, and make additional checks such as human annotations, and evaluation with existing metrics to ensure the tools are performing the required tasks effectively. Specifically, we understand that tasks such as RoT generation can also provide inconsistent results, since we are using LLMs for them. While we acknowledge that this may cause minor inconsistencies in our experiments, we rely on previous works that show effective generation of RoTs (Kim et al., 2022; Ziems et al., 2022), and current LLM capabilities in text generation (Khalatbari et al., 2023) to ensure reliable generation of RoTs. We also show that the RoTs we generated are reliable, through our human annotations.\\n\\n9. Acknowledgements\\n\\nThis work was done during the first (Vamshi Krishna Bonagiri) and third (Priyanshul Govil) author\u2019s internship at KAI2 Lab of UMBC. We would like to thank Abhinav Menon, Anmol Goel, Daniel Paleka, Gaurav Singh, Mukund Choudhary, Pratyaksh Gautam, Shashwat Singh, and Vanshpreet Singh Kohli for their valuable feedback.\\n\\n10. References\\n\\nAmirhossein Abaskohi, Sascha Rothe, and Yadollah Yaghoobzadeh. 2023. Lm-cppf: Paraphrasing-guided data augmentation for contrastive prompt-based few-shot fine-tuning. arXiv preprint arXiv:2305.18169.\\n\\nOlga Abramov and Tatiana Lokot. 2011. Typology by means of language networks: Applying information theoretic measures to morphological derivation networks. Towards an Information Theory of Complex Networks: Statistical Methods and Applications, pages 321\u2013346.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lrec-2024-main-1243", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anmol Goel, Charu Sharma, and Ponnurangam Kumaraguru. 2022. An unsupervised, geometric and syntax-aware quantification of polysemy. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 10565\u201310574.\\n\\nSuchin Gururangan, Swabha Swayamdipta, Omer Levy, Roy Schwartz, Samuel Bowman, and Noah A Smith. 2018. Annotation artifacts in natural language inference data. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), pages 107\u2013112.\\n\\nDylan Hadfield-Menell, McKane Andrus, and Gillian Hadfield. 2019. Legible normativity for AI alignment: The value of silly rules. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, pages 115\u2013121.\\n\\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and Weizhu Chen. 2020. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations.\\n\\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2019. The curious case of neural text degeneration. In International Conference on Learning Representations.\\n\\nPeng Hu, Yaobin Lu, and Yeming (Yale) Gong. 2021. Dual humanness and trust in conversational AI: A person-centered approach. Computers in Human Behavior, 119:106727.\\n\\nMyeongjun Jang, Deuk Sin Kwon, and Thomas Lukasiewicz. 2022. Becel: Benchmark for consistency evaluation of language models. In Proceedings of the 29th International Conference on Computational Linguistics, pages 3680\u20133696.\\n\\nMyeongjun Jang and Thomas Lukasiewicz. 2023. Consistency Analysis of ChatGPT. ArXiv:2303.06273 [cs].\\n\\nLiwei Jiang, Jena D. Hwang, Chandra Bhagavatula, Ronan Le Bras, Jenny Liang, Jesse Dodge, Keisuke Sakaguchi, Maxwell Forbes, Jon Borchartd, Saadia Gabriel, Yulia Tsvetkov, Oren Etzioni, Maarten Sap, Regina Rini, and Yejin Choi. 2022. Can Machines Learn Morality? The Delphi Experiment. ArXiv:2110.07574 [cs].\\n\\nZhijing Jin, Sydney Levine, Fernando Gonzalez Adauto, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Josh Tenenbaum, and Bernhard Sch\u00f6lkopf. 2022a. When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment. ArXiv:2210.01478 [cs].\\n\\nZhijing Jin, Sydney Levine, Fernando Gonzalez, Ojasv Kamal, Maarten Sap, Mrinmaya Sachan, Rada Mihalcea, Josh Tenenbaum, and Bernhard Sch\u00f6lkopf. 2022b. When to make exceptions: Exploring language models as accounts of human moral judgment. Advances in neural information processing systems, 35:28458\u201328473.\\n\\nHassan Kan\u00e9, Yusuf Kocyigit, Pelkins Ajanoh, Ali Abdalla, and Mohamed Coulibali. 2019. Towards neural similarity evaluator. In Workshop on Document Intelligence at NeurIPS 2019.\\n\\nMasahiro Kaneko and Naoaki Okazaki. 2023. Reducing sequence length by predicting edit operations with large language models. arXiv preprint arXiv:2305.11862.\\n\\nLeila Khalatbari, Yejin Bang, Dan Su, Willy Chung, Saeed Ghadimi, Hossein Sameti, and Pascale Fung. 2023. Learn What NOT to Learn: Towards Generative Safety in Chatbots. Publisher: arXiv Version Number: 2.\\n\\nHyunwoo Kim, Youngjae Yu, Liwei Jiang, Ximing Lu, Daniel Khashabi, Gunhee Kim, Yejin Choi, and Maarten Sap. 2022. ProsocialDialog: A Prosocial Backbone for Conversational Agents. ArXiv:2205.12688 [cs].\\n\\nSebastian Krugel, Andreas Ostermaier, and Matthias Uhl. 2023. ChatGPT's inconsistent moral advice influences users\u2019 judgment. Sci Rep, 13(1):4569. Number: 1 Publisher: Nature Publishing Group.\\n\\nRaymond ST Lee. 2020. Artificial intelligence in daily life. Springer.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nStephanie Lin, Jacob Hilton, and Owain Evans. 2022. TruthfulQA: Measuring How Models Mimic Human Falsehoods. ArXiv:2109.07958 [cs].\\n\\nYang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. 2023. Trustworthy LLMs: a Survey and Guideline for Evaluating Large Language Models\u2019 Alignment. ArXiv:2308.05374 [cs].\\n\\nJia-Liang Lu, Fabrice Valois, Mischa Dohler, and Dominique Barthel. 2008. Quantifying organization by means of entropy. IEEE communications letters, 12(3):185\u2013187.\\n\\nXiao Ma, Swaroop Mishra, Ahmad Beirami, Alex Beutel, and Jilin Chen. 2023. Let's do\"}"}
{"id": "lrec-2024-main-1243", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a thought experiment: Using counterfactuals to improve moral reasoning.\\n\\nRuth Barcan Marcus. 1980. Moral dilemmas and consistency. The Journal of Philosophy, 77(3):121\u2013136.\\n\\nRoger C. Mayer, James H. Davis, and F. David Schoorman. 1995. An Integrative Model of Organizational Trust. The Academy of Management Review, 20(3):709\u2013734. Publisher: Academy of Management.\\n\\nEric Mitchell, Joseph Noh, Siyan Li, Will Armstrong, Ananth Agarwal, Patrick Liu, Chelsea Finn, and Christopher Manning. 2022. Enhancing Self-Consistency and Performance of Pre-Trained Language Models through Natural Language Inference. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1754\u20131768, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics.\\n\\nHarold J Morowitz. 1955. Some order-disorder considerations in living systems. The bulletin of mathematical biophysics, 17:81\u201386.\\n\\nJohn Morris, Eli Lifland, Jin Yong Yoo, Jake Grigsby, Di Jin, and Yanjun Qi. 2020. Textattack: A framework for adversarial attacks, data augmentation, and adversarial training in NLP. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 119\u2013126.\\n\\nOpenAI. 2023. GPT-4 technical report.\\n\\nAlexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, and Dan Hendrycks. 2023. Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior in the MACHIAVELLI Benchmark. ArXiv:2304.03279 [cs].\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pages 311\u2013318.\\n\\nHerbert James Paton. 1971. The Categorical Imperative: A Study in Kant\u2019s Moral Philosophy, volume 1023. University of Pennsylvania Press.\\n\\nAnat Prior and Maayan Geffet. 2019. Mutual information and semantic similarity as predictors of word association strength: Modulation by association type and semantic relation. In Proceedings of EuroCogSci, pages 265\u2013270.\\n\\nHerbert James Paton. 1971. The Categorical Imperative: A Study in Kant\u2019s Moral Philosophy, volume 1023. University of Pennsylvania Press.\\n\\nNicolas Rashevsky. 1955. Life, information theory, and topology. The bulletin of mathematical biophysics, 17:229\u2013235.\\n\\nNils Reimers and Iryna Gurevych. 2019. Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks. ArXiv:1908.10084 [cs].\\n\\nNino Scherrer, Claudia Shi, Amir Feder, and David M. Blei. 2023. Evaluating the Moral Beliefs Encoded in LLMs.\\n\\nClaude Elwood Shannon. 1948. A Mathematical Theory of Communication. The Bell system technical journal, 27(3):379\u2013423.\\n\\nLingfeng Shen, Lemao Liu, Haiyun Jiang, and Shuming Shi. 2022. On the Evaluation Metrics for Paraphrase Generation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3178\u20133190.\\n\\nZeerak Talat, Hagen Blix, Josef Valvoda, M. I. Ganesh, Ryan Cotterell, and Adina Williams. 2021. A Word on Machine Ethics: A Response to Jiang et al. (2021). ArXiv.\\n\\nTianyi Tang, Hongyuan Lu, Yuchen Eleanor Jiang, Haoyang Huang, Dongdong Zhang, Wayne Xin Zhao, and Furu Wei. 2023. Not all metrics are guilty: Improving NLG evaluation with LLM paraphrasing. ArXiv preprint arXiv:2305.15067.\\n\\nSanta Clara University. Consistency and Ethics.\\n\\nLaura Weidinger, John Mellor, Maribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen Huang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa Kasirzadeh, Zac Kenton, Sasha Brown, Will Hawkins, Tom Stepleton, Courtney Biles, Abeba Birhane, Julia Haas, Laura Rimell, Lisa Anne Hendricks, William Isaac, Sean Legassick, Geoffrey Irving, and Iason Gabriel. 2021. Ethical and Social Risks of Harm from Language Models. ArXiv:2112.04359 [cs].\\n\\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A Broad-Coverage Challenge Corpus for Sentence Understanding Through Inference. ArXiv preprint arXiv:1704.05426.\\n\\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4791\u20134800.\"}"}
{"id": "lrec-2024-main-1243", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jianing Zhou and Suma Bhat. 2021. Paraphrase generation: A survey of the state of the art. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 5075\u20135086.\\n\\nCaleb Ziems, Jane A. Yu, Yi-Chia Wang, Alon Halevy, and Diyi Yang. 2022. The Moral Integrity Corpus: A Benchmark for Ethical Dialogue Systems. ArXiv:2204.03021 [cs].\"}"}
