{"id": "acl-2024-long-224", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maven-Arg: Completing the Puzzle of All-in-One Event Understanding\\nDataset with Event Argument Annotation\\nXiaozhi Wang, Hao Peng, Yong Guan, Kaisheng Zeng, Jianhui Chen, Lei Hou, Xu Han, Yankai Lin, Zhiyuan Liu, Ruobing Xie, Jie Zhou, Juanzi Li\\n\\nAbstract\\nUnderstanding events in texts is a core objective of natural language understanding, which requires detecting event occurrences, extracting event arguments, and analyzing inter-event relationships. However, due to the annotation challenges brought by task complexity, a large-scale dataset covering the full process of event understanding has long been absent. In this paper, we introduce MAVEN-ARG, which augments MAVEN datasets with event argument annotations, making the first all-in-one dataset supporting event detection, event argument extraction (EAE), and event relation extraction. As an EAE benchmark, MAVEN-ARG offers three main advantages: (1) a comprehensive schema covering 162 event types and 612 argument roles, all with expert-written definitions and examples; (2) a large data scale, containing 98,191 events and 290,613 arguments obtained with laborious human annotation; (3) the exhaustive annotation supporting all task variants of EAE, which annotates both entity and non-entity event arguments in document level. Experiments indicate that MAVEN-ARG is quite challenging for both fine-tuned EAE models and proprietary large language models (LLMs). Furthermore, to demonstrate the benefits of an all-in-one dataset, we preliminarily explore a potential application, future event prediction, with LLMs. MAVEN-ARG and codes can be obtained from https://github.com/THU-KEG/MAVEN-Argument.\\n\\n1 Introduction\\nConveying information about events is a core function of human languages (Levelt, 1993; Pinker, 2013; Miller and Johnson-Laird, 2013), which highlights event understanding as a major objective for natural language understanding and a foundation for various downstream applications (Ding et al., 2015; Li et al., 2018a; Goldfarb-Tarrant et al., 2019; Huang et al., 2019; Wang et al., 2021a). As illustrated in Figure 1, event understanding is typically organized as three information extraction tasks (Ma et al., 2021; Peng et al., 2023b): event detection (ED), which detects event occurrences by identifying event triggers and classifying event types; event argument extraction (EAE), which extracts event arguments and classifies their argument roles; event relation extraction (ERE), which analyzes the coreference, temporal, causal, and hierarchical relationships among events.\\n\\nDespite the importance of event understanding, a large-scale dataset covering all the event understanding tasks has long been absent. Established sentence-level event extraction (ED and EAE) datasets like ACE 2005 (Walker et al., 2006) and TAC KBP (Ellis et al., 2015, 2016; Getman et al., 2017) do not involve event relation types besides the basic coreferences. RAMS (Ebner et al., 2020) and WikiEvents (Li et al., 2021) extend EAE to the document level but do not involve event relations. ERE datasets are mostly developed independently for coreference (Cybulska and Vossen, 2014), temporal (Chambers et al., 2014; Ning et al., 2018), causal (Mirza et al., 2014; Mostafazadeh et al., 2016b; Caselli and Vossen, 2017), and subevent (Hovy et al., 2013; Glava\u0161...\"}"}
{"id": "acl-2024-long-224", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and \u0160najder, 2014) relationships and do not cover event arguments. Given annotation challenges from task complexity, these datasets often cover only thousands of events. Due to the inconsistent event schemata and data, these datasets cannot be unified. This status quo hinders the development of end-to-end event understanding methods and limits the potential for event-based downstream applications.\\n\\nMAVEN (Wang et al., 2020) is the largest human-annotated ED dataset, with a high-coverage event schema for general-domain events. Based on it, Wang et al. (2022) further annotates the first unified ERE dataset MAVEN-ERE, which covers all four types of event relationships and has a massive scale with more than one million event relations. Building on the sustained efforts of these works over years, we complete the puzzle of an all-in-one event understanding dataset in this work. We construct MAVEN-ARG, which provides exhaustive event argument annotations based on MAVEN.\\n\\nBeyond finishing an all-in-one event understanding dataset, three main advantages of MAVEN-ARG make it a valuable EAE benchmark. (1) Comprehensive Event Schema. The original MAVEN schema only defines event types but without argument roles. We engage experts to enhance MAVEN schema with argument roles and to write detailed definitions for them, which help annotators and can also serve as task instructions for prompting large language models. The resulting event schema contains 162 event types, 612 argument roles, and 14,655 words of definitions, which well cover general-domain events. (2) Large Data Scale. MAVEN-ARG comprises 107,507 event mentions, 290,613 event arguments, and 129,126 entity mentions, all of which are human annotated. To our knowledge, this makes it the largest EAE dataset currently available. (3) Exhaustive Annotation. The development of EAE has seen many variations in task settings, including annotating only the topic event (Ebner et al., 2020; Tong et al., 2022) of a document or all fine-grained events (Walker et al., 2006), annotating event arguments at the sentence level (Walker et al., 2006) or document level (Ebner et al., 2020; Li et al., 2021), and limiting event arguments to entities (Walker et al., 2006; Li et al., 2021) or including non-entity arguments (Grishman and Sundheim, 1996; Parekh et al., 2023). MAVEN-ARG adopts the most exhaustive annotation. We annotate event arguments for all fine-grained events at the document level, covering both entity and non-entity arguments. This enhances the dataset\u2019s utility for benchmarking and developing a wide range of EAE methods.\\n\\nIn the experiments, we reproduce several recent state-of-the-art EAE models as baselines and also evaluate large language models with in-context learning. Experimental results show that they can only achieve at most 40% F1 scores, which is far from promising. It indicates that MAVEN-ARG is quite challenging and more research efforts are needed to develop practical EAE methods. Furthermore, to demonstrate the advantage of an all-in-one event understanding dataset for enabling sophisticated event-based applications, we conduct a preliminary exploration of future event prediction. We sample causally related event chains from MAVEN-ARG and prompt LLMs to predict future events, including their types and arguments. Experiments show that while most of the predictions are reasonable, they seldom align with the actual future. We encourage future work to further explore this application and hope MAVEN-ARG can help improve EAE and develop diverse event-based applications.\"}"}
{"id": "acl-2024-long-224", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Event schema statistics of MAVEN-A RG compared with other datasets.\\n\\n| Dataset # | Event Type | Argument Role |\\n|-----------|------------|---------------|\\n| ACE 2005  | 33         | 36            |\\n| DocEE     | 59         | 356           |\\n| WikiEvents | 50        | 59            |\\n| RAMS      | 139        | 65            |\\n| MEE       | 16         | 23            |\\n| GENEV A   | 115        | 220           |\\n| MAVEN-ERE | 162        | 612           |\\n\\nEvent types that are similar to others and renamed 4 event types for clarity. (2) The basic schema is constructed from a simplification and modification of FrameNet (Baker et al., 1998). The frame elements in FrameNet are widely considered akin to argument roles (Aguilar et al., 2014; Parekh et al., 2023), but they are often too complex for ordinary annotators to comprehend since FrameNet is primarily constructed for linguistic experts (Aguilar et al., 2014). Therefore, for each event type, we manually select frame elements related to describing events and suitable for annotation as MAVEN-A RG argument roles from their FrameNet equivalents, and we rewrite the definitions and examples. (3) Extending argument roles based on the 5W1H analysis (What, Where, When, Why, Who, How) for describing events (Karaman et al., 2017; Hamborg et al., 2019). Temporal and causal relations from event relation extraction describe When and Why, while the event type describes What. We primarily refer to Who (participants), Where (locations), and How (manners, instruments, etc.) to design argument roles. (4) Considering the hierarchical structure. When designing subordinate types, we inherit and refine the argument roles of their superordinate types. (5) Sampling data to check if any event argument is missing.\\n\\nSchema Statistics\\n\\nAfter the schema design, the final MAVEN-A RG schema contains 162 event types, 612 unique argument roles, and 143,655 words of definitions. Taking inspiration from semantic role labeling (Fillmore, 1976; Banarescu et al., 2013), we tend to let the argument roles sharing the same semantic role use the same name but distinguish them with different textual definitions. For instance, we do not use Killer for the Killing event type and use Attacker for the Attack event type. Instead, we use Agent to denote them both but write different definitions for them. This is to encourage the knowledge transfer between EAE for different event types. Therefore, 612 is the number of argument roles with unique definitions, and there are 143 unique names for all the argument roles. Table 1 compares the event schema size of MAVEN-A RG with existing EAE datasets, including ACE 2005 (Walker et al., 2006), DocEE (Tong et al., 2022), WikiEvents (Li et al., 2021), RAMS (Ebner et al., 2020), MEE (Pouran Ben Veyseh et al., 2022), and GENEVA (Parekh et al., 2023). We can observe that MAVEN-A RG has the largest event schema, which more comprehensively covers the broad range of diverse events and will help develop more generalizable methods.\\n\\n2.2 Entity Annotation\\n\\nThe mainstream task setting for EAE (Walker et al., 2006; Li et al., 2021) confines event arguments to entities, which reduces the task's complexity to some extent and provides more definite and standardized extraction results. Hence, before annotating event arguments, we annotate entities for the 4,480 MAVEN documents. We follow the task definition and guidelines of a recent named entity recognition benchmark Few-NERD (Ding et al., 2021), but we only annotate coarse-grained entity types, including Person, Organization, Location, Building, Product, Art, and MISC. To deliver more unambiguous EAE results and reduce the argument annotation difficulty, we also annotate entity coreference, which means judging whether multiple entity mentions refer to the same entity. During entity annotation, we engage 47 annotators, including 8 senior annotators selected during the annotation training. Each document is annotated by three independent annotators and further checked by one senior annotator. The final annotation results are aggregated via majority voting. If the senior annotator judged the accuracy of a document's annotation to be below 90%, the document will be returned to the three first-stage annotators for re-annotation. To check data quality, we calculate Fleiss' kappa (Fleiss, 1971) to measure the inter-annotator agreements. The result for entity recognition is 73.2%, and for entity coreference is 78.4%, both indicating high consistency.\\n\\n2.3 Event Argument Annotation\\n\\nBased on the event detection annotations of MAVEN and event coreferences of MAVEN-ERE, GENEVA has a larger ontology without data. Here we compare with its schema actually used in the dataset.\"}"}
{"id": "acl-2024-long-224", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of MAVEN-A RG compared to existing widely-used EAE datasets. \u201cDoc.\u201d is short for \u201cDocument\u201d and \u201cArg.\u201d is short for \u201cArgument\u201d. \u201c-\u201d denotes not applicable due to lack of document structure or corresponding annotations. \u201cFine-grained Event\u201d means annotating all the events rather than only one topic event for a document. \u201cDoc. Level\u201d means annotating arguments within the whole document rather than only the sentence containing the trigger. For multilingual datasets, we only compare with its English subset. We conduct event argument annotations. For multiple coreferent event mentions (triggers), only one of them is displayed during annotation to reduce annotation overhead. Once the annotator selects an event trigger, the corresponding argument roles for its event type are displayed on the annotation interface, along with definitions and examples. This ensures that annotators do not have to memorize the lengthy event schema or frequently refer to the annotation guidelines. To annotate an event argument, annotators can either choose an entity from the whole document or select a continuous textual span; once an entity mention is selected, all of its coreferent entity mentions are automatically selected. Annotators have the option to report errors in the event type annotation of a trigger, which allows for the discarding of that trigger. In the annotation process, approximately 4% of triggers are discarded.\\n\\nWe employ 202 annotators, including 71 senior annotators selected during annotation training and 33 experts with rich annotation experiences. The experts are recommended by the commercial data annotation companies we employed, and they have undergone at least ten data annotation projects and led at least one. The annotation is divided into three phases. Each document is first annotated by an ordinary annotator, and then modified by a senior annotator. Finally, an expert will check whether the annotation accuracy reaches 90%. If not, the document\u2019s annotation will be returned to the second phase. To measure data quality, we randomly sample 100 documents and conduct the three-phrase annotation for them twice with different annotator groups. The Fleiss\u2019 kappa is 0.68%, which indicates a satisfactory level of annotation agreement. More annotation details are shown in appendix A.\\n\\n3 Data Analysis\\n\\nTo provide intuitive descriptions for MAVEN-A RG, we conduct data analyses in this section.\\n\\n3.1 Data Statistics\\n\\nTable 2 shows the main statistics of MAVEN-A RG compared with various existing EAE datasets. Appendix B.1 further shows the statistics of different splits. We can observe that MAVEN-A RG has two advantages: (1) MAVEN-A RG has the largest data scale, surpassing previous datasets by several times. This ensures that even for long-tail event types, MAVEN-A RG has sufficient data to fully train and stably evaluate EAE models. (2) The exhaustive annotation of MAVEN-A RG makes it the only dataset that covers all settings of EAE task. MAVEN-A RG includes complete annotations of entity and event coreference and annotates both entity and non-entity arguments for all fine-grained events at the document level. This allows MAVEN-A RG to support the evaluation of all variants of EAE methods and the development of comprehensive event understanding applications.\\n\\n3.2 Data Distribution\\n\\nWe present the distributions of the annotated entity and event arguments of MAVEN-A RG in Figure 2. Argument roles with the same name across different event types are merged for presentation clarity. We observe that: (1) The distribution of entity types is generally similar to that of FewNERD (Ding et al., 2021), demonstrating sufficient diversity. (2) The three most frequent basic argument roles (Agent, Patient, and Location) account for over 60% of event arguments. This highlights their ubiquity and encourages knowledge...\"}"}
{"id": "acl-2024-long-224", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For clarity, only the top event argument roles are shown and the others are summed up in \\\"Others\\\". The long-tailed distribution of MAVEN-ARG poses a significant challenge to model generalizability.\\n\\n3.3 Trigger-argument Distance\\n\\nWe analyze the distribution of trigger-argument distances in Figure 3. For events with multiple coreferent triggers and entity arguments with multiple entity mentions, the distance is calculated between the nearest trigger-argument pairs. The overall average trigger-argument distance is 37.8. From Figure 3, we observe that while the majority of event arguments are located near their triggers, which is natural for human writing, a substantial number of arguments are situated far from their triggers, with the furthest exceeding 800 words. This data characteristic challenges the ability of EAE methods to capture long-distance dependencies.\\n\\n4 Experiment\\n\\n4.1 Experimental Setup\\n\\nModels\\n\\nTo assess the challenge of MAVEN-ARG, we evaluate multiple advanced methods. For fine-tuned EAE models, we implement several state-of-the-art models, including DMBERT (Wang et al., 2019), CLEVE (Wang et al., 2021b), BERT+CRF (Wang et al., 2020), EEQA (Li et al., 2020), Text2Event (Lu et al., 2021), and PAIE (Ma et al., 2022). These methods cover all the mainstream EAE modeling paradigms (Peng et al., 2023c). Their detailed descriptions and implementations are introduced in appendix C.1. Recent works have achieved improvements in low-resource event extraction settings (Liu et al., 2023a; Ma et al., 2023). Considering our experiments prioritize understanding the challenge of large-scale MAVEN-ARG over conducting comprehensive evaluations for existing models, we choose not to include those low-resource methods.\\n\\nWe also evaluate large language models (LLMs) with in-context learning on MAVEN-ARG. Specifically, we select two advanced LLMs, GPT-3.5 (OpenAI, 2022) and GPT-4 (OpenAI, 2023), and evaluate them with 2-shot in-context learning. Here 2-shot means using full annotations of two documents as demonstrations. Considering time and cost constraints, we sample 50 documents from the test set for experimentation. We employ the gold trigger evaluation approach (Peng et al., 2023c) to directly assess their EAE performance.\\n\\nEvaluation Metric\\n\\nConsidering that MAVEN-ARG covers non-entity argument annotations, traditional evaluation metrics (Peng et al., 2023c) designed only for entity arguments are no longer applicable. By taking each argument role as a question to the document, we propose to view EAE as a multi-answer question answering task and adopt its evaluation metrics (Rajpurkar et al., 2016; Amouyal et al., 2022; Yao et al., 2023), including bag-of-words F1 and exact match (EM).\\n\\nConventional evaluation calculates the micro average over all the entity and event mentions, which we dub it as mention-level evaluation. Considering that real-world applications only require the accurate prediction for one of all the coreferent mentions, we propose to consider entity (Li et al., 2021) and event coreference in evaluation. Specifically, for entity coreference level evaluation, an entity argument is considered as predicted correctly if one of its mentions is predicted correctly. For event coreference level evaluation, an argument is considered as predicted correctly if it is predicted correctly for one of the coreferent triggers.\\n\\n2 A single role may correspond to multiple argument spans (answers).\"}"}
{"id": "acl-2024-long-224", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2 Experiment Results of Fine-tuned Models\\n\\nThe results of fine-tuned EAE models are shown in Table 3, and we have the following observations:\\n\\n(1) Existing state-of-the-art EAE models exhibit moderate performance on MAVEN-ARG, which is significantly worse than their results on existing datasets (Peng et al., 2023c). This indicates that MAVEN-ARG is challenging and there is a need for increased efforts in developing practical event understanding models.\\n\\n(2) The BERT+CRF and PAIE models exhibit the best performance, potentially attributable to their ability to model rich interactions between different event arguments.\\n\\n(3) The previous top-performing classification-based models (DMBERT and CLEVE) (Peng et al., 2023c) perform poorly on MAVEN-ARG, which is due to their inability to handle non-entity arguments. Therefore, future research necessitates more flexible approaches to tackle the complex and real-world scenario in MAVEN-ARG.\\n\\n(4) Text2Event notably underperforms. This is potentially due to the intensive annotations of MAVEN-ARG, i.e., a high volume of events and argument annotations within a single document, making generating all events and arguments at once difficult. It indicates that generating complex structured outputs remains a major challenge for generation models (Peng et al., 2023a), requiring further exploration.\\n\\n4.3 Experiment Results of LLMs\\n\\nThe results of LLMs with in-context learning are presented in Table 4, revealing that while LLMs with in-context learning are competitive compared to some fine-tuned EAE models, they still fall significantly short of the state-of-the-art. This is consistent with previous findings, suggesting that existing LLMs with in-context learning perform notably worse on specification-heavy information extraction tasks (Peng et al., 2023a; Li et al., 2023; Han et al., 2023). The LLMs' bag-of-words F1 scores are notably higher than their exact match scores, suggesting that the LLMs' predictions tend to be free-format and do not strictly match human annotations (Han et al., 2023).\\n\\nOne possible reason for the suboptimal performance is that LLMs cannot easily understand the schema from their names. Therefore, we conduct experiments with more informative prompts by incorporating definitions for each used argument role into the prompt, which are high-quality instructions used for guiding human annotators during data annotation. The results of these enhanced prompts are also shown in Table 4 (w/ definition). There is an obvious but marginal improvement after adding definitions, possibly due to the LLMs' limitations in understanding long contexts (Shaham et al., 2022; Peng et al., 2023a; Liu et al., 2023b).\\n\\n4.4 Analysis on Trigger-Argument Distance\\n\\nAs shown in Figure 3, MAVEN-ARG provides document-level annotations, covering data with varying trigger-argument distances. We conduct an analytical experiment on the impact of trigger-argument distance to model performance. Specifically, we break down the predictions and annotations according to their positions within the documents.\"}"}
{"id": "acl-2024-long-224", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Experimental results (%) of LLMs with 2-shot in-context learning on MAVEN-A RG.\\n\\n| Model      | Entity Coref Level | Event Coref Level |\\n|------------|--------------------|-------------------|\\n| GPT-3.5    | 21.3               | 20.9              |\\n|            | 19.9               | 14.3              |\\n|            | 14.3               | 14.3              |\\n| GPT-4      | 25.6               | 27.2              |\\n|            | 25.1               | 25.1              |\\n\\nTable 5: Mention-level results (%) of EAE models on entity and non-entity arguments. Classification-based models, e.g., DMBERT and CLEVE, are not applicable to non-entity arguments.\\n\\n4.5 Analysis on Entity and Non-Entity Arguments\\n\\nMAVEN-A RG provides comprehensive annotations, including both entity and non-entity arguments. We analyze the performance breakdown of investigated EAE models on these two types of arguments. The results are presented in Table 5, which reveals that EAE models generally perform better on non-entity arguments. The possible reason may be that there are more non-entity arguments in MAVEN-A RG and non-entity arguments are often presented in a looser form, making it easier for the models to learn the patterns and extract them. An exception is observed for the generation-based model Text2Event, which exhibits poorer performance on non-entity arguments. This may be because non-entity arguments are typically longer, which are harder to generate at once. It suggests that further exploration is needed to investigate how to effectively handle EAE with generation methods.\\n\\n4.6 Error Analysis\\n\\nTable 3 shows that all the baselines can only achieve moderate performance on MAVEN-A RG, which demonstrates the challenge of our new dataset. To better understand the challenge of MAVEN-A RG and provide insights for future improvements, we conduct an error analysis for the top-performing PAIE (Ma et al., 2022) model. One of the authors manually checked the erroneous predictions on the arguments of 50 randomly sampled event triggers and categorized the errors into four categories: (1) False Negative (52.2%), which means PAIE falsely ignores some event arguments; (2) False Positive (12.5%), which means the model falsely identifies non-argument entities/spans as event arguments; (3) Span Error (35.3%), which means the model partially corrects predicts an event argument but fails to precisely predict its exact textual span; (4) Classification Error (0.02%), which means the model correctly predicts the span of an event argument but misclassifies its argument role. We can see that similar to MAVEN (Wang et al., 2020), the majority of errors are still identification errors, which means that identifying event arguments from numerous entities/spans is much more challenging than determining specific argument roles.\\n\\n5 Future Event Prediction Demonstration\\n\\nMAVEN-A RG, in conjunction with MAVEN and MAVEN-E RE, creates the first all-in-one event understanding benchmark, which covers the full process of ED, EAE, and ERE. Beyond serving as an evaluation benchmark for these tasks, an all-in-one event dataset naturally enables a variety of\"}"}
{"id": "acl-2024-long-224", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Future event prediction results (%), averaged over 2 evaluators and 3 prompts. Reasonable denotes the rate of predictions judged as reasonable to happen next. Matched denotes the rate of predictions matched with the actual future events.\\n\\nEvent-based applications, especially considering the recent advances brought by LLMs. Here we preliminarily explore an application case, future event prediction, as a demonstration. Predicting future events based on causality can help decision-making, which is of self-evident importance. Therefore, since the early script learning (Schank and Abelson, 1975; Mooney and DeJong, 1985), future event prediction has continually attracted research interest (Chambers and Jurafsky, 2008; Jans et al., 2012; Granroth-Wilding and Clark, 2016; Hu et al., 2017; Chaturvedi et al., 2017; Li et al., 2018b; Lee and Goldwasser, 2019; Zhao, 2021). However, due to the lack of high-quality event resources, the evaluation of future event prediction often compromises by merely predicting verbs and subjects (Chambers et al., 2014), predicting according to textual order (Jans et al., 2012), or selecting story endings (Mostafazadeh et al., 2016a; Chaturvedi et al., 2017). The MAVEN series of datasets, with annotations of complete event structures and rich causal relations, may aid in predicting future events in real-world scenarios.\\n\\n**Experiment Setup**\\n\\nWe sample 100 event chains, each consisting of 3\u20135 events, from the training and validation sets. In each chain, preceding events cause the subsequent ones. Events are described in a structured JSON format, containing event type, event trigger, and event arguments. For each event chain, we hold out the last event and input the remaining incomplete chain into two proprietary LLMs, GPT-3.5 and GPT-4 (OpenAI, 2023), requiring them to predict the next occurring event. These LLMs are prompted with detailed task instructions and 5 demonstration event chains. To minimize the influence of the demonstrations, predictions are made independently three times under different demonstrations. More experimental details are shown in appendix D. We employ manual evaluation, with two experts engaged to judge (1) whether the prediction is reasonable, and (2) whether the prediction matches the actual future event.\\n\\n**Experimental Results**\\n\\nExperimental results are shown in Table 6. From these, we can see that the powerful LLMs can produce highly reasonable event predictions. However, their predictions seldom align with the actual future, making them not directly helpful. These observations suggest that using LLMs for future event prediction is promising, but there remain topics to explore on how to build practical future event prediction systems with LLMs. For instance, using retrieval-augmented methods may help LLMs access more timely evidence when making future predictions. As a preliminary attempt, the experiments demonstrate how our all-in-one event understanding dataset can assist in conveniently building and evaluating event-based applications. We hope that future works can explore using the MAVEN series datasets to build diverse applications.\\n\\n**6 Related Work**\\n\\n**Event Argument Extraction Datasets**\\n\\nSince the early MUC datasets (Grishman and Sundheim, 1996), event argument extraction (EAE) as a part of event extraction has received widespread attention. To reduce task complexity and provide standardized extraction results, the ACE datasets (Doddington et al., 2004) are designed with a schema covering 33 event types, limiting event argument annotation to entities within the same sentence as the trigger. ACE 2005 (Walker et al., 2006) has been the most widely used dataset for a long time, and the practice of ACE has been broadly adopted. Rich ERE (Song et al., 2015) expands ACE schema to 38 event types and constructs the TAC KBP datasets (Ellis et al., 2014, 2015, 2016; Getman et al., 2017). MEE (Pouran Ben Veyseh et al., 2022) follows the ACE schema to build a multilingual dataset. With the advancement of NLP methods, some works break some of the constraints of ACE task definition to construct more practical datasets. RAMS (Ebner et al., 2020), WikiEvents (Li et al., 2021), and DocEE (Tong et al., 2022) extends the annotation scope to the whole documents. However, RAMS and DocEE only annotate one topic event per document, ignoring fine-grained events within documents. MA VEN (Wang et al., 2020) and GENEVA (Parekh et al., 2023) both construct high-coverage general event schemata with over 100 event types. MA VEN supports only event...\"}"}
{"id": "acl-2024-long-224", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"detection. GENEVA extends event arguments to cover non-entity spans but focuses on testing the generalizability rather than developing practical EAE methods. Its data are repurposed from FrameNet (Baker et al., 1998) examples, which are individual sentences without document structure.\\n\\nMAVEN-ARG meticulously designs 612 unique argument roles for MAVEN schema and conducts large-scale exhaustive annotation, which annotates both entity and non-entity arguments for fine-grained events at the document level.\\n\\nEvent Argument Extraction Methods\\n\\nTraditional EAE methods primarily involve (1) Classification-based methods (Chen et al., 2015a, 2017; Sha et al., 2018; Wadden et al., 2019; Wang et al., 2019; Lin et al., 2020; Wang et al., 2021b; Zhou and Mao, 2022): employing text encoders like CNN (Krizhevsky et al., 2012) and BERT (Devlin et al., 2019), followed by an information aggregator, such as dynamic multi-pooling mechanism (Chen et al., 2015a), to obtain role-specific representations for classification. (2) Sequence labeling methods (Nguyen et al., 2016; Yang and Mitchell, 2017; Nguyen et al., 2021; Peng et al., 2023c): mainly adopting the conditional random field (CRF) (Lafferty et al., 2001) as the output layer to model structured dependencies between different arguments. Recently, increasing attention has been paid to transforming EAE into a question-answering task, transferring question-answering capabilities to boost EAE (Liu et al., 2020; Du and Cardie, 2020; Li et al., 2020; Ma et al., 2022; Lu et al., 2023; Nguyen et al., 2023). Additionally, some research focuses on using generation models to directly generate structured outputs containing events and their arguments (Lu et al., 2021; Li et al., 2021; Lu et al., 2022; Ren et al., 2023; You et al., 2022, 2023; Hsu et al., 2022, 2023; Zhang et al., 2023; Ren et al., 2023; Liu et al., 2023a; Ma et al., 2023), which has been becoming increasingly important with the advance of large language models.\\n\\nConclusion and Future Work\\n\\nWe introduce MAVEN-ARG, an event argument extraction dataset with comprehensive schema, large data scale, and exhaustive annotation. Experiments indicate that MAVEN-ARG is quite challenging for both fine-tuned EAE models and proprietary large language models. Together with MAVEN and MAVEN-E RE, MAVEN-ARG completes an all-in-one dataset covering the entire process of event understanding. An application case of future event prediction demonstrates how an all-in-one dataset can enable broad event-based applications. In the future, we will explore constructing multilingual resources under this framework and developing practical EAE methods with MAVEN-ARG.\\n\\nAcknowledgements\\n\\nThis work is supported by a grant from the Institute for Guo Qiang, Tsinghua University (2019GQB0003). We thank all the annotators for their efforts and the anonymous reviewers for their valuable comments.\\n\\nLimitations\\n\\n(1) MAVEN-ARG currently includes only English corpus, which limits its potential applications and coverage for diverse linguistic phenomena. In future work, we will try to support more languages under our framework and we also encourage community efforts in developing multilingual event understanding benchmarks. (2) MAVEN-A RG, along with MAVEN (Wang et al., 2020) and MAVEN-E RE (Wang et al., 2022), exclusively supports mainstream event understanding tasks. However, these datasets do not cover more broad event-related tasks such as event factuality identification (Qian et al., 2019, 2022) and event salience identification (Liu et al., 2018). We encourage future explorations in building more challenging and diverse tasks and applications on top of MAVEN data. (3) While previous research has found that LLMs perform poorly on specification-heavy tasks (Peng et al., 2023c; Han et al., 2023; Li et al., 2023) including the EAE task, there is no in-depth exploration of effective LLM-based approaches addressing the EAE task in this paper. We leave the exploration of how to better leverage LLMs for EAE tasks in future work.\\n\\nEthical Considerations\\n\\nIn this section, we discuss the ethical considerations of this work: (1) Intellectual property. The MAVEN dataset is released under the CC BY-SA 4.0 license 3. The MAVEN-E RE is shared under GPLv3 4 license and the original Wikipedia corpus is shared under the CC BY-SA 3.0 license 5.\\n\\n3 https://creativecommons.org/licenses/by-sa/4.0/\\n4 https://www.gnu.org/licenses/gpl-3.0.html\\n5 https://creativecommons.org/licenses/by-sa/3.0/\"}"}
{"id": "acl-2024-long-224", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The usage of these data in this work strictly adheres to the corresponding licenses and intended use. (2) Intended use.\\n\\nMAVEN-ARG is an event argument extraction dataset. Researchers and practitioners can utilize MAVEN-ARG to train and evaluate models for event argument extraction, thereby advancing the field of event understanding. (3) Potential risk control.\\n\\nMAVEN-ARG is constructed based on publicly available data. We believe that the underlying public data has been adequately desensitized and anonymized. The event argument annotation does not involve judgments about social issues and thus we believe MAVEN-ARG will not involve additional risks. To avoid unfair comparisons caused by mismatched evaluation implementations (Peng et al., 2023c) and potential cheating behaviors, the event argument annotations of MAVEN-ARG test set will not be publicly released. Instead, following previous works (Rajpurkar et al., 2016; Wang et al., 2020, 2022), we will maintain an online judgment system with a leaderboard, allowing users to submit predictions and obtain evaluation results. (4)\\n\\nWorker Treatments are discussed in appendix A.2.\\n\\nReferences\\nJacqueline Aguilar, Charley Beller, Paul McNamee, Benjamin Van Durme, Stephanie Strassel, Zhiyi Song, and Joe Ellis. 2014. A comparison of the events and relations across ACE, ERE, TAC-KBP, and FrameNet annotation standards. In Proceedings of the Second Workshop on EVENTS: Definition, Detection, Coreference, and Representation, pages 45\u201353.\\n\\nSamuel Joseph Amouyal, Ohad Rubin, Ori Yoran, Tomer Wolfson, Jonathan Herzig, and Jonathan Berant. 2022. QAMPARI: An open-domain question answering benchmark for questions with many answers from multiple paragraphs. CoRR, abs/2205.12665.\\n\\nCollin F. Baker, Charles J. Fillmore, and John B. Lowe. 1998. The Berkeley FrameNet project. In Proceedings of ACL-COLING, pages 86\u201390.\\n\\nLaura Banarescu, Claire Bonial, Shu Cai, Madalina Georgescu, Kira Griffitt, Ulf Hermjakob, Kevin Knight, Philipp Koehn, Martha Palmer, and Nathan Schneider. 2013. Abstract Meaning Representation for sembanking. In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse, pages 178\u2013186.\\n\\nTommaso Caselli and Piek Vossen. 2017. The event StoryLine corpus: A new benchmark for causal and temporal relation extraction. In Proceedings of the Events and Stories in the News Workshop, pages 77\u201386.\\n\\nNathanael Chambers, Taylor Cassidy, Bill McDowell, and Steven Bethard. 2014. Dense event ordering with a multi-pass architecture. Transactions of the Association for Computational Linguistics, 2:273\u2013284.\\n\\nNathanael Chambers and Dan Jurafsky. 2008. Unsupervised learning of narrative event chains. In Proceedings of ACL-HLT, pages 789\u2013797.\\n\\nSnigdha Chaturvedi, Haoruo Peng, and Dan Roth. 2017. Story comprehension for predicting what happens next. In Proceedings of EMNLP, pages 1603\u20131614.\\n\\nYubo Chen, Shulin Liu, Xiang Zhang, Kang Liu, and Jun Zhao. 2017. Automatically Labeled Data Generation for Large Scale Event Extraction. In Proceedings of ACL, pages 409\u2013419.\\n\\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao. 2015a. Event extraction via dynamic multi-pooling convolutional neural networks. In Proceedings of ACL-IJCNLP, pages 167\u2013176.\\n\\nYubo Chen, Liheng Xu, Kang Liu, Daojian Zeng, and Jun Zhao. 2015b. Event extraction via dynamic multi-pooling convolutional neural networks. In Proceedings of ACL, pages 167\u2013176.\\n\\nAgata Cybulska and Piek Vossen. 2014. Using a sledgehammer to crack a nut? lexical diversity and event coreference resolution. In Proceedings of LREC, pages 4545\u20134552.\\n\\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.\\n\\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang, Xu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan Liu. 2021. Few-NERD: A few-shot named entity recognition dataset. In Proceedings of ACL, pages 3198\u20133213.\\n\\nXiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. 2015. Deep learning for event-driven stock prediction. In Proceedings of IJCAI.\\n\\nGeorge Doddington, Alexis Mitchell, Mark Przybocki, Lance Ramshaw, Stephanie Strassel, and Ralph Weischedel. 2004. The automatic content extraction (ACE) program \u2013 tasks, data, and evaluation. In Proceedings of LREC.\\n\\nXinya Du and Claire Cardie. 2020. Event extraction by answering (almost) natural questions. In Proceedings of EMNLP, pages 671\u2013683.\\n\\nSeth Ebner, Patrick Xia, Ryan Culkin, Kyle Rawlins, and Benjamin Van Durme. 2020. Multi-sentence argument linking. In Proceedings of ACL, pages 8057\u20138077.\"}"}
{"id": "acl-2024-long-224", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-224", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fayuan Li, Weihua Peng, Yuguang Chen, Quan Wang, Lu Pan, Yajuan Lyu, and Yong Zhu. 2020. Event extraction as multi-turn question answering. In Findings of EMNLP, pages 829\u2013838.\\n\\nSha Li, Heng Ji, and Jiawei Han. 2021. Document-level event argument extraction by conditional generation. In Proceedings of NAACL-HLT, pages 894\u2013908.\\n\\nZhongyang Li, Xiao Ding, and Ting Liu. 2018a. Constructing narrative event evolutionary graph for script event prediction. In Proceedings of IJCAI, pages 4201\u20134207.\\n\\nZhongyang Li, Xiao Ding, and Ting Liu. 2018b. Constructing narrative event evolutionary graph for script event prediction. In Proceedings of IJCAI.\\n\\nYing Lin, Heng Ji, Fei Huang, and Lingfei Wu. 2020. A joint neural model for information extraction with global features. In Proceedings of ACL, pages 7999\u20138009.\\n\\nJian Liu, Yubo Chen, Kang Liu, Wei Bi, and Xiaojiang Liu. 2020. Event extraction as machine reading comprehension. In Proceedings of EMNLP, pages 1641\u20131651.\\n\\nJian Liu, Chen Liang, Jinan Xu, Haoyan Liu, and Zhe Zhao. 2023a. Document-level event argument extraction with a chain reasoning paradigm. In Proceedings of ACL, pages 9570\u20139583.\\n\\nNelson F Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, and Percy Liang. 2023b. Lost in the middle: How language models use long contexts. arXiv preprint arXiv:2307.03172.\\n\\nZhengzhong Liu, Chenyan Xiong, Teruko Mitamura, and Eduard Hovy. 2018. Automatic event salience identification. In Proceedings of EMNLP, pages 1226\u20131236.\\n\\nDi Lu, Shihao Ran, Joel Tetreault, and Alejandro Jaimes. 2023. Event extraction as question generation and answering. In Proceedings of ACL, pages 1666\u20131688.\\n\\nYaojie Lu, Hongyu Lin, Jin Xu, Xianpei Han, Jialong Tang, Annan Li, Le Sun, Meng Liao, and Shaoyi Chen. 2021. Text2Event: Controllable sequence-to-structure generation for end-to-end event extraction. In Proceedings of ACL-IJCNLP, pages 2795\u20132806.\\n\\nYaojie Lu, Qing Liu, Dai Dai, Xinyan Xiao, Hongyu Lin, Xianpei Han, Le Sun, and Hua Wu. 2022. Unified structure generation for universal information extraction. In Proceedings of ACL, pages 5755\u20135772.\\n\\nMingyu Derek Ma, Jiao Sun, Mu Yang, Kung-Hsiang Huang, Nuan Wen, Shikhar Singh, Rujun Han, and Nanyun Peng. 2021. EventPlus: A temporal event understanding pipeline. In Proceedings of NAACL: Demonstrations, pages 56\u201365.\\n\\nYubo Ma, Yixin Cao, Yong Hong, and Aixin Sun. 2023. Large language model is not a good few-shot information extractor, but a good reranker for hard samples! In Findings of EMNLP, pages 10572\u201310601.\\n\\nYubo Ma, Zehao Wang, Yixin Cao, Mukai Li, Meiqi Chen, Kun Wang, and Jing Shao. 2022. Prompt for extraction? PAIE: Prompting argument interaction for event argument extraction. In Proceedings of ACL, pages 6759\u20136774.\\n\\nGeorge A Miller and Philip N Johnson-Laird. 2013. Language and perception. In Language and Perception. Harvard University Press.\\n\\nParamita Mirza, Rachele Sprugnoli, Sara Tonelli, and Manuela Speranza. 2014. Annotating causality in the TempEval-3 corpus. In Proceedings of the EACL 2014 Workshop on Computational Approaches to Causality in Language (CAtoCL), pages 10\u201319.\\n\\nRaymond J Mooney and Gerald DeJong. 1985. Learning schemata for natural language processing. In Proceedings of IJCAI, pages 681\u2013687.\\n\\nNasrin Mostafazadeh, Nathanael Chambers, Xiaodong He, Devi Parikh, Dhruv Batra, Lucy Vanderwende, Pushmeet Kohli, and James Allen. 2016a. A corpus and cloze evaluation for deeper understanding of commonsense stories. In Proceedings of NAACL-HLT, pages 839\u2013849.\\n\\nNasrin Mostafazadeh, Alyson Grealish, Nathanael Chambers, James Allen, and Lucy Vanderwende. 2016b. CaTeRS: Causal and temporal relation scheme for semantic annotation of event structures. In Proceedings of the Fourth Workshop on Events, pages 51\u201361.\\n\\nChien Nguyen, Hieu Man, and Thien Nguyen. 2023. Contextualized soft prompts for extraction of event arguments. In Findings of ACL 2023, pages 4352\u20134361.\\n\\nMinh Van Nguyen, Tuan Ngo Nguyen, Bonan Min, and Thien Huu Nguyen. 2021. Crosslingual transfer learning for relation and event extraction via word category and class alignments. In Proceedings of EMNLP, pages 5414\u20135426.\\n\\nThien Huu Nguyen, Kyunghyun Cho, and Ralph Grishman. 2016. Joint event extraction via recurrent neural networks. In Proceedings of NAACL-HLT, pages 300\u2013309.\\n\\nQiang Ning, Hao Wu, and Dan Roth. 2018. A multi-axis annotation scheme for event temporal relations. In Proceedings of ACL, pages 1318\u20131328.\\n\\nOpenAI. 2022. Introducing ChatGPT.\\n\\nOpenAI. 2023. GPT-4 technical report. arXiv preprint arXiv:2303.08774.\"}"}
{"id": "acl-2024-long-224", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-224", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. 2020. Transformers: State-of-the-art natural language processing. In Proceedings of EMNLP: System Demonstrations, pages 38\u201345.\\n\\nBishan Yang and Tom M. Mitchell. 2017. Leveraging knowledge bases in lstms for improving machine reading. In Proceedings of ACL, pages 1436\u20131446.\\n\\nZijun Yao, Yantao Liu, Xin Lv, Shulin Cao, Jifan Yu, Juanzi Li, and Lei Hou. 2023. KoRC: Knowledge-oriented reading comprehension benchmark for deep text understanding. In Findings of the Association for Computational Linguistics: ACL 2023, pages 11689\u201311707.\\n\\nHuiling You, David Samuel, Samia Touileb, and Lilja \u00d8vrelid. 2022. EventGraph: Event extraction as semantic graph parsing. In Proceedings of the 5th Workshop on Challenges and Applications of Automated Extraction of Socio-political Events from Text (CASE), pages 7\u201315.\\n\\nHuiling You, Lilja \u00d8vrelid, and Samia Touileb. 2023. JSEEGraph: Joint structured event extraction as graph parsing. In Proceedings of the 12th Joint Conference on Lexical and Computational Semantics (*SEM 2023), pages 115\u2013127.\\n\\nKaihang Zhang, Kai Shuang, Xinyue Yang, Xuyang Yao, and Jinyu Guo. 2023. What is overlap knowledge in event argument extraction? APE: A cross-datasets transfer learning model for EAE. In Proceedings of ACL, pages 393\u2013409.\\n\\nLiang Zhao. 2021. Event prediction in the big data era: A systematic survey. ACM Comput. Surv., 54(5).\\n\\nHanzhang Zhou and Kezhi Mao. 2022. Document-level event argument extraction by leveraging redundant information and closed boundary loss. In Proceedings of NAACL-HLT, pages 3041\u20133052.\"}"}
{"id": "acl-2024-long-224", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendices\\n\\nA Data Collection Details\\n\\nA.1 Annotation Instruction\\n\\nAs introduced in \u00a7 2.1, we create a detailed event schema for both defining the task and instructing the annotators. We present the annotation instructions for the event type **Incident** in Table 7, including its argument schema and annotation examples. The overall event schema is released along with the dataset. To support the highly customized annotation process designed for us, we developed a new online annotation platform. A screenshot for the annotation platform is shown in Figure 5 to help understand the annotation operations.\\n\\nA.2 Annotation Coordination\\n\\nWe employ annotators (including senior annotators and expert annotators) from multiple commercial data annotation companies. 61% of them are female and 39% of them are male. All annotators hold undergraduate degrees. Annotators for entity and event argument annotation have no overlap since we cooperated with different companies for the two annotation tasks. The experts involved in schema creation are invited by the authors through personal connections. All the workers are fairly paid with agreed salaries and workloads. All employment is under contract and in compliance with local regulations. The overall annotation cost, including annotating entities and event arguments as well as developing and maintaining annotation platforms, is about 85,000 USD.\\n\\nB Additional Data Statistics\\n\\nB.1 Data Split Statistics\\n\\nThe detailed statistics of different data splits of MAVEN-A are shown in Table 8.\\n\\nB.2 Differences with Predecessors\\n\\nMAVEN-ARG inherits the efforts of previous works MAVEN (Wang et al., 2020) and MAVEN-E (Wang et al., 2022). MAVEN supports the event detection task by annotating event triggers and event types, along with a preliminary version of event coreferences. MAVEN-E supports the event relation extraction task by annotating event coreference, temporal, causal, and hierarchical relations. MAVEN-ARG completes the all-in-one event understanding dataset by adding the annotations of event arguments, which supports the event argument extraction task. The construction of MAVEN-E and MAVEN-ARG involves fixing or ignoring the erroneous and ambiguous annotations of event triggers and coreference clusters in MAVEN, which results in minor statistical differences shown in Table 9.\\n\\nC EAE Experimental Details\\n\\nC.1 Fine-tuning Implementation Details\\n\\nHere we provide brief descriptions of the fine-tuning-based models involved in our experiments. (1) DMBERT (Wang et al., 2019) utilizes BERT (Devlin et al., 2019) as the text encoder and a dynamic multi-pooling mechanism (Chen et al., 2015b) on top of BERT to aggregate argument-specific features and map them onto the distribution in the label space. (2) CLEVE (Wang et al., 2021b) is an event-oriented pre-trained language model, which is pre-trained using contrastive pre-training objectives on large-scale unsupervised data and their semantic structures. (3) BERT+CRF (Wang et al., 2020) is a sequence labeling model, which leverages BERT as the backbone and the conditional random field (CRF) (Lafferty et al., 2001) as the output layer to model the structural dependencies of predictions. (4) EEQA (Li et al., 2020) is a span prediction model, which formulates event extraction as a question-answering task and outputs start and end positions to indicate triggers and arguments. (5) Text2Event (Lu et al., 2021) is a conditional generation model, which proposes a sequence-to-structure paradigm and generates structured outputs containing triggers and corresponding arguments with constrained decoding. (6) PAIE (Ma et al., 2022) adopts prompt tuning (Lester et al., 2021) to train two span selectors for each argument role in the provided prompt and conduct joint optimization to find optimal role-span assignments. We adopt the same backbones with their original papers for all EAE models in our experiments. We employ pipeline evaluation as suggested by Peng et al. (2023c). Specifically, for PAIE, we conduct EAE experiments based on the triggers predicted by CLEVE. For the other models, the EAE experiments are based on the triggers extracted by corresponding models.\"}"}
{"id": "acl-2024-long-224", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Event Arguments:\\n1. Participant: Entities involved in the accident (individuals, institutions, organizations, and even trains, ships, etc.). They can be the ones causing the accident or the ones affected by it. Similar to the combination of Agent and Patient in previous events, but due to the difficulty in distinguishing between Agent and Patient in accidents, they are uniformly labeled as Participants.\\n2. Location: The location or position where the incident occurred. If the incident involves multiple locations during the process, they should be marked separately.\\n3. Content: In general, only one annotation is needed, which accurately indicates the content and type of the accident.\\n4. Loss: The losses caused by accidents can include the number of deaths and injuries, property damage, and so on.\\n\\nAnnotation Examples:\\n1. British losses were confined to a single man wounded by an accident aboard \\\"Crescent\\\".\\n2. On 6 June 1982, during the Falkland's war, the British Royal Navy type 42 destroyer engaged and destroyed a British army gazelle helicopter, serial number \\\"XX377\\\", Participant + Loss, in a friendly fire incident, killing all four occupants.\\n\\nTable 7: Example annotation instructions for event type Incident. Different argument roles are denoted by different background colors.\\n\\n| Trigger | Arg. | Entity | Mention | Entity | Mention |\\n|---------|------|--------|---------|--------|---------|\\n| Headed  |      |        |         |        |         |\\n\\nFigure 5: Screenshot for the annotation platform. The trigger \\\"headed\\\" is selected for annotation (in the right panel) and entities are highlighted in green as the options for annotating event arguments.\\n\\nDataset #Doc. #Event #Trigger #Arg. #Entity #Entity\\n\\n| Train | 2 | 913 | 64 | 197 | 923 | 70 |\\n|-------|---|-----|----|-----|-----|----|\\n| Dev   | 710 | 15 | 64 | 197 | 923 | 70 |\\n| Test  | 857 | 18 | 64 | 197 | 923 | 70 |\\n\\nTable 8: Statistics of the data splits of MAVEN-ARG. \\\"Doc.\\\" is short for \\\"Document\\\" and \\\"Arg.\\\" is short for \\\"Argument\\\".\"}"}
{"id": "acl-2024-long-224", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 9: Statistical differences between MAVEN-A and predecessors in number of event triggers and coreference clusters.\\n\\nAll open-source models are downloaded from the HuggingFace Transformers community (Wolf et al., 2020). Each of our fine-tuning experiments is conducted only once, on Nvidia A100 GPUs, consuming approximately 800 GPU hours in total. The hyper-parameters of the model are set based on prior experience and references from previous papers (Lu et al., 2021; Ma et al., 2022; Peng et al., 2023b). All hyper-parameters are shown in Table 10.\\n\\nC.2 LLM Experimental Details\\n\\nWe access ChatGPT and GPT-4 through the official OpenAI interfaces, namely gpt-3.5-turbo and gpt-4, respectively. The API access period spans from October 1 to October 31, 2023. The decoding sampling temperature for both models is set to 0. An example of the prompt, input, output, and ground-truth of this experiment are presented in Table 11. Model outputs are automatically extracted and evaluated using the evaluation approach mentioned in \u00a7 4.1.\\n\\nD Event Prediction Experimental Details\\n\\nThe future event prediction experiments (\u00a7 5) were conducted in October and November, 2023. We use OpenAI API endpoints gpt-3.5-turbo and gpt-4 for GPT-3.5 and GPT-4 experiments, specifically. To ensure the consistency among different runs, we set temperature=0.0. Detailed instructions and example input and output are shown in Table 12.\\n\\nE More Experimental Results\\n\\nIn this section, we present more experimental results of using different proportions of training data for training (appendix E.1) and results on entity and non-entity arguments (appendix E.2).\\n\\nE.1 Analysis on Data Size\\n\\nThe data volume of MAVEN-ARG significantly exceeds that of commonly used datasets. To examine the benefits of increased data scale, we train models on training data of varying sizes and observe their performance on the original test set. The experimental results are shown in Figure 6, which demonstrates that more training data indeed enhances model performance and allows for a comprehensive comparison of different models. The extensive data of MAVEN-ARG make it feasible to train a large language model (LLM) for general event understanding, which we leave as future work. Table 13 shows the detailed experimental results, i.e., mention level, entity coreference level, and event coreference level.\\n\\nE.2 Entity and Non-Entity Arguments\\n\\nTable 14 presents the overall results on entity and non-entity arguments of MAVEN-ARG. The non-entity arguments do not have coreferential relationship with each other and hence there is no entity coreference level evaluation for them.\"}"}
{"id": "acl-2024-long-224", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 10: Hyper-parameters of fine-tuning EAE models on MAVEN-A RG. PAIE utilizes 10^6 gradient update steps to optimize the parameters.\\n\\n| Learning Rate         | Weight Decay | Batch Size | Epoch |\\n|-----------------------|--------------|------------|-------|\\n| 5 \u00d7 10^{-5}           | 1 \u00d7 10^{-5}  | 32         | 6     |\\n| 5 \u00d7 10^{-5}           | 5 \u00d7 10^{-5}  | 128        | 5     |\\n| 5 \u00d7 10^{-5}           | 5 \u00d7 10^{-5}  | 64         | 10    |\\n| 2 \u00d7 10^{-5}           | 1 \u00d7 10^{-5}  | 32         | 10    |\\n| 5 \u00d7 10^{-5}           | 5 \u00d7 10^{-5}  | 16         | 30    |\\n| 5 \u00d7 10^{-5}           | 1 \u00d7 10^{-5}  | 8          | 5     |\\n\\n**INP**ut:\\nThe February 2010 Australian <event>cyberattacks</event> were a series of denial-of-service attacks conducted by the Anonymous online community against the Australian government in response to proposed web censorship regulations. Operation Titstorm was the name given to the cyber attacks by the perpetrators. They resulted in lapses of access to government websites on 10 and 11 February 2010. This was accompanied by emails, faxes, and phone calls harassing government offices. The actual size of the attack and number of perpetrators involved is unknown but it was estimated that the number of systems involved ranged from the hundreds to the thousands. The amount of traffic caused disruption on multiple government websites.\\n\\nAustralian Telecommunications Minister Stephen Conroy proposed the regulations that would mainly filter sites with pornographic content. Various groups advocating uncensored access to the Internet, along with companies like Google and Yahoo!, object to the proposed filter. A spokesperson for Conroy said that the actions were not a legitimate form of protest and called it irresponsible. The attacks also drew criticism from other filter protest groups. The initial stage was followed by small in-person protests on 20 February that were called \\\"Project Freeweb.\\\" Roleset: [Agent, Location, Patient, Consequence]. The event trigger is: <event>cyberattacks</event>.\\n\\n**GPT-3.5 OUP**ut:\\nAnswer: Patient: the Australian government; Consequence: lapses of access to government websites; Agent: the Anonymous online community\\n\\n**GPT-4 OUP**ut:\\nAnswer: Agent: the Anonymous online community; Location: Australia; Patient: Australian government; Consequence: lapses of access to government websites on 10 and 11 February 2010; Location: Australian\\n\\n**ROUND-TRUTH**:\\nAgent: Anonymous online community; Patient: Australian government; Consequence: lapses of access to government websites on 10 and 11 February 2010; Location: Australian\\n\\nTable 11: Example prompt, input, output, and ground-truth for the LLMs experiments. Demonstrations are omitted.\\n\\n**PROMPT**: You need to complete the event chain prediction task. I will provide you with an event chain consisting of multiple events, where earlier events cause the occurrence of subsequent events. You need to predict what the next most likely event in this chain will be. Events will be provided in a structured JSON format, including event type, core event trigger words, and event elements. I will first give you 5 example chains: ... Now please predict the next event for the following event chain:\\n\\n**INP**ut:\\nEvent No.1: {\"event type\": \\\"Attack\\\", \\\"event trigger\\\": \\\\[\\n  \\\"attack\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"unknown assailant\\\"\\n\\\\], \\\"Patient\\\": \\\\[\\n  \\\"Boris Nemtsov\\\"\\n\\\\], \\\"Location\\\": \\\\[\\n  \\\"Bolshoy Moskvoretsky Bridge\\\"\\n\\\\]}}\\n\\nEvent No.2: {\"event type\": \\\"Use_firearm\\\", \\\"event trigger\\\": \\\\[\\n  \\\"fired\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"unknown assailant\\\"\\n\\\\], \\\"Patient\\\": \\\\[\\n  \\\"Nemtsov\\\"\\n\\\\], \\\"Location\\\": \\\\[\\n  \\\"Bolshoy Moskvoretsky Bridge\\\"\\n\\\\], \\\"Instrument\\\": \\\\[\\n  \\\"Makarov pistol\\\"\\n\\\\]}}\\n\\nEvent No.3: {\"event type\": \\\"Bodily_harm\\\", \\\"event trigger\\\": \\\\[\\n  \\\"hit\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"shots\\\"\\n\\\\], \\\"Patient\\\": \\\\[\\n  \\\"Boris Nemtsov\\\"\\n\\\\], \\\"Location\\\": \\\\[\\n  \\\"Bolshoy Moskvoretsky Bridge\\\"\\n\\\\]}}\\n\\n**GPT-3.5 OUP**ut:\\nBased on the provided event chains, the next most likely event in the given event chain is: Event No.4: {\"event type\": \\\"Criminal_investigation\\\", \\\"event trigger\\\": \\\\[\\n  \\\"indicted\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"unknown assailant\\\"\\n\\\\], \\\"Suspect\\\": \\\\[\\n  \\\"Boris Nemtsov\\\"\\n\\\\], \\\"Incident\\\": \\\\[\\n  \\\"shooting on Bolshoy Moskvoretsky Bridge\\\"\\n\\\\], \\\"Conclusion\\\": \\\\[\\n  \\\"investigation and charges\\\"\\n\\\\]}}\\n\\n**GPT-4 OUP**ut:\\nEvent No.4: {\"event type\": \\\"Death\\\", \\\"event trigger\\\": \\\\[\\n  \\\"died\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"Boris Nemtsov\\\"\\n\\\\], \\\"Cause\\\": \\\\[\\n  \\\"gunshot wounds\\\"\\n\\\\], \\\"Location\\\": \\\\[\\n  \\\"Bolshoy Moskvoretsky Bridge\\\"\\n\\\\]}}\\n\\n**ROUND-TRUTH**:\\nEvent No.4: {\"event type\": \\\"Death\\\", \\\"event trigger\\\": \\\\[\\n  \\\"died\\\"\\n\\\\], \\\"event argument\\\": {\\\"Agent\\\": \\\\[\\n  \\\"Boris Nemtsov\\\"\\n\\\\], \\\"Location\\\": \\\\[\\n  \\\"Bolshoy Moskvoretsky Bridge\\\"\\n\\\\]}}\"}"}
{"id": "acl-2024-long-224", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this experiment, we adopt the gold trigger evaluation approach (Peng et al., 2023c).\"}"}
{"id": "acl-2024-long-224", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model         | #Params | Mention Level | Entity Coref Level | Event Coref Level | Entity Argument | Non-Entity Argument |\\n|--------------|---------|---------------|--------------------|-------------------|----------------|--------------------|\\n| DMBERT       | 110     | M             | 19.7               | 19.7              | 19.7           | 12.5               |\\n| CLEVE        | 355     | M             | 22.1               | 22.1              | 22.1           | 13.2               |\\n| BERT+CRF     | 110     | M             | 18.6               | 18.5              | 18.5           | 12.3               |\\n| EEQA         | 110     | M             | 6.3                | 6.2               | 6.2            | 9.1                |\\n| Text2Event   | 770     | M             | 5.5                | 5.6               | 5.6            | 4.0                |\\n| PAIE         | 406     | M             | 20.4               | 20.5              | 20.3           | 21.0               |\\n\\n| Model         | #Params | Mention Level | Entity Coref Level | Event Coref Level | Entity Argument | Non-Entity Argument |\\n|--------------|---------|---------------|--------------------|-------------------|----------------|--------------------|\\n| BERT+CRF     | 110     | M             | 24.8               | \u2212                 | \u2212              | \u2212                  |\\n| EEQA         | 110     | M             | 18.9               | \u2212                 | \u2212              | \u2212                  |\\n| Text2Event   | 770     | M             | 1.7                | \u2212                 | \u2212              | \u2212                  |\\n| PAIE         | 406     | M             | 39.4               | \u2212                 | \u2212              | \u2212                  |\\n\\nTable 14: Experimental results (%) of existing state-of-the-art fine-tuned EAE models on entity and non-entity arguments of MAVEN-ARG. Classification-based models, e.g., DMBERT and CLEVE, are inapplicable to non-entity arguments.\"}"}
