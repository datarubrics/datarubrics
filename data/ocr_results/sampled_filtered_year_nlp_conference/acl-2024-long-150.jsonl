{"id": "acl-2024-long-150", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAlthough Large Language Models (LLMs) are showing impressive performance on a wide range of Natural Language Processing tasks, researchers have found that they still have limited ability to conduct induction. Recent works mainly adopt \\\"post processes\\\" paradigms to improve the performance of LLMs on induction (e.g., the hypothesis search & refinement methods), but their performance is still constrained by the inherent inductive capability of the LLMs. In this paper, we propose a novel framework, Induction through Deduction (ItD), to enable the LLMs to teach themselves induction through deduction. The ItD framework is composed of two main components: a Deductive Data Generation module to generate induction data and a Naive Bayesian Induction module to optimize the fine-tuning and decoding of LLMs. Our empirical results show case the effectiveness of ItD on two induction benchmarks, achieving relative performance improvement of 36% and 10% compared with previous state-of-the-art, respectively. Our ablation study verifies the effectiveness of two key modules of ItD. We also verify the effectiveness of ItD across different LLMs and deductors. The data and code of this paper can be found at https://github.com/forangel2014/ItD.\\n\\n1 Introduction\\n\\nInduction can take we humans from the observed to the unobserved (Sloman and Lagnado, 2005). The task of Induction aims to discover consistent transformations from a set of input-output pairs, where the transformations map the inputs to the outputs well (Wang et al., 2023). As shown in Figure 1, given the input-output pairs \\\\( \\\\{x_i, y_i\\\\}_{i=1}^n \\\\), the model needs to predict the latent transformation \\\\( f \\\\). For a detailed example, given the input \\\\([1, 2]\\\\) with the output \\\\([1]\\\\) and other input-output pairs, the tested model is supposed to figure out the transformation output the first element of the input list. The Induction task is an important task in Natural Language Processing (NLP) and the mastery of the induction ability is an important sign of intelligence (Peirce, 1868; Lake et al., 2017; Chollet, 2019).\\n\\nCurrently, humans have already mastered the capability of induction and have found thousands of laws from the physical world and human society. However, machine intelligence still struggles...\"}"}
{"id": "acl-2024-long-150", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hypothesis Search & Refinement\\n\\nItD\\n\\nInput\\n\\nInduction\\n\\nResult:... Results\\n\\n:0.1 :0.7... ...\\n\\nHypothesis\\n\\nRefinement\\n\\nWeak Inductor\\n\\npost processes\\n\\nDeductive\\n\\nData\\n\\nGeneration\\n\\nFigure 2: Comparison of ItD and Previous Methods.\\n\\nPrevious hypothesis search & refinement methods are essentially \\\"post processes\\\" to the raw induction results of LLMs, leaving LLMs as Weak Inductors. By contrast, ItD fine-tunes the LLMs and proposes a novel decoding algorithm to make them Strong Inductors.\\n\\nRecently, with the rapid development of Large Language Models (LLMs), many works have begun to adopt the LLMs to induce the transformations given the input-output observations of various tasks and express the induced transformations as rules (Yang et al., 2023; Sun et al., 2023; Zhu et al., 2023; Zhao et al., 2023), guidelines (Pang et al., 2023), instructions (Honovich et al., 2022), and codes (Alet et al., 2021; Wang et al., 2023). These methods take advantage of the interpretability and generalization ability of LLMs in solving the Induction task.\\n\\nHowever, recent research (Bang et al., 2023; Mitchell et al., 2023; Mirchandani et al., 2023; Gendron et al., 2023) have revealed that LLMs have inherently limited ability in induction. To tackle such a limitation, work like Hypothesis Search (Wang et al., 2023) proposes to select the generated hypotheses from LLMs by evaluating them on the observations, while another following work Iterative Hypothesis Refinement (Qiu et al., 2023) proposes to further refine them through LLMs based on the evaluating results on the observations. Nevertheless, as shown in Figure 2(a), these hypothesis search & refinement methods are essentially \\\"post processes\\\" to the directly induced hypotheses of LLMs. They still heavily rely on the inherent induction ability of LLMs which are Weak Inductors.\\n\\nEven though LLMs are limited in induction, recent work finds out that they possess much better capability in deduction (Bang et al., 2023; Tang et al., 2023). Different from induction, deduction aims to infer the correct output given the transformation and the input. Despite the distinction that induction associates multiple \\\\((x, y)\\\\) pairs with the latent transformation \\\\(f\\\\), whereas deduction links \\\\(x\\\\) and \\\\(f\\\\) to the resultant \\\\(y\\\\), both approaches fundamentally share the commonality of reasoning within the framework of input, output, and transformation \\\\((x, y, f)\\\\). Therefore, it motivates us to propose a novel framework ItD (Induction through Deduction), to enable the LLMs to teach themselves induction through deduction. Different from previous methods, ItD fine-tunes the LLMs on their deduced data to make them Strong Inductors, as shown in Figure 2(b).\\n\\nFor a given induction task, ItD first proposes Deductive Data Generation to leverage the deductive capability of the LLMs to generate a set of task data \\\\((x, y, f)\\\\), which is simple yet effective and does not rely on human annotations or any larger LLMs' assistance. The data will then be used to fine-tune the LLMs to obtain better inductive capability.\\n\\nHowever, it is non-trivial to utilize the deduced data. We find out that directly fine-tuning the LLMs with the IO prompt (\u00a72.2) used in the previous methods (Honovich et al., 2022; Wang et al., 2023; Qiu et al., 2023) cannot effectively leverage the observed samples (as shown in Figure 5). Thus, ItD further proposes Naive Bayesian Induction as a strategy to optimize the use of each sample. Moreover, we also observe performance gains with the increase in the number of samples using our approach. Specifically, this novel technique fine-tunes the LLM to predict \\\\(f\\\\) conditioned on single pair \\\\((x, y)\\\\) instead of \\\\(n\\\\) pairs \\\\((p(f|x, y))\\\\) instead of \\\\(n\\\\) pairs \\\\((p(f|\\\\{x_i, y_i\\\\}_{i=1}^n))\\\\). During the decoding phase, it utilizes the Naive Bayesian approach to equivalently infer the probability distribution of \\\\(f\\\\) under all \\\\(n\\\\) \\\\((x, y)\\\\) conditions \\\\((p(f|\\\\{x_i, y_i\\\\}_{i=1}^n))\\\\) with the probability distribution of \\\\(f\\\\) under a single \\\\((x, y)\\\\) condition \\\\((p(f|x, y))\\\\).\\n\\nWe conduct experiments on two different types of induction tasks for evaluation: Instruction Induction and List Function. Compared with previous methods, the experiment results show that ItD is...\"}"}
{"id": "acl-2024-long-150", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"superior to the existing methods in assisting LLMs\\nin induction, and both the Deductive Data Genera-\\ntion and the Naive Bayesian Induction components\\neffectively contribute to ItD. We also make discus-\\nsions to show that ItD can be effectively applied to\\ndifferent LLMs, and a more powerful deductor, e.g.\\nChatGPT, will further improve the performances of\\nItD. In summary, the major contributions of this\\npaper are as follows:\\n\\n\u2022 We propose a novel framework ItD to enable\\nthe LLMs to teach themselves induction through\\ndeduction.\\n\\n\u2022 We propose Deductive Data Generation to ef-\\neffectively leverage the deductive capability of\\nLLMs to generate task data, which is fully self-\\nsupervised and needs no human annotations or\\nany larger LLMs to assist.\\n\\n\u2022 We propose Naive Bayesian Induction to allow\\nLLMs to optimize the use of each observed sam-\\nple and be able to take advantage of the increase\\nin the number of observed samples.\\n\\n2 Preliminary\\n\\n2.1 Induction Task\\n\\nAs shown in Figure 1, induction aims to infer the\\nlatent transformation, $f$, from a few of observed\\nsamples, $\\\\{x_i, y_i\\\\}_{i=1}^n$, where $y_i = f(x_i)$.\\n\\nAn induction task $T$ will include multiple input-\\noutput data pairs $D = \\\\{x_i, y_i\\\\}_{i=1}^m$, and all\\n$(x_i, y_i)$ share the same latent ground truth transforma-\\ntion $f$. The complete task data $D$ of task $T$ is then split\\ninto an induction set $D_{in}$ and a deduction set $D_{de}$.\\nThe testing model is asked to first run the in-\\nduction process on $D_{in}$. $D_{in}$ is split into multi-\\nple batches, with each batch containing $n$ samples $\\\\{x_i, y_i\\\\}_{i=1}^n$. The batches will be fed into the model\\nsequentially. The testing model observes the input\\nbatches and induces the predicted transformation $f^\\\\ast$. All\\n$f^\\\\ast$ induced from $D_{in}$ will be collected for\\ndeduction.\\n\\nIn the deduction process, a shared Reasoner $R$ is used to execute all induced\\n$f^\\\\ast$ from different\\nmethods on $D_{de}$ for fairness. For all test samples $(x_{test}, y_{test})$ from $D_{de}$, the candidate $f^\\\\ast$ and test\\ninput $x_{test}$ will be fed into $R$ and then $R$ generates\\nthe prediction $y^\\\\ast$. Finally, we evaluate the metric\\nbetween $y_{test}$ and $y^\\\\ast$ and average it over\\n$f^\\\\ast$.\\n\\n2.2 IO Prompt\\n\\nAs the induction task offers the model\\n$n$ observed\\nsamples at a time, it is natural to organize the sam-\\nples into the IO (Input-Output) prompt as follows:\\n$x_1, y_1; x_2, y_2; \\\\ldots; x_n, y_n$, which is also widely used\\nby previous works (Honovich et al., 2022; Wang\\net al., 2023; Qiu et al., 2023). Note that we omit\\nthe instructions and other connection words in the\\nprompt above. For example, for the\\nInput in Fig-\\nure 1, the IO prompt can be:\\n\\nPlease figure out the transformation that transforms the following input\\nlists to the output lists: Input:[1,2], Output:[1],\\n......, Input:[5,1], Output:[5]. So the transforma-\\ntion is:\\n\\n3 Framework\\n\\nIn this section, we introduce ItD, a framework\\nfor empowering the induction capability of LLMs\\nthrough their own deduction capability. As shown\\nin Figure 3, ItD is composed of two modules:\\nDeductive Data Generation, and Naive Bayesian In-\\nduction. For a given induction task, Deductive Data\\nGeneration will first leverage the deductive capabil-\\nity of the LLMs to generate the task data. Then we\\npropose Naive Bayesian Induction to allow LLMs\\nto optimize the use of each observed sample, while\\ntaking advantage of the increase in the number of\\nobserved samples.\\n\\n3.1 Deductive Data Generation\\n\\nTo empower the induction ability of LLMs, a set of\\ntraining data $(x, y, f)$ is needed. Here we consider\\nsampling from their joint distribution $p(x, y, f)$.\\nAs we introduced in \u00a71, compared with induction\\n$p(f|x, y)$, LLMs are better at deduction\\n$p(y|f, x)$. Thus we propose the following derivation to lever-\\nage the LLMs to generate the task data in a deduc-\\ntive behavior.\\n\\n$$p(x, y, f) = p(y|x, f)p(x|f)p(f) \\\\tag{1}$$\\n\\nAs shown in Eq (1), to generate data $(x, y, f)$,\\nwe propose to sample $p(f)$, $p(x|f)$, and $p(y|x, f)$ sequen-\\ntially. The pipeline of Deductive Data Gen-\\neration is shown in the upper half of Figure 3.\\n\\n3.1.1 Sampling $p(f)$ through Initial Induction\\n\\nTo ensure that the generated data $(x, y, f)$ ap-\\nproximates the real task data distribution, we first need\\nto sample the transformation $f$ that approximates\\nthe ground truth transformation of the task. Thus,\\nwe let LLMs induce $f$ on the induction set $D_{in}$\\nin the sampling decoding mode with the IO prompt,\\nand we consider the induced $f$ as samples from the\\nprior distribution $p(f)$.\"}"}
{"id": "acl-2024-long-150", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fine-tuning: Translate the inputs into more formal language\\n\\nHypothesis Samples: Translate the inputs into Spanish\\n\\nNegate the input sentences\\n\\nDeduced Data: Reply quickly.\\n\\nPlease reply as soon as possible.\\n\\nAre you busy today?\\n\\n\u00bfEst\u00e1s ocupado hoy?\\n\\nThis apple tasted good.\\n\\nThis apple tasted bad.\\n\\n\u2026\\n\\nNaive Bayesian Induction\\n\\nLoRA\\n\\n\u2026\\n\\nInduction Result:\\n\\nOutput\\n\\nNext-token Log-probs (tokens)\\n\\nFigure 3: The framework of ItD. ItD includes two main parts, i.e. Deductive Data Generation and Naive Bayesian Induction. Given the induction set $D$ in \u00a73.1, ItD will first leverage the deductive capability of LLMs to generate data that closely resembles the distribution of the task data. Then Naive Bayesian Induction is adopted to optimize the use of each observed sample while achieving better performances with the increase in the number of samples.\\n\\n3.1.2 Deduction with In-Context Learning\\n\\nFor the $p(y|x, f)$ part in Eq (1), this paper leverages the deductive capability with In-Context Learning (ICL) of LLMs to generate samples $(x, y)$. We first manually create several cases of deduction as the few-shot demonstrations and then ask LLMs to generate samples $(x, y)$ for each $f$ (Figure 4).\\n\\nAs shown in Figure 4, the upper half is the fixed prompt and the content of the last instruction is replaced by each $f$ from \u00a73.1.1. In the lower half, the LLMs will follow the demonstrations to continuously first generate an input $x_i$ according to the instruction $f$, and then generate $y_i$ based on their deductive capability. We then parse the output text of LLMs to obtain the samples $\\\\{x_i, y_i\\\\}_{n_i=1}^n$. The deductive capabilities of LLMs will determine the extent to which $(x, y)$ satisfies the given $f$. For each $f$, we generate $n$ corresponding $(x, y)$ pairs for later tuning.\\n\\n3.2 Naive Bayesian Induction\\n\\nHaving obtained the generated task data, we propose Naive Bayesian Induction which incorporates tuning and decoding to empower the inductive capability of LLMs. The pipeline of Naive Bayesian Induction is shown in the lower half of Figure 3. Instead of the plain IO prompt (\u00a72.2), Naive Bayesian Induction proposes the Group Decoding (GD) prompt template as follows: $x, y$.\\n\\nCompared with the IO prompt, the GD prompt contains only one input-output pair $(x, y)$. By using the GD prompt in Naive Bayesian Induction, we allow LLMs to optimize the use of each observed sample ($p(f|x, y)$) and can take advantage of the increase in the number of observed samples. Naive Bayesian Induction further proposes Naive Bayesian Group Decoding, which enables us to equivalently infer the probability distribution of $f$ under all $n(x, y)$ conditions ($p(f|\\\\{x_i, y_i\\\\}_{n_i=1}^n)$) with the probability distribution of $f$ under a single $(x, y)$ condition ($p(f|x, y)$).\\n\\nSpecifically, the IO prompt and GD prompt fine-tune the LLM and decode with the LLMs according to the following distribution respectively.\\n\\n\u2022 IO prompt: $p_{LLM}(f|\\\\{x_i, y_i\\\\}_{n_i=1}^n)$\\n\\n\u2022 GD prompt: $p_{LLM}(f|x, y)$\\n\\n3.2.1 Fine-tuning on the Deduced Data\\n\\nFor the shared fine-tuning data collected in \u00a73.1, we organize them into the training data with IO prompt and GD prompt, respectively. Then, we adapt LoRA (Hu et al., 2021) and QLoRA (Dettmers et al., 2023) to fine-tune the original LLMs to gain a better capability of induction.\\n\\n3.2.2 Naive Bayesian Group Decoding\\n\\nFor the model trained with IO prompt, in the induction stage, we directly convert the $n$ observed sample from $D$ into the IO prompt, feed it into the model, and use beam search to decode the $f$. This method is denoted as ItD-IO.\\n\\nFor the model trained with GD prompt, ItD proposes the following Naive Bayesian Group Decoding.\"}"}
{"id": "acl-2024-long-150", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Deductive Data Generation\\n\\nPrompt Template\\nYou are a smart assistant, now please help me generate corresponding input-output pairs that satisfy the given instruction.\\n\\nInstruction: describe the major color of the given object.\\nInput: watermelon.\\nOutput: green.\\nInput: panda.\\nOutput: black and white.\\nInput: ocean.\\nOutput: blue.\\nInput: blood.\\nOutput: red.\\nInput: snow.\\nOutput: white.\\n\\nInstruction: answer the capital of the given country.\\nInput: USA.\\nOutput: Washington.\\nInput: China.\\nOutput: Beijing.\\nInput: Russia.\\nOutput: Moscow.\\nInput: France.\\nOutput: Paris.\\nInput: UK.\\nOutput: London.\\n\\nInstruction: \\\\{f\\\\} \\nOutput\\nInput: \\\\(x_1\\\\)\\nOutput: \\\\(y_1\\\\) \\nInput: \\\\(x_2\\\\) \\nOutput: \\\\(y_2\\\\) \\n......\\n\\nFigure 4: The prompt used for Deduction with In-context Learning. LLMs will generate multiple samples \\\\((x, y)\\\\) for each \\\\(f\\\\) in a deductive behavior.\\n\\nDeducing (NBGD) algorithm. NBGD allows us to equivalently infer the probability distribution of \\\\(f\\\\) under all \\\\(n\\\\) \\\\((x, y)\\\\) conditions \\\\((p(f|\\\\{x_i, y_i\\\\}_{n_i=1}))\\\\) with the probability distribution of \\\\(f\\\\) under a single \\\\((x, y)\\\\) condition \\\\((p(f|\\\\{x, y\\\\}))\\\\).\\n\\n\\\\[\\np(f|\\\\{x_i, y_i\\\\}_{n_i=1}) = p(\\\\{x_i, y_i\\\\}_{n_i=1}|f) p(f)\\\\]\\n\\n\\\\[\\n= p(\\\\{x_i, y_i\\\\}_{n_i=1}) \\\\propto p(f) - (n-1) \\\\prod_{i=1}^{n} p(f|\\\\{x, y\\\\})\\n\\\\]\\n\\nHere we assume that given the transformation \\\\(f\\\\), the input-output pairs \\\\((x, y)\\\\) are independent to each other, i.e. \\\\(p(\\\\{x_i, y_i\\\\}_{n_i=1}|f) = \\\\prod_{n_i=1}^{n} p(x_i, y_i|f).\\\\) This assumption is quite natural in the scene of induction, where each \\\\(y_i\\\\) is only determined by \\\\(f\\\\) and the corresponding \\\\(x_i\\\\).\\n\\nAs shown in Eq (2), we derive the probability \\\\(p(f|\\\\{x_i, y_i\\\\}_{n_i=1})\\\\) into two parts, i.e. the prior term \\\\(p(f)\\\\) and the product term \\\\(\\\\prod_{n_i=1}^{n} p(f|\\\\{x, y\\\\})\\\\) respectively. Suppose the text of \\\\(f\\\\) is a sentence \\\\(t = [t_1, t_2, ..., t_m]\\\\). For the \\\\(\\\\prod_{n_i=1}^{n} p(f|\\\\{x, y\\\\})\\\\), we modify the ordinary beam search decoding process as follows:\\n\\n\\\\[\\n\\\\sum_{i=1}^{n} \\\\log p(t|x_i, y_i) = \\\\sum_{i=1}^{n} \\\\sum_{j=1}^{m} \\\\log p(t_j|x_i, y_i, t_{<j})\\n\\\\]\\n\\n\\\\[\\n= \\\\sum_{j=1}^{m} \\\\sum_{i=1}^{n} \\\\log p(t_j|x_i, y_i, t_{<j})\\n\\\\]\\n\\n(3)\\n\\nAs shown in Eq (3) and Figure 3, in the induction stage, NBGD will first convert all samples \\\\((x_i, y_i)\\\\) into GD prompt, tokenize them and feed them into the LLMs in a batch. Then in each step of decoding \\\\((j)\\\\), the LLMs receive the already decoded part of transformation \\\\(t_{<j}\\\\), and every sample \\\\((x_i, y_i)\\\\) and generate the next-token scores (log-probabilities) for \\\\(t_j\\\\). Then we will add up the next-token scores from all the samples \\\\((i)\\\\). Like ordinary beam search, in each step \\\\(j\\\\), we will maintain the top-k beams with the largest beam scores.\\n\\nAfter NBGD decodes the top-k \\\\(f\\\\), we finally re-rank them through the prior term \\\\(p(f) - (n-1)\\\\). In the log scale, we only need to calculate the log probabilities \\\\(\\\\log p(f)\\\\) with the same LLMs and add \\\\(- (n-1) \\\\log p(f)\\\\) to their beam scores. We consider this training & decoding method as the complete method of our framework, denoted as ItD.\\n\\n4 Experiments\\n\\n4.1 Dataset and Setups\\nWe use two datasets to test the inductive capability of LLMs on two types of induction tasks: common-sense inductive reasoning and symbolic inductive reasoning.\\n\\nFor commonsense inductive reasoning, we adapt the task Instruction Induction (Honovich et al., 2022). The input \\\\(x\\\\) and output \\\\(y\\\\) are two short sentences while the transformation \\\\(f\\\\) is an instruction. This dataset contains 24 sub-tasks. For symbolic inductive reasoning, we adapt the task List Function (Rule, 2020). The input \\\\(x\\\\) and output \\\\(y\\\\) are two\"}"}
{"id": "acl-2024-long-150", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The main results of our experiments and the Effectiveness of Deductive Data Generation and Naive Bayesian Induction. ItD is superior to all of the previous methods on both datasets, while both Deductive Data Generation and Naive Bayesian Induction effectively contribute to the performance of ItD.\\n\\nWe adopt ChatGPT as the Reasoner for both datasets and all tested methods (Note that the Reasoner will not participate in Deductive Data Generation but only execute the induced \\\\( f \\\\) for evaluation). The reported results are average execution scores (Honovich et al., 2022) over all sub-tasks. The detailed setups of the experiments can be found in Appendix A and the detailed results of each sub-task of all methods can be found in Appendix B.\\n\\n4.2 Baselines\\n\\nWe adopt the following baselines to compare with our proposed ItD:\\n\\n- **IO** (input-output, Honovich et al. 2022). This baseline is the plain prompt, i.e. directly splice the observations \\\\( x_1, y_1, x_2, y_2, \\\\ldots, x_n, y_n \\\\) as the IO prompt, and feed this prompt for LLMs to conduct induction.\\n\\n- **SC** (self-consistency, Wang et al. 2022). Based on the IO prompt, the SC method will sample \\\\( k \\\\) hypotheses and select the most consistent one by taking a majority vote.\\n\\n- **HS** (hypothesis search, Wang et al. 2023). Based on the IO prompt, the HS method will evaluate the generated hypotheses by applying the hypotheses to the observed samples. The deductive reasoning results will be used to filter out the most qualified hypothesis.\\n\\n- **HS&R** (hypothesis search & refinement, Qiu et al. 2023). After selecting the best hypothesis, this baseline allows LLMs to refine the hypothesis to a better one based on the execution results.\\n\\n4.3 Main Results\\n\\nWe first compare ItD with previous methods to see whether ItD trains the LLMs to become better inductors. For Instruction Induction, we adopt Llama-2-7b-chat as the LLM for all methods. For List Function, as List Function is a task of symbolic reasoning and LLMs are found poor at symbolic deduction than semantic deduction (Tang et al., 2023), we adopt a larger and more powerful LLM, Mixtral-8x7B, for all methods.\\n\\nAs shown in Table 1, ItD is significantly superior to all existing methods on both datasets, bringing relative performance improvement of 193% and 16% compared with the base model (IO), while bringing relative performance improvement of 35% and 10% compared with the previous SOTA (HS&R). These results verify that ItD is better than previous methods in empowering the inductive capability of LLMs.\\n\\n4.4 The Effectiveness of Deductive Data Generation\\n\\nTo verify the effectiveness of Deductive Data Generation, we here compare the ItD-IO version with the base model (IO). The only difference between these two models is that ItD-IO is fine-tuned with the same data generated by Deductive Data Generation but with the IO prompt.\\n\\nAs shown in Table 1, ItD-IO is superior to the base model on both datasets, bringing the relative performance improvement of 146% and 8%. These results indicate that Deductive Data Generation can produce effective fine-tuning data for the LLMs.\\n\\n4.5 The Effectiveness of Naive Bayesian Induction\\n\\nThe Naive Bayesian Induction allows us to optimize the use of each observed sample and to take advantage of the increase in the number of observed samples. To verify the effectiveness of Naive Bayesian Induction, we first compare the complete ItD with ItD-IO. As shown the Table 1, the model trained with complete ItD significantly outperforms ItD-IO on both datasets, indicating the effectiveness of Naive Bayesian Induction.\\n\\nMoreover, we conduct experiments to verify that Naive Bayesian Induction can benefit from the increase in the number of observed samples. While ItD-IO is tuned with 5 pairs of \\\\((x, y)\\\\) per batch, we...\"}"}
{"id": "acl-2024-long-150", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5: Naive Bayesian Induction can benefit from the increase in the number of observed samples.\\n\\nTable 2: ItD is effective for LLMs of different sizes. * denotes that models only use the ItD-IO version as we are not able to modify the decoding algorithms of these black-box LLMs. Human denotes the results that the Reasoner directly adopts the human-written references for evaluation.\\n\\nTable 3: Both ItD and baseline method HS can benefit from a more powerful deductor (ChatGPT, denoted as +D). Compared with conducting deduction by the tested model, both methods with ChatGPT helping in conducting deduction will have better performances.\\n\\n4.6 Discussion\\n4.6.1 The Effectiveness of ItD on Different Sizes of LLMs\\nTo verify whether ItD is effective with different sizes of LLMs, we adopt extra LLMs of different sizes for each task: For Instruction Induction, besides Llama-2-7b-chat, we adopt Llama-2-13b-chat, and ChatGPT for experiments. For the List Function, besides Mixtral-8x7B, we adopt ChatGPT for the experiments of this task. Note that for ChatGPT, as we are not able to modify the decoding algorithm during its inference time, we only apply the ItD-IO version for it. We use the official API for the fine-tuning and inference of the ChatGPT.\"}"}
{"id": "acl-2024-long-150", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of different methods on selected tasks.\\n\\n| Task                  | IO    | ItD    | ItD-OOD |\\n|-----------------------|-------|--------|---------|\\n| sum                   | 22.33 | 50.02  | 25.21   |\\n| translation_en-de     | 11.50 | 50.82  | 15.25   |\\n| antonyms              | 40.82 | 80.40  | 48.28   |\\n| first_word_letter     | 12.56 | 71.27  | 88.50   |\\n| sentiment             | 2.01  | 87.49  | 0.03    |\\n\\n4.6.2 A More Powerful Deductor Can Bring Further Improvements for ItD\\n\\nBoth HS and ItD need a deductor to improve the induction process. For HS, the deductor is used to search for the best-proposed hypothesis by evaluating them on the observed samples. For ItD, the deductor is used to deduce data for fine-tuning. In the experiments above, the deductor used in these two methods is both the tested model itself. However, here we would like to discuss whether a more powerful deductor will further improve these methods. So we adopt the Reasoner of the tasks, i.e. ChatGPT, as a more powerful deductor for these methods as the comparison.\\n\\nAs shown in Table 3, After being equipped with a more powerful deductor (denoted as +D), both HS and ItD gain performance improvements on both datasets, while ItD still consistently outperforms HS whatever the deductor is the base model or ChatGPT. These results further inform us that the more powerful the Deductor, the better it helps in training the Inductor.\\n\\n4.6.3 Held-out Task Generalization\\n\\nTo investigate the inductive capability of LLMs under the Instruction-tuning (ItD) framework on out-of-distribution (OOD) tasks, we selected 5 held-out tasks from the 24 sub-tasks in the Instruction Induction dataset: sum, translation_en-de, antonyms, first_word_letter, and sentiment. These tasks were drawn from the Spelling, Lexical Semantics, Numerical, Multilingual, and GLUE categories (categorized by Honovich et al., 2022), respectively.\\n\\nDuring the Deductive Data Generation stage, we masked the function \\\\( f \\\\) and its corresponding \\\\( x, y \\\\) pairs for these 5 held-out tasks, making them OOD tasks. We then fine-tuned the LLM using the \\\\( f, x, y \\\\) pairs generated from the remaining sub-tasks. This setting is referred to as ItD-OOD.\\n\\nThe results, as shown in Table 4, indicate that compared to the full ItD framework, ItD-OOD generally exhibits a significant performance drop on the held-out tasks. However, compared with the naive IO baseline, ItD-OOD shows improvements on most tasks, and even surpasses ItD on the first_word_letter sub-task. This suggests that the ItD framework has a certain degree of cross-task generalization capability, but the effectiveness of this generalization depends on the similarity between the transformations \\\\( f \\\\) of different sub-tasks.\\n\\n5 Related Work\\n\\n5.1 Capability of Induction of LLMs\\n\\nAlthough LLMs have shown great power in a large number of fields of NLP (Chen et al., 2024b,a; Li et al., 2024; Ling et al., 2023; Xu et al., 2024), it is shown by previous research that they are poor on induction. Mirchandani et al. 2023 and Gendron et al. 2023 found that LLMs are poor on abstract induction tasks like Abstraction and Reasoning Corpus (Chollet, 2019). Another research (Mitchell et al., 2023) found that even GPT-4 and GPT-4V are still not able to robustly form abstractions and reason in contexts not previously seen in their training data. However, Bang et al. 2023 and Tang et al. 2023 have made quantitative evaluations on LLMs and found that they are much better at deduction than induction. Inspired by the findings of these works, we propose a novel framework, ItD, to leverage the powerful deductive capability of LLMs to enhance their inductive capability.\\n\\n5.2 Memory-Oriented Induction\\n\\nLLMs have shown strong ability in reasoning in various down-stream tasks. However, they still struggle when it comes to an unfamiliar task. Thus, many previous works have designed a working memory to help LLMs store and use task-specific knowledge (Yang et al., 2023; Sun et al., 2023; Zhu et al., 2023; Zhao et al., 2023). The LLMs are prompted to induce task-specific knowledge in the form of facts or rules and store them in the memory during the induction stage. In the deductive reasoning stage, a retriever will be called to retrieve relevant knowledge about the current question from the memory and prompt it to the LLMs. For these applications, ItD is supposed to be a powerful framework for these methods to tune the LLMs to gain better inductive capability to further improve their...\"}"}
{"id": "acl-2024-long-150", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.3 Hypothesis Search and Refinement\\nSome previous works have proposed methods to improve the induced hypotheses of LLMs by conducting Hypothesis Search and Refinement. Hypothesis Search (Wang et al., 2023) proposes to implement the natural language hypothesis to the Python program and then execute them on the observed samples, the execution results are then used to filter out the better hypotheses. Based on Hypothesis Search, Iterative Hypothesis Refinement (Qiu et al., 2023) proposes to iteratively refine the hypothesis through LLMs based on the feedback of execution results. Compared with these methods, ItD improves the inherent inductive capability of LLMs by fine-tuning them with high-quality deduced data and producing a better induction algorithm.\\n\\n5.4 Naive Bayes-based Context Extension\\nNBCE (Su, 2023) is recently proposed as an effective method to extend the context for LLMs. It is proposed for the scenes of conducting QA with a batch of documents. However, the documents are likely to be coupled with others and thus cause NBCE poorly infer the answers. Compared with NBCE, Naive Bayesian Induction applies this derivation to the problem of induction, where the samples are conditionally independent of each other given in nature. Moreover, we involve the tuning process with GD prompt in ItD, which not only optimize the use of each observed sample but also take advantage of the increase in the number of samples.\\n\\n6 Conclusion\\nIn this paper, we propose a novel framework, ItD, to enable LLMs to teach themselves induction through deduction. We conduct a series of experiments on two types of induction datasets and verify that ItD is superior to existing methods in empowering the inductive capability of LLMs. Moreover, we verify the effectiveness of Deductive Data Generation and Naive Bayesian Induction. More experiment results support that ItD can be effectively applied to LLMs of different sizes, and a more powerful deductor can further improve the performance of ItD.\\n\\nLimitations\\nWith our ItD framework, we can improve both the symbolic deductive reasoning and semantic deductive reasoning tasks. However, constrained by the limited capability of LLMs in symbolic reasoning, the performance of ItD on List Function (a symbolic deductive task) is not as satisfying as it is on Instruction Induction (a semantic deductive task). Besides, our proposed Naive Bayesian Group Decoding is still categorized to greedy algorithms. It does not involve planning and may likely fall into local optima. We leave further exploration of these directions as future work.\\n\\nEthics Statement\\nThis paper proposes a method for LLMs to teach themselves induction through deduction. All experiments are conducted on publicly available datasets. Thus there is no data privacy concern. Meanwhile, this paper does not involve human annotations, and there are no related ethical concerns.\\n\\nAcknowledgements\\nThis work was supported by the National Key R&D Program of China (No.2022ZD0160503) and the National Natural Science Foundation of China (No.62376270) and OPPO Research Fund.\\n\\nReferences\\nFerran Alet, Javier Lopez-Contreras, James Koppe1, Maxwell Nye, Armando Solar-Lezama, Tomas Lozano-Perez, Leslie Kaelbling, and Joshua Tenenbaum. 2021. A large-scale benchmark for few-shot program induction and synthesis. In International Conference on Machine Learning, pages 175\u2013186. PMLR.\\n\\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wenliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei Ji, Tiezheng Yu, Willy Chung, et al. 2023. A multi-task, multilingual, multimodal evaluation of chatgpt on reasoning, hallucination, and interactivity. arXiv preprint arXiv:2302.04023.\\n\\nPei Chen, Boran Han, and Shuai Zhang. 2024a. Comm: Collaborative multi-agent, multi-reasoning-path prompting for complex problem solving.\\n\\nPei Chen, Soumajyoti Sarkar, Leonard Lausen, Balasubramaniam Srinivasan, Sheng Zha, Ruihong Huang, and George Karypis. 2024b. Hytrel: Hypergraph-enhanced tabular data representation learning. Advances in Neural Information Processing Systems, 36.\"}"}
{"id": "acl-2024-long-150", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2024-long-150", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Setups\\n\\nFor Instruction Induction, it contains 24 sub-tasks, with the induction set $D$ of each sub-task including 100 batches, each batch includes $n = 5$ pairs of $(x, y)$. The deduction set $D_{de}$ of each sub-task including 100 pairs of $(x, y)$ for testing. For List Function, it contains 250 sub-tasks, with the induction set $D_{in}$ of each sub-task including 3 batches, each batch includes $n = 5$ pairs of $(x, y)$. The deduction set $D_{de}$ of each sub-task including 17 pairs of $(x, y)$ for testing.\\n\\nInduction on both tasks is conducted in a zero-shot manner by LLMs. For both ordinary beam search and Naive Bayesian Group Decoding used during the induction phase, we adopt the beam size of 5. For Deductive Data Generation, we adopt top-p = 0.95 and temperature = 0.3 to sample 5 transformations $f$ from each batch of $D_{in}$. And then we generate 5 pairs of $(x, y)$ for each $f$. For both ItD and ItD-IO, we fine-tune them using the same data above, with a learning rate of 1e-4 and for 3 epochs. For Naive Bayesian Group Decoding, we create a patch for the utils.py in the transformer library, it can be easily installed and uninstalled using our scripts.\\n\\nThe prompts used in Induction (\u00a73.2.2) and Deduction with In-Context Learning (\u00a73.1.2) for Instruction Induction and List Function are shown in Table 5 and Table 6, respectively. Note that the text in the Induction part is shared by both the IO prompt and the GD prompt (for the IO prompt, $n > 1$, and for the GD prompt, $n = 1$).\\n\\nB Detailed Results\\n\\nThe detailed results of Instruction Induction are shown in Table 7. As the List Function contains 250 sub-tasks and we have 19 methods in all, the table of its detailed results will be too large for the paper. Instead, you can find it at https://github.com/forangel2014/ItD.\\n\\nDataset Instruction Induction\\n\\nInduction\\n\\nI gave a friend an instruction and an input. The friend read the instruction and wrote an output for the input. Here is the input-output pair:\\n\\nInput: $\\\\{x_1\\\\}$\\nOutput: $\\\\{y_1\\\\}$\\n\\n......\\n\\nInput: $\\\\{x_n\\\\}$\\nOutput: $\\\\{y_n\\\\}$\\n\\nThe instruction was\\n\\nDeduction\\n\\nYou are a smart assistant, now please help me generate corresponding input-output pairs that satisfy the given instruction. Do not repeat the instructions in the inputs.\\n\\ninstruction: describe the major color of the given object.\\nInput: watermelon.\\nOutput: green.\\nInput: panda.\\nOutput: black and white.\\nInput: ocean.\\nOutput: blue.\\nInput: blood.\\nOutput: red.\\nInput: snow.\\nOutput: white.\\n\\ninstruction: answer the capital of the given country.\\nInput: USA\\nOutput: Washington.\\nInput: China.\\nOutput: Beijing.\\nInput: Russia.\\nOutput: Moscow.\\nInput: France.\\nOutput: Paris.\\nInput: UK.\\nOutput: London.\\n\\nInstruction: $\\\\{f\\\\}$\\n\\nTable 5: The prompts used for Instruction Induction.\"}"}
{"id": "acl-2024-long-150", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The transformation is:\\n\\n- Remove the first and the second element.\\n- Retain the elements that greater than 5.\\n- Reverse the input list.\\n- Append 5 to the input list.\\n\\nTable 6: The prompts used for List Function.\"}"}
{"id": "acl-2024-long-150", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Task         | IO (L7) | SC (L7) | HS (L7) | HS&R (L7) | HS+D (L7) | ItD (L7) | ItD+D (L7) |\\n|-------------|---------|---------|---------|-----------|-----------|----------|----------|\\n| active_to_passive | 56.18   | 3.23    | 9.55    | 20.1      | 16.13     | 90.15    | 100.00   |\\n| antonyms    | 40.82   | 79.75   | 81.59   | 80.97     | 83.50     | 80.40    | 83.00    |\\n| cause_and_effect | 16.74   | 15.70   | 24.78   | 21.52     | 28.78     | 45.84    | 57.06    |\\n| common_concept | 0.96    | 6.47    | 7.00    | 6.98      | 6.67      | 17.58    | 3.21     |\\n| diff        | 1.14    | 3.32    | 9.78    | 15.46     | 14.70     | 17.00    | 34.49    |\\n| first_word_letter | 12.56   | 58.03   | 58.90   | 54.28     | 81.49     | 71.27    | 100.00   |\\n| informal_to_formal | 40.51   | 34.59   | 40.99   | 43.16     | 43.04     | 26.22    | 48.16    |\\n| larger_animal | 0.04    | 11.86   | 22.34   | 23.29     | 28.54     | 6.05     | 30.06    |\\n| letters_list | 0.12    | 0.31    | 1.15    | 1.23      | 1.22      | 0.04     | 0.00     |\\n| negation    | 9.52    | 6.51    | 13.16   | 14.44     | 15.39     | 44.95    | 43.45    |\\n| num_to_verbal | 3.00    | 3.00    | 4.00    | 7.58      | 7.48      | 97.00    | 100.00   |\\n| orthography_starts_with | 3.02    | 6.94    | 5.71    | 7.26      | 9.76      | 1.86     | 43.20    |\\n| rhyme       | 26.64   | 2.93    | 3.00    | 2.72      | 2.45      | 0.19     | 2.25     |\\n| second_word_letter | 4.73    | 2.13    | 1.23    | 1.92      | 5.36      | 8.64     | 2.28     |\\n| sentence_similarity | 0.00    | 0.00    | 0.00    | 0.00      | 0.00      | 0.00     | 0.00     |\\n| sentiment   | 2.01    | 17.49   | 26.41   | 27.18     | 45.82     | 87.49    | 36.96    |\\n| singular_to_plural | 28.58   | 94.05   | 97.93   | 96.94     | 98.78     | 97.86    | 99.95    |\\n| sum         | 22.33   | 50.54   | 83.64   | 84.37     | 84.13     | 50.02    | 10.37    |\\n| synonyms    | 8.56    | 0.39    | 1.22    | 1.05      | 1.43      | 2.79     | 1.42     |\\n| taxonomy_animal | 0.00    | 0.37    | 1.56    | 0.77      | 2.27      | 1.59     | 8.08     |\\n\\n| Task         | IO (L7) | SC (L7) | HS (L7) | HS&R (L7) | HS+D (L7) | ItD (L7) | ItD+D (L7) |\\n|-------------|---------|---------|---------|-----------|-----------|----------|----------|\\n| active_to_passive | 37.11   | 56.76   | 56.58   | 53.34     | 79.82     | 93.25    | 98.98    |\\n| antonyms    | 77.99   | 67.24   | 66.55   | 75.45     | 76.79     | 82.48    | 83.29    |\\n| cause_and_effect | 26.96   | 22.44   | 18.28   | 12.44     | 42.14     | 42.10    | 36.98    |\\n| common_concept | 4.45    | 5.02    | 4.87    | 7.66      | 16.69     | 17.72    | 17.72    |\\n| diff        | 0.93    | 2.17    | 8.74    | 5.94      | 19.00     | 16.00    | 37.00    |\\n| first_word_letter | 51.96   | 43.84   | 33.36   | 26.69     | 65.93     | 79.49    | 69.21    |\\n| informal_to_formal | 35.36   | 31.87   | 33.31   | 38.23     | 27.13     | 26.03    | 23.73    |\\n| larger_animal | 41.31   | 20.86   | 12.21   | 16.35     | 8.54      | 3.39     | 0.00     |\\n| letters_list | 0.00    | 0.00    | 0.00    | 0.22      | 0.04      | 0.06     | 0.07     |\\n| negation    | 33.47   | 31.54   | 39.46   | 29.33     | 50.86     | 50.45    | 56.17    |\\n| num_to_verbal | 98.30   | 96.86   | 96.63   | 99.01     | 96.00     | 99.00    | 100.00   |\\n| orthography_starts_with | 7.42    | 3.61    | 2.87    | 2.94      | 2.51      | 2.12     | 1.01     |\\n| rhyme       | 1.64    | 0.75    | 0.67    | 0.87      | 0.43      | 0.20     | 0.04     |\\n| second_word_letter | 3.55    | 4.25    | 1.43    | 0.00      | 8.94      | 8.79     | 12.54    |\\n| sentence_similarity | 0.05    | 0.00    | 0.02    | 0.00      | 0.00      | 0.00     | 0.00     |\\n| sentiment   | 21.75   | 31.59   | 35.10   | 61.95     | 80.33     | 88.11    | 89.00    |\\n| singular_to_plural | 91.11   | 97.46   | 96.77   | 91.52     | 97.39     | 97.37    | 97.35    |\\n| sum         | 68.16   | 64.07   | 67.33   | 85.26     | 42.02     | 52.00    | 75.00    |\\n| synonyms    | 4.92    | 8.32    | 6.59    | 6.81      | 2.01      | 3.05     | 2.09     |\\n| taxonomy_animal | 6.21    | 2.77    | 0.96    | 0.13      | 2.16      | 1.11     | 0.46     |\\n| translation_en-de | 50.54   | 56.46   | 55.35   | 55.65     | 51.67     | 52.77    | 51.39    |\\n| translation_en-es | 55.85   | 59.43   | 60.74   | 51.04     | 58.11     | 58.79    | 58.02    |\\n| translation_en-fr | 40.11   | 48.32   | 52.40   | 52.22     | 35.04     | 37.84    | 35.81    |\\n| word_in_context | 11.11   | 24.01   | 26.22   | 24.85     | 35.82     | 42.67    | 46.14    |\\n\\n| Task         | IO (L13) | SC (L13) | HS (ChatGPT) | HS&R (ChatGPT) | HS+D (ChatGPT) | ItD-IO (ChatGPT) | ItD (ChatGPT) | IO (reference) |\\n|-------------|----------|----------|-------------|----------------|----------------|-----------------|-------------|---------------|\\n| active_to_passive | 93.43    | 100.00   | 100.00      | 100.00         | 100.00         | 100.00         | 100.00      | 100.00        |\\n| antonyms    | 73.36    | 81.23    | 77.54       | 73.80          | 81.11          | 81.11          | 81.11       | 81.11         |\\n| cause_and_effect | 10.52   | 57.16    | 30.24       | 44.04          | 39.33          | 39.33          | 39.33       | 39.33         |\\n| common_concept | 3.91    | 9.17     | 7.84        | 9.21           | 12.00          | 12.00          | 12.00       | 12.00         |\\n| diff        | 32.57    | 91.56    | 93.00       | 99.00          | 99.89          | 99.89          | 99.89       | 99.89         |\\n| first_word_letter | 26.10   | 9.13     | 100.00      | 100.00         | 99.89          | 99.89          | 99.89       | 99.89         |\\n| informal_to_formal | 52.01   | 42.38    | 54.56       | 53.94          | 59.55          | 59.55          | 59.55       | 59.55         |\\n| larger_animal | 35.98    | 87.67    | 68.95       | 77.73          | 91.78          | 91.78          | 91.78       | 91.78         |\\n| letters_list | 3.59     | 7.03     | 77.88       | 94.02          | 89.44          | 89.44          | 89.44       | 89.44         |\\n| negation    | 52.05    | 61.44    | 75.09       | 73.03          | 74.50          | 74.50          | 74.50       | 74.50         |\\n| num_to_verbal | 44.18    | 100.00   | 99.90       | 100.00         | 93.00          | 93.00          | 93.00       | 93.00         |\\n| orthography_starts_with | 2.20    | 12.12    | 25.28       | 40.42          | 52.50          | 52.50          | 52.50       | 52.50         |\\n| rhyme       | 1.17     | 0.18     | 1.75        | 6.67           | 11.38          | 11.38          | 11.38       | 11.38         |\\n| second_word_letter | 1.08    | 0.12     | 50.60       | 85.81          | 99.00          | 99.00          | 99.00       | 99.00         |\\n| sentence_similarity | 0.00    | 0.00     | 0.00        | 0.00           | 0.33           | 0.33           | 0.33        | 0.33          |\\n| sentiment   | 74.84    | 39.01    | 53.82       | 66.81          | 82.75          | 82.75          | 82.75       | 82.75         |\\n| singular_to_plural | 82.45   | 100.00   | 94.74       | 94.53          | 99.88          | 99.88          | 99.88       | 99.88         |\\n| sum         | 7.81     | 20.74    | 97.00       | 100.00         | 98.87          | 98.87          | 98"}
