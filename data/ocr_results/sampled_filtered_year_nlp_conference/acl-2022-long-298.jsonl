{"id": "acl-2022-long-298", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Model-specific Analyses\\n\\n| Language | DE | ES | IT | RU | ZH |\\n|----------|----|----|----|----|----|\\n| Accuracy | 42.02 | 36.75 | 37.07 | 42.50 | 39.73 |\\n| MFS | 26.96 | 30.00 | 25.14 | 35.19 | 22.35 |\\n| MFS+ | 59.13 | 61.78 | 62.82 | 45.25 | 59.35 |\\n| SFII | 87.30 | 88.03 | 88.81 | 84.16 | 92.45 |\\n| ESCHER | 74.71 | 73.84 | 76.10 | 69.69 | 82.17 |\\n| Mean | 67.18 | 66.86 | 68.50 | 67.69 | 69.82 |\\n\\nFigure 10: Evaluation on M2M100 LG\\n\\nFigure 11: Overall Language Cooccurrence Heatmap for M2M100 LG\"}"}
{"id": "acl-2022-long-298", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"| Language | DE  | ES  | IT  | RU  | ZH  |\\n|---------|-----|-----|-----|-----|-----|\\n| MISS    | 40.24 | 39.29 | 40.10 | 44.16 | 44.35 |\\n| Accuracy | 28.73 | 33.89 | 29.34 | 36.06 | 31.21 |\\n| MFS     | 58.89 | 60.17 | 62.90 | 47.39 | 50.66 |\\n| MFS+ SPDI | 89.72 | 91.10 | 87.50 | 87.20 | 89.87 |\\n| SFII    | 73.86 | 71.06 | 71.51 | 70.11 | 73.14 |\\n| ESCHER  | 84.10 | 77.13 | 78.67 | 73.86 | 80.39 |\\n\\n**Mean**\\n- DE: 41.63\\n- ES: 31.85\\n- IT: 56.00\\n- RU: 89.08\\n- ZH: 71.94\\n\\n**Figure 12:** Evaluation on MBart50\\n\\n**Figure 13:** Overall Language Cooccurrence Heatmap for MBart50\"}"}
{"id": "acl-2022-long-298", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"**Figure 14:** Evaluation on MBart50 MTM\\n\\n| Language | DE | ES | IT | RU | ZH |\\n|----------|----|----|----|----|----|\\n| MISS     | 41.25 | 41.06 | 43.29 | 45.18 | 44.59 |\\n| Accuracy | 28.65 | 32.66 | 30.54 | 33.33 | 34.15 |\\n| MFS      | 55.82 | 63.09 | 68.97 | 44.91 | 54.17 |\\n| MFS+     | 89.56 | 91.85 | 91.81 | 87.96 | 90.28 |\\n| SPDI     | 74.24 | 71.57 | 69.48 | 72.87 | 71.50 |\\n| SFII     | 84.95 | 79.06 | 79.41 | 78.58 | 76.59 |\\n| ESCHER   | 67.77 | 67.18 | 65.81 | 64.29 | 69.58 |\\n\\n**Mean**\\n- MISS: 31.87\\n- Accuracy: 57.39\\n- MFS: 90.29\\n- MFS+: 71.93\\n- SPDI: 79.72\\n- SFII: 66.93\\n- ESCHER: 66.93\\n\\n**Figure 15:** Overall Language Cooccurrence Heatmap for MBart50 MTM\"}"}
{"id": "acl-2022-long-298", "page_num": 20, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 16: Full page version of Figure 2.\"}"}
{"id": "acl-2022-long-298", "page_num": 21, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"He poured a shot of whiskey.\\n\\nA small drink of liquor.\\n\\n---\\n\\nTable 1: Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.\\n\\n---\\n\\nTable 2: Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.\"}"}
{"id": "acl-2022-long-298", "page_num": 22, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"If you take off for Thanksgiving you must work Christmas and vice versa.\\n\\nTo absent oneself from work or other responsibility, especially with permission.\\n\\n| German | \u2713 | \u2717 |\\n|--------|---|---|\\n| sich eine Auszeit nehmen | losgehen | \u2717 |\\n| sich freinehmen | starten | \u2717 |\\n\\n| Spanish | \u2713 | \u2717 |\\n|---------|---|---|\\n| pedir un permiso | salir | \u2717 |\\n| coger | llevar | \u2717 |\\n\\n| Italian | \u2713 | \u2717 |\\n|---------|---|---|\\n| prendersi dei giorni | togliersi | \u2717 |\\n| prendersi un permesso | decollare | \u2717 |\\n\\n| Russian | \u2713 | \u2717 |\\n|---------|---|---|\\n| \u0431\u0440\u0430\u0442\u044c \u0432\u044b\u0445\u043e\u0434\u043d\u043e\u0439 | \u0432\u044b\u0447\u0435\u0441\u0442\u044c | \u2717 |\\n| \u043e\u0442\u0434\u044b\u0445\u0430\u0442\u044c | \u0443\u0431\u0438\u0442\u044c | \u2717 |\\n\\n| Chinese | \u2713 | \u2717 |\\n|---------|---|---|\\n| \u8bf7 | \u5047 | \u2717 |\\n| \u79bb | \u4f11 | \u2717 |\\n| \u51cf | | |\\n\\nTable 3: Example of item annotated in all languages. First row is the example, target word is in bold, second row is the definition of the synset associated with the word in the example.\"}"}
{"id": "acl-2022-long-298", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DIBI: A Novel Benchmark for Measuring Word Sense Disambiguation Biases in Machine Translation\\n\\nNiccol\u00f2 Campolungo \u2217\\nSapienza University of Rome\\ncampolungo@di.uniroma1.it\\n\\nFederico Martelli \u2217\\nSapienza University of Rome\\nmartelli@di.uniroma1.it\\n\\nFrancesco Saina\\nSSML Carlo Bo, Rome\\nf.saina@ssmlcarlobo.it\\n\\nRoberto Navigli\\nSapienza University of Rome\\nnavigli@diag.uniroma1.it\\n\\nAbstract\\nLexical ambiguity poses one of the greatest challenges in the field of Machine Translation. Over the last few decades, multiple efforts have been undertaken to investigate incorrect translations caused by the polysemous nature of words. Within this body of research, some studies have posited that models pick up semantic biases existing in the training data, thus producing translation errors. In this paper, we present DIBI, the first entirely manually-curated evaluation benchmark which enables an extensive study of semantic biases in Machine Translation of nominal and verbal words in five different language combinations, namely, English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish. Furthermore, we test state-of-the-art Machine Translation systems, both commercial and non-commercial ones, against our new test bed and provide a thorough statistical and linguistic analysis of the results. We release DIBI at https://nlp.uniroma1.it/dibimt as a closed benchmark with a public leaderboard.\\n\\n1 Introduction\\nThe polysemous nature of words poses a long-standing challenge in a wide range of Natural Language Processing (NLP) tasks such as Word Sense Disambiguation (Navigli, 2009; Bevilacqua et al., 2021) (WSD), Information Retrieval (Krovetz and Croft, 1992) (IR) and Machine Translation (Emelin et al., 2020) (MT).\\n\\nIn MT, some research works have addressed the ability of systems to disambiguate polysemous words. For instance, given the sentence He poured a shot of whiskey, the polysemous target word shot unequivocally means a small quantity and therefore a possible translation into Italian could be: Vers\u00f2 un goccio di whiskey. However, some MT systems propose the following translation: Vers\u00f2 uno sparo di whiskey in which the noun sparo means gun-shot. This is one of many examples that seem to encourage a deeper performance analysis in scenarios in which MT systems are required to deal with polysemous words and, specifically, with infrequent meanings of polysemous words. Although state-of-the-art MT systems, both commercial and non-commercial ones, achieve impressive BLEU scores on standard benchmarks, in our work we demonstrate that they still present significant limitations when dealing with infrequent word senses, which standard metrics fail to recognize.\\n\\nIn the last few decades, attempts have been made to investigate the aforementioned phenomena. In fact, recent studies have observed a direct correlation between semantic biases in the training data and semantic errors in translation. However, their findings are limited by the following shortcomings: i) they are not based on entirely manually-curated benchmarks; ii) they rely heavily on automatically-generated resources to determine the correctness of a translation; and iii) they do not cover multiple language combinations.\\n\\nIn this work, we address the aforementioned drawbacks and present DIBI, to the best of our knowledge the first fully manually-curated evaluation benchmark aimed at investigating the impact of semantic biases in MT in five language combinations, covering both nouns and verbs. This benchmark allows the community not only to better explore the described phenomena, but also to devise innovative MT systems which better deal with lexical ambiguity. Specifically, the contributions of the present work are threefold:\\n\\n\u2022 We present DIBI, a novel gold-quality test bed for semantic biases in MT that goes beyond a simple accuracy score, covering five language combinations, namely English and one or other of the following languages: Chinese, German, Italian, Russian and Spanish;\"}"}
{"id": "acl-2022-long-298", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"He poured a shot of whiskey.\\n\\n- SPANISH: goccio (lit. a drop), but not in Spanish, pistolero (a person who shoots).\\n- RUSSIAN: \u0448\u043e\u0442, \u0440\u044e\u043c\u043a\u0430 (cup), \u0441\u0442\u0440\u0435\u043b\u043e\u043a (gun), \u0432\u044b\u0441\u0442\u0440\u0435\u043b (shot).\\n- CHINESE: \u676f (cup), \u5c0f\u676f (small cup), \u67aa\u624b (gunman), \u672c\u5792\u6253 (home run).\\n- GERMAN: Schl\u00fcckchen, Schuss (shot), Injektion (injection), Schlag (strike).\\n- ITALIAN: goccio (lit. a drop), iniezione (injection), sparo (shot), trago (shot), chupito (shot).\\n\\nWe define four novel metrics that better clarify the semantic biases within MT models; we provide a thorough statistical and linguistic analysis in which we compare 7 state-of-the-art MT systems, including both commercial and non-commercial ones, against our new benchmark. Furthermore, we extensively discuss the results.\\n\\nTo enable further research, we release DIBIMT as a closed benchmark with a public leaderboard at https://nlp.uniroma1.it/dibimt.\\n\\n2 Related Work\\n\\nOver the course of the last few decades, several approaches to the evaluation of the lexical choice in MT have been proposed. To this end, cross-lingual benchmarks were created in which systems were required to provide the translation or a substitute for a given target word in context in a target language (Vickrey et al., 2005; Mihalcea et al., 2010; Lefever and Hoste, 2013).\\n\\nMore recently, Gonzales et al. (2017) put forward ContraWSD, a dataset which includes 7,200 instances of lexical ambiguity for German \u2192 English, and 6,700 for German \u2192 French. This dataset pairs every reference translation with a set of contrastive examples which contain incorrect translations of a polysemous target word. For each instance, the answer provided by systems is considered correct if the reference translation is scored higher. Based on a denoised version of the ContraWSD dataset and focusing on the language combination German \u2192 English, Gonzales et al. (2018) present the Word Sense Disambiguation Test Suite which, unlike ContraWSD, evaluates MT output directly rather than by scoring translations. The suite consists of a collection of 3,249 sentence pairs in which the German source sentences contain one ambiguous target word. As target words, the authors considered only words in German whose translation into English does not cover multiple senses, thus making the evaluation more straightforward. Despite their effectiveness, such benchmarks do not allow systems to be tested in multiple language combinations, and only cover a very limited number of words and senses.\\n\\nTo address these limitations, Raganato et al. (2019) proposed MuCoW, an automatically-created test suite covering 16 language pairs, with more than 200,000 sentence pairs derived from word-aligned parallel corpora. Other research studies investigated the disambiguation capabilities of MT systems by exploring their internal representations (Marvin and Koehn, 2018; Michel et al., 2019), or improving them via context-aware word embeddings (Liu et al., 2018).\\n\\nMore recently, Emelin et al. (2020) introduced a statistical method for the identification of disambiguation errors in neural MT (NMT) and demonstrated that models capture data biases within the training corpora, which leads these models to produce incorrect translations. Although the authors expected their approach to be transferable to other language combinations, they only focused on German \u2192 English.\\n\\nBased on the findings and open research questions raised in the aforementioned works, the present paper aims at investigating not only the presence, but also, most importantly, the nature and properties of semantic biases in MT in multiple language combinations, via a novel entirely manually-curated benchmark called DIBIMT and a thorough performance analysis.\\n\\n3 Building DIBIMT\\n\\nThe DIBIMT benchmark focuses on detecting Word Sense Disambiguation biases in NMT, i.e., biases of certain words towards some of their more frequent meanings. The creation of such a dataset requires i) a set of unambiguous and grammatically-correct sentences containing a polysemous target word; ii) a set of correct and incorrect translations of each target word into the languages to be covered. Figure 1 depicts an example of a dataset item.\\n\\n3.1 Preliminaries\\n\\nSimilarly to previous studies, we rely on BabelNet (Navigli et al., 2021), a large multilingual NLP resource containing more than 30,000,000 words and phrases extracted from over 400 sources in 105 languages.\"}"}
{"id": "acl-2022-long-298", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A gual encyclopedic dictionary whose nodes are concepts represented by synsets, i.e., sets of synonyms, containing lexicalizations in multiple languages and coming from various heterogeneous resources, including, inter alia, WordNet (Miller et al., 1990) and Wiktionary.\\n\\nLet us define $B$ as an abstraction used to query the subset of synsets in BabelNet that contain at least one sense from WordNet and one or more senses in languages other than English, while only considering senses coming from high-quality sources, i.e., language-specific wordnets.\\n\\n**Formal Notation**\\n\\nGiven an arbitrary synset $\\\\sigma$, we define $\\\\Lambda_L(\\\\sigma)$ as the set of lexicalizations of $\\\\sigma$ in language $L$. As an example, let us consider the synset $\\\\tilde{\\\\sigma}$ corresponding to the drink meaning of the word shot. $\\\\tilde{\\\\sigma}$ contains lexicalizations in different languages, including: Shot $\\\\text{DE}$, shot $\\\\text{EN}$, nip $\\\\text{EN}$, chupito $\\\\text{ES}$, trago $\\\\text{ES}$, bicchierino $\\\\text{IT}$ and goccio $\\\\text{IT}$. Hence, $\\\\Lambda_{\\\\text{EN}}(\\\\tilde{\\\\sigma}) = \\\\{\\\\text{shot}, \\\\text{nip}\\\\}$, while $\\\\Lambda_{\\\\text{ES}}(\\\\tilde{\\\\sigma}) = \\\\{\\\\text{chupito}, \\\\text{trago}\\\\}$.\\n\\nFurthermore, let $\\\\lambda_P$ represent a (lemma, part of speech) pair, where $P$ is the part of speech. We denote $\\\\Omega_L(\\\\lambda_P) = \\\\{\\\\sigma_1, \\\\ldots, \\\\sigma_n\\\\}$ as the set of synsets which contain $\\\\lambda_P$ as a lexicalization in language $L$ according to $B$. Additionally, we define $\\\\delta_L(\\\\lambda_P) = |\\\\Omega_L(\\\\lambda_P)|$ as the polysemy degree, i.e., the number of senses, of $\\\\lambda_P$ in language $L$. For example, given $\\\\lambda_P = \\\\text{shot NOUN}$, $\\\\Omega_{\\\\text{EN}}(\\\\lambda_P)$ would be the set of synsets associated with the nominal term shot (e.g., the act of firing, a photograph and a drink, among others).\\n\\n### 3.2 Sentence Selection Process\\n\\nIn this section, we detail the creation process of our dataset, i.e., the selection of our sentences as well as the construction and filtering of our items.\\n\\n**Item Structure and Notation**\\n\\nBefore we proceed, let us formally state how each item in the dataset is structured: given a source sentence $s = [w_1, \\\\ldots, w_n]$ as a sequence of words, and given a target word $w_i$ in $s$ tagged with some synset $\\\\sigma$, we consider $X = (s, w_i, \\\\sigma)$ as an initial item of the dataset, i.e., an instance composed of an English sentence $s$, a target word $w_i$ and its associated synset $\\\\sigma$.\\n\\nThis instance can be annotated for candidate translations of $w_i$ in some language $L$. We also denote $\\\\lambda_X$ as the (lemma, POS) pair of $w_i$.\\n\\n#### 3.2.1 Starting Sentence Pool\\n\\nWe collect our initial items from two main sources: WordNet and Wiktionary. Specifically, we use the examples from WordNet Tagged Glosses (Langone et al., 2004), where each sentence's target word was manually associated with its synset, thereby readily providing the first batch of initial items.\\n\\nAs for Wiktionary, instead, we start by obtaining every usage example $s$ and its associated definition $d$ (filtering out archaic usages and slang), then, we automatically extract the target words from the corresponding example.\\n\\nNow, the only step that remains in order to construct an initial item is to associate a synset $\\\\sigma$ with the word $w_i$ used in the example $s$. We perform this association in two phases: first, we try to map the definition $d$ related to the example $s$ to a BabelNet synset by relying on the automatic mappings available in BabelNet between WordNet and Wiktionary, discarding examples for which this association can not be found; second, we manually validate and correct these successful associations to ensure that our initial items are of high quality.\\n\\n#### 3.2.2 Sentence Filtering\\n\\nWe apply a filtering step to the original sentences in order to select examples that are likely to be more challenging for the models to translate: i) we discard every initial item $X$ for which $\\\\delta_{\\\\text{EN}}(\\\\lambda_X) < 3$, i.e., we retain only sentences whose associated $(lemma, POS)$ pair has a polysemy degree of at least 3 in $B_{\\\\text{EN}}$; ii) we retain at most only one sentence per sense per source; iii) differently from previous works, which impose a strict requirement on synsets that are monosemous in the target language, we retain sentences satisfying the following requirement. Let us consider the nominal senses of the word bank: among them, one represents a specific aviation maneuver. In Italian, this synset.\\n\\n---\\n\\n3. We use the dump of September 2021.\\n4. Which we convert from WordNet to BabelNet.\\n5. In Wiktionary, target words are marked in bold inside the example sentence.\\n6. The reasoning for this choice is twofold: on the one hand, oftentimes Wiktionary has multiple examples for the same synset, that differ in only one or two words, thus we skip them to avoid repetitions; on the other hand, we obtain an increase in sense coverage without worsening the annotator load.\"}"}
{"id": "acl-2022-long-298", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"includes one lexicalization, avvitamento; although this is not monosemous in Italian (e.g., avvitamento might also refer to a screw thread), neither of the other possible senses of avvitamento has bank as an English lexicalization, which, for Italian, satisfies our third condition. If the same holds true for all languages, the synset passes the test and thus the sentence is retained.\\n\\n3.3 Annotating the Dataset\\n\\nOnce the set of initial items is ready, we can proceed with the annotation phase, which will produce our annotated items. Specifically, given a language L and an initial item X = (s, wi, \u03c3), we associate a set of good (GL) and bad (BL) translation candidates with X, which represent words that, respectively, we do, and do not, expect to see in a translation of sentence s in language L. Finally, we refer to X_L as an annotated item, i.e., the tuple (s, wi, \u03c3, GL, BL).\\n\\n3.3.1 Pre-annotation Item Creation\\n\\nBefore moving forward with the annotation phase, we pre-populate the sets of good (GL) and bad (BL) lexicalizations for a given initial item X in language L extracting them from B. Formally, we assign GL = \u039b_L(\u03c3), i.e., the set of lemmas in language L of the BabelNet synset associated with \u03c3; furthermore, we set BL = \u222a\u0302\u03c3 \u2208 \u03a9_L(\u03bb_X)\\\\{\u03c3}\\\\\u039b_L(\u0302\u03c3), i.e., the set of all lemmas in language L of BabelNet synsets associated with any \u0302\u03c3 excluding \u03c3. With this step, we produce an automatically populated version of our annotated items.\\n\\n3.3.2 Annotation Guidelines\\n\\nWe instruct annotators to update the set of good (GL) and bad (BL) lexicalizations of wi \u2208 s such that each lexicalization contained in the respective set can be considered a good or a bad translation equivalent for the target word in the provided potential context.\\n\\nWe also instruct annotators to discard sentences in which i) the target word wi is an idiomatic expression or a proper noun, and ii) the semantic context is not sufficient to properly disambiguate wi. Given the expertise required to carry out this task, we rely on three highly qualified translators: one for Italian, German and Russian; one for Spanish and one for Chinese. Our annotators satisfy the following requirements: they are native speakers or hold C2-level certifications and work as professional translators in the given language combinations. The full instructions provided to the annotators can be found in Appendix C.\\n\\n3.3.3 Resulting Dataset\\n\\nOur annotators analyzed around 800 sentences, discarding 200 of them, finally obtaining approximately 600 annotated items in 5 languages. Due to a coverage issue of the Russian language in BabelNet, we retain only sentences tagged with nominal or verbal synsets. Dataset statistics are reported in Table 1.\\n\\nAs expected, we note that the lexicalizations found in B have been substantially refined by our annotators in all languages, as reported in Table 2. Indeed, across languages, on average, 54% of the good lexicalizations have been added by our annotators, while 42% of the pre-existing lexicalizations have been removed. More importantly, given a language and two sentences containing words referring to the same synset, on average only in 55% of cases do they also share those words' good\"}"}
{"id": "acl-2022-long-298", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"lexicalizations, confirming that the assumption that all synonyms of a word are valid replacements can lead to incorrect results.\\n\\nThese statistics lead us to a straightforward, but important, conclusion: only in a limited number of cases is a lexicalization belonging to a given synset to be considered as a suitable translation equivalent for the provided target word and its context. Examined jointly, these metrics suggest that relying on synset lexicalizations from BabelNet alone is prone to producing errors, either due to BabelNet's intrinsic noise, or due to the lack of different granularity of synsets and contextualized words.\\n\\nAs we stated in Section 3.2.1, the sentences we annotate are all usage examples of specific concepts obtained from WordNet or Wiktionary. Such examples are typically short main clauses with no subordinates, featuring on average 9 words (around 50 characters per sentence). All selected sentences include a semantic context which allows the meaning of the target word to be properly identified.\\n\\n### 3.4 Analysis Procedure\\n\\nDI-BI-MT's analysis procedure is fairly simple: given an annotated item $X_L = (s, w_i, \\\\sigma, G_L, B_L)$ and a translation model $M$, we compute $t_L = M(s)$, i.e., the translation of $s$ in language $L$ according to $M$. Then, we use Stanza (Qi et al., 2020) to perform tokenization, part-of-speech tagging and lemmatization of $t_L$ and, finally, we check if there is any match between the lemmas of the translated sentence and those contained in $G_L$ or $B_L$. In case there is no match, we mark the translation as a MISS; otherwise, we mark it as GOOD or BAD depending on which set matched the lemma.\\n\\nThis produces an analyzed item, which for simplicity we denote as $X_M = (X_L, t_L, \\\\mathcal{R}, \\\\omega_L)$, where $\\\\mathcal{R}$ is one of GOOD, BAD or MISS and $\\\\omega_L$ represents the matched lemma in case there was a match (GOOD or BAD), $\\\\epsilon$ otherwise.\\n\\n### 4 Results and Discussion\\n\\nWe now: i) use DI-BI-MT to carry out an evaluation of 7 different machine translation systems; ii) report the obtained results, including a thorough statistical and linguistic evaluation; iii) extensively discuss our findings, providing multiple measures of semantic bias; and iv) offer some insights into the causes of such biases. In Appendix D we include a model-specific breakdown of the various scores and metrics reported throughout this section.\\n\\n#### 4.1 Comparison Systems\\n\\nWe test a wide range of models, both commercial and non-commercial ones, and report their performances on DI-BI-MT's evaluation metrics:\\n\\n- **DeepL Translator**, a state-of-the-art commercial NMT system.\\n- **Google Translate**, arguably the most popular commercial NMT system.\\n- **OPUS** (Tiedemann and Thottingal, 2020), the smallest state-of-the-art NMT model available to date, a base Transformer (each model has approximately 74M parameters) trained on a single language pair on large amounts of data.\\n- **MBart50** (Tang et al., 2021), multilingual BART fine-tuned on the translation task for 50 languages (610M parameters). We refer to MBart50 as the English-to-many model, and to MBart50 MTM as the many-to-many model.\\n- **M2M100** (Fan et al., 2021), a multilingual model able to translate from/to 100 languages. We test both versions of the model, the 418M parameter one (which we dub M2M100) and the 1.2B parameter one (dubbed M2M100 LG).\\n\\n#### 4.2 Discussion of MISS\\n\\nFigure 2 reports general results of the analysis per (model, language) pair. Given the high percentage of analyzed items classified as MISS, we asked our annotators to perform an inspection on a random sample of 70 items per language in order to unearth the reasons, with varying results. We identified multiple causes, namely:\\n\\n- i) word omission in the translation (around 19% of items, mostly in Chinese and Italian);\\n- ii) issues with Stanza's tokenization (around 11%, mostly Chinese and Russian) and lemmatization (around 12%, mostly Italian and German);\\n- iii) words translated as themselves (approximately 5%, often in multilingual neural models);\\n- iv) translations which have nothing to...\"}"}
{"id": "acl-2022-long-298", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3 General Results\\n\\nTable 3 reports accuracy for non-MISS analyzed items (i.e., #GOOD/#GOOD + #BAD). With the sole exception of DeepL, which greatly outperforms every other competitor, models achieve extremely low scores, in the range of 20%-33%. Surprisingly, Google Translate performs worst across languages.\\n\\n4.4 Analyzing the Semantic Biases\\n\\nIn addition to accuracy, DIBI MT analyzes the semantic biases of a translation model via four novel metrics, which we define in detail in what follows.\\n\\nSense Frequency Index Influence (SFII)\\n\\nWe study the sensitivity of models to disambiguating senses with respect to their frequency. To do this, we define $\\\\mu_\\\\lambda P(\\\\sigma_k)$ as the index of synset $\\\\sigma_k$ in $\\\\Omega_{EN}(\\\\lambda P)$ ordered according to WordNet's sense frequency, as computed from SemCor. That is, in Figure 3(a), we plot the number and percentage of errors made on average by the models, grouping items by $\\\\mu_\\\\lambda X P(\\\\sigma_k)$, where $X$ is a non-MISS analyzed item. As expected, the less frequent a meaning for a given word is, the harder it is for the model to correctly disambiguate it.\\n\\nFinally, given a (model, language) pair, we define the Sense Frequency Index Influence (SFII) as the average percentage of errors, for each group, that we detected. Values are reported in Table 4. Interestingly, DeepL proves once again to be the best, obtaining a score of 51%, far below the average 80% achieved by the other models, with most non-commercial models performing $\\\\leq 80%$.\\n\\nSense Polysemy Degree Importance (SPDI)\\n\\nSimilarly to SFII, we also study the extent to which the polysemy degree, i.e., how many senses a given word can have, impacts the models' disambiguation capabilities. This experiment mirrors SFII, but groups items by their lemma's polysemy degree $\\\\delta_{EN}(\\\\lambda_X P)$ instead of $\\\\mu_\\\\lambda$. Figure 3(b) reports the results on all items. Unsurprisingly, similarly to the frequency index, we observe that higher polysemy leads to more errors, confirming that models still struggle with very polysemous words. Similarly to SFII, SPDI is defined as the average percentage of\"}"}
{"id": "acl-2022-long-298", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Frequency Analysis: MFS represents the average percentage of times the model mistakenly translates the target word into a lexicalization belonging to the Most Frequent Sense associated with $\\\\lambda_P$. MFS+, instead, checks whether the wrong translation belongs to any synset that is more frequent than the target one. Lower is better.\\n\\n| Language | MFS | MFS+ |\\n|---------|-----|------|\\n| DE      | 53.68 | 84.21 |\\n| ES      | 59.89 | 87.91 |\\n| IT      | 68.08 | 86.38 |\\n| RU      | 50.00 | 83.33 |\\n| ZH      | 49.07 | 88.89 |\\n\\nMean: 56.14\\n\\nTable 6: Results by PoS tag. Numbers represent the mean value of each score introduced in the paper. The column ALL summarizes the results reported in the other tables.\\n\\n| PoS     | Accuracy | MISS | MFS | MFS+ |\\n|---------|----------|------|-----|------|\\n| NOUN    | 32.11    | 38.03| 57.86| 88.68|\\n| VERB    | 34.15    | 29.36| 60.13| 87.57|\\n\\nWe can observe a few interesting results: first, on average, almost 60% of the time a mistake reflects the Most Frequent Sense of the target word (second-last column); second, almost 90% of the errors concern translations towards more frequent senses of the target word (last column). Importantly, these results are consistent across systems, whether commercial or not. Although it might seem straightforward, NMT models are still strongly biased towards senses that are more likely to be encountered during training; while this could be related to the pattern-matching nature of neural networks, it also depends heavily on the training data the model was trained upon, and this needs to be further investigated in future research.\\n\\n4.5 Are verbs harder than nouns?\\n\\nThe existing literature in WSD points to the fact that verbs are generally harder than nouns, mostly due to their highly polysemous nature (Barba et al., 2021b). We try to analyze whether MT models...\"}"}
{"id": "acl-2022-long-298", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are affected by the same phenomenon: in Table 6, we report the average results obtained by running DIIBI MT on all its sentences (column ALL) and the subset of sentences whose target word was either a NOUN or a VERB. In general, we observe an average drop of accuracy of 4 points, as well as an astounding difference of 18 percentage points in MISS handling, which we will investigate more thoroughly in future work. Interestingly, MT models are much more inclined to translate nouns into their most frequent sense; we attribute this difference to the generally higher polysemy of verbs compared to nouns, which increases the size of the space of possible translations for a given verb, thus decreasing the chance that it gets translated into the MFS. Aside from this, we draw the same conclusion as that drawn by previous works in the field of WSD, with nouns being generally easier to translate than verbs.\\n\\n4.6 Is the encoder disambiguating?\\n\\nWe try to assess to what extent, in a multilingual encoder-decoder architecture, the encoder is determining the implicit disambiguation of the source sentence before generating the translation. For instance, we ask ourselves this question: given an ambiguous word \\\\( w \\\\) in the source sentence \\\\( s \\\\), how often does the model translate it into a lexicalization representing the same sense, if prompted to translate \\\\( s \\\\) into different languages? Intuitively, if the encoder was the sole contributor to the implicit disambiguation performed by the model, we would expect to see the meaning to always be the same, regardless of the target language.\\n\\nTo measure this, we perform the following experiment: given a model \\\\( M \\\\), two languages \\\\( L_1 \\\\) and \\\\( L_2 \\\\), and an initial item \\\\( X \\\\), we take \\\\( M \\\\)'s analyzed items \\\\( X_{M L_1} \\\\) and \\\\( X_{M L_2} \\\\) and check if translations in \\\\( L_1 \\\\) and \\\\( L_2 \\\\) have a synset in common, i.e., \\\\( |\\\\Omega_{L_1}(\\\\omega_{L_1}) \\\\cap \\\\Omega_{L_2}(\\\\omega_{L_2})| > 0 \\\\). The results of this experiment are reported in Figure 4.\\n\\nWe observe that, on average, this phenomenon occurs around 70% of the time. Hence, it is safe to assume that, while the encoder certainly plays an important role in the disambiguation of the input sentence, the decoder is also contributing significantly. Another interesting observation is that the alphabet of the target language does not seem to matter for this task. We also disregard DeepL and Google Translate as their architecture is proprietary.\"}"}
{"id": "acl-2022-long-298", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: WSD Results: ESCHER\u2019s accuracy on the set of English sentences of non-MISS analyzed samples for each (model, language) pair. Higher is better.\\n\\nTable 8: Model Errors: percentage of times a model thought its BAD translation was better than a GOOD one. For each (model, language) pair, we sample a BAD translation (t_{BAD}), pair it with a GOOD translation (t_{GOOD}) produced by another model (prioritizing DeepL), and ask annotators to check their correctness and apply corrections where needed, then compute the perplexities according to M with the corresponding English sentence, and call them p_{GOOD} and p_{BAD} respectively. We repeat this sampling 50 times per (M, L) pair and check how often p_{BAD} < p_{GOOD}. Table 8 shows that, on average, this happens in 93% of cases, thus confirming that most semantic biases are embedded within models and are not caused by the decoding strategy.\\n\\n5 Conclusions\\n\\nIn this work, we presented DIBIMT, a novel benchmark for measuring and understanding semantic biases in NMT, which goes beyond simple accuracy and provides novel metrics that summarize how biased NMT models are. We tested DIBIMT on 7 widely adopted NMT systems, extensively discussing their performances and providing novel insights into the possible causes and relations of semantic biases within NMT models.\\n\\nFurthermore, statistics of our annotations suggest that, when dealing with translations, synsets\u2019 lexicalizations cannot be used interchangeably, as their choice depends heavily on the context. In the future, we plan to improve DIBIMT by introducing better heuristics to recognize and handle MISS cases, especially covering the linguistic phenomena we described (see Section 4.2); we also aim at widening language coverage and increasing the number of sentences in the benchmark, consequently improving word and sense coverage. To enable further research, we release DIBIMT as a closed benchmark with a public leaderboard at: https://nlp.uniroma1.it/dibimt.\\n\\nWe do this to make the translations more grammatically fluent, and not to correct the disambiguation of the target term, which was never detected as being wrong in the sampled cases.\"}"}
{"id": "acl-2022-long-298", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nThe authors gratefully acknowledge the support of the ERC Consolidator Grant MOUSSE No. 726487 and the ELEXIS project No. 731015 under the European Union's Horizon 2020 research and innovation programme, and the PerLIR project (Personal Linguistic resources in Information Retrieval) funded by the MIUR Progetti di ricerca di Rilevante Interesse Nazionale programme (PRIN 2017).\\n\\nThis work was also partially supported by the MIUR under the grant \\\"Dipartimenti di eccellenza 2018-2022\\\" of the Department of Computer Science of Sapienza University.\\n\\nReferences\\n\\nEdoardo Barba, Tommaso Pasini, and Roberto Navigli. 2021a. Esc: Redesigning WSD with extractive sense comprehension. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4661\u20134672.\\n\\nEdoardo Barba, Luigi Procopio, and Roberto Navigli. 2021b. ConSeC: Word Sense Disambiguation as continuous sense comprehension. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1492\u20131503, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nMichele Bevilacqua, Tommaso Pasini, Alessandro Raganato, Roberto Navigli, et al. 2021. Recent trends in word sense disambiguation: A survey. In Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21. International Joint Conference on Artificial Intelligence, Inc.\\n\\nDenis Emelin, Ivan Titov, and Rico Sennrich. 2020. Detecting word sense disambiguation biases in machine translation for model-agnostic adversarial attacks. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 7635\u20137653, Online. Association for Computational Linguistics.\\n\\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi Ma, Ahmed El-Kishky, Siddharth Goyal, Mandeep Baines, Onur Celebi, Guillaume Wenzek, Vshrav Chaudhary, et al. 2021. Beyond English-Centric Multilingual machine translation. Journal of Machine Learning Research, 22(107):1\u201348.\\n\\nAnnette Rios Gonzales, Laura Mascarell, and Rico Sennrich. 2017. Improving Word Sense Disambiguation in Neural Machine Translation with Sense Embeddings. In Proceedings of the Second Conference on Machine Translation, pages 11\u201319.\\n\\nAnnette Rios Gonzales, Mathias M\u00fcller, and Rico Sennrich. 2018. The Word Sense Disambiguation Test Suite at WMT18. In Proceedings of the Third Conference on Machine Translation: Shared Task Papers, pages 588\u2013596.\\n\\nRobert Krovetz and W Bruce Croft. 1992. Lexical ambiguity and information retrieval. ACM Transactions on Information Systems (TOIS), 10(2):115\u2013141.\\n\\nHelen Langone, Benjamin R Haskell, and George A Miller. 2004. Annotating WordNet. In Proceedings of the Workshop Frontiers in Corpus Annotation at HLT-NAACL 2004, pages 63\u201369.\\n\\nEls Lefever and V\u00e9ronique Hoste. 2013. Semeval-2013 task 10: Cross-lingual Word Sense Disambiguation. In Second Joint Conference on Lexical and Computational Semantics (* SEM), Volume 2: Proceedings of the Seventh International Workshop on Semantic Evaluation (SemEval 2013), pages 158\u2013166.\\n\\nFrederick Liu, Han Lu, and Graham Neubig. 2018. Handling Homographs in Neural Machine Translation. In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers), pages 1336\u20131345, New Orleans, Louisiana. Association for Computational Linguistics.\\n\\nRebecca Marvin and Philipp Koehn. 2018. Exploring Word Sense Disambiguation Abilities of Neural Machine Translation Systems. In Proceedings of the 13th Conference of the Association for Machine Translation in the Americas (Volume 1: Research Track), pages 125\u2013131.\\n\\nPaul Michel, Xian Li, Graham Neubig, and Juan Pino. 2019. On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 3103\u20133114, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nRada Mihalcea, Ravi Sinha, and Diana McCarthy. 2010. Semeval-2010 task 2: Cross-Lingual Lexical Substitution. In Proceedings of the 5th international workshop on semantic evaluation, pages 9\u201314.\\n\\nGeorge A Miller, Richard Beckwith, Christiane Fellbaum, Derek Gross, and Katherine J Miller. 1990. Introduction to wordnet: An on-line lexical database. International journal of lexicography, 3(4):235\u2013244.\"}"}
{"id": "acl-2022-long-298", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "acl-2022-long-298", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Analysis Procedure Details\\n\\nOur analysis procedure, which we described in Section 3.4, involves steps that go beyond simple lemma matching. For instance, in case of multi-word expressions, we allowed annotators to specify a wildcard, i.e., any number of tokens (including zero) were allowed to expand and still trigger a match. Additionally, since Stanza has multi-word expansion tokenization for some of the languages in our list, when available, we try to perform matching on both the list of words (alongside the list of tokens) in the translated sentence. Finally, in case no match is produced by the aforementioned steps, we apply a surface-level string matching heuristic which, especially in Chinese, helps us increase coverage.\\n\\nB Neural Models Implementation\\n\\nWe use HuggingFace's Transformers library (Wolf et al., 2020) for all neural models. As per standard practice, we generate translations using beam search as decoding algorithm with beam size 5.\\n\\nC Instructions for Dataset Annotation\\n\\nIn this work, we investigate semantic biases in Machine Translation across languages. You are provided with a spreadsheet containing 300 instances, each including the following information: a lemma, its part of speech, a definition and some good and bad translation candidates derived from BabelNet. Your task is to manually verify the correctness of the good candidates and add new good candidates if deemed necessary. Furthermore, you are asked to verify that all bad candidates are wrong. From a translation perspective, a good candidate is a word which correctly translates the English target word in the given context. Instead, a bad candidate is a wrong translation of the English target word in the given context.\\n\\nPlease adopt the following guidelines while annotating:\\n\\n- Do not annotate idioms.\\n- Do not annotate instances in which the semantic context does not allow us to unequivocally determine the meaning of the target word.\\n- Do not annotate proper names, e.g., \\\"Run\\\" in the sentence The military campaign near that creek was known as \\\"The battle of Bull Run\\\".\\n- You are allowed to include cross-PoS candidates (that is, candidates whose PoS is different from that of the target word), in this case please include the candidate in square brackets like this: [candidate_with_different_pos|POS], where x represents the part-of-speech tag of the translated word. Do this for multi-word expressions as well.\\n- Mark with the tag \\\"DISCUSS\\\" difficult instances which you would like to discuss.\\n\\nD Model-specific Analyses\\n\\nWe include model-specific analyses with per-language breakdown of the scores achieved on our benchmark. The column named ESCHER provides the scores of the WSD system on the subset of sentences of the specified model and language, and should be treated as an additional baseline to compare with the accuracy achieved by the system. Details can be found in Section 4.\"}"}
{"id": "acl-2022-long-298", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table\\n\\n|        | DE  | ES  | IT  | RU  | ZH  |\\n|--------|-----|-----|-----|-----|-----|\\n| MISS   | 36.07 | 25.26 | 20.21 | 35.04 | 31.86 |\\n| Accuracy | 74.60 | 57.87 | 53.49 | 71.58 | 46.00 |\\n| MFS    | 53.68 | 59.89 | 68.08 | 50.00 | 49.07 |\\n| MFS+   | 84.21 | 87.91 | 86.38 | 83.33 | 88.89 |\\n| SPDI   | 28.30 | 46.14 | 49.01 | 33.64 | 59.58 |\\n| SFII   | 34.78 | 56.04 | 57.71 | 41.97 | 64.97 |\\n| ESCHER | 66.86 | 67.89 | 66.67 | 66.76 | 68.42 |\\n| Mean   |       |       |       |       | 67.32 |\\n\\n### Figure 5\\n\\n(a) Sense Frequency Index\\n\\n(b) Sense Polysemy Degree\"}"}
{"id": "acl-2022-long-298", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Model-specific Analyses\\n\\n| Acronym | MISS | Accuracy | MFS | MFS+ | SPDI | SFII | ESCHER |\\n|---------|------|----------|-----|------|------|------|--------|\\n| DE      | 35.87| 21.90    | 56.76| 86.82| 79.54| 86.61| 71.04  |\\n| ES      | 23.29| 22.54    | 61.96| 89.05| 78.41| 83.84| 72.76  |\\n| IT      | 23.12| 18.04    | 61.96| 87.23| 80.62| 85.47| 72.58  |\\n| RU      | 35.37| 22.89    | 48.12| 83.28| 83.49| 84.01| 69.55  |\\n| ZH      | 32.49| 15.04    | 56.05| 88.20| 87.98| 91.97| 71.89  |\\n| Mean    | 30.03| 20.08    | 56.97| 86.92| 82.01| 86.38| 71.56  |\\n\\nFigure 6: Evaluation on Google\\n\\n(a) Sense Frequency Index\\n\\n(b) Sense Polysemy Degree\"}"}
{"id": "acl-2022-long-298", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7: Evaluation on OPUS\\n\\n(a) Sense Frequency Index\\n(b) Sense Polysemy Degree\"}"}
{"id": "acl-2022-long-298", "page_num": 16, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8: Evaluation on M2M100\\n\\nFigure 9: Overall Language Cooccurrence Heatmap for M2M100\\n\\n| Language | MISS Accuracy MFS | MISS Accuracy MFS+ | SPDI | SFII | ESCHER |\\n|----------|------------------|--------------------|------|------|--------|\\n| DE       | 49.41            | 22.19              | 61.28| 87.23| 76.15  |\\n| ES       | 41.91            | 25.51              | 61.81| 89.37| 77.95  |\\n| IT       | 42.44            | 21.83              | 60.75| 86.79| 76.58  |\\n| RU       | 51.77            | 26.22              | 47.87| 83.41| 78.34  |\\n| ZH       | 48.66            | 16.99              | 59.06| 91.34| 87.18  |\\n| Mean     | 46.84            | 22.55              | 58.15| 87.63| 79.24  |\"}"}
