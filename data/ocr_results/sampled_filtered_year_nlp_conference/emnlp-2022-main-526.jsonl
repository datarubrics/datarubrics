{"id": "emnlp-2022-main-526", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present CLID SUM, a benchmark dataset towards building cross-lingual summarization systems on dialogue documents. It consists of 67k+ dialogue documents and 112k+ annotated summaries in different target languages. Based on the proposed CLID SUM, we introduce two benchmark settings for supervised and semi-supervised scenarios, respectively. We then build various baseline systems in different paradigms (pipeline and end-to-end) and conduct extensive experiments on CLID SUM to provide deeper analyses. Furthermore, we propose mDIALBART which extends mBART via further pre-training, where the multiple objectives help the pre-trained model capture the structural characteristics as well as key content in dialogues and the transformation from source to the target language. Experimental results show the superiority of mDIALBART, as an end-to-end model, outperforms strong pipeline models on CLID SUM. Finally, we discuss specific challenges that current approaches faced with this task and give multiple promising directions for future research. We have released the dataset and code at https://github.com/krystalan/ClidSum.\\n\\n1 Introduction\\n\\nCross-Lingual Summarization (XLS) aims at generating a summary in the target language from a given document in a different source language. Existing XLS datasets mainly focus on single-participant documents, such as news reports (Zhu et al., 2019; Nguyen and Daum\u00e9 III, 2019; Bai et al., 2021), how-to guides (Ladhak et al., 2020) and encyclopedia articles (Perez-Beltrachini and Lapata, 2021). Many efforts have been devoted to improving XLS for single-participant documents and achieved great success (Yao et al., 2015; Shen et al., 2018; Ouyang et al., 2019; Cao et al., 2020; Zhu et al., 2020; Maurya et al., 2021; Chi et al., 2021; Liang et al., 2022; Wang et al., 2022).\\n\\nDifferent from single-participant documents, dialogue is a discourse produced by more than one person (Haviland, 1990). The multi-participant dialogue documents, which record the communications between human and human/machine, have attracted wide research attention due to their key role in daily interpersonal interaction (Zhang et al., 2020c). Meanwhile, the globalization progress has prompted conversations among interlocutors of different native languages and brought many scenarios, e.g., international academic conferences and business meetings. Thus, it is valuable to provide speakers with the summary in their familiar language to help them efficiently grasp the gist of a foreign language dialogue. Nevertheless, dialogue-oriented XLS is still under-explored due to the lack of corresponding datasets.\\n\\nTo this end, we introduce Cross-Lingual Dialogue Summarization (XLDS) task that aims to summarize a dialogue in the source language into a different language. To promote the XLDS research, we construct CLID SUM (Cross-Lingual Dialogue SUMmarization), the first large-scale XLDS benchmark dataset with three features: (1) The proposed CLID SUM is based on two existing...\"}"}
{"id": "emnlp-2022-main-526", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"monolingual dialogue summarization datasets, i.e., SAMSum (Gliwa et al., 2019) and MediaSum (Zhu et al., 2021). We choose these two datasets under the consideration of their quality and diversity. (2) To make these datasets suitable for XLDS, we employ professional translators to translate original English summaries of SAMSum and MediaSum to German and Chinese. Eventually, the translated corpora constitute CLIDSUM, which totally contains ~56.4k En\u21d2De and ~56.4k En\u21d2Zh XLDS samples. (3) Besides the supervised benchmark setting that has been discussed by most previous XLS work, we argue that it is necessary to utilize large-scale monolingual dialogue summarization pairs for XLDS due to the dearth of cross-lingual samples. Thus, we design a semi-supervised setting, where a large number of monolingual pairs together with a relatively small scale of cross-lingual pairs are used to build XLDS systems.\\n\\nBased on CLIDSUM, we build and evaluate various baseline systems, including summarize-then-translate, translate-then-summarize and end-to-end paradigms: (1) For summarize-then-translate baselines, we use a monolingual dialogue summarizer to generate summaries and further translate them to the target language through a machine translation (MT) model/service. (2) For translate-then-summarize baselines, we utilize a MT model/service to translate the dialogue documents from the source language to the target language, and then obtain summaries via a monolingual dialogue summarizer. (3) As for end-to-end paradigm, we adopt mBART-50 (Tang et al., 2021), a multi-lingual sequence-to-sequence (Seq2Seq) model which has been pre-trained on a large-scale corpora across 50 languages using denoising objectives. Though such a general model achieves surprising results in many downstream tasks, its performance will naturally degrade when there is a relatively large gap between the pre-training and fune-tuning stages (Lai et al., 2021; Zhong et al., 2022). Therefore, to narrow the gap and fully use large-scale monolingual dialogue summarization pairs provided in semi-supervised setting, we propose mDIALBART that extends mBART-50 through the second stage of pre-training with four objectives: action infilling, utterance permutation, monolingual dialogue summarization and machine translation. Specifically, action infilling and utterance permutation encourage the model to effectively capture the structural characteristics in dialogues. Monolingual dialogue summarization gives the model the ability to summarize dialogues. Machine translation enables the model to learn the transformation from the source language to the target language. The pre-training corpora are provided in the semi-supervised setting, which means we do not use additional data other than CLIDSUM. The experimental results show that mDIALBART outperforms strong pipeline baselines on CLIDSUM. We also conduct human studies to compare generated summaries from different methods and discuss specific challenges that current approaches faced with XLDS. We hope that our work could promote the development of XLDS.\\n\\nOur main contributions are concluded as follows:\\n\\n\u2022 We introduce XLDS task and present CLIDSUM, the first large-scale XLDS benchmark dataset collecting about 56.4k En\u21d2De and 56.4k En\u21d2Zh samples via crowd-sourcing. Furthermore, we design supervised and semi-supervised benchmark settings on CLIDSUM.\\n\\n\u2022 We elaborately build and evaluate various baselines of different paradigms (i.e., translate-then-summarize, summarize-then-translate and end-to-end) to provide deeper analyses.\\n\\n\u2022 We propose the mDIALBART model that extends mBART-50 via the second pre-training stage. Experimental results show that mDIALBART outperforms existing baselines.\\n\\n\u2022 To provide a deeper understanding of the XLDS task and the CLIDSUM dataset, we analyze the results of different methods and conclude the critical difficulties of XLDS on CLIDSUM.\\n\\n2 Related Work\\n\\nCross-Lingual Summarization. We divide existing XLS datasets into synthetic datasets and multi-lingual website datasets according to the construction methods. (1) Synthetic datasets are constructed through translating the summaries of existing text summarization datasets to a target language, such as En2ZhSum, Zh2EnSum (Zhu et al., 2019) and En2DeSum (Bai et al., 2021). These datasets are further equipped with the round-trip translation strategy (Zhu et al., 2019; Zhang et al., 2021; Lai et al., 2022) to filter out low-quality samples. (2) Multi-lingual website datasets are collected from websites which provide multi-lingual versions for their articles. For instance, WikiLingual (Ladhak et al., 2020) collects articles from the WikiHow...\"}"}
{"id": "emnlp-2022-main-526", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"website, where many English articles are translated to non-English versions by human writers. Each article also links to parallel articles in other languages, if available. Thus, it is handy to collect different language versions for one article. Next, the summary of each language-specific article is extracted through a heuristic strategy. In this way, the article in one language and its summary in a different language could constitute an XLS sample. In the similar way, Global Voices (Nguyen and Daum\u00e9 III, 2019) and XWikis (Perez-Beltrachini and Lapata, 2021) collect multi-lingual articles from Global Voices and Wikipedia websites, respectively. Early XLS methods (Wan et al., 2010; Zhang et al., 2016; Ayana et al., 2018; Ouyang et al., 2019) are based on pipeline paradigms due to the scarcity of parallel corpus. Recently, Zhu et al. (2019) propose the first large-scale XLS dataset and further explore the multi-task learning on end-to-end (e2e) XLS systems which achieve great improvement over pipeline methods. Subsequently, many efforts have contributed to the e2e XLS systems. Among them, Zhu et al. (2020) exploit the translation patterns in XLS. Cao et al. (2020) propose a framework that jointly learns to align and summarize for XLS. Xu et al. (2020) explore pre-training strategy on XLS. Liang et al. (2022) adopt conditional variational auto-encoder (VAE) (Sohn et al., 2015) to deal with XLS. Different from existing XLS work, we shift research attention from single-participant documents to multi-participant dialogues.\\n\\n3.1 Data Selection\\n\\nAs discussed in Section 2, there are two types of XLS datasets: synthetic datasets and multi-lingual website datasets. To our knowledge, there is no public website that provides multi-lingual dialogue-summary pairs. Therefore, we decide to construct XLDS dataset through translating original summaries of existing dialogue summarization datasets. After carefully comparing existing datasets, we finally choose SAMSum (Gliwa et al., 2019) and MediaSum (Zhu et al., 2021) datasets due to the following reasons: (1) these datasets are of high quality, both of which consist of real-world or human-labeled monolingual dialogue-summary pairs; (2) these datasets collect dialogues in a wide range of domains in daily life.\\n\\n3.2 Data Annotation\\n\\nSince the size of MediaSum (~464k) is much larger than any other dialogue summarization datasets (typically less than 30k), it is costly to translate all summaries from MediaSum to target languages via crowd-sourcing. Hence, we randomly select 20k samples from its training set, which together with all samples of validation set and testing set form MediaSum40k (totally 40k samples) subset. The remaining monolingual data is denoted as MediaSum424k. We then translate original English summaries from SAMSum and MediaSum40k to German and Chinese through data annotation. There are 55 professional translators, 3 data reviewers and 1 data expert participating in the annotation process. All En $\\\\Rightarrow$ Zh translators have passed the TEM-8 while all En $\\\\Rightarrow$ De translators have passed both the TEM-8 and the PGH. The Test for English Majors\u2014Band 8 (TEM-8) and Pr\u00fcfung f\u00fcr das Germanistik Hauptstudium (PGH) are two language qualifications in China, which measure the overall English and German proficiency of undergraduates majoring in English and German, respectively.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Dataset | Domain | Trans. | Src Lang. | Tgt Lang. | Documents | Doc. Length | Src Summ Length | Tgt Summ Length |\\n|---------|--------|--------|-----------|-----------|-----------|-------------|----------------|----------------|\\n| En2ZhSum | News | Auto | En | Zh | 370,687 | 755.0 | 55.2 | 96.0 |\\n| Zh2EnSum | News | Auto | Zh | En | 1,699,713 | 103.7 | 17.9 | 13.7 |\\n| En2DeSum | News | Auto | En | De | 437,797 | 31.0 | 8.5 | 7.5 |\\n| XSAMSum | Chat | Manual | En | De/Zh | 16,369 | 83.9 | 20.3 | 19.9/33.0 |\\n| XMediaSum40k | Interview | Manual | En | De/Zh | 40,000 | 1555.4 | 14.4 | 14.8/30.0 |\\n| MediaSum424k | Interview | - | En | - | 423,596 | - | - | - |\\n\\nTable 1: Statistics of C\\\\text{LID}S\\\\text{UM} and previous synthetic XLS datasets. Trans. indicates the translation method (automatic or manual) to construct dataset. Src Lang. and Tgt Lang. denote the source and target languages (En: English, Zh: Chinese or De: German) for each dataset. Documents represents the size of each dataset. Doc. Length, Src Summ Length and Tgt Summ Length show the average length of documents, source summaries and target summaries (word-level for English and German while character-level for Chinese) for each dataset, respectively.\\n\\nTable 2: Statistics of the dialogue characteristics in (X)SAMSum and (X)MediaSum40k. Dial. indicates the number of dialogue documents for each subset of these datasets. Turns and Speakers represent the average number of utterances and speakers in dialogues.\\n\\nData reviewers and the expert are proficient in En-English, German and Chinese, and have rich experiences in checking translation quality. During the annotation process, 20% of the summaries translated by each translator are checked by a reviewer. If the accuracy is lower than 95%, the translator needs to modify all his/her translated summaries under the guidance of the reviewer. To guarantee the overall quality, after the above process for one translation direction of SAMSum or MediaSum40k, 2% of the translated summaries are randomly sampled and checked by the data expert. If the accuracy is lower than 95%, the corresponding translators and reviewers need to revise their translated summaries again. This quality control loop is executed 74, 1, 1 and 2 times for SAMSum(En \u21d2 Zh), SAMSum(En \u21d2 De), MediaSum40k(En \u21d2 Zh) and MediaSum40k(En \u21d2 De), respectively.\\n\\nIn the end, we collect 56,369 Chinese and 56,369 German summaries. We denote the summary-translated SAMSum and MediaSum40k as XSAMSum and XMediaSum40k, respectively, which also inherit the data splitting of original datasets. The three datasets together constitute C\\\\text{LID}S\\\\text{UM} benchmark dataset.\\n\\n3.3 Data Analysis\\nWe compare C\\\\text{LID}S\\\\text{UM} with previous synthetic datasets in Table 1. C\\\\text{LID}S\\\\text{UM} is the first large-scale XLS dataset towards dialogues, and it is also the first manually translated XLS dataset. While one may argue that the scale of previous synthetic datasets is much larger than XSAMSum and XMediaSum40k, it is worth noting that 1) the scale of all dialogue summarization datasets is less than 30k except MediaSum, and 2) the quality of XSAMSum and XMediaSum40k is much higher than those automatically constructed ones. In addition, as shown in Table 2, the dialogues from (X)MediaSum40k contain more turns (~30 vs. ~11) and more speakers (~9.3 vs. ~2.4) than (X)SAMSum, leading to more challenges for XLDS research.\\n\\n3.4 Task Overview\\nGiven a dialogue $D = \\\\{u_1, u_2, \\\\ldots, u_|D|\\\\}$ in the source language, where $u_i$ denotes the $i$-th utterance in $D$, the XLDS task aims to generate corresponding summary $Y_t = \\\\{y_1, y_2, \\\\ldots, y_{|Y_t|}\\\\}$ in a different target language, where $y_j$ denotes the $j$-th token in $Y_t$.\\n\\n3.5 Benchmark Settings\\nWe design two benchmark settings for supervised and semi-supervised scenarios, respectively. In the Supervised Setting, an XLDS system is established on the training set of XSAMSum or XMediaSum40k and evaluated on the corresponding test set. In the Semi-Supervised Setting, the training set of XMediaSum40k and the whole MediaSum424k are used to build XLDS systems which are evaluated on the test set of XMediaSum40k.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4 Baselines\\n\\n4.1 Pipeline Method\\nThe main idea of the pipeline method is decomposing XLDS task into dialogue summarization and machine translation sub-tasks. It can be further divided into summarize-then-translate and translate-then-summarize paradigms.\\n\\n**Summarize-then-translate.** In this paradigm, a monolingual dialogue summarizer is used to generate summary in the same language with the given dialogue, and then obtain target language summary through machine translation. The following models are adopted as the dialogue summarizers:\\n\\n- PEGASUS (Zhang et al., 2020a) is a pre-trained abstractive summarization model.\\n- T5 (Raffel et al., 2020) is a jointly pre-trained Seq2Seq model for many downstream NLP tasks.\\n- BART (Lewis et al., 2020) is another pre-trained Seq2Seq model using denoising objectives during pre-training stage.\\n- mBART-50 (Tang et al., 2021) is a multi-lingual version of BART, which could be also employed on monolingual tasks, though it does not take advantage of its multi-lingual ability.\\n- MV-BART (Chen and Yang, 2020) is a BART-based multi-view dialogue summarization model which utilizes conversation information from different views, e.g., topic view and stage view.\\n- BART(DALL) (Feng et al., 2021) encodes additional dialogue characteristics to BART. The characteristics are extracted by a DialoGPT (Zhang et al., 2020c) annotator.\\n\\nTo translate the generated summaries from the source to the target language, previous XLS work (Ladhak et al., 2020; Perez-Beltrachini and Lapata, 2021) only adopts sophisticated translation services, e.g., Google Translation and AWS Translation. Though great performances, the black-box APIs provided by translation services are constantly updating, which leads to poor reproducibility. Thus, we decide to adopt both translation services and models. Specifically, we consider three translation methods: (1) Google Translation; (2) OPUS-MT (Tiedemann and Thottingal, 2020) releases lots of transformer-based MT models which are trained on OPUS corpus with different translation directions; (3) Trans-WMT20: we train two transformer-based Seq2Seq models on WMT20 parallel corpus from scratch, including En\u21d2De and En\u21d2Zh.\\n\\n**Translate-then-summarize.** This paradigm first translates English dialogues from CLIDSUM to German and Chinese through the same translation methods as the summarize-then-translate, and then generate summaries via mBART-50.\\n\\n4.2 End-to-End Method\\nThe end-to-end method needs simultaneously learn both dialogue summarization and machine translation. We finetune mBART-50 with input dialogues from the source language, and summaries from the target language. Note that mBART-50 is used in both pipeline and end-to-end paradigms, where the languages of input and output sequences are the same when used in the pipeline paradigm while different in the end-to-end paradigm. To indicate the input and output languages, mBART-50 appends language identifiers (e.g., En, De and Zh) at both the encoder and the decoder sides.\\n\\n5 mDIALBART\\nAs suggested by previous work (Zhu et al., 2020; Ladhak et al., 2020), building an end-to-end model is preferable to a pipeline one due to: 1) the pipeline models suffer from error propagation; 2) the translation systems in pipeline paradigm require either a large parallel corpus to train MT models or the monetary cost of paid MT services; 3) the pipeline models have a recurring latency during inference (Ladhak et al., 2020). Thus, it is valuable and urgent to explore end-to-end models on XLDS. To this end, we propose mDIALBART which extends mBART-50 via a second stage of pre-training, where the objectives help the pre-trained model better adapt to the XLDS task (cf., Figure 2).\\n\\n5.1 Pre-Training Tasks and Corpora\\n**Action Infilling (AcI).** Action triples (i.e., \\\"who-doing-what\\\") help the pre-trained model explicitly be aware of the actions within utterances for generating more factual summaries (Chen and Yang, 2021). Following Chen and Yang (2021), we extract action triples through OpenIE systems (Angeli et al., 2015). Then we randomly sample text spans from action triples and replace each span with a [MASK] token. About 15% of tokens in an utterance will be masked. The action infilling task requires the pre-trained model to reconstruct original dialogues based on the masked dialogues.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Maria: Are you preparing slides?\\n\\nUtterance 1, Utterance 2, Utterance 3\\n\\nAction Infilling (a)\\n\\nUtterance 1, Utterance 2, Utterance 3\\n\\nUtterance Permutation (b)\\n\\nUtterance 1, Utterance 2, Utterance 3\\n\\nMonolingual Dialogue Summarization (c)\\n\\nMachine Translation (d)\\n\\nFigure 2: The second stage of pre-training in mDIALBART.\\n\\nThere will be a meeting...\\n\\nFigure 3: Finetune mDIALBART on XLDS.\\n\\n5.2 Multi-Task Pre-Training\\n\\nFor each iteration of the second pre-training stage, training samples from the four tasks are randomly selected and used to calculate the summation loss and update the parameters. Thus, it is necessary to make the pre-trained model distinguish different tasks, to avoid the confusion brought by the joint training. Specifically, AcI, UP and MT could be regarded as reconstruction tasks. To distinguish MDS from others, a special token \\\\[SUM\\\\] is appended at the beginning of input sequences of MDS.\\n\\n6 Experiments\\n\\nWe evaluate mDIALBART and various baselines on CLIDSUM. Four automatic metrics are adopted in our experiments: ROUGE-1 (R1) and ROUGE-2 (R2) (Lin, 2004) evaluate unigram and bigram overlap between the generated summaries and corresponding references, respectively. ROUGE-L (R-L) (Lin, 2004) is applied to find the length of the longest common subsequence. BERTScore (B-S) (Zhang et al., 2020b) evaluates the semantic similarity of generated sentences against the references. For evaluation toolkits and the implementation details of all models, please refer to Appendix A.\\n\\n6.1 Main Results\\n\\nTable 3 shows the experimental results. We first analyze the performance of pipeline baselines, and then compare them with end-to-end baseline. Secondly, we compare mDIALBART with all baselines to demonstrate its superiority. Lastly, we introduce a simple data augmentation strategy to further improve the performance of end-to-end methods. For generated cases, please refer to Appendix B.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Experimental results on C_{LID}S. The bold and underline denote the best and the second scores, respectively. \\n\\n\u2020 represents that the evaluated models utilize the monolingual dialogue-summary pairs provided by the semi-supervised setting. Sum-Trans: Summarize-then-translate, Trans-Sum: Translate-then-summarize.\\n\\nTable 4: Translation results on WMT20 (En\u21d2De and En\u21d2Zh newstest2020). \u2191 indicates higher is better. \u2193 indicates lower is better.\\n\\nfind that Trans-Sum outperforms Sum-Trans accompanied with the same summarizer (row 19-21 vs. row 10-12) due to the limited amount of monolingual dialogue-summary pairs in the source language. Besides, the English-centric dialogue summarization work (Chen and Yang, 2020; Feng et al., 2021) reduces the summarization error and helps Sum-Trans overtake Trans-Sum (row 15/18 vs. row 21). Moreover, as shown in Table 4, we test the performance of the adopted MT methods on WMT20-newstest2020. Google Trans performs best in both En\u21d2De and En\u21d2Zh directions, and OPUS-MT outperforms Trans-WMT in En\u21d2De translation, but is worse in En\u21d2Zh translation. With the same dialogue summarizer, the XLDS performance of pipeline methods is consistent with the performance of the corresponding MT method.\\n\\nEnd-to-End vs. Pipeline Baselines. Comparing the performance between end-to-end and pipeline baselines, mBART-50 achieves competitive results with the strong Trans-Sum baselines (row 22 vs. row 21), but performs worse than the strong Sum-Trans baselines (row 22 vs. row 15/18). This is because: (1) the strong pipeline baselines adopt the sophisticated translation service which leverages a large amount of parallel data; (2) the end-to-end models need both the abilities to translate and summarize, which requires a large amount of cross-lingual training data. Nevertheless, existing datasets can not fully meet this requirement.\\n\\nmDIAL BART vs. All Baselines. To further explore the end-to-end paradigm on XLDS, we propose mDIAL BART that extends mBART-50 through the second pre-training stage. Experimental results show that mDIAL BART outperforms all baselines in all automatic metrics (row 24). Note that the data used in the second pre-training stage is provided in the semi-supervised setting of C_{LID}S. That is, mDIAL BART is a semi-supervised XLDS system and thus we only evaluate mDIAL BART on the test set of XMediaSum40k.\\n\\nData Augmentation. Moreover, we construct a large number of pseudo-XLDS samples by translating original English summaries from MediaSum424k to German and Chinese via Google.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6.2 Ablation Study\\n\\nAs shown in Table 5, we conduct ablation studies on XMediaSum40k to evaluate the contribution of each pre-training task. All four tasks contribute to the second pre-training stage. The most important one is MDS, which is most relevant to XLDS in all pre-training tasks. Both MDS and XLDS need the pre-trained model to understand the main content of dialogues and further summarize them. MT and UP bring less benefits than others due to the fewer pre-training samples and the lower required capability, respectively. Specifically, the number of MT pre-training samples is 20k, which is significantly fewer than other tasks. UP only requires the pre-trained model to restore the order of utterances in noisy dialogues rather than predicting new words. Such NLU-style task would make it easier for models to learn shortcuts rather than the semantic understanding and reasoning (Du et al., 2021). For ablation studies of combining two or three tasks, please refer to Appendix C.\\n\\n6.3 Human Study\\n\\nWe conduct human studies to further evaluate the performances of the strong baselines under each paradigm and our pre-trained model, i.e., BART(ALL) + Google Trans, Google Trans + mBART, mBART and mDIALBART. We randomly select 50 samples from the test set of XMediaSum40k. Seven crowd workers with high levels of fluency in English and Chinese are asked to assess the generated Chinese summaries from three aspects: grammaticality (Gram.), informativeness (Info.) and conciseness (Conci.). Following the Best-Worst Scaling method (Kiritchenko and Mohammad, 2017), crowd workers are asked to select the best and the worst generated summaries on each criteria. The result scores are calculated based on the percentage of times each model is selected as best minus the times it is selected as worst. Thus, the final scores should range from -1 (worst) to 1 (best). Table 6 shows the results of human studies. Our mDIALBART outperforms strong baselines in all metrics, indicating its superiority. The Fleiss\u2019 Kappa scores (Fleiss, 1971) of Gram., Info. and Conci. are 0.37, 0.26 and 0.43, respectively, indicating a good inter-agreement among our evaluators.\\n\\n6.4 XLDS Difficulties\\n\\nTo further study the specific challenges of end-to-end XLDS and give multiple promising directions for future research, we take a closer look at CLID-SUM and model generation errors. We conclude the following difficulties worthy of research attention:\\n\\n- **Multiple Topics.** The dialogues in MediaSum40k are interview transcripts from NPR and CNN (Zhu et al., 2021), and each transcript usually records multiple topics from different events. Previous dialogue summarization work (Chen and Yang, 2020; Feng et al., 2021) has proved the effectiveness of topic information and proposed topic-aware summarizers. Thus, it is also necessary to explore topic-aware end-to-end XLDS models.\\n\\n- **Low Resource.** End-to-end XLDS models learn both dialogue summarization and machine translation simultaneously, which require a large amount of training data. Intuitively, as a more difficult task, XLDS needs more training samples than MT when building an end-to-end model. Following previous MT work (Lin et al., 2020), a parallel corpus is considered as an extremely low resource when the number of its samples is less than 100k. Therefore, it is hard to learn XLDS well when only utilizing...\"}"}
{"id": "emnlp-2022-main-526", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"supervised data provided by CLID SUM. We believe the low-resource XLDS is also worth studying.\\n\\nDomain Adaption. There are various dialogue domains in daily life (e.g., chat, interview and debates), and it is impractical to construct XLDS datasets for each domain. Therefore, it is valuable to utilize XLDS data of one domain to improve the model performance of others. Moreover, we find that mDIALBART performs slightly worse than mBART baseline when fine-tuning on XSAMSum, indicating its limited domain adaption capability.\\n\\n7 Conclusion\\n\\nIn this paper, we introduce XLDS task and present CLIDSUM, the first large-scale XLDS benchmark dataset. We also propose mDIALBART, a XLDS-oriented pre-trained model that extends mBART-50 via the second pre-training stage. The carefully designed pre-training tasks help mDIALBART better adapt to the XLDS task. Experiments on CLIDSUM demonstrate that our mDIALBART outperforms strong baseline models.\\n\\nEthical Considerations\\n\\nWe discuss the main ethical considerations of CLIDSUM benchmark dataset as follows: (1) Licenses. CLIDSUM is derived from the SAMSum (Gliwa et al., 2019) and the MediaSum (Zhu et al., 2021), both of which are well-constructed and published datasets. We will follow the CC BY-NC-ND 4.0 license of SAMSum to make XSAMSum public. Following the requirements of MediaSum, we restrict our usage to research purpose only, and will release the translated MediaSum under CC-BY-NC-SA 4.0 license. (2) Compensation. During the translation annotation, the salary for annotating each summary is determined by the average time of annotation and local labor compensation standard. (3) Data characteristics. We refer readers to the content and (Gliwa et al., 2019; Zhu et al., 2021) for more detailed characteristics. (4) Potential problems. While principled measures are taken to ensure the quality of the dataset, there might still be potential problems with the dataset quality, which may lead to incorrect translations in applications. We also consider potential ethical issues of our mDIALBART pre-trained model: mDIALBART inherits mBART-50 (Tang et al., 2021) and is further trained on the XMediaSum40k and MediaSum424k corpora. Therefore, mDIALBART could reflect the same biases and toxic behaviors exhibited by language models, such as biases about race and gender (Sheng et al., 2020).\\n\\nLimitations\\n\\nWhile we show that mDIALBART performs best on XMediaSum40k, there are limitations that provide avenues for future work. (1) As we discussed in Section 6.4, the domain adaption capability of mDIALBART is limited. We think future work can focus on designing general or unified XLDS pre-trained models which could be applied in multiple dialogue domains. (2) Not all the parameters of mDIALBART are completely useful, such as the token embeddings of irrelevant languages, which reduces the inference speed.\\n\\nAcknowledgements\\n\\nWe would like to thank anonymous reviewers for their suggestions and comments. This work is supported by the National Natural Science Foundation of China (No.62072323, 62102276), Shanghai Science and Technology Innovation Action Plan (No. 22511104700), the Natural Science Foundation of Jiangsu Province (Grant No. BK20210705), and the Natural Science Foundation of Educational Commission of Jiangsu Province, China (Grant No. 21KJD520005).\\n\\nReferences\\n\\nGabor Angeli, Melvin Jose Johnson Premkumar, and Christopher D. Manning. 2015. Leveraging linguistic structure for open domain information extraction. In Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 344\u2013354, Beijing, China. Association for Computational Linguistics.\\n\\nAyana, Shiqi Shen, Yun Chen, Cheng Yang, Zhiyuan Liu, and Maosong Sun. 2018. Zero-shot cross-lingual neural headline generation. IEEE/ACM Transactions on Audio, Speech, and Language Processing, 26:2319\u20132327.\\n\\nYu Bai, Yang Gao, and Heyan Huang. 2021. Cross-lingual abstractive summarization with limited parallel resources. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 6910\u20136924, Online. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2022-main-526", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-526", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-526", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2022-main-526", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Implementation Details\\n\\nA.1 Automatic Evaluation\\nTo calculate ROUGE scores, we employ the multilingual ROUGE toolkit that considers segmentation and stemming algorithms for various languages. To calculate BERTScore, we use the bert-score toolkit.\\n\\nA.2 Translation Models\\nFor OPUS-MT models, we directly utilize the pre-trained OPUS-MT-en-de and OPUS-MT-en-zh models. For Trans-WMT translation model, we follow Liang et al. (2021) to train transformer-base models (Vaswani et al., 2017) (512 hidden size, 2048 filter size, 8 multi-head attention, 6 encoder layers and 6 decoder layers) on WMT20 parallel corpus. Specifically, for En$\\\\Rightarrow$De translation, we combine six corpora including Euporal, ParaCrawl, CommonCrawl, TildeRapid, NewsCommentary, and WikiMatrix. And for En$\\\\Rightarrow$Zh translation, we combine News Commentary v15, Wiki Titles v2, UN Parallel Corpus V1.0, CCMT Corpus, and WikiMatrix. To preprocess the raw data, we employ a series of open-source/in-house scripts, including full-/half-width conversion, unicode conversation, punctuation normalization, and tokenization (Wang et al., 2020). After filtering steps, we generate subwords via joint BPE (Sennrich et al., 2016) with 32K merge operations. Finally, we obtain 45,541,367 sentence pairs for En$\\\\Rightarrow$De and 22,244,006 sentence pairs for En$\\\\Rightarrow$Zh, respectively.\\n\\nA.3 Pre-Trained Language Models\\nThe implementation of pre-trained models used in our baselines is provided by the Huggingface Transformers (Wolf et al., 2020), i.e., PEGASUS-large, T5-large, BART-large and mBART-50. In the fine-tuning process of pipeline baselines, we set the batch size of PEGASUS, T5, BART and mBART to 16 for XSAMSum and 24 for XMediaSum40k. The corresponding learning rates are set to 4e-5 and 2e-5, respectively. All these models are fine-tuned with 20 epochs. The hyperparameters of MV-BART (Chen and Yang, 2020) and BART(DALL) (Feng et al., 2021) are the same as original paper. In the fine-tuning process of end-to-end baseline, mBART-50 is fine-tuned with 4 batch size, 5e-6 learning rate and 20 epochs.\\n\\nA.4 mDIALBART\\nOur mDIALBART first inherits from the mBART-50 (1024 hidden size, 16 multi-head attention, 12 encoder layers and 12 decoder layers) and then suffers from the second stage of pre-training, which are conducted utilizing 8 NVIDIA Tesla V100 GPUs with 32GB memory. mDIALBART is trained using pytorch-lightning framework with 5e-6 learning rate and 8 batch size. We set the warmup steps and total steps to 5,000 and 1,000,000 steps, respectively. The pre-trained mDIALBART is next fine-tuned on the XLDS task using a single GPU with 4 batch size and 5e-6 learning rate. The maximum number of tokens for input sequences is 1024. In the test process, the beam size is 5, and the maximum decoded length is 150.\\n\\nB Case Study\\nWe give the generated summaries of strong baselines and mDIALBART in Figure 4. The dialogue in the left example only discusses one event, i.e., Hillary Clinton declares candidacy for the U.S. Senate seat from New York. All these models can generate a suitable summary that matches this event.\\n\\n14https://huggingface.co/google/pegasus-large\\n15https://huggingface.co/t5-large\\n16https://huggingface.co/facebook/bart-large\\n17https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt\\n18https://github.com/PyTorchLightning/pytorch-lightning\"}"}
{"id": "emnlp-2022-main-526", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hillary Clinton announces candidacy for New York Senate.\\n\\nTwo Americans were abducted by pirates in Nigeria.\\n\\nFirefighter pilot dies in Australian wildfires; pirates take two Americans from a US oil supply vessel.\\n\\nHillary Clinton announces candidacy for the US Senate of New York.\\n\\nLarge-scale wildfires in Australia; pirates kidnap two Americans from a US oil supply vessel.\\n\\nHillary Clinton announces her Senate candidacy for New York.\\n\\nThe first lady of the Democrats announces her Senate candidacy.\\n\\nEducation and health-care reform among the first lady's primary issues. Her agenda...\\n\\nMr. Sweeney, what holds the key?\\n\\nIt is the people, and it's not whether they're going to vote Democratic or Republican, they're going to vote for what is proven.\\n\\nMalveaux: All right, Vladimir, thank you so much. We certainly hope they're safe.\\n\\nRandall: Mr. Sweeney, what holds the key?\\n\\nSweeney: It is the people, and it's not whether they're going to vote Democratic or Republican, they're going to vote for what is proven.\\n\\nTable 7: Ablation study of mDIAL BART.\\n\\nHowever, the content of the right dialogue across many events, we find that all baselines capture only one event (i.e., the pirate event) while our mDIAL BART is aware of the two different events in the dialogue and exactly summarize it.\\n\\nC Effect of Each Pre-Training Task\\n\\nTable 7 shows the complete ablation studies of mDIAL BART on XMediaSum40k. We find that combining two tasks in the second pre-training stage outperforms using only one of them, e.g., mBART (+AcI+UP) outperforms both mBART (+AcI) and mBART (+UP). mBART (+MDS+MT) performs best in the setting of combining two tasks due to the high relevance with XLDS. As for combining three or four tasks, we find that MDS and MT are vital to the second pre-training stage. The performance of the pre-trained model with both MDS and MT is significantly better than others (row 14-16 vs. row 12-13). In addition, the performance of mBART (+AcI+MDS+MT), mBART (+UP+MDS+MT) and mDIAL BART is similar. We think this is because both AcI and UP help the pre-trained model capture the structural characteristics in dialogues and these two tasks are redundant to some extent.\\n\\nXLDS could be regarded as the combination of MDS and MT.\"}"}
