{"id": "emnlp-2023-main-105", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nGenerative AI models exhibit remarkable potential; however, hallucinations across various tasks present a significant challenge, particularly for longer inputs that current approaches struggle to address effectively. We introduce SCALE (Source Chunking Approach for Large-scale Inconsistency Evaluation), a task-agnostic model for detecting factual inconsistencies using a novel chunking strategy. Specifically, SCALE is a Natural language inference (NLI) based model that uses large text chunks to condition over long texts. This approach achieves state-of-the-art performance in factual inconsistency detection for diverse tasks and long inputs. Additionally, we leverage the chunking mechanism and employ a novel algorithm to explain SCALE\u2019s decisions through relevant source sentence retrieval. Our evaluations reveal that SCALE outperforms existing methods on both standard benchmarks and a new long-form dialogue dataset ScreenEval we constructed. Moreover, SCALE surpasses competitive systems in efficiency and model explanation evaluations. We have released our code and data publicly to GitHub.\\n\\n1 Introduction\\nLarge Language Models (LLMs) have shown immense promise in various applications, but deploying them in real-time presents certain challenges such as hallucinations (Cao et al., 2018; Falke et al., 2019; Krysci\u0144ski et al., 2019; Fabbri et al., 2021a; Honovich et al., 2022). Hallucinations, or factual inconsistencies generated by a model relative to a source document, can mislead the user and undermine trust in LLMs. Thus, detecting factual inconsistency in LLM generations is crucial for the future of LLMs, especially with the growing popularity of platforms like ChatGPT.\\n\\nPrior research on inconsistency detection has predominantly dealt with short documents in offline settings (Laban et al., 2022; Schuster et al., 2022; Utama et al., 2022) and relied heavily on sentence-level text matching techniques. Consequently, these methods exhibit slow performance in processing longer documents and suffer from poor calibration. Such characteristics pose substantial challenges in implementing them in real-world online environments, where incorporating inconsistency detection could potentially result in a substantial increase in latency. Additionally, the absence of well-calibrated scores complicates the balancing act between mitigating the risk of incorporating hallucinations and excluding pertinent information from the model output. Given the exponential growth in context sizes (maximum allowed tokens of an input) of contemporary large language models (LLMs), there is an increasing urgency to develop efficient and effective approaches for inconsistency detection in lengthy documents.\\n\\nIn addressing the challenges, we introduce SCALE (Source Chunking Approach for Large-scale Inconsistency Evaluation), a method designed for efficient detection of factual inconsistencies in generated sentences by identifying related source text snippets. SCALE consists of two crucial components. First, it builds on a Natural language inference (NLI) based method, integrating a novel chunking mechanism for rapid and accurate online performance in diverse natural language generation (NLG) tasks. Second, model explanation is essential for real-time deployment of inconsistency detection systems, facilitating swift human inspection to determine model configurations. We show that our chunking mechanism improves calibration scores and enables the use of a binary search tree algorithm for rapidly locating relevant source text snippets for a target sentence, ultimately enhancing...\"}"}
{"id": "emnlp-2023-main-105", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the explanation of model behaviors.\\n\\nCurrent benchmark datasets for factual inconsistency detection predominantly feature short documents. In order to evaluate SCALE using a more realistic dataset with long documents, we introduce ScreenEval \u2014 a novel dataset designed to assess the factual inconsistency of summary sentences generated by humans, Longformer, and GPT-4 in comparison to actual long-form dialogues. ScreenEval encompasses 52 dialogues, averaging over 6,000 tokens per dialogue. The use of dialogue in this dataset poses a considerable unique challenge, such as long-distance coreference resolution and significant noise between utterances. To the best of our knowledge, ScreenEval is the longest dialogue based dataset for factual inconsistency detection presently available.\\n\\nIn our experiments, we first show that SCALE outperforms and is better calibrated than baseline methods across various NLG tasks on the standard factual inconsistency detection benchmark TRUE (Honovich et al., 2022). We then assess accuracy, speed, and model explanation (via relevant text retrieval evaluation) on the new ScreenEval dataset for long document factual inconsistency detection. Our findings indicate that SCALE surpasses strong competitors in the majority of tests.\\n\\nThe key contributions of this paper are:\\n\\n\u2022 We introduce SCALE, a reference-free, NLI-based factual inconsistency detection method with a novel chunking strategy for versatility across domains and extended documents.\\n\u2022 We show SCALE's broad applicability in NLG domains by attaining state-of-the-art performance on the TRUE benchmark.\\n\u2022 We build ScreenEval, a novel dataset designed for factual inconsistency detection in long dialogues, and then demonstrate SCALE's superiority in accuracy, efficiency, and model explanation evaluations on the dataset.\\n\\n2 Related Work\\n\\nFactual Inconsistency Detection\\n\\nThere are two main directions in factual inconsistency detection: Natural language inference (NLI) based and question answering (QA) based methods. In NLI based methods, pretrained NLI models can be utilized to determine whether a given \\\"premise\\\" factually entails a \\\"hypothesis.\\\" Although initial attempts encountered challenges (Khot et al., 2018), recent advancements have shown that NLI models can effectively assess the factual consistency of generated text (hypothesis) with respect to a source (premise) (Utama et al., 2022). This progress can largely be attributed to addressing the granularity problem, which arises from the abundance of current NLI datasets predominantly comprised of short, single-sentence premises and hypotheses (Williams et al., 2017; Nie et al., 2019; Thorne et al., 2018a; Schuster et al., 2021a).\\n\\nSCALE is an NLI based method and our findings indicate that utilizing larger premise chunks enhances efficiency and outperforms sentence decomposition. Although SeNtLI (Schuster et al., 2022) extended NLI based methods to longer documents, it adhered to the sentence decomposition assumption and focused solely on summarization tasks. SummaC (Laban et al., 2022) investigated various aggregation techniques for NLI scores obtained from sentence decomposition to generate overall summary scores. Meanwhile, SWING (Huang et al., 2023) developed a loss function to train models for improved NLI performance, yielding mixed outcomes.\\n\\nIn QA based methods, a question is first generated based on a summary sentence, and a QA system is used to give an answer. A summary is considered factually consistent if the generated answer significantly overlaps with the original summary (Durmus et al., 2020). Prior research focused on using different question generation strategies (Scialom et al., 2019) or overlap measures (Deutsch and Roth, 2021). In the experiments, we consider the most competitive QuestEval (Scialom et al., 2021a) and QAFactEval (Fabbri et al., 2021b).\\n\\nDetecting Factual Inconsistencies in Long Documents\\n\\nPrior work on factual inconsistency performance in long source documents has been limited in scope. For example, ContractNLI (Koreeda and Manning, 2021) concentrates on legal documents, which differ significantly from dialogues in terms of challenges. Likewise, LongEval (Krishna et al., 2023) emphasizes human evaluation strategies for scoring, without considering dialogues. To our knowledge, this paper presents the first dataset for evaluating factual inconsistency in long-form dialogues, ScreenEval, thus addressing a substantial gap in the literature.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3 SCALE\\n\\nIn this section we elaborate on our approach taken for our inconsistency detection model SCALE.\\n\\nFirstly, we formally define the use of chunks and NLI in SCALE, aiming at improving the accuracy and efficiency of the system. Secondly, we propose to explain the model output by retrieving the relevant source sentence for a target, and show how the relevant sentence retrieval can be improved through the use of chunks.\\n\\n3.1 Chunking Mechanism for NLI based Model\\n\\nOur approach uses NLI (Dagan et al., 2006) as a building block for factual inconsistency detection. An NLI model $M$ provides the relationship between a premise $p$ and a hypothesis $h$, $M(p, h)$ with probabilities of three labels: entailed, neutral, and contradictory. For example, given an NLI model $M(p, h)$, source document $D$ with a set of facts $F_D$, and a generated text $G$ with a set of facts $F_G$, if $F_G \\\\subseteq F_D$ we would expect $M(D, G)$ to produce high entailment probability.\\n\\nWe define factual consistency between a generated text and source document as $F_G \\\\subseteq F_D$.\\n\\nCanonical NLI models cannot be properly used for factual inconsistency detection because both $p$ and $h$ are commonly single sentences in NLI models, however in the factual inconsistency task their equivalents $D$ and $G$ almost always contain multiple sentences which $M$ cannot effectively handle, leading to an issue known as the granularity problem (Utama et al., 2022). To bypass the granularity problem, a natural generalization is to split both $D$ and $G$ into sentences and run $M$ pairwise on each of those sentences then using an aggregation function $f$ to generate the final entailment probability. Numerous papers have used this approach to generate competitive results (Schuster et al., 2022; Laban et al., 2022) however this generalization is hindered by a few shortcomings.\\n\\nFirst, the sentence decomposition of $D$ and $G$ does not properly capture the context provided in $D$. By decomposing $D$ and $G$ into single sentences $D = (d_1, d_2, \\\\ldots, d_i, \\\\ldots, d_{|D|})$ and $G = (g_1, g_2, \\\\ldots, g_j, \\\\ldots, g_{|G|})$ and put into the model to evaluate as $M(d_i, g_j)$, the context and long term dependencies present in $D$ that may have factually supported $g_j$ very likely could not be represented in $d_i$. Multiple sentences (e.g., $\\\\bigcup_{i \\\\in \\\\{1, 3, 6\\\\}} d_i$) together in unison may be needed to support a single claim $g_j$. However, evaluating $g_j$ against each sentence individually $M(d_1, g_j), M(d_3, g_j), M(d_6, g_j)$ would likely lead to artificially low scores. Second, evaluating pairwise sentences of $D$ and $G$ is slow. It requires $|D| \\\\cdot |G|$ model runs to obtain a final score for one sentence $g_j$.\\n\\nSCALE poses a different solution to the granularity problem by decomposing $D$ into much larger chunks, which can be visualized in Figure 1. Formally, SCALE decomposes document $D$ into a set of $N$ chunks $C = c_1, c_2, \\\\ldots, c_N$ such that $\\\\bigcup_{c \\\\in C} c = D$. SCALE can handle chunks of arbitrary length only limited by memory requirements, thus drastically increasing the context window.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Visualization of SCALE\u2019s in context retrieval using chunks to find the most relevant source utterance given a sentence. Each chunk is scored by SCALE, as shown by the gray boxes.\\n\\nThe generated text $G$ is broken into sentences $G = (g_1, g_2, \\\\ldots, g_j, \\\\ldots, g_{|G|})$. We propose that decomposing $D$ using chunks rather than sentences does not negatively affect the granularity problem but rather enables superior contextual capture in model $M$, boosts accuracy, and requires significantly less model runs.\\n\\nSCALE uses Flan-T5 (Chung et al., 2022) as a backbone NLI model $M$. SCALE obtains the probability that a chunk $c_i$ entails a generated sentence $g_j$ through the following steps. First, logits are obtained by prompting $M$ with the following: logits = $M(\\\\{c_i\\\\} \\\\text{ Question: does this imply } \\\\{g_j\\\\}? \\\\text{ Yes or no?})$.\\n\\nThe entailment probability between $g_j$ and $c_i$ is then calculated by $P_{\\\\text{entail}} = \\\\text{SoftMax}(\\\\text{logits}[\\\\text{\\\"Yes\\\"}], \\\\text{logits}[\\\\text{\\\"No\\\"}])[0]$.\\n\\nTo obtain the overall entailment score for a generated sentence $g_j$, the results are aggregated over all possible $c_i$ by $SCALE(C, g_j) = \\\\max_{i=1}^{N} (P_{\\\\text{entail}}(c_i, g_j))$ to obtain a final measure of factual consistency.\\n\\n3.2 Model Explanation via Relevant Source Text Retrieval\\n\\nTo produce explainable and interpretable scores for factual consistency and empower necessary human inspection, it is important to justify the score by retrieving relevant text from the source. Formally, the retrieval task involves finding the most relevant sentence $d_i$ in document $D$ with respect to a hypothesis $h$. Using a new search tree approach enabled by chunking, SCALE is able to retrieve $d_i$ in context while using far fewer model runs than previous approaches. We use a greedy search tree approach that evaluates a hypothesis using SCALE against progressively smaller chunks to find the highly relevant text from the source document. For the following example, assume we use a binary search tree (BST) at each level, dividing the text into two chunks. This process can be visualized in Figure 2. Given a hypothesis $h$, we want to find the most relevant utterance $d_i$ in the source text. We begin by dividing the entire source document into two large chunks. SCALE is used to calculate the score between both chunks and the hypothesis $h$ and then use the higher scoring chunk as the new source text. The new source text is then divided into two chunks, and continues to descend in this manner until the chunk size becomes a single sentence or utterance $d_i$. The best scoring chunk is then chosen to be the supporting proof of the hypothesis from the source document.\\n\\nThis retrieval approach is able to significantly reduce the number of model calls needed to find the relevant text from a source document. Previous approaches commonly break the source document down by sentence which requires $O(n)$ model calls for a source document with $n$ sentences. Whereas our BST approach only needs $O(\\\\log(n))$ model calls in order to find the most relevant utterance in the same source document of $n$ sentences.\\n\\nNotice that we proposed the binary search scheme due to its simplicity and its connection to the popular binary search tree. In practice, dividing the source text into only two chunks might cause out of GPU memory issues. In this case, we could generalize the proposed approach into different chunk splits. For example, we could divide the remaining tokens into three chunks or larger for the search of each block until the model fits the chunk. We could also use different chunk sizes for different layers so long as it fits in the memory.\\n\\n4 ScreenEval Dataset\\n\\nWe introduce a novel dataset for evaluating inconsistency detection on long form dialogues called ScreenEval. This dataset uses TV scripts and summaries pulled from the SummScreen (Chen et al., 2021) dataset. In addition to the provided human summaries, we generate summaries using Longformer and GPT-4 on 52 scripts from the SummScreen dataset.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics for the ScreenEval dataset. These statistics use the Flan-T5 tokenizer.\\n\\nWe then hire human annotators to classify the factual inconsistency of each summary sentence and identify relevant supporting utterances for factually consistent summary sentences. ScreenEval is released publicly. Details of how we use Longformer and GPT-4 and collect human annotation can be found in the Appendix A.\\n\\nThe SummScreen dataset is comprised of 2 sub datasets pulled from different sources Forever-Dreaming and TVMegaSite. We use the Forever-Dreaming subset of SummScreen, (SummScreen-FD) to create ScreenEval due to its manageable summary size and diversity of shows and genres, spanning a total of 21 genres. SummScreen-FD uses human-written gold summaries from Wikipedia and TVMaze. Table 1 shows statistics related to ScreenEval. Notably, the average number of tokens in a source document is 6,073, which, to the best of our knowledge, makes ScreenEval the longest dialogue based inconsistency detection dataset created. We provide 52 documents with an associated 624 summary sentences, 455 of which are artificially generated using Longformer and GPT-4. Summaries are kept at a high level, covering major plot points and character developments. This leads to shorter and more concise summaries that on average only run 101 tokens.\\n\\n5 Experiments\\n\\n5.1 Datasets\\n\\nTRUE\\n\\nThe TRUE benchmark contains 11 datasets from 4 different NLG tasks. We compare our approach to others on this dataset to show how SCALE performs across a multitude of NLG tasks on inconsistency detection. Notably, the average number of tokens per example in the TRUE benchmark is small, generally less than 512. Each dataset in the TRUE benchmark is condensed down into a source document, generated text, and factual inconsistency label. The datasets are distributed across different tasks as shown in Table 2.\\n\\n| Task          | Examples | Datasets |\\n|---------------|----------|----------|\\n| Summarization | 5,245    | 5        |\\n| Dialogue      | 10,613   | 3        |\\n| Fact Verification | 81,263 | 2        |\\n| Paraphrasing  | 8,000    | 1        |\\n\\nTable 2: Number of examples and datasets for each task in the TRUE benchmark.\\n\\nSummarization datasets are from FRANK (Pagnoni et al., 2021), SummEval (Fabbri et al., 2021a), MNBM (Maynez et al., 2020), QAGS-CNNDM (Wang et al., 2020), and QAGS-XSum (Wang et al., 2020). Dialogue datasets include BE-BING (Dziri et al., 2022), Q\u06f1 (Honovich et al., 2021), and DialFact (Gupta et al., 2021). Fact Verification datasets are FEVER (Thorne et al., 2018b) and VitaminC (Schuster et al., 2021b). The Paraphrasing dataset is PAWS (Zhang et al., 2019b).\\n\\nScreenEval\\n\\nOur dataset ScreenEval compares the inconsistency detection ability of methods on long form dialogue. Details can be found in Sec. 4.\\n\\n5.2 Competitive Systems\\n\\nTRUE provides 9 inconsistency detection baselines from 4 different inconsistency detection styles, namely n-gram based methods (token level F1), model based methods (BERTScore (Zhang et al., 2019a), BLEURT (Sellam et al., 2020), FactCC (Kryscinski et al., 2020), BARTScore (Yuan et al., 2021), CTC (Deng et al., 2021)), NLI based methods (ANLI (Honovich et al., 2022), SummaC (Laban et al., 2022)), and question answering (QA) based methods (Q\u06f1 (Honovich et al., 2021), QuestEval (Scialom et al., 2021b)).\\n\\nFor ScreenEval we compare SCALE to 8 models which use NLI, QA, modern GPT systems, and older n-gram and semantic similarity methods. The baseline models consist of two NLI based sentence decomposition approaches seNtLI (Schuster et al., 2022), and SummaC_conv (Laban et al., 2022), a state-of-the-art QA based model QAFactEval (Fabbri et al., 2022), a multidimensional QA model UniEval (Zhong et al., 2022), a semantic similarity model based method BERTScore (Zhang et al., 2019a), an n-gram overlap method ROUGE (Lin, 2004), and the two recent OpenAI models Chat-GPT, and GPT-4.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also compare the performance of SCALE's search tree based relevant utterance retrieval with other recent retrieval models. We compare SCALE to the retrieval performance of Super-Pal (Ernst et al., 2020) which was shown to have superior retrieval capabilities in LongEval. We also compare against seNtLI (Schuster et al., 2022) which was designed to perform retrieval to identify factual inconsistencies over long documents.\\n\\nFor SCALE, we include three variants of Flan-T5 as the backbone, namely base, XL, and XXL.\\n\\n5.3 Metrics\\n\\nAccuracy Evaluation\\n\\nWe compare the performance of methods primarily using four metrics, ROC_AUC score, Pearson correlation, Kendall_Tau correlation, and F1_Macro score. We employ the ROC_AUC score to quantify the ability of different methods in accurately identifying true positives and true negatives. Pearson and Kendall_Tau correlations show the relationship between methods and labels by measuring the correlations between the two. Finally the F1_Macro score is used to compare the continuous outputs of SCALE to the discrete outputs of GPT-4 and ChatGPT. To obtain an F1 score for SCALE we use the optimal threshold to convert its continuous output into discrete values.\\n\\nEfficiency Evaluation\\n\\nWe measure wall clock time in seconds for all of our experiments on ScreenEval. Wall clock time demonstrates how SCALE can be used efficiently in an online setting especially when compared to other models.\\n\\nModel Explanation Evaluation\\n\\nWe evaluate Calibration and Relevant Source Text Retrieval for model explanation.\\n\\nCalibration is the measure of how close the pseudo-probability outputs of a model are to the actual probability of a correct prediction. For example, a well calibrated model that produces a score of 0.2 would have a 20% probability of being classified as 1. In this paper, we use Expected Calibration Error (ECE) (Guo et al., 2017) to compare the calibration of SCALE to other commonly used models.\\n\\nGiven model outputs spanning from 0 to 1, ECE separates the outputs into K equally sized bins \\\\( B_k \\\\) between 0 and 1 and takes the difference between accuracy \\\\( acc \\\\) and confidence \\\\( conf \\\\) in each one.\\n\\nThe accuracy of a bin \\\\( B_k \\\\) is the average amount of predicted labels that match true class labels in a bin, formally defined as\\n\\n\\\\[\\nacc(B_k) = \\\\frac{1}{|B_k|} \\\\sum_{i \\\\in B_k} 1(\\\\hat{y}_i = y_i),\\n\\\\]\\n\\nwhere \\\\( \\\\hat{y}_i \\\\) and \\\\( y_i \\\\) are the predicted and true class labels for sample \\\\( i \\\\).\\n\\nConfidence in a bin \\\\( B_k \\\\) shows the average predicted score in a bin, formally defined as\\n\\n\\\\[\\nconf(B_k) = \\\\frac{1}{|B_k|} \\\\sum_{i \\\\in B_k} \\\\hat{p}_i,\\n\\\\]\\n\\nwhere \\\\( \\\\hat{p}_i \\\\) is the model output score for sample \\\\( i \\\\).\\n\\nThen the following equation is used to calculate ECE,\\n\\n\\\\[\\nECE = \\\\sum_{k=1}^{K} \\\\left| \\\\frac{1}{|B_k|} \\\\sum_{i \\\\in B_k} 1(\\\\hat{y}_i = y_i) - \\\\frac{1}{|B_k|} \\\\sum_{i \\\\in B_k} \\\\hat{p}_i \\\\right|,\\n\\\\]\\n\\nusing equation (1) and (2). A lower ECE indicates a better calibration.\\n\\nRelevant Source Text Retrieval tests if each model could return the correct utterance identified as relevant by human labelers. We report the recall of retrieval results.\\n\\n6 Results\\n\\n6.1 TRUE\\n\\nWe first evaluate SCALE on the TRUE benchmark to confirm SCALE is NLG task agnostic and generalizes well to the factual inconsistency detection.\\n\\nAccuracy Evaluation Results\\n\\nFor the TRUE benchmark as shown in Table 3, SCALE XXL provides superior performance in 10 out of the 11 datasets, and SCALE XL achieves superior performance in 8 datasets compared to other non SCALE models. Notably, other models were not previously able to perform well across all tasks, with Q2 metric having superior performance across 3 datasets and ANLI having superior performance across 5. These results demonstrate SCALE's ability to perform well across domains and against a large variety of model types.\\n\\nModel Explanation Evaluation Results\\n\\nNot only does SCALE provide superior performance on the TRUE benchmark, but it is also highly calibrated across NLG tasks. Table 4 shows the ECE of multiple methods across the TRUE benchmark datasets. Note that a lower ECE is better. SCALE large provides the best calibration on...\"}"}
{"id": "emnlp-2023-main-105", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: TRUE benchmark results. ROC_AUC scores multiplied by 100 for readability. Since SC ZS uses FEVER and VitC in training, these two datasets are excluded when computing the average.\\n\\nTable 4: TRUE Calibration Results. ECE of each method on TRUE benchmark datasets (lower is better).\\n\\nA visual example of the calibration results can be analyzed with the calibration curves in Figure 3. While most models are severely uncalibrated and underestimate the fraction of positives in Figure 3, SCALE is capable of sticking extremely close to the perfectly calibrated line. The closest model SummaC can be seen overestimating positive examples before scores reach 0.4. We hypothesize that the large context window is the key to better calibration in SCALE as it includes more information. This makes the underlying NLI model less likely to be biased toward a specific range of tokens which leads to extreme confidence based on certain short text. To empirically justify this, we perform further experiments on the proposed ScreenEval dataset shown in Figure 4. We can observe that for chunk size < 400, the calibration score (the lower the better) is much higher than larger chunk size 500 to 1000. This shows that a larger chunk size could enable the NLI model to extract more useful information to provide appropriate confidence when making the prediction. We also use this knowledge to support our decision to use 512 tokens as our chunk size for all experiments in this paper. The enhanced calibration achieved by SCALE allows it to be more interpretable as a probability, making it a valuable tool for comparison tasks.\\n\\nFigure 3: Calibration curves on the PAWS benchmark.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Effect of different chunk sizes on calibration performance on ScreenEval dataset.\\n\\n| Model       | Metric     | Pearson | Kendall-Tau | ROC-AUC | Time (s) |\\n|-------------|------------|---------|-------------|---------|----------|\\n| Rouge-1     |            | 0.120   | 0.093       |         | 57.6     |\\n| BERTScore   |            | 0.130   | 0.076       |         | 56.2     |\\n| SummaC      |            | 0.030   | 0.011       |         | 50.9     |\\n| UniEval     |            | 0.066   | 0.094       |         | 1139     |\\n| seNtLI      |            | 0.005   | 0.096       |         | 12688    |\\n| QAFactEval  |            | 0.331   | 0.293       |         | 12132    |\\n| SCALE base  |            | 0.28    | 0.24        |         | 678      |\\n| SCALE large |            | 0.391   | 0.322       |         | 1991     |\\n\\nTable 5: Factual inconsistency detection results on ScreenEval. ROC_AUC is multiplied by 100 for readability.\\n\\nSummaC conv and seNtLI, which are designed to deal with long documents, have poor performance on ScreenEval. Along with its superior performance, SCALE is able to run faster than other LLM based methods on ScreenEval also shown in Table 5. For a fair comparison, we set the batch size to 1 for all models and run with all other default settings. We do not include BERTScore due to its truncation of the document, making timing not comparable. Most notably QAFactEval, which was closest in performance to SCALE large, was 6 times slower than SCALE large in wall clock time. Even faster though was SCALE base which was 17 times faster than QAFactEval while only achieving slightly worse performance across all metrics on ScreenEval, and outperforming all non-SCALE methods other than QAFactEval. The SCALE base model running at 1.1 seconds per score for long documents could realistically be used in an online setting to more accurately evaluate factual inconsistency.\\n\\nChunk size proves to have a large effect on the ability of SCALE\u2019s performance and time as seen in Figure 5. SCALE large sees a sharp increase in performance up until the chunk size is 1000 tokens. Similarly, there is a sharp decrease in model run time up until 1000 tokens. Figure 5 substantiates our approach to the granularity problem by illustrating that a larger number of tokens in the premise leads to a more effective method.\\n\\nModel | F1-score macro | Cost | Time (s) |\\n|------|---------------|------|----------|\\n| SCALE XL | 73.86 | - | 5425 |\\n| SCALE large | 68.74 | - | 1991 |\\n| GPT-4   | 77.95  | $102 | 7255  |\\n| ChatGPT | 56.50  | $5  | 2544  |\\n\\nTable 6: Compare SCALE with ChatGPT and GPT-4 on ScreenEval.\\n\\nWe additionally compare SCALE with ChatGPT and GPT-4 on ScreenEval in Table 6. Due to the discrete nature of GPT-4 and ChatGPT\u2019s output, we choose an ideal threshold for SCALE and compare macro F1-Scores on the ScreenEval dataset.\\n\\nWhile GPT-4 is able to outperform SCALE XL in macro F1-Score, SCALE shows to be significantly better in terms of time and cost. ChatGPT is more comparable in terms of time and cost to SCALE; however, there is a significant performance drop in macro F1-Score. ChatGPT is also limited by its 4096 token length limit at the time of writing and must use truncated conversations from ScreenEval. These results help us conclude that while GPT-4 has superior performance, SCALE is able to provide a faster, more affordable model that can be used locally in an online setting to produce continuous scores.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7: Relevant source text retrieval results on ScreenEval. Time is measured in average number of seconds taken to retrieve the most relevant utterance on ScreenEval conversations.\\n\\n| Model        | Explanation | Evaluation | Results |\\n|--------------|-------------|------------|---------|\\n| SCALE XL     | BST retrieval approach | SuperPal and seNtLI | SCALE outperforms both in terms of time and performance as shown in Table 7. SCALE XL identifies the most relevant utterances correctly 47% of the time compared to 34% for seNtLI and 41% for SuperPal. SCALE's BST retrieval requires significantly fewer model calls, allowing it to pinpoint relevant utterances without having to score each one individually like the other methods. This results in higher retrieval recall for both SCALE large and SCALE XL. Moreover, because SCALE large requires far less model calls it is able to provide faster results than Super-Pal or seNtLI without compromising effectiveness. This enhanced performance shows how SCALE could be used in an online setting for fast and accurate results.\\n\\n7 Conclusion\\nIn this paper, we introduce a cutting-edge NLI based factual inconsistency detection method called SCALE. We show that SCALE is an NLG task agnostic factual inconsistency detection method by achieving state-of-the-art results across four distinct NLG tasks and 11 datasets within the TRUE benchmark. We show that across NLG tasks SCALE is also superior in calibration, providing scores that are interpretable and enables accurate comparison of scores. We introduce a new dataset called ScreenEval that is the first of its kind for long form dialogue factual inconsistency evaluation. SCALE is shown to significantly outperform all models other than GPT-4 in factual inconsistency detection on this dataset. However, we show that SCALE is significantly more cost effective and faster than GPT-4 for evaluation on ScreenEval. Moreover, we introduce a new retrieval strategy enabled by SCALE that significantly decreases the time to retrieve relevant utterances from a long document with increased recall accuracy.\\n\\n8 Limitations\\nWhile SCALE performs well at inconsistency detection there are some limitations to this approach. SCALE only uses the \u201cYes\u201d and \u201cNo\u201d logits to compute it\u2019s entailment score, however only using those two logits specifically could lead to loss of accuracy due to other information possibly flowing to similar tokens such as \u201cyes\u201d, \u201cno\u201d. Using logits for scoring purposes may cause a loss of information to other similar logits.\\n\\nFinally, even though SCALE is able to achieve better calibration in the aggregate, it still struggles with calibration on certain tasks and this can even vary by model size. Consistent calibration of scoring methods across NLG tasks should be a goal for future methods.\\n\\nReferences\\nZiqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018. Faithful to the original: Fact aware neural abstractive summarization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.\\nMingda Chen, Zewei Chu, Sam Wiseman, and Kevin Gimpel. 2021. Summscreen: A dataset for abstractive screenplay summarization. arXiv preprint arXiv:2104.07091.\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\\nIdo Dagan, Oren Glickman, and Bernardo Magnini. 2006. The pascal recognising textual entailment challenge. In Machine Learning Challenges. Evaluating Predictive Uncertainty, Visual Object Classification, and Recognising Textual Entailment: First PASCAL Machine Learning Challenges Workshop, MLCW 2005, Southampton, UK, April 11-13, 2005, Revised Selected Papers, pages 177\u2013190. Springer.\\nMingkai Deng, Bowen Tan, Zhengzhong Liu, Eric Xing, and Zhiting Hu. 2021. Compression, transduction, and creation: A unified framework for evaluating natural language generation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7580\u20137605, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\nDaniel Deutsch and Dan Roth. 2021. Understanding the extent to which content quality metrics measure the information quality of summaries. In Proceedings of the 25th Conference on Computational Natural Language Learning, pages 300\u2013309.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Esin Durmus, He He, and Mona Diab. 2020. Feqa: A question answering evaluation framework for faithfulness assessment in abstractive summarization. arXiv preprint arXiv:2005.03754.\\n\\nNouha Dziri, Hannah Rashkin, Tal Linzen, and David Reitter. 2022. Evaluating attribution in dialogue systems: The begin benchmark. Transactions of the Association for Computational Linguistics, 10:1066\u20131083.\\n\\nOri Ernst, Ori Shapira, Ramakanth Pasunuru, Michael Lepioshkin, Jacob Goldberger, Mohit Bansal, and Ido Dagan. 2020. Summary-source proposition-level alignment: Task, datasets and supervised baseline. arXiv preprint arXiv:2009.00590.\\n\\nAlexander Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. QAFactEval: Improved QA-based factual consistency evaluation for summarization. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 2587\u20132601, Seattle, United States. Association for Computational Linguistics.\\n\\nAlexander R Fabbri, Wojciech Krystynski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. 2021a. Summeval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391\u2013409.\\n\\nAlexander R Fabbri, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021b. Qafacteval: Improved qa-based factual consistency evaluation for summarization. arXiv preprint arXiv:2112.08542.\\n\\nTobias Falke, Leonardo FR Ribeiro, Prasetya Ajie Utama, Ido Dagan, and Iryna Gurevych. 2019. Ranking generated summaries by correctness: An interesting but challenging application for natural language inference. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2214\u20132220.\\n\\nChuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. 2017. On calibration of modern neural networks. In International conference on machine learning, pages 1321\u20131330. PMLR.\\n\\nPrakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2021. Dialfact: A benchmark for fact-checking in dialogue. arXiv preprint arXiv:2110.08222.\\n\\nOr Honovich, Roee Aharoni, Jonathan Herzig, Hagai Taitelbaum, Doron Kukliansy, Vered Cohen, Thomas Scialom, Idan Szpektor, Avinatan Hassidim, and Yossi Matias. 2022. True: Re-evaluating factual consistency evaluation. arXiv preprint arXiv:2204.04991.\\n\\nOr Honovich, Leshem Choshen, Roee Aharoni, Ella Neeman, Idan Szpektor, and Omri Abend. 2021. q2: Evaluating factual consistency in knowledge-grounded dialogues via question generation and question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 7856\u20137870, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nKung-Hsiang Huang, Siffi Singh, Xiaofei Ma, Wei Xiao, Feng Nan, Nicholas Dingwall, William Yang Wang, and Kathleen McKeown. 2023. Swing: Balancing coverage and faithfulness for dialogue summarization. arXiv preprint arXiv:2301.10483.\\n\\nTushar Khot, Ashish Sabharwal, and Peter Clark. 2018. Scitail: A textual entailment dataset from science question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32.\\n\\nYuta Koreeda and Christopher D Manning. 2021. Contractnli: A dataset for document-level natural language inference for contracts. arXiv preprint arXiv:2110.01799.\\n\\nKalpesh Krishna, Erin Bransom, Bailey Kuehl, Mohit Iyyer, Pradeep Dasigi, Arman Cohan, and Kyle Lo. 2023. Longeval: Guidelines for human evaluation of faithfulness in long-form summarization. arXiv preprint arXiv:2301.13298.\\n\\nWojciech Krystynski, Bryan McCann, Caiming Xiong, and Richard Socher. 2019. Evaluating the factual consistency of abstractive text summarization. arXiv preprint arXiv:1910.12840.\\n\\nWojciech Krystynski, Bryan McCann, Caiming Xiong, and Richard Socher. 2020. Evaluating the factual consistency of abstractive text summarization. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 9332\u20139346, Online. Association for Computational Linguistics.\\n\\nPhilippe Laban, Tobias Schnabel, Paul N Bennett, and Marti A Hearst. 2022. Summac: Re-visiting nli-based models for inconsistency detection in summarization. Transactions of the Association for Computational Linguistics, 10:163\u2013177.\\n\\nChin-Yew Lin. 2004. Rouge: A package for automatic evaluation of summaries. In Text summarization branches out, pages 74\u201381.\\n\\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and Ryan McDonald. 2020. On faithfulness and factuality in abstractive summarization. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 1906\u20131919, Online. Association for Computational Linguistics.\\n\\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, Jason Weston, and Douwe Kiela. 2019. Adversarial nli: A new benchmark for natural language understanding. arXiv preprint arXiv:1910.14599.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Artidoro Pagnoni, Vidhisha Balachandran, and Yulia Tsvetkov. 2021. Understanding factuality in abstractive summarization with FRANK: A benchmark for factuality metrics. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4812\u20134829, Online. Association for Computational Linguistics.\\n\\nTal Schuster, Sihao Chen, Senaka Buthpitiya, Alex Fabrikant, and Donald Metzler. 2022. Stretching sentence-pair nli models to reason over long documents and clusters. arXiv preprint arXiv:2204.07447.\\n\\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021a. Get your vitamin C! robust fact verification with contrastive evidence. arXiv preprint arXiv:2103.08541.\\n\\nTal Schuster, Adam Fisch, and Regina Barzilay. 2021b. Get your vitamin C! robust fact verification with contrastive evidence. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 624\u2013643, Online. Association for Computational Linguistics.\\n\\nThomas Scialom, Paul-Alexis Dray, Patrick Gallinari, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, and Alex Wang. 2021a. Questeval: Summarization asks for fact-based evaluation. arXiv preprint arXiv:2103.12693.\\n\\nThomas Scialom, Paul-Alexis Dray, Sylvain Lamprier, Benjamin Piwowarski, Jacopo Staiano, Alex Wang, and Patrick Gallinari. 2021b. QuestEval: Summarization asks for fact-based evaluation. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6594\u20136604, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.\\n\\nThomas Scialom, Sylvain Lamprier, Benjamin Piwowarski, and Jacopo Staiano. 2019. Answers unite! unsupervised metrics for reinforced summarization models. arXiv preprint arXiv:1909.01610.\\n\\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational Linguistics.\\n\\nJames Thorne, Andreas Vlachos, Christos Christodoulopoulos, and Arpit Mittal. 2018a. Fever: a large-scale dataset for fact extraction and verification. arXiv preprint arXiv:1803.05355.\\n\\nJames Thorne, Andreas Vlachos, Oana Cocarascu, Christos Christodoulopoulos, and Arpit Mittal. 2018b. The fact extraction and VERification (FEVER) shared task. In Proceedings of the First Workshop on Fact Extraction and VERification (FEVER), pages 1\u20139, Brussels, Belgium. Association for Computational Linguistics.\\n\\nPrasetya Ajie Utama, Joshua Bambrick, Nafise Sadat Moosavi, and Iryna Gurevych. 2022. Falsesum: Generating document-level nli examples for recognizing factual inconsistency in summarization. arXiv preprint arXiv:2205.06009.\\n\\nAlex Wang, Kyunghyun Cho, and Mike Lewis. 2020. Asking and answering questions to evaluate the factual consistency of summaries. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5008\u20135020, Online. Association for Computational Linguistics.\\n\\nAdina Williams, Nikita Nangia, and Samuel R Bowman. 2017. A broad-coverage challenge corpus for sentence understanding through inference. arXiv preprint arXiv:1704.05426.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. Advances in Neural Information Processing Systems, 34:27263\u201327277.\\n\\nTianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Weinberger, and Yoav Artzi. 2019a. Bertscore: Evaluating text generation with bert. arXiv preprint arXiv:1904.09675.\\n\\nYuan Zhang, Jason Baldridge, and Luheng He. 2019b. PAWS: Paraphrase adversaries from word scrambling. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 1298\u20131308, Minneapolis, Minnesota. Association for Computational Linguistics.\\n\\nMing Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. 2022. Towards a unified multidimensional evaluator for text generation. arXiv preprint arXiv:2210.07197.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Appendix\\nA Details of Construction of ScreenEval Dataset\\n\\nWe create summaries for ScreenEval using two summarization models, GPT-4 and Longformer, as well as human generated summaries. The models that we use for summarization are designed to have a large token context window, giving them both the ability to globally attend over the long source dialogues we provide. Below, we will first explain how to leverage Longformer, GPT-4 to generate summaries, and then explain the process and the cost of human labeling.\\n\\nA.1 Building the Dataset\\n\\nWe first generated Longformer summaries for each script in the test set. To keep the annotation task reasonable, and to filter out any rambling summaries, we filter out any TV scripts with a Longformer or human summary that had more than 6 sentences or only 1 sentence. We still preserve 52 of the TV scripts by doing this, as the median number of summary sentences in a Longformer summary was 4 and a human summary was 3. In order to meet GPT-4's token limit requirements, from the remaining TV scripts we chose all that had less than 8,100 tokens. Our final dataset consists of 52 TV scripts that have an average length of 6073 tokens with 624 summary sentences.\\n\\nA.2 Longformer\\n\\nWe use the same baseline model as in SummScreen to generate summaries in ScreenEval, a Longformer model finetuned on SummScreenFD's training set. This model uses a transformer based sequence to sequence architecture to globally attend over the entire dialogue. Longformer is able to take as many as 16384 tokens as input.\\n\\nA.3 GPT-4\\n\\nGPT-4 is a large language model that has shown near human level performance in a wide variety of tasks, including summarization. We task GPT-4 to summarize each document in ScreenEval using the prompt \\\"Summarize in 5 sentences or less: \\\". To accommodate for the roughly 8k token limit on the GPT-4 model, we specifically select documents in ScreenEval that are under 8k tokens.\\n\\nA.4 Human Annotation\\n\\nWe label ScreenEval using workers from amazon mechanical turks with the prompt shown in Figure 6 and Figure 7. For each task, a worker is presented with a TV script from ScreenEval along with a highlighted summary sentence. The worker is instructed to first read through the source dialogue. Then, the worker is instructed to click either a Yes or No button to indicate whether the highlighted summary sentence is consistent with the source document. Each utterance in the TV script will be presented alongside a check box where if the worker chose \\\"Yes\\\" to the consistency question, they will be asked to select the relevant utterances that led to their answer.\\n\\nWe had 3 human annotators label each instance, and 61% of the time all three annotators agreed. Workers were paid 0.27 per task. We ensured the quality of annotators through a number of methods. First, we filtered annotators to just those located in the US and Canada to increase the chances of high fluency in English on our reading comprehension task. Additionally the workers had to have an MTurk \\\"Master\\\" qualification, greater than a 95% task approval rate, and greater than 5000 tasks approved. The dataset was labeled in batches of 30 at a time and closely monitored by the authors. Workers were only rejected if they did not list relevant utterances as instructed or listed non-existent utterances, and these workers were able to dispute rejection via email.\\n\\nB Prompts Used for GPT\\n\\nB.1 ChatGPT/GPT-4\\n\\nQuestion: does the previous conversation factually imply \\\"{Summary Sentence}\\\"?\\nAnswer Yes or No.\"}"}
{"id": "emnlp-2023-main-105", "page_num": 13, "content": "{\"primary_language\":null,\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
