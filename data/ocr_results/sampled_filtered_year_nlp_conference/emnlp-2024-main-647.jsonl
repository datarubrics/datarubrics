{"id": "emnlp-2024-main-647", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SciEx: Benchmarking Large Language Models on Scientific Exams with Human Expert Grading and Automatic Grading\\nTu Anh Dinh, Carlos Mullov, Leonard B\u00e4rmann, Zhaolin Li, Danni Liu, Simon Rei\u00df, Jueun Lee, Nathan Lerzer, Jianfeng Gao, Fabian Ternava, Tobias R\u00f6ddiger, Alexander Waibel, Tamim Asfour, Michael Beigl, Rainer Stiefelhagen, Carsten Dachsbacher, Klemens B\u00f6hm, Jan Niehues\\nKarlsruhe Institute of Technology, Karlsruhe, Germany\\n{firstname}.{lastname}@kit.edu\\n\\nAbstract\\nWith the rapid development of Large Language Models (LLMs), it is crucial to have benchmarks which can evaluate the ability of LLMs on different domains. One common use of LLMs is performing tasks on scientific topics, such as writing algorithms, querying databases or giving mathematical proofs. Inspired by the way university students are evaluated on such tasks, in this paper, we propose SciEx - a benchmark consisting of university computer science exam questions, to evaluate LLMs' ability on solving scientific tasks. SciEx is (1) multilingual, containing both English and German exams, and (2) multi-modal, containing questions that involve images, and (3) contains various types of freeform questions with different difficulty levels, due to the nature of university exams. We evaluate the performance of various state-of-the-art LLMs on our new benchmark. Since SciEx questions are freeform, it is not straightforward to evaluate LLM performance. Therefore, we provide human expert grading of the LLM outputs on SciEx. We show that the free-form exams in SciEx remain challenging for the current LLMs, where the best LLM only achieves 59.4% exam grade on average. We also provide detailed comparisons between LLM performance and student performance on SciEx. To enable future evaluation of new LLMs, we propose using LLM-as-a-judge to grade the LLM answers on SciEx. Our experiments show that, although they do not perform perfectly on solving the exams, LLMs are decent as graders, achieving 0.948 Pearson correlation with expert grading.\\n\\n1 Introduction\\nIn recent years, Large Language Models (LLMs) have proven their usefulness across a wide range of tasks, from conversational agents to code generation (Rajkumar et al., 2022; Abbasian et al., 2023; Liao et al., 2023). Given the fast pace of development in the field, with an increasing number of LLMs being trained and released, it is important to have indicators of LLM performance on different domains. This can be achieved by establishing evaluation benchmarks that assess the capabilities of LLMs across diverse use cases.\\n\\nOne use case of LLMs is to handle scientific tasks. Some previous works have introduced benchmarks containing questions on science topics (Welbl et al., 2017; Lu et al., 2022; Gilson et al., 2022; Schubert et al., 2023; Zhang et al., 2024). However, these benchmarks are limited to multiple-choice questions. This restricts the variability of questions, such as instruction-follow ones like \\\"write a mathematical proof for this statement...\\\". Additionally, it is difficult to ask certain types of questions in a multiple-choice way without including the answer in the question itself. Multiple-choice benchmarks therefore create a gap between testing and actual usage, since they only evaluate whether the LLMs choose the correct answer, whereas in real life, the users are more likely to ask open-ended questions to the LLMs. In contrast, some other works have introduced freeform question benchmarks. These works either convert multiple-choice questions to freeform questions (Bhakthavatsalam et al., 2021), or focus on a specific type of problem such as answering questions related to a paper (Dasigi et al., 2021), thus still limiting the variability of the questions.\\n\\nIn this paper, we introduce a new benchmark, termed SciEx (Scientific Exams), designed to evaluate this capability. Inspired by the way students are evaluated in university, we created the benchmark by evaluating the performance of LLMs on university computer science exams. SciEx's questions are in various formats, from multiple choice to open-ended, thus making it suitable to evaluate LLM's capabilities of generating free-text answers that fit the requirements of the questions. It is multilingual, containing exams in both German and English. It is multimodal, as exam questions can...\"}"}
{"id": "emnlp-2024-main-647", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"also contain figures. The set of questions is a good mix of different difficulty levels since they are designed for university exams. This enables us to evaluate LLMs on different levels, and we found that stronger LLMs tend to perform better on more difficult questions.\\n\\nUnlike the previous multiple-choice benchmarks, the questions in SciEx are freeform, making it non-trivial how to evaluate the LLM output. Therefore, we make use of expert grading, i.e., having the lecturers grade the LLM output the same way they would grade student answers. We also ask the experts to perform qualitative analysis of the LLM output. With expert grading, we provide a highly reliable way of evaluating LLMs, which is more reliable than previous work that uses crowdsourced evaluation. Expert grading by lecturers also provides an opportunity to compare LLMs' performance to university student performance in a similar setting. We find that the stronger LLMs, i.e., Claude and GPT-4V, are able to outperform the student average. However, they are still far from perfect, achieving only 59% across SciEx exams.\\n\\nSince new LLMs are constantly being released, we cannot fully rely on expert grading for evaluation. Therefore, we provide an automatic grading scheme by using LLM as a judge so that future LLMs can also be evaluated on SciEx. Interestingly, we find that, although LLMs do not perform too well as examinees, they perform well as graders, achieving over 0.948 Pearson correlation to expert grading in the best setting.\\n\\nIn summary, our contributions are as follows:\\n\\n\u2022 SciEx - a freeform, multimodal, multilingual benchmark consisting of university computer science exams, outputs of various LLMs on the exams, and expert grading of the LLM output.\\n\u2022 Detailed quantitative and qualitative analysis comparing LLM to student performance.\\n\u2022 Automatic grading with 0.948 Pearson correlation to expert grading\\n\\nRelated Work\\n\\nGeneral-Purpose LLM Benchmarks\\nIn order to rank different LLMs, there are several commonly used public benchmarks. For example, Zheng et al. (2024) introduced MT-bench and Chatbot Arena. MT-bench is a multi-turn question set; and Chatbot Arena is a crowdsourced battle platform for LLMs where the users can ask their questions and vote for the better LLM answer. Another benchmark is MMLU (Hendrycks et al., 2020), which is a multitask dataset covering multiple domains such as mathematics, US history and law.\\n\\nScientific LLM Benchmarks\\nTo specifically focus on the scientific domain, previous studies have established benchmarks, such as SciQ (Welbl et al., 2017) and ScienceQA (Lu et al., 2022), which feature questions spanning various scientific subjects. More recent works have focused on benchmarking LLMs on solving exam questions on some narrow science domains such as medical (Gilson et al., 2022) or neurology (Schubert et al., 2023). M3Exam (Zhang et al., 2024), in contrast, provides exam questions to benchmark LLMs which span over multiple topics and multiple educational levels (primary, middle, and high school). However, all benchmarks mentioned above are limited to multiple-choice questions. While this simplifies the evaluation process, it does not allow us to assess the LLMs' capability to generate natural text. Other studies have instead provided scientific benchmarks with open-ended questions. Some examples are Qasper (Dasigi et al., 2021) and ARC-DA (Bhakthavatsalam et al., 2021). However, Qasper only focuses on questions about NLP papers rather than on general computer science topics. ARC-DA is closer to our work, since it contains open-ended questions taken from science exams and quiz sources. However, these are created by converting questions that were originally multiple-choice, thus not covering certain types of typical freeform questions (e.g. those that require mathematical proofs, or long explanations).\\n\\nDifferent from these works, SciEx is created from university computer science exams, thus naturally providing diversity in the types of questions as well as having freeform format.\\n\\nFreeform Answer Evaluation\\nCompared to benchmarks with multiple-choice questions, evaluating LLMs' performance on freeform questions is not straightforward. Similar to evaluation conditions in tasks such as machine translation or summarization, there are multiple correct answers, or multiple ways to express a correct answer for a single input. Therefore, it is insufficient to evaluate a model's output by comparing it to a\"}"}
{"id": "emnlp-2024-main-647", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gold standard answer. Ideally, in these cases, we can evaluate by human judgment. For example, the ARC-DA benchmark (Bhakthavatsalam et al., 2021) uses a crowdscoring pipeline for evaluation. Chatbot Arena (Zheng et al., 2024) also uses crowd-sourcing, where the users vote between pairs of LLM output. However, human evaluation is inherently non-scalable. Therefore, previous works have used automated metrics. Some traditional metrics such as BLEU (Papineni et al., 2002) or ROUGE (Lin, 2004) compare the model\u2019s output to some gold-standard answer on the surface level, i.e., word matching. More advanced metrics, such as BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020), and BARTScore (Yuan et al., 2021), are model-based, thus being able to evaluate answers on the semantic level.\\n\\nOne recent approach is to use LLMs for evaluation, termed \u201cLLM-as-a-judge\u201d. Liu et al. (2023); Chiang and Lee (2023a); Zheng et al. (2024) find that, although still prone to biases, LLM-as-a-judge for textual modality has high agreement with human scoring when a strong judge LLM is used. However, when including images, Chen et al. (2024) find that the performance of LLM-as-a-judge is no longer as well correlated to human judgment. Nevertheless, LLM-as-a-judge is a promising way to perform scalable evaluation.\\n\\nIn our work, we make use of LLM-as-a-judge for automatic grading of LLM answers on SciEx exams, and find that they have good correlation to human expert grading on both text-only and image-related questions.\\n\\nThe SciEx Benchmark\\n\\nThe components of SciEx are as follows. University Exams SciEx contains university computer science exams in a unified JSON format. The exams are taken from the following computer science courses at the Karlsruhe Institute of Technology from the 2022/2023/2024 semesters:\\n\\n- Natural Language Processing (NLP)\\n- Advanced Artificial Intelligence (AI2)\\n- Deep Learning and Neural Networks (DLNN)\\n- Deep Learning for Computer Vision (DL4CV2)\\n- Human\u2013Computer Interaction (HCI)\\n- Databases (DBS) for the years 2022 and 2023\\n- Computer Graphics (CG)\\n- Theoretical Foundations of Computer Science (TGI)\\n- Algorithms (ALGO)\\n\\nDescriptions of the exams are in Appendix A. In total, SciEx contains 10 exams, among which 5 exams are in English and 7 exams are in German (some exams are provided in both languages). There are in total 154 unique questions. Each question is annotated with (1) the maximum points that can be achieved and (2) a difficulty level among Easy, Medium, Hard.\\n\\nMost questions are provided with gold reference answers and average student performance. The detailed per-question statistics are shown in Table 1.\\n\\n| Question count | Total | English / German | Text-only / Image-related | Easy / Medium / Hard | With / Without reference | With / Without student average |\\n|----------------|-------|------------------|--------------------------|----------------------|------------------------|-------------------------------|\\n|                | 154   | 95 / 97          | 121 / 33                 | 51 / 71 / 32         | 120 / 34               | 117 / 37                      |\\n\\n*: Some questions are provided bilingually.\\n\\nTable 1: Question-level statistics for SciEx.\\n\\nLLM-Generated Answers\\n\\nSciEx contains answers produced by 7 LLMs on the exam questions. The details of the LLMs are shown in Table 2. In Table 2, only Llama3 was not used to solve the exam, since it was released at a later point of conducting this paper. In total, we obtained 1120 question-answer pairs.\\n\\nExpert Grading and Automatic Grading\\n\\nEach question-answer pair is assigned a score by an expert. In order to guide future work to evaluate new LLMs on SciEx without relying on human expert grading, we also provide automatic grading generated by Mixtral, Llama3 and GPT4V.\\n\\n3.1 Data Creation\\n\\nThe data creation process is as follows.\\n\\nExam Collection\\n\\nWe collect university exams from different courses. We additionally ask the lecturers to provide us with the reference answers, the difficulty level of each question, and the average student grades on each question.\\n\\nExam Formatting\\n\\nWe convert every exam into a unified JSON format. Each exam includes a list of questions with gold standard answers.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Details of the LLMs in consideration.\\n\\n- **Proprietary**: Claude Claude-3-opus-20240229 - - yes\\n- **GPT-4v**: gpt-4-vision-preview - - yes\\n- **GPT-3.5**: gpt-3.5-turbo-0125 - - no\\n- **Open source**: Llama3 MaziyarPanahi/Meta-Llama-3-70B-Instruct-GGUF 70B 4 bit no\\n- **Mixtral**: Mistralai/Mixtral-8x7B-Instruct-v0.1 8x7B 5 bit no\\n- **Qwen**: Qwen/Qwen-72B 72B 2 bit no\\n- **Mistral**: Mistralai/Mistral-7B-Instruct-v0.2 7B - no\\n- **Llava**: Llava-hf/Llava-v1.6-Mistral-7b-hf 7B - yes\\n\\nWe pass the exams to the LLMs listed in Table 2 (except Llama3 due to later release), one question at a time. Questions that contain images are handled differently depending on the LLM. For the text-only LLMs, we exclude the images and only pass the question text to the models. For Llava, since it is trained to handle only 1 image at a time, we concatenate the images into one, with blank padding around the images as separators before feeding it to the model. Claude and GPT-4V can take multiple images, however, there is no pre-defined way of referencing the image within the text. In our work, we reference the image by mentioning the image caption within the question text, and add the text caption to the image. Since the considered LLMs can only output text, for questions asking to draw on images, we ask the LLMs to describe in text what should be drawn. The detailed prompts for LLMs to generate the answers are shown in Appendix C.\\n\\nExpert Grading\\nWe then give the LLM answers back to the lecturers, who proceed with grading the LLM output the same way they would grade student answers. We anonymized the LLMs' names in order to avoid bias during exam grading. We also build a user interface for collecting the grades (see Appendix E for more details).\\n\\nWith expert grading, the evaluation of the LLM output is highly reliable. Most importantly, the expert graders are generally the ones who designed the exam questions. We additionally ask the expert graders to provide their comments on the LLM output to further understand LLMs' behaviors when solving the exams.\\n\\n3.2 Automatic Grading\\nIn addition to expert grading, we also provide automatic grading using LLM-as-a-judge, so that we can evaluate future LLMs on SciEx. We use the stronger models, i.e., Mixtral, Llama3 and GPT-4V, to conduct the grading. Given a tuple containing question, answer, and maximum score, we ask the LLMs to output a single score between 0 and the maximum. We include reference answers to the grading prompt. We ask the LLMs to provide chain-of-thought reasoning (Wei et al., 2022; Chiang and Lee, 2023b) before giving the grade. We also include examples for grading in the prompt, so-called few-shot judge (Zheng et al., 2024). Each exam example is a tuple consisting of a question, an answer, and the expert-provided grade. We try out different settings to select the examples, as described below.\\n\\nLet's say we want to grade Question M from Exam A, answered by Examinee X. Then the shot examples can be chosen in one of the three ways:\\n\\n- **Same question**: Select examples from the same Question M from Exam A, but answered by a different Examinee Y. This mimics the real-life scenario where we use the expert resource to grade some answers of the same exam, then use it to guide the LLM graders.\\n- **Same exam**: select examples from a different Question N from the same Exam A, Answered by a different Examinee Y. Here the examples are in the same domain as the question-answer pair in consideration. This mimics the real-life scenario where, e.g., we have expert grading on exams of the same course from previous years to guide LLM graders.\\n- **Different exam**: select examples from a different Question N, from a different Exam B, Answered by a different Examinee Y. This mimics the real-life scenario where, e.g., we\"}"}
{"id": "emnlp-2024-main-647", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"have expert grading for an exam of another course to guide the LLM graders. Examinee Y and Question N are chosen randomly. For Exam B, we opt to select the exams that do not heavily require images for simplicity in the prompt. Intuitively, the example-selection settings above have decreasing levels of relevance to the actual grading query, but increasing easiness to collect. The detailed prompts for LLMs to grade answers are shown in Appendix D.\\n\\n4 Experiments\\n\\nIn this section, we describe our experiments and results. For prompting the proprietary LLMs, we use their APIs, namely OpenAI and Anthropic. For the open-source models, we obtain model check-points from the Huggingface model hub. We perform inference with the LLMs using llama.cpp with the default sampling strategy. The experiments with open-sourced models are conducted on an NVIDIA RTX A6000 GPU with 48GB VRAM.\\n\\nFor our analysis, we consider the exam-level and question-level grades. An exam-level grade is the sum of the grades of all questions in the exam.\\n\\n### 4.1 Quantitative Analysis\\n\\nWe analyze the performance of the LLMs on SciEx with expert grading. For both exam level and question level, we normalize the grade to be between 0 and 100%, since they have different scales. The normalization is done by taking the scores obtained by the examinee divided by the maximum score possible per exam/question, where the maximum scores possible are predefined by the lecturers. We also report on the German grade scale. In the German scale, the grades range from 1.0 to 5.0, where 1.0 is the highest grade and 4.0 is the passing threshold. The detailed mapping from the scores to the German grade scale is defined by the lectures, adjusted based on the overall performance of the students taking the exams.\\n\\nWe compare the performance of the LLMs to students from different aspects: language, difficulty level, and modality, i.e., questions with or without images.\\n\\n#### 4.1.1 General Observations\\n\\n**SciEx is Challenging**\\n\\nThe performance of the LLMs on SciEx provided by expert grading is shown in Table 3. The bigger-sized LLMs (Claude, GPT-4V, GPT-3.5, Mixtral and Qwen) can achieve exam passing grades (i.e., grades that are better than 4.0 in the German scale). However, the best-performing model (Claude) only achieves 59.4% of the maximum points, which is far from perfect. Compared to the student average, most LLMs have worse performance. Only the strongest proprietary LLMs, i.e., Claude and GPT-4V, can achieve grades that are better than the students'.\\n\\n| Grade (%) | \u2191 German Scale | \u2193 Proprietary | \u2193 Open source |\\n|-----------|----------------|---------------|---------------|\\n| Claude    | 59.4           | 2.4           |               |\\n| GPT-4V    | 58.2           | 2.5           |               |\\n| GPT-3.5   | 32.8           | 3.9           |               |\\n| Mixtral   | 41.1           | 3.5           |               |\\n| Qwen      | 35.4           | 3.7           |               |\\n| Mistral   | 25.9           | 4.2           |               |\\n| Llava     | 21.5           | 4.3           |               |\\n| Student avg. | 45.3       | 3.1           |               |\\n\\nTable 3: Average performance of LLMs, exam level.\\n\\n**SciEx Versus Other Benchmarks**\\n\\nThe ranking of the LLMs on SciEx in Table 3 generally agrees with other public benchmarks. However, SciEx seems to be more challenging. For example, the best LLM accuracy achieved on MMLU's various tasks is 88.8%. The best accuracy achieved on M3Exam multiple choice questions is 72.92%. Although these scores are not directly comparable, it indicates that SciEx provides a more challenging test set for future LLMs.\\n\\n**4.1.2 Influential Factors**\\n\\n**Difficulty Levels**\\n\\nFigure 1a shows the influence of the difficulty level on the examinee grades. As can be seen, the student performance aligns with the difficulty level of the questions: they perform better on easier questions. Some weaker LLMs, e.g., Mixtral, Qwen, GPT-3.5, Llava, align with the students. However, the stronger LLMs, i.e., Claude and GPT-4V, perform better on harder questions. This is an indication that difficulty levels from human perspective do not always align with LLMs' perspective. This is also confirmed by looking at the Pearson correlations between the LLMs\u2019 grades and the student average grades on the question level. These correlations are between 0.4 and...\"}"}
{"id": "emnlp-2024-main-647", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Examinee\\n0.0\\n0.1\\n0.2\\n0.3\\n0.4\\n0.5\\n0.6\\nAverage Grade Normalized\\neasy\\nmedium\\nhard\\n\\n(a) LLMs' and students' scores.\\n\\nDifference to Student Grade\\neasy\\nmedium\\nhard\\n\\n(b) Difference between LLM scores and student scores.\\n\\nFigure 1: Question-level scores grouped by difficulty.\\n\\n0.6, indicating that LLM grades and the student grades are not highly correlated.\\n\\nOne possible explanation for the mismatch between LLMs performance and question difficulty level could be that, in some exams, there can be some \\\"template questions\\\", i.e., questions that are repeated over the years, where students can just learn by heart how to systematically solve them. While this would be marked as \\\"easy\\\" by the lecturer, it might not be as easy for the LLMs, since the LLMs are not previously exposed to these \\\"template questions\\\". Another potential explanation is that math-type easy questions are hard for the LLMs, while long-text hard questions are easy for them.\\n\\nIn Figure 1b, we plot the difference between LLM scores and student scores. The stronger LLMs, i.e., Claude and GPT-4V, outperform the students the most on hard questions. Weaker LLMs, on the other hand, generally fall behind students the most on hard questions. Looking at each difficulty level independently, we observe that the ranking of the LLMs changes across different levels. This aligns with the findings made by Li et al. (2024), where they show that the LLM rankings change on a subset of evaluation prompts that are artificially labeled as hard.\\n\\nText-only versus Image-related Questions\\n\\nFigure 2 shows the influence of images on the difference between LLM and student scores. Recall that for the text-only LLMs, we exclude the images and only pass the question text to the models. Trivially, the text-only LLMs perform poorly on the image-related questions. The strong, multi-modal LLMs, i.e., Claude and GPT4, outperform the students on both image-related questions and text-only questions, but the performance gap is still larger for text-only questions. Llava, although can handle images, still falls behind student performance by a large margin on image-related questions. This shows that LLMs' image-handling capability is still not as advanced as for text.\\n\\nLanguage\\n\\nFigure 3 shows the influence of languages on the difference between LLM scores and student scores. When the questions are in English, all LLMs, except for GPT-3.5, outperform the student average. However, for German, either the LLMs outperform students by a smaller gap, or fall behind student performance. It can be concluded that LLMs are still superior in English than other languages like German, although German can be considered a high-resource language.\\n\\nSince some models are not made to deal with images, or with languages other than English, we additionally analyze LLMs' performance on text-\"}"}
{"id": "emnlp-2024-main-647", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"only and English-only questions. On this subset of questions, the grades obtained by the models are generally better, and more models would outperform the student average. More details can be found in Appendix F.\\n\\n4.2 Qualitative Analysis\\n\\nIn this section, we summarize the observations made by the graders while grading the LLMs.\\n\\nGeneral Behaviours\\n\\nThe graders observed some common behaviors made by the LLMs. Some solutions of the LLMs were good language-wise but low-quality content-wise. For students, good language usually correlates strongly with good content. The LLMs tend to output lengthy answers, since, unlike the students, they do not have a time constraint. Some LLMs even ignore when the question specifies that they should \\\"answer briefly\\\". There are also some failure cases, although not frequent: (1) Claude refuses to answer the question with \\\"I apologize, but I do not feel comfortable providing answers related to ...\\\" or (2) some LLMs get stuck in decoding loops. Sometimes, instead of answering the question, LLMs give some text that is (or seems) related to the task; rephrase the task; or describe how a task of this nature may be approached in general.\\n\\nKnowledge-type Questions\\n\\nOn some exams such as AI2, DL4CV2, DLNN, CG, questions which students can answer by learning the lecture content by heart are quite easy for the LLM. For the DL4CV2 exam, very specific questions about neural network architectures which are covered in our lecture seem to be quite common knowledge in the LLMs, which might be due to those papers being included in the training data. However, for other exams such as HCI, the models lacked specific course context, which was important for answering many theoretical and open-ended questions.\\n\\nMath-related Ability\\n\\nThe LLMs tend to fail on the math-related questions, even the basic ones. For example, they miscount the number of words in a piece of text, or have trouble comparing numbers. For questions that require writing mathematical proof in the TGI exam, all LLMs except for GPT-4V and Claude failed. For GPT-4V and Claude, they are able to pass the TGI exam. Their mistakes are more in line with those that students would make. That is, they are often not successful when making actual proof, and the points where the proof breaks sometimes are the same as the students. Even the better models handle simple geometry questions poorly and/or struggle to follow the instructions of a simple algorithm.\\n\\nReasoning Ability\\n\\nThe LLMs do not perform well on questions that require deep thinking and reasoning. For questions of the type \\\"is this statement true or false; reason for your solution\\\", the LLMs often said \\\"true\\\" and then just repeated the statement or reasoned for the opposite of their claim. This is a similar behavior often seen in students. Sometimes they make self-contradicting arguments: making a statement and then providing arguments for the other side.\\n\\nImage Handling\\n\\nGPT-4V, Claude and Llama can handle images. However, only GPT-4V and Claude have reasonable performance. When the question is about drawing on top of the figures, sometimes the LLMs successfully describe in words what needs to be drawn, but occasionally they just hallucinate a non-existing figure file path.\\n\\n4.3 Automatic Grading\\n\\nIn this section, we evaluate the performance of LLM-as-a-judge approach to automatic grading. We use the expert grades as the gold standard to evaluate automatic graders. We use Pearson correlation on the normalized scores as our metric. Since the LLMs are asked to provide the scores on the same scale as the expert scores, we also provide the Root Mean Squared Error (RMSE) on the originally-scaled scores as a secondary metric. Note that RMSE would correctly put more weight on the questions that have more points, however, it is not as easily interpretable as the Pearson correlation. Therefore, we only report RMSE in Appendix G.2. The main results are discussed as follows.\\n\\n4.3.1 General\\n\\nLLMs Perform Well as Graders\\n\\nOn the exam level, LLM-as-a-judge performs well for automatic grading. The best Pearson correlation to expert grading on the exam level, at 0.948, is achieved by GPT-4V. The open-source Llama3 achieves 0.883 Pearson correlation to expert grading. The LLM ranking based on average exam-level grades provided by the GPT-4V grader in comparison to expert grading is shown in Table 5. As can be seen, the ranking is quite identical, except for Mixtral and Qwen's positions being swapped.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The high correlations between expert grading and LLM-as-a-judge grading indicate that, although being far from perfect in solving SciEx exams (discussed in Section 4.1.1), the stronger LLMs are quite reliable for grading the exams. This is useful since we would have to rely less on expert grading to evaluate newly developed LLMs' performance on SciEx. The details of graders' performance under different settings on the exam level are in Appendix G.1.\\n\\nOn the question level, the performance of LLM-as-a-judge is shown in Table 4. The highest Pearson correlation achieved by the LLMs is now around 0.7, which is lower than on the exam level, but still quite high. Surprisingly, the performance of GPT-4V on grading image-related questions is quite comparable to grading text-only questions. This contradicts the finding made by Chen et al. (2024). This could potentially be due to the small number of image-related questions in SciEx, thus the results might not be generalizable.\\n\\n### Table 4: LLM grading's Pearson correlation to expert grading on the question level.\\n\\n| LLM      | 0 shot | 1 shot | 2 shot |\\n|----------|--------|--------|--------|\\n| Mixtral  | 0.232  | 0.352  | 0.395  |\\n| Llama3   | 0.311  | 0.299  | 0.316  |\\n| GPT-4V   | 0.311  | 0.377  | 0.364  |\\n\\n### Table 5: LLM examinees ranking with expert grader and GPT-4V grader.\\n\\n| LLM      | Examinee Avg. grade (%, sorted) |\\n|----------|---------------------------------|\\n| Claude   | (59.4, 57.7)                   |\\n| GPT-4V   | (58.2, 56.2)                   |\\n| Mixtral  | (41.1, 38.2)                   |\\n| Qwen     | (42.0, 35.4)                   |\\n| GPT-3.5  | (32.8, 38.0)                   |\\n| Mistral  | (25.9, 24.6)                   |\\n| Llama 2 | (21.5, 24.2)                   |\\n\\nFew-shot and References Help\\n\\nThe performance of the graders on the question level is shown in Table 4. We observe that adding examples (shots) and adding reference answers in the prompt generally increases the performance of the LLM graders. GPT-4V is the strongest grader, followed by Llama3 and Mixtral. This shows that proprietary LLMs are still stronger as judges, aligning with previous studies (Zheng et al., 2024).\\n\\n### 4.3.2 Grader-specific Behaviours\\n\\n**Mixtral Grader Tends to Give Full Points**\\n\\nAs can be seen from Table 4, Mixtral has the worst performance on grading the exams. We observe that Mixtral tends to give full points to the answers. Without reference and without examples (0-shot), the portion of answers where Mixtral outputs full points is 67.6%, significantly higher than Llama3 and GPT-4V, at 19.1% and 15.1%, respectively. As a result, Mixtral's precision on giving full points, at 0.181, is much lower than Llama3 and GPT-4V, at 0.380 and 0.527 respectively. As we add more examples and/or add the reference answer to the prompt, the problem is lessened. More details can be found in Appendix G.3.\\n\\n**Mixtral and GPT-4V Copy Grade of Example**\\n\\nFor Mixtral and GPT-4V graders, when having one example (shot) from the same question in the prompt without reference, the performance is worse than having the example from the same exam or without ref.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from a different exam. We hypothesize that this is due to these graders tend to copy the grades of the examples when having a chunk of duplicated text (i.e., the question description) in the example. This is verified when looking at the statistics: Mixtral and GPT-4V copy the grade of the example 25% of the time, whereas Llama3 does it 13% of the time. As a result, Llama3 can best make use of examples from the same question. The problem is reduced when having more than 1 shot or when the reference answer is included.\\n\\n4.3.3 Influential Factors\\n\\nDifferent Examinees\\n\\nAs can be seen in Table 6, GPT-4V grader has better performance than others, but is more inconsistent: it does worse on grading some LLMs, especially Claude. This is potentially due to Claude being a better examinee than GPT-4V itself, as shown in Section 4.1.1. When using the scores from GPT-4V grader to rank the LLMs, we find that, without reference answer, GPT-4V always ranks itself higher than Claude. This emphasizes the importance of reference answers for grading, especially when the grader is weaker than the examinee.\\n\\n| Graders | Mixtral | Llama3 | GPT-4V |\\n|---------|---------|--------|--------|\\n| Claude  | 0.304   | 0.460  | 0.482  |\\n| GPT-4V  | 0.353   | 0.528  | 0.612  |\\n| Mixtral | 0.251   | 0.472  | 0.564  |\\n| Qwen    | 0.351   | 0.556  | 0.736  |\\n| GPT-3.5 | 0.333   | 0.522  | 0.697  |\\n| Mistral | 0.291   | 0.467  | 0.601  |\\n| Llama4  | 0.387   | 0.716  | 0.812  |\\n\\nTable 6: LLM graders performance (i.e., Pearson correlation to expert grading) on different examinees.\\n\\nDifficulty Levels\\n\\nLooking at Table 7, the weaker graders, i.e., Mixtral and Llama3, perform better on grading easier questions. In contrast, GPT-4V performs better in grading harder questions.\\n\\n| Graders | Mixtral | Llama3 | GPT-4V |\\n|---------|---------|--------|--------|\\n| Easy    | 0.374   | 0.602  | 0.628  |\\n| Medium  | 0.293   | 0.524  | 0.690  |\\n| Hard    | 0.224   | 0.496  | 0.732  |\\n\\nTable 7: LLM grader's performance (i.e., Pearson correlation to expert grading) on different difficulty levels.\\n\\n5 Conclusion\\n\\nIn this paper, we proposed SciEx - a benchmark consisting of scientific university exams, along with expert grading and automatic grading, to evaluate the abilities of LLMs on science topics. SciEx is multilingual, multi-modal, and contains a variety of free-form questions. Our experiments show that SciEx is still quite challenging for current LLMs, where the best LLM can only achieve 59.4% of the exam score on average. Despite that, the LLMs perform well as graders, achieving 0.948 Pearson correlation to the expert grades. This is a promising observation, since we can use strong LLMs for automatic grading of new LLM examinees on SciEx, rather than relying on expert grading. We encourage the research community as well as LLM developers and users to make use of SciEx for evaluating LLMs' scientific capabilities.\\n\\nLimitations\\n\\nThere are certain biases that can occur for SciEx. Firstly, the LLMs do not have time pressure. Therefore, they can output longer answers, which helps them get better grades, as there is a higher likelihood that something will be correct. Secondly, the grading process can not be fully anonymized. It is not easy to mix the LLM answers with student answers for the lecturer to grade, since student answers are usually handwritten. Additionally, the LLMs' answers content itself might also be easily distinguishable from the students', since the LLMs tend to, e.g., give longer answers or repeat the questions. Therefore, the lecturers know when they are grading an LLM, thus can bias the score they give. Thirdly, the comparison between the LLMs and the students might be unfair, since the students studied the centralized course material specifically for the exams, while this is not the case for the LLMs. Lastly, due to the reliance on expert resources, the size of SciEx is quite small compared to other scientific benchmarks.\\n\\nEthics\\n\\nOur work makes use of student statistics to compare against LLMs' performance. However, we only use the average of the student grades, without disclosing any individual student's information. The student answers are never directly used, as we only ask for the average graders from the lecturers. Automatic grading, regardless of the high correlation to expert grading, can still be imperfect.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We are not suggesting to use LLMs to evaluate students, but to evaluate new models coming out when it is not possible to do human evaluation.\\n\\nRegarding data consent, we had group meetings and email exchanges to come to an agreement from all lecturers that the data would be made public under the CC BY-NC-SA 4.0 license.\\n\\nAcknowledgments\\n\\nThis work was supported by the Helmholtz Programme-oriented Funding, with project number 46.24.01, project name AI for Language Technologies. It was also supported by funding from the pilot program Core-Informatics of the Helmholtz Association (HGF).\\n\\nWe thank the lecturers for their contribution during the creation of the dataset: Kunyu Peng, Alexander Jaus, David Schneider, Ruiping Liu, Zdravko Marinov, Yufan Chen, Mikl\u00f3s Borsi, Florian Kalinke, Federico Matteucci, Fabian Richter, Bela B\u00f6hnke, Jose Cribeiro-Ramallo, Daniel Ebi, Florian Kalinke, Adrian Feilhauer, Wendy Yi, Laura Merker, Miriam Goetze, Jean-Pierre von der Heydt, Max G\u00f6ttlicher, Thomas Bl\u00e4sius, Marcus Wilhelm, Michael Z\u00fcndorf.\\n\\nReferences\\n\\nMahyar Abbasian, Iman Azimi, Amir M Rahmani, and Ramesh Jain. 2023. Conversational health agents: A personalized llm-powered agent framework. arXiv preprint arXiv:2310.02374.\\n\\nSumithra Bhakthavatsalam, Daniel Khashabi, Tushar Khot, Bhavana Dalvi Mishra, Kyle Richardson, Ashish Sabharwal, Carissa Schoenick, Oyvind Tafjord, and Peter Clark. 2021. Think you have solved direct-answer question answering? try arcda, the direct-answer ai2 reasoning challenge. arXiv preprint arXiv:2102.03315.\\n\\nDongping Chen, Ruoxi Chen, Shilin Zhang, Yinuo Liu, Yaochen Wang, Huichi Zhou, Qihui Zhang, Pan Zhou, Yao Wan, and Lichao Sun. 2024. Mllm-as-a-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark. arXiv preprint arXiv:2402.04788.\\n\\nCheng-Han Chiang and Hung-yi Lee. 2023a. Can large language models be an alternative to human evaluations? In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 15607\u201315631, Toronto, Canada. Association for Computational Linguistics.\\n\\nCheng-Han Chiang and Hung-yi Lee. 2023b. A closer look into using large language models for automatic evaluation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 8928\u20138942, Singapore. Association for Computational Linguistics.\\n\\nPradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah A. Smith, and Matt Gardner. 2021. A dataset of information-seeking questions and answers anchored in research papers. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 4599\u20134610, Online. Association for Computational Linguistics.\\n\\nAidan Gilson, Conrad Safranek, Thomas Huang, Vimig Socrates, Ling Chi, R Andrew Taylor, and David Chartash. 2022. How does chatgpt perform on the medical licensing exams? the implications of large language models for medical education and knowledge assessment. MedRxiv, pages 2022\u201312.\\n\\nDan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.\\n\\nTianle Li, Wei-Lin Chiang, and Lisa Dunlap. 2024. Introducing hard prompts category in chatbot arena. Published on LMSYS.\\n\\nLizi Liao, Grace Hui Yang, and Chirag Shah. 2023. Proactive conversational agents in the post-chatgpt world. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR \u201923, page 3452\u20133455, New York, NY, USA. Association for Computing Machinery.\\n\\nChin-Yew Lin. 2004. ROUGE: A package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 74\u201381, Barcelona, Spain. Association for Computational Linguistics.\\n\\nYang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignment. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 2511\u20132522, Singapore. Association for Computational Linguistics.\\n\\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. 2022. Learn to explain: Multimodal reasoning via thought chains for science question answering. In The 36th Conference on Neural Information Processing Systems (NeurIPS).\\n\\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311\u2013318, Philadelphia, Pennsylvania, USA. Association for Computational Linguistics.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nitarshan Rajkumar, Raymond Li, and Dzmitry Bahdanau. 2022. Evaluating the text-to-sql capabilities of large language models. arXiv preprint arXiv:2204.00498.\\n\\nMarc Cicero Schubert, Wolfgang Wick, and Varun Venkataramani. 2023. Performance of large language models on a neurology board\u2013style examination. JAMA network open, 6(12):e2346721\u2013e2346721.\\n\\nThibault Sellam, Dipanjan Das, and Ankur Parikh. 2020. BLEURT: Learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 7881\u20137892, Online. Association for Computational Linguistics.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:24824\u201324837.\\n\\nJohannes Welbl, Nelson F. Liu, and Matt Gardner. 2017. Crowdsourcing multiple choice science questions. In Proceedings of the 3rd Workshop on Noisy User-generated Text, pages 94\u2013106, Copenhagen, Denmark. Association for Computational Linguistics.\\n\\nWeizhe Yuan, Graham Neubig, and Pengfei Liu. 2021. Bartscore: Evaluating generated text as text generation. In Advances in Neural Information Processing Systems, volume 34, pages 27263\u201327277. Curran Associates, Inc.\\n\\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations.\\n\\nWenxuan Zhang, Mahani Aljunied, Chang Gao, Yew Ken Chia, and Lidong Bing. 2024. M3exam: A multilingual, multimodal, multilevel benchmark for examining large language models. Advances in Neural Information Processing Systems, 36.\\n\\nLianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Exam Description\\n\\nThe overall description of each exam in SciEx is as follows:\\n\\n1. Natural Language Processing (NLP): exam contains questions about word and sequence representation, language modeling, and pre-trained models.\\n2. Advanced Artificial Intelligence (AI2): exam contains questions about natural language processing, signal processing, automatic speech recognition and cognitive robotics.\\n3. Deep Learning and Neural Networks (DLNN): exam contains questions about neural network fundamentals, in-depth questions about multi-head self attention and calculation questions on backpropagation.\\n4. Deep Learning for Computer Vision (DL4CV2): exam contains questions about semi-supervised learning, weakly supervised learning, multi-modal text-image models, continual learning, representation learning, interactive segmentation, transfer learning and generative models from recent literature.\\n5. Human\u2013Computer Interaction (HCI): exam encompasses fundamental HCI subjects like observational studies, human perception and information processing, user studies, and system design and design analysis. It requires students to utilize theoretical knowledge and perform brief analyses based on the given context.\\n6. Databases (DBS): 2 exams from 2022 and 2023, containing questions about ER (Entity Relationship) modeling, SQL writing and comprehension, relational algebra, and transaction management.\\n7. Computer Graphics (CG): exam contains questions about color and perception, raytracing, shading, data structures, transformations, textures, OpenGl, blending, shaders, procedural modeling, and bezier curves.\\n8. Theoretical Foundations of Computer Science (TGI): exam contains questions about finite automata, regular languages, pushdown automata, grammars (Chomsky hierarchy), Turing machine, formal languages, NP-completeness, approximation algorithms, decidability. Most questions require writing mathematical proofs.\\n9. Algorithms (ALGO): exam contains questions on writing proofs (correctness of algorithms, asymptotic run time analysis), knowing basic algorithms and data structures, designing simple new algorithms, selecting the right data structure or algorithm for a given task at hand.\\n\\nB Exam Formatting\\n\\nOriginally, exams were in different formats, depending on their creator. We convert the exams into JSON format, with file paths to images if any. An example is shown in Figure 4.\\n\\nC LLM Answer Generation Prompts\\n\\nWe provide the prompt in the same language as the exam question to the LLMs to generate answers. The English prompt is shown in Figure 5, and the German prompt is shown in Figure 6.\\n\\nD LLM Grader Prompts\\n\\nWe provide the prompt in the same language as the exam question to the LLMs to perform automatic grading. The English prompt is shown in Figure 7, and the German prompt is shown in Figure 8.\\n\\nE User Interface for Expert Grading\\n\\nWe instructed the expert grader to use our user interface (UI) for grading. Figure 9 shows the open page of the UI, where the grader can choose their exam and enter their password. Figure 10 shows the page for the grading, where the expert is shown with the question, the LLM answer to the question, and a text box to enter the grade. The expert can choose the examinee to grade from the dropdown on the left-hand side. Figure 11 shows the page to enter additional information about the exam questions, including the maximal achievable score, average student performance, gold answer, and difficulty level.\\n\\nOnce the data is collected, we also ask the experts and have their consent to make the data public.\\n\\nF Performance on Text-only, English-only Questions\\n\\nThe performance of the LLMs on text-only, English-only questions is shown in Table 8.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"subset of questions, besides GPT-4V and Claude, we can see that Mixtral and Qwen also have better performance than the student average.\\n\\n|                | Grade (%) German Scale |\\n|----------------|------------------------|\\n| Proprietary     |                        |\\n| GPT-4V         | 70.8 1.4               |\\n| Claude         | 69.2 1.6               |\\n| GPT-3.5        | 47.8 2.9               |\\n| Open source    |                        |\\n| Mixtral        | 61.2 2.0               |\\n| Qwen           | 56.8 2.4               |\\n| Mistral        | 48.0 3.2               |\\n| Llava          | 42.4 3.5               |\\n| Student avg.   | 56.5 2.4               |\\n\\nTable 8: Average performance of the LLMs on the exam level, provided by expert grading, text-only and English-only questions.\\n\\nThe performance of LLM-as-a-judge for automatic grading on the exam level is shown in Table 9. Note that Mixtral and Llava3 graders have disadvantage since they cannot take image input for image-related questions.\\n\\nSince the LLM graders are asked to output the scores in the original scale, RMSE would be the most informative metric, since it also reflects the importance of the questions that have higher maximum scores. The LLM graders' performance in RMSE is shown in Table 10.\\n\\nThe performance of the LLM graders on assigning full points to the answers is shown in Table 11.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-647", "page_num": 15, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You are a university professor. Please grade the following exam question.\\n\\nThe exam question, examinee's answer, correct answer, and the maximum possible score are provided in the format:\\n\\n- [question]\\n- [answer]\\n- [correct_answer]\\n- [max_score]\\n\\nThe question is provided in JSON format, but the answer can be freeform text. The provided figures in the question (if any) each contain its path at the bottom, which matches the path provided in the JSON. The answer is text-only. If the question asks to draw on the figure, then the answer should contain a text description of how the drawing should be.\\n\\nPlease provide the grade between [0, <max_score>]. Please provide the reasoning for your grade. Please provide your output in the format:\\n\\n- [reason]\\n- [grade]\\n\\nBelow you are provided with examples on how to perform the grading:\\n\\nHere is your input:\\n\\n<example text>\\n\\nFigure 7: Grading prompt in English.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 16, "content": "{\"primary_language\":\"de\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sie sind Universit\u00e4tsprofessor. Bitte bewerten Sie die folgende Pr\u00fcfungsfrage.\\n\\nDie Pr\u00fcfungsfrage, die Antwort des Pr\u00fcflings, die richtige Antwort und die maximal m\u00f6gliche Punktzahl werden im Format bereitgestellt:\\n\\n- [question]<Pr\u00fcfungsfrage>[/question]\\n- [answer]<Antwort>[/answer]\\n- [correct_answer]<korrekteAntwort>[/correct_answer]\\n- [max_score]<maxPunkt>[/max_score]\\n\\nDie Frage wird im JSON-Format bereitgestellt, die Antwort kann jedoch Freiformtext sein. Die bereitgestellten Abbildungen in der Frage (falls vorhanden) enthalten jeweils unten ihren Pfad, der mit dem im JSON bereitgestellten Pfad \u00fcbereinstimmt. Die Antwort ist nur Text. Wenn es sich bei der Frage darum handelt, auf der Abbildung zu zeichnen, sollte die Antwort eine Textbeschreibung dar\u00fcber enthalten, wie die Zeichnung aussehen soll.\\n\\nBitte geben Sie die Note zwischen [0, <maxPunkt>] an. Bitte begr\u00fcnden Sie Ihre Note. Bitte geben Sie Ihre Ausgabe im Format an:\\n\\n- [reason]<Grundsatz>[/reason]\\n- [grade]<Note>[/grade]\\n\\nNachfolgend finden Sie ein Beispiel f\u00fcr die Durchf\u00fchrung der Benotung:\\n\\n<example text>\\n\\nHier ist Ihre Eingabe:\\n<input text>\\n\\nFigure 8: Grading prompt in German.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 17, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 9: LLM graders' Pearson correlation to expert graders on exam level, scores normalized. Note that there are only a single scores for zero-shot, since they do not have different shot settings.\\n\\n| Method      | 0 shot | 1 shot | 2 shot | 0 shot | 1 shot | 2 shot |\\n|-------------|--------|--------|--------|--------|--------|--------|\\n| Mixtral     | 0.404  | 0.542  | 0.564  | 0.445  | 0.549  | 0.505  |\\n| Llama3      | 0.649  | 0.812  | 0.771  | 0.677  | 0.731  | 0.729  |\\n| GPT-4V      | 0.911  | 0.886  | 0.921  | 0.938  | 0.906  | 0.910  |\\n\\nTable 10: LLM grading's RMSE compared to expert grading on the question level. Note that there are only single scores for zero-shot, since they do not have different shot settings.\\n\\n| Method      | 0 shot | 1 shot | 2 shot | 0 shot | 1 shot | 2 shot |\\n|-------------|--------|--------|--------|--------|--------|--------|\\n| Mixtral     | 3.25   | 2.68   | 2.69   | 2.96   | 2.90   | 2.83   |\\n| Llama3      | 2.66   | 1.89   | 1.88   | 2.09   | 2.30   | 2.36   |\\n| GPT-4V      | 1.56   | 1.34   | 1.25   | 1.20   | 1.32   | 1.29   |\"}"}
{"id": "emnlp-2024-main-647", "page_num": 18, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10: Grading UI showing the question, the answer and textbox to input the grade.\\nFigure 11: Grading UI where grader can fill in additional information about the question.\"}"}
{"id": "emnlp-2024-main-647", "page_num": 19, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model     | No ref | Ref | No ref | Ref |\\n|-----------|--------|-----|--------|-----|\\n| Mixtral   | 0      | 0.181 | 0.196 | 67.0 | 37.2 |\\n|           | 1      | 0.251 | 0.325 | 46.3 | 26.6 |\\n|           | 2      | 0.258 | 0.256 | 45.3 | 37.1 |\\n| Llama3    | 0      | 0.380 | 0.636 | 19.1 | 5.9  |\\n|           | 1      | 0.438 | 0.579 | 19.9 | 8.5  |\\n|           | 2      | 0.447 | 0.483 | 17.8 | 10.5 |\\n| GPT-4V    | 0      | 0.527 | 0.405 | 15.0 | 11.3 |\\n|           | 1      | 0.422 | 0.526 | 16.8 | 10.2 |\\n|           | 2      | 0.458 | 0.560 | 15.9 | 9.8  |\\n\\nTable 11: Performance on giving full points.\"}"}
