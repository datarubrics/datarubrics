{"id": "emnlp-2024-main-862", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Effective Synthetic Data and Test-Time Adaptation for OCR Correction\\n\\nShuhao Guan1, Cheng Xu1, Moule Lin2, and Derek Greene1\\n\\n1 School of Computer Science, University College Dublin, Ireland\\n2 School of Computer Science and Statistics, Trinity College Dublin, Ireland\\n{shuhao.guan,cheng.xu1}@ucdconnect.ie, moulel@tcd.ie, derek.greene@ucd.ie\\n\\nAbstract\\nPost-OCR technology is used to correct errors in the text produced by OCR systems. This study introduces a method for constructing post-OCR synthetic data with different noise levels using weak supervision. We define Character Error Rate (CER) thresholds for \\\"effective\\\" and \\\"ineffective\\\" synthetic data, allowing us to create more useful multi-noise level synthetic datasets. Furthermore, we propose Self-Correct-Noise Test-Time Adaptation (SCN-TTA), which combines self-correction and noise generation mechanisms. SCN-TTA allows a model to dynamically adjust to test data without relying on labels, effectively handling proper nouns in long texts and further reducing CER. In our experiments we evaluate a range of models, including multiple PLMs and LLMs. Results indicate that our method yields models that are effective across diverse text types. Notably, the ByT5 model achieves a CER reduction of 68.67% without relying on manually annotated data.\\n\\n1 Introduction\\nThe task of preserving historical data has seen a transformative impact due to advancements in Optical character recognition (OCR) (Wei et al., 2024). While these systems have facilitated the digitization of historical books, they have also introduced a new set of challenges, particularly when dealing with texts involving complex layouts, unusual typefaces, or degraded quality (Jatowt et al., 2019). Post-OCR methods have been proposed to correct these errors (Nguyen et al., 2021), improving the usability of digitized texts \u2013 an important factor for cultural analytics research and digital humanities in general.\\n\\nRecent research has framed the post-OCR task as a Seq2Seq Neural Machine Translation (NMT) task (Amrhein and Clematide, 2018; Mokhtar et al., 2018; Nastase and Hitschler, 2018; H\u00e4m\u00e4l\u00e4inen and Hengchen, 2019). The reliability and generalization performance of NMT models is closely tied to the quality and the volume of the training data (Vu et al., 2020). Therefore, synthetic data is commonly used in this area. However, most previous work considers synthetic data with only a single noise ratio for model training (Rijhwani et al., 2020; Jasonarson et al., 2023; Xie and Anastasopoulos, 2023), which is not consistent with real digitized collections where quality will often vary. Even in cases where researchers have considered different noise ratios, this has involved conducting separate sub-experiments for each noise level (D\u2019hondt et al., 2017). To the best our knowledge, there has been no attempt to merge synthetic data with multiple different noise levels for model training, nor has anyone explored which noise levels to merge to construct the most useful synthetic data.\\n\\nIn addition to the original errors that occur during digitization, subsequent post-OCR procedures can introduce further inaccuracies. This issue becomes particularly severe with proper nouns (PNs), like personal and place names, which are more susceptible to these post-OCR errors and are challenging to correct. This often arises either because the PNs in the test data were not present in the training data, or because they are hard to distinguish. For instance, in OCR text, it is unclear whether \\\"MarL\\\" should be corrected to \\\"Mark\\\" or \\\"Marl\\\". Errors like these can negatively impact downstream tasks, such as retrieval and recommendation systems (Bazzo et al., 2020; Van Strien et al., 2020).\\n\\nIn response to the challenges above, in Section 3.1 we propose a process that employs a weakly-supervised learning approach to generate synthetic data with varying noise levels. Furthermore, we explore how to merge synthetic data with different noise levels to create the most useful dataset for model training. In Section 3.2 we...\"}"}
{"id": "emnlp-2024-main-862", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"evaluate a range of different models using this synthetic data, achieving competitive results for post-OCR correction on multiple benchmarks. In Section 4.1 we also propose Self-Correct-Noise Test-Time Adaptation (SCN-TTA) to enable the model to better handle PNs, further improving the quality of the output text. The experiments in Section 4 demonstrate that this method is effective across diverse text types and can reduce the CER by up to 68.67%.\\n\\n2 Related Work\\n\\nPost-OCR has evolved over many years, with several methods leveraging multiple OCR engines and combining their outputs to yield improved results (Lin, 2001; Lund and Ringger, 2011). Recent advances involve using pre-trained models and seq2seq architectures for error detection and correction (Nguyen et al., 2020). Amrhein and Clematide (2018) employed both NMT and Statistical Machine Translation (SMT) models for post-OCR tasks. Following this, Schaefer and Neudecker (2020) introduced a two-step method that involves detection followed by correction. An unsupervised approach combining multiple OCR views is proposed by Gupta et al. (2021). Ramirez-Orta et al. (2022) split texts into character ngrams and combine their individual corrections into the final output. Other methods have involved using LSTMs for post-processing (D\u2019hondt et al., 2017), establishing benchmarks using pre-trained models on niche datasets (Maheshwari et al., 2022) and using an adapted hill-climbing algorithm (Nguyen et al., 2023). A number of recent studies have used encoder-decoder pre-trained language models (Soper et al., 2021; Maheshwari et al., 2022; Wolters and Van Cranenburgh, 2024; L\u00f6fgren and Dann\u00e9lls, 2024) and large language models (Boros et al., 2024; Thomas et al., 2024).\\n\\n3 Effective Synthetic Data\\n\\nWe now introduce a new approach for generating effective synthetic data for the post-OCR task.\\n\\n3.1 Data Generation with Error Levels\\n\\nTo address the issue of limited training data, we extract the frequency of OCR errors from existing data. We then insert these errors into clean text from our domain of interest at different ratios to create data with varying error levels. Firstly, we require two input document sets: the source data and the target data. The source data, which consists of pairs of noisy OCR text and corrected ground truth (GT), serves as the source of OCR errors. These errors are used to construct rules for our proposed Data Generator (DG). The target data is a collection of clean texts free from OCR errors. The DG will be used to insert OCR errors into this target data, thereby generating the synthetic data.\\n\\nIn this paper, we chose the English datasets...\"}"}
{"id": "emnlp-2024-main-862", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from ICDAR2017/2019 (Rigaud et al., 2019) as our source data, totaling 6.2 million characters. These datasets include OCR texts aligned with GT texts at the character level, permitting the examination of original characters in erroneous OCR outputs. For instance, a sample OCR input \u201cINEVEI3\u201d will be aligned as \u201cI@NEVEI3\u201d with a corresponding GT \u201cI NEVE@R\u201d. The padding symbol \u201c@\u201d in the source data allows us to identify which characters have been deleted and which have been recognized as multiple characters (strings).\\n\\nWe have observed that the annotations in the ICDAR datasets are not entirely reliable. Specifically, some texts appear to be improperly aligned, as the alignment was done using an automated process. We could view such annotations as a form of \u201cimperfect\u201d or \u201cimprecise\u201d labels. Therefore, we propose adopting a weakly supervised learning approach (Zhou, 2018). By carefully extracting OCR errors from these noisy, misaligned datasets, we can make use of the inherent errors and noise in the data to augment a post OCR-correction model\u2019s ability to generalize to various OCR errors, because this can increase the diversity of OCR error within the synthetic data. We also compare the results of not using weak supervision (i.e., filtering out \u201cimprecise\u201d data from the source data) later in Experiments 2 & 4. We now describe the two phases of the DG process.\\n\\nPhase 1 \u2013 DG construction. Firstly, we use the labeled pairs of examples in the source data to compute the likelihoods of individual characters being replaced by other characters or strings during OCR processing. This includes the likelihoods of all characters, including spaces and punctuation, being replaced. Here, each value $P(j|i)$ represents the probability that character $i$ is replaced by string $j$ based on the source data. Note that $i$ and $j$ can be the same, corresponding to the case where, after OCR processing, a character is mapped to itself (i.e., it is correctly recognized). These values represent the replacement \u201crules\u201d that will be used to add OCR-like errors to the target data. We apply these rules and then remove any padding symbols to build synthetic text.\\n\\nDuring the proposed process, common OCR errors are simulated as follows: recognition errors, where characters are replaced by others; insertion errors, where characters are replaced with lengthy strings; deletion errors, where characters are replaced by the padding symbol \u201c@\u201d, which will be removed later; and segmentation errors, where spaces, considered as characters, are replaced by \u201c@\u201d, leading to word segmentation issues when removed.\\n\\nPhase 2 \u2013 DG application. Next, we apply DG to the target data to introduce OCR errors. Here the target data serves as the ground truth during training. In the target data, 0.03% of random words can be replaced with the \u201c<unk>\u201d token, the \u201c<unk>\u201d token will not be affected by DG, this is preparation for SCN-TTA, it does not affect the performance of general post-OCR tasks.\\n\\nTo generate texts with different noise levels, we introduce the concept of an Error Level (EL), denoted $e$. This involves using the probabilities $P(j|i)$ calculated in the previous step to calculated the weight $W(j|i)$ for character $i$ being replaced by string $j$. $W(j|i)$ as a weight, controls the probability of various character replacements occurring when generating synthetic data, such that $W(j|i)(e) = \\\\begin{cases} P(j|i)P(i|i) & \\\\text{if } i = j \\\\\\\\ e \\\\cdot P(j|i)P(i|i) + e \\\\cdot \\\\sum_{k \\\\in S_{i,k} \\\\neq i} P(k|i) & \\\\text{if } i \\\\neq j \\\\end{cases}$ where $S_i$ denotes a set containing possible strings that might replace character $i$. By increasing $e$, the weight $W(i|i)$ of a character being replaced by itself will decrease, whereas the weights for it being replaced by other strings will increase, making errors more likely.\\n\\n3.2 Effective Data Threshold and Range\\n\\nWhen generating synthetic data, a key question relates to the extent of errors to be introduced into clean text. The most common approach is to insert errors at the same rate observed in the source data (Jasonarson et al., 2023; Rijhwani et al., 2020). However, this method has its limitations. Typically, models trained on low-CER data perform better on low-CER text, while models trained on high-CER data perform better on high-CER text. This raises the related question of whether merging data with varying CER levels can produce a model that performs well on test collections where documents have different CER levels. Specifically, if data with multiple CER levels are merged, is there a range of CER values in the training data that results in the best overall performance across different CER levels in test data? Additionally, does excessive insertion of OCR errors compromise the integrity of the original sentence structure and meaning, causing the model to \u201covercorrect\u201d?\"}"}
{"id": "emnlp-2024-main-862", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"To answer these questions, we undertake three experiments. Experiment 1 examines how the CER of training and test data affects model performance. For the synthetic data, models trained on single EL datasets with CER > 21.02 rarely achieve strong performance on test sets. This demonstrates that while the ICDAR data is not entirely reliable, it still represents a useful source of weak supervision when extracting error generation rules, as discussed in Section 3.1. The data is divided into two equally sized sets. The first set provides error information for the synthetic training-validation dataset, while the second set is used for the synthetic test dataset. For our source data, we use the English collection of the Byt5 Setup.\\n\\n### Table 1: Scores for CER and CERR across all models\\n\\n| Model    | CER | CERR | WERR |\\n|----------|-----|------|------|\\n| mBART    | 1.08(43.9) | 0.95(50.3) | 1.02(46.8) |\\n| Flan-T5  | 3.15(-64.0) | 0.87(54.6) | 1.19(38.0) |\\n| ByT5     | 0.79(58.9) | 0.77(59.7) | 0.68(64.6) |\\n| ...      | ... | ... | ... |\\n\\nIn order to investigate the relationship between the data CER levels, when compared to real-world data, Synthetic data allows more precise control over the error distribution. In Experiment 1, we merge the training sets from all different ELs, producing a single TrEL dataset which is then randomly assigned in a 70:15:15 ratio to form the training, validation, and test sets. Further WER results are provided in Table 7 in the appendix.\\n\\nFinally, Experiment 3 extends testing to include a broader range of models and includes benchmarks for different types of text.\\n\\nIn conclusion, Experiment 1 using real-world data showed that the CER of training and test data affects model performance. Experiments 2 and 3 further explored this relationship using synthetic data, demonstrating the importance of error distribution in machine learning tasks.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Based on the observations above, we propose an Optimal Alignment Curve (OAC), with CER as the primary metric. This curve helps to identify the optimal training data for test data with diverse CER levels when considering single error level datasets. Specifically, the OAC indicates which training dataset with a specific CER (e.g., a dataset with a CER of 5%) is most effective for testing datasets that have different CER levels. However, the OAC is not intended to select the training data CER based on the test data CER. Rather, its primary purpose is to estimate the CER threshold for \\\"effective and \\\"ineffective\\\" training data, determining the CER range of the synthetic data in the merged dataset for subsequent use.\\n\\nTo calculate the OAC, we analyzed the performance of three models (mBART, Flan-T5, and ByT5) across various TrEL and TeEL levels. For each TeEL, we identified the best-performing TrEL for each model. This resulted in 24 data points (i.e., 8 TeEL levels \u00d7 3 models). We then plotted these data points and performed a quadratic polynomial regression to fit the OAC. The peak of this curve represents the threshold between \\\"effective\\\" and \\\"ineffective\\\" training data. The resulting quadratic polynomial regression is given by\\n\\n$$f(x) = \\\\begin{cases} \\n1.01 + 1.68x - 0.04x^2 & \\\\text{if } 0 < x \\\\leq 22.71 \\\\\\\\\\n20.1 & \\\\text{if } x > 22.71 \\n\\\\end{cases}$$\\n\\nwhere $x$ represents the CER of the test set, and $f(x)$ denotes the empirically-observed optimal CER of the training set. The 24 data points, alongside the corresponding OAC, are illustrated in Figure 1. This indicates that the empirically observed CER threshold delineating between \\\"effective\\\" and \\\"ineffective\\\" is approximately 20.1.\\n\\n### 3.2.2 Exp. 2: Validation on Real Data\\n\\nWe now evaluate the performance of models trained on synthetic datasets when tested on real-world datasets. Additionally, we aim to verify whether the definitions of \\\"effective\\\" and \\\"ineffective\\\" synthetic data from Exp. 1 are applicable in real-world scenarios. This evaluation is conducted on the RETAS benchmarks (Yalniz and Manmatha, 2011), which consist of 100 classic novels from the 18th and 19th centuries. These were processed using the Abbyy FineReader OCR engine, resulting in a CER of 6.64% and a WER of 20.83%. The novels share the same domain as the 50 novels used for training, but there is no overlap.\\n\\n**Setup.** We select three models for comparison: ByT5-base (Xue et al., 2022), which performed well in our previous experiments; Transformer-big (Vaswani et al., 2017); an ngram-based model (Ramirez-Orta et al., 2022) from the literature, which achieved SOTA performance on multiple language datasets of the ICDAR dataset. Additionally, we test three methods for generating synthetic data: random generation (Random), non-weak supervision (Non-WeakSup), and weak supervision (WeakSup).\\n\\nRandom refers to adding OCR errors through random substitutions, insertions, and deletions. The Non-WeakSup, common in post-OCR and Grammatical Error Correction (GEC) tasks (Jansonarson et al., 2023; Ing\u00f3lfsd\u00f3ttir et al., 2023), involves extracting error distributions from correctly-annotated datasets and then inserting these errors into clean text to generate synthetic data. In our case, we filtered out sentence pairs from the ICDAR dataset with CER > 50% to obtain correctly-annotated data.\\n\\nWeakSup follows a similar process, but does not filter out high-CER sentences. We create datasets with different degrees of errors, represented by $[n, m]$, indicating that each dataset merged seven synthetic datasets with CERs ranging from $n$ to $m$. Random used $[1, 20.1]$, while WeakSup and Non-WeakSup used $[1, 10]$, $[1, 20.1]$, and $[1, 30]$. Specifically, $[1, 10]$ represents all \\\"effective\\\" data but not the full range; $[1, 20.1]$ exactly covers the effective range; $[1, 30]$ covers the effective range and includes \\\"ineffective\\\" data.\\n\\n**Discussion.** From the results in Table 2, we see that both ByT5 and Transformer models outperform the ngram-based model. Models trained with the dataset $[1, 20.1]$ demonstrate better performance.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: CER for the RETAS datasets after correction using different models, trained on different synthetic data. Original is 6.64%.\\n\\nthan those trained with \\\\([1,10]\\\\) and \\\\([1,30]\\\\), suggesting the applicability of \u201ceffective\u201d and \u201cineffective\u201d synthetic data thresholds derived from the OAC in real-world scenarios. Models trained on \\\\([1,10]\\\\) perform well on texts with a CER < 10, but cannot yield high performance with higher CER values. Although this range uses \u201ceffective\u201d data, it does not cover the full \u201ceffective range\u201d, since the RETAS dataset also contains higher CER sentences. Conversely, training with \\\\([1,30]\\\\), which includes \u201cineffective\u201d data above CER 20, may cause overcorrection and a decline in performance. So \\\\([1,20.1]\\\\), which exactly covers the \u201ceffective range\u201d, results in the best performance.\\n\\nWe also observe the slight superiority of WeakSup over Non-WeakSup. This can be attributed to the fact that, while the Non-WeakSup method accurately extracts the error distribution of a specific OCR engine, the OCR texts requiring correction might originate from a different system. In contrast, the WeakSup method enhances the model\u2019s robustness by enabling it to learn from a broader array of error types, even those from incorrect annotations, thereby improving its capability to correct texts from different OCR systems.\\n\\n3.2.3 Exp. 3: Tests on Further Data\\n\\nWe now extend our experiments to evaluate the effectiveness of synthetic data across source different datasets and models.\\n\\nSetup.\\nWe generate \\\\([1,20.1]\\\\) training data by inserting OCR errors into the GT of the training data from these benchmarks using the WeakSup method, without using their actual OCR text for training. The benchmarks include Overproof-2, Overproof-3 (Evershed and Fitch, 2014), TCP (Dong and Smith, 2018) and BLN600 (Booth et al., 2024). Since the original authors did not split the Overproof-2 and Overproof-3 datasets, we conduct a 5-fold cross-validation on these datasets. Details for the datasets are given in Table 3.\\n\\nBaselines for comparison include the CER and WER values from the original benchmark papers, the Hunspell spellchecker (Ooms, 2018), and one-shot results from Llama2 13B (Touvron et al., 2023) and GPT-4o. We trained or fine-tuned the following models: those proposed by Ramirez-Orta et al. (2022) and Schaefer and Neudecker (2020), mBART large, ByT5 base, Flan-T5 base and Llama2 13B. For Llama2 13B, we used LoRA (Hu et al., 2022) for instruction-tuning. The training parameters for each model, as well as the prompts and data formats for LLM are provided in Appendix B.\\n\\nDiscussion.\\nFrom the results in Table 4, we see that the PLMs ByT5 and Flan-T5 significantly enhance OCR outputs across most datasets, even without using real OCR text in training, reducing CER by over 60% when data is sufficient. This demonstrates that in post-OCR tasks, robust models can be trained with \u201ceffective\u201d synthetic data, without the need for a manually annotated training set. Our tests also show that LLMs achieve \\\\(\\\\approx 40\\\\%\\\\) reduction in CER with one-shot performance, and up to 50% after LoRA fine-tuning. Furthermore, the CER and WER values achieved by LMs fine-tuned with purely synthetic data generally surpass those reported in the original benchmark papers. The methods of the Schaefer and Neudecker (2020) and Ramirez-Orta et al. (2022) models lag behind those of the PLMs and LLMs. We consider the ByT5 model to be the most suitable for post-OCR tasks and focus on this in our next experiment.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ths vulgar but expressive phrase, is \\\"sulky as a bear with a sore head,\\\" seemed made for him expressly, for in no case could it have been more justly applied. The second mate, Grinnerson, was a gentleman fellow, but a most terrible anct joker. Cadets had...\"}"}
{"id": "emnlp-2024-main-862", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 4: The Word Restorer component changes the `<unk>` token back to the original PNs. In this way, we obtain clean sentences, which will be used to generate an effective synthetic dataset for the second round of fine-tuning.\\n\\nStep 5: Applying the DG, we use the clean text from Step 4 to generate synthetic text with varying CERs 7 times within the effective range of $[1, 20]$, and then merge them. In this step, PNs are also inserted with various OCR errors, allowing the model to learn to correct PNs containing OCR errors.\\n\\nStep 6: Based on the synthetic data produced from text B, we perform a second round of fine-tuning on the model that has been fine-tuned in the first round to help the model learn the content of B and perform TTA. The parameters are in Appendix C.\\n\\nStep 7: Finally, we apply the fine-tuned model from Step 6 to perform full-text correction on B.\\n\\n4.2 Exp. 4: Ablation Study and SCN-TTA\\n\\nWe now conduct an ablation experiment to investigate the effects of multi-noise level training, weak supervision, and the SCN-TTA method from Section 4.1. We focus on CER, WER, and handling PNs. We generate synthetic training data in different ways to fine-tune the model, then apply SCN-TTA to improve the identification of PNs.\\n\\nSetup. Using the 50 clean texts from Exp. 1, we generated three synthetic training datasets: Single, Multi, and MultiW. \u201cSingle\u201d indicates datasets generated using only TrEL 5.0, \u201cW\u201d indicates the use of weak supervision, and the absence of \u201cW\u201d indicates that weak supervision was not used (filtering out \u201cimprecise\u201d data from the source data). See Appendix C for details on the datasets. Testing is performed on the RETAS dataset, previously described in Exp. 2. In this experiment, we primarily test the ByT5 model, as it has shown good performance in previous experiments and similar tasks (Maheshwari et al., 2022; Jentoft, 2023). As baselines we consider Llama2-13B (Touvron et al., 2023) and Hunspell (Ooms, 2018).\\n\\nWe introduce two additional metrics in this experiment \u2013 Correct Word Retention Rate (CWRR) and Incorrect Word Correction Rate (IWCR):\\n\\n$$\\\\text{CWRR} = \\\\frac{\\\\text{CC}}{\\\\text{CC} + \\\\text{CI}}$$\\n\\n$$\\\\text{IWCR} = \\\\frac{\\\\text{IC}}{\\\\text{IC} + \\\\text{II}}$$\\n\\nHere $\\\\text{CC}$ and $\\\\text{CI}$ denote the number of PNs correctly recognized in the OCR output but correctly retained and incorrectly altered after OCR correction, respectively. $\\\\text{IC}$ and $\\\\text{II}$ represent the number of PNs incorrectly recognized in the OCR output but correctly and incorrectly altered post OCR correction, respectively.\\n\\nDiscussion. The results in Table 5 again validate the conclusion from Exp. 1 and Exp. 2 \u2013 using weak supervision to generate synthetic data with multiple noise levels enhances model performance. Training ByT5 with a single noise level can reduce the CER from 6.64 to 2.78, whereas multi-noise training further reduce it to 2.51. Applying weak supervision brings the CER down to 2.46. Applying SCN-TTA to weak supervision multi-noise training further improves performance, reducing CER to 2.08, and boosting CWRR to 0.887 and IWCR to 0.734. The SCN-TTA process takes approximately 2-5 minutes per book, significantly increasing the probability of correctly handling PNs. Llama reduces CER and WER less effectively than ByT5, but its CWRR and IWCR are notably high. We suspect this is due to benchmark data contamination (Xu et al., 2024), as Llama's pre-training data likely includes content from the RETAS dataset, but in real-world applications, the text requiring correction is often not widely circulated online and unlikely to be in pre-training data. Therefore, Llama's performance in handling PNs might not be as good as observed in this experiment. The performance improvement of the Llama model after instruction-tuning is mainly due to more stable output formatting.\\n\\nSamples of corrections produced by the ByT5 model, combined with weakly-supervised multi-noise training, are provided in Figure 4 in Appendix C. The results show that this combination yields strong correction capabilities.\\n\\n| Setup | WER | CER | CWRR | IWCR |\\n|-------|-----|-----|------|------|\\n| None  | 20.8| 6.64| -    | -    |\\n| Hunspell | 13.8| 3.73| 0.822| 0.433|\\n| Llama (one-shot) | 16.8| 3.95| 0.942| 0.763|\\n| Llama + SCN-TTA | 11.2| 3.21| 0.945| 0.798|\\n| Single | 10.8| 2.78| 0.736| 0.377|\\n| Multi | 9.10| 2.51| 0.695| 0.441|\\n| MultiW | 8.54| 2.46| 0.711| 0.458|\\n| MultiW + SCN-TTA | 6.68| 2.08| 0.887| 0.734|\\n\\nTable 5: Comparison of results on the RETAS dataset. Single denotes training with one OCR error level, while Multi refers to using multiple error levels. \u201cLlama + SCN-TTA\u201d refers to instruction-tuning using data generated by SCN-TTA.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Conclusion\\n\\nIn this paper, we explored how to generate the most effective synthetic data for post-OCR correction tasks. Our experiments show that using weak supervision and effective synthetic data with multiple noise levels can reduce the Character Error Rate of OCR texts by 62.95% when used in conjunction with the ByT5 model. Additionally, we have demonstrated that, by incorporating a novel test-time adaptation approach SCN-TTA, we can improve the model's ability to handle proper nouns, leading to a 68.67% reduction in CER.\\n\\nLimitations\\n\\nWe observe that the efficacy of SCN-TTA is reduced in texts with high OCR errors and shorter texts. In some cases it also struggles to reliably repair short PNs. The NMT model often fails to correct numerical errors in noisy text, a common issue across correction models. It also occasionally mishandles paired punctuation marks, such as quotation marks and parentheses. This paper focuses solely on the post-OCR task for English and does not present results for other languages, which we intend to investigate in future work.\\n\\nAcknowledgments\\n\\nThis publication is part of a project that has received funding from (i) the European Research Council (ERC) under the Horizon 2020 research and innovation programme (Grant agreement No. 884951); (ii) Science Foundation Ireland (SFI) to the Insight Centre for Data Analytics under grant No 12/RC/2289 P2.\\n\\nReferences\\n\\nChantal Amrhein and Simon Clematide. 2018. Supervised OCR error detection and correction using statistical and neural machine translation methods. Journal for Language Technology and Computational Linguistics (JLCL), 33(1):49\u201376.\\n\\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2016. Neural machine translation by jointly learning to align and translate. Preprint, arXiv:1409.0473.\\n\\nDavid Bamman, Ted Underwood, and Noah A Smith. 2014. A bayesian mixed effects model of literary character. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 370\u2013379.\\n\\nPratyay Banerjee, Tejas Gokhale, and Chitta Baral. 2021. Self-supervised test-time learning for reading comprehension. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1200\u20131211, Online. Association for Computational Linguistics.\\n\\nGuilherme Torresan Bazzo, Gustavo Acauan Lorentz, Danny Suarez Vargas, and Viviane P Moreira. 2020. Assessing the impact of OCR errors in information retrieval. In Advances in Information Retrieval: 42nd European Conference on IR Research, ECIR 2020, Lisbon, Portugal, April 14\u201317, 2020, Proceedings, Part II 42, pages 102\u2013109. Springer.\\n\\nCallum William Booth, Alan Thomas, and Robert Gaizauskas. 2024. BLN600: A parallel corpus of machine/human transcribed nineteenth century newspaper texts. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 2440\u20132446, Torino, Italia. ELRA and ICCL.\\n\\nEmanuela Boros, Maud Ehrmann, Matteo Romanello, Sven Najem-Meyer, and Fr\u00e9d\u00e9ric Kaeplan. 2024. Post-correction of historical text transcripts with large language models: An exploratory study. In Proceedings of the 8th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2024), pages 133\u2013159, St. Julians, Malta. Association for Computational Linguistics.\\n\\nHyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2022. Scaling instruction-finetuned language models. arXiv preprint arXiv:2210.11416.\\n\\nEvgenii Davydkin, Aleksandr Markelov, Egor Iuldashev, Anton Dudkin, and Ivan Krivorotov. 2023. Data generation for post-OCR correction of cyrillic handwriting. arXiv preprint arXiv:2311.15896.\\n\\nRui Dong and David A Smith. 2018. Multi-input attention for unsupervised OCR correction. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2363\u20132372.\\n\\nEva D'hondt, Cyril Grouin, and Brigitte Grau. 2017. Generating a training corpus for OCR post-correction using encoder-decoder model. In Proceedings of the 8th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 1006\u20131014.\\n\\nJohn Evershed and Kent Fitch. 2014. Correcting noisy OCR: Context beats confusion. In Proceedings of the First International Conference on Digital Access to Textual Cultural Heritage, pages 45\u201351.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marzieh Fadaee, Arianna Bisazza, and Christof Monz. 2017. Data augmentation for low-resource neural machine translation. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 567\u2013573, Vancouver, Canada. Association for Computational Linguistics.\\n\\nRoman Grundkiewicz, Marcin Junczys-Dowmunt, and Kenneth Heafield. 2019. Neural grammatical error correction systems with unsupervised pre-training on synthetic data. In Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 252\u2013263.\\n\\nShuhao Guan and Derek Greene. 2024. Advancing post-OCR correction: A comparative study of synthetic data. In Findings of the Association for Computational Linguistics ACL 2024, pages 6036\u20136047, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics.\\n\\nHarsh Gupta, Luciano Del Corro, Samuel Broscheit, Johannes Hoffart, and Eliot Brenner. 2021. Unsupervised multi-view post-ocr error correction with language models. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 8647\u20138652.\\n\\nMika H\u00e4m\u00e4l\u00e4inen and Simon Hengchen. 2019. From the past to the future: a fully automatic NMT and word embeddings method for OCR post-correction. arXiv preprint, (1910.05535).\\n\\nAhmed Hamdi, Axel Jean-Caurant, Nicolas Sid\u00e8re, Micka\u00ebl Coustaty, and Antoine Doucet. 2020. Assessing and minimizing the impact of OCR quality on named entity recognition. In Digital Libraries for Open Knowledge: Proc. 24th International Conference on Theory and Practice of Digital Libraries (TPDL 2020), pages 87\u2013101. Springer.\\n\\nRaphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations. In Proceedings of the 49th annual meeting of the association for computational linguistics: human language technologies, pages 541\u2013550.\\n\\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations.\\n\\nMinhao Hu, Tao Song, Yujun Gu, Xiangde Luo, Jieneng Chen, Yinan Chen, Ya Zhang, and Shaoting Zhang. 2021. Fully test-time adaptation for image segmentation. In Medical Image Computing and Computer Assisted Intervention\u2013MICCAI 2021: 24th International Conference, Strasbourg, France, September 27\u2013October 1, 2021, Proceedings, Part III 24, pages 251\u2013260. Springer.\\n\\nSvanhv\u00edt Lilja Ing\u00f3lfsd\u00f3ttir, P\u00e9tur Orri Ragnarsson, Haukur P\u00e1ll J\u00f3nsson, Haukur Barri S\u00edmonarson, V\u00edrhj\u00e1lmur \u00deorsteinsson, and V\u00e9steinn Sn\u00e6bjarnarson. 2023. Byte-level grammatical error correction using synthetic and curated corpora. arXiv preprint arXiv:2305.17906.\\n\\nEmi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thepchai Supnithi, and Hitoshi Isahara. 2003. Automatic error detection in the Japanese learners' English spoken data. In The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics, pages 145\u2013148.\\n\\nAtli Jasonarson, Stein\u00fe\u00f3r Steingr\u00edmsson, Einar Freyr Sigur\u00f0sson, \u00c1rni Dav\u00ed\u00f0 Magn\u00fasson, and Finnur \u00c1g\u00fast Ingimundarson. 2023. Generating errors: OCR post-processing for Icelandic. In The 24rd Nordic Conference on Computational Linguistics.\\n\\nAdam Jatowt, Mickael Coustaty, Nhu-Van Nguyen, Antoine Doucet, et al. 2019. Deep statistical analysis of OCR errors for effective post-OCR processing. In 2019 ACM/IEEE Joint Conference on Digital Libraries (JCDL), pages 29\u201338. IEEE.\\n\\nMatias Jentoft. 2023. Grammatical error correction with byte-level language models. Master's thesis.\\n\\nAmrith Krishna, Bodhisattwa P. Majumder, Rajesh Bhat, and Pawan Goyal. 2018. Upcycle your OCR: Reusing OCRs for post-OCR text correction in Romanised Sanskrit. In Proceedings of the 22nd Conference on Computational Natural Language Learning, pages 345\u2013355, Brussels, Belgium. Association for Computational Linguistics.\\n\\nSusan Leavy, Gerardine Meaney, Karen Wade, and Derek Greene. 2019. Curatr: a platform for semantic analysis and curation of historical literary texts. In Metadata and Semantic Research: 13th International Conference (MTSR 2019), pages 354\u2013366. Springer.\\n\\nJian Liang, Ran He, and Tieniu Tan. 2023. A comprehensive survey on test-time adaptation under distribution shifts. Preprint, arXiv:2303.15361.\\n\\nXiaofan Lin. 2001. Reliable OCR solution for digital content re-mastering. In Document Recognition and Retrieval IX, volume 4670, pages 223\u2013231. SPIE.\\n\\nViktoria L\u00f6fgren and Dana Dann\u00e9lls. 2024. Post-OCR correction of digitized Swedish newspapers with ByT5. In Proceedings of the 8th Joint SIGHUM Workshop on Computational Linguistics for Cultural Heritage, Social Sciences, Humanities and Literature (LaTeCH-CLfL 2024), pages 237\u2013242, St. Julians, Malta. Association for Computational Linguistics.\\n\\nWilliam B Lund and Eric K Ringger. 2011. Error correction with in-domain training across multiple OCR system outputs. In 2011 International Conference on Document Analysis and Recognition, pages 658\u2013662. IEEE.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-862", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Maritinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodr\u00edguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288.\\n\\nDaniel Van Strien, Kaspar Beelen, Mariona Coll Ardanuy, Kasra Hosseini, Barbara McGillivray, and Giovanni Colavizza. 2020. Assessing the impact of ocr quality on downstream NLP tasks. In ICAART (1), pages 484\u2013496.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nTu Vu, Tong Wang, Tsendsuren Munkhdalai, Alessandro Sordoni, Adam Trischler, Andrew Mattarella-Micke, Subhransu Maji, and Mohit Iyyer. 2020. Exploring and predicting transferability across NLP tasks. arXiv preprint arXiv:2005.00770.\\n\\nYanshan Wang, Sunghwan Sohn, Sijia Liu, Feichen Shen, Liwei Wang, Elizabeth J Atkinson, Shreyasee Amin, and Hongfang Liu. 2019. A clinical text classification paradigm using weak supervision and deep representation. BMC medical informatics and decision making, 19:1\u201313.\\n\\nHaoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuan Peng, Chunrui Han, and Xiangyu Zhang. 2024. General ocr theory: Towards ocr-2.0 via a unified end-to-end model. Preprint, arXiv:2409.01704.\\n\\nAndre Wolters and Andreas Van Cranenburgh. 2024. Historical dutch spelling normalization with pre-trained language models. Computational Linguistics in the Netherlands Journal, 13:147\u2013171.\\n\\nRuoyu Xie and Antonios Anastasopoulos. 2023. Noisy parallel data alignment. In Findings of the Association for Computational Linguistics: EACL 2023, pages 1501\u20131513, Dubrovnik, Croatia. Association for Computational Linguistics.\\n\\nCheng Xu, Shuhao Guan, Derek Greene, and M-Tahar Kechadi. 2024. Benchmark data contamination of large language models: A survey. Preprint, arXiv:2406.04244.\"}"}
{"id": "emnlp-2024-main-862", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"ByT5 model fine-tuning with MultiW are provided in Figure 4.\\n\\nTable 6: Comparison of the text quality in the synthetic\\n\\n| Error Level | TrCER | TrWER | TeCER | TeWER |\\n|-------------|-------|-------|-------|-------|\\n| 0.3         | 1.03% | 6.87% | 49.28%| 1.92% |\\n| 1.0         | 2.37% | 13.12%| 43.03%| 18.79%|\\n| 3.0         | 5.44% | 26.00%| 35.49%| 26.00%|\\n| 5.0         | 8.02% | 35.49%| 35.49%| 26.00%|\\n| 7.0         | 10.33%| 43.03%| 35.49%| 26.00%|\\n| 9.0         | 12.45%| 43.03%| 35.49%| 26.00%|\\n\\nThe model had a window size of 20, used beam search for decoding, and applied uniform weight-\\n\\nTable 7: Scores for WER and WERR across all models\\n\\n| Model        | TrWER | TeWER | WERR |\\n|--------------|-------|-------|------|\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| mBART        | 0.2   | 0.2   | 0.2  |\\n| m"}
{"id": "emnlp-2024-main-862", "page_num": 14, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Post-OCR correction samples from the ByT5 model with weak supervision and multi-noise training.\"}"}
