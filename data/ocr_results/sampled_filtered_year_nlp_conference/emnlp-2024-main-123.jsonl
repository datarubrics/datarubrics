{"id": "emnlp-2024-main-123", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Optimizing Code Retrieval: High-Quality and Scalable Dataset Annotation through Large Language Models\\n\\nRui Li1, Qi Liu1, 2*, Liyang He1, Zheng Zhang1, Hao Zhang1, Shengyu Ye1, Junyu Lu1, 2, Zhenya Huang1, 2\\n\\n1 State Key Laboratory of Cognitive Intelligence, University of Science and Technology of China\\n2 Institute of Artificial Intelligence, Hefei Comprehensive National Science Center\\n\\n{ruili2000, heliyang, zhangzheng, zh2001, ysy007, lujunyu}@mail.ustc.edu.cn\\n{qiliuql, huangzhy}@ustc.edu.cn\\n\\nAbstract\\n\\nCode retrieval aims to identify code from extensive codebases that semantically aligns with a given query code snippet. Collecting a broad and high-quality set of query and code pairs is crucial to the success of this task. However, existing data collection methods struggle to effectively balance scalability and annotation quality. In this paper, we first analyze the factors influencing the quality of function annotations generated by Large Language Models (LLMs). We find that the invocation of intra-repository functions and third-party APIs plays a significant role. Building on this insight, we propose a novel annotation method that enhances the annotation context by incorporating the content of functions called within the repository and information on third-party API functionalities. Additionally, we integrate LLMs with a novel sorting method to address the multi-level function call relationships within repositories. Furthermore, by applying our proposed method across a range of repositories, we have developed the Query4Code dataset. The quality of this synthesized dataset is validated through both model training and human evaluation, demonstrating high-quality annotations. Moreover, cost analysis confirms the scalability of our annotation method.\\n\\n1 Introduction\\n\\nCode retrieval aims to find the most relevant code snippet in a database given a user query, facilitating the reuse of programs in the software development process (Bui et al., 2021; Li et al., 2022; He et al., 2024) and driving recent research on retrieval-augmented code generation (Zhou et al., 2022; Zhao et al., 2024). To achieve good performance in practical applications, the key lies in collecting a wide range of high-quality, dual-modal pairing data between natural language queries and code snippets.\\n\\nAn efficient approach to collect code retrieval datasets involves directly gathering code data from online repositories (e.g., GitHub2) and processing it to extract code snippets along with their corresponding docstrings. As depicted in Figure 1, since the docstring serves as a description of the function code, it can be utilized as a query. However, a significant difference exists between the docstring and the user's query, resulting in a deviation from queries encountered in real-world scenarios. To bridge this gap and obtain queries that closely resemble those of actual users, some researchers (Heyman and Van Cutsem, 2020; Yao et al., 2018) tend to collect user questions and the corresponding code snippets from programming communities such as Stack Overflow3. Another approach explored by researchers (Rao et al., 2021; Huang et al., 2021) involves gathering user search queries from browser logs and subsequently enlisting experts to annotate corresponding code snippets based on these queries. Regrettably, the for-\\n\\n2https://github.com\\n3https://stackoverflow.com\\n\\n---\\n\\n```python\\ndef export_nb(nb_path):\\n    exporter = PythonExporter()\\n    output, res = exporter.from_filename(nb_path)\\n    if 'outputs' in res:\\n        for filename, content in res['outputs'].items():\\n            savefile(filename, content)\\n    return output\\n```\\n\\nFigure 1: Example of code snippet and corresponding query and docstring.\\n\\nHow to export the content of a Jupyter Notebook file.\\n\\nDocstring\\n\\nExport content from a Jupyter notebook file.\\n\\nParameters:\\n- nb_path : The file path of the Jupyter notebook to be exported.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mer approach often produces code snippets of inferior quality because of the presence of block and statement-level code within the community. On the other hand, the latter approach allows for the acquisition of a high-quality dataset but proves to be cost-prohibitive and challenging to scale. Therefore, we pose a question: Can a more efficient, low-cost method be developed to obtain a high-quality code retrieval dataset?\\n\\nThe formidable capabilities of Large Language Models (LLMs) present a remarkable opportunity. Firstly, previous research (Rodriguez-Cardenas et al., 2023) has demonstrated the profound code comprehension ability of LLMs in various code understanding tasks, such as code summarization (Geng et al., 2023). Secondly, existing LLMs, employing preference alignment techniques (Geng et al., 2023), can generate content that aligns with human preferences. In the domain of search, some studies (Bonifacio et al., 2022; Dai et al., 2022) have proposed generating the query from the documents, yielding highly promising outcomes. Hence, a straightforward approach is to employ LLMs to generate user-like queries from the code snippets. However, there are some differences between code snippets and traditional documents. For instance, intra-repository function calls refer to the calls between different functions within a repository project, as depicted in Figure 1. Function export_nb calls function savefile, which makes it challenging for LLMs to comprehend function export_nb if only provided as input, without considering the function savefile it calls. Additionally, third-party API calls involve invoking functions from external APIs, as shown in Figure 1. Function export_nb calls the third-party API PythonExporter.from_filename, and LLM needs to understand the functionality of this API for a better understanding of the function.\\n\\nIn this paper, we first analyze the main factors affecting the quality of annotations for functions in repositories. Through preliminary experiments on a development set from 100 selected repositories, we observe that the presence of intra-repository function calls exerts a substantial influence on the quality of annotations, with a greater number of call relationships resulting in a heightened degree of impact. Additionally, we uncover that infrequent third-party calls have the greatest impact on annotation quality. This observation may be attributed to the limited pretraining knowledge of LLMs regarding these external libraries. Based on these findings, we propose an annotation algorithm aimed at using LLMs for high-quality code retrieval query annotations. We start by parsing the relationships of intra-repository function calls and use a topological sorting approach to guide the LLM annotation sequence. For third-party function calls, we select third-party functions based on popularity and use web scraping to annotate features of unpopular third-party functions, adding this information to the annotation context.\\n\\nTo substantiate the efficacy of our annotation approach, we initially employed our method to obtain a large-scale code retrieval dataset Query4Code, which includes 237.2K queries and code pairs from 12.3K repositories. We use Query4Code as a pretraining corpus for various code retrieval models. Subsequently, comprehensive evaluations on multiple real-world benchmarks confirmed that our method significantly enhances the performance of code retrieval models in real scenarios.\\n\\n2 Related Work\\n\\n2.1 Code Retrieval Datasets\\n\\nRepresentation learning (Zhang et al., 2023b; Gao et al., 2021; Liu et al., 2023) has achieved significant results in multiple fields. The previous code retrieval methods (Sedykh et al., 2023) of code retrieval data collection can be summarized into three categories: 1). Some researchers (Wang et al., 2023b) parse functions and corresponding docstrings from online repositories to form pairs. For example, Husain et al. (2019) collected 2.1M paired data of 6 programming languages from an open-source repository on GitHub, constituting the CodeSearchNet. 2). Others (Yin et al., 2018) gather questions posted by users on Stack Overflow along with the accepted code snippets to create datasets suitable for code searching. Heyman and Van Cutsem (2020) attempts this by collecting the most popular dataset posts on Stack Overflow and gathering code snippets from highly upvoted responses. 3). The use of manual annotation methods: Huang et al. (2021) initially collects human queries used in code searches from search engines and then manually gathers relevant code snippets from GitHub to match these queries. However, these methods present a trade-off between data quality and scalability. Therefore, we propose a low-cost and scalable annotation method.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2 Code Retrieval Models\\n\\nIn token-level pre-training methods, CodeBERT (Feng et al., 2020) attempts to leverage the extensive programming and natural language bimodal data within repositories for pre-training. Building upon this, GraphCodeBERT (Guo et al., 2021) endeavors to incorporate data flow graph signals to devise new pre-training tasks, thereby enhancing the understanding of code semantics. UniXcoder (Guo et al., 2022) introduces a unified cross-modal pre-training model specifically designed for programming languages. Recently, some studies have explored the use of contrastive learning approaches to augment code search tasks. ContraCode (Jain et al., 2021) and Corder (Bui et al., 2021) employ semantic-preserving variation techniques for data augmentation and utilize contrastive learning objectives to distinguish between similar and dissimilar code snippets. CodeRetriever (Li et al., 2022) attempts to combine unimodal and bimodal contrastive learning to train code search models.\\n\\n2.3 LLM in Data Annotation\\n\\nGiven the strong generalization capabilities exhibited by Large Language Models (LLMs), they apply across multiple domains (Samuel et al., 2023; Zhang et al., 2024) for data synthesis, facilitating the transfer of rich knowledge from larger models to smaller ones. In Unnatural Instructions (Honovich et al., 2023) and Self-Instruct (Wang et al., 2023a), LLMs utilize to generate the instructional datasets required during the fine-tuning phase. Samuel et al. (2023) utilize a minimal set of original data to guide LLMs in generating datasets required for reading comprehension tasks. West et al. (2022) propose a two-step process for symbolic knowledge distillation rather than the creation of content-related datasets. In the field of information retrieval, Zhang et al. (2023a) utilize LLMs to generate positive and negative samples during the training process of contrastive learning.\\n\\nThis paper is the first to use LLMs to annotate code retrieval dataset, focusing on the key factors that affect LLMs in generating queries: library calls and third-party API calls.\\n\\n3 Preliminary Analysis\\n\\nThe direct use of LLMs for annotating functions often results in a lack of contextual information about the annotated functions. Therefore, this section attempts to analyze the impact of intra-repository calls and third-party API calls on LLM annotated queries. Experiments are conducted using the GPT-3.5-turbo (Achiam et al., 2023) and CodeLlama-Instruct 7B (Roziere et al., 2023) models, with all prompts and detailed information being provided in Appendix A.\\n\\n3.1 Setup\\n\\nBased on the selection of high-quality repositories identified from prior research (Husain et al., 2019), we randomly chose 100 repositories to form our development set. Subsequently, we employ the tree-sitter library to parse code files within these repositories, acquiring all function-level code snippets and their invocation relationships. These relationships are further categorized into intra-repository calls and third-party API calls.\\n\\n3.2 Impact of Intra-Repository Function Calls\\n\\nDue to the existence of multiple functions in the repository, these functions are usually involved in complex call relationships. After parsing, from Table 1, we can observe the proportion of functions with call relationships, as well as the average and maximum call frequencies. We observe that 46.5% of the code has call relationships, and the maximum number of calls can reach 137 times. This highlights the widespread use of function calls in repositories.\\n\\n![Table 1: Statistics on the number and proportion of calls to intra-repository and third-party library APIs.](https://tree-sitter.github.io)\"}"}
{"id": "emnlp-2024-main-123", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The overview of our annotation method. (a) Files in the repository. (b) Function call graph obtained from parsing. (c) API calls obtained from parsing and their corresponding popularity. (d) Construct annotated context based on call relationships and current API calls. (e) Pipeline for annotation method.\\n\\nThe impact of third-party APIs with Different Popularity Levels on LLM Understanding.\\n\\n3.3 Impact of Third-Party APIs Calls\\n\\nAfter analyzing the invocation of third-party APIs in functions, as shown in Table 1, we observe that 53.5% of the functions involve third-party API calls, with the maximum number of calls reaching 120 times. We next examine the impact of third-party APIs on annotation quality. Inspired by previous research (Mallen et al., 2023), we consider that the impact of APIs on annotation quality is closely related to the API's popularity. Therefore, we initially use the frequency of API calls in the repositories as a proxy for API popularity. We then annotate functions in our development set using LLMs, including all available API documentation. GPT-4-turbo is used to compare LLM explanations of API functions against the actual API documentation, with results categorized according to popularity. Our findings, presented in Figure 4, show that LLMs often lack a comprehensive grasp of many API details, particularly for unpopular APIs. This phenomenon adversely affects the quality of LLM annotations for queries. And even for models with stronger performance (e.g., gpt-3.5-turbo), the understanding of low-popularity APIs is also poor.\\n\\n4 Approach\\n\\n4.1 Overview\\n\\nIn the preceding analysis, we demonstrate how the invocation relationships within a repository and those in third-party libraries can impact the quality of LLM annotations.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of Large Language Models (LLMs) in annotating queries. As shown in Figure 3, we attempt to propose an annotation method to address these issues. We endeavor to collect information about functions with invocation relationships, as well as functionalities of unpopular APIs, and incorporate them into the annotation context. Then, we use this context to prompt LLMs to generate queries (see the prompt in Appendix B).\\n\\n4.2 Task Decomposition\\n\\nInspired by previous research work (Wei et al., 2022), a complex task can be simplified by decomposing it into multiple simpler tasks, thereby easing the model\u2019s inference load. For the task of query annotation, we consider that the model first needs to understand the code of the currently annotated function and then generate queries that a user might write during the development process based on this understanding of code semantics. As shown in Figure 3 (e), we initially use LLMs for code interpretation and then proceed to annotate queries based on the interpretation and the content of the code snippets:\\n\\n$$s = \\\\text{LLM}(c), q = \\\\text{LLM}(s, c).$$\\n\\nIn the code interpretation stage, we mainly rely on the LLM\u2019s understanding of the code, while in the query generation stage, the alignment capability of LLMs with human intent is primarily utilized.\\n\\n4.3 Analyzing Function and API Calls\\n\\nSince in Section 3, we have analyzed that the main factors affecting the quality of LLM annotations for queries are function calls within the repository and third-party API calls. Therefore, as shown in the upper of Figure 3, for a given repository, we first use the tree-sitter tool to parse all functions in the code files within the repository. Then, we analyze each function\u2019s calls to other intra-repository functions and third-party APIs separately.\\n\\n4.4 Annotation Algorithm Based on Function Call Graph\\n\\nHaving established the function invocation relationships within the repository, a straightforward approach would be to include the relevant context of the function to be annotated along with the query into the LLM\u2019s input context. However, as shown in Figure 3 (b), there are multi-level call relationships between functions in the repository. Understanding the train function requires knowing the \\\\texttt{train_batch} function because it calls the \\\\texttt{train_batch} function, which then calls the \\\\texttt{contrastive_loss} function. Similarly, to grasp the \\\\texttt{train_batch} function properly, it\u2019s essential to understand the \\\\texttt{contrastive_loss} function. Directly incorporating all functions into the context would pose challenges associated with multi-level reasoning.\\n\\nThus, we propose a novel annotation algorithm based on topological ordering. The intuition behind this algorithm is the decoupling of multi-level invocation relationships into single-level relationships. Specifically, we first construct a directed graph $G(V, E)$ of function calls, where each node $v \\\\in V$ represents a function in the repository. If function $A$ is called by function $B$, there will be a directed edge $e \\\\in E$ from $v_A$ to $v_B$. Based on topological sorting, we first annotate functions without dependency relationships. During the annotation process, when encountering recursive calls, we randomly delete an edge to continue with the annotation. Subsequently, we annotate functions with invocation relationships, thus breaking down multi-level invocation relationships into single-level relationships. For the annotation context of the function currently being annotated, it is only necessary to include information about its directly called functions. We summarized the algorithm in Algorithm 1.\\n\\nAlgorithm 1\\n\\nInput: A directed function call graph, $G(V, E)$;\\nOutput: The annotation order of functions, $L$;\\n1: Initialize sorted elements list $L \\\\leftarrow \\\\emptyset$\\n2: Compute in-degrees $d_{in}(v), \\\\forall v \\\\in V$\\n3: Initialize a queue $Q \\\\leftarrow \\\\{v \\\\in V: d_{in}(v) = 0\\\\}$\\n4: while $Q \\\\neq \\\\emptyset$ or $|L| \\\\neq |V|$ do\\n5: while $Q = \\\\emptyset$ and $|L| \\\\neq |V|$ do\\n6: $e \\\\leftarrow \\\\text{RandomSelect}(E)$\\n7: $E \\\\leftarrow E \\\\{e\\\\}$\\n8: $Q \\\\leftarrow \\\\{v \\\\in V: d_{in}(v) = 0\\\\}$\\n9: end while\\n10: $v \\\\leftarrow \\\\text{Dequeue}(Q)$\\n11: $L \\\\leftarrow L \\\\cup \\\\{v\\\\}$\\n12: for $u \\\\in \\\\text{Adjacent}(v)$ do\\n13: $d_{in}(u) \\\\leftarrow d_{in}(u) - 1$\\n14: if $d_{in}(u) = 0$ then\\n15: $Q \\\\leftarrow Q \\\\cup \\\\{u\\\\}$\\n16: end if\\n17: end for\\n18: end while\\n19: return $L$\"}"}
{"id": "emnlp-2024-main-123", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.5 Collection of Third-Party API Documentation Based on Popularity\\n\\nIn Section 3, our analysis indicates that LLMs struggle to understand unpopular APIs. Therefore, we aim to add descriptions of unpopular third-party API functionalities in the annotation context. As shown in Figure 3 (c), first, we need to assess the popularity of APIs, using the frequency of API calls in the repository as a basis for popularity. Our analysis concludes that LLMs understand APIs better if they exceed a popularity threshold. Therefore, we set a popularity threshold and for third-party APIs below this threshold in the function, we use the DuckDuckGo search engine to look up documentation and employ LLM to summarize the API functionalities. Then, we add this information into the annotation context.\\n\\n4.6 Data Filtering\\n\\nTo further enhance the quality of generated queries and improve the explainability of the annotation process, we attempt to incorporate a reverse validation and an explanation phase for the query and code snippet pairs into the annotation framework. Specifically, as shown in Figure 3 (e), after completing the annotation to obtain aligned query and code snippet pairs, we first use LLMs for reverse validation. Inspired by Huang et al. (2021), we notice that the code in the annotated query-code pairs cannot fully answer the query. It may exceed, partially satisfy, or completely fail to meet the query requirements. Specifically, we focus on the following four scenarios: 1) If the code can answer and exceed the query requirements, it is considered a correct answer. 2) If the code can satisfy certain categories of query requirements, it is also deemed a correct answer. 3) If the code satisfies less than 50% of the query requirements, it cannot correctly answer the query. 4) The code has almost no relevance to the query. Based on this principle, we construct the \\\\( f(q,c) = \\\\text{LLM}(q,c,\\\\text{CLS}) \\\\).\\n\\nThen, we will filter out the code snippets of categories 1 and 2 from the original constructed dataset \\\\( C \\\\) to obtain \\\\( C_{\\\\text{filtered}} \\\\):\\n\\n\\\\[\\nC_{\\\\text{filtered}} = \\\\{ c \\\\in C | f(q,c) \\\\in \\\\{1,2\\\\} \\\\}.\\n\\\\]\"}"}
{"id": "emnlp-2024-main-123", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Compare the zero-shot and fine-tune performance of code representation models pre-trained on Code-SearchNet (CSN) and Query4Code (Q4C) datasets.\\n\\n| Model          | CoNaLa | SO-DS | StaQC | CoSQA | WebQueryTest |\\n|---------------|--------|-------|-------|-------|--------------|\\n| **Zero-Shot** |        |       |       |       |              |\\n| CodeBERT      | 21.65  | 25.45 | 18.42 | 18.98 | 14.26        |\\n| GraphCodeBERT | 23.70  | 28.88 | 19.01 | 21.56 | 16.90        |\\n| UniXCoder     | 25.47  | 29.07 | 18.78 | 19.85 | 16.45        |\\n| StarEncoder   | 25.72  | 28.14 | 17.31 | 19.65 | 15.55        |\\n| **Fine-Tuning** |       |       |       |       |              |\\n| CodeBERT      | 22.41  | 26.83 | 23.24 | 25.76 | 23.75        |\\n| GraphCodeBERT | 25.01  | 29.15 | 24.05 | 25.92 | 24.41        |\\n| UniXCoder     | 26.27  | 29.96 | 23.59 | 25.90 | 23.38        |\\n| StarEncoder   | 26.05  | 29.58 | 24.31 | 26.83 | 24.07        |\\n\\n5.2.2 Benchmark and Metric\\n\\nIn order to evaluate the performance of the model in real-world code retrieval scenarios, we have selected a wide range of benchmarks for validation. Among them, the datasets CoNaLa (Yin et al., 2018), SO-DS (Heyman and Van Cutsem, 2020), and StaQC (Yao et al., 2018) are collected from Stackoverflow questions, and queries in CoSQA (Huang et al., 2021) and WebQueryTest (Lu et al., 2021) are collected from web search engines. Therefore, the queries in these datasets are closer to real code search scenarios. The statistics of benchmark datasets are listed in Table 2. Following prior research works (Kanade et al., 2020; Li et al., 2024), we employed Mean Reciprocal Rank (MRR) (He et al., 2023) as the evaluation metric:\\n\\n\\\\[\\nMRR = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\frac{1}{rank_i},\\n\\\\]\\n\\nwhere rank_i is the rank of the correct code snippet related to the i-th query.\\n\\n5.2.3 Training Objective\\n\\nGiven a paired query q and code c+, we adopt the contrastive learning InfoNCE objective function commonly used in existing code retrieval tasks for model training. Furthermore, we employ an in-batch negative sampling approach for selecting negative samples c- in contrastive learning:\\n\\n\\\\[\\nL = -E \\\\left[ \\\\log \\\\frac{\\\\exp (q \\\\cdot c^+)}{\\\\exp (q \\\\cdot c^+) + \\\\sum_{j=1}^{N} \\\\exp (q \\\\cdot c^-_j)} \\\\right],\\n\\\\]\\n\\nwhere N represents batch size.\\n\\n5.2.4 Implementation details\\n\\nAll experiments are implemented using PyTorch. During the pre-training phase, for all settings related to model architecture and hyperparameters, we follow the original paper. During the fine-tuning phase, to adapt to variations between different datasets, we conduct a grid search on the downstream dataset to find the learning rate, setting the range in our experiments as {1e-5, 2e-5, 5e-5}, and utilize the AdamW optimizer. The options for batch size included {32, 64, 128}. Training is set for 10 epochs and to prevent overfitting, we adopt an early stopping strategy. The experiments described in this paper are conducted with three random seeds: 0, 1, and 2, and we will report the average results in the paper. All experiments meet the p< 0.01 significance threshold. Experiments are conducted on a GeForce RTX 4090 GPU.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.2.5 Results\\n\\nZero-shot Performance\\n\\nThe final zero-shot experimental results, as shown in Table 3, indicate that pre-training on the Query4Code dataset significantly enhances performance compared to pre-training on the CodeSearchNet dataset, with improvements observed across multiple code representation models. Additionally, we note substantial performance gains on both the CoSQA and WebQueryTest datasets. We attribute this improvement to the fact that the queries in these two datasets were extracted from logs of real-world search engines, which closely match the distribution of our annotated queries. Conversely, the improvement on the SO-DS dataset was minimal, likely due to a greater disparity between the code snippets in the SO-DS dataset and our annotated dataset.\\n\\nFine-tuning Performance\\n\\nIn the fine-tuning experiment, it is worth noting that since the WebQueryTest dataset is specifically designed for assessing real-world code retrieval task performance without available training data, its related results were not reported. The final experiments demonstrate that pretraining with the Query4Code dataset before fine-tuning yielded superior performance across all other datasets, confirming that models pretrained through Query4Code exhibit enhanced adaptability in real-world code retrieval scenarios.\\n\\n5.3 The potential of the dataset\\n\\nTable 4: Using different data pairs with Query4Code to train CodeBERT for zero-shot performance.\\n\\n| Dataset          | (q,c)  | (s,c) | (q,c) + (s,c) |\\n|------------------|--------|-------|---------------|\\n| CoNaLa           | 25.45  | 23.28 | 26.39         |\\n| SO-DS            | 18.98  | 19.35 | 20.17         |\\n| StaQC            | 15.74  | 15.92 | 16.51         |\\n| CoSQA            | 59.80  | 58.46 | 61.93         |\\n| WebQueryTest     | 35.61  | 35.07 | 36.55         |\\n\\nAlthough this paper mainly focuses on generating annotations for query retrieval of code, our two-stage annotation method can obtain functional summaries of functions. We are interested in whether the functional summary of functions can enhance the ability of the current code retrieval model. As shown in Table 4, compared with only using (q,c) pairs (denoted as C_qc), using only (s,c) pairs (denoted as C_sc) achieved comparable performance and performed better on the SO-DS and CoSQA datasets. Furthermore, utilizing both annotated query q and summary c data achieved the best performance. For detailed experimental settings, please refer to Appendix B.2. This demonstrates the potential of the our annotation method.\\n\\n5.4 Human Evaluation\\n\\nTo evaluate the quality of the data generated by the annotation algorithm we proposed, we employed a manual assessment approach. We extracted 200 pairs of queries and code snippets from the Query4Code dataset and invited three experts to score them according to the four types mentioned in Section 4.6. We then calculate the Pearson's r and Kendall's \u03c4 correlation coefficients between the scores and the results generated by the model. The results are summarized in Table 5. Observation reveals that the query-code pairs we annotate demonstrate a strong correlation, confirming the effectiveness of our filtering method.\\n\\nTo understand the correlation of annotations among experts, we calculated Krippendorff's Alpha for the scores of three experts, resulting in a final consistency score of 0.858, which proves that there is a high level of consistency in the scores among the experts.\\n\\n| Expert   | r    | \u03c4    | score |\\n|----------|------|------|-------|\\n| Expert1  | 0.652| 0.483| 2.47  |\\n| Expert2  | 0.630| 0.469| 2.65  |\\n| Expert3  | 0.623| 0.471| 2.58  |\\n\\nTable 5: Results of human evaluation.\\n\\n5.5 Cost Analysis\\n\\nOur annotation algorithm surpasses traditional expert annotation methods in both cost-effectiveness and time efficiency. The API call cost for the GPT-3.5-turbo model we used generally ranges from $0.001 to $0.004, allowing for the processing of approximately 3K requests per minute. In contrast, based on crowdsourcing platform rates, the cost for pairing a query with a code snippet is around $0.2; meanwhile, the time required for an expert to annotate, including reading the query and finding a matching code snippet, typically takes about 3 minutes. This demonstrates the superior scalability of our method.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"def escape_shell_arg(shell_arg):\\n    if isinstance(shell_arg, six.text_type):\\n        msg = \\\"ERROR: escape_shell_arg() expected string argument but got '{%s}' of type '{%s}'. \\\" % (repr(shell_arg), type(shell_arg))\\n        raise TypeError(msg)\\n    return \\\"'%s'\\\" % shell_arg.replace('\\'', r'\\\\')\\n\\n\"\"\"Escape shell argument shell_arg by placing it within single-quotes. Any single-quotes found within the shell argument string will be escaped.\\n\\n@param shell_arg: The shell argument to be escaped.\\n@type shell_arg: string...\\n\\n6 Conclusion\\n\\nIn this paper, we addressed the trade-off between quality and scalability inherent in the construction methods of previous code retrieval datasets by attempting to generate queries based on Large Language Models (LLMs). Initially, we analyzed the key factors affecting the annotation of queries by LLMs and identified that both intra-repository function calls and third-party API calls significantly impacted annotation quality. Based on this understanding, we had designed an annotation algorithm that constructed appropriate contexts by parsing call relationships to generate function queries. Moreover, we had utilized existing code snippets to create the Query4Code dataset. Through model validation and manual assessment, the high quality of the Query4Code dataset was confirmed, and cost analysis had demonstrated the scalability of our annotation approach.\\n\\nLimitations\\n\\nThis study primarily focuses on utilizing Large Language Models (LLMs) for the construction of code retrieval datasets and demonstrates the significant impact of call relations on the understanding of function-level code snippets in repositories by language models. However, this paper has certain limitations. Due to cost considerations, we only analyzed and annotated a Python dataset. Although our analytical method is adaptable across different programming languages, we cannot guarantee that our conclusions will perform consistently across various languages. Therefore, we aim to explore the construction of code retrieval datasets for other programming languages using LLMs in future work.\\n\\nEthical consideration\\n\\nThis paper explores how large language models (LLMs) can be used for code retrieval data synthesis, focusing on their advantages and challenges. One major issue is that LLMs may produce hallucinations, meaning that the information they generate sometimes appears correct but is actually incorrect or irrelevant. This inaccuracy can undermine the quality of the synthetic data, leading to errors in code retrieval. Additionally, using synthetic data may introduce biases, which could affect the effectiveness of the retrieval process, potentially making it less accurate or fair.\\n\\nAcknowledgments\\n\\nThis research was supported by grants from the National Natural Science Foundation of China (Grants No. 62337001, 623B1020), the Fundamental Research Funds for the Central Universities, and the CIPSCSMP-Zhipu.AI Large Model Cross-Disciplinary Fund.\\n\\nReferences\\n\\nJosh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774.\\n\\nLuiz Bonifacio, Hugo Abonizio, Marzieh Fadaee, and Rodrigo Nogueira. 2022. Inpars: Data augmentation for information retrieval using large language models. arXiv preprint arXiv:2202.05144.\"}"}
{"id": "emnlp-2024-main-123", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-123", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "emnlp-2024-main-123", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use the CodeLlama-Instruct 7B and GPT-3.5-turbo, where we load the checkpoint for CodeLlama-Instruct 7B from huggingface. For GPT-3.5-turbo, we chose to experiment with the gpt-3.5-turbo-0613 version. And we use the GPT-4-turbo model for scoring, where we select the gpt-4-1106-preview version for experimentation.\\n\\nFor GPT model, we use the official OpenAI API and employ the default temperature parameters and sampling methods.\\n\\nA.1 LLM Inference Details\\nIn the inference process of CodeLlama-Instruct 7B, we adopt a sampling method with a temperature parameter of 0.2 and top-p of 0.95. Additionally, we utilize the vLLM (Kwon et al., 2023) inference library, which integrates various decoding techniques to accelerate sampling during generation.\\n\\nA.2 Prompts for Analysis\\n\\n**System Prompt for Directly Generating Query**\\nPlease act as a query generator. For the given function-level code snippet in the repository, please provide a query that the user might use. This query should be able to search for that function in a search engine. Note that you should not provide any other information.\\n\\n**User Input**\\nCode: {code snippet}\\n\\n**System Prompt for Generating Query (w/ Context)**\\nPlease act as a query generator. For the given function-level code snippet in the repository and the information about functions called within those code snippets, please provide a query that the user might use. This query should be able to search for that function in a search engine. Note that you should not provide any other information.\\n\\n**User Input**\\nCode: {code snippet}\\nCalled Function: {called code snippet}\\n\\n**Verification System Prompt for Query**\\nPlease play the role of a programming expert. For the given user queries and function pairs, please judge whether the code can meet the needs of the user's query based on the following principles:\\n\\n1. The code can answer and exceed the requirements for query needs (3 points);\\n2. The code can satisfy a certain category of query needs (2 points);\\n3. The code only meets less than 50% of query needs (1 points);\\n4. The code is only minimally related to the query (0 point).\\n\\nPlease provide an explanation along with corresponding scores, noting that you need to output in JSON format as follows: `{\"Explanation\": <explanation>, \\\"Score\\\": <score>}`, without providing any other information.\\n\\n**User Input**\\nCode: {code snippet}\\nQuery: {query}\\n\\n**System Prompt for API Explanation**\\nPlease provide a detailed explanation of the functionality of the third-party library API and the role of its mandatory parameters. Please note that you do not need to provide any additional output.\\n\\n**User Input**\\nAPI: {API}\\n\\n**System Prompt for API Explanation (w/ Document)**\\nPlease summarize the functions of the API and the roles of its mandatory parameters based on the API and document information. Please note that you do not need to provide any additional output.\\n\\n**User Input**\\nAPI: {API}\\nDocument: {doc}\"}"}
{"id": "emnlp-2024-main-123", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"User Input\\nAPI Documentation Explanation: {function}\\nUser-Provided description: {description}\"}"}
