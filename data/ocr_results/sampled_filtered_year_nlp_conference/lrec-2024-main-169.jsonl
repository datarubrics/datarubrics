{"id": "lrec-2024-main-169", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Automatic Data Visualization Generation from Chinese Natural Language Questions\\n\\nYan Ge\u2020, Victor Junqiu Wei\u2217\u2020, Yuanfeng Song3\u2217, Jason Chen Zhang2, Raymond Chi-Wing Wong1\\n1The Hong Kong University of Science and Technology, Clear Water Bay, Kowloon, Hong Kong, China\\n2The Hong Kong Polytechnic University, Hung Hom, Kowloon, Hong Kong, China\\n3WeBank Co., Ltd, Shenzhen, China\\nyangecn@hotmail.com, jason-c.zhang@polyu.edu.hk\\n{jweiad, songyf, raywong}@cse.ust.hk\\n\\nAbstract\\nData visualization has emerged as an effective tool for getting insights from massive datasets. Due to the hardness of manipulating the programming languages of data visualization, automatic data visualization generation from natural languages (Text-to-Vis) is becoming increasingly popular. Despite the plethora of research efforts on the English Text-to-Vis, studies have yet to be conducted on data visualization generation from questions in other languages like Chinese. Motivated by this, we propose the first Chinese Text-to-Vis dataset named CNvBench in the paper and then demonstrate our first attempt to tackle this problem. Our model integrates multilingual BERT as the encoder, boosts the cross-lingual ability, and infuses the n-gram information into our word representation learning. Our experimental results show that our dataset is challenging and deserves further research.\\n\\nKeywords: Data visualization, Chinese Text-to-Vis, Dataset construction\\n\\n1. Introduction\\nData visualization (Qin et al., 2020; Wang et al., 2021; Allen et al., 2019; Waskom, 2021) has become increasingly popular since it provides insights into data of massive size. In the pipeline of data visualization, an inevitable and inherent component is the creation of the specifications, which is achieved through the declarative visualization languages (DVL), (e.g., Vega-Lite (Satyanarayan et al., 2016) and EChart (Li et al., 2018)). This DVL specifies what data is required and how the data is supposed to be visualized. It requires users to have expertise and knowledge of the data domain and also good programming skills of DVL, which is not quite practical, esp. for novices.\\n\\nMotivated by this, automatic DVL generation from natural language, or Text-to-Vis, is becoming an emerging topic since it could provide a much more user-friendly interface. Many research studies have been invested in this problem, such as (Cui et al., 2019; Gao et al., 2015; Luo et al., 2020; Narechania et al., 2020). Given a natural language question and a database, Text-to-Vis aims to automatically translate the question into the specification in some DVLs for data visualization. Despite the variety of studies on this topic, we observe that all existing datasets for Text-to-Vis are for English only, and no previous studies have been conducted on Chinese Text-to-Vis datasets and methodology. Chinese is one of the languages that enjoy the most users worldwide. The lack of Chinese datasets prevents using Text-to-Vis services among these users.\\n\\nThis work presents a Chinese Text-to-Vis dataset that imposes three challenges to the Text-to-Vis tasks. Firstly, the names of the attributes/columns in each table are typically represented in English, whereas the natural language questions are written...\"}"}
{"id": "lrec-2024-main-169", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This discrepancy requires the model to have cross-lingual ability. Secondly, the most basic units for denoting columns or cells can be Chinese characters, but the word segmentation can be erroneous. Third, the target chart that needs to be generated contains both Chinese and English linguistic elements. This requires the model to have multi-lingual generation capabilities, being able to produce outputs containing both Chinese and English words. This differs from existing parallel studies like Chinese Text-to-SQL (e.g., CSpider (Min et al., 2019)), where the target SQL query usually only contains English keywords. The cross-lingual nature of generating charts based on Chinese text introduces an additional complexity not found in prior English-only Text-to-SQL tasks. Figure 1 is an example of the Chinese Text-to-Vis task.\\n\\nWe present a novel neural model dedicated to this task. The model is designed to generate vivid and accurate visualizations directly from Chinese text descriptions. Specifically, the model contains the multilingual BERT (Kenton and Toutanova, 2019) as part of the encoder to boost the cross-lingual ability and also infuse n-gram information into the word representation learning process. In addition to introducing the first Chinese Text-to-Vis dataset, this work represents our initial effort to address the Chinese Text-to-Vis challenge. We demonstrate its capabilities on the proposed CNvBench dataset across various visualization types. The resource of this paper is available at https://github.com/yangecn/CNvBench.\\n\\nIn a nutshell, our contributions are summarized as follows.\\n\\n\u2022 We propose a Chinese Text-to-Vis dataset named CNvBench in this paper. To our knowledge, this is the first Chinese Text-to-Vis dataset. We detail our construction method in this paper and release our dataset to promote the development of this field.\\n\\n\u2022 We propose our model, the first attempt at this Chinese Text-to-Vis problem. It integrates the multilingual BERT and n-gram information to boost cross-lingual performance and word representation learning.\\n\\n\u2022 We conduct extensive experiments on the proposed dataset, and the results show that our proposed Chinese Text-to-Vis task is challenging and the proposed model could achieve relatively good performance.\\n\\nThe rest of this paper is structured as follows: we first introduce some closely related work in Section 2. Then, we discuss the details of our proposed dataset in Section 3, followed by the discussion of our proposed model in Section 4. The experimental setup and results are listed in Section 5. Finally, we conclude the work in Section 6.\\n\\n2. Related Work\\n\\nThis study is closely related to three fields, data visualization, Text-to-Vis methods, and Text-to-Vis datasets, as is briefly surveyed in the following.\\n\\n2.1. Data Visualization\\n\\nData visualization, which converts abstract data into concrete, graphical representations, is naturally well-suited for providing an overview of large amounts of data. Data visualization can highlight patterns, trends, and relationships in the data that may not be immediately apparent from looking at raw data. To help data analysts gain more intuitive insights from their data, researchers in this area have done a lot of work to make it easier to convert data into visualizations. For example, Data-Driven Documents (D3) (Bostock et al., 2011) is a unique approach to creating visualizations for the web that focuses on transparency and direct manipulation of the underlying data. Vega-lite (Satyanarayan et al., 2016) is a high-level language for creating interactive graphics and visualizations. It is designed to be easy to use and understand, even for users without previous experience in data visualization. VizQL (Hanrahan, 2006) is a domain-specific language for data analysis and visualization. It supports an extensive range of visual expressions, allowing for easy customization and control over visualizations.\\n\\n2.2. Text-to-Vis Methods\\n\\nText-to-Vis focuses on using Natural Language Processing (NLP) techniques to automatically generate visualizations from text data, this technique requires both natural language understanding for machine comprehension of natural language questions and translation algorithms for generating target visualizations using visualization language. DeepEye (Luo et al., 2018) is such a rule-based method that enables users to express their query intent using non-specific or ambiguous statements. Then the natural language input by the user is converted into an internal visualization language to generate potential visualizations. Recently, some Text-to-Vis methods based on state-of-the-art NLP techniques...\"}"}
{"id": "lrec-2024-main-169", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The overall structure of the proposed model, we use a cross-lingual pre-trained encoder to solve the language mismatch problem between natural language questions and database schema, and also integrate Chinese n-grams in the model, making it better able to encode Chinese semantics.\\n\\nhave been proposed. NcNet (Luo et al., 2021b) is an end-to-end solution that employs a Transformer-based model to translate natural language questions to visualization. The authors proposed a novel and concise visualization grammar that enables Text-to-Vis to be performed in a machine translation way. Different from the end-to-end models, RGVisNet (Song et al., 2022) resolves the task in two phases: retrieval and revision. The authors first construct a Data Visualization (DV) codebase in advance. When a new natural language question comes, the model retrieves the codebase to find the most relevant DV query candidate as a prototype, and then based on the prototype, the model revises to generate the most appropriate query.\\n\\n2.3. Text-to-Vis Datasets\\n\\nThe emergence of deep learning technology has greatly benefited the field of NLP. However, the biggest obstacle currently hindering the development of deep learning in Text-to-Vis technology is not the existence of corresponding NLP techniques, but the lack of massive data for training deep learning models. To alleviate this issue, Luo et al. (2021a) released a public Text-to-Vis benchmark named NvBench, which contains 25,750 NL-Vis pairs across 105 domains, making it possible to use learning-based methods to solve the Text-to-Vis problem. In addition, another recent study (Srinivasan et al., 2021) also released a curated dataset containing 893 natural language questions distributed across three datasets. However, the relatively small amount of data means that its significance is more in the field of human-computer interaction rather than constructing learning-based methods.\\n\\nAlthough some researchers have proposed datasets in this field, to the best of our knowledge, there is currently no Chinese Text-to-Vis dataset available. This absence hinders the development of Text-to-Vis research in the Chinese context.\\n\\n3. Dataset: CNvBench\\n\\nIn this section, we describe the construction and splitting of CNvBench, the first Chinese Text-to-Vis dataset.\\n\\n3.1. Dataset Construction\\n\\nWe manually translated the NvBench dataset (Luo et al., 2021a) into Chinese. It should be noted that, in NvBench, both the natural language questions, the visualizations, and the databases (including table names, column names, and the stored values) are represented in English, but we only translated the questions and the x and y axis title of the visualizations into Chinese, which is shown as 3. This approach is based on the fact that professionals often construct databases using English to represent the database schema, as it adheres to programming conventions and facilitates database maintenance. In addition, the construction of this dataset aims to explore the capability of models to comprehend the semantic structure of Chinese questions and transform them into corresponding visualization query language (VQL) queries. This objective remains detached from the specific data languages stored within the database. The NvBench dataset...\"}"}
{"id": "lrec-2024-main-169", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For those records from the products and each product's manufacturer, draw a bar chart about the distribution of name and code, and group by attribute founder, rank from low to high by the x axis.\\n\\nFigure 3: A comparison of the original data of NvBench and our translated data. We manually translate English natural language queries into Chinese, and, to better suit the Chinese application context, we also translate the names of the x and y axis within the visualization to Chinese. Includes 25,750 pairs of natural language questions and visualizations, with a total of 7,247 unique visualizations in four levels of hardness. Statistics for different types of visualization are shown as follows. We translated all English questions in NvBench, and we named the final Chinese dataset CNvBench. The translation work was completed by two NLP researchers and a computer science student. The questions were first translated by one annotator, then reviewed and revised by a second annotator. Finally, a third annotator compared the original and revised versions to ensure accuracy. This process was carried out for each question to ensure the highest level of accuracy and thoroughness. When translating the questions, the translators are asked to preserve the style and structure of the original sentence if a literal translation is possible. Otherwise, if the question is complex, the translators are asked to rephrase it based on the semantic meaning of the VQL query, which is an intermediate representation of natural language question and DVL, to produce a more natural Chinese translation.\\n\\n3.2. Dataset Split\\nTo properly assess the model's performance, it is important to ensure that the data used for training is not visible to the model during evaluation. As described in the Text-to-SQL task (Iacob et al., 2020), we believe that there are also three aspects to be considered when splitting our dataset since both of these tasks involve retrieving data from a database.\\n\\n| Vis Type     | Vis & query pairs |\\n|--------------|-------------------|\\n| Pie          | 520 1750          |\\n| Bar          | 5523 19407        |\\n| Stacked Bar  | 359 1172          |\\n| Scatter      | 266 1041          |\\n| Grouping Scatter | 127 547    |\\n| Line         | 380 1562          |\\n| Grouping Line | 72 271           |\\n| Total        | 7247 25750        |\\n\\nTable 1: Statistics of different types of visualizations in the dataset.\\n\\nIn the question-based split, the same VQLs are allowed to appear in different sets (e.g., training, development, or test), but the precondition is that the questions corresponding to these VQLs should not be the same. In other words, the question statements should not overlap between the different sets, this ensures that the model is not biased towards a specific question during evaluation and can generalize to new, unseen results. A query-based split method makes that identical VQLs do not appear in the same subset. Finally, in a database split method, all questions related to a particular database are required to appear in different subsets. This way of splitting aims to test how well the model performs when applied to new domains, rather than just those it has seen during training. In our experiments, we only use a question-based split to evaluate the performance of our proposed baseline model.\\n\\n4. Method\\nIn this section, we present our baseline model in response to the aforementioned cross-lingual Text-to-Vis challenges, which is inspired and inherited from the BRIDGE model (Lin et al., 2020) due to its simple yet efficient architecture. Our model contains a BERT-based question-schema encoder for cross-lingual encoding, followed by a sequential pointer-generator to generate the corresponding VQL, which will be executed to obtain the visualization of the data. The overall structure of our model is shown in Figure 2.\"}"}
{"id": "lrec-2024-main-169", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The schema is composed of a set of tables denoted as $T = \\\\{t_1, t_2, \\\\ldots, t_N\\\\}$ and a set of columns represented as $C = \\\\{c_{11}, c_{12}, \\\\ldots, c_{1|t_1|}, \\\\ldots, c_{N|t_N|}\\\\}$, where $t_i$ represents the $i$th table, and $c_{ij}$ denotes the $j$th column within the $i$th table.\\n\\n### 4.2. Injecting N-grams information for Chinese Encoding\\n\\nIn the Chinese Text-to-Vis task, it is possible that the WordPiece (Kenton and Toutanova, 2019) segmentation (which treats each Chinese character as a token and is unaware of the boundaries of Chinese words) could cause the encoder to overlook potential database schema mentioned in Chinese questions, preventing the model from establishing connections between them and leading to the generation of incorrect table or column names during the decoding phase.\\n\\nTo address or alleviate this issue, we adopted a method akin to the ZEN model (Diao et al., 2020). We extracted $n$-grams from the Chinese question and utilized an external encoder to encode these $n$-grams. Subsequently, we injected the representations of the $n$-grams into the original cross-lingual question-schema encoder. Specifically, for encoding the input $n$-grams, we employed a multi-layer Transformer (Vaswani et al., 2017a) as the $n$-gram encoder. The $n$-gram encoder processed the embedding vectors of the $n$-grams to produce their representations. These representations of each character and its associated $n$-grams were then combined to form an enhanced representation, which was further passed to the subsequent layer of the original encoder. This process was iterated layer-by-layer in conjunction with the original encoder.\\n\\nWe adopted a BERT-style input structure for structuring natural language questions and their corresponding schema. To represent each table name and its associated column names, we utilized special tokens, denoted as $[T]$ and $[C]$ respectively.\\n\\n$X = [CLS], Q, [SEP], [T], t_1, [C], c_{11}, \\\\ldots, c_{1|t_1|}, [T], t_i, [C], c_{i1}, \\\\ldots, c_{i|t_i|}, [SEP]$. We input $X$, along with the $n$-gram and $n$-gram position matrix corresponding to the natural language question $Q$, into both the multi-lingual Transformer and the $n$-gram encoder separately. With this encoding approach, we can establish a mapping between the Chinese natural language question and the English schema, while also enhancing the representation of the Chinese text by leveraging $n$-grams. For more details, please refer to ZEN (Diao et al., 2020).\\n\\n### 4.3. LSTM-based Pointer-Generator Decoder\\n\\nTo generate the final VQL statements, we use an attention-based (Vaswani et al., 2017b) LSTM with pointer-generator (See et al., 2017) as the decoder. During the generation phase, the decoder has the ability to selectively incorporate specific parts of the input sequence into the output by \\\"pointing\\\" to them. The decoder is initialized using the hidden state from the encoder. Then at each time step, the decoder has two options: one is generating a VQL keyword from the vocabulary $V$; the other is using the pointer network to copy a component from the schema $S$ or to copy a token from the natural language question $Q$. These options allow the decoder to create a VQL query while also incorporating relevant information from the schema.\\n\\nTo generate the VQL, at each decoding step $t$, the decoder calculates the multi-head attention as described in Vaswani et al. (2017b):\\n\\n$$e(h)_{ti} = s_{ti}W_u(h_iW_v)^\\\\top p_{n/H}(1)$$\\n\\n$$\\\\alpha(h)_{ti} = \\\\text{softmax}_{in}e(h)_{ti}o$$\\n\\n$$z(h)t = \\\\sum_{i=1}^{L} \\\\alpha(h)_{ti}(h_iW_v)(3)$$\\n\\n$$z_t = h_z(1)t; z_t(2)t; \\\\ldots; z_t(H)t(4)$$\\n\\nwhere $h$ represents the head number and $H$ represents the total number of attention heads. $L$ is the sum of the number of tokens in question $Q$ and the number of components in Schema $S$. $h_i$ stands for the $i$th vector of the encoder representation $h \\\\in \\\\mathbb{R}^{L \\\\times n}$.\\n\\nTo decide whether to generate from the vocabulary or copy the token, the model defines the probabilities in the following form:\\n\\n$$p_{t \\\\text{gen}} = \\\\text{sigmoid}(s_{tW_s\\\\text{gen}} + z_{tW_z\\\\text{gen}} + b_{\\\\text{gen}})(5)$$\\n\\n$$p_{t \\\\text{copy}} = 1 - p_{t \\\\text{gen}}(6)$$\\n\\nwhere $p_{t \\\\text{gen}}$ represents the probability of generating from the vocabulary, while $p_{t \\\\text{copy}}$ represents the probability of copying from $Q$ or $S$.\\n\\n### 5. Experiments\\n\\n#### 5.1. Experimental Setup\\n\\nWe conducted several quantitative experiments to evaluate our method. In addition to the approach proposed in this paper, we also conducted experiments under a variety of settings, mainly focusing\"}"}
{"id": "lrec-2024-main-169", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the experiment, we test the model presented in Section 4, denoted by CT2V\\\\textsubscript{MN} (Cross-lingual Text-to-Vis), which integrates both the multilingual BERT and our proposed \\\\textit{n}-gram injection method in the encoder. We also test its variant CT2V\\\\textsubscript{M} which only utilizes multilingual BERT but does not use the \\\\textit{n}-gram encoder.\\n\\nTo assess the effectiveness of our proposed joint-encoder method, we also test our model with an LSTM as the encoder instead. It adopts the Tencent multilingual embeddings\\\\textsuperscript{2} as the pre-trained word embedding. We use two different word segmentation tools, Jieba\\\\textsuperscript{3} and HanNLP\\\\textsuperscript{4} to investigate the effect of Chinese word segmentation methods on the final results. Correspondingly, the models utilizing Jieba and HanNLP for word segmentation are named LSTM\\\\textsubscript{J} and LSTM\\\\textsubscript{H}, respectively.\\n\\n### 5.2. Evaluation Metrics\\n\\nWe evaluate our proposed model with the metrics described in Luo et al. 2021a. We employ the tree matching accuracy to assess the overall results. This evaluation method necessitates the transformation of VQL into an Abstract Syntax Tree (AST) and comparing it with the ground truth. The calculation method for tree accuracy is denoted as\\n\\\\[\\nAcc_{\\\\text{tree}} = \\\\frac{N_{\\\\text{tree}}}{N}\\n\\\\]\\nwhere \\\\(N_{\\\\text{tree}}\\\\) denotes the number of generated VQL ASTs identical to the ground truth, and \\\\(N\\\\) represents the total number of VQL ASTs in the test data.\\n\\nIn addition, we also utilize the vis component matching accuracy to offer a more comprehensive evaluation of the model's performance regarding each specific component of the visualization. This metric allows for a fine-grained analysis of the model's capabilities. This metric breaks down as follows: evaluating visualization types involves the \\\"Visualize\\\" part of the generated VQL query; assessing the x/y/z-axis component relates to the \\\"Select\\\" part of the query; and analyzing data includes aspects such as \\\"Group,\\\" \\\"Filter,\\\" \\\"Order,\\\" and \\\"Superlative\\\" components. The metric is defined as\\n\\\\[\\nAcc_{\\\\text{com}} = \\\\frac{N_{\\\\text{com}}}{N}\\n\\\\]\\nwhere \\\\(N_{\\\\text{com}}\\\\) denotes the number of components correctly matched with ground truth \\\\(N\\\\).\\n\\n### 5.3. Overall Results\\n\\nTable 2 shows the overall VQL tree matching accuracy of our baseline model in different hardness levels.\\n\\n| Method | Top1 | Top3 | Top5 | All |\\n|-------|------|------|------|------|\\n| LSTM  | 0.463 | 0.489 | 0.494 | 0.562 |\\n| LSTM\\\\textsubscript{J} | 0.452 | 0.503 | 0.529 | 0.553 |\\n| CT2V\\\\textsubscript{M} | 0.791 | 0.822 | 0.864 | 0.907 |\\n| CT2V\\\\textsubscript{MN} | 0.798 | 0.829 | 0.857 | 0.891 |\\n\\nTable 3 summarizes the model performance in different settings. Notably, the CT2V\\\\textsubscript{MN} method stands out as the most effective in capturing the semantic relationships between text and visualization. Its implementation yields the best performance with a Top1 accuracy (we use a beam search when decoding) of 0.798. Additionally, the performance of CT2V\\\\textsubscript{MN} surpasses CT2V\\\\textsubscript{M} at Top-1 and Top-3 accuracies. This observation signifies that the \\\\textit{n}-gram injection approach enables a more comprehensive understanding of the text's underlying semantics by taking into account not only individual Chinese characters but also the contextual relationships between consecutive sequences of characters.\\n\\nFurthermore, considering that Chinese sentences require segmentation prior to LSTM processing, we examined how the choice of segmentation tool for the questions impacts the performance. Our...\"}"}
{"id": "lrec-2024-main-169", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5.4. Results on Different Parts of the Visualization Component\\n\\nTable 4 reports the vis component matching accuracy on different encoders. Overall, the n-gram based encoder performs well in each visualization component prediction task. When predicting the visualization types, all three models obtained good performance, especially on the \\\"Bar\\\" charts. However, for stacked bar (SB) predictions, performance drops across all models due to stacked bars sometimes being implicitly referenced in the questions, which requires the model to infer from the sentence context. For predicting the axis part, only the LSTM encoder model obtains a poor result, possibly because there are often some corresponding aggregate functions occurring in the \\\"Select\\\" clause in the VQL, and the LSTM encoder is not able to well capture this type of information in the question. For data parts, both models utilizing pre-trained encoders achieved good results, with LSTM still performing the worst in this part.\\n\\n5.5. Error Analysis and Future Work\\n\\nTo identify the causes of errors, we conducted an error analysis on our test set of 2562 VQL examples. Utilizing CT2V, we identified several sources of errors from the 519 failed examples out of 2562, we discuss some typical cases below.\\n\\nFor about 36 examples, the model produces wrong predictions for the visualization part. For example, the model produced a wrong VQL for the question \u201c\u5bf9\u4e8e\u4ea7\u54c1\u548c\u6bcf\u4e2a\u4ea7\u54c1\u7684\u5236\u9020\u5546\u7684\u8bb0\u5f55\uff0c\u5236\u9020\u5546\u548c\u4ee3\u7801\u4e4b\u95f4\u7684\u5173\u7cfb\u662f\u4ec0\u4e48\uff0c\u5e76\u6309\u603b\u90e8\u8fdb\u884c\u5206\u7ec4\uff1f\u201d, the model incorrectly predicted the visualization type as \\\"bar\\\" when it is actually \\\"scatter\\\", this is due to the lack of explicit mention of the visualization type in the question. Additionally, due to the uneven distribution of visualization types among the total train samples, the model may perform well on the majority of types but poorly on others.\\n\\nFor about 314 examples, the model generates wrong column names or table names in the axis part. For example, considering the question \u201c\u5c55\u793a\u6765\u81ea\u4ea7\u54c1\u548c\u6bcf\u4e2a\u4ea7\u54c1\u7684\u5236\u9020\u5546\u7684\u8bb0\u5f55\uff0c\u8fd4\u56de\u4e00\u4e2a\u5173\u4e8e\u4ef7\u683c\u548c\u6536\u5165\u76f8\u5173\u7684\u6563\u70b9\u56fe\uff0c\u5e76\u6309\u603b\u5c5e\u884c\u8fdb\u884c\u5206\u7ec4\u3002\u201d, the model made a wrong prediction on column name \\\"Price\\\" as \\\"Manufacturer\\\", in addition to errors in predicting the column or table name, the model may also make wrong predictions on the number of column names or table names and insert extra ones into the VQL.\\n\\nErrors in the data part of the VQL mean that the model makes mistakes in predicting the keywords \\\"where\\\", \\\"group\\\", \\\"bin\\\", and \\\"order\\\" of the VQL. There were a total of 169 samples with errors in this part. For the question \u201c\u4f7f\u7528\u67f1\u72b6\u56fe\u5c55\u793a\u6bcf\u5929\u6700\u9ad8\u6e29\u5ea6\u5927\u4e8e\u6216\u7b49\u4e8e80\u5ea6\u7684\u65e5\u671f\u6709\u591a\u5c11\uff0c\u5e76\u6309Y\u8f74\u4ece\u9ad8\u5230\u4f4e\u6392\u5e8f\u3002\u201d, the model made the mistake of predicting \\\"bin by weekday\\\" as \\\"bin by month\\\".\\n\\nAfter a comprehensive analysis of the errors, we believe that improving the model\u2019s ability to understand and interpret the nuances of Chinese natural language questions is crucial. This includes addressing cases where the model makes incorrect predictions due to the absence of explicit information in the question. Future work should focus on enhancing the natural language understanding component, which could involve more advanced language models, fine-tuning, and domain-specific training.\\n\\n6. Conclusion\\n\\nWe construct the first large-scale Chinese Text-to-Vis dataset. We also present a strong baseline model and conduct extensive experiments in different configurations. We find that Chinese semantic parsing and cross-lingual question-schema linking are important factors affecting the experimental results. We hope that our dataset can play an active role in addressing Chinese Text-to-Vis with a data-driven approach.\"}"}
{"id": "lrec-2024-main-169", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Ethical Considerations\\n\\nWe have considered the potential ethical issues when conducting this study. The purpose of the translation of the dataset into Chinese is to advance research and development in the field of converting Chinese text into VQL. Our primary objective is to facilitate and promote research, analysis, and innovation in this field.\\n\\nThe original dataset (Luo et al., 2021a) is from publicly available sources on the internet. Our data collection process is in strict compliance with all relevant laws, regulations, and ethical guidelines governing data usage, translation, and dissemination. We have ensured that we are not infringing upon any copyrights, privacy rights, or intellectual property rights during the dataset's translation process. This commitment to legal and ethical compliance underscores the legitimacy and responsible handling of the data throughout its translation and subsequent usage.\\n\\n9. Bibliographical References\\n\\nMicah Allen, Davide Poggiali, Kirstie Whitaker, Tom Rhys Marshall, and Rogier A Kievit. 2019. Raincloud plots: a multi-platform tool for robust data visualization. Wellcome open research, 4.\\n\\nMichael Bostock, Vadim Ogievetsky, and Jeffrey Heer. 2011. D\u00b3 data-driven documents. IEEE transactions on visualization and computer graphics, 17(12):2301\u20132309.\\n\\nBSI. 1973a. Natural Fibre Twines, 3rd edition. British Standards Institution, London. BS 2570.\\n\\nBSI. 1973b. Natural fibre twines. BS 2570, British Standards Institution, London. 3rd. edn.\\n\\nA. Castor and L. E. Pollux. 1992. The use of user modelling to guide inference and learning. Applied Intelligence, 2(1):37\u201353.\\n\\nJ.L.Chercheur. 1994. Case-Based Reasoning, 2nd edition. Morgan Kaufman Publishers, San Mateo, CA.\\n\\nN. Chomsky. 1973. Conditions on transformations. In A festschrift for Morris Halle, New York. Holt, Rinehart & Winston.\\n\\nWeiwei Cui, Xiaoyu Zhang, Yun Wang, He Huang, Bei Chen, Lei Fang, Haidong Zhang, Jian-Guan Lou, and Dongmei Zhang. 2019. Text-to-viz: Automatic generation of infographics from proportion-related natural language statements. IEEE transactions on visualization and computer graphics, 26(1):906\u2013916.\\n\\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and Yonggang Wang. 2020. Zen: Pre-training chinese text encoder enhanced by n-gram representations. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4729\u20134740.\\n\\nVictor Dibia and \u00c7a\u011fatay Demiralp. 2019. Data2vis: Automatic generation of data visualizations using sequence-to-sequence recurrent neural networks. IEEE computer graphics and applications, 39(5):33\u201346.\\n\\nUmberto Eco. 1990. The Limits of Interpretation. Indian University Press.\\n\\nTong Gao, Mira Dontcheva, Eytan Adar, Zhicheng Liu, and Karrie G Karahalios. 2015. Datatone: Managing ambiguity in natural language interfaces for data visualization. In Proceedings of the 28th annual acm symposium on user interface software & technology, pages 489\u2013500.\\n\\nPat Hanrahan. 2006. Vizql: a language for query, analysis and visualization. In Proceedings of the 2006 ACM SIGMOD international conference on Management of data, pages 721\u2013721.\\n\\nPaul Gerhard Hoel. 1971a. Elementary Statistics, 3rd edition. Wiley series in probability and mathematical statistics. Wiley, New York, Chichester. ISBN 0 471 40300.\\n\\nPaul Gerhard Hoel. 1971b. Elementary Statistics, 3rd edition, Wiley series in probability and mathematical statistics, pages 19\u201333. Wiley, New York, Chichester. ISBN 0 471 40300.\\n\\nRadu Cristian Alexandru Iacob, Florin Brad, Elena-Simona Apostol, Ciprian-Octavian Truic\u0103, Ionel Alexandru Hosu, and Traian Rebedea. 2020. Neural approaches for natural language interfaces to databases: A survey. In Proceedings of the 28th International Conference on Computational Linguistics, pages 381\u2013395.\\n\\nOtto Jespersen. 1922. Language: Its Nature, Development, and Origin. Allen and Unwin.\"}"}
{"id": "lrec-2024-main-169", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL-HLT, pages 4171\u20134186.\\n\\nDeqing Li, Honghui Mei, Yi Shen, Shuang Su, Wenli Zhang, Junting Wang, Ming Zu, and Wei Chen. 2018. Echarts: a declarative framework for rapid construction of web-based visualization. Visual Informatics, 2(2):136\u2013146.\\n\\nXi Victoria Lin, Richard Socher, and Caiming Xiong. 2020. Bridging textual and tabular data for cross-domain text-to-sql semantic parsing. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 4870\u20134888.\\n\\nYuyu Luo, Xuedi Qin, Chengliang Chai, Nan Tang, Guoliang Li, and Wenbo Li. 2020. Steerable self-driving data visualization. IEEE Transactions on Knowledge and Data Engineering, 34(1):475\u2013490.\\n\\nYuyu Luo, Xuedi Qin, Nan Tang, and Guoliang Li. 2018. Deepeye: Towards automatic data visualization. In 2018 IEEE 34th international conference on data engineering (ICDE), pages 101\u2013112. IEEE.\\n\\nYuyu Luo, Nan Tang, Guoliang Li, Chengliang Chai, Wenbo Li, and Xuedi Qin. 2021a. Synthesizing natural language to visualization (nl2vis) benchmarks from nl2sql benchmarks. In Proceedings of the 2021 International Conference on Management of Data, pages 1235\u20131247.\\n\\nYuyu Luo, Nan Tang, Guoliang Li, Jiawei Tang, Chengliang Chai, and Xuedi Qin. 2021b. Natural language to visualization by neural machine translation. IEEE Transactions on Visualization and Computer Graphics, 28(1):217\u2013226.\\n\\nQingkai Min, Yuefeng Shi, and Yue Zhang. 2019. A pilot study for Chinese SQL semantic parsing. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 3652\u20133658.\\n\\nDominik Moritz, Chenglong Wang, Greg L Nelson, Halden Lin, Adam M Smith, Bill Howe, and Jeffrey Heer. 2018. Formalizing visualization design knowledge as constraints: Actionable and extensible models in draco. IEEE transactions on visualization and computer graphics, 25(1):438\u2013448.\\n\\nArpit Narechania, Arjun Srinivasan, and John Stasko. 2020. Nl4dv: A toolkit for generating analytic specifications for data visualization from natural language queries. IEEE Transactions on Visualization and Computer Graphics, 27(2):369\u2013379.\\n\\nXuedi Qin, Yuyu Luo, Nan Tang, and Guoliang Li. 2020. Making data visualization more efficient and effective: a survey. The VLDB Journal, 29(1):93\u2013117.\\n\\nIgnacio Rocco, Mircea Cimpoi, Relja Arandjelovi\u0107, Akihiko Torii, Tomas Pajdla, and Josef Sivic. 2020. Ncnet: Neighborhood consensus networks for estimating image correspondences. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(2):1020\u20131034.\\n\\nArvind Satyanarayan, Dominik Moritz, Kanit Wongsuphasawat, and Jeffrey Heer. 2016. Vega-lite: A grammar of interactive graphics. IEEE transactions on visualization and computer graphics, 23(1):341\u2013350.\\n\\nAbigail See, Peter J Liu, and Christopher D Manning. 2017. Get to the point: Summarization with pointer-generator networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1073\u20131083.\\n\\nCharles Joseph Singer, E. J. Holmyard, and A. R. Hall, editors. 1954\u201358. A history of technology. Oxford University Press, London. 5 vol.\\n\\nYuanfeng Song, Xuefang Zhao, Raymond Chi-Wing Wong, and Di Jiang. 2022. Rgvisnet: A hybrid retrieval-generation neural framework towards automatic data visualization generation. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 1646\u20131655.\\n\\nArjun Srinivasan, Nikhila Nyapathy, Bongshin Lee, Steven M Drucker, and John Stasko. 2021. Collecting and characterizing natural language utterances for specifying data visualizations. In Proceedings of the 2021 CHI Conference on Human Factors in Computing Systems, pages 1\u201310.\\n\\nJannik Str\u00f6tgen and Michael Gertz. 2012. Temporal tagging on different domains: Challenges, strategies, and gold standards. In Proceedings of the Eight International Conference on Language Resources and Evaluation (LREC\u201912), pages 3746\u20133753, Istanbul, Turkey. European Language Resource Association (ELRA).\\n\\nS. Superman, B. Batman, C. Catwoman, and S. Spiderman. 2000. Superheroes experiences with books, 20th edition. The Phantom Editors Associates, Gotham City.\"}"}
{"id": "lrec-2024-main-169", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017a. Attention is all you need. In Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017b. Attention is all you need. Advances in neural information processing systems, 30.\\n\\nQianwen Wang, Zhutian Chen, Yong Wang, and Huamin Qu. 2021. A survey on ml4vis: Applying machine learning advances to data visualization. IEEE Transactions on Visualization and Computer Graphics.\\n\\nMichael L Waskom. 2021. Seaborn: statistical data visualization. Journal of Open Source Software, 6(60):3021.\"}"}
