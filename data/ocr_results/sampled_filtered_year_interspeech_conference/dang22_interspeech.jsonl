{"id": "dang22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch\\n\\nTing Dang, Thomas Quinnell, Cecilia Mascolo\\n\\n1 Department of Computer Science and Technology, University of Cambridge, UK\\n\\ntd464@cam.ac.uk, tq215@cam.ac.uk, cm542@cam.ac.uk\\n\\nAbstract\\nWhile there has been recent success in audio-based COVID-19 detection, challenges still exist in developing more reliable and generalised models due to the limited amount of high quality labelled audio recordings. With a substantial amount of unlabelled audio recordings available, exploring semi-supervised learning (SSL) may benefit COVID-19 detection by incorporating this extra data. In this paper, we propose a SSL framework which adjusted FixMatch, one of the most advanced SSL approaches, to audio signals and explored its effectiveness in COVID-19 detection. The proposed framework is validated with a crowd-sourced audio database collected from our app, and showed superior performance over supervised models with a maximum of 7.2% relative improvement. Furthermore, we demonstrated that the proposed framework significantly benefits model development using imbalanced datasets, which is a common challenge in clinical data. It can also improve model generalisation. This potentially paves a new pathway of utilising unlabelled data effectively to build more accurate and reliable COVID-19 detection tools.\\n\\n1. Introduction\\nThe outbreak of COVID-19 in 2020 has caused considerable socio-economic impact and threatened human life. Policy responses, vaccine development, and effective test tools have greatly reduced the spread and brought the pandemic under control. While the most commonly used test tools for COVID-19 detection such as polymerase chain reaction (PCR) tests [1, 2] and lateral flow device antigen (LFD) tests [3] are effective, digital technologies that employ machine learning using different biomarkers also demonstrated great potential for scalable, flexible and fast detection [4, 5, 6, 7].\\n\\nExtensive attention has been paid to using audio biomarkers for COVID-19 detection, such as cough and speech, due to its numerous advantages (e.g. flexibility in data collection and convenience in a home monitoring context). A variety of studies have demonstrated its potential in detecting COVID-19 infections using deep learning techniques [4, 5, 8, 9, 10]. However, most of the work is validated in relative small datasets [4, 8, 10], which may struggle to generalise and cannot be employed for unseen data. The extensive amount of annotated audio recordings required for reliable data analysis and model development is generally infeasible, as it requires experts to label the data with a huge labor force and may also get delayed by prioritising system development over data gathering. As it is easy to collect unlabelled audio recordings at a large scale, exploring semi-supervised learning (SSL) for COVID-19 detection is of great interest, combining the unlabelled audio recordings in conjunction with a small amount of labelled data to improve the model performance.\\n\\nA variety of SSL schemes have been investigated across a variety of different tasks, while the most commonly adopted algorithms include pseudo-labelling [11], mean-teacher [12], MixMatch (MM) [13] and its variant ReMixMatch (RMM) [14]. However, they are either highly dependent on the reliability of the supervised model, or suffer from high computational cost. FixMatch (FMM) was recently proposed for image recognition tasks, and showed superior performance while significantly simplifying existing SSL methods [15]. However, it has been mainly investigated in the image domain, and not been well explored in audio-related tasks.\\n\\nIn this paper, we proposed a SSL framework which explores the potential of semi-supervised learning for audio based COVID-19 detection tasks, with FixMatch adjusted to audio signals. We compared the proposed approach with supervised and other SSL approaches, and showed a 6.2% relative improvement in terms of ROC-AUC over the supervised model. Furthermore, we demonstrated how our approach can benefit COVID-19 detection in different sub-tasks, e.g. distinguishing symptomatic/asymptomatic positive users from symptomatic/asymptomatic negative users. The results showed great advantages of the proposed approach in dealing with imbalanced datasets, with a maximum relative improvement of 7.2% further validating its potential in developing more accurate and generalised detection tools.\\n\\n2. Related work\\nExisting studies have shown the effectiveness of audio signals for COVID-19 detection [4, 5, 8, 9, 10, 16]. Coughs are first explored using deep learning techniques [9]. One of the studies investigated the dynamics of the glottal flow waveform during speech production (e.g. phonemes) to identify COVID-19, given the evidence that infection affects the respiratory system which in turn affects the speech [10]. Further, different sound types including cough, breathing and speech were combined to improve the detection performance [4, 5, 8]. Various machine learning techniques have also been validated, ranging from traditional Support Vector Machines [8] to more advanced deep learning approaches such as pre-trained VGGish [5] and ResNet [4] models. However, most of these models were developed using a small dataset ranging from 19 participants [10] to 355 participants [4], making it hard to generalise to participants unseen by the model. Furthermore, asymptomatic positive patients may not volunteer to get tested, thus, the dataset used for model development may lack these samples. This makes it hard or even impossible for the model to recognize asymptomatic patients. However, their audio recordings can be available (no associated test label) within a large amount of unlabelled audio...\"}"}
{"id": "dang22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semi-supervised learning (SSL), combining unlabelled data with labelled data, has attracted tremendous attention. Pseudo-labelling is one of the most widely adopted techniques [11], and serves as part of the pipeline for many advanced SSL algorithms. Recent SSL approaches which employ consistency regularization on unlabelled data show improved results [12, 13, 14, 15, 17, 18]. This minimises discrepancies between predictions for weakly and strongly deformed unlabelled samples, forcing the model to be versatile when faced with outliers and benefiting model development.\\n\\nFixMatch (FMM), one such algorithm that simply combines consistency regularisation and pseudo-labelling, demonstrated superior performance.\\n\\n3. Methods\\n3.1. FixMatch for COVID-19 detection\\nAn overview of the model pipeline using FMM for COVID-19 detection is shown in Figure 1. Labelled samples are first used to develop the supervised model, which is then adopted to gather the predictions for the weakly augmented unlabelled samples. Those with the predicted probability above a threshold for each class are selected as the confident samples. Their predictions are served as the artificial labels for the corresponding strongly augmented samples, which are combined with the labelled dataset to further optimise the model.\\n\\n3.1.1. Model pipeline\\nThe supervised model $f$ is developed and optimized using cross-entropy loss $L$:\\n\\n$$l_1 = \\\\frac{1}{N} \\\\sum_{n=1}^{N} L(\\\\hat{y}_{ln}, y_{ln})$$\\n\\nwhere $\\\\hat{y}_{ln}$ and $y_{ln}$ represent the prediction and test label for the $n$th labelled sample. The model is also applied to weakly augmented unlabelled spectrograms to obtain pseudo-labels $\\\\hat{y}_{um}$ as:\\n\\n$$\\\\hat{y}_{um} = f(\\\\varphi_w(x_{um}))$$\\n\\nwhere $\\\\varphi_w(\\\\cdot)$ represents the weak augmentation, and $\\\\hat{y}_{um}$ represents the predicted probabilities for the $m$th unlabelled sample.\\n\\nTo discard potentially incorrect pseudo-labels, only the unlabelled samples with the predicted probability $\\\\hat{y}_{um}$ of one class larger than a threshold $\\\\tau$ are selected. The predictions of these selected samples are converted to a one-hot pseudo label $y_{um}$ by binarising $\\\\hat{y}_{um}$ using the larger probability (e.g. argmax). They are then used as the artificial labels (i.e. assumed positive or negative test results in our task) for the corresponding strongly augmented spectrograms.\\n\\nThe loss function for the selected unlabelled dataset is estimated as:\\n\\n$$l_2 = \\\\frac{1}{M_1} \\\\sum_{m_1=1}^{N} L(f(\\\\varphi_s(x_{um})), \\\\hat{y}_{um1})$$\\n\\nwhere $\\\\varphi_s(\\\\cdot)$ represents the strong augmentation and $M_1$ is the number of selected unlabelled audio samples after weak augmentation. The final loss computed over both the labelled and unlabelled dataset is:\\n\\n$$l = l_1 + \\\\alpha l_2$$\\n\\nwhere $\\\\alpha$ controls the relative weight of the loss for the unlabelled data.\\n\\n3.1.2. Data Augmentation\\nSpecAugment is used as the augmentation method, which shows success in automatic speech recognition [19] and acoustic scene classification [20]. Time masking and frequency masking are applied to the spectrogram. For each spectrogram of size $T \\\\times F$, time masking is first applied to a range of consecutive time frames $[t_1, t_1 + \\\\Delta t]$ by replacing these elements with 0, where $t_1$ and $\\\\Delta t$ are randomly selected from a uniform distribution to introduce randomness. Similarly, frequency masking is applied along the frequency dimension. To produce the weakly and strongly augmented spectrograms, we mask an equal or larger number of random bins in the strongly augmented spectrograms than in the weakly augmented ones.\\n\\n3.1.3. Training strategies and model structure\\nTwo different training strategies are proposed, referred to as static and dynamic training. Static training selects the unlabelled samples using the supervised model $f$ once, and includes these samples in the training data to further optimize the model. The reliability of the selected unlabelled samples are highly dependent on the accuracy of the supervised model, and the errors introduced in the selected unlabelled samples may propagate and disrupt the model development.\"}"}
{"id": "dang22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Model structure. Three different modalities are used, and VGGish is used for feature extraction. These features from different modalities are concatenated and processed by fully connected layers for binary classification.\\n\\nThe dynamic training scheme instead selects the unlabelled samples dynamically at each iteration and combines them with the labelled data. Specifically, it iterates through samples, to select the confident unlabelled samples and combine them with labelled samples. For each iteration, the selected samples will be different as the model's parameters are optimised. Through experiments, we have observed more unlabelled samples are included as the model's confidence improves over time.\\n\\nThe model structure is shown in Figure 2. Three different audio modalities are used: cough, breathing and speech. A pre-trained VGGish network [21], optimized for acoustic event detection, is used as the feature extractor. Two fully-connected layers are employed as the classifier for COVID-19 detection.\\n\\n3.2. Tasks\\n\\nAccording to participants' clinical symptoms, a series of binary classification tasks are explored, to provide more insights into how FMM aids in detecting different subgroups of patients.\\n\\n- Task 1: Distinguish positive participants from negative (healthy) participants, which is the general case and referred as 'Pos-Neg'.\\n- Task 2: Distinguish symptomatic positive participants who reported at least one symptom from asymptomatic negative participants. This is expected to be a simple task as the audio sounds may show clear difference between the two subgroups. This task is referred as 'sPos-aNeg'.\\n- Task 3: Distinguish symptomatic positive participants from symptomatic negative participants, refereed as 'sPos-sNeg'.\\n- Task 4: Distinguish asymptomatic positive participants from asymptomatic negative participants, refereed as 'aPos-aNeg'.\\n\\n4. Experimental setup\\n\\n4.1. Data\\n\\nWe have collected a crowdsourced audio data set for COVID-19 detection via a mobile app [21], which collects three types of audio recordings for each participant (cough, breathing and speech), along with their demographics, medical history, symptoms, and COVID-19 test results. More details can be found in [22].\\n\\nWe selected a subset of 1000 participants with 1486 labelled samples (734 positive and 752 negative samples). We divide the data into training, validation and test partitions with 70%, 10% and 20% respectively, with relatively balanced gender, age and symptoms to avoid any bias. The test participants are not included in the training and validation set. Further, we randomly selected the unlabelled samples, from 2381 participants contributing 2778 unlabelled samples in total.\\n\\n4.2. Settings\\n\\nAll the audio recordings were automatically checked using YAMNet [21] to remove the unqualified ones (e.g. noisy background, etc.). Each recording was then resampled to 16 kHz, converted to mono channel and normalised to make the maximum amplitude 1. Silent regions at the beginning and the end of the recordings were removed. We adopted the same model structure (Figure 2) as in [5] due to its comprehensive validations and strong performance using supervised learning for COVID-19 detection task, but the data and the resources used to train the model differ. Our model is trained using one GPU in the high-performance computing clusters.\\n\\nTwo fully connected layers with 64 neurons were used as the classifier. VGGish is fine-tuned jointly with the classifier, with the initial learning rate set as $5 \\\\times 10^{-5}$ for VGGish and $1 \\\\times 10^{-4}$ for the classifier. It is decayed by 2% for every 1000 training samples. The model is trained for 20 epochs with the Adam optimiser. The final 5 epochs include the validation data to further boost the model performance. We empirically chose 5% time and frequency masking for the weakly augmented spectrograms, and optimised within the range of $[5, 35]$ percent with a step size of 10 for the strong augmentation strength. The unlabelled loss weight $\\\\alpha$ was set to 0.33 as an approximation to the ratio of labelled and unlabelled data. The threshold $\\\\tau$ is empirically chosen as 0.95. The code was implemented using Tensorflow [23], and can be found here.\\n\\nThe model performance was evaluated using ROC-AUC, Sensitivity and Specificity. ROC-AUC shows the overall capability of the model in correctly classifying the positive and negative participants. Sensitivity illustrates the model capability in correctly identifying positive patients, while specificity shows that in correctly identifying healthy participants. A 95% Confidence Interval (CI) for the model performance is estimated using bootstrap [24].\"}"}
{"id": "dang22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: System performance for SL and FMM\\\\(d\\\\) for Tasks 2-4. Number of samples for each task is included in parenthesis (training/test).\\n\\nFMM\\\\(d\\\\) shows great advantages in balancing sensitivity and specificity.\\n\\n| Task | System | Accuracy (0.63-0.75) | ROC-AUC (0.74-0.86) | Sensitivity (0.55-0.65) | Specificity (0.69-0.76) |\\n|------|--------|----------------------|---------------------|------------------------|------------------------|\\n| T2: sPos (433/150)-aNeg (282/88) | SL     | 0.69         | 0.8                  | 0.55                   | 0.83                   |\\n|      | FMM\\\\(d\\\\) | 0.74     | 0.78              | 0.69                  | 0.78                  |\\n| T3: sPos (433/150)-sNeg (336/113) | SL     | 0.57         | 0.62                | 0.73                   | 0.41                   |\\n|      | FMM\\\\(d\\\\) | 0.58     | 0.63              | 0.57                  | 0.59                  |\\n| T4: aPos (82/30)-aNeg (282/88) | SL     | 0.55         | 0.66                | 0.27                   | 0.84                   |\\n|      | FMM\\\\(d\\\\) | 0.54     | 0.6                | 0.43                  | 0.72                  |\\n\\nFigure 4: Latent space visualisation for (a) SL train; (b) SL test; (c) FMM\\\\(d\\\\) train; (d) FMM\\\\(d\\\\) test. SL is overfitting while FMM\\\\(d\\\\) increases the error tolerance and improves generalisation.\\n\\n5. Results and discussion\\n\\n5.1. Comparison with supervised model\\n\\nThe comparison of the proposed system to the supervised learning (SL) model and pseudo-labelling (PL) approach for Task 1 (Pos-Neg) is shown in Table 1. It can be observed that pseudo-labelling with either static or dynamic training could not benefit COVID-19 detection, possibly due to the weakness in the supervised model which generates incorrect artificial labels. FMM\\\\(s\\\\) and FMM\\\\(d\\\\) show a 3.1\\\\% and 6.2\\\\% relative improvement of ROC-AUC over the supervised model respectively. In addition, FMM\\\\(d\\\\) leads to more balanced sensitivity and specificity, and a significant higher specificity with a 12.5\\\\% relative improvement, suggesting that FMM is able to select more reliable unlabelled samples, benefiting the task.\\n\\n5.2. Evaluation for different subtasks\\n\\nThe system performance for Tasks 2-4 (T2-T4) with SL and FMM\\\\(d\\\\) are shown in Table 2. ROC-AUC can be misleading as the number of samples in the positive and negative class for each subtask differ. We additionally reported the balanced accuracy, which is computed as the average of recall obtained on each class and mitigates this problem. SL easily skews to one of the classes for all three subtasks, either with a high sensitivity and low specificity, or vice versa. This is likely due to the imbalanced dataset (number in parenthesis). FMM\\\\(d\\\\) demonstrates superior performance for T2-T3 in terms of accuracy, showing great advantages in balancing sensitivity and specificity. This suggests that the proposed approach is able to select reliable but unfamiliar unlabelled samples to aid model development. For T4, though FMM\\\\(d\\\\) shows comparable performance in terms of accuracy, it still yields a decreased discrepancy between sensitivity and specificity. The overall unsatisfying performance for T4 could be the extreme scarce samples in the positive class.\\n\\n5.3. Size of labelled training data\\n\\nWe further evaluated the model performance using FMM\\\\(d\\\\) with different percentages of labelled data for Task 1 (a general task). As shown in Figure 3, FMM\\\\(d\\\\) shows superior or comparable performance with the SL model for different percentages of labelled data ranging within \\\\([100, 75, 50]\\\\). A higher relative improvement of 6.6\\\\% using 50\\\\% labelled data over a 6.2\\\\% improvement using 100\\\\% labelled data is observed. Further, FMM\\\\(d\\\\) achieves more balanced sensitivity and specificity over SL using 75\\\\% labelled data. This evidence suggests that FMM\\\\(d\\\\) shows greater advantages when less labelled data is available.\\n\\n5.4. Visualisation in latent space\\n\\nThe latent vectors from the last hidden layer were projected into a 2-dimensional space using t-SNE [25] for both the SL and FMM\\\\(d\\\\) for Task 1, as shown in Figure 4. SL yields a model which can cluster and separate positive and negative samples accurately for the training samples, but not for test samples, suggesting overfitting possibly due to the limited training dataset size. On the contrary, FMM\\\\(d\\\\) maps a few positive and negative samples in the wrong cluster, but still well separates positive and negative clusters. These few samples might not be wrongly clustered, as the test labels are self-reported which might be noisy and introduce a potential time lag (i.e. recovered but not tested and continuously reported positive test result). The positive and negative samples in the test set are clustered better using FMM\\\\(d\\\\), indicating a better generalisation capability.\\n\\n6. Conclusion\\n\\nA semi-supervised learning framework (SSL) using adjusted FixMatch is proposed for audio-based COVID-19 detection. It is validated in a crowdsourced dataset and the superior performance over supervised learning and commonly adopted pseudo-labelling demonstrates its effectiveness. The improved performance on different tasks further showed that the proposed approach benefits learning in imbalanced datasets, which is common in clinical data. We showed the potential of the proposed framework in developing a more accurate and generalised model incorporating the great source of unlabelled data. Future work includes investigating more advanced augmentation methods for audio signals, and improved fusion strategies of different modalities under SSL schemes.\\n\\n7. Acknowledgements\\n\\nThis work was supported by ERC Project 833296 (EAR). We thank everyone who volunteered their data.\"}"}
{"id": "dang22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] M. Cevik, K. Kuppalli, J. Kindrachuk, and M. Peiris, \u201cVirology, transmission, and pathogenesis of SARS-CoV-2,\u201d British Medical Journal, vol. 371, pp. 1\u20136, 2020.\\n\\n[2] C. B. Vogels, A. F. Brito, A. L. Wyllie, J. R. Fauver, I. M. Ott, C. C. Kalinich, M. E. Petrone, A. Casanovas-Massana, M. C. Muenker, A. J. Moore et al., \u201cAnalytical sensitivity and efficiency comparisons of SARS-CoV-2 RT\u2013qPCR primer\u2013probe sets,\u201d Nature Microbiology, vol. 5, no. 10, pp. 1299\u20131305, 2020.\\n\\n[3] I. Torjesen, \u201cCovid-19: How the uk is using lateral flow tests in the pandemic,\u201d BMJ, vol. 372, 2021.\\n\\n[4] H. Coppock, A. Gaskell, P. Tzirakis, A. Baird, L. Jones, and B. Schuller, \u201cEnd-to-end convolutional neural network enables covid-19 detection from breath and cough audio: a pilot study,\u201d BMJ innovations, vol. 7, no. 2, 2021.\\n\\n[5] J. Han, T. Xia, D. Spathis, E. Bondareva, C. Brown, J. Chauhan, T. Dang, A. Grammenos, A. Hasthanasombat, A. Floto et al., \u201cSounds of covid-19: exploring realistic performance of audio-based digital testing,\u201d arXiv preprint arXiv:2106.15523, 2021.\\n\\n[6] T. Mishra, M. Wang, A. A. Metwally, G. K. Bogu, A. W. Brooks, A. Bahmani, A. Alavi, A. Celli, E. Higgs, O. Dagan-Rosenfeld et al., \u201cPre-symptomatic detection of covid-19 from smartwatch data,\u201d Nature biomedical engineering, vol. 4, no. 12, pp. 1208\u20131220, 2020.\\n\\n[7] G. Quer, J. M. Radin, M. Gadaleta, K. Baca-Motes, L. Ariniello, E. Ramos, V. Kheterpal, E. J. Topol, and S. R. Steinhubl, \u201cWearable sensor data and self-reported symptoms for covid-19 detection,\u201d Nature Medicine, vol. 27, no. 1, pp. 73\u201377, 2021.\\n\\n[8] C. Brown, J. Chauhan, A. Grammenos, J. Han, A. Hasthanasombat, D. Spathis, T. Xia, P. Cicuta, and C. Mascolo, \u201cExploring automatic diagnosis of covid-19 from crowdsourced respiratory sound data,\u201d arXiv preprint arXiv:2006.05919, 2020.\\n\\n[9] J. Laguarta, F. Hueto, and B. Subirana, \u201cCovid-19 artificial intelligence diagnosis using only cough recordings,\u201d IEEE Open Journal of Engineering in Medicine and Biology, vol. 1, pp. 275\u2013281, 2020.\\n\\n[10] S. Deshmukh, M. Al Ismail, and R. Singh, \u201cInterpreting glottal flow dynamics for detecting covid-19 from voice,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 1055\u20131059.\\n\\n[11] D.-H. Lee et al., \u201cPseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,\u201d in Workshop on challenges in representation learning, ICML, vol. 3, no. 2, 2013, p. 896.\\n\\n[12] A. Tarvainen and H. Valpola, \u201cMean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results,\u201d Advances in neural information processing systems, vol. 30, 2017.\\n\\n[13] D. Berthelot, N. Carlini, I. Goodfellow, N. Papernot, A. Oliver, and C. A. Raffel, \u201cMixmatch: A holistic approach to semi-supervised learning,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.\\n\\n[14] D. Berthelot, N. Carlini, E. D. Cubuk, A. Kurakin, K. Sohn, H. Zhang, and C. Raffel, \u201cRemixmatch: Semi-supervised learning with distribution alignment and augmentation anchoring,\u201d arXiv preprint arXiv:1911.09785, 2019.\\n\\n[15] K. Sohn, D. Berthelot, N. Carlini, Z. Zhang, H. Zhang, C. A. Raffel, E. D. Cubuk, A. Kurakin, and C.-L. Li, \u201cFixmatch: Simplifying semi-supervised learning with consistency and confidence,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 596\u2013608, 2020.\\n\\n[16] M. Pahar, M. Klopper, R. Warren, and T. Niesler, \u201cCovid-19 cough classification using machine learning and global smartphone recordings,\u201d Computers in Biology and Medicine, vol. 135, p. 104572, 2021.\\n\\n[17] S. Calderon-Ramirez, R. Giri, S. Yang, A. Moemeni, M. Umana, D. Elizondo, J. Torrents-Barrena, and M. A. Molina-Cabello, \u201cDealing with scarce labelled data: Semi-supervised deep learning with mix match for covid-19 detection using chest x-ray images,\u201d in 2020 25th International Conference on Pattern Recognition (ICPR). IEEE, 2021, pp. 5294\u20135301.\\n\\n[18] V. Verma, K. Kawaguchi, A. Lamb, J. Kannala, Y. Bengio, and D. Lopez-Paz, \u201cInterpolation consistency training for semi-supervised learning,\u201d arXiv preprint arXiv:1903.03825, 2019.\\n\\n[19] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmentation method for automatic speech recognition,\u201d arXiv preprint arXiv:1904.08779, 2019.\\n\\n[20] H. Wang, Y. Zou, and W. Wang, \u201cSpecaugment++: A hidden space data augmentation method for acoustic scene classification,\u201d arXiv preprint arXiv:2103.16858, 2021.\\n\\n[21] S. Hershey, S. Chaudhuri, D. P. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold et al., \u201cCnn architectures for large-scale audio classification,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 131\u2013135.\\n\\n[22] T. Xia, D. Spathis, J. Ch, A. Grammenos, J. Han, A. Hasthanasombat, E. Bondareva, T. Dang, A. Floto, P. Cicuta et al., \u201cCovid-19 sounds: A large-scale audio dataset for digital respiratory screening,\u201d in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\\n\\n[23] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \u201cTensorflow: Large-scale machine learning on heterogeneous distributed systems,\u201d arXiv preprint arXiv:1603.04467, 2016.\\n\\n[24] T. J. DiCiccio and B. Efron, \u201cBootstrap confidence intervals,\u201d Statistical science, vol. 11, no. 3, pp. 189\u2013228, 1996.\\n\\n[25] L. Van der Maaten and G. Hinton, \u201cVisualizing data using t-sne.\u201d Journal of machine learning research, vol. 9, no. 11, 2008.\"}"}
