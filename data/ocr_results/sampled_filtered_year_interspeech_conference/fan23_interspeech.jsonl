{"id": "fan23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nCode-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the test man and test sge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction.\\n\\nIndex Terms: code-switching, speech recognition, language-specific, non-autoregressive, language change detection\\n\\n1. Introduction\\nThe primary objective of code-switching (CS) is to facilitate effective communication across diverse linguistic or technical domains. CS entails the practice of alternating between two or more languages in a single sentence. However, incorporating words or phrases from multiple languages can result in transcription errors and confusion, which makes code-switching speech recognition (CSSR) a more challenging task [1].\\n\\nThe CSSR task has been studied for decades. In the early days, most works are conducted on the hybrid framework [2, 3, 4, 5, 6, 7]. However, as the end-to-end (E2E) model become increasingly popular, researchers begin pursuing an end-to-end strategy to resolve the CSSR task. The attention-based E2E models are first applied to the CSSR task, and to improve speech recognition performance, language identification (LID) is used as an auxiliary task [8, 9]. Additionally, the language-aware encoder (LAE) structures and language-aware training (LAT) are applied to connectionist temporal classification (CTC) and neural transducer systems to disentangle language-specific information and generate frame-level language-aware representations during encoding [10, 11, 12, 13, 14, 15, 16]. For the decoder, language-related attention mechanisms [17], non-autoregressive structures [18, 19] and internal language model estimation (ILME) [20] based language models are all used to alleviate the confusion brought by the code-switching of different languages. To model both the monolingual and the cross-lingual sequential dependency, Lee et al. propose a bilingual attention language model (BALM) that simultaneously performs language modeling objectives with a quasi-translation objective [21]. Multi-encoder-decoder (MED) explores the language-related structure on both the encoder and decoder [22].\\n\\nIn general, the E2E ASR model consists of the encoder, decoder, and alignment mechanism. Most of the existing E2E CSSR models only focus on optimizing the encoder and decoder structure, while few works explore whether the alignment mechanism needs to be language-specific. On the other hand, most previous works use a mixture of Mandarin characters and English subwords as modeling units [8, 9, 10, 11, 12, 13, 17, 18, 22]. Mandarin character typically represents a single syllable in Mandarin Chinese [23], and their acoustic boundary is clear [24]. However, subwords are obtained without referring to any acoustic knowledge [25], and their acoustic boundary may be blurred [24]. To obtain good acoustic boundaries (alignment) for both Mandarin and English in the CSSR system, it is intuitive to conduct language-specific boundary learning.\\n\\nIn this paper, we employ the CIF-based model [24] as our backbone, which utilizes a weight estimator to predict acoustic boundaries for aligning the encoder and decoder. For the CSSR task, we introduce language-specific weight estimators (LSWE) to enable language-specific boundary learning for different languages. Additionally, a non-autoregressive decoder is utilized to assist in learning the language-specific boundaries. To further enhance the model's ability to detect intra-sentence language changes, we design a language change detection (LCD) module. It's worth noting that the non-autoregressive decoder and LCD module are eliminated in the inference stage, which means our model has almost no increase in the number of parameters compared to the vanilla CIF-based model. We evaluate the proposed method on two datasets: SEAME, a public conversational Mandarin-English corpus, and an in-house meeting code-switching dataset. On the SEAME benchmark, our method outperforms strong baselines and achieves a new state-of-the-art performance, obtaining an MER of 16.29% and 22.81% on the test man and test sge, respectively. On the in-house meeting CS dataset with 9000 hours of speech, our method shows a relative MER reduction of 7.9%, further validating its effectiveness in real-world scenarios. As far as we know, this work is the first to consider language-specific boundary learning in the CSSR task.\"}"}
{"id": "fan23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Schematic diagram of the proposed method. The left part illustrates the overall structure, and the right part provides details on utilized modules. Modules with the same color correspond to each other in the left and right parts.\\n\\nThe CIF mechanism can be ma, en, or mix. In the upper left corner, there is an example of generating monolingual labels from the target sequences. The system assigns to the language that corresponds to the last token. The modules with dotted borders are eliminated in the inference stage.\\n\\nIn the CIF mechanism, corresponding frames are located as acoustic boundaries. All frame-level encoded outputs between every two adjacent boundaries are then weighted and summed using the information weights to obtain a token-level acoustic embedding. The information weights corresponding to boundary frames are divided into two parts to ensure that the sum of information weight between two adjacent boundaries equals 1.\\n\\nIn summary, the CIF mechanism can convert the frame-level acoustic representation to the token level using the weight information predicted by the weight estimator. However, the CIF mechanism involves mathematical calculations and does not have trainable parameters. Therefore, the weight estimator is the only trainable component that ultimately determines the location of the acoustic boundaries. Thus, all our language-specific boundary learning is focused on the weight estimator.\\n\\n3. Proposed method\\n\\n3.1. Overview\\n\\nThis article builds upon the CIF model [24] and proposes improvements to enable language-specific boundary learning for the CSSR task. The details of the model architecture are presented in Figure 1. The model consists of six components: an encoder, language-specific weight estimators (LSWE), a CIF calculator, an autoregressive (AR) decoder, a non-autoregressive (NAR) decoder, and a language change detection (LCD) module. The input feature sequence $x$ is transformed into an encoded output $h$ by the encoder, which comprises conformer layers [26]. The encoded output $h$ is passed through two language-specific weight estimators, which predict the acoustic boundary and information contained in each frame. The predictions for Mandarin and English are denoted as $\\\\mathbf{\\\\alpha}_{ma}$ and $\\\\mathbf{\\\\alpha}_{en}$, respectively. The mixture weight $\\\\mathbf{\\\\alpha}_{mix}$ is the frame-level addition of $\\\\mathbf{\\\\alpha}_{ma}$ and $\\\\mathbf{\\\\alpha}_{en}$.\\n\\nThe CIF calculator utilizes the language-specific information weight $\\\\mathbf{\\\\alpha}_{ma}$, $\\\\mathbf{\\\\alpha}_{en}$, and $\\\\mathbf{\\\\alpha}_{mix}$ to convert the frame-level acoustic representation $h$ into token-level acoustic embeddings, namely $c_{ma}$, $c_{en}$, and $c_{mix}$ for Mandarin, English, and the mixture, respectively. The AR decoder takes in acoustic embedding $c_{mix}$ to generate target prediction $y_{mix}$. The NAR decoder takes in monolingual acoustic embeddings $c_{ma}$ and $c_{en}$ to predict the corresponding monolingual token sequence $y_{ma}$ and $y_{en}$. Lastly, the LCD module takes in $c_{mix}$ and the previously predicted token from the AR decoder to predict language change probability $l_{mix}$.\\n\\nDuring inference, the NAR decoder and LCD module are not utilized, resulting in almost identical parameter numbers between our method and the baseline model. Moreover, the calculation of monolingual acoustic embeddings $c_{ma}$ and $c_{en}$ is omitted. The primary role of the NAR decoder and LCD module is to assist in the learning of $\\\\mathbf{\\\\alpha}_{ma}$ and $\\\\mathbf{\\\\alpha}_{en}$, which are strongly correlated with the acoustic boundary.\\n\\n3.2. Language-specific weight estimator\\n\\nIn the vanilla CIF-based ASR model [24], the information weight predicted by the weight estimator can decide the acoustic boundary through the CIF calculation. To achieve language-specific boundary learning, we design two weight estimators for Mandarin and English, respectively. The $\\\\mathbf{\\\\alpha}_{ma}$ and $\\\\mathbf{\\\\alpha}_{en}$ represent the Mandarin and English information contained in each frame and determine the location of the acoustic boundary. The mixed information weight is fused from the two information weight as follow:\\n\\n$$\\\\mathbf{\\\\alpha}_{mix} = \\\\text{Dropout}(\\\\mathbf{\\\\alpha}_{ma}, p) + \\\\text{Dropout}(\\\\mathbf{\\\\alpha}_{en}, p)$$\\n\\n(1)\\n\\nwhere $p$ represents the dropout rate [27]. To prevent the model from relying too heavily on either language and encourages it to learn from both, we apply dropout to the information weights of both languages before adding them. Additionally, we modify the scaling operation used during training in the vanilla CIF-based model to be language-specific.\\n\\n$$\\\\mathbf{\\\\alpha}_{\\\\tau} = \\\\mathbf{\\\\alpha}_{\\\\tau}^{\\\\star} \\\\frac{U \\\\mathbf{\\\\alpha}_{\\\\tau}}{T}$$\\n\\n(2)\\n\\nwhere the $\\\\mathbf{\\\\alpha}_{\\\\tau}$ can be ma, en or mix.\\n\\nThe quantity loss function is utilized to train the language-specific weight estimator, enabling it to accurately predict the number of tokens in a sentence for the corresponding languages.\\n\\n$$L_{qua} = |U_{mix} \\\\sum_{t=1}^{T} \\\\mathbf{\\\\alpha}_{mix}| + 0.5 \\\\left( |U_{ma} \\\\sum_{t=1}^{T} \\\\mathbf{\\\\alpha}_{ma}| + |U_{en} \\\\sum_{t=1}^{T} \\\\mathbf{\\\\alpha}_{en}| \\\\right)$$\\n\\n(3)\"}"}
{"id": "fan23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the Uma, Uen, and Umix represent the token numbers of Mandarin, English, and mixture, respectively. T 0 represents the length of frame-level representation h.\\n\\n3.3. Auxiliary modules\\n\\nThe non-autoregressive decoder solely processes monolingual acoustic embeddings and utilizes a monolingual cross-entropy loss to assist in the learning of LSWE. The degradation of Lnar guides the LSWE to predict better acoustic boundaries for monolingual acoustic embeddings cma and cen.\\n\\n$L_{nar} = \\\\sum_{t=1}^{T_0} \\\\log p(y_{ma}^t) | y_{ma}^t < t, c_{ma}^t) + \\\\sum_{t=1}^{T_0} \\\\log p(y_{en}^t) | y_{en}^t < t, c_{en}^t)$\\n\\n(4)\\n\\nThe monolingual labels that supervise the NAR decoder are generated by splitting the target sequence on the fly. An example of the monolingual label generation process is depicted in the upper left corner of Figure 1. It should be noted that in contrast to previous works [10, 11], our monolingual labels do not include a placeholder for the other language.\\n\\nThe LCD module is employed to improve the model's capacity to detect language change within a sentence. This is achieved by incorporating a token-level binary cross-entropy loss.\\n\\n$L_{lcd} = \\\\sum_{t=1}^{T_0} \\\\left[ \\\\hat{l}_{mix}^t \\\\log p(l_{mix}^t | y_{mix}^t < t, c_{mix}^t) + (1 - \\\\hat{l}_{mix}^t) \\\\log (1 - p(l_{mix}^t | y_{mix}^t < t, c_{mix}^t)) \\\\right]$ (5)\\n\\nwhere $\\\\hat{l}_{mix}^t$ represents the ground truth label for language change detection.\\n\\n3.4. Loss function\\n\\nWe employ a joint training strategy to optimize all parameters, where the overall objective function is a weighted sum of all the losses mentioned above.\\n\\n$L = L_{ar} + L_{ctc} + L_{qua} + L_{nar} + L_{lcd}$\\n\\n(6)\\n\\nwhere $ctc$, $qua$, $nar$, and $lcd$ are tunable hyper-parameters.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\nWe perform experiments on two datasets: SEAME [28] and an in-house meeting code-switching dataset. SEAME comprises approximately 105 hours of spontaneous Mandarin-English intra-sentential code-switching speech from Singapore and Malaysia. Two evaluation subsets, biased towards Mandarin and Southeast Asian English, are denoted as test man and test sge, respectively. Due to space constraints, we refer readers to [20] for further details on SEAME. The in-house meeting code-switching dataset is collected from the recording of real meetings and manually transcribed. It comprises 3000 hours of monolingual English training data, 3000 hours of monolingual Mandarin training data, and 3000 hours of Mandarin-English code-switching training data. Two monolingual and one code-switching evaluation sets are utilized, and they are denoted as test ma, test n, and test cs, respectively. They comprise 8.9, 10.78, and 10.93 hours of speech, respectively.\\n\\n4.2. Experimental settings\\n\\nWe use 80-dimensional log-Mel filter-bank features, computed with a 25 ms window and shifted every 10 ms, for all experiments. A convolutional layer with 64 filters and 1/2 temporal downsampling is used as the front end of the encoder. The encoder comprises 15 conformer [26] layers with 4 attention heads, 256 attention dimensions, and 1024 feed-forward network (FFN) dimensions. There is a max-pooling layer for 1/2 temporal downsampling after the eighth layers. The Mandarin and English weight estimators have identical structures, consisting of three 1-dimensional convolutional layers and an FC layer. The kernel size of the convolutional layer is set to (3,1,3), and the filter number is 256. The FC layer has one output unit with sigmoid activation. The autoregressive decoder, non-autoregressive decoder, and LCD module have the same structure, consisting of 3 transformer [29] layers with 4 attention heads, 256 attention dimensions, and 1024 FFN dimensions. The only difference is in the input projection layers. The threshold used in the CIF mechanism is set to 1. The modeling units comprise 1686 English BPE subwords [25] and 2641 Mandarin characters. All the dropout rate is set to 0.1. The hyper-parameter $ctc$, $qua$, $nar$, and $lcd$ are set to 0.5, 0.01, 0.2, 0.1, respectively.\\n\\nIn the training stage, we employ the Adam optimizer [30]. The learning rate warms up for the first 1k iterations to a peak of $10^3$ and holds on for the next 40k iterations, and then linearly decays to $10^4$ for the last 30k iterations. The last 10 checkpoints are averaged to obtain the final model for evaluation. For decoding, we employ beam-search [31] with a beam size of 10.\\n\\n4.3. Metrics\\n\\nWe use mix error rate (MER) as the evaluation metric for speech recognition. For monolingual cases, we use character error rate (CER) for Mandarin and word error rate (WER) for English. To evaluate the quality of the token acoustic boundary, we use the f1-score (F1).\\n\\n$F1 = \\\\frac{2 \\\\times \\\\text{Precision} \\\\times \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}}$ (7)\\n\\n$\\\\text{Precision} = \\\\frac{\\\\# \\\\text{hit reference boundary}}{\\\\# \\\\text{reference boundary}}$ (8)\\n\\n$\\\\text{Recall} = \\\\frac{\\\\# \\\\text{hit hypothesis boundary}}{\\\\# \\\\text{hypothesis boundary}}$ (9)\\n\\nwhere the \\\"hit hypothesis boundary\\\" means that at least one reference boundary falls within a tolerance window before and after the hypothesis boundary. Similarly, \\\"hit reference boundary\\\" means that at least one predicted boundary falls within the same tolerance window before and after the reference boundary. For all our evaluations, the tolerance is set to 50 ms.\\n\\n4.4. Results\\n\\nTable 1 presents a comparison between our model and a few strong baseline models on SEAME test man/test sge. Among these baselines, the result of CIF is our implementation of the vanilla CIF-based model [24], and the conformer-AED [20] is the state-of-the-art model before our proposed method. In terms of the F1, our proposed method improves the acoustic boundary prediction compared to the CIF baseline. We attribute these improvements to the language-specific weight estimators. Figure 2 visualizes the acoustic boundaries and information weight predicted by our Mandarin and English weight estimators. The information weight predicted by our LSWE has obvious language discrimination, and our proposed method gives better boundary predictions. The results of MER show that our method has achieved a significant improvement on both the two test sets of\"}"}
{"id": "fan23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The upper part of the figure shows the weight estimator output in the vanilla CIF model, while the lower part shows the output of our two language-specific weight estimators. The green dotted lines mark the boundaries. The figure is for the utterance index 46NC41MBP in the test set.\\n\\nTable 1: The overall performance (MER %) and boundary metric (PR %) of our proposed method on the SEAME corpus. Results from previous papers and our implementation of the vanilla CIF model are shown for comparison.\\n\\n| Method                  | test man | test sge |\\n|-------------------------|----------|----------|\\n| Baseline                | 22.10    | 30.90    |\\n| LF-MMI [8]              | 25.60    | 37.00    |\\n| LSTM-AED [8]            | 19.20    | 26.90    |\\n| Transducer [13]         | 17.89    | 25.10    |\\n| Transformer-AED [18]    | 16.40    | 23.20    |\\n| Vanilla CIF [24]       | 40.09    | 16.96    |\\n| Our proposed           | 50.64    | 16.29    |\\n\\nSEAME corpus compared with the strong baselines, and set a new state-of-the-art performance.\\n\\nOur proposed method includes three modifications compared to the vanilla CIF-based model: language-specific weight estimators (LSWE), a NAR decoder trained with the monolingual label, and a LCD module. To evaluate the importance of these modifications, we conduct an ablation study on the SEAME corpus and the in-house meeting code-switching dataset. Table 2 shows the impact of each change on our method. The results indicate that LSWE is the most important modification, as removing it causes the largest performance degradation on both datasets. Secondly, ablating the NAR decoder also results in a large performance loss, while the LCD module brings some benefits to our method but is not as crucial as other modifications. In addition, our results on the in-house meeting code-switching datasets show that our method achieves a relative MER reduction of 2.97%, 6.4%, and 7.9% on the Mandarin, English, and code-switching test sets, respectively, compared with the vanilla CIF model (the result in the last row of Table 2). This indicates that our method improves the performance of the model in the CSSR task, particularly for code-switching speech.\\n\\nIn the full model of our method, the NAR decoder takes in both Mandarin and English token-level monolingual acoustic embedding to assist in learning the two language-specific weight estimators. Table 3 shows the results of ablating the NAR decoder inputs. When we feed no input to the NAR decoder, the model obtains unsatisfactory results for both Mandarin and English in the two evaluation sets. However, when the NAR decoder takes in Mandarin or English token-level acoustic embedding, the recognition performance of our method for the corresponding language improves. The best setting is to feed both Mandarin and English token-level acoustic embeddings to the NAR decoder. The results in Table 3 demonstrate that the NAR decoder can use the monolingual token-level acoustic embeddings generated by the LSWE to independently or simultaneously optimize the recognition performance of Mandarin and English.\\n\\n5. Conclusion\\n\\nThis paper employs the CIF model to address the CSSR task from the perspective of language-specific boundary learning. We introduce two language-specific weight estimators designed to handle boundary learning for Mandarin and English separately. Additionally, we introduce a NAR decoder and an LCD module to facilitate the training of the language-specific weight estimators. Our experimental results on the SEAME corpus and an in-house meeting code-switching dataset demonstrate that the proposed method improves the model's boundary prediction and speech recognition performance for both Mandarin and English.\"}"}
{"id": "fan23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] S. Sitaram, K. R. Chandu, S. K. Rallabandi, and A. W. Black, \u201cA survey of code-switched speech and language processing,\u201d arXiv preprint arXiv:1904.00784, 2019.\\n\\n[2] H. Adel, K. Kirchhoff, D. Telaar, N. T. Vu, T. Schlippe, and T. Schultz, \u201cFeatures for factored language models for code-switching speech,\u201d in Proc. Spoken Language Technologies for Under-Resourced Languages (SLTU), 2014, pp. 32\u201338.\\n\\n[3] H. Adel, N. T. Vu, K. Kirchhoff, D. Telaar, and T. Schultz, \u201cSyntactic and semantic features for code-switching factored language models,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 23, no. 3, pp. 431\u2013440, 2015.\\n\\n[4] H. Adel, N. T. Vu, and T. Schultz, \u201cCombination of recurrent neural networks and factored language models for code-switching language modeling,\u201d in Proc. Annual Meeting of the Association for Computational (ACL), 2013, pp. 206\u2013211.\\n\\n[5] N. T. Vu, D.-C. Lyu, J. Weiner, D. Telaar, T. Schlippe, F. Blaicher, E.-S. Chng, T. Schultz, and H. Li, \u201cA first speech recognition system for mandarin-english code-switch conversational speech,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2012, pp. 4889\u20134892.\\n\\n[6] E. Y\u0131lmaz, H. v. d. Heuvel, and D. A. van Leeuwen, \u201cAcoustic and textual data augmentation for improved asr of code-switching speech,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH). ISCA, 2018, pp. 1933\u20131937.\\n\\n[7] P. Guo, H. Xu, L. Xie, and E. S. Chng, \u201cStudy of semi-supervised approaches to improving english-mandarin code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2018, pp. 1928\u20131932.\\n\\n[8] Z. Zeng, Y. Khassanov, V. T. Pham, H. Xu, E. S. Chng, and H. Li, \u201cOn the end-to-end solution to mandarin-english code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2019, pp. 2165\u20132169.\\n\\n[9] N. Luo, D. Jiang, S. Zhao, C. Gong, W. Zou, and X. Li, \u201cTowards end-to-end code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2020, pp. 4776\u20134780.\\n\\n[10] J. Tian, J. Yu, C. Zhang, C. Weng, Y. Zou, and D. Yu, \u201cLae: Language-aware encoder for monolingual and multilingual asr,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2022, pp. 3178\u20133182.\\n\\n[11] T. Song, Q. Xu, H. Lu, L. Wang, H. Shi, Y. Lin, Y. Yang, and J. Dang, \u201cMonolingual recognizers fusion for code-switching speech recognition,\u201d arXiv preprint arXiv:2211.01046, 2022.\\n\\n[12] T. Song, Q. Xu, M. Ge, L. Wang, H. Shi, Y. Lv, Y. Lin, and J. Dang, \u201cLanguage-specific characteristic assistance for code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2022, pp. 3924\u20133928.\\n\\n[13] S. Dalmia, Y. Liu, S. Ronanki, and K. Kirchhoff, \u201cTransformer-transducers for code-switched speech recognition,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5859\u20135863.\\n\\n[14] B. Yan, M. Wiesner, O. Klejch, P. Jyothi, and S. Watanabe, \u201cTowards zero-shot code-switched speech recognition,\u201d arXiv preprint arXiv:2211.01458, 2022.\\n\\n[15] S. Zhang, J. Yi, Z. Tian, J. Tao, and Y. Bai, \u201cRnn-transducer with language bias for end-to-end mandarin-english code-switching speech recognition,\u201d in Proc. International Symposium on Chinese Spoken Language Processing (ISCSLP). IEEE, 2021, pp. 1\u20135.\\n\\n[16] K. Li, J. Li, G. Ye, R. Zhao, and Y. Gong, \u201cTowards code-switching asr for end-to-end ctc models,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6076\u20136080.\\n\\n[17] S. Zhang, J. Yi, Z. Tian, J. Tao, Y. T. Yeung, and L. Deng, \u201cReducing language context confusion for end-to-end code-switching automatic speech recognition,\u201d arXiv preprint arXiv:2201.12155, 2022.\\n\\n[18] Y. Peng, J. Zhang, H. Xu, H. Huang, and E. S. Chng, \u201cMinimum word error training for non-autoregressive transformer-based code-switching asr,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7807\u20137811.\\n\\n[19] S.-P. Chuang, H.-J. Chang, S.-F. Huang, and H.-y. Lee, \u201cNon-autoregressive mandarin-english code-switching speech recognition,\u201d in Proc. Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 465\u2013472.\\n\\n[20] Y. Peng, Y. Liu, J. Zhang, H. Xu, Y. He, H. Huang, and E. S. Chng, \u201cInternal language model estimation based language model fusion for cross-domain code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2022, pp. 1666\u20131670.\\n\\n[21] G. Lee and H. Li, \u201cModeling code-switch languages using bilingual parallel corpus,\u201d in Proc. Annual Meeting of the Association for Computational Linguistics (ACL), 2020, pp. 860\u2013870.\\n\\n[22] X. Zhou, E. Y\u0131lmaz, Y. Long, Y. Li, and H. Li, \u201cMulti-encoder-decoder transformer for code-switching speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2020, pp. 1042\u20131046.\\n\\n[23] Y. Yang and B. Wang, \u201cAcoustic correlates of hierarchical prosodic boundary in mandarin,\u201d in Proc. Speech prosody international conference, 2002.\\n\\n[24] L. Dong and B. Xu, \u201cCif: Continuous integrate-and-fire for end-to-end speech recognition,\u201d in Proc. International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6079\u20136083.\\n\\n[25] R. Sennrich, B. Haddow, and A. Birch, \u201cNeural machine translation of rare words with subword units,\u201d in Proc. Annual Meeting of the Association for Computational (ACL), 2016, pp. 1715\u20131725.\\n\\n[26] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2020, pp. 5036\u20135040.\\n\\n[27] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting,\u201d The journal of machine learning research, vol. 15, no. 1, pp. 1929\u20131958, 2014.\\n\\n[28] D.-C. Lyu, T.-P. Tan, E. S. Chng, and H. Li, \u201cSeame: a mandarin-english code-switching speech corpus in south-east asia,\u201d in Proc. Annual Conference of the International Speech Communication Association (INTERSPEECH), 2010, pp. 1986\u20131989.\\n\\n[29] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proc. Advances in Neural Information Processing Systems (NIPS), 2017, pp. 5998\u20136008.\\n\\n[30] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. International Conference on Learning Representations (ICLR), 2015.\\n\\n[31] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d in Proc. International Conference on Machine Learning (ICML), 2012.\"}"}
