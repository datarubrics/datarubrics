{"id": "takeuchi22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval\\n\\nDaiki Takeuchi, Yasunori Ohishi, Daisuke Niizumi, Noboru Harada, and Kunio Kashino\\n\\nNTT Corporation, Japan\\ndaiki.takeuchi.ux@hco.ntt.co.jp\\n\\nAbstract\\n\\nThe amount of audio data available on public websites is growing rapidly, and an efficient mechanism for accessing the desired data is necessary. We propose a content-based audio retrieval method that can retrieve a target audio that is similar to but slightly different from the query audio by introducing auxiliary textual information which describes the difference between the query and target audio. While the range of conventional content-based audio retrieval is limited to audio that is similar to the query audio, the proposed method can adjust the retrieval range by adding an embedding of the auxiliary text query-modifier to the embedding of the query sample audio in a shared latent space. To evaluate our method, we built a dataset comprising two different audio clips and the text that describes the difference. The experimental results show that the proposed method retrieves the paired audio more accurately than the baseline. We also confirmed based on visualization that the proposed method obtains the shared latent space in which the audio difference and the corresponding text are represented as similar embedding vectors.\\n\\nIndex Terms\\n\\ncontent-based audio retrieval, contrastive learning, crossmodal representation learning, deep neural network\\n\\n1. Introduction\\n\\nA massive amount of audio data is available on public websites, and it will continue to increase. Audio retrieval and environmental sound recognition by deep learning have been widely explored as way to use audio data effectively [1\u201316]. Audio retrieval is particularly essential for extracting desired audio data from the massive amount of available data.\\n\\nThe audio retrieval methods using an audio query are called content-based audio retrieval [58]. Those methods retrieve audio data with acoustic features similar to the query [14, 15, 17\u201319]. The methods can retrieve any audio data as long as it is similar to the audio query. However, it is necessary to prepare an audio query similar to the target audio data in advance.\\n\\nOn the other hand, the audio retrieval methods using a text query retrieve audio data paired with the text query [8\u201310, 20]. Some methods retrieve audio clips associated with the text query, and others retrieve audio clips whose embedding in the latent space is similar to that of the text query. Compared to content-based audio retrieval, we can easily prepare and edit text queries. However, it is not always easy to describe the audio contents precisely in text.\\n\\nTo overcome the above issues, we propose a new content-based audio retrieval framework that combines the auxiliary text query modifier with the given audio query. With this approach, one possible scenario could be as follows: First, we retrieve the initial audio by a method such as a text-query-based one, in which case the initial audio may not be close enough to the target one. Next, we find another audio clip by using a query that combines the initial audio and an additional text that describes the difference between the target and the initial audio. This combines the content-based and the text-query-based methods. We expect that the second search result gets closer to the target audio; then, we can further repeat the second step until the result becomes acceptable.\\n\\nWe implemented the framework as a method using neighborhood search in the latent space where the audio and difference described by text are embedded together. The method learns the representations in the latent space using crossmodal contrastive learning. We also introduced multi-task learning with a loss function that learns to classify audio contents to enhance differences in audio clips. To evaluate the proposed method, we built a dataset comprising two sounds with a difference and a description of the difference. Then, we conducted a comparison experiment with a baseline method that only uses sample audio as the query. We evaluated the retrieval accuracy and visualized the embeddings in the shared latent space and verified that the proposed method can adjust the retrieval range by means of the text query-modifier.\\n\\n2. Related work\\n\\nIn content-based audio retrieval, audio data with features similar to the audio query in the latent space are retrieved. Wold et al. attempted to retrieve audio on the basis of acoustic features, including loudness, pitch, brightness, and bandwidth [17]. Guo et al. proposed using Mel-frequency cepstral coefficients as a feature and the latent space trained by a support vector machine [21]. Other methods used DNNs as embedding models to construct the latent space. They trained the embedding models based on contrastive learning [14] and unsupervised learning [12]. The advantage of the content-based method is that it can search for similar audio at the level of acoustic features, making it possible to retrieve audio closer to the query in terms of similarity. The text-based method can describe audio contents more precisely than the content-based method, but it is not always easy to prepare and edit text queries.\"}"}
{"id": "takeuchi22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Add the sound of thunder\\n\\nQuery-modifier text\\n\\nTarget audio\\n\\nThe method with a model structure and training method that can be regarded as a neighborhood search problem in the latent space. A candidate data by using audio query. We construct and auxiliary text that describes the difference between the sample audio and the target audio as a search query. We select the content of the retrieved audio data by adding a difference that explain the content of a video query. Guzhov et al. [23, 24] proposed methods to retrieve audio captions of audio data [8] and text captions of audio data [9\u201311] have been used as the queries in these methods. Unlike these methods, we proposed a method to obtain the latent space integrating three modalities: audio, visual, and text [25]. Unlike these methods, we proposed a method to retrieve audio data corresponding to a video query. \\n\\nAudio retrieval methods using a text query train the content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal contrastive loss connects the audio difference to its textual representation. The content classification loss. The crossmodal"}
{"id": "takeuchi22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Labels used to synthesize APwD-Dataset\\n\\n| Rain          | Traffic                           |\\n|---------------|-----------------------------------|\\n| background    | rain                              |\\n| event         | dog, chirping birds               |\\n|               | thunder, footsteps                |\\n|               | car                               |\\n|               | horn, church bells                |\\n\\n4. Experiment\\n\\nIn the experiment, we focused on three types of difference: an increase/decrease in a background sound, addition/removal of a sound event, and an increase/decrease of a sound event. We built a dataset consisting of a pair of sounds with the above differences and the corresponding text and evaluated the proposed method by training and testing with its dataset.\\n\\n4.1. Audio Pair with Difference Dataset\\n\\nWe built an Audio Pair with Difference Dataset (APwD-Dataset) to evaluate the proposed method. The APwD-Dataset was a set of two similar audio clips synthesized using the FSD50K [2] and ESC-50 [31] audio data, and an auxiliary text describing the differences between the similar audios. Scaiper [32] was used to synthesize the similar audio data. In this experiment, the dataset was created by setting up two scenes: Rain and Traffic.\\n\\nTo synthesize a Rain scene, data labeled \u201crain\u201d in FSD50K was used as background, and data labeled \u201cdog\u201d, \u201cchirping birds\u201d, \u201cthunder\u201d, or \u201cfootsteps\u201d in ESC-50 was used as events added to background. To synthesize a Traffic scene, data labeled \u201ccar passing\u201d in FSD50K was used as background, and data labeled \u201cdog\u201d, \u201cchirping birds\u201d, \u201ccar horn\u201d, or \u201cchurch bells\u201d in ESC-50 was used as events.\\n\\nThe sounds with specific labels used for synthesis in the FSD50K and ESC-50 are shown in Table 1. The development set and evaluation set for each scene contain 50,000 and 1,000 data, respectively.\\n\\nThe APwD dataset was synthesized using the following procedure. First, the audio data with the labels were extracted from FSD50K and ESC-50. The data assigned to the training and validation split of FSD50K and folds 1\u20134 of ESC-50 were used to synthesize the development set of the APwD-dataset, and the data assigned to the evaluation split of FSD50K and fold 5 of ESC-50 were used to synthesize the evaluation set. Afterwards, audio data containing audible noise and other audio events were manually excluded.\\n\\nNext, we synthesized pairs of similar audio samples, $\\\\alpha$ and $\\\\beta$, which only consist of background audio. Two pieces of data were cropped from the same audio file for 10 s at random locations and assigned to $\\\\alpha$ and $\\\\beta$. Then, the difference was given between the pair of sound data by adding another background audio and/or event audio to $\\\\alpha$ and $\\\\beta$.\\n\\nWe consider the following six types of differences listed below and describe how they were synthesized. Note that because it is difficult to semantically reduce or eliminate background audio and event audio, they are simulated by addition.\\n\\n(a) Increase volume of background audio of $\\\\alpha$:\\nRandomly select the background sound data, cut out 10s, and add it to audio sample $\\\\beta$.\\n\\n(b) Decrease volume of background audio of $\\\\alpha$:\\nApply operation (a) and replace $\\\\alpha$ and $\\\\beta$.\\n\\n(c) Add sound of audio event to $\\\\alpha$:\\nRandomly select the data used in the audio event and add it to audio sample $\\\\beta$.\\n\\n(d) Remove sound of audio event from $\\\\alpha$:\\nApply operation (c) and replace $\\\\alpha$ and $\\\\beta$.\\n\\n(e) Increase volume of audio event of $\\\\alpha$:\\nRandomly select two audio data with the same label for the audio event and normalize them. After that, the amplitude of one data is reduced. The one with the larger amplitude is added to audio sample $\\\\beta$; the one with the smaller amplitude is added to audio sample $\\\\alpha$.\\n\\n(f) Decrease volume of audio event of $\\\\alpha$:\\nApply operation (e) and replace $\\\\alpha$ and $\\\\beta$.\\n\\nWe provided one or two types of differences for each pair of audio samples.\\n\\nFinally, a description corresponding to the difference was assigned. This description was written in the form of an imperative sentence, whose content described how to change the audio sample $\\\\alpha$ to the audio $\\\\beta$. For example, if difference (a) is given to the paired data of the Rain scene, the description is \u201cincrease the sound of rain\u201d; if (f) is given for car horn and (c) for dog, the description is \u201cmake car horn lower and add dog bark.\u201d\\n\\n4.2. Experimental conditions\\n\\nWe used 10% of the development set for validation. The optimizer is Adam [33]. The number of epochs was set to 300, and the model with the smallest loss of validation data was used for evaluation. We used recall@$K$ (R@$K$) to evaluate the accuracy of audio retrieval. R@$K$ is the rate at which the ground-truth audio files are within the $K$th rank of the retrieval result.\\n\\nWe compared three models: the proposed method without classif. loss ($\\\\rho = 0$), the proposed method with classif. loss ($\\\\rho = 1$), and the baseline method. For the baseline, we used the method of retrieving the target audio using only the sample audio without auxiliary text. The parameters of the audio encoder were the same as in the proposed method. Therefore, $z_{k,l} = d_{cossim}(A(a_k), A(b_l))$ was applied to Eq. (6) instead of Eq. (5) In a preliminary study, we examined another method that takes an existing captioning system and a text query-modifier. However, it was not as effective as the baseline method using a sample audio query, so we did not include it in the comparison.\\n\\n4.3. Results\\n\\n4.3.1. Comparison between proposed method and baseline\\n\\nWe conducted an experiment to compare the proposed method with the baseline, the results of which are shown in Table 2. Bold font indicates the highest scores. The retrieval accuracy of the proposed method was higher than that of the baseline method in all conditions. Table 3 shows the Recall@1 for each background and event sound that was given a difference. \u201cbackground\u201d is \u201crain\u201d in the Rain scene and \u201ccar passing\u201d in the Traffic scene. The results show that the proposed method outperformed the baseline method for all background and event sounds. The retrieval accuracy of the differences in the background audio of each scene, \u201crain\u201d and \u201ccar passing\u201d, was significantly lower than the others. Thus, dealing with differences in the background audio was more difficult than dealing with one in the audio event.\\n\\n4.3.2. UMAP visualization of embedding vector\\n\\nTo verify that the proposed method handles difference information appropriately, we visualized the embedding vectors in the latent space. For visualization, we created 100 datasets each\"}"}
{"id": "takeuchi22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison between proposed and baseline method\\n\\n| Method                        | Text info. | Classif. loss | R@1 | R@5 | R@10 | R@1 | R@5 | R@10 |\\n|-------------------------------|------------|---------------|-----|-----|------|-----|-----|------|\\n| (a) Baseline                  | \u00d7          | \u00d7             | 0.256 | 0.611 | 0.699 | 0.260 | 0.500 | 0.593 |\\n| (b) Proposed w/o classif. loss| \u2713          | \u00d7             | 0.388 | 0.681 | 0.745 | 0.361 | 0.590 | 0.675 |\\n| (c) Proposed w/ classif. loss | \u2713          | \u2713             | 0.445 | 0.721 | 0.769 | 0.391 | 0.622 | 0.695 |\\n\\nTable 3: R@1 comparison for different audio events\\n\\n| Method                        | Scene     | background | dog | chirping | bird | thunder | footsteps | car | horn | church | bells |\\n|-------------------------------|-----------|------------|-----|----------|------|---------|-----------|-----|------|--------|-------|\\n| (a) Baseline                  | Rain      | 0.035      | 0.304 | 0.205    | 0.346 | N/A     | N/A       |     |      |        |       |\\n| (b) Proposed w/o classif. loss| Rain      | 0.061      | 0.448 | 0.360    | 0.490 | 0.450   | N/A       |     |      |        |       |\\n| (c) Proposed w/ classif. loss | Rain      | 0.061      | 0.538 | 0.438    | 0.534 | 0.522   | N/A       |     |      |        |       |\\n\\n(a) Baseline (b) Proposed w/o classif. loss (c) Proposed w/ classif. loss\\n\\nFigure 3: UMAP visualization of the difference in audio embedding vectors and text embedding vector. In (c), the proposed method with classification loss, the audio difference of \\\"the addition of thunder\\\" (blue circle) and that of \\\"the addition of dog\\\" (orange triangle) form different distributions for each type of difference. In addition, the text embedding (thunder: blue circle outline, dog: red triangle outline) belongs to the same distribution as the corresponding difference in audio embedding vectors.\\n\\nThe visualization results are shown in Fig. 3, where (a), (b), and (c) show the embedding vectors for the baseline, the proposed method without classification loss, and the proposed method with classification loss, respectively. In the baseline, embedding vectors were placed regardless of the type of difference. In the proposed method without classification loss, the distributions of the embedding vectors of \\\"the addition of thunder\\\" and \\\"the addition of dog\\\" were slightly biased to the lower right and upper left, respectively. In the proposed method with classification loss, the distributions of the embedding vectors of \\\"the addition of thunder\\\" and \\\"the addition of dog\\\" were clearly biased to the lower right and upper left, respectively. Focusing on the text embedding vectors, they were placed such that they belonged to the same distribution as the corresponding difference of audio embedding vectors in the proposed method with classification loss. Therefore, the training classification task for audio data should enhance the recognition of differences.  \\n\\n5. Conclusion\\n\\nWe proposed a content-based audio retrieval method that can retrieve target audio that is similar to but slightly different from the query audio by introducing an auxiliary text-query modifier which describes the difference between the query and the target audio. The proposed method adjusts the retrieval range to obtain the target sound by adding the embedding vector of the auxiliary text-query modifier to that of the query audio in shared latent space. We experimentally verified that the proposed method can obtain audio data with the difference from the query sound by utilizing the information in the introduced auxiliary text. After visualizing the embedding vectors in the latent space, we also verified that the proposed method learns the relation between the audio difference and its textual representation.\\n\\nFuture work includes improving the accuracy of the search for increases and decreases in background so that it as accurate as the search for differences in audio events. We also aim to collect data from actual recordings annotated by humans to build a model that can handle a wider variety of descriptions and differences.\"}"}
{"id": "takeuchi22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \\\"Audio set: An ontology and human-labeled dataset for audio events,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2017, pp. 776\u2013780.\\n\\n[2] E. Fonseca, X. Favory, J. Pons, F. Font, and X. Serra, \\\"Fsd50k: an open dataset of human-labeled sound events,\\\" arXiv preprint arXiv:2010.00475, 2020.\\n\\n[3] C. D. Kim, B. Kim, H. Lee, and G. Kim, \\\"AudioCaps: Generating captions for audios in the wild,\\\" in Proc. Conf. N. Am. Chapter Assoc. Comput. Linguist., 2019, pp. 119\u2013132.\\n\\n[4] K. Drossos, S. Adavanne, and T. Virtanen, \\\"Clotho: An audio captioning dataset,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP), 2019, pp. 736\u2013740.\\n\\n[5] D. Takeuchi, Y. Koizumi, Y. Ohishi, N. Harada, and K. Kashino, \\\"Effects of word-frequency based pre- and post- processings for audio captioning,\\\" in Proc. Detect. Classif. Acoust, Scenes Events Workshop (DCASE), November 2020.\\n\\n[6] Y. Gong, Y.-A. Chung, and J. Glass, \\\"AST: Audio spectrogram transformer,\\\" in Proc. Interspeech, 2021, pp. 571\u2013575.\\n\\n[7] A. Mesaros, T. Heittola, and T. Virtanen, \\\"TUT database for acoustic scene classification and sound event detection,\\\" in Proc. 24th Eur. Signal Process. Conf. (EUSIPCO). IEEE, 2016, pp. 1128\u20131132.\\n\\n[8] B. Elizalde, S. Zarar, and B. Raj, \\\"Cross modal audio search and retrieval with joint embeddings based on text and audio,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2019, pp. 4095\u20134099.\\n\\n[9] A.-M. Oncescu, A. Koepke, J. Henriques, Z. Akata, and S. Albanie, \\\"Audio retrieval with natural language queries,\\\" in Proc. Interspeech, 2021.\\n\\n[10] A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and S. Albanie, \\\"Audio retrieval with natural language queries: A benchmark study,\\\" IEEE Trans. Multimedia, 2022.\\n\\n[11] H. Xie, S. Lipping, and T. Virtanen, \\\"Dcase 2022 challenge task 6b: Language-based audio retrieval,\\\" arXiv preprint arXiv:2206.06108, 2022.\\n\\n[12] P. Panyapanuwat, S. Kamonsantiroj, and L. Pipanmaekaporn, \\\"Unsupervised learning hash for content-based audio retrieval using deep neural networks,\\\" in Proc. 11th Int. Conf. Knowl. Smart Technol. (KST). IEEE, 2019, pp. 99\u2013104.\\n\\n[13] S. Ikawa and K. Kashino, \\\"Acoustic event search with onomatopoeic query: measuring distance between onomatopoeic words and sounds,\\\" in Proc. Detect. Classif. Acoust. Scenes Events (DCASE) Workshop, 2018, pp. 59\u201363.\\n\\n[14] P. Manocha, R. Badlani, A. Kumar, A. Shah, B. Elizalde, and B. Raj, \\\"Content-based representations of audio using siamese neural networks,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2018, pp. 3136\u20133140.\\n\\n[15] B. Kim and B. Pardo, \\\"Improving content-based audio retrieval by vocal imitation feedback,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2019, pp. 4100\u20134104.\\n\\n[16] B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, \\\"Clap: Learning audio concepts from natural language supervision,\\\" arXiv preprint arXiv:2206.04769, 2022.\\n\\n[17] E. Wold, T. Blum, D. Keislar, and J. Wheaten, \\\"Content-based classification, search, and retrieval of audio,\\\" IEEE multimedia, vol. 3, no. 3, pp. 27\u201336, 1996.\\n\\n[18] S. Sundaram and S. Narayanan, \\\"Audio retrieval by latent perceptual indexing,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2008, pp. 49\u201352.\\n\\n[19] T. M\u00e4kinen, S. Kiranyaz, J. Raitoharju, and M. Gabbouj, \\\"An evolutionary feature synthesis approach for content-based audio retrieval,\\\" EURASIP J. Audio Speech Music Process., vol. 2012, no. 1, pp. 1\u201323, 2012.\\n\\n[20] S. Kim, P. Georgiou, S. Narayanan, and S. Sundaram, \\\"Using na\u00efve text queries for robust audio information retrieval,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2010, pp. 2406\u20132409.\\n\\n[21] G. Guo and S. Z. Li, \\\"Content-based audio classification and retrieval by support vector machines,\\\" IEEE trans. Neural Netw., vol. 14, no. 1, pp. 209\u2013215, 2003.\\n\\n[22] D. Sur\u00eds, A. Duarte, A. Salvador, J. Torres, and X. Gir\u00f3-i Nieto, \\\"Cross-modal embeddings for video and audio retrieval,\\\" in Proc. Eur. Conf. Comput. Vis. (ECCV) Workshops, 2018, pp. 711\u2013716.\\n\\n[23] A. W. Boggust, K. Audhkhasi, D. Joshi, D. Harwath, S. Thomas, R. S. Feris, D. Gutfreund, Y. Zhang, A. Torralba, M. Picheny, and J. Glass, \\\"Grounding spoken words in unlabeled video.\\\" in Proc. Conf. Comput. Vis. Pattern Recognit. Workshops, 2019, pp. 29\u201332.\\n\\n[24] A. Rouditchenko, A. Boggust, D. Harwath, B. Chen, D. Joshi, S. Thomas, K. Audhkhasi, H. Kuehne, R. Panda, R. Feris, B. Kingsbury, M. Picheny, A. Torralba, and J. Glass, \\\"A VLnet: Learning audio-visual language representations from instructional videos,\\\" in Proc. Interspeech, 2021.\\n\\n[25] A. Guzhov, F. Raue, J. Hees, and A. Dengel, \\\"AudioCLIP: Extending clip to image, text and audio,\\\" arXiv preprint arXiv:2106.13043, 2021.\\n\\n[26] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, R. C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney, R. J. Weiss, and K. Wilson, \\\"CNN architectures for large-scale audio classification,\\\" in Proc. IEEE Int. Conf. Acoust. Speech Signal Process. (ICASSP). IEEE, 2017, pp. 131\u2013135.\\n\\n[27] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \\\"PANNs: Large-scale pretrained audio neural networks for audio pattern recognition,\\\" IEEE/ACM Trans. Audio Speech Lang. Process., vol. 28, pp. 2880\u20132894, 2020.\\n\\n[28] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, \\\"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter,\\\" arXiv preprint arXiv:1910.01108, 2019.\\n\\n[29] D. Hendrycks and K. Gimpel, \\\"Gaussian error linear units (GELUs),\\\" arXiv preprint arXiv:1606.08415, 2016.\\n\\n[30] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, \\\"Learning transferable visual models from natural language supervision,\\\" arXiv preprint arXiv:2103.00020, 2021.\\n\\n[31] K. J. Piczak, \\\"ESC: Dataset for Environmental Sound Classification,\\\" in Proc. 23rd Annual ACM Conf. Multimedia. ACM Press, pp. 1015\u20131018.\\n\\n[32] J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello, \\\"Scaper: A library for soundscape synthesis and augmentation,\\\" in Proc. IEEE Workshop Appl. Signal Process. Audio Acoust. (WASPAA). IEEE, 2017, pp. 344\u2013348.\\n\\n[33] D. P. Kingma and J. Ba, \\\"Adam: A method for stochastic optimization,\\\" in Proc. in Int. Conf. Learn. Represent. (ICLR), 2014.\\n\\n[34] L. McInnes, J. Healy, and J. Melville, \\\"UMAP: Uniform manifold approximation and projection for dimension reduction,\\\" arXiv preprint arXiv:1802.03426, 2018.\"}"}
