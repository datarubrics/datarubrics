{"id": "gothi24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The authors gratefully acknowledge the Centre for Machine Intelligence and Data Science (C-MInDS) at the Indian Institute of Technology Bombay for financially supporting this research work.\\n\\nReferences\\n\\n[1] \\\"The global coalition for foundational learning,\\\" 2022, https://tcg.uis.unesco.org/wp-content/uploads/sites/4/2023/05/The-Global-Coalition-for-Foundational-Learning-Narrative.pdf.\\n\\n[2] C. Cucchiarini and H. Van hamme, \\\"The jasmin speech corpus: recordings of children, non-natives and elderly people,\\\" Essen-\\ntential Speech and Language Technology for Dutch: Results by the STEVIN programme, pp. 43\u201359, 2013.\\n\\n[3] J. Proenc\u00b8a, D. Celorico, S. Candeias, C. Lopes, and F. Perdig \u02dcao, \\\"The letsread corpus of portuguese children reading aloud for per-\\nformance evaluation,\\\" in Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16), 2016, pp. 781\u2013785.\\n\\n[4] M. P. Black, J. Tepperman, and S. S. Narayanan, \\\"Automatic pre-\\ndiction of children's reading ability for high-level literacy assess-\\nment,\\\" IEEE Transactions on Audio, Speech, and Language Pro-\\ncessing, vol. 19, no. 4, pp. 1015\u20131028, 2010.\\n\\n[5] C. Hacker, A. Batliner, S. Steidl, E. N\u00a8oth, H. Niemann, and T. Cin-\\ncarek, \\\"Assessment of non-native children's pronunciation: Hu-\\nman marking and automatic scoring,\\\" Proceedings of the 10th In-\\nternational Conference on SPEECH and COMPUTER, vol. 1, pp. 123\u2013126, 2005.\\n\\n[6] L. Cleuren, J. Duchateau, P. Ghesquiere et al., \\\"Children's oral reading corpus (chorec): description and assessment of annotator agreement,\\\" LREC 2008 Proceedings, pp. 998\u20131005, 2008.\\n\\n[7] B.-C. Yan, M.-C. Wu, H.-T. Hung, and B. Chen, \\\"An End-to-\\nEnd Mispronunciation Detection System for L2 English Speech Leveraging Novel Anti-Phone Modeling,\\\" in Proc. Interspeech, 2020, pp. 3032\u20133036.\\n\\n[8] D. Bola \u02dcnos, R. A. Cole, W. Ward, E. Borts, and E. Svirsky, \\\"Flora: Fluent oral reading assessment of children's speech,\\\" ACM Trans-\\ntactions on Speech and Language Processing (TSLP), vol. 7, no. 4, pp. 1\u201319, 2011.\\n\\n[9] J. Duchateau, M. Wigham, K. Demuynck, and H. van Hamme, \\\"A flexible recogniser architecture in a reading tutor for children,\\\" in Proc. ITRW on Speech Recognition and Intrinsic Variation, 2006, pp. 59\u201364.\\n\\n[10] J. Cheng, \\\"Real-Time Scoring of an Oral Reading Assessment on Mobile Devices,\\\" in Proc. Interspeech, 2018, pp. 1621\u20131625.\\n\\n[11] J. Proenc\u00b8a, C. Lopes, M. Tjalve, A. Stolcke, S. Candeias, and F. Perdigao, \\\"Mispronunciation detection in children's reading of sentences,\\\" IEEE/ACM Transactions on Audio, Speech, and Lan-\\nguage Processing, vol. 26, no. 7, pp. 1207\u20131219, 2018.\\n\\n[12] B. Molenaar, C. Tejedor-Garcia, C. Cucchiarini, and H. Strik, \\\"Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics,\\\" in Proc. INTERSPEECH, 2023, pp. 5232\u20135236.\\n\\n[13] T. Piton, E. Hermann, A. Pasqualotto, M. Cohen, M. Magimai-Doss, and D. Bavelier, \\\"Using Commercial ASR Solutions to Assess Reading Skills in Children: A Case Report,\\\" in Proc. INTERSPEECH, 2023, pp. 4573\u20134577.\\n\\n[14] M. Ellis, \\\"Supporting young l2 english learners with word recog-\\nnition: design of early reading materials,\\\" Neofilolog, vol. 59, no. 2, pp. 126\u2013143, 2022.\\n\\n[15] IIT Madras Speech Lab, \\\"IIT-Madras Hindi-Tamil-English ASR Challenge,\\\" 2021, https://sites.google.com/view/indian-language-asrchallenge/home.\\n\\n[16] K. group, \\\"Discussion about failure of chain models for gop,\\\" 2020, \\\"See jimbozhang's comment on May 22, 2020\\\". [Online]. Available: https://github.com/kaldi-asr/kaldi/issues/3675\\n\\n[17] N. Ruiz and M. Federico, \\\"Phonetically-oriented word error align-\\nment for speech recognition error analysis in speech translation,\\\" in 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 296\u2013302.\\n\\n[18] H. Jiang, \\\"Confidence measures for speech recognition: A sur-\\nvey,\\\" Speech communication, vol. 45, no. 4, pp. 455\u2013470, 2005.\\n\\n[19] S. M. Witt and S. J. Young, \\\"Phone-level pronunciation scoring\\nand assessment for interactive language learning,\\\" Speech com-\\nmunication, vol. 30, no. 2-3, pp. 95\u2013108, 2000.\\n\\n[20] W. Hu, Y . Qian, and F. K. Soong, \\\"An improved dnn-based ap-\\nproach to mispronunciation detection and diagnosis of l2 learners'\\nspeech.\\\" SLaTE, vol. 5, pp. 71\u201376, 2015.\\n\\n[21] S. Kanters, C. Cucchiarini, and H. Strik, \\\"The goodness of pro-\\nnunciation algorithm: a detailed performance study,\\\" in Proc. Speech and Language Technology in Education, 2009, pp. 49\u201352.\\n\\n[22] H. Ji, T. Patel, and O. Scharenborg, \\\"Predicting within and across language phoneme recognition performance of self-supervised learning speech pre-trained models,\\\" 2022.\\n\\n[23] A. Graves, S. Fern \u00b4andez, F. Gomez, and J. Schmidhuber, \\\"Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\\\" in Proceedings of the 23rd International Conference on Machine Learning, ser. ICML '06. New York, NY , USA: Association for Computing Machinery, 2006, p. 369\u2013376. [Online]. Available: https://doi.org/10.1145/1143844.1143891\\n\\n[24] X. Xu, Y . Kang, S. Cao, B. Lin, and L. Ma, \\\"Explore wav2vec 2.0 for Mispronunciation Detection,\\\" in Proc. Interspeech, 2021, pp. 4428\u20134432.\\n\\n[25] D. Oneat \u00b8\u02d8a, A. Caranica, A. Stan, and H. Cucu, \\\"An evaluation of word-level confidence estimation for end-to-end automatic speech recognition,\\\" in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 258\u2013265.\\n\\n[26] A. Laptev and B. Ginsburg, \\\"Fast entropy-based methods of word-level confidence estimation for end-to-end automatic speech recognition,\\\" in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 152\u2013159.\"}"}
{"id": "gothi24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Dataset and Two-pass System for Reading Miscue Detection\\n\\nRaj Gothi1, Rahul Kumar2, Mildred Pereira2, Nagesh Nayak2, Preeti Rao1,2\\n\\n1Centre for Machine Intelligence and Data Science, IIT Bombay\\n2Department of Electrical Engineering, IIT Bombay\\n{22m2160,22m1163,mildredp,nageshnayak,prao}@iitb.ac.in\\n\\nAbstract\\nAutomatic speech recognition (ASR) has long been viewed as a promising solution to the resource-intensive task of oral reading fluency assessment. The demands on ASR accuracy, however, tend to be high, especially when applied to obtaining reliable reading diagnostics. The prior knowledge of reading prompts is typically used to limit the system WER. The accurate detection of mispronounced words, which can be relatively few in number, while limiting false positives, remains challenging. In this work, we present a new manually transcribed dataset of 1,110 elementary school children reading connected text in L2 English with wide-ranging proficiencies. Apart from local features derived from alternate decodings under different linguistic context constraints, we use an additional deep acoustic model. We discuss the performance gains achieved in a second pass over initial hybrid ASR hypotheses.\\n\\nIndex Terms: reading miscues, automatic assessment, speech recognition\\n\\n1. Introduction\\nFoundational literacy has been a target of heightened attention since 2022, when global coalitions were formed to support country-led action to address the learning crisis [1]. The monitoring of outcomes and generation of learning data are among the key components of the effort, making objective and scalable methods for literacy measurement highly desirable. Even as the estimated words read correctly per minute (WCPM) is utilized to obtain fluency benchmarks, the precise nature of the reading errors is necessary to determine a child's reading level and the appropriate intervention. Motivated by the time and resource intensive nature of manual assessments, research in the automatic evaluation of oral reading has been pursued for close to three decades by dedicated academic groups in Europe and the U.S. This has given rise to children's oral reading corpora for isolated words, sentences and, to a lesser extent, connected texts, in languages such as Dutch, French, Italian, English and Portuguese by between 40 to 500 speakers [2\u20135]. Closest to the dataset presented in this work, in terms of type of reading prompts and number of speakers, is CHOREC, comprising of 300 Dutch-speaking children of Grades 1-4 reading stories [6].\\n\\nIn the related field of CAPT (computer-aided pronunciation training), the focus has been on detecting phonetic errors in the adult second language learning context, as opposed to word reading miscues [7]. Hybrid ASR systems have been the dominant choice of solution for the oral reading accuracy task, given the ease of integrating the acoustic model with a language model (LM) of specific context constraints. Constraining the ASR with the knowledge of the reading prompts (via a task-specific LM) is helpful in limiting the WER and therefore improving the prediction of correctly uttered words, followed possibly by a rescoring of the segmented words using local features derived using less constrained LMs. To briefly review representative earlier work, Bolanos et al. [8] used a GMM-HMM system trained on children's speech corpora to evaluate reading fluency of 313 students of Grades 1-4 reading text passages using a trigram LM for each passage. They obtained a high correlation for the estimated words correct per minute (WCPM) with human raters but did not report the miscue detection accuracy. Duchateau et al. [9] use the careful design of an FST with all expected text-dependent reading miscues. Cheng et al. [10] used a Kaldi DNN-HMM system with an item-specific LM to achieve the reliable prediction of WCPM. Proenca et al. [11] applied a second pass to the decoded output from a DNN-HMM ASR to flag reading miscues based on GOP-like features computed on word segments. They showed a further improvement with the use of phone edit distance with reference to the output of free phone recognition, although this was limited by the accuracy of phone recognition.\\n\\nVery recently, Molenaar et al. [12] compared Kaldi TDNN systems trained on adult speech combined with 4 different LMs and two general-purpose Whisper-based ASR systems (without and with prompts). The Kaldi system outperformed Whisper, particularly when an LM enriched by the manually transcribed reading was used. Similarly Piton et al. [13] evaluated a few commercial ASR systems on French and Italian children's reading assessment to conclude that the analysis results are not sufficiently fine-grained and recommend a second pass of the ASR output. The challenges lie in that reading miscues (typically a small fraction of the total words read) must be reliably flagged while limiting false positives. The accuracy of identifying mispronounced words tends to be poor with reference to manually detected reading errors due to the mostly out-of-vocabulary substitutions made by beginning readers who are not yet fully familiar with the letter-to-sound rules of the language.\\n\\nIn this work, we present a dataset that considerably enriches the available set of children's oral reading corpora with its large number of unique speakers reading connected text in L2 English, manually transcribed and labeled for use in reading miscue studies. We test the performance of a hybrid ASR, the Kaldi based TDNN (a type of DNN-HMM acoustic model or AM), trained on adult Indian English speech, with two distinct LM constraints. Taking forward the LM that is constrained only on the reading prompts (i.e. the canonical text), as the scenario most commonly arising in practice, we present results for a variety of second-pass local features, and their fusion, that serve to enhance the miscue detection performance, measured in terms of miscue miss-rate for a fixed false positive rate (FPR). Apart from...\"}"}
{"id": "gothi24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"From the widely used goodness of pronunciation (GOP), we experiment with phonetic edit distance with the use of wav2vec2 XLSR (pretrained via self-supervised learning (SSL) on 53 languages) and fine-tuned by us to produce relatively accurate phonetic transcripts. In the next section, we describe our new test dataset, as well as the training datasets used in this work, followed by the presentation of the methods and experiments.\\n\\n2. Datasets\\n\\nAs part of a benchmarking exercise for reading levels in elementary school, audio recordings of oral reading of L2 English grade-appropriate texts were collected for close to 2,500 students in 10 government schools across the Indian states of Maharashtra and Goa in Grades 3, 4 and 5 (age 7-11 years) in the summer of 2023. Ethics clearance was obtained for the audio recording with anonymised speaker information but for grade and gender. The students, who come with diverse home languages, are introduced to both Hindi and English reading and writing in Grade 1. Unlike L1 English learners of reading, they have a non-existent (or very limited) vocabulary for English, and are therefore encountering new words and their written forms simultaneously. They cannot draw on any aural memory to sound the written form and hence make a variety of errors in pronunciation owing to the opaque orthography of English as well as phonotactic constraints of their home language [14]. For instance, the word 'shepherd' in the Grade 3 text is uttered as 'seperd' or 'sheep hard' among other variations, and 'bears' as 'beers' in Grade 4. Although we find some L1 accent-caused variations as well, we compensate for this with our phoneme-based lexicon with alternate pronunciations when applicable.\\n\\nThe text prompts comprise of the 2 paragraphs of a single story, each between 60-80 words, with a unique story assigned to each grade. Manual transcription at the word level is underway. The reading errors are observed to comprise a number of non-English words, which are transcribed phonemically. Of the already transcribed dataset, we select for this work those utterances (i.e. the recording of one story paragraph) where at least 70% of the canonical words have been attempted. These amount to 1600 utterances across 1110 unique speakers, distributed close to uniformly across the 3 grades, for a total audio duration of 19 hours. We term this the MPS (for Maharashtra Primary Schools) dataset. The background noise varies from barely audible to classroom noise in the vicinity of the speaker. This dataset serves as the test dataset in this work with ground-truth miscue labels (Cor/Sub/Del/Ins) derived by comparison of the manual transcription with suitably aligned text prompts (using the phonetically oriented alignment discussed in Sec. 3.1).\\n\\nTable 1 shows the distribution of attempted words across the dataset of 1600 prompts, where we note that about 10% of the total attempted words constitute reading miscues (i.e. DEL + SUB, including different types of SUB), while the per-utterance miscues range from 0 to 24 words. The manual transcription is detailed enough to label the type of reading disfluency. The test dataset is divided into 6 folds with non-overlapping speakers and uniform distribution of grades and reading levels (in terms of number of miscues per utterance) to facilitate the cross-validation (CV) training and testing of miscue detection classifier of the second pass.\\n\\nOur AM and LM training datasets in this work summarised in Table 2 are (i) IITM: Indian English adult speech [15], (ii) W AP: Indian children (11-15 age group) reading a variety of English text prompts, collected by us over WhatsApp voice messaging during the pandemic (2020-2021). This dataset brings in the context of read speech in children's voices (even if the group is older in age compared with the MPS test data cohort).\\n\\n### Table 1: Distribution of word reading errors across the 1,10,898 attempted words in MPS dataset\\n\\n| Tags     | Number (%) | Description             |\\n|----------|------------|-------------------------|\\n| COR      | 99,554 (89.77%) | Prompt word is correctly pronounced |\\n| INS      | 3269 (2.94%)   | Inserted word, not part of the prompt |\\n| DEL      | 1331 (1.20%)   | Prompt word omitted     |\\n| SUB-1    | 3452 (3.11%)   | Substitution: one-phone difference |\\n| SUB-2    | 6561 (5.91%)   | Gross substitution (>1 phone variation) |\\n\\n3. Method\\n\\nOur two-pass system incorporates a hybrid ASR in the first pass with a strongly constrained LM, i.e. word trigram trained on the text prompts. Anticipating the consequent low recall of miscues, the second pass re-evaluates the canonical words labeled 'correct' in Pass 1 via local features computed on the corresponding acoustic segment. In this section, we describe the initial segmentation and labeling achieved by the first pass hybrid ASR and the computation of features for the second pass.\\n\\n3.1. Segmentation and alignment with text prompt\\n\\nThe hybrid system is based on the 'nnet3' Time Delay Neural Network (TDNN) (no-chain model, which is more suited to GOP computation than is the chain model [16]) trained with the Kaldi Librispeech recipe on the IITM dataset. Frame labels for TDNN model training were obtained by forced alignment using a GMM-HMM model trained beforehand. The number of modeled phones were 40 (39 speech and a silence phone). For the LM, we use 3-gram models on training text as detailed in the experiments section.\\n\\nFor the reading assessment task, the alignment of the prompt words with the ASR hypothesis words (or with the manual transcript) needs to be more nuanced than that achievable with conventional word-level edit distance, i.e. where exact match is considered and errors comprise substitutions, deletions, or insertions, based on the Levenshtein distance. In order to predict the precise types of reading miscues accurately and also obtain the correct word boundaries for the subsequent stage of local feature computation, the ASR output sequence of words needs to be aligned with the reading prompt sequence of words in a manner that is phonetically informed. This exploits the expected phonetic similarity between the uttered word and attempted prompt word. We modify the alignment algorithm of Ruiz et al. [17] to accommodate the peculiarities of the reading task.\"}"}
{"id": "gothi24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"application such as the occurrence of word splitting or merging.\\nFalse starts, a common event in oral reading, are accommodated\\nwith bias towards right alignment. We also account for the possibly\\nmultiple valid pronunciations of a prompt word. Eventually, we\\nassociate a decoder output word with each canonical word (unless\\nit is found deleted by the alignment algorithm).\\n\\nThe canonical word is labeled Correct if it matches the decoder\\nword and Substituted otherwise. Insertions are ignored in oral\\nreading accuracy measurement.\\n\\n3.2. Word-level features for miscue detection\\nFrom the Pass 1, we obtain the acoustic word boundaries asso-\\nciated with each canonical word labeled correct. Next, local\\nfeatures as described below are extracted. In the Pass 2, we\\ncontrol the trade-off between miss-rate and FPR by applying a\\nthreshold to the feature to relabel those ASR-predicted Correct\\nwords with score below the threshold as Substitutions.\\n\\n3.2.1. Lattice-based confidence scores\\nLattice based confidence scores are obtained from the word\\ngraph generated during the decoding of the utterance. The word\\ngraph serves as a condensed representation of the hybrid de-\\ncoder search space. Word posterior, likelihood ratio test (LRT)\\nand hypothesis density are different measures of the dominance\\nof the decoded word over the alternative hypotheses in the word\\ngraph, and represent decoder confidence [18].\\n\\n3.2.2. Goodness of Pronunciation (GOP)\\nGOP is an established approach to segmental error detection\\n[19], that measures the acoustic quality of phone realization by\\nits posterior probability, which is subsequently normalized and\\naggregated to get a word-level pronunciation score. A GOP\\nscore for a specific phone segment is calculated by taking the\\ndifference of the log probability of the forced alignment of the\\nhypothesis and the log probability of a less constrained recogni-\\ntion phase. Common word-level aggregates are across-phones\\nminimum and mean, and across-frames mean (which weights\\nthe phones by duration). We use the Kaldi implementation of\\nGOP obtained from DNN AM senone probabilities [20]. GOP,\\nwhile being highly effective, is also limited due to the con-\\nsiderable overlap of values across poor and correct pronunci-\\natations [21].\\n\\n3.2.3. Phone recognition features\\nPhone recognition, achieved with a less constrained LM such\\nas a phone bigram, is a promising approach to identifying read-\\ning miscues corresponding to substitutions that are otherwise\\nmasked by the word decoding with strongly constrained LMs.\\nThe mismatch between the uttered word and the corresponding\\ncanonical word pronunciation is measured by the Levenshtein\\nedit distance between the two phone sequences [11]. However,\\nthis requires reliable phone recognition which is typically hard\\nto achieve. We consider using a phone bigram LM with the\\nhybrid AM to obtain the phone sequence of the utterance un-\\nder weak LM constraints. Further, based on the growing re-\\nsearch on SSL pretrained models that shows their effectiveness\\nin capturing the characteristics of basic acoustic units more ac-\\ncurately, and yielding correspondingly superior PER in within-\\nand cross-language scenarios [22], we investigate wav2vec2\\nbased phone recognition. The pretrained model is fine-tuned on\\nour Indian English training datasets using phone alignments of\\nthe manual transcripts as obtained from the hybrid ASR system.\\n\\nWe added a linear layer on top of the wav2vec2-model, aimed at\\nmapping the final encoder layer\u2019s 1024 dimensions to 46 tokens.\\nThese tokens include all the phones, special tokens, and an addi-\\ntional token (*) denoting word boundary, which serves to group\\nphone sequences within words. This facilitates the comparison\\nof the corresponding word-level phone sequences.\\n\\n4. Experiments and Results\\nWe evaluate our first-pass system, followed by the gains\\nachieved for each of the considered second-pass features ap-\\nplied to the decoder segments labeled Correct in the first pass.\\nFinally, we report the performance with the fusion of features\\nvia a trained classifier. Performance is reported in terms of mis-\\ncue detection rate at FPR = 5%. We show DET curves (suitable\\nfor imbalanced data like ours, [23]) in Figure 1 and also report\\nthe AUC (area under the ROC) as another measure of perfor-\\nmance.\\n\\n4.1. First pass performance\\nWe obtain on our test set, the WER with the two LMs: word\\ntrigram trained on (i) manual transcript of the test data, and (ii)\\ncanonical text prompts only. In all cases LM interpolation is\\napplied. The case (i) has the prior knowledge of the reading\\nerrors and therefore provides an upper bound on the first pass\\nperformance as a reference. The case (ii) is the one expected\\nin practice. From Table 3, we see a close to 3% absolute in-\\ncrease in WER in the second case and a much larger increase in\\nthe miss rate with most reading miscues getting decoded as the\\nattempted canonical word (which happens to match the obser-\\nvations of [12]). Our aim in this work is to process the first pass\\noutput in case (ii) to improve the recall of miscues to the extent\\npossible while limiting the FPR to a reasonable value.\\n\\nTable 3:\\nWER and miscue detection performance of Hybrid ASR\\nwith different LM constraints on the test dataset.\\n\\n| Pass 1 LM       | WER   | Miss-Rate | FPR   |\\n|-----------------|-------|-----------|-------|\\n| Trigram on Transcripts | 10.17% | 0.21      | 0.046 |\\n| Trigram on Prompts   | 13.01% | 0.72      | 0.011 |\\n\\n4.2. GOP and lattice based features\\nTable 5 shows the performance of the word-level features de-\\nrived from the hybrid decoder outputs. Of the confidence\\nscores, the posterior performs the best. The GOP measures\\n(including the related LLR measure [20]) do much better, with\\nminimum phone GOP being the best word level feature.\\n\\n4.3. Phone recognition features\\nWe start with a comparison of the phone error rate (PER) of\\nthe hybrid and wav2vec2 systems under matching training data\\nto the extent possible (given that the latter is already SSL pre-\\ntrained). We use both the training datasets to fine-tune the\\nwav2vec2 using the CTC loss function [24]. The CNN feature\\nencoder layer was frozen, while all transformer encoder lay-\\ners were fine-tuned and supplemented with an additional linear\\nlayer. The fine-tuning procedure consisted of 2 epochs using\\nthe IITM dataset, followed by an additional 10 epochs using the\\nWAP dataset. The hybrid system is trained with a phone bigram\\nLM. We test it with the original IITM trained AM, as well as a\"}"}
{"id": "gothi24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"further trained version using the WAP dataset in order to make the training data used comparable with that for the wav2vec2.\\n\\nTable 4: PER on MPS dataset for Hybrid and Wav2vec2 systems with various context constraints and training datasets, where Hybrid ASR's LM phone-bigram is trained on WAP transcripts\\n\\n| Model (Training data) | Test PER(%) |\\n|-----------------------|-------------|\\n| Hybrid (IITM AM, WAP LM) | 24.73 |\\n| Hybrid (IITM + WAP AM, WAP LM) | 18.15 |\\n| Wav2Vec2-XLSR (IITM+WAP) | 9.67 |\\n\\nTable 4 shows the PER on the test set of the phone recognition by the hybrid system and wav2vec2. The wav2vec2 provides 20 ms frame-level logits. The phone sequence is obtained from the non-blank symbol frames across the word using the boundaries obtained in the first pass hybrid decoder. We note that the PER is substantially lower with the wav2vec2 decoding with the same training data, reinforcing past observations that pretrained models learn better discriminant representations in fine-tuning compared to the same applied to the TDNN hybrid [25].\\n\\nWe observed that the wav2vec2 decoded phone sequence was by and large an accurate match to the pronunciation as recorded in the manual transcript. Errors sometimes appear in terms of short subsequences of ground-truth phones replaced entirely with blank symbols in the wav2vec2 output. It is difficult to explain these errors in view of the black-box nature of the model and its weak and implicit language context constraints. Given the observed superior PER of wav2vec2, we employ this output for the phone edit distance feature. Apart from the simple Levenshtein distance (Lev), we leveraged the phone confusion matrix obtained from the WAP training data to compute a cost-weighted phone distance (CostLev) where pairs of confusable phones received a lower substitution cost. Table 5 shows the resulting decrease in the miss rate.\\n\\nWith a view to diminishing the influence of wav2vec2 phone recognition errors on system performance, we investigated the utility of confidence scoring, an area of active research for end-to-end (E2E) systems [26]. While the probability of the best hypothesis is a natural way of estimating confidence, its effectiveness is limited when the probability distribution is skewed towards the best hypothesis (the prediction overconfidence of E2E). We therefore normalize the raw probability values of wav2vec2 frame outputs using temperature scaling and then summarise at frame-level with one of log-max or negative entropy [27]. Finally, a word-level confidence is computed by summing frames across the word. Table 5 shows that the phonetic distance clearly benefits from the confidence measures, with temperature-scaled entropy sum being most successful.\\n\\n4.4. Feature fusion\\n\\nIn the interest of combining features, we use the logistic regression classifier with balanced bagging for ensembling. The MPS dataset was tested in 6-fold CV to obtain the results reported for feature fusion in Table 5. We note that feature fusion is clearly helpful in lowering miss-rate over any one feature category.\\n\\n5. Summary and Conclusion\\n\\nFigure 1 summarises the performance of the miscue detection methods of this work on our new dataset. The leftmost starting point of the curves is the achieved metric of the hybrid system employed for segmentation and initial labeling of canonical words. Our goal was to explore other attainable performance points in the 2d space with the most favourable trade-off between miss-rate and FPR. The posterior, directly derived from the hybrid system word lattice, gives a potential trajectory but one that falls too slowly with increase in FPR. This is not surprising given that the word lattice is expected to be very sparse and therefore not informative in the context of the strongly constrained LM of reading prompts. We see thatGOP improves the trade-off significantly. The similar, or slightly superior, performance is delivered by the wav2vec2 based phonetic distance, especially when accompanied with a confidence score computed on the scaled probabilities. Further, that the features actually contain complementary information about the local acoustic characteristics is borne out by the clear superiority of the fusion curves.\\n\\nFuture work could explore the deeper integration of SSL pretrained models with hybrid networks for this task where both word-level segmentation of the utterance and phone recognition accuracy play equally critical roles.\"}"}
