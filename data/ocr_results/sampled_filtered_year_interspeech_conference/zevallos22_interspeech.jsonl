{"id": "zevallos22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Data Augmentation for Low-Resource Quechua ASR Improvement\\n\\nRodolfo Zevallos1, Nuria Bel1, Guillermo C\u00b4ambara1, Mireia Farr\u00b4us2,3, Jordi Luque4\\n\\n1Universitat Pompeu Fabra (UPF)\\n2Centre de Llenguatge i Computaci\u00b4o (CLiC), Universitat de Barcelona (UB)\\n3Institut de Recerca en Sistemes Complexos (UBICS), Universitat de Barcelona (UB)\\n4Telef\u00b4onica I+D, Research\\n\\nrodolfojoel.zevallos@upf.edu\\n\\nAbstract\\n\\nAutomatic Speech Recognition (ASR) is a key element in new services that helps users to interact with an automated system. Deep learning methods have made it possible to deploy systems with word error rates below 5% for ASR of English. However, the use of these methods is only available for languages with hundreds or thousands of hours of audio and their corresponding transcriptions. For the so-called low-resource languages to speed up the availability of resources that can improve the performance of their ASR systems, methods of creating new resources on the basis of existing ones are being investigated. In this paper we describe our data augmentation approach to improve the results of ASR models for low-resource and agglutinative languages. We carry out experiments developing an ASR for Quechua using the wav2letter++ model. We reduced WER by 8.73% through our approach to the base model. The resulting ASR model obtained 22.75% WER and was trained with 99 hours of original resources and 99 hours of synthetic data obtained with a combination of text augmentation and synthetic speech generation.\\n\\nIndex Terms\\n\\nQuechua, low-resource languages, data augmentation, Automatic Speech Recognition (ASR)\\n\\n1. Introduction\\n\\nLanguage Technologies are at the core of the amazing results of Artificial Intelligence recent success. Deep learning technologies have made it possible to use collections of textual and other language resources to create linguistic tools that are widely used in an increasing number of domains. In particular, dialogue systems help users to interact with a system by just speaking their language, which is creating new services in many areas of daily life. For example, conversational agents are being used for medical attention, user's support in retail, banks and e-commerce applications.\\n\\nIn recent years, deep learning methods are being applied to the development of Automatic Speech Recognition (ASR) systems with excellent results. However, deep learning methods require large amounts of data, i.e. thousands of hours of fully transcribed audios for each language. Languages that do not count with a sufficient amount of these resources have no options to deploy good enough ASR systems. For these so called low-resource languages, the performance of deep learning models created with a limited amount of data lags far behind that of languages such as English, Spanish, Chinese, etc., with very large amounts of data. Since the collection of data requires of a significant amount of effort and funds, there have been different attempts to explore methods of automatically creating new data on the basis of the available resources to achieve the critical amount of data that can improve the performance of ASR systems. Thus, data augmentation (DA) methods are being developed with some success [1, 2, 3, 4]. In this paper we present a DA method to improve the results of ASR models for Southern Quechua. Our method is based on a pragmatic approach combining DA for texts and DA for audios. In DA for texts we generate synthetic texts using the [4] DA method but with a variant on the delexicalisation algorithm. In the DA for audios we propose to use the automatically generated texts to generate synthetic audio with a TTS model. Our research aims to contribute to ongoing efforts to support the development of language technologies for indigenous languages of the Americas, most of which are agglutinative and low-resource languages.\\n\\nThe contributions of our research are: (1) a speech corpus for Southern Quechua of 99 transcribed and cleaned hours; (2) an approach combining DA methods for text and DA for audio to generate better diversity in synthetic audios; (3) a variant of the text DA method for resource-poor and agglutinative languages; and (4) an ASR model for Southern Quechua, which to our knowledge is the first deep learning model with acceptable results.\\n\\n2. Quechua Language\\n\\nQuechua is a family of languages spoken by about 10 million people in South America, mostly in the Andean regions and also in the valleys and plains connecting the Amazon jungle and the Pacific coast. Quechua languages are considered highly agglutinating with a subject-object-verb (SOV) sentence structure as well as mostly postpositional. 130 suffixes are recognized as common to all dialects. The same suffix may have different forms between dialects, but still fulfill the same function. Their orthography has been normalized among all dialects and the proposal by [5] is currently adopted as the Quechua writing standard by the ministries of education both of Peru and Bolivia.\\n\\nAs for phonetics, Southern Quechua can be mainly divided into two dialects: Chanca Quechua, which has a total of 15 consonants, most of them voiceless, and Collao Quechua, which also has a glottal and an aspirated version of each plosive consonant, leading to a total of 25 consonants. Moreover, the use of voiced consonants present in the phonemic inventory of Spanish is common in all Quechua dialects due to the large number of borrowings.\\n\\n3. Related Work\\n\\nIn this section we first review the existing literature for speech systems developed for Quechua, and how they locate concerning the newest advances in the TTS and ASR research areas. Secondly, we introduce the current state of the art in DA.\"}"}
{"id": "zevallos22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Text-to-Speech\\nOver time in the development of text-to-speech conversion, different techniques have been developed. Today, most TTS models are developed using deep learning models. Tacotron [6], a seq2seq architecture for producing magnitude spectrograms from a sequence of characters developed in 2017, simplifies the traditional speech synthesis process by replacing the production of these linguistic and acoustic features with a single neural network trained solely from the training audio. To vocalize the resulting magnitude spectrograms, Tacotron uses the Griffin-Lim algorithm [7] for phase estimation, followed by a short-time inverse Fourier transform. As the authors point out, this was simply a placeholder for future neural vocoder approaches, as Griffin-Lim produces characteristic artifacts and inferior audio quality compared to approaches such as WaveNet. Tacotron 2 [8] is an end-to-end model that combines the Tacotron model with a modified WaveNet vocoder [9], achieving a very natural voice with hours transcribed audio. In this research, as part of the new DA technique, the Tacotron 2 model will be used to create the TTS model for Quechua as it generates a more natural voice and can be used to generate new audios with good quality that can be used for enlarging the ASR training corpus.\\n\\n3.2. Automatic Speech Recognition\\nDeep learning applied to ASR has achieved state-of-the-art performance in a variety of proposals, based on different building blocks like Transformers [10], RNNs [11, 12] or ResNets [13]. From these models we highlight the one proposed in the wav2letter++ paper [14], which is purely convolutional, therefore avoiding the common vanishing/exploding gradients issues from RNNs. Also, the inductive bias in convolutions is helpful in data scarcity scenarios like ours, where Transformer models might underperform due to their need of big data for better generalization [15]. For such reasons, we use the wav2letter++ model for our ASR baselines.\\n\\nAs for Quechua, [16] developed an ASR model exclusively for numbers using Mel-Frequency Cepstral Coefficients (MFCC), Dynamic Time Warping (DTW) and KNN. It achieved a WER of 8.9% with a corpus of 5 hours of transcribed audio. The authors in [17] crowdsourced a corpus of 97.5 hours of fully transcribed audio for Southern Quechua and used it for the construction of a pilot ASR system which achieved a 17.2% WER. This pilot system was developed using the K-Nearest Neighbor (KNN) using the Hidden Markov Model Toolkit [18]. [19] developed a new ASR model based on the monophone HMM topology, a model with five states per HMM and no jumps, that achieved a WER of 12.7%. This model was trained using the corpus of 97.5 hours of [17]. Finally, [20] presented the first attempt to train an ASR model for Quechua with deep learning using the Kaldi tool. The model was trained with a corpus of 18 hours of transcribed audio created by themselves, with a WER of 40.8%.\\n\\n3.3. Data Augmentation\\nAugmentation methods are techniques that generate synthetic data from an existing dataset. They can generate both text and audio data that would help to cope with the problems that low-resource languages meet when trying to use deep learning methods. The DA techniques to generate synthetic text have been explored mostly to improve text classification systems [2], and dialogue and question-answering systems [1]. [4] presented a DA method based on a seq2seq model to generate synthetic text by replacing some words with other words from the same semantic field. Their delexicalisation method replaces segments of a sentence with labels based on their semantic frame. This DA method was successful to improve the accuracy of a language comprehension model for a dialogue system by 6.38%. [3] presented Easy Data Augmentation (EDA) as a set of universal data augmentation techniques. EDA employs four operations (synonym substitution, random insertion, random swapping, and random deletion) to create synthetic texts, improving the experimental classification models by 0.8% accuracy.\\n\\nAudio distortion methods have been evolving to improve data quality. [21] distorted the vocal tract length (VTLP) managing to improve ASR models by 2.5% TER (Token Error Rate). [22] made a substantial improvement in this technique by distorting noise addition, velocity adjustment and pitch shifting in the original audios, managing to reduce WER by 5.1%. DA techniques using a TTS model are also delivering good results. [23] used a TTS model to generate new audios and improve their ASR for low-resource languages. By synthesizing 4.4 hours on the basis of an English movie subtitles corpus, the improvement of their English ASR was 0.81 absolute points. With 11.3 hours of synthesized blog utterances, the improvement was of 0.93 absolute point WER reduction. [24] used a technique combining audio distortion and DA to obtain a 14.8% WER reduction. DA techniques by modifying the ASR model have acquired more and more relevance. [25] presented SpecAugment, a DA method that modifies the spectrogram image of the original audio by masking and image warping techniques, achieving a 6% WER reduction.\\n\\n4. Methodology\\nOur approach consists of three main steps:\\n\u2022 Synthetic text generation: the synthetic text is created using the delexicalisation method proposed by [4] but modifying the delexicalisation algorithm so that it can be used for agglutinative and resource-poor languages.\\n\u2022 Synthetic audio generation: the synthetic text obtained in step 1 is used as input to generate synthetic audio using a TTS model. [8]\\n\u2022 Finally, both synthetic text and synthetic audio are aligned and used together with the original corpus to train an ASR module with wav2letter++. [26]\\n\\n4.1. The Corpora\\nFor our experiments, we used the Siminchik corpus [17], with a total of 97.5 hours of fully transcribed audio, and the Lurin corpus [27], with a total of 83.3 hours (see Table 1) of fully transcribed audio. The Lurin corpus, has 8,000 sentences. The sentences were collected from different sources: the Collao-Spanish and Chanka-Spanish dictionaries published by the Peruvian Ministry of Education (2021), from Soto's Quechua-English functional dictionary (2007) and from the book Autobiograf\u00eda de Condori. We used the Quechua morphological parser and normalizer developed by [27] to convert the corpus into standard Southern Quechua as in (1).\\n\\n(1) Original: Qichwa siminchik kan\\nNormalized: Qichwa simi-nchik ka-n\\nLiteral translation: Quechua mouth-ours is\\n\\nBoth the Siminchik corpus and the Lurin corpus can be found at https://github.com/Llamacha/quechua_resources\"}"}
{"id": "zevallos22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Both corpora were preprocessed to improve the quality of the audios, eliminating excess background noise, audios with no voice, those with background music and those not spoken in Quechua. The latter feature was added thanks to the first automatic speech recognition developed for Quechua [19]. Finally, audios longer than 30 seconds were divided into segments of no more than 30 seconds and transformed to mono channel, 16 kHz sampling, 16-bit precision encoding and WAV format. After cleaning the two resources, our corpus for the experiments contained 123.75 hours that were divided as described in the Table 2.\\n\\n| Corpus          | Train | Dev  | Test | Total |\\n|-----------------|-------|------|------|-------|\\n| Siminchik + Lurin | 150   | 19   | 19   | 188   |\\n| Preprocessed     | 99    | 13   | 12   | 124   |\\n\\nTable 2: Speech Corpus Statistics.\\n\\n4.2. Synthetic text generation\\n\\nWe describe text DA methodology for agglutinative languages based on the study of DA for language understanding (LU) using semantic frames [4]. The methodology is composed of 4 methods: delexicalisation, diversity classification, text generation using a seq2seq model and surface realization.\\n\\nIn our experiments, we used the corpus shown in the table 2 with a total of 8,320 sentences, and a Quechua lexicon [28] that provides us with the labels of the semantic frames. The sentences were preprocessed using a tokenizer, a POS tagger and a lemmatizer for Quechua [27] in order to find the lemmas to be tagged with their corresponding semantic frame. Differently to [4], in case of not finding the semantic frame of the lemmatized word, we used a Quechua-English bilingual dictionary to find the corresponding one for the Quechua lemmatized word in English. This English word is labeled with its semantic frame using the NLTK library. After obtaining the lemmas with their semantic frames, we evaluate the most frequent semantic frames in all sentences. The most frequent semantic frames are labeled following the BIO standard [29]. In our experiment, we found that most of the sentences had the following tags: \\\"month-name\\\", \\\"city-name\\\" and \\\"time-name\\\". An example of the list of tagged words is shown in Table 3.\\n\\n| Quechua Word | Semantic Frame Code | English Word |\\n|--------------|---------------------|--------------|\\n| Qayna        | I-date month-name   | Last         |\\n| p'unchay     | B-date month-name   | day          |\\n| Qusqupim     | B-from loc city-name| in Cuzco     |\\n| wawqiykunata | O                   | to my brothers |\\n| watukurqani  | O                   | I visited you |\\n| kunan        | I-date month-name   | this         |\\n| p'unchaytaq  | B-date month-name   | day          |\\n| Punomanmi    | B-from loc city-name| to Puno      |\\n| risaq        | O                   | I will go to |\\n\\nTable 3: Example of a delexicalised Quechua sentence; also, its translation into English.\\n\\n4.3. Synthetic audio generation\\n\\nWe trained a TTS model for Quechua based on the Tacotron 2 model [8] in order to achieve good performance with 15 hours of transcribed audio recorded by a single speaker from the corpus used in this research (see section 4.1). The model was trained in 1500 epochs with 1 GPU, the learning rate was 0.001, a batch size of 32 was used, the Adam optimizer algorithm was also used and finally we applied a gradient clipping threshold of 0.1. The Quechua TTS model achieved a mean opinion score (MOS) of 3.15% which was obtained by consulting Quechua speakers from southern Peru. We used the Quechua TTS to generate 99 hours of synthetic audio from the 8,320 synthetic sentences (described in section 4.2).\\n\\n4.4. Training of Southern Quechua ASR system\\n\\nThe Quechua ASR model was trained with the wav2letter++ toolkit [26] for 242 epochs. Stochastic gradient descent with Nesterov momentum was used along with a minibatch of 4 utterances. The initial learning rate was set to 0.002 for faster convergence and it was annealed with a constant factor of 1.2 after each epoch, with momentum set to 0. The architecture of the ASR model is the one from the Conv GLU model proposed in the wav2letter++ WSJ recipe [14]. The model was optimized following the Auto Segmentation Criterion (ASG) [14]. The created vocabulary contains 34 graphemes: the standard Spanish alphabet plus the apostrophe, silence and 6 graphemes of vowels with accents and diaeresis, plus a lexicon with all the words of the corpus separated at letter level. The hyperparameters of the architecture, as well as those of the decoder, were adjusted using the validation set. The MFCC features were computed with 13 coefficients, a 25 ms sliding window and a 10 ms interval. First and second order derivatives were included. Power spectrum features were calculated with a 25 ms window, 10 ms interval and 257 components. All features were normalized (mean 0, std 1) per input sequence.\\n\\nThe Southern Quechua ASR model for our experiment was trained with these parameters.\"}"}
{"id": "zevallos22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"trained with 99 hours of original transcribed audio and 99 hours of the synthetic transcribed audio obtained from the 8,320 synthetic sentences. The 8,320 synthetic sentences were added to the Siminchik corpus, as used by [17], to train the ASR language model using the modified Kneser-Ney n-gram model [32]. Given the high number of hapax, a singleton pruning rate with a \u201cK\u201d of 0.04 was used to randomly replace only a \u201cK\u201d fraction of the once-occurring words in the training data with a global UNK symbol. The word-level 4-gram language model developed showed a perplexity of 282.45.\\n\\n4.5. Baselines\\n\\n4.5.1. Original data Baseline\\n\\nAn ASR model with the same parameters as described in the previous section but trained with only the original 99 hours of transcribed audio of our corpus as shown in table 4 was used as a baseline to see WER improvement.\\n\\n4.5.2. Distortion Baseline\\n\\nAs a second baseline, the method of DA through audio distortion proposed by [22] was developed to augment the speech data of the original corpus. The nlpaug library was used to manipulate the training audios (99 hours) by modifying the speed according to a randomly selected coefficient in the range between 0.85 and 1.15, which is where this DA technique performs best. By applying this speed distortion to the training audios, we were able to duplicate the training data to 198 hours of transcribed audio. The ASR model of Quechua was trained with the same configuration as explained in section 4.4.\\n\\n4.5.3. More Data Baseline\\n\\nAs a third baseline, the ASR model was trained following the same steps of the Original Baseline but repeating the 99 hours of transcribed audio, i.e., 198 total hours of original transcribed audio but with repetitions. The language model of [17] was used for this baseline.\\n\\n5. Results\\n\\nFour experiments were conducted to evaluate the new DA method using a seq2seq model to generate synthetic text and a TTS model to generate synthetic speech, in order to improve the result of the ASR model of Southern Quechua as trained with the original data available. The results of the experiments are shown in Table 4.\\n\\n| Data Training Hours | WER (%) |\\n|--------------------|---------|\\n| Original Data      | 99      | 31.5   |\\n| Distorted Data     | 99+99   | 25.1   |\\n| More Data          | 99+99   | 26.1   |\\n| Synthetic Data     | 99+99   | 22.8   |\\n\\nTable 4: Results of the experiments.\\n\\nThe baseline ASR model used 99 hours of transcribed audio as described in section 4.4 achieved a WER of 31.5%. The distortion baseline managed to decrease the WER from 31.5% to 25.1%, that is a relative improvement of 6.4% in WER. The more data baseline achieved a reduction of WER from 31.5% to 26.1%. Therefore, a relative improvement of 5.4% was obtained. However, better results are achieved with the distorted audio baseline. The results of our DA method showed a decrease of the WER of the ASR model from 31.5% to 22.8%. This represents a relative improvement of 8.7% validating that it is the most successful approach.\\n\\n6. Analysis of the results\\n\\nOur DA method was compared with other DA methods to verify whether it could obtain better results for the ASR model of Quechua. Our DA method achieved the best results with a relative improvement of 8.7% WER reduction with respect to the original dataset baseline.\\n\\nOur DA method generates synthetic text and audio that contributes to improve the ASR model of Quechua. With the More Data Baseline experiment we prove that it is not just an effect of having more data, because by duplicating the original data to train the ASR model, the improvement is lower than training the model with synthetic data. Finally, our results also show that synthetic data is a more effective approach than the distortion method that achieved lower WER reduction.\\n\\nIn order to better understand the results of our DA method, we performed two more experiments. We were curious to see the role of the new language model, also trained with synthetic texts, in the results of the ASR system. For that purpose, first, we run an experiment with the ASR model trained only with the original data and with the new Language Model, that is the one trained with also synthetic texts; second, we run another experiment with the ASR model trained with the original and the synthetic data and the old language model, i.e. without synthetic data. The experiment one got 29.7% WER results. Experiment two got 25.6%, proving that the major contribution comes from the synthetic audio created from the seq2seq generated texts.\\n\\nIt is important to mention that we have performed the experiments using the WER metric in order to compare our contribution with other models. In future work we will use the TER (Token error rate) metric, because it evaluates a subword (token) within a word and not the whole word. This is more convenient for agglutinative languages since words are usually composed of subwords (suffixes).\\n\\n7. Conclusions\\n\\nIn this research, a DA method was developed to improve the results of ASR models for agglutinative and low-resource languages such as Quechua. A delexicalisation method for agglutinative languages was built as part of the DA method; furthermore, a language model was developed with the new synthetic texts generated by the DA seq2seq model; also, a TTS based on the Tacotron 2 model was built to generate synthetic audio and finally an ASR based on the wav2letter model was developed. Some experiments were performed with the original training data and with the DA using the different methods in order to compare the performance of the new DA method. The results showed that the DA technique works well for improving speech recognition systems in the case of agglutinative and low-resource languages. In this case, a relative improvement of 8.73% of WER was obtained using the combination of text DA and synthetic speech DA.\\n\\n8. Acknowledgements\\n\\nWe thank all the Quechua speakers and in particular to Alex Peir\u00f3 who collaborated in this research. This work has been partially supported by the Project PID2019-104512GB-I00, Ministerio de Ciencia e Innovaci\u00f3n and Agencia Estatal de Investigaci\u00f3n (Spain), and the INGENIOUS project from the European Union's Horizon 2020 Research and Innovation Program under grant numbers 833435. The third author has been funded by the Agencia Estatal de Investigaci\u00f3n (AEI), Ministerio de Ciencia, Innovaci\u00f3n y Universidades and the Fondo Social Europeo (FSE) under grant RYC-2015-17239 (AEI/FSE, UE).\"}"}
{"id": "zevallos22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. References\\n\\n[1] A. W. Yu, D. Dohan, T. Luong, R. Zhao, K. Chen, and Q. Le, \u201cQanet: Combining local convolution with global self-attention for reading comprehension,\u201d in 6th International Conference on Learning Representations, ICLR, 2018.\\n\\n[2] S. Kobayashi, \u201cContextual augmentation: Data augmentation by words with paradigmatic relations,\u201d in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018.\\n\\n[3] J. Wei and K. Zou, \u201cEda: Easy data augmentation techniques for boosting performance on text classification tasks,\u201d in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, 2019.\\n\\n[4] Y. Hou, Y. Liu, W. Che, and T. Liu, \u201cSequence-to-sequence data augmentation for dialogue language understanding,\u201d in Proceedings of the 27th International Conference on Computational Linguistics. Association for Computational Linguistics, 2018.\\n\\n[5] R. Cerr\u00f3n-Palomino, \u201cQuechua sure\u00f1o, diccionario unificado quechua-castellano, castellano-quechua.\u201d Biblioteca Nacional del Per\u00fa, Lima, 1994.\\n\\n[6] Y. Wang, R. J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. V. Le, Y. Agiomyrgiannakis, R. A. J. Clark, and R. A. Saurous, \u201cTacotron: Towards end-to-end speech synthesis,\u201d in INTERSPEECH, 2017.\\n\\n[7] A. Tamamori, T. Hayashi, K. Kobayashi, K. Takeda, and T. Toda, \u201cSpeaker-dependent wavenet vocoder,\u201d in INTERSPEECH, 2017.\\n\\n[8] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \u201cNatural tts synthesis by conditioning wavenet on mel spectrogram predictions,\u201d 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\\n\\n[9] J. M. R. Sotelo, S. Mehri, K. Kumar, J. F. Santos, K. Kastner, A. C. Courville, and Y. Bengio, \u201cChar2wav: End-to-end speech synthesis,\u201d in ICLR, 2017.\\n\\n[10] Y. Wang, A. Mohamed, D. Le, C. Liu, A. Xiao, J. Mahadeokar, H. Huang, A. Tjandra, X. Zhang, F. Zhang et al., \u201cTransformer-based acoustic modeling for hybrid speech recognition,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\n\\n[11] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro, J. Chen, M. Chrzanowski, A. Coates, G. Diamos, E. Elsen, J. H. Engel, L. Fan, C. Fougner, T. Han, A. Y. Hannun, B. Jun, P. LeGresley, L. Lin, S. Narang, A. Y. Ng, S. Ozair, R. Prenger, J. Raiman, S. Satheesh, D. Seetapun, S. Sengupta, Y. Wang, Z. Wang, C. Wang, B. Xiao, D. Yogatama, J. Zhan, and Z. Zhu, \u201cDeep speech 2: End-to-end speech recognition in english and mandarin,\u201d CoRR, vol. abs/1512.02595, 2015.\\n\\n[12] J. Li, R. Zhao, H. Hu, and Y. Gong, \u201cImproving rnn transducer modeling for end-to-end speech recognition,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019.\\n\\n[13] G. Synnaeve, Q. Xu, J. Kahn, T. Likhomanenko, E. Grave, V. Pratap, A. Sriram, V. Liptchinsky, and R. Collobert, \u201cEnd-to-end asr: from supervised to semi-supervised learning with modern architectures,\u201d arXiv preprint arXiv:1911.08460, 2019.\\n\\n[14] R. Collobert, C. Puhrsch, and G. Synnaeve, \u201cWav2letter: an end-to-end convnet-based speech recognition system,\u201d arXiv preprint arXiv:1609.03193, 2016.\\n\\n[15] S. d'Ascoli, H. Touvron, M. L. Leavitt, A. S. Morcos, G. Biroli, and L. Sagun, \u201cConvit: Improving vision transformers with soft convolutional inductive biases,\u201d in International Conference on Machine Learning, 2021, pp. 2286\u20132296.\\n\\n[16] H. F. C. Chuctaya, R. N. M. Mercado, and J. J. G. Gaona, \u201cIsolated automatic speech recognition of quechua numbers using mfcc, dtw and knn,\u201d International Journal of Advanced Computer Science and Applications, vol. 9, 2018.\\n\\n[17] R. Cardenas, R. Zevallos, R. Baquerizo, and L. Camacho, \u201cSiminchik: A speech corpus for preservation of southern quechua,\u201d in Proceedings of the International Conference on Language Resources and Evaluation (LREC\u201918). European Language Resource Association (ELRA), 2018.\\n\\n[18] S. Young, G. Evermann, M. Gales, T. Hain, D. Kershaw, X. Liu, G. Moore, J. Odell, D. Ollason, D. Povey, A. Ragni, V. Valtchev, P. Woodland, and C. Zhang, \u201cThe HTK Book (version 3.5a),\u201d 2015.\\n\\n[19] R. Zevallos, J. Cordova, and L. Camacho, \u201cAutomatic speech recognition of quechua language using hmm toolkit,\u201d in Information Management and Big Data. Springer International Publishing, 2020.\\n\\n[20] F. Aimituma Suyo and J. C. Carbajal Luna, \u201cConversor de voz a texto para el idioma quechua usando la herramienta de reconocimiento de voz KALDI y una red neuronal profunda,\u201d 2019.\\n\\n[21] A. Ragni, K. Knill, S. P. Rath, and M. J. F. Gales, \u201cData augmentation for low resource languages,\u201d in INTERSPEECH 2014: 15th Annual Conference of the International Speech Communication Association, 2014.\\n\\n[22] Q. Lu, Y. Li, Z. Qin, X. Liu, and Y. Xie, \u201cSpeech recognition using efficientnet,\u201d in Proceedings of the 2020 5th International Conference on Multimedia Systems and Signal Processing. Association for Computing Machinery, 2020.\\n\\n[23] F. Aimituma Suyo and J. C. Carbajal Luna, \u201cUsing Synthesized Speech to Improve Speech Recognition for Low\u2013Resource Languages,\u201d 2015.\\n\\n[24] R. Gokay and H. Yalcin, \u201cImproving low resource turkish speech recognition with data augmentation and tts,\u201d in 2019 16th International Multi-Conference on Systems, Signals Devices (SSD), 2019.\\n\\n[25] D. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. Cubuk, and Q. Le, \u201cSpecaugment: A simple data augmentation method for automatic speech recognition,\u201d 2019.\\n\\n[26] V. Pratap, A. Y. Hannun, Q. Xu, J. Cai, J. Kahn, G. Synnaeve, V. Liptchinsky, and R. Collobert, \u201cWav2letter++: A fast open-source speech recognition system,\u201d IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6460\u20136464, 2019.\\n\\n[27] A. R. Gonzales, \u201cA basic language technology toolkit for quechua,\u201d Procesamiento del Lenguaje Natural, 2016.\\n\\n[28] A. Rudnick, \u201cTowards cross-language word sense disambiguation for quechua,\u201d in RANLP, 2011.\\n\\n[29] L. A. Ramshaw and M. P. Marcus, \u201cText chunking using transformation-based learning,\u201d in Natural language processing using very large corpora. Springer, 1999, pp. 157\u2013176.\\n\\n[30] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush, \u201cOpennmt: Open-source toolkit for neural machine translation,\u201d in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics-System Demonstrations, 2017.\\n\\n[31] T. Luong, H. Pham, and C. D. Manning, \u201cEffective approaches to attention-based neural machine translation,\u201d in Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, 2015.\\n\\n[32] K. Heafield, I. Pouzyrevsky, J. Clark, and P. Koehn, \u201cScalable modified kneser-ney language model estimation,\u201d in Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, 2013.\"}"}
