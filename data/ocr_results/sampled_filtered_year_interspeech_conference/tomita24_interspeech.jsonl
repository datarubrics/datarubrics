{"id": "tomita24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Analysis and Visualization of Directional Diversity in Listening Fluency of World Englishes Speakers in the Framework of Mutual Shadowing\\n\\nYu Tomita\\\\textsuperscript{1}, Yingxiang Gao\\\\textsuperscript{1}, Nobuaki Minematsu\\\\textsuperscript{1}, Noriko Nakanishi\\\\textsuperscript{2}, Daisuke Saito\\\\textsuperscript{1}\\n\\n\\\\textsuperscript{1}The University of Tokyo, \\\\textsuperscript{2}Kobe Gakuin University\\n\\n\\\\{yu.tomita52,gyx,mine,dsk\\\\}@gavo.t.u-tokyo.ac.jp, nakanisi@gc.kobegakuin.ac.jp\\n\\nAbstract\\n\\nEnglish is spoken as a lingua franca with a diversity of pronunciations, called accents, and they have been well studied so far. In this study, a diversity of listening behaviors are focused on, and listening disfluencies are measured objectively while listening to World Englishes (WE). When speaker X listens to Y fluently, it does not always mean that Y listens to X fluently. After collecting different passages read aloud by different WE speakers, the collected oral passages are shadowed by the speakers themselves to quantify their listening disfluencies. Results show that, when X listens to Y, X's listening disfluency becomes larger when Y's pronunciation deviates from X's to a larger degree. Further, a method is proposed to visualize simultaneously a) how fluently a speaker listens to WE speakers and b) how fluently the WE speakers listen to that specific speaker. With this visualization, WE speakers are grouped based on their communicability in global contexts.\\n\\nIndex Terms: Listening disfluency, pronunciation gap, World Englishes, mutual shadowing, communicability chart\\n\\n1. Introduction\\n\\nAs of 2024, more than 1.5 billion people share and speak one language as a tool for global communication, which is English. About a quarter of the people are speaking it as L1, another quarter are speaking it as L2 or one of the official languages in their community, and the remaining majority are speaking English totally as foreign language. Due to this, English spoken in international contexts exhibits various changes from English spoken by native speakers [1, 2, 3, 4]. These changes include pragmatic, semantic, syntactic, morphosyntactic, morphological, and phonological variations. In this paper, the authors focus on phonological variations, called foreign accents. Varieties of English accents have been discussed in studies of phonetics and applied linguistics. In [5], pronunciation changes found in learners' English are described separately for their L1 and in [6], a corpus of World Englishes is provided, where the shared passage was read aloud and recorded by more than 2,000 speakers from more than 150 countries. Those recordings were phonetically transcribed. Recently, a non-native English corpus is released [7], where 250 speakers' recordings are contained, a half of whom are children. Phoneme-level assessment scores are provided in the corpus. In [8], a diversity of pronunciations of World Englishes are visualized from a language learner's viewpoint [8]. Figure 1 shows an example, where the learner is centered at the origin and the pronunciation gap to each of the other learners is represented as the distance from the origin to that learner. Their gender and age are also shown. These research activities concentrate only on diversity in speaking behaviors, but how about diversity in listening behaviors?\\n\\nLearners' listening performance is often assessed in listening comprehension tests, but we can say that what the tests assess is learners' performance of listening as well as guessing after listening. Dictation tests are also used for assessing learners' ability of listening [9]. However, similar drawbacks are claimed again in [10]. Strictly speaking, dictation tests also assess learners' ability of listening and guessing after listening, and only short audio samples are available, because long audio samples cannot be dictated if they are presented only once. Phonetic transcription is a result of researchers' observation of a speaker's behaviors while s/he is speaking. How to observe a listener's behaviors while s/he is listening? In [11, 12, 13], listeners' behaviors, i.e., listening disfluencies, were measured acoustically with a microphone, and that with no brain sensing techniques. The method proposed in [11, 12, 13] is explained in details in the following section. In this paper, for a group of World Englishes speakers, we apply the method to measure a) how fluently a speaker listens to the other speakers as well as b) how fluently the other speakers listen to that specific speaker. In global contexts such as INTERSPEECH conferences, every attendee is surrounded by World Englishes speakers. Quantification of a) and b) for a speaker corresponds to quantification of global communicability of that speaker. It is naturally expected that even when speaker X listens to speaker Y fluently, it does not always mean that Y listens to X fluently. Listening behaviors are directional in nature. With this fact in mind, in the latter of this paper, the results of quantifying each speaker's global communicability are visualized, and based on the visualized results, the authors explain some salient patterns of global communicability found in real World Englishes speakers.\\n\\nAs far as the authors know, this paper is the initial scientific and technical trial to measure a) and b) for individual speakers with no use of brain sensing techniques, to visualize the results, and to group the speakers to show some salient patterns found in their global communicability.\"}"}
{"id": "tomita24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: The first languages of the 28 participants with their typological characteristics\\n\\n| Group A | ID | lang. family | C-chart |\\n|---------|----|--------------|---------|\\n| 1       | CHN| ST           | e       |\\n| 2       | CHN| ST           | a,e     |\\n| 3       | JPN| TU           | a,e     |\\n| 4       | KOR| TU           | a,e     |\\n| 5       | KOR| TU           | a,e     |\\n| 6       | FRA| IE (IT)      | d,e     |\\n| 7       | ITA| IE (IT)      | c,f     |\\n| 8       | HIN| IE (II)      | d,f     |\\n| 9       | SRB| IE (SL)      | c,e     |\\n| 10      | HUN| UR (FU)      | c,e     |\\n\\n| Group B | ID | lang. family | C-chart |\\n|---------|----|--------------|---------|\\n| 1       | CHN| ST           | d,e     |\\n| 2       | CHN| ST           | b,e,f   |\\n| 3       | JPN| TU           | d       |\\n| 4       | KOR| TU           | b,e     |\\n| 5       | FRA| IE (IT)      | c,e     |\\n| 6       | ITA| IE (IT)      | c,e     |\\n| 7       | HIN| IE (II)      | d       |\\n| 8       | UKR| IE (SL)      | c,f     |\\n| 9       | MAL| DR           | d       |\\n\\n| Group C | ID | lang. family | C-chart |\\n|---------|----|--------------|---------|\\n| 1       | CHN| ST           | d,e     |\\n| 2       | CHN| ST           | c,e     |\\n| 3       | JPN| TU           | b,e,f   |\\n| 4       | KOR| TU           | b,e     |\\n| 5       | FRA| IE (IT)      | b,f     |\\n| 6       | SPN| IE (IT)      | d,f     |\\n| 7       | HIN| IE (II)      | b,e     |\\n| 8       | UKR| IE (SL)      | f       |\\n| 9       | HUN| UR (FU)      | c       |\\n\\nLanguages\\n- CHN Chinese\\n- JPN Japanese\\n- KOR Korean\\n- FRA French\\n- ITA Italian\\n- SPN Spanish\\n- HIN Hindi\\n- SRB Serbian\\n- UKR Ukrainian\\n- HUN Hungarian\\n- MAL Malay\\n\\nLang. families\\n- ST Sino-Tibetan\\n- TU Trans-Eurasian\\n- IE Indo-European\\n- UR Uralic\\n- DR Dravidian\\n\\nLang. sub-families\\n- IT Italic\\n- II Indo-Iranian\\n- SL Slavic\\n- FU Finno-Ugric\\n\\n2. Shadowing-based acoustic measurement of listening disfluency\\n\\n2.1. Shadowing as voluntary and vocalized mirroring\\n\\nListening is a mental process and, if one wants to observe the process in a scientific way, s/he may have to use brain sensing techniques as in [14]. Before these techniques became available to researchers, they tried to measure and model what is happening in a listener's mind by imposing a speech task on the listener and by measuring the listener's behaviors of completing the task [15]. In behavioral psychology, various techniques were developed to analyze the listeners' mental states. One of these techniques is shadowing, where a speech sample is presented to a listener, who has to repeat it while listening, and that with as short delay as possible [16]. After brain sensing techniques became available, researchers found shadowing-like neuron activities in human brains, which are mirror neuron activities observed while listening to a given speech [17, 18, 19]. Speech-based neuron mirroring is considered to be involuntary, which seems to copy the movements of articulatory organs required to reproduce the content of the input speech. It is reasonable to regard shadowing as voluntary and vocalized mirroring and mirroring is as involuntary and silent shadowing [20].\\n\\n2.2. Shadowing as a tool to measure listening disfluency\\n\\nIn language education, shadowing was introduced to language classes in 1990s [21] to improve learners in their listening performance. In shadowing exercise, a model speech is presented to learners auditorily, who have to shadow it several times. After that, the text of the model speech is shown to learners, who shadow the speech again but while viewing the text. This type of shadowing is called as script-shadowing in this paper. Then after script-shadowing, only the text is presented and learners read aloud the text without any audio input. Here, M, S, SS, and R are used to refer to model, i-th shadowing, script-shadowing, and reading aloud speeches, respectively. In [11, 12, 13], after converting these audios into their phonetic posteriorgram, the pronunciation gap (PG) from the model to a learner was measured via Dynamic Time Warping (DTW), denoted as PG(M,R). The shadowing disfluency of the learner was also measured as DTW between SS and S1, which was interpreted as listening disfluency (LD) and denoted as LD(SS,S1). This is because in S1, LD happens not rarely but in SS, it never happens.\\n\\nIn the current study, after recording World Englishes speakers' reading alouds, where different speakers read different passages, mutual shadowing exercise is imposed on the speakers. A speaker listens to a variety of World English speakers, who listen to that specific speaker, and their LDs are measured objectively. Using the results, directional behaviors in listening to World Englishes are analyzed to be visualized.\\n\\n3. Data collection\\n\\n3.1. Recruitment and screening of participants\\n\\nParticipants were recruited from Japanese classes at universities in Tokyo, which a large number of non-Japanese international students attended. Additionally, Japanese participants were also recruited from the universities. Screening was made via two steps: 1) to examine their listening skills for General American (GA) English. and 2) to guarantee a diversity of language background of the participants who are finally selected. About a hundred of students replied to our call for participation. The first screening was made by asking all the participants to shadow about a 30-sec oral passage with the GA accent and script-shadow it. The averaged score of LD(SS,S1) was calculated for each participant. In the experiments of this paper, all the GA oral passages were extracted from listening sections of Eiken Grade-2 Tests (EG2) [22]. They are designed to be used for testing high-school students, and Automated Readability Index (ARI) [23] of all the passages extracted for this experiment varied from 6.2 to 7.0, indicating that all the passages were very easy for university students to read. All the materials of EG2 are with the GA accent, which were converted to RP (Received Pronunciation in UK) versions for the main experiment by using a high-performance commercial text-to-speech converter [24]. By referring to the average LD scores and the language profile information of the individual participants, we finally selected 28 students to be divided into three groups of A, B, and C. The numbers of members were 10, 9, and 9, respectively.\\n\\nTable 1 shows all the members in each group and their L1's typological characteristics [25, 26, 27]. While A8, B7, B9, and C7 speak English as an official language in their communities, which is indicated by \u2217 in the table, all the others speak it as foreign language. All of the four speakers are from India.\\n\\n3.2. Recording of World Englishes samples\\n\\nFor the selected 28 participants \\\\{X_i\\\\}, where \\\\(X=\\\\text{A, B, or C,}\\\\) and \\\\(i\\\\) is ID in Table 1, 28 oral passages were extracted from EG2. Since they are reading aloud samples with the GA accent, they are named as \\\\(R_{GA}\\\\). Any \\\\(R_{GA}\\\\) is about 30-sec long and contains about 60 words. By using the commercial speech synthesizer, its UK version, \\\\(R_{RP}\\\\), was also obtained. We had 28 pairs of \\\\(R_{GA}\\\\) and \\\\(R_{RP}\\\\), and to record World Englishes samples, an \\\\(R_{GA}\\\\) was presented to participant \\\\(X_i\\\\), who had to script-shadow it several times for rehearsal. After that, the participant read the text aloud without hearing any audio. Multiple recording was allowed for error-free recording. \\\\(X_i\\\\)'s reading aloud is referred to as \\\\(R_{Xi}\\\\). The pronunciation gap (PG) from GA to \\\\(X_i\\\\) was measured via DTW, denoted as PG(GA,R).\"}"}
{"id": "tomita24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Correlation of LD(\\\\(X_j;R_X_i\\\\)) and PG(\\\\(X_j,X_i;R_X_i\\\\))\\n\\nOne's LD for \\\\(R_X_j\\\\) is as GA accent, RP accent, and one\u2019s own accent\\n\\n| Counts | LD(\\\\(X_j;R_X_i\\\\)) | PG(GA, \\\\(X_i;R_X_i\\\\)) | PG(RP, \\\\(X_i;R_X_i\\\\)) | PG(GA, \\\\(X_i;R_X_i\\\\)) |\\n|--------|-------------------|------------------------|------------------------|------------------------|\\n| A      | Corr. = 0.79      | Corr. = -0.47          | Corr. = 0.24           |\\n| B      | Corr. = 0.85      | Corr. = -0.47          | Corr. = 0.24           |\\n| C      | Corr. = 0.85      | Corr. = -0.47          | Corr. = 0.24           |\\n\\nFigure 2: Recording of shadowing, where \\\\(X_j\\\\) is shadowed \\\\(R_X_j\\\\) twice (S1 and S2), \\\\(X_i\\\\) is listen to \\\\(R_X_i\\\\), and \\\\(X_j\\\\) is read aloud its text (R) in both script-shadowing and reading aloud, multiple recording was already done for error-free recording. Then, \\\\(X_i\\\\) was measured via DTW between \\\\(X_i\\\\) and \\\\(S_1\\\\), which is denoted compactly as LD(\\\\(X_j;R_X_i\\\\)). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, 13], such as configuration for Kaldi [28] was used. How to evaluate the degree of easiness in the pronunciation gap (PG) from GA, RP, and one's own accent? [29] conducted a set of shadowing experiments, after which, the shadower listened to her own shadowed \\\\(X_j\\\\) (SS), and read aloud its text (R). In both experiments, after listening to the input speech, \\\\(X_i\\\\) judge whether the input speech should be judged as difficult to listen to fluently or not. In the current paper, this value is used as threshold to a conclusion that, if the averaged LD score is larger than 1.5, \\\\(X_i\\\\) perceived the input speech as difficult to listen to fluently, and that from \\\\(X_j\\\\) is very likely to be lower than \\\\(X_j\\\\). To calculate PGs and LDs via DTW, the authors followed the experimental setup in [11, 12, "}
{"id": "tomita24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Visualization of global communicability for A\\n\\nWhy this happens? The reason will be clarified by investigating and visualizing each participant's global communicability.\\n\\n4.3. Visualization of global communicability\\n\\nUsing the results of LD and PG, global communicability is visualized for each participant. We call our visualization method as communicability chart or C-chart. Figure 6 shows the C-chart of A_6, who is centered at the origin. While the red and upper semicircle visualize how fluently the members of group A listen to A_6, the blue and lower semicircle does how fluently A_6 listens to them. In both semicircles, LD is represented as distance from the origin (A_6) to each member of group A. As explained in Section 4.1, 1.5 is a valid threshold for LD, and a white circle with its radius being 1.5 is drawn. On the other hand, PG is represented as angle, but no threshold is drawn. It should be noted that, in the red semicircle, LD and PG are measured using only the passage selected for A_6. In the blue semicircle, they are measured using each participant's passage. This is why PGs vary to some degree between the two semicircles. As mentioned in Section 4.1, every participant shadowed his/her own reading aloud, which was recorded several days or a week ago. It is easily assumed that s/he remembered the content of reading aloud to some degree, and within-participant LD and PG for A_6 are shown as 6', not 6 in Figure 6.\\n\\n4.4. Salient patterns found in the 28 C-charts\\n\\nBy visual and careful inspection of the 28 C-charts, we detected some salient patterns found in the C-charts, shown in Figure 7.\\n\\n(a) Intelligible speaker: To this speaker, (almost) all of the members show smaller LD than the threshold. (a) logically derives the opposite pattern, labeled as (\u00aca).\\n\\n(b) Fluent listener: This speaker's LD to (almost) all of the members is smaller than the threshold. (b) also logically derives the opposite pattern, labeled as (\u00acb).\\n\\n(c) Ideal learner: This speaker satisfies both (a) and (b). On the other hand, (\u00acc) is derived, which satisfies both (\u00aca) and (\u00acb).\\n\\n(d) Fluent listener but unintelligible speaker: This speaker satisfies both (b) and (\u00aca), and (d) logically derives the opposite pattern of (\u00acd), disfluent listener but intelligible speaker.\\n\\n(e) Dependent listening: This speaker's LD to a member is proportional to the PG from the speaker to that member. Considering the correlation between LD(X_j; R_{Xi}) and PG(X_j, X_i; R_{Xi}), the third graph in Figure 3, this pattern will be found frequently.\\n\\n(f) Speaker with a very unique accent: This speaker has large PGs, or large angles to the other members in both semicircles. As shown in Figure 3, this speaker tends to be labeled as (\u00aca). In Table 1, the above labels, (x) and (\u00acy), are assigned to each participant. One participant can have multiple labels.\\n\\n4.5. Discussion on the C-chart labels of the 28 participants\\n\\nThe number of intelligible speakers, (a), (c), or (\u00acd), are highest in A, and that of speakers with a unique accent (f) is lowest in A. It seems that A is the most communicable group. Participants labeled as (\u00acc) are poor at communicating with World Englishes speakers although they listen to GA English smoothly. Participants labeled as (d) and (\u00acd) have unbalanced skills of speaking and listening. In B, six of the nine participants are of (\u00acc), (d), or (\u00acd). This is considered to be why negative correlation is found in B. In all the groups, only three ideal learners of (c) are found, who are A_9, A_10, and C_9. It is very interesting that they are from Hungary or Serbia, which are located next to each other in Eastern Europe. According to the questionnaire, they never stayed in foreign countries before grown-up. Out of the four speakers of English as official language, three of them are labeled as (d). Even native speakers might be unintelligible to non-native speakers when the native speakers' regional accents are strong and unfamiliar. As expected from the third graph of Figure 3, more-than-half participants are labeled as (e).\\n\\n5. Conclusions and future work\\n\\nIn the framework of mutual shadowing among World Englishes speakers, each speaker's global communicability was measured and visualized with C-chart. Using the results, directional properties of the global communicability were discussed. However, the following issues remain to be solved. In this study, differences of prosodic control between two speakers were ignored at all, although speech prosody influences intelligibility [30]. PG should be redefined in a different way. Further, we could not collect data from participants from Africa or North and South America. More extensive collection of data is desirable.\"}"}
{"id": "tomita24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] B. B. Kachru, Y. Kachru, and C. L. Nelson, The Handbook of World Englishes, ser. Blackwell Handbooks in Linguistics. John Wiley & Son, 2009.\\n\\n[2] J. Jenkins, Global English: A resource book for students. Routledge, 2014.\\n\\n[3] H. A. Manzouri, Z. Tajeddin, and G. R. Kiany, \u201cTeachers\u2019 and learners\u2019 beliefs about World Englishes, EIL, and ELF: A systematic review,\u201d Asian Englishes, pp. 1\u201317, 2024.\\n\\n[4] R. Hickey, A dictionary of varieties of English. Wiley-Blackwell, 2014.\\n\\n[5] M. Swan, Learner English: A teacher\u2019s guide to interference and other problems. Cambridge University Press, 2001.\\n\\n[6] S. Weinberger, Speech Accent Archive, http://accent.gmu.edu.\\n\\n[7] J. Zhang, Z. Zhang, Y. Wang, Z. Yan, Q. Song, Y. Huang, K. Li, D. Povey, and Y. Wang, \u201cSpeechocean762: An open-source non-native English speech corpus for pronunciation assessment,\u201d in Proc. INTERSPEECH, 2021, pp. 3710\u20133714.\\n\\n[8] Y. Kawase, N. Minematsu, D. Saito, and K. Hirose, \u201cVisualization of pronunciation diversity of World Englishes from a speaker\u2019s self-centered viewpoint,\u201d in Proc. O-COCOSDA, 2014, pp. 1\u20135.\\n\\n[9] R. I. Thomson, \u201cMeasurement of accentedness, intelligibility, and comprehensibility,\u201d in Assessment in Second Language Pronunciation, O. Kang and A. Ginther, Eds. Routledge, 2017, pp. 11\u201329.\\n\\n[10] M. J. Munro and T. M. Derwing, \u201cForeign accent, comprehensibility and intelligibility, redux,\u201d Journal of Second Language Pronunciation, vol. 6, no. 3, pp. 283\u2013309, 2020.\\n\\n[11] C. Zhu, R. Hakoda, D. Saito, N. Minematsu, N. Nakanishi, and T. Nishimura, \u201cMulti-granularity annotation of instantaneous intelligibility of learners\u2019 utterances based on shadowing techniques,\u201d in Proc. Automatic Speech Recognition and Understanding, 2021, pp. 1071\u20131078.\\n\\n[12] T. Kunihara, C. Zhu, N. Minematsu, and N. Nakanishi, \u201cGradual improvements observed in learners\u2019 perception and production of L2 sounds through continuing shadowing practices on a daily basis,\u201d in Proc. INTERSPEECH, 2022, pp. 1303\u20131307.\\n\\n[13] N. Minematsu, N. Nakanishi, Y. Gao, and H. Sun, \u201cA unified framework to improve learners\u2019 skills of perception and production based on speech shadowing and overlapping,\u201d in Proc. INTERSPEECH, 2023, pp. 3667\u20133668.\\n\\n[14] A. S. Ihara, A. Matsumoto, S. Ojima, J. Katayama, K. Nakamura, Y. Yokota, H. Watanabe, and Y. Naruse, \u201cPrediction of second language proficiency based on electroencephalographic signals measured while listening to natural speech,\u201d Frontiers in Human Neuroscience, vol. 15, 2021.\\n\\n[15] W. D. Pierce and C. D. Cheney, Behavior Analysis and Learning. Psychology Press, 2014.\\n\\n[16] W. D. Marslen-Wilson, \u201cSpeech shadowing and speech comprehension,\u201d Speech Communication, vol. 4, pp. 55\u201373, 1985.\\n\\n[17] S. M. Wilson, A. P. Saygin, M. I. Sereno, and M. Iacoboni, \u201cListening to speech activates motor areas involved in speech production,\u201d Nature Neuroscience, vol. 7, no. 7, pp. 701\u2013702, 2004.\\n\\n[18] F. Pulverm\u00fcller, M. Huss, F. Kherif, F. M. del Prado Martin, O. Hauk, and Y. Shtyrov, \u201cMotor cortex maps articulatory features of speech sounds,\u201d in Proc. National Academy of Sciences of the United States of America, vol. 103, 2006, pp. 7865\u20137870.\\n\\n[19] G. B. Cogan, T. Thesen, C. Carlson, W. Doyle, O. Devinsky, and B. Pesaran, \u201cSensory-motor transformations for speech occur bilaterally,\u201d Nature, vol. 507, pp. 94\u201398, 2014.\\n\\n[20] S. Kadota, Shadowing as a Practice in Second Language Acquisition: Connecting Inputs and Outputs. Routledge, 2019.\\n\\n[21] K. Tamai, \u201cThe effect of shadowing on listening comprehension,\u201d in The Study of current English, vol. 36, 1997, pp. 105\u2013116.\\n\\n[22] E. F. of Japan, EIKEN English Proficiency Test, https://www.eiken.or.jp/eiken/.\\n\\n[23] R. Senter and E. Smith, \u201cAutomated Readability Index,\u201d AMRL TR, pp. 1\u201314, 1967.\\n\\n[24] IIElevenLabs, https://elevenlabs.io.\\n\\n[25] T. Cole and E. Siebert-Cole, Family Tree of LANGUAGES \u2013 Part 1: INDO-EUROPEAN, 2024.\\n\\n[26] \u2014\u2014, Family Tree of LANGUAGES \u2013 Part II: American, Trans-eurasian, Sino-Tibetan, Hmong-Mien, Kra-Dai, Austro-Asiatic, Austronesian, 3 2022.\\n\\n[27] \u2014\u2014, Family Tree of LANGUAGES \u2013 Part 3: African, Dravidian, Uralic, Caucasian, Afro-Asiatic, 3 2024.\\n\\n[28] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motl\u00ed\u010dek, Y. Qian, P. Schwarz, J. Silovsk\u00fd, G. Stemmer, and K. Vesel\u00fd, \u201cThe Kaldi speech recognition toolkit,\u201d in Proc. ASRU, 2011.\\n\\n[29] J. Choi, L. Zhand, Y. Gao, N. Minematsu, D. Saito, and N. Nakanishi, \u201cShadowing-based subjective annotation of semantic listening disfluency measured while listening to L2 speech,\u201d in Proc. Speech Research Meeting, The Acoustical Society of Japan, 2024, pp. 1\u20136.\\n\\n[30] P. Degrave, \u201cThe contribution of prosody to intelligibility, comprehensibility and accentedness in foreign language acquisition,\u201d Nederlandse Taalkunde, vol. 26, no. 2, 2021.\"}"}
