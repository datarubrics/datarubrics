{"id": "bhogale24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi, A. Baevski, Y. Adi, X. Zhang, W. Hsu, A. Conneau, and M. Auli, \\\"Scaling speech technology to 1,000+ languages,\\\" CoRR, vol. abs/2305.13516, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.13516\\n\\n[2] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang, Z. Meng, K. Hu, A. Rosenberg, R. Prabhavalkar, D. S. Park, P. Haghani, J. Riesa, G. Perng, H. Soltau, T. Strohman, B. Ramabhadran, T. Sainath, P. Moreno, C.-C. Chiu, J. Schalkwyk, F. Beaufays, and Y. Wu, \\\"Google usm: Scaling automatic speech recognition beyond 100 languages,\\\" 2023.\\n\\n[3] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \\\"Robust speech recognition via large-scale weak supervision,\\\" in Proceedings of the 40th International Conference on Machine Learning, ser. ICML\u201923. JMLR.org, 2023.\\n\\n[4] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \\\"Librispeech: An ASR corpus based on public domain audio books,\\\" in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2015, South Brisbane, Queensland, Australia, April 19-24, 2015. IEEE, 2015, pp. 5206\u20135210. [Online]. Available: https://doi.org/10.1109/ICASSP.2015.7178964\\n\\n[5] K. S. Bhogale, A. Raman, T. Javed, S. Doddapaneni, A. Kunchukuttan, P. Kumar, and M. M. Khapra, \\\"Effectiveness of mining audio and text pairs from public data for improving ASR systems for low-resource languages,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing ICASSP 2023, Rhodes Island, Greece, June 4-10, 2023. IEEE, 2023, pp. 1\u20135. [Online]. Available: https://doi.org/10.1109/ICASSP49357.2023.10096933\\n\\n[6] Q. Wang and T. Breckon, \\\"Unsupervised domain adaptation via structured prediction based selective pseudo-labeling,\\\" in Proceedings of the AAAI conference on artificial intelligence, vol. 34, no. 04, 2020, pp. 6243\u20136250.\\n\\n[7] B. C. Benato, A. C. Telea, and A. X. Falc \u02dcao, \\\"Iterative pseudo-labeling with deep feature annotation and confidence-based sampling,\\\" in 2021 34th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI), 2021, pp. 192\u2013198.\\n\\n[8] P. Cascante-Bonilla, F. Tan, Y. Qi, and V. Ordonez, \\\"Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning,\\\" in Proceedings of the AAAI conference on artificial intelligence, vol. 35, no. 8, 2021, pp. 6912\u20136920.\\n\\n[9] D. S. Park, Y. Zhang, Y. Jia, W. Han, C.-C. Chiu, B. Li, Y. Wu, and Q. V. Le, \\\"Improved noisy student training for automatic speech recognition,\\\" arXiv preprint arXiv:2005.09629, 2020.\\n\\n[10] Q. Xu, T. Likhomanenko, J. Kahn, A. Hannun, G. Synnaeve, and R. Collobert, \\\"Iterative pseudo-labeling for speech recognition,\\\" arXiv preprint arXiv:2005.09267, 2020.\\n\\n[11] T. Likhomanenko, Q. Xu, J. Kahn, G. Synnaeve, and R. Collobert, \\\"slimipl: Language-model-free iterative pseudo-labeling,\\\" arXiv preprint arXiv:2010.11524, 2020.\\n\\n[12] D.-H. Lee et al., \\\"Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks,\\\" in Workshop on challenges in representation learning, ICML, vol. 3, no. 2. Atlanta, 2013, p. 896.\\n\\n[13] A. Laptev and B. Ginsburg, \\\"Fast entropy-based methods of word-level confidence estimation for end-to-end automatic speech recognition,\\\" in 2022 IEEE Spoken Language Technology Workshop (SLT), 2023, pp. 152\u2013159.\\n\\n[14] P.-A. Duquenne, H. Schwenk, and B. Sagot, \\\"Sonar: sentence-level multimodal and language-agnostic representations,\\\" arXiv e-prints, pp. arXiv\u20132308, 2023.\\n\\n[15] K. S. Bhogale, S. Sundaresan, A. Raman, T. Javed, M. M. Khapra, and P. Kumar, \\\"Vistaar: Diverse benchmarks and training sets for Indian language ASR,\\\" CoRR, vol. abs/2305.15386, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2305.15386\\n\\n[16] L. Lugosch, T. Likhomanenko, G. Synnaeve, and R. Collobert, \\\"Pseudo-labeling for massively multilingual speech recognition,\\\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7687\u20137691.\\n\\n[17] Y. Chen, W. Ding, and J. Lai, \\\"Improving noisy student training on non-target domain data for automatic speech recognition,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[18] R. N. Nandi, M. H. Menon, T. A. Muntasir, S. Sarker, Q. S. Mustaseem, M. T. Islam, S. A. Chowdhury, and F. Alam, \\\"Pseudo-labeling for domain-agnostic bangla automatic speech recognition,\\\" arXiv preprint arXiv:2311.03196, 2023.\\n\\n[19] T. Javed, J. A. Nawale, E. I. George, S. Joshi, K. S. Bhogale, D. Mehendale, I. V. Sethi, A. Ananthanarayanan, H. Faquih, P. Palit, S. Ravishankar, S. Sukumaran, T. Panchagnula, S. Murali, K. S. Gandhi, A. R, M. K. M, C. V. Vaijayanthi, K. S. R. Karunganni, P. Kumar, and M. M. Khapra, \\\"Indicvoices: Towards building an inclusive multilingual speech dataset for Indian languages,\\\" 2024.\\n\\n[20] N. R, M. S, J. F, A. Gangwar, M. N. J, S. Umesh, R. Sarab, A. K. Dubey, G. Divakaran, S. V. K, and S. V. Gangashetty, \\\"SPRING-INX: A multilingual Indian language speech corpus by SPRING lab, IIT madras,\\\" CoRR, vol. abs/2310.14654, 2023. [Online]. Available: https://doi.org/10.48550/arXiv.2310.14654\\n\\n[21] T. Javed, S. Doddapaneni, A. Raman, K. S. Bhogale, G. Ramesh, A. Kunchukuttan, P. Kumar, and M. M. Khapra, \\\"Towards building ASR systems for the next billion users,\\\" in Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022. AAAI Press, 2022, pp. 10 813\u201310 821. [Online]. Available: https://doi.org/10.1609/aaai.v36i10.21327\\n\\n[22] A. Gulati, J. Qin, C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \\\"Conformer: Convolution-augmented transformer for speech recognition,\\\" in Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, H. Meng, B. Xu, and T. F. Zheng, Eds. ISCA, 2020, pp. 5036\u20135040. [Online]. Available: https://doi.org/10.21437/Interspeech.2020-3015\\n\\n[23] F. Boyer, Y. Shinohara, T. Ishii, H. Inaguma, and S. Watanabe, \\\"A study of transducer based end-to-end ASR with espnet: Architecture, auxiliary loss and decoding strategies,\\\" CoRR, vol. abs/2201.05420, 2022. [Online]. Available: https://arxiv.org/abs/2201.05420\\n\\n[24] O. Kuchaiev, J. Li, H. Nguyen, O. Hrinchuk, R. Leary, B. Ginsburg, S. Kriman, S. Beliaev, V. Lavrukhin, J. Cook et al., \\\"Nemo: a toolkit for building ai applications using neural modules,\\\" arXiv preprint arXiv:1909.09577, 2019.\"}"}
{"id": "bhogale24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling\\n\\nKaushal Santosh Bhogale1,2, Deovrat Mehendale2, Niharika Parasa2, Sathish Kumar Reddy G1,2, Tahir Javed1,2, Pratyush Kumar3, Mitesh M. Khapra1,2\\n\\n1Indian Institute of Technology Madras, India\\n2AI4Bharat, India\\n3Sarvam AI, India\\n\\ncs22d006@cse.iitm.ac.in, deovrat.mehendale@gmail.com, niharikasri.parasa@iiitb.ac.in, {gsathish8421,tahirjmakhdoomi}@gmail.com, {pratyush,miteshk}@cse.iitm.ac.in\\n\\nAbstract\\nIn this study, we tackle the challenge of limited labeled data for low-resource languages in ASR, focusing on Hindi. Specifically, we explore pseudo-labeling, by proposing a generic framework combining multiple ideas from existing works. Our framework integrates multiple base models for transcription and evaluators for assessing audio-transcript pairs, resulting in robust pseudo-labeling for low resource languages. We validate our approach with a new benchmark, INDICYT, comprising diverse YouTube audio files from multiple content categories. Our findings show that augmenting pseudo labeled data from YouTube with existing training data leads to significant performance improvements on INDICYT, without affecting performance on out-of-domain benchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR capabilities for low-resource languages. The benchmark, code and models developed as a part of this work will be made publicly available.\\n\\nIndex Terms: multilingual speech recognition, pseudo labeling, low-resource languages\\n\\n1. Introduction\\nRecent advancements in ASR have led to significant improvements for English, with single digit WERs on multiple benchmarks [1, 2, 3]. These English models are typically trained on a large amount of labelled data (at least 1000 hours [4]). However, such large scale high quality datasets are not available for low resource languages, such as, Hindi. While there are recent efforts in building labelled datasets for Indian languages, these efforts are still nascent with the amount of labelled data much lesser than that used for training large-scale English models. One alternative to overcome this challenge is to use weakly supervised data by aligning transcripts and audio files available on public platforms like YouTube [3, 5]. However, for low resource languages such data is again very sparse. Another alternative is to use pseudo-labeling wherein an existing high quality ASR model is used to transcribe readily available raw audio, with the resulting audio-transcript pairs added to the original training data.\\n\\nIn this work, we study pseudo labeling in the context of a low resource language, viz., Hindi. While doing so, we propose a generic framework which combines ideas from existing works [6, 7, 8, 9, 10, 11, 12] to provide an effective way of generating pseudo labeled data for low resource languages where a high quality base model may not be available to begin with. Our framework allows for multiple base models as pseudo transcribers so that an agreement between these weak transcribers can be used to improve the robustness of the generated pseudo labeled data. We further allow for evaluators which can assign a score to the generated audio-transcript pairs thereby allowing a mechanism to reject pairs with scores below a certain threshold. We acknowledge multiple such evaluators used in existing works [13, 14] and consider two types of evaluators in our work: one which relies on confidence scores of ASR models and another which relies on similarity scores computed using robust multimodal embeddings such as SONAR [14] which map audio and transcripts in a common multimodal embedding space. Our experiments show that while individual pseudo-transcribers and evaluators may not be adequate, using a combination of them works well in low resource settings.\\n\\nNext, to evaluate the effectiveness of this framework, we create a robust benchmark called INDICYT by collecting audio files from YouTube which are available with a permissible CC-BY-4.0 license. These audio files are sourced from multiple channels corresponding to a diverse set of domains such as business, education, health, science, sports, cooking and so on. We show that starting with a base model trained on multiple data sources for Hindi, we can significantly improve the performance on INDICYT by augmenting the original training data with pseudo labeled data generated using our framework. These performance gains are consistent across all domains in INDICYT. Further, we also show that adding data from one source/genre (i.e., YouTube) does not affect the performance on out-of-domain benchmarks. To do so we evaluate the model on the comprehensive VISTAAR [15] benchmark which contains test sets from multiple sources/domains. Our main contributions can thus be summarized as:\\n\\n\u2022 A robust framework for pseudo labeling for low resource languages\\n\u2022 A new diverse benchmark, INDICYT, for evaluating ASR models for Hindi on multiple content categories from YouTube\\n\u2022 A state of the art ASR model for Hindi, which performs well on INDICYT as well as existing benchmarks.\\n\\n2. Related Work\\nSeveral prior works [6, 7, 8] have used pseudo-labeling as a semi-supervised learning technique for improving ASR systems. These methods typically use a combination of the following stages (i) generate pseudo labels (ii) refine/filter the pseudo-labeled data (iii) retrain the model and (iv) iterate. For example, Noisy Student Training [9] filters the augmented pseudo-labeled data using a Language model (LM) between self-training iterations. Iterative Pseudo-labeling (IPL) [10] performs multiple iterations of pseudo-labeling on the unlabeled data as the acoustic model evolves. SlimIPL [11] improves upon the IPL algorithm by iteratively regenerating transcripts by feeding them back into the acoustic model. More recently, IMM [12] further improves upon the SlimIPL algorithm by using multiple streams in the decoder of the acoustic model. Our framework builds upon these ideas and proposes a generic way of generating pseudo labeled data for low resource languages.\"}"}
{"id": "bhogale24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pseudo-labeling has also been extended to multilingual ASR [16], by (i) training a supervised multilingual model, (ii) performing monolingual finetuning to get language specific models (iii) generate pseudo-labels using monolingual models and then (iv) train a final multilingual model using pseudo-labels from all languages. To improve quality of pseudo-labeling on non target domain data, [17] propose a data selection strategy using an LM, while [18] uses two expert models and metrics based on confidence score and min-max audio duration to filter data.\\n\\nIn this work, we combine ideas from prior work and propose a generic framework for pseudo-labeling.\\n\\n3. Pratinidhi: A generic framework for large-scale pseudo-labeling\\n\\nIn this section, we present Pratinidhi, a generic framework to perform pseudo-labeling on large-scale datasets by combining ideas from several existing works.\\n\\n3.1. The Framework\\n\\nThe framework consists of Pseudo-Transcribers and Evaluators, which participate in labeling and filtering, respectively. Given access to \\\\( N \\\\) Pseudo-Transcribers and \\\\( K \\\\) Evaluators, the framework tries to estimate the true transcript \\\\( t^* \\\\) of an unlabeled audio segment \\\\( X \\\\). We next describe the components of our framework.\\n\\nPseudo-Transcribers.\\n\\nPseudo-transcribers are trained ASR models that can act as base models to generate pseudo-transcripts \\\\( t \\\\) for an unlabeled audio segment. We propose using multiple pseudo-transcribers to improve quality of pseudo-labels. In practice, multiple such pseudo-transcribers can be obtained by training models with different architectures on the same dataset, or by training models on different domains, or by accessing open-source models or public speech-to-text APIs.\\n\\nEvaluators.\\n\\nEvaluators are metrics that assign a score to pseudo-transcripts. Specifically, an Evaluator \\\\( g(\\\\cdot) \\\\) estimates a score \\\\( s = g(t, X) \\\\) where \\\\( t \\\\) is a pseudo-transcript for an unlabeled audio segment, \\\\( X \\\\). We identify two types of metrics that can be used as evaluators: one which relies on confidence scores of ASR models [13] and another which relies on similarity scores computed using multimodal audio/sentence embeddings [14].\\n\\nPseudo-Labeling.\\n\\nGiven a set of \\\\( N \\\\) pseudo-transcripts \\\\( \\\\{t_1, t_2, \\\\ldots, t_N\\\\} \\\\), the pseudo-labeling algorithm returns \\\\( t_{\\\\text{max}} \\\\), the candidate that has maximum agreement with other pseudo-transcript candidates. We first calculate a text matching score between two texts \\\\( p \\\\) and \\\\( q \\\\), given by\\n\\n\\\\[\\nM(p, q) = 1 - \\\\frac{LD(p, q)}{|p| + |q|}\\n\\\\]\\n\\nwhere \\\\( LD \\\\) is the Levenshtein distance between the two texts and \\\\( |\\\\cdot| \\\\) denotes the length of the text, in characters. Then, for each \\\\( t_i \\\\), we define Agreement -\\n\\n\\\\[\\nA_{ki} = \\\\begin{cases} \\n1, & \\\\text{if } M(t_i, t_k) \\\\geq \\\\tau_0 \\\\\\\\\\n0, & \\\\text{otherwise} \\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( \\\\tau \\\\) is a hyperparameter. Next, we calculate the agreement score\\n\\n\\\\[\\nC_i = \\\\prod_{k=1}^{N} A_{ni}\\n\\\\]\\n\\nFinally, the pseudo-transcript \\\\( t_{\\\\text{acpt}} \\\\) is considered as accepted if\\n\\n\\\\[ C_i > \\\\delta \\\\]\\n\\nIf multiple pseudo-transcripts satisfy the threshold, then the transcript with the maximum \\\\( C_i \\\\) is selected, with ties broken arbitrarily. If no transcript satisfies the above condition then this audio is not considered for pseudo labeling.\\n\\nFiltering.\\n\\nAfter obtaining an initial pseudo-transcript candidate \\\\( t_{\\\\text{acpt}} \\\\) for an audio segment \\\\( X \\\\), it is evaluated by \\\\( K \\\\) Evaluators. For each evaluator \\\\( k \\\\in K \\\\), we first calculate,\\n\\n\\\\[\\nB(t_{\\\\text{acpt}}, k) = \\\\begin{cases} \\n1, & \\\\text{if } g_k(t_{\\\\text{acpt}}, X) \\\\geq \\\\rho_k \\\\\\\\\\\\n0, & \\\\text{otherwise} \\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( \\\\rho_k \\\\) is a hyperparameter specific to each evaluator. We define the filter score\\n\\n\\\\[\\nF(t_{\\\\text{acpt}}) = \\\\prod_{k=1}^{K} B(t_{\\\\text{acpt}}, k)\\n\\\\]\\n\\nFinally, the pseudo-transcript \\\\( t_{\\\\text{acpt}} \\\\) is considered as a proxy for the true transcript if \\\\( F(t_{\\\\text{acpt}}) > \\\\lambda \\\\), else the audio is not considered for pseudo-labelling.\\n\\n3.2. Application to a Low-resource Indian Language\\n\\nWe show that our framework is particularly useful for low-resource languages. In this paper, we focus on Hindi, a language used by a population exceeding 600 million people, yet falling short in ASR resources when compared to English. Next, we describe the problem setting for Hindi including labeled datasets, unlabeled datasets, Pseudo-Transcribers and Evaluators. For convenience we abbreviate Pratinidhias PN below.\\n\\nPN-lab.\\n\\nTo train ASR models that can act as Pseudo-Transcribers, we first collate all publicly available labeled datasets for Hindi. Specifically, we extend the previous effort of VISTAAR [15], which collated 2150 hours of diverse training data for Hindi. We add two recently released Indian language datasets to their collection, viz., INDIC VOICES [19] and SPRING-INX [20], containing 65 hours and 316 hours of training data for Hindi, respectively. The total size of our combined training data is 2531 hours. We refer to it as PN-lab.\\n\\nPN-unlab.\\n\\nWe identify an unlabeled dataset available for Hindi - Dhwani [21]. It contains 1075 hours of audio from YouTube. In order to increase the scale of the dataset further, we collect publicly accessible videos from YouTube. We make use of the YouTube Search API to automate the process of discovering videos. The search terms were crafted carefully to ensure diversity in domains in the collected data. We follow the data preprocessing steps described in [21]. After filtering and data preprocessing, we obtain a corpus of 28616 hours of unlabeled audio. We refer to it as PN-unlab.\\n\\nPseudo-Transcribers.\\n\\nTo obtain base models, we train a Conformer-L [22] model with a hybrid CTC-RNNT [23] decoder on PN-lab. We can derive two ASR models from this single training, one using the CTC head and another using the RNNT head of the hybrid model. We first evaluate the efficacy of these models on the Vistaar benchmark. As shown in Table 2, in comparison to other publicly available and open-source models like Google USM, Whisper, MMS, IndicWhisper, we obtain improvement of 1.7 point in WER, thus establishing it as a good base model.\\n\\nEvaluators.\\n\\nWe use two evaluators. The first is linearly normalized Renyi entropy-based confidence [13] of the RNN-T...\"}"}
{"id": "bhogale24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model, as this was shown to be effective in our initial experiments. The second is SONAR similarity [14], which computes cosine similarity using multimodal audio/sentence embeddings. Our choice of SONAR is motivated by the fact that SONAR already supports 14 Indian languages, and can be extended to other low-resource Indian languages.\\n\\n3.3. PN-pseudolab\\n\\nUsing the Pratinidhi framework, we perform pseudo-labeling on PN-unlab using the RNN-T and CTC models as pseudo-transcribers. We first perform pseudo-labeling with $\\\\tau = 1$ and $\\\\delta = 1$, which is equivalent to accepting the pseudo-label if there was an exact match in the transcripts of the two models. Next, we run the filtering algorithm with $\\\\rho_{\\\\text{SONAR}} = 0.8$ and $\\\\rho_{\\\\text{RNNT}} = 0.7$. The thresholds were chosen based on manually verifying the quality of accepted pseudo-transcripts on a set of samples. We used $\\\\lambda = 2$ indicating that samples were accepted when threshold criteria was met for both SONAR and RNN-T. Starting with a set of 28616 hours of unlabeled data, we get 4862 hours after pseudo-labeling (the rest gets rejected as the transcripts of the 2 models do not match). Consequently, we get 1840 hours after performing the filtering using the two evaluators. Next, we use the combined set of PN-lab and PN-pseudolab, called PN, to retrain the base models and report results in Section 5. While the framework allows for multiple iterations of the above process, in this work we only perform one iteration.\\n\\n4. INDICYT Benchmark\\n\\nTo allow for a robust evaluation of Hindi ASR models across multiple domains, we create and release a benchmark called INDICYT. This benchmark was created by first curating diverse audio data with CC-BY-4.0 license from YouTube and then transcribing a 2 hour subset of this data with the help of human annotators, ensuring high quality. Our process involved the following steps:\\n\\nIdentification of topics for collection. In order to maximize the diversity of the curated content, our search process began with identifying a list of categories that can be used by our experts to search for rich content on YouTube. We defined 3 major categories, viz., How-To-Do videos, Household activities, News media. The motivation for choosing these categories was to ensure the expansion of this search approach to any medium/low resource language. Once the broad categories were fixed, we created a list of subcategories which can be directly used to query YouTube for content. Table 1 shows the curated list of domains and their sub-categories.\\n\\nCollection. We recruited language experts proficient in Hindi to do this task. Each language expert was asked to find at least 35 videos from a particular category, having CC-BY-4.0 license. The experts were instructed not to collect more than 3 videos from a single channel to ensure speaker diversity.\\n\\nTranscription. Next, we used Shoonya as a platform to transcribe a two hour subset of collected data and transcribed it using the same guideline as discussed in [19]. Before transcription, the videos were converted into audios and passed through Voice activity detection (VAD) to extract the voiced segments from the audios. Next, the subset selection was done by random sampling one VAD segment for each audio, resulting in a total of 2 hours of benchmark data.\\n\\nStatistics. Table 1 shows some statistics about INDICYT. Specifically, it shows the number of minutes of transcribed data for each category, the number of channels from which the data was sourced and the number of utterances/segments as identified by VAD.\\n\\nTable 1: Data Statistics\\n\\n| Domain                  | Dur(mins) | #Channels | #Utt |\\n|-------------------------|-----------|-----------|------|\\n| Business News           | 10.48     | 33        | 76   |\\n| Cooking                 | 11.84     | 76        | 138  |\\n| Debates                 | 7.86      | 6         | 56   |\\n| Education - Technology  | 9.86      | 43        | 110  |\\n| Headlines               | 7.48      | 19        | 51   |\\n| Health                  | 9.28      | 18        | 86   |\\n| Household activities    | 8.57      | 17        | 68   |\\n| How-to Technology        | 10.24     | 61        | 121  |\\n| Interviews/Panels       | 9.53      | 24        | 98   |\\n| Maths                   | 9.01      | 27        | 86   |\\n| On-field reporting      | 8.46      | 6         | 64   |\\n| Science                 | 7.30      | 14        | 64   |\\n| Social Science          | 9.78      | 22        | 84   |\\n| Sports News             | 8.15      | 5         | 53   |\\n\\n5. Results and Discussion\\n\\nExperiment Setup. We train Conformer-L [22] models, consisting of 120M parameters, as the encoder, with a hybrid CTC-RNNT [23] decoder. The model has 17 conformer blocks with 512 as the model dimension. All our models are trained using standard recipes from the NeMo [24] library. The same model is trained on PN-lab as well as PN-lab+PN-pseudolab with consistent hyperparameters, to study the effect of pseudo-labelling. The results of our experiments are summarised in Tables 2,3,4 and Figure 1. Below we discuss some of the interesting observations from our results.\\n\\nQualitative Analysis of Pseudo-Labeled Samples. We first do a qualitative analysis of the pseudo-labeled samples generated using Pratinidhi. We randomly select 500 samples from the PN-pseudolab, spanning various domains. We then asked human evaluators to listen to audios and read the transcripts to...\"}"}
{"id": "bhogale24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model               | Kathbath | Kathbath-Hard | FLEURS | CommonVoice | IndicTTS | MUCS | Gramvaani | Average |\\n|---------------------|----------|--------------|--------|-------------|----------|------|-----------|---------|\\n| Google STT          | 14.3     | 16.7         | 19.4   | 20.8        | 18.3     | 17.8 | 59.9      | 23.9    |\\n| IndicWav2vec        | 12.2     | 16.2         | 18.3   | 20.2        | 15.0     | 22.9 | 42.1      | 21.0    |\\n| Azure STT           | 13.6     | 15.1         | 24.3   | 14.6        | 15.2     | 15.1 | 42.3      | 20.0    |\\n| Nvidia-medium       | 14.0     | 15.6         | 19.4   | 20.4        | 12.3     | 12.4 | 41.3      | 19.4    |\\n| Nvidia-large        | 12.7     | 14.2         | 15.7   | 21.2        | 12.2     | 11.8 | 42.6      | 18.6    |\\n| IndicWhisper        | 10.3     | 12.0         | 11.4   | 15.0        | 7.6      | 12.0 | 26.8      | 13.6    |\\n| PN-lab              | 8.8      | 10.0         | 11.3   | 11.2        | 6.8      | 10.0 | 23.7      | 11.9    |\\n| PN                 | 8.9      | 10.1         | 11.9   | 10.6        | 7.7      | 9.0  | 24.2      | 11.9    |\\n\\nIdentify errors. The evaluators reported that 325/500 transcripts had no errors. Of the 175 erroneous samples, 65 had additions, deletions or substitutions whereas remaining 110 samples had minor spelling errors, such as interchanged long and short vowels or alternative non-standard spellings for code-mixed words.\\n\\nComparison of our model on the INDICYT benchmark. We compare the performance of PN-lab and PN-lab+PN across the different domains of the INDICYT benchmark. We observe that the addition of the PN-pseudolab dataset helps across all domains, with an average improvement of 8.6% across domains. Figure 1 gives a holistic picture of the improvement across multiple domains. It is interesting to note that while the base model (green) performs poorly on inherently difficult domains, such as Maths, Science, Health, Tech (Edu.), adding pseudo-labeled data (purple) significantly improves the performance on these difficult domains.\\n\\nRobustness to audio lengths. Next, we assess the robustness of the model across utterances of different lengths. Specifically, Table 4 compares the performance of the base model (Model 1) trained on PN-lab with our final model (Model 2) trained on PN-lab and PN-pseudolab, on audio segments of different lengths. We observe larger improvements for shorter and longer audios and a moderate improvement for mid-sized audios. We hypothesise that this is because existing training data largely contain medium sized audio files whereas PN-pseudolab contains short and long utterances also as the audio files are segmented using VAD which may generate shorter segments if there are frequent pauses or the speech is slow and longer segments for rapidly spoken content without notable pauses.\\n\\nEffect of combining pseudo-transcribers and evaluators. Next, we compare 4 different methods for pseudo-labeling (i) PN-RNNT which just uses the transcripts generated by the RNNT decoder without using any evaluator for filtering or using any other pseudo-transcriber for matching (ii) PN-SONAR which combines the RNNT transcriber with a SONAR based evaluator to filter audio-transcript pairs which have lower cosine similarity in the SONAR embedding space (iii) PN No-Filter just relies on two pseudo transcribers (RNN-T and CTC) and does not use any evaluator based filtering and (iv) PN which is our final approach using two pseudo transcribers and the SONAR evaluator. The results in Table 3 clearly show that combining multiple pseudo-transcribers and evaluators (as in the last column, PN) gives the best results. This establishes the effectiveness of our framework integrating multiple systems for pseudo labeling for low resource languages.\\n\\nComparison on out-of-style benchmarks. One could argue that adding pseudo-labeled data of a specific style (YouTube) in this case, could affect performance on benchmarks containing audio which is different from YouTube content. To evaluate this, we test our final model on the robust VISTAAR benchmark which contains audio of multiple styles, e.g., crowdsourced read speech, on-field data, call centre data and so on. The results in Table 2 show that even though our final training data is dominated by pseudo-labeled content from YouTube, the performance on other benchmarks is not affected, with the average WER being comparable (last two rows of Table 2).\\n\\n### Table 2: Comparison of publicly-available models on the Hindi subset of the Vistaar benchmark\\n\\n| Model             | Kathbath | Kathbath-Hard | FLEURS | CommonVoice | IndicTTS | MUCS | Gramvaani | Average |\\n|-------------------|----------|--------------|--------|-------------|----------|------|-----------|---------|\\n| Google STT        | 14.3     | 16.7         | 19.4   | 20.8        | 18.3     | 17.8 | 59.9      | 23.9    |\\n| IndicWav2vec      | 12.2     | 16.2         | 18.3   | 20.2        | 15.0     | 22.9 | 42.1      | 21.0    |\\n| Azure STT         | 13.6     | 15.1         | 24.3   | 14.6        | 15.2     | 15.1 | 42.3      | 20.0    |\\n| Nvidia-medium     | 14.0     | 15.6         | 19.4   | 20.4        | 12.3     | 12.4 | 41.3      | 19.4    |\\n| Nvidia-large      | 12.7     | 14.2         | 15.7   | 21.2        | 12.2     | 11.8 | 42.6      | 18.6    |\\n| IndicWhisper      | 10.3     | 12.0         | 11.4   | 15.0        | 7.6      | 12.0 | 26.8      | 13.6    |\\n| PN-lab            | 8.8      | 10.0         | 11.3   | 11.2        | 6.8      | 10.0 | 23.7      | 11.9    |\\n| PN                | 8.9      | 10.1         | 11.9   | 10.6        | 7.7      | 9.0  | 24.2      | 11.9    |\\n\\n### Table 3: Effect of combining pseudo-transcribers and evaluators\\n\\n| Domains             | PN Lab | PN RNNT | PN SONAR | PN No Filter | PN |\\n|---------------------|--------|---------|----------|--------------|----|\\n| Headlines           | 25.0   | 20.3    | 19.7     | 19.0         |    |\\n| On-field reporting  | 29.7   | 26.1    | 25.3     | 25.3         |    |\\n| Debates             | 25.0   | 23.0    | 20.4     | 18.4         |    |\\n| Interview - Panels  | 34.4   | 28.8    | 27.3     | 27.3         |    |\\n| Sports News         | 29.4   | 27.3    | 24.4     | 24.4         |    |\\n| Business News       | 38.5   | 33.5    | 29.4     | 29.4         |    |\\n| Education - Technology | 42.2  | 32.5    | 27.2     | 25.8         |    |\\n| Health              | 37.2   | 35.1    | 26.6     | 25.0         |    |\\n| Social Science      | 38.2   | 34.6    | 29.2     | 27.8         |    |\\n| Maths               | 38.7   | 32.2    | 29.1     | 29.0         |    |\\n| Science             | 38.3   | 34.2    | 26.4     | 23.3         |    |\\n| How-to Technology   | 35.6   | 30.7    | 27.6     | 27.6         |    |\\n| Cooking             | 26.2   | 21.1    | 19.5     | 18.9         |    |\\n| Household activities| 24.2   | 21.8    | 22.5     | 21.9         |    |\\n| ALL                 | 33.3   | 28.8    | 26.0     | 24.7         |    |\\n\\n### Table 4: Robustness to audio lengths\\n\\n| Duration (seconds) | PN-lab | PN |\\n|--------------------|--------|----|\\n| 2-10               | 35.70  | 26.29 |\\n| 10-20              | 28.61  | 22.57 |\\n| 20-30              | 34.30  | 23.27 |\\n\\n6. Conclusion\\n\\nWe address the challenge of limited labeled data for low-resource languages in ASR, with a specific focus on Hindi. We propose a pseudo-labeling framework for generating high-quality pseudo-labeled data. This approach leverages the consensus among multiple base models and employs evaluators to ensure the reliability of the generated audio-transcript pairs. Our experiments reveal marked improvements in ASR performance on the INDICYT benchmark created as a part of this work, affirming the value of pseudo-labeled data in enhancing ASR capabilities for languages with scant resources, without compromising performance on external benchmarks.\"}"}
