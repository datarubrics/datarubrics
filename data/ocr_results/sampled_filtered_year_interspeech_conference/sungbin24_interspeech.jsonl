{"id": "sungbin24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Acknowledgments\\n\\nThis research was supported by a grant from KRAFTON AI, and also partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (RS-2022-II220124, Development of Artificial Intelligence Technology for Self-Improving Competency-Aware Learning Capabilities; RS-2021-II212068, Artificial Intelligence Innovation Hub; RS-2019-II191906, Artificial Intelligence Graduate School Program (POSTECH)).\\n\\n6. References\\n\\n[1] C. Liu, \u201cAn analysis of the current and future state of 3d facial animation techniques and systems,\u201d Simon Fraser University, 2009.\\n\\n[2] C. Sondermann and M. Merkt, \u201cLike it or learn from it: Effects of talking heads in educational videos,\u201d Computers & Education, 2023.\\n\\n[3] I. Wohlgenannt, A. Simons, and S. Stieglitz, \u201cVirtual reality,\u201d Business & Information Systems Engineering, 2020.\\n\\n[4] D. Cudeiro, T. Bolkart, C. Laidlaw, A. Ranjan, and M. J. Black, \u201cCapture, learning, and synthesis of 3d speaking styles,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[5] T. Karras, T. Aila, S. Laine, A. Herva, and J. Lehtinen, \u201cAudio-driven facial animation by joint end-to-end learning of pose and emotion,\u201d ACM Transactions on Graphics (SIGGRAPH), 2017.\\n\\n[6] A. Richard, M. Zollh\u00f6fer, Y. Wen, F. De la Torre, and Y. Sheikh, \u201cMeshtalk: 3d face animation from speech using cross-modality disentanglement,\u201d in IEEE International Conference on Computer Vision (ICCV), 2021.\\n\\n[7] Y. Fan, Z. Lin, J. Saito, W. Wang, and T. Komura, \u201cFaceformer: Speech-driven 3d facial animation with transformers,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[8] J. Xing, M. Xia, Y. Zhang, X. Cun, J. Wang, and T.-T. Wong, \u201cCodetalker: Speech-driven 3d facial animation with discrete motion prior,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[9] Z. Peng, H. Wu, Z. Song, H. Xu, X. Zhu, H. Liu, J. He, and Z. Fan, \u201cEmotalk: Speech-driven emotional disentanglement for 3d face animation,\u201d in IEEE International Conference on Computer Vision (ICCV), 2023.\\n\\n[10] R. Dan\u010dek, K. Chhatre, S. Tripathi, Y. Wen, M. J. Black, and T. Bolkart, \u201cEmotional speech-driven animation with content-emotion disentanglement,\u201d in ACM Transactions on Graphics (SIGGRAPH Asia), 2023.\\n\\n[11] K. Sung-Bin, L. Hyun, D. H. Hong, S. Nam, J. Ju, and T.-H. Oh, \u201cLaughtalk: Expressive 3d talking head generation with laughter,\u201d in IEEE Winter Conference on Applications of Computer Vision (WACV), 2024.\\n\\n[12] B. Thambiraja, I. Habibie, S. Aliakbarian, D. Cosker, C. Theobalt, and J. Thies, \u201cImitator: Personalized speech-driven 3d facial animation,\u201d in IEEE International Conference on Computer Vision (ICCV), 2022.\\n\\n[13] H. Huang, Z. Wu, S. Kang, D. Dai, J. Jia, T. Fu, D. Tuo, G. Lei, P. Liu, D. Su et al., \u201cSpeaker independent and multilingual/mixlingual speech-driven talking head generation using phonetic posteriorgrams,\u201d in 2021 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2021.\\n\\n[14] G. Fanelli, J. Gall, H. Romsdorfer, T. Weise, and L. Van Gool, \u201cA 3-d audio-visual corpus of affective communication,\u201d IEEE Transactions on Multimedia, 2010.\\n\\n[15] P. P. Filntisis, G. Retsinas, F. Paraperas-Papantoniou, A. Katsamanis, A. Roussos, and P. Maragos, \u201cSpectre: Visual speech-informed perceptual 3d facial expression reconstruction from videos,\u201d in IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023.\\n\\n[16] T. Li, T. Bolkart, M. J. Black, H. Li, and J. Romero, \u201cLearning a model of facial shape and expression from 4d scans.\u201d ACM Transactions on Graphics (SIGGRAPH), 2017.\\n\\n[17] E. Ng, H. Joo, L. Hu, H. Li, T. Darrell, A. Kanazawa, and S. Giannasar, \u201cLearning to listen: Modeling non-deterministic dyadic facial motion,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[18] A. Van Den Oord, O. Vinyals et al., \u201cNeural discrete representation learning,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\n[19] Z. Peng, Y. Luo, Y. Shi, H. Xu, X. Zhu, H. Liu, J. He, and Z. Fan, \u201cSelftalk: A self-supervised commutative training diagram to comprehend 3d talking faces,\u201d in ACM International Conference on Multimedia (MM), 2023.\\n\\n[20] M. Anwar, B. Shi, V. Goswami, W.-N. Hsu, J. Pino, and C. Wang, \u201cMuavic: A multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation,\u201d in Conference of the International Speech Communication Association (INTERSPEECH), 2023.\\n\\n[21] J. Yu, H. Zhu, L. Jiang, C. C. Loy, W. Cai, and W. Wu, \u201cCelebvox: A large-scale facial text-video dataset,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[22] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d in Conference of the International Speech Communication Association (INTERSPEECH), 2018.\\n\\n[23] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoxceleb: A large-scale speaker identification dataset,\u201d in Conference of the International Speech Communication Association (INTERSPEECH), 2017.\\n\\n[24] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning (ICML), 2023.\\n\\n[25] R. Tao, Z. Pan, R. K. Das, X. Qian, M. Z. Shou, and H. Li, \u201cIs someone speaking? exploring long-term temporal features for audio-visual active speaker detection,\u201d in ACM International Conference on Multimedia (MM), 2021.\\n\\n[26] C. Lugaresi, J. Tang, H. Nash, C. McClanahan, E. Uboweja, M. Hays, F. Zhang, C.-L. Chang, M. G. Yong, J. Lee et al., \u201cMediapipe: A framework for building perception pipelines,\u201d arXiv preprint arXiv:1906.08172, 2019.\\n\\n[27] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\n[28] Y. Bengio, N. L\u00e9onard, and A. Courville, \u201cEstimating or propagating gradients through stochastic neurons for conditional computation,\u201d arXiv preprint arXiv:1308.3432, 2013.\\n\\n[29] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised cross-lingual representation learning for speech recognition,\u201d arXiv preprint arXiv:2006.13979, 2020.\\n\\n[30] H. McGurk and J. MacDonald, \u201cHearing lips and seeing voices,\u201d Nature, 1976.\\n\\n[31] J. H. Yeo, M. Kim, S. Watanabe, and Y. M. Ro, \u201cVisual speech recognition for low-resource languages with automatic labels from whisper model,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2024.\\n\\n[32] P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, \u201cAuto-avsr: Audio-visual speech recognition with automatic labels,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023.\"}"}
{"id": "sungbin24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset\\n\\nKim Sung-Bin1\u2217, Lee Chae-Yeon2\u2217, Gihun Son1\u2217, Oh Hyun-Bin1, Janghoon Ju3, Suekyeong Nam3, Tae-Hyun Oh1,2,4\\n\\n1Dept. of Electrical Engineering and 2Grad. School of Artificial Intelligence, POSTECH, Korea\\n3KRAFTON, Korea\\n4Institute for Convergence Research and Education in Advanced Technology, Yonsei University, Korea\\n\\n{ sungbin, chaeyeon.lee, gihun.son, taehyun }@postech.ac.kr\\n\\nAbstract\\nRecent studies in speech-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input speech in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. In this work, we introduce a novel task to generate 3D talking heads from speeches of diverse languages. We collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. With our proposed dataset, we present a multilingually enhanced model that incorporates language-specific style embeddings, enabling it to capture the unique mouth movements associated with each language. Additionally, we present a metric for assessing lip-sync accuracy in multilingual settings. We demonstrate that training a 3D talking head model with our proposed dataset significantly enhances its multilingual performance. Codes and datasets are available at https://multi-talk.github.io/.\\n\\nIndex Terms: Speech-driven 3D talking head, Video dataset, Multilingual, Audio-visual speech recognition\\n\\n1. Introduction\\nSpeech-driven 3D talking heads are key components in virtual avatars, enhancing realism and improving user engagement in diverse multimedia applications [1, 2, 3]. Recent advancements in deep learning have significantly advanced the field of 3D talking heads. Earlier efforts [4, 5, 6, 7, 8] have focused on enhancing lip synchronization, while more recent studies aim to enable the expression of various emotions [9, 10] and non-verbal signals [11], or even to develop personalized models [12].\\n\\nHowever, the multilingual capabilities of 3D talking heads have received less attention and remain underexplored. Despite claims from previous studies [7, 4, 13] that their models are language-agnostic, we observe that Huang et al. [13] cover only two languages, and the quality of the generated meshes from prior works [7, 4] degrades when input speech deviates from the English language family. We hypothesize that this limitation stems from the scarcity of diverse 3D talking head datasets. Existing datasets, such as VOCASET [4] and BIWI [14], are not only small in scale but also limited in expressiveness, diversity, and language scope (English-only). Even a more sophisticated model, designed to handle diverse languages, may be constrained by the styles and motion characteristics of available datasets.\\n\\nTo tackle this challenge, we introduce a novel task of generating 3D talking heads from speeches in diverse languages, i.e., multilingual 3D talking heads. For this task, we collect the MultiTalk dataset, comprising in-the-wild 2D talking videos across 20 different languages, paired with corresponding pseudo 3D meshes and transcripts (see Fig. 1). We design an automated data collection pipeline to parse short utterances of frontal talking videos in diverse languages from YouTube. As these 2D videos lack 3D metadata, we leverage an off-the-shelf 3D reconstruction model [15] to generate reliable and robust pseudo ground-truth 3D mesh vertices [16] for the collected 2D videos.\\n\\nTo demonstrate the effectiveness of our dataset for multilingual 3D talking heads, we introduce a strong baseline model, MultiTalk, by training on a subset of our dataset. Inspired by previous works [8, 17], we start by training a vector-quantized autoencoder (VQ-V AE) [18] to learn a discrete codebook, which encodes expressive 3D facial motions across various languages. By utilizing this discrete codebook, we then train a temporal autoregressive model to synthesize sequences of 3D faces, conditioned on both the input speech and the learnable language embedding. This language embedding captures the stylistic nuances of facial motions specific to each language family.\\n\\nWe validate our baseline model against existing 3D talking head models [4, 7, 8, 19] trained on the English-only VOCASET dataset. As this task is novel, we propose a new evaluation metric, Audio-Visual Lip Readability (AVLR), which assesses the lip-sync accuracy of 3D talking heads on multilingual speeches using a pre-trained Audio-Visual Speech Recognition (AVSR) model [20]. Through the experiments, we show that our model performs favorably across diverse languages compared to previous works. Our main contributions are summarized as follows:\\n\\n\u2022 Proposing a new task, multilingual 3D talking head, accompanied by an evaluation metric to measure the lip synchronization accuracy on multilingual speech.\\n\u2022 Collecting the MultiTalk dataset, featuring over 420 hours of 2D videos with paired 3D metadata in 20 different languages.\\n\u2022 Introducing a strong baseline, MultiTalk, capable of generating accurate and expressive 3D faces from multilingual speech.\"}"}
{"id": "sungbin24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present a 2D talking video dataset that is well-balanced across 20 languages (each accounting for 2.0-9.7%), accompanied by pseudo 3D mesh vertices and transcripts for each video.\\n\\n| Statistics of our MultiTalk dataset |\\n|-------------------------------------|\\n| Num. of languages | 20 |\\n| Num. of video clips | 294k |\\n| Total hours | 423.2 h |\\n| Avg. duration | 5.2 sec. |\\n\\n2. Learning multilingual 3D talking head\\n\\nIn this section, we introduce our new multilingual video dataset in Sec. 2.1 and describe the proposed baseline model for multilingual 3D talking head generation in Sec. 2.2.\\n\\n2.1. MultiTalk dataset\\n\\nWe introduce the MultiTalk dataset, featuring over 420 hours of multilingual 2D talking videos across 20 languages. Despite the abundance of 2D video datasets [21, 22, 23], we aim to curate a dataset with more balanced statistics across a broader range of languages. Each video in the MultiTalk dataset is annotated with the language type of a speech and a pseudo-transcript generated using Whisper [24], and a subset among the videos is annotated with pseudo 3D mesh vertices. Samples and statistics of the MultiTalk dataset are shown in Fig. 1 and Table 1, respectively.\\n\\nWe design an automated pipeline to obtain short utterances of talking videos in diverse languages, described as follows.\\n\\nCollecting 2D videos.\\n\\nWe begin by designing various queries that incorporate keywords, such as \u201cnationality,\u201d \u201cinterviews,\u201d and \u201cconversation.\u201d These queries are prompted to YouTube to retrieve in-the-wild human talking 2D videos in different languages and diverse scenarios.\\n\\nActive speaker verification.\\n\\nThe goal here is to trim a raw video into segments that only contain talking faces synchronized to speech, while removing clips with non-active speakers. To achieve this, we leverage TalkNet [25], which performs audio-visual cross-attention to identify the visible person in speaking. We set conservative thresholds to minimize false positives and ensure that only the cleaned video is left. We further trim the video when gaps occur during speaking, resulting in short utterances.\\n\\nFrontal face verification.\\n\\nThe faces in the filtered videos do not always face the front, which can prevent the model from learning clear facial movements. Thus, we measure the angle of yaw and pitch of the face using Mediapipe [26] and filter out videos with abrupt angle changes (indicating abrupt head movements) or large yaw or pitch angles (side faces). This process concludes the automated pipeline for collecting cleaned 2D frontal talking face videos with short utterances in diverse languages. We further leverage Whisper [24] trained on each language, to annotate the pseudo-transcript for each video clip.\\n\\nLifting 2D video to 3D.\\n\\nFrom the subset of collected 2D talking videos, we reconstruct 3D meshes that are synchronized with both the audio and facial movements of the video clips. Similar to prior arts [10, 11] that demonstrate the effectiveness of pseudo-3D reconstructions for training 3D talking heads, we leverage SPECTRE [15] to reconstruct accurate and robust pseudo 3D meshes from 2D talking videos. Unlike existing datasets, e.g., VOCASET [4], which are limited in small scale and English-only speeches, our newly annotated 3D mesh dataset encompasses expressive facial motions, paired with speeches that vary in diverse tones and pitches across a wide range of languages.\\n\\n2.2. Speech-driven multilingual 3D talking head\\n\\nUsing the subset of the dataset we collected, as detailed in Sec. 2.1, we aim to develop a model capable of generating accurate 3D talking head synchronized with input speech in various languages. Despite the increased diversity of the dataset, naively dumping all data into the model could result in learning only average facial movements. To address this, we break down the task into sub-problems and introduce a baseline model, MultiTalk. MultiTalk undergoes a two-stage training process: first, learning a general facial motion prior through discrete codes, then training a speech-driven temporal autoregressive model to animate 3D faces with the learned discrete codes. Echoing the success of previous works [18, 8, 17], the disentanglement of the learning process allows the model to effectively construct rich discrete motion prior from the diverse talking faces of various languages, which is then leveraged in the second stage for synthesis.\\n\\nThe task formulation is specified as follows: Let \\\\( V_1:T = (v_1, \\\\ldots, v_T) \\\\) denote a temporal sequence of ground-truth facial motions, where each frame \\\\( v_t \\\\in \\\\mathbb{R}^{N \\\\times 3} \\\\) consisting \\\\( N \\\\) vertices, represents the 3D facial movement. Additionally, let \\\\( S_1:T = (s_1, \\\\ldots, s_T) \\\\) be a sequence of speech representations. By conditioning on input speech \\\\( S_1:T \\\\), the goal in this task is to sequentially predict facial movements \\\\( \\\\hat{V}_1:T \\\\), similar to \\\\( V_1:T \\\\).\\n\\nLearning discrete facial motion.\\n\\nFollowing CodeTalker [8], we extend the use of vector quantized autoencoder (VQ-VAE) [18] to learn a discrete codebook of context-rich facial motions (refer to Stage 1 in Fig. 2). As speech data is not required for training VQ-VAE, we utilize a large amount of 3D motion sequences from the MultiTalk dataset for learning the prior. This enables the learned prior to cover a broad spectrum of facial motions observed in speakers of diverse languages.\"}"}
{"id": "sungbin24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning speech-driven motion synthesis. In this stage, we develop a model that maps input speech to a sequence of discrete codes, which are later decoded into realistic continuous motions. The VQ-V AE in the first stage is utilized in the subsequent stage to learn the speech-conditioned codebook of discrete facial motions, these discrete motions are then reconstructed back into continuous motions.\\n\\nThe VQ-V AE decoder \\\\( \\\\hat{Z} \\\\) is then reconstructed back into continuous motions \\\\( \\\\hat{V} \\\\) through an element-wise quantization function \\\\( q \\\\) that maps the continuous motion features \\\\( \\\\hat{Z} \\\\) to its nearest codebook entry:\\n\\n\\\\[\\n\\\\hat{v}_t = q(\\\\hat{z}_t),\\n\\\\]\\n\\nwhere \\\\( \\\\hat{v}_t \\\\) is the past predicted motion and \\\\( \\\\hat{z}_t \\\\) is the currently predicted motion. The entire model is trained with the following loss:\\n\\n\\\\[\\nL = \\\\lambda_1 \\\\sum_{t=1}^{T-2} \\\\| \\\\hat{z}_t - q(\\\\hat{z}_t) \\\\|^2_2 + \\\\lambda_2 \\\\sum_{t=1}^{T-2} \\\\| \\\\hat{V}_t - q(\\\\hat{V}_t) \\\\|^2_2 + \\\\lambda_3 \\\\sum_{t=1}^{T-2} \\\\| \\\\hat{v}_t - q(\\\\hat{v}_t) \\\\|^2_2\\n\\\\]\\n\\nwhere \\\\( \\\\lambda_1, \\\\lambda_2, \\\\lambda_3 \\\\) are weight factors. After training the model and measure the Word Error Rate (WER) to evaluate the audio alongside the rendered 3D faces to a pre-trained A VSR and A VSR with Signal-to-Noise Ratio (SNR) settings of \\n\\n\\\\[\\n\\\\rho \\\\text{ (Spearman's correlation coefficient), which ranges from -1 to 1; a value of 1 indicates the highest correlation with human evaluations.}\\n\\\\]\\n\\nTable 2: Preliminary experiment. We collect meshes from the ground-truth VOCASET [4] dataset and those generated by FaceFormer [7] and VOCA [4]. We measure the WER for each model using VSR, and AVSR with Signal-to-Noise Ratio (SNR) settings of\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{Method} & \\\\quad \\\\text{AVLR (SNR=-7.5)} & \\\\quad \\\\text{AVLR (SNR=-10)} & \\\\quad \\\\text{VSR} \\\\\\\\\\n\\\\text{VOCASET (GT)} & \\\\quad 39.4 & \\\\quad 43.8 & \\\\quad 111.2 \\\\\\\\\\n\\\\text{FaceFormer} & \\\\quad 50.7 & \\\\quad 56.9 & \\\\quad 136.3 \\\\\\\\\\n\\\\end{align*}\\n\\\\]\\n\\nindicating its suitability as a metric for measuring the lip readability of the 3D talking head.\\n\\nTo evaluate lip synchronization of generated mesh with the input audio, we introduce a new metric to evaluate the lip readability of 3D talking heads. We hypothesize that supplementing visual information with subtle audio cues may reduce this ambiguity, leading to a more robust lip readability metric compared to using visual cues alone. Specifically, we supply noisy accompanying speech. However, relying solely on visual cues introduces ambiguity in inferring words. For example, distinguishing between \\\"ba\\\" and \\\"ma\\\" is challenging by merely observing mouth shapes [30].\"}"}
{"id": "sungbin24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Quantitative comparison to existing methods.\\n\\nWe compare MultiTalk (Ours) with existing methods on the test split of the Multitalk dataset on 4 languages: English (En), Italian (It), French (Fr), and Greek (El). LVE is measured in $\\\\times 10^{-4}\\\\text{mm}$ scale, and AVSR (SNR=-7.5) is measured in WER (%).\\n\\n| Method       | LVE (\u2193) | AVLR (\u2193) |\\n|--------------|---------|----------|\\n| En | It | Fr | El | En | It | Fr | El |\\n| VOCA          | 1.95    | 2.78    | 1.93 | 2.18 | 50.8 | 60.4 | 74.9 | 82.1 |\\n| FaceFormer   | 1.82    | 2.56    | 1.78 | 1.99 | 50.8 | 58.9 | 70.9 | 79.0 |\\n| CodeTalker   | 1.98    | 2.56    | 1.99 | 2.09 | 50.0 | 59.4 | 74.9 | 77.8 |\\n| SelfTalk     | 1.99    | 2.59    | 1.98 | 2.11 | 42.8 | 56.5 | 68.3 | 80.3 |\\n| MultiTalk (Ours) | 1.16   | 1.06    | 1.39 | 1.26 | 42.4 | 50.5 | 63.0 | 74.2 |\\n\\nFigure 3: Qualitative comparisons.\\n\\nCompared to existing methods, MultiTalk (Ours) demonstrates detailed facial expressions with accurately synchronized lip movements to the input speech.\\n\\nWe then compute the Spearman's correlation coefficient, $\\\\rho$, to compare the model rankings with human evaluation rankings. As shown in Table 2, AVSR exhibits the highest correlation with human evaluations. Furthermore, VSR produces WERs exceeding 110%, confirming its unsuitability as a metric. These findings highlight the efficacy of our proposed AVLR metric in assessing lip accuracy.\\n\\nUtilizing the metrics described above, we conduct a quantitative comparison of our MultiTalk model against four different approaches: VOCA [4], FaceFormer [7], CodeTalker [8], and SelfTalk [19]. Table 3 summarizes the LVE and AVLR over the test set of the MultiTalk dataset, with AVLR assessed across four different languages. Notably, MultiTalk achieves superior performance compared to the other methods across all metrics. Specifically, in the AVLR, the recent method SelfTalk shows comparable performance in English, but MultiTalk excels in languages other than English. These results highlight the effectiveness of both our proposed method and the dataset in establishing multilingual capabilities for 3D talking head models.\\n\\nFor a more comprehensive comparison, we visualize the generated samples in Fig. 3. As shown, the meshes generated by our MultiTalk model exhibit detailed and expressive lip movements for closures and openings in sync with the input speech.\\n\\nWe postulate that such expressiveness could be learned from our dataset, which reflects the inherent diversity and includes various facial movements across languages.\\n\\n3.2. User study\\n\\nWe incorporate human perception as a metric through a user study. We first generate 24 3D face videos using MultiTalk (A) and compare them with those generated by existing methods (B) from the test split of the MultiTalk dataset. We design an A vs. B test and report the percentage (%) of preferences for A (Ours) over B, assessing the generated meshes on lip sync and realism.\\n\\n| Aspect     | vs. VOCA | vs. FaceFormer | vs. CodeTalker | vs. SelfTalk |\\n|------------|----------|----------------|----------------|--------------|\\n| Lip sync   | 93.28    | 84.17          | 76.92          | 53.78        |\\n| Realism    | 93.28    | 91.53          | 82.05          | 66.95        |\\n\\n3.3. Ablation study\\n\\nWe conduct ablation studies to validate our design choices, as in Table 5. Comparing (a) and (c), we observe that utilizing the language style embedding stabilizes the learning and yields favorable performance. Moreover, comparisons between (b) and (c) indicate that incorporating the multilingual speech encoder facilitates the extraction of language-agnostic features. This enables the model to focus on universal speech representations, thereby accommodating motion synthesis from multiple languages.\\n\\nTable 5: Ablation studies on design choices.\\n\\nWe compare different configurations of our method by either incorporating the language style embedding $l_{emb.}$ or utilizing different speech encoders $E_m$. \\\"Multi.\\\" and \\\"En.\\\" denote the speech encoders trained on multilingual and English-only speeches, respectively.\\n\\n| $l_{emb.}$ | $E_m$ type | LVE (\u2193) | AVLR (\u2193) |\\n|------------|------------|---------|----------|\\n| En | (a) Multi. | 1.78    | 2.07    | 2.45 | 1.82 | 41.9 | 52.4 | 64.1 | 72.0 |\\n| \u2713 | \u2713 | (b) En. | 1.56    | 1.34    | 1.91 | 1.37 | 50.3 | 55.6 | 71.7 | 77.3 |\\n| \u2713 | \u2713 | (c) Multi. | 1.16    | 1.06    | 1.39 | 1.26 | 42.4 | 50.5 | 63.0 | 74.2 |\\n\\nB test, prompting participants to choose between two samples based on lip synchronization and realism. To accurately evaluate multilingual capability, participants from various countries participated in this study. As indicated in Table 4, MultiTalk is preferred by users, notably excelling in realism compared to other methods. These results emphasize the expressiveness and multilingual capability of our model.\\n\\n3.4. Discussion and conclusion\\n\\nIn this work, we introduce a novel task of animating 3D talking heads from multilingual speeches. Recognizing the lack of diversity in existing datasets for learning multilingual capabilities, we have collected the MultiTalk dataset, consisting of 2D talking videos in multiple languages, each paired with 3D metadata and transcripts. Moreover, we present MultiTalk, a baseline model trained in two stages on our dataset. Considering the novelty of this task, we have devised an audio-visual lip readability metric to assess the model's multilingual capability. Our experiments demonstrate the effectiveness of our approach, showcasing robust lip synchronization performance across diverse languages.\\n\\nLimitation and future work.\\n\\nWhile our dataset offers extensive annotations, the transcripts and 3D meshes are pseudo-annotated, which might introduce some level of noise compared to human annotations. Despite this limitation, these pseudo-annotations have proven to be effective in enhancing model performance in prior arts [31, 32, 11, 10]. Future work will focus on refining these annotations. We would like to note that our proposed multilingual video dataset and the Audio-Visual Lip Readability metric have broader usage beyond our immediate task, e.g., (audio) visual speech recognition and 2D talking heads. Furthermore, the rich facial motion prior learned by diverse faces across various languages holds significant potential to advance research in facial motion synthesis for further exploration.\"}"}
