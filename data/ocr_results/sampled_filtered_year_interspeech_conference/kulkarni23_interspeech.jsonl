{"id": "kulkarni23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus\\nAjinkya Kulkarni\u2217, Atharva Kulkarni\u2020\\nSara Abedalmon'em Mohammad Shatnawi\u2217, Hanan Aldarmaki\u2217\\n\\n\u2217MBZUAI, UAE, \u2020Erisha Labs, India\\n\u2217firstname.lastname@mbzuai.ac.ae, \u2020atharva7kulkarni@gmail.com\\n\\nAbstract\\nWe present a Classical Arabic Text-to-Speech (ClArTTS) corpus to facilitate the development of end-to-end TTS systems for the Arabic language. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 Hz. In this paper, we describe the process of corpus creation, details of corpus statistics, and a comparison with existing resources. Furthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and illustrate the performance of the resulting systems via subjective and objective evaluations. The ClArTTS corpus is publicly available at www.clartts.com for research purposes, along with the baseline TTS systems and an interactive demo.\\n\\nIndex Terms: arabic speech corpus, text-to-speech, corpus creation\\n\\n1. Introduction\\n\\nNeural text-to-speech (TTS) models are becoming mainstream due to their superior performance in synthesizing intelligible and natural-sounding speech [1, 2, 3]. Compared to older concatenative (e.g. [4]) or HMM-based [5] TTS models, neural models can generate raw waveform directly from text inputs without complex pre-processing and phonetic feature extraction. Neural TTS models commonly have two main components: an acoustic model that generates acoustic features (e.g. mel-spectrograms) directly from the text, and a vocoder to generate a waveform from the acoustic features (see for example [6]). Fully end-to-end TTS models that combine both stages have also been explored [7]. While these neural architectures can be complex, end-to-end training alleviates the need for feature engineering and other design choices that are prone to be sub-optimal. One of the bottlenecks in TTS system design, however, is the availability and quality of the speech corpora used for training. Unlike ASR datasets, where it is desirable to have a variety of speakers and recording conditions to achieve robust performance, it is far more advantageous to have consistent single-speaker corpora for TTS to achieve intelligible and natural-sounding synthesis. Therefore, speech data used for training TTS models need to have more consistent acoustic features that ideally only vary along phonetic and prosodic dimensions.\\n\\nSuch datasets can be conveniently extracted from pre-existing audiobooks; for example, the LJ Speech corpus includes \u223c24 hours of speech extracted from seven audiobooks by the same female speaker. Meanwhile, due to the scarcity of freely available speech corpora of this kind, a larger gap exists in Arabic TTS research and development. Most of the existing freely available Arabic speech corpora are not suitable for TTS training as they contain multi-speaker casual speech with variations in recording conditions and quality, whereas the corpora curated for speech synthesis are generally small in size and not suitable for training state-of-the-art end-to-end models [8, 9]. The existing corpora for Arabic TTS are carefully designed and reduced datasets that are optimized for phonetic coverage while maintaining a relatively small number of units [10, 11]. This choice is partially a remnant of early concatenative models that have a real-time computational cost proportional to the size of the dataset. Another reason for this choice is the relative difficulty of constructing consistent datasets that are suitable for TTS training, especially if they need to be annotated at the phonetic level for traditional TTS systems, so a reduced dataset that maintains phonetic coverage is more manageable to construct. For example, one of the most commonly used public TTS datasets for Arabic is the Arabic Speech Corpus (ASC) [10], which has around 3.4 hours of speech. The ASC was designed to maximize phonetic coverage using a greedy optimization strategy. While such optimization technique is commonly used in most TTS data construction projects, there is some evidence that a random subset of the same size could potentially lead to similar or even more natural-sounding speech synthesis [12]. In addition, for neural TTS models, quantity is more beneficial to the overall quality of the synthesized speech as they are more robust to small variations in input conditions. Moreover, neural TTS models can work directly with text utterances as input without the need for phonetic annotations, which makes the construction of larger datasets more feasible.\\n\\nIn this work, we construct a relatively large single-speaker corpus to enable wider exploration and adaptation of end-to-end TTS approaches for the Arabic language and bridge this gap in data availability, which is a stepping stone towards more inclusive spoken language technologies. To our knowledge, ClArTTS is the largest freely available single-speaker speech corpus in the Arabic language. In particular, the corpus consists of audio recordings by a male speaker of a book written in Classical Arabic sampled at 40100 Hz, which is available in the LibriVox project. To create a corpus for TTS synthesis, we segmented the audio into short utterances, checked for quality and consistency of recording conditions, and manually annotated the audio segments with fully diacritized transcriptions. The final corpus comprises 12 hours and 10 minutes of speech, which is segmented into 10334 utterances. We also built several neural TTS systems using this corpus to demonstrate the quality of the synthesized speech using subjective and objective evaluations. We evaluate the speech synthesis performance for both Classical and Modern Standard Arabic.\"}"}
{"id": "kulkarni23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related work\\n\\nThe most commonly used approaches for Arabic speech synthesizers are either based on unit selection or parametric speech synthesis [13, 14, 15]. The Arabic Speech Corpus [10], which is one of the most cited corpora for Arabic speech synthesis, contains around 3.4 hours of Modern Standard Arabic (MSA) speech recorded at 48KHz. To reduce the size of the corpus, diphone-based greedy optimization strategies were used, and nonsense or dummy utterances were added to cover the gaps of underrepresented phonemes. The Standard Arabic Single Speaker Corpus (SASSC) [16] is another MSA corpus containing 7 hours of speech, professionally recorded for TTS and ASR applications. Zine et al. [17] describe a process of extracting a 4-hour Arabic speech synthesis corpus using a pre-recorded audiobook from the Masmoo3 Audiobooks website, which was then used to construct a concatenative TTS system. Amrouche et al. [18] describe the process of creating a phonetically balanced speech corpus for Arabic, and use the constructed dataset to train a Hidden Markov Model (HMM)-based speech synthesis model. In [19], an end-to-end TTS system based on Tacotron architectures [2] is described. The models were trained using in-house professionally recorded data by two speakers. The resulting systems generate high-quality speech, but the speech datasets are not publicly available for research use.\\n\\nIn terms of the availability of resources for training high-quality TTS systems, Arabic is still considered a low-resource language. The ClArTTS corpus is an attempt to fill the resource gap in Arabic speech research by providing a relatively large single-speaker corpus that can be used to train end-to-end TTS models. The LibriVox project contains free public-domain audiobooks in many languages and has been the basis for many text-to-speech corpora, including the LJ Speech Dataset, the M-AILABS multi-lingual corpus, and the HI-FI English TTS corpus [20]. The ClArTTS corpus is the first speech corpus extracted from the LibriVox project for the Arabic language.\\n\\n3. Corpus construction\\n\\n3.1. Audio pre-processing\\n\\nFor the creation of a classical Arabic text-to-speech (ClArTTS) corpus, we selected an audiobook recorded by a single speaker from the LibriVox project. The classical book is titled Kitab Adab al-Dunya w'al-Din by Abu al-Hasan al-Mawardi (972-1058 AD). The audiobook is recorded by a single speaker and consists of approximately 16 hours of audio without accompanying text. While scanned copies of the book exist, we opted for manual annotation of the audio data to create text transcripts that truly match the audio recording using the Praat annotation tool.\\n\\nThe audiobook consists of 20 long audio files, each representing a chapter of the book in MP3 format. We converted this audio to WAV format using ffmpeg command-line tool to ensure compatibility with the Praat program. We kept the original sampling rate of 40100 Hz. We ran a rule-based Praat script to mark pauses and speech segments in the long audio files. This script created a TextGrid object for a LongSound object and set boundaries at pauses based on intensity analysis. We validated the marking of pauses and speech segments provided by the Praat tool using energy-based VAD from the Kaldi toolkit.\\n\\n3.2. Annotation process\\n\\nThe process of annotating the audiobook involved transcribing audio content into written text, along with additional tags for speech pauses, background noise, inaudible speech segments, and stuttering. The Praat tool was used for the annotation, and the annotators were given TextGrid Praat files that contained the audio recording and a framework for marking speech and pause segments. This helped the annotators efficiently and accurately transcribe the speech segments into written text.\\n\\nA team of three Arabic annotators was involved in the transcription process to ensure a reliable and accurate final transcript that considered multiple perspectives. To enhance the quality of the transcripts, two rounds of validation were conducted. The first validation was done by the annotators themselves, followed by a check by two other annotators for accuracy and consistency. The text transcripts were marked with Arabic diacritical marks to increase the accuracy of the transcripts for speech analysis and pronunciation.\\n\\nIn addition to the TextGrid Praat files, the annotators were also given a text image of the original book for reference. This made it easier for the annotators to transcribe the speech segments accurately by referring to the original text. Guidelines were provided to the annotators during the annotation process, including instructions for using abbreviations, numbers, special characters, and punctuation according to Arabic language rules. Specific speech segments were marked with tags, including [B] for background noise, [H] for stuttering or hesitation, [*] for unclear speech, and [O] for human noise. The combination of the Praat tool, three annotators, two levels of validation, text transcripts with Arabic diacritization markers, and reference materials assisted in ensuring the accuracy and reliability of the final transcripts.\\n\\n3.3. Final corpus creation\\n\\nThe total amount of original audio is around 16 hours, spanning 20 chapters, so it was recorded in multiple sessions. We observed slight variations in speaking style between the chapters, even though it was neutral (non-emotional) overall. Therefore, we conducted subjective listening tests by listening to random parts of each chapter and removed three chapters that diverge in speaking style compared to the rest.\\n\\nFor segmentation, we split each long-audio file using the textgrid obtained through the Praat tool and the manual annotation process with speech and silence segments. For ensuring high audio quality, we used a signal-to-noise ratio (SNR) to guide the selection process. We estimated the waveform amplitude distribution analysis SNR [21] by taking into account the noise power in silence (non-speech) segments adjacent to the given speech segment. We used a threshold value of 20dB SNR for the first level of speech segment selection. We concentrated adjacent speech segments to create a minimum speech segment duration of 2 seconds. Furthermore, during the concatenation process, we kept only 20% of silence segments between two speech segments if the silence segment duration was exceeding the average silence duration computed across the long audio. We also removed the preamble speech segments, during which the reader briefly talked about the LibriVox project, stated their name and book information, and may have mentioned copyright descriptions or LibriVox project-related content.\\n\\n6https://kaldi-asr.org\"}"}
{"id": "kulkarni23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Corpus statistics comparison between Arabic speech corpus (ASC), Balanced Arabic corpus (BAC) and ClArTTS.\\n\\n| Count         | ASC | BAC | ClArTTS |\\n|---------------|-----|-----|---------|\\n| Sentences     | 1,913 | 202 | 10,334 |\\n| Words         | 17,275 | 1,254 | 82,970 |\\n| Words/sentence| 9     | 6   | 8       |\\n| Unique words  | 12,144 | 975 | 27,870 |\\n| Phonemes      | 135,232 | 6,174 | 518,682 |\\n| Diphones      | 72,797 | 3,614 | 282,487 |\\n| Unique diphones | -   | -  | 682     |\\n\\nTable 2: Percentage of a subset of frequent (Top) and infrequent (bottom) diphones in the ClArTTS corpus vs. a larger text corpus (Tashkeela).\\n\\n| Diphone                | ClArTTS | Tashkeela |\\n|------------------------|---------|-----------|\\n| /char0b/charf0         | 3.62%   | 3.21%     |\\n| /char05/char0b/charcb | 3.09%   | 3.00%     |\\n| /char0b/char0d/char40 | 2.89%   | 3.53%     |\\n| /char42                | 2.82%   | 1.60%     |\\n| /charc8/char40         | 2.53%   | 1.39%     |\\n| /char05/char0b/charab | 2.32%   | 2.34%     |\\n| /char05/char0b/chard3 | 2.24%   | 1.95%     |\\n| /char41/char09/char4b | 2.19%   | 1.03%     |\\n| /char91/char0c/char1d | 0.00035%| 0.00065% |\\n| /chara1/char0c/char1d | 0.00035%| 0.00175% |\\n| /char11/char49/char0c/char4b | 0.00035%| 0.00034% |\\n| /chara1/char1d/char0b | 0.00035%| 0.00163% |\\n| /char05/charaa/char0c/char4b | 0.00035%| 0.00009% |\\n| /char05/char6d/char2e/char1a/char0b/char27 | 0.00070%| 0.00011% |\\n| /char05/char09/charaa/char0c/char4b | 0.00070%| 0.00154% |\\n\\nDuring the segmentation process, we ensured that each segmented speech utterance had a duration of at least 2 seconds and a maximum duration of 10 seconds. We also observed that the Praat pause marking script was unable to tag the last silence segments. Therefore, we manually removed the silence frame in the last audio segments marked by Praat tools. We also removed the speech segments consisting of text transcripts with non-Arabic characters.\\n\\nWe used 3% of the corpus as the test set and 97% as the training set, which results in 10000 utterances in the training set with a total duration of 11 hours and 45 minutes. For the test set, we have a total of 334 utterances, for a total duration of 25 minutes. All text files were saved in UTF-16 encoding and non-Arabic characters were removed. In addition to Arabic transcripts, we have also provided the Buckwalter [22] transliterated transcripts.\\n\\n4. Corpus statistics\\n\\nSpeech corpora that are recorded specifically for the purpose of speech synthesis typically follow a specific procedure to maximize phonetic coverage while minimizing total corpus size [11]. However, since we do not record the corpus and instead use a pre-existing audiobook, we are constrained only by the size of the audiobook. As a result, ClArTTS may not include all possible phonetic combinations, but instead follows the phonetic distribution of the language. In Figure 1, we illustrate a comparison of monophone coverage across three corpora: the Arabic Speech Corpus (ASC), the Tashkeela text corpus [23], and our ClArTTS corpus. The ASC was curated for maximum phonetic coverage, whereas the Tashkeela corpus contains randomly sampled text from various resources, which we assume represents a better representation of the natural phonetic coverage in the Arabic language. We picked this corpus in particular as it contains manually diacritized text, which is essential for our comparison. As shown in Figure 1, ClArTTS phonetic distribution is closer to the natural text distribution than the curated ASC corpus.\\n\\nWe compare our corpus statistics with previously published statistics, namely the Balanced Arabic Corpus described in [11] and the ASC corpus. The statistics are shown in Table 1. ClArTTS is the largest corpus in terms of the number of sentences, words, unique words, phonemes, and diphones, indicating that it is a more extensive and diverse corpus than the other two. ASC has the second-largest number of sentences and words, but the number of unique words is less than half of that in ClArTTS. BAC is the smallest corpus in terms of all the measures listed in the table. The only statistic where we observe a shortage in ClArTTS compared to ASC is the number of unique diphones. In the ASC, dummy utterances were recorded to artificially maximize the total number of diphones, even though these diphones are rare or impossible in the language. Therefore, this shortage in diphone coverage is unlikely to degrade TTS performance for most utterances. In Table 2, we show a subset of frequent and infrequent diphones and compare their coverage in ClArTTS and the Tashkeela text corpus, which shows that ClArTTS has good representation of frequent diphones, roughly similar to their natural distribution in the language.\\n\\n5. Baseline TTS systems\\n\\nTo verify the usability of the ClArTTS corpus for speech synthesis, we compare the performance of two baseline text-to-speech (TTS) systems, Grad-TTS [24] and Glow-TTS [25], trained on ClArTTS corpus vs. the Arabic Speech Corpus (ASC). We used the default network parameters as mentioned in the papers [24] and [25] respectively for these TTS systems without using any explicit Arabic grapheme to phoneme model.\"}"}
{"id": "kulkarni23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation metrics computed to measure the performance of baseline end-to-end TTS systems on two Arabic speech synthesis corpora, namely Arabic speech corpus (ASC) and Classical Arabic TTS corpus (ClArTTS).\\n\\n| System     | Corpus   | MOS     | PESQ   | MCD    | Lf0 RMSE | BAP | Speaker similarity |\\n|------------|----------|---------|--------|--------|----------|-----|--------------------|\\n| GroundTruth| ASC      | 4.01 \u00b1 0.1 | \u2014      | \u2014      | \u2014        | \u2014   | \u2014                  |\\n| GroundTruth| ClArTTS  | 4.39 \u00b1 0.1 | \u2014      | \u2014      | \u2014        | \u2014   | \u2014                  |\\n| Grad-TTS   | ASC      | 3.02 \u00b1 0.2 | 1.48   | 6.38   | 12.25    | 1.14| 0.51               |\\n| Glow-TTS   | ASC      | 3.19 \u00b1 0.2 | 1.41   | 6.27   | 10.03    | 1.12| 0.56               |\\n| Grad-TTS   | ClArTTS  | 3.63 \u00b1 0.2 | 2.25   | 4.94   | 9.03     | 0.85| 0.71               |\\n| Glow-TTS   | ClArTTS  | 3.84 \u00b1 0.1 | 2.23   | 4.83   | 8.04     | 0.93| 0.78               |\\n\\nTo synthesize the speech from the predicted Mel spectrograms, we opted for a Hi-Fi GAN-based neural vocoder [26]. The ASC and ClArTTS corpora have speech utterances with different sampling rates, 48000 Hz and 40100 Hz, respectively. Therefore, we trained two Hi-Fi GAN neural vocoders to be compatible with the different sampling rates of both corpora. We used the V1 configuration of the Hi-Fi GAN neural vocoder for training both neural vocoders as detailed in [26]. We applied the short-time Fourier transform (STFT) with an FFT length of 1024, a hop length of 256, and a window size of 1024, and extracted Mel spectrograms using 80 Mel filters.\\n\\n6. Evaluation and results\\n\\nIn Table 3, we present the performance of the two TTS systems using subjective and objective evaluations. We evaluated E2E TTS systems using a Mean Opinion Score (MOS) [27] based listening test. Each listener had to assign a score for synthesized speech utterance on a scale between 1 to 5 considering the intelligibility, naturalness, and quality of speech utterance. The speech utterances for subjective evaluation were selected randomly from test sets of ClArTTS and ASC. A total of 30 Arabic listeners participated in this MOS test and results are displayed in Table 3 with an associated 95% confidence interval.\\n\\nFurthermore, to validate the coherence of subjective listening test with objective evaluation, we opted for Perceptual Evaluation of Speech Quality (PESQ) [28] as an automated assessment of audio quality which takes into account various factors such as Audio sharpness, volume, background noise, lag in audio, clipping and audio interference. PESQ is computed on a scale from -0.5 to 4.5, where 4.5 represents the best similarity.\\n\\nWe used MCD (Mel Cepstral Distortion), an objective evaluation metric that measures the spectral distortion between the synthesized speech and the original speech signal. Lf0 RMSE (Root Mean Square Error of Log F0): an objective evaluation metric that measures the pitch accuracy of synthesized speech. BAP (Band Aperiodicity): an objective evaluation metric that measures the spectral envelope accuracy of synthesized speech. These evaluations are conducted by computing errors between reference speech utterances and synthesized speech utterances aligned using the dynamic time-warping algorithm. We selected a cosine distance-based speaker similarity score [29] to measure the consistency of the speaker's voice quality in synthesized speech. We utilized the pre-trained ECAPA-TDNN based speaker embedding extractor to measure the similarity scores from synthesized speech and reference speech from the original speech synthesis corpus [30].\\n\\nTable 3 shows that the ground truth samples of both corpora have higher MOS scores than the synthesized speech generated by the two TTS systems. The Glow-TTS system outperforms the Grad-TTS system in terms of MOS and PESQ scores for both corpora. The ClArTTS corpus has higher MOS scores and lower MCD, Lf0 RMSE, and BAP scores than the ASC corpus, indicating the ClArTTS corpus provides better synthesis quality. The speaker similarity scores of the synthesized speech are relatively low for ASC-based TTS systems, compared to the ClArTTS corpus counterpart. Thus, it shows that ClArTTS-based systems are better at retaining the speaker's voice characteristics in synthesized speech. The synthesized speech quality of baseline TTS systems can further be improved by using grapheme to phoneme system as additional pre-processing.\\n\\n7. Conclusion\\n\\nIn this work, we presented the ClArTTS corpus, a single male speaker corpus extracted from a pre-recorded LibriVox audio book. The ClArTTS corpus was developed to facilitate research in Arabic end-to-end TTS synthesis, which generally requires larger datasets compared to earlier models. The final corpus consists of a total of 12 hours and 10 mins of annotated speech, the largest single-speaker corpus freely available in the Arabic language. We illustrated the comparative advantage via corpus statistics compared to the available smaller corpora that were previously curated for the purpose of speech synthesis. In addition, we trained Glow-TTS and Grad-TTS systems using our ClArTTS corpus and compared the performance against the systems trained on the smaller Arabic Speech Corpus. Using both subjective and objective evaluations, our results indicate an overall better quality of synthesized speech using the ClArTTS corpus. The results also indicate that there is room for improvement, which we leave for future research. The ClArTTS corpus is now freely available for research purposes along with an interactive TTS demo at www.clartts.com.\\n\\nFinally, we emphasize the caveat that the corpus is based on Classical Arabic text, which can be different from Modern Standard Arabic (MSA) in some aspects, such as lexical distribution. While many Arabic datasets liberally mix the two variants (e.g. the Tashkeela corpus contains text from both CA and MSA) as they seem indistinguishable, they do have differences that may prove to be consequential [31]. In our reported results, we included MSA utterances from ASC as well as utterances from the ClArTTS test set to reflect the potential quality in both of these variants. We leave any additional analysis of these differences for future work.\"}"}
{"id": "kulkarni23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] N. Li, S. Liu, Y. Liu, S. Zhao, and M. Liu, \u201cNeural speech synthesis with transformer network,\u201d in Association for the Advancement of Artificial Intelligence (AAAI), 2019.\\n\\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \u201cNatural tts synthesis by conditioning wavenet on mel spectrogram predictions,\u201d 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 4779\u20134783, 2017.\\n\\n[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech: Fast, robust and controllable text to speech,\u201d in proceedings of Conference on Neural Information Processing Systems (NeurIPS), 2019.\\n\\n[4] A. J. Hunt and A. W. Black, \u201cUnit selection in a concatenative speech synthesis system using a large speech database,\u201d in 1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings, vol. 1. IEEE, 1996, pp. 373\u2013376.\\n\\n[5] H. Zen, T. Nose, J. Yamagishi, S. Sako, T. Masuko, A. W. Black, and K. Tokuda, \u201cThe hmm-based speech synthesis system (hts) version 2.0.\u201d SSW, vol. 6, pp. 294\u2013299, 2007.\\n\\n[6] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. V. Le, Y. Agiomyrgiannakis, R. Clark, and R. Saurous, \u201cTacotron: Towards end-to-end speech synthesis,\u201d in proceedings of INTERSPEECH, 2017.\\n\\n[7] R. J. Weiss, R. Skerry-Ryan, E. Battenberg, S. Mariooryad, and D. P. Kingma, \u201cWave-tacotron: Spectrogram-free end-to-end text-to-speech synthesis,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5679\u20135683.\\n\\n[8] Z. Alyafeai, M. Masoud, M. Ghaleb, and M. S. Al-shaibani, \u201cMasader: Metadata sourcing for arabic text and speech data resources,\u201d in International Conference on Language Resources and Evaluation, 2021.\\n\\n[9] A. Ahmed, N. Ali, M. S. Alzubaidi, W. Zaghouani, A. A. Abd-alrazaq, and M. J. Househ, \u201cFree and accessible arabic corpora: A scoping review,\u201d Computer Methods and Programs in Biomedicine Update, 2022.\\n\\n[10] N. Halabi, \u201cModern standard arabic phonetics for speech synthesis,\u201d Ph.D. dissertation, UNIVERSITY OF SOUTHAMPTON, 2016.\\n\\n[11] A. Amrouche, A. Abed, K. Ferrat, K. N. Boubakeur, Y. Bentrcia, and L. Falek, \u201cBalanced arabic corpus design for speech synthesis,\u201d International Journal of Speech Technology, vol. 24, no. 3, pp. 747\u2013759, 2021.\\n\\n[12] T. Lambert, N. Braunschweiler, and S. Buchholz, \u201cHow (not) to select your voice corpus: random selection vs. phonologically balanced.\u201d in SSW. Citeseer, 2007, pp. 264\u2013269.\\n\\n[13] R. Abdelmalek and Z. Mnasri, \u201cHigh quality arabic text-to-speech synthesis using unit selection,\u201d 2016 13th International Multi-Conference on Systems, Signals & Devices (SSD), pp. 1\u20135, 2016.\\n\\n[14] A. A. Shalaby, O. A. Dakkak, and N. Ghneim, \u201cAn arabic text to speech based on semi-syllable concatenation,\u201d International Review on Computers and Software, vol. 11, pp. 1178\u20131186, 2016.\\n\\n[15] O. O. Khalifa, M. Z. Obaid, A. W. Naji, and J. I. Daoud, \u201cA rule-based arabic text-to-speech system based on hybrid synthesis technique,\u201d vol. 5, pp. 342\u2013354.\\n\\n[16] I. A. Almosallam, A. Alkhalifa, M. Al-Ghamdi, M. I. Alkanhal, and A. Alkhairy, \u201cSassc: a standard arabic single speaker corpus,\u201d in Speech Synthesis Workshop, 2013.\\n\\n[17] O. Zine and A. Meziane, \u201cNovel approach for quality enhancement of arabic text to speech synthesis,\u201d 2017 International Conference on Advanced Technologies for Signal and Image Processing (ATSIP), pp. 1\u20136, 2017.\\n\\n[18] A. Amrouche, A. Abed, K. Ferrat, K. N. Boubakeur, Y. Bentrcia, and L. Falek, \u201cBalanced arabic corpus design for speech synthesis,\u201d International Journal of Speech Technology, vol. 24, pp. 747\u2013759, 2021.\\n\\n[19] A. Abdelali, N. Durrani, C. Demiro \u02d8glu, F. Dalvi, H. Mubarak, and K. Darwish, \u201cNatiq: An end-to-end text-to-speech system for arabic,\u201d Proceedings of the The Seventh Arabic Natural Language Processing Workshop (WANLP), pp. 394\u2013398, 2022.\\n\\n[20] E. Bakhturina, V. Lavrukhin, B. Ginsburg, and Y. Zhang, \u201cHi-fi multi-speaker english tts dataset,\u201d in Interspeech, 2021.\\n\\n[21] C. Kim and R. M. Stern, \u201cRobust signal-to-noise ratio estimation based on waveform amplitude distribution analysis,\u201d in Interspeech, 2008.\\n\\n[22] T. Buckwalter, \u201cBuckwalter arabic morphological analyzer version 1.0,\u201d in Linguistic Data Consortium, 2002.\\n\\n[23] T. Zerrouki and A. Balla, \u201cTashkeela: Novel corpus of arabic vocalized texts, data for auto-diacritization systems,\u201d Data in Brief, vol. 11, pp. 147\u2013151, 2017.\\n\\n[24] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. A. Kudinov, \u201cGrad-TTS: A diffusion probabilistic model for text-to-speech,\u201d proceedings of International Conference on Machine Learning (ICML), 2021.\\n\\n[25] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-TTS: A generative Flow for text-to-speech via monotonic alignment search,\u201d In proceedings of Conference on Neural Information Processing Systems (NIPS), 2020.\\n\\n[26] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in proceedings of Conference on Neural Information Processing Systems (NIPS), 2020.\\n\\n[27] M. D. Polkosky and J. R. Lewis, \u201cExpanding the MOS: development and psychometric evaluation of the MOS-R and MOS-X,\u201d International Journal of Speech Technology, vol. 6, pp. 161\u2013182, 2003.\\n\\n[28] A. W. Rix, J. G. Beerends, M. Hollier, and A. P. Hekstra, \u201cPerceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,\u201d 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221), vol. 2, pp. 749\u2013752, vol.2, 2001.\\n\\n[29] A. Kulkarni, V. Colotte, and D. Jouvet, \u201cAnalysis of expressivity transfer in non-autoregressive end-to-end multispeaker tts systems,\u201d in Interspeech, 2022.\\n\\n[30] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d in Interspeech, 2020.\\n\\n[31] I. S. Alkhazi and W. J. Teahan, \u201cClassifying and segmenting classical and modern standard arabic using minimum cross-entropy,\u201d International Journal of Advanced Computer Science and Applications, vol. 8, no. 4, 2017.\"}"}
