{"id": "behera23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Multi-Lingual Audio Question Answering\\n\\nSwarup Ranjan Behera, Pailla Balakrishna Reddy, Achyut Mani Tripathi,\\nMegavath Bharadwaj Rathod, Tejesh Karavadi\\n\\nReliance Jio - Artificial Intelligence Centre of Excellence (AICoE), Hyderabad, India\\n\\nAbstract\\n\\nAudio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA.\\n\\nIndex Terms: audio question answering, multi-lingual audio question answering, cross-modal task\\n\\n1. Introduction\\n\\nAudio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer [1]. Similar to visual question answering being important for enabling accessibility for visually impaired individuals [2, 3], AQA tasks have the potential to assist hearing-impaired individuals [4]. For example, a hearing-impaired person can utilize AQA on her wearable device to ask for and receive information about the surrounding acoustic environment in a question-answer style using natural language.\\n\\nAQA is a core task in multi-modal research encompassing audio and language modeling. However, datasets for AQA task are only available in English. We present an example of AQA data in Table 1. AQA research has seen only three datasets, ClothoAQA [1], CLEAR [5], and DAQA [4]. ClothoAQA is a crowdsourced audio question-answering dataset generated from the audio files of the Clotho dataset [6], which contains day-to-day sounds occurring in the environment, such as water, nature, city, etc. In contrast, CLEAR and DAQA datasets are generated programmatically. Though CLEAR and DAQA have significantly more question-answer pairs than ClothoAQA, they lack diversity and challenges that are present in real-world data. We present a comparative overview of these three datasets in Table 2.\\n\\nBecause English-based AQA has garnered all the attention so far, AQA's promise applies exclusively to a privileged subset of human populations. In this paper, we take a step towards addressing this problem. We present a machine-translated multi-lingual audio question-answering dataset, mClothoAQA. mClothoAQA is built by generating multi-lingual question-answer pairs from the recently-proposed ClothoAQA dataset [1]. We also propose a multi-lingual AQA model to demonstrate the usage of our mClothoAQA dataset. In summary, our main contributions are\\n\\n\u2022 We present a multi-lingual audio question answering dataset, mClothoAQA, in eight diverse languages: English (en), French (fr), Hindi (hi), German (de), Spanish (es), Italian (it), Dutch (nl), and Portuguese (pt). The mClothoAQA dataset comprises 1991 audio files and 35838 question-answer pairs for each language.\\n\\nThe rest of this paper is organized as follows. Section 2 briefly reviews the related work. Section 3 introduces the mClothoAQA dataset and presents its details. Section 4 presents the proposed multi-lingual AQA baseline model. Section 5 reports and analyzes the results of the experiments. Finally, Section 6 concludes the paper.\\n\\nTable 1: Sample audio question answering data from ClothoAQA [1] dataset containing the \u27e8audio, question, answer\u27e9 triplets.\\n\\n| Audio: river mouth3.wav | Time (s) | Question | Answer |\\n|-------------------------|----------|----------|--------|\\n| 0                       | 0.5      | Are there waves? | Yes |\\n| 1.5                     | 2        | How many times does the water splash? | Eleven |\\n| 3.4                     | 4        | Is the area dry? | No |\\n| 4.5                     | 6        | Is the sound falling water? | Yes |\\n| 5.5                     | 8        | Is water flowing? | Yes |\\n| 6.5                     | 10       | What is flowing? | Water |\\n\\nINTERSPEECH 2023\\n20-24 August 2023, Dublin, Ireland\"}"}
{"id": "behera23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparative overview of AQA datasets. Number of audios (#Audios). Audio duration in hours (Dur(h)). Maximum duration of audios in seconds (MD(s)). Average duration of audio in seconds (AD(s)). Question-answer source (QAS) indicates whether questions and answers are crowdsourced (C) or generated programmatically (P). Language (L) of the question and answer: English (E). Number of unique questions (#Ques). Number of unique answers (#Ans). Number of audio files in the training (Train), validation (Val), and test (Test) subsets.\\n\\n| Dataset  | #Audios | Dur(h) | MD(s) | AD(s) | QAS | L | #Ques | #Ans | Train | Val | Test |\\n|----------|---------|--------|-------|-------|-----|---|-------|------|-------|-----|------|\\n| ClothoAQA | 1991    | 12.44  | 30    | 21    | C   | E | 9153  | 830  | 1174  | 344 | 473  |\\n| CLEAR    | 50000   | 3.12   | 0.4   | 0.25  | P   | E | 130957| 47   | 35000 | 7500| 7500 |\\n| DAQA     | 100000  | 2244.4 | 178.2 | 80.8  | P   | E | 599294| 36   | 80000 | 10000| 10000|\\n\\n2. Related Work\\n\\nQuestion Answering (QA) refers to the task of providing natural language answers to questions posed in natural language. There is a large body of work in QA depending on the type of additional input on which the question is targeted: (i) Textual Question Answering (TQA) when a piece of text is used as an additional input, (ii) Visual Question Answering (VQA) when an image is used as an additional input, (iii) Audio Question Answering (AQA) when an audio signal is used as an additional input, and (iv) Video Question Answering (VideoQA) when a video is used as an additional input.\\n\\nAQA is a relatively new area of research. To the best of the authors' knowledge, only three works have been published in AQA research so far: ClothoAQA, CLEAR, and DAQA. In terms of data, the CLEAR dataset is similar to the DAQA dataset. These are both synthetic datasets of audio sequences, questions, and answers. CLEAR and DAQA datasets are generated programmatically, while ClothoAQA is a crowdsourced dataset generated from the audio files of Clotho dataset, an audio captioning dataset. Although CLEAR and DAQA have significantly more question-answer pairs than ClothoAQA, they lack diversity and challenges that are present in real-world data. This is why we built our multi-lingual mClothoAQA dataset from the ClothoAQA dataset. Refer to Table 2 for more details on these datasets. In terms of modeling, all of the previous models on AQA are built for English. Further, it treats AQA as a classification task over a predefined set of the top answers. In this paper, we are introducing a multi-lingual AQA dataset and a multi-lingual AQA model for the first time.\\n\\n3. mClothoAQA Dataset\\n\\nLike most machine learning tasks, obtaining high-quality labeled data is the main bottleneck for multi-lingual AQA. To address this, we have built a multi-lingual AQA dataset called mClothoAQA. We translated the questions and answers from the ClothoAQA dataset into seven other languages using Google's machine translation API. We also tested a few other open-source machine translation tools, but their translation quality did not match that of Google's. We evaluated the translations using metrics such as BLEU, ROUGE, and METEOR. For this work, we selected seven diverse languages: French (fr), Hindi (hi), German (de), Spanish (es), Italian (it), Dutch (nl), and Portuguese (pt). After the machine translation, the question-answer pairs were verified and adjusted by humans. Please refer to Table 3 for examples of mClothoAQA.\"}"}
{"id": "behera23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Distribution of the first four words for all questions in the training set of mClothoAQA-en dataset. The innermost ring represents the first word, and radiating rings represent subsequent words. Arc lengths are proportional to the number of questions containing the word. Words with a frequency less than 35 are omitted for clarity.\\n\\nFigure 2: Word clouds of the answers for different question types in the training set of mClothoAQA-en. (a) all the questions, (b) \u2018what\u2019 type questions, (c) \u2018how many\u2019 type questions, and (d) questions other than \u2018what\u2019 and \u2018how many\u2019 type such as \u2018is/are/was/were/do/does/did/the\u2019.\\n\\nSimilar analyses can be conducted for the other language subsets (e.g., mClothoAQA-hi, the Hindi subset of mClothoAQA) of the mClothoAQA dataset.\\n\\n4. Modeling\\nAll of the previous work on AQA has been built for English. Further, it treats AQA as a classification task over a pre-defined set of the top (English) answers. In this section, we present a multi-lingual AQA baseline model.\\n\\nOur Bidirectional Long Short-Term Memory (BiLSTM) [25] based model architecture for multi-lingual AQA is shown in Figure 3. The inputs to the model are the audio signal and the question (in a particular language) related to the audio. The mClothoAQA dataset has a relatively small amount of audio files and texts (questions) to learn a good representation. We take advantage of existing pre-trained audio embeddings (OpenL3 [26]) and text embeddings (fastText [27]) to extract audio and text features, respectively. Both the audio and text sides extract their respective audio features and text features and generate fixed-size representations. These representations are concatenated and passed through fully connected layers for classification. The model\u2019s output is either \u2018yes\u2019 or \u2018no\u2019 in the case of binary classification or one of 828 single-word answers in the case of multi-class classification.\\n\\nOn the audio side, we first extract $X \\\\in \\\\mathbb{R}^{T \\\\times 128}$ Mel spectrogram from the input audio signal with 128 mel bands and $T$ time frames. The Mel spectrogram is then fed to OpenL3 [26] environment model to compute $X_{emb} \\\\in \\\\mathbb{R}^{T' \\\\times 512}$ deep audio embeddings, where $T'$ is the number of output time frames from the OpenL3 model and 512 is the audio embedding size. These embeddings are extracted using a hop size of 0.1 seconds. In order to understand temporal correlations and transform them into a fixed-size representation, the audio embeddings are then processed through a succession of BiLSTM layers, $\\\\text{BiLSTM}_n(X_{n-1})$. The bidirectional LSTM is given by $X_{n} = \\\\text{BiLSTM}_n(X_{n-1})$ (1) where $X_{0} = X_{emb}$. If $h$ is the number of hidden units in the BiLSTM, then $X_{n} \\\\in \\\\mathbb{R}^{T' \\\\times 2h}$. We then choose as output $x_{n} \\\\in \\\\mathbb{R}^{2h}$, the final time step of the last BiLSTM layer to represent the fixed-size audio embedding.\\n\\nOn the text side, a Language Identifier (LangId = L) module specifies the language of the input question. The LangId is used to select the pre-trained text embeddings for that specific language. To encode the input question into word embeddings, we utilize FastText [27] pre-trained word vectors. These word vectors for 157 languages can be accessed here 2. If the input question $Q$ has $K$ words, the word embeddings using FastText are $\\\\text{FastText}$.https://tinyurl.com/w658k9ah.\"}"}
{"id": "behera23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are denoted as $Q_{\\\\text{emb}} \\\\in \\\\mathbb{R}^{K \\\\times 300}$, where 300 represents the dimensionality of the word embeddings. The word embeddings are then processed through a series of bidirectional LSTM layers. If $h'$ is the size of the hidden units in the BiLSTM, we again choose as output $q_n \\\\in \\\\mathbb{R}^{2h'}$, the final time step of the last BiLSTM layer to represent the fixed size word embedding for the input question.\\n\\nFor both the audio and text side, we use a hidden size of 128 for the BiLSTM layers with a dropout of 0.2 for the binary classifier and a hidden size of 512 for the multi-class single-word answers classifier. The audio and text side outputs are concatenated and passed through a series of fully connected layers $\\\\text{Dense}_k$ with $k = 1, 2$ with ReLU non-linearity. The fully connected layers combine the learned features of both the audio and the textual question. This is given by $D_n = \\\\text{Dense}_k(D_{k-1})$ (2)\\n\\nwhere $D_0 = \\\\text{concat}[x_n, q_n]$, and $\\\\text{concat}$ represents concatenation operator. The binary classifier's two dense layers have 256 and 128 neurons each. However, the multi-class classifier's two dense layers have 1024 neurons each, increasing the model's capacity to capture finer details and classify among 828 answer classes.\\n\\nThe last dense layer's output is then passed to a classification layer, which is another dense layer with the number of neurons equal to the number of distinct answer classes. The output of the classification layer is $\\\\hat{y} \\\\in [0, 1]^C$, where $C$ represents the number of answer classes in the dataset. For the 'yes' or 'no' binary classifier, the classification layer is a simple logistic regressor with one neuron. In the multi-class classifier, the classification layer comprises 828 neurons, each representing a distinct single-word answer. The softmax activation function is then applied after the classification layer.\\n\\n5. Experimental Results and Analysis\\n\\nWe trained and evaluated the multi-lingual AQA models separately on each of the eight languages of mClothoAQA dataset. In order to create the splits for the binary classifier, we select the 'yes' or 'no' questions from the associated data splits. 1174, 344, and 473 audio files with twelve 'yes' or 'no' question-answer pairings per file are used for training, validation, and testing, respectively. We train and evaluate the binary classifier on the following three subsets of data to examine how well it performs when different annotators provide contradictory answers to the same question.\\n\\n\u2022 Unfiltered: All the question-answer pairs are considered, even if they have contradicting answers.\\n\u2022 Unanimous: Only those question-answer pairs are considered where all three annotators have responded unanimously.\\n\u2022 Majority: For each question, the answer provided by at least two of the three annotators is considered.\\n\\nSimilarly, for the multi-class classification task, we select the single-word answers from the respective data splits, resulting in the same number of audio files as the original splits, with each file having six question-answer pairs.\\n\\nAll the models are trained with the Adam optimizer with a learning rate of 0.001, $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$. The models are trained for 100 epochs with cross-entropy loss, and the model with the best validation score is used for testing. We use 'accuracy' as the evaluation metric for the binary classification task. For the multi-class classification task, we use 'top-1', 'top-5', 'top-10' accuracy. The 'top-1' accuracy is conventional, i.e., the model's answer must match the expected answer. The 'top-5' accuracy means any of the model's five highest probability answers must match the expected answer.\\n\\nTable 4 summarises the results of our binary classification experiments on the mClothoAQA dataset for each of the eight languages. It is evident from the results that the model performs better when the answers are unanimous, indicating the intelligible presence of the answer in the audio.\\n\\nTable 5: Accuracies (%) of single-word answer multi-class classifier on multi-lingual AQA data.\\n\\n| Languages | Top-1 Acc | Top-5 Acc | Top-10 Acc |\\n|-----------|-----------|-----------|------------|\\n| English (en) | 53.73 | 91.68 | 98.57 |\\n| French (fr) | 51.96 | 90.24 | 95.93 |\\n| Hindi (hi) | 53.31 | 91.54 | 97.41 |\\n| German (de) | 51.29 | 89.50 | 96.52 |\\n| Spanish (es) | 52.07 | 90.19 | 97.09 |\\n| Italian (it) | 51.62 | 89.69 | 95.70 |\\n| Dutch (nl) | 53.09 | 91.05 | 95.38 |\\n| Portuguese (pt) | 52.48 | 90.70 | 96.96 |\\n\\nand 'top-10' accuracy. The 'top-1' accuracy is conventional, i.e., the model's answer must match the expected answer. The 'top-5' accuracy means any of the model's five highest probability answers must match the expected answer.\\n\\nTable 4 summarises the results of our binary classification experiments on the mClothoAQA dataset for each of the eight languages. It is evident from the results that the model performs better when the answers are unanimous, indicating the intelligible presence of the answer in the audio.\\n\\nThe results of our single-word multi-lingual multi-class classifier are presented in Table 5. We also use top-5 and top-10 accuracy metrics to evaluate the classifier's performance, considering the high number of unique answer classes (828). The results indicate that the model is beginning to capture the relationships between the multi-modal data.\\n\\nThis is the first work on multi-lingual AQA; therefore, conducting a comparative analysis is out of scope. Our future goals to enhance multi-lingual AQA classification accuracy are as follows: (i) developing a cross-lingual, unified, extensible, open-ended, and end-to-end transformer-based mAQA model, and (ii) creating a scalable translation-based framework for generating high-quality mAQA data based on audio captions.\\n\\n6. Conclusions\\n\\nWe take initial steps towards multi-lingual AQA by proposing scalable solutions for data creation and modeling. Our contributions include the creation of a multi-lingual AQA dataset in eight diverse languages, aimed at driving progress in multi-lingual AQA modeling. Additionally, we establish a baseline multi-lingual AQA model that demonstrates good performance across the eight languages. We believe that the mClothoAQA dataset, the multi-lingual AQA model, and the experimental results presented in this work will serve as benchmarks for future research in multi-lingual audio question answering.\"}"}
{"id": "behera23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, \u201cClotho-aqa: A crowdsourced dataset for audio question answering,\u201d in 30th European Signal Processing Conference (EUSIPCO). IEEE, 2022, pp. 1140\u20131144.\\n\\n[2] D. Gurari, Q. Li, A. J. Stangl, A. Guo, C. Lin, K. Grauman, J. Luo, and J. P. Bigham, \u201cVizwiz grand challenge: Answering visual questions from blind people,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Computer Vision Foundation / IEEE Computer Society, 2018, pp. 3608\u20133617.\\n\\n[3] W. S. Lasecki, P. Thiha, Y. Zhong, E. L. Brady, and J. P. Bigham, \u201cAnswering visual questions with conversational crowd assistants,\u201d in 15th International ACM SIGACCESS Conference on Computers and Accessibility (ASSETS). ACM, 2013, pp. 18:1\u201318:8.\\n\\n[4] H. M. Fayek and J. Johnson, \u201cTemporal reasoning via audio question answering,\u201d IEEE ACM Transactions on Audio, Speech, and Language Processing (TASLP), vol. 28, pp. 2283\u20132294, 2020.\\n\\n[5] J. Abdelnour, G. Salvi, and J. Rouat, \u201cClear: A dataset for compositional language and elementary acoustic reasoning,\u201d 2019. [Online]. Available: https://dx.doi.org/10.21227/7x26-a025\\n\\n[6] K. Drossos, S. Lipping, and T. Virtanen, \u201cClotho: an audio captioning dataset,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 736\u2013740.\\n\\n[7] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSquad: 100,000+ questions for machine comprehension of text,\u201d in Conference on Empirical Methods in Natural Language Processing (EMNLP). The Association for Computational Linguistics, 2016, pp. 2383\u20132392.\\n\\n[8] P. Rajpurkar, R. Jia, and P. Liang, \u201cKnow what you don\u2019t know: Unanswerable questions for squad,\u201d in 56th Annual Meeting of the Association for Computational Linguistics (ACL). Association for Computational Linguistics, 2018, pp. 784\u2013789.\\n\\n[9] A. Trischler, T. Wang, X. Yuan, J. Harris, A. Sordoni, P. Bachman, and K. Suleman, \u201cNewsqa: A machine comprehension dataset,\u201d in 2nd Workshop on Representation Learning for NLP, Rep4NLP@ACL. Association for Computational Linguistics, 2017, pp. 191\u2013200.\\n\\n[10] A. Agrawal, J. Lu, S. Antol, M. Mitchell, C. L. Zitnick, D. Parikh, and D. Batra, \u201cVQA: visual question answering,\u201d International Journal of Computer Vision, vol. 123, no. 1, pp. 4\u201331, 2017.\\n\\n[11] K. Kafle and C. Kanan, \u201cVisual question answering: Datasets, algorithms, and future challenges,\u201d Computer Vision and Image Understanding, vol. 163, pp. 3\u201320, 2017.\\n\\n[12] H. Gao, J. Mao, J. Zhou, Z. Huang, L. Wang, and W. Xu, \u201cAre you talking to a machine? dataset and methods for multilingual image question,\u201d in Annual Conference on Neural Information Processing Systems, 2015, pp. 2296\u20132304.\\n\\n[13] Y. Goyal, T. Khot, A. Agrawal, D. Summers-Stay, D. Batra, and D. Parikh, \u201cMaking the V in VQA matter: Elevating the role of image understanding in visual question answering,\u201d International Journal of Computer Vision, vol. 127, no. 4, pp. 398\u2013414, 2019.\\n\\n[14] J. Lei, L. Yu, M. Bansal, and T. L. Berg, \u201cTVQA: localized, compositional video question answering,\u201d in Conference on Empirical Methods in Natural Language Processing (EMNLP). Association for Computational Linguistics, 2018, pp. 1369\u20131379.\\n\\n[15] K. Kim, M. Heo, S. Choi, and B. Zhang, \u201cDeepstory: Video story QA by deep embedded memory networks,\u201d in 26th International Joint Conference on Artificial Intelligence (IJCAI), 2017, pp. 2016\u20132022.\\n\\n[16] D. Patel, R. Parikh, and Y. Shastri, \u201cRecent advances in video question answering: A review of datasets and methods,\u201d CoRR, vol. abs/2101.05954, 2021.\\n\\n[17] Z. Yu, D. Xu, J. Yu, T. Yu, Z. Zhao, Y. Zhuang, and D. Tao, \u201cActivitynet-qa: A dataset for understanding complex web videos via question answering,\u201d in 33rd AAAI Conference on Artificial Intelligence (AAAI). AAAI Press, 2019, pp. 9127\u20139134.\\n\\n[18] M. Tapaswi, Y. Zhu, R. Stiefelhagen, A. Torralba, R. Urtasun, and S. Fidler, \u201cMovieqa: Understanding stories in movies through question-answering,\u201d in IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE Computer Society, 2016, pp. 4631\u20134640.\\n\\n[19] K. Zeng, T. Chen, C. Chuang, Y. Liao, J. C. Niebles, and M. Sun, \u201cLeveraging video descriptions to learn video question answering,\u201d in 31st AAAI Conference on Artificial Intelligence (AAAI). AAAI Press, 2017, pp. 4334\u20134340.\\n\\n[20] L. Zhu, Z. Xu, Y. Yang, and A. G. Hauptmann, \u201cUncovering the temporal context for video question answering,\u201d International Journal of Computer Vision, vol. 124, no. 3, pp. 409\u2013421, 2017.\\n\\n[21] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in 40th Annual Meeting on Association for Computational Linguistics (ACL). Association for Computational Linguistics, 2002, pp. 311\u2013318.\\n\\n[22] C.-Y. Lin, \u201cRouge: A package for automatic evaluation of summaries,\u201d in Text Summarization Branches Out: Proceedings of the ACL-04 Workshop. Association for Computational Linguistics, 2004, pp. 74\u201381.\\n\\n[23] A. Lavie and A. Agarwal, \u201cMeteor: An automatic metric for mt evaluation with high levels of correlation with human judgments,\u201d in Proceedings of the Second Workshop on Statistical Machine Translation. Association for Computational Linguistics, 2007, pp. 228\u2013231.\\n\\n[24] K. Sechidis, G. Tsoumakas, and I. P. Vlahavas, \u201cOn the stratification of multi-label data,\u201d in European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases (ECML PKDD), vol. 6913. Springer, 2011, pp. 145\u2013158.\\n\\n[25] M. Schuster and K. Paliwal, \u201cBidirectional recurrent neural networks,\u201d IEEE Transactions on Signal Processing, vol. 45, pp. 2673 \u2013 2681, 1997.\\n\\n[26] A. L. Cramer, H.-H. Wu, J. Salamon, and J. P. Bello, \u201cLook, listen, and learn more: Design choices for deep audio embeddings,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 3852\u20133856.\\n\\n[27] T. Mikolov, E. Grave, P. Bojanowski, C. Puhrsch, and A. Joulin, \u201cAdvances in pre-training distributed word representations,\u201d CoRR, vol. abs/1712.09405, 2017.\"}"}
