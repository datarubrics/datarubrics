{"id": "faustini23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. References\\n\\n[1] C. Rzepka, \\\"Examining the use of voice assistants: A value-focused thinking approach,\\\" 2019.\\n\\n[2] I. Lopatovska, K. Rink, I. Knight, K. Raines, K. Cosenza, H. Williams, P. Sorsche, D. Hirsch, Q. Li, and A. Martinez, \\\"Talk to me: Exploring user interactions with the Amazon Alexa,\\\" Journal of Librarianship and Information Science, vol. 51, no. 4, pp. 984\u2013997, 2019.\\n\\n[3] M. Tabassum, T. Kosinski, A. Frik, N. Malkin, P. Wijesekera, S. Egelman, and H. R. Lipford, \\\"Investigating users' preferences and expectations for always-listening voice assistants,\\\" Proc. ACM Interact. Mob. Wearable Ubiquitous Technol., vol. 3, no. 4, pp. 153:1\u2013153:23, 2019. [Online]. Available: https://doi.org/10.1145/3369807\\n\\n[4] H. Sahijwani, J. I. Choi, and E. Agichtein, \\\"Would You Like to Hear the News? Investigating Voice-Based Suggestions for Conversational News Recommendation,\\\" New York, NY, USA: Association for Computing Machinery, 2020, p. 437\u2013441. [Online]. Available: https://doi.org/10.1145/3343413.3378013\\n\\n[5] X. Ma and A. Liu, \\\"Challenges in supporting exploratory search through voice assistants,\\\" in Proceedings of the 2nd Conference on Conversational User Interfaces, CUI 2020, Bilbao, Spain, July 22-24, 2020, M. I. Torres, S. Schl\u00f6gl, L. Clark, and M. Porcheron, Eds. ACM, 2020, pp. 47:1\u201347:3. [Online]. Available: https://doi.org/10.1145/3405755.3406152\\n\\n[6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and S. Vigna, \\\"Query suggestions using query-flow graphs,\\\" in Proceedings of the 2009 workshop on Web Search Click Data, WSCD@WSDM 2009, Barcelona, Spain, February 9, 2009, N. Craswell, R. Jones, G. Dupret, and E. Viegas, Eds. ACM, 2009, pp. 56\u201363. [Online]. Available: https://doi.org/10.1145/1507509.1507518\\n\\n[7] C. Rosset, C. Xiong, X. Song, D. Campos, N. Craswell, S. Tiwary, and P. N. Bennett, \\\"Leading conversational search by suggesting useful questions,\\\" in WWW \u201920: The Web Conference 2020, Taipei, Taiwan, April 20-24, 2020, Y. Huang, I. King, T. Liu, and M. van Steen, Eds. ACM / IW3C2, 2020, pp. 1160\u20131170. [Online]. Available: https://doi.org/10.1145/3366423.3380193\\n\\n[8] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \\\"BART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\\\" in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, D. Jurafsky, J. Chai, N. Schluter, and J. R. Tetreault, Eds. Association for Computational Linguistics, 2020, pp. 7871\u20137880. [Online]. Available: https://doi.org/10.18653/v1/2020.acl-main.703\\n\\n[9] M. Su\u00f1er, \\\"About indirect questions and semi-questions,\\\" Linguistics and Philosophy, pp. 45\u201377, 1993.\\n\\n[10] A. R. Puigdollers, \\\"Indirect questions in ancient Greek: meaning and internal classification,\\\" in Les compl\u00e9tives en grec ancien: actes du Colloque international de Saint-Etienne, 3-5 septembre 1998, vol. 18. Universit\u00e9 de Saint-Etienne, 1999, p. 129.\\n\\n[11] B. L. Webber, M. Stone, A. K. Joshi, and A. Knott, \\\"Anaphora and discourse structure,\\\" Comput. Linguistics, vol. 29, no. 4, pp. 545\u2013587, 2003. [Online]. Available: https://doi.org/10.1162/089120103322753347\\n\\n[12] M. A. K. Halliday and R. Hasan, Cohesion in English. London: Longman, 1976.\\n\\n[13] H. P. Grice, \\\"Logic and conversation,\\\" in Speech acts. Brill, 1975, pp. 41\u201358.\\n\\n[14] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \\\"Exploring the limits of transfer learning with a unified text-to-text transformer,\\\" Journal of Machine Learning Research, vol. 21, no. 140, pp. 1\u201367, 2020. [Online]. Available: http://jmlr.org/papers/v21/20-074.html\\n\\n[15] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \\\"BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\\\" in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Online: Association for Computational Linguistics, Jul. 2020, pp. 7871\u20137880. [Online]. Available: https://aclanthology.org/2020.acl-main.703\\n\\n[16] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \\\"Bleu: a method for automatic evaluation of machine translation,\\\" in Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics. Philadelphia, Pennsylvania, USA: Association for Computational Linguistics, Jul. 2002, pp. 311\u2013318. [Online]. Available: https://aclanthology.org/P02-1040\\n\\n[17] C.-Y. Lin, \\\"ROUGE: A package for automatic evaluation of summaries,\\\" in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74\u201381. [Online]. Available: https://aclanthology.org/W04-1013\\n\\n[18] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \\\"Bertscore: Evaluating text generation with bert,\\\" in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr\"}"}
{"id": "faustini23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants\\n\\nPedro Faustini\\\\(^1\\\\)\\\\(^\\\\ast\\\\), Besnik Fetahu\\\\(^2\\\\), Giuseppe Castellucci\\\\(^2\\\\), Anjie Fang\\\\(^2\\\\), Oleg Rokhlenko\\\\(^2\\\\), Shervin Malmasi\\\\(^2\\\\)\\n\\n\\\\(^1\\\\)Macquarie University, Sydney, NSW, Australia\\n\\\\(^2\\\\)Amazon.com, Inc., Seattle, WA, USA\\n\\npedro.arrudafaustini@hdr.mq.edu.au, \\\\{besnikf,giusecas,njfn,olegro,malmasi\\\\}@amazon.com\\n\\nAbstract\\n\\nThe adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users instant access to information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting questions with compact and natural voice hints to allow users to ask follow-up questions. We first define the task of composing speech-based hints, ground it in syntactic theory, and outline linguistic desiderata for spoken hints. We propose a sequence-to-sequence approach to generate spoken hints from a list of questions. Using a new dataset of 6,681 input questions and human written hints, we evaluate models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our most sophisticated approach applies a linguistically-motivated pretraining task and was strongly preferred by humans for producing the most natural hints.\\n\\n1. Introduction\\n\\nVoice assistants, like Alexa or Google Assistant provide ubiquitous services through a variety of devices (e.g., smart speakers, phones, etc.). Users interact with voice assistants for different purposes [1, 2] such as question answering, e-commerce, or entertainment. With increasing adoption, user expectations also grow and related content recommendation is a valued feature [3].\\n\\nThe issue of how to present proactive suggestions is an open one, and recent work has examined how content like news articles can be recommended over voice [4]. Query and question recommendation (see Fig. 1 (a)) have become well-established research topics, and are common in screen-based Web search experiences (i.e., those from Google/Bing). However, such functionality does not exist for voice-based systems. Suggestions enable useful exploratory search capabilities, and we aim to provide a similar experience over voice (see Fig. 1 (b)), where a follow-on hint suggests related topics users can ask about.\\n\\nContrary to Web search suggestions, this poses several challenges in voice assistants [5], such as (i) modality: voice lacks the advantages of visual interfaces used on the Web (e.g., showing a list), (ii) transmitted information: to ensure comprehension, the amount of transmitted information in an utterance is limited in terms of time and number of words, and (iii) content structure: simply reading out a list of questions is not natural over voice.\\n\\nWe propose an approach on how to deliver voice-based question suggestions using hints. We do not consider what to suggest as this is widely explored [6, 7]. Figure 1 provides an overview. For an input question, we assume the voice assistants can retrieve related questions from which a suggestion hint is generated. Differently from Web search suggestions (Fig. 1 (a)) where new related questions are listed, we aim to synthesize a spoken hint (Fig. 1 (b)) suggesting the same questions. The hint does not contain questions, rather, it contains several subordinate clauses describing knowledge that the user can ask.\\n\\nOur overarching contribution is a framework for generating voice-friendly hints. We begin with a grounded linguistic description of the task, outlining the characteristics of a good hint (e.g., cohesion, length), and the syntactic transformations needed to construct such utterances. Next, we frame the task as a seq2seq approach based on Transformers [8]. For an input question and a set of top\u20133 related questions, covering a diverse set of topics (unrelated topics to the initial question's topic), a voice hint is synthesized to meet the desiderata in Table 1. We create a dataset of voice-friendly hints, consisting of the triple: initial question, related questions, follow-on hint, in 9 different domains. We evaluate hint generation on our dataset by means of automated metrics and human evaluation studies. To summarize, our contributions are:\\n\\n1. To our knowledge, we are the first to define the task of question suggestion via speech-based hints, allowing users to explore related topics about their original question;\\n2. A large real-world hint generation dataset of 6,681 instances, covering 9 domains, that will become publicly available;\\n3. A seq2seq approach with task-specific training strategies for voice hint generation.\\n\\n2. Linguistic Task and Background\\n\\nTo generate a spoken hint, our objective is to take a set of standalone questions (interrogative sentences), and convert them into a single sentence (or independent clause) that informs the listener about the different pieces of information available. Direct questions (\\\"can a dog eat peanuts?\\\") can be presented as an indirect question (\\\"Alice asked if dogs can eat...\\\"), etc.\\n\\nFigure 1: (a) Question suggestion in web search (available in Google/Bing) for a user question. (b) Proposed voice-based hint for the same questions users can ask as follow-on questions to a voice assistant such as Alexa.\\n\\nThe issue of how to present proactive suggestions is an open one, and recent work has examined how content like news articles can be recommended over voice [4]. Query and question recommendation (see Fig. 1 (a)) have become well-established research topics, and are common in screen-based Web search experiences (i.e., those from Google/Bing). However, such functionality does not exist for voice-based systems. Suggestions enable useful exploratory search capabilities, and we aim to provide a similar experience over voice (see Fig. 1 (b)), where a follow-on hint suggests related topics users can ask about.\\n\\nContrary to Web search suggestions, this poses several challenges in voice assistants [5], such as (i) modality: voice lacks the advantages of visual interfaces used on the Web (e.g., showing a list), (ii) transmitted information: to ensure comprehension, the amount of transmitted information in an utterance is limited in terms of time and number of words, and (iii) content structure: simply reading out a list of questions is not natural over voice.\\n\\nWe propose an approach on how to deliver voice-based question suggestions using hints. We do not consider what to suggest as this is widely explored [6, 7]. Figure 1 provides an overview. For an input question, we assume the voice assistants can retrieve related questions from which a suggestion hint is generated. Differently from Web search suggestions (Fig. 1 (a)) where new related questions are listed, we aim to synthesize a spoken hint (Fig. 1 (b)) suggesting the same questions. The hint does not contain questions, rather, it contains several subordinate clauses describing knowledge that the user can ask.\\n\\nOur overarching contribution is a framework for generating voice-friendly hints. We begin with a grounded linguistic description of the task, outlining the characteristics of a good hint (e.g., cohesion, length), and the syntactic transformations needed to construct such utterances. Next, we frame the task as a seq2seq approach based on Transformers [8]. For an input question and a set of top\u20133 related questions, covering a diverse set of topics (unrelated topics to the initial question's topic), a voice hint is synthesized to meet the desiderata in Table 1. We create a dataset of voice-friendly hints, consisting of the triple: initial question, related questions, follow-on hint, in 9 different domains. We evaluate hint generation on our dataset by means of automated metrics and human evaluation studies. To summarize, our contributions are:\\n\\n1. To our knowledge, we are the first to define the task of question suggestion via speech-based hints, allowing users to explore related topics about their original question;\\n2. A large real-world hint generation dataset of 6,681 instances, covering 9 domains, that will become publicly available;\\n3. A seq2seq approach with task-specific training strategies for voice hint generation.\"}"}
{"id": "faustini23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"All direct questions can have an indirect equivalent, and the embedded clause of the indirect version is said to refer to the direct question. While both direct and indirect questions can be used to ask about the same information, indirect questions are more polite and often preferred in formal conversations.\\n\\nWe experiment with BART and T5 models. We encode when an indirect question's main clause reports information (e.g., I know). To learn how to perform the necessary rewrite operations for converting a question to RS format, we change the input to consist of a single question and output the rewritten clause syntax change, such as verb tense, pronoun and word or-dinate clauses from Q into RS format, and can focus on learning how to perform the necessary rewrite operations for converting a question to RS format. Generating RS requires converting independent questions into subordinate clauses, which represent the knowl-edge, fact or entity that is being interrogated in the question. The contents of a question can be framed as noun clauses, which describe the information stated or included in them. A key aspect of ensuring that hints sound natural and are easy to comprehend. The characteristics were derived from our preliminary experiments on how English speakers create hints.\\n\\nTable 1: Linguistic properties of a natural spoken hint.\\n\\n| Aspect   | Description | Start Patterns |\\n|----------|-------------|----------------|\\n| Length   | The hint should not be exceedingly long in terms of words and listening time. |\\n| Information content | Questions must be converted to an interrogative content clause, just as they would be embedded in an indirect version of the same question. |\\n| Actionability | The main hint clause should be action oriented, e.g., You can/may/might/could ask/also ask/be interested/also be interested |\\n| Naturalness | The hint should reference facts or knowledge that can be asked. |\\n| Start Patterns | A special token in pre-trained models for marking text boundaries. |\\n\\nFor a hint to be considered as voice-friendly, i.e., sound like a natural spoken utterance, several aspects detailed in Table 1 must be fulfilled. These desiderata are based on the principles of cohesion and coherence and Gricean maxims of conversation. They ensure that hints sound natural and are easy to comprehend. The characteristics were derived from our preliminary experiments on how English speakers create hints.\\n\\nTo learn F through operations, such as: (i) using the encoder representation of the input question q, Q, the hint generation model, we propose a pretraining strategy to overcome such challenges. We provide guidelines to annotators to create voice-friendly hints. Based on the intuitions from \u00a72, we collect a larger sample of single-hints. Most of it is used for pre-training of our hint generation approaches.\\n\\nIn general, these are the same changes used to generate reported or indirect speech, and can include subject-auxiliary complex. Inversion, changes in tense, and other lexical substitutions. This takes the clauses as direct objects. A key aspect of ensuring that hints sound natural and are easy to comprehend. The characteristics were derived from our preliminary experiments on how English speakers create hints.\"}"}
{"id": "faustini23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Follow-on voice friendly hints data statistics for SINGLE-HINTS and MULTI-HINTS, respectively.\\n\\n| Domain | SINGLE-HINTS | MULTI-HINTS |\\n|--------|--------------|-------------|\\n| Animal | 2,806 | 2,780 | 24.1% | 75.9% |\\n| Place  | 2,105 | 1,369 | 3.2% | 96.8% |\\n| Technology | 928 | 897 | 5.4% | 96.4% |\\n| Politician | 956 | 766 | 8.2% | 91.8% |\\n| Food   | 537 | 329 | 59.3% | 40.7% |\\n| Athlete | 352 | 209 | 16.3% | 83.7% |\\n| Wearables | 180 | 177 | - | 100% |\\n| Holiday | 60 | 54 | 5.6% | 94.4% |\\n| Total  | 7,932 | 6,581 | 1,132 | 5,449 |\\n\\n5. Experimental Setup\\n5.1. Datasets\\nPre-training RS.\\nSINGLE-HINTS also referred to as RS data are used for pretraining the hint generation approaches.\\n\\nHint Generation.\\nFor the main task, we randomly sample questions from Table 2, where 81% are hints generated from 3 questions, 17% with 2 questions, and the remaining 2% are SINGLE-HINTS.\\n\\nTable 3: Pretraining and training hint generation datasets.\\n\\n|          | train | dev  | test |\\n|----------|-------|------|------|\\n| RS pretraining | 4,262 | 1,831 | -    |\\n| Hint Generation | 4,008 | 668  | 2,005|\\n\\n5.2. Baselines and Approaches\\nFor all Transformer-based approaches, we experimented with both BART [15] and T5 [14].\\n\\nTemplate Baseline \u2013TB.\\nHints are constructed according to manually defined templates, by first choosing a start pattern (cf. Table 1) and then concatenating question from $Q_{rel}$ using \u201cor\u201d.\\n\\nReported Speech Baseline \u2013RSB.\\nWe train a seq2seq model on SINGLE-HINTS only, where questions in $Q_{rel}$ are converted into RS format, then using TB, they are concatenated into a hint.\\n\\nDirect Hint Generation \u2013DHG.\\nThis represents our approach without pretraining. The limitation of DHG is that it has to jointly learn all aspects of constructing voice-friendly hints.\\n\\nHint Generation with RS Pretraining \u2013PTG.\\nThis represents our final approach with pretraining on the RS task. Breaking down the training into two stages, PTG first learns RS rewriting, then it learns to avoid repetitions and ensure hint coherence.\\n\\n5.3. Evaluation Metrics\\nEvaluating hint quality is not trivial. Given the task novelty and the lack of metrics that capture voice-friendliness, we opt for a combination of automatic metrics and human evaluations.\\n\\n5.3.1. Automated Metrics\\nTo assess the similarity of generated hints against the ground truth, we use BLEU [16], ROUGE [17] and F1-BertScore [18]. BLEU captures accuracy in terms of n-grams, while ROUGE quantifies coverage. BERTScore computes hint semantic similarity, accounting for the use of different paraphrases in hints.\\n\\n5.3.2. Human Evaluation\\nWe devise a set of human evaluations which judge the correctness and naturalness of a hint. For a realistic evaluation, the studies are performed in voice modality, apart from Question coverage and syntactic correctness. We consider the following studies:\\n\\nSyntactic Correctness.\\nAssess whether a hint is syntactically correct, and if the hint uses idiomatic expressions in English.\\n\\nQuestion Coverage.\\nGiven a hint $h$ and $Q_{rel}$, annotators assess if $h$ covers all questions in $Q_{rel}$.\\n\\nPairwise Hint Comparison.\\nFor two generated hints $h_a$ and $h_b$ from the same set of questions $Q_{rel}$ and two different approaches, annotators choose their preferred hint. To reduce any positional bias, hints are ordered randomly.\\n\\nQuestion Retention.\\nWe consider retention of a hint\u2019s information in annotator\u2019s memory as a proxy for its simplicity and comprehensibility. Hints cannot be considered actionable if listeners cannot remember them. To emulate interaction with a voice assistant, annotators first listen to the hint, after which a mandatory 5 seconds pause is enforced. Then they need to choose the correct question covered in $h$ from a set of four questions shown to them. Only one of the questions is present in $h$. We select the three distractor questions, one chosen at random, and the other two are either relevant to the entity and topic covered by $h$, or the entity only.\\n\\n6. Evaluation with Automated Metrics\\nTable 4 shows the performance measured on the automated metrics for the different approaches.\\n\\nTable 4: PTG-BART achieves the highest performance across nearly all evaluation metrics, obtaining statistically highly significant results ($p < 0.01$) against all its counterparts. B1\u2013B4 represent BLEU scores, and R1\u2013R4 represent ROUGE scores.\\n\\n|          | B1    | B2    | B3    | B4    | R1    | R2    | R3    | R4    | BERTScore |\\n|----------|-------|-------|-------|-------|-------|-------|-------|-------|-----------|\\n| TB       | 0.509 | 0.401 | 0.323 | 0.254 | 0.713 | 0.488 | 0.358 | 0.278 | 0.536     |\\n| RSB      | 0.519 | 0.415 | 0.341 | 0.274 | 0.717 | 0.501 | 0.375 | 0.292 | 0.494     |\\n| DHG-T5   |       |       |       |       |       |       |       |       |           |\\n| DHG-BART |       |       |       |       |       |       |       |       |           |\\n| PTG-T5   | 0.629 | 0.524 | 0.442 | 0.373 | 0.739 | 0.534 | 0.410 | 0.329 | 0.643     |\\n| PTG-BART | 0.630 | 0.527 | 0.446 | 0.378 | 0.742 | 0.539 | 0.413 | 0.333 | 0.642     \\n\\nBaseline Performance:\\nTB achieves the lowest scores across all metrics (except for BERTScore). This is expected, since concatenated questions are compared w.r.t the ground-truth hints, written by annotators. RSB obtains a consistent improvement across all metrics. It rewrites individual questions into content clauses, which then are concatenated using TB. However, RSB does not reduce lexical repetition via anaphora, and simple concatenation results in lower coherence. Overall, as expected, TB and RSB, achieve low scores, however better insight are provided in Section 7, which capture hint voice friendliness.\\n\\nApproach Performance:\\nOur approaches, DHG and PTG, show a consistent improvement over TB and RSB across all metrics. Comparing PTG and DHG in Table 4, we note a significant improvement in terms of BLEU scores due to the pretraining phase. This follows our intuition that pretraining helps PTG to convert questions into subordinate clauses, a key aspect of natural hints. In the fine-tuning stage, PTG can focus only on reducing lexical redundancy, resulting in more coherent hints. While PTG employs multi-stage training, in DHG all operations are learned end-to-end. This represents a complex training regime, requiring optimization of several rewrite tasks. In terms of ROUGE metrics, only PTG-T5 obtains significantly better results than DHG-T5 for ROUGE1. For the rest, the differences are not significant. Finally, for BERTScore the differences are significant between PTG-BART over DHG-BART. Finally, the difference in performance between PTG and DHG, demonstrates that for complex rewriting tasks, end-to-end training may be sub-optimal.\"}"}
{"id": "faustini23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Human Evaluation Studies\\n\\n7.1. Syntactic Correctness and Coverage\\n\\nTable 5 shows the results on question coverage and hint syntactic correctness. For a random sample of 500 hints and the corresponding Qrel, we assess if all input questions are present in a generated hint, and if the hint is syntactically correct.\\n\\nTable 5: Syntactic correctness and question coverage results (significant results between PTG and DHG are marked with \u2021).\\n\\n| Approach     | Syntactic Correctness | Question Coverage |\\n|--------------|------------------------|-------------------|\\n| TB           | 449 (89.8%)            | 500 (100%)        |\\n| RSB          | 461 (92.2%)            | 484 (96.8%)       |\\n| DHG-T5       | 434 (86.8%)            | 464 (92.8%)       |\\n| DHG-BART     | 428 (85.6%)            | 466 (93.2%)       |\\n| PTG-T5       | 431 (86.2%)            | 485 (97.0%)       |\\n| PTG-BART     | 455 (91.0%) \u2021          | 485 (97.0%)       |\\n\\nSyntactic Correctness.\\n\\nTable 5 shows a consistent pattern in terms of syntactic correctness: the baseline RSB and PTG-BART have the most syntactically correct hints as judged by the annotators, with 92% and 91%, respectively. Generating hints from multiple questions is not trivial, as it involves syntactic and stylistic changes in h, allowing room for errors for generative models, especially in terms of syntactic errors.\\n\\nThe high RSB and PTG-BART scores can be interpreted as follows. RSB is trained on SINGLE-HINTS, which does a syntactic conversion of the input question into their RS format, and through simple rules concatenates content clauses. This allows the model to generate hints that are syntactically correct in 92% of the cases. Similarly, PTG-BART, that is pretrained on SINGLE-HINTS, has the same capabilities as RSB, and generates in 91% of the cases syntactically correct hints. However, contrary to RSB, PTG-BART additionally fine-tunes for voice-friendliness, which ensures hint coherence and redundancy. While RSB generates syntactic hints, its hints are far less natural than those of PTG-BART (cf. Section 7.2).\\n\\nCoverage.\\n\\nFor question coverage, we note that the PTG approaches achieve the highest coverage among the learning based approaches, with 97% of the hints covering all the questions. TB has perfect coverage, given that its hints are generated by simply concatenating the input questions. Finally, the DHG approaches have the lowest coverage, with 92.8% of hints having full coverage. This indicates that end-to-end learning of all hint generation tasks is challenging.\\n\\n7.2. Pairwise Hint Comparison\\n\\nHere we measure which approaches generate hints that are considered more natural by humans. As DHG has consistently lower performance than PTG, we only compare PTG-BART, RSB, and TB. To understand the naturalness of the hints in a spoken format, they are converted to audio. After listening to the hints, annotators judge which hint they find more natural and easier to understand. To avoid positional bias, the order in which the hints are played is randomized.\\n\\nTable 6: Pairwise hint comparison.\\n\\n| Comparison          | PTG-BART chosen | Baseline chosen |\\n|---------------------|-----------------|-----------------|\\n| PTG-BART vs. TB     | 300 (68%)       | 141 (32%)       |\\n| PTG-BART vs. RSB    | 267 (61%)       | 174 (39%)       |\\n\\nTable 6 shows the pairwise comparisons the different models. We run the comparison on the 441 hints that were judged to be syntactically correct in Table 5. This is done to avoid any bias stemming from syntactically incorrect hints. In both comparisons, PTG-BART produces more natural hints than baselines. Against TB and RSB, it is preferred in 68% and 60% of the cases, respectively. Both results represent statistically highly significant differences (as per Wilcoxon's signed-rank test).\\n\\n7.3. Question Retention Evaluation\\n\\nHere we measure how actionable the generated hints are. Beyond being natural or correct, the main aim of generating follow-on hints is for them to be actionable such that listeners (i.e., users of voice assistants) can ask follow-up questions.\\n\\nUsing the same set of 441 syntactically correct hints (cf. Table 5), annotators listen to the hints, after which a set of four questions is shown, where only one was actually part of the hint. The ability to correctly recognize this question is a proxy for whether the listeners could comprehend and remember the hint's information content. In a conversational scenario with a voice assistant, they could follow-up by asking this question.\\n\\nTable 7 shows the retention for different approaches. PTG-BART and DHG-BART achieve significantly better retention than TB and RSB. This finding demonstrates that retention is negatively impacted by incoherent (TB due to simple concatenation) and repetitive (RSB due to it not using anaphora) hints.\\n\\nTable 7: Number of hints correctly recognized by annotators.\\n\\n| Model       | # Recognized Questions | Hint Length (# characters) |\\n|-------------|------------------------|---------------------------|\\n| Templates (TB) | 356 (80.7%)           | 152.72 \u00b1 34.6            |\\n| RSB         | 348 (78.9%)            | 158.02 \u00b1 34.6            |\\n| DHG-BART    | 383 (86.8%)            | 139.85 \u00b1 33.8            |\\n| PTG-BART    | 384 (87.1%)            | 140.78 \u00b1 34.5            |\\n\\n7.4. Examples of Model Generated Spoken Hints\\n\\n| Qrel | TB               | RSB               | DHG               | PTG               |\\n|------|------------------|-------------------|-------------------|-------------------|\\n| How much money does Cristiano Ronaldo earn? | You may want to know how much money does Cristiano Ronaldo earn, or how many children does Cristiano Ronaldo have, or who is the mother of Cristiano Ronaldo child. | You may want to know how much money Cristiano Ronaldo earns, or how many children Cristiano Ronaldo has, or who is the mother of Cristiano Ronaldo child. | You may want to know how much money does Cristiano Ronaldo earn, or how many children he has, or who is the mother of his child. | You may want to know how much money Cristiano Ronaldo earns, or how many children he has, or who is the mother of his child. |\\n\\n8. Conclusions\\n\\nWe presented a novel approach for spoken question suggestion. Our work enables the creation of new voice-based experiences where users can receive compact and natural hints about additional questions they can ask. Question suggestion is a standard feature in screen-based search experiences, and our work takes a key first step in bringing this capability to voice interfaces.\\n\\nOur contributions are manifold: (i) a novel task of suggesting questions with voice hints; (ii) outlined the linguistic desiderata and processes to decompose questions into interrogative content clauses, and recompose them into declarative hints; and (iii) a new dataset of over 6,681 input questions and hints (14k when considering both SINGLE-HINTS and MULTI-HINTS) using carefully constructed annotation guidelines and quality checks.\\n\\nWe defined seq2seq models to generate hints. Using both automatic metrics and human evaluations, we conclusively showed that our most sophisticated approach PTG, which utilizes a linguistically motivated pretraining task was strongly preferred by humans with most natural hints.\"}"}
