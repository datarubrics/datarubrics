{"id": "lu24f_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. Nautsch, X. Wang, N. Evans, T. H. Kinnunen, V. Vestman, M. Todisco, H. Delgado, M. Sahidullah, J. Yamagishi, and K. A. Lee, \u201cASVspoof 2019: spoofing countermeasures for the detection of synthesized, converted and replayed speech,\u201d IEEE Transactions on Biometrics, Behavior, and Identity Science, vol. 3, no. 2, pp. 252\u2013265, 2021.\\n\\n[2] J. Yi, R. Fu, J. Tao, S. Nie, H. Ma, C. Wang, T. Wang, Z. Tian, Y. Bai, C. Fan et al., \u201cAdd 2022: the first audio deep synthesis detection challenge,\u201d in Proceedings of ICASSP. IEEE, 2022, pp. 9216\u20139220.\\n\\n[3] Y. Eom, Y. Lee, J. S. Um, and H. R. Kim, \u201cAnti-Spoofing Using Transfer Learning with Variational Information Bottleneck,\u201d in Proc. Interspeech 2022, 2022, pp. 3568\u20133572.\\n\\n[4] Y. Xie, H. Cheng, Y. Wang, and L. Ye, \u201cLearning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 2808\u20132812.\\n\\n[5] J. M. Mart\u00edn-Do\u02dcnas and A. \u00b4Alvarez, \u201cThe vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9241\u20139245.\\n\\n[6] N. M \u00a8uller, P. Czempin, F. Diekmann, A. Froghyar, and K. B \u00a8ottinger, \u201cDoes Audio Deepfake Detection Generalize?\u201d in Proc. Interspeech 2022, 2022, pp. 2783\u20132787.\\n\\n[7] J. Frank and L. Sch \u00a8onherr, \u201cWaveFake: A Data Set to Facilitate Audio Deepfake Detection,\u201d in Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2021.\\n\\n[8] C. Sun, S. Jia, S. Hou, and S. Lyu, \u201cAi-synthesized voice detection using neural vocoder artifacts,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023, pp. 904\u2013912.\\n\\n[9] X. Wang and J. Yamagishi, \u201cCan large-scale vocoded spoofed data improve speech spoofing countermeasure with a self-supervised front end?\u201d arXiv preprint arXiv:2309.06014, 2023.\\n\\n[10] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi et al., \u201cAudiolm: a language modeling approach to audio generation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2023.\\n\\n[11] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D \u00b4efossez, J. Copet, D. Parikh, Y . Taigman, and Y . Adi, \u201cAudiogen: Textually guided audio generation,\u201d in The Eleventh International Conference on Learning Representations, 2022.\\n\\n[12] C. Wang, S. Chen, Y . Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y . Liu, H. Wang, J. Li et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023.\\n\\n[13] A. Agostinelli, T. I. Denk, Z. Borsos, J. Engel, M. Verzetti, A. Caillon, Q. Huang, A. Jansen, A. Roberts, M. Tagliasacchi et al., \u201cMusiclm: Generating music from text,\u201d arXiv preprint arXiv:2301.11325, 2023.\\n\\n[14] Z. Zhang, L. Zhou, C. Wang, S. Chen, Y . Wu, S. Liu, Z. Chen, Y . Liu, H. Wang, J. Li et al., \u201cSpeak foreign languages with your own voice: Cross-lingual neural codec language modeling,\u201d arXiv preprint arXiv:2303.03926, 2023.\\n\\n[15] T. Wang, L. Zhou, Z. Zhang, Y . Wu, S. Liu, Y . Gaur, Z. Chen, J. Li, and F. Wei, \u201cViola: Unified codec language models for speech recognition, synthesis, and translation,\u201d arXiv preprint arXiv:2305.16107, 2023.\\n\\n[16] J. Copet, F. Kreuk, I. Gat, T. Remez, D. Kant, G. Synnaeve, Y . Adi, and A. D \u00b4efossez, \u201cSimple and controllable music generation,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.\\n\\n[17] X. Wang, M. Thakker, Z. Chen, N. Kanda, S. E. Eskimez, S. Chen, M. Tang, S. Liu, J. Li, and T. Yoshioka, \u201cSpeechx: Neural codec language model as a versatile speech transformer,\u201d arXiv preprint arXiv:2308.06873, 2023.\\n\\n[18] Q. Chen, Y . Chu, Z. Gao, Z. Li, K. Hu, X. Zhou, J. Xu, Z. Ma, W. Wang, S. Zheng et al., \u201cLauragpt: Listen, attend, understand, and regenerate audio with gpt,\u201d arXiv preprint arXiv:2310.04673, 2023.\\n\\n[19] D. Yang, J. Tian, X. Tan, R. Huang, S. Liu, X. Chang, J. Shi, S. Zhao, J. Bian, X. Wu et al., \u201cUniaudio: An audio foundation model toward universal audio generation,\u201d arXiv preprint arXiv:2310.00704, 2023.\\n\\n[20] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, \u201cSoundstream: An end-to-end neural audio codec,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 495\u2013507, 2021.\\n\\n[21] A. Van Den Oord, O. Vinyals et al., \u201cNeural discrete representation learning,\u201d Advances in neural information processing systems, vol. 30, 2017.\\n\\n[22] X. Zhang, D. Zhang, S. Li, Y . Zhou, and X. Qiu, \u201cSpeechtokenerizer: Unified speech tokenizer for speech language models,\u201d in The Twelfth International Conference on Learning Representations, 2024.\\n\\n[23] Z. Du, S. Zhang, K. Hu, and S. Zheng, \u201cFuncodec: A fundamental, reproducible and integrable open-source toolkit for neural speech codec,\u201d arXiv preprint arXiv:2309.07405, 2023.\\n\\n[24] A. D \u00b4efossez, J. Copet, G. Synnaeve, and Y . Adi, \u201cHigh fidelity neural audio compression,\u201d arXiv preprint arXiv:2210.13438, 2022.\\n\\n[25] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\\n\\n[26] Y .-C. Wu, I. D. Gebru, D. Markovi\u00b4c, and A. Richard, \u201cAudiodec: An open-source streaming high-fidelity neural audio codec,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[27] J. Su, Z. Jin, and A. Finkelstein, \u201cHifi-gan: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks,\u201d arXiv preprint arXiv:2006.05694, 2020.\\n\\n[28] D. Yang, S. Liu, R. Huang, J. Tian, C. Weng, and Y . Zou, \u201cHifi-codec: Group-residual vector quantization for high fidelity audio codec,\u201d arXiv preprint arXiv:2305.02765, 2023.\\n\\n[29] R. Kumar, P. Seetharaman, A. Luebs, I. Kumar, and K. Kumar, \u201cHigh-fidelity audio compression with improved rvqgan,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.\\n\\n[30] L. Ziyin, T. Hartwig, and M. Ueda, \u201cNeural networks fail to learn periodic functions and how to fix it,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 1583\u20131594, 2020.\\n\\n[31] H. Zen, V . Dang, R. Clark, Y . Zhang, R. J. Weiss, Y . Jia, Z. Chen, and Y . Wu, \u201cLibritts: A corpus derived from librispeech for text-to-speech,\u201d in Proc. Interspeech, Sep. 2019.\\n\\n[32] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[33] C. Veaux, J. Yamagishi, and K. MacDonald, \u201cCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,\u201d 2017.\\n\\n[34] Y . Shi, H. Bu, X. Xu, S. Zhang, and M. Li, \u201cAISHELL-3: A Multi-Speaker Mandarin TTS Corpus,\u201d in Proc. Interspeech 2021, 2021, pp. 2756\u20132760.\\n\\n[35] J.-w. Jung, H.-S. Heo, H. Tak, H.-j. Shim, J. S. Chung, B.-J. Lee, H.-J. Yu, and N. Evans, \u201cAasist: Audio anti-spoofing using integrated spectro-temporal graph attention networks,\u201d in Proceedings of the ICASSP, 2022, pp. 6367\u20136371.\\n\\n[36] G. Lavrentyeva, S. Novoselov, A. Tseren, M. V olkova, A. Gor-\\n\\nlanov, and A. Kozlov, \u201cSTC Antispoofing Systems for the ASVspoof2019 Challenge,\u201d in Proc. Interspeech 2019, 2019, pp. 1033\u20131037.\\n\\n[37] M. Todisco, X. Wang, V . Vestman, M. Sahidullah, H. Delgado, A. Nautsch, J. Yamagishi, N. Evans, T. H. Kinnunen, and K. A. Lee, \u201cASVspoof 2019: Future Horizons in Spoofed and Fake Audio Detection,\u201d in Proc. Interspeech 2019, 2019, pp. 1008\u20131012.\"}"}
{"id": "lu24f_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio\\n\\nYi Lu1,2,\u2020, Yuankun Xie1,3,\u2020, Ruibo Fu1,\u2217, Zhengqi Wen1, Jianhua Tao4,5, Zhiyong Wang1,2, Xin Qi1,2, Xuefei Liu1, Yongwei Li1, Yukun Liu2, Xiaopeng Wang1,2, Shuchen Shi6\\n\\n1 Institute of Automation, Chinese Academy of Sciences\\n2 School of Artificial Intelligence, University of Chinese Academy of Sciences\\n3 School of Information and Communication Engineering, Communication University of China\\n4 Department of Automation, Tsinghua University\\n5 Beijing National Research Center for Information Science and Technology, Tsinghua University\\n6 Shanghai Polytechnic University\\n\\nluyi22@mails.ucas.ac.cn, xieyuankun@cuc.edu.cn, ruibo.fu@nlpr.ia.ac.cn\\n\\nAbstract\\n\\nWith the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from handcrafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods. Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.\\n\\nIndex Terms: neural codec, neural vocoder, audio deepfake detection, deepfake dataset\\n\\n1. Introduction\\n\\nWith the development of Large Language Model (LLM) based audio generation models, an increasing number of deepfake audios are being produced. Traditional deepfake audio generation methods like speech synthesis typically consist of three parts: text analysis, acoustic model, and vocoder. In contrast, LLM-based audio generation models adopt only an end-to-end generation approach. They utilize the neural codec method to discretize latent representations, transforming the generation task from an autoregressive task into a discrete representation classification task, which improves efficiency and stability of the generation model. Currently, the growing prevalence of LLM-based deepfake audio poses significant challenges to current audio deepfake detection (ADD) models.\\n\\nIn light of these challenges, the necessity for developing methods of detecting the LLM-based deepfake audio becomes increasingly apparent. Currently, researchers have undertaken a series of studies on deepfake audio detection around competitions such as ASVspoof series [1] and ADD Challenge series [2]. Advanced methods have been proposed that can achieve an intra-domain Equal Error Rate (EER) of less than 1% [3, 4, 5]. However, these methods often experience performance degradation when faced with real-world scenarios [6]. This can be attributed to the inability of ADD models to generalize and detect unseen spoofing methods that were not present in the training set.\\n\\n\u2020 denotes equal contribution to this work. * denotes corresponding author.\\n\\nTable 1: Comparison of different audio language models.\\n\\n| ALM          | Task         | Codec        |\\n|--------------|--------------|--------------|\\n| AudioLM [10] | SC, PC       | SoundStream  |\\n| AudioGen [11] | AC           | SoundStream  |\\n| V ALL-E [12] | TTS          | EnCodec      |\\n| MusicLM [13] | MG           | SoundStream  |\\n| V ALL-E X [14] | TTS, S2ST   | EnCodec      |\\n| VioLA [15]   | ASR, S2TT, TTS, MT | EnCodec |\\n| MusicGen [16] | MG, SG       | EnCodec      |\\n| SpeechX [17] | SE, SR, TSE, TTS, SPED | EnCodec |\\n| LauraGPT [18] | ASR, S2TT, TTS, MT, SE, AAC, SER, SLU | FunCodec |\\n| UniAudio [19] | TTS, VC, SE, TSE, SVS, TTSO, TTM, AUED, SD, ITTS, SPED | EnCodec |\\n\\nTo address the performance degradation of ADD models in the presence of unknown forgery methods, the most direct approach is data augmentation, which aims to expand the training distribution for improved generalizability. Especially in the field of ADD, training ADD models with novel deepfake methods can significantly enhance model performance. For traditional audio deepfake generation methods, a vocoder is the core in the generation backend. Therefore, it is significant to augment the data with various vocoders to effectively detect vocoder-based deepfake audio. Wavefake [7] presents a vocoder-based audio deepfake dataset collected from ten sample sets across six different network architectures, spanning two languages. Wavefake datasets covers commonly used vocoders and demonstrate that training with Wavefake dataset enables high accuracy in detecting vocoder-based deepfake audio. After Wavefake, more vocoder-based approach have been introduced to propel the development of the ADD field. Sun et al. [8] construct a vocoder-based dataset called LibriSeV oc and explore the artifact of different vocoder. More Recently, Wang et al. [9] employed multiple neural vocoders to create a large-scale dataset of vocoded data and demonstrated that this approach can improve generalizability.\\n\\nThe above studies have explored the core component, vocoders, of deepfake audio and introduced new datasets to better detect vocoder-based deepfake audio. However, current mainstream LLM-based deepfake audio does not utilize vocoder for waveform generation; instead, they employ neural codec approaches to generate audio from discrete codec. As illustrated in Table 1, existing audio language model (ALM) universally adopt neural codecs to construct discrete encodings. Those codec-based ALMs frequently support a variety of audio\"}"}
{"id": "lu24f_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Mel-spectrogram of the original audio alongside seven codec-based audio samples generated from the original. The top row is generated from VCTK, and the bottom row is generated from AISHELL3. Generation tasks and exhibit strong generalization capabilities on unseen audio generation tasks [19]. Therefore, there is an urgent need to establish a dataset based on neural codec for research, aiming to effectively detect LLM-based deepfake audio.\\n\\nIn this paper, we propose Codecfake, an initial dataset for detecting LLM-based deepfake audio, spanning two languages. We select seven representative open-source neural codec models to construct fake audio. These codec types encompass the current mainstream LLM-based audio generation models. We initially observe that the features of the original and codec-based audio exhibit only subtle differences, as shown in Figure 1. This poses a significant challenge for ADD methods.\\n\\nTo effectively detect the codec-based audio, we first investigate whether the current vocoder-trained ADD models can effectively detect codec-based audio. Then, we explore the effectiveness of codec-trained ADD models in detecting codec-based audio. Additionally, we test their performance in out-of-distribution (OOD) scenarios, specifically when faced with novel codec and detecting vocoder-based audio. Experiments demonstrate that vocoder-trained ADD models are insufficient for effectively detecting codec-based audio. Leveraging the proposed Codecfake dataset for training, we achieve a 41.406% reduction in EER on Codecfake test set compared to vocoder-trained ADD model.\\n\\n2. Method\\n\\n2.1. Architectures for generating codec-based fake audio\\n\\nIn this section, we provide a brief overview of the architectures and differences of the seven neural audio codecs models (F01-F07) used to construct our dataset. Those seven methods encompass all major neural audio codecs models proposed in recent years, ensuring coverage of the codecs types employed by mainstream audio language models and potentially superior neural audio codecs models that may be adopted by future audio language models.\\n\\nF01-SoundStream [20]. SoundStream is a milestone achievement in neural audio codecs, employing a classic architecture consisting of encoder, quantizer, and decoder components. Its quantizer utilizes residual vector quantizers (RVQ) [21], and it integrates denoising functionality throughout the entire architecture.\\n\\nF02-SpeechTokenizer [22]. SpeechTokenizer utilizes semantic tokens from Hubert L9 as a teacher for the RVQ process. This aids in decoupling content information from acoustic information, with content information processed in the first layer of the tokenizer, while acoustic information is retained in subsequent layers.\\n\\nF03-FunCodec [23]. FunCodec encodes audio in the frequency domain, requiring fewer parameters to achieve the same effect. Additionally, it evaluates the impact of semantic information on speech codecs, thereby enhancing speech quality at low bit rates.\\n\\nF04-EnCodec [24]. The structure of Encodec is similar to that of SoundStream. It integrates additional LSTM [25] layers and lightweight Transformer-based language models to model RVQ encoding, thereby accelerating inference speed while maintaining quality.\\n\\nF05-AudioDec [26]. Building upon Encodec, AudioDec introduces improvements by employing a group convolution mechanism to accelerate inference speed for real-time transmission. Additionally, it incorporates a multi-period discriminator from HiFi-GAN [27] to ensure the generation of high-fidelity audio.\\n\\nF06-AcademicCodec [28]. AcademiCodec introduces group-residual vector quantization and utilizes multiple parallel RVQ groups, aiming to enhance audio reconstruction performance within the constraints of codebook size.\\n\\nF07-Descript-audio-codec (DAC) [29]. DAC is a versatile neural codec model capable of maintaining high-quality audio across a wide frequency range. It employs enhanced residual vector quantization and utilizes periodic activation functions [30].\\n\\n2.2. The Generation Process of codec-based fake audio\\n\\nIn this section, we will introduce the training details of neural audio codecs, the datasets used, as well as the inference process and the construction of the Codecfake dataset.\\n\\nDuring the training phase, neural codec models are trained using the LibriTTS [31] dataset, which is a multi-speaker English corpus comprising approximately 585 hours of read English speech at a sampling rate of 24kHz. The LibriTTS corpus is specifically designed for research in text-to-speech systems and is derived from the original materials of the LibriSpeech [32] corpus. We typically set the parameters of each model to the default values mentioned in the original paper or to parameter settings widely adopted by audio language models for training, continuing training until the model converges.\\n\\nDuring the inference phase, we utilize the trained seven neural codec models to re-encode and decode data from the VCTK [33] (This CSTR VCTK Corpus includes speech data uttered by 110 English speakers with various accents.) and AISHELL3 [34] (The corpus contains roughly 85 hours of emotion-neutral recordings spoken by 218 native Chinese mandarin speakers) datasets separately. More neural codec models details is illustrated in Table 3.\"}"}
{"id": "lu24f_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Number of samples in each subset of Codecfake dataset.\\n\\n| Type   | Seen  | Unseen |\\n|--------|-------|--------|\\n|        | Train | Dev    |\\n|        | C1    | C2     | C3     | C4     | C5     | C6     | C7     |\\n| Real   | 105821| 13228  | 13228  | 13228  | 13228  | 13228  | 13228  |\\n| Fake   | 634926| 79368  | 13228  | 13228  | 13228  | 13228  | 132277 |\\n| Total  | 740747| 92596  | 26456  | 26456  | 26456  | 26456  | 145505 |\\n\\nTable 3: Neural codec models detail.\\n\\n| Type   | Codec   | SR | BPS | Quantizers |\\n|--------|---------|----|-----|------------|\\n| F01    | SoundStream | 16k | 4k  | 8           |\\n| F02    | SpeechToknizer | 16k | 4k  | 8           |\\n| F03    | FuncCodec | 16k | 16k | 32          |\\n| F04    | EnCodec  | 24k | 6k  | 8           |\\n| F05    | AudioDec | 24k | 6.4k| 8           |\\n| F06    | AcademicCodec | 24k | 3k  | 4           |\\n| F07    | DAC      | 44k | 8k  | 9           |\\n\\n3. Experiments\\n3.1. Data details\\nOur proposed Codecfake dataset consists of 1,058,216 audio samples, including 132,277 real samples (44,242 samples from VCTK and 88,035 samples from AISHELL3) and 925,939 fake samples generated by seven different codec methods. Regarding ADD experiments, we divided the 132,277 source real samples into a training subset of 105,821 samples, a development subset of 13,228 samples, and an evaluation subset of 13,228 samples. The same dataset splitting strategy was applied to the fake audio generated using the six different forgery methods (F01-F06). We designated F07 as an unseen fake methods for the generalization verification of the ADD model. This implies that there are no F07 utterances in either the training or development sets. The training set of Codecfake comprises a total of 740,747 samples, and the development set contains 92,596 samples. For evaluation, we established seven conditions (C1-C7) for codec methods F01-F07, with each condition including the evaluation subset of real audio. Details are provided in Table 2.\\n\\n3.2. Implementation details\\nWe comprehensively evaluated the Codecfake dataset using state-of-the-art ADD methods, namely AASIST [35] and LCNN [36]. In terms of features, we utilized the hand-crafted feature mel-spectrogram and self-supervised feature wav2vec2 (W2V2) representations. For the mel-spectrogram, we extracted a 80-dimensional mel-spectrogram. For W2V2, we employed the Wav2Vec-XLS-R model with frozen parameters, extracting the 1024-dimensional hidden states as the feature representation. All audio samples were downsampled to 16000 Hz and trimmed or padded to a duration of 4 seconds. All models used Adam optimizer with a learning rate of $10^{-4}$ and cosine annealing learning rate decay. We conducted training for 100 epochs using weighted cross-entropy, with the weight for the real class set to 10 and for the fake class set to 1. For the codec-trained ADD model, training was performed for 10 epochs. The model that exhibited the best performance on the validation set was selected as the evaluation model.\\n\\n4. Results and Discussion\\n4.1. Vocoder-trained ADD models results\\nFirst, we explore whether the vocoder-trained ADD model can effectively detect codec-based audio, as outlined in Table 4. Concretely, we conducted training on three distinct models: Mel-LCNN, W2V2-LCNN and W2V2-AASIST, utilizing the ASVspoof2019LA (19LA) [37] training set. From the results, we can first observe that they achieve promising result in 19LA test sets and W2V2-AASIST achieve the lowest EER with 0.122%. As the 19LA training set incorporates six spoofing methods, and the test set encompasses a total of 19 methods, achieving favorable results on the 19LA test set suggests that the detection system exhibits generalizability. However, when the vocoder-trained ADD models evaluate on the Codecfake test sets, the results are quite unfavorable. Mel-LCNN, W2V2-LCNN, and W2V2-AASIST achieve average EER of 43.910%, 36.123%, and 41.583%, respectively, which represents a decrease in performance compared to the 19LA test set, with increases of 38.826%, 35.498%, and 41.461%, respectively. This indicates that models trained with vocoders cannot generalize to effectively detect codec-based audio. Furthermore, it also suggests that codec-based audio exhibits minimal differences from real speech, posing a significant challenge to the field of ADD.\\n\\n4.2. Codec-trained ADD models results\\nDue to the poor performance of vocoder-trained ADD models on the Codecfake test set, we proceed to train the baseline models using the Codecfake training set. The results are presented in the Table 5. Mel-LCNN, W2V2-LCNN, and W2V2-AASIST achieve average EER of 13.203%, 1.205%, and 0.177%, exhibited reductions in average EER of 30.707%, 34.918%, and 41.406%, respectively, compared to the vocoder-trained model. All ADD models trained with the Codecfake dataset exhibit promising results under seen conditions C1-C6. Among these, W2V2-AASIST achieved the best average EER of 0.177%. In the unseen codec test condition C7, W2V2-AASIST achieves an EER of 0.884%, indicating that training with the Codecfake dataset can generalize to detecting unknown codec-based audio. Furthermore, we found that using only the codec-trained ADD model can detect vocoder-based audio. Specifically, W2V2-AASIST achieves a 3.806% EER on the 19LA test set.\"}"}
{"id": "lu24f_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: EER (%) results for ADD model trained by 19LA training set. AVG represents the average EER across C1-C7.\\n\\n| Model       | 19LA  | C1    | C2    | C3    | C4    | C5    | C6    | C7    | AVG  |\\n|-------------|-------|-------|-------|-------|-------|-------|-------|-------|------|\\n| Mel-LCNN    | 5.084 | 36.127| 49.304| 39.234| 49.160| 46.991| 36.967| 49.584| 43.910|\\n| W2V2-LCNN   | 0.625 | 19.481| 45.260| 45.010| 20.683| 34.321| 44.390| 43.717| 36.123|\\n| W2V2-AASIST | 0.122 | 40.142| 42.908| 44.564| 33.580| 39.197| 44.889| 45.804| 41.583|\\n\\nTable 5: EER (%) results for ADD model trained by Codecfake training set. AVG represents the average EER across C1-C7.\\n\\n| Model       | 19LA  | C1    | C2    | C3    | C4    | C5    | C6    | C7    | AVG  |\\n|-------------|-------|-------|-------|-------|-------|-------|-------|-------|------|\\n| Mel-LCNN    | 26.826| 2.147 | 9.563 | 8.618 | 12.239| 20.033| 8.951 | 30.867| 13.203|\\n| W2V2-LCNN   | 4.433 | 0.030 | 0.151 | 0.144 | 0.113 | 0.635 | 2.185 | 5.178 | 1.205 |\\n| W2V2-AASIST | 3.806 | 0.167 | 0.008 | 0.023 | 0.015 | 0.038 | 0.106 | 0.884 | 0.177 |\\n\\nFigure 2: The confusion matrices under different test conditions. (a), (b), (c), and (d) correspond to W2V2-AASIST trained on the Codecfake training set and tested on C3, C6, 19LA, and C7, respectively.\\n\\n4.3. Discussion\\n\\nThrough our experiments, we have identified some limitations and areas for further improvement in our work. Future efforts will focus on the following items to make enhancements.\\n\\n- **More diverse LLM-based audio Dataset**: Codecfake incorporates mainstream codec methods. However, training on Codecfake still performs inadequately on unknown codec methods. Therefore, there is a need to establish a diverse codec dataset to encompass neural codec generation methods in ALM.\\n\\n- **Influential factors of codec-based audio**: The generation of codec-based audio is influenced by various parameters, including the number of quantizers, bps, and other codec-specific settings. Exploring the impact of these parameters is crucial for effectively detecting codec-based audio.\\n\\n- **Generalized ADD methods**: The current ADD methods, such as W2V2-AASIST, exhibit a significant performance decline in OOD scenarios. Designing a generalized approach that can detect both vocoder-based audio and LLM-based audio is currently a crucial challenge.\\n\\n- **Source tracing methods**: It is crucial to develop source tracing methods, as this significantly contributes to safeguarding the copyright of ALM developers. Specifically, we need to design a source tracing method with generalization capabilities. This method should be able to perform classification in the ID ALM and identify novel ALM in OOD situations, which can effectively addressing the continuous emergence of ALM algorithms.\\n\\n5. Conclusion\\n\\nIn this paper, we propose Codecfake dataset for detect the audio based on LLM. We select seven representative neural codec methods to construct fake audio. With this dataset, we first investigate the performance of vocoder-trained ADD models for codec deepfake detection task. Due to the unsatisfactory outcomes achieved by vocoder-trained ADD methods, we turned to training with the Codecfake dataset. The experimental results underscore the efficacy of training with the Codecfake dataset, as the W2V2-AASIST model attains an average lowest EER of 0.177%, reflecting a significant decrease of 41.406% compared to the vocoder-trained models.\\n\\n6. Acknowledgements\\n\\nThis work is supported by the National Natural Science Foundation of China (NSFC) (No.62101553, No.62306316, No.U21B20210, No. 62201571).\"}"}
