{"id": "kim22p_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric\\nSuyoun Kim, Duc Le, Weiyi Zheng, Tarun Singh, Abhinav Arora, Xiaoyu Zhai, Christian Fuegen, Ozlem Kalinli, Michael L. Seltzer\\nMeta AI, USA\\nsuyounkim@fb.com\\n\\nAbstract\\nMeasuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception/judgement of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a higher impact on user perception. In this work, we propose evaluating ASR output hypotheses quality with SemDist that can measure semantic correctness by using the distance between the semantic vectors of the reference and hypothesis extracted from a pre-trained language model. Our experimental results of 71K and 36K user annotated ASR output quality show that SemDist achieves higher correlation with user perception than WER. We also show that SemDist has higher correlation with downstream Natural Language Understanding (NLU) tasks than WER.\\n\\nIndex Terms: ASR metric, user perception, user satisfaction\\n\\n1. Introduction\\nAs voice-driven interfaces to devices become mainstream, measuring speech recognition system that can reflect user perception/judgement becomes increasingly important. Word Error Rate (WER) has been traditionally used to measure automatic speech recognition (ASR) system quality, However, it is sometimes not correlated to user perception of ASR transcription quality. This is because WER weights every word equally, and does not consider semantic correctness which has more impact on user perception. Figure 1 shows an example where WER does not reflect the user perception. When the reference is \\\"set an alarm for 7 am\\\" and two ASR hypotheses are: \\\"set a alarm for 7 am\\\" and \\\"cancel an alarm for 7 am\\\", then the former hypothesis would be preferred by the users or downstream tasks. However, WER by itself cannot identify which hypothesis is better as the error rates are identical.\\n\\nOver the years, prior work has attempted to address some of WER's issues by taking word importance weight into account [1] or adopting information retrieval to measure the performance [2\u20134]. All of these metrics have been based on literal-level surface-form word correctness and are not able to measure semantic level correctness.\\n\\nMeanwhile, many prior studies on transformer [5] based pre-trained neural language models, such as a Bidirectional Encoder Representations from Transformers (BERT), a Robustly Optimized BERT (RoBERTa), and a cross-lingual language models (XLM) [6\u201310] showed promising results in Natural Language Processing (NLP) and Natural Language Understanding (NLU) tasks. These general-purpose language models are pre-trained on billions of words, and have shown the ability to represent textual semantic information in the form of low-dimensional continuous vectors (i.e., embeddings) in textual similarity, question answering, paraphrasing, and sentiment analysis tasks [7, 8].\\n\\nRecently, [11\u201316] have attempted to use these embedding features generated from the pre-trained language models to evaluate NLP/NLU systems such as machine translation and image captioning systems, and have shown their metric correlates better with human judgments and provides stronger model selection performance than existing metrics. Thus far, the research in measuring semantic correctness has been more focused on NLP/NLU systems. More recently, Semantic Distance (SemDist) [17] was proposed to measure semantic correctness for ASR systems by using semantic embeddings and showed higher correlation with the downstream NLU task of intent recognition and semantic parsing, compared to WER. To the best of our knowledge, there have been no studies on user perception/judgement of ASR quality with SemDist metric.\\n\\nIn this work, we first focus on studying the user perception/judgement of ASR quality using SemDist metric. We evaluated 71K and 36K user annotated ASR output quality and show that SemDist achieves higher correlation with user perception than WER. Secondly, we explore a variety of strategies to compute SemDist as well. We show that the latest XLM-based [9,10] token pairwise method [12] performed more robustly than RoBERTa-based mean-pooling method that was used in previous work [17]. Additionally, we show SemDist results on NLU tasks and its higher correlation than WER. Finally, we build a user perception/judgement model and show SemDist helps to estimate user perception accurately and provides insight into model selection.\"}"}
{"id": "kim22p_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Semantic Distance (SemDist)\\n\\nIn this work, we measure ASR output hypothesis quality using SemDist [17] approach with transformer-based [5] pre-trained LM [8,9]. SemDist is calculated in two steps. First, we forward the reference transcription and corresponding ASR hypothesis to the pre-trained LM and obtain semantic embeddings of the reference ($e_{\\\\text{ref}}$) and hypothesis ($e_{\\\\text{hyp}}$). Second, we calculate the distance between these two embeddings, ($e_{\\\\text{ref}}$) and ($e_{\\\\text{hyp}}$), by using cosine distance function. Although the raw value of SemDist theoretically has the same range as cosine similarity (-1 to 1), we observe that SemDist has a more limited range in practice. Thus, once we obtain SemDist, we optionally multiply it with a scalar ($\\\\alpha = 1,000$) just for improving readability. Depending on the users' readability preference, $\\\\alpha$ can be chosen, and it will not affect the experimental correlation results or the model comparison results.\\n\\nSemDist can be obtained in various ways depending on (1) which pre-trained language model we use and (2) how to extract the embeddings. In our experiments, we compare four different SemDist: SemDist(RoBERTa-mean-pooling), SemDist(XLM-mean-pooling), SemDist(XLM-[CLS]), and SemDist(XLM-pairwise-token).\\n\\n2.1. SemDist(RoBERTa-mean-pooling)\\n\\nIn this method, we obtain SemDist by using RoBERTa model [8] as described in [17]. We first obtain the semantic embeddings of the reference ($e_{\\\\text{mean}}\\\\text{ref}$) and hypothesis ($e_{\\\\text{mean}}\\\\text{hyp}$) by performing mean-pooling over all output token embeddings from RoBERTa model. We then calculate the cosine distance between ($e_{\\\\text{mean}}\\\\text{ref}$) and ($e_{\\\\text{mean}}\\\\text{hyp}$) to obtain SemDist.\\n\\n$$\\\\text{SemDist} = 1 - \\\\cos sim(e_{\\\\text{mean}}\\\\text{ref}, e_{\\\\text{mean}}\\\\text{hyp})$$\\n\\n2.2. SemDist(XLM-mean-pooling)\\n\\nThe process of this method is same as SemDist(RoBERTa-mean-pooling), but we use the XLM-R model [9, 10] instead of RoBERTa model [8] to extract the semantic embeddings. We can compare SemDist(XLM-mean-pooling) and SemDist(RoBERTa-mean-pooling) to see how the pre-trained LMs affect the correlation results (will be shown in 3).\\n\\n2.3. SemDist(XLM-[CLS])\\n\\nIn this method, we directly use the embedding from the [CLS] token, instead of mean-pooling over all output token embeddings. The [CLS] token is a special token for the BERT-based models that is prepended to the first word and trained to hold information about the entire sentence. Once we obtain $e_{\\\\text{CLS}}\\\\text{ref}$, $e_{\\\\text{CLS}}\\\\text{hyp}$, we compute the cosine distance between them and obtain the SemDist(XLM-[CLS]).\\n\\n$$\\\\text{SemDist} = 1 - \\\\cos sim(e_{\\\\text{CLS}}\\\\text{ref}, e_{\\\\text{CLS}}\\\\text{hyp})$$\\n\\n2.4. SemDist(XLM-pairwise-token)\\n\\nThis method inspired by [12]. Instead of using cosine distance between $e_{\\\\text{ref}}$ and $e_{\\\\text{hyp}}$ which represent the sentence-level semantic information, we use the token-level semantic information ($e_{\\\\text{tok}}\\\\text{ref}$ and $e_{\\\\text{tok}}\\\\text{hyp}$). We use the concept of the \\\"F1\\\" measure which is the combination of \\\"Precision\\\" ($p_{\\\\text{hyp}}$) and \\\"Recall\\\" ($r_{\\\\text{ref}}$) of each token of the reference. We first compute the pairwise cosine similarity between token embeddings that generated from the XLM-R model [9, 10] on the reference and the hypothesis. The \\\"Precision\\\" ($p_{\\\\text{hyp}}$) represents the fraction of the reference token with the highest similarity score among the hypothesis tokens (in Equation 4) and \\\"Recall\\\" ($r_{\\\\text{ref}}$) is the fraction of the hypothesis token with the highest similarity score among the reference tokens (in Equation 5). Finally, we use 1 - F1-score (the harmonic mean between $p_{\\\\text{hyp}}$ and $r_{\\\\text{ref}}$), $d_{\\\\text{ref}}$ and $d_{\\\\text{hyp}}$ as our SemDist (in Equation 6).\\n\\n$$p_{\\\\text{hyp}} = \\\\frac{1}{|e_{\\\\text{hyp}}|} \\\\sum_{\\\\text{tok} \\\\in e_{\\\\text{hyp}}} \\\\max_{\\\\text{tok} \\\\in e_{\\\\text{ref}}} (\\\\cos sim(e_{\\\\text{tok}}\\\\text{ref}, e_{\\\\text{tok}}\\\\text{hyp}))$$\\n\\n$$r_{\\\\text{ref}} = \\\\frac{1}{|e_{\\\\text{ref}}|} \\\\sum_{\\\\text{tok} \\\\in e_{\\\\text{ref}}} \\\\max_{\\\\text{tok} \\\\in e_{\\\\text{hyp}}} (\\\\cos sim(e_{\\\\text{tok}}\\\\text{ref}, e_{\\\\text{tok}}\\\\text{hyp}))$$\\n\\n$$\\\\text{SemDist} = 1 - \\\\frac{2}{p_{\\\\text{hyp}} + r_{\\\\text{ref}}}$$\\n\\nFigure 1 illustrates the overall procedure to obtain SemDist with simple examples of two hypotheses A (set a alarm for 7 am) and B (cancel an alarm for 7 am) given the reference (set an alarm for 7 am) and shows how SemDist and WER differ from each other. Naturally, the users or downstream tasks prefer hypothesis A over B, because A has only minor syntactic error (an \u2192 a) which does not hurt its meaning. As seen in this example, WER cannot separate these two hypotheses (16.7% vs. 16.7%) because it only measures literal word-level correctness and both hypotheses A and B has one incorrect word. However, SemDist can indicate that hypothesis A is better than B (0.7 vs. 38.0) by measuring semantic correctness.\"}"}
{"id": "kim22p_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Correlation between user judgement and downstream NLU tasks and various ASR metric: CER, WER, and four different SemDist: SemDist-RM (RoBERTa-mean-pooling), SemDist-XM (XLM-mean-pooling), SemDist-XC (XLM-[CLS]), and SemDist-XT (XLM-pairwise-token). The Pearson correlation coefficients are reported.\\n\\n| Task               | utter Word Len. | CER | WER | SemDist-RM | SemDist-XM | SemDist-XC | SemDist-XT |\\n|--------------------|-----------------|-----|-----|------------|------------|------------|------------|\\n| User Judgement task| UserChoice      | 36k | 9.8 | 0.13       | 0.28       | 0.31       | 0.35       | 0.32       | 0.39       |\\n| User Judgement task| UserRating      | 71k | 9.8 | 0.14       | 0.36       | 0.47       | 0.52       | 0.51       | 0.59       |\\n| NLU task           | IntentAcc       | 10k | 2.4 | 0.32       | 0.33       | 0.32       | 0.38       | 0.37       | 0.37       |\\n| NLU task           | SemanticParsing(EM) | 10k | 2.4 | 0.24       | 0.26       | 0.28       | 0.31       | 0.30       | 0.31       |\\n| NLU task           | SemanticParsing(EMTree) | 10k | 2.4 | 0.28       | 0.29       | 0.29       | 0.34       | 0.34       | 0.33       |\\n\\nFigure 2: Comparison of the distribution of SemDist and WER for each user-rating. The box extends from the lower to upper quartile values, with a line at the median.\\n\\n3.1. Hypothesis Rating (HypRating)\\nHypRating consists of 73k user-ratings of ASR hypotheses. We collected the HypRating user annotation twice on our 36k evaluation set. The annotators are asked to listen to the audio and rate the hypotheses. There are four rating levels: \u2018exact match\u2019, \u2018useful hyp\u2019, \u2018wrong hyp\u2019, \u2018nonsense hyp\u2019. A \u2018useful hyp\u2019 can be thought of as a hyp which has errors, but the downstream task can still be successful. In order to quantify hypothesis ratings, we assign the integer 0, 1, 2, and 3, to \u2018exact match\u2019, \u2018useful hyp\u2019, \u2018wrong hyp\u2019, and \u2018nonsense hyp\u2019, respectively.\\n\\n3.2. Side-by-Side Hypothesis Choice (HypChoice)\\nHypChoice consists of 38k user annotations for ASR hypothesis pairs. The annotators are asked to listen to the audio and choose which hypothesis is better between two hypotheses A and B, and answer one of three: \u2018hyp A\u2019; A is better than B, \u2018hyp B\u2019; B is better than A, and \u2018equal\u2019; both are equally good (or bad). In order to quantify the user\u2019s choice, we assign the integer -1, 1, and 0, to \u2018hyp A\u2019, \u2018hyp B\u2019, and \u2018equal\u2019, respectively.\\n\\n4. Downstream NLU tasks\\n4.1. Intent Recognition and Semantic Parsing\\nWe next investigated the correlation of SemDist to three NLU tasks: intent recognition (IntentAcc), semantic parsing (EM), and semantic parsing (EMTree) [22]. We used 10k Assistant domain ASR hypotheses that generated from our strong baseline ASR system then evaluated these hypotheses with our NLU system. The detail of the ASR and NLU system is in [17]. Note that the size of evaluation set for this NLU task (10k) is smaller than the original assistant domain evaluation set (15k) because we selected the utterances that their annotations (i.e. intention, slot) are available. For the intent recognition task, we used 351 intent types. For the semantic parsing tasks, we used the decoupled semantic representation form [23] that allows the nested intents and slots. The EM is the strictest metric, which is 1 only when all the intents and the slots in the utterance are predicted correctly. The EM Tree is similar to EM but it only allows ASR errors in recognizing slot tokens.\\n\\n5. Experiments and Results\\n5.1. Correlation Results\\nWe first evaluated the correlation of SemDist to the various user judgement tasks and downstream NLU tasks. For calculating correlation for UserChoice, we used the subtraction of SemDist between two hyp A and B (SemDist_HypA - SemDist_HypB), the subtraction of WER (WER_HypA - WER_HypB), and the subtraction of CER (CER_HypA - CER_HypB). For NLU tasks, we used 1 - IntentAcc, 1 - EM, and 1 - EMTree to consistently generate positive correlations across all tasks. Table 1 shows the Pearson correlation coefficients results on each tasks: User Judgement (UserChoice, UserRating) and downstream NLU (IntentAcc, SemanticParsing(EM), SemanticParsing(EMTree)) with various ASR metric: CER, WER, and four different SemDist: SemDist-RM (RoBERTa-mean-pooling), SemDist-XM (XLM-mean-pooling), SemDist-XC (XLM-[CLS]), and SemDist-XT (XLM-pairwise-token). As seen in Table 1, we observed that SemDist is significantly higher correlated to all user judgement tasks as well as downstream NLU tasks than WER. We also observed that XLM-based SemDist better correlates than RoBERTa-based SemDist with all tasks, and pairwise-token based SemDist shows the highest correlation especially with user judgement task. These results indicate that SemDist can be a better indicator for user judgement and downstream tasks than WER, and using a good semantic embedding is important.\\n\\n5.2. How Do We Interpret SemDist Value?\\nOne possible drawback to SemDist metric is that the value of SemDist is less intuitive than WER and hard to interpret.\"}"}
{"id": "kim22p_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Gap | WER | SemDist | Ref/Hyp |\\n|-----|-----|---------|---------|\\n| 12011 | 16.67 | 0.00 | Ref: hey portal play mister blue sky  |\\n| 8742 | 50.00 | 0.00 | Ref: I smell hot dogs  |\\n| 6390 | 6.67 | 0.01 | Ref: keep away it is a nightmare thank God we are separated about four thousand kilometres |\\n| 3807 | 20.00 | 0.02 | Ref: keep time zones in mind for the next zoom call |\\n| 2725 | 66.67 | 3.14 | Ref: I'm eagerly waiting |\\n\\n(a) top-5 (Rank WER - Rank SemDist) gap\\n\\nTo provide a better understanding of how the SemDist values should be interpreted, we show SemDist distribution for each user-rating in Figure 2. As seen this figure, the users perceive approximately 12 percent of WER and 50 percent of SemDist as good enough hypothesis and over 50 percent of WER and 200 percent of SemDist as nonsense hypothesis.\\n\\n5.3. How Are WER and SemDist Different?\\nWe next investigated how WER and SemDist evaluate differently for the same hypothesis. To do so, we first defined the gap between WER and SemDist as the change of their ranking within the entire evaluation set (36k). We assigned the ranking of WER (Rank WER) and the ranking of SemDist (Rank SemDist) for each utterance by sorting WER, and SemDist value. Table 2 (a) shows the top-5 (Rank WER - Rank SemDist) gap and Table 2 (b) shows the top-5 (Rank SemDist - Rank WER) gap. We observed that SemDist more robustly measures ASR errors by not penalizing errors that do not hurt sentence meanings (i.e. contractions and compound words) as seen in Table 2 (a). We also found that SemDist detects semantically nonsense word errors which occur only once within a sentence as seen in Table 2 (b).\\n\\n5.4. Modeling User Judgement\\nOne promising aspect of SemDist is the potential to create models that can predict the user satisfaction of voice-driven applications. This can be done by training a model on pairs of SemDist and user-rating. We created three linear regression models from 71k of pairs of user-rating and (1) WER only, (2) SemDist only, and (3) both SemDist and WER. Table 3 shows the comparison of $R^2$, MAE, and MSE of three models. The results show that SemDist only can achieve 0.35 of $R^2$ and significantly outperforms than WER only. Thus, using SemDist can be a promising method for estimating user satisfaction without requiring the data annotation cost.\\n\\n|        | WER only | SemDist only | WER + SemDist |\\n|--------|----------|--------------|---------------|\\n| $R^2$  | 0.12     | 0.35         | 0.36          |\\n| MAE    | 0.38     | 0.29         | 0.29          |\\n| MSE    | 0.30     | 0.23         | 0.23          |\\n\\n6. Conclusion\\nWe evaluated 71k and 32k of user annotated ASR quality and showed that SemDist correlates significantly higher with user judgement than traditional metric, WER or CER. Key aspect of SemDist is measuring semantic correctness of ASR output by using semantic embeddings from the pre-trained language model for general purpose. In addition, we explored various strategies to compute SemDist and found that pairwise-token-based SemDist performs best in user judgement of ASR quality. We also showed SemDist correlates higher with downstream NLU task as well. Moreover, we demonstrated the potential of SemDist for providing insight into model selection by estimating user judgement more accurately. Moving forward, we will explore ways to use SemDist for training ASR systems.\"}"}
{"id": "kim22p_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] J. S. Garofolo, E. M. Voorhees, C. G. Auzanne, V. M. Stanford, and B. A. Lund, \\\"1998 TREC-7 spoken document retrieval track overview and results,\\\" NIST SPECIAL PUBLICATION SP, pp. 79\u201390, 1999.\\n\\n[2] J. Makhoul, F. Kubala, R. Schwartz, R. Weischedel et al., \\\"Performance measures for information extraction,\\\" in Proceedings of DARPA broadcast news workshop. Herndon, VA, 1999, pp. 249\u2013252.\\n\\n[3] M. J. Hunt, \\\"Figures of merit for assessing connected-word recognisers,\\\" Speech Communication, vol. 9, no. 4, pp. 329\u2013336, 1990.\\n\\n[4] I. A. McCowan, D. Moore, J. Dines, D. Gatica-Perez, M. Flynn, P. Wellner, and H. Bourlard, \\\"On the use of information retrieval measures for speech recognition evaluation,\\\" IDIAP, Tech. Rep., 2004.\\n\\n[5] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \\\"Attention is all you need,\\\" in NeurIPS, 2017.\\n\\n[6] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer, \\\"Deep contextualized word representations,\\\" in NAACL, 2018.\\n\\n[7] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \\\"BERT: Pre-training of deep bidirectional transformers for language understanding,\\\" in NAACL, 2019.\\n\\n[8] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \\\"RoBERTa: A robustly optimized BERT pretraining approach,\\\" arXiv preprint arXiv:1907.11692, 2019.\\n\\n[9] G. Lample and A. Conneau, \\\"Cross-lingual language model pre-training,\\\" arXiv preprint arXiv:1901.07291, 2019.\\n\\n[10] J. Du, M. Ott, H. Li, X. Zhou, and V. Stoyanov, \\\"General purpose text embeddings from pre-trained language models for scalable inference,\\\" arXiv preprint arXiv:2004.14287, 2020.\\n\\n[11] N. Reimers and I. Gurevych, \\\"Sentence-BERT: Sentence embeddings using Siamese BERT-networks,\\\" arXiv preprint arXiv:1908.10084, 2019.\\n\\n[12] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \\\"BERTscore: Evaluating text generation with BERT,\\\" arXiv preprint arXiv:1904.09675, 2019.\\n\\n[13] W. Zhao, M. Peyrard, F. Liu, Y. Gao, C. M. Meyer, and S. Eger, \\\"MoverScore: Text generation evaluating with contextualized embeddings and earth mover distance,\\\" arXiv preprint arXiv:1909.02622, 2019.\\n\\n[14] R. Rei, C. Stewart, A. C. Farinha, and A. Lavie, \\\"COMET: A neural framework for MT evaluation,\\\" arXiv preprint arXiv:2009.09025, 2020.\\n\\n[15] T. Sellam, D. Das, and A. P. Parikh, \\\"BLEURT: Learning robust metrics for text generation,\\\" arXiv preprint arXiv:2004.04696, 2020.\\n\\n[16] W. Yuan, G. Neubig, and P. Liu, \\\"BARTscore: Evaluating generated text as text generation,\\\" Advances in Neural Information Processing Systems, vol. 34, pp. 27263\u201327277, 2021.\\n\\n[17] S. Kim, A. Arora, D. Le, C.-F. Yeh, C. Fuegen, O. Kalinli, and M. L. Seltzer, \\\"Semantic Distance: A New Metric for ASR Performance Analysis Towards Spoken Language Understanding,\\\" in Proc. INTERSPEECH, 2021.\\n\\n[18] A. Graves, \\\"Sequence transduction with recurrent neural networks,\\\" in ICML Representation Learning Workshop, 2012.\\n\\n[19] Y. Shi, Y. Wang, C. Wu, C. Yeh, J. Chan, F. Zhang, D. Le, and M. L. Seltzer, \\\"Emformer: Efficient Memory Transformer Based Acoustic Model For Low Latency Streaming Speech Recognition,\\\" in Proc. ICASSP, 2021.\\n\\n[20] J. Mahadeokar, Y. Shangguan, D. Le, G. Keren, H. Su, T. Le, C. Yeh, C. Fuegen, and M. L. Seltzer, \\\"Alignment Restricted Streaming Recurrent Neural Network Transducer,\\\" in Proc. SLT, 2021.\\n\\n[21] D. Le, M. Jain, G. Keren, S. Kim, Y. Shi, J. Mahadeokar, J. Chan, Y. Shangguan, C. Fuegen, O. Kalinli, Y. Saraf, and M. L. Seltzer, \\\"Contextualized Streaming End-to-End Speech Recognition with Trie-Based Deep Biasing and Shallow Fusion,\\\" in Proc. INTERSPEECH, 2021, pp. 1772\u20131776.\\n\\n[22] S. Gupta, R. Shah, M. Mohit, A. Kumar, and M. Lewis, \\\"Semantic parsing for task oriented dialog using hierarchical representations,\\\" in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, E. Riloff, D. Chiang, J. Hockenmaier, and J. Tsujii, Eds. Association for Computational Linguistics, 2018, pp. 2787\u20132792. [Online]. Available: https://doi.org/10.18653/v1/d18-1300\\n\\n[23] A. Aghajanyan, J. Maillard, A. Shrivastava, K. Diedrick, M. Haeger, H. Li, Y. Mehdad, V. Stoyanov, A. Kumar, M. Lewis, and S. Gupta, \\\"Conversational semantic parsing,\\\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, B. Webber, T. Cohn, Y. He, and Y. Liu, Eds. Association for Computational Linguistics, 2020, pp. 5026\u20135035. [Online]. Available: https://doi.org/10.18653/v1/2020.emnlp-main.408\"}"}
