{"id": "zhang23r_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nPersonalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency.\\n\\nIndex Terms: personalised speech enhancement, speech separation, Transformers\\n\\n1. Introduction\\n\\nAudio source separation can greatly improve voice calls and speech recognition. Deep neural networks have lately shown great performance improvements in audio source separation tasks [1, 2, 3]. In many cases, such as on a mobile phone, data from the target user is available to improve performance further. The process of removing all other audio components from the corrupted input utterance, both environmental noise and other voices than the target user, is called personalised speech enhancement (PSE). For the mobile phone scenario, response time and application size are critical factors. Thus, streaming-based inference, latency, and model size are of paramount importance when designing on-device PSE systems.\\n\\nFor PSE systems, personalisation relies on a previously recorded enrolment clip of the target user. Previous works typically use a separate embedding neural network to summarise the entire enrolment clip into a static embedding vector, and this embedding vector is viewed as a voice profile of the user [4, 5]. A widely-used method of utilising the voice profile is to concatenate the embedding with the intermediate features of the PSE network [4, 5, 6, 7, 8, 9]. Even previous works, which utilise attention mechanisms, execute the attention between the corrupted input utterance and the single static embedding vector [10, 11, 12]. However, the single static vector may fail to capture the variability of the target user's speech.\\n\\nTo represent the variability in the enrolment audio, we propose to use a sequence of hidden states instead of a single static embedding vector. As shown in related fields, such as text-to-speech (TTS) and voice cloning, a sequence of hidden states captures the speaker characteristics better than a single vector [13, 14]. To allow a PSE system to rely on an entire sequence of vectors, we propose a fully Transformer-based model with a novel cross-attention mechanism. For each input frame, the proposed method dynamically constructs a suitable speaker representation for the separation task by choosing the most relevant enrolment audio frames. Thus, compared to [10, 11, 12], this leads to more flexible and relevant speaker representations.\\n\\nTo better match real-world applications than existing datasets, such as WSJ0-2mix [15] and LibriMix [16], we build a dataset to reflect the on-device close-talk microphone scenarios, where there are numerous types of environmental ambient noise and both of the ambient and babble noise can happen anywhere in the audio input. Through extensive experiments, we compare our proposed models with strong baselines. Thanks to the flexibility of the proposed cross-attention, our model achieves 1\u20132 dB absolute gain in terms of signal-to-distortion ratio (SDR) and 10\u201330 % relative word error rate (WER) reduction in the downstream automatic speech recognition (ASR) tasks. Even with only half the model size, our proposed model still surpasses the baselines consistently and significantly. Further, due to the halved model size, our online model gives similar or smaller latency compared to the baselines. Thus, the key contributions of our work include: (1) a novel cross-attention mechanism which utilises the dynamics of both the enrolment audio and the input audio; (2) a fully Transformer-based on-line streaming PSE model which outperforms strong baselines in the three critical aspects (i.e., PSE and downstream ASR performance, model size, and latency) of the real-world on-device scenarios; (3) a dataset for real-world on-device PSE tasks.\\n\\n2. Background\\n\\n2.1. Single Speaker Embedding Vector Extraction\\n\\nPSE systems typically condition on a single embedding vector for the speaker, which is usually extracted from an enrolment clip of the user. Previous works typically use a separate embedding neural network to summarise the entire enrolment clip into a static embedding vector, and this embedding vector is viewed as a voice profile of the user [4, 5]. A widely-used method of utilising the voice profile is to concatenate the embedding with the intermediate features of the PSE network [4, 5, 6, 7, 8, 9]. Even previous works, which utilise attention mechanisms, execute the attention between the corrupted input utterance and the single static embedding vector [10, 11, 12]. However, the single static vector may fail to capture the variability of the target user's speech.\"}"}
{"id": "zhang23r_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. For instance, a normalised i-vector [17] or a d-vector [18] is commonly used as the single embedding vector. Fig. 1 shows the workflow of using a d-vector network to extract a single enrolment embedding vector. Once the enrolment audio has been recorded, the single embedding vector does not change.\\n\\n2.2. PSE as Mask Prediction\\n\\nGiven a corrupted input utterance \\\\( X' = (x'_1, x'_2, \\\\ldots, x'_t) \\\\) and the enrolment utterance \\\\( A = (a_1, a_2, \\\\ldots, a_p) \\\\) of a target speaker, where \\\\( t \\\\) and \\\\( p \\\\) are the sequence length for \\\\( X' \\\\) and \\\\( A \\\\) respectively, a PSE task is to extract the target speaker's speech \\\\( X = (x_1, x_2, \\\\ldots, x_t) \\\\) accurately. Since our focus is on-device, we do not use convolutional layers to extract features from the raw wave [1], in order to reduce model size, overall computations, and latency. Here, we consider PSE as a spectral-subtraction task [19]. Thus, in the scope of the paper, both \\\\( X' \\\\) and \\\\( X \\\\) are spectrograms. For the corrupted input \\\\( X' \\\\), the PSE model generates a mask whose elements are in \\\\([0, 1] \\\\) by conditioning on \\\\( A \\\\). Then, the mask is applied on \\\\( X' \\\\) and the output should be as close to \\\\( X \\\\) as possible.\\n\\n3. Related Work\\n\\nFor PSE systems, the main trend to combine a static vector with the speech enhancement model is to simply concatenate it with the hidden representations originating from the corrupted input signal [4, 5, 6, 7, 8, 9, 20, 21, 22, 23, 24, 25]. Fig. 2(a) gives an example of a previous work [8] that uses concatenation. Although the authors of [8] claim to use cross-attention, as shown in Fig. 2(a), the input to the attention network is the concatenation of corrupted audio hidden states and a single static vector. Other approaches consist of integrating factorized [6] or conditioning layers [26, 27] such as FiLM [28] or learnable activation [29] to the PSE model to blend the static speaker information in a learnable way. For previous works that use attention mechanism to inject the speaker profile [10, 11, 12], the attention is still between the corrupted input and the single static vector. Fig. 2(b) illustrates the architecture of a previous work [12] which applies attention on the single static vector. In our work however, we propose to use cross-attention to attend the full sequence of enrolment utterance hidden states, since a sequence of hidden states typically better captures the speaker characteristics than a single static vector [13, 14].\\n\\nA different but related use case is where a voice profile is available for not just the target speaker, but also for an interfering speaker. [30, 31] use an attention layer to attend both the target and the interference enrolment audio once, but otherwise use only recurrent neural networks. The scenario where interfering speakers are not only known but have also enrolled could be relevant for home smart speakers, but it is not considered plausible for this work, which focuses on mobile phones. [32] uses attention mechanisms to soft-select a single static embedding for the current speaker from multiple enrolled users. This is not suitable for our mobile phone use case, where the device is typically used by a single user, and the motivation in [32] is not to produce dynamic speaker representations. [6] applies an attention mechanism only within the enrolment utterance itself to generate a single static speaker embedding. In this work however, we propose to execute attention between the enrolment clip and the corrupted input clip to generate dynamic speaker representations.\\n\\nFigure 2: Baselines which use on a single static embedding vector. We implement SpEX pc following [8] and AvaTr following [12]. TCN refers to temporal convolutional network [7, 8] and attention refers to multi-head attention layer [33].\\n\\nFigure 3: Proposed models which utilise cross-attention to attend the full sequence of enrolment hidden states.\\n\\n4. Method\\n\\nTo better capture the dynamics of both the enrolment utterance and the corrupted input audio, we propose two variations of Transformer-based encoder-decoder models. Fig. 3 presents an architectural overview of our proposed models. Considering the on-device use case, we further make these models streaming.\\n\\n4.1. Self-attention Encoder\\n\\nWe use a self-attention encoder to transform a corrupted input utterance \\\\( X' = (x'_1, x'_2, \\\\ldots, x'_t) \\\\) to a sequence of hidden states \\\\( Z = (z_1, z_2, \\\\ldots, z_t) \\\\), where \\\\( X' \\\\in \\\\mathbb{R}^{t \\\\times d_{fft}} \\\\) and \\\\( Z \\\\in \\\\mathbb{R}^{t \\\\times d_{model}} \\\\) for the proposed architectures (see Fig. 3). \\\\( d_{fft} \\\\) refers to the number of features after a fast Fourier transform (FFT). Denoting \\\\( X' \\\\) or the output of the previous encoder layer as \\\\( S \\\\), then each encoder layer can be described as:\\n\\n\\\\[\\nS' = LN(S + MHA(S, S, S)) \\\\tag{1}\\n\\\\]\\n\\n\\\\[\\nS'' = LN(S' + FFN(S')) \\\\tag{2}\\n\\\\]\\n\\nwhere \\\\( MHA, LN, FFN \\\\) denote multi-headed attention [33], layer normalisation [34] and the feed-forward network in [33], respectively. To achieve real-time streaming, we apply a relative positional encoding (RPE) [35, 36] and a mask for the MHA component such that at each time step, each encoder layer only looks back k frames.\"}"}
{"id": "zhang23r_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As discussed in the previous sections, a single static speaker embedding may fail to capture all of the speaker's characteristics. One decoder layer in [8] takes \\\\( O(n) \\\\) time, while one encoder convolutional sub-layer of [8] has a lower time-complexity than one encoder MHA sub-layer of our proposed model. Nevertheless, in Section 5, we will show that with only about half of the parameters, our proposed models can give superior results than [8]. Due to the 16kHz sample rate, the number of samples in each window is 400 and the number of subsequent features is 201. The corrupted audio is then converted to a spectrogram via the short-time Fourier transform. The spectrograms are made up of overlapping windows, with a step of 10ms and length 25ms.\\n\\nThe corrupted audio is then matched with 1 or 5 enrolment utterances, which are random audio clips from the same speaker in LibriSpeech. The remaining 10% are left as clean audio, we pair it with either an ambient noise (45%) from a different noise such as FreeSoundDataset [39] or a babble noise (45%) from a different noise such as LibriMix [16]. Since public datasets such as WSJ0-2mix [15] and LibriSpeech [38] do not perfectly match our use case, we build our own dataset to reflect the on-device close-talk microphone scenario: (1) we add periods of silence; (2) we consider overlapping and non-overlapping noise; (3) the noise can start or end at any point of the input clip; (4) The types of environmental noise are enormous. For ground-truth clean speech, we use LibriSpeech [38], which is already split into train, dev and eval sets. For training, we use all 460 hours available. For each 5-hour epoch, we leave 2-hour epochs as the development and testing set, respectively. We use the LibriMix [16] dataset as well, where both the encoder and decoder have 3 and 6 layers respectively. All MHA components have 8 heads and each head has dimension 32. The FFN has one hidden layer of dimension 320. We apply dropout [42] with a probability 0.1 to reduce the power-law compressed spectral distance \\\\([43]\\\\) between the cleaned spectrograms produced by the model.\\n\\nTherefore, based on the current input frame, the cross-attention layer in the decoder can be described as:\\n\\n\\\\[\\nY_{t+1} = LN(Y_t) + MHA(Q_t, K_t, V_t)\\n\\\\]\\n\\nwhere \\\\( Y \\\\) denotes the last time step of the enrolment clip. For the scenario where only one enrolment clip of the target user is available, we apply a pre-trained d-vector model [18] to generate a sequence of hidden states \\\\( H_{q,p} = [H_{q,1}, H_{q,2}, \\\\ldots, H_{q,h}] \\\\), motivated by attention-based neural machine translation models [33, 37], we propose to apply cross-attention better reflects the variability of the speaker's voice features over different utterances. Among the related works in Section 3, which utilise a single static embedding vector through concatenation or attention mechanisms, [4, 8, 12] make a comprehensive set of different baselines for comparisons. [8, 12] are the convolutional networks [8] and MHA networks [8, 12]. Thus, we choose [4, 8, 12] as baselines for comparisons. [8, 12] are the reduced model size and number of layers, the actual run-time of our proposed models can give superior results than [8]. Due to the lower latency, our model is similar to [8]. [12] is a Transformer-based model with only half of the parameters, resulting in lower latency. However, as we will show in Section 5, our model can surpass [8] and has a same level of complexity compared to our model. Therefore, based on the current input frame, the cross-attention is more flexible and accurate. We denote as \\\\( Y_{t+1} = LN(Y_t) + MHA(Q_t, K_t, V_t) \\\\). By a slight abuse of notation, we denote both encoder hidden states \\\\( Z_{q,p} = [Z_{q,1}, Z_{q,2}, \\\\ldots, Z_{q,h}] \\\\) and the decoder hidden states along the time axis of the single enrolment audio, as the hidden state sequence, where \\\\( q \\\\) and \\\\( p \\\\) are the user and enrolment, respectively. When \\\\( q \\\\neq p \\\\), we propose to use \\\\( H_{q,p} \\\\) as the Query (q) and we view the hidden representation of the current corrupted input frames as the Key and Value (k and v). As shown in Fig. 3, to execute the cross-attention, we map the output of the previous decoder layer. To apply the dot-product attention is more flexible and accurate. The speaker representation produced by the proposed cross-attention layer takes \\\\( O(t^2) \\\\) time, while one encoder convolutional sub-layer of our proposed model takes \\\\( O(n) \\\\) time, and especially in the multiple enrolment case, usually \\\\( t \\\\ll n \\\\). Thus, for the decoder part, our method has a lower time complexity. Typically, the time complexity of the encoder is \\\\( O(n^2) \\\\) while the decoder time complexity is \\\\( O(t^2) \\\\).\"}"}
{"id": "zhang23r_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: SDR (dB, higher is better) of models with offline and online scenarios. Cross-Seq+ 5R indicates the cross-attention is applied among 5 different enrolment clips. For the online Cross-Seq+ models, we train each model 3 to 9 times and report the mean SDR along with the standard deviation.\\n\\n| Model       | Offline Dev SDR | Offline Eval SDR | #Param | Speed |\\n|-------------|-----------------|-----------------|--------|-------|\\n| VoiceFilter | 16.71           | 16.09           | 7.8M   | N/A   |\\n| SpEX        | 17.95           | 17.86           | 11.7M  | N/A   |\\n| AvaTr       | 18.02           | 17.42           | 12.3M  | N/A   |\\n| Cross-Seq   | 18.23           | 17.54           | 10.8M  | N/A   |\\n| Cross-Seq+  | 18.70           | 18.16           | 6.1M   | N/A   |\\n| Cross-Seq+  | 18.96           | 18.41           | 12.0M  | N/A   |\\n\\n| Model (online) | Ambient WER | Babble WER | #Param | Ground Truth |\\n|----------------|-------------|------------|--------|--------------|\\n| No PSE         | 18.32       | 72.32      | N/A    | 8.34         |\\n| SpEX           | 17.18       | 32.70      | 11.7M  |              |\\n| AvaTr          | 16.20       | 25.71      | 12.3M  |              |\\n| Cross-Seq      | 16.36       | 30.09      | 10.8M  |              |\\n| Cross-Seq+      | 15.66\u00b10.15  | 26.03\u00b10.51 | 6.1M   |              |\\n| Cross-Seq+      | 14.98\u00b10.07  | 22.24\u00b10.67 | 12.0M  |              |\\n| Cross-Seq+ 5R  | 15.51\u00b10.08  | 22.65\u00b10.08 | 12.0M  |              |\\n\\nIn addition to the PSE performance, we also measure model size and inference speed, which are critical factors for on-device systems. Impressively, as shown in Table 1, the Cross-Seq+ base model outperforms SpEX pc, AvaTr, and Cross-Seq large, even with approximately half of the model size. For the streaming models, we record their speed of processing 5000 corrupted utterances through a single NVIDIA A40 GPU. We view the speed factor of streaming Cross-Seq+ base as 1.00. Table 1 shows the inference speed of streaming Cross-Seq+ base is similar to streaming SpEX pc and faster than all other streaming models. It is worth noting that the encoder of SpEX pc is a convolutional network, which is typically less computational expensive than a MHA network with a similar amount of parameters. However, due to the expressiveness of our proposed cross-attention, compared to streaming SpEX pc, streaming Cross-Seq+ base gives higher SDRs with halved model size, and this results in a similar decoding speed between Cross-Seq+ base and SpEX pc.\\n\\nWe further focus on the downstream ASR task. Table 2 shows the WERs. First, we apply the ASR system on the clean utterances to have a performance upper bound. To have a lower bound, we apply the ASR engine to the corrupted clips without any PSE module. Then, we apply an inverse fast Fourier transform (IFFT) to the output of the PSE models and feed the output of the IFFT to the ASR engine. In terms of WERs, the SpEX pc baseline gives the highest WERs among all models, although its PSE performance measured by SDR is not the worst. With halved model size, our proposed Cross-Seq+ base has significantly lower ambient WERs compared to all the baselines and similar babble WERs compared to AvaTr. When the model sizes are similar, the Cross-Seq+ large models outperform all models by a large margin.\\n\\n6. Conclusion\\nWe proposed fully Transformer-based streaming PSE models, which utilise a novel cross-attention approach to generate dynamic target speaker representations. Compared to existing PSE systems, which uses a single static speaker embedding vector, our proposed approach better captures the variability of the speech of the target speaker. The improved expressiveness of the model leads to better performance in both PSE and downstream ASR tasks as well as reduced model size and latency when compared to baselines. Our proposed cross-attention can also be integrated into PSE models with backbone architectures other than Transformers, which we left as a future work.\"}"}
{"id": "zhang23r_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Y. Luo and N. Mesgarani, \\\"Conv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\\\" TASLP, 2019.\\n\\n[2] J. Chen, Q. Mao, and D. Liu, \\\"Dual-path transformer network: Direct context-aware modeling for end-to-end monaural speech separation,\\\" in INTERSPEECH, 2020.\\n\\n[3] C. Subakan, M. Ravanelli, S. Cornell, M. Bronzi, and J. Zhong, \\\"Attention is all you need in speech separation,\\\" in ICASSP, 2021.\\n\\n[4] Q. Wang, H. Muckenhirn, K. Wilson, P. Sridhar, Z. Wu, J. Hershey, R. A. Saurous, R. J. Weiss, Y. Jia, and I. L. Moreno, \\\"Voice-filter: Targeted voice separation by speaker-conditioned spectrogram masking,\\\" in INTERSPEECH, 2019.\\n\\n[5] Q. Wang, I. L. Moreno, M. Saglam, K. Wilson, A. Chiao, R. Liu, Y. He, W. Li, J. Pelecanos, M. Nika et al., \\\"Voicefilter-lite: Streaming targeted voice separation for on-device speech recognition,\\\" in INTERSPEECH, 2020.\\n\\n[6] K. \u017dmol\u00edkov\u00e1, M. Delcroix, K. Kinoshita, T. Ochiai, T. Nakatani, L. Burget, and J. \u010cernock\u00fd, \\\"Speakerbeam: Speaker aware neural network for target speaker extraction in speech mixtures,\\\" JSTSP, 2019.\\n\\n[7] M. Ge, C. Xu, L. Wang, E. S. Chng, J. Dang, and H. Li, \\\"Spex+: A complete time domain speaker extraction network,\\\" in INTERSPEECH, 2020.\\n\\n[8] W. Wang, C. Xu, M. Ge, and H. Li, \\\"Neural speaker extraction with speaker-speech cross-attention network.\\\" in INTERSPEECH, 2021.\\n\\n[9] P. Shen, S. He, and X. Zhang, \\\"Exarn: self-attending rnn for target speaker extraction,\\\" arXiv preprint arXiv:2212.01106, 2022.\\n\\n[10] T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, \\\"A unified framework for neural speech separation and extraction,\\\" in ICASSP, 2019.\\n\\n[11] J. Han, W. Rao, Y. Long, and J. Liang, \\\"Attention-based scaling adaptation for target speech extraction,\\\" in ASRU, 2021.\\n\\n[12] S. X. Hu, M. R. Arefin, V.-N. Nguyen, A. Dipani, X. Pitkow, and A. S. Tolias, \\\"Avatr: One-shot speaker extraction with transformers,\\\" in INTERSPEECH, 2021.\\n\\n[13] H. Zhan, H. Zhang, W. Ou, and Y. Lin, \\\"Improve cross-lingual text-to-speech synthesis on monolingual corpora with pitch contour information,\\\" in INTERSPEECH, 2021.\\n\\n[14] F. Lux, J. Koch, and N. T. Vu, \\\"Exact prosody cloning in zero-shot multispeaker text-to-speech,\\\" in IEEE SLT, 2023.\\n\\n[15] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \\\"Deep clustering: Discriminative embeddings for segmentation and separation,\\\" in ICASSP, 2016.\\n\\n[16] J. Cosentino, M. Pariente, S. Cornell, A. Deleforge, and E. Vincent, \\\"Librimix: An open-source dataset for generalizable speech separation,\\\" 2020.\\n\\n[17] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \\\"Front-end factor analysis for speaker verification,\\\" TASLP, 2010.\\n\\n[18] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, \\\"Generalized end-to-end loss for speaker verification,\\\" in ICASSP, 2018.\\n\\n[19] S. V. Vaseghi, Advanced Digital Signal Processing and Noise Reduction. New York: John Wiley & Sons, 2008.\\n\\n[20] C. Xu, W. Rao, E. S. Chng, and H. Li, \\\"Time-domain speaker extraction network,\\\" in ASRU, 2019.\\n\\n[21] T. Li, Q. Lin, Y. Bao, and M. Li, \\\"Atss-net: Target speaker separation via attention-based neural network,\\\" in INTERSPEECH, 2020.\\n\\n[22] X. Ji, M. Yu, C. Zhang, D. Su, T. Yu, X. Liu, and D. Yu, \\\"Speaker-aware target speaker enhancement by jointly learning with speaker embedding extraction,\\\" in ICASSP, 2020.\\n\\n[23] S. E. Eskimez, T. Yoshioka, H. Wang, X. Wang, Z. Chen, and X. Huang, \\\"Personalized speech enhancement: New models and comprehensive evaluation,\\\" in ICASSP, 2022.\\n\\n[24] R. Giri, S. Venkataramani, J.-M. Valin, U. Isik, and A. Krishnaswamy, \\\"Personalized percepnet: Real-time, low-complexity target voice separation and enhancement,\\\" in INTERSPEECH, 2021.\\n\\n[25] M. Thakker, S. E. Eskimez, T. Yoshioka, and H. Wang, \\\"Fast real-time personalized speech enhancement: End-to-end enhancement network (e3net) and knowledge distillation,\\\" in INTERSPEECH, 2022.\\n\\n[26] T. O'Malley, A. Narayanan, Q. Wang, A. Park, J. Walker, and N. Howard, \\\"A conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,\\\" in ASRU, 2021.\\n\\n[27] B. Gfeller, D. Roblek, and M. Tagliasacchi, \\\"One-shot conditional audio filtering of arbitrary sounds,\\\" in ICASSP, 2021.\\n\\n[28] E. Perez, F. Strub, H. de Vries, V. Dumoulin, and A. Courville, \\\"Film: Visual reasoning with a general conditioning layer,\\\" in AAAI, 2018.\\n\\n[29] A. G. C. P. Ramos, A. Mehrotra, N. D. Lane, and S. Bhattacharya, \\\"Conditioning sequence-to-sequence networks with learned activations,\\\" in ICLR, 2022.\\n\\n[30] X. Xiao, Z. Chen, T. Yoshioka, H. Erdogan, C. Liu, D. Dimitriadis, J. Droppo, and Y. Gong, \\\"Single-channel speech extraction using speaker inventory and attention network,\\\" in ICASSP, 2019.\\n\\n[31] P. Wang, Z. Chen, X. Xiao, Z. Meng, T. Yoshioka, T. Zhou, L. Lu, and J. Li, \\\"Speech separation using speaker inventory,\\\" in ASRU, 2019.\\n\\n[32] R. Rikhye, Q. Wang, Q. Liang, Y. He, and I. McGraw, \\\"Closing the gap between single-user and multi-user voicefilter-lite,\\\" arXiv preprint arXiv:2202.12169, 2022.\\n\\n[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \\\"Attention is all you need,\\\" in NeurIPS, 2017.\\n\\n[34] J. L. Ba, J. R. Kiros, and G. E. Hinton, \\\"Layer normalization,\\\" arXiv preprint arXiv:1607.06450, 2016.\\n\\n[35] P. Shaw, J. Uszkoreit, and A. Vaswani, \\\"Self-attention with relative position representations,\\\" CoRR, 2018.\\n\\n[36] Z. Dai, Z. Yang, Y. Yang, J. Carbonell, Q. V. Le, and R. Salakhutdinov, \\\"Transformer-xl: Attentive language models beyond a fixed-length context,\\\" in ACL, 2019.\\n\\n[37] D. Bahdanau, K. Cho, and Y. Bengio, \\\"Neural machine translation by jointly learning to align and translate,\\\" in ICLR, 2015.\\n\\n[38] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \\\"Librispeech: an asr corpus based on public domain audio books,\\\" in ICASSP, 2015.\\n\\n[39] E. Fonseca, M. Plakal, F. Font, D. P. W. Ellis, X. Favory, J. Pons, and X. Serra, \\\"General-purpose tagging of freesound audio with audioset labels: Task description, dataset, and baseline. in proceedings of dcase 2018 workshop,\\\" 2018. [Online]. Available: https://arxiv.org/abs/1807.09902\\n\\n[40] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S. Corrado, A. Davis, J. Dean, M. Devin et al., \\\"Tensorflow: Large-scale machine learning on heterogeneous distributed systems,\\\" arXiv preprint arXiv:1603.04467, 2016.\\n\\n[41] A. Sergeev and M. Del Balso, \\\"Horovod: fast and easy distributed deep learning in tensorflow,\\\" arXiv preprint arXiv:1802.05799, 2018.\\n\\n[42] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \\\"Dropout: a simple way to prevent neural networks from overfitting,\\\" JMLR, 2014.\\n\\n[43] S. Braun and I. Tashev, \\\"A consolidated view of loss functions for supervised deep learning-based speech enhancement,\\\" in ICTSP, 2021.\\n\\n[44] S. Team, \\\"Silero models: pre-trained enterprise-grade stt/tts models and benchmarks,\\\" https://github.com/snakers4/silero-models, 2021.\"}"}
