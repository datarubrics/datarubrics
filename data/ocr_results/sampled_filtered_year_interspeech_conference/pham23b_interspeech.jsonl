{"id": "pham23b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \u201cVoxceleb: Large-scale speaker verification in the wild,\u201d Computer Speech & Language, vol. 60, p. 101027, 2020.\\n\\n[2] J. Son Chung, A. Nagrani, and A. Zisserman, \u201cVoxCeleb2: Deep speaker recognition,\u201d arXiv e-prints, pp. arXiv\u20131806, 2018.\\n\\n[3] M. McLaren, L. Ferrer, D. Castan, and A. Lawson, \u201cThe speakers in the wild (SITW) speaker recognition database.\u201d in Interspeech, 2016, pp. 818\u2013822.\\n\\n[4] Y. Fan, J. Kang, L. Li, K. Li, H. Chen, S. Cheng, P. Zhang, Z. Zhou, Y. Cai, and D. Wang, \u201cCN-Celeb: a challenging chinese speaker recognition dataset,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7604\u20137608.\\n\\n[5] \u201cZalo-AI challenge,\u201d https://challenge.zalo.ai/.\\n\\n[6] V. T. Dat, P. V. Thanh, and N. T. T. Trang, \u201cVLSP 2021 - SV challenge: Vietnamese speaker verification in noisy environments,\u201d VNU Journal of Science: Computer Science and Communication Engineering, vol. 38, no. 1, 2022.\\n\\n[7] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, \u201cS3FD: Single shot scale-invariant face detector,\u201d CoRR, vol. abs/1708.05237, 2017. [Online]. Available: http://arxiv.org/abs/1708.05237\\n\\n[8] R. Tao, Z. Pan, R. K. Das, X. Qian, M. Z. Shou, and H. Li, \u201cIs someone speaking? Exploring long-term temporal features for audio-visual active speaker detection,\u201d in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 3927\u20133935.\\n\\n[9] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, \u201cRetinaFace: Single-stage dense face localisation in the wild,\u201d CoRR, vol. abs/1905.00641, 2019. [Online]. Available: http://arxiv.org/abs/1905.00641\\n\\n[10] J. Deng, J. Guo, and S. Zafeiriou, \u201cArcFace: Additive angular margin loss for deep face recognition,\u201d CoRR, vol. abs/1801.07698, 2018. [Online]. Available: http://arxiv.org/abs/1801.07698\\n\\n[11] L. Li, R. Liu, J. Kang, Y. Fan, H. Cui, Y. Cai, R. Vipperla, T. F. Zheng, and D. Wang, \u201cCN-Celeb: multi-genre speaker recognition,\u201d CoRR, vol. abs/2012.12468, 2020. [Online]. Available: https://arxiv.org/abs/2012.12468\\n\\n[12] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A Framework for Self-supervised Learning of Speech Representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[13] D. V. Thanh, T. P. Viet, and T. N. T. Thu, \u201cDeep speaker verification model for low-resource languages and Vietnamese dataset,\u201d in Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation. Shanghai, China: Association for Computational Linguistics, 11 2021, pp. 442\u2013451. [Online]. Available: https://aclanthology.org/2021.paclic-1.47\\n\\n[14] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,\u201d in Proc. Interspeech 2020, 2020, pp. 3830\u20133834.\\n\\n[15] J. Son Chung, J. Huh, S. Mun, M. Lee, H. S. Heo, S. Choe, C. Ham, S. Jung, B.-J. Lee, and I. Han, \u201cIn defence of metric learning for speaker recognition,\u201d arXiv e-prints, pp. arXiv\u20132003, 2020.\\n\\n[16] R. K. Das, R. Tao, and H. Li, \u201cHLT-NUS SUBMISSION FOR 2020 NIST CONVERSATIONAL TELEPHONE SPEECH SRE,\u201d arXiv preprint arXiv:2111.06671, 2021.\\n\\n[17] D. Snyder, G. Chen, and D. Povey, \u201cMusan: A music, speech, and noise corpus,\u201d arXiv preprint arXiv:1510.08484, 2015.\\n\\n[18] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, \u201cA study on data augmentation of reverberant speech for robust speech recognition,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 5220\u20135224.\"}"}
{"id": "pham23b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThe success of speaker recognition systems heavily depends on large training datasets collected under real-world conditions. While common languages like English or Chinese have vastly available datasets, low-resource ones like Vietnamese remain limited. This paper presents a large-scale spontaneous dataset gathered under noisy environments, with over 87,000 utterances from 1,000 Vietnamese speakers of many professions, covering 3 main Vietnamese dialects. To build the dataset, we propose a sophisticated construction pipeline that can also be applied to other languages, with efficient visual-aided processing techniques to boost data precision. With the state-of-the-art x-vector model, training with the proposed dataset shows an average absolute and relative EER improvement of 5.48% and 41.61% when compared to the model trained on VLSP 2021, a publicly available Vietnamese speaker dataset.\\n\\nIndex Terms: speaker recognition, speaker verification, vietnamese dataset\\n\\n1. Introduction\\nSpeaker recognition is the task of identifying or verifying the identity of an individual based on their speech segments. After decades of research, speaker recognition systems have seen significant advancements, and have been deployed to various practical applications. However, all such fields require speaker recognition systems to excel under real-world conditions, where there exist multiple uncertain factors, including environmental noises, channel and microphone effects, or intrinsic variations such as speaking styles, accent, or physiological status.\\n\\nThe development of speaker recognition systems highly depends on the size and reliability of the data. At the moment, there have been some available datasets designed for this task but most of them are in commonly used languages such as English or Chinese. Some of the successful and freely available datasets include VoxCeleb [1], the VoxCeleb2 [2], the Speakers In the Wild (SITW) dataset [3] or the CN-Celeb [4] dataset. However, with low-resource languages, including Vietnamese, possessing a great number of speakers is still necessary. Therefore, having the same number of celebrities as CN-Celeb can be considered challenging due to the low population and number of celebrities compared to other big countries. Currently, the two publicly available Vietnamese datasets, ZaloAI [5] and VLSP 2021 [6], perform well for this speaker verification task, with an Equal Error Rate (EER) of 5.305%. However, the reliability of these datasets is still a question mark. The data-building pipeline of these datasets is insufficient since it lacks visual-aided pre-processing techniques applied in VoxCeleb or CN-Celeb. This can cause data mislabeling on the datasets. As previous studies have not completely solved the problem of building a low-resource language dataset with high reliability, in this study, we propose Vietnam-Celeb - a well-built dataset for Vietnamese speaker recognition. This dataset consists of more than 87,000 utterances from 1,000 Vietnamese celebrities. We take Youtube and Tiktok to be ideal places to take celebrities utterances. To preserve the precision of the data, we combine effective data-collecting strategies with sufficient visual-aided data processing techniques, such as face tracking, face verification, and active speaker verification. Furthermore, to our knowledge, this is the first Vietnamese speaker dataset that includes information about the speaker's gender and his/her dialect. These additional features help our data to be much more useful for the Vietnamese speaker verification model and any further speech processing system that involves Vietnamese.\\n\\nThe rest of this paper is organized as follows: Section 2 describes our data collecting and pre-processing pipeline, while Section 3 discusses the process and result of building the new dataset. Our experimental results are provided in Section 4. Finally, we draw conclusions in Section 5.\\n\\n2. Dataset collection & construction\\n2.1. Overall pipeline\\n\\nFigure 1: Dataset collection & construction pipeline.\\n\\nWe propose a speaker dataset-building pipeline describing our multi-stage approach to collecting a large speaker recognition dataset. Figure 1 illustrates the stages in the pipeline. The proposed pipeline can also be applied to other languages, as the chosen data sources are common for many languages. Starting...\"}"}
{"id": "pham23b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"from a list of POIs, images and videos are crawled for the designated POI, then we apply processing procedures and visual-aided filtering, such as S3FD [7] for face tracking and TalkNet [8] for active speaker verification.\\n\\nAs low-resource languages like Vietnamese data are more challenging to collect compared to Chinese (CN-Celeb) or Multilingual data (V oxCeleb), several modifications are added to our proposed pipeline. The following sections will discuss the main stages with significant enhancements compared to the pioneering works of V oxCeleb and CN-Celeb.\\n\\n2.2. Designing list of Persons of Interest (POI)\\n\\nWe manually select a list of Vietnamese celebrities from Wikipedia. The list includes 700 celebrities, which covers a wide range of professions, including actors, singers, business people, and athletes. We also select manually 200 TikTok channels of Vietnamese celebrities. Additionally, we also choose various YouTube playlists to obtain additional speakers. Only playlists with video titles named in the same patterns are chosen so that we can use simple rules to easily mark out the name of POI. At last, we obtain a list of about 1300 POI in total. However, designing the POI list in this way will result in duplicated speakers. We will deal with this problem at a later stage.\\n\\n2.3. Image crawling and processing\\n\\nFor image crawling, we use different strategies for different data sources. In the case of POI coming from TikTok, we download the TikTok profile picture for each POI. For the rest of the POI list, we download 100 images from Google Images for each POI. When searching for images, we add the phrase portrait of to the query to make sure the images contain the face of the target POI.\\n\\nAfter crawling the images, we begin to process them. For the images crawled from Google, we have to filter out irrelevant images for each POI. For each downloaded image, we perform face detection using RetinaFace [9], then we use ArcFace [10] to extract the face embeddings for each detected area. With a list of face embeddings for a POI, we perform k-means clustering and for each cluster, we first remove the image samples that fall into the following categories:\\n\\n- Image size is too small (below 64x64).\\n- Average cosine similarity score between the face embeddings of that image and other embeddings in the same cluster is lower than a pre-defined threshold.\\n\\nFinally, we remove the clusters having below 10 valid images. Figure 2a and Figure 2b illustrate two invalid clusters that will be removed. After performing the processing steps, the average number of images for each POI is 33.\\n\\n2.4. Video crawling\\n\\nThe two sources where we collected our data are YouTube and TikTok. We choose to crawl 100 top videos for each TikTok POI. For the POI taken from YouTube video titles, we download all videos of the playlists where the POI names appear in the titles.\\n\\nFor the list of celebrities obtained from Wikipedia, we apply YouTube Advanced Search Operators when searching for videos to ensure the POI\u2019s name and the keyword interview appear in the video title, which should increase the chance that the person is speaking in the video. With each search result of a POI, we collect only the top 20 videos, as we find that in many cases, the videos after the top 20 are mostly irrelevant, which either do not contain the desired keyword or POI\u2019s name.\\n\\n2.5. Audio-visual processing\\n\\nAfter obtaining cleaned video segments of every POI from stage 5, several refinement steps are conducted to ensure the validity of the collected utterances. The first step is removing duplicated utterances, as the collected videos may contain re-uploads. We detect duplicated utterances by using a speech representation model pre-trained on Vietnamese data - Wav2Vec2 [12]. We then compute the cosine distance between every pair of utterances in the dataset and keep only one of them if their distance is below a very conservative threshold.\\n\\nThe second step is to remove invalid speakers. A speaker is considered invalid if the cross-similarity score among his utterances is low. The cross-similarity score is the average cosine score of the pairwise similarity matrix between the utterances.\\n\\nThe third step is removing noisy utterances using outlier detection, in which we follow our previous work [13]. Let $Q_1$ and $Q_3$ be the first quartile and third quartile of the set of average cosine similarity score $a_i = \\\\frac{1}{n} \\\\sum_{j=0}^{n-1} S_{i,j}$. By using interquartile range ($IQR$), the valid utterance range $[a_{min}, a_{max}]$ of set $a$ can be determined in (1) and (2). Utterances having similarity scores $a_i$ outside of the valid range $[a_{min}, a_{max}]$ are considered to be noisy utterances and will be removed from the dataset.\\n\\n$$a_{min} = Q_1 - 1.5 \\\\times IQR;$$\\n$$a_{max} = Q_3 + 1.5 \\\\times IQR$$\\n\\n(1)\\n\\n(2)\\n\\nThe last step is merging duplicated speakers, as there are likely duplicated speakers in the POI list. The merging is based on the average audio similarity scores of utterances and the average visual similarity scores between speakers.\\n\\n2.6. Annotating speaker genders and Vietnamese dialects\\n\\nThe last step in our pipeline is creating the gender and dialect labels for each speaker. Dialect is an important factor when designing a Vietnamese speech dataset, as speeches uttered in different dialects have very different characteristics. There are three main dialects in the Vietnamese language: Northern dialect, Central dialect, and Southern dialect. Hence, theoretically, we can probably obtain the dialect/accent labels by processing information about the birthplaces of celebrities. However, there is one problem with Vietnamese celebrities, which is that they tend to switch to a more common dialect (Northern\"}"}
{"id": "pham23b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of some existing speaker recognition datasets\\n\\n| Name        | Environment       | Language | Data Sources       | Visual-aided | # of Spks | # of Utters | # of Hours |\\n|-------------|-------------------|----------|--------------------|--------------|-----------|------------|------------|\\n| SITW [3]    | interview         | English  | open-source media  | No           | 299       | 2,800      | -          |\\n| VoxCeleb1 [1]| mostly interview | Mostly English | YouTube | Yes   | 1,251     | 153,516    | 352        |\\n| VoxCeleb2 [2]| mostly interview | Multilingual | YouTube | Yes | 6,112     | 1,128,246  | 2,794      |\\n| CN-Celeb1 [4]| multi-genre      | Chinese  | Bilibili           | Yes          | 1,000     | 130,109    | 274        |\\n| CN-Celeb2 [11]| multi-genre    | Chinese  | multi-media        | Yes          | 2,000     | 529,485    | 1,090      |\\n| VLSP 2021 [6]| multi-genre      | Vietnamese | ASR datasets       | No           | 1,305     | 31,600     | 41         |\\n| Vietnam-Celeb| multi-genre      | Vietnamese | YouTube, TikTok | Yes | 1,000     | 87,140     | 187        |\\n\\nTo address this problem, we decide to manually create gender and dialect labels for every speaker. We sample 5 utterances for each speaker, annotated by majority voting between 3 annotators. The three annotators whom we choose come from 3 different Vietnamese regions, Northern, Central, and Southern regions so they have a high chance of correctly classifying speaker dialects.\\n\\n3. The Vietnam-Celeb dataset\\n\\nAfter going through the pipeline, we obtain the Vietnam-Celeb dataset, which consists of 1,000 speakers and more than 87,000 utterances. The total duration of the dataset is 187 hours, with all utterances resampled to 16,000 Hz. Our data covers a wide range of challenging scenarios, including interviews, podcasts, game shows, talk shows, and other types of entertainment videos. The audio samples also represent real-world conditions, where there are various types of noises, such as background chatting, music, and cheers. The sections below will discuss several statistics of Vietnam-Celeb. Vietnam-Celeb is publicly available for researchers under a GitHub repository [1].\\n\\n3.1. Utterance length distribution\\n\\nTable 2: Utterance length distribution\\n\\n| Length (s) | # of Utterances | Proportion (%) |\\n|------------|-----------------|---------------|\\n| <2         | 4,889           | 5.8%          |\\n| 2-5        | 38,159          | 43.8%         |\\n| 5-10       | 23,112          | 26.4%         |\\n| 10-20      | 14,155          | 16.1%         |\\n| 20-30      | 4,518           | 5.2%          |\\n| >30        | 2,297           | 2.7%          |\\n\\nTable 2 shows the utterance length distribution of the dataset. Short utterances make up a high amount of our data, which represents the audio the real-world speaker recognition task, where the audio inputs are mostly short.\\n\\n3.2. Gender and dialect\\n\\nThe proposed dataset is gender-balanced, with 552 male speakers, accounting for 55.2% of the speakers. For the dialect distribution, Table 3 shows the statistics of dialect in each data source. As mentioned in the previous section, Vietnamese celebrities tend to switch to their northern or southern dialect when speaking, which explains the small number of central-dialect speakers in our dataset.\\n\\n3.3. Comparison with existing datasets\\n\\nTable 1 shows the comparison of several speaker recognition datasets with Vietnam-Celeb. Compared to the initial builds of VoxCeleb and CN-Celeb, Vietnam-Celeb has a roughly smaller number of speakers, as this is the first Vietnamese speaker dataset that employs visual-aided processing in the building pipeline. Compared to another publicly available Vietnamese speaker dataset - VLSP 2021 - Vietnam-Celeb is a more complex and refined dataset:\\n\\n- The number of utterances and the total duration of Vietnam-Celeb is significantly higher than those of VLSP 2021.\\n- Vietnam-Celeb covers a wider variety of speech scenarios and can represent real-world noisy speech conditions.\\n- Vietnam-Celeb employs visual-aided processing to ensure the correctness of the data. VLSP 2021 is built using only a speaker recognition system in the pipeline.\\n- Vietnam-Celeb includes gender and dialect labels for all speakers, which is crucial in building a Vietnamese speech dataset.\\n\\n3.4. Final dataset\\n\\nTable 4: Statistics of Vietnam-Celeb subsets\\n\\n| Subset          | # of Spks | # of Utters | # of Utter Pairs |\\n|-----------------|-----------|-------------|-----------------|\\n| Vietnam-Celeb-T | 880       | 82,907      |                 |\\n| Vietnam-Celeb-E | 120       | 4,207       | 55,015          |\\n| Vietnam-Celeb-H | 120       | 4,217       | 55,015          |\\n\\nFor later research to experiment with our dataset, we have split the training set and two test sets from Vietnam-Celeb. Table 4 shows the statistics of each set. To obtain the test sets, we sampled 120 speakers from the data, with consideration to making sure the test data is gender-balanced and dialect-balanced. Additionally, inspired by the work of VoxCeleb1 [1] when creating the test sets, 120 speakers are chosen among the speakers who have the highest speech similarity scores and visual similarity scores. As the number of central-dialect speakers is very...\"}"}
{"id": "pham23b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We took all of them into building the test sets. With the obtained speakers, we then created two test sets:\\n\u2022 Vietnam-Celeb-E: An easy test set of Vietnam-Celeb. The negative pairs in the set - pairs of different speakers - are sampled randomly.\\n\u2022 Vietnam-Celeb-H: A hard test set of Vietnam-Celeb, which takes gender and dialect information into account. For creating the negative pairs, we make sure that two speakers in each pair have the same gender and dialect labels.\\n\\n4. Experiments\\n\\n4.1. Model architecture\\nWe chose the ECAPA-TDNN architecture [14] for our experiments. The input to the network is 80-dimensional MFCCs from a 25 ms window with a 10 ms frameshift. To convert the frame-level features into utterance-level features, attentive statistics pooling (ASP) is used. The utterance-level features are then passed through a fully connected layer (FC) to produce speaker embeddings. Finally, the model is optimized using AAM-Softmax Loss [15].\\n\\n4.2. Experimental setup\\nThe evaluation metric to be used in experiments is Equal Error Rate (EER). To assess the performance of our models trained on Vietnam-Celeb-T, we compare them with the ECAPA-TDNN models trained on VoxCeleb and VLSP 2021, which follow the same model configurations.\\n\\nFor the VoxCeleb ECAPA-TDNN model, we used the pre-trained model from [16]. The models trained with Vietnam-Celeb-T and VLSP 2021 are trained from scratch using the Adam optimizer for 100 epochs, with a batch size of 100 and an initial learning rate of 5e-4, decayed linearly. For data augmentation, we use the MUSAN dataset [17] for noise addition, along with the Room Impulse Response and Noise Database [18] for room impulse responses simulation.\\n\\nWe also experimented with fine-tuning the pre-trained VoxCeleb model with either VLSP 2021 or Vietnam-Celeb. For fine-tuning, we used the same training configurations as above except that we set the number of epochs to 30. The trained models are denoted as follows:\\n\u2022 Vox: The VoxCeleb pre-trained model.\\n\u2022 VLSP: The model trained on the VLSP 2021 training set.\\n\u2022 Vietnam-Celeb: The model trained on Vietnam-Celeb-T.\\n\u2022 Vox + VLSP: The VoxCeleb pre-trained model which is fine-tuned with the VLSP 2021 training set.\\n\u2022 Vox + Vietnam-Celeb: The VoxCeleb pre-trained model which is fine-tuned with Vietnam-Celeb-T.\\n\\n4.3. Experimental results\\nWe evaluate the models on 3 test sets: Vietnam-Celeb-E, Vietnam-Celeb-H, and VLSP 2021 to test the proposed dataset capabilities when being used as either a standalone training set or as a supporting dataset served for fine-tuning.\\n\\nWe first compare the results of models on the two evaluation sets of Vietnam-Celeb. Table 5 shows the experimental results. In every case, the EER on Vietnam-Celeb-E is lower than that on Vietnam-Celeb-H, as the latter contains hard negative pairs.\\n\\n| Model             | Vietnam-Celeb-E | Vietnam-Celeb-H |\\n|-------------------|-----------------|-----------------|\\n| Vox               | 13.19           | 16.52           |\\n| VLSP              | 11.58           | 14.30           |\\n| Vietnam-Celeb     | 6.31            | 8.62            |\\n| Vox + VLSP        | 10.92           | 13.49           |\\n| Vox + Vietnam-Celeb | 7.33          | 9.37            |\\n\\nThe Vox model performs the worst on the two sets. Vietnam-Celeb also outperforms VLSP with an average relative EER improvement of 41.61%. In cases of the fine-tuned models, while Vox + VLSP achieves better results compared to VLSP, using VoxCeleb as the pre-trained model does not improve the results when fine-tuning with Vietnam-Celeb-T. These results emphasize the capability of Vietnam-Celeb when used as a standalone training set.\\n\\nTable 6: EER (%) results on the VLSP test set (lower is better)\\n\\n| Model            | VLSP 2021 test set |\\n|------------------|--------------------|\\n| Vox              | 5.92               |\\n| Vox + VLSP       | 4.57               |\\n| Vox + Vietnam-Celeb | 3.69         |\\n\\nTo assess the supporting capability of Vietnam-Celeb, we experimented with different models on the VLSP 2021 test set, as shown in Table 6. The overall results are better than those of Vietnam-Celeb evaluation sets, which shows that this test set is much less challenging compared to Vietnam-Celeb test sets. While Vox has already achieved a decent EER, of about 5.92%, Vox + Vietnam-Celeb illustrates the lowest EER, of about 3.69%, which demonstrates the highly efficient supporting ability of our proposed dataset when used for fine-tuning.\\n\\n5. Conclusion\\nThis paper presents Vietnam-Celeb - a new large-scale dataset for Vietnamese speaker recognition. The dataset is built with a sophisticated data construction pipeline, with efficient visual-aided processing techniques to preserve the precision of the data. Vietnam-Celeb contains over 87,000 utterances from 1,000 Vietnamese celebrities and is the first Vietnamese speaker recognition dataset to come with gender and dialect labels. To conduct our experiments, we apply different training strategies with 3 datasets, VoxCeleb, VLSP 2021, and Vietnam-Celeb. The results show that in every case, models trained with our proposed dataset outperform those trained with a publicly available Vietnamese dataset, VLSP 2021 where they show an average absolute and relative EER improvement of 5.48% and 41.61%, respectively. We believe that this dataset will be highly useful for the research community on Vietnamese. In future works, we aim to extend Vietnam-Celeb with coverage to more various speech environments, as well as improve the quality of the current data-building pipeline.\\n\\n6. Acknowledgements\\nThis work was supported by the NA VER Corporation within the framework of collaboration with the International Research Center for Artificial Intelligence (BKAI), School of Information and Communication Technology, Hanoi University of Science and Technology under Project NA VER.2022.DA08.\"}"}
