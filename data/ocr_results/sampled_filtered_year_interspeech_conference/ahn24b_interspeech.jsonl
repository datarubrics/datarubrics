{"id": "ahn24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset generalises to the out-of-domain VCC2018 dataset.\\n\\nIndex Terms: speaker similarity, neural speech synthesis, mean opinion score, automatic assessment\\n\\n1. Introduction\\n\\nIn many areas of machine learning, the objective is to train models that emulate human cognitive abilities and aim to match human-level performance [1, 2]. However, there are cases where AI has outperformed human capabilities [3, 4]. Speaker recognition is a notable domain where AI models have shown superiority over human abilities. ECAPA-TDNN [5] represents significant progress in the field of speaker verification, demonstrating an Equal Error Rate (EER) of less than 1% on the VoxCeleb benchmark dataset [6]. Recently, self-supervised learning-based models have shown even superior verification performance [7, 8]. In contrast, human performance in speaker identification falls considerably short. Huh et al. [9] report that Amazon Mechanical Turk crowdworkers achieve an EER of 26.51% on the same dataset, and even expert researchers in speaker recognition demonstrate an EER of 15.77%. This indicates a substantial gap between the speaker characteristics that speaker recognition systems can extract and what humans can discern.\\n\\nThis gap leads to several issues, particularly when evaluating the speaker similarity of synthesised speech using speaker recognition systems. One common goal of speech generative models is to produce a consistent voice that closely matches a reference voice [10, 11, 12, 13]. Therefore, part of the evaluation of these systems is to verify the speaker similarity between the synthesised speech and the reference voice. An objective verification method involves extracting speaker feature embeddings from both voices using a speaker recognition model and measuring their cosine similarity [14, 15, 16, 17]. However, this score often significantly deviates from what humans perceive [18, 19]. As a result, the evaluation of synthesised speech relies heavily on subjective evaluation, a process that requires considerable time and resources.\\n\\nTo address this problem, techniques to automate speaker similarity assessment for synthetic speech are needed but have not been well-explored. To the best of our knowledge, there have been only two attempts at this automation. SVSNet [18] is the first end-to-end neural network model designed to evaluate speaker similarity between converted and natural speech in a voice conversion task. SVSNet takes raw waveforms as input rather than using handcrafted features to analyse speech more accurately and introduces a co-attention mechanism to resolve length and content mismatches between the two voices. This structure achieves an utterance-level linear correlation coefficient of 0.574 on the VCC2018 [20] speaker similarity evaluation dataset, released by the voice conversion challenge. Deja et al. [19] propose an automated method to evaluate speaker similarity by extending the speaker verification system. They synthesise speech samples using 354 modern text-to-speech systems and collect MUSHRA scores [21] from listening tests to build their own dataset. The authors build a regression model to predict MUSHRA speaker similarity scores from speaker embeddings of two utterances and propose a loss to compensate for data imbalance.\\n\\nThe primary challenge in developing automated speech evaluation models is the scarcity of public data [22, 23]. The VCC2018 dataset, utilised for training SVSNet, comprises 30,864 speaker similarity scores for 21,562 pairs of converted and natural utterances from 36 voice conversion systems. Each pair is evaluated by 1 to 8 subjects through crowdsourced listening tests. In Deja et al.'s study, the model is trained and evaluated on data collected by the authors, which has not been made publicly available. The training data are limited not only in size but also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featu-\\n\\nTable 1: Data statistics for VCC2018, internal dataset from Deja et al. [19], and our VoxSim.\\n\\n| # spks. | # pairs | # ratings | Unseen test spks. | Public |\\n|---------|---------|-----------|-------------------|--------|\\n| VCC2018 | 12      | 21,562    | 30,864            | \u2717      | \u2713 |\\n| Deja et al. [19] | 13      | 18,493    | 730,308           | \u2717      | \u2717 |\\n| VoxSim  | 1,251   | 41,578    | 69,409            | \u2713      | \u2713 |\\n\\nmeasuring their cosine similarity [14, 15, 16, 17]. However, this score often significantly deviates from what humans perceive [18, 19]. As a result, the evaluation of synthesised speech relies heavily on subjective evaluation, a process that requires considerable time and resources.\\n\\nTo address this problem, techniques to automate speaker similarity assessment for synthetic speech are needed but have not been well-explored. To the best of our knowledge, there have been only two attempts at this automation. SVSNet [18] is the first end-to-end neural network model designed to evaluate speaker similarity between converted and natural speech in a voice conversion task. SVSNet takes raw waveforms as input rather than using handcrafted features to analyse speech more accurately and introduces a co-attention mechanism to resolve length and content mismatches between the two voices. This structure achieves an utterance-level linear correlation coefficient of 0.574 on the VCC2018 [20] speaker similarity evaluation dataset, released by the voice conversion challenge. Deja et al. [19] propose an automated method to evaluate speaker similarity by extending the speaker verification system. They synthesise speech samples using 354 modern text-to-speech systems and collect MUSHRA scores [21] from listening tests to build their own dataset. The authors build a regression model to predict MUSHRA speaker similarity scores from speaker embeddings of two utterances and propose a loss to compensate for data imbalance.\\n\\nThe primary challenge in developing automated speech evaluation models is the scarcity of public data [22, 23]. The VCC2018 dataset, utilised for training SVSNet, comprises 30,864 speaker similarity scores for 21,562 pairs of converted and natural utterances from 36 voice conversion systems. Each pair is evaluated by 1 to 8 subjects through crowdsourced listening tests. In Deja et al.'s study, the model is trained and evaluated on data collected by the authors, which has not been made publicly available. The training data are limited not only in size but also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featu-\\n\\nmeasuring their cosine similarity [14, 15, 16, 17]. However, this score often significantly deviates from what humans perceive [18, 19]. As a result, the evaluation of synthesised speech relies heavily on subjective evaluation, a process that requires considerable time and resources.\\n\\nTo address this problem, techniques to automate speaker similarity assessment for synthetic speech are needed but have not been well-explored. To the best of our knowledge, there have been only two attempts at this automation. SVSNet [18] is the first end-to-end neural network model designed to evaluate speaker similarity between converted and natural speech in a voice conversion task. SVSNet takes raw waveforms as input rather than using handcrafted features to analyse speech more accurately and introduces a co-attention mechanism to resolve length and content mismatches between the two voices. This structure achieves an utterance-level linear correlation coefficient of 0.574 on the VCC2018 [20] speaker similarity evaluation dataset, released by the voice conversion challenge. Deja et al. [19] propose an automated method to evaluate speaker similarity by extending the speaker verification system. They synthesise speech samples using 354 modern text-to-speech systems and collect MUSHRA scores [21] from listening tests to build their own dataset. The authors build a regression model to predict MUSHRA speaker similarity scores from speaker embeddings of two utterances and propose a loss to compensate for data imbalance.\\n\\nThe primary challenge in developing automated speech evaluation models is the scarcity of public data [22, 23]. The VCC2018 dataset, utilised for training SVSNet, comprises 30,864 speaker similarity scores for 21,562 pairs of converted and natural utterances from 36 voice conversion systems. Each pair is evaluated by 1 to 8 subjects through crowdsourced listening tests. In Deja et al.'s study, the model is trained and evaluated on data collected by the authors, which has not been made publicly available. The training data are limited not only in size but also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very small number of speakers. The VCC2018 dataset also in the diversity of speakers, with both datasets featuring a very"}
{"id": "ahn24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Distributions of (a) individual scores, (b) average scores per utterance pair, (c) scores for the same speaker pairs, and (d) scores for different speaker pairs. The x-axis represents the score label and the y-axis represents the number of ratings.\\n\\nIncludes 12 speakers and 32 speaker combinations, whereas the other dataset contains only 13 target speakers. This limitation suggests that the trained similarity prediction models may not perform well on speech pairs from unseen speakers and are not suitable for evaluating zero-shot systems [24, 25]. Moreover, both datasets only consist of pairs of natural speech and speech synthesised by a few selected speech synthesis systems, indicating a lack of generalisability across different domains.\\n\\nIn this work, we introduce VoxSim, a large-scale open dataset that evaluates cognitive speaker similarity scores for speech pairs. To the best of our knowledge, this is the first dataset specifically crafted for training models that automate voice similarity assessment. VoxSim consists of approximately 70k similarity ratings from over 1k speakers. Since the utterances are sampled from V oxCeleb1 [6], the evaluation model is exposed to a variety of channel effects and noise during training, enhancing its generalisation performance across speech domains.\\n\\nTable 1 compares VoxSim with the previous two datasets in terms of the number of speakers, pairs, ratings, the unseen status of test speakers, and their availability to the public. We provide baseline results for various speaker similarity prediction models on the VoxSim dataset and demonstrate the generalisability of our data through testing on the VCC2018 dataset.\\n\\n2. VoxSim Dataset\\n\\nData source. Speaker similarity ratings are collected using utterances from V oxCeleb1, which serves as the benchmark dataset for speaker identification and verification tasks. It consists of utterances extracted from videos uploaded to YouTube, thus the speech segments contain various acoustic environments. The speakers encompass a wide range of nationalities and ages. We create 50k random utterance pairs and conduct a listening test to evaluate the voice similarity. Diverse speaker combinations are created by organising speaker pairs to ensure a 1.5 times higher number of different speaker pairs compared to the same speaker pairs.\\n\\nListening test procedure. Twelve evaluators participate in this listening test and they are asked to evaluate the speaker similarity of each pair on a 6-point Likert scale; 1: Definitely different speakers, 2: Probably different speakers, 3: Possibly different speakers, 4: Possibly the same speaker, 5: Probably the same speaker, and 6: Definitely the same speaker. Evaluators are requested to rate as evenly as possible from 1 to 6 points, aiming to prevent the dataset's score distribution from being skewed towards either 1 or 6. Given the diverse environmental and linguistic contexts present in V oxCeleb dataset, the evaluators are guided to focus solely on the voice characteristics of the main speaker, regardless of the content of the utterance, language, and the acoustic environment. An example annotation page is shown in Fig. 2.\\n\\nQuality control. During the listening test, the quality of the collected scores is controlled through periodic reviews of the speaker verification rate. Min-max normalisation is utilised to project the scores between 0 and 1, enabling the calculation of the EER against the actual speaker labels. If an evaluator's EER significantly deviates from the average EER of all evaluators, we provide feedback and ask the evaluator to conduct a re-assessment. At the end of the collection, the average speaker verification rate for all evaluators is an EER of 17.7%. After the listening test, pairs with a difference of more than 3 points between the highest and lowest evaluation scores are considered outliers and excluded.\\n\\nData statistics. The total number of speakers in the collected data is 1,251, which matches the total number of speakers in V oxCeleb1. Train and test sets are structured to include distinct speakers, thereby ensuring the trained models' generalisation capabilities for unseen speakers. This division results in 1,142 speakers for training and 109 speakers for testing. After segregating the speakers, refining the collected scores yields 69,409 ratings for 41,578 pairs. In summary, dataset statistics and the train/test split are provided in Table 2, and the distributions of scores are illustrated in Fig. 1.\\n\\nTable 2: VoxSim Train and Test set statistics.\\n\\n| Set   | # spks. | # spk combs. | # pairs | # ratings |\\n|-------|---------|--------------|---------|-----------|\\n| Train | 1,142   | 24,764       | 38,802  | 63,845    |\\n| Test  | 109     | 904          | 2,776   | 5,564     |\\n| Total | 1,251   | 25,668       | 41,578  | 69,409    |\\n\\nFigure 2: Speaker similarity annotation page.\"}"}
{"id": "ahn24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 3: VoxSim test set results.\\n\\n| Model          | LCC \u2191 | SRCC \u2191 | R2 \u2191 | MSE \u2193 | ACC \u2191 |\\n|----------------|-------|--------|------|-------|-------|\\n| ECAPA-TDNN     | 0.768 | 0.758  | 0.521| 1.471 | 0.316 |\\n| fine-tune      | 0.827 | 0.824  | 0.681| 0.981 | 0.412 |\\n| \\n| WavLM-ECAPA    | 0.752 | 0.736  | 0.505| 1.520 | 0.306 |\\n| fine-tune      | 0.833 | 0.835  | 0.690| 0.951 | 0.402 |\\n| \\n| SVSNet         |       |        |      |       |       |\\n| train on indiv. | 0.758 | 0.753  | 0.549| 1.384 | 0.397 |\\n| train on av.   | 0.747 | 0.742  | 0.530| 1.443 | 0.378 |\\n\\n---\\n\\n### 3. Experimental Setup\\n\\n#### 3.1 Model architectures\\n\\nWe adopt three model architectures for speaker similarity prediction experiments. ECAPA-TDNN [5] is a state-of-the-art model designed for automatic speaker verification. Given the attempts [26, 27] to apply self-supervised learning (SSL) based models to develop Mean Opinion Scores (MOS) prediction models to enhance generalisation performance across multiple speech datasets, WavLM-ECAPA is adopted. This model uses SSL-based WavLM [8] as a feature encoder. Finally, we also experiment with SVSNet [18], the only publicly available model specifically designed for predicting speaker similarity.\\n\\n**ECAPA-TDNN.** ECAPA-TDNN enhances the traditional Time Delay Neural Network (TDNN) architecture by incorporating Squeeze-and-Excitation (SE) blocks to recalibrate channel-wise feature responses dynamically. Additionally, the model employs a multi-layer feature aggregation mechanism that enhances its ability to capture speaker characteristics across various temporal resolutions. This architecture leverages the power of attention mechanisms and convolutional layers to achieve superior performance in speaker verification.\\n\\n**WavLM-ECAPA.** In Chen et al. [7], the authors leverage speech representations extracted from SSL-based pre-trained models for automatic speaker verification. The integration of a pre-trained feature encoder and ECAPA-TDNN downstream network demonstrates significant improvement in verification performance. In our experiments, we use WavLM as the feature extraction model, which has shown strong performance in several speech processing tasks. WavLM is trained with masked speech denoising and prediction in the pre-training, which makes it robust in complex acoustic environments and effective in preserving speaker identity.\\n\\n**SVSNet.** SVSNet takes raw waveforms as input to fully utilise speech information for prediction and aligns the representations of the two inputs in two directions through a co-attention module. The model can be trained either in a regression manner using an L2 loss or in a classification manner using a cross-entropy loss, with the regression approach being shown to be superior.\\n\\n---\\n\\n### 3.2 Implementation details\\n\\nOur implementation is based on the PyTorch [28] framework and is trained on an NVIDIA RTX A6000 with 48GB of memory. During the training of ECAPA-TDNN and WavLM-ECAPA, a 3-second segment from each utterance is randomly sampled to form a batch. Additionally, for ECAPA-TDNN, an 80-dimensional filterbank is extracted as input. These models extract a 256-dimensional speaker embedding for each utterance and predict speaker similarity by computing the cosine similarity between the extracted embeddings. The predicted score is compared to a similarity label projected on a 0 to 1 scale, and the model is trained using MSE loss. To facilitate effective speaker feature extraction at the beginning of training, we use models pre-trained with a speaker identification approach on the VoxCeleb dataset. An Adam [29] optimizer is employed with an initial learning rate of $10^{-5}$, which decreases by 5% at each epoch. For SVSNet, a regression-based model is adopted, following the experimental setup of the original paper [18]. Every experiment is conducted three times independently to reduce the impact of random initialisation, and the average and standard deviation of these experiments are reported.\\n\\n### 3.3 Evaluation metrics\\n\\nThe evaluation is based on the model's predicted similarity score compared to the average similarity score of the utterance pair. The model is evaluated with the following metrics: linear correlation coefficient (LCC), Spearman's rank correlation coefficient (SRCC), coefficient of determination (R2), mean squared error (MSE), and accuracy (ACC). For accuracy, a prediction is considered a true positive if the predicted similarity score is within 0.5 from the ground-truth label.\\n\\n---\\n\\n### 4. Results\\n\\n#### 4.1 Results on VoxSim test set\\n\\nWe first compare ECAPA-TDNN, WavLM-ECAPA, and SVSNet on the VoxSim test set, training each model using either individual scores or average scores per utterance pair. ECAPA-TDNN and WavLM-ECAPA are fine-tuned from a pre-trained speaker recognition model, whereas SVSNet is trained from scratch as it is not specifically designed for speaker recognition. The experimental results are summarised in Table 3. Although the pre-trained speaker recogniser ECAPA-TDNN and WavLM-ECAPA exhibit strong performance on VoxCeleb1, with speaker verification EERs of 0.96% and 0.43%, respectively, they achieve LCCs of only 0.768 and 0.733 for speaker similarity prediction, showing much lower correlation than the models fine-tuned with the VoxSim train set. Notably, WavLM-ECAPA achieves an LCC of 0.837, indicating superior performance in speaker similarity prediction.\\n\\n---\\n\\n1. [https://github.com/n1243645679976/SVSNet](https://github.com/n1243645679976/SVSNet)\"}"}
{"id": "ahn24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speaker recognition pre-training ablation.\\n\\n| Model                          | LCC    | MSE     |\\n|-------------------------------|--------|---------|\\n| SVSNet                        | 0.747  | \u00b10.001  |\\n| ECAPA-TDNN w/o speaker pre-train | 0.761  | \u00b10.001  |\\n| ECAPA-TDNN w/ speaker pre-train | 0.829  | \u00b10.001  |\\n| WavLM-ECAPA w/o speaker pre-train | 0.806  | \u00b10.002  |\\n| WavLM-ECAPA w/ speaker pre-train | 0.835  | \u00b10.002  |\\n\\nFigure 3: t-SNE plot for embeddings extracted from (left) speaker recognition model and (right) speaker similarity prediction model. Different colours represent distinct speakers, and the number next to the dashed line represents the human-assessed speaker similarity between two utterances.\\n\\nECAPA, despite its higher speaker verification performance, shows a lower speaker similarity prediction rate compared to ECAPA-TDNN. This suggests that the speaker recogniser may lose features related to speaker similarity while focusing on distinguishing between different speakers and aggregating embeddings of the same speaker. There is no clear superiority between models trained on individual scores and those trained on average scores. WavLM-ECAPA outperforms the others in all metrics except for accuracy, where ECAPA-TDNN achieves the highest accuracy.\\n\\nSpeaker recognition pre-training ablation.\\n\\nIn particular, the fine-tuned ECAPA-TDNN and WavLM-ECAPA exhibit significant predictive performance compared to SVSNet, which can be attributed to the speaker recognition pre-training. To establish the effectiveness of speaker recognition pre-training, we train ECAPA-TDNN and WavLM-ECAPA from scratch. As shown in Table 4, both models perform similarly to SVSNet when trained without speaker recognition pre-training. However, this pre-training improves LCC by 8.9% and 3.6%, respectively. This demonstrates that speaker recognition pre-training significantly enhances speaker similarity prediction performance.\\n\\nQualitative results from t-SNE plot.\\n\\nTo demonstrate that the speaker similarity prediction model fine-tuned on VoxSim captures perceptual similarity, we visualise the speaker embeddings extracted from the model using t-SNE plots. To clearly show the effect of training, plots for embeddings extracted from the pre-trained speaker recognition model are also provided for...\"}"}
{"id": "ahn24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Acknowledgement\\nThis work was supported by Samsung Research.\\n\\n7. References\\n[1] M. I. Jordan and T. M. Mitchell, \u201cMachine Learning: Trends, Per-\\nspectives, and Prospects,\u201d Science, vol. 349, no. 6245, pp. 255\u2013\\n260, 2015.\\n[2] Y. LeCun, Y. Bengio, and G. Hinton, \u201cDeep Learning,\u201d Nature,\\nvol. 521, no. 7553, pp. 436\u2013444, 2015.\\n[3] J. S. Chung and A. Zisserman, \u201cLip reading in the wild,\u201d in\\nProc. ACCV, 2017.\\n[4] J.-H. Kim, J. Kim, and J. S. Chung, \u201cLet There Be Sound: Re-\\nconstructing High Quality Speech from Silent Videos,\u201d in\\nProc. AAAI, 2024.\\n[5] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-\\nTDNN: Emphasized Channel Attention, Propagation and Ag-\\ngregation in TDNN Based Speaker Verification,\u201d in Proc. Inter-\\nspeech, 2020.\\n[6] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoCEleb: A Large-\\nscale Speaker Identification Dataset,\u201d in Proc. Interspeech,\\n2017.\\n[7] Z. Chen, S. Chen, Y. Wu, Y. Qian, C. Wang, S. Liu, Y. Qian, and\\nM. Zeng, \u201cLarge-Scale Self-Supervised Speech Representation\\nLearning for Automatic Speaker Verification,\u201d in Proc. ICASSP,\\n2022.\\n[8] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li,\\nN. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavLM: Large-Scale\\nSelf-Supervised Pre-Training for Full Stack Speech Processing,\u201d\\nIEEE Journal of Selected Topics in Signal Processing,\\nvol. 16, no. 6, pp. 1505\u20131518, 2022.\\n[9] J. Kang, J. Huh, H. S. Heo, and J. S. Chung, \u201cAugmentation\\nAdversarial Training for Self-Supervised Speaker Representation\\nLearning,\u201d IEEE Journal of Selected Topics in Signal Processing,\\nvol. 16, no. 6, pp. 1253\u20131262, 2022.\\n[10] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and\\nM. A. Ponti, \u201cYourTTS: Towards Zero-Shot Multi-Speaker TTS\\nand Zero-Shot Voice Conversion for Everyone,\u201d in Proc. ICML,\\n2022.\\n[11] K. Shen, Z. Ju, X. Tan, Y. Liu, Y. Leng, L. He, T. Qin, S. Zhao, and\\nJ. Bian, \u201cNaturalspeech 2: Latent Diffusion Models Are Natural\\nand Zero-Shot Speech and Singing Synthesizers,\u201d in Proc. ICLR,\\n2024.\\n[12] H.-S. Choi, J. Yang, J. Lee, and H. Kim, \u201cNANSY++: Unified\\nVoice Synthesis with Neural Analysis and Synthesis,\u201d in Proc.\\nICLR, 2023.\\n[13] J. Choi, J. Hong, and Y. M. Ro, \u201cDiffV2S: Diffusion-based Video-\\nto-Speech Synthesis With Vision-Guided Speaker Embedding,\u201d in\\nProc. CVPR, 2023.\\n[14] J.-H. Kim, H.-S. Yang, Y.-C. Ju, I.-H. Kim, and B.-Y. Kim,\\n\u201cCrossSpeech: Speaker-Independent Acoustic Representation for\\nCross-Lingual Speech Synthesis,\u201d in Proc. ICASSP, 2023.\\n[15] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz,\\nM. Williamson, V. Manohar, Y. Adi, J. Mahadeokar et al., \u201cVoice-\\nbox: Text-Guided Multilingual Universal Speech Generation at\\nScale,\u201d in Proc. NeurIPS, 2024.\\n[16] J. Kim, K. Lee, S. Chung, and J. Cho, \u201cCLaM-TTS: Improving\\nNeural Codec Language Model for Zero-Shot Text-to-Speech,\u201d in\\nProc. ICLR, 2024.\\n[17] Y. Jang, J.-H. Kim, J. Ahn, D. Kwak, H.-S. Yang, Y.-C. Ju, I.-H. Kim,\\nB.-Y. Kim, and J. S. Chung, \u201cFaces that speak: Jointly\\nsynthesising talking face and speech from text,\u201d in Proc. CVPR,\\n2024.\\n[18] C.-H. Hu, Y.-H. Peng, J. Yamagishi, Y. Tsao, and H.-M. Wang,\\n\u201cSVSNet: An End-to-End Speaker Voice Similarity Assessment\\nModel,\u201d Signal Processing Letters, vol. 29, pp. 767\u2013771, 2022.\\n[19] K. Deja, A. Sanchez, J. Roth, and M. Cotescu, \u201cAutomatic Evalu-\\nation of Speaker Similarity,\u201d in Proc. Interspeech, 2022.\\n[20] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicen-\\ncio, T. Kinnunen, and Z. Ling, \u201cThe Voice Conversion Challenge\\n2018: Promoting Development of Parallel and Nonparallel Meth-\\nods,\u201d in Odyssey, 2018.\\n[21] B. Series, \u201cMethod for the Subjective Assessment of Intermediate\\nQuality Level of Audio Systems,\u201d International Telecommunica-\\ntion Union Radiocommunication Assembly, 2014.\\n[22] C. K. Reddy, V. Gopal, and R. Cutler, \u201cDNSMOS: A Non-\\nn intrusive Perceptual Objective Speech Quality Metric to Evaluate\\nNoise Suppressors,\u201d in Proc. ICASSP, 2021.\\n[23] S. Maiti, Y. Peng, T. Saeki, and S. Watanabe, \u201cSpeechlmscore:\\nEvaluating Speech Generation Using Speech Language Model,\u201d in\\nProc. ICASSP, 2023.\\n[24] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, P. Nguyen,\\nR. Pang, I. Lopez Moreno, Y. Wu et al., \u201cTransfer Learning From\\nSpeaker Verification to Multispeaker Text-To-Speech Synthesis,\u201d in\\nProc. NeurIPS, 2018.\\n[25] S. Arik, J. Chen, K. Peng, W. Ping, and Y. Zhou, \u201cNeural Voice\\nCloning With a Few Samples,\u201d in Proc. NeurIPS, 2018.\\n[26] E. Cooper, W.-C. Huang, T. Toda, and J. Yamagishi, \u201cGeneral-\\nization Ability of MOS Prediction Networks,\u201d in Proc. ICASSP,\\n2022.\\n[27] G. Maniati, A. Vioni, N. Ellinas, K. Nikitaras, K. Klapsas, J. S.\\nSung, G. Jho, A. Chalamandaris, and P. Tsiakoulis, \u201cSOMOS:\\nThe Samsung Open MOS Dataset for the Evaluation of Neural\\nText-to-Speech Synthesis,\u201d in Proc. Interspeech, 2022.\\n[28] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan,\\nT. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An\\nImperative Style, High-Performance Deep Learning Library,\u201d in\\nProc. NeurIPS, 2019.\\n[29] D. P. Kingma and J. Ba, \u201cAdam: A Method for Stochastic Opti-\\nmization,\u201d in Proc. ICLR, 2015.\\n[30] L. Van der Maaten and G. Hinton, \u201cVisualizing Data Using t-\\nSNE.\u201d Journal of Machine Learning Research, vol. 9, no. 11,\\n2008.\"}"}
