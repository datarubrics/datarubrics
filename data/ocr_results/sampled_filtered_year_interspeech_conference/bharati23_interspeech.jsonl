{"id": "bharati23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] H. Hao, J. Wang, and H. Abudureyimu, \u201cMaximum f1-score discriminative training for automatic mispronunciation detection in computer-assisted language learning.\u201d in INTERSPEECH, 2012, pp. 815\u2013818.\\n\\n[2] D. Luo, Y. Qiao, N. Minematsu, Y. Yamauchi, and K. Hirose, \u201cRegularized-mllr speaker adaptation for computer-assisted language learning system,\u201d in Eleventh annual conference of the international speech communication association, 2010.\\n\\n[3] A. Raux and T. Kawahara, \u201cAutomatic intelligibility assessment and diagnosis of critical pronunciation errors for computer-assisted pronunciation learning.\u201d in INTERSPEECH, 2002.\\n\\n[4] S. N. Saha, \u201cSegmental and suprasegmental aspects of non-native (l2) english by native (l1) bengali speakers compared to native (l1) english: A study,\u201d Ph.D. dissertation, IIT, Kharagpur, 2016.\\n\\n[5] M. P. Lewis, G. F. Simons, and C. D. Fennig, \u201cEthnologue: languages of ecuador,\u201d SIL International, Dallas, 2015.\\n\\n[6] T. Mostafa, \u201cA contrastive analysis between bangla and english phonology: Some pedagogical recommendations,\u201d 2013.\\n\\n[7] B. Barman, \u201cA contrastive analysis of english and bangla phonemics,\u201d Dhaka University Journal of Linguistics, vol. 2, no. 4, pp. 19\u201342, 2009.\\n\\n[8] D. D. Miller, \u201cProduction of bangla stops by native english speakers learning bangla: an acoustic analysis,\u201d 2008.\\n\\n[9] T. Pongkittiphan, N. Minematsu, T. Makino, and K. Hirose, \u201cImprovement of intelligibility prediction of spoken word in japanese accentuated english using phonetic pronunciation distance and word confusability,\u201d Proc. O-COCOSDA, pp. 276\u2013281, 2014.\\n\\n[10] S. Sudhakara, M. K. Ramanathi, C. Yarra, and P. K. Ghosh, \u201cAn improved goodness of pronunciation (gop) measure for pronunciation evaluation with dnn-hmm system considering hmm transition probabilities.\u201d in INTERSPEECH, 2019, pp. 954\u2013958.\\n\\n[11] A. M. Harrison, W.-K. Lo, X.-j. Qian, and H. Meng, \u201cImplementation of an extended recognition network for mispronunciation detection and diagnosis in computer-assisted pronunciation training,\u201d in International Workshop on Speech and Language Technology in Education, 2009.\\n\\n[12] W.-K. Lo, S. Zhang, and H. Meng, \u201cAutomatic derivation of phonological rules for mispronunciation detection in a computer-assisted pronunciation training system,\u201d in Eleventh annual conference of the international speech communication association, 2010.\\n\\n[13] S. P. Rath, D. Povey, K. Vesel y, and J. Cernock y, \u201cImproved feature processing for deep neural networks.\u201d in Interspeech, 2013, pp. 109\u2013113.\\n\\n[14] Q. B. Nguyen, B. Q. Dam, M. H. Le et al., \u201cDevelopment of a vietnamese speech recognition system for viettel call center,\u201d in 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1\u20135.\\n\\n[15] Y. Shinohara, \u201cAdversarial multi-task learning of deep neural networks.\u201d in Interspeech. San Francisco, CA, USA, 2016, pp. 2369\u20132372.\\n\\n[16] S. Sultana, M. S. Rahman, and M. Z. Iqbal, \u201cRecent advancement in speech recognition for bangla: A survey,\u201d Int. J. Adv. Comput. Sci. Appl, vol. 12, no. 3, pp. 546\u2013552, 2021.\\n\\n[17] W.-K. Leung, X. Liu, and H. Meng, \u201cCnn-rnn-ctc based end-to-end mispronunciation detection and diagnosis,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 8132\u20138136.\\n\\n[18] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society, 2011.\\n\\n[19] K. O'Shea and R. Nash, \u201cAn introduction to convolutional neural networks,\u201d arXiv preprint arXiv:1511.08458, 2015.\\n\\n[20] A. Graves, \u201cLong short-term memory,\u201d Supervised sequence labelling with recurrent neural networks, pp. 37\u201345, 2012.\\n\\n[21] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\\n\\n[22] V. Likic, \u201cThe needleman-wunsch algorithm for sequence alignment,\u201d Lecture given at the 7th Melbourne Bioinformatics Course, Bi021 Molecular Science and Biotechnology Institute, University of Melbourne, pp. 1\u201346, 2008.\\n\\n[23] S. N. Saha and S. K. D. Mandal, \u201cPhonetic and phonological interference of english pronunciation by native bengali (l1-bengali, l2-english) speakers,\u201d in 2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA). IEEE, 2014, pp. 1\u20136.\\n\\n[24] S. Kibria, M. S. Rahman, M. R. Selim, and M. Z. Iqbal, \u201cAcoustic analysis of the speakers\u2019 variability for regional accent-affected pronunciation in bangladeshi bangla: a study on sylheti accent,\u201d IEEE Access, vol. 8, pp. 35 200\u201335 221, 2020.\\n\\n[25] F. Alam, S. Habib, D. A. Sultana, and M. Khan, \u201cDevelopment of annotated bangla speech corpora,\u201d 2010.\\n\\n[26] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sondereregger, \u201cMontreal forced aligner: Trainable text-speech alignment using kaldi.\u201d in Interspeech, vol. 2017, 2017, pp. 498\u2013502.\\n\\n[27] P. Boersma, \u201cPraat: doing phonetics by computer,\u201d http://www.praat.org/, 2007.\\n\\n[28] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett, \u201cDarpa timit acoustic-phonetic continuous speech corpus cd-rom. nist speech disc 1-1.1,\u201d NASA STI/Recon technical report n, vol. 93, p. 27403, 1993.\\n\\n[29] J. Yamagishi, C. Veaux, K. MacDonald et al., \u201cCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92),\u201d 2019.\\n\\n[30] J. Kominek and A. W. Black, \u201cThe cmu arctic speech databases,\u201d in Fifth ISCA workshop on speech synthesis, 2004.\\n\\n[31] C. Hansakunbuntheung and S. Thatphithakkul, \u201cUnsupervised graphoneme alignment evaluation for grapheme-to-phoneme conversion on complex asian-language orthographies,\u201d in 2016 Conference of The Oriental Chapter of International Committee for Coordination and Standardization of Speech Databases and Assessment Techniques (O-COCOSDA). IEEE, 2016, pp. 194\u2013199.\\n\\n[32] S. S. S. Yee and K. M. Soe, \u201cMyanmar dialogue act recognition using bi-lstm rnn,\u201d in 2020 23rd Conference of the Oriental CO-COSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA). IEEE, 2020, pp. 89\u201393.\\n\\n[33] V. Peddinti, D. Povey, and S. Khudanpur, \u201cA time delay neural network architecture for efficient modeling of long temporal contexts,\u201d in Sixteenth annual conference of the international speech communication association, 2015.\"}"}
{"id": "bharati23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In the last few decades, English has become a popular language as it helps us to communicate with the global world. A large population of English learners find it challenging to achieve an 'acceptable' and 'intelligible' pronunciation. To overcome these issues, various computer-assisted pronunciation training tools are designed where automatic pronunciation error detection (APED) is a core component of the system. Most of the works of APED are based on European English speech, but there is no such work reported for Bengali English speech. This paper proposes a system for pronunciation error detection of L2 English speech (L1 Bengali) at the phoneme/segmental level using a hybrid convolutional neural network and long short-term memory modules with CTC loss. Experiments are done based on newly created L2 English speaker (L1 Bengali) speech data. The results demonstrate that the proposed system outperforms the goodness of pronunciation-based methods by 15% in terms of F1 score using fbank.\\n\\nIndex Terms\\nPhoneme detection, automatic pronunciation error detection, convolutional neural network, computer-assisted pronunciation training, long short-term memory.\"}"}
{"id": "bharati23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The remainder of the paper is divided into the following sections. Section 2 provides a brief overview of the proposed system design. Experiments and their results are demonstrated in section 3. Finally, this paper is concluded with some discussion about future potential work in section 4.\\n\\n2. Proposed system design\\n\\nPhone recognizer\\n\\nTarget phonemes\\n\\nPredicted phonemes\\n\\nAlignment\\n\\nPronunciation error\\n\\nCanonical pronunciation\\n\\nFeature Extractor\\n\\nData Preparing\\n\\nStage\\n\\nData Outputting\\n\\nStage\\n\\nFigure 1: Functional block diagram of pronunciation error detection system\\n\\nFor pronunciation error detection, the input of the system is acoustic features extracted from the speech waveform such as spectrogram, PLP, MFCC and fbank and target text (or human annotated text) at the phone level. The phone recogniser consists of hybrid convolutional neural network (CNN) - Long short-term memory (LSTM) modules with connectionist temporal classification (CTC) loss inspired by CNN-RNN-CTC [17]. It predicts the possible phones as an output. The predicted phones are then compared with canonical (native-English) pronunciation using a pronunciation error detector. It gives us a predicted pronunciation error. The ground truth of pronunciation error or actual pronunciation error is acquired by comparing human-annotated text and canonical pronunciation text at the phone level. System performance can be measured from predicted and actual pronunciation errors. This process is depicted in figure 1.\\n\\n2.1. Feature extractor\\n\\nFeatures are extracted from speech utterances. Four types of features such as spectrogram, MFCC, PLP and fbank are extracted. This is done to know which features are performing better. The extraction of different features is done by Kaldi toolkit [18].\\n\\n2.2. Phone recognizer\\n\\nThis block predicts the probability of phones from features. It consists of hybrid CNN-LSTM modules with CTC loss as shown in figure 2. First, it is trained with speech features with their respective text at the phone level. Then, it is tested with new speech utterances and predicts the phones.\\n\\n2.2.1. CNN\\n\\nConvolutional neural network (CNN) [19] has been used in image processing and gives a better performance compared to other algorithms. Recently, CNN has given promising results in the domain of speech processing. It takes out spatial information and discovers local information hierarchically. This network primarily consists of the convolutional, pooling, and activation layers. Suppose there are 2D vectors of speech features $H$ and 2D filter (kernel) $F$, then convolution operation:\\n\\n$$G[i,j] = \\\\sum_{u=-\\\\infty}^{\\\\infty} \\\\sum_{v=-\\\\infty}^{\\\\infty} H[u,v] \\\\cdot F[i-u,j-v]$$\\n\\n2.2.2. LSTM\\n\\nLong short-term memory (LSTM) [20] is a feed-forward neural network which is good with sequence processing. It is a higher version of RNN, which overcomes the drawback of vanishing gradient. Unlike RNN, it can process longer sequences of data. It extracts temporal information from the speech data. Suppose $x_\\\\tau$ be the input sequence, then\\n\\n$$f_\\\\tau = \\\\phi_g(\\\\Omega f \\\\times x_\\\\tau + \\\\Psi f \\\\times \\\\lambda_\\\\tau - 1 + \\\\beta_f)$$\\n\\n$$i_\\\\tau = \\\\phi_g(\\\\Omega i \\\\times x_\\\\tau + \\\\Psi i \\\\times \\\\lambda_\\\\tau - 1 + \\\\beta_i)$$\\n\\n$$o_\\\\tau = \\\\phi_g(\\\\Omega o \\\\times x_\\\\tau + \\\\Psi o \\\\times \\\\lambda_\\\\tau - 1 + \\\\beta_o)$$\\n\\n$$c'_\\\\tau = \\\\phi_g(\\\\Omega c \\\\times x_\\\\tau + \\\\Psi c \\\\times \\\\lambda_\\\\tau - 1 + \\\\beta_c)$$\\n\\n$$c_\\\\tau = f_\\\\tau \\\\cdot c_{\\\\tau-1} + o_\\\\tau \\\\cdot \\\\phi_c(c_\\\\tau)$$\\n\\nWhere, $f_\\\\tau$ stands for the forget gate, $i_\\\\tau$ for the input gate, $o_\\\\tau$ means the output gate, $c_\\\\tau$ stands for the cell state, $\\\\lambda_\\\\tau$ for the hidden state, $\\\\phi_g$ is sigmoid function, $\\\\phi_c$ indicates tanh function, $\\\\Omega$ and $\\\\beta$ be weight of the input and biases for the respective gate $x$, $\\\\Psi$ stands for weight of the hidden layer of respective gate $x$ and $(\\\\cdot)$ be the element wise multiplication.\\n\\n2.2.3. CTC\\n\\nConnectionist temporal classification (CTC) [21] helps align the sequences where aligning is difficult. For example, aligning each phone to its respective feature vector.\"}"}
{"id": "bharati23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tors. Here, it calculates loss between time series vectors and target phones. For this, different path combinations are searched to find the most likely phone sequences. Then sum over all possible paths that generate the same phone sequence.\\n\\n$$L_{CTC} = -\\\\log P(S|X)$$\\n\\nWhere,\\n\\n$$S =$$\\n\\nThe ground truth of the word sequence, and\\n\\n$$X =$$\\n\\nacoustic frames.\\n\\n$$P(S|X) = \\\\sum_{c \\\\in A(S)} P(C|X)$$\\n\\nwhere,\\n\\n$$c \\\\in A(S)$$\\n\\nis sum of all possible path.\\n\\n$$P(C|X) = T \\\\prod_{t=1}^{y(c,t)}$$\\n\\nis the joint probability of a path.\\n\\n2.3. Pronunciation error detector\\n\\nThis module aligns all three phone sequences, predicted phonemes, human-annotated phonemes and canonical phonemes illustrated in figure 3. Predicted phonemes are predicted by the phone recognizer, human-annotated/target phonemes are the actual phonemes that an L2 English learner (Bengali speaker) pronounces, and canonical phonemes are those how native English speaker would utter those phonemes. Aligning is done using the Needleman-Wunsch algorithm [22]. The alignment process is such that it calculates the smallest possible number of edit operations, such as insertion, substitution, and deletion, to change from one sequence to other. After aligning, the system performance is measured.\\n\\nFigure 3: Hierarchical evaluation structure\\n\\n3. Experiments and discussions\\n\\n3.1. Experimental Corpus\\n\\nFor this experiment, English speech spoken by Bengali speakers was recorded as there is no reported available dataset for Bengali speakers' English speech. Stimuli/text were selected according to contrastive analysis between Bengali and English phonemes set [23, 24]. As per IPA notation of English consonant phonemes compared to Bengali consonant phonemes, it is observed that English consonant phonemes /p/, /t/, /k/, /b/, /d/, /g/, /\u00d8/, /\u00c3/, /m/, /N/, /s/, /S/, /h/ are identical and the consonant phonemes /n/, /l/, /r/ are similar and consonant phonemes /f/, /v/, /8/, /D/, /z/, /Z/ are new to L1 Bengali speakers [23]. In case of vowel phonemes, based on height of the tongue (F1), position of the tongue (F2), rounded of lips (F3), there are differences between English and Bengali vowel phonemes [4]. Bengali speakers face difficulties in pronouncing similar and new phonemes. So, words, sentences and passages of recordings were designed in such a way that the occurrence of similar and new phonemes were more [25]. Twenty four participants (12 male and 12 female L1 Bengali speaker) were selected for the experiment. The speech was recorded using a recording app which shows the word or sentence on the screen one by one.\\n\\nAll the collected speech data were annotated automatically using canonical pronunciation with the help of Montreal forced aligner [26] as shown in Figure 4. To make the gold standard dataset, some portions of each speaker's speech are annotated manually by a linguistics expert using Praat software [27]. These speech utterances were divided into subgroups for training, validation, and testing. The TIMIT corpus [28] and a small fraction of the VCTK (modified it to phone level) corpus [29] were utilised to build an appropriate amount of native (L1) English speech data that was used to bootstrap the training of this model. Table 1 summarises some key statistics of these voice datasets. Canonical phoneme sets were defined based on the CMU pronunciation dictionary [30].\\n\\nFigure 4: A TextGrid after automatic annotation of Bengali speaker speech using canonical pronunciation\\n\\nTable 1: Statistics of speech corpus used in the experiment\\n\\n| Speech corpus | Subsets | Speaker | Utterances |\\n|---------------|---------|---------|------------|\\n| Modified VCTK| Train   | 50      | 1000       |\\n| TIMIT         | Train   | 630     | 6300       |\\n| Bengali speech dataset | Train | 12 | 1800 |\\n|               | Validation | 6 | 897 |\\n|               | Test     | 6 | 900 |\\n\\n3.2. Data pre-processing\\n\\nTIMIT corpus were already annotated at the phone level. But VCTK corpus was at the word level. So, to convert from word sequences to phone sequences, g2pE [31] is applied. Then, speech-to-phone alignment was done with Montreal Forced Aligner (MFA) [26] and phone-level time stamps are obtained. After that, training, validation, and testing sets are separated, and all four features sets are extracted using the Kaldi toolkit.\\n\\n3.3. Model Architecture\\n\\nWith a window size of 30 ms and a time step of 10 ms, features were extracted. The model was started with a convolutional layer followed by batch normalization, ReLu and a dropout layer which repeats five times. Then the output feature vectors of CNN were fed to four stacked Bidirectional LSTM layers.\"}"}
{"id": "bharati23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ers (380 hidden units in each layer), batch normalization and dropout layers. After that, four dense layers were employed with a softmax layer as shown in Figure 2. The CTC loss, batch size 20, and learning rate 0.001 were used to train the phone recognizer. It is trained for 500 epochs. The system configuration used for training was 'Intel\u00ae Core\u2122 i7-10700 CPU @ 2.90GHz \u00d7 16' desktop with NVIDIA T600 GPU.\\n\\n3.4. Performance Evaluation\\nThe performance of phone recognizer was measured by phone error rate (PER).\\n\\n\\\\[\\n\\\\text{Phone Error Rate} = 1 - \\\\frac{\\\\text{Accuracy}}{\\\\text{Accuracy}} \\\\tag{10}\\n\\\\]\\n\\nand,\\n\\n\\\\[\\n\\\\text{Accuracy} = \\\\frac{\\\\text{N}}{\\\\text{N} - \\\\text{S} - \\\\text{D} - \\\\text{I}} \\\\tag{11}\\n\\\\]\\n\\nwhere, I stands for insertions, D for deletions, S for substitutions, and N for the total number of phone sequences.\\n\\nThe hierarchical evaluation structure is used for pronunciation error detection as shown in Figure 3, and Table 2 illustrates the related confusion matrix. In order to assess the effectiveness of pronunciation error detection, the values of several metrics are computed, such as precision, recall, and the F-1 measure (the harmonic mean of the recall and precision), based on the statistics gathered from the four test circumstances [32]. The metrics for pronunciation error detection of the proposed system are calculated as follows:\\n\\n\\\\[\\n\\\\text{Precision} = \\\\frac{TP}{TP + FP} \\\\tag{12}\\n\\\\]\\n\\n\\\\[\\n\\\\text{Recall} = \\\\frac{TP}{TP + FN} \\\\tag{13}\\n\\\\]\\n\\n\\\\[\\n\\\\text{F}_1 \\\\text{-measure} = \\\\frac{2 \\\\times \\\\text{Precision} \\\\times \\\\text{Recall}}{\\\\text{Precision} + \\\\text{Recall}} \\\\tag{14}\\n\\\\]\\n\\nWhere, TP indicates True Positive, FP stands for False Positive, FN for False Negative.\\n\\nTable 2: Confusion matrix of pronunciation error detection\\n\\n| Total Conditions | Ground Truth | Actual Positive | Actual Negative |\\n|------------------|--------------|-----------------|-----------------|\\n|                  | Predicted    | Predicted       |                |\\n|                  | Positive     | True Positive   | False Positive  |\\n|                  | Negative     | False Negative  | True Negative   |\\n\\n3.5. Experimental results\\nFirst, phone recognition error is analysed using the phone error rate (PER) for the proposed system trained with hand-crafted acoustic features (spectrogram, mfcc, plp and fbank). In this experiment, a test subset of speech corpus is used. Table 3 provides the summary of PER of the findings. It is clear from the experiment that fbank features outperform other acoustic feature sets for our dataset.\\n\\nWith regard to pronunciation error detection, performances for spectrogram, MFCC, PLP, and fbank are compared. The goodness of pronunciation (GOP) technique is also provided, which is based on automatic speech recognition (ASR) using a hybrid deep neural network and hidden Markov model (DNN-HMM) model and pronunciation scoring-based approach. GOP's DNN component especially consists of a 4-layer time delay neural network (TDNN) [33]. The proposed model can detect identical and new phonemes' pronunciation easily but get confused with similar phonemes' pronunciation. Table 4 presents comparable outcomes. As shown, the GOP-based approach cannot compete with hybrid-based CNN-LSTM-based solutions, which have at least 15% gains in the F1 measures. This suggests that phone recognition technology can improve the effectiveness of pronunciation error detection. It is also seen from this experiment that fbank as an input feature performs better than the other features for the Bengali dataset.\\n\\nTable 3: Comparison of phone error rate\\n\\n| Input Features | % PER |\\n|----------------|-------|\\n| Spectrogram    | 28.23 |\\n| MFCC           | 27.92 |\\n| PLP            | 27.52 |\\n| FBANK          | 26.37 |\\n\\nTable 4: Results of the proposed technique and the GOP-based method for pronunciation error detection\\n\\n| Model           | Mispronunciation Detection |\\n|-----------------|----------------------------|\\n| GOP             | 54.26                      |\\n| PR-SPECTROGRAM  | 67.02                      |\\n| PR-MFCC         | 65.32                      |\\n| PR-PLP          | 64.24                      |\\n| PR-FBANK        | 62.46                      |\\n\\n4. Conclusions and future works\\nIn this paper, a system pipeline is proposed for detecting non-native English (native Bangla) speech phoneme pronunciation errors with respect to native English, especially insertion, deletion and substitution of phones (segmental errors). This pipeline includes a feature extractor, phoneme recognizer and error detector. Different types of features are extracted, such as spectrogram, perceptual linear prediction (PLP), fbank, and mel-frequency cepstral coefficients (MFCC) for the input of the phone recognizer. Phoneme recognizer include convolutional neural networks and long short-term memory modules which recognize phones of speech. Then the error detector compares output phones with the canonical and target phones and calculates pronunciation errors. All extracted features are compared with each other. The proposed system performed better when fbank is used as feature sets. Finally, a non-native English (L1 Bengali) CAPT prototype system has been developed that detects segmental (phoneme level) pronunciation errors and helps English as second language instructors as well as learners in real-world teaching and learning circumstances. In future, this system will be tested with different non-native English and a system can also be designed for detecting stress and intonation (supra-segmental) errors for L2 English speakers.\\n\\n5. Acknowledgements\\nThis work is partially supported by the grant from Science and Engineering Research Board (SERB), Government of India.\"}"}
