{"id": "maniati22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SOMOS: The Samsung Open MOS Dataset\\nfor the Evaluation of Neural Text-to-Speech Synthesis\\n\\nGeorgia Maniati1, Alexandra Vioni1, Nikolaos Ellinas1, Karolos Nikitaras1, Konstantinos Klapsas1, June Sig Sung2, Gunu Jho2, Aimilios Chalamandaris1, Pirros Tsiakoulis1\\n\\n1 Innoetics, Samsung Electronics, Greece\\n2 Mobile Communications Business, Samsung Electronics, Republic of Korea\\ng.maniati@samsung.com, a.vioni@partner.samsung.com\\n\\nAbstract\\nIn this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 different TTS systems including a variety of vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the variations in the final samples depend only on the acoustic models. The synthesized utterances provide a balanced and adequate domain, length and phoneme coverage. MOS naturalness evaluations are collected via crowdsourcing on Amazon Mechanical Turk.\\n\\nWe present in detail the design of the SOMOS dataset, as well as provide baseline results by training and evaluating state-of-the-art MOS prediction models, while we show the problems that these models face when assigned to evaluate TTS samples.\\n\\nIndex Terms: neural speech synthesis, mean opinion score, naturalness, listening test, crowdsourcing, Amazon Mechanical Turk\\n\\n1. Introduction\\nRecent advances in deep learning have resulted in the dominance of neural text-to-speech (TTS) systems in the field of speech synthesis, with state-of-the-art (SOTA) systems now approaching human-level quality. However, synthetic speech evaluation still relies on subjective mean opinion score (MOS) tests, performed by human listeners who are entrusted with the task of rating speech utterances on a five-point absolute category scale. Despite being laborious and expensive, this procedure remains prevalent in TTS systems evaluation on speech quality and naturalness, since it cannot be substituted by the existing objective assessment metrics, which do not always correlate well with human perception.\\n\\nThe development of objective metrics and models for the evaluation of generated speech relies heavily on the availability of large-scale subjective evaluation data and their respective synthetic stimuli. Such data have been publicly released by the organizers of challenges that provide a common training set and a common listening test for researchers to compare their approaches. The Blizzard Challenge (BC) [1], an annual workshop aiming to compare corpus-based speech synthesizers on the same data, has released data since 2008, which have been since used for the development of non-intrusive metrics. The limited BC data from individual years have initially fostered efforts on the extraction of hand-crafted acoustic features which predict the quality of synthetic speech [2\u20134], while larger aggregated BC 2008-2013 data have been employed for feature extraction in the first attempt to use feed-forward networks for the MOS prediction task [5]. In the same year, AutoMOS [6] explored deep recurrent architectures for predicting MOS values, which required a much larger proprietary dataset acquired over multiple years of TTS evaluation. Quality-Net [7], based on bidirectional LSTM (BLSTM), was later trained on the TIMIT dataset to predict frame-level PESQ scores.\\n\\n1.1. Relevant Work\\nNowadays, research focuses on training models for utterance-level and system-level evaluation predictions, in which feature extraction is automatically performed by the model itself, from the spectrograms provided as input to the model. This paradigm requires adequately sized datasets. For TTS, the available BC data from early challenges (2008-2015) comprise systems of earlier TTS technologies, while data from latest challenges and modern synthesizers are multilingual [8, 9]. The aggregation of data from different tests is not expected to construct an inherently predictable dataset [10], since scores are relative to contextual factors.\\n\\nThe first large-scale public dataset of subjective evaluations from a single test was released by the Voice Conversion Challenge (VCC) in 2018 [11], enabling the development of deep learning based models on open data, such as MOSNet [12], a CNN-BLSTM model for the evaluation of converted speech. MOSNet and the large VCC data have been since used as the basis for improving MOS prediction for both converted and synthesized speech [13\u201315]. However, [16] report that pretrained VC models do not generalize well to TTS, and instead use data from ASVspoof 2019 [17], which contain both synthetic and converted samples, for MOSNet training. Cooper et al [18] use the aforementioned pretrained model as an objective metric and find very weak correlation with human listener's evaluations.\\n\\nResponding to the need for standardized open datasets, the BVCC dataset [18] was released as the basis of the VoiceMOS Challenge [19]. It comprises a wide variety of TTS systems, from unit selection to neural synthesis, as well as VC systems, evaluated in a single listening test by Japanese listeners. Due to the varied vocoding quality of this dataset, MOS naturalness scores are heavily influenced by the signal quality and artifacts, thus they cannot be interpreted as reflecting the samples' prosodic adequateness. BVCC consists of about 7K utterances, imposing constrains on its use for training parameter-heavy deep learning models, e.g. need for data augmentation techniques or fine-tuning of pretrained models.\"}"}
{"id": "maniati22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1.2. Contribution\\nIn this work, we describe the SOMOS dataset, a curated dataset of synthetic speech and subjective evaluations that can foster advancements in automatic MOS prediction research. To the best of our knowledge, our work constitutes the first resource consisting of speech entirely produced by neural TTS models and their MOS ratings, adequately sized so as to train deep learning based systems that will be focused on acoustic model evaluation. Its scope is not to enable the training of a general purpose MOS prediction model, but rather assist research on TTS evaluation, in a single speaker scenario, where new TTS system ideas trained on LJ Speech can be quickly evaluated using a MOS prediction model developed from the SOMOS dataset.\\n\\nThe remainder of this document is organized as follows. In Section 2, we present the SOMOS speech dataset design. Section 3 elaborates on the construction of the subjective evaluations\u2019 dataset and the listening test results. The performance of modern MOS prediction models trained on the SOMOS dataset is discussed in Section 4, and Section 5 concludes the paper indicating directions for future work.\\n\\n2. Speech Dataset\\nWe design an English, single speaker dataset, as research has shown that listeners\u2019 preferences on a speaker\u2019s voice has significant effect on their perceived speech quality [20]. Regarding the synthetic dataset\u2019s voice, we have selected LJ Speech [21], a public domain speech dataset with satisfactory quality, which is traditionally used for training benchmark systems in the TTS community. The speech dataset consists of 20,100 utterances of 201 systems (200 TTS systems described in Section 2.1 and the natural LJ Speech) and 2,000 unique sentences (Section 2.2).\\n\\n2.1. Systems\\nIn order to synthesize speech utterances for our dataset, where the focus is on speech naturalness based on voice prosody rather than vocoding quality, we have used the same vocoder, LPC-Net [22, 23] trained on LJ Speech, with all the TTS acoustic models. Our goal was not to include all existing acoustic models, but rather induce prosodic variability, thus we chose Tacotron-based models, which were familiar to us and suitable for producing multiple variations. The prosodically diverse synthetic utterances were generated by implicitly or explicitly varying the models\u2019 prosodic parameters, such as intonation, stress, rhythm and pauses. In total, we have included 200 different neural TTS \u201csystems\u201d. We define a \u201csystem\u201d as a neural TTS model with a fixed configuration of prosodic or style parameters. Thus, from a single TTS model that offers prosody or style manipulation capabilities, many \u201csystems\u201d can be derived, by using a distinct configuration of parameters for each one.\\n\\n2.2. Sentences\\nWe opted to create an inference corpus with a variety of linguistic contents for synthesis, so that the models that would be trained on the generated speech corpus would be able to generalize over it. The corpus comprises 2,000 sentences, out of which, 100 were randomly selected from the LJ Speech script and excluded from the LJ Speech training dataset. The remainder of the corpus is composed in such a way to ensure domain, length and phoneme coverage. English sentences from the Blizzard Challenges of years 2007-2016 were selected, mainly from the conversational, news, reportorial and novel domains, as well as 100 semantically unpredictable sentences (SUS). Additionally, Wikipedia and \u201cgeneral\u201d public domain sentences from the web, which are common for conversational agents, have been included. The length of the corpus\u2019 sentences ranges from 3 to 38 words, with a median of 10 words. More details can be found in Table 1. In our speech dataset, each system utters 100 sentences and each sentence is uttered by 10 distinct systems, except for the LJ Speech sentences which are uttered by 10 TTS systems and the natural LJ Speech voice.\\n\\n3. Mean Opinion Scores\u2019 Dataset\\nWe have run a large-scale MOS test and gathered 395,318 crowdsourced ratings for our speech dataset, of which 359,380 concern the synthetic utterances. To our knowledge, this is the largest open subjective evaluations\u2019 dataset exclusively for neural TTS models.\\n\\nVariant samples and info about the dataset\u2019s release can be found at: https://innoetics.github.io/publications/somos-dataset/index.html\"}"}
{"id": "maniati22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Listening Test Design\\nAll subjective evaluations were conducted online with crowd-sourced naive listeners via Amazon Mechanical Turk (AMT) [34]. Listeners were asked to evaluate the naturalness of each sample on a 5-point Likert scale, while listening through headphones and being in a quiet setting. The task was \u201cRate how natural each sample sounds in a scale from 1 [very unnatural] to 5 [completely natural]\u201d and the scale options were: \u201c1: very unnatural\u201d, \u201c2: somewhat unnatural\u201d, \u201c3: neither natural nor unnatural\u201d, \u201c4: somewhat natural\u201d, \u201c5: completely natural\u201d. To control for potential spurious participants, in each test page we included: (a) a ground truth sample from the LJ Speech recordings, (b) a validation sample, where listeners were instructed by the synthetic voice to select a given response from 1 to 5, and (c) a reCAPTCHA one-box tick, to deter malicious spam workers from participating. Each test page, or Human Intelligence Task (HIT) in AMT terms, comprised 22 audio samples, of which 20 were synthetic of distinct systems and distinct sentences, 1 was ground truth, and 1 was validation. All audio of the page had to be played at least once for the worker to be able to submit the page. The test samples were structured in such a way that in 10 successive test pages all 200 systems were included.\\n\\nWith regards to the workers\u2019 payment, we have considered the required time per HIT completion and ensured compliance with the minimum hourly wage per locale.\\n\\nThree tests were published simultaneously, each in a distinct English locale; United States (US), United Kingdom (GB) and Canada (CA). This choice was made as we wanted our results to generalize to a varied population, as well as to allow for the identification of patterns in certain locales. We also attempted to recruit Australian workers. However, we were unable to collect a sufficient number of scores from the AU locale. Only residents of the respective locale, according to AMT\u2019s qualification, were recruited in each test, with a past HIT approval rate of at least 90%. We required only native English speakers to participate, but as this requirement could not be enforced, we have included a native/non-native checkbox in each test page. Every audio sample was evaluated by at least 17 unique participants (up to 23), of which at least 10 were US, 5 GB and 2 CA. In total, 987 workers participated in the evaluation. The US test was completed in 4 hours by 785 workers; the GB test attracted 151 workers and took 3 weeks to complete; in CA, 51 listeners participated in the course of 2 weeks.\\n\\n3.2. Quality of Ratings\\nAlthough crowdsourcing is a quick way to collect a large number of annotations for language tasks, it has been shown that workers are not always reliable [35]. This is even more prominent for speech naturalness ratings, where the annotations consist of subjective opinions, and there is not one universally accepted answer. To mitigate this problem, we have followed the design recommendations in [36], by inserting explicitly verifiable validation questions, and in [37], by inputting gold standard data (natural speech) in each test page. Additionally, by enabling page submission only if all audio on the page has been played, we have constrained users to spend on each page the same amount of time that would be required to complete the task honestly, which has been demonstrated to improve quality of crowdsourced responses [36]. Specifically for our task, through manual review of a small number of HITs, we have identified certain patterns that can be combined to indicate potentially unreliable responses: first, pages where all scores are the same, excluding that of the validation utterance, even more so since all samples have been generated from different systems as per the test\u2019s design; second, pages where the average synthetic score is higher or similar (smaller by 0.1) to the natural sample. These can be combined with the miss of the validation utterance score and the extremely low scores (1 or 2) given on natural utterances, in order to filter out suspicious HITs and create a \u201cclean\u201d subset of the dataset. The percentages of HITs per locale falling in any of the above quality control categories are displayed in Table 2. They do not add up to 100 as the controls are not mutually exclusive. As can be observed, the US locale bears the larger portion of suspicious HITs, with half of the collected data being potentially unreliable. On the other hand, CA workers\u2019 consistency is remarkable, with almost no HITs falling into 2 out of 4 control categories. We hence refer to SOMOS-clean to allude to the subset of the dataset that does not abide to any of the suspicious patterns, as opposed to SOMOS-full, the dataset comprising all crowdsourced scores. The distributions of scores for the two are illustrated in Figure 1.\\n\\nTable 2: Quality controls on crowdsourced ratings per locale\\n\\n| Control           | US   | GB   | CA   | All  |\\n|-------------------|------|------|------|------|\\n| Wrong validation  | 12.0 | 2.8  | 0.2  | 7.8  |\\n| Low natural      | 10.8 | 10.4 | 2.4  | 9.7  |\\n| Same scores      | 2.5  | 6.3  | 0.0  | 3.4  |\\n| Similar/higher  | 45.5 | 32.0 | 5.7  | 36.4 |\\n| None of above    | 49.0 | 66.2 | 93.5 | 59.8 |\\n\\nWe have also collected the responses of an experienced US native linguist for \u2248100 successive pages of the test, comprising all 200 TTS systems 10 times. As expected, 100% of their HITs do not fall in any suspicious category. Although the expert\u2019s scores average lower than crowdsourced scores, a large system-level positive relation is observed with all locales\u2019 results, using both the Pearson (PCC) and the Spearman rank correlation coefficient (SRCC). This suggests that the test design has led to consistent answers that match the expert judgements. The locale with strongest correlation to expert ratings is GB (SRCC at $r = 0.88$, PCC at $r = 0.84$), while the weakest correlation is observed in US (SRCC at $r = 0.85$, PCC at $r = 0.81$). Interestingly, the correlation to the expert gets stronger for the entire dataset (SRCC at $r = 0.90$) suggesting the importance of combining ratings of several workers, as also showcased in [38, 39].\\n\\n3.3. Results\\nA histogram of the raw ratings for all 200 systems and natural speech in SOMOS-clean, arranged from lowest to highest average MOS score per system, can be seen in Figure 2. The darker colour concentration corresponds to more assembled ratings in the respective MOS choice for the respective system. The white dots depict the average MOS score per system.\\n\\nIn order to determine the inherent correlation among listeners, we have used the bootstrap method [12,40], in SOMOS-full and SOMOS-clean separately. For each of 1000 iterations, we randomly select and exclude half of the listeners, and calculate\"}"}
{"id": "maniati22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Raw MOS scores and average score per system for SOMOS-clean dataset\\n\\nThe linear correlation coefficient (LCC), Spearman's rank correlation coefficient (SRCC) and mean square error (MSE) on utterance-level and system-level average scores, between the resulting partial dataset and the whole dataset. The mean metrics over 1000 iterations are presented in Table 3.\\n\\n### Table 3: Inherent correlations of human evaluations for SOMOS-full and SOMOS-clean at utterance and system level\\n\\n|                | Utterance-level | System-level |\\n|----------------|-----------------|--------------|\\n| SOMOS-full     |                 |              |\\n| MSE            | 0.091           | 0.004        |\\n| LCC            | 0.787           | 0.987        |\\n| SRCC           | 0.780           | 0.983        |\\n| SOMOS-clean    |                 |              |\\n| MSE            | 0.157           | 0.006        |\\n| LCC            | 0.822           | 0.991        |\\n| SRCC           | 0.822           | 0.988        |\\n\\n### 4. MOS Prediction Baselines\\n\\nAs there has been significant research on neural MOS prediction, we assess the performance of 3 SOTA models, namely MOSNet [12], LDNet [41] and SSL-MOS [42], on our dataset. A preliminary split of SOMOS-full/clean into training (85%) and validation (15%) sets is used for these experiments.\\n\\nIn order to cover a variety of TTS systems, the validation set contains all utterances of 13 systems unseen with respect to the training set as well as utterances of 19 seen systems. The MOS prediction models are trained from scratch on the preliminary split and their performance is evaluated by calculating the MSE, LCC and SRCC. Also, openly available checkpoints for each model pretrained in different datasets are evaluated on the SOMOS-clean validation set. Results can be viewed in Table 4.\\n\\nThe clean version of the dataset yields better performance on all cases. As expected, pretrained models on converted and synthetic speech of older technologies perform poorly on solely TTS data. VC systems convert the speaker identity of a given utterance while maintaining its linguistic content and prosody, while older TTS systems present artifacts and discontinuities. As human evaluators presented with such samples may often attribute naturalness to vocoding quality, these data are not suitable for learning to predict the MOS of prosodically variant synthetic utterances. On the model side, SSL-MOS has the best overall performance, even on its pretrained version, confirming its superior generalization capability compared to other models.\\n\\nThe high system-level performances observed can be attributed to the evaluation setup, as the validation set contains systems which are seen during training and the unseen systems have similar model variants in the training set. To confirm this hypothesis, we have experimented with subtracting an entire model's variants (41 systems) from training, leading to LDNet system-level performance of SRCC dropping to 0.73.\\n\\nThe clean version of the dataset yields better performance on all cases. As expected, pretrained models on converted and synthetic speech of older technologies perform poorly on solely TTS data. VC systems convert the speaker identity of a given utterance while maintaining its linguistic content and prosody, while older TTS systems present artifacts and discontinuities. As human evaluators presented with such samples may often attribute naturalness to vocoding quality, these data are not suitable for learning to predict the MOS of prosodically variant synthetic utterances. On the model side, SSL-MOS has the best overall performance, even on its pretrained version, confirming its superior generalization capability compared to other models.\\n\\nThe high system-level performances observed can be attributed to the evaluation setup, as the validation set contains systems which are seen during training and the unseen systems have similar model variants in the training set. To confirm this hypothesis, we have experimented with subtracting an entire model's variants (41 systems) from training, leading to LDNet system-level performance of SRCC dropping to 0.73.\\n\\nWith the dataset's release, we provide a carefully designed train-validation-test split (70%-15%-15%) with unseen systems, listeners and texts, which can be used for further experimentation.\\n\\n### Table 4: Utterance-level and system-level prediction results of baseline MOS prediction models on SOMOS validation set\\n\\n| Model                  | Utterance-level | System-level |\\n|------------------------|-----------------|--------------|\\n|                        | MSE (trainset)  | LCC | SRCC | MSE | LCC | SRCC |\\n| MOSNet (VCC2018)       | 0.843           | -0.075 | -0.084 | 0.167 | -0.390 | -0.418 |\\n| MOSNet (SOMOS-full)    | 0.598           | 0.218 | 0.238 | 0.035 | 0.758 | 0.839 |\\n| MOSNet (SOMOS-clean)   | 0.729           | 0.352 | 0.347 | 0.123 | 0.815 | 0.816 |\\n| LDNet (BVCC)           | 1.011           | 0.040 | 0.032 | 0.221 | 0.369 | 0.354 |\\n| LDNet (SOMOS-full)     | 0.581           | 0.262 | 0.275 | 0.027 | 0.850 | 0.854 |\\n| LDNet (SOMOS-clean)    | 0.642           | 0.397 | 0.386 | 0.034 | 0.950 | 0.905 |\\n| SSL-MOS (BVCC)         | 2.217           | 0.229 | 0.230 | 1.597 | 0.792 | 0.805 |\\n| SSL-MOS (SOMOS-full)   | 0.564           | 0.296 | 0.313 | 0.016 | 0.846 | 0.893 |\\n| SSL-MOS (SOMOS-clean)  | 0.625           | 0.453 | 0.444 | 0.041 | 0.977 | 0.947 |\\n\\nThe high system-level performances observed can be attributed to the evaluation setup, as the validation set contains systems which are seen during training and the unseen systems have similar model variants in the training set. To confirm this hypothesis, we have experimented with subtracting an entire model's variants (41 systems) from training, leading to LDNet system-level performance of SRCC dropping to 0.73. An important finding is that while system-level correlation scores are high, utterance-level scores are consistently very low, even when the systems are seen, suggesting that the plain frame-level acoustic information that these models process is not adequate to accurately evaluate a synthetic utterance. Such low scores present a limitation of current MOS prediction models which is crucial for speech synthesis. Utterance-level evaluations are most important for TTS, as the \\\"holy grail\\\" of modern speech synthesis is the prosodic pertinence of a given synthetic utterance to its content, and current obstacles towards naturalness are prosodic inconsistencies, such as wrong stress or intonation patterns.\\n\\n### 5. Conclusions\\n\\nWe have presented the SOMOS dataset, a dataset of TTS-synthesized speech and crowdsourced MOS naturalness evaluations. We have elaborated on the speech dataset's design, comprising 200 systems derived from Tacotron-based models suitable for prosodic variation and 2,000 sentences with domain coverage. We have gathered a large-scale dataset of MOS evaluations via crowdsourcing in 3 English AMT locales, and have reported in detail our test design as well as the measures taken to ensure the reliability of crowdsourced data. Finally, we have trained and tested state-of-the-art MOS prediction models on our dataset and have demonstrated the challenges they face in utterance-level evaluation as regards synthesized utterances with prosodic variability. In future work, our goal is to utilize this dataset so as to develop models which correlate better to human evaluators on the utterance-level TTS assessment task.\\n\\n### 6. Acknowledgements\\n\\nWe thank our colleagues Georgios Vardaxoglou at Innoetics, Patrick Hegarty and Srinivas Ponakala at Samsung Research America who provided insight and expertise of great assistance.\\n\\n2391\"}"}
{"id": "maniati22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. W. Black and K. Tokuda, \u201cThe Blizzard Challenge - 2005: Evaluating Corpus-Based Speech Synthesis on Common Datasets,\u201d in Proc. Interspeech, 2005.\\n\\n[2] T. H. Falk, S. M\u00f6ller, V. Karaiskos, and S. King, \u201cImproving instrumental quality prediction performance for the Blizzard Challenge,\u201d in Proc. Blizzard Challenge Workshop, 2008.\\n\\n[3] F. Hinterleitner, S. M\u00f6ller, T. H. Falk, and T. Polzehl, \u201cComparison of Approaches for Instrumentally Predicting the Quality of Text-to-Speech Systems: Data from Blizzard Challenges 2008 and 2009,\u201d in Proc. Blizzard Challenge Workshop, 2010.\\n\\n[4] C. R. Norrenbrock, F. Hinterleitner, U. Heute, and S. M\u00f6ller, \u201cTowards Perceptual Quality Modeling of Synthesized Audiobooks \u2013 Blizzard Challenge 2012,\u201d in Proc. Blizzard Challenge Workshop, 2012.\\n\\n[5] T. Yoshimura, G. E. Henter, O. Watts, M. Wester, J. Yamagishi, and K. Tokuda, \u201cA Hierarchical Predictor of Synthetic Speech Naturalness Using Neural Networks,\u201d in Proc. Interspeech, 2016.\\n\\n[6] B. Patton, Y. Agiomyrgiannakis, M. Terry, K. Wilson, R. A. Saurous, and D. Sculley, \u201cAutoMOS: Learning a non-intrusive assessor of naturalness-of-speech,\u201d in Proc. NIPS, 2016.\\n\\n[7] S.-W. Fu, Y. Tsao, H.-T. Hwang, and H.-m. Wang, \u201cQuality-Net: An End-to-End Non-intrusive Speech Quality Assessment Model Based on BLSTM,\u201d in Proc. Interspeech, 2018.\\n\\n[8] X. Zhou, Z.-H. Ling, and S. King, \u201cThe Blizzard Challenge 2020,\u201d in Proc. Joint Workshop for the Blizzard Challenge and Voice Conversion Challenge, 2020.\\n\\n[9] Z.-H. Ling, X. Zhou, and S. King, \u201cThe Blizzard Challenge 2021,\u201d in Proc. Blizzard Challenge Workshop, 2021.\\n\\n[10] T. Ho\u00dffeld, P. E. Heegaard, M. Varela, and S. M\u00f6ller, \u201cQoE beyond the MOS: an in-depth look at QoE via better metrics and their relation to MOS,\u201d Quality and User Experience, 2016.\\n\\n[11] J. Lorenzo-Trueba, J. Yamagishi, T. Toda, D. Saito, F. Villavicencio, T. Kinnunen, and Z. Ling, \u201cThe Voice Conversion Challenge 2018: Promoting development of parallel and nonparallel methods,\u201d in Proc. Odyssey, 2018.\\n\\n[12] C.-C. Lo, S.-W. Fu, W.-C. Huang, X. Wang, J. Yamagishi, Y. Tsao, and H.-M. Wang, \u201cMOSNet: Deep Learning-Based Objective Assessment for Voice Conversion,\u201d in Proc. Interspeech, 2019.\\n\\n[13] Y. Choi, Y. Jung, and H. Kim, \u201cDeep MOS Predictor for Synthetic Speech Using Cluster-Based Modeling,\u201d in Proc. Interspeech, 2020.\\n\\n[14] G. Mittag and S. M\u00f6ller, \u201cDeep Learning Based Assessment of Synthetic Speech Naturalness,\u201d in Proc. Interspeech, 2020.\\n\\n[15] Y. Leng, X. Tan, S. Zhao, F. Soong, X.-Y. Li, and T. Qin, \u201cMB-Net: MOS Prediction for Synthesized Speech with Mean-Bias Network,\u201d in Proc. ICASSP, 2021.\\n\\n[16] J. Williams, J. Rownicka, P. Oplustil, and S. King, \u201cComparison of speech representations for automatic quality estimation in multi-speaker text-to-speech synthesis,\u201d in Proc. Odyssey, 2020.\\n\\n[17] M. Todisco, X. Wang, V. Vestman, M. Sahidullah, H. Delgado, A. Nautsch, J. Yamagishi, N. Evans, T. Kinnunen, and K. A. Lee, \u201cASVspoof 2019: Future horizons in spoofed and fake audio detection,\u201d in Proc. Interspeech, 2019.\\n\\n[18] E. Cooper and J. Yamagishi, \u201cHow do voices from past speech synthesis challenges compare today?\u201d in Proc. SSW, 2021.\\n\\n[19] W.-C. Huang, E. Cooper, Y. Tsao, H.-M. Wang, T. Toda, and J. Yamagishi, \u201cThe Voicemos Challenge 2022,\u201d arXiv preprint arXiv:2203.11389, 2022.\\n\\n[20] F. Hinterleitner, C. Manolaina, and S. M\u00f6ller, \u201cInfluence of a voice on the quality of synthesized speech,\u201d in Proc. QoMEX, 2014.\\n\\n[21] K. Ito and L. Johnson, \u201cThe LJ Speech Dataset,\u201d https://keithito.com/LJ-Speech-Dataset/, 2017.\\n\\n[22] J.-M. Valin and J. Skoglund, \u201cLPCNet: Improving neural speech synthesis through linear prediction,\u201d in Proc. ICASSP, 2019.\\n\\n[23] R. Vipperla, S. Park, K. Choo, S. Ishtiaq, K. Min, S. Bhattacharya, A. Mehrotra, A. G. C. P. Ramos, and N. D. Lane, \u201cBunched lpc-net: Vocoder for low-cost neural text-to-speech systems,\u201d in Proc. Interspeech, 2020.\\n\\n[24] N. Ellinas, G. Vamvoukakis, K. Markopoulos, A. Chalamandaris, G. Maniati, P. Kakoulidis, S. Raptis, J. S. Sung, H. Park, and P. Tsiakoulis, \u201cHigh quality streaming speech synthesis with low, sentence-length-independent latency,\u201d in Proc. Interspeech, 2020.\\n\\n[25] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., \u201cTacotron: Towards End-to-End Speech Synthesis,\u201d in Proc. Interspeech, 2017.\\n\\n[26] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerry-Ryan et al., \u201cNatural TTS Synthesis by Conditioning Wavenet on MEL Spectrogram Predictions,\u201d in Proc. ICASSP, 2018.\\n\\n[27] J. Shen, Y. Jia, M. Chrzanowski, Y. Zhang, I. Elias, H. Zen, and Y. Wu, \u201cNon-Attentive Tacotron: Robust and Controllable Neural TTS Synthesis Including Unsupervised Duration Modeling,\u201d arXiv preprint arXiv:2010.04301, 2020.\\n\\n[28] M. Honnibal and M. Johnson, \u201cAn Improved Non-monotonic Transition System for Dependency Parsing,\u201d in Proc. EMNLP, 2015.\\n\\n[29] K. E. A. Silverman, M. E. Beckman, J. F. Pitrelli, M. Osten-dorf, C. W. Wightman, P. J. Price, J. B. Pierrehumbert, and J. Hirschberg, \u201cTOBI: a standard for labeling English prosody,\u201d in Proc. ICSLP, 1992.\\n\\n[30] Y. Zou, S. Liu, X. Yin, H. Lin, C. Wang, H. Zhang, and Z. Ma, \u201cFine-grained prosody modeling in neural speech synthesis using ToBI representation,\u201d in Proc. Interspeech, 2021.\\n\\n[31] M. Dom\u00ednguez, P. L. Rohrer, and J. Soler-Company, \u201cPyToBI: A Toolkit for ToBI Labeling Under Python,\u201d in Proc. Interspeech, 2019.\\n\\n[32] K. Klapsas, N. Ellinas, J. S. Sung, H. Park, and S. Raptis, \u201cWord-Level Style Control for Expressive, Non-attentive Speech Synthesis,\u201d in Proc. SPECOM, 2021.\\n\\n[33] T. Raitio, R. Rasipuram, and D. Castellani, \u201cControllable neural text-to-speech synthesis using intuitive prosodic features,\u201d in Proc. Interspeech, 2020.\\n\\n[34] K. Crowston, \u201cAmazon Mechanical Turk: A Research Tool for Organizations and Information Systems Scholars,\u201d in Shaping the future of ICT research. Methods and Approaches. Springer, 2012.\\n\\n[35] R. Snow, B. O\u2019connor, D. Jurafsky, and A. Y. Ng, \u201cCheap and fast\u2013but is it good? evaluating non-expert annotations for natural language tasks,\u201d in Proc. EMNLP, 2008.\\n\\n[36] A. Kittur, E. H. Chi, and B. Suh, \u201cCrowdsourcing user studies with Mechanical Turk,\u201d in Proc. SIGCHI, 2008.\\n\\n[37] C. Callison-Burch and M. Dredze, \u201cCreating speech and language data with amazon\u2019s mechanical turk,\u201d in Proc. NAACL HLT 2010 workshop on creating speech and language data with Amazon\u2019s Mechanical Turk, 2010.\\n\\n[38] S. Novotney and C. Callison-Burch, \u201cCheap, fast and good enough: Automatic speech recognition with non-expert transcription,\u201d in Proc. NACL-HLT, 2010.\\n\\n[39] M. Marge, S. Banerjee, and A. I. Rudnicky, \u201cUsing the Amazon Mechanical Turk for transcription of spoken language,\u201d in Proc. ICASSP, 2010.\\n\\n[40] M. Bisani and H. Ney, \u201cBootstrap estimates for confidence intervals in ASR performance evaluation,\u201d in Proc. ICASSP, 2004.\\n\\n[41] W.-C. Huang, E. Cooper, J. Yamagishi, and T. Toda, \u201cLDNet: Unified Listener Dependent Modeling in MOS Prediction for Synthetic Speech,\u201d in Proc. ICASSP, 2022.\\n\\n[42] E. Cooper, W.-C. Huang, T. Toda, and J. Yamagishi, \u201cGeneralization ability of MOS prediction networks,\u201d in Proc. ICASSP, 2022.\"}"}
