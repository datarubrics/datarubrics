{"id": "li24s_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research\\n\\nSong Li, Yongbin You, Xuezhi Wang, Zhengkun Tian, Ke Ding, Guanglu Wan\\nMeituan\\n{lisong39, dingke02, wanguanglu}@meituan.com\\n\\nAbstract\\nRecently, multilingual artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic speech recognition (ASR) has also garnered significant attention, as evidenced by systems like Whisper. However, the proprietary nature of the training data has impeded researchers\u2019 efforts to study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale multilingual corpus for speech recognition research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR.\\n\\nIndex Terms: speech recognition, multilingual, corpus\\n\\n1. Introduction\\nThanks to the rapid development of deep learning, research in speech recognition has gradually shifted from hybrid systems based on Hidden Markov Models to end-to-end ASR systems entirely built on neural networks [1\u20136]. In fact, the swift progress of end-to-end ASR has also benefited from the contribution of open-source corpora, such as the commonly used LibriSpeech [7] and GigaSpeech [8] for English, as well as AISHELL [9] and WenetSpeech [10] for Chinese. These open-source corpora have facilitated research in the field of speech recognition by both academia and industry. In the multilingual domain, the Common Voice project, alongside the multilingual LibriSpeech (MLS) corpus released by Meta, has greatly promoted research in multilingual ASR. In recent times, the success of OpenAI's Whisper model has demonstrated that big data combined with large models can yield improved performance. However, Whisper has not made its training data public, hindering researchers' ability to replicate the results. The MSR-86K corpus introduced in this paper aims to bridge this gap, further advancing research in multilingual ASR.\\n\\nExisting multilingual ASR corpora have two main shortcomings: firstly, most corpora are dominated by English and Western European languages, lacking sufficient linguistic diversity. Secondly, although some corpora have a broad coverage of languages, the duration of recordings for each language is often minimal, insufficient for building a usable ASR system. The MSR-86K corpus addresses these issues by ensuring substantial coverage of languages and providing enough data per language to independently train a robust ASR system. We constructed a series of protocols to automatically retrieve publicly accessible videos from YouTube and set up a data processing pipeline to automatically generate the MSR-86K corpus, significantly reducing the costs associated with data collection and labeling.\\n\\nTable 1: Compare MSR-86K to common public multilingual ASR corpora.\\n\\n| Corpus      | # Languages | Total Hours(Transcribed) | Domains          | Speech Type |\\n|-------------|-------------|--------------------------|------------------|-------------|\\n| BABEL [13]  | 17          | 1k                       | Conversational   | Spontaneous |\\n| Common Voice [11] | 112          | 18k                      | Open domain      | Read        |\\n| MLS [12]    | 8           | 50.5k                    | Spontaneous      |              |\\n| CVSS [20]   | 22          | 1.1k                     | Open domain      | Read/Synthetic |\\n| MSR-86K (ours) | 15          | 86.3k                    | YouTube          | Spontaneous |\\n\\nWhisper is an excellent multilingual model, but its best-performing variant has a large number of parameters, which results in slower inference speed and greater memory overhead. In this paper, we introduce how to use easily accessible unsupervised data for pre-training and fine-tuning with MSR-86K and other open-source corpora to build a robust multilingual ASR model that is faster, smaller in size, and has performance that matches or even exceeds that of the Whisper large model.\\n\\nThe rest of the paper is organized as follows. In Section 2, the process of constructing the MSR-86K corpus is described. In Section 3, we introduce our experiments and discussions. Finally, the paper is concluded in Section 4.\\n\\n2. Corpus Construction\\nThis section describes the major steps involved in creating the MSR-86K corpus, and Figure 1 illustrates this process.\\n\\n2.1. Data collection\\nCreating keyword lists.\\nFirst, we start by generating a preliminary list of keywords through querying Wikipedia articles in the target language. Recognizing the presence of numerous non-target language terms within these entries, we then implement a keyword filtering module to refine our list. The module selectively filters and retains terms that are likely to be significant keywords, ensuring relevance in the target language.\\n\\nRetrieving video IDs.\\nNext, we use the YouTube search engine to search the keyword list, obtaining a list of video IDs. Since different keywords may lead to the same videos, it is necessary to deduplicate the video ID list. We hope to share the dataset, so we further filter out videos that are available for public download, and remove private, paid, and restricted videos.\\n\\nDetecting video subtitles.\\nIn order to guarantee the quality of the transcriptions, we ensure that each video has at least one video subtitle track. This is necessary because not all videos on YouTube have video subtitles.\\n\\nTranscription.\\nAfter downloading the videos, we transcribe the video subtitles into text. We use multiple transcription tools to ensure accuracy.\\n\\n4. Conclusion\\nIn this paper, we introduced MSR-86K, an evolving, large-scale multilingual corpus for speech recognition research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduced how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR.\"}"}
{"id": "li24s_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ity of our corpora annotations to the greatest extent, we implement a subtitle detection process for videos, filtering out those that feature manually uploaded subtitles. The rest of the videos that lack subtitles are relegated to function as unsupervised data sources, utilizing solely their audio components.\\n\\nDownloading audio and subtitles.\\n\\nWe download the audio tracks of videos and their corresponding manually uploaded subtitles through the YouTube download engine as the primary data source for MSR-86K. Additionally, we download the audio from some videos without subtitles to serve as the data source for unsupervised pre-training. Each audio file is converted into a single-channel wav format, sampled at a 16 kHz rate.\\n\\n2.2. ASR corpus construction\\n\\nText normalization.\\n\\nVideo subtitles contain several non-semantic symbols. To streamline further processing, we need to normalize the text. This involves transforming the case, removing punctuation and emojis, converting numbers, and eliminating special symbols associated with specific languages.\\n\\nForced alignment.\\n\\nEven though video subtitles come with timestamps, we often notice a lot of them aren't accurate, thus necessitating a re-alignment of the audio with the subtitles. Thanks to the work of predecessors, we use a pre-trained ASR model based on the connectionist temporal classification (CTC) criterion for alignment, and take the median of the alignment scores as the cutoff for filtering.\\n\\nFigure 2: The description of duration balance.\\n\\nDuration balance.\\n\\nDue to memory constraints, subtitles are usually segmented for forced alignment. However, each segment does not necessarily correspond to the exact endpoint of the speaker's utterance, resulting in a relatively short distribution of audio duration. To balance the audio duration and ensure the integrity of the speech content as much as possible, we conducted voice activity detection (VAD) based on the output of the CTC model, and limited the maximum duration to 20 seconds.\\n\\nFigure 2 shows the duration statistics before and after VAD.\\n\\nLID filter.\\n\\nAfter reviewing the outcomes of forced alignment, we noticed that there were still some inaccuracies. The most common issues included mismatched languages between the audio and the subtitles, subtitles that were categorized as descriptive captions, and audio that was either purely music or completely silent. Consequently, we develop a language identification (LID) model that effectively filters out sentences where discrepancies exist between the audio and the subtitles, significantly improving the quality of the data.\\n\\nASR filter.\\n\\nTo further improve data quality, we train an ASR model using both existing open-source data and the data filtered by the LID model. This ASR model is used to decode the data processed by the LID filter and calculate the word error rate (WER). By filtering out segments with higher WER, we ensure greater accuracy in our dataset annotations.\\n\\nTable 2: Duration of different languages in the MSR-86K corpus and the results of the Monolingual ASR on the dev set.\\n\\n| Language  | Duration (hrs) | Monolingual train | Monolingual dev | WER/CER (%) |\\n|-----------|----------------|-------------------|-----------------|-------------|\\n| Spanish   | 13976.84       | 18.63             | 6.36            |\\n| Korean    | 10338.66       | 18.56             | 4.79            |\\n| English   | 9795.46        | 17.42             | 5.61            |\\n| French    | 8316.70        | 15.84             | 8.43            |\\n| German    | 6862.00        | 14.38             | 6.39            |\\n| Hindi     | 5986.50        | 11.62             | 9.90            |\\n| Vietnamese| 5957.47        | 11.54             | 3.51            |\\n| Italian   | 5691.28        | 11.40             | 5.18            |\\n| Dutch     | 4138.50        | 9.67              | 5.99            |\\n| Portuguese| 3737.54        | 9.62              | 7.43            |\\n| Thai      | 3674.70        | 9.47              | 4.17            |\\n| Russian   | 3188.52        | 9.35              | 8.90            |\\n| Indonesian| 1982.87        | 9.12              | 6.75            |\\n| Japanese  | 1779.03        | 8.54              | 3.44            |\\n| Arabic    | 873.84         | 4.95              | 9.40            |\\n| Total/Avg| 86299.91       | 180.11            | 6.42            |\\n\\nData split.\\n\\nBased on the forced alignment scores, LID scores, and WER, we select a portion of the data with the highest quality to serve as the development set, while the remaining data is allocated for the training set. The distribution of durations across different languages is detailed in Table 2. For the test set, we use the test portion of the Common Voice corpus, which has undergone stringent manual verification to ensure the high quality required for multilingual ASR testing.\"}"}
{"id": "li24s_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3. Unsupervised corpus construction\\n\\nFor audio without subtitles, we employ a sound event detection model to filter out music and noise, and segment the audio at points of silence into clips shorter than 30 seconds. Ultimately, we obtain a total of 200k hours of unsupervised data.\\n\\n3. Experiments and Discussions\\n\\nIn this section, we first introduce the evaluation of the MSR-86K corpus, assessing the overall quality of the corpus. Secondly, we describe how to use unsupervised data for pre-training, and then fine-tune with MSR-86K and other open-source data to obtain a non-autoregressive multilingual speech recognition model that outperforms the whisper large model.\\n\\n3.1. Data evaluation\\n\\nTo evaluate the quality of the MSR-86K corpus, we trained a monolingual model using the training set for each language. Then, we performed Beam Search decoding on the MSR-86K development set and calculated the word error rate and character error rate (CER). Our evaluation model utilizes the Transformer-CTC architecture, in which $d_{\\\\text{model}}=768$, $d_{\\\\text{ffn}}=3072$, $d_{\\\\text{head}}=12$, $\\\\text{num layers}=12$. In addition, a convolutional front-end was used to sub-sample the acoustic features by a factor of 6. Moreover, each language is equipped with its own respective vocabulary, which employs a byte-level byte-pair encoding (BPE) model with a vocabulary size of 2000.\\n\\nAs shown in Table 2, the monolingual ASR models for 15 languages all achieved a WER or CER below 10% on their respective development sets, with some languages reaching below 5%, and an average error rate of 6.42% across all languages. Considering that our evaluation model does not employ state-of-the-art ASR models and given the spontaneous nature of YouTube audio, an overall error rate of 6.42% meets our expectations, indicating that the data quality has reached a relatively ideal level. Therefore, in practice, the development set of MSR-86K can serve as a multilingual test set of the YouTube domain for other open-source corpora.\\n\\n3.2. Multilingual ASR construction\\n\\nWhisper is an excellent multilingual model that performs well across mainstream languages around the world. However, Whisper has not made its training data public, making it difficult for researchers to replicate its results. Our MSR-86K corpus effectively bridges this gap and can facilitate researchers' studies on large-scale multilingual speech recognition. Additionally, the best-performing model of Whisper has a high parameter count of up to 1.55 billion, which results in slower inference speed and also requires more memory and computational resources. In this section, we explain how to leverage easily accessible unsupervised data for pre-training, and then fine-tune with the MSR-86K and other existing multilingual open-source corpora to develop a multilingual ASR model that has a smaller parameter size, faster speed, and performance that is comparable to or even surpasses that of Whisper. The workflow of our multilingual ASR model training is illustrated in Figure 3.\\n\\nData preparation.\\n\\nWhisper (v2) was trained using 680k hours of annotated data, while Whisper larger-v3 has reached a scale of 5 million hours, which is daunting for the average researcher. As illustrated in Table 3, we employed our contributed MSR-86K and various other open-source multilingual corpora for our transcribed data. In addition, to reduce the model's dependency on transcribed data, we explored unsupervised pre-training methods. By leveraging the data listed in Table 3 and incorporating the unsupervised data detailed in Section 2.3, we amassed a comprehensive corpus of 400k hours.\\n\\nTable 3: Summary of multilingual open-source corpora for our experiments.\\n\\n| Language | Corpus                  | Total Hours   |\\n|----------|-------------------------|---------------|\\n| English  | Librispeech [7], mTEDx [18], Gigaspeech [8], MLS [12], The People's Speech [22], CommonVoice [11], MSR-86K | 78180.96 |\\n| Chinese  | AISHELL-1 [9], AISHELL-2 [23], Thchs30 [24], Aidatatang200zh [25], Primewords [26], TALASR [27], TALCSASR [28], WenetSpeech [10], CommonVoice | 12227.58 |\\n| Spanish  | mTEDx, CommonVoice, MLS, MSR-86K | 15507.07 |\\n| Korean   | CommonVoice, Zeroth Korean [29], MSR-86K | 10418.17 |\\n| French   | CommonVoice, MLS, MSR-86K | 10125.3 |\\n| German  | Bundestag [30], MLS, CommonVoice, MSR-86K | 10278.88 |\\n| Hindi    | CommonVoice, MSR-86K | 5994.73 |\\n| Vietnamese | CommonVoice, MSR-86K | 5960.94 |\\n| Italian  | CommonVoice, MSR-86K, MLS | 6183.40 |\\n| Dutch    | CommonVoice, MSR-86K, MLS | 5750.03 |\\n| Portuguese | CommonVoice, MSR-86K, MLS, mTEDx | 4074.15 |\\n| Thai     | CommonVoice, MSR-86K | 3726.11 |\\n| Russian  | CommonVoice, OpenSTT [32], Golos [31], MSR-86K | 8149.94 |\\n| Indonesian | CommonVoice, MSR-86K | 1994.47 |\\n| Japanese | CommonVoice, MSR-86K, JTubeSpeech [33], Reazonspeech [34] | 21774.56 |\\n| Arabic   | CommonVoice, MSR-86K, MGB2 [35], QASR [36] | 4162.44 |\\n| Indonesian | CommonVoice, MSR-86K | 1994.47 |\\n| Japanese | CommonVoice, MSR-86K, JTubeSpeech [33], Reazonspeech [34] | 21774.56 |\\n| Arabic   | CommonVoice, MSR-86K, MGB2 [35], QASR [36] | 4162.44 |\\n\\nPre-training.\\n\\nWe first conducted unsupervised pre-training with the prepared data. Given the superior performance of HuBERT [37], we chose it as the criterion for unsupervised pre-training. We used a Transformer encoder similar to the one described in Section 3.1 as the acoustic encoder, where $d_{\\\\text{model}}=1024$, $d_{\\\\text{ffn}}=4096$, $d_{\\\\text{head}}=16$, $\\\\text{num layers}=24$.\\n\\nFine-tuning.\\n\\nNext, we fine-tuned the pre-trained HuBERT model using the dataset presented in Table 3, with CTC as the training criterion. Similar to Whisper, our vocabulary is shared across all languages. We trained a byte-level BPE model with a vocabulary size of 10,000 using the texts from the corpora presented in Table 3 to establish the lexicon for CTC, to which we added an extra token to signify the blank symbol.\\n\\nLID Prompt-tuning.\\n\\nMultilingual ASR typically encounters two usage scenarios. The first scenario is where the language of the speech to be recognized is not known in advance, necessitating the model to identify it autonomously. In the second scenario, the model must automatically determine the appropriate language for the input text.\"}"}
{"id": "li24s_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Compare our multilingual ASR model and Whisper model in terms of WER/CER (%) on the open-source test set.\\n\\n| Language  | Test set       | Whisper Medium (769M) | Larger v2 (1.55B) | HuBERT-CTC (362M) without LID | with LID | with LID | with LID | with LID |\\n|-----------|----------------|-----------------------|-------------------|----------------------|--------|---------|---------|---------|\\n| English   | Librispeech test-clean | 3.5                   | 2.8                | 3.4                   | 2.6    | 2.5     | 2.3     |         |\\n|           | Librispeech test-other | 6.5                   | 6.2                | 5.3                   | 4.9    | 5.2     | 4.9     |         |\\n|           | Gigaspeech       | 13.6                  | 12.7               | 11.8                  | 10.6   | 11.8    | 10.5    |         |\\n|           | MLS              | 8.1                   | 7.0                | 7.3                   | 6.8    | 6.6     | 6.4     |         |\\n|           | TEDLIUM2         | 4.6                   | 4.5                | 4.5                   | 4.4    | 4.3     | 4.2     |         |\\n|           | CommonVoice      | 22.4                  | 13.4               | 17.6                  | 10.8   | 11.2    | 10.7    |         |\\n| Chinese   | AISHELL-1        | 7.1                   | 7.3                | 6.2                   | 6.2    | 3.2     | 2.8     |         |\\n|           | AISHELL-2 test-mic | 6.8                   | 6.1                | 5.3                   | 5.3    | 4.8     | 4.3     |         |\\n|           | THCHS-30         | 8.6                   | 8.6                | 6.8                   | 6.8    | 6.2     | 6.0     |         |\\n|           | WenetSpeech dev  | 11.7                  | 11.5               | 10.4                  | 10.4   | 7.1     | 6.5     |         |\\n|           | WenetSpeech test-meeting | 22.0              | 22.5               | 22.1                  | 21.6   | 12.7    | 10.1    |         |\\n|           | CommonVoice      | 16.1                  | 16.0               | 15.7                  | 14.2   | 10.2    | 9.8     |         |\\n| Spanish   | CommonVoice      | 9.9                   | 7.8                | 6.5                   | 6.3    | 6.3     | 5.8     |         |\\n| Korean    |                 | 7.9                   | 7.9                | 6.2                   | 6.2    | 6.0     | 5.8     |         |\\n| French    |                 | 17.9                  | 14.7               | 12.9                  | 11.6   | 10.4    | 10.1    |         |\\n| German    |                 | 10.1                  | 8.3                | 7.6                   | 6.5    | 6.4     | 8.1     | 8.6     | 7.5     | 8.1     | 8.6     | 7.5     |\\n| Hindi     |                 | 69.0                  | 47.4               | 7.5                   |        |         |         |         |\\n| Vietnamese|                 | 28.8                  | 19.2               | 3.1                   |        |         |         |         |\\n| Italian   |                 | 6.2                   | 5.8                | 4.1                   |        |         |         |         |\\n| Dutch     |                 | 8.5                   | 7.3                | 5.1                   |        |         |         |         |\\n| Portuguese|                 | 8.7                   | 7.8                | 6.1                   |        |         |         |         |\\n| Thai      |                 | 27.9                  | 21.7               | 3.0                   |        |         |         |         |\\n| Russian   |                 | 9.7                   | 9.7                | 7.3                   |        |         |         |         |\\n| Indonesian|                 | 13.7                  | 11.0               | 5.0                   |        |         |         |         |\\n| Japanese  |                 | 7.6                   | 6.9                | 3.4                   |        |         |         |         |\\n| Arabic    |                 | 31.7                  | 30.7               | 21.8                  | 21.2   | 19.8    | 15.5    |         |\\n\\nAvg 14.9 12.8 12.2 10.5 8.4 7.6\\n\\nTable 5: Comparison of WER/CER (%) on the MSR-86K development set for Whisper and our system with LID provided in advance.\\n\\n| Language  | Whisper Medium (769M) | Larger v2 (1.55B) | HuBERT-CTC (362M) |\\n|-----------|-----------------------|-------------------|-------------------|\\n| English   | 4.9                   | 4.4               | 4.0               |\\n| Spanish   | 7.4                   | 7.1               | 5.6               |\\n| Korean    | 9.6                   | 7.2               | 4.5               |\\n| French    | 9.8                   | 9.4               | 7.0               |\\n| German    | 6.3                   | 5.6               | 5.0               |\\n| Hindi     | 69.0                  | 47.4              | 7.5               |\\n| Vietnamese| 28.8                  | 19.2              | 3.1               |\\n| Italian   | 6.2                   | 5.8               | 4.1               |\\n| Dutch     | 8.5                   | 7.3               | 5.1               |\\n| Portuguese| 8.7                   | 7.8               | 6.1               |\\n| Thai      | 27.9                  | 21.7              | 3.0               |\\n| Russian   | 9.7                   | 9.7               | 7.3               |\\n| Indonesian| 13.7                  | 11.0              | 5.0               |\\n| Japanese  | 7.6                   | 6.9               | 3.4               |\\n| Arabic    | 67.4                  | 44.5              | 6.3               |\\n\\nAvg 19.0 14.3 5.1\\n\\nOn the MSR-86K development set, the language information is provided in advance, guiding the model to bolster its performance in recognizing the specified language. To enable the CTC model to accommodate both scenarios, we employed the method proposed in [38], using language identity (LID) as a prompt to enhance the recognition performance of the target language.\\n\\nNNLM Training. To further enhance the performance of the HuBERT-CTC multilingual ASR model, we trained a simple LSTM-based language model using the text from the corpora in Table 3, and employed shallow fusion for decoding. Through the four steps mentioned above, we obtained a high-performance multilingual ASR model with a total parameter size of 362M, which is substantially smaller than the Whisper larger model, making it more suitable for deployment.\\n\\n3.3. Multilingual ASR evaluation\\n\\nDue to the differences in the scale of training data, our primary benchmark for the multilingual ASR model is the Whisper larger-v2 model. We use the pipeline provided by Hugging-Face for inference, testing both models where LID is provided in advance and where LID is not provided. Most previous papers have not tested the performance of Whisper without LID, and we believe that the results without LID are also meaningful. The recognition results for all languages were subjected to text normalization prior to the calculation of WER or CER. It is important to note that for Chinese, converting from traditional to simplified characters is necessary for calculation accuracy.\\n\\nAs shown in Table 4, our multilingual ASR model outperforms the Whisper medium and larger-v2 models across all languages, regardless of whether the LID is provided in advance or not, and was trained with less transcribed data. It's worth mentioning that Whisper's performance significantly declines on the Common Voice English test set when the LID is not specified beforehand. This performance dip can be largely ascribed to erroneous LID predictions, which exacerbate the inherent error propagation found in autoregressive models, culminating in less-than-ideal outcomes. On the other hand, our model demonstrates robustness and maintains stable performance, unaffected by the presence or absence of LID information. The results in Table 5 once again demonstrate that our model surpasses Whisper on the MSR-86K development set, which is indicative of the advanced nature of our algorithms.\\n\\n4. Conclusions\\n\\nIn this paper, we introduce the MSR-86K corpus, an evolving, multilingual corpus with 86,300 hours of transcribed audio for speech recognition research. We believe that such a large-scale corpus will propel the research in multilingual speech algorithms. We also hope that more researchers will contribute to open-source data and work together to advance the development of the intelligent speech field. Additionally, we explain how to effectively leverage readily available unsupervised data, MSR-86K, and other open-source corpora to train a robust ASR model that is competitive with Whisper in terms of performance but smaller in size and faster, allowing everyone to use open-source data to train their own multilingual ASR model.\"}"}
{"id": "li24s_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\\n\\n[2] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d arXiv preprint arXiv:1211.3711, 2012.\\n\\n[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in 2016 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2016, pp. 4960\u20134964.\\n\\n[4] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang et al., \u201cGoogle usm: Scaling automatic speech recognition beyond 100 languages,\u201d arXiv preprint arXiv:2303.01037, 2023.\\n\\n[5] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28492\u201328518.\\n\\n[6] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., \u201cScaling speech technology to 1,000+ languages,\u201d arXiv preprint arXiv:2305.13516, 2023.\\n\\n[7] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[8] G. Chen, S. Chai, G. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang et al., \u201cGigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,\u201d arXiv preprint arXiv:2106.06909, 2021.\\n\\n[9] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, \u201cAishell-1: An open-source mandarin speech corpus and a speech recognition baseline,\u201d in 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech systems and assessment (O-COCOSDA). IEEE, 2017, pp. 1\u20135.\\n\\n[10] B. Zhang, H. Lv, P. Guo, Q. Shao, C. Yang, L. Xie, X. Xu, H. Bu, X. Chen, C. Zeng et al., \u201cWenetspeech: A 10000+ hours multi-domain mandarin corpus for speech recognition,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 6182\u20136186.\\n\\n[11] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d arXiv preprint arXiv:1912.06670, 2019.\\n\\n[12] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMls: A large-scale multilingual dataset for speech research,\u201d arXiv preprint arXiv:2012.03411, 2020.\\n\\n[13] M. J. Gales, K. M. Knill, A. Ragni, and S. P. Rath, \u201cSpeech recognition and keyword spotting for low-resource languages: Belief project research at cued,\u201d in Fourth International workshop on spoken language technologies for under-resourced languages (SLTU-2014). ISCA, 2014, pp. 16\u201323.\\n\\n[14] A. W. Black, \u201cCmu wilderness multilingual speech dataset,\u201d in IEEE ICASSP. IEEE, 2019, pp. 5971\u20135975.\\n\\n[15] C. Wang, A. Wu, and J. Pino, \u201cCovost 2 and massively multilingual speech-to-text translation,\u201d arXiv preprint arXiv:2007.10310, 2020.\\n\\n[16] J. Iranzo-S\u00e1nchez, J. A. Silvestre-Cerda, J. Jorge, N. Rosell\u00f3, A. Gim\u00e9nez, A. Sanchis, J. Civera, and A. Juan, \u201cEuroparl-st: A multilingual corpus for speech translation of parliamentary debates,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 8229\u20138233.\\n\\n[17] R. Cattoni, M. A. Di Gangi, L. Bentivogli, M. Negri, and M. Turchi, \u201cMust-c: A multilingual corpus for end-to-end speech translation,\u201d Computer Speech & Language, vol. 66, p. 101155, 2021.\\n\\n[18] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri, M. Turchi, D. W. Oard, and M. Post, \u201cThe multilingual tedx corpus for speech recognition and translation,\u201d arXiv preprint arXiv:2102.01757, 2021.\\n\\n[19] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d arXiv preprint arXiv:2101.00390, 2021.\\n\\n[20] Y. Jia, M. T. Ramanovich, Q. Wang, and H. Zen, \u201cCvss corpus and massively multilingual speech-to-speech translation,\u201d arXiv preprint arXiv:2201.03713, 2022.\\n\\n[21] L. K\u00fcrzinger, D. Winkelbauer, L. Li, T. Watzel, and G. Rigoll, \u201cCtc-segmentation of large corpora for german end-to-end speech recognition,\u201d in International Conference on Speech and Computer. Springer, 2020, pp. 267\u2013278.\\n\\n[22] D. Galvez, G. Diamos, J. Ciro, J. F. Ceron, K. Achorn, A. Gopi, D. Kanter, M. Lam, M. Mazumder, and V. J. Reddi, \u201cThe people\u2019s speech: A large-scale diverse english speech recognition dataset for commercial usage,\u201d arXiv preprint arXiv:2111.09344, 2021.\\n\\n[23] J. Du, X. Na, X. Liu, and H. Bu, \u201cAishell-2: Transforming mandarin asr research into industrial scale,\u201d arXiv preprint arXiv:1808.10583, 2018.\\n\\n[24] D. Wang and X. Zhang, \u201cThchs-30: A free chinese speech corpus,\u201d arXiv preprint arXiv:1512.01882, 2015.\\n\\n[25] Aidata, \u201chttps://openslr.magicdatatech.com/62/,\u201d openslr, 2019.\\n\\n[26] Y. Choi and B. Lee, \u201cPansori: Asr corpus generation from open online video contents,\u201d arXiv preprint arXiv:1812.09798, 2018.\\n\\n[27] TAL, \u201chhttps://ai.100tal.com/dataset,\u201d TAL, 2021.\\n\\n[28] C. Li, S. Deng, Y. Wang, G. Wang, Y. Gong, C. Chen, and J. Bai, \u201cTalcs: An open-source mandarin-english code-switching corpus and a speech recognition baseline,\u201d arXiv preprint arXiv:2206.13135, 2022.\\n\\n[29] W. L. Lucas Jo, \u201chttps://www.openslr.org/40/,\u201d openslr, 2019.\\n\\n[30] J. Wirth and R. Peinl, \u201cAsr bundestag: A large-scale political debate dataset in german,\u201d arXiv preprint arXiv:2302.06008, 2023.\\n\\n[31] N. Karpov, A. Denisenko, and F. Minkin, \u201cGolos: Russian dataset for speech research,\u201d arXiv preprint arXiv:2106.10161, 2021.\\n\\n[32] Slizhikova, \u201cRussian open speech to text dataset.\u201d 2021.\\n\\n[33] S. Takamichi, L. K\u00fcrzinger, T. Saeki, S. Shiota, and S. Watanabe, \u201cJtubespeech: corpus of japanese speech collected from youtube for speech recognition and speaker verification,\u201d arXiv preprint arXiv:2112.09323, 2021.\\n\\n[34] Y. Y. D. M. S. Fujimoto, \u201cReazonspeech: A free and massive corpus for japanese asr,\u201d 2016.\\n\\n[35] A. Ali, P. Bell, J. Glass, Y. Messaoui, H. Mubarak, S. Renals, and Y. Zhang, \u201cThe mgb-2 challenge: Arabic multi-dialect broadcast media recognition,\u201d in 2016 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2016, pp. 279\u2013284.\\n\\n[36] H. Mubarak, A. Hussein, S. A. Chowdhury, and A. Ali, \u201cQasr: Qcri aljazeera speech resource\u2013a large scale annotated arabic speech corpus,\u201d arXiv preprint arXiv:2106.13000, 2021.\\n\\n[37] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[38] S. Li, Y. You, X. Wang, K. Ding, and G. Wan, \u201cEnhancing multilingual speech recognition through language prompt tuning and frame-level language adapter,\u201d arXiv preprint arXiv:2309.09443, 2023.\"}"}
