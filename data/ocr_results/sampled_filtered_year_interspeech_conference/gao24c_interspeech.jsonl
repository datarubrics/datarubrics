{"id": "gao24c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech Corpus Release and Customized System Design\\n\\nMing Gao, Hang Chen, Jun Du, Xin Xu, Hongxiao Guo, Hui Bu, Jianxing Yang, Ming Li, Chin-Hui Lee\\n\\n1 University of Science and Technology of China, China\\n2 Beijing AISHELL Technology Co., Ltd., China\\n3 Beijing Polytechnic, China\\n4 Duke Kunshan University, China\\n5 Georgia Institute of Technology, USA\\n\\nbjundu@ustc.edu.cn\\n\\nAbstract\\n\\nSmart home technology has gained widespread adoption, facilitating effortless control of devices through voice commands. However, individuals with dysarthria, a motor speech disorder, face challenges due to the variability of their speech. This paper addresses the wake-up word spotting (WWS) task for dysarthric individuals, aiming to integrate them into real-world applications. To support this, we release the open-source Mandarin Dysarthria Speech Corpus (MDSC), a dataset designed for dysarthric individuals in home environments. MDSC encompasses information on age, gender, disease types, and intelligibility evaluations. Furthermore, we perform comprehensive experimental analysis on MDSC, highlighting the challenges encountered. We also develop a customized dysarthria WWS system that showcases robustness in handling intelligibility and achieving exceptional performance. MDSC will be released on https://www.aishelltech.com/AISHELL_6B.\\n\\nIndex Terms: dysarthria speech, wake-up word spotting, customized wake-up system, speech disorder\\n\\n1. Introduction\\n\\nWith the rapid advancement of artificial intelligence, voice-controlled applications have become an essential part of our lives. As the initial step towards enabling convenience, voice wake-up has received significant research attention [1\u20135]. However, the rising prevalence of voice wake-up technology poses a potential exclusionary risk for individuals with dysarthria. Dysarthria is a speech disorder characterized by impairments in articulation, fluency, volume, clarity, and pace [6, 7]. People with dysarthria due to neurological conditions like cerebral palsy and Parkinson's disease rely heavily on voice-activated technology to meet their daily needs. It is crucial to prioritize inclusivity in designing applications that include dysarthria users.\\n\\nIn recent decades, the availability of dysarthria corpora [8, 9] has catalyzed technological advances and significantly improved the robustness of speech-controlled systems in the face of dysarthria challenges. The most widely used databases are UA-Speech [10] and Torgo [11]. UA-Speech focuses on cerebral palsy patients, capturing speech data using an eight-microphone array and synchronized video recordings. Torgo focuses on cerebral palsy and amyotrophic lateral sclerosis patients, collecting acoustic and articulatory organ motion data through electromagnetic articulography. EasyCall [12] incorporates command-like terms, serving as a resource for developing Italian command recognition systems. IDEA [13] is also an Italian articulatory disorder database.\\n\\nRecent research has shifted towards collecting speech data in diverse, naturalistic environments, exemplified by the Euphonia [14], which offers a substantial amount of real-life speech data. Three reported databases for Chinese articulatory disorders: CUDYS [15] focuses on acoustic features of pronunciation and prosody. MSDM [16] targets subacute stroke patients, capturing audio and facial motion data. However, both databases have small datasets (<10 hours), limiting their use for speech recognition models. CDSD [17] records audio and video data for Chinese dysarthria speech recognition. To our knowledge, there is no dedicated Mandarin dysarthria speech database for wake-up spotting tasks.\\n\\nIn light of this, this paper proposes the first Mandarin dysarthria wake-up word corpus called Mandarin Dysarthria Speech Corpus (MDSC) and a customized WWS system to make voice-activated technologies more accessible for individuals with dysarthria. The main contributions of our study can be outlined as follows:\\n\\n- releasing the MDSC featuring 9.4 hours of recordings from 21 dysarthric speakers and 7.6 hours of corresponding control recordings from speakers with standard speech patterns.\\n- conducting comprehensive experimental analysis on the MDSC, revealing two major challenges: significant in-domain variance and limited data volume.\\n- proposing a customized system robust to intelligibility for dysarthria WWS tasks, demonstrating outstanding performance.\\n\\n2. MDSC\\n\\n2.1. Statistics\\n\\nMDSC includes 18,630 recordings totaling 17 hours, of which 10,125 are from non-dysarthric recordings (Control) totaling 7.6 hours, and 8,505 are from dysarthric recordings (Dysarthria) totaling 9.4 hours. We record utterances from 21 dysarthric (12 females, 9 males) and 25 non-dysarthric (13 females, 12 males) speakers. The participants with dysarthric speakers have the following characteristics:\\n\\n- Native Mandarin speakers;\\n- Broad age distribution (from 18 to 48) and gender balance;\"}"}
{"id": "gao24c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Diverse etiologies contribute to dysarthria, including cerebral palsy and hepatolenticular degeneration;\\nTable 1 delineates the distribution of Control and Dysarthria across training, development, and test sets, providing specifics on the duration and speakers' count. There is no overlap in speakers among the data in each subset.\\n\\nWithin the Dysarthria test set (D-test), six individuals are assigned names D1-D6 in descending order of intelligibility, ensuring a balanced representation of genders. The corresponding test data for each individual is specifically labeled as D1-test to D6-test. D1 demonstrates the highest level of intelligibility, exhibiting pronunciation nearly indistinguishable from individuals without dysarthria. Conversely, D6 shows the lowest level of intelligibility, characterized by unclear articulation, effortful pronunciation, slower speech rate, and issues with insufficient breath support. For each of these six individuals, we reserve 3 minutes of enrollment utterances (D1-enroll to D6-enroll) in advance, not included in any subset.\\n\\n2.2. Collection\\nThe recordings consist of 10 wake-up words repeated five times at varying speeds. MDSC also includes 355 non-wake-up words, encompassing fixed command words, free command words, household instructions, and other phrases. The single-person text list has 295 non-repeated sentences.\\n\\nWe transcribe the recordings using our self-developed mobile application. The recordings, sampled at 16kHz, take place in a quiet indoor environment, with the participants positioned approximately 20cm (followed [18, 19]) away from the microphone.\\n\\n2.3. Intelligibility Evaluation\\nWe employ a comprehensive evaluation framework that combines subjective and objective criteria to quantify the intelligibility of recordings from individuals with dysarthria. The first criterion is annotation accuracy, which entails the transcription of recordings by five expert annotators who compare them against the standard text in the corpus. Each annotator calculates the percentage of accurately transcribed words, and the average of these five values is used to derive the intelligibility score for each dysarthric speaker. We define the results evaluated by annotation accuracy as subjective intelligibility.\\n\\nThe second criterion is recognition accuracy. We utilize open-source ASR models (Paraformer [20] and WENET [21]) to transcribe the speech of each speaker and calculate the Word Error Rate (WER). We refer to the outcomes assessed by recognition accuracy as objective intelligibility.\\n\\n3. Description of Dysarthria WWS System\\n3.1. Baseline WWS Models\\nWe utilize the framework provided by the WEKWS toolkit [22]. The model architecture of the baseline method consists of four components, starting with a global cepstral mean and variance normalization (CMVN) [23] layer, which normalizes the input acoustic features to follow a Gaussian distribution. Next is a preprocessing module that maps the dimensionality of the input features to the desired dimension. Following that is the backbone network, where we employ a depthwise separable temporal convolutional network (DS-TCN) [24] as the backbone. At the end of the model, as described in [25], for each keyword, we add a separate binary classifier after the backbone network to handle scenarios involving multiple keywords. For the MDSC, there are ten keywords, resulting in the network having ten binary classifiers at the final stage.\\n\\nWe employ the following data augmentation techniques inspired by [26]:\\n- Spectrogram augmentation.\\n  - We use frequency and time masking for each randomly selected audio.\\n- Speed perturbation.\\n  - The audio speed is randomly changed to be faster or slower, with the ratio in the range [0.9,1.1].\\n- White noise.\\n  - We add white noise to the original audio signals with a random SNR from -15 to 15 dB.\\n\\nWe use the training set of Control (C-train) and apply data augmentation techniques before feeding it into the network. The resulting model serves as the Speaker-independent Control (SIC) WWS model.\\n\\nTo accommodate the variability in dysarthric speech, we construct a speaker-independent model that can be generalized across individuals with different speech characteristics. We use the SIC model and conduct fine-tuning using the augmented Dysarthria training set (D-train). The resulting model is called the Speaker-independent Dysarthria (SID) WWS model.\\n\\n3.2. Speaker-dependent Dysarthria WWS Model\\nAcknowledging the substantial inter-individual variability and limited data volume in dysarthria, we develop a speaker-dependent model to cater to each individual's unique speech characteristics. We employ the SID model for pre-training and perform fine-tuning using augmented personalized enrollment utterances from each individual (D1-enroll to D6-enroll). Enrollment utterances are specific speech samples reserved for each individual in the dataset, typically used in speaker-dependent training or speaker verification tasks [27, 28]. This process yields the Speaker-dependent Dysarthria (SDD) WWS model.\\n\\nWe investigate the impact of the ratio of positive and negative instances and the overall duration of enrollment utterances in the speaker-dependent WWS experiments. To commence, we designate 10 distinct wake-up words, each with varying content and collectively spanning approximately 30 seconds, as our fixed positive instances. The negative instances are randomly selected from recordings of non-wake-up word utterances by the same individuals, ensuring a proportional duration as required for each session. Subsequently, we gradually vary the number of training samples for both categories to effectively alter the duration of enrollment utterances while maintaining a constant ratio between wake-up and non-wake-up word instances.\\n\\n4. Experiments and Analysis\\n4.1. Metrics\\nFollowing the [29], the combination of False Reject Rate (FRR) and False Alarm Rate (FAR) is adopted as the criterion, which is defined as follows:\\n\\n\\\\[\\n\\\\text{Score} = \\\\frac{N_{FR}}{N_{wake}} + \\\\frac{N_{FA}}{N_{non-wake}}\\n\\\\]\\n\\nwhere \\\\(N_{wake}\\\\) and \\\\(N_{non-wake}\\\\) denote the number of samples with and without wake-up words in the evaluation set, respectively. \\\\(N_{FR}\\\\) denotes the number of samples containing the wake-up word while not recognized by the system. \\\\(N_{FA}\\\\) denotes the number of samples containing no wake words while\"}"}
{"id": "gao24c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1: The framework of speaker-dependent dysarthria WWS. The upper part displays the specific network details, where the network architecture of the Speaker-independent Control (SIC), Speaker-independent Dysarthria (SID), and Speaker-dependent Dysarthria (SDD) WWS models are identical. The lower part illustrates the relationships among these models and the overall training process. The SIC model is trained using this network architecture on the C-train dataset. Based on the SIC model, the SID model is fine-tuned with the D-train dataset. Furthermore, the SDD model specific to an individual is fine-tuned using their corresponding D-enroll dataset, building upon the foundation of the SID model.\\n\\nFigure 2: Intelligibility-score relationship for individuals with dysarthria on a conventional WWS system. The lower the score, the better the system performance.\\n\\n4.2. Analysis of the Baseline WWS Models\\n\\nDue to the disparities between the wake-up words used in commercial WWS systems and those in the MDSC, it is not feasible to directly test the wake-up performance for dysarthric individuals using commercial systems. Consequently, we test all samples of individuals with dysarthria using the SIC model to examine the performance of the conventional WWS system on dysarthric speech. Figure 2 is the intelligibility-score relationship graph. We observe that higher intelligibility scores are associated with better WWS performance, indicating a strong positive correlation between these two variables.\\n\\nMoreover, we conduct separate tests on the SIC model using the C-test and D-test to evaluate the distinctions between speech samples of individuals with and without dysarthria. We also test the SID model on the D-test. From the results of the SIC models in Table 2, we observe a performance discrepancy of the SIC model between the C-test and D-test, indicating significant speech feature differences between individuals with and without dysarthria. It is challenging to create a system applicable to dysarthric individuals solely based on existing datasets from non-dysarthric people. Additionally, comparing the experimental results of SIC and SID on the D-test, we discover a notable improvement when incorporating dysarthric speech data.\\n\\nFurthermore, as shown for SIC and SID in Figure 3, we provide the testing results for each individual (D1-D6) within the D-test subset using the SIC and SID models to assess the impact of intelligibility on model enhancement. It is apparent that individuals with moderate intelligibility, namely D3 and D4, demonstrate the most substantial relative enhancements, achieving improvements of 51% and 67%, respectively. This may be attributed to D3 and D4 representing the average speech fea-\\n\\nTable 2: Experimental results for SIC, SID and SDD models\\n\\n| Model | Test Set | FAR | FRR | Score |\\n|-------|----------|-----|-----|-------|\\n| SIC   | C-test   | 0.0148 | 0.0000 | 0.0148 |\\n| SIC   | D-test   | 0.1630 | 0.3708 | 0.5339 |\\n| SID   | D-test   | 0.1538 | 0.1875 | 0.3413 |\\n| SDD   | D-test   | 0.0555 | 0.0833 | 0.1388 |\"}"}
{"id": "gao24c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Wake-up performance of SIC, SID and SDD models on D1-D6 test sets. A lower score indicates better performance.\\n\\n... and thereby making them more representative of a wider population. However, the improvement is not general; note that for the individual with the highest intelligibility, D1, the results worsened after fine-tuning with D-train. This unexpected result can be attributed to D1's pronunciation characteristics being more different from those of the other dysarthric individuals. Therefore, fine-tuning the model on the dysarthria dataset results in a decrease in performance.\\n\\nBased on the above analysis, we can summarize several distinct characteristics of dysarthric speech:\\n\\n\u2022 Significant in-domain variance. Each dysarthric individual exhibits unique speech characteristics, leading to substantial variations in pitch, speech rate, breath patterns, and sentence boundaries.\\n\\n\u2022 Limited data volume. It is reflected in the scarcity of recruiting dysarthric individuals and the difficulty of recording. Due to the disorder's impact, it is often challenging to conduct prolonged recording sessions. Therefore, we propose a speaker-dependent dysarthria WWS model as a promising direction. Adapting the system for each speaker can ignore in-domain variance and requires only a tiny amount of speaker-specific data.\\n\\nFigure 4: (a) The performance for different positive-to-negative ratios of enrollment utterances. The x-axis represents the duration ratio of positive to negative instances, ranging from 1:0 to 1:10. (b) The performance for different durations of enrollment utterances. The x-axis represents the entire duration of enrollment utterances, ranging from 1 minute to 3 minutes.\\n\\nAfter that, we conduct tests on D1 to D6 test sets using the corresponding SDD model and record their average results in the SDD model of Table 2. Utilizing only 3-minute enrollment utterances for model customization leads to significant improvements. This demonstrates the potential application of speaker-dependent approaches. Moreover, we also give the results of the SDD model tested on D1-D6 in Figure 3. By comparing it with the SIC and SID models, it becomes apparent that the wake-up performance of the SDD model is minimally affected by intelligibility, indicating its robustness in handling various levels of intelligibility. However, despite the notable improvement, individuals with extremely low intelligibility (D6) still exhibit relatively poor results. This highlights the ongoing challenge in cases of severe dysarthria. Further investigation and adaptation techniques are required to address the specific needs of individuals with extremely low intelligibility.\\n\\nAs shown in Figure 5, we also observe failure cases. These samples demonstrate unique speech characteristics specific to individuals with dysarthria, which may challenge the system to make accurate judgments.\\n\\n5. Conclusion\\n\\nIn this paper, firstly, we construct and release the Mandarin Dysarthria Speech Corpus (MDSC), providing a valuable resource for dysarthria wake-up research in home environments. Secondly, we conduct a comprehensive experimental analysis on MDSC, introducing realistic challenges for dysarthria wake-up systems. Finally, we propose a customized dysarthria WWS system that demonstrates robustness in handling intelligibility and achieving exceptional performance. This study represents an initial exploration of dysarthric speech. We intend to delve further into language-related research for dysarthria. By doing so, our goal is to foster greater societal awareness and comprehension of dysarthria while actively working towards eradicating discrimination and prejudice against individuals with this condition.\"}"}
{"id": "gao24c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Acknowledgements\\nThis work was supported by the National Natural Science Foundation of China under Grant No.62171427.\\n\\n7. References\\n[1] M. Sun, A. Raju, G. Tucker, S. Panchapagesan, G. Fu, A. Mandal, S. Matsoukas, N. Strom, and S. Vitaladevuni, \\\"Max-pooling loss training of long short-term memory networks for small-footprint keyword spotting,\\\" in 2016 IEEE spoken language technology workshop (SLT). IEEE, 2016, pp. 474\u2013480.\\n\\n[2] C. Shan, J. Zhang, Y. Wang, and L. Xie, \\\"Attention-based end-to-end models for small-footprint keyword spotting,\\\" arXiv preprint arXiv:1803.10916, 2018.\\n\\n[3] Y. Gao, Y. Mishchenko, A. Shah, S. Matsoukas, and S. Vitaladevuni, \\\"Towards data-efficient modeling for wake word spotting,\\\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7479\u20137483.\\n\\n[4] C. Ding, J. Li, M. Zong, and B. Li, \\\"Speed-robust keyword spotting via soft self-attention on multi-scale features,\\\" in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 1104\u20131111.\\n\\n[5] S. Y. Sahai, J. Liu, T. Muniyappa, K. M. Sathyendra, A. Alexandridis, G. P. Strimel, R. McGowan, A. Rastrow, F.-J. Chang, A. Mouchtaris et al., \\\"Dual-attention neural transducers for efficient wake word spotting in speech recognition,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[6] I. Arsenic, N. J. Simic, M. P. Lazic, I. Sehovic, and B. Drljan, \\\"Characteristics of speech and voice as predictors of the quality of communication in adults with hypokinetic dysarthria,\\\" Serbian journal of experimental and clinical research, vol. 22, no. 2, pp. 157\u2013165, 2019.\\n\\n[7] K. Hux, J. Rankin-Erickson, N. Manasse, and E. Lauritzen, \\\"Accuracy of three speech recognition systems: Case study of dysarthric speech,\\\" Augmentative and Alternative Communication, vol. 16, no. 3, pp. 186\u2013196, 2000.\\n\\n[8] J. R. Deller, M. S. Liu, L. J. Ferrier, and P. Robichaud, \\\"The Whitaker database of dysarthric (cerebral palsy) speech,\\\" The Journal of the Acoustical Society of America, vol. 93, no. 6, pp. 3516\u20133518, Jun. 1993.\\n\\n[9] X. Menendez-Pidal, J. B. Polikoff, S. M. Peters, J. E. Leonzio, and H. T. Bunnell, \\\"The nemours database of dysarthric speech,\\\" in Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP'96, vol. 3. IEEE, 1996, pp. 1962\u20131965.\\n\\n[10] H. Kim, M. Hasegawa-Johnson, A. Perlman, J. Gunderson, T. Huang, K. Watkin, and S. Frame, \\\"Dysarthric Speech Database for Universal Access Research,\\\" 2008.\\n\\n[11] F. Rudzicz, A. K. Namasivayam, and T. Wolff, \\\"The TORGO database of acoustic and articulatory speech from speakers with dysarthria,\\\" Language Resources and Evaluation, vol. 46, no. 4, pp. 523\u2013541, Dec. 2012.\\n\\n[12] R. Turrisi, A. Braccia, M. Emanuele, S. Giulietti, M. Pugliatti, M. Sensi, L. Fadiga, and L. Badino, \\\"EasyCall corpus: A dysarthric speech dataset,\\\" in Interspeech 2021, Aug. 2021, pp. 41\u201345.\\n\\n[13] M. Marini, M. Vigan\u00f2, M. Corbo, M. Zettin, G. Simoncini, B. Fortori, C. d'Anna, M. Donati, and L. Fanucci, \\\"Idea: an italian dysarthric speech database,\\\" in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 1086\u20131093.\\n\\n[14] R. L. MacDonald, P.-P. Jiang, J. Cattiau, R. Heywood, R. Cave, K. Seaver, M. A. Ladewig, J. Tobin, M. P. Brenner, P. C. Nelson, J. R. Green, and K. Tomanek, \\\"Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project EuPhonia,\\\" in Interspeech 2021. ISCA, Aug. 2021, pp. 4833\u20134837.\\n\\n[15] K. H. Wong, Y. T. Yeung, E. H. Y. Chan, P. C. M. Wong, G.-A. Levow, and H. Meng, \\\"Development of a Cantonese dysarthric speech corpus,\\\" in Interspeech 2015. ISCA, Sep. 2015, pp. 329\u2013333.\\n\\n[16] J. Liu, X. Du, S. Lu, Y.-M. Zhang, H. An-ming, M. Lawrence Ng, R. Su, L. Wang, and N. Yan, \\\"Audio-video database from subacute stroke patients for dysarthric speech intelligence assessment and preliminary analysis,\\\" Biomedical Signal Processing and Control, vol. 79, p. 104161, Jan. 2023.\\n\\n[17] M. Sun, M. Gao, X. Kang, S. Wang, J. Du, D. Yao, and S.-J. Wang, \\\"CDSD: Chinese Dysarthria Speech Database,\\\" Oct. 2023.\\n\\n[18] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, \\\"Aishell-1: An open-source mandarin speech corpus and a speech recognition baseline,\\\" in 2017 20th conference of the oriental chapter of the international coordinating committee on speech databases and speech I/O systems and assessment (O-COCOSDA). IEEE, 2017, pp. 1\u20135.\\n\\n[19] J. Du, X. Na, X. Liu, and H. Bu, \\\"Aishell-2: Transforming mandarin asr research into industrial scale,\\\" arXiv preprint arXiv:1808.10583, 2018.\\n\\n[20] Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, \\\"Paraformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition,\\\" arXiv preprint arXiv:2206.08317, 2022.\\n\\n[21] Z. Yao, D. Wu, X. Wang, B. Zhang, F. Yu, C. Yang, Z. Peng, X. Chen, L. Xie, and X. Lei, \\\"Wenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit,\\\" arXiv preprint arXiv:2102.01547, 2021.\\n\\n[22] J. Wang, M. Xu, J. Hou, B. Zhang, X.-L. Zhang, L. Xie, and F. Pan, \\\"WeKws: A production first small-footprint end-to-end Keyword Spotting Toolkit,\\\" Oct. 2022.\\n\\n[23] O. M. Strand and A. Egeberg, \\\"Cepstral mean and variance normalization in the model domain,\\\" in COST278 and ISCA Tutorial and Research Workshop (ITRW) on Robustness Issues in Conversational Interaction, 2004.\\n\\n[24] A. Coucke, M. Chlieh, T. Gisselbrecht, D. Leroy, M. Poumeyrol, and T. Lavril, \\\"Efficient keyword spotting using dilated convolutions and gating,\\\" in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6351\u20136355.\\n\\n[25] J. Hou, Y. Shi, M. Ostendorf, M.-Y. Hwang, and L. Xie, \\\"Mining effective negative training samples for keyword spotting,\\\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7444\u20137448.\\n\\n[26] H. Chen, H. Zhou, J. Du, C.-H. Lee, J. Chen, S. Watanabe, S. M. Siniscalchi, O. Scharenborg, D.-Y. Liu, B.-C. Yin et al., \\\"The first multimodal information based speech processing (misp) challenge: Data, tasks, baselines and results,\\\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9266\u20139270.\\n\\n[27] L. Li, D. Wang, C. Zhang, and T. F. Zheng, \\\"Improving short utterance speaker recognition by modeling speech unit classes,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 6, pp. 1129\u20131139, 2016.\\n\\n[28] A. K. Sarkar, H. Sarma, P. Dwivedi, and Z.-H. Tan, \\\"Data augmentation enhanced speaker enrollment for text-dependent speaker verification,\\\" in 2020 3rd International Conference on Energy, Power and Environment: Towards Clean Energy Technologies. IEEE, 2021, pp. 1\u20136.\\n\\n[29] H. Wang, M. Cheng, Q. Fu, and M. Li, \\\"The DKU Post-Challenge Audio-Visual Wake Word Spotting System for the 2021 MISP Challenge: Deep Analysis,\\\" in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). Rhodes Island, Greece: IEEE, Jun. 2023, pp. 1\u20135.\"}"}
