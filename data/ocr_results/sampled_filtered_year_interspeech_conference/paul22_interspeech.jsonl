{"id": "paul22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation\\n\\nDebjyoti Paul, Yutong Pang, Szu-Jui Chen, Xuedong Zhang\\n\\nMeta Platforms, Inc.\\ndebjyotipaul@fb.com, yutongpang@fb.com, srayc@fb.com, xuedong@fb.com\\n\\nAbstract\\n\\nInverse text normalization (ITN) is used to convert the spoken form output of an automatic speech recognition (ASR) system to a written form. Traditional handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile, neural modeling approaches require quality large-scale spoken-written pair samples in the same or similar domain as the ASR system (in-domain data), to train. Both these approaches require costly and complex annotation. In this paper, we present a data augmentation technique with neural machine translation that effectively generates rich spoken-written pairs for high and low resource languages. We empirically demonstrate that ITN models (in target language) trained using our data augmentation with machine translation technique can achieve similar performance as ITN models (en) trained directly with in-domain language.\\n\\nIndex Terms:\\ninverse text normalization, data augmentation, data-driven modeling, speech recognition, machine translation\\n\\n1. Introduction\\n\\nInverse Text Normalization (ITN) is used to convert spoken form output from an automatic speech recognition (ASR) system to the corresponding written form. ITN can be challenging since multiple different spoken forms can express identical written expressions. For example, both twenty twenty (the year) and two thousand twenty (numeric) can be transcribed to \u20182020\u2019. Conversely, the same spoken form can be transcribed to two or more different written expressions depending on the context. For example, twenty twenty can be transcribed to 2020 (for the year), to 20/20 (to denote eye vision), or to 20:20 (to represent time). Such a many-to-many mapping between spoken and written forms and dependence on context makes ITN an interesting and challenging problem in speech recognition.\\n\\nThere has been a renewed interest in the deep learning community to explore data-driven approaches to ITN. A popular ITN approach is to use a set of simple hand-written rules together with a neural model that can statistically learn how and when to apply these rules [1\u20134]. But creating rules for individual languages are tedious and time consuming. We explore the techniques to distillate knowledge from large natural language models to train multi-language ITN models with minimal human interventions.\\n\\nTable 1: Examples of Spoken-Written pairs.\\n\\n| Spoken form       | Written form       |\\n|-------------------|--------------------|\\n| twenty twenty     | 2020               |\\n| twenty twenty     | 20/20              |\\n| twenty twenty     | 20:20              |\\n| twenty twenty     | 20%                |\\n| twenty twenty     | 20:20              |\\n| twenty twenty     | 20:20              |\\n\\nTable 2: Examples of generated spoken form using conventional TN system and our data augmentation system.\\n\\nTo generate training data for ITN models, a common approach is to use a text normalization (TN) system [5]. However, since the TN system only outputs one flawless spoken form per written input, it does not cover the variations of spoken forms that can be generated from a single written form. If we train a model with the over simplified spoken-written pairs, the model usually over-fits to the TN system, reflecting high accuracy for the curated entities, but the model can struggle to generalize to real-world use cases.\\n\\nHence, we make the following contributions in this paper:\\n\\n\u2022 We propose a text normalization method for English that transforms written-form texts to spoken-form texts. Unlike conventional text normalization system, our data augmentation system generates more possible variants of spoken forms; which can help make robust ITN system.\\n\\n\u2022 With the recent development and improvements of neural machine translation, we propose to use a knowledge distillation approach for internationalization of the ITN models. We apply neural machine translation on English spoken-written text pairs to generate spoken-written pairs on target languages; and it helps ITN expanding to more languages.\\n\\n2. Data Driven ITN\\n\\n2.1. Data Augmentation\\n\\nConsidering that people can speak a particular written form in multiple ways in the real world - for example, 5.0 could be verbalized as five point zero, five point o, five dot zero, etc. - we have developed a specialized data augmentation method for data-driven ITN modeling.\\n\\nUnlike a conventional written to spoken TN system, our ITN augmentation system is capable of generating diversified spoken forms by introducing almost all possible spoken variations to the written forms as shown in Table 2; it produces $22 \\\\times$ factor more variations than conventional TN.\\n\\nOur augmentation system performs a series of steps, such as, (i) extraction of ITN entities (ii) reformat and clean ITN entities, (iii) apply augmentation with rewrite rules (iv) rewrite the input sentence producing spoken forms; step-by-step procedure depicted in Figure 1. Table 2 present a few examples of input and outputs from the data augmentation system.\\n\\n| Written Text | Spoken Text from Conventional TN | Spoken Text from Data Augmentation System |\\n|--------------|--------------------------------|----------------------------------------|\\n| $123$        | one hundred twenty three dollars | one hundred twenty three dollars        |\\n| $6:15$ am    | six fifteen a.m.                | six fifteen a.m.                       |\"}"}
{"id": "paul22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2. Augmentation with Machine Translation\\n\\nRecent advances in natural language translation with large-scale neural machine translation (NMT) models have empowered us to distillate ITN knowledge from such models and use it for training ITN models supporting more languages. Spoken-written text pairs in English generated from the data augmentation system are translated to respective target languages with NMT models, as shown in Figure 1 and Table 3. We used multiple open-sourced NMT models and a home-grown NMT model to evaluate our approach. One of the challenges using NMT models for translation is unpredictable text normalization behavior, such as, conversion of spoken form source language to written text in target language and vice-versa. For example,\\n\\n\\\\begin{align*}\\nI \\\\text{ have thirty dollars} &\\\\rightarrow \\\\text{Ho 30 dollari (in Italian)}; \\\\\\\\\\n\\\\text{ hist\u00f3rica de enero} &\\\\rightarrow \\\\text{treinta y un grados (in Spanish)}.\\n\\\\end{align*}\\n\\nMoreover, NMT models are still far from generating perfect translation, and errors from translation models can propagate to ITN models. We try to eliminate these drawbacks by filtering out translated spoken-written pairs where text normalization forms are not intact and choosing NMT models with reasonable BLEU scores on target languages over ITN texts. With both the data augmentation system and the translation pipeline in place, we can generate a significant number of spoken-written pairs synthetically for training high and low resource languages.\\n\\n| Language   | Augmentation | Accuracy % |\\n|------------|--------------|------------|\\n| English    |              | 76.8       |\\n| French     |              | 76.3       |\\n| Italian    |              | 79.0       |\\n| Spanish    |              | 79.3       |\\n\\nTable 4: Accuracy performance comparison of ITN model with augmentation vs baseline.\\n\\n3. Experiments\\n\\nFrom the experimental results with human-annotated datasets, we find that models trained with synthetic data generated with NMT augmentation perform equally well on other languages, such as, French, Italian, and Spanish for numerical entities.\"}"}
