{"id": "suwanbandit23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12449\u201312460, 2020.\\n\\n[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[3] W. Chan, N. Jaitly, Q. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 4960\u20134964.\\n\\n[4] H. A. Alsayadi, I. Hegazy, Z. T. Fayed, B. Alotaibi, and A. A. Abdelhamid, \u201cDeep investigation of the recent advances in dialectal Arabic speech recognition,\u201d IEEE Access, 2022.\\n\\n[5] R. Imaizumi, R. Masumura, S. Shiota, H. Kiya et al., \u201cEnd-to-end Japanese multi-dialect speech recognition and dialect identification with multi-task learning,\u201d APSIPA Transactions on Signal and Information Processing, vol. 11, no. 1, 2022.\\n\\n[6] Z. Dan, Y. Zhao, X. Bi, L. Wu, and Q. Ji, \u201cMulti-task transformer with adaptive cross-entropy loss for multi-dialect speech recognition,\u201d Entropy, vol. 24, no. 10, 2022. [Online]. Available: https://www.mdpi.com/1099-4300/24/10/1429\\n\\n[7] E. Chuangsuwanich, Y. Zhang, and J. Glass, \u201cMultilingual data selection for training stacked bottleneck features,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5410\u20135414.\\n\\n[8] H. Inaguma, J. Cho, M. K. Baskar, T. Kawahara, and S. Watanabe, \u201cTransfer learning of language-independent end-to-end ASR with language model fusion,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6096\u20136100.\\n\\n[9] N. Hjortn\u00e6s, N. Partanen, M. Rie\u00dfler, and F. M. Tyers, \u201cThe relevance of the source language in transfer learning for ASR,\u201d in Proceedings of the Workshop on Computational Methods for Endangered Languages, vol. 1, 2021, pp. 63\u201369.\\n\\n[10] S. Khare, A. R. Mittal, A. Diwan, S. Sarawagi, P. Jyothi, and S. Bharadwaj, \u201cLow resource ASR: The surprising effectiveness of high resource transliteration,\u201d in Interspeech, 2021, pp. 1529\u20131533.\\n\\n[11] B. Pulugundla, M. K. Baskar, S. Kesiraju, E. Egorova, M. Karafi\u00e1t, L. Burget, and J. Cernock\u00fd, \u201cBUT system for low resource Indian language ASR,\u201d in Interspeech, 2018, pp. 3182\u20133186.\\n\\n[12] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and M. Auli, \u201cRobust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,\u201d in Proc. Interspeech 2021, 2021, pp. 721\u2013725.\\n\\n[13] A. Misra, D. Hwang, Z. Huo, S. Garg, N. Siddhartha, A. Narayanan, and K. C. Sim, \u201cA comparison of supervised and unsupervised pre-training of end-to-end models,\u201d in Proc. Interspeech 2021, 2021, pp. 731\u2013735.\\n\\n[14] G. Karakasidis, \u201cComparison of new curriculum criteria for end-to-end ASR,\u201d Master\u2019s thesis, Aalto University. School of Science, 2022. [Online]. Available: http://urn.fi/URN:NBN:fi:aalto-202208285156\\n\\n[15] Y. Bengio, J. Louradour, R. Collobert, and J. Weston, \u201cCurriculum learning,\u201d in Proceedings of the 26th Annual International Conference on Machine Learning, ser. ICML \u201909. New York, NY, USA: Association for Computing Machinery, 2009, p. 41\u201348. [Online]. Available: https://doi.org/10.1145/1553374.1553380\\n\\n[16] S. Braun, D. Neil, and S.-C. Liu, \u201cA curriculum learning method for improved noise robustness in automatic speech recognition,\u201d in 2017 25th European Signal Processing Conference (EUSIPCO). IEEE, 2017, pp. 548\u2013552.\\n\\n[17] T. N. Electronics, \u201cC.T.C.: Benchmark for Enhancing the Standard of Thai Language Processing 2010,\u201d 2010.\\n\\n[18] W. Phatthiyaphaibun, K. Chaovavanich, C. Polpanumas, A. Suriyawongkul, L. Lowphansirikul, and P. Chormai, \u201cPythainlp: Thai natural language processing in python,\u201d URL: http://doi.org/10.5281/zenodo.3519354, vol. 3519354, 2016.\\n\\n[19] A. Diller, J. Edmondson, and Y. Luo, The Tai-Kadai Languages. Routledge, 2004.\\n\\n[20] C. Goddard, \u201cSemantic primes and universal grammar in Malay (Bahasa Melayu),\u201d in Meaning and universal grammar: Theory and empirical findings. John Benjamins Publishing Company, 2002.\\n\\n[21] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proceedings of Interspeech, 2018, pp. 2207\u20132211.\\n\\n[22] S. Watanabe, T. Hori, S. Kim, J. R. Hershey, and T. Hayashi, \u201cHybrid ctc/attention architecture for end-to-end speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 11, no. 8, pp. 1240\u20131253, 2017.\\n\\n[23] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5206\u20135210.\"}"}
{"id": "suwanbandit23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition\\n\\nArtit Suwanbandit\\n1, Burin Naowarat 1, Orathai Sangpetch 2, Ekapol Chuangsuwanich 1\\n1 Chulalongkorn University, Thailand\\n2 CMKL university, Thailand\\n{6372130221, 6270145221} @student.chula.ac.th, orathai@cmkl.ac.th, ekapol.c@chula.ac.th\\n\\nAbstract\\nWe release 840 hours of read speech multi-dialect ASR corpora consisting of 700 hours of main Thai dialect, named Thai-central, and 40 hours for each local dialect, named Thai-dialect, with transcripts and their translations to Thai. The dialects, selected to represent different regions of Thailand, are Khummuang, Korat, and Pattani. We also release the baseline dialectal ASR models trained using the curriculum learning approach. We found that the pre-training with the high-resource main dialect and target dialect generally yields the best performance. We believe that the availability of our corpora would contribute to the problem of low-resource Thai dialects. The corpus data will be available on Github.\\n\\nIndex Terms: Speech recognition, Dialect ASR, Deep learning, Transfer-learning, curriculum learning\\n\\n1. Introduction\\nThe development of ASR has made it possible for machines to understand human speech. There are a large number of local dialects in Thailand, mainly influenced by traditions, histories, and ethnicities; such a diverse language landscape has resulted in the decreased accuracy of ASR systems for low-resource local dialects. To address this issue, we have created the Thai-central and Thai-dialect datasets in order to improve ASR systems' performance for local dialects. The Thai-central corpus consists of more than 700 hours of Thai read speech data while the Thai-dialect is a collection of dialectal speech corpora from three different regions in Thailand: Khummuang from the north, Korat from the northeast, and Pattani from the south with each dialectal speech corpus containing more than 40 hours of data. The Thai-dialect corpus is designed so that they are parallel with the Thai-central corpus creating another venue for further research such as speech-to-text and speech-to-speech translation. In this work we also investigate the use of this dataset for dialect and low-resource ASR research.\\n\\nDeep learning, particularly transformer-based models like Wav2Vec2.0 [1] and HuBERT [2], has become the standard method for ASR since the introduction of the Listen, Attend, and Spell (LAS) model [3]. Recent studies have highlighted the success of deep learning approaches in dialectal Arabic ASR [4]. However, multi-dialect systems have generally been found to perform poorly compared to single-dialect systems. For Japanese dialectal ASR, multi-task learning with dialect identification (DID) and multi-dialect ASR has been proposed, but it was found that incorrect predictions of DID tasks negatively affect ASR performance [5]. Multi-task methods with soft multi-task learning (Soft-MTL) models, such as those presented by Z. Dan, et al. [6] use an additional speech encoder to achieve promising results.\\n\\nIn our work, we observed that Thai dialectal languages often have a high degree of similarity in speech to the main dialect. Chuangsuwanich, et al. [7] demonstrated transfer learning helps the most if the target language is more similar to the source language. Transfer learning has also been shown to be a promising approach for low-resource ASR in several studies [8, 9, 10, 11]. Therefore, we aim to examine the effectiveness of transfer learning in the context of our Thai dialect dataset.\\n\\nTransfer learning is a simple yet effective approach for improving deep learning performance in low-resource settings. In the field of ASR, [12] investigated the effectiveness of self-supervised pre-training on various English datasets. Additionally, an experiment in [13] explored the impact of supervised and self-supervised pre-training on ASR performance under domain and language mismatch scenarios. The method of utilizing transfer learning in ASR has been referred to as transfer-based learning in [14]. Our work focuses on transfer-based curriculum learning for ASR in Thai-dialect datasets.\\n\\nCurriculum learning (CL) is an approach for ordering data to optimize training efficiency, inspired by the \\\"starting small\\\" concept. When used correctly, CL can reduce training steps and improve accuracy [15]. According to Gkarakasidis, et al. [14], CL can be categorized into three approaches: metadata-based, adaptive-based, and transfer-based. Their work focused on an adaptive-based approach using loss/metric-based methods. Meanwhile, S. Braun, et al. [16] demonstrated that adding noise over time can improve the robustness of ASR.\\n\\nIn our work, we explore the effectiveness of transfer-based curriculum learning for guiding supervised ASR pre-training and fine-tuning of low-resource dialect ASR using a high-resource main-dialect dataset.\\n\\n2. Thai-central and Thai-dialect corpora\\nOne part of our corpus is collected from standard Thai dialect called Thai-central. The other portion, Thai-dialect, consist of three Thai dialects, Khummuang, Korat, and Pattani. In this section, we describe the recording procedure, corpora information, and the characteristics of the dialects.\\n\\n2.1. Data collection procedure\\n2.1.1. Text prompts\\nOur corpus creation process starts by gathering text prompts which will be used to record spoken utterances from volunteers. In order to improve diversity of the prompts, sentences are obtained from seven different sources. Additional sentences were specifically also created for two special domains of inter-\\n\\n10.21437/Interspeech.2023-1828\"}"}
{"id": "suwanbandit23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"est, namely chatting for e-commerce, and daily communication sentences. We call the two domains E-commerce and Survival, respectively. The E-commerce and Survival subsets were generated using templates where nouns, verbs, product names, etc. were replaced. Due to the variety of the templates, the Survival prompts are more challenging than E-commerce. The final breakdown of our text-prompts is shown in Table 1.\\n\\nFor the Thai-central corpus, every sentence from all sources were included. However, the Thai-dialect corpora contain only the E-commerce and Survival sentences, which were translated to their respective dialectal transcriptions. We made sure that the sentences only contain Thai letters and digits, removing equations, parentheses, brackets, and special characters. In addition, numbers, special signs, and times were converted to text beforehand to ensure consistency.\\n\\nTable 1:\\n\\n| Data source                      | Data detail | #Sentence |\\n|----------------------------------|-------------|-----------|\\n| E-commerce shopping sentences    |             | 33063     |\\n| Survival daily sentences         |             | 6062      |\\n| BEST2010 [17] articles, novels,  |             | 7113      |\\n| cyclopedias, news                |             |           |\\n| Dek-D teen webboard              |             | 6187      |\\n| Pantip general webboard          |             | 3499      |\\n| Wiki Wikipedia                   |             | 4147      |\\n| Others other sentences           |             | 6850      |\\n| total                            |             | 66921     |\\n\\n2.1.2. Audio recording\\n\\nThe collection process was carried out in between 2020 and 2021. We utilized a crowd-sourcing platform, https://www.wang.in.th, to record Thai-central. To minimize human errors, the platform only allows speakers who have passed a prerequisite test to record audio. Moreover, every audio recording is validated by another expert-level crowd worker to ensure data quality. The dialect recordings were done locally by recruiting participants from target regions. Since our recording were done during the COVID-19 pandemic, most recordings for Thai-dialect were conducted online by sending application links to speakers. All recordings were collected in the wild with minimal environmental control.\\n\\n2.1.3. Audio Verification\\n\\nTo curate the quality of the recordings, we employed three screening criteria: Voice Activity Detection (VAD), Signal to Noise Ratio (SNR), and signal energy. We utilized our in-house VAD to detect the parts of speech and noise in the audios. Every audio file must meet all of the criteria to be considered acceptable.\\n\\n2.1.4. Tokenization\\n\\nSince the Thai writing system does not mark word boundaries, tokenization of the text into words is also required. For the Thai-central corpus, we utilized a maximal-matching method in Pythainlp [18] for word tokenization. However, since there are no established tokenization standards for the dialects, we relied on dialectal specialists to tokenize the sentences. To ensure labeling consistency, we employed maximal matching tokenizers on subsets of the data to flag any potential disagreements with the specialist's tokenization. If there were disagreements between automatic tokenization and the specialist, the sentence would be re-examined. The final tokenization accuracy values are 68.52, 54.81, 72.06 for Khummuang, Korat, and Pattani, respectively.\\n\\n2.2. Corpus statistics and dialect details\\n\\nThai-central is the most comprehensive dialect in our corpus as it includes all seven sentence sources, making it the most generalized. In Thai-dialect, there are three dialects: Khummuang, Korat, and Pattani, which were spoken in the North, Northeastern, and South regions of Thailand, respectively. Thai-central and Thai-dialect information has shown in Table 3. Note that gender statistics were obtained using a pitch-based gender classifier.\\n\\nTo better understand the dialect similarities, we visualized the overlap in vocabulary between each dialect in the corpus using two out-of-vocabulary (OOV) metrics: type OOV and token OOV. Type OOV counts the percentage of non-overlapping unique words, while token OOV counts the frequency of non-overlapping words. As shown in Figure 1, Korat and Khummuang, both being Zhuang-Tai languages, have relatively low type and token OOV values compared to Thai-central. This is due to their shared grammar structure and vocabulary [19]. In contrast, Pattani is the most distinct dialect as it is more closely related to the Malay language from Malaysia, which belongs to the Austronesian language family [20]. In our Pattani corpus, the Thai writing system and alphabets are used instead of Arabic to maintain consistency with the rest of the corpus.\\n\\nIn our corpus, metadata consists of utterance ID, speaker ID, and the transcription. Thai-translated transcriptions were included in each dialect corpus. For the ASR task, we divided each dialect corpus into three sets: Train, Dev, and Test. We ensured that text transcriptions and speaker ID in the Dev, Test, and Train sets did not overlap. Evaluation can be further divided to only evaluate on E-commerce and Survival utterances. For Dev and Test, we set the ratio between Survival and E-commerce to 1:3. The corpus statistics are shown in Table 2.\"}"}
{"id": "suwanbandit23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Statistics of Thai-central and Thai-dialect datasets.\\n\\n|          | Thai-central (Th) | Khummuang (Kh) | Korat (Ko) | Pattani (Pa) |\\n|----------|------------------|----------------|------------|-------------|\\n| Utterances | 33,567           | 5,465          | 5,522      | 2,373       |\\n| Speakers  | 5,422            | 363            | 332        | 277         |\\n| Duration (hr) | 683.9        | 10.1           | 10.1       | 32.4        |\\n| Unique transcripts | 56,702       | 5,087          | 5,132      | 8,029       |\\n| Type OOV (%) | -              | 10.72          | 8.84       | -           |\\n| Token OOV (%) | -              | 1.17           | 0.83       | -           |\\n\\nTable 3: Information of each dialect in Thai-central and Thai-dialect corpus.\\n\\n| Dialect | Thai-Central | Khummuang | Korat | Pattani |\\n|---------|--------------|-----------|-------|--------|\\n| #Utterance | 433,814      | 60,350    | 45,307| 36,842 |\\n| #Speaker  | 6,713        | 844       | 571   | 891    |\\n| #Sentence | 66,921       | 37,269    | 8,546 | 15,570 |\\n| Hours    | 862          | 98        | 53    | 70     |\\n| Male     | 22.7%        | 23.6%     | 19.6% | 8.0%   |\\n| Female   | 77.3%        | 76.4%     | 81.4% | 92.0%  |\\n\\nFigure 2: Overview of our experiments.\\n\\nTo perform well on every dialect, which potentially degrades its effectiveness for smaller dialects. In our work, we explore the use of curriculum learning to study its effect in our dataset.\\n\\nWe studied the effectiveness of curriculum training for Thai dialect ASR using three different approaches, as shown in Figure 2. These three experiments were categorized by how we pre-trained the networks. In experiment I, we directly fine-tuned English HuBERT encoders using dialectal speech. Experiment II and III both comprise two training steps. The first step used the English HuBERT encoder as initialization for training on Thai audios. The second step fine-tuned the weights from the first step using dialect-specific training. We referred to the first step as pre-training and the second step as fine-tuning. The only difference between experiment II and III is the dataset we used in the pre-training stage. Concretely, we used only Thai-central for experiment II but also included the dialect corpus for experiment III.\\n\\n3.1. Architecture details\\n\\nWe conducted our experiment using ESPNet toolkit [21] and used transformer-based hybrid CTC/Attention models [22]. The encoder of the model was HuBERT [2], pre-trained on 960 hours of English Librispeech [23]. We used 0.3 as the weight for the CTC prediction head and used the label smoothing of 0.1. In the fine-tuning step, only the encoders were transferred from the pre-trained version, but the decoder was trained from scratch. Since all dialects share the same character set, all fine-tuned models has the same output setup.\\n\\n3.2. Training details\\n\\nFor each curriculum training step, we trained the models for 200k steps unless there were no improvements in the last 30 epochs. This criteria of 200k steps was applied to both pre-training and fine-tuning steps. We used a maximum training batch size of 562.5 seconds. For decoding, we used a beam size of 10. Since Thai writing does not have the standard use of word boundaries, we used our word tokenizer, described in Section 2.1.4, to re-tokenize predicted transcriptions before computing WER.\\n\\n3.3. Evaluation\\n\\nWe evaluated the models using the standard Word Error Rate (WER) computed on dialect specific test sets. Since every test set consists of two subsets (Survival and E-commerce), we reported performance for each subset as well. Specifically, we denoted S-WER and E-WER as the WERs for Survival and E-commerce subsets, respectively. Besides WER, Character Error Rate (CER) are also reported to remove any potential tokenization issues. No language model was used in order to directly measure the performance of the acoustic models.\\n\\n4. Experiments\\n\\n4.1. Experiment I: Baselines\\n\\nThe results of experiment I, which shows simple fine-tuning baselines from pre-trained English Hubert, are shown in Table 4.\\n\\nWe also tried a method proposed in [5] which performed\"}"}
{"id": "suwanbandit23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DID and ASR in a multitask manner. To perform DID, the end of sentence token was replaced with a dialect ID, so that the model learns the discrepancies between dialects. However, the model did not perform well, even though it had the DID accuracy of 96.78%. This is because few incorrect DID predictions dramatically degrade CER and WER values. Our finding also aligns with [5].\\n\\nTable 4: Experiment I: English Hubert fine-tuning Baselines\\n\\n| Dialect fine-tune | CER   | WER   | S-WER | E-WER |\\n|-------------------|-------|-------|-------|-------|\\n| Kh                | 7.19  | 9.26  | 15.46 | 6.94  |\\n| Kh+Th             | 6.65  | 8.99  | 12.95 | 7.51  |\\n| Kh+Ko+Pa          | 5.83  | 8.12  | 12.98 | 6.31  |\\n| Kh+Ko+Pa+Th       | 5.82  | 8.24  | 12.89 | 6.49  |\\n\\nTable 5: Experiment II: Thai-central pre-training\\n\\n| Dialect fine-tune | CER   | WER   | S-WER | E-WER |\\n|-------------------|-------|-------|-------|-------|\\n| Kh+Ko+Pa+Th       | 6.61  | 8.84  | 13.42 | 7.13  |\\n| Ko                | 9.16  | 13.73 | 27.06 | 9.29  |\\n| Pa                | 19.17 | 37.87 | 57.96 | 30.47 |\\n\\n4.2. Experiment II: Thai-central pre-training\\n\\nTable 5 shows how the main dialect, Thai-central, can be useful to other dialects. There are improvements in CER and WER when compared to English pre-training. This finding is congruent with what we found in both single and multi-dialect systems of experiment I. Even though HuBERT has demonstrated excellent performance in English ASR [2], pre-training on Thai-central displays language adaptation, which is the key to dialect ASR. Although Pattani is the distinct dialect, it also improves by 5.9% relatively in WER compared to the English pre-trained model.\\n\\n4.3. Experiment III: Thai-central and Thai-dialect pre-training\\n\\nIn Th+Kh and Th+Ko pre-training, single-dialect fine-tuning achieves satisfactory results in the overall CER and WER. On the other hand, the performance of Th+Pa becomes worse compared to just pre-training using Th. This suggests training size and distinctiveness of target dialects are factors for pre-training effectiveness. When all languages are used to pre-train (Th+Kh+Ko+Pa), both single and multi-dialect systems tend to perform better and are the best overall. The gap in performance between E-commerce and Survival subsets also decreases. The superiority of the multi-dialect system highlights the importance of the similarities between languages used for pre-training and fine-tuning. Our finding also aligns with [12].\\n\\n4.4. Further study on the effect of the Pattani dialect\\n\\nSince Pattani has a high OOV rate compared to other dialects, we hypothesize that it might hurt performance of other languages in a multi-dialect system. We therefore compared performances of multi-dialect systems with and without Pattani. Table 6 shows only small differences in both CER and WER compared to other methods. Including distant dialects might not harm the performance of multi-dialect system.\\n\\nTable 6: Ablation results on using only Th, Ko, and Kh\\n\\n| Dialect fine-tune | CER   | WER   | S-WER | E-WER |\\n|-------------------|-------|-------|-------|-------|\\n| Th                | 5.82  | 7.51  | 11.95 | 6.57  |\\n| Ko                | 8.64  | 12.54 | 28.63 | 7.82  |\\n| Pa                | 23.19 | 37.85 | 60.81 | 29.40 |\\n\\n5. Conclusion\\n\\nWe introduced the Thai-central and Thai-dialect corpus which have 700 hours of Thai and more than 40 hours of Thai dialectal language with parallel transcriptions. We investigated the use of transfer-based curriculum learning for creating strong dialectal ASR systems. Using the high-resource main dialect with the target dialect for pre-training and fine-tuning on the target dialect usually gives the most favorable results. Additionally, multi-dialect ASR systems can perform better than single systems in overall performance in Thai-dialectal languages.\\n\\n6. Acknowledgement\\n\\nThe authors would like to thank the PMU-C grant (Thai Language Automatic Speech Recognition Interface for Community E-Commerce, C10F630122) for the support of this research. We also would like to acknowledge the Apex compute cluster team which provides compute support for this project.\"}"}
