{"id": "pereztoro22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings\\n\\nP. A. P\u00e9rez-Toro1,2 \u22c6, P. Klumpp1, A. Hernandez1, T. Arias-Vergara1,2,7, P. Lillo3, A. Slachevsky3, A. M. Garc\u00eda4,5,6, M. Schuster7, A. K. Maier1, E. N\u00f6th1, J. R. Orozco-Arroyave1,2\\n\\n1Pattern Recognition Lab. Friedrich-Alexander Universit\u00e4t, Erlangen-N\u00fcrnberg, Erlangen, Germany\\n2GITA Lab, Facultad de Ingenier\u00eda. Universidad de Antioquia, Medell\u00edn, Colombia\\n3Departamento de Neurolog\u00eda Sur, Facultad de Medicina, Universidad de Chile, Santiago, Chile\\n4Centro de Neurociencias Cognitivas, Universidad de San Andr\u00e9s, Vito Dumas 284, Buenos Aires, Argentina\\n5Global Brain Health Institute, University of California, San Francisco, US\\n6Departamento de Lengu\u00edstica y Literatura, Facultad de Humanidades, Universidad de Santiago de Chile, Santiago, Chile\\n7Department of Otorhinolaryngology, Head and Neck Surgery. Ludwig-Maximilians Universit\u00e4t, Munich, Germany\\n\\n\u22c6corresponding authors: paula.andrea.perez@fau.de\\n\\nAbstract\\nCross-lingual approaches are growing in popularity in the machine learning domain, where large amounts of data are required to obtain better generalizations. Moreover, one of the biggest problems is the availability of clinical speech data, where most of the resources are in English. For instance, not many available Alzheimer's Disease (AD) corpora in different languages can be found in the literature. Despite the phonological and phonemic differences between Spanish and English, fortunately, there are also similarities between these two languages, e.g., around 40% of all words in English have a related word in Spanish. In this work, we want to investigate the feasibility of combining information from English and Spanish languages to discriminate AD. Two datasets were considered: part of the Pitt Corpus, which is composed of English speakers, and a Spanish AD dataset composed of speakers from Chile. We based our analysis on known acoustic (Wav2Vec) and word (BERT, RoBERTa) embeddings using different classifiers. Strong language dependencies were found, even using multilingual representations. We observed that linguistic information was more important for classifying English AD (F-Score=0.76) and acoustic for Spanish AD (F-Score=0.80). Using knowledge transferred from English to Spanish achieved F-scores of up to 0.85 for discriminating AD.\\n\\nIndex Terms: Alzheimer's Disease, Cross-Lingual Analysis, Acoustic Embeddings, Linguistic Embeddings\\n\\n1. Introduction\\nThe most common type of dementia is Alzheimer's Disease (AD), which is characterized by progressive cognitive decline. It is caused by brain tissue changes and a loss of a chemical vital to brain function called acetylcholine [1]. This adversely affects memory, language, understanding, and behavior. Thus, language and para-linguistic aspects can be highly compromised, triggering problems related to the capability to produce coherent language.\\n\\nClinical assessments draw heavily on neuropsychological tests, such as the Mini-Mental State Examination [2], which is a 30-point scale, where classically scores of over 24 indicates normal cognition. However, this test proves suboptimally reliable [3, 4, 5] and requires specialized examiners who are vastly outnumbered by current and prospective patients, calling for new diagnostic-support tools.\\n\\nAutomatic speech and language analyses have been considered in the literature for supporting the diagnosis and evaluation of AD and a majority of them have been conducted only using English corpora. One of the best-known Alzheimer's speech datasets available is the Pitt Corpus from the Dementia Bank [6], which is exclusively composed of native American English speakers. Since this dataset is publicly available, a large prior work has been reported in the literature using this corpus [7, 8, 9, 10, 11, 12], including two Interspeech Challenges [13, 14]. Moreover, there are some studies that used English cross-corpus learning for detecting Alzheimer's [15].\\n\\nConversely, very few AD studies have used non-English data [16, 17, 18]. Some examples include: (1) the \\\"Hungarian MCI-mAD database\\\" that was used in [16], where accuracies of up to 86% were obtained, (2) the \\\"Mandarin Lu corpus\\\" from the Dementia Bank in [17], which reported an area under the receiver operating characteristic curve of up to 0.90, and (3) a Chilean-Spanish AD corpus in [18], which achieved accuracies of up to 71%.\\n\\nCross-lingual studies are a major gap in the field, potentially representing a new source of global inequity in dementia research. Dementia has a prevalence of 7.1% in Latin America [19], where the risk factors explain 56% of dementia cases [20]. The Latin American population is made up mostly of native Spanish speakers. Spanish is the second most spoken language in the world only behind Mandarin Chinese and above English [21]. English and Spanish exhibit mainfold differences, crucially including phonological and lexical distinctions. One of the most noticeable differences is that excluding diphthongs, English has fifteen vowels whereas Spanish has five. Whereas there are differences between these two languages, there are also some similarities such as the use of the Roman alphabet, 40% of all English words have cognates in Spanish, learning uses similar basic processes, and despite some exceptions related to adjectives, there are similarities in sentence structures [22].\\n\\nThe reasons mentioned above motivated the use of multilingual techniques to transfer information from English to Spanish in this study. A set of fine-tuned multilingual acoustic and linguistic embeddings based on \\\"Transformers\\\" [23] is considered to perform transfer learning, since in the literature, those methods have shown good results on similar tasks [7, 10, 8, 24]. We explored these approaches on cross-lingual data because most studies performed experiments only on one language. Our findings show that by using transfer learning from one language\"}"}
{"id": "pereztoro22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to another the classification accuracy is improved. As expected, linguistic information was strongly linked to the language structure when transferring information from English to Spanish. However, the acoustic embeddings were more suitable for transferring knowledge for classifying Spanish AD.\\n\\n2. Methods\\n\\n2.1. Data\\n\\nWe employed datasets previously reported for English [6] and Spanish [18]. All the recordings were denoised using the proposed model in [25], which is based on recurrent networks that uses complex linear coding to estimate a denoising mask.\\n\\n2.1.1. The Pitt Corpus\\n\\nA sub-set with a total of 186 speakers from the Pitt Corpus [6] was considered in this study. The participants are native English speakers, and the group with AD was matched by age and sex with respect to the group of Healthy Control (HC) individuals. The speech task consisted of semi-spontaneous speech recordings [26] and transcripts describing the \u201ccookie theft picture\u201d [27]. The speech of the interviewer was removed according to the timestamps provided by the dataset. After segmentation, the average duration was 87 \u00b1 45 seconds for the AD patients and 65 \u00b1 23 seconds for the HC subjects. Demographic information of the participants and the MMSE are provided in Table 1.\\n\\nTable 1: Demographic and clinical information of the subjects in the subset of the Pitt corpus\\n\\n| AD patients | HC subjects |\\n|-------------|-------------|\\n| F / M       | F / M       |\\n| Number of subjects | 33 / 60 | 37 / 56 |\\n| Age [years]  | 66.5 (7.8) / 70.0 (7.3) | 64.5 (8.1) / 63.3 (8.0) |\\n| Education [years] | 13.5 (2.8) / 11.8 (2.2) | 13.7 (2.5) / 14.1 (2.6) |\\n| MMSE        | 20.0 (5.5) / 19.3 (4.9) | 28.8 (1.1) / 29.3 (1.0) |\\n\\nValues are expressed as mean (standard deviation). F: female. M: male. Age and education are given in years.\\n\\n2.1.2. Chilean Spanish AD\\n\\nThis database was recorded by the Universidad de Chile and Hospital del Salvador (Chile) [18]. The data consist of semi-spontaneous speech recordings and their transliterations from 39 Chilean Spanish speakers (21 AD and 18 HC) while describing the \u201ccookie theft picture\u201d [27]. Participants were matched by sex, age, and education. They were evaluated by an expert neurologist following the National Institute of Neurological and Communicative Diseases and Stroke-Alzheimer\u2019s Disease and Related Disorders Association clinical criteria for AD. The recordings were manually segmented and transcribed. After removing the interviewer\u2019s parts, the average duration of the recordings is 84 \u00b1 29 seconds for the AD patients and 73 \u00b1 15 seconds for the HC subjects. The demographic information of the participants is provided in Table 2.\\n\\nTable 2: Demographic and clinical information of the subjects in the Chilean Spanish AD corpus\\n\\n| AD patients | HC subjects |\\n|-------------|-------------|\\n| F / M       | F / M       |\\n| Number of subjects | 13 / 8 | 14 / 4 |\\n| Age [years]  | 79.1 (6.8) / 77.0 (7.1) | 74.5 (1.7) / 76.4 (4.6) |\\n| Education [years] | 11.1 (3.9) / 11.4 (3.7) | 7.5 (3.6) / 14.4 (3.1) |\\n| MMSE        | 20.7 (4.9) / 21.9 (3.1) | \u2013 / \u2013 |\\n\\nValues are expressed as mean (standard deviation). F: female. M: male. Age and education are given in years.\\n\\n2.2. Pre-trained Embeddings\\n\\nA set of multilingual pre-trained embeddings based on \u201cTransformers\u201d [23] was considered. The main idea of these models is that the speech and language characteristics common in several languages can be learned at the same time. Thus, a compressed representation obtained from speech and text will contain transferable contextual knowledge to be applied in different languages.\\n\\n2.2.1. Linguistic Embeddings\\n\\nThe methods considered here are based on the \u201cword-embeddings\u201d paradigm, which aims to capture semantic and syntactic relationships between words as well as contextual information. In this study, we considered the use of two pre-trained word-embeddings based on the encoder from the \u201cTransformers\u201d architecture [23], namely: Bidirectional Encoder Representations from Transformers (BERT) [28] and A Robustly Optimized BERT Pretraining Approach (RoBERTa) [29]. These methods process all elements simultaneously by forming direct connections between individual elements through a process known as attention. BERT learns multiple attention mechanisms, called heads, which operate in parallel to one another. The model captures a broader range of relationships between the words via multi-head attention. It is trained based on transfer learning because the model is first trained on two unsupervised tasks. The first one is based on Masked Language Modeling (MLM), to predict missing words (masked) in a sentence. The second one is Next Sentence Prediction (NSP), where the system is trained to predict whether a sentence follows another. RoBERTa uses the same concept as BERT, but removes the NSP part and uses larger batch sizes. In addition, this method introduces dynamic masking for the MLM, which masked tokens change during the training epochs. We considered the BERT-Base model trained with the Wikicorpus data from 102 languages and RoBERTA-Base model trained with filtered CommonCrawl data from 100 languages [30]. The last layer (768 units) is taken as the word-embedding representation in both methods. The mean of the overall word-embeddings is computed for the classification task.\\n\\nThe source code is also available online [31].\\n\\n2.2.2. Acoustic Embeddings\\n\\nThese embeddings are created with the Wav2Vec 2.0 [32] model, which aims to learn representations from raw speech signals using self-supervised methods. Wav2Vec\u2019s initial idea was to take raw audio as input and then compute an encoded representation to be used as input for a speech recognition system. The model consists of three main parts: feature extraction, a context network, and a linear projection to the output. The input consists of 16 kHz raw audio divided into chunks of 25 ms. The feature extraction part is composed of several temporal convolutions which convert the speech input into a latent space representation. Similar to the masking idea used in BERT for the self-supervised training, segments of audio are masked (instead of words), and then quantized. Further, a contextualized representation is obtained through a combination of a \u201cTransformers\u201d -based approach with contrastive learning. This approach\"}"}
{"id": "pereztoro22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"has been fine tuned in a broader range of tasks, covering from emotions recognition and speaker verification to the assessment of speech disorders. We considered two different pre-trained Wav2Vec models: XLSR-53 [33], which is a multilingual approach trained with CommonVoice, BABEL, and Multilingual LibriSpeech (MLS) datasets; and Wav2Vec for Xvectors [34], which consists of an XVector feature extraction head on top, which was trained with the VoxCeleb corpus [35]. For the XLSR-53 model, we used the output of the last convolutional layer of the model (number of chunks $\\\\times 512$) and computed the mean value over the chunks (512 dimensions). We do not consider the embeddings extracted from the last layer because they did not provide relevant results to this approach. The resulting embedding obtained from the Wav2Vec for Xvectors, was fixed per speaker and had a total of 512 dimensions.\\n\\n3. Experiments and Results\\n\\nTwo different experiments were performed in this work: (1) the classification of AD vs. HC using the pre-trained embeddings and (2) transfer learning using English data to enhance the AD discrimination in Spanish data. Three different classifiers were considered for comparison purposes: a Radial basis function Support Vector Machine (SVM), XGBoost, and a Multi Layer Perceptron (MLP). The classifiers were optimized following a 4-fold cross-validation strategy. The optimal parameters of each classifier were found through a grid search, where for the SVM $C \\\\in \\\\{10^{-4}, 10^{-3}, \\\\ldots, 10^4\\\\}$ and $\\\\gamma \\\\in \\\\{10^{-4}, 10^{-3}, \\\\ldots, 10^4\\\\}$, for the XGBoost the number of estimators $\\\\in \\\\{50, 70, \\\\ldots, 300\\\\}$ and the maximum depth $\\\\in \\\\{1, 3, 5, 7\\\\}$. For the MLP only a single layer of 768 neurons was considered. Since the folds were randomly chosen and label-balanced, the classification was performed 10 times.\\n\\n3.1. Classification using the pre-trained embeddings\\n\\nWe performed a visual inspection by using Principal Component Analysis (PCA). We noticed strong language dependencies, even though the embeddings were trained with multilingual data. Figure 1 shows an example for the case of the XLSR-53 model.\\n\\n![Figure 1: Principal component analysis projection of the original XLSR-53 Wav2Vec embeddings for each target group. ES: Spanish. EN: English](image)\\n\\nClassification results are shown in Table 3, where each embedding was considered separately, and a comparison between different classifiers and languages was performed. The classification is performed considering English only, Spanish only, and the combination of English and Spanish. Since there are more samples for English than for Spanish, the number of samples was balanced for training and testing.\\n\\n![Table 3: F-scores obtained with the original pre-trained embeddings for English (EN), Spanish (ES), and their combination.](table)\\n\\n| Source Emb | Language | SVM     | XGBoost | MLP     |\\n|------------|----------|---------|---------|---------|\\n| EN         |          | 0.68 (0.01) | 0.69 (0.02) | 0.68 (0.01) |\\n| ES         |          | 0.53 (0.08) | 0.46 (0.14) | 0.51 (0.10) |\\n| EN-ES      |          | 0.60 (0.06) | 0.54 (0.02) | 0.52 (0.03) |\\n\\nLinguistic BT: BERT. RBT: RoBERTa. W2: the XLSR-53 Wav2Vec model. WX: Wav2Vec for Xvectors model.\\n\\nNotice that the highest performance for discriminating AD in English speakers was obtained by using linguistic embeddings (RoBERTa = 0.76), while for Spanish were the acoustic embeddings (XLSR-53 Wav2Vec = 0.80). Regarding the classifiers, SVM and MLP yielded similar results. The classification of AD vs. HC was also performed combining the two datasets, achieving lower results compared to those obtained with each language separately.\\n\\n3.2. Transfer knowledge from English to Spanish\\n\\nA transfer learning based approach was proposed to enhance the classification of AD.\\n\\n![Figure 2: Scheme from English to Spanish transfer learning.](image)\"}"}
{"id": "pereztoro22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. For \\\"projecting\\\" the pre-trained embedding onto the Spanish data, we took the output of the linear layer (trained in English data) before the classification layer. Later on, these new embeddings were used for classifying AD in the Spanish data.\\n\\nThe main assumption is that adjusting the embeddings to a different task, despite being in a different language, improves the performance. By visual inspection using PCA in Figure 3, it can be observed that the projected embeddings improved the separation between samples while maintaining good inter-class separability. This type of behavior can be useful for the classifier, which can better distinguish classes using the projected space.\\n\\nFigure 3: Principal component analysis of the original XLSR-53 Wav2Vec embeddings for Spanish (right) and the projected XLSR-53 Wav2Vec embeddings obtained from the transfer learning for Spanish (left). Proj: projected XLSR-53 Wav2Vec embeddings obtained from the transfer learning.\\n\\nTable 4 shows the results using the projected embedding obtained from the transfer learning for classifying AD in Spanish speakers. A slight improvement is observed for the XLSR-53 model; however, the rest of the results show a bigger improvement for AD discrimination. Although the performance increased for the linguistic models, the acoustic model is still the one with more accurate results.\\n\\nTable 4: F-scores obtained with the projected pre-trained embeddings extracted from the transfer learning process of the Spanish data.\\n\\n| Source      | Emb | SVM | XGBoost | MLP |\\n|-------------|-----|-----|---------|-----|\\n| Linguistic  | BT  | 0.60 (0.08) | 0.59 (0.09) | 0.58 (0.02) |\\n|             | RBT | 0.68 (0.06) | 0.51 (0.06) | 0.51 (0.08) |\\n| Acoustic    | W2  | 0.83 (0.02) | 0.81 (0.05) | 0.83 (0.03) |\\n|             | WX  | 0.78 (0.05) | 0.73 (0.09) | 0.76 (0.04) |\\n\\nValues are expressed as mean (standard deviation). Emb: Embeddings. BT: BERT. RBT: RoBERTa. W2: the XLSR-53 Wav2Vec model. WX: Wav2Vec for Xvectors model.\\n\\nFinally, in Table 5 the highest performance for the early fusion of embeddings in the language combination is shown. The EN + ES model do not consider fine-tunning, both dataset are combined and the classifiers are trained, while for the EN \u2192 ES model the fine-tunning with the EN corpus is performed. For those that involved English the MLP classifier performed better, while for Spanish the better classifier was the SVM. Although the SVM is the most basic classifier, its behavior concerning the data seems more stable, especially when not too much data is available. RoBERTa and XLSR-53 were the ones with the most accurate results in linguistics and acoustics, respectively.\\n\\nTable 5: Best classification results obtained with the combination of the different pre-trained embeddings. EN: Pitt corpus. ES: Chilean Spanish corpus. EN + ES: Pitt and Chilean Spanish corpora together. EN \u2192 ES: transfer learning from the Pitt to the Chilean Spanish corpus (Spanish only).\\n\\n| Language | Emb | Classifier | F-Score |\\n|----------|-----|------------|---------|\\n| EN       | RBT| MLP        | 0.81 (0.02) |\\n| ES       | W2 | SVM        | 0.81 (0.03) |\\n| EN + ES  | RBT| MLP        | 0.78 (0.01) |\\n| EN \u2192 ES  | RBT+W2 | MLP+W2 | 0.85 (0.01) |\\n\\nValues are expressed as mean (standard deviation). Emb: Embeddings. BT: BERT. RBT: RoBERTa. W2: the XLSR-53 Wav2Vec model. WX: Wav2Vec for Xvectors model.\\n\\nMoreover, the fusion of embeddings helped in improving the classification for all the language combinations.\\n\\n4. Discussion and Conclusions\\n\\nThe use of multilingual embeddings along with a transfer learning-based approach was proposed for a cross-lingual analysis of AD. The aim was to improve inter-class separability while exploring the possibility of using these embeddings in multilingual analyses. As expected, strong language dependencies were observed, especially for linguistics embeddings, which also performed better in English. A lower language dependency was observed for the acoustics embeddings, but unlike linguistics, it achieved better results on the Spanish corpus. We believe that this is due to the fact that the phonetic \\\"space\\\" of sounds in Spanish is simpler and more predictable than in English. Spanish contains 25 phonemes while English has 44 phonemes [22] and the structure of the first is mainly based on the consonant-vowel combination, while the second includes also several consonant clusters. Besides, we want to highlight that Wav2Vec results should be taken with caution, as they may be markedly influenced by acquisition conditions. Further research is required in these two aspects hopefully with more data to validate our observations. Another contribution of this work is the use of a transfer learning approach to improve the separability between AD and HC classes. Our numerical and visual findings suggest that by transferring information of the same task from English to Spanish improves the inter-classes separability. The results with the linguistic models suggest that more exploration is needed due to the grammatical and semantic complexity of each language. We are aware of the limitations of this study, mainly regarding the need for additional data and also for a systematic evaluation of possible acoustic differences among recording sessions. Future work will consider different features that may be useful for cross-linguistic analyses in AD, especially those that show higher inter-class separability and less inter-language dependency. Other fusion methods will be explored to see whether classification accuracies improve.\\n\\n5. Acknowledgements\\n\\nThis study was partially funded by CODI at UdeA grant # PRG2020-34068. Adolfo Garc\u00eda received funding from the Global Brain Health Institute (GBHI), Alzheimer's Association, and Alzheimer's Society (Alzheimer's Association GBHI ALZ UK-22-865742); ANID, FONDECYT Regular [1210176]; and Programa Interdisciplinario de Investigaci\u00f3n Experimental en Comunicaci\u00f3n y Cognici\u00f3n (PIIECC), Facultad de Humanidades, USACH.\"}"}
{"id": "pereztoro22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] D. S. Knopman, H. Amieva, R. C. Petersen et al., \\\"Alzheimer disease,\\\" Nature reviews Disease primers, vol. 7, no. 1, pp. 1\u201321, 2021.\\n\\n[2] M. F. Folstein, L. N. Robins, and J. E. Helzer, \\\"The mini-mental state examination,\\\" Archives of general psychiatry, vol. 40, no. 7, pp. 812\u2013812, 1983.\\n\\n[3] S. T. Creavin, S. Wisniewski, A. H. Noel-Storr, C. M. Trevelyan et al., \\\"Mini-Mental State Examination (MMSE) for the detection of dementia in clinically unevaluated people aged 65 and over in community and primary care populations,\\\" Cochrane Database of Systematic Reviews, no. 1, 2016.\\n\\n[4] A. J. Mitchell, \\\"A meta-analysis of the accuracy of the mini-mental state examination in the detection of dementia and mild cognitive impairment,\\\" Journal of psychiatric research, vol. 43, no. 4, pp. 411\u2013431, 2009.\\n\\n[5] Y. C. Lee, S. C. Lee, and E. C. Chiu, \\\"Practice effect and test-retest reliability of the Mini-Mental State Examination-2 in people with dementia,\\\" BMC geriatrics, vol. 22, no. 1, pp. 1\u20138, 2022.\\n\\n[6] J. T. Becker, F. Boiler, O. L. Lopez et al., \\\"The natural history of Alzheimer's disease: description of study cohort and accuracy of diagnosis,\\\" Archives of Neurology, vol. 51, no. 6, pp. 585\u2013594, 1994.\\n\\n[7] M. Martinc and S. Pollak, \\\"Tackling the ADReSS challenge: a multimodal approach to the automated recognition of Alzheimer's dementia,\\\" Proc. Interspeech 2020, pp. 2157\u20132161, 2020.\\n\\n[8] R. Pappagari, J. Cho, L. Moro-Velazquez et al., \\\"Using state of the art speaker recognition and natural language processing technologies to detect Alzheimer's disease and assess its severity,\\\" Proc. Interspeech 2020, pp. 2177\u20132181, 2020.\\n\\n[9] M. S. S. Syed, M. Lech, and E. Pirogova, \\\"Automated Screening for Alzheimer's Dementia through Spontaneous Speech,\\\" Proc. Interspeech 2020, pp. 1\u20135, 2020.\\n\\n[10] A. Balagopalan, B. Eyre, F. Rudzicz, and J. Novikova, \\\"To BERT or Not To BERT: Comparing Speech and Language-based Approaches for Alzheimer's Disease Detection,\\\" Proc. Interspeech 2020, 2020.\\n\\n[11] P. A. P\u00b4erez-Toro, S. P. Bayerl, T. Arias-Vergara et al., \\\"Influence of the Interviewer on the Automatic Assessment of Alzheimer's Disease in the Context of the ADReSSo Challenge,\\\" in Proc. Interspeech 2021, 2021, pp. 3785\u20133789.\\n\\n[12] Z. Ye, S. Hu, J. Li et al., \\\"Development of the cuhk elderly speech recognition system for neurocognitive disorder detection using the dementiabank corpus,\\\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6433\u20136437.\\n\\n[13] S. Luz, F. Haider, S. de la Fuente et al., \\\"Alzheimer\u2019s Dementia Recognition through Spontaneous Speech: The ADReSS Challenge,\\\" in Proc. Interspeech 2020, 2020, pp. 2172\u20132176. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2020-2571\\n\\n[14] \u2014\u2014, \\\"Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge,\\\" in Proc. Interspeech 2021, 2021, pp. 3780\u20133784.\\n\\n[15] S. de la Fuente Garcia, F. Haider, and S. Luz, \\\"Cross-corpus feature learning between spontaneous monologue and dialogue for automatic classification of alzheimer\u2019s dementia speech,\\\" in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 2020, pp. 5851\u20135855.\\n\\n[16] V. Gosztolya, G. Vincze, L. T \u00b4oth et al., \\\"Identifying Mild Cognitive Impairment and mild Alzheimer\u2019s disease based on spontaneous speech using ASR and linguistic features,\\\" Computer Speech & Language, vol. 53, pp. 181\u2013197, 2019.\\n\\n[17] Y. W. Chien, S. Y. Hong, W. T. Cheah et al., \\\"An automatic assessment system for Alzheimer\u2019s disease based on speech using feature sequence generator and recurrent neural network,\\\" Scientific Reports, vol. 9, no. 1, pp. 1\u201310, 2019.\\n\\n[18] C. Sanz, F. Carrillo, A. Slachevsky et al., \\\"Automated text-level semantic markers of Alzheimer\u2019s disease,\\\" Alzheimer's & Dementia: Diagnosis, Assessment & Disease Monitoring, vol. 14, no. 1, p. e12276, 2022.\\n\\n[19] N. Custodio, A. Wheelock, D. Thumala, and A. Slachevsky, \\\"Dementia in Latin America: epidemiological evidence and implications for public policy,\\\" Frontiers in aging neuroscience, vol. 9, p. 221, 2017.\\n\\n[20] M. A. Parra, S. Baez, L. Sede \u02dcno et al., \\\"Dementia in Latin America: paving the way toward a regional action plan,\\\" Alzheimer's & Dementia, vol. 17, no. 2, pp. 295\u2013313, 2021.\\n\\n[21] G. Julian, \\\"What are the most spoken languages in the world,\\\" Retrieved May, vol. 31, p. 2020, 2020.\\n\\n[22] R. P. Stockwell, J. D. Bowen, and J. W. Martin, The grammatical structures of English and Spanish. University of Chicago Press, 1965, vol. 4.\\n\\n[23] A. Vaswani, N. Shazeer, N. Parmar et al., \\\"Attention is all you need,\\\" Advances in neural information processing systems, vol. 30, 2017.\\n\\n[24] P. A. P\u00b4erez-Toro, J. C. V \u00b4asquez-Correa, T. Arias-Vergara et al., \\\"Acoustic and Linguistic Analyses to Assess Early-Onset and Genetic Alzheimer\u2019s Disease,\\\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 8338\u20138342.\\n\\n[25] H. Schr \u00a8oter, T. Rosenkranz, A. Maier et al., \\\"CLC: Complex Linear Coding for the DNS 2020 Challenge,\\\" arXiv preprint arXiv:2006.13077, 2020.\\n\\n[26] V. Boschi, E. Catricala, and M. a. Consonni, \\\"Connected speech in neurodegenerative language disorders: a review,\\\" Frontiers in psychology, vol. 8, p. 269, 2017.\\n\\n[27] H. Goodglass et al., \\\"Cookie Theft picture,\\\" Boston diagnostic aphasia examination. Philadelphia, PA: Lea & Febiger, 1983.\\n\\n[28] J. Devlin, M. W. Chang, K. Lee et al., \\\"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\\\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171\u20134186. [Online]. Available: https://www.aclweb.org/anthology/N19-1423\\n\\n[29] L. Zhuang, L. Wayne, S. Ya et al., \\\"A Robustly Optimized BERT Pre-training Approach with Post-training,\\\" in Proceedings of the 20th Chinese National Conference on Computational Linguistics, 2021, pp. 1218\u20131227.\\n\\n[30] R. Sch \u00a8afer, \\\"CommonCOW: massively huge web corpora from CommonCrawl data and a method to distribute them freely under restrictive EU copyright laws,\\\" in Proceedings of the tenth international conference on language resources and evaluation (LREC\u201916), 2016, pp. 4500\u20134504.\\n\\n[31] P. A. Perez-Toro, \\\"PauPerezT/WEBERT: Word Embeddings using BERT,\\\" https://doi.org/10.5281/zenodo.3964244, Jul. 2020.\\n\\n[32] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \\\"wav2vec 2.0: A framework for self-supervised learning of speech representations,\\\" Advances in Neural Information Processing Systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[33] A. Conneau, A. Baevski, R. Collobert et al., \\\"Unsupervised cross-lingual representation learning for speech recognition,\\\" arXiv preprint arXiv:2006.13979, 2020.\\n\\n[34] S. W. Yang, P. H. Chi, Y. S. Chuang et al., \\\"Superb: Speech processing universal performance benchmark,\\\" arXiv preprint arXiv:2105.01051, 2021.\\n\\n[35] A. Nagrani, J. S. Chung, and A. Zisserman, \\\"V oxceleb: a large-scale speaker identification dataset,\\\" arXiv preprint arXiv:1706.08612, 2017.\"}"}
