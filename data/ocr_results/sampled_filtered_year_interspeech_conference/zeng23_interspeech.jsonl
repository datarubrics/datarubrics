{"id": "zeng23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms\\n\\nChang Zeng\\\\textsuperscript{1,2}, Xin Wang\\\\textsuperscript{1}, Xiaoxiao Miao\\\\textsuperscript{1}, Erica Cooper\\\\textsuperscript{1}, Junichi Yamagishi\\\\textsuperscript{1,2}\\n\\n\\\\textsuperscript{1}National Institute of Informatics, Japan\\n\\\\textsuperscript{2}SOKENDAI, Japan\\n\\n\\\\{zengchang, wangxin, xiaoxiaomiao, ecooper, jyamagishi\\\\}@nii.ac.jp\\n\\nAbstract\\n\\nThe ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms. The results on our cross-genre evaluation dataset for anti-spoofing show that the proposed method significantly improved the generalization ability of the countermeasures compared with the baseline system in the genre mismatch scenario.\\n\\nIndex Terms: anti-spoofing, generalization, multi-task learning, DeepFake detection\\n\\n1. Introduction\\n\\nWith the development of deep neural networks, DeepFake detection has attracted much attention from academia and industry since synthesized media, such as video and speech by artificial intelligence, has brought huge risks. The ASVspoof challenge series [1, 2, 3, 4] has been proposed to explore and promote the investigation of anti-spoofing for automatic speaker verification (ASV) systems. The Audio Deep Synthesis Detection (ADD) challenge [5] was held to study countermeasure models in a scenario with low-quality audio. The generalization ability in anti-spoofing is an important topic, and typical distribution shifts between training and testing data, such as unseen synthesis methods [3] and unseen acoustic environments [4], have been investigated in the challenges. However, there is room for further investigations.\\n\\nHere, we focus on a new mismatch scenario related to the audio genre, in which fake audio may be generated from real audio with unseen genres because the effects of the speaker\u2019s styles and intrinsic factors associated with the specific genre have not been studied thoroughly in anti-spoofing. The definition of audio genre in our study is the same as [6, 7]. To analyze the effects and propose a more robust system, we utilize the copy-synthesis method [8, 9, 10, 11] to produce spoofed data using multiple vocoders from real mel-spectrograms extracted from the waveforms of the CN-Celeb1&2 datasets [6, 7], which contain more than 600,000 utterances with 11 different genres. The new dataset is called CN-Spoof. To visually show the genre mismatch, we randomly select eight genres from the CN-Celeb1&2 and CN-Spoof datasets and train a lightweight convolutional neural network (LCNN) [3, 12]. We use the well-trained LCNN to extract the countermeasure embeddings from the waveforms with two seen and two unseen genres and visualize them in two-dimensional space by T-SNE [13] as Fig. 1 shows. From the figure, it is obvious that the embedding with seen \u201cplay\u201d and \u201cspeech\u201d genres can be classified well by the LCNN model. However, for the embedding with unseen \u201csinging\u201d and \u201crecitation\u201d genres, part of the real and fake embeddings overlap, which means it is hard to distinguish the fake audio samples with unseen genres by the LCNN model.\\n\\nTo address this genre mismatch, we make four training protocols by grouping different genres. For each protocol, we randomly select utterances from several genres as the training dataset, and all training protocols share the same evaluation dataset. On the basis of the training and evaluation datasets, in this paper, we propose a novel multi-task learning method and design two auxiliary regularization objectives in addition to the main anti-spoofing objective to improve the generalization ability of the countermeasure model. The first objective is to simulate the genre mismatch scenario in the training stage by meta-optimization [14, 15, 16, 17]. In addition, for the second objective, we utilize a genre alignment module that contains a gradient reversal layer (GRL) [18, 19] to remove the genre information in the countermeasure embedding. Finally, the two auxiliary regularization objectives are combined with the main anti-spoofing objective by using uncertainty loss weights [20, 21], which can be learned from the data instead of setting them by hand resulting in inferior optimization. The experimental results on the evaluation dataset show that the proposed method significantly improves the genre generalization ability compared with the baseline system.\"}"}
{"id": "zeng23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. CN-Spoof dataset and protocols\\n\\nSince there are no related datasets in the research on anti-spoofing for the genre mismatch scenario, we leverage three pretrained neural vocoders (Multi-band MelGAN [22], Parallel WaveGAN [23], and HiFiGAN [24]), and two DSP vocoders (WORLD [25] and Griffin-Lim [26]), to reconstruct the waveforms from the real mel-spectrograms of the CN-Celeb1&2 datasets which are collected for speaker verification in the multi-genre scenario. Specifically, for each vocoder, we randomly select 20,000 and 80,000 utterances from the CN-Celeb1 and CN-Celeb2 datasets, respectively. Then, vocoded waveforms of the selected utterances using the above five vocoders are treated as fake data. As a result, the CN-Spoof dataset contains 500,000 fake utterances in total.\\n\\nAfter generating the CN-Spoof dataset, we combine it with the CN-Celeb1&2 datasets and randomly sample 200,000 utterances from all genres as the evaluation dataset, whose genre distribution and real/fake class ratio are shown in Fig. 2. The remaining portions are utilized to construct the training dataset for different cross-genre protocols (CGP). We first divide ten genres included in the remaining portions into four groups, shown in Table 1. Note that the \\\"advertisement\\\" genre is discarded from the dataset since its number is 2,929, which is limited.\\n\\nFor each CGP, we randomly sample 660,000 utterances with three genre groups as the training dataset from the remaining portions. In this way, each training dataset has an unseen genre group that exists in the evaluation dataset, as Table 2 shows.\\n\\n3. Proposed multi-task learning method\\n\\nThe proposed multi-task learning method is illustrated concretely in this section. Since our method contains meta-optimization and genre alignment regularization objectives, we first describe a genre sampling strategy for constructing a task set for all objectives. Then, the main objective and auxiliary regularization objectives are depicted, respectively. Finally, we give details on combining multiple objectives with the uncertainty loss weights.\\n\\n3.1. Genre sampling\\n\\nAs shown in Fig. 3, we randomly sample data from the training dataset $D = \\\\{D_1, \\\\ldots, D_G | G > 1\\\\}$, which includes $G$ seen genres, to construct a task set $T = \\\\{T_1, \\\\ldots, T_K | K > 1\\\\}$, which contains $K$ tasks. A task is composed of a meta-train dataset that contains $G_{mtr} (G_{mtr} < G)$ genres and is used for the main anti-spoofing objective as well as the auxiliary genre alignment objective, and a meta-test dataset is used that contains the remaining $G_{mte} = G - G_{mtr}$ genres for the auxiliary meta-optimization objective. In our experiments, for each training protocol, we set $G_{mtr}$ as $G - 1$ for the meta-train dataset, and the remaining one is used as the meta-test dataset.\\n\\n3.2. Main anti-spoofing objective\\n\\nThe main anti-spoofing objective is to distinguish whether input speech is real. In our proposed method, LCNN [3, 12] is selected as the backbone, as shown in Fig. 3. Countermeasure embeddings are extracted by the LCNN model from the meta-train dataset, which is represented by a formula:\\n\\n$$e_{mtr} = f_{\\\\theta_{b}}(x_{mtr}), \\\\quad (1)$$\\n\\nwhere $e_{mtr}$ and $x_{mtr}$ represent the countermeasure embedding and corresponding speech in the meta-train dataset, respectively. $f_{\\\\theta_{b}}(\\\\cdot)$ means the transformation function of the LCNN backbone, whose parameters are $\\\\theta_{b}$.\\n\\nThen, a binary classifier, including an affine layer and a Sigmoid function, transforms the embeddings $e_{mtr}$ into probability values, representing the possibility of speech being real. Finally, we perform a binary cross-entropy (BCE) loss function on the probability values and the corresponding labels. The process can be formulated as:\\n\\n$$P(e_{mtr}) = \\\\frac{1}{1 + \\\\exp(-e_{mtr}^{T} \\\\theta_{c})}, \\\\quad (2)$$\\n\\n$$L_{\\\\text{main}} = -\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\left[I(y_{i_{mtr}} = 1) \\\\log P(e_{i_{mtr}}) + I(y_{i_{mtr}} \\\\neq 1) \\\\log(1 - P(e_{i_{mtr}}))\\\\right], \\\\quad (3)$$\\n\\nwhere $L_{\\\\text{main}}$ represents the cost of the main anti-spoofing objective on the meta-train dataset, which has $N$ speech samples. $\\\\theta_{c}$ denotes the parameters of the classifier. Note that for concise description, we ignore the bias parameter of the affine layer in Eq. (2). $I(\\\\cdot)$ is an indicator function that returns 1 if the condition is true and 0 otherwise, and $y_{i_{mtr}}$ is the $i$-th label. When $e_{i_{mtr}}$ is from real speech, $y_{i_{mtr}}$ equals 1, otherwise 0. In the following part, we use $\\\\theta$ to denote the $\\\\theta_{b}$ and $\\\\theta_{c}$.\"}"}
{"id": "zeng23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Architecture of proposed multi-task learning method.\\n\\n3.3. Meta-optimization regularization objective\\nIn the genre sampling stage, we divide a task into a meta-train and a meta-test dataset without overlapping genres. In this way, we can simulate the genre mismatch in the training stage by evaluating the performance on the meta-test dataset of the model trained on the meta-train dataset. Specifically, as Fig. 3 shows, we maintain a counterpart of the model parameters and update it by the loss $L_{main}$ with the learning rate $\\\\beta$:\\n\\n$$\\\\theta' = \\\\theta - \\\\beta \\\\cdot \\\\frac{\\\\partial L_{main}}{\\\\partial \\\\theta}, \\\\quad (4)$$\\n\\nwhere $\\\\theta'$ denotes the updated parameters of the counterpart, including $\\\\theta'_b$ and $\\\\theta'_c$. Next, we evaluate the model performance on the meta-test dataset and compute the loss:\\n\\n$$e_{mte} = f_{\\\\theta'_b}(x_{mte}), \\\\quad (5)$$\\n\\n$$P(e_{mte}) = \\\\frac{1}{1 + \\\\exp(-e_{mte}^\\\\top \\\\theta'_c)} \\\\quad (6)$$\\n\\n$L_{MO}$ denotes the loss on the meta-test dataset for the meta-optimization regularization objective.\\n\\n3.4. Genre alignment regularization objective\\nIn addition to simulating the genre mismatch in the training stage by meta-optimization, we use another auxiliary genre alignment regularization objective that is realized by using a multi-layer perceptron (MLP) with the GRL component to improve the generalization ability of the LCNN model because it can remove the genre information contained in countermeasure embeddings by adversarial training [18]. Before inputting the countermeasure embedding $e_{mtr}$ to the MLP for genre classification, we first apply the GRL component to it as Fig. 3 shows. The MLP contains three layers, and each has 128 neural units. Instead of utilizing plain cross-entropy as the loss function, here we use focal loss [27] because it can improve the discriminative ability of the genre alignment module, which is beneficial for further filtering out the genre information in countermeasure embeddings. The loss function of the genre alignment module is formulated as:\\n\\n$$P(e_{mtr}, g_c) = \\\\exp(h_{\\\\theta_{gcga}}(e_{mtr})) \\\\sum_{G_j=1} \\\\exp(h_{\\\\theta_{gj}}(e_{mtr})) \\\\quad (8)$$\\n\\n$$L_{GA} = -\\\\frac{1}{N} \\\\sum_i \\\\left(1 - P(e_{mtr}, g_i)^\\\\gamma \\\\log P(e_{mtr}, g_i) \\\\right) \\\\quad (9)$$\\n\\nwhere $g_c$ is the genre label of the countermeasure embedding $e_{mtr}$. $h_{\\\\theta_{gcga}}(\\\\cdot)$ denotes a transformation for outputting the un-normalized possibility of the $c$-th genre from the genre alignment module, whose parameters are $\\\\theta_{ga}$. $P(e_{mtr}, g_c)$ is the probability that the countermeasure embedding $e_{mtr}$ belongs to the correct genre, and $\\\\gamma$ is a hyper-parameter that can adjust the gradient contribution of different samples in accordance with their difficulties. Note here that we ignore the hyper-parameter $\\\\alpha$ in [18] since all genres are treated equally in our method. Due to the GRL component, the gradient after the GRL in the backward propagation is reversed compared with that without the GRL.\\n\\n3.5. Learnable loss weights\\nAs the proposed method has multiple loss terms, the model performance is extremely sensitive to loss weight selection [20, 21]. To this end, we use a common strategy in multi-task learning to combine multiple loss terms with learnable loss weights, the aim of which is learning the optimal weights. As our auxiliary objectives can be discarded in the inference stage, we treat the loss of auxiliary objectives as the regularization terms [21]. Thus, the total loss can be formulated as:\\n\\n$$L_{total} = \\\\frac{1}{2} \\\\cdot \\\\lambda_{main}^2 \\\\cdot L_{main} + \\\\ln(1 + \\\\lambda_{main}^2) + \\\\frac{1}{2} \\\\cdot \\\\lambda_{MO}^2 \\\\cdot L_{MO} + \\\\ln(1 + \\\\lambda_{MO}^2) + \\\\frac{1}{2} \\\\cdot \\\\lambda_{GA}^2 \\\\cdot L_{GA} + \\\\ln(1 + \\\\lambda_{GA}^2), \\\\quad (10)$$\\n\\nwhere $\\\\lambda_{main}$, $\\\\lambda_{MO}$, and $\\\\lambda_{GA}$ are the learnable parameters for each loss term, respectively. As for the logarithmic terms in Eq. (10), they are constraint conditions for avoiding trivial solutions [20, 21].\"}"}
{"id": "zeng23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: EER (%) of experimental results on CGP. For each protocol, the genre group in the bracket does not appear in the training dataset. A bold number means the best performance of this genre.\\n\\n| Protocol System  | Overall | Group I     | Group II    | Group III   | Group IV   |\\n|------------------|---------|-------------|-------------|-------------|------------|\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n|                  |         | Group I     | Group II    | Group III   | Group IV   |\\n\\n### 4. Experimental results\\n\\n#### 4.1. Experimental setup\\n\\nAs described in Section 2, the CN-Spoof dataset was combined with the CN-Celeb1&2 datasets, and we divided these data in accordance with the cross-genre protocols. For each protocol, the training dataset contained 660,000 utterances. All protocols shared the same evaluation dataset, which contained 200,000 utterances from all genres.\\n\\nAs for the systems used for comparison in the paper, the LCNN model was selected as the baseline system. Additionally, we also constructed two other experimental systems: one incorporating the L-main and L-MO losses, and the other incorporating the L-main and L-GA losses. These systems were developed to thoroughly examine the impact of various regularizations.\\n\\n#### 4.2. Training methodology\\n\\nFor the systems without meta-optimization, we randomly selected 64 samples from the dataset as a mini-batch. For the systems including meta-optimization, we randomly selected 64 samples from the dataset. One genre was randomly selected from this subset as the meta-test dataset, while the remaining samples were used as the meta-train dataset. We trained all systems for 40 epochs using an SGD optimizer with a 0.001 initial learning rate, 0.9 momentum, and 0.0001 L2 regularization. The learning rate was decayed by 0.9 every epoch. As for other hyper-parameters, \u03b2 in Eq. (4) was set to 0.001, and \u03b3 in Eq. (9) was set to 5.\\n\\n#### 4.3. Results and analysis\\n\\nThe experimental results are shown in Table 3. As we can see, for each protocol, the countermeasure performance without the regularization terms on the unseen genres was generally worse than the counterparts for the other protocols except for the \u201cmovie\u201d genre in CGP II and \u201cdrama\u201d genre in CGP IV. This result proves that a model without the regularization terms cannot generalize well on unseen genres, which is consistent with our hypothesis.\\n\\nAlthough the GRL component has shown the capacity to generalize well on unseen domains in the speaker verification task, we found that the genre alignment loss L-GA only slightly improved the generalization ability compared with the baseline model in the genre mismatch scenario. In contrast, the meta-optimization loss L-MO improved the EER numbers on some unseen genres, such as the \u201csinging\u201d genre for CGP I and \u201cmovie\u201d genre for CGP II. However, there are some other genres that the system had difficulty generalizing well, such as the \u201cspeech\u201d genre for CGP III.\\n\\nAs for our multi-task learning method that integrates the meta-optimization and genre alignment regularization objectives using learnable loss weights, it significantly improved the generalization ability in the genre mismatch scenario by comparing its result with the baseline system for unseen genres. Even for some challenging genres like the \u201crecitation\u201d and \u201cspeech\u201d genres, which cannot be generalized well by using either L-MO or L-GA only, combining them further improve the generalization ability.\\n\\nMoreover, we can see that our proposed system not only performs well on unseen genres but also on seen genres. For protocol CGP I and II, our system obtained the best performance on all genres compared with the other systems. As for the protocol CGP III and IV, although our approach performed slightly worse than the other systems for several genres, it still obtained the best EER numbers for most genres.\\n\\n### 5. Conclusions\\n\\nIn this paper, we explored a new mismatch scenario for the anti-spoofing objective, in which the fake speech may come from the real speech with unseen genres. Since there is no anti-spoofing data related to this scenario, we utilized the copy-synthesis method to create a spoofed dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Our proposed multi-task learning method, which combines the meta-optimization loss and genre alignment loss as the regularization terms by using learnable loss weights, shows the potential to improve the generalization ability of the countermeasure models under this scenario. The experimental results on four different cross-genre protocols proved that our method is more robust than the baseline system, even when facing difficult genres.\"}"}
{"id": "zeng23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Z. Wu, T. Kinnunen, N. Evans, J. Yamagishi, C. Hanilc \u00b8i, M. Sahidullah, and A. Sizov, \\\"Asvspoof 2015: the first automatic speaker verification spoofing and countermeasures challenge,\\\" in Sixteenth annual conference of the international speech communication association, 2015.\\n\\n[2] T. H. Kinnunen, M. Sahidullah, H. Delgado, M. Todisco, N. W. D. Evans, J. Yamagishi, and K.-A. Lee, \\\"The asvspoof 2017 challenge: Assessing the limits of replay spoofing attack detection,\\\" in Interspeech, 2017.\\n\\n[3] X. Wang, J. Yamagishi, M. Todisco, H. Delgado, A. Nautsch, N. Evans, M. Sahidullah, V. Vestman, T. Kinnunen, K. A. Lee et al., \\\"Asvspoof 2019: A large-scale public database of synthesized, converted and replayed speech,\\\" Computer Speech & Language, vol. 64, p. 101114, 2020.\\n\\n[4] J. Yamagishi, X. Wang, M. Todisco, M. Sahidullah, J. Patino, A. Nautsch, X. Liu, K. A. Lee, T. Kinnunen, N. Evans et al., \\\"Asvspoof 2021: accelerating progress in spoofed and deepfake speech detection,\\\" in ASVspoof 2021 Workshop-Automatic Speaker Verification and Spoofing Countermeasures Challenge, 2021.\\n\\n[5] J. Yi, R. Fu, J. Tao, S. Nie, H. Ma, C. Wang, T. Wang, Z. Tian, Y. Bai, C. Fan et al., \\\"Add 2022: the first audio deep synthesis detection challenge,\\\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 9216\u20139220.\\n\\n[6] Y. Fan, J. Kang, L. Li, K. Li, H. Chen, S. Cheng, P. Zhang, Z. Zhou, Y. Cai, and D. Wang, \\\"Cn-celeb: a challenging chinese speaker recognition dataset,\\\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7604\u20137608.\\n\\n[7] L. Li, R. Liu, J. Kang, Y. Fan, H. Cui, Y. Cai, R. Vipperla, T. F. Zheng, and D. Wang, \\\"Cn-celeb: multi-genre speaker recognition,\\\" Speech Communication, vol. 137, pp. 77\u201391, 2022.\\n\\n[8] W. J. Holmes, \\\"Copy synthesis of female speech using the jsru parallel formant synthesiser.\\\" in EUROSPEECH, 1989, pp. 2513\u20132516.\\n\\n[9] M. Pal, D. Paul, and G. Saha, \\\"Synthetic speech detection using fundamental frequency variation and spectral features,\\\" Computer Speech & Language, vol. 48, pp. 31\u201350, 2018.\\n\\n[10] J. C. Frank and L. Sch\u00f6nherr, \\\"Wavefake: A data set to facilitate audio deepfake detection,\\\" in Proc. NeurIPS Datasets and Benchmarks 2021, 2021.\\n\\n[11] X. Wang and J. Yamagishi, \\\"Spoofed training data for speech spoofing countermeasure can be efficiently created using neural vocoders,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (accepted). IEEE, 2023.\\n\\n[12] G. Lavrentyeva, S. Novoselov, A. Tseren, M. Volkova, A. Gorlanov, and A. Kozlov, \\\"STC Antispoofing Systems for the ASVspoof2019 Challenge,\\\" in Proc. Interspeech 2019, 2019, pp. 1033\u20131037.\\n\\n[13] L. Van der Maaten and G. Hinton, \\\"Visualizing data using t-sne.\\\" Journal of machine learning research, vol. 9, no. 11, 2008.\\n\\n[14] C. Finn, P. Abbeel, and S. Levine, \\\"Model-agnostic meta-learning for fast adaptation of deep networks,\\\" in International conference on machine learning. PMLR, 2017, pp. 1126\u20131135.\\n\\n[15] J. Kang, R. Liu, L. Li, Y. Cai, D. Wang, and T. F. Zheng, \\\"Domain-invariant speaker vector projection by model-agnostic meta-learning,\\\" in Interspeech, 2020.\\n\\n[16] H. Zhang, L. Wang, K. A. Lee, M. Liu, J. Dang, and H. Chen, \\\"Learning domain-invariant transformation for speaker verification,\\\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7177\u20137181.\\n\\n[17] Zhang, Hanyi and Wang, Longbiao and Lee, Kong Aik and Liu, Meng and Dang, Jianwu and Chen, Hui, \\\"Meta-learning for cross-channel speaker verification,\\\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 5839\u20135843.\\n\\n[18] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette, M. Marchand, and V. Lempitsky, \\\"Domain-adversarial training of neural networks,\\\" The journal of machine learning research, vol. 17, no. 1, pp. 2096\u20132030, 2016.\\n\\n[19] Q. Wang, W. Rao, S. Sun, L. Xie, E. S. Chng, and H. Li, \\\"Unsupervised domain adaptation via domain adversarial training for speaker recognition,\\\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4889\u20134893.\\n\\n[20] A. Kendall, Y. Gal, and R. Cipolla, \\\"Multi-task learning using uncertainty to weigh losses for scene geometry and semantics,\\\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7482\u20137491.\\n\\n[21] L. Liebel and M. Korner, \\\"Auxiliary tasks in multi-task learning,\\\" arXiv preprint arXiv:1805.06334, 2018.\\n\\n[22] G. Yang, S. Yang, K. Liu, P. Fang, W. Chen, and L. Xie, \\\"Multi-band melgan: Faster waveform generation for high-quality text-to-speech,\\\" in 2021 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2021, pp. 492\u2013498.\\n\\n[23] R. Yamamoto, E. Song, and J.-M. Kim, \\\"Parallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,\\\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6199\u20136203.\\n\\n[24] J. Kong, J. Kim, and J. Bae, \\\"Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\\\" Advances in Neural Information Processing Systems, vol. 33, pp. 17 022\u201317 033, 2020.\\n\\n[25] M. Morise, F. Yokomori, and K. Ozawa, \\\"World: a vocoder-based high-quality speech synthesis system for real-time applications,\\\" IEICE TRANSACTIONS on Information and Systems, vol. 99, no. 7, pp. 1877\u20131884, 2016.\\n\\n[26] D. W. Griffin and J. S. Lim, \\\"Multiband excitation vocoder,\\\" IEEE Transactions on acoustics, speech, and signal processing, vol. 36, no. 8, pp. 1223\u20131235, 1988.\\n\\n[27] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \\\"Focal loss for dense object detection,\\\" in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980\u20132988.\\n\\n[28] S.-i. Amari, \\\"Backpropagation and stochastic gradient descent method,\\\" Neurocomputing, vol. 5, no. 4-5, pp. 185\u2013196, 1993.\"}"}
