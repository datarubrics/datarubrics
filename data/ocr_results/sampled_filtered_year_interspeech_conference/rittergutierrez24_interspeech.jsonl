{"id": "rittergutierrez24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset Distillation (DD) has emerged, showing great promise for reducing training costs. DD aims to learn discriminative and informative samples and form a smaller synthetic dataset hoping to keep as much performance as the original one. DD deviates from the \\\"data-selection\\\" paradigm where a smaller dataset is created by selecting representative data points in the dataset. In contrast, DD learns abstract representations that convey the dataset's most discriminative information, which may or may not look realistic.\\n\\nDD is a popular emerging paradigm in Computer Vision (CV) yet it has not been explored for speech processing tasks. DD for speech processing introduces unique challenges due to the inherent differences between speech signals and images. Speech is a temporal signal with temporal dependencies. Hence, there is relevant information to distil across time. This paper proposes a first attempt of DD on speech processing task, aiming to 1) significantly reduce the disk storage requirement compared to the original dataset, 2) reduce training time computation on the downstream task, 3) make speaker identity harder to recover to enhance privacy and 4) alleviate data label imbalance. Such goals should be achieved without considerably hurting downstream model performance when training with the distilled dataset.\\n\\nSpeech emotion recognition (SER) task in the IEMOCAP dataset [14] is chosen as a case study for the following reasons. First, SER is an utterance-level classification task, where the variable length speech sequence is mapped into a single vector for classification. This is a favorable starting point to analyze the feasibility of this research direction on speech processing before extending the approach to speech tasks that make predictions over frames of the speech sequence. Second, while utterance-level classification makes the task more manageable, the subtleties needed to model emotions are challenging and interesting. The DD algorithm will need to convey discriminative information of a speech signal for ER classification.\\n\\nFig. 1 shows the usage scenario of the proposed method. Rather than training a downstream model with the original dataset, which requires expensive model training due to hyperparameter tuning, downstream architecture selection, and so on, we propose to learn a distribution that summarizes the training data, and that is controlled only by the emotion class labels. By learning a distribution that summarizes the training data across emotion labels, we do not need to retain a record of the original speech. Hence, our proposal implicitly enhances privacy. Nonetheless, this does not mean the proposal guarantees private generated representations. Once this summary distribution is learned as a generative model, a custom budget of samples per class can be generated to train downstream models, perform parameter tuning and so on. While training a generator incurs a cost, our proposal aims to provide a generator that replaces the dataset, meaning that training the generator is a once-for-all process.\\n\\nThe method, depicted in Fig 2, employs a Generative Adversarial Network (GAN) for DD in IEMOCAP, favored over a Diffusion Probabilistic Model (DPM) due to its smaller size, higher computational efficiency, and quicker on-the-fly data generation capabilities [15]. Nonetheless, GANs have been designed to generate real-looking data, differing from our goal of learning a summary distribution of the dataset useful for downstream training. Hence, to make the GAN learn discriminative information useful for downstream performance, we propose to bias the GAN by adding a term that minimizes the Kullback-Leibler (KL) divergence between the softmax probabilities of emotion classes of downstream forward passes between the real and synthetic data. We prevent the GAN from merely memorizing the softmax probability distribution by sampling from a variety of downstream model checkpoints, thereby introducing\\n\\nDataset-Distillation Generative Model for Speech Emotion Recognition\\nFabian Ritter-Gutierrez1,2, Kuan-Po Huang3,4, Jeremy H.M Wong2, Dianwen Ng1,5, Hung-yi Lee3, Nancy F. Chen1,2, Eng-Siong Chng1\\n1Nanyang Technological University, Singapore\\n2Institute for Infocomm Research (I2R), Singapore\\n3National Taiwan University, Taiwan\\n4ASUS Intelligent Cloud Services, Taiwan\\n5Speech Lab of DAMO Academy, Alibaba Group, Singapore\\n\\nInterspeech 2024\\n1-5 September 2024, Kos, Greece\\n10.21437/Interspeech.2024-1430\"}"}
{"id": "rittergutierrez24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a range of possible KL divergence targets. Furthermore, a di-\\nversity penalty term is added to make the GAN sample more\\ndata on smaller synthetic dataset sizes. To test the effi-\\nciency of the proposed method, we do ablations to see real data\\ntest set performance on IEMOCAP. The results obtained show\\nthat our proposal consistently maintains close accuracy perfor-\\nmance comparable to a model trained on the real IEMOCAP\\ndataset and it is consistently better than a GAN [16] trained\\nwithout our proposed criteria with statistical significance at a\\np-value of 0.05. The proposed method reduces the dataset size\\nand training time by 95% with minimal performance degrada-\\ntion. Additionally, it improves SER over the real data training\\nwhen our method samples balanced datasets. Hence, the pro-\\nposal alleviates data imbalance issues inherent in IEMOCAP.\\nFinally, this proposal implicitly decreases speaker identity in-\\nformation which fosters possibilities for privacy related appli-\\ncations.\\n\\nFigure 1: Usage scenario for DD on speech processing tasks.\\n\\nFigure 2: Schematic representation of the proposed DD. The\\nblue dashed lines represent the standard training of a GAN\\n\\n2. Related Work\\nWhile there is no work directly aiming to distill a dataset for\\nSER or any speech processing task, there is work that lever-\\nages GANs for data augmentation in SER. In [17], an uncon-\\ntditional and conditional GAN was trained for the IEMOCAP\\ndataset. [18], uses a conditional GAN to do mel-spectrogram\\naugmentation to improve performance on less representative\\nemotion classes for IEMOCAP. [19] investigates CycleGAN\\nfor emotion style transfer, aiming to generate realistic emo-\\ntion data. The study adds an evaluation of real test sets for\\nmodels trained on synthetic data only, revealing a performance\\ngap above 8% between training on real versus synthetic data.\\nThere are more similar works using GANs for data augmen-\\ntation such as [20\u201322] but with different GANs architectures\\nand some recent work has attempted speech emotion recogni-\\ntion data augmentation using denoising diffusion probabilistic\\nmodels (DDPM) [23].\\n\\n3. Dataset Distillation\\nGenerally speaking, DD aims to learn a small dataset that\\nachieves comparable performance to the original dataset that it\\nis distilling from. Let $T = (x_i, y_i)_{i=1}^{|T|}$ be the real dataset\\nconsisting on data-label pairs $x_i, y_i$ with $x_i \\\\in \\\\mathbb{R}^d$ with\\nd the feature dimension and $y_i \\\\in \\\\mathbb{R}^c$ with $c$ the number of\\nclasses. DD aims to create a synthetic dataset $S = (s_j, y_j)_{j=1}^{|S|}$\\n($|S| \\\\ll |T|$). Once $S$ is learned, the dataset is deployed to train\\na downstream model, and that model is evaluated on the real\\ndata test set.\\n\\nDD methods are based on three strategies [7]: i) per-\\nformance matching: monitoring performance achieved by a\\nneural network with the original dataset versus the synthetic\\ndataset [9]; ii) gradient matching: match the gradient in a neu-\\nral network of the original and synthetic dataset at each itera-\\ntion [10]; iii) distribution or feature matching: match the fea-\\ntures produced on a neural network for the real and synthetic\\ndata [11, 12]. In general, the algorithm will define a fixed bud-\\nget of number of elements per class when doing DD. Hence, if\\na different budget is needed, a whole DD training must be done\\nagain. Differently, the works [24, 25] distill CV datasets into a\\ngenerative model. Hence, rather than directly learning a dataset\\n$S$, they learn a generator $g$ that can sample different datasets\\nbased on a sample per class budget. Our proposal is motivated\\nby these ideas.\\n\\n4. Dataset Distillation for Speech Emotion\\nIn this paper, rather than directly learning a dataset $S$, a gener-\\native model $g$ is learnt to generate summary distributions of\\n$T$. Once $g$ is learnt, custom-defined samples per emotion can be\\ngenerated. Small-size generative models are designed, thereby\\nsignificantly decreasing storage requirements of the original\\ndataset as seen in Table 1.\\n\\nThe proposed approach consists of two stages, the first\\nstage is a standard GAN training, particularly the condi-\\ntional Wasserstein GAN implementation with gradient penalty\\n(WGAN-GP) [16]. In WGAN-GP, the discriminator $d_\\\\omega$, with\\n$\\\\omega$ the weight parameters, is optimized as,\\n\\n$$L_{D\\\\text{ADV}}(w) = E_{z \\\\sim P(z)}[d_\\\\omega(g_{\\\\phi}(z))] - E_{x \\\\sim P(x)}[d_\\\\omega(x)] + \\\\lambda_1 L_{GP}(w),$$\\n\\nwhere $g_{\\\\phi}$ is the generator parametrized by the weights\\n$\\\\phi$, $P(z), P(x)$ denotes the distribution of noise (latent) vectors and real\\nsamples respectively. A noise vector $z \\\\sim P(z)$ contains the\\ninformation of the label $y$ in the form of a one-hot vector,\\ni.e. $z \\\\equiv [y \\\\oplus e]$, with $\\\\oplus$ the concatenation operation and\\n$e \\\\sim \\\\mathcal{N}(0, 1)$.\\n\\nThe gradient penalty $L_{GP}(w)$ is needed to have a valid\\nWasserstein distance computation and $\\\\lambda_1$ controls the impor-\\ntance of this term. We use $\\\\lambda_1 = 10$ as in the original WGAN-\\nGP [16].\\n\\nThe generator in WGAN-GP is trained to minimize,\\n\\n$$L_{G\\\\text{ADV}}(\\\\phi) = -E_{z \\\\sim P(z)}[d_\\\\omega(g_{\\\\phi}(z))].$$\\n\\nAdditionally, motivated by speech processing research on mel-\\nspectrogram inversion [26,27], we add a feature matching (FM)\\nloss, shown to improve stability for generator training. The FM\\nloss is defined as,\\n\\n$$L_{FM}(g_{\\\\phi}, d_\\\\omega) = E_{x \\\\sim P(x)} z \\\\sim P(z) \\\\sum_{l=1}^{M} \\\\frac{1}{M} d(l)_w(x) - d(l)_w(g_{\\\\phi}(z)).$$\\n\"}"}
{"id": "rittergutierrez24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( d(l) \\\\) is the feature map at the \\\\( l \\\\)-th layer of the discriminator \\\\( dw \\\\), and \\\\( M \\\\) the number of layers. Eq. (3) helps the generator to sample features on the same space than the real data. Eq. (4) and (5) are designed to generate data that resembles the distribution of the emotion classes \\\\( y \\\\) with \\\\( CE \\\\) denoting the cross-entropy loss, \\\\( \\\\lambda \\\\) a scalar that bounds the diversity penalty for stability. Furthermore, inspired by [28], a diversity penalty is introduced to encourage the generation of a wider variety of samples. Rather than producing samples clustered around a mode, the goal is to span the support of the real distribution of the emotion classes \\\\( y \\\\) while retaining the information useful for downstream tasks.\\n\\nThe proposed softmax matching loss (SML) enforces the generator to memorize the logits distribution of a single model. Two small size GAN's architectures, GAN-CNN and GAN-ATT, are considered. The first, named GAN-CNN, is a CNN-based architecture composed of 8 2D-CNN layers, each featuring layer normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\nTable 1 analyzes the effect of training with a traditional WGAN-GP model utilizing solely convolutional layers (CNN) architectures are considered. The first, named GAN-CNN, is a CNN-based architecture composed of 8 2D-CNN layers, each featuring layer normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\nTo generate key discriminative information that serves downstream tasks rather than producing samples clustered around a mode, the goal is to span the support of the real distribution of the emotion classes \\\\( y \\\\) while retaining the information useful for downstream tasks. This is achieved by providing the conditioning class label information in the proposed DD method, using a diversity penalty for stability.\\n\\nThe semi-supervised learning setup is chosen and the SUPERB [29] framework is followed for easy reproducibility and comparison with real data training. Experiments are done with Unweighted Average Recall (UAR) evaluation on HuBERT Base [1] SSL representations and SSL feats followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\nTable 2 analyzes such results under a low points per class (ppc) budget of 50 ppc (5.6% of the size of the original dataset) and 100 ppc (11.2% of the size of the original dataset). Additionally, to analyze how the proposal scales to even bigger datasets, evaluations are done with Unweighted Average Recall (UAR) evaluation on HuBERT Base [1] SSL representations and SSL feats followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\n5.1. Implementation details\\n\\nTable 2 analyzes the effect of training with a traditional WGAN-GP model utilizing solely convolutional layers (CNN) architectures are considered. The first, named GAN-CNN, is a CNN-based architecture composed of 8 2D-CNN layers, each featuring layer normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\n5.2. GAN as a dataset distillator\\n\\nTable 1 are important when scaling up to bigger datasets. Two small size GAN's architectures, GAN-CNN and GAN-ATT, are considered. The first, named GAN-CNN, is a CNN-based architecture composed of 8 2D-CNN layers, each featuring layer normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category. The generator in GAN-CNN employs 2D CNN and transposed convolution layers followed by batch normalization and leaky-relu activation. The final CNN layer connects to two feed-forward layers: one calculates the Wasserstein distance, and the other predicts the class category.\\n\\nIn order to evaluate the efficiency of the discriminative information modelled, it is computed how much percentage of the original SSL features are not limited to the \\\\([-1,1]\\\\) range. There is no tanh operation at the generator's output, because the generator follows a softmax probability matching method that contains the information useful for downstream task prediction. For any sampled model checkpoint, this range is needed to avoid the GAN memorizing the logits distribution of a single model. This approach diverges from conventional uses of GANs for creating real-looking data. Instead, the focus is on harnessing GANs to generate key discriminative information that serves downstream tasks rather than producing samples clustered around a mode. In this paper, DD is defined as, \\\\( \\\\theta_{set} \\\\), where \\\\( \\\\Theta \\\\) represents the set of model checkpoints. For any sampled model checkpoint, this range is needed to avoid the GAN memorizing the logits distribution of a single model. This approach diverges from conventional uses of GANs for creating real-looking data. Instead, the focus is on harnessing GANs to generate key discriminative information that serves downstream tasks rather than producing samples clustered around a mode.\"}"}
{"id": "rittergutierrez24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: SER UAR (%\u2191) for downstream model trained only with generated data. Two generators are evaluated, GAN-CNN and GAN-ATT, under 50 points per class (ppc) and 100 ppc. Baseline denotes the GAN without DD criterions. \u2020 denotes a McNemar\u2019s test statistically significant difference over the Baseline. \u2021 denotes significance over the + L\\\\_SML model.\\n\\n| Model     | GAN-CNN  | GAN-ATT  |\\n|-----------|----------|----------|\\n| ppc       | 50       | 100      |\\n| UAR       | 47.75%   | 48.52%   |\\n| Baseline  |          |          |\\n| + L\\\\_DIV  |          |          |\\n| UAR       | 48.52%   | 53.79%   |\\n| + L\\\\_SML  |          |          |\\n| UAR       | 53.79%   | 54.99%   |\\n| + L\\\\_DIV + L\\\\_SML | | |\\n| UAR       | 54.99%   | 60.99%   |\\n\\nTable 3: UAR (%\u2191) test performance for real data training (Real SSL) and for GAN-ATT trained for balanced and imbalanced class labels distribution scenarios.\\n\\n| Method            | 2447 points | Full data |\\n|-------------------|-------------|-----------|\\n|                  | Balanced    | Imbalanced|\\n| Real SSL          | 64.09%      | -         |\\n| GAN-ATT           | 64.47%      | 63.21%    |\\n\\nbigger data samples, results with 800 ppc and 1800 ppc are included for GAN-ATT. All results are evaluated on the real data of Session 1 in IEMOCAP. From Table 2, it is evident that incorporating the proposed L\\\\_DIV and L\\\\_SML into the Baseline WGAN-GP significantly improves UAR across both generator architectures (GAN-CNN and GAN-ATT) and for both 50 ppc and 100 ppc dataset budgets. Furthermore, it can be seen that both terms are complementary. When comparing the Baseline with models using either L\\\\_DIV or L\\\\_SML individually, there is a noticeable improvement in performance. For instance, in the GAN-CNN architecture at 50 ppc, the UAR improves from 47.75% to 48.52% with L\\\\_DIV and to 53.79% with L\\\\_SML and to 54.99% when both terms are used together. For the GAN-ATT architecture, the trends are analogous. Interestingly, the GAN-ATT generator using both terms proposed for a budget of 11.2% of the original dataset size can achieve an UAR score of 61.95% which is only 2.25% less than the model trained with the full original training data (see first row in Table 3). For bigger data budgets, GAN-ATT surpasses the performance of the model trained with the original training set. Notably, two-tailed McNemar\u2019s test is performed at a p-value of 0.05 and results shows statistical significance for the L\\\\_SML and L\\\\_DIV + L\\\\_SML models when compared to the Baseline. Additionally, the Baseline + L\\\\_DIV + L\\\\_SML model is also statistically significant when compared with + L\\\\_SML only.\\n\\nTable 3 compares GAN-ATT\u2019s performance against real data training in both balanced and imbalanced scenarios. IEMOCAP is a well known imbalanced dataset, meaning that some classes are represented more than others. Training with imbalanced data may hurt performance and hence using a GAN to alleviate this issue may be of importance. Last column of Table 3 shows that using the same size than the original dataset but with balanced classes improves performance than training with the original dataset. On the other hand, Full data Imbalanced column assess GAN-ATT under the same class label distribution of the original train set which shows similar performance than the model trained with real SSL. Besides, in order to test the real data set in a balance scenario, we select all the datapoints from the minority emotion class (693 utterances) and randomly select 693 utterances for each of the rest of emotion classes. Similarly, we analyze performance of 693 ppc for the GAN-ATT (2447 points in total) and finally we see performances of real SSL and GAN-ATT under the imbalance scenario for 2447 datapoints. Findings in Table 3 suggest that the proposed method can be used to alleviate data label imbalance because GAN only training can improve performance versus real data training. Such results suggest that having a generative model that can modify the train data class label distribution is beneficial and is a strength of this proposed method. Finally, we noticed that using GAN data makes the downstream model quickly converge on the real validation set, making the model to be trained in less than 5 minutes. On the other hand, real data training convergence is slower, taking around 90 minutes to train which is nearly a 95% time reduction for downstream model training. This efficiency facilitates quicker hyperparameter optimization for downstream models, showcasing another advantage of our approach.\\n\\n5.3. On the privacy aspect\\n\\nAlthough this method does not inherently guarantee privacy, its use of GANs to learn SSL-like representations, conditioned solely on emotion labels, does not seem optimal to retain other forms of information. This section focuses on speaker identity, but similar arguments can be made about the retention of information such as content. The model\u2019s design, results in the generation of abstract representations that enhance downstream model performance for SER. This implicitly bolsters privacy by limiting the frame-level information necessary for accurate speech reconstruction. To assess the potential for retaining speaker information, we propose testing using the downstream model\u2019s first layer as a speaker embedding, a technique widely recognized in speaker identification (SID) studies [32, 33]. Table 4 shows such results for SUPERB SID task, where our proposed method reduces speaker information by 6.88% compared to the linear layer of a downstream model trained for SER with real SSL representations. While these results do not mean the GAN-ATT ensures privacy, it does mean there is an implicit reduction on speaker identity modelling which could serve as a starting point for explicitly training DD that ensures privacy. This will be investigated in future work.\\n\\nTable 4: SID Accuracy (Acc) with different speaker embeddings.\\n\\n| Speaker Embedding | SID downstream model | SER downstream model with Real SSL | SER downstream with Baseline WGAN-GP | SER downstream with Proposed GAN-ATT |\\n|-------------------|----------------------|----------------------------------|--------------------------------------|--------------------------------------|\\n| Acc (%\u2193)          |                      |                                  |                                      |                                      |\\n\\n6. Conclusions\\n\\nThis study introduced DD for SER by leveraging a GAN to generate datasets that are useful for downstream model training. A softmax probability matching loss is proposed to achieve such goal. Diversity penalty is proposed to sample more variety of synthetic datapoints. The method achieves performance on par compared to real data downstream model training while substantially reducing dataset size and downstream training time. Our method can alleviate data label imbalance. Our method as well carries less speaker information which could serve as a starting point for an application on privacy preserving dataset distillation. Future work will analyze this direction as well as scaling to bigger datasets.\"}"}
{"id": "rittergutierrez24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgments\\nI want to deeply thank my friend Nikita Kuzmin for the great discussions on the privacy aspect. Unfortunately, I end up not adding such results on this manuscript.\\n\\nThe computational work for this article was fully performed on resources of the National Supercomputing Centre, Singapore (https://www.nscc.sg).\\n\\n8. References\\n[1] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia et al., \\\"Hubert: Self-supervised speech representation learning by masked prediction of hidden units,\\\" in IEEE/ACM TASLP, 2021.\\n[2] S. Chen, C. Wang, Z. Chen, Y. Wu, et al., \\\"Wavlm: Large-scale self-supervised pre-training for full stack speech processing,\\\" in IEEE J-STSP, 2021.\\n[3] A. Mohamed, H. yi Lee, L. Borgholt, J. D. Havtorn et al., \\\"Self-supervised speech representation learning: A review,\\\" in IEEE J-STSP, 2022.\\n[4] Y. Peng, J. Tian, B. Yan et al., \\\"Reproducing whisper-style training using an open-source toolkit and publicly available data,\\\" in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2023, pp. 1\u20138.\\n[5] E. Strubell, A. Ganesh, and A. McCallum, \\\"Energy and policy considerations for deep learning in NLP,\\\" in ACL, 2019.\\n[6] J. Droppo and O. H. Elibol, \\\"Scaling laws for acoustic models,\\\" in Interspeech, 2021.\\n[7] R. Yu, S. Liu, and X. Wang, \\\"Dataset distillation: A comprehensive review,\\\" in IEEE TPAMI, 2023.\\n[8] K. Killamsetty, X. Zhao, F. Chen, and R. Iyer, \\\"Retrieve: Core-set selection for efficient and robust semi-supervised learning,\\\" NeurIPS, 2021.\\n[9] T. Wang, J.-Y. Zhu, A. Torralba, and A. A. Efros, \\\"Dataset distillation,\\\" arXiv preprint, 2018.\\n[10] B. Zhao, K. R. Mopuri, and H. Bilen, \\\"Dataset condensation with gradient matching,\\\" ICLR, 2021.\\n[11] B. Zhao and H. Bilen, \\\"Dataset condensation with distribution matching,\\\" in WACV, 2023.\\n[12] K. Wang, B. Zhao, X. Peng, Z. Zhu et al., \\\"Cafe: Learning to condense dataset by aligning features,\\\" in CVPR, 2022.\\n[13] D. Zhou, K. Wang, J. Gu, X. Peng, D. Lian et al., \\\"Dataset quantization,\\\" in ICCV, 2023.\\n[14] C. Busso, M. Bulut, C.-C. Lee, E. A. Kazemzadeh et al., \\\"Iemo-cap: interactive emotional dyadic motion capture database,\\\" Language Resources and Evaluation, 2008.\\n[15] X. Zhang, J. Wang, N. Cheng, and J. Xiao, \\\"Voice conversion with denoising diffusion probabilistic gan models,\\\" in ADMA, 2023.\\n[16] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, \\\"Improved training of wasserstein gans,\\\" NeurIPS, 2017.\\n[17] S. Sahu, R. Gupta, and C. Espy-Wilson, \\\"On enhancing speech emotion recognition using generative adversarial networks,\\\" in Interspeech, 2018.\\n[18] A. Chatziagapi, G. Paraskevopoulos, D. Sgouropoulos, G. Panta-zopoulos, M. Nikandrou et al., \\\"Data augmentation using gans for speech emotion recognition.\\\" in Interspeech, 2019.\\n[19] B. Fang, N. Michael, and V. N. Thang, \\\"Cyclegan-based emotion style transfer as data augmentation for speech emotion recognition.\\\" in Interspeech, 2019.\\n[20] Y. Lu and M. Man-wai, \\\"Adversarial data augmentation network for speech emotion recognition,\\\" in APSIPA ASC, 2019.\\n[21] L. Yi and M. wai Mak, \\\"Improving speech emotion recognition with adversarial data augmentation network,\\\" in IEEE TNNLS, 2022.\\n[22] H. Xiangheng, C. Junjie, R. Georgios, and S. B. W, \\\"An improved stargan for emotional voice conversion: Enhancing voice quality and data augmentation,\\\" in Interspeech, 2021.\\n[23] I. Malik, S. Latif, R. Jurdak, and B. Schuller, \\\"A preliminary study on augmenting speech emotion recognition using a diffusion model,\\\" in Interspeech, 2023.\\n[24] B. Zhao and H. Bilen, \\\"Synthesizing informative training samples with gan,\\\" in NeurIPS 2022 Workshop on Synthetic Data for Empowering ML Research, 2022.\\n[25] K. Wang, J. Gu, D. Zhou, Z. H. Zhu, W. Jiang, and Y. You, \\\"Dim: Distilling dataset into generative model,\\\" ArXiv, 2023.\\n[26] K. Kumar, R. Kumar, T. de Boissiere et al., \\\"Melgan: Generative adversarial networks for conditional waveform synthesis,\\\" in NeurIPS, 2019.\\n[27] J. Kong, J. Kim, and J. Bae, \\\"Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\\\" NeurIPS, 2020.\\n[28] D. Yang, S. Hong, Y. Jang, T. Zhao, and H. Lee, \\\"Diversity-sensitive conditional generative adversarial networks,\\\" in ICLR, 2019.\\n[29] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. Lai, K. Lakhotia et al., \\\"Superb: Speech processing universal performance benchmark,\\\" in Interspeech, 2021.\\n[30] J. Han, Z. Zhang, Z. Ren, F. Ringeval, and B. Schuller, \\\"Towards conditional adversarial training for predicting emotions from speech,\\\" ICASSP, 2018.\\n[31] F. Eyben, F. Weninger, F. Gross, and B. Schuller, \\\"Recent developments in opensmile, the munich open-source multimedia feature extractor,\\\" in ACM Multimedia, 2013.\\n[32] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \\\"X-vectors: Robust dnn embeddings for speaker recognition,\\\" ICASSP, 2018.\\n[33] T. Liu, K. A. Lee, Q. Wang, and H. Li, \\\"Disentangling voice and content with self-supervision for speaker recognition,\\\" NeurIPS, 2024.\"}"}
