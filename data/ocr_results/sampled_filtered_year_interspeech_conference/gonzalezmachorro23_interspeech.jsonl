{"id": "gonzalezmachorro23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] R. King, \\\"Atlas of ms, 3rd edition,\\\" The Multiple Sclerosis International Federation, 2020.\\n\\n[2] M. Barzegar, S. Najdaghi, A. Afshari-Safavi, N. Nehzat, O. Mir-mosayyeb, and V. Shaygannejad, \\\"Early predictors of conversion to secondary progressive multiple sclerosis,\\\" Multiple Sclerosis and Related Disorders, vol. 54, no. 103115, p. 103115, 2021.\\n\\n[3] J. W. L. Brown, A. Coles, D. Horakova, E. Havrdova, G. Izquierdo, A. Prat, M. Girard, P. Duquette, M. Trojano, A. Lugaresi, R. Bargar-maschi, P. Grammond, R. Alroughani, R. Hupperts, P. McCombe, V. Van Pesch, P. Sola, D. Ferraro, F. Grand'Maison, M. Terzi, J. Lechner-Scott, S. Flechter, M. Slee, V. Shaygannejad, E. Pucci, F. Granella, V. Jokubaitis, M. Willis, C. Rice, N. Scolding, A. Wilkins, O. R. Pearson, T. Ziemssen, M. Hutchinson, K. Harding, J. Jones, C. McGuigan, H. Butzkueven, T. Kalincik, N. Robertson, and MSBase Study Group, \\\"Association of initial disease-modifying therapy with later conversion to secondary progressive multiple sclerosis,\\\" JAMA, vol. 321, no. 2, pp. 175\u2013187, 2019.\\n\\n[4] G. Noffs, T. Perera, S. C. Kolbe, C. J. Shanahan, F. M. C. Boonstra, A. H. Evans, H. Butzkueven, A. van der Walt, and A. P. Vogel, \\\"What speech can tell us: A systematic review of dysarthria characteristics in multiple sclerosis,\\\" Autoimmunity reviews, vol. 17, no. 12, pp. 1202\u20131209, 2018.\\n\\n[5] A. V. Feij\u00b4o, M. A. Parente, M. Behlau, S. Haussen, M. C. de Vec-cino, and B. C. d. F. Martignago, \\\"Acoustic analysis of voice in multiple sclerosis patients,\\\" Journal of Voice, vol. 18, no. 3, pp. 341\u2013347, 2004.\\n\\n[6] L. Hartelius, B. Runmarker, and O. Andersen, \\\"Prevalence and characteristics of dysarthria in a multiple-sclerosis incidence cohort: Relation to neurological data,\\\" Folia Phoniatrica et Logopaedica, vol. 52, pp. 160\u2013177, 2000.\\n\\n[7] M. Milling, F. B. Pokorny, K. D. Bartl-Pokorny, and B. W. Schuller, \\\"Is speech the new blood? recent progress in ai-based disease detection from audio in a nutshell,\\\" Frontiers in Digital Health, vol. 4, 2022.\\n\\n[8] P. Hecker, N. Steckhan, F. Eyben, B. W. Schuller, and B. Arnrich, \\\"Voice analysis for neurological disorder recognition\u2013a systematic review and perspective on emerging trends,\\\" Frontiers in Digital Health, vol. 4, 2022.\\n\\n[9] B. Schuller, S. Steidl, A. Batliner, S. Hantke, F. H \u00a8onig, J. R. Orozco-Arroyave, E. N\u00a8oth, Y . Zhang, and F. Weninger, \\\"The interspeech 2015 computational paralinguistics challenge: Nativeness, parkinson's & eating condition,\\\" in Sixteenth Annual Conference of the International Speech Communication Association, 2015.\\n\\n[10] S. Luz, F. Haider, S. de la Fuente Garcia, D. Fromm, and B. MacWhinney, \\\"Alzheimer's dementia recognition through spontaneous speech,\\\" Frontiers in computer science, vol. 3, 2021.\\n\\n[11] M. P. McGinley, C. H. Goldschmidt, and A. D. Rae-Grant, \\\"Diagnosis and treatment of multiple sclerosis: a review,\\\" Jama, vol. 325, no. 8, pp. 765\u2013779, 2021.\\n\\n[12] B. G. Weinshenker, B. Bass, G. P. Rice, J. Noseworthy, W. Carriere, J. Baskerville, and G. C. Ebers, \\\"The natural history of multiple sclerosis: a geographically based study: I. clinical course and disability,\\\" Brain, vol. 112, no. 1, pp. 133\u2013146, 1989.\\n\\n[13] P. Vizza, D. Mirarchi, G. Tradigo, M. Redavide, R. B. Bossio, and P. Veltri, \\\"V ocal signal analysis in patients affected by multiple sclerosis,\\\" Procedia Computer Science, vol. 108, pp. 1205\u20131214, 2017.\\n\\n[14] K. M. Rosen, J. V . Gooz\u00b4ee, and B. E. Murdoch, \\\"Examining the effects of multiple sclerosis on speech production: does phonetic structure matter?\\\" J. Commun. Disord., vol. 41, no. 1, pp. 49\u201369, 2008.\\n\\n[15] V . Piacentini, I. Mauri, D. Cattaneo, M. Gilardone, A. Montesano, and A. Schindler, \\\"Relationship between quality of life and dysarthria in patients with multiple sclerosis,\\\" Archives of Physical Medicine and Rehabilitation, vol. 95, no. 11, pp. 2047\u20132054, 2014.\\n\\n[16] G. Noffs, F. M. C. Boonstra, T. Perera, H. Butzkueven, S. C. Kolbe, F. Maldonado, L. E. Cofre Lizama, M. P. Galea, J. Stankovich, A. Evans, A. van der Walt, and A. P. Vogel, \\\"Speech metrics, general disability, brain imaging and quality of life in multiple sclerosis,\\\" European Journal of Neurology, vol. 28, no. 1, pp. 259\u2013268, 2021.\\n\\n[17] J. Rusz, B. Benova, H. Ruzickova, M. Novotny, T. Tykalova, J. Hlavnicka, T. Uher, M. Vaneckova, M. Andelova, K. Novotna, L. Kadrnozkova, and D. Horakova, \\\"Characteristics of motor speech phenotypes in multiple sclerosis,\\\" Multiple Sclerosis and Related Disorders, vol. 19, pp. 62\u201369, 2018.\\n\\n[18] G. Gosztolya, L. T\u00b4oth, V . Svindt, J. B\u00b4ona, and I. Hoffmann, \\\"Using acoustic deep neural network embeddings to detect multiple sclerosis from speech,\\\" in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6927\u20136931.\\n\\n[19] P. Hecker, F. B. Pokorny, K. D. Bartl-Pokorny, U. Reichel, Z. Ren, S. Hantke, F. Eyben, D. M. Schuller, B. Arnrich, and B. W. Schuller, \\\"Speaking corona? human and machine recognition of covid-19 from voice,\\\" in Proceedings Interspeech 2021, 2021, pp. 1029\u20131033.\\n\\n[20] I. P. Association, Handbook of the International Phonetic Association: A guide to the use of the International Phonetic Alphabet. Cambridge University Press, 1999.\\n\\n[21] A. Kertesz, \\\"Western aphasia battery\u2013revised,\\\" 2007.\\n\\n[22] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr \u00b4e, C. Busso, L. Y . Devillers, J. Epps, P. Laukka, S. S. Narayanan, and K. P. Truong, \\\"The geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing,\\\" IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2016.\\n\\n[23] F. Eyben, M. W\u00a8ollmer, and B. Schuller, \\\"Opensmile: The munich versatile and fast open-source audio feature extractor,\\\" in Proceedings of the 18th ACM International Conference on Multimedia. Association for Computing Machinery, 2010, pp. 1459\u20131462.\\n\\n[24] W. Xue, C. Cucchiarini, R. van Hout, and H. Strik, \\\"Acoustic correlates of speech intelligibility: the usability of the egemaps feature set for atypical speech,\\\" in Proceedings SLaTE 2019: 8th ISCA Workshop on Speech and Language Technology in Education, 2019, pp. 48\u201352.\\n\\n[25] K. D. Bartl-Pokorny, F. B. Pokorny, A. Batliner, S. Amiriparian, A. Semertzidou, F. Eyben, E. Kramer, F. Schmidt, R. Sch\u00a8onweiler, M. Wehler, and B. W. Schuller, \\\"The voice of COVID-19: Acoustic correlates of infection in sustained vowels,\\\" The Journal of the Acoustical Society of America, vol. 149, no. 6, pp. 4377\u20134966, 2021.\\n\\n[26] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \\\"Scikit-learn: Machine learning in Python,\\\" Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.\\n\\n[27] A. Slegers, R.-P. Filiou, M. Montembeault, and S. M. Brambati, \\\"Connected speech features from picture description in alzheimer's disease: A systematic review,\\\" Journal of Alzheimer's Disease, vol. 65, no. 2, pp. 519\u2013542, 2018.\\n\\n[28] G. C. DeLuca, R. L. Yates, H. Beale, and S. A. Morrow, \\\"Cognitive impairment in multiple sclerosis: clinical, radiologic and pathologic insights,\\\" Brain Pathology, vol. 25, no. 1, pp. 79\u201398, 2015.\\n\\n[29] K. D. Mueller, B. Hermann, J. Mecollari, and L. S. Turkstra, \\\"Connected speech and language in mild cognitive impairment and alzheimer's disease: A review of picture description tasks,\\\" Journal of Clinical and Experimental Neuropsychology, vol. 40, no. 9, pp. 917\u2013939, 2018.\"}"}
{"id": "gonzalezmachorro23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features\\n\\nMonica Gonzalez-Machorro1,2*, Pascal Hecker1,2*, Uwe D. Reichel1, Helly N. Hammer3, Robert Hoepner3, Lisa Pedrotti3, Alisha Zmutt3, Hesam Sagha1, Johan van Beek4, Florian Eyben1, Dagmar M. Schuller1, Bj\u00f6rn W. Schuller1,5,6, Bert Arnrich2\\n\\n1 audEERING GmbH, Gilching, Germany\\n2 Digital Health \u2013 Connected Healthcare, Hasso Plattner Institute, University of Potsdam, Germany\\n3 Department of Neurology, Inselspital, Bern University Hospital and University of Bern, Freiburgstrasse, Bern, Switzerland\\n4 Biogen Switzerland AG, Baar, Switzerland\\n5 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany\\n6 GLAM \u2013 Group on Language, Audio, & Music, Imperial College, UK\\n\\nmgonzalez@audeering.com, phecker@audeering.com\\n\\nAbstract\\n\\nMultiple sclerosis (MS) is a neuroinflammatory disease that affects millions of people worldwide. Since dysarthria is prominent in people with MS (pwMS), this paper aims to identify acoustic features that differ between people with MS and healthy controls (HC). Additionally, we develop automatic classification methods to distinguish between pwMS and HC. In this work, we present a new dataset of a German-speaking cohort which contains 39 patients with low disability of relapsing MS and 16 HC. Findings suggest that certain interpretable speech features could be useful in diagnosing MS, and that machine learning methods could potentially support fast and unobtrusive screening in clinical practice. The study emphasises the importance of analysing free speech compared to read speech.\\n\\nIndex Terms: multiple sclerosis, speech analysis, dysarthria, German language, machine learning\\n\\n1. Introduction\\n\\nMultiple sclerosis (MS) is a neuroinflammatory disease that affects a population of around 2.8 million people worldwide [1]. In the course of this disease, the myelin sheath of the glia cells is getting attacked by the patient's immune system. The resulting lesions in the brain and spinal cord affect a variety of functions of the central nervous system.\\n\\nRelapsing MS (RMS) is the most common course of MS with a prevalence of around 80%, and consists of alternating periods of asymptomatic and symptomatic phases. This course could transition to secondary progressive MS (SPMS) [2, 3], a stage in which symptoms worsen gradually until complete disability.\\n\\nAmong others, roughly 45% of affected patients suffer from dysarthria [4] \u2013 making it the most common speech disorder among people with MS (pwMS). Dysarthria is reported to be one of the earliest symptoms of the disease in pwMS [5], and is a mix of ataxic and spastic speech characteristics [6].\\n\\nIn this paper, we describe a unique dataset with speech of fully ambulatory pwMS in the German language. The aim is to assess whether the voice can be used to reliably screen for pwMS with low disability. To do so, we employ three well-known speech tasks 1) to identify acoustic features that differ significantly between pwMS and healthy control (HC), and 2) to develop supervised machine learning (ML) methods to distinguish pwMS against HC.\\n\\nSpeech analysis for healthcare purposes yields the promise to enable fast and unobtrusive screening [7]. Small perturbations in the voice have been shown to correlate with various neurological disorders [8]. The research community has approached the quantification of neurological disorders with various challenges to assess the current state-of-the-art. Some exemplary approaches were to describe Parkinson's disease (PD) [9] and Alzheimer's disease (AD) [10].\\n\\nIn clinical practice, an initial screening system that supplements clinical routine shows great promise to support overburdened clinicians. This is because diagnosis encompasses multiple modalities, such as magnetic resonance imaging (MRI) and laboratory diagnostics [11]. In the context of MS, even though the disease manifests itself through specific brain lesions, the diagnosis usually depends on ruling out other diseases, since there are no specific screening tests [12].\\n\\n1.1. Related work\\n\\nSeveral studies have investigated speech changes in pwMS. Acoustic features that have been identified in pwMS are, among others, a loss of speech intensity and pitch control, vocal instability, higher jitter and shimmer, slower speech rate, compromised intelligibility, reduced breath support, as well as a decrease of the vowel space area [4, 8, 13]. An increase of harmonics-to-noise-ratio (HNR) has also been reported [13], which suggests a higher articulation effort. The maximum slope of the second formant (F2) is particularly affected in pwMS and is a good indicator to distinguish between mild and moderate dysarthria [14]. Studies have also concluded that dysarthria in pwMS is predominantly mild [15]. These findings suggest that certain speech features could be useful in diagnosing MS.\\n\\nFurther, the frequency and severity of dysarthria are associated with an expanded disability status scale (EDSS) score higher than 4 [16], even though speech impairment is present in all the disease stages [16]. The EDSS score depends on seven different functional systems: visual, brain-stem, pyramidal, cerebellar, sensory, bowel and bladder, and cerebral. An EDSS below 4 refers to patients who are ambulatory. An EDSS between 4 and 5 indicates a limited walking range. Finally, an EDSS of up to 8 relates to a requirement for walking assistance.\"}"}
{"id": "gonzalezmachorro23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Overview over the participants' metadata across the data partitions.\\n\\n|            | All RRMS | HC      | Whole dataset |\\n|------------|----------|---------|---------------|\\n| Participants| 55       | 16      | 39            |\\n| Gender     | 39F, 16M | 29F, 10M | 10F, 6M       |\\n| Age        | 36.62\u00b110.5 | 37.26\u00b110.38 | 35.06\u00b110.99   |\\n| EDSS       | -1.08\u00b11.0 | -       | -             |\\n\\nTrain set\\n\\n|            | 44       | 12      | 32            |\\n| Gender     | 29F, 15M | 7F, 5M  | 22F, 10M      |\\n| Age        | 37.2\u00b110.31 | 33.83\u00b110.07 | 38.47\u00b110.27   |\\n| EDSS       | -1.2\u00b11.0 | -       | -             |\\n\\nTest set\\n\\n|            | 11       | 4       | 7             |\\n| Gender     | 10F, 1M  | 3F, 1M  | 7F, 0M        |\\n| Age        | 34.27\u00b111.45 | 31.7\u00b119.67 | 38.75\u00b114.41   |\\n| EDSS       | 0.81\u00b11.19 | -       | -             |\\n\\nCompared to other neurodegenerative diseases, such as AD and PD, automatic classification of MS patients from a healthy cohort using ML methods appears to be less explored in the literature. Rusz et al. 2018 reported 78% accuracy between asymptomatic pwMS and control speakers using a linear support vector machine (SVM) [17]. Gosztolya et al. 2022 used deep neural network (DNN) embeddings to classify some specific non-word utterances of pwMS and achieved the accuracy of 90% [18].\\n\\nThis contribution is divided as follows: in section 2, we describe the dataset employed and the experiment design. The outcomes of these experiments are presented in section 3 and put into context in section 4. Section 5 provides the conclusions.\\n\\n2. Materials and Methods\\n\\n2.1. Data\\n\\nData was collected at the Department of Neurology, Inselspital, Bern, Switzerland and the University of Bern, Switzerland. This observational clinical study was approved by the ethics committee of the Canton of Bern, Switzerland (approval no. 2021-02423, ClinicalTrials.gov identifier: NCT05561621), which unfortunately does not permit the publication of the recorded data. All participants provided their written informed consent.\\n\\nThe recording setup consisted of a laptop computer and a microphone system: an AKG 555L headset condenser microphone and a PreSonus Studio 18 audio interface for A/D conversion. The native sampling rate was 44,100 Hz. The framework employed for voice recording was AI SoundLab [19], which is a web app, in which each patient could navigate through a voice recording session under the supervision of the study nurse. The voice recording session comprised several speech tasks, including verbal responses of the symbol digit modalities test (SDMT), sustained utterances, and diadochokinesis. Further included were free speech tasks and read speech tasks: the text passages 'North Wind and the Sun (NWS)' (German version) [20] and the 'Buttergeschichte', which are both widely used in the field of phonetics. Each session took around 60 minutes.\\n\\nFor brevity, within the scope of this paper, we addressed the following three speech tasks: the picture description task (PDT) of the Western Aphasia Battery \u2013 Revised [21], the NWS (read twice during each session: once at the very beginning and once at the very end), and all read speech tasks combined (NWS and Buttergeschichte). We selected these tasks since they are commonly used in similar neurodegenerative diseases and contain more speech segments, which can be useful for ML-based analysis.\\n\\nThe dataset consisted of 55 speakers, 39 pwMS, and 16 HC participants without any known neurological disorders. The inclusion criteria for the MS patients were as follows: 1) diagnosis with relapsing MS (RMS) due to its prominence, 2) age range 18-60 years, 3) ambulatory patients with low disability (EDSS score lower than four), 4) the capability of written informed consent, and 5) fluency in the German language.\\n\\n2.2. Data processing\\n\\nFor data processing, audio files were downsampled to 16 kHz and segmented by applying a voice activity detection (VAD) algorithm, which due to license restrictions is not available as open source, but its underlying architecture is based on the Speech & Music Interpretation by Large-space Extraction (openSMILE) interface. The minimum turn length was 0.76 s, the maximum turn length 6.0 s, the length of speech until a segment start was detected was set to 0.15 s, and the length of non-speech, until a segment end was detected, was set to 0.25 s. Manual audio quality checks were carried out before analyses. As a variable for the experiments, the signal volume was loudness-normalised with -24 dB LUFS.\\n\\nWe defined a fixed, hold-out test set, which was speaker-disjunct from the train set. We employ the variables sex and age to stratify each split. The test set corresponds to 20% of the data. An algorithm, that spawned 30 different combinations, was used to evaluate the quality of each split using the Jensen-Shannon divergence. The distance was measured between the target (MS/HC), sex, and age distribution in the train and test sets. The test split with the lowest distance from the train set was selected. The resulting test set has an information radius of 0.08. This approach was implemented to ensure that the test set was as representative as possible of the train set.\\n\\nTable 1 respectively summarises the participants' metadata across the different dataset partitions. Using the VAD algorithm, the following number of speech segments was obtained: a) PDT: 652 segments (train: 528, test: 124); b) read speech tasks: 2,348 segments (train: 1,874, test: 474); c) NWS: 1,133 segments (train: 899, test: 234).\\n\\n2.3. Methods\\n\\nThe extended Geneva minimalistic acoustic parameter set (eGeMAPS) [22] was extracted using the openSMILE feature extraction tool by the audEERING GmbH [23]. It contains 88 acoustic features and previous work has reported promising results in speech disorders [24]. Speech summary statistics of the eGeMAPS feature set were extracted. Summary statistics can be understood as aggregated information over an entire utterance. Examples of these are the mean, median, standard deviation (std), and skewness [22].\\n\\nThe statistical method employed for an exploratory feature analysis was a two-tailed Mann-Whitney U test, since valid...\\n\\n1 powered by audEERING GmbH\"}"}
{"id": "gonzalezmachorro23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Interpretable, significant acoustic features to distinguish MS from HC in the picture description task (PDT).\\n\\n| Features Description Interpretation | Interpretation | r | p-value |\\n|-------------------------------------|---------------|---|---------|\\n| Unvoiced Segment Length Mean of unvoiced segment lengths Phonatory control | | 0.38 | < 0.02 |\\n| Spectral Flux std Standard deviation of the distance between spectra of adjacent frames | Speech rate variation and hesitations | 0.28 | < 0.05 |\\n| Loudness peaks per second Number of loudness peaks per second | Speech rate | -0.27 | < 0.05 |\\n\\n\\nDue to space limitations, from the identified features the top 5 features are interpreted and discussed in this paper. For interpretability, the real \\\\( r \\\\) is reported. Table 2 shows the top five features obtained in the PDT. We portray the features for this speech task in detail since it is one of the most commonly used ones in similar neurodegenerative diseases such as AD [27]. The top five features for PDT are: arithmetic mean of F1 relative amplitude and F2 relative amplitude, unvoiced segment lengths, spectral flux std normalised by mean, and loudness peaks per second. In line with previous work, we expect to find that pwMS present a more unstable phonatory control, lower voice excitation strength, decrease in loudness, and lower speech rate compared to healthy speech. Table 2 presents the feature description and interpretation, \\\\( r \\\\) value, \\\\( U \\\\) statistic corresponding with the MS sample, and \\\\( p \\\\)-value.\\n\\n### 3.2. Predictive Modelling\\n\\nTable 3 shows the results of the predictive modelling approaches. Using all eGeMAPS features, the best-performing model reaches an UAR of 71.1% using a SVM on the PDT. NWS performance is higher with KNN. Figure 1 depicts the ROC curve of the model with the highest UAR. Due to SVM obtaining the highest...\"}"}
{"id": "gonzalezmachorro23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance of the predictive modelling approaches when only regarding the significantly correlating features described in section 3.1. Abbreviations as in Table 3.\\n\\n| Task      | Model | UAR | ROC | AUC | AP |\\n|-----------|-------|-----|-----|-----|----|\\n| PDT       | SVM   | 67.2% | 73.2% | 60.6% |    |\\n| Read Speech | SVM  | 65.8% | 74.1% | 61.7% |    |\\n| NWS       | SVM   | 64.3% | 71.4% | 59.0% |    |\\n\\nFigure 1: Receiver operating characteristic (ROC) curves of the best-performing model for the picture description task. 71.7% UAR, see Table 3. Green portrays the ROC curve of the model on all samples of the train partition and blue the one on all samples in the test partition.\\n\\n4. Discussion\\n\\nIn this study, we analysed acoustic features and ML models to distinguish between pwMS and HC when eliciting the PDT, NWS, and a combination of the NWS and the Buttergeschichte task. For initial screening, a dataset representing a cohort of patients that would be encountered in an ambulatory setting for initial diagnosis, is important. Therefore, we recruited a cohort of pwMS with a particularly low disability score (Table 1).\\n\\nFeature interpretability is key in speech analysis for healthcare, since it is needed for a potential clinical application, where patients and clinicians need to understand the speech changes in a particular disorder or disease. In this case, eGeMAPS is an adequate feature set, since it was curated by specialists in psychology, linguistics, medicine, and signal engineering [22].\\n\\nTherefore, from the top five features presented in Table 2, we found that F1 and F2 relative amplitude are lower for MS speech, which suggests that pwMS present a lower voice excitation strength than the HC. Due to muscle weakness being a common condition in MS, this would be the expected behaviour of MS speech. Furthermore, the lengths of unvoiced segments are higher in MS speech compared to healthy speech. This finding suggests that pwMS present lower phonatory control when switching from unvoicing to voicing. Nonetheless, a more indicative result could be found if the speech task would be more controlled, for example, a sustained utterance, as done in [25]. The std of spectral flux shows that pwMS have a higher speech rate variation, which suggests, among other things, that MS speech contains more filled pauses and unsystematic speech rate changes than HC. This finding is of special interest when regarding the PDT, since it hints at a higher cognitive load. This finding aligns with the fact that cognitive impairment is observed in MS even at early stages of the disease [28]. The std of spectral flux can also indicate varying amount of coarticulation. This might be explained by an increased proportion of articulatory target undershoot for pwMS. The final feature, loudness peaks per second, indicates a lower speech rate: results hint that MS presents lower speech rate and higher speech variation.\\n\\nAs for the ML experiments, models based on SVMs to distinguish pwMS and HC perform the best and reach an UAR of 71.7% for the PDT. This is slightly lower than the 78% accuracy, which was reported by [17]. Lower performance reported in our paper might be attributable to the use of diadochokinetic data in [17]. When regarding the subset of significant features (Table 4), performance decreased for the three speech tasks. This decline in performance might be due to excessively constraining the function space of the ML models.\\n\\nMoreover, the PDT performed best across the ML models. This result is consistent with previous work for other neurodegenerative diseases, in which the PDT was described as a valuable clinical tool [27, 29]. Thus, our findings suggest that the PDT, a free speech task, better embodies speech differences between pwMS and HC than the employed read speech tasks.\\n\\n5. Conclusions\\n\\nIn this study, we demonstrated that speech analysis can be used as a tool to distinguish between the speech of pwMS with low disability and HC. By using acoustic features, we reached an UAR of up to 71.1%. Feature interpretation of low disability MS speech is consistent with related work and dysarthric speech characteristics. Therefore, this paper can be considered as a step towards an assistive tool for clinicians, in which acoustic features could be used as additional tools to support MS diagnosis.\\n\\nFuture work will focus on including further participants and longitudinal data collection for monitoring MS progression. In order to have an initial screening tool based on speech analysis, it is important to investigate the difference between patients with low and high EDSS scores and to further investigate model interpretability to understand resulting predictions. It will also be valuable to identify specific features for pwMS in the German language and across multiple languages. Future work should also systematically study speech characteristics of different MS types.\\n\\n6. Acknowledgements\\n\\nWe thank all speakers, who donated their data and the AI Sound-Lab team for customising the platform for this data collection. Thanks to Moritz Jacobshagen and Johan van Beek for coordinating Biogen's sponsorship of the study.\"}"}
