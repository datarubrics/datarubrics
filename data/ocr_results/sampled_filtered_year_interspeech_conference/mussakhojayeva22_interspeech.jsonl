{"id": "mussakhojayeva22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] B.-H. Juang and L. R. Rabiner, \u201cAutomatic speech recognition\u2013a brief history of the technology development.\u201d\\n\\n[2] D. Yu and L. Deng, *Automatic Speech Recognition: A Deep Learning Approach*. Springer Publishing Company, 2014.\\n\\n[3] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P. Nguyen, T. N. Sainath et al., \u201cDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,\u201d IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82\u201397, 2012.\\n\\n[4] A. Graves and N. Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in Proc. of the International Conference on Machine Learning, ICML 2014, Beijing, China, 21-26 June 2014, vol. 32, 2014, pp. 1764\u20131772. [Online]. Available: http://proceedings.mlr.press/v32/graves14.html\\n\\n[5] A. Hannun, C. Case, J. Casper, B. Catanzaro, G. Diamos, E. Elsen, R. Prenger, S. Satheesh, S. Sengupta, A. Coates et al., \u201cDeep speech: Scaling up end-to-end speech recognition,\u201d arXiv preprint arXiv:1412.5567, 2014.\\n\\n[6] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \u201cListen, attend and spell: A neural network for large vocabulary conversational speech recognition,\u201d in Proc. of the IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2016, Shanghai, China, March 20-25, 2016. IEEE, 2016, pp. 4960\u20134964. [Online]. Available: https://doi.org/10.1109/ICASSP.2016.7472621\\n\\n[7] L. Besacier, E. Barnard, A. Karpov, and T. Schultz, \u201cAutomatic speech recognition for under-resourced languages: A survey,\u201d Speech Communication, vol. 56, pp. 85\u2013100, 2014.\\n\\n[8] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, \u201cAISHELL-1: an open-source mandarin speech corpus and a speech recognition baseline,\u201d in Proc. of the Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment, O-COCOSDA 2017, Seoul, South Korea, November 1-3, 2017. IEEE, 2017, pp. 1\u20135. [Online]. Available: https://doi.org/10.1109/ICSDA.2017.8384449\\n\\n[9] B. Dave, *Kazakhstan-ethnicity, language and power*. Routledge, 2007.\\n\\n[10] Y. Khassanov, S. Mussakhojayeva, A. Mirzakhmetov, A. Adiyev, M. Nurpeiissov, and H. A. Varol, \u201cA crowdsourced open-source Kazakh speech corpus and initial speech recognition baseline,\u201d in Proc. of the European Chapter of the Association for Computational Linguistics (EACL). Online: Association for Computational Linguistics, Apr. 2021, pp. 697\u2013706. [Online]. Available: https://aclanthology.org/2021.eacl-main.58\\n\\n[11] S. Mussakhojayeva, Y. Khassanov, and H. A. Varol, \u201cKazakhTTS2: Extending the open-source kazakh TTS corpus with more data, speakers, and topics,\u201d arXiv preprint arXiv:2201.05771, 2022.\\n\\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proc. NIPS, 2017, pp. 5998\u20136008.\\n\\n[13] R. Yeshpanov, Y. Khassanov, and H. A. Varol, \u201cKazN-ERD: Kazakh named entity recognition dataset,\u201d arXiv preprint arXiv:2111.13419, 2021.\\n\\n[14] S. Mussakhojayeva, A. Janaliyeva, A. Mirzakhmetov, Y. Khassanov, and H. A. Varol, \u201cKazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset,\u201d in Proc. Interspeech 2021, 2021, pp. 2786\u20132790.\\n\\n[15] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Hendretty, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon Voice: A massively-multilingual speech corpus,\u201d in LREC. ELRA, 2020, pp. 4218\u20134222.\\n\\n[16] S. Mussakhojayeva, Y. Khassanov, and H. A. Varol, \u201cA study of multilingual end-to-end speech recognition for Kazakh, Russian, and English,\u201d in Proc. of the International Conference on Speech and Computer, SPECOM 2021, St. Petersburg, Russia, September 27-30, 2021, Proceedings, ser. Lecture Notes in Computer Science, A. Karpov and R. Potapova, Eds., vol. 12997. Springer, 2021, pp. 448\u2013459. [Online]. Available: https://doi.org/10.1007/978-3-030-87802-3\\n\\n[17] A. Slizhikova, A. Veysov, D. Nurtdinova, and D. Voronin, \u201cRussian open speech to text dataset,\u201d https://github.com/snakers4/openstt, accessed: 2021-01-15.\\n\\n[18] Y. Shi, A. Hamdullah, Z. Tang, D. Wang, and T. F. Zheng, \u201cA free Kazakh speech database and a speech recognition baseline,\u201d in Proc. of the Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), Kuala Lumpur, Malaysia, December 12-15, 2017, 2017, pp. 745\u2013748. [Online]. Available: https://doi.org/10.1109/APSIPA.2017.8282133\\n\\n[19] O. Makhambetov, A. Makazhanov, Z. Yessenbayev, B. Matkarimov, I. Sabyrgaliyev, and A. Sharafudinov, \u201cAssembling the Kazakh language corpus,\u201d in Proc. of the Conference on Empirical Methods in Natural Language Processing (EMNLP), 18-21 October 2013, Seattle, Washington, USA, 2013, pp. 1022\u20131031. [Online]. Available: https://www.aclweb.org/anthology/D13-1104/\\n\\n[20] O. Mamyrbayev, K. Alimhan, B. Zhumazhanov, T. Turdalykyzy, and F. Gusmanova, \u201cEnd-to-end speech recognition in agglutinative languages,\u201d in Proc. of the Asian Conference on Intelligent Information and Database Systems (ACIIDS), Phuket, Thailand, March 23-26, 2020, N. T. Nguyen, K. Jearanthanakij, A. Selamat, B. Trawinski, and S. Chittayasothorn, Eds., vol. 12034, 2020, pp. 391\u2013401. [Online]. Available: https://doi.org/10.1007/978-3-030-42058-1\\n\\n[21] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proc. of the Annual Conference of the International Speech Communication Association (INTERSPEECH), Hyderabad, India, 2-6 September 2018, 2018, pp. 2207\u20132211. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2018-1456\\n\\n[22] D. S. Park, W. Chan, Y. Zhang, C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecaugment: A simple data augmentation method for automatic speech recognition,\u201d in Proc. of the Annual Conference of the International Speech Communication Association (INTERSPEECH), Graz, Austria, 15-19 September 2019, G. Kubin and Z. Kacic, Eds., 2019, pp. 2613\u20132617. [Online]. Available: https://doi.org/10.21437/Interspeech.2019-2680\\n\\n[23] A. Graves, S. Fern\u00e1ndez, F. J. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. of the International Conference on Machine Learning, (ICML), Pittsburgh, Pennsylvania, USA, June 25-29, 2006, W. W. Cohen and A. W. Moore, Eds., 2006, pp. 369\u2013376. [Online]. Available: https://doi.org/10.1145/1143844.1143891\"}"}
{"id": "mussakhojayeva22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus\\n\\nSaida Mussakhojayeva, Yerbolat Khassanov, Huseyin Atakan Varol\\n\\nInstitute of Smart Systems and Artificial Intelligence (ISSAI), Nazarbayev University, Nur-Sultan, Kazakhstan\\n\\nAbstract\\n\\nWe present the first industrial-scale open-source Kazakh speech corpus for automatic speech recognition research and development. Our corpus subsumes two previously presented corpora: 1) Kazakh speech corpus (KSC) and 2) Kazakh text-to-speech 2 (KazakhTTS2). We also provide additional data from other sources, including television news, television and radio programs, parliament speeches, and podcasts. Our corpus, which we have named KSC2, contains over a thousand hours of high-quality transcribed data, which is triple the size of KSC. KSC2 was manually transcribed with the help of native Kazakh speakers and validated via preliminary speech recognition experiments on various evaluation sets. Moreover, it contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. We believe that our corpus will facilitate speech processing research for Kazakh, which is widely considered an under-resourced language. To ensure the reproducibility of experiments, we share the KSC2 corpus, training recipes, and pretrained models.\\n\\nIndex Terms: speech corpus, Kazakh, speech recognition, streaming ASR, spontaneous, code-switching, agglutinative\\n\\n1. Introduction\\n\\nAutomatic speech recognition (ASR), an essential step towards natural human\u2013machine interaction, has been an intensive research area for decades [1]. However, interest in speech recognition technologies has surged in the past decade, largely due to the increased demand for speech-enabled applications, such as voice search, voice command, message dictation, and virtual assistants (e.g., Apple's Siri, Google Now, Microsoft's Cortana, Amazon Alexa, and Samsung's Bixby) [2].\\n\\nThe success of ASR can be attributed to the newly developed deep learning approaches, which are often considered data hungry [3, 4, 5, 6]. While industry practitioners can mostly meet the data requirement for building robust deep learning-based ASR architectures, it is usually not the case for academic researchers, especially in regard to low-resource languages [7]. Consequently, industry mainly focuses on ASR research using large-scale datasets that are unavailable to academia. This leads to a research divergence between industry and academia, as discoveries made in the latter are often inapplicable or ineffective in industry-scale ASR applications [8].\\n\\nIn this work, we focus on Kazakh speech recognition. Specifically, we aim to bridge the gap between industry and academia by introducing a large-scale open-source speech corpus. Kazakh is the official language of Kazakhstan, spoken by over 13 million people worldwide [2]. It is an agglutinative language belonging to the family of Turkic languages. During the Soviet era, Kazakh was overshadowed by the Russian language, but, with the help of various government and community initiatives, it is currently gaining momentum [9].\\n\\nRecently, two major open-source speech corpora for Kazakh have been introduced: 1) Kazakh speech corpus (KSC) [10] and 2) Kazakh text-to-speech 2 (KazakhTTS2) [11]. KSC was built for ASR applications and contains around 332 hours of transcribed data crowdsourced from over a thousand volunteers. KazakhTTS2, on the other hand, was mainly built for speech synthesis applications and contains around 271 hours of transcribed data collected from five professional speakers (three females and two males). The main disadvantage of KSC and KazakhTTS2 is that they consist of read speech only, which might be ineffective for building spontaneous speech recognition systems. Additionally, they do not contain utterances with Kazakh-Russian code-switching.\\n\\nTo address the limitations of existing work, we introduce a new speech corpus called KSC2. KSC2 subsumes the KSC and KazakhTTS2 corpora and further extends them with 238 and 48 hours of transcribed data, respectively. Additionally, KSC2 incorporates 238 hours of newly collected data from other sources, such as television news, television and radio programs, parliament speeches, and podcasts. In total, KSC2 contains around 1,128 hours of high-quality transcribed data. Notably, to the best of our knowledge, KSC2 is the first publicly available speech corpus containing Kazakh-Russian code-switching utterances.\\n\\nTo validate the reliability of KSC2, we conducted preliminary speech recognition experiments using Transformer-based ASR, where promising word error rate (WER) results were achieved. To enable experiment reproducibility and facilitate future research, the KSC2 corpus, training recipes, and pretrained models have been made publicly available [1]. We hope that our corpus will stimulate the development of Kazakh speech-enabled applications and promote speech processing research for other Turkic languages.\\n\\nThe rest of the paper is organized as follows: Section 2 reviews previous work on the development of Kazakh language resources. Section 3 describes the specifications of KSC2. Section 4 describes the experimental setup and presents the recognition results obtained. Section 5 discusses future work and concludes the paper.\\n\\n2. Related Work\\n\\nThe Kazakh language is widely considered under-resourced due to the lack of freely available corpora. To address this problem, in recent years, several open-source resources for the Kazakh language have been developed. For example, Yeshpanov et al. [13] developed an open-source Kazakh named entity recognition (NER) tool, which is available online.\\n\\nHowever, the majority of Kazakhs are bilingual; hence, Kazakh-Russian code-switching is frequently practised in daily communication. To address this, we include Kazakh-Russian code-switching utterances in our corpus, which is a unique feature compared to the existing corpora.\"}"}
{"id": "mussakhojayeva22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The most prominent open-source speech corpus suitable for building a Kazakh ASR is KSC [10]. KSC consists of 332 hours of transcribed data crowdsourced from over a thousand volunteers. Specifically, it was collected over the Internet, with volunteers asked to read sentences displayed in a web browser, an approach similar to Common Voice [15]. Although KSC is substantially larger than any other freely available dataset in Kazakh, it consists only of read speech recordings, which are ineffective for building spontaneous speech recognition systems. Additionally, this corpus lacks Kazakh-Russian code-switching utterances.\\n\\nTo alleviate the Kazakh-Russian code-switching problem, Mussakhojayeva et al. [16] introduced the OpenSTT-CS334 corpus, a 334-hour clean subset of the OpenSTT dataset [17]. OpenSTT-CS334 contains recordings in the Russian language from audiobooks and YouTube that were manually re-segmented and re-transcribed by fluent Russian speakers. Although OpenSTT-CS334 can be used in combination with other Kazakh speech corpora to address inter-sentential code-switching, it is ineffective for intra-sentential cases, where language switching occurs within a single sentence.\\n\\nThere have been other speech corpora developed for Kazakh ASR. For example, Shi et al. [18] introduced a 78-hour open-source corpus recorded with the help of 96 students from Xinjiang University in China. The main limitation of the corpus is that it contains reading-style recordings only and is non-representative of a general public, as the speakers belong to the same social group (i.e., age group, social status, region, etc.). In another work, Makhambetov et al. [19] developed a Kazakh speech corpus containing around 40 hours of transcribed read speech data recorded in a sound-proof studio. Similarly, Mamyrbayev et al. [20] collected 123 hours of data using a professional recording booth. The IARPA Babel project has released a Kazakh language pack consisting of around 50 hours of conversational and 14 hours of scripted telephone speech. Unfortunately, the aforementioned speech corpora are either publicly unavailable or of an insufficient size to build robust Kazakh ASR systems.\\n\\n### 3. KSC2 Specifications\\n\\nIn this section, we describe the specifications of KSC2, including its structure and statistics. The KSC2 project was conducted with the approval of the Institutional Research Ethics Committee of Nazarbayev University.\\n\\n#### 3.1. Structure\\n\\nKSC2 consists of two previously released speech corpora and additional data collected from various sources. The two corpora are KSC and KazakhTTS2, which are thoroughly described in the subsections to follow. The additional data include television news, television and radio programs, parliament speeches, and podcasts, which are also described below. Note that all recordings of KSC2 were downsampled to 16 kHz. All recordings were also manually transcribed by native Kazakh and Russian speakers, except for the crowdsourced recordings, which were partially checked using our ASR system.\\n\\n#### 3.1.1. KSC\\n\\nThe original KSC [10] contained over 332 hours of data collected via a web-based speech recording platform that asked volunteers to read sentences extracted from books, laws, Wikipedia, news portals, and blogs. KSC is also highly diverse in terms of speakers and audio characteristics, as it consists of recordings submitted using various devices (e.g., smartphone, tablet, laptop, etc.) from different parts of Kazakhstan. Since the initial release of KSC, we have crowdsourced an additional 238 hours of data, bringing the total size to 570 hours. The KSC recordings were partially checked by an ASR system. Specifically, the system automatically accepted utterances that were recognized with a character error rate (CER) of 0%, while the remaining utterances were left to human transcribers.\\n\\n#### 3.1.2. KazakhTTS2\\n\\nKazakhTTS2 [11] contains 271 hours of transcribed data recorded by five professional speakers (three females and two males) using high-quality microphones. The speakers were assigned to read a book, Wikipedia and news articles covering various topics, such as sports, business, politics, history, science, and so on. In this work, we collected another 48 hours with the help of the existing speakers, increasing the total size to 319 hours. Although the corpus was built for speech synthesis applications, it can be used for the ASR task as well.\\n\\n#### 3.1.3. Television news\\n\\nThe television news recordings were obtained from the Khabar Agency, a major broadcasting network in Kazakhstan. With the agency's permission, we extracted news bulletins in Kazakh posted on its official website. The news included reports on local and international politics, economy, sports, religion, culture, and education. The news recordings are mostly read speech, except some interviews taken from public officials and people. Overall, we transcribed around 104 hours of news data.\\n\\n#### 3.1.4. Television programs\\n\\nThe television programs consist of a single talk show in Kazakh in which hosts and guests discuss important social, political, or religious issues and events. The talk show is mostly in the form of an interview or a simple conversation. Although the discussion topics of the talk show were predetermined, the conversations were mostly spontaneous. Additionally, the conversations contain Kazakh-Russian code-switching and overlapping speech. We extracted 7 episodes of the talk show, each episode of which lasts from several minutes up to several hours. The total length of television programs is around 6 hours.\\n\\n5https://khabar.kz\"}"}
{"id": "mussakhojayeva22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Source          | Speech style | CS rate | Duration | Utterances | Tokens | Unique tokens |\\n|-----------------|--------------|---------|----------|------------|--------|---------------|\\n| KSC             | Train        | 1.9%    | 556.6 hr | 265,035    | 2,948.0k | 193.6k        |\\n|                 | Valid        | 1.8%    | 7.1 hr   | 3,282      | 35.3k  | 13.5k         |\\n|                 | Test         | 1.9%    | 7.1 hr   | 3,333      | 35.9k  | 13.9k         |\\n| KazakhTTS2      | Train        | 1.0%    | 319.5 hr | 160,384    | 2,072.1k | 107.4k        |\\n|                 | Television news | 1.7% | 98.2 hr | 79,962     | 755.7k | 66.3k        |\\n|                 | Valid        | 1.7%    | 2.9 hr   | 2,426      | 22.7k  | 7.5k          |\\n|                 | Test         | 1.3%    | 2.9 hr   | 2,618      | 23.4k  | 7.9k          |\\n| Television programs | Train     | 10.2%  | 5.0 hr  | 3,436      | 38.6k  | 9.2k          |\\n|                 | Valid        | 5.1%    | 0.7 hr   | 487        | 5.9k   | 1.9k          |\\n|                 | Test         | 6.3%    | 0.7 hr   | 399        | 6.2k   | 2.1k          |\\n| Radio programs  | Train        | 5.8%    | 46.9 hr  | 35,060     | 334.9k | 36.8k        |\\n|                 | Valid        | 5.8%    | 0.9 hr   | 786        | 7.5k   | 2.6k          |\\n|                 | Test         | 3.1%    | 0.9 hr   | 755        | 7.8k   | 2.5k          |\\n| Parliament      | Train        | 3.7%    | 19.5 hr  | 9,508      | 138.1k | 23.7k        |\\n|                 | Valid        | 2.1%    | 1.48 hr  | 723        | 9.3k   | 3.6k          |\\n|                 | Test         | 2.5%    | 1.76 hr  | 865        | 11.5k  | 4.0k          |\\n| Podcasts        | Train        | 9.6%    | 51.0 hr  | 31,561     | 394.5k | 42.3k        |\\n|                 | Valid        | 9.0%    | 1.9 hr   | 1,111      | 15.5k  | 4.2k          |\\n|                 | Test         | 8.6%    | 2.9 hr   | 1,564      | 22.1k  | 5.5k          |\\n| Total           | Train        | 2.4%    | 1,096.6 hr | 504,984     | 5,926.2k | 279.3k     |\\n|                 | Valid        | -       | 3.2%    | 8,815      | 96.2k  | 23.5k        |\\n|                 | Test         | 3.2%    | 16.3 hr  | 9,534      | 106.9k | 25.1k        |\\n\\n3.1.5. Radio programs\\nThe radio programs consist of a single show hosted in Kazakh and contain news, music, call-ins (i.e., telephone conversations with the audience), interviews, and discussions with guests. In total, we extracted 93 episodes of the show, with each episode lasting from a few minutes to an hour. Code-switching and speech overlap are present. The overall size of the transcribed data is around 48 hours.\\n\\n3.1.6. Parliament speeches\\nThe parliament speeches consist of talks given by deputies. From the official YouTube channel of the Parliament, we extracted a total of 20 plenary sessions that took place in different time periods. The plenary sessions last up to a few hours, with each session consisting of several talks delivered either in Kazakh or Russian. The total length of the parliament speeches is around 23 hours, with most of the speeches being in reading style, as deputies usually read prepared scripts.\\n\\n3.1.7. Podcasts\\nThe podcasts consist of conversations in Kazakh in which a host (or hosts) and a guest (or guests) engage in a discussion on a particular topic or a current event. Some of the podcasts are carefully scripted, while others are completely improvised. Kazakh-Russian code-switching and speech overlap are present in this stream of data. In total, we extracted 155 episodes from nine podcast programs, comprising around 56 hours of transcribed data.\\n\\n3.2. Statistics\\nThe statistics for KSC2 are given in Table 1. The total size of the corpus is around 1,128 hours of transcribed audio, comprising over 520 thousand utterances. Crowdsourced data constitute the largest portion of the recordings (i.e., KSC), while the smallest portion is comprised of television programs. To enable experiment reproducibility, we split each source into training, validation, and test sets. Note that, for the KSC source, the validation and test sets are the same as in [10]. We did not generate validation and test sets for KazakhTTS2, because it contains only five speakers. Although we tried to minimize the speaker overlap between different sets, this is not fully guaranteed. The total number of unique token types among the three sets is around 300 thousand, which demonstrates the broad vocabulary coverage of the corpus.\\n\\nFor each source, we also computed the rate of sentences with code-switching. The rate was computed by counting sentences consisting of words in both Kazakh and Russian. We observed a non-zero rate in some completely Kazakh sources. We assume that this is due to loanwords. The highest code-switching rate was found in podcasts, television and radio programs.\\n\\nSince intra-word code-switching is also widespread among Kazakh speakers, we counted Kazakh words with Russian suffixes as Russian, and Russian words with Kazakh suffixes as Kazakh.\"}"}
{"id": "mussakhojayeva22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The CER and WER results on various evaluation sets obtained by Transformer-based streaming ASR trained using KSC2\\n\\n| Source                  | CER (%) | WER (%) |\\n|-------------------------|---------|---------|\\n| Valid Test              |         |         |\\n| KSC                     | 2.3     | 1.7     |\\n| Television news         | 2.2     | 2.2     |\\n| Television programs     | 14.9    | 19.0    |\\n| Radio programs          | 11.6    | 11.4    |\\n| Parliament              | 3.4     | 3.8     |\\n| Podcasts                | 13.8    | 13.4    |\\n| Overall                 | 5.6     | 5.9     |\\n\\n4. Speech Recognition Experiments\\n\\nIn this section, we describe the speech recognition experiments conducted to demonstrate the utility of the KSC2 corpus.\\n\\n4.1. Experimental Setup\\n\\nWe used the ESPnet framework [21] to build Transformer-based ASR models. The ASR models were trained on A100 GPUs running on the NVIDIA DGX A100 server using the training sets. The hyper-parameters were tuned using the validation sets, and the best-performing model was evaluated on the test sets. We did not use any external data and did not apply n-best and lattice rescoring.\\n\\nThe Transformer-based ASR system consisted of 18 encoder and six decoder blocks. We set the number of heads in the self-attention layer to eight with 512-dimension hidden states and the feed-forward network dimensions to 2,048. To avoid overfitting, we set the dropout rate to 0.1 and augmented the training data using spectral augmentation [22]. The model was trained for 150 epochs. We report the results of an average model constructed using the best 10 checkpoints. The Transformer model was jointly trained with the connectionist temporal classification (CTC) [23] objective function under the multi-task learning framework. The interpolation weight for the CTC objective was set to 0.3. Additionally, during the decoding stage, we employed a character-level language model (LM). The LM was built using the training set transcripts as a 16-layer Transformer network with eight attention heads, 512-dimension hidden states, and a 2,048-dimension feed-forward network. The other details of the model configuration can be found in our GitHub repository.\\n\\n4.2. Experiment Results\\n\\nThe results of the speech recognition experiment are given in Table 2. The overall WER result on the test set is 15.6%. As expected, the lowest WER results are obtained for the reading-style sources (i.e., KSC, television news, and parliament speeches), while the WER results for the spontaneous-style sources are over 27%. The poor WER results for the spontaneous sources can be attributed to the high code-switching rate, the lack of corresponding data, and the spontaneous nature of these sources, which is considered more challenging. It is also important to mention that the WER result on the KSC test set is 6.3%, which is an absolute improvement of 2.4% over the previous work [10]. These results successfully demonstrate the utility of the KSC2 corpus for the speech recognition task.\\n\\nTable 3: The data efficiency experiment results on overall validation and test sets obtained by Transformer-based streaming ASR\\n\\n| Training data                  | CER (%) | WER (%) |\\n|--------------------------------|---------|---------|\\n| Valid Test                     |         |         |\\n| KSC                            | 11.1    | 12.1    |\\n| + KazakhTTS2                   | 10.4    | 11.5    |\\n| + Television news              | 8.0     | 8.8     |\\n| + Others                       | 5.6     | 5.9     |\\n\\nWe also conducted the data efficiency experiment to analyze the performance improvement due to the addition of new sources. The experiment results in Table 3 indicate that the addition of new sources is effective. For example, the addition of KazakhTTS2 reduces WER on the test set by 1.9%. Adding television news further reduces WER on the test set by 5.7%. We observe that the above WER improvements mostly come from the evaluation sets of reading-style sources. The addition of the remaining sources significantly reduced WER on the test set from 22.0% to 15.6%, which suggests that additional spontaneous data collection is required.\\n\\n5. Conclusion\\n\\nIn this work, we have presented KSC2, the first industrial-scale publicly available speech corpus for the Kazakh language. The corpus contains over a thousand hours of high-quality transcribed data from various sources. Specifically, it incorporates the previously released KSC and KazakhTTS2 corpora and further extends them with new transcribed recordings. We have also added data from other sources, such as television news, television and radio programs, parliament speeches, and podcasts. A large portion of the newly added sources are spontaneous conversations, which are essential for building ASR models for real-world applications. Importantly, the new sources also contain Kazakh-Russian code-switching samples, including intra-sentential, inter-sentential, and intra-word variants, which are commonly practised communication modes among Kazakhs. We have described the corpus specifications and validated its utility using a Transformer-based ASR model with promising results. Specifically, the overall WER results on the validation and test sets are 15.1% and 15.6%, respectively. To enable experiment reproducibility, we have shared KSC2, the training recipes, and the pre-trained models in our GitHub repository. We strongly believe that our corpus will promote Kazakh language use in speech-based digital technologies and advance research in Kazakh speech processing. In future work, we plan to collect additional spontaneous and Kazakh-Russian code-switching data.\\n\\n6. Acknowledgements\\n\\nThe authors would like to thank Aigerim Boranbayeva, Almas Mirzakhmetov, and Rustem Yeshpanov for their help in data collection and paper revision. The authors would also like to thank the anonymous speakers for contributing their voices and the Kazakh language moderators for their help in transcribing and aligning the data.\"}"}
