{"id": "chang23c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] M. Shridhar, J. Thomason, D. Gordon, Y. Bisk, W. Han, R. Motaghi, L. Zettlemoyer, and D. Fox, \u201cALFRED: A benchmark for interpreting grounded instructions for everyday tasks,\u201d in Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[2] X. Puig, K. Ra, M. Boben, J. Li, T. Wang, S. Fidler, and A. Torralba, \u201cVirtualhome: Simulating household activities via programs,\u201d in Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[3] A. Padmakumar, J. Thomason, A. Shrivastava, P. Lange, A. Narayan-Chen, S. Gella, R. Piramuthu, G. Tur, and D. Hakkani-Tur, \u201cTEACh: Task-driven embodied agents that chat,\u201d in Conference on Artificial Intelligence (AAAI), 2022.\\n\\n[4] M. Marge, C. Espy-Wilson, N. G. Ward, A. Alwan, Y. Artzi, M. Bansal, G. Blankenship, J. Chai, H. Daum\u00e9 III, D. Dey et al., \u201cSpoken language interaction with robots: Recommendations for future research,\u201d Computer Speech & Language, 2022.\\n\\n[5] T. Srinivasan, R. Sanabria, and F. Metze, \u201cLooking enhances listening: Recovering missing speech using images,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\n\\n[6] J. Gu, E. Stefani, Q. Wu, J. Thomason, and X. Wang, \u201cVision-and-language navigation: A survey of tasks, methods, and future directions,\u201d in Association for Computational Linguistics (ACL), 2022.\\n\\n[7] P. Anderson, Q. Wu, D. Teney, J. Bruce, M. Johnson, N. Sunderhauf, I. Reid, S. Gould, and A. Van Den Hengel, \u201cVision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments,\u201d in Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[8] Y. Qi, Q. Wu, P. Anderson, X. Wang, W. Y. Wang, C. Shen, and A. v. d. Hengel, \u201cREVERIE: Remote embodied visual referring expression in real indoor environments,\u201d in Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[9] H. Chen, A. Suhr, D. Misra, N. Snavely, and Y. Artzi, \u201cTouchdown: Natural language navigation and spatial reasoning in visual street environments,\u201d in CVPR, 2019.\\n\\n[10] J. Thomason, M. Murray, M. Cakmak, and L. Zettlemoyer, \u201cVision-and-dialog navigation,\u201d in Conference on Robot Learning (CoRL), 2019.\\n\\n[11] S. Banerjee, J. Thomason, and J. Corso, \u201cThe RobotSlang benchmark: Dialog-guided robot localization and navigation,\u201d in Conference on Robot Learning (CoRL), 2021.\\n\\n[12] A. Suhr, C. Yan, J. Schluger, S. Yu, H. Khader, M. Mouallem, I. Zhang, and Y. Artzi, \u201cExecuting instructions in situated collaborative interactions,\u201d in Empirical Methods in Natural Language Processing and International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.\\n\\n[13] A. Ku, P. Anderson, R. Patel, E. Ie, and J. Baldridge, \u201cRoom-Across-Room: Multilingual vision-and-language navigation with dense spatiotemporal grounding,\u201d in Empirical Methods for Natural Language Processing (EMNLP), 2020.\\n\\n[14] C. Chen, U. Jain, C. Schissler, S. V. A. Gari, Z. Al-Halah, V. K. Ithapu, P. Robinson, and K. Grauman, \u201cSoundspaces: Audio-visual navigation in 3D environments,\u201d in European Conference on Computer Vision (ECCV), 2020.\\n\\n[15] S. Paul, A. Roy-Chowdhury, and A. Cherian, \u201cA VLEN: Audio-visual-language embodied navigation in 3D environments,\u201d in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[16] C. Bregler, H. Hild, S. Manke, and A. Waibel, \u201cImproving connected letter recognition by lipreading,\u201d in International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1993.\\n\\n[17] P. Duchnowski, U. Meier, and A. Waibel, \u201cSee me, hear me: integrating automatic speech recognition and lip-reading,\u201d in International Conference on Spoken Language Processing (ICSLP), 1994.\\n\\n[18] Y. Miao and F. Metze, \u201cOpen-domain audio-visual speech recognition: A deep learning approach.\u201d in INTERSPEECH, 2016.\\n\\n[19] S. Palaskar, R. Sanabria, and F. Metze, \u201cEnd-to-end multimodal speech recognition,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018.\\n\\n[20] R. Sanabria, O. Caglayan, S. Palaskar, D. Elliott, L. Barrault, L. Specia, and F. Metze, \u201cHow2: A large-scale dataset for multimodal language understanding,\u201d in Visually Grounded Interaction and Language (ViGIL) Workshop, Neural Information Processing Systems (NeurIPS), 2018.\\n\\n[21] R. Corona, J. Thomason, and R. J. Mooney, \u201cImproving black-box speech recognition using semantic parsing,\u201d in International Joint Conference on Natural Language Processing (IJCNLP), 2017.\\n\\n[22] O. Caglayan, R. Sanabria, S. Palaskar, L. Barraul, and F. Metze, \u201cMultimodal grounding for sequence-to-sequence speech recognition,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019.\\n\\n[23] T. Srinivasan, R. Sanabria, F. Metze, and D. Elliott, \u201cFine-grained grounding for multimodal speech recognition,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2020, 2020.\\n\\n[24] S. Team, \u201cSilero models: pre-trained enterprise-grade stt/tts models and benchmarks,\u201d GitHub, 2021.\\n\\n[25] O. Caglayan, P. S. Madhyastha, L. Specia, and L. Barrault, \u201cProbing the need for visual context in multimodal machine translation,\u201d in North American Chapter of the Association for Computational Linguistics (NAACL), 2019.\\n\\n[26] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015.\\n\\n[28] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark et al., \u201cLearning transferable visual models from natural language supervision,\u201d in International Conference on Machine Learning (ICML), 2021.\\n\\n[29] S. Bird, E. Klein, and E. Loper, Natural language processing with Python: analyzing text with the natural language toolkit. O\u2019Reilly Media, 2009.\\n\\n[30] T. Srinivasan, R. Sanabria, F. Metze, and D. Elliott, \u201cMultimodal speech recognition with unstructured audio masking,\u201d in NLP Beyond Text Workshop, Empirical Methods in Natural Language Processing (EMNLP), 2020.\\n\\n[31] A. Pashevich, C. Schmid, and C. Sun, \u201cEpisodic Transformer for Vision-and-Language Navigation,\u201d in International Conference on Computer Vision (ICCV), 2021.\\n\\n[32] S. Y. Min, D. S. Chaplot, P. Ravikumar, Y. Bisk, and R. Salakhutdinov, \u201cFILM: Following Instructions in Language with Modular Methods,\u201d in International Conference on Learning Representations (ICLR), 2022.\\n\\n[33] V. Blukis, C. Paxton, D. Fox, A. Garg, and Y. Artzi, \u201cA persistent spatial semantic representation for high-level natural language instruction execution,\u201d in Conference on Robot Learning (CoRL), 2021.\"}"}
{"id": "chang23c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multimodal Speech Recognition for Language-Guided Embodied Agents\\n\\nAllen Chang1,2, Xiaoyuan Zhu1,2, Aarav Monga1,2, Seoho Ahn1,2, Tejas Srinivasan1, Jesse Thomason1\\n\\n1Department of Computer Science, University of Southern California, USA\\n2Center for Artificial Intelligence in Society, University of Southern California, USA\\n\\nchangall@usc.edu\\n\\nAbstract\\n\\nBenchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agents' ability to complete tasks. We propose training a multimodal ASR model that utilizes the accompanying visual context to reduce errors in spoken instruction transcripts. We train our model on a dataset of synthetic spoken instructions, derived from the ALFRED household task dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that spoken instructions transcribed by multimodal ASR models result in higher task completion success rates for a language-guided embodied agent.\\n\\ngithub.com/Cylumn/embodied-multimodal-asr\\n\\nIndex Terms\\n\\nspeech recognition, multimodal learning, human-robot interaction, embodied learning\\n\\n1. Introduction\\n\\nSeveral benchmarks aim to train simulation agents to complete household chores [1, 2, 3]. These agents are trained to receive and follow written instructions, neglecting the more realistic scenario of receiving spoken instructions. However, physical hardware agents and robots require the ability to understand spoken language for effective interaction with human users [4]. Automatic Speech Recognition (ASR) methods can transcribe spoken instructions into written text. However, erroneous ASR transcripts can hurt the embodied agents' ability to complete the required tasks. One proposed solution for improving ASR robustness is to use visual context, when available, to recover inaudible words. Previous work studied situations where spoken language described a single image [5]. We hypothesize that the visual observations made by embodied agents during task completion will be similarly helpful for disambiguating inaudible words. In particular, we test whether visual observations from an embodied agent can be utilized by multimodal ASR models to reason about and recover words that are masked in the audio signal. Consider transcribing \\\"Put the egg in the . . . \\\" where the destination word is too noisy to understand (Figure 1). A unimodal ASR model must use language priors alone to guess, for example, \\\"fridge. \\\" By contrast, a multimodal ASR model can use the agent's visual observation to correctly reason that the user wants the egg to be put in the \\\"microwave. \\\"\\n\\nWe apply the insight that visual scene information can inform noisy spoken language transcription in the context of an embodied, instruction-following agent. We synthesize a benchmark of text-to-speech (TTS) generated spoken instructions based on written instructions in the Action Learning From Realistic Environments and Directives (ALFRED) dataset [1]. To simulate acoustic noise, we systematically mask audio segments corresponding to words in the instructions. We train unimodal and multimodal ASR models on spoken instructions and evaluate their ability to recover masked words in seen versus unseen visual environments and heard versus unheard TTS speakers. We quantify the downstream impact of erroneous ASR transcriptions on an off-the-shelf ALFRED embodied agent.\\n\\nWe demonstrate that multimodal models can effectively use visual observations to transcribe noisy spoken instructions. We find that the additional visual context is useful in both seen and unseen household environments. Visual context also can help the ASR model generalize better to new speakers. We demonstrate that multimodal ASR models can mitigate the effect of noise in spoken instructions on a text-trained embodied agent's ability to complete tasks. These findings are promising for building embodied agents that follow spoken instructions.\\n\\n2. Background\\n\\nThis work addresses a gap in the embodied task completion literature. Most embodied agent training assumes clean text input, but human speech will be encountered in deployed settings. We leverage insights from existing ASR literature to develop a multimodal ASR model for noisy spoken instructions.\"}"}
{"id": "chang23c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Embodied Task Completion. Existing benchmarks aim to train embodied agents to complete tasks by following instructions [6]. These benchmarks range from vision-language navigation [7, 8, 9, 10, 11], where an agent must follow the user's instructions by executing actions to reach the desired destination, to embodied task completion [1, 2, 12, 3], where agents interact with objects in the environment to complete the user's instructions. These benchmarks provide language as written text, and while a small number of benchmarks involve spoken instructions [13], almost all modeling attempts assume text-based instructions. There are some works at the intersection of embodied learning and acoustic signals [14, 15], but these do not involve spoken instructions. We explore multimodal ASR modeling for language-guided embodied agents.\\n\\nMultimodal Speech Recognition. Previous works have explored augmenting speech recognition with visual information such as lip readings [16, 17], visual scenes [18, 19, 20], and task semantics [21]. When the audio signal is clear, the visual modality has been shown to regularize the model [22] rather than assisting with semantic disambiguation. In contrast, visual semantics have been shown to be helpful when the audio signal is degraded [5, 23]. When visually salient words are masked in the audio signal, multimodal ASR can utilize the visual input to recover the masked words [24]. We apply these findings to an embodied setting, where agents receiving instructions with noisy or degraded speech can leverage their visual observations to recover the instruction text and complete the requested task.\\n\\n3. Methodology\\n\\nWe adapt the ALFRED [1] language-guided instruction benchmark to create synthesized speech commands, apply noising policies to that speech, and attempt to recover ground truth instructions using a novel, multimodal ASR model.\\n\\n3.1. Preliminaries: ALFRED Instruction Following Task\\n\\nALFRED [1] is a benchmark in which an embodied agent must follow language instructions to complete tasks by navigating a room and interacting with objects. Tasks consist of a language goal (e.g., \u201cFind an egg in the fridge and microwave it.\u201d), a sequence of K sub-goal instructions I1...K to achieve that goal, and target environment state conditions (e.g., an egg has been heated). Training and validation tasks include an annotated sequence of actions to accomplish the goal. At each timestep, the agent receives a single-frame visual observation v \u2208 R^W\u00d7H\u00d73 and executes a discrete navigation step (e.g., RotateRight) or object interaction (e.g., PickUp(Egg)) which updates the environment state. The task is successfully completed when the environment state satisfies the target state conditions.\\n\\n3.2. Building a Dataset of Spoken Instructions\\n\\nWe create a dataset of synthetic spoken instructions because of the lack of embodied learning datasets with speech inputs. We extract sub-goal text instructions I from the ALFRED dataset and apply off-the-shelf TTS models 1 to generate synthetic speech instructions S(I). We pair each instruction with a visual context v, the agent\u2019s observation when the previous sub-goal is completed. For example, if the agent has navigated to a microwave as the previous subgoal, the observation accompanying \u201cPut the egg in the microwave\u201d will be of a kitchen counter with a microwave (Figure 1). Multimodal ASR models 1 TTS models are sourced from the open-source Silero [25] library.\\n\\n3.3. Injecting Noise into Spoken Instructions\\n\\nPrior studies in machine translation [26] and ASR [5] have shown that visual context is helpful when the primary modality is degraded. Building on these insights, we corrupt the audio in our dataset by masking audio segments in the instructions. Following prior work [5], we apply masking at the word-level. After segmenting instructions with wav2vec 2.0 [27] to identify word boundaries, we mask words by substituting their speech segments with Gaussian noise. In our controlled setting, we can mask out different sets of words to evaluate the utility of the visual context under different conditions.\\n\\n3.4. Modeling\\n\\nWe train unimodal and multimodal ASR models and evaluate their ability to transcribe noised speech. The unimodal ASR model consists of a speech encoder and a language decoder. The speech encoder is a frozen wav2vec 2.0 [27], pre-trained on Librispeech [28], which encodes the spoken instruction S(I) into a sequence of speech encodings. The decoder is a 4-layer Transformer trained from scratch, which jointly attends over the speech encodings and decoded words.\\n\\n---\\n\\n1 TTS models are sourced from the open-source Silero [25] library.\"}"}
{"id": "chang23c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To decouple the role of the visual modality from model architecture in ASR performance, our multimodal ASR model has a near-identical architecture (Figure 2) to the unimodal ASR model. The visual context $v$ is encoded using a frozen CLIP-ViT image encoder [29] into a feature vector. At every timestep of generation, the CLIP feature vector is concatenated to the decoder's input word embedding, linearly projected back to the original embedding dimension, and passed through the Transformer for generating the next word.\\n\\n### 4. Experiments\\n\\nWe train unimodal and multimodal ASR models on our spoken instructions dataset (Section 4.1) across several audio masking policies (Section 4.2). We evaluate the added benefit of the visual modality when the audio signal is noised (Section 4.3).\\n\\n#### 4.1. Training and Inference Implementation Details\\n\\nAcross all versions of our spoken instructions dataset, we train unimodal and multimodal ASR models with cross-entropy loss. Models are trained for 50 epochs using the Adam optimizer with a $10^{-4}$ learning rate. During inference, transcriptions are generated using beam search with a beam width of 5.\\n\\n#### 4.2. Audio Masking Policies\\n\\nTo evaluate the benefit of the visual modality with respect to the degree of acoustic noise, we conduct experiments across six noise policies for word masking. Three masking policies consist of masking only nouns, since these words are the most visually salient. We identify the 100 most frequent nouns in our training set using the Natural Language Toolkit [30] and create three versions of our dataset where 20%, 40% and 100% of these nouns in each spoken instruction $S(I)$ are masked. Following [31], we use two noise policies that mask words at random, where 20% and 40% of words in each $S(I)$ are masked. Finally, a trivial masking policy is to use the original, un-noised audio. We apply each policy at both training and inference time.\\n\\n#### 4.3. Evaluation Metrics\\n\\nWe evaluate ASR models on their Word Error Rate (WER), as well as their Recovery Rate (RR), which measures the percentage of correctly transcribed noised words in the dataset [5]. Further, to quantify the added benefit from the visual modality, we introduce two new metrics.\\n\\n**Relative Change in WER ($\\\\triangle WER$):** The relative change in Word Error Rate for the Multimodal model over Unimodal.\\n\\n$$\\\\triangle WER = \\\\frac{WER_M - WER_U}{WER_U} \\\\times 100\\\\%.$$  \\n\\n**Relative Change in RR ($\\\\triangle RR$):** The relative change in Recovery Rate for the Multimodal model over Unimodal.\\n\\n$$\\\\triangle RR = \\\\frac{RR_M - RR_U}{RR_U} \\\\times 100\\\\%.$$  \\n\\n### 5. Results and Discussion\\n\\n#### 5.1. Multimodal ASR Improves Masked Word Recovery in both Seen and Unseen Environments\\n\\nWe test whether incorporating visual information improves ASR for spoken instructions $S_A(I)$ using a single, American English TTS model in seen versus unseen visual environments. While both unimodal and multimodal ASR models suffer from higher WER and lower RR as the level of audio masking increases, multimodal ASR models consistently outperform their unimodal counterparts across all masking policies (Table 1).\\n\\nWe further observe that the utility of multimodal ASR is directly proportional to the level of audio degradation, evidenced by increases in $\\\\triangle WER$ and $\\\\triangle RR$ as the proportion of masked words increases (Figure 3). This trend suggests that visual observations are more advantageous as speech signals become more degraded, and multimodal ASR may be more beneficial to agents when performing tasks in noisier environments. The multimodal models' WER and RR improvements generalize to unseen environments, though the improvements are less pronounced than in seen environments. These results demonstrate the viability of this approach for embodied learning, where training and evaluation environments are often different.\\n\\n#### 5.2. Multimodal ASR Generalizes Better to New Speakers\\n\\nNext, we test whether incorporating multimodal visual information improves ASR for spoken instructions of multiple speakers when those speakers are heard $S_{heard}(I)$ or unheard $S_{unheard}(I)$ during training. Multimodal ASR is more helpful when evaluated on 5 unheard Indic English speakers, compared to the 10 Indic English speakers present in the training data (Figure 4).\"}"}
{"id": "chang23c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: $\\\\Delta$RR between the unimodal and multimodal models across masking policies in both seen and unseen environments, evaluated on heard and unheard Indic English speakers.\\n\\nTable 2: $\\\\Delta$RR on the subset of words corresponding to nouns and non-nouns from the random all-words masking policies.\\n\\n| Speaker(s) POS | 20% Masking | 40% Masking |\\n|----------------|--------------|--------------|\\n| Seen Seen      | +12.7%       | +12.7%       |\\n| Seen Unseen    | +12.7%       | +12.7%       |\\n| Seen Unseen    | +12.7%       | +12.7%       |\\n| Unseen Unseen  | +12.7%       | +12.7%       |\\n\\nThese relationships indicate that visual signals additionally regularize model training to adapt to audio from unheard speakers. Importantly, this relationship is observed in the joint case of unseen environments and unheard speakers, as will be the case for newly deployed embodied agents in the world.\\n\\n5.3. Multimodal ASR is Helpful For Visually Salient Words\\n\\nWe investigate whether multimodal ASR is helpful for the right reasons by evaluating whether it recovers masked words that are more visually observable. For the masking policies that mask words at random, we evaluate $\\\\Delta$RR on the subsets of the masked words corresponding to nouns and non-nouns. In Table 2, we observe that masked nouns are much more likely to be recovered than other masked words by multimodal ASR. $\\\\Delta$RR between nouns and non-nouns are most different in the 40% random all-words masking policy for seen environments, suggesting that multimodal ASR is most helpful when audio is heavily perturbed but visual observations are still familiar. The inverse is true with 20% random all-words masking in unseen environments, which has the lowest $\\\\Delta$RR. These trends reveal that multimodal models are effective when masked words have strong visual salience, particularly in familiar environments.\\n\\n5.4. Multimodal ASR Helps Agents Complete Tasks Better\\n\\nWe now investigate the impact of ASR transcription quality on an embodied agent\u2019s ability to complete tasks and whether leveraging visual observations for ASR mitigates the agent\u2019s performance degradation in the presence of noisy speech. We perform our analysis on the Episodic Transformer (E.T.) [32], an off-the-shelf ALFRED agent that is trained to receive a goal text $G$ and $K$ sub-goal instruction texts $I_1,\\\\ldots,K$.\\n\\nWe begin by extracting transcripts for the spoken instruction $S(I_i)$ corresponding to every sub-goal instruction $I_i$ for each episode. Transcripts are extracted for the American English speaker from unimodal and multimodal ASR models, across four different masking policies (no masking and 20%, 40%, and 100% noun masking). The ground-truth goal text $G$ and the transcribed sub-goal instructions are passed to E.T., and the agent attempts to complete the task using the transcribed instructions. We evaluate the agent\u2019s Task Success Rate on the ALFRED validation set, in the seen environments.\\n\\nWe observe that transcribed instructions from both unimodal and multimodal ASR models lead to lower model performance than the ground-truth instructions (Figure 5). However, the multimodal ASR models\u2019 transcriptions lead to lower performance degradation. We further observe that as the level of audio masking increases, the performance gap between the unimodal and multimodal ASR increases. These findings demonstrate that not only does multimodal ASR lead to more accurate transcription of spoken instructions, it also results in better downstream task completion for the embodied agent.\\n\\n6. Conclusions\\n\\nIn this work, we address the challenge of embodied task completion by following spoken instructions. We demonstrate that embodied agents can use their visual observations to improve ASR when transcribing spoken instructions. ASR models using visual observations achieve better WER and RR across home environments, speakers of varied demographics, and levels of audio degradation. These models also show higher improvement when spoken instructions become noisier and when transcribing speech from unheard speakers, improving the robustness of ASR. Finally, we demonstrate that multimodal ASR can improve a pre-trained embodied agent\u2019s ability to complete tasks successfully from spoken instructions. These findings motivate the use of visual observations in the implementation of ASR for language-guided embodied agents.\\n\\nOur work presents a proof-of-concept for spoken instruction following by creating a synthetic speech dataset and systematically masking words in the audio. Future work should investigate how our findings generalize to real human speakers and more realistic audio degradation. Further, while we use only a single visual observation, future work should explore utilizing 3D scene representations built by embodied agents [33, 34], as well as other strategies to actively search the visual environment, to resolve ambiguities in ASR.\"}"}
