{"id": "macaire24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Speech-to-Pictograms Translation\\nC\u00b4ecile Macaire\\nChlo\u00b4e Dion\\nDidier Schwab\\nBenjamin Lecouteux\\nEmmanuelle Esperanc \u00b8a-Rodier\\n\\nUniv. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\\nfirstName.lastName@univ-grenoble-alpes.fr\\n\\nAbstract\\nThe automatic translation of speech into pictogram terms (Speech-to-Pictos) represents a novel NLP task with the potential to enhance communication for individuals with language impairments. Recent research has not explored the adaptation of state-of-the-art methods to this task, despite its significance. In this work, we investigate two approaches: (1) the cascade approach, which combines a speech recognition system with a machine translation system, and (2) the end-to-end approach, which tailors a speech translation system. We compare state-of-the-art architectures trained on an aligned speech-to-pictogram dataset, specially created and released for this study. We conduct an in-depth automatic and human evaluation to analyze their behavior on pictogram translation. The results highlight the cascade approach's ability to generate relevant translations from everyday read speech, while the end-to-end approach achieves competitive results with challenging acoustic data.\\n\\nIndex Terms: automatic speech recognition, machine translation, speech translation, pictograms, augmentative and alternative communication.\\n\\n1. Introduction\\nAlternative and Augmentative Communication (AAC) encompasses tools and strategies to facilitate communication for individuals facing language impairment [1]. These disorders affect various language abilities, from speech production to listening, reading, and writing. Genetic diseases, autistic spectrum disorders, and intellectual deficits can be the cause of it. In AAC, pictograms serve to convey messages in everyday life situations. It is a graphic representation associated with a concept (object, person, action, etc.) [2], offering benefits to visualize syntax, manipulate words, and facilitate language access [3]. Moreover, pictograms are widely utilized resources among medical institutes, caregivers, and families. From a social perspective, a 2021 French Red Cross survey identified stress reduction, increased autonomy, and positive well-being for AAC users. However, the study identifies various environmental barriers that limit its use and dissemination. The survey specifically mentions a lack of awareness among caregivers about the potential of AAC and the difficulty of accessing tools (lack of information, training, financial resources, and time).\\n\\nWe believe that implementing speech-to-pictograms translation systems, a task we will refer to as Speech-to-Pictos (S2P), could help address these challenges. As printed in Table 1, the S2P system predicts a sequence of terms (pictos tokens), each associated with a unique ARASAAC pictogram (arasaac pictos), from an audio segment.\\n\\nTable 1: Illustration of the S2P task, with the audio segment as input, and output the sequence of terms (pictos tokens).\\n\\n| audio transcript | pictos tokens | arasaac pictos |\\n|------------------|--------------|---------------|\\n| j'ai fait une d\u00e9couverte incroyable | pass\u00e9 je faire une d\u00e9couverte incroyable | |\\n\\nAutomatic translation from french speech to a sequence of pictograms was studied for the first time by Vaschalde et al. [4]. Their methodology adapts the Text2Picto system [5] by integrating four modules: an Automatic Speech Recognition (ASR) system, a simplification system, a word sense disambiguation model, and a module to display the sequence of pictograms. The preliminary study does not report automatic or human evaluations. To our knowledge, there exists no other studies exploring the translation of speech into pictograms for French.\\n\\nContributions:\\n1. We introduce two approaches to automatically translate speech into pictogram terms, cascaded and end-to-end.\\n2. We construct and release two freely available corpora of aligned speech-text-pictograms for this task.\\n3. We implement and publish state-of-the-art Automatic Speech Recognition (ASR), Machine Translation (MT), and Speech Translation (ST) models adapted to the datasets.\\n4. We present both an automatic and an original human evaluation for the S2P task, which yield competitive results with a cascade approach for our target group.\\n\\n2. Proposed methods\\nIn this work, we explore both cascade and end-to-end approaches for the Speech-to-Pictos (S2P) task, focusing on state-of-the-art models presented in the following sections.\\n\\n2.1. Cascade approach\\nThe cascade approach consists of an ASR system and an MT system. The transcription provided by the ASR system is the entry point for the MT system, whose goal is to translate a source language sentence \\\\( X = (x_1, \\\\ldots, x_n) \\\\) (the French transcript) into the target language sentence \\\\( Y = (y_1, \\\\ldots, y_n) \\\\) (the picto tokens sequence). For the S2P task, the target language is the \\\"pictographic language\\\" corresponding to the sequence \\\"language\\\" corresponding to the sequence.\\n\\n3. The code is released at https://github.com/macairececile/speech-to-pictograms\"}"}
{"id": "macaire24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A sequence of terms (single word, multi-word expression, or entire sentence), each associated with an ARASAAC pictogram.\\n\\nAutomatic speech recognition. Impressive results have been demonstrated in downstream speech tasks with the application of self-supervised learning (SSL) [6, 7]. We consider Wav2Vec 2.0 [8], which learns robust speech representations from a large collection of unlabeled data. The architecture is based on convolutional layers and Transformers [9]. Wav2vec 2.0 is then fine-tuned on labeled data with a Connectionist Temporal Classification (CTC) loss [10]. Recent works have introduced massively multimodal and multilingual models, achieving competitive ASR results without the need for a fine-tuning step. Whisper [11] employs the Transformer architecture [9] trained in a weakly supervised pre-training fashion with 680,000 hours of multilingual labeled data (from 100 languages). SeamlessM4T is a multimodal and multilingual machine translation model covering a hundred languages. The architecture combines a Transformer textual encoder-decoder NLLB, a Conformer-based W2v-BERT 2.0 speech encoder [12], a Text-to-Unit encode-decoder and a HiFi-GAN unit-vocoder [13]. SeamlessM4T preserves elements of prosody and vocal style in all covered languages.\\n\\nMachine translation. The MT landscape explores different approaches. Ott et al. [14] present NMT, a sequence-to-sequence machine translation Transformer model trained from scratch. The architecture takes a common vocabulary for each language pair, and the data are tokenized into sub-word units using the Byte-Pair Encoding (BPE) algorithm. Recent works investigate pre-training approaches. Liu et al. [15] introduce mBART, a sequence-to-sequence auto-encoder model pre-trained on a large-scale monolingual data in multiple languages with the BART objective [16]. This work emphasizes its advantage for languages not present in the pre-training data. Raffel et al. [17] propose T5, an encoder-decoder model with a transfer learning-based approach. Each textual data is treated as a text-to-text problem, enabling the model to perform multiple tasks (document summarization, machine translation, etc.) through a single model. The pre-training includes both supervised and unsupervised training on 20 TB of textual data from English, French, Romanian, and German languages. Finally, Costa-juss\u00e0 [5] introduce NLLB, a massively multilingual Transformer-type model capable of automatically translating into 200 languages. This linguistic coverage can be beneficial between two related languages through interlinguistic transfer [18].\\n\\n2.2. End-to-end approach\\n\\nThe second approach adapts end-to-end Speech Translation (ST) systems to our task. An ST model performs a direct translation from an audio sequence in a source language \\\\( s = (s_1, ..., s_s) \\\\) to the text \\\\( y = (y_1, ..., y_y) \\\\) in the target language.\\n\\nEnd-to-End ST circumvents the need for intermediary text and reduces the risk of error propagation.\\n\\nIn this work, we seek to leverage a system suitable for low-resource contexts. A recent work by Ye et al. [19] present ConST based on a contrastive learning approach. It aims to encode similar audio and textual representations in a close space. Comprising four modules, ConST integrates a vocal encoder using Wav2Vec 2.0 representations, a lexical embedding layer, and a Transformer encoder-decoder. The reported BLEU scores on MUST-C [20] demonstrate state-of-the-art performance, especially for low-resource language pairs.\\n\\n3. Experiments\\n\\n3.1. Dataset construction\\n\\nWe construct a dataset from a pre-existing spontaneous speech corpora to train our approaches. Another dataset is built to evaluate this task. Each corpus \\\\( C \\\\) is a tuple \\\\( (s, x, y) \\\\) where \\\\( s = (s_1, ..., s_s) \\\\) is the audio segment, \\\\( x = (x_1, ..., x_x) \\\\) is the transcription, and \\\\( y = (y_1, ..., y_y) \\\\) is the pictogram terms translation.\\n\\nPropicto-orf\u00e8o is built upon the aligned speech/text data from the Corpus d'Etude pour le Fran\u00e7ais Contemporain (CEFC) [21]. We extracted 290,036 audio segments, representing 233 hours. From the transcriptions, we applied the method of Macaire et al. [22] to generate a pictogram-based translation, following specific rules and a restricted lexicon. Propicto-orf\u00e8o covers multiple spontaneous speech situations, such as meetings and conversations in various domains.\\n\\nWe construct a dataset for evaluation, Propicto-eval, a corpus of read speech with 62 unique speakers, containing 3,011 sentences. The sentences are derived from children's stories, everyday situations, and sentences from the medical domain. These contexts are relevant as they mirror the types of interactions of our target audience. The dataset creation involved a three-step process: gathering sentences from publicly available ARASAAC PDFs, conducting a recording campaign spanning six months to get the corresponding audio, and generating pictogram translations using the method of Macaire et al. [22]. This process was overseen by the Data Protection Officer to ensure compliance with data protection rights.\\n\\n3.2. Training details\\n\\nDataset pre-processing. We split the Propicto-orf\u00e8o data into training, validation, and test sets, following an 80/10/10 distribution. We remove punctuation and convert transcriptions to lowercase. Each audio segment has a sampling rate of 16kHz with an upper duration limit of 30 seconds to maximize downstream performance. We select a subset of Propicto-eval comprising 100 sentences from 62 speakers with an equal distribution of female and male voices for evaluation. Data details are outlined in Table 2.\\n\\nTable 2:\\n\\n|           | Propicto-orf\u00e8o | Propicto-eval |\\n|-----------|----------------|---------------|\\n| # utterances | 231,374        | 3,011         |\\n| # hours    | 192            | 100           |\\n\\nASR training and inference. We use the SpeechBrain toolkit [23] and the provided recipe [7] to fine-tune the Wav2Vec2.0 model on Propicto-orf\u00e8o, with a French pre-trained Wav2vec 2.0 model LeBenchmark/wav2vec2-FR-7K-large [24]. Audio segments of less than 3 seconds and longer than 10 seconds were excluded from the training to avoid empty audio segments and overfitting scenario (representing 45h). The training is performed with 4 Nvidia V100 GPUs with 32 GB of memory.\"}"}
{"id": "macaire24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We employ the latest released Whisper model namely Whisper large-v3 [11] and follow the steps provided by the whisper repository 8 for inference. Finally, we use the HuggingFace Transformers library [25] to run the SeamlessM4T-Large v2 model for ASR, setting the target language to French ('fra').\\n\\nWe use two toolkits for MT, Fairseq [26] and HuggingFace [25]. All experiments are performed on a single Nvidia V100 GPU with 32 GB of memory. We adapt the recipe provided by Fairseq to train the NMT model from scratch. A tookenization step with BPE segments the text into sub-word units. A vocabulary of 10,000 tokens is generated. The same toolkit is employed to fine-tune the mbart-large-cc25 model learned from 25 languages. We adapt the suggested recipe 9 for translating from English into Romanian to our data. The T5-large and NLLB-200 (facebook/nllb-200-1.3B) models are fine-tuned by using the recipe 10 from HuggingFace for MT.\\n\\nWe follow the pipeline integrated to Fairseq to train ConST 11. The pre-trained French Wav2vec 2.0 model LeBenchmark/wav2vec2-FR-7K-base is initialized as the speech encoder, for computational reasons. A single Nvidia V100 GPU with 32 GB of memory is employed. The main parameters of each model 12 are described in Table 3.\\n\\nTable 3: Main parameters of ASR, MT and ST models for training and inference with the number of parameters, the learning rate, the batch size, the number of epochs and the total running time on Propicto-orf\u00e9e training data.\\n\\n| Model                | # params | lr   | # batch | # epochs | # time (h) |\\n|----------------------|----------|------|---------|----------|------------|\\n| Whisper              | 1550M    | \u2014    | \u2014       | \u2014        | \u2014          |\\n| SeamlessM4T          | 2.3B     | \u2014    | \u2014       | \u2014        | \u2014          |\\n| Wav2Vec2-LeBenchmark | 318.7M   | 1e-4 | 8       | 30       | 22.5       |\\n| NMT                  | 51M      | 5e-4 | 8       | 40       | 1.25       |\\n| mBART25              | 610M     | 3e-5 | 8       | 40       | 18         |\\n| T5-large             | 220M     | 2e-5 | 32      | 40       | 16         |\\n| NLLB-200             | 600M     | 2e-5 | 32      | 40       | 30.5       |\\n| ConST                | 150M     | 1e-4 | 8       | 40       | 100        |\\n\\nResults and Discussion\\n\\n4.1. ASR models\\n\\nIn the first set of experiments, we assess the performance of inference models in Table 4. Table 4 presents the Word Error Rate (WER) [27] on Propicto-orf\u00e9e and Propicto-eval test sets. On Propicto-orf\u00e9e, the WER with both the Whisper and Seamless models is recorded at 37.69 and 46.50, respectively. In contrast, the WER on Propicto-eval remains below 10%. This discrepancy suggests that inference models, having predominantly trained on read speech, therefore poorly generalize when applied to challenging corpora. In the second phase of our experiments, our objective is to contrast these results with a fine-tuned Wav2Vec2.0 approach on spontaneous speech (see Table 5). This approach, with a WER of 27.56 on the test set, demonstrates its effectiveness in handling spontaneous situations characterized by overlaps and disfluencies.\\n\\n4.2. MT models\\n\\nThe results of the Machine Translation models are presented in Table 6. We report the BLEU score with sacreBLEU [28] on Propicto-orf\u00e9e and Propicto-eval. The BLEU score was chosen because it offers greater nuance than the PER (Picto Error Rate) in a translation approach. The score is calculated by comparing the sequence of predicted pictogram terms with the sequence of source pictogram terms. For both corpora, mBART exhibits a substantial deviation from the other models, with a difference exceeding 12 BLEU points on Orfeo-picto. Moreover, NMT model trained from scratch outperforms mBART. When applying the models trained on Propicto-orf\u00e9e to Propicto-eval, the results underscore the substantial contribution of multilingual pre-trained models T5 and NLLB to this translation task.\\n\\n4.3. Combining ASR and MT for cascade S2P\\n\\nWe assess the performance of our cascade approach for Speech-to-Pictograms translation by combining the ASR models with the MT systems. Table 7 presents the BLEU scores on the test data for each model combination with ASR inference models. The performance of Propicto-orf\u00e9e experiences a significant decline when the translation system uses ASR system's predicted transcriptions as input to the MT model. In particular, we observe a decrease of over 28 points in the BLEU score, reaching 58.82 with the combination of Whisper and NLLB-200. The ASR system strongly influences the pictogram translation performance. On Propicto-eval, the best BLEU scores are given by NLLB-200 and T5-large with Whisper. We note a gap of 4.93 between the two MT models, which could be explained by the larger amount of data used to train NLLB, and therefore generalizes better to unseen data.\\n\\nFinally, we compare the performance of the fine-tuned ASR approach with MT models on Propicto-orf\u00e9e in Table 8. While...\"}"}
{"id": "macaire24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the NMT model performs the best in MT, NLLB-200 and T5-large with Wav2Vec2.0 achieve the highest BLEU scores in our cascade approach. We hypothesize that massively pre-trained multilingual models are more robust when dealing with terms distorted by the ASR system.\\n\\nTable 7: BLEU scores on Propicto-orf\u00e9o and Propicto-eval test sets, with the combination of inference ASR models with the MT models trained on the Propicto-orf\u00e9o training data.\\n\\n| ASR model                | MT model      | test         |\\n|--------------------------|---------------|--------------|\\n|                          | Propicto-orf\u00e9o| Propicto-eval|\\n| Whisper large-v3         | NMT           | 58.07        |\\n|                          |               | 68.23        |\\n| mBART25                  |               | 52.05        |\\n|                          |               | 59.49        |\\n| T5-large                 |               | 57.80        |\\n|                          |               | 72.25        |\\n| NLLB-200                 |               | 58.82        |\\n|                          |               | 77.18        |\\n| SeamlessM4T-Large v2     | NMT           | 52.38        |\\n|                          |               | 66.14        |\\n| mBART25                  |               | 48.71        |\\n|                          |               | 56.66        |\\n| T5-large                 |               | 53.96        |\\n|                          |               | 67.32        |\\n| NLLB-200                 |               | 54.86        |\\n|                          |               | 71.56        |\\n\\nTable 8: BLEU scores on Propicto-orf\u00e9o test set by combining Wav2vec2.0 ASR model with the MT models.\\n\\n| ASR model                | MT model | Propicto-orf\u00e9o | Propicto-eval |\\n|--------------------------|----------|----------------|---------------|\\n| Wav2Vec 2.0-LeBenchmark  | NMT      | 61.37          |               |\\n| mBART25                  |          | 55.49          |               |\\n| T5-large                 |          | 61.66          |               |\\n| NLLB-200                 |          | 62.48          |               |\\n\\n4.4. End-to-end S2P\\n\\nWe conclude our experiments by presenting the BLEU scores for the end-to-end speech translation model ConST on the Propicto-orf\u00e9o development and test sets, as well as the Propicto-eval test set in Table 9. On clean data with a low Word Error Rate, the cascade approach outperforms ConST, as denoted by the BLEU score on Propicto-eval test set. However, in ecological acoustic conditions with Propicto-orf\u00e9o, we observe nearly identical results with the cascade approach. Hence, we do not discount the potential of end-to-end approaches.\\n\\nTable 9: BLEU scores on Propicto-orf\u00e9o dev and test sets with ConST, and on Propicto-eval test set, with the bracketed score showing the highest BLEU with the cascade approach.\\n\\n| Model              | dev         | test         | Propicto-eval |\\n|--------------------|-------------|--------------|---------------|\\n| ConST              | 62.21       | 60.21 (62.48)| 54.47 (77.18) |\\n\\n4.5. Human evaluation\\n\\nWe perform a human evaluation, as the BLEU score does not offer precise insights into the specific behavior of each approach in the context of pictogram translation. This evaluation is performed by adapting an analytical framework MQM [29], which gives guidelines and procedures for measuring translation quality. It determines whether the proposed translation meets the specifications agreed by the stakeholders. Each expert annotator assigns to each identified error in the text (source and/or target) a specific type and severity level. In this work, annotators had the option to select from 12 types of errors (addition, omission, unintelligible, etc.) and 4 severity levels (neutral, minor, major, critical).\\n\\nIn this study, 100 randomly chosen sentences from the Propicto-orf\u00e9o test data were annotated by two expert annotators from the project, along with the sentences from Propicto-eval. Table 10 presents the Overall Quality Score (OQS) for the top two cascade models based on the highest BLEU score, and the end-to-end model ConST. OQS is calculated by multiplying the penalty score (resulting from the combination of the number of errors per category and the severity level, weighted accordingly) by the maximum value (usually 100). The translation system is not validated if the score is below the threshold value. Based on various observations, stakeholders have defined the limit for comprehension and usability of a translation to two major errors and one minor error, which corresponds to a threshold value of 89.\\n\\nTable 10: Overall quality score computed on 100 randomly annotated sentences of Propicto-orf\u00e9o, and the test set of Propicto-eval of the two best cascade models and the end-to-end model. ConST.\\n\\n| Model                                      | OQS   |\\n|--------------------------------------------|-------|\\n| Propicto-orf\u00e9o Wav2Vec2 + CTC / T5-large-orf\u00e9o | 45.78 |\\n| Wav2Vec2 + CTC / NLLB-200-orf\u00e9o             | 44.56 |\\n| ConST-orf\u00e9o                                | 62.62 |\\n| Propicto-eval Whisper large-v3 / T5-large-orf\u00e9o | 75.29 |\\n| Whisper large-v3 / NLLB-200-orf\u00e9o           | 85.47 |\\n| ConST-orf\u00e9o                                | 60.73 |\\n\\nThe best translation systems for Propicto-orf\u00e9o and Propicto-eval fall below the set threshold, thereby rejecting their use with the target audience. This result can be attributed to certain behaviors, such as the inaccurate translation of named entities, or the mistranslation of specific terms (mainly due to errors introduced by the ASR systems). However, the OQS with ConST on Propicto-orf\u00e9o stands at 62.62, exhibiting a gap of 16.84 compared to the best-performing cascade model. While automatic evaluation failed to validate the end-to-end approach, human evaluation underscores its efficacy in real-world acoustic scenarios. For Propicto-eval, the cascade approach demonstrates superior performance, with an OQS score nearly reaching the threshold value, which confirms its effectiveness to read speech in everyday situations. Future research could improve results by addressing untranslated and poorly translated terms, exploring novel end-to-end approaches, and employing generative systems for pictogram generation.\\n\\n5. Conclusion\\n\\nThis article introduces two approaches for the automatic translation of speech into pictogram terms. We present data specifically designed for this task, encompassing various acoustic scenarios and domains. While the cascade approach demonstrates superior results compared to the end-to-end approach but competitive outcomes are observed with the latter on acoustically challenging data. Consequently, we do not discount the end-to-end approach for future exploration. Human evaluation exposes several limitations, including the substantial impact of speech recognition systems on translation and challenges in translating specific linguistic phenomena such as polylexical units and named entities.\"}"}
{"id": "macaire24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Acknowledgements\\nThis project was funded by the Agence Nationale de la Recherche (ANR) through the project PROPICTO (ANR-20-CE93-0005). This work was performed using HPC resources from GENCI-IDRIS (Grant 2023-AD011013625R1). We thank everyone who contributed to recording the sentences for the creation of the Propicto-eval corpus.\\n\\n7. References\\n[1] D. R. Beukelman and P. Mirenda, Communication alternative et am\u00e9lior\u00e9e: Aider les enfants et les adultes avec des difficult\u00e9s de communication. De Boeck Sup\u00e9rieur, 2017.\\n[2] J. A. Pereira, D. Mac\u00eado, C. Zanchettin, A. L. I. de Oliveira, and R. do Nascimento Fidalgo, \u201cPictobert: Transformers for next pictogram prediction,\u201d Expert Systems with Applications, vol. 202, p. 117231, 2022.\\n[3] E. Cataix-Negre, Communiquer autrement: Accompagner les personnes avec des troubles de la parole ou du langage. De Boeck Sup\u00e9rieur, 2017.\\n[4] C. Vaschalde, P. Trial, E. Esperan\u00e7a-Rodier, D. Schwab, and B. Lecouteux, \u201cAutomatic pictogram generation from speech to help the implementation of a mediated communication,\u201d in Conference on Barrier-free Communication, 2018.\\n[5] V. Vandeghinste, I. S. L. Sevens, and F. Van Eynde, \u201cTranslating text into pictographs,\u201d Natural Language Engineering, vol. 23, no. 2, pp. 217\u2013244, 2017.\\n[6] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n[7] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\\n[8] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. &. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, vol. 30. Curran Associates, Inc., 2017.\\n[10] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\\n[11] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28 492\u201328 518.\\n[12] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cw2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2021, pp. 244\u2013250.\\n[13] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 17 022\u201317 033, 2020.\\n[14] M. Ott, S. Edunov, D. Grangier, and M. Auli, \u201cScaling neural machine translation,\u201d in Proceedings of the Third Conference on Machine Translation: Research Papers. Brussels, Belgium: Association for Computational Linguistics, Oct. 2018, pp. 1\u20139.\\n[15] Y. Liu, J. Gu, N. Goyal, X. Li, S. Edunov, M. Ghazvininejad, M. Lewis, and L. Zettlemoyer, \u201cMultilingual denoising pre-training for neural machine translation,\u201d in Transactions of the Association for Computational Linguistics, vol. 8, pp. 726\u2013742, 2020.\\n[16] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Jul. 2020, pp. 7871\u20137880.\\n[17] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d Journal of Machine Learning Research, vol. 21, no. 140, pp. 1\u201367, 2020.\\n[18] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, \u201cUnsupervised cross-lingual representation learning at scale,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, Jul. 2020, pp. 8440\u20138451.\\n[19] R. Ye, M. Wang, and L. Li, \u201cCross-modal contrastive learning for speech translation,\u201d in Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. Seattle, United States: Association for Computational Linguistics, Jul. 2022, pp. 5099\u20135113.\\n[20] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, \u201cMuST-C: a Multilingual Speech Translation Corpus,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 2012\u20132017.\\n[21] C. Benzitoun, J.-M. Debaisieux, and H.-J. Deulofeu, \u201cLe projet orf\u00e9o: un corpus d\u2019\u00e9tude pour le fran\u00e7ais contemporain,\u201d Corpus, no. 15, 2016.\\n[22] C. Macaire et al., \u201cA multimodal French corpus of aligned speech, text, and pictogram sequences for speech-to-pictogram machine translation,\u201d in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024). Torino, Italia: ELRA and ICCL, May 2024, pp. 839\u2013849.\\n[23] M. Ravanelli et al., \u201cSpeechBrain: A general-purpose speech toolkit,\u201d 2021.\\n[24] S. Evain et al., \u201cLeBenchmark: A Reproducible Framework for Assessing Self-Supervised Representation Learning from Speech,\u201d in INTERSPEECH 2021: Conference of the International Speech Communication Association, Brno, Czech Republic, Aug. 2021.\\n[25] T. Wolf et al., \u201cTransformers: State-of-the-art natural language processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Association for Computational Linguistics, Oct. 2020, pp. 38\u201345.\\n[26] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, \u201cfairseq: A fast, extensible toolkit for sequence modeling,\u201d in Proceedings of NAACL-HLT 2019: Demonstrations, 2019.\\n[27] J. Woodard and J. Nelson, \u201cAn information theoretic measure of speech recognition performance,\u201d in Workshop on standardization for speech I/O technology, Naval Air Development Center, Warminster, PA, 1982.\\n[28] M. Post, \u201cA call for clarity in reporting BLEU scores,\u201d in Proceedings of the Third Conference on Machine Translation: Research Papers. Belgium, Brussels: Association for Computational Linguistics, Oct. 2018, pp. 186\u2013191.\\n[29] A. Burchardt, \u201cMultidimensional quality metrics: a flexible system for assessing translation quality,\u201d in Proceedings of Translating and the Computer 35. London, UK: Aslib, Nov. 28-29 2013.\"}"}
