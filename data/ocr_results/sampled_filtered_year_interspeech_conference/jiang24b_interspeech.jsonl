{"id": "jiang24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper is supported by National Science and Technology Major Project (2022ZD0118002), Research Project of Institute of Software, Chinese Academy of Sciences (ISCAS-ZD-202401, ISCAS-JCMS-202306), National Natural Science Foundation of China (62106255, U23B2018 and 62271477), Youth Innovation Promotion Association CAS Grant (2023119), Shenzhen Peacock Team Project (KQTD20200820113106007), Hong Kong RGC GRF grant No.14200021, 14200218, 14200220, TRS T45-407/19N, Innovation & Technology Fund grant No. ITS/254/19, ITS/218/21.\\n\\n7. References\\n[1] R. D. Kent, \u201cResearch on speech motor control and its disorders: A review and prospective,\u201d Journal of Communication disorders, vol. 33, no. 5, pp. 391\u2013428, 2000.\\n[2] L. Rampello, L. Rampello, F. Patti, and M. Zappia, \u201cWhen the word doesn\u2019t come out: A synthetic overview of dysarthria,\u201d Journal of the neurological sciences, vol. 369, pp. 354\u2013360, 2016.\\n[3] N. M. Joy and S. Umesh, \u201cImproving acoustic models in torgo dysarthric speech database,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 26, no. 3, pp. 637\u2013645, 2018.\\n[4] M. Geng, S. Liu, J. Yu, X. Xie, S. Hu, Z. Ye, Z. Jin, X. Liu, and H. Meng, \u201cSpectro-temporal deep features for disordered speech assessment and recognition,\u201d arXiv preprint arXiv:2201.05554, 2022.\\n[5] R. Takashima, T. Takiguchi, and Y. Ariki, \u201cTwo-step acoustic model adaptation for dysarthric speech recognition,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 6104\u20136108.\\n[6] J. Shor, D. Emanuel, O. Lang, O. Tuval, M. Brenner, J. Cattiau, F. Vieira, M. McNally, T. Charbonneau, M. Nollstadt, A. Hasidim, and Y. Matias, \u201cPersonalizing asr for dysarthric and accented speech with limited data,\u201d 2019.\\n[7] P. Swietojanski, J. Li, and S. Renals, \u201cLearning hidden unit contributions for unsupervised acoustic model adaptation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 8, pp. 1450\u20131463, 2016.\\n[8] M. Geng, X. Xie, R. Su, J. Yu, Z. Jin, T. Wang, S. Hu, Z. Ye, H. Meng, and X. Liu, \u201cOn-the-fly feature based rapid speaker adaptation for dysarthric and elderly speech recognition,\u201d 2022.\\n[9] S. Liu, M. Geng, S. Hu, X. Xie, M. Cui, J. Yu, X. Liu, and H. Meng, \u201cRecent progress in the cuhk dysarthric speech recognition system,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, p. 2267\u20132281, 2021.\\n[10] Z. Yue, E. Loweimi, H. Christensen, J. Barker, and Z. Cvetkovic, \u201cAcoustic modelling from raw source and filter components for dysarthric speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2968\u20132980, 2022.\\n[11] N. Dehak, P. J. Kenny, R. Dehak, P. Dumouchel, and P. Ouellet, \u201cFront-end factor analysis for speaker verification,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 4, pp. 788\u2013798, 2010.\\n[12] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. Mcleavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proceedings of the 40th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 23\u201329 Jul 2023, pp. 28492\u201328518.\\n[13] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d 2021.\\n[14] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d 2020.\\n[15] S. Hu, X. Xie, Z. Jin, M. Geng, Y. Wang, M. Cui, J. Deng, X. Liu, and H. Meng, \u201cExploring self-supervised pre-trained asr models for dysarthric and elderly speech recognition,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n[16] M. Geng, Z. Jin, T. Wang, S. Hu, J. Deng, M. Cui, G. Li, J. Yu, X. Xie, and X. Liu, \u201cUse of speech impairment severity for dysarthric speech recognition,\u201d arXiv preprint arXiv:2305.10659, 2023.\\n[17] M. K. Baskar, T. Herzig, D. Nguyen, M. Diez, T. Polzehl, L. Burget, J. \u010cernock\u00fd et al., \u201cSpeaker adaptation for wav2vec2 based dysarthric asr,\u201d arXiv preprint arXiv:2204.00770, 2022.\\n[18] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, \u201cLora: Low-rank adaptation of large language models,\u201d 2021.\\n[19] N. Houlsby, A. Giurgiu, S. Jastrzebski, B. Morrone, Q. De Laroussilhe, A. Gesmundo, M. Attariyan, and S. Gelly, \u201cParameter-efficient transfer learning for nlp,\u201d in International Conference on Machine Learning. PMLR, 2019, pp. 2790\u20132799.\\n[20] X. Liu, Y. Zheng, Z. Du, M. Ding, Y. Qian, Z. Yang, and J. Tang, \u201cGpt understands, too,\u201d AI Open, 2023.\\n[21] P. Enderby, \u201cChapter 22 - disorders of communication: dysarthria,\u201d in Neurological Rehabilitation, ser. Handbook of Clinical Neurology, M. P. Barnes and D. C. Good, Eds. Elsevier, 2013, vol. 110, pp. 273\u2013281.\\n[22] A. Jaegle, F. Gimeno, A. Brock, A. Zisserman, O. Vinyals, and J. Carreira, \u201cPerceiver: General perception with iterative attention,\u201d 2021.\\n[23] J. Liu, X. Du, S. Lu, Y.-M. Zhang, H. An-ming, M. L. Ng, R. Su, L. Wang, and N. Yan, \u201cAudio-video database from subacute stroke patients for dysarthric speech intelligence assessment and preliminary analysis,\u201d Biomedical Signal Processing and Control, vol. 79, p. 104161, 2023.\\n[24] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proceedings of Interspeech, 2018, pp. 2207\u20132211.\\n[25] G. Eren and The Coqui TTS Team, \u201cCoqui TTS,\u201d Jan. 2021. [Online]. Available: https://github.com/coqui-ai/TTS\\n[26] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d 2020.\"}"}
{"id": "jiang24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition\\n\\nYicong Jiang1,4\u2217, Tianzi Wang1,2\u2217, Xurong Xie1,4\u2020, Juan Liu3, Wei Sun1, Nan Yan3, Hui Chen1,4, Lan Wang3, Xunying Liu2, Feng Tian1,4\u2020\\n\\n1Institute of Software, Chinese Academy of Sciences, 2The Chinese University of Hong Kong, 3Shenzhen Institute of Advanced Technology, CAS, 4University of Chinese Academy of Sciences\\n\\n{xurong, tianfeng}@iscas.ac.cn, jiangyicong231@mails.ucas.ac.cn, twang@se.cuhk.edu.hk\\n\\nAbstract\\nDisordered speech recognition profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Dysarthric speech recognition encounters challenges including limited data, substantial dissimilarities between dysarthric and non-dysarthric speakers, and significant speaker variations stemming from the disorder. This paper introduces Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the Whisper large-scale model. We first fine-tune Whisper using LoRA and then integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, to improve model recognition of Chinese dysarthric speech. Experimental results from our Chinese dysarthric speech dataset demonstrate consistent improvements in recognition performance with Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the fine-tuned Whisper.\\n\\nIndex Terms: disordered speech, speaker adaptation, Whisper, P-tuning, Perceiver\\n\\n1. Introduction\\nWhile significant strides have been made in conventional speech recognition, research into disordered speech recognition remains important due to its profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Considering dysarthria as neuro-motor impairments resulting from neural damage affecting the muscular control involved in speech production, patients typically present with unclear articulation, variable speech rate, and disrupted speech rhythm [1, 2]. Moreover, the scarcity of relevant datasets currently observed is attributable to the difficulty in collecting extensive data from individuals afflicted with dysarthria who experience physical disabilities and mobility constraints. These factors collectively contribute to (1) limited training data, (2) large mismatch between people with dysarthria and those without, and (3) large variations among speakers due to the effects of the disorder, posing significant challenges for dysarthric speech recognition.\\n\\nTo address these challenges, many studies have opted to train specialized ASR models using data from individuals with dysarthria. In [3], researchers achieved the highest accuracy on the TORGO dysarthric speech database by adjusting acoustic model parameters, employing standardized cepstral features, and constructing complex DNN-HMM models. Other researchers proposed a method utilizing spectral-temporal deep features for both speech recognition and speaker adaptation [4], demonstrating its superiority over baseline methods through experimentation on the UASpeech corpus. A model adaptation approach for speaker-dependent dysarthric speech recognition systems [5] first adapts to the general speaking style of multiple dysarthric speakers, followed by further adaptation to the target speaker. [6] achieves improved results by fine-tuning a specific subset of layers in the model, which involves significantly fewer parameters. LHUC (Learning Hidden Unit Contributions) is a parameter learning method for neural networks that enhances model performance by learning the importance weights of hidden units, particularly suitable for model adaptation and performance improvement [7\u20139]. There are also methods based on i-vectors to model speaker and channel variability, thereby extracting speaker-specific information to enable the model to be applicable to disordered speech recognition tasks [10, 11].\\n\\nWith the emergence of pre-trained models such as Whisper [12], Hubert [13], Wav2Vec 2.0 [14], among others, which leverage extensive data for pre-training, they can acquire universal speech representations from normal speech data. Such methodologies have the potential to compensate for the scarcity of dysarthric speech data. Fine-tuning these pre-trained models has become a recent trend. Both [15] and [16] have explored various approaches to incorporate domain-adapted self-supervised learning pre-trained models into speech recognition systems to address challenges encountered in recognizing dysarthric and elderly speech. Meanwhile, a few works [17] try to use x-vector and fMLLR for speaker adaptation on the self-supervised pre-trained Wav2vec2.0. However, speaker adaptation on large-scale pre-trained models, e.g. Whisper, remains a relatively underexplored area.\\n\\nEnsuring pre-trained models exhibit robust performance on specific tasks constitutes a significant concern. To address this issue, numerous studies have yielded promising outcomes. LoRA [18] achieves rapid fine-tuning of large-scale language models by introducing low-rank parameters, thereby enhancing the model's efficiency and resource utilization in adapting to particular tasks or domains. Adapter Tuning [19] involves adding small adapter layers to the parameters of pre-trained models, facilitating task-specific fine-tuning without altering the overall architecture. Particularly noteworthy is P-Tuning [20], which incorporates trainable prompt embeddings optimized by a prompt encoder into inputs for improved performance, eliminating the need for manual prompt design. P-Tuning is a method which is suitable for speaker adaptation in scenarios with limited speaker data. Its advantages lie in: (1) efficient utilization of limited data per speaker, (2) scalability to large-scale models with billions of parameters, and (3) the flexibility to capture different information with various configurations. Building upon this, we propose the Perceiver-Prompt for Speaker Adaptation on the Whisper large-scale model.\"}"}
{"id": "jiang24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Perceiver-Prompt is a sequence of vectors derived by a P-tuning trained Perceiver using data from the same speaker. This approach aims to enhance the adaptation of the model to disordered speech recognition tasks, effectively leveraging data to address the variability and the scarcity of data. In order to enable the large-scale pre-trained model to compensate for the scarcity of disordered speech data, we employ LoRA to fine-tune the medium-sized Whisper model on our constructed dataset of disordered speech. Whereafter, a Perceiver is trained by P-tuning and serves as a speaker-dependent prompt generator of Whisper, using input from utterances of the same speaker. Subsequent evaluations are conducted on our dataset of Chinese dysarthric speech, where the proposed Perceiver-Prompt based speaker adaptation obtains consistent recognition performance improvements using various configurations.\\n\\nThe main contributions of this paper are summarized as follows: (1) To the best of our knowledge, this is the first work of applying P-Tuning for speaker adaptation on the Whisper large-scale speech model; (2) Our proposed Perceiver-Prompt can handle variable-length inputs and produce fixed-length speaker prompts; (3) Experiments conduct on our dysarthric speech dataset shows that, our method achieves relative reduction up to 13.04% in Character Error Rate (CER) over the fine-tuned Whisper; Particularly, on the speech with highest severity level, the relative CER reduction is up to 51.38%; (4) Further investigation is conducted for the correlation between the effectiveness of our proposed Perceiver-Prompt and the ability to discriminate between speakers or dysarthria severity levels.\\n\\nThe rest of this paper is organized as follows. Section 2 introduces the large-scale pre-trained model Whisper and the foundational P-Tuning for speaker adaptation. Section 3 introduces a prompt encoder Perceiver and proposes a training method Perceiver-Prompt that combines Perceiver with P-Tuning. Section 4 presents the experimental methodology and results. Finally, conclusions are drawn in Section 5.\\n\\n2. Preliminary\\n\\n2.1. Large-scale pre-trained model Whisper and LoRA\\nWhisper [12] is an automatic speech recognition (ASR) system trained on a large dataset of 680,000 hours of multilingual and multitask supervised data. It employs an end-to-end encoder-decoder architecture based on Transformer. Input audio is split into 30-second segments and converted into log-Mel spectrograms, while the decoder predicts text tokens with specialized tokens for tasks like language identification and translation to English. With about one-third of its data being non-English, Whisper is suitable for Mandarin recognition due to its multilingual capabilities [12].\\n\\nDespite Whisper's extensive training and demonstrated accuracy in various scenarios, it struggles to generalize to disordered speech datasets due to their variability and low clarity. This can lead to recognition errors like unpredictable sentence terminations or misidentification of excessive nasality as speech [21]. Fine-tuning on specific datasets is necessary to enhance Whisper's adaptability to dysarthric speech recognition tasks.\\n\\nConsidering computational costs and the effectiveness of fine-tuning, we opt to employ LoRA for fine-tuning Whisper on our dataset of disordered speech. LoRA is designed to encode the parameters of pre-trained models using a low-rank matrix, which does not incur additional inference time and is more amenable to optimization [18]. The process of fine-tuning using LoRA proceeds as follows:\\n\\n$$W_0 + \\\\Delta W = W_0 + BA, \\\\quad B, A \\\\in \\\\mathbb{R}^{d \\\\times r}, A \\\\in \\\\mathbb{R}^{r \\\\times k}$$\\n\\nwhere $W_0$ represents the frozen parameters in the pre-trained model, $\\\\Delta W$ denotes the parameters requiring fine-tuning, $B$ and $A$ are the up-projection matrix and down-projection matrix and $r \\\\ll \\\\min(d, k)$ respectively. During training, only $B$ and $A$ are trained. $r$ is empirically set as 8 during training and fixed throughout the experiments of this paper.\\n\\n2.2. P-tuning\\nIntegrating discrete or continuous prompts as additional inputs into language models can yield improved performance [20]. However, [20] indicates that the use of discrete prompts imposes high design requirements and improper utilization may lead to model instability, potentially limiting the exploitation of gradient descent in discrete space searches. Therefore, P-Tuning utilizes a separate Prompt Encoder to generate corresponding continuous Prompt Embeddings as additional inputs, achieving superior results. This design can be applied to Speaker Adaptation in speech recognition, utilizing the Prompt Encoder to encode certain speaker data into Prompt Embeddings representing Speaker Features as additional inputs to the model. In tasks involving disordered speech recognition, distinct Speaker Features may signify different articulatory disorder symptoms, aiding the model's adaptation to disordered speech recognition tasks.\\n\\n3. Perceiver-Prompt\\n\\n3.1. Employing Perceiver as a Prompt Encoder\\nTo implement P-Tuning for speaker adaptation in disordered speech recognition tasks, it is necessary to augment the Whisper with a Prompt Encoder to handle complex temporal speech data and vectorize its speaker characteristics to form the model's Prompt. Perceiver [22] emerges as an excellent choice for this purpose. The advantage of Perceiver lies in its ability to generate Prompts without making specific domain assumptions and handle input with arbitrary form. It generates fixed-length embedding vectors based on input, facilitating concatenation with input data without requiring data preprocessing. Additionally, another crucial factor is Perceiver's capability to generate speaker prompts online, enabling relatively efficient adaptation during inference. The Perceiver-based prompt Encoder is shown in figure 1 (left, green).\\n\\n3.2. Perceiver-Prompt for speaker adaptation\\nFor speaker adaptation, the proposed Perceiver-Prompt can be trained with P-tuning in a flexible way by employing various configurations. In general, the speaker prompt is a sequence of vectors generated by a Perceiver followed by a linear transform. The Perceiver input is an embedding sequence consisting of various number of history utterances from the same speaker on the adapted Whisper layer. The linear transform maps the speaker prompt to the same feature space with the input embeddings. Subsequently, the obtained speaker prompt is concatenated with the input embedding sequence at various positions to form a new embedding sequence for speaker adaptation. Such positions can be the beginning, end, or both sides of the sequence, as shown in figure 1, right, blue. Finally, parameters of the Perceiver-Prompt is trained with P-tuning by fixing the parameters of Whisper. Taking the concatenation at the end of\"}"}
{"id": "jiang24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The flexible concatenation method of Perceiver-Prompt (optionally including data from the same speaker). This process can be represented as follows:\\n\\n\\\\[\\n\\\\text{Prompt} = \\\\text{Perceiver}(X_p), \\\\quad \\\\text{Prompt} \\\\in \\\\mathbb{R}^{M,D}\\n\\\\]\\n\\n\\\\[\\nX_{\\\\text{new}} = \\\\text{Concat}(X, (W(\\\\text{Prompt}) + b)), \\\\quad X \\\\in \\\\mathbb{R}^{N,D}, \\\\quad X_{\\\\text{new}} \\\\in \\\\mathbb{R}^{M+N,D}\\n\\\\]\\n\\nwhere \\\\(X\\\\) is the length-\\\\(N\\\\) embedding sequence with dimensionality \\\\(D\\\\) of a layer, \\\\(X_{\\\\text{new}}\\\\) denotes the new length-(\\\\(M+N\\\\)) embedding sequence with dimensionality \\\\(D\\\\) of the layer, \\\\(X_p = \\\\text{Concat}(X, S_1, S_2, \\\\ldots, S_n)\\\\) denotes the input of Perceiver, and \\\\([S_1, \\\\ldots, S_n]\\\\) represent the utterances from the same speaker utilized. For decoding, the speaker prompt capturing speaker information can be generated from the Perceiver-Prompt efficiently by using the history utterances from the target speaker as input, and then used by the Whisper for speaker adaptation.\\n\\nIn practice, the Perceiver-Prompt can be positioned on the input log-mel features of Whisper, or positioned before the encoder blocks in Whisper. Moreover, additional information can also be employed as auxiliary supervision for Perceiver-Prompt training to assist the model in better adapting to disorder speech recognition tasks, such as speaker identity or disorder severity.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDataset: We constructed a dysarthric speech dataset following the approach outlined in [23], comprising Chinese speech data collected from 64 patients with varying degrees of dysarthria (approximately 31 hours) and 20 healthy participants (approximately 18 hours). Collection tasks included reading tasks involving Chinese characters, words, and sentences. Additionally, we gathered Frenchay Dysarthria Assessment (FDA) scale information from patients, segmented into four severity levels based on scores of \\\\(\\\\{135, 90, 70\\\\}/145\\\\). The test set comprised speech data from 11 patients with varying degrees of dysarthria, while the remaining data from 53 patients and 20 healthy participants were used for training. In the subsequent experiments, we will evaluate the model performance under different levels of FDA severity and various speech tasks. In the following experiments, F.1 represents the highest severity level of articulatory disorders, while F.4 represents the least severe. Additionally, T.1 denotes Chinese character recognition tasks, T.2 denotes word recognition tasks, and T.3 denotes sentence recognition tasks.\\n\\nWhisper: We chose Whisper-medium as the experimental base due to its combination of effective performance and relatively conservative computational resources. The ESPnet [24] recipe provided us with a pre-configured setup for fine-tuning Whisper-medium using LoRA. Both Encoder and Decoder are stacked with 24 Residual Attention Blocks, each block consisting of multi-head Attention and a Feed-Forward Network. The fine-tuning objective of LoRA targets components within the Attention module such as \\\"query\\\", \\\"key\\\", \\\"value\\\", and \\\"att.out\\\", with a LoRA rank set to 8. In Whisper, all audio is resampled to 16,000 Hz and represented using 80-channel log amplitude mel-spectrograms computed with a 25-millisecond window and a 10-millisecond stride.\\n\\nPerceiver: We utilized the Perceiver provided by [25] to generate a latent array of length 32, which represents speaker features. The feature dimension was set to 1024, matching the input of the Whisper Encoder blocks, with the number of heads in the internal multi-head attention set to 8.\\n\\n4.2. General Result Analysis\\n\\nWhile Whisper has been trained on a considerably large dataset and can achieve human-like recognition results in most tasks without the need for fine-tuning, it is not directly applicable to dysarthric speech recognition tasks. We directly performed inference using the pre-fine-tuned Whisper-medium and Whisper-large, obtaining CERs of 141.9% and 87.9%, respectively. The higher CER results from Whisper's difficulty in predicting sentence terminations on the dysarthric speech dataset before fine-tuning. Hence, all Whisper systems used later have been fine-tuned with LoRA using the dysarthric speech.\\n\\nThe state-of-the-art End-to-end Conformer [26] and hybrid TDNN [27] systems without pre-training are employed for comparison with our approach. Meanwhile, i-vector, as an effective and widely used method for extracting speaker features, is also of interest for comparison. The result shows in table 1 indicate that with the adaptation on Whisper by our proposed Perceiver-Prompt, the model achieved a reduction of 13.04% relative (0.9% absolute) in CER compared to the baseline model, Whisper-medium, and outperformed the i-vector adapted Whisper as well as the Conformer and TDNN systems without pre-training. For speech samples with higher level of articulatory disorders (F.1, F.2), the proposed approach still demonstrates the best performance, particularly for the most severe F.1, with a significant relative CER reduction of 35.78% relative (3.9% absolute).\\n\\n4.3. Different Configuration\\n\\nPerceiver-Prompt demonstrates promising results across various configurations due to its highly flexible. We conducted experiments on the placement of Perceiver, concatenation positions with inputs, the number of historical speech instances used, the length of Speaker Prompt, and other configurations. The experimental outcomes, as presented in Table 2, showcase the superiority of Perceiver-Prompt in dysarthric speech recognition tasks. It adapts to different tasks by adjusting configurations; for instance, employing 5 historical utterances from the...\"}"}
{"id": "jiang24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The three subplots (a), (b), and (c) respectively represent the t-SNE clustering results of Conf.14, Conf.2, and Conf.8. The left column represents t-SNE analysis for speakers, with different colors indicating different speakers. The right column represents t-SNE analysis for FDA severity levels, with different colors indicating different FDA severity levels.\\n\\nThis approach has the flexibility to produce adaptable prompt to accommodate varying dysarthric severity levels or different speech tasks. On our Chinese dysarthric speech dataset, the fine-tuned Whisper applying Perceiver-Prompt outperforms the Conformer, TDNN, and fine-tuned Whisper with/without using i-vector, by achieving a relative CER reduction up to 13.04% across all configurations. To the best of our knowledge, this is the first work of applying P-Tuning for speaker adaptation on the Whisper largescale speech model.\"}"}
