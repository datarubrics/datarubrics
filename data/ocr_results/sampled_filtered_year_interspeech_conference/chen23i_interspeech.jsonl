{"id": "chen23i_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] N. L. Downing, D. W. Bates, and C. A. Longhurst, \u201cPhysician burnout in the electronic health record era: Are we ignoring the real cause?\u201d Annals of Internal Medicine, vol. 169, no. 1, pp. 50\u201351, 2018, pMID: 29801050. [Online]. Available: https://www.acpjournals.org/doi/abs/10.7326/M18-0139\\n\\n[2] A. D. Misra-Hebert, L. Amah, A. Rabovsky, S. Morrison, M. Cantave, C. A. Sinsky, and M. B. Rothberg, \u201cMedical scribes: how do their notes stack up?\u201d Journal of Family Practice, vol. 65, no. 3, pp. 155\u2013160, 2016.\\n\\n[3] R. Gidwani, C. Nguyen, A. Kofoed, C. Carragee, T. Rydel, I. Neligan, A. Sattler, M. Mahoney, and S. Lin, \u201cImpact of scribes on physician satisfaction, patient satisfaction, and charting efficiency: a randomized controlled trial,\u201d The Annals of Family Medicine, vol. 15, no. 5, pp. 427\u2013433, 2017.\\n\\n[4] K. Krishna, S. Khosla, J. Bigham, and Z. C. Lipton, \u201cGenerating SOAP notes from doctor-patient conversations using modular summarization techniques,\u201d in Proc. of the 59th Annual Meeting of the ACL and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Online: ACL, Aug. 2021, pp. 4958\u20134972. [Online]. Available: https://aclanthology.org/2021.acl-long.384\\n\\n[5] J. Su, L. Zhang, H. R. Hassanzadeh, and T. Schaaf, \u201cExtract and abstract with BART for clinical notes from doctor-patient conversations,\u201d in Proc. INTERSPEECH 2022 \u2013 23rd Annual Conference of the International Speech Communication Association, Incheon, Korea, Sep. 2022, pp. 2488\u20132492.\\n\\n[6] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d J. Mach. Learn. Res., vol. 21, no. 1, Jan. 2020.\\n\\n[7] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\u201d in Proc. of the 58th Annual Meeting of the ACL. Online: ACL, Jul. 2020, pp. 7871\u20137880. [Online]. Available: https://aclanthology.org/2020.acl-main.703\\n\\n[8] L. Zhang, R. Negrinho, A. Ghosh, V. Jagannathan, H. R. Hassanzadeh, T. Schaaf, and M. R. Gormley, \u201cLeveraging pretrained models for automatic summarization of doctor-patient conversations,\u201d in Findings of the ACL: EMNLP 2021. Punta Cana, Dominican Republic: ACL, Nov. 2021, pp. 3693\u20133712. [Online]. Available: https://aclanthology.org/2021.findings-emnlp.313\\n\\n[9] C. Grambow, L. Zhang, and T. Schaaf, \u201cIn-domain pre-training improves clinical note generation from doctor-patient conversations,\u201d in Proc. of the First Workshop on Natural Language Generation in Healthcare. Waterville, Maine: ACL, Jul. 2022, pp. 9\u201322. [Online]. Available: https://aclanthology.org/2022.nlg4health-1.2\\n\\n[10] A. Yalunin, D. Umerenkov, and V. Kokh, \u201cAbstractive summarization of hospitalisation histories with transformer networks,\u201d arXiv preprint arXiv:2204.02208, 2022.\\n\\n[11] Y. Yang, C. Malaviya, J. Fernandez, S. Swayamdipta, R. Le Bras, J.-P. Wang, C. Bhagavatula, Y. Choi, and D. Downey, \u201cGenerative data augmentation for commonsense reasoning,\u201d in Findings of the ACL: EMNLP 2020. Online: ACL, Nov. 2020, pp. 1008\u20131025. [Online]. Available: https://aclanthology.org/2020.findings-emnlp.90\\n\\n[12] V. Kumar, A. Choudhary, and E. Cho, \u201cData augmentation using pre-trained transformer models,\u201d in Proc. of the 2nd Workshop on Life-long Learning for Spoken Language Systems. Suzhou, China: ACL, Dec. 2020, pp. 18\u201326. [Online]. Available: https://aclanthology.org/2020.lifelongnlp-1.3\\n\\n[13] B. Chintagunta, N. Katariya, X. Amatriain, and A. Kannan, \u201cMedically aware GPT-3 as a data generator for medical dialogue summarization,\u201d in Proc. of the Second Workshop on Natural Language Processing for Medical Conversations. Online: ACL, Jun. 2021, pp. 66\u201376. [Online]. Available: https://aclanthology.org/2021.nlpmc-1.9\\n\\n[14] Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan, \u201cDIALOGPT: Large-scale generative pre-training for conversational response generation,\u201d in Proc. of the 58th Annual Meeting of the ACL: System Demonstrations. Online: ACL, Jul. 2020, pp. 270\u2013278. [Online]. Available: https://aclanthology.org/2020.acl-demos.30\\n\\n[15] I. Beltagy, M. E. Peters, and A. Cohan, \u201cLongformer: The long-document transformer,\u201d arXiv preprint arXiv:2004.05150, 2020.\\n\\n[16] C. Meister, T. Pimentel, G. Wiher, and R. Cotterell, \u201cLocally Typical Sampling,\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 102\u2013121, 01 2023. [Online]. Available: https://doi.org/10.1162/tacl_a_00536\\n\\n[17] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in Proc. of the 2020 EMNLP: System Demonstrations. Online: ACL, Oct. 2020, pp. 38\u201345. [Online]. Available: https://aclanthology.org/2020.emnlp-demos.6\\n\\n[18] M. Zhong, Y. Liu, Y. Xu, C. Zhu, and M. Zeng, \u201cDialoglm: Pre-trained model for long dialogue understanding and summarization,\u201d in Proc. of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 11 765\u201311 773.\\n\\n[19] C.-Y. Lin, \u201cROUGE: A package for automatic evaluation of summaries,\u201d in Text Summarization Branches Out. Barcelona, Spain: ACL, Jul. 2004, pp. 74\u201381. [Online]. Available: https://aclanthology.org/W04-1013\\n\\n[20] W. Kryscinski, B. McCann, C. Xiong, and R. Socher, \u201cEvaluating the factual consistency of abstractive text summarization,\u201d in Proc. of the 2020 EMNLP (EMNLP). Online: ACL, Nov. 2020, pp. 9332\u20139346. [Online]. Available: https://aclanthology.org/2020.emnlp-main.750\\n\\n[21] T. Zhang, V. Kishore, F. Wu, K. Q. Weinberger, and Y. Artzi, \u201cBertscore: Evaluating text generation with bert,\u201d in International Conference on Learning Representations, 2020. [Online]. Available: https://openreview.net/forum?id=SkeHuCVFDr\\n\\n[22] F. Moramarco, A. Papadopoulos Korfiatis, M. Perera, D. Juric, J. Flann, E. Reiter, A. Belz, and A. Savkov, \u201cHuman evaluation and correlation with automatic metrics in consultation note generation,\u201d in Proc. of the 60th Annual Meeting of the ACL (Volume 1: Long Papers). Dublin, Ireland: ACL, May 2022, pp. 5739\u20135754. [Online]. Available: https://aclanthology.org/2022.acl-long.394\\n\\n[23] O. Bodenreider, \u201cThe unified medical language system (umls): integrating biomedical terminology,\u201d Nucleic acids research, vol. 32, no. suppl 1, pp. D267\u2013D270, 2004.\\n\\n[24] L. Soldaini and N. Goharian, \u201cQuickumls: a fast, unsupervised approach for medical concept extraction,\u201d in Proc. of the 2nd SIGIR workshop on Medical Information Retrieval (MedIR), 2016, pp. 1\u20134.\\n\\n[25] X. Yang, J. Bian, W. R. Hogan, and Y. Wu, \u201cClinical concept extraction using transformers,\u201d Journal of the American Medical Informatics Association, vol. 27, no. 12, pp. 1935\u20131942, 2020.\\n\\n[26] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\\n\\n[27] A. E. Johnson, T. J. Pollard, L. Shen, L.-w. H. Lehman, M. Feng, M. Ghassemi, B. Moody, P. Szolovits, L. Anthony Celi, and R. G. Mark, \u201cMimic-iii, a freely accessible critical care database,\u201d Scientific data, vol. 3, no. 1, pp. 1\u20139, 2016.\\n\\n[28] \u00a8O. Uzuner, B. R. South, S. Shen, and S. L. DuVall, \u201c2010 i2b2/va challenge on concepts, assertions, and relations in clinical text,\u201d Journal of the American Medical Informatics Association, vol. 18, no. 5, pp. 552\u2013556, 2011.\"}"}
{"id": "chen23i_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization\\n\\nSiyuan Chen1\u2217, Colin A. Grambow2\u2217, Mojtaba Kadkhodaie Elyaderani2, Alireza Sadeghi2, Federico Fancellu2, Thomas Schaaf2\\n\\n1 University of Illinois at Urbana-Champaign\\n2 3M Health Information Systems\\n\\nsiyuanc2@illinois.edu, colingrambow@gmail.com, {mkadkhodaieelyaderani, asadeghi, ffancellu, tschaaf}@mmm.com\\n\\nAbstract\\nLarge-scale pre-training has been a successful strategy for training transformer models. However, maintaining a large clinical dataset for pre-training is not always possible, and access to data in this domain can be time-limited and costly. We explore using synthetic data in pre-training sequence-to-sequence (seq-to-seq) transformer models to generate clinical notes from Doctor-Patient-Conversations (DoPaCos). Using a generative language model fine-tuned on authentic conversations, a synthetic DoPaCo dataset was created and used with a corpus of clinical notes to pre-train a Longformer-Encoder-Decoder (LED) model. Results show that synthetic data leads to comparable performance in the downstream summarization task compared to pre-training with authentic data. Pre-training on synthetic conversations first, followed by clinical notes, yields higher performance across most of our evaluation metrics.\\n\\nIndex Terms: Natural Language Generation, Data Augmentation, Pre-training, Doctor Patient Conversation Summarization.\\n\\n1. Introduction\\nClinical notes written by U.S. physicians have nearly doubled in length since 2009 and are now nearly four times longer on average than those in other countries across the same electronic health records (EHR) system [1]. A new medical scribe industry has emerged to alleviate the additional workload associated with EHR use [2] and has been shown to improve physician satisfaction and quality of clinical notes [3]. However, this has merely shifted the burden from physicians to scribes, and continued reliance on human experts has resulted in substantial costs.\\n\\nMany recent works seek to alleviate this burden on physicians and medical scribes by exploring the use of seq-to-seq Transformer models for automatically generating clinical notes from doctor-patient conversations (DoPaCos). A challenge commonly encountered in this task is that DoPaCos are typically lengthy and thus can easily violate the input limits of most pre-trained Transformer models. To circumvent this limitation, extract-then-abstract approaches have been introduced in [4, 5], where a classifier first filters out the irrelevant utterances for the summarization task, and then a pre-trained model, such as T5 [6] or BART [7], summarizes extracted text. An alternative approach is to advocate for a two-stage summarization scheme as in [8]. Others have employed end-to-end abstractive summarization methods and tackled the input length limits using specialized models designed for longer documents. A variety of seq-to-seq models have been investigated in [9], where authors pre-trained Transformer-based summarizers on medical conversations and evaluated their performance with automatic clinical concept overlap metrics. A similar work [10] has fine-tuned a summarizer model consisting of a Longformer encoder and a BERT decoder after pre-training it on an EHR dataset.\\n\\nSome recent works on this topic have taken advantage of in-domain pre-training of general-purpose pre-trained models before fine-tuning them on the target task. However, licensed access to datasets in the medical domain, especially those that include sensitive patient information, is expensive and often time-limited. Following the trails of recent research in data augmentation using pre-trained language models [11, 12], [13] has attempted to augment the training data for the DoPaCo summarization task by using a GPT-3 ensemble model to generate summaries for unannotated DoPaCos. Although it is promising that certain models trained on synthetic summaries were outperforming those trained on human-generated ones, and that a clinical NER system was incorporated during the ensemble generation process to enhance medical concept coverage for the synthetic summaries, these models do not address the generation of DoPaCos, which are typically scarce.\\n\\nTo address this shortcoming, this paper investigates the use of synthetic DoPaCos in pre-training Transformer models for generating the History of Present Illness (HPI) section of clinical notes based on medical conversation transcripts. We first fine-tune a DialoGPT model [14] on a set of authentic, unlabelled DoPaCo transcripts and use it to generate a large number of synthetic DoPaCos. These synthetic conversations can then be used for in-domain pre-training of a Longformer-Encoder-Decoder (LED) model [15], before it is finally fine-tuned on a small set of annotated conversations for the downstream summarization task. We assess the performance of our summarization models using a variety of metrics, including N-gram overlap metrics, as well as semantic similarity and named entity-based metrics (Section 2.4). We show that synthetic conversations offer benefits comparable to authentic data when used for in-domain pre-training of LED models (Section 3.1). Our results also suggest that separated pre-training of encoder and decoder weights using either conversations or clinical notes may provide improvements compared to naively pre-training the model on all data in one pass (Section 3.2).\\n\\n2. Methods\\n2.1. DoPaCo and clinical note datasets\\nWe start with a dataset of roughly 80k manually transcribed and de-identified authentic DoPaCos. The conversations in this dataset lack reference summaries, which prevents us from using them for fine-tuning summarization models. Moreover, our access to this dataset is time-limited. We use this dataset to fine-\"}"}
{"id": "chen23i_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tune a DialoGPT dialogue response generation model, which is then used to generate about 160k synthetic DoPaCos (see Section 2.2). We intentionally generated more synthetic conversations because they are assumed to be of lower quality than real conversations, and it is hoped that increasing the quantity of data generated at little cost can compensate the decrease in quality. For fine-tuning of our DoPaCo summarization models, we use a separate set of 1342 annotated DoPaCos with corresponding human-written HPI summaries. The annotated set is split into 939/201/202 conversations for training, validation, and testing, respectively. The HPI notes have a mean length of 126 words and a standard deviation of 83 words.\\n\\nIn addition, we have access to a large corpus of proprietary, de-identified clinical notes spanning various specialties. About 475k of these notes were sampled from the corpus and used for pre-training of LED models. The clinical notes dataset contains complete SOAP notes with a mean length of 374 words, standard deviation of 302 words, and minimum length of 50 words. Best efforts have been made to de-identify the data, but it is possible that protected health information (PHI) still persists. Adding to the concern of possible PHI, the datasets used are proprietary and, due to contractual obligations cannot be shared.\\n\\n2.2. Synthetic DoPaCo generation\\nWe use a medium-sized DialoGPT model with 345 million parameters to synthesize DoPaCos. The model is fine-tuned on 95 percent of our 80k unannotated DoPaCos for in-domain adaptation, with the remaining 5 percent of conversations left out for model validation. We set the learning rate to $2.5 \\\\times 10^{-5}$, total training steps to $10^6$ steps, training batch size to 8, and use a linear learning rate scheduler with 1000 warm-up steps.\\n\\nTo generate synthetic DoPaCos, we explore multiple generation heuristics, including beam search, top-k and top-p sampling combinations, and typical sampling. For the final generation of synthetic data, we choose a combination of top-k (with $k = 100$) and top-p (with $p = 0.95$). This is because of their computational efficiency as well as the proximity of the word-distribution of generated DoPaCos to that of the authentic DoPaCos as measured by the Jensen-Shannon divergence.\\n\\nThe multi-turn conversation generation process is as follows. We first obtain a set of generic prompts from the authentic DoPaCos and pass a prompt with its speaker role label into the model. The model output is then appended to the end of the generated conversation string. To preserve the context of the conversation for the next generation round, we keep the last-generated 32 words as the new prompt, prepended with the corresponding speaker role label. We repeat the procedure until the generated conversation reaches the predefined maximum length of 800 tokens. We use a length penalty of 0.7 and a no-repeat N-gram size of 3. An example of a generated conversation snippet and its associated prompt is shown in Table 1.\\n\\n### Table 1: An example of a generated conversation snippet.\\n\\n**Prompt (excerpt from real conversation transcripts).**\\n\\n[DR]: Oh, okay. Yeah, so, you know, I mean, that's, you are doing everything you should do. Now, since, you know, you're, depression is a medicine resistant, I know you're on Prozac.\\n\\n**Generated conversation.**\\n\\n[PT]: Um-hum. Yeah. I'm still on it. I just don't take it as often as I should. I know I need to take it more often. Um, I don't know if there's anything else that I can take that would help with the anxiety. I've been taking the Klonopin for a long time.\\n\\n[DR]: Yeah, I mean, you can take Ativan or lorazepam. Have you taken that before? Does that sound familiar? It's a little bit like Valium, but it's not really a Valium. It's, uh, it's a benzodiazepine. So it might help you to get through the day a little better.\\n\\n[PT]: Okay. I'll try it. [LAUGHING]. I'm willing to try anything. [INAUDIBLE]. I don't want to be drugged. I'm not that kind of person. I've never been drugged, but I've heard horror stories about it. So I just want to make sure that I'm, I'm alert and -. Okay.\\n\\n2.3. Longformer summarizer pre-training and fine-tuning\\nWe base all experiments on pre-training and fine-tuning of LED model and initialize all models by the pre-trained weights publically available in the Transformers Library. We adopt a DialogLED-style window-based denoising pre-training task [18] for both conversations and clinical notes, which for the LED-large model was found to show improvement in DoPaCo summarization tasks when pre-trained with authentic conversations [9]. We do not use sentence or turn permutation as they are found to be detrimental for the performance of the pre-trained models on the downstream task. When applied to clinical notes, the pre-training method can be considered as a basic windowed text denoising task, as clinical notes lack speaker labels that are relied on by the DialogLED-style turn splitting/merging pre-training tasks.\\n\\nUsing these models, we first investigate the utility of synthetic DoPaCos by comparing the performance of LED-large models without any further pre-training and those pre-trained with authentic vs. synthetic DoPaCos on the downstream note generation task. We then experiment with pre-training of the encoder and the decoder weights separately; we formulate \u201cmix-and-match\u201d experiments by combining encoder and decoder weights from LED-large models pre-trained on only synthetic DoPaCos, only the clinical notes corpus, or both. Doing so results in nine possible combinations, and we proceed to assemble and fine-tune them on the same downstream training set. This experiment follows the intuition that synthetic conversations are more similar to the source format of the note generation task, while many documents from the clinical notes corpus highly resemble the destination format, i.e., HPI section of clinical notes, and it may be beneficial to expose the model to data of both modalities during pre-training.\\n\\nTo fine-tune the pre-trained models on the annotated DoPaCo dataset, we adopt an automated method for early stopping and selecting checkpoints for evaluation. At fixed intervals during fine-tuning, the geometric mean of ROUGE F1 scores is monitored, and training is stopped upon reaching the early-stopping patience. Then, the best checkpoint on the validation set is selected and evaluated on the test set. We do not monitor concept-based scores during training or validation, as doing so would result in excessively long training times. The hyperparameters used for further pre-training and fine-tuning of LED models on all datasets are shown in Table 2. The average run...\"}"}
{"id": "chen23i_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Hyper-parameters used for further pre-training and fine-tuning of the LED-large model.\\n\\n| General     | Max. encoder length 5120 | Max. decoder length 1024 |\\n|-------------|--------------------------|--------------------------|\\n|             | Max. gradient norm 1.0   |                          |\\n\\n**Pre-training specific**\\n\\n|             | Learning rate 2.0 \u00d7 10^{-5} |\\n|-------------|-----------------------------|\\n|             | Max. epochs 1               |\\n|             | Window ratio 0.1            |\\n|             | Max. window size 512        |\\n|             | Warm-up ratio 0.01          |\\n|             | Weight decay 0.001          |\\n|             | Speaker mask ratio 0.5      |\\n|             | Text infilling ratio 0.15   |\\n\\n**Fine-tuning specific**\\n\\n|             | Learning rate 0.001         |\\n|-------------|-----------------------------|\\n|             | Warm-up steps 200           |\\n|             | Weight decay 0.001          |\\n|             | Steps between evaluation 50 |\\n|             | Early stopping patience 5   |\\n|             | Max. generation length 512  |\\n|             | Number of beams 5           |\\n|             | No-repeat n-gram size 3     |\\n\\n\\n2.4. Evaluation metrics\\n\\nWe evaluate our systems using a mix of N-gram overlap-based, semantic similarity-based, and entity-based metrics. We employ ROUGE-1/2/3/L \\\\cite{rouge} to compute N-gram (longest common sub-sequence for ROUGE-L) lexical overlaps between generated and human-written summaries.\\n\\nTo account for embedding-based semantic similarity \\\\cite{embedding-sem-sim}, we use BERT score \\\\cite{bert-score}, which utilizes contextualized token embeddings obtained from a pre-trained Transformer model to compute the semantic similarity between generated and reference summaries. We use the official BERTscore package with the recommended DeBERTa-large-mnli model and rescale the scores with baselines. Both BERT scores and ROUGE-3/4/L scores are found to correlate with human evaluation results in clinical note generation tasks \\\\cite{correlation}.\\n\\nTo further investigate the conceptual relevance between generated and reference summaries, we rely on the Unified Medical Language System (UMLS) repository \\\\cite{umls}. We extract the UMLS concepts from our generated summaries and compare them with those obtained from references using string matching algorithms; see, e.g., QuickUMLS \\\\cite{quickumls}. By extracting and matching concepts, we can compute F1 scores and aggregate them across all test set conversations.\\n\\nThe extraction of biomedical concepts is based on string matching algorithms, which sometimes can lead to the extraction of irrelevant strings from the text. To circumvent this challenge, we leverage the clinical NER-based extraction method introduced in \\\\cite{clinical-ner}. Specifically, we adopt a RoBERTa model \\\\cite{roberta} pre-trained on MIMIC-III clinical notes \\\\cite{mimic} and fine-tune it on the i2b2 2010 dataset \\\\cite{i2b2} to extract clinical concepts. With this RoBERTa-based NER model, we can then match extracted concepts from generated and reference summaries to UMLS concepts and drop the ones we cannot find matches for. Finally, we can compute F1 scores across extracted concepts from the generated summaries and compare them with those from human-written summaries.\\n\\n3. Results\\n\\nIn this section, we discuss the effects of in-domain pre-training using synthetic conversations. We first compare the performance of LED-large models without any further pre-training with those pre-trained with authentic or synthetic conversations on the DoPaCo summarization task in section 3.1. Then, the effects of pre-training with multiple data modalities are discussed in section 3.2, where the LED models are pre-trained on a combination of synthetic conversations and authentic clinical notes.\\n\\n3.1. Pre-training using authentic vs. synthetic DoPaCos\\n\\nTable 3 compares the performance of LED-large models pre-trained on authentic and synthetic DoPaCos to a baseline model without in-domain pre-training. The results show that ROUGE F1 scores and BERT scores consistently improve after further pre-training, regardless of whether authentic or synthetic conversations are used. This indicates that additional pre-training, even when using synthetic DoPaCos, may lead to improved overlap between the generated and reference summaries. This finding is consistent with the results of previous research \\\\cite{previous-research}.\\n\\nIn terms of UMLS and NER clinical concept-based metrics, however, pre-training with authentic DoPaCos did not significantly improve the baseline performance. This may be due to a stronger baseline compared to \\\\cite{previous-research}. An edited set of training reference summaries with some boilerplate language removed was used for fine-tuning of all models in this work, and likely contributed to more medical concepts being included in the generated summaries. With synthetic data, we observed that concept extraction scores have decreased following pre-training. For the UMLS concept metrics, the baseline model achieved precision of 32.87% and recall of 51.12%, versus 31.10% and 50.08% from the model pre-trained on synthetic conversations. This may be due to synthetic data not being as representative of real-world medical conversations as authentic data.\\n\\nOverall, we find that further pre-training with authentic DoPaCos leads to improvements in terms of ROUGE and BERT scores, but the benefits on clinical concept-based metrics are relatively muted. A similar trend has been observed for synthetic data, but with slight decrease in performance across all metrics. Still, the model pre-trained with synthetic data outperforms the baseline model in 6 of 8 metrics. We conclude that although in-domain pre-training on synthetic data may not provide benefits surpassing pre-training on authentic data, it is indeed better than not performing any in-domain pre-training at all.\\n\\nApart from quantitative results as discussed above, qualitative human evaluations suggest that pre-training with conversations has led to perceivable improvements in the overall quality of the generated summaries. A comparison of sample generated summaries can be found in Table 4. We have also observed that further pre-training using DoPaCos resulted in improved stability (in terms of validation F1 scores, median generation length, and other monitored metrics) during fine-tuning.\\n\\n3.2. Pre-training on multiple datasets\\n\\nIn Table 5 we present the test set results for the combination models. In most cases, combining pre-trained weights from two models harms the performance when compared to pre-training the encoder and decoder together. The exception is experiment No. 7, where the encoder pre-trained first on synthetic conversations and then on clinical notes (DoPaCos \u2192 Notes) is combined with a decoder pre-trained on synthetic DoPaCos only (DoPaCos), which outperforms many other combinations especially in ROUGE F1 scores.\\n\\nWe find that pre-training LED-large on only clinical notes (Experiment No. 5) leads to the worst performance among the experimented options. This could imply that pre-training on conversation-like data before fine-tuning on the conversation...\"}"}
{"id": "chen23i_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Test set F1 scores (in percentages) of the LED-large models fine-tuned on the annotated DoPaCo dataset.\\n\\n| Pre-training | ROUGE Scores | Concept Scores | BERT Score | Median Length |\\n|--------------|--------------|----------------|------------|---------------|\\n|              | R-1          | R-2            | R-3        | R-4           | R-L | UMLS Clinical NER |\\n| None         | 37.99        | 14.70          | 6.25       | 2.96          | 25.81 | 35.91 | 49.84 | 32.67 | 106.5 |\\n| Authentic DoPaCos | 38.21        | 14.79          | 6.36       | 3.10          | 26.12 | 36.76 | 49.19 | 32.94 | 105 |\\n| Synthetic DoPaCos | 38.88        | 14.85          | 6.35       | 3.12          | 27.21 | 34.20 | 48.39 | 33.38 | 97.5 |\\n\\nTable 4: An example DoPaCo with its reference (human-written) and generated HPI summaries. All generated summaries use wrong pronouns, since they were presented with de-identified conversation transcripts. The model with no DoPaCo pretraining is missing an important piece of information that \u201cthe patient's viral load is down to 90 copies\u201d. The italicized sentences are not factual.\\n\\n**Summaries**\\n\\n**Conversation snippets**\\n\\n**Reference.** The patient presents to the clinic for evaluation of HIV and HCV. She has completed a course of Harvoni. Her last HIV viral load was 90. She states she is feeling well.\\n\\n[DR]: So, um, you took all the Harvoni, you took all the -\\n\\n[PT]: That is correct.\\n\\n[DR]: And your viral load is down to, to 90 copies. And the thing to tell you too is at some point the thing you're taking is going to be a single pill.\\n\\n[PT]: Okay.\\n\\n[DR]: So, that'll make life easier for you too, but your hep C, once that's treated, it usually does not come back. ...\\n\\n[DR]: How is the new cocktail? Any problems?\\n\\n[PT]: I was having them, um, every once in a while. I think -\\n\\n[DR]: Just as long as you eat with that thing, you'll be okay.\\n\\n[PT]: Yeah. Every once in a while, I'll have a, I call it a breakout. [LAUGHING].\\n\\n[DR]: It's basically the toughest thing you can take for HIV. ...\\n\\n[DR]: We don't have the flu shot in our office this year, so I want you to run to like a CVS or a Walgreens -\\n\\n[PT]: No, I can't -...\\n\\n[DR]: Yeah. So, I'm pretty happy with this. It'll still Prezcobix. I think you're doing well on it, and you just keep going on. ...\\n\\n[DR]: So. You're doing well. I want to see you in like four months.\\n\\n**No DoPaCo pre-training.**\\n\\nThe patient presents to the clinic today for a follow-up visit. The patient is a year old male presenting for f/u appointment. He is currently taking Harvoni and Prezcobix. He reports that he is compliant with his medications. He notes that he has been having occasional breakout of his HIV medication. He states that he does not have the flu shot.\\n\\n**Pre-training with authentic DoPaCos.**\\n\\nThe patient is a male presenting to the clinic today for a follow-up visit. He is doing well overall. He has been compliant with all of his medications. The patient reports that his last viral load was 90 copies. He reports that he has not had any issues with his medication. He does report that he does have occasional pruritus. He denies any known side effects or adverse reactions.\\n\\n**Pre-training with synthetic DoPaCos.**\\n\\nThe patient is a male presenting to the clinic today for a follow-up visit. Hepatitis C - He is currently taking Harvoni and is compliant with all of his medications. HIV - His last viral load was 90 copies. He is on Prezcobix.\\n\\nTable 5: Test set F1 scores (in percentages) of the LED-large models pre-trained using various combinations of the two datasets: 160k synthetic conversations and 475k clinical notes. Best results for each metric are shown in bold.\\n\\n| Index | Pre-training | ROUGE Scores | Concept Scores | BERT Score | Median Length |\\n|-------|--------------|--------------|----------------|------------|---------------|\\n|       | Encoder Decoder | R-1          | R-2            | R-3        | R-4           | R-L | UMLS Clinical NER |\\n| 0     | None         | 37.99        | 14.70          | 6.25       | 2.96          | 25.81 | 35.91 | 49.84 | 32.67 | 106.5 |\\n| 1     | DoPaCos      | 38.88        | 14.85          | 6.35       | 3.12          | 27.21 | 34.20 | 48.39 | 33.38 | 97.5 |\\n| 2     | DoPaCos \u2192 Notes | 37.56        | 14.49          | 6.10       | 2.86          | 25.61 | 36.59 | 47.80 | 32.23 | 97 |\\n| 3     | DoPaCos      | 38.55        | 14.59          | 6.25       | 3.01          | 27.09 | 32.42 | 47.35 | 32.36 | 92 |\\n| 4     | Notes        | 38.51        | 14.72          | 6.16       | 2.88          | 26.46 | 36.42 | 52.07 | 33.81 | 98 |\\n| 5     | Notes        | 36.89        | 13.59          | 5.51       | 2.56          | 24.91 | 37.57 | 47.44 | 31.80 | 95 |\\n| 6     | Notes \u2192 Notes | 38.34        | 14.72          | 6.15       | 2.88          | 26.66 | 35.09 | 46.68 | 32.90 | 86 |\\n| 7     | DoPaCos \u2192 Notes | 39.34        | 14.77          | 6.39       | 3.12          | 27.32 | 35.58 | 51.83 | 33.38 | 98 |\\n| 8     | DoPaCos \u2192 Notes | 37.37        | 13.79          | 5.64       | 2.70          | 25.19 | 35.41 | 46.90 | 32.25 | 92 |\\n| 9     | DoPaCos \u2192 Notes | 37.38        | 14.10          | 6.01       | 2.89          | 26.36 | 32.47 | 41.84 | 32.40 | 80 |\\n\\nSummarization task is beneficial, even if the data used for pre-training, in this case, are relatively noisy and of lower quality. Contrary to our expectations, combining encoder weights pre-trained on conversations and decoder weights pre-trained on clinical notes (experiment No. 2) leads to low performance among the nine combinations. In fact, all combinations with decoders pre-trained only on clinical notes tend to underperform, especially in ROUGE and BERT metrics. Additionally, although pre-training the entire model on both datasets generally leads to high performance, the strongest model was obtained by combining encoder weights pre-trained on both datasets, and decoder weights pre-trained only on conversation data (Experiment No. 7). The underlying mechanism for this phenomenon is not entirely clear and warrants further investigation.\\n\\n4. Conclusions\\n\\nWe studied pre-training practices for seq-to-seq models in automatic generation of clinical notes from doctor-patient conversations via abstractive summarization. A method for generating synthetic conversation snippets in medical domain using a dialog response generation model was described, and the synthetic conversations were used for pre-training of a summarization model. We found that pre-training with synthetic data was similarly beneficial as pre-training with authentic data, which suggests that authentic data could be saved for the fine-tuning stage. We also found that separated pre-training of the encoder and decoder weights in an encoder-decoder summarizer model using datasets of different modalities may be beneficial. This approach is inexpensive and viable when multiple datasets of distinctive nature are available for pre-training.\"}"}
