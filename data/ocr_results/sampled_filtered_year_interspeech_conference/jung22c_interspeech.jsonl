{"id": "jung22c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThe first spoofing-aware speaker verification (SASV) challenge aims to integrate research efforts in speaker verification and anti-spoofing. We extend the speaker verification scenario by introducing spoofed trials to the usual set of target and impostor trials. In contrast to the established ASVspoof challenge where the focus is upon separate, independently optimised spoofing detection and speaker verification sub-systems, SASV targets the development of integrated and jointly optimised solutions. Pre-trained spoofing detection and speaker verification models are provided as open source and are used in two baseline SASV solutions. Both models and baselines are freely available to participants and can be used to develop back-end fusion approaches or end-to-end solutions. Using the provided common evaluation protocol, 23 teams submitted SASV solutions. When assessed with target, bona fide non-target and spoofed non-target trials, the top-performing system reduces the equal error rate of a conventional speaker verification system from 23.83% to 0.13%. SASV challenge results are a testament to the reliability of today's state-of-the-art approaches to spoofing detection and speaker verification.\\n\\n1. Introduction\\nAutomatic speaker verification (ASV) systems determine whether or not an input utterance contains speech uttered by a given, known speaker. As one of the most efficient, convenient, natural and non-intrusive biometric characteristics, ASV has found widespread application, most notably, in telephony-based scenarios. Reliability is crucial and must be maintained not only in the face of target and impostor trials or utterances, but also so-called spoofed utterances, namely manipulated, synthesised or specially crafted utterances designed to deceive or manipulate the ASV system.\\n\\nThe resistance of ASV systems to spoofing attacks has been studied only relatively recently and within the context of the ASVspoof initiative and associated challenge series [1]. ASVspoof tackles the threat of spoofing attacks using countermeasures (CMs), separate detection sub-systems in the form of binary classifiers designed to distinguish between bona fide and spoofed inputs. While there are other approaches [2, 3], CMs are usually combined with the ASV system in the form of a gate, the role of which is to detect and reject spoofed utterances such that they are never treated by the ASV sub-system.\\n\\nWhile the combination of CM and ASV sub-systems have the potential to improve security through increased robustness to spoofing attacks (rejection of spoofed trials), there is also the potential for degraded usability (rejection of bona fide target trials). An integrated approach to assessment is hence desirable and should gauge the impact of spoofing and CMs upon the ASV system. Such a strategy implies that neither CMs nor ASV systems should be assessed independently from each other. Ultimately, there is but a single goal \u2013 reliable ASV.\\n\\nThe minimum tandem detection cost function (min t-DCF) [4] metric has been developed with this vision and is one approach to the assessment of integrated CM and ASV sub-systems. Given the potential dependence between CM and ASV sub-systems, and given the integrated approach to evaluation, it seems logical that the sub-systems themselves should also be jointly developed and optimised. This is not the case with ASVspoof, for which CMs are developed by challenge participants, while the ASV system is designed by the organisers. Herein lies the original goal of the new Spoofing-Aware Speaker Verification (SASV) Challenge.\\n\\nSASV aims to promote the study of jointly optimised or fused CM+ASV solutions in addition to single, integrated solutions [5\u201313]. The former is a more flexible and incremental approach to SASV. Fusion can be applied using current CM and ASV sub-systems and can hence exploit future advances in both. With the addition of a new fusion model, however, complexity is increased, even if perhaps only modestly. The fusion of two separate sub-systems is also somewhat at odds from the spirit of solutions to the single task of reliable ASV.\\n\\nSingle, integrated solutions, envisaged on a longer-term horizon, represent more of a step change and a more substantive SASV solution; they demand the design of entirely new solutions to the single problem of reliable ASV. The development of single, integrated solutions is expected to be more challenging; it calls for the learning of a new latent space for the representation of both speaker identity and artefacts related to utterance authenticity (spoofing). No matter what the solution strategy, the primary objective is progress in reliable ASV, where reliability implies resistance to both spoofed as well as bona fide non-target inputs.\\n\\nIn building upon long-established best practice in addition to the momentum generated through ASVspoof, the new SASV Challenge is built upon a common evaluation protocol and benchmarking framework, metrics, open source baseline solutions and use of publicly available ASVspoof data, specifically the 2019 Logical Access (LA) database [14]. The two different solution strategies are described in Section 2. SASV\"}"}
{"id": "jung22c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related Works\\n\\nIn this section, we describe prior, related work in two approaches to SASV: (i) back-end fusion; (ii) single, integrated models [15\u201322]. Among the first category are decision-level, score-level, and embedding-level approaches to back-end fusion. Decision-level fusion includes various cascade approaches. Approaches to score-level fusion can be either parameter-free (e.g., score-sum ensemble) or parameter-driven (e.g., Gaussian mixture model) where both utilise separate scores from ASV and CM sub-systems [18]. Embedding-level fusion can also be achieved using a model operating upon embeddings that lie in different latent spaces [21\u201323]. Prior work includes Gomez-Alanis et al. [22] and Shim et al. [21] which both propose deep neural network (DNN)-based models to jointly optimise ASV and CM embeddings and hence produce single SASV scores and decisions. Kanervisto et al. [23] reports a tandem solution to jointly optimise ASV and CM systems using reinforcement learning.\\n\\nSingle, integrated models have also been proposed. These models classify utterances into target and non-target classes where the latter comprises both traditional bona fide as well as spoofed non-target trials [3, 19, 20, 24]. Sizov et al. [3] propose a two-stage PLDA approach to the joint optimisation of speaker and synthesis (spoofing) channel variations in an i-vector space. First, a PLDA model is trained using only embeddings extracted from bona fide speech. Then, a synthesis (spoofing) channel subspace is trained using only embeddings extracted from spoofed speech. Zhao et al. [24] propose an integrated spoofing-robust automatic speaker verification (SR-ASV) system. It uses a multi-task learning framework with max feature map activation [25] and residual convolutional blocks to extract discriminative embeddings and scores from task-specific, CM and ASV layers. None of the work described above was performed using common databases and protocols.\\n\\n3. Databases, protocols and metrics\\n\\nDescribed in this section are the two publicly available databases used for the SASV challenge. The VoxCeleb2 [26] database is used for the training of ASV sub-systems (see Section 4.1). The ASVspoof 2019 LA database [14] is used for the training of CM sub-systems (see Section 4.2).\\n\\n3.1. VoxCeleb2\\n\\nThe VoxCeleb2 database was collected by crawling online videos of celebrity interviews. The database is extracted from 150,480 unique videos with an average individual utterance length of 7.8 seconds. The development partition of the VoxCeleb2 database is used for the training of ASV sub-systems. It contains over 2,000 hours of data corresponding to 5,994 speakers (61% male). The absence of spoofed utterances necessitates use of the VoxCeleb2 database in conjunction with additional databases containing spoofed utterances.\\n\\n3.2. ASVspoof 2019\\n\\nThe ASVspoof 2019 LA database [14] is generated from the VCTK source database [27], which includes speech data captured from 107 speakers (46 male, 61 female). It consists of disjoint train, development, and evaluation partitions. Each partition contains both bona fide and spoofed utterances. The later are generated using 19 state-of-the-art VC, TTS and hybrid TTS-VC attack algorithms (6 for the train and development partitions, 13 for the evaluation partition). Containing 25,380 utterances corresponding to 20 different speakers and both bona fide and spoofed utterances, the ASVspoof database can be used for the training of CM sub-systems but can also be used in conjunction with the VoxCeleb2 database for SASV research.\\n\\n3.3. SASV protocols\\n\\nWhile the SASV protocols exploit ASVspoof 2019 LA data, they are different to those used by participants of the ASVspoof challenge; they are not CM protocols and are, instead, ASV protocols or, more specifically, SASV protocols. The latter involve three types of trial: (i) target, bona fide trials uttered by the same speaker as enrolment utterance(s); (ii) bona fide non-target trials uttered by a different speaker as enrolment utterance(s); (iii) spoofed non-target trials containing speech which is either synthesised or converted in order to resemble the voice of the same speaker as enrolment utterance(s). Disjoint protocols are provided to challenge participants to support their development and evaluation of SASV solutions.\\n\\n3.4. Metrics\\n\\nSASV performance is assessed using the classical EER (SASV-EER) as the primary metric. In keeping with [17, 18, 21], the task hence remains one of binary classification: target vs non-target. We define non-target as the set of bona fide non-target (impostor) trials and spoofed non-target trials. Without adequate countermeasures, both can cause increases in the false acceptance rate. As depicted in Table 1, two additional EER estimates serve as secondary metrics. The speaker verification EER (SV-EER) involves combinations of target trials and bona fide non-target trials whereas the spoofing EER (SPF-EER) involves combinations of target trials and spoofed non-target trials. The SV-EER and SPF-EER are estimated using different subsets of the full protocol used in estimating the SASV-EER. As such, they reflect the reliability of the model under different, extreme conditions in which there are either no spoofed non-target trials or no bona fide non-target trials.\"}"}
{"id": "jung22c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There are two baseline SASV systems. Both comprise standalone CM and ASV sub-systems.\\n\\n### 4.1. ASV and CM sub-systems\\n\\nECAPA-TDNN is an efficient, robust state-of-the-art speaker embedding extractor [28] consisting of a Res2net backbone architecture [29] with squeeze-excitation (SE) modules [30], a channel and context dependent statistics pooling layer and multi-layer feature aggregation to aggregate frame-level embeddings into utterance-level embeddings. Inputs are 80-dimensional mel-filterbank features. After aggregating frame-level representations, ASV embeddings are extracted using an affine transform with a fully-connected (FC) layer. Data augmentation techniques are applied using the room impulse response database [31] and additive noise recordings from the MUSAN database [32]. The network is trained using the recipe in [33] and a reproducible open source implementation. Further details are available in [28].\\n\\nAASIST [34] is an end-to-end state-of-the-art spoofing countermeasure system. It is based upon a RawNet2-based encoder [35, 36] and a spectro-temporal graph attention network (RawGAT-ST) [37], novel heterogeneous graph attention layers and max graph operations to integrate temporal and spectral representations. The output is generated using a readout operation and a hidden FC layer with two neurons. CM embeddings of 160 dimensions are extracted prior to the FC output layer. AASIST is also available as open source. Further details are available in [34].\\n\\n### 4.2. Baselines\\n\\nOpen source baselines include:\\n\\n1. **B1**: Score-sum fusion; and\\n2. **B2**: DNN back-end fusion.\\n\\nBoth utilise pre-trained (ECAPA-TDNN and AASIST) ASV and CM models. As illustrated in Figure 1-(a), baseline **B1** is a score-level back-end fusion method that combines ASV and CM sub-system outputs through score addition. Since it requires neither additional training nor fine-tuning, back-end processing is parameter-free. ASV scores are the cosine similarity between enrolment and test utterances. A softmax non-linearity is optionally applied to the CM scores, which are otherwise unbounded, to normalise the scores within a (0,1) range. The version of B1 with the softmax non-linearity is referred to as B1-v2.\\n\\nAs illustrated in Figure 1-(b), baseline **B2** utilises an embedding-level fusion whereby a DNN operates upon a pair of speaker embeddings extracted from enrolment and test utterances, and a CM embedding extracted only from the test utterance. The DNN is a vanilla multi-layer perception comprising three FC layers with leaky ReLU non-linear activation functions after each layer. The output layer consists of two neurons which correspond to the target and non-target (both bona fide and spoofed) classes. The model is trained using the training partition of the ASVspoof 2019 LA database. The larger-scale VoxCeleb2 database cannot be used for training since it contains only bona fide utterances. Full details regarding challenge baselines are available in [38].\\n\\n### 5. Challenge results\\n\\nFrom among 53 registrations, we received 23 submissions. SASV-EER results for each submission and the baselines for the evaluation partition are presented in Table 2. Without any further explanation, the results are as follows:\\n\\n| Team ID | EER (%) |\\n|---------|---------|\\n| IDV oice | 0.13 |\\n| DKU | 0.21 |\\n| Hyu | 0.28 |\\n| DoubleRoc | 0.37 |\\n| FlySpeech | 0.56 |\\n| IRLAB | 0.56 |\\n| VicomSpeech | 0.84 |\\n| CUHK NTU | 0.89 |\\n| MARG | 1.15 |\\n| NII TJU | 1.19 |\\n| UR AIR | 1.34 |\\n| clips | 1.36 |\\n| orange Lium | 1.48 |\\n| magnum | 1.71 |\\n| VTCC | 1.86 |\\n| SHELEZY AKA | 2.48 |\\n| DeepASV | 2.77 |\\n| xmuspeech | 2.89 |\\n| HCCL | 4.30 |\\n| SHELEZY | 4.48 |\\n| CAU KU | 4.95 |\\n| Tandem | 6.22 |\\n| NII | 6.54 |\\n| ASV sub-system | 6.54 |\\n| B1 | 19.31 |\\n| B1-v2 | 23.83 |\\n| B2 | 24.32 |\\n\\nFull results are available at [https://sasv-challenge.github.io/challenge_results](https://sasv-challenge.github.io/challenge_results)\"}"}
{"id": "jung22c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"countermeasure, the ECAPA-TDNN ASV sub-system gives an SASV-EER of 23.83%. Among the baseline solutions and with an SASV-EER of 1.71%, baseline B1-v2 gives the best performance. With v2 having been developed after the release of the evaluation plan, in the remainder of this paper we emphasise v1 of the B1 baseline which gives an SASV-EER of 19.31%, a result which serves to show the importance of score normalisation prior to fusion. B2 gives an SASV-EER of 6.54%.\\n\\nFrom among the 23 submissions, 21 outperformed the B1 and B2 baselines. The submissions of 8 teams show SASV-EERs of below 1%, a substantial reduction from that of the B1 baseline and is even below the SV-EER of 1.63% (assessment without spoofed trials). These results are extremely encouraging and demonstrate the synergistic merit of combined ASV and CM systems. The top-performing system achieves an SASV-EER of 0.13% which corresponds to a 92% relative reduction compared to the SV-EER of the ECAPA-TDNN ASV system.\\n\\nDetection error trade-off (DET) curves for all 23 submissions in addition to the baseline are plotted in Figure 2. Highlighted green, brown, red and blue profiles correspond to the top-performing submission and the B2, B1 baselines and ECAPA-TDNN ASV system respectively. The plots show substantial differences in the trade-off between false rejections and false acceptances and point toward the potential for tuning combined ASV and CM solutions for operating points other than the SASV-EER. Even so, the DET profile for the top-performing submission is below that of every other system. The elbow that is visible in many profiles stems from assessment being performed using both bona fide and spoofed non-target trials. Even if SASV-EER results show higher correlation with SV-EER than with SPF-EER results, the SASV-EER is still a composite of the SV-EER and the SPF-EER.\\n\\n6. Solution strategies\\n\\nAll three systems with the lowest SASV-EER employed similar strategies [39\u201341]. First, they are all fusions of independent ASV and CM sub-systems. Second, they are all ensembles of multiple ASV+CM systems. Third, each of the three systems gives similarly low SV-EERs and SPF-EERs indicating that they are insensitive to the proportion of spoofed non-target trials. SV-EER and SPF-EER results for each system, for which we have no space to report here, are available online.\\n\\nThere are differences too. The IDVoice [39] system fuses ASV and CM sub-systems at the score-level. The DKU system performs fusion at the decision-level, whereas embedding level fusion is employed in the Hyu system. Among their different systems, IDVoice use four different scores: ASV cosine similarity with and without score normalisation; CM cosine similarity; the score output of an end-to-end CM. Quality measurement functions [42] are also used to normalise the score for each trial according to various meta information such as the duration of speech. The DKU [40] system uses a modified cascade framework which combines decisions produced by the ASV sub-system with scores produced by the CM. Team HYU [41] focused on developing a latent space in which both speaker identity and spoofing artefacts can be captured using ASV and CM sub-system embeddings. A DNN with condition layers is used to map ASV and CM embeddings into a single SASV embedding [43]. There are further differences in the training strategies which are described in the participants' system descriptions and associated articles.\\n\\n7. Discussion and Conclusions\\n\\nThe SASV Challenge was formed to promote the study of jointly optimised or fused and single solutions to reliable ASV. Reliability in this sense implies resistance to both bona fide and spoofed non-target trials. We received submissions from 23 participating teams who all designed competing solutions using a common training and development protocol. Results show substantial improvements over two Challenge baselines. At 0.13%, the lowest equal error rate for a spoofing-aware speaker verification system is even well below the equal error rate of a conventional, state-of-the-art speaker verification system assessed in the absence of spoofed trials. This is an especially encouraging result which not only indicates the reliability of today's speaker verification and spoofing countermeasure technologies, but also the synergistic merit in their combination. There is also evidence that such impressive levels of reliability are consistent no matter what fraction of non-target trials that are spoofed; similarly low SV-EER and SPF-EER results show that performance is insensitive to the spoofing prior. We hope that these findings help to belay concerns of vulnerabilities to spoofing.\\n\\nLooking to the future, we acknowledge some limitations of the current database. All top-performing systems remain ensemble combinations of independent countermeasure and speaker verification sub-systems. We anticipate future advances in joint optimisation stemming from the development of truly integrated, single system approaches. Research in this direction will likely depend on the availability of larger databases containing not only spoofing attacks generated with diverse attack algorithms, but also data collected from many more speakers.\\n\\nWe hope that these developments may improve yet further on what are already extremely encouraging results.\\n\\n8. Acknowledgements\\n\\nWe thank members of the 23 teams who submitted results to this, inaugural edition. We also thank advisory committee members, H\u00b4ector Delgado, Kong Aik Lee, Massimiliano Todisco, Md Sahidullah, and Xuechen Liu, for their help and support.\"}"}
{"id": "jung22c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. References\\n\\n[1] J. Yamagishi, X. Wang et al., \u201cASVspoof2021: Accelerating progress in spoofed and deep fake speech detection,\u201d in Proc. ASVspoof 2021 Workshop (Interspeech satellite), 2021.\\n\\n[2] P. L. De Leon, M. Pucher, J. Yamagishi et al., \u201cEvaluation of speaker verification security and detection of hmm-based synthetic speech,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 8, pp. 2280\u20132290, 2012.\\n\\n[3] A. Sizov, E. Khoury, T. Kinnunen et al., \u201cJoint speaker verification and antispoofing in the i-vector space,\u201d IEEE Transactions on Information Forensics and Security, vol. 10, no. 4, 2015.\\n\\n[4] T. Kinnunen, H. Delgado et al., \u201cTandem assessment of spoofing countermeasures and automatic speaker verification: Fundamentals,\u201d IEEE/ACM TASLP, vol. 28, 2020.\\n\\n[5] J.-w. Jung, H. Tak, H.-j. Shim et al., \u201cSASV Challenge 2022: A Spoofing Aware Speaker Verification Challenge Evaluation Plan,\u201d arXiv preprint:2201.10283, 2022.\\n\\n[6] Y. Zhang, G. Zhu et al., \u201cA Probabilistic Fusion Framework for Spoofing Aware Speaker Verification,\u201d arXiv preprint:2202.05253, 2022.\\n\\n[7] P. Zhang, P. Hu and X. Zhang, \u201cNorm-constrained score-level ensemble for spoofing aware speaker verification,\u201d in Proc. Interspeech, 2022.\\n\\n[8] Z. Teng, Q. Fu et al., \u201cSA-SASV: An End-to-End Spoofing-Aggregated Spoofing-Aware Speaker Verification System,\u201d in Proc. Interspeech, 2022.\\n\\n[9] W. Kang, J. Alam and A. Fathan, \u201cEnd-to-end framework for spoof-aware speaker verification,\u201d in Proc. Interspeech, 2022.\\n\\n[10] X. Liu, M. Sahidullah and T. Kinnunen, \u201cSpoofing-Aware Speaker Verification with Unsupervised Domain Adaptation,\u201d arXiv preprint:2203.10992, 2022.\\n\\n[11] L. Zhang, Y. Li, H. Zhao and L. Xie, \u201cBackend Ensemble for Speaker Verification and Spoofing Countermeasure,\u201d in Proc. Interspeech, 2022.\\n\\n[12] H. Wu, L. Meng, J. Kang et al., \u201cSpoofing-Aware Speaker Verification by Multi-Level Fusion,\u201d in Proc. Interspeech, 2022.\\n\\n[13] Z. Chang, L. Zhang et al., \u201cSpoofing-Aware Attention based ASV Back-end with Multiple Enrollment Utterances and a Sampling Strategy for the SASV Challenge 2022,\u201d in Proc. Interspeech, 2022.\\n\\n[14] X. Wang, J. Yamagishi et al., \u201cASVspoof 2019: a large-scale public database of synthetized, converted and replayed speech,\u201d Computer Speech & Language (CSL), vol. 64, 2020, 101114.\\n\\n[15] F. Alegre, A. Amehraye and N. Evans, \u201cSpoofing countermeasures to protect automatic speaker verification from voice conversion,\u201d in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2013, pp. 3068\u20133072.\\n\\n[16] E. Khoury, T. Kinnunen, A. Sizov et al., \u201cIntroducing i-vectors for joint anti-spoofing and speaker verification,\u201d in Proc. Interspeech. Citeseer, 2014.\\n\\n[17] M. Sahidullah, H. Delgado, M. Todisco et al., \u201cIntegrated spoofing countermeasures and automatic speaker verification: An evaluation on ASVspoof 2015,\u201d in Proc. Interspeech, 2016.\\n\\n[18] M. Todisco, H. Delgado, K. A. Lee et al., \u201cIntegrated presentation attack detection and automatic speaker verification: Common features and gaussian back-end fusion,\u201d in Proc. Interspeech, 2018.\\n\\n[19] J. Li, M. Sun and X. Zhang, \u201cMulti-task learning of deep neural networks for joint automatic speaker verification and spoofing detection,\u201d in Proc. APSIPA, 2019.\\n\\n[20] J. Li, M. Sun, X. Zhang and Y. Wang, \u201cJoint decision of anti-spoofing and automatic speaker verification by multi-task learning with contrastive loss,\u201d IEEE Access, 2020.\\n\\n[21] H.-j. Shim, J.-w. Jung, J.-h. Kim and H.-j. Yu, \u201cIntegrated replay spoofing-aware text-independent speaker verification,\u201d Applied Sciences, 2020.\\n\\n[22] A. Gomez-Alanis, J. A. Gonzalez-Lopez et al., \u201cOn Joint Optimization of Automatic Speaker Verification and Anti-Spoofing in the Embedding Space,\u201d IEEE Transactions on Information Forensics and Security, vol. 16, 2021.\\n\\n[23] A. Kanervisto, V. Hautam\u00e4ki, T. Kinnunen and J. Yamagishi, \u201cOptimizing Tandem Speaker Verification and Anti-Spoofing Systems,\u201d IEEE/ACM Transactions on Audio Speech and Language Processing (TASLP), vol. 30, pp. 477\u2013488, 2022.\\n\\n[24] Y. Zhao, R. Togneri and V. Sreeram, \u201cMulti-task learning-based spoofing-robust automatic speaker verification system,\u201d Circuits, Systems, and Signal Processing, pp. 1\u201322, 2022.\\n\\n[25] X. Wu, R. He, Z. Sun and T. Tan, \u201cA light cnn for deep face representation with noisy labels,\u201d IEEE Transactions on Information Forensics and Security, vol. 13, no. 11, 2018.\\n\\n[26] J. S. Chung, A. Nagrani and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d in Proc. Interspeech, 2018.\\n\\n[27] J. Yamagishi, C. Veaux, K. MacDonald et al., \u201cCSTR VCTK corpus: English multi-speaker corpus for cstr voice cloning toolkit,\u201d 2019, http://dx.doi.org/10.7488/ds/1994.\\n\\n[28] B. Desplanques, J. Thienpondt and K. Demuynck, \u201cECAPA-TDNN: Emphasized Channel Attention, Propagation and Aggregation in TDNN Based Speaker Verification,\u201d in Proc. Interspeech, 2020.\\n\\n[29] S. Gao, M.-M. Cheng, K. Zhao et al., \u201cRes2net: A new multi-scale backbone architecture,\u201d IEEE transactions on pattern analysis and machine intelligence, 2019.\\n\\n[30] J. Hu, L. Shen and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in Proc. CVPR, 2018.\\n\\n[31] T. Ko, V. Peddinti, D. Povey et al., \u201cA study on data augmentation of reverberant speech for robust speech recognition,\u201d in Proc. ICASSP, 2017.\\n\\n[32] D. Snyder, G. Chen and D. Povey, \u201cMusan: A music, speech, and noise corpus,\u201d arXiv preprint:1510.08484, 2015.\\n\\n[33] R. K. Das, R. Tao and H. Li, \u201cHLT-NUS SUBMISSION FOR 2020 NIST Conversational Telephone Speech SRE,\u201d arXiv preprint:2111.06671, 2021.\\n\\n[34] J.-w. Jung, H.-S. Heo, H. Tak et al., \u201cAASIST: Audio Anti-Spoofing using Integrated Spectro-Temporal Graph Attention Networks,\u201d in Proc. ICASSP, 2022.\\n\\n[35] J.-w. Jung, S.-b. Kim, H.-j. Shim et al., \u201cImproved rawnet with filter-wise rescaling for text-independent speaker verification using raw waveforms,\u201d in Proc. Interspeech, 2020, pp. 1496\u20131500.\\n\\n[36] H. Tak, J. Patino, M. Todisco et al., \u201cEnd-to-end anti-spoofing with Rawnet2,\u201d in Proc. ICASSP, 2021.\\n\\n[37] H. Tak, J.-w. Jung, J. Patino et al., \u201cEnd-to-end spectro-temporal graph attention networks for speaker verification anti-spoofing and speech deepfake detection,\u201d in Proc. ASVspoof 2021 Workshop (Interspeech satellite), 2021.\\n\\n[38] H. Shim, H. Tak, X. Liu et al., \u201cBaseline systems for the first spoofing-aware speaker verification challenge: score and embedding fusion,\u201d in Proc. Speaker Odyssey Workshop, 2022.\\n\\n[39] A. Alenin, N. Torgashov, A. Okhotnikov et al., \u201cA Subnetwork Approach for Spoofing Aware Speaker Verification,\u201d in Proc. Interspeech, 2022.\\n\\n[40] X. Wang, X. Qin, Y. Wang et al., \u201cThe DKU-OPPO System for the 2022 Spoofing-Aware Speaker Verification challenge,\u201d in Proc. Interspeech, 2022.\\n\\n[41] J.-H. Choi, J.-Y. Yang, Y. Jeoung and J.-H. Chang, \u201cHYU submission for the SASV challenge 2022: Reforming speaker embeddings with spoofing-aware conditioning,\u201d in Proc. Interspeech, 2022.\\n\\n[42] A. Alenin, A. Okhotnikov, R. Makarov et al., \u201cThe ID R&D System Description for Short-duration Speaker Verification Challenge 2021,\u201d in Proc. Interspeech, 2021, pp. 2297\u20132301.\\n\\n[43] E. Perez, F. Strub, H. De Vries et al., \u201cFilm: Visual reasoning with a general conditioning layer,\u201d in Proc. AAAI, 2018.\"}"}
