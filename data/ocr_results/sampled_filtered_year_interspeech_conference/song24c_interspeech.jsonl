{"id": "song24c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ED-sKWS: Early-Decision Spiking Neural Networks for Rapid, and Energy-Efficient Keyword Spotting\\n\\nZeyang Song1, Qianhui Liu\u22171, Qu Yang1, Yizhou Peng1, Haizhou Li1,2\\n\\n1 National University of Singapore, Singapore\\n2 School of Data Science, Shenzhen Research Institute of Big Data, The Chinese University of Hong Kong, Shenzhen, Guangdong, 518172, P.R. China\\n\\nzeyang.song@u.nus.edu, qhliu@nus.edu.sg\\n\\nAbstract\\n\\nKeyword Spotting (KWS) is essential in edge computing requiring rapid and energy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for KWS for their efficiency and temporal capacity for speech. To further reduce the latency and energy consumption, this study introduces ED-sKWS, an SNN-based KWS model with an early-decision mechanism that can stop speech processing and output the result before the end of speech utterance. Furthermore, we introduce a Cumulative Temporal (CT) loss that can enhance prediction accuracy at both the intermediate and final timesteps. To evaluate early-decision performance, we present the SC-100 dataset including 100 speech commands with beginning and end timestamp annotation. Experiments on the Google Speech Commands v2 and our SC-100 datasets show that ED-sKWS maintains competitive accuracy with 61% timesteps and 52% energy consumption compared to SNN models without early-decision mechanism, ensuring rapid response and energy efficiency.\\n\\nIndex Terms: Keyword Spotting, Spiking Neural Network, Real-Time, Early Decision, Energy efficiency\\n\\n1. Introduction\\n\\nKeyword Spotting (KWS) refers to the task of identifying specific words or phrases within audio streams, necessitating both rapid and accurate recognition. This capability is foundational to numerous applications in edge computing, such as smartphones, smart speakers, and in-vehicle audio systems, where priorities include rapid real-time responses, energy efficiency, and high accuracy [1, 2].\\n\\nSpiking Neural Networks (SNNs) are the third generation of neural networks that more closely mimic the behavior of biological neural networks in the brain [3, 4, 5, 6]. Unlike traditional neural networks that use continuous values for activation, SNNs use discrete events (spikes) for information communication. Within an SNN, neurons remain inactive until their membrane potential surpasses a specific threshold. This event-driven mechanism endows SNNs with superior energy efficiency, making them especially suitable for KWS tasks [7, 8, 9, 10, 11]. Furthermore, the inherent temporal dynamics of spiking neurons align closely with the temporal nature of speech, rendering them inherently well-suited for speech processing [12, 4, 13].\\n\\nThis work was supported in part by Shenzhen Science and Technology Research Fund (Fundamental Research Key Project Grant No. JCYJ20220818103001002), Shenzhen Science and Technology Program ZDSYS20230626091302006 and IAF, A*STAR, SOITEC, NXP and National University of Singapore under FD-fAbrICS: Joint Lab for FD-SOI Always-on Intelligent & Connected Systems (Award I2001E0053).\\n\\n\u2217 Corresponding author.\\n\\nConsidering the timestep-by-timestep property of feed-forward SNN, similar to vanilla RNNs, SNNs are suitable for real-time KWS [14], which needs to process the input speech frame by frame.\\n\\nSeveral current SNN methods, like [7, 8, 9], generally require the complete input samples before making a final decision (called late-decision in this paper), thereby not leveraging SNNs' inherent ability for real-time processing to its fullest. To harness SNNs' real-time capabilities for rapid response and energy efficiency to their maximum, an early-decision mechanism has been incorporated into SNN models. This mechanism enables models to terminate processing and deliver outcomes at intermediate frames, facilitating significant energy conservation and improved response speed. However, this approach may lead to potential information loss due to premature data at the point of early decision. This trade-off highlights the need for training techniques that maintain high accuracy for early decision.\\n\\nThis paper proposes an early-decision SNN-based model for KWS, referred to as ED-sKWS (as shown in Fig. 1). This model is grounded in a fully-connected SNN architecture for real-time processing, coupled with a proposed Cumulative Temporal (CT) loss tailored for early-decision training. By feeding the feature of each speech frame into the corresponding timestep of SNN, the model can process speech information in a frame-synchronized manner, thus facilitating real-time processing and immediate response. The CT loss is proposed to enhance the prediction performance at early-decision timesteps, considering not only the current frame information but also the comprehensive contribution of historical information. This dual consideration allows for a more complete understanding and use of temporal information.\\n\\nExisting KWS datasets, like Google Speech Commands [15], adopt a standardized format by padding extracted utterances with zeros to ensure a consistent sample length. However, this poses a challenge in our early decision study as we cannot know the specific beginning and end time of the keyword, which complicates the accurate assessment of response speed. Even though we can consider the beginning and end of the sample as the beginning and end time of the keyword, it will lead to considerable inaccuracies. To overcome this limitation and offer an extensive KWS dataset for model evaluation, we present SC-100 including 100 speech commands with detailed timestamp annotations for the beginning and end timestamps of the commands. These annotations enable precise tracking of the command's actual occurrence, serving not only to quantify the benefits of early decision but also to be applied to other related speech processing applications, such as Voice Activity Detection (VAD).\\n\\nThe main contributions can be summarized as follows:\\n\\n\u2022 We introduce ED-sKWS, an early-decision SNN-based model for KWS.\\n\u2022 We present the SC-100 dataset including 100 speech commands with detailed timestamp annotations for the beginning and end timestamps of the commands.\\n\\nInterspeech 2024\\n1-5 September 2024, Kos, Greece\\n\\n10.21437/Interspeech.2024-1609\"}"}
{"id": "song24c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"The overall diagram of ED-sKWS with CT loss and early decision mechanism. The ED-sKWS system processes the fbank features of the input audio using a feed-forward SNN, which is indicated by the grey box. In the feedforward SNN, we feed frame $t$ to timestep $t$ of SNN, processing the data frame-by-frame. With an early-decision mechanism, the ED-sKWS ends processing and delivers the output once the confidence score ($CS_t$) exceeds a predefined threshold ($C$).\\n\\nFramework designed for fast, energy-efficient, and accurate keyword spotting in real time.\\n\\n- We propose a novel Cumulative Temporal (CT) loss for enhancing the early decision capabilities of SNNs.\\n- We develop a large-scale keyword spotting dataset SC-100 for early-decision evaluation, including 100 commonly used speech commands relevant to daily life. This dataset is characterized by the inclusion of precise beginning and end timestamps for each command, facilitating a more accurate and detailed analysis of keyword detection and timing.\\n\\n## 2. Methods\\n\\n### 2.1. SNN for real-time keyword spotting\\n\\nTo fully harness the capabilities of SNNs for real-time KWS, our ED-sKWS model, designed for real-time keyword spotting, utilizes a feed-forward fully connected SNN. Unlike the Spiking Convolutional Neural Network (SCNN) approaches [8, 9] that treat the spectrogram of input audio as an image, requiring additional timesteps for processing, our fully connected SNNs process a single frame of the spectrogram at one timestep. This process eliminates the need for extra timesteps and synchronizes the SNN model's timesteps with the audio utterances' time dimension, akin to Recurrent Neural Networks (RNNs). This frame synchronization facilitates predictions at every timestep, enabling real-time prediction and the potential for early decision.\\n\\nTo further enhance the temporal capacity of our model, we use adaptive Leaky Integrate-and-Fire (LIF) neurons similar to [16] as the spiking neuron. Our choice of the adaptive LIF neuron model, a variant of the standard LIF model [17], is motivated by its adaptive mechanisms that enhance control over information flow within the network. The adaptive LIF model's dynamics are described by the following equations:\\n\\n$$I_l[t] = \\\\beta \\\\sum_i w_i S_{l-1}[t-1] + a U_l[t-1] + b S_l[t-1]$$\\n\\n$$U_l[t] = \\\\alpha (U_l[t-1] - V_{th} S_{l-1}[t-1]) + I_l[t]$$\\n\\n$$S_l[t] = H(U_l[t] - V_{th})$$\\n\\nwhere $S_{l-1}[t-1]$ represents the input spike from layer $l-1$ at timestep $t-1$. The terms $I_l[t]$ and $U_l[t]$ represent the adaptive synaptic current and the membrane potential in layer $l$ at timestep $t$, respectively. The constants $\\\\alpha$ and $\\\\beta$ determine the information decays within the neuron. The adaptation mechanism is further controlled by parameters $a$ and $b$, which are associated with subthreshold dynamics and spike-triggered responses. The firing of the neuron is governed by a Heaviside step function, as denoted in Eq. 3. When the membrane potential $U_l[t]$ surpasses the firing threshold $V_{th}$, an output spike is generated.\\n\\n### 2.2. Cumulative Temporal (CT) loss\\n\\nIn this section, we describe the proposed training loss for our ED-sKWS model. In previous studies, several loss functions like the average spike-rate based loss [18] and cumulative loss [16] are used in various late-decision SNNs. They only compute the loss given the output in the last timestep, failing to optimize the output in the intermediate timesteps. TET loss [19] solved this problem by optimizing outputs in each isolated timestep, and proved to be effective in the static dataset where inputs in all timestep are identical. However, it does not perform well in continuously varying data like speech commands, as the TET loss fails to model the historical information. Therefore, we propose the Cumulative Temporal (CT) loss, which considers not only the current frame information but also the comprehensive contribution of historical information, allowing for a more complete understanding and use of temporal information. The CT loss is described as:\\n\\n$$O[t] = \\\\sum_{i=0}^{T} \\\\text{softmax}(U_R[i])$$\\n\\n$$L_{CT} = \\\\frac{1}{T} \\\\sum_{t=0}^{T} L_{CE}[O[t], y]$$\\n\\nwhere $O[t]$ denotes the cumulative historical information up to the current timestep, expressed as a softmax cumulative sum of potential over time. By optimizing $O[t]$ at each timestep, the cumulative output aligns more closely with the target distribution, facilitating accurate predictions in the intermediate timestep.\\n\\n### 2.3. Early decision method\\n\\nIn addition to the CT loss, an early-decision mechanism is essential for the ED-sKWS model to determine the optimal stopping point. We utilize the confidence score, denoted as $CS_t = \\\\max(\\\\text{softmax}(O[t]))$, as a proxy for this purpose. This score quantifies the model's confidence in classifying an input pattern into one of the object classes\u2014the higher the $CS_t$, the greater the model's confidence in its prediction. An early-decision threshold, $C$, is established: when $CS_t$ surpasses $C$, the model deems its prediction sufficiently reliable to halt further processing. Conversely, if $CS_t$ is below this threshold, the model persists in integrating information from the following frames until it reaches a level of confidence to make a decision.\"}"}
{"id": "song24c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To effectively assess the performance of early-decision SNN models in KWS, we utilize two key metrics: early-decision accuracy (Acc_t) and late-decision accuracy (Acc_T). Acc_t evaluates predictions made during the early-decision phase, whereas Acc_T measures the accuracy based on the output at the final timestep.\\n\\n3. SC-100 Dataset\\n\\nIn this study, we present the SC-100 dataset, specifically designed for the evaluation of early-decision performance in KWS systems. For an accurate assessment of response speed, we need to know when the end time of keywords in each sample. However, the widely used Google Speech Command dataset [15] offers keyword samples without temporal annotations indicating the end of keyword utterances. To address this gap, we develop a large-scale SC-100 dataset including 100 speech commands in our daily life, along with detailed timestamps annotations for the beginning and end time of each command within the audio samples, aiming to enhance the precision of early-decision assessments in KWS applications.\\n\\nTo develop SC-100 dataset, we first employ the Keyword-Miner tool [20] to extract the words from the LibriSpeech dataset [21]. The KeywordMiner consists of an aligner to obtain the word and the corresponding timestamp in the sentence samples and a segmenter to export the words from sentences in LibriSpeech. However, the raw output generated by KeywordMiner is not immediately suitable for direct use in KWS systems. First, not all segmented words can be considered keywords because their meanings may be inconsistent with the functional requirements of the KWS system, such as \u201ca\u201d or \u201cto\u201d; second, the duration of some word utterances is too short to be used as training or testing samples. To improve the quality of the dataset, we select 100 keywords that are representative and have a duration of more than 0.4 seconds.\\n\\nThe selected 100 keywords are partitioned to reflect the multifaceted scenarios of everyday life:\\n\u2022 Smart home: change, turn, set, on, off, light, white, dark, up, down, slowly, open, close, door, window.\\n\u2022 Entertainment: show, point, end, next, first, second, last, back, again, second, voice, quiet, black, blue, white, red, green, play, ready, rest.\\n\u2022 Alarm: set, stop, change.\\n\u2022 Robot Helper: go, return, come, stay, left, right, there, here, forward, in, out, straight, wait, hold, put, above, below, before, after, north, south, outside, top, find, give, make, get, leave.\\n\u2022 Office: book, use, table, room, time, hour, minute, work, zero, one, two, three, four, five, six, seven, eight, nine, office.\\n\u2022 Self assistant: call, send, speak, talk, read, take, bring, box, later, remember, tell, water, morning, afternoon, evening.\\n\\nThe SC-100 dataset contains 313,951 keyword utterances, with each class containing between 1,000 and 4,000 utterances. The potential class imbalances can be addressed by applying uniform sampling weights across all classes to facilitate fair training conditions. The extracted utterances vary in duration from 0.4 to 1 second. To offer a standardized format, we created 1-second audio samples by padding the extracted utterances with a random number of zeros at the beginning and end, similar to the Google Speech Commands dataset. However, it introduces a challenge in our early decision study or other related tasks: we cannot know the specific beginning and end time of the keyword, which complicates the accurate assessment of response speed. To address this issue, we meticulously annotate the beginning and end times of each keyword utterance within the samples. These timestamp annotations are crucial for precisely validating the early-decision mechanism, as detailed in Section 4.4. Additionally, these annotations are expected to facilitate the advancement of related technologies, such as VAD models.\\n\\n4. Experiment results\\n\\n4.1. Experiment setup\\n\\nIn this section, we evaluate the KWS performance on Google Speech Command Dataset V2 [15], which contains 105,829 one-second utterances of 35 commands, and our proposed SC-100 dataset. The sampling rate of these two dataset is 16kHz.\\n\\nFor preprocessing, we extract fbank features on the raw audio, using 40 filters with a window length of 25ms. Given that the sample length across both datasets is fixed at 1 second, this results in a total of 98 frames, which is also the number of timesteps in SNN.\\n\\nIn our proposed ED-sKWS model, we use feed-forward SNN with two hidden layers and a readout layer [16]. For fair comparison with previous works, our evaluation includes two model configurations: one with 128 hidden neurons (27.63K parameters) and another with 512 hidden neurons (306.80K parameters).\\n\\n4.2. Earlier decision time with comparable accuracy\\n\\nWe compared our proposed ED-sKWS with various existing SNN-based KWS models. Table 1 demonstrates superior performance with shorter decision time and lower energy consumption. This advantage stems from our early decision mechanism. Taking the example of model with 512 hidden neurons, our ED-sKWS can make predictions approximately 36 timesteps (also 36 frames) before the sample ends, reducing about 38% inference latency in the KWS system.\\n\\nFollowing [22, 23, 24], we estimate our computational energy consumption based on 45nm CMOS technology [25], with MAC operations consuming 4.6pJ of energy and accumulations consuming 0.9pJ. Compared to the adLIF baseline method [16], our ED-sKWS significantly reduces energy consumption by about 48% while only experiencing a performance decrease of 0.08%. This capability highlights the potential of our proposed ED-sKWS in KWS applications that demand high performance, rapid response, and energy efficiency.\"}"}
{"id": "song24c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: ED-sKWS with different loss functions\\n\\n| Loss type                | Acc<sub>t</sub>(%) | \u02dc<sub>td</sub> | Acc<sub>T</sub>(%) |\\n|-------------------------|--------------------|---------------|-------------------|\\n| Spike-rate loss [18]    | 88.18              | 65.52         | 92.73             |\\n| TET [19]                | 91.24              | 61.24         | 91.37             |\\n| Cumulative [16]         | 93.02              | 66.23         | 93.12             |\\n| CT loss                 | 93.04              | 60.46         | 93.15             |\\n\\nTable 3: Results on SC-100 dataset\\n\\n| Loss type                | Acc<sub>t</sub>(%) | \u02dc<sub>td</sub> | \u2206<sub>td</sub> | Acc<sub>T</sub>(%) |\\n|-------------------------|--------------------|---------------|--------------|-------------------|\\n| Spike-rate loss [18]    | 90.72              | 70.32         | +7.33        | 91.76             |\\n| TET [19]                | 91.17              | 64.21         | +1.22        | 91.20             |\\n| Cumulative [16]         | 93.20              | 68.28         | +6.32        | 93.07             |\\n| ED-sKWS                 | 93.21              | 59.11         | -3.85        | 93.16             |\\n\\n4.3. Ablation study on different loss functions\\n\\nIn this section, we present an ablation study for the effectiveness of our proposed CT loss. We focus on comparing the average decision time among all samples \u02dc<sub>td</sub>, early-decision accuracy Acc<sub>t</sub>, late-decision accuracy at the last timestep Acc<sub>T</sub> and average spike rate over the whole sample R<sub>T</sub> across different types of loss functions. The loss functions for comparison are listed below:\\n\\n- Spike-rate loss: \\n  \\\\[ L_{CE} = L_{CE}(U_R[t], y) \\\\]\\n\\n- TET loss: \\n  \\\\[ L_{TET} = \\\\frac{1}{TP}[\\\\min(t, \\\\tau)] L_{CE}(U_R[t], y) \\\\]\\n\\n- Cumulative loss: \\n  \\\\[ L_{CE} = L_{CE}(P_t=0 \\\\text{softmax}(U_R[i]), y) \\\\]\\n\\nThe comparative results are presented in Table 2. This results reveals that the CT loss not only demonstrates superior performance in the final timestep of the sample, where all temporal information is available, but also achieves comparable early-decision accuracy with fewer timesteps. This finding highlights the efficacy of CT loss in optimizing decision-making accuracy both in the early stages and after the input sequence.\\n\\n4.4. Experiments on SC-100 dataset\\n\\nWe also test our ED-sKWS model on our proposed SC-100 dataset with beginning and end annotation for each keyword. A crucial part of our experiments involved comparing the model's early-decision time with the end time of the keyword (t<sub>end</sub>). We use the metric \u2206<sub>td</sub> = \\\\( \\\\frac{1}{NP} \\\\sum_{i=0}^{t_{end}} (t_{d} - t_{end}) \\\\) to measure the average discrepancy between the model's decision time (t<sub>d</sub>) and t<sub>end</sub>. This metric evaluates the precision of our model's early stopping ability. As shown in Table 3, our model, powered by the proposed CT loss, not only predicts keywords ahead of the end time of the keywords but also outperforms other loss functions in terms of early-decision accuracy Acc<sub>t</sub>, while maintaining the late-decision performance.\\n\\nFurthermore, an interesting observation from the SC-100 dataset is the higher early-decision accuracy Acc<sub>t</sub> compared to late-decision accuracy Acc<sub>T</sub>. Unlike the Google Speech Commands dataset, the average end time of keyword utterances in SC-100 dataset is around 62.96 timesteps, which means there will be a long silence after the keyword utterances. This scenario poses a memory challenge for spiking neural networks, as they need to preserve learned features until the final timestep. Here, our early-decision mechanism proves to be beneficial, enabling the model to predict flexibly around the estimated keyword endpoint, thereby mitigating the risk of information loss due to extended silences.\\n\\n4.5. How the early decision reduce energy consumption?\\n\\nIn this section, we explore the energy efficiency benefits of the early-decision SNN. As we mentioned Section 4.2, our proposed ED-sKWS can reduce energy consumption. For more detailed analysis, we do visualization on a sample from SC-100 dataset with the keyword utterance ending time annotation. Figure 2 shows the spike firing rate in all timesteps, which is correlated with energy consumption.\\n\\nIn Figure 2, at the beginning the spike firing rate remains notably low before the keyword begin time (t<sub>begin</sub>). This observation highlights the event-driven nature of the SNN model, which saves energy by not activating spiking neurons without audio inputs. The spiking neurons start to activate at the beginning of the keyword (t<sub>begin</sub>), marked by a yellow dashed line, leading to a noticeable increase in the spike firing rate to 0.2. At the keyword end time (t<sub>d</sub>), our ED-sKWS model has enough confidence to make the prediction, which is before the end of speech, demonstrating the rapid response. Interestingly, the spike firing rate does not immediately decrease after the keyword ends at (t<sub>end</sub>). Instead, it remains at about 0.175 for several timesteps before gradually decreasing. This is because the spiking neuron stores the historical information in the membrane potential U, and may still fire without the additional input. Our early-decision strategy can end the processing immediately after the early decision, significantly reducing the number of computation operations from the early-decision timestep (t<sub>d</sub>) to the late-decision timestep (t<sub>T</sub>), thereby enhancing the model's energy efficiency.\\n\\n5. Conclusion\\n\\nWe introduce ED-sKWS, an SNN-based model designed for early decision-making in real-time Keyword Spotting (KWS) tasks. This model is capable of making predictions at each timestep. Furthermore, we present the SC-100 dataset, featuring 100 daily life speech commands, each annotated with precise start and end timestamps to facilitate the evaluation of early decision timing. Experimental results on both Google Speech Command and SC-100 datasets reveal that our ED-sKWS model can achieve competitive performance with about 61% timesteps and 52% energy consumption, highlighting its capacity for rapid and energy-efficient keyword spotting. Additionally, the proposed CT loss significantly lowers the spike firing rate, further enhancing the model's energy efficiency.\"}"}
{"id": "song24c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] I. L\u00f3pez-Espejo, Z.-H. Tan, J. H. Hansen, and J. Jensen, \u201cDeep spoken keyword spotting: An overview,\u201d IEEE Access, vol. 10, pp. 4169\u20134199, 2021.\\n\\n[2] Z. Song, Q. Liu, Q. Yang, and H. Li, \u201cKnowledge distillation for in-memory keyword spotting model.\u201d in INTERSPEECH, 2022, pp. 4128\u20134132.\\n\\n[3] J. Wu, Y. Chua, M. Zhang, H. Li, and K. C. Tan, \u201cA spiking neural network framework for robust sound classification,\u201d Frontiers in Neuroscience, vol. 12, p. 836, 2018.\\n\\n[4] B. Yin, F. Corradi, and S. M. Botelho, \u201cAccurate and efficient time-domain classification with adaptive spiking recurrent neural networks,\u201d Nature Machine Intelligence, vol. 3, no. 10, pp. 905\u2013913, 2021.\\n\\n[5] Q. Liu, G. Pan, H. Ruan, D. Xing, Q. Xu, and H. Tang, \u201cUnsupervised aer object recognition based on multiscale spatio-temporal features and spiking neurons,\u201d IEEE Transactions on Neural Networks and Learning Systems, vol. 31, no. 12, pp. 5300\u20135311, 2020.\\n\\n[6] Q. Liu, M. Ge, and H. Li, \u201cIntelligent event-based lip reading word classification with spiking neural networks using spatio-temporal attention features and triplet loss,\u201d Information Sciences, vol. 675, p. 120660, 2024.\\n\\n[7] J. Wu, Y. Chua, and H. Li, \u201cA biologically plausible speech recognition framework based on spiking neural networks,\u201d in 2018 International Joint Conference on Neural Networks. IEEE, 2018, pp. 1\u20138.\\n\\n[8] E. Y\u0131lmaz, O. B. Gevrek, J. Wu, Y. Chen, X. Meng, and H. Li, \u201cDeep convolutional spiking neural networks for keyword spotting,\u201d in INTERSPEECH, 2020, pp. 2557\u20132561.\\n\\n[9] Q. Yang, Q. Liu, and H. Li, \u201cDeep residual spiking neural network for keyword spotting in low-resource settings.\u201d in INTERSPEECH, 2022, pp. 3023\u20133027.\\n\\n[10] P. Sun, E. Eqlimi, Y. Chua, P. Devos, and D. Botteldooren, \u201cAdaptive axonal delays in feedforward spiking neural networks for accurate spoken word recognition,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2023, pp. 1\u20135.\\n\\n[11] Z. Song, J. Wu, M. Zhang, M. Z. Shou, and H. Li, \u201cSpiking-leaf: A learnable auditory front-end for spiking neural networks,\u201d in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 226\u2013230.\\n\\n[12] G. Bellec, D. Salaj, A. Subramoney, R. Legenstein, and W. Maass, \u201cLong short-term memory and learning-to-learn in networks of spiking neurons,\u201d Advances in Neural Information Processing Systems, vol. 31, 2018.\\n\\n[13] Q. Yang, Q. Liu, N. Li, M. Ge, Z. Song, and H. Li, \u201cSV AD: A robust, low-power, and light-weight voice activity detection with spiking neural networks,\u201d in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2024, pp. 221\u2013225.\\n\\n[14] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim, and S. Ha, \u201cTemporal convolution for real-time keyword spotting on mobile devices,\u201d arXiv preprint arXiv:1904.03814, 2019.\\n\\n[15] P. Warden, \u201cSpeech Commands: A Dataset for Limited-Vocabulary Speech Recognition,\u201d ArXiv e-prints, Apr. 2018. [Online]. Available: https://arxiv.org/abs/1804.03209\\n\\n[16] A. Bittar and P. N. Garner, \u201cA surrogate gradient spiking baseline for speech command recognition,\u201d Frontiers in Neuroscience, vol. 16, p. 865897, 2022.\\n\\n[17] W. Gerstner and W. M. Kistler, Spiking neuron models: Single neurons, populations, plasticity. Cambridge university press, 2002.\\n\\n[18] Y. Wu, L. Deng, G. Li, J. Zhu, and L. Shi, \u201cSpatio-temporal backpropagation for training high-performance spiking neural networks,\u201d Frontiers in Neuroscience, vol. 12, p. 331, 2018.\\n\\n[19] S. Deng, Y. Li, S. Zhang, and S. Gu, \u201cTemporal efficient training of spiking neural network via gradient re-weighting,\u201d in International Conference on Learning Representations, 2022. [Online]. Available: https://openreview.net/forum?id=XNtisL32jv\\n\\n[20] M. C. Meneses, R. B. Holanda, L. V. Peres, and G. D. Rocha, \u201cSidi kws: A large-scale multilingual dataset for keyword spotting.\u201d in INTERSPEECH, 2022, pp. 4616\u20134620.\\n\\n[21] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in 2015 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2015, pp. 5206\u20135210.\\n\\n[22] Y. Li, Y. Guo, S. Zhang, S. Deng, Y. Hai, and S. Gu, \u201cDifferential spike: Rethinking gradient-descent for training spiking neural networks,\u201d Advances in Neural Information Processing Systems, vol. 34, pp. 23 426\u201323 439, 2021.\\n\\n[23] N. Rathi and K. Roy, \u201cDiet-snn: A low-latency spiking neural network with direct input encoding and leakage and threshold optimization,\u201d IEEE Transactions on Neural Networks and Learning Systems, 2021.\\n\\n[24] K. Che, L. Leng, K. Zhang, J. Zhang, Q. Meng, J. Cheng, Q. Guo, and J. Liao, \u201cDifferentiable hierarchical and surrogate gradient search for spiking neural networks,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 24 975\u201324 990, 2022.\\n\\n[25] M. Horowitz, \u201c1.1 computing's energy problem (and what we can do about it),\u201d in 2014 IEEE International Solid-state Circuits Conference Digest of Technical Papers. IEEE, 2014, pp. 10\u201314.\\n\\n[26] X. He, Y. Li, D. Zhao, Q. Kong, and Y. Zeng, \u201cMsat: biologically inspired multistage adaptive threshold for conversion of spiking neural networks,\u201d Neural Computing and Applications, pp. 1\u201317, 2024.\"}"}
