{"id": "patterson22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] D. Wang, \u201cTime-frequency masking for speech separation and its potential for hearing aid design,\u201d Trends in amplification, vol. 12, no. 4, pp. 332\u2013353, 2008.\\n\\n[2] S. Wang, G. Naithani, and T. Virtanen, \u201cLow-latency deep clustering for speech separation,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2019, pp. 76\u201380.\\n\\n[3] I. Fedorov, M. Stamenovic, C. Jensen, L.-C. Yang, A. Mandell, Y. Gan, M. Mattina, and P. N. Whatmough, \u201cTinylstms: Efficient neural speech enhancement for hearing aids,\u201d Proc. Interspeech 2020, pp. 4054\u20134058, 2020.\\n\\n[4] Q. Kong, Y. Wang, X. Song, Y. Cao, W. Wang, and M. D. Plumbley, \u201cSource separation with weakly labelled data: An approach to computational auditory scene analysis,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 101\u2013105.\\n\\n[5] S. Wisdom, A. Jansen, R. J. Weiss, H. Erdogan, and J. R. Hershey, \u201cSparse, efficient, and semantic mixture invariant training: Taming in-the-wild unsupervised sound separation,\u201d in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2021, pp. 51\u201355.\\n\\n[6] A. Hussain, J. Barker, R. Marxer, A. Adeel, W. Whitmer, R. Watt, and P. Derleth, \u201cTowards multi-modal hearing aid design and evaluation in realistic audio-visual settings: Challenges and opportunities,\u201d in Proc. International Workshop on Challenges in Hearing Assistive Technology, 2017.\\n\\n[7] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, \u201cLooking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation,\u201d ACM Transactions on Graphics (TOG), vol. 37, no. 4, pp. 1\u201311, 2018.\\n\\n[8] M. Gogate, K. Dashtipour, A. Adeel, and A. Hussain, \u201cCochleanet: A robust language-independent audio-visual model for real-time speech enhancement,\u201d Information Fusion, vol. 63, pp. 273\u2013285, 2020.\\n\\n[9] A. J. Kolarik, B. C. Moore, P. Zahorik, S. Cirstea, and S. Pardhan, \u201cAuditory distance perception in humans: a review of cues, development, neuronal bases, and effects of sensory loss,\u201d Attention, Perception, & Psychophysics, vol. 78, no. 2, pp. 373\u2013395, 2016.\\n\\n[10] E. Milanova and E. Milanov, \u201cProximity effect of microphone,\u201d in Audio Engineering Society Convention 110. Audio Engineering Society, 2001.\\n\\n[11] E. N. Milanov and E. B. Milanova, \u201cProximity effect frequency characteristics of directional microphones,\u201d in Audio Engineering Society Convention 108. Audio Engineering Society, 2000.\\n\\n[12] C. M. Harris, \u201cAbsorption of sound in air versus humidity and temperature,\u201d The Journal of the Acoustical Society of America, vol. 40, no. 1, pp. 148\u2013159, 1966.\\n\\n[13] H. L. Van Trees, Optimum array processing: Part IV of detection, estimation, and modulation theory. John Wiley & Sons, 2004.\\n\\n[14] J. G. Ryan and R. A. Goubran, \u201cNear-field beamforming for microphone arrays,\u201d in 1997 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, 1997, pp. 363\u2013366.\\n\\n[15] Y. R. Zheng, R. A. Goubran, and M. El-Tanany, \u201cRobust near-field adaptive beamforming with distance discrimination,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 12, no. 5, pp. 478\u2013488, 2004.\\n\\n[16] M. Yiwere and E. J. Rhee, \u201cDistance estimation and localization of sound sources in reverberant conditions using deep neural networks,\u201d Int. J. Appl. Eng. Res, vol. 12, no. 22, pp. 12 384\u201312 389, 2017.\\n\\n[17] E. L. Ferguson, S. B. Williams, and C. T. Jin, \u201cSound source localization in a multipath environment using convolutional neural networks,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2018, pp. 2386\u20132390.\\n\\n[18] M. Yiwere and E. J. Rhee, \u201cSound source distance estimation using deep learning: an image classification approach,\u201d Sensors, vol. 20, no. 1, p. 172, 2019.\\n\\n[19] D. A. Krause, A. Politis, and A. Mesaros, \u201cJoint direction and proximity classification of overlapping sound events from binaural audio,\u201d in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2021, pp. 331\u2013335.\\n\\n[20] H. Gamper and I. J. Tashev, \u201cBlind reverberation time estimation using a convolutional neural network,\u201d in Proc. IEEE International Workshop on Acoustic Signal Enhancement (IWAENC), 2018, pp. 136\u2013140.\\n\\n[21] N. J. Bryan, \u201cImpulse response data augmentation and deep neural networks for blind room acoustic parameter estimation,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 1\u20135.\\n\\n[22] A. F. Genovese, H. Gamper, V. Pulkki, N. Raghuvanshi, and I. J. Tashev, \u201cBlind room volume estimation from single-channel noisy speech,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2019, pp. 231\u2013235.\\n\\n[23] T. Nakatani, K. Kinoshita, and M. Miyoshi, \u201cHarmonicity-based blind dereverberation for single-channel speech signals,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 15, no. 1, pp. 80\u201395, 2006.\\n\\n[24] E. A. Habets, \u201cSpeech dereverberation using statistical reverberation models,\u201d in Speech Dereverberation. Springer, 2010, pp. 57\u201393.\\n\\n[25] Y. Luo and N. Mesgarani, \u201cReal-time single-channel dereverberation and separation with time-domain audio separation network,\u201d in Proc. Interspeech, 2018, pp. 342\u2013346.\\n\\n[26] O. Ernst, S. E. Chazan, S. Gannot, and J. Goldberger, \u201cSpeech dereverberation using fully convolutional networks,\u201d in Proc. European Signal Processing Conference (EUSIPCO), 2018, pp. 390\u2013394.\\n\\n[27] S. Wisdom, J. R. Hershey, K. Wilson, J. Thorpe, M. Chinen, B. Patton, and R. A. Saurous, \u201cDifferentiable consistency constraints for improved deep speech enhancement,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2019.\\n\\n[28] S. Hochreiter and J. Schmidhuber, \u201cLong short-term memory,\u201d Neural Computation, vol. 9, no. 8, pp. 1735\u20131780, 1997.\\n\\n[29] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. International Conference on Learning Representations (ICLR), 2015.\\n\\n[30] J. B. Allen and D. A. Berkley, \u201cImage method for efficiently simulating small-room acoustics.\u201d in The Journal of the Acoustical Society of America, 1979, p. 65(4):943\u2013950.\\n\\n[31] Z.-Q. Wang, H. Erdogan, S. Wisdom, K. Wilson, and J. R. Hershey, \u201cSequential Multi-Frame Neural Beamforming for Speech Separation and Enhancement,\u201d in Proc. Spoken Language Technology Workshop (SLT), 2021.\\n\\n[32] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar`e, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-light: A benchmark for asr with limited or no supervision,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2020, pp. 7669\u20137673, https://github.com/facebookresearch/libri-light.\\n\\n[33] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in Proc. IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[34] J. Le Roux, S. Wisdom, H. Erdogan, and J. R. Hershey, \u201cSDR\u2013half-baked or well done?\u201d in Proc. ICASSP, 2019, pp. 626\u2013630.\"}"}
{"id": "patterson22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Distance-Based Sound Separation\\nKatharine Patterson, Kevin Wilson, Scott Wisdom, John R. Hershey\\nGoogle Research, Cambridge MA\\n{kappi, kwwilson, scottwisdom, johnhershey}@google.com\\n\\nAbstract\\nWe propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds.\\n\\nIndex Terms: distance-based sound separation\\n\\n1. Introduction\\nExtracting estimates of clean speech in the presence of interference is a long-standing research problem in signal processing. This task is referred to as speech enhancement when the interference is non-speech, and speech separation when the interference is speech. More generally, sound separation refers to the extraction of a subset of sounds from a mixture of sounds. Hearing aids and other assisted listening devices are an important application for these methods, e.g. [1, 2, 3], with a typical task being to aid in conversations held in a noisy space. But implementing sound separation for assisted listening is challenging, requiring low algorithmic latency and limited computation and memory. Further, it is not always clear which sounds the user would like to hear.\\n\\nThe problem of selecting which sounds to enhance has been approached by focusing only on speech [2], or otherwise classifying the sounds [4, 5], as well as using visual input [6, 7, 8] to select the sounds of interest. However, these methods aren't always appropriate. Selective listening methods may be cumbersome and require user effort to control which sounds are enhanced. They also typically exclude sounds other than speech that the user may want to hear, such as nearby music, the clinking of wine glasses at dinner, or the tell-tale sound of a dropped set of keys. Class-based methods that try to include non-speech sounds can fail whenever there are interfering sounds of the same classes as the desired sounds.\\n\\nWe propose an alternative approach called distance-based sound separation, illustrated in Figure 1, in which we assume the user would like to hear any sounds that occur within a local region around them, and block sounds coming from farther away. A system that accomplishes this would allow the user to engage in normal conversations without the interference of a crowded environment, and without becoming deaf to non-speech sounds in their immediate area. One hypothesis is that there are cues for distance that are related to superficial characteristics of the sound, rather than to its fine spectro-temporal structure. If so, a system that relies on such cues might have both a computational advantage, and an advantage in terms of generalization, compared to a system that has to perform deep pattern recognition in order to separate sounds. Although humans have the perceptual ability to estimate the relative distance of sounds [9], there has been no prior work showing that distance-based separation is feasible.\\n\\nPossible cues for distance perception can be traced to physical effects. The intensity $I$ of the direct path component of the sound varies with distance $d$ according to inverse-square law $I \\\\propto 1/d^2$. In contrast, in an enclosed area, the intensity of the sound's reverberation, is roughly independent of its distance to the microphone. Thus the direct-to-reverberation (DRR) ratio decreases with distance, and has been shown to be a cue for human distance perception [9]. There is also a proximity effect with directional microphones in which low-frequencies are more accentuated for closer sources, but the effect is only strong for sources less than one meter away [10, 11]. Absorption by the air is a frequency-dependent effect of distance, and plays a strong role over distances beyond tens of meters [12]. Other effects of distance come from spatial effects that could be detected using an array of microphones. In the present work, however, we focus on what can be inferred using a single microphone.\\n\\nTo validate the concept of distance-based sound separation, we train neural network separation models using mixtures of near and far sounds, where the acoustic properties of the sounds have been emulated using an acoustic room simulator. This allows us to have ground-truth targets for the near and far signals relative to a distance threshold. In this work, we focus on the case where all sounds, both near and far, are speech. We leave the case where there are non-speech sounds for future work. Our results show that it is possible to perform separation solely based on distance. In particular, in some scenarios, such as with a distance threshold of 1.5 m, a single nearby speaker, and as many as four distant speakers, distance-based sound separation can achieve improvements of up to 4.4 dB in scale-invariant signal to noise ratio.\"}"}
{"id": "patterson22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related Work\\n\\nBeamforming [13] uses multiple microphones and generally uses the direction of arrival (DOA) of sources as a cue to separate them. Near-field beamforming methods can additionally estimate source distance using the spherical nature of acoustic wavefronts [14, 15], though they can only distinguish distances of nearby sounds within a range limited by the size of the microphone array. In contrast, our method can learn to operate over a wider range of distances. Distance estimation has also been investigated using neural networks with two microphones [16, 17, 18, 19]. However, our method only requires a single microphone, and it performs separation.\\n\\nPrior work has demonstrated the feasibility of estimating reverberation parameters such as T60 and DRR [9]. Neural networks can also be trained to estimate reverberation parameters from a single microphone [20, 21], as well as room volume [22]. The success of these methods suggests that neural networks are able to perceive cues from raw audio to accurately predict properties of acoustic transfer functions, and it is likely that neural networks could also be trained to estimate the distance of a source. Rather than predict reverb parameters, our proposed model implicitly leverages acoustic cues to identify near and far sources in order to separate them.\\n\\nSingle-channel dereverberation methods [23, 24, 25, 26] solve a somewhat related task to distance-based sound separation, in that they try to separate the direct path of a reverberant signal (the shortest path of sound propagation) from the reverberant components. In contrast, the goal of distance-based sound separation is to preserve the reverberance of sources, and to group sources together based only on whether they are near or far based on a distance threshold. Also, dereverberation methods often make the assumption of a single reverberant source, while our method can handle multiple sources.\\n\\nTo our knowledge, the work introduced here is the first to propose and demonstrate the separation of speech from a single microphone based solely on distance cues.\\n\\n3. Methods\\n\\n3.1. Model\\n\\nTo separate based on distance, we experimented with an architecture [27], that uses short-time Fourier transform (STFT) masking. For the STFT, we use a 32 ms square-root Hann window with 16 ms hop. The 0.3-power-compressed magnitude of the STFT, \\\\( Y \\\\), of the time-domain mixture, \\\\( y \\\\), is fed as input to \\\\( L \\\\) layers of uni-directional LSTMs [28] with \\\\( N \\\\) units each. Then, a fully-connected layer with a sigmoid activation is applied to create two masks for the input STFT \\\\( Y \\\\), \\\\( M_{\\\\text{near}} \\\\) and \\\\( M_{\\\\text{far}} \\\\). To ensure STFT consistency [27], the masked STFTs for near and far are each passed through inverse and forward STFT operations:\\n\\n\\\\[\\n\\\\hat{X}_{\\\\text{near}}|_{\\\\text{far}} = \\\\text{STFT}\\\\{\\\\text{iSTFT}(M_{\\\\text{near}}|_{\\\\text{far}} \\\\odot Y)\\\\}.\\n\\\\]\\n\\nThe training loss is mean-squared error between 0.3-power-compressed magnitude of target \\\\( X \\\\) and estimate \\\\( \\\\hat{X} \\\\) [27]:\\n\\n\\\\[\\nL(X, \\\\hat{X}) = \\\\sum_{f,t} (|X_{f,t}|^{0.3} - |\\\\hat{X}_{f,t}|^{0.3})^2.\\n\\\\]\\n\\nWe use weights of 0.8 on the near loss and 0.2 on the far loss, \\\\( 0.8L(X_{\\\\text{near}}, \\\\hat{X}_{\\\\text{near}}) + 0.2L(X_{\\\\text{far}}, \\\\hat{X}_{\\\\text{far}}) \\\\), which encourages the model to focus on the performance of near targets, which is more likely to be desired as the output in a practical application. For all experiments, we used the Adam optimizer [29] with a learning rate of \\\\( 3 \\\\times 10^{-5} \\\\), batch size 128, and trained for one million steps on 16 Google Cloud TPU v3 cores.\\n\\n3.2. Acoustic Simulation\\n\\nTo create a large amount of training data, we use an image-method [30] acoustic room simulator with frequency-dependent wall filters [31] to generate reverb impulse responses (RIRs) for rooms with varied acoustic properties, with randomized microphone and source locations. Basic distance-related phenomena such as the amplitude and DRR effects are well simulated. However, our simulation does not model the proximity effect and the air absorption effect, which might provide additional cues for distance-based separation in the real world. Clean speech recordings are randomly assigned to the source locations within each room and convolved with the corresponding RIRs. We created training examples for each room by combining all sources within a threshold distance into a near target \\\\( x_{\\\\text{near}} \\\\), and all sources beyond a threshold distance into a far target \\\\( x_{\\\\text{far}} \\\\). The near and far targets are added to create mixture \\\\( y = x_{\\\\text{near}} + x_{\\\\text{far}} \\\\), and fed into the model to be separated.\\n\\nWithin each of the randomly generated rooms, we randomly generate 5 speaker locations and 1 microphone location, such that the distance between the microphone and each source was uniformly distributed, subject to the rejection of samples falling outside each room. The resulting distribution of distances from the microphones is illustrated in Figure 2. Note that the spatial distribution differs from what might be expected in, for example, a restaurant setting, where the average number of sources at a given distance radius will increase as the distance increases.\\n\\n4. Experiments\\n\\n4.1. Data Preparation\\n\\nFor the experiments, we use speech recordings from the Libri-light dataset [32] for training, with validation, and test partitions coming from LibriSpeech [33], so that speaker IDs are unique for each partition. For our synthetic room data, rooms are generated with dimensions varying from 3.0 \u00d7 4.0 \u00d7 2.13 meters to 7.0 \u00d7 8.0 \u00d7 3.05 meters.\\n\\nDuring training, randomly chosen Libri-light clips are reverbated and mixed according to a randomly chosen RIR to create clips that are 10 seconds in duration. Source utterances shorter than 10 seconds are offset at random intervals within the 10 second clip, and source utterances longer than 10 seconds are clipped to a random 10 second interior segment.\\n\\nThe source and microphone locations are used to determine which sources are near or far in relation to the microphone in the given room, for a given threshold distance. To vary the number of sources, we apply a source presence probability (SPP) to each source, so that the total number of sources in a room varies from 0 to 5, with a distribution dependent on the chosen SPP.\"}"}
{"id": "patterson22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Because the rooms are generated independently of a specific distance threshold, the choice of distance threshold affects the number of sources considered near versus far for any given room. In particular, it affects the fraction of examples where all the sources are considered near or all sources are considered far, leading to silent far targets or silent near targets. Distances thresholds of 0.8/1.5/3.0 meters result in training data with silent far target fractions of 0.0/0.001/0.04, respectively and silent near target fractions of 0.59/0.30/0.03 respectively. It is notable that some thresholds result in a majority of training examples containing a silent target, even for an SPP of 1.0.\\n\\n### 4.2. Metrics\\n\\nFor training examples where both near and far targets have sources present (i.e. neither target is silent), we use scale-invariant SDR improvement [34] (\\\\(\\\\text{SI-SDR}_x(\\\\hat{x}) = \\\\text{SI-SDR}_x(x) - \\\\text{SI-SDR}_y(x,y)\\\\)). SI-SDR measures signal fidelity with respect to a reference signal while allowing for a gain mismatch:\\n\\n\\\\[\\n\\\\text{SI-SDR}_x(x, \\\\hat{x}) = 10 \\\\log_{10} \\\\left( \\\\frac{\\\\|\\\\alpha x\\\\|^2}{\\\\|\\\\alpha x - \\\\hat{x}\\\\|^2} \\\\right),\\n\\\\]\\n\\nwhere \\\\(\\\\alpha = \\\\text{argmin}_a \\\\|ax - \\\\hat{x}\\\\|^2 = \\\\frac{x^T \\\\hat{x}}{\\\\|x\\\\|^2} \\\\).\\n\\nSI-SDRi diverges when one of the targets is silent, because the non-silent target will be exactly equal to the input mixture and achieve \\\\(\\\\pm \\\\infty\\\\) dB SI-SDR, and the silent target will have \\\\(-\\\\infty\\\\) dB SI-SDR. This makes any improvement calculation meaningless. For examples with a silent near target, we use a noise reduction metric, to measure how much of the far sound leaks into the silent near output:\\n\\n\\\\[\\n\\\\text{NoiseReduction}_y(\\\\hat{x}_{\\\\text{near}}) = 10 \\\\log_{10} \\\\left( \\\\frac{\\\\|y\\\\|^2}{\\\\|\\\\hat{x}_{\\\\text{near}}\\\\|^2} \\\\right),\\n\\\\]\\n\\nwhich measures the power reduction of the separated output \\\\(\\\\hat{x}_{\\\\text{near}}\\\\) relative to the input audio mixture power \\\\(y\\\\).\\n\\n### 5. Results\\n\\nResults are shown in Table 1 for an evaluation set of 1000 examples with a distance threshold of 1.5 meters, model size of \\\\(L = 4\\\\) and \\\\(N = 400\\\\) (6.3M parameters), SPP of 0.5, and with all 5 speakers present. They are bucketed according to number of near speakers (1 near speaker implies 4 far speakers, etc).\\n\\nWe compare our distance-based separation network to a conventional speech separation baseline in which we train the same network architecture (but with more masking outputs and a permutation-invariant loss) to output 5 separate source estimates, one for each possible speaker. To upper-bound the performance of this baseline, we use oracle knowledge of the true near and far targets to find the best grouping of the 5 separate source estimates into near and far estimates. In all but one condition, direct distance-based separation outperforms this upper bound on our \u201cseparate-all\u201d baseline.\\n\\nFigure 3 shows scatter plots of near input SI-SDR versus SI-SDR improvement for individual examples. The mean noise reduction (3) for this model across the 271 evaluation examples with silent near targets (n/a in Table 1) is 48.9 dB. Demos are online at https://google-research.github.io/sound-separation/papers/distance-based-separation-interspeech2022.\\n\\n### 5.1. Effect of distance threshold\\n\\nThe results of training and evaluating with different distance thresholds is shown in Table 2.\\n\\n| Distance (m) | SI-SDRi, 1 near source | Noise Reduction |\\n|-------------|------------------------|-----------------|\\n| 0.8         | 4.0                    | 63.9            |\\n| 1.5         | 4.4                    | 48.9            |\\n| 3.0         | 1.9                    | 22.8            |\\n\\nPerformance generally degrades with increased distance threshold values, despite the fact that higher thresholds resulted in more non-silent training targets. This can be seen in Figure 2 and Section 4.1 where a threshold at 3 meters is closer to the center of the distribution than a threshold of 0.8 or 1.0, and thus will change the training examples to have fewer silent targets.\\n\\nThe correlation between increased performance and decreased silent targets seen in Table 4 suggests that the degradation seen in Table 2 with increased distance threshold is a true artifact of the distance, as opposed to an effect of the change in training data between models.\\n\\n### 5.2. Effect of model size\\n\\nAll combinations of LSTM depth \\\\(L \\\\in \\\\{2, 4, 6\\\\}\\\\) and width \\\\(N \\\\in \\\\{200, 400, 600\\\\}\\\\) are evaluated, except \\\\(L = 6\\\\) and \\\\(N = 600\\\\), which was too large for our resource constraints. Overall, larger models performed better, with LSTM width \\\\(N\\\\) contributing more to performance increases. The SI-SDR difference between the smallest model and the largest model for distance thresholds of 0.8 m, 1.5 m, and 3.0 m was 0.9 dB near / 1.2 dB far, 1.5 dB near / 1.6 dB far, and 1.8 dB near / 1.4 dB far, respectively. Note that for increasing model size, SI-SDRi improves more for higher distance thresholds.\\n\\n### 5.3. Effect of varied speaker count in evaluation set\\n\\nTo explore whether the model uses the relationship between the number of near sources and the number of far sources, we vary the SPP. For instance, an SPP of 1.0 indicates that all 5 sources are present in the mixture, and in general a SPP of \\\\(p\\\\) induces a binomial distribution \\\\(B(5, p)\\\\) over number of sources. This also modifies the distribution of near and far speaker counts. Our baseline model evaluated in Table 1 was trained with a SPP of 0.5, but our evaluation set used in other sections uses a SPP of 1.0 for ease of slicing data by number of near speakers. This means that training data and the evaluation data differ in terms...\"}"}
{"id": "patterson22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In order to study this mismatch, we also created a version of the evaluation set with a SPP of 0.5. This provides a set with 527 silent near target examples, 411 examples with sound in near and far (i.e. 1-4 speakers in each target, sum \u2264 5), and 62 examples with silent far targets. We also trained our baseline model with SPP of 1.0 instead of 0.5. Results for the baseline model and eval data at SPPs of 0.5 and 1.0 are in Table 3.\\n\\n| Model | SPP | Eval SPP | SI-SDRi all non-silent | Noise reduction |\\n|-------|-----|----------|------------------------|-----------------|\\n|       |     |          | Near | Far |       |          | Near | Far |\\n|       | 0.5 | 1        | 2.9  | 6.8 | 48.9  |         |       |     |\\n|       | 0.5 | 0.5      | 3.2  | 9.6 | 56.3  |         |       |     |\\n|       | 1   | 1        | 3.4  | 7.2 | 40.6  |         |       |     |\\n|       | 1   | 0.5      | 2.8  | 2.6 | 46.8  |         |       |     |\\n\\nModels do best when their training SPP matches the eval data's SPP, as expected. However, the effect of mismatch for near SI-SDRi is relatively minor. For far SI-SDRi, we observe more degradation with mismatched SPPs, indicating that perhaps source count is an important cue for far source estimation.\\n\\n5.4. Effect of silent targets\\n\\nTo study the effect of silent targets, we pre-filtered our RIRs to achieve certain percentages of silent targets (before applying SPP). Results are shown in Table 4 for SPPs of 1.0 and 0.5. Note that SPP and percentage of rooms with silent targets are compounding effects. As expected, showing the model more examples with non-silent targets increases SI-SDRi on non-silent examples, but degrades noise reduction on silent near examples.\\n\\n| SPP   | % rooms silent target | SI-SDRi all non-silent | Noise reduction |\\n|-------|-----------------------|------------------------|-----------------|\\n|       |                       | Near | Far |       |          | Near | Far |\\n| 1.0   | 0                     | 6.7  | 7.8 | 23.7  |         |       |     |\\n| 1.0   | 30                    | 5.0  | 6.9 | 40.6  |         |       |     |\\n| 0.5   | 0                     | 4.6  | 6.5 | 35.4  |         |       |     |\\n| 0.5   | 30                    | 4.4  | 6.8 | 48.9  |         |       |     |\\n\\n6. Conclusions\\n\\nWe proposed the novel task of separating mixtures of signals based on their distance from a single microphone. To address this task we applied an existing high-performing sound separation model architecture, trained on synthetic data prepared to exemplify the primary qualities of sound that vary with distance. Experiments showed promising initial performance on this task, with our model benchmark 1.5 meter distance threshold model producing an average improvement of 4.4 dB in SI-SDR for nearby sounds.\\n\\nIn future work, we plan to handle more realistic scenarios with both speech and non-speech and to investigate multi-channel approaches. We also aim to elucidate the relative importance of acoustic cues, such as relative loudness and DRR, in distance-based separation networks, and evaluate their performance in real acoustic conditions. Training with more realistic acoustic effects, including air absorption and proximity effects, may also be useful for generalization to real world conditions, and we plan to investigate this using improved simulations.\\n\\n7. Acknowledgements\\n\\nWe would like to acknowledge Dick Lyon and Malcolm Slaney for helpful discussions during the preparation of this work, and Hakan Erdogan for useful comments on the manuscript.\"}"}
