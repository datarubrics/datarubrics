{"id": "kothare24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How Consistent are Speech-Based Biomarkers in Remote Tracking of ALS Disease Progression Across Languages? A Case Study of English and Dutch\\n\\nHardik Kothare, Michael Neumann, Cathy Zhang, Jackson Liscombe, Jordi W J van Unnik, Lianne C M Botman, Leonard H van den Berg, Ruben P A van Eijk, Vikram Ramanarayanan\\n\\nAbstract\\n\\nPrevious work has demonstrated the utility of speech-based digital biomarkers for remotely tracking longitudinal progression in people with Amyotrophic Lateral Sclerosis (pALS). Here, we investigate the responsiveness of these biomarkers across languages for consistency. We collected audiovisual data using a cloud-based multimodal dialogue platform, where pALS interacted with a virtual guide to perform several speaking exercises. We automatically extracted speech, linguistic and orofacial metrics from 143 English-speaking pALS (36 bulbar onset, 107 non-bulbar onset) and 26 Dutch-speaking pALS (10 bulbar, 16 non-bulbar onset). We used growth curve models to estimate the trajectory of these metrics over time. We observe that for most of these metrics, English-speaking pALS and Dutch-speaking pALS follow similar trajectories, i.e. the slopes are not statistically different from each other, demonstrating the potential of such speech-based biomarkers for remote monitoring across languages.\\n\\nIndex Terms: multimodal digital biomarkers, amyotrophic lateral sclerosis, remote patient monitoring\\n\\n1. Introduction\\n\\nAmyotrophic Lateral Sclerosis (ALS) or motor neuron disease is a neurodegenerative disorder in which the degeneration of upper and lower motor neurons leads to muscle weakness and paralysis [1]. Median survival time from disease onset ranges from 20 to 48 months [2]. Approximately 30% of people with ALS (pALS) present with bulbar onset, marked by rapid deterioration in speech and swallowing abilities [3]. The remaining majority present with non-bulbar onset, which manifests as muscle wasting in the limbs and torso [4]. However, a significant portion of those with non-bulbar onset eventually develop bulbar symptoms as the disease advances [5]. This rapid decline of bulbar function makes it possible to use speech-based objective biomarkers to monitor and track pALS. Indeed, objective speech and facial kinematic measures have shown utility in early detection of bulbar symptoms [6, 7, 8, 9, 10, 11, 12]. Eshghi et al. [13] showed that speaking rate and speech intelligibility can predict speech loss based on pre-defined thresholds and that these objective speech biomarkers are more responsive to functional decline than patient-reported ALSFRS-R scores. Yunusova et al. [14] reported that changes in kinematics of the jaw and lips are detectable prior to changes in vowel acoustics and speech intelligibility. Stegmann et al. [15] demonstrated that disease progression in bulbar onset and non-bulbar onset pALS can be predicted using speaking rate and articulatory precision through data collected remotely via a mobile application. Multimodal speech-based biomarkers also show great potential in tracking the progression of bulbar decline in pALS [16, 14, 15, 13, 17]. Deployment of technology that is capable of collecting such multimodal speech-based digital biomarkers remotely shows promise in cost-effective and geographically-distributed clinical trials [18]. However, are speech-based biomarkers consistent in remote tracking of disease progression in pALS speaking different languages? Prior work suggests that certain characteristics of dysarthric speech in neurodegenerative disorders are language independent [19, 20, 21]. In this work we ask the following research questions using English and Dutch as a case study:\\n\\n1. Of the speech-based biomarkers that show statistically significant differences between bulbar onset and non-bulbar onset English-speaking pALS, which metrics show consistent differences in Dutch-speaking pALS cohorts?\\n2. Is the rate of change of biomarker values over time consistent across English-speaking and Dutch-speaking pALS?\\n\\nTo our knowledge, this is the first study that systematically analyses multimodal speech biomarkers collected in pALS via a dialogue-based remote assessment platform across multiple languages.\\n\\n2. Data\\n\\nData was collected from 143 English-speaking pALS and 26 Dutch-speaking pALS (see Table 1) through two ongoing studies in collaboration with EverythingALS and the Peter Cohen Foundation (English-speaking pALS) and the University Medical Center Utrecht (Dutch-speaking pALS). Both studies were approved by Institutional Review Boards [22, 23]. Audiovisual data was collected using the Modality platform, a cloud-based multimodal dialogue system in which a virtual guide, Tina, engages participants in a semi-structured conversation to elicit speech and facial behaviours [22, 23]. The following tasks were included in both the English and the Dutch protocol:\\n\\n- (a) read speech (sentence intelligibility test (SIT); Reading Passage (RP; English: Bamboo passage 99 words, Dutch: De Auto passage 103 words),\\n- (b) oral diadochokinesis (DDK, repeating the syllables /pA/, /tA/ and /kA/ in rapid succession),\\n- (c) single breath counting (SBC), and\\n- (d) free speech in form of a picture description task (PD).\\n\\n3. Methods\\n\\nRelevant speech acoustic, linguistic and orofacial metrics were automatically extracted from audio data collected using these tasks. Speech metrics were extracted using Praat (v6.2.17) [24] and the Montreal Forced Aligner (v2.0.0.a22) [25]. Facial video metrics were derived from facial landmarks generated using MediaPipe Face Mesh [26]. MediaPipe Face Detection, which is based on BlazeFace [27] is used to determine the (x,y) coordinates of facial landmarks.\"}"}
{"id": "kothare24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Participant demographics; age, number of sessions and time span reported as mean (standard deviation)\\n\\n|                      | F    | M    | Age in years | #Sessions | Time span per participant in months |\\n|----------------------|------|------|--------------|-----------|-------------------------------------|\\n| English-speaking     | 18   | 18   | 62.83 (8.23) | 15.97 (18.48) | 7.86 (8.18)                        |\\n| Bulbar onset         |      |      |              |           |                                     |\\n| Non-bulbar onset     | 52   | 55   | 62.02 (8.02) | 25.93 (25.61) | 13.63 (11.28)                     |\\n| Dutch-speaking       | 4    | 6    | 67.85 (3.69) | 4 (3.62)   | 3.3 (3.74)                         |\\n| Bulbar onset         |      |      |              |           |                                     |\\n| Non-bulbar onset     | 2    | 14   | 67.84 (7.56) | 5.31 (3.59) | 4.5 (3.71)                         |\\n\\nTable 2: Overview of extracted metrics. For facial metrics, functionals (minimum, maximum, average) are applied to produce one value across all video frames of an utterance. Facial metrics are measured in pixels and are normalized by dividing them by the intercaruncular distance (distance between inner corners of the eyes) for each participant. *specific to DDK task\\n\\n| Domain                | Exemplar Metrics                                                                 |\\n|-----------------------|----------------------------------------------------------------------------------|\\n| Energy                | shimmer (%), intensity (dB), signal-to-noise ratio (dB)                          |\\n| Speech                |                                                                                   |\\n| Timing                | speaking and articulation duration (sec.), articulation and speaking rate (WPM), |\\n|                       | percent pause time (PPT, %), canonical timing agreement (CTA, %), cycle-to-     |\\n|                       | cycle temporal variability* (cTV , sec.), syllable rate* (syl./sec.), # of       |\\n|                       | syllables*                                                                         |\\n| Voice quality         | cepstral peak prominence (CPP, dB), harmonics-to-noise ratio (HNR, dB)           |\\n| Frequency             | mean, max., min. fundamental frequency F0 (Hz), jitter (%)                        |\\n| Linguistic            |                                                                                   |\\n| Lexico-semantic       | word count, percentage of content words, noun rate, verb rate, pronoun rate,     |\\n|                       | noun-to-verb ratio, noun-to-pronoun ratio, closed class word ratio, idea density  |\\n| Mouth (distances)     |                                                                                   |\\n| Lip aperture/opening  |                                                                                   |\\n| Lip width             |                                                                                   |\\n| Mouth surface area    |                                                                                   |\\n| Mean symmetry ratio   |                                                                                   |\\n| Orofacial             |                                                                                   |\\n| Lip/Jaw Movement      | velocity, acceleration, jerk, and speed of lower lip and jaw center               |\\n| Eyes                  | number of eye blinks per sec., eye opening, vertical displacement of eyebrows     |\\n\\nTable 3: Feature clusters from hierarchical clustering and the selected representative features. AUC represents the mean AUC for distinguishing bulbar onset and non-bulbar onset pALS samples across five cross validation folds. Only features with AUC > 0.65 were included in the analysis. (RP: reading passage, DDK: diadochokinesis, cTV: cycle-to-cycle temporal variability, CTA: canonical timing alignment, PD: picture description, SIT: sentence intelligibility test, PPT: percentage pause time, HNR: harmonics-to-noise ratio, CPP: cepstral peak prominence, SBC: single breath count.)\\n\\n| Cluster description | Selected representative | AUC  |\\n|--------------------|-------------------------|------|\\n| Timing: duration and rates | Speaking duration (RP) | 0.84 |\\n| Temporal DDK measures | cTV (DDK) | 0.83 |\\n| Timing alignment | CTA (RP) | 0.83 |\\n| Duration and word count for PD | Word count (PD) | 0.83 |\\n| Eyebrow displacement | Max. eyebrow vert. displ. (SIT) | 0.78 |\\n| Pause time | PPT (SIT) | 0.77 |\\n| Lip width | Max. lip width (RP) | 0.72 |\\n| Voice quality (read/free speech) | HNR (SIT) | 0.71 |\\n| Cepstral peak prominence (CPP) | CPP (RP) | 0.69 |\\n| Voice quality for SBC and DDK | HNR (DDK) | 0.68 |\\n| Lip aperture, mouth surface area | Mean lip aperture (SIT) | 0.68 |\\n| Eye opening measures | Max. eye opening (SIT) | 0.68 |\\n| Content and closed class words | Closed class word ratio (PD) | 0.67 |\\n| Min. and mean F0 | Mean F0 (RP) | 0.67 |\\n| Jaw velocity for SIT | Max. jaw velocity down (SIT) | 0.66 |\\n| Duration measures for SBC and DDK | Syllable count (DDK) | 0.65 |\\n| Jaw velocity for RP | Max. jaw velocity up (RP) | 0.65 |\\n\\ny)-coordinates of the face for every video frame. We used 14 key landmarks to compute metrics like dynamics of articulators (jaw, lower lip), surface area of the mouth, and eyebrow raises. These facial features were normalised by dividing their values by the inter-caruncular distance to account for cross-participant variability due to position and movement relative to the camera [28]. Linguistic metrics were computed only for the picture description task, using the Python package spaCy [29] and automatic transcriptions of participant speech obtained using AWS Transcribe [2]. For an overview of automatically-extracted metrics, please see Table 2.\\n\\n3.1. Feature Selection\\n\\nTo handle multicollinear features and to identify a good set of representative features, we applied hierarchical clustering on the Spearman rank-order correlations, similar to the approach in [30]. For this feature selection, data from 135 healthy English-\\n\\nhttps://aws.amazon.com/transcribe/\"}"}
{"id": "kothare24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"speaking controls\\n\\n3 (71 female; mean age (standard deviation) = 59.9 (10.3) years) was considered in order to avoid data leakage in the experimental design and because data from healthy controls is most representative of normative feature ranges and correlations between features. Note that all subsequent analyses focus on patient data. Ward's method [31] was used for clustering and we plotted a dendrogram for visual inspection of the feature clusters. A distance threshold of 1.0 was chosen manually to select clusters that represented sensible feature groupings in terms of the domain. This threshold resulted in 27 clusters.\\n\\nNext, for every feature, receiver operating characteristic (ROC) curve analysis was run in a 5-fold cross validation setup (sklearn's StratifiedGroupKFold function) to determine the area under the ROC curve (AUC) for distinguishing bulbar onset participants from non-bulbar onset participants. There was no overlap of a participant's data between training and test folds.\\n\\nTo further filter features, we imposed a minimum threshold for the ROC-AUC. Only features with an AUC \u2265 0.65 were considered for further analysis. These features are enumerated in Table 3 with their AUC values.\\n\\n3.2. Group differences\\n\\nWe conducted non-parametric Kruskal-Wallis tests for the 17 metrics selected through feature selection, to test for differences between bulbar onset and non-bulbar onset pALS in both the English-speaking and Dutch-speaking cohorts. For this, we considered all data collected from the participants, irrespective of the timepoint in their disease progression. Effect sizes were measured as Glass' \u2206 [32] for all 17 metrics.\\n\\n3.3. Test-retest reliability\\n\\nFor all 17 metrics, we also calculated test-retest reliability in the form of average Pearson's correlation coefficients. Pairs of sessions from the same participant, within a 31-day span of each other, were considered for this correlation. Test-retest reliability values are mentioned in parentheses after metric names in Figure 1.\\n\\n3.4. Longitudinal trajectory\\n\\nGrowth curve models (GCMs) [33], which provide a linear fit for a non-linear mixed effects model, were run in R to estimate the trajectory of a metric over time in the English-speaking and Dutch-speaking cohorts, with random slopes and intercepts for each participant [15]. Growth curve models produce estimates of smoothed trajectories of change over time by using observed repeated measures of each individual. GCM curves for distinct cohorts can help identify differences in the longitudinal trajectory of measures in the two cohorts.\\n\\n4. Results\\n\\nFigure 1 shows effect sizes for all 17 metrics. For nine of the 17 metrics that showed differences between English-speaking bulbar pALS and non-bulbar pALS, the Dutch-speaking cohort also showed differences. These differences had the same directionality (bulbar > non-bulbar or vice versa) for all nine metrics in both language groups. Eight of the nine speech metrics showed differences in both language groups, whereas only one of the six facial metrics and none of the linguistic metrics showed differences in the Dutch bulbar versus Dutch non-bulbar comparison. Based on a visual inspection of Figure 1, we see that the magnitude of the effect size was similar across the English-speaking and Dutch-speaking pALS for two metrics in\\n\\n3 Data from Dutch-speaking controls was not available.\\n\\n4 The maximum distance at which all features would be combined into one single cluster was 5.4\\n\\nFigure 1: Effect sizes of speech, orofacial and linguistic metrics shown with 95% confidence interval. The numbers in parentheses after metric names are test-retest reliability values for English and Dutch data, in that order. Positive effect size indicates greater values in bulbar pALS. RP: reading passage, CPP: cepstral peak prominence, DDK: diadochokinesis, cTV: cycle-to-cycle temporal variability, SIT: sentence intelligibility test, PPT: percentage pause time, HNR: harmonics-to-noise ratio, PD: picture description, CTA: canonical timing alignment.\\n\\nparticular: SIT HNR and RP CTA. Most of the metrics showed similar test-retest reliability values in the Dutch and English cohorts except for jaw velocity metrics, DDK cTV and SIT PPT. When it came to the longitudinal trajectories of metrics, there were no differences in slopes for 14 of the 17 metrics. For three facial metrics \u2014 SIT maximum eyebrow vertical displacement, SIT mean lip aperture and SIT maximum eye opening \u2014 the Dutch-speaking cohort and English-speaking cohort displayed statistically different slopes or longitudinal trajectories (see Table 4). Since SIT maximum eye opening shows differences between the bulbar and non-bulbar cohorts in Dutch-speaking pALS (Figure 1), we looked at whether the differences between English-speaking pALS and Dutch-speaking pALS are driven by either the bulbar or non-bulbar cohorts. To do this, we compared longitudinal trajectories in Dutch bulbar pALS to those in English bulbar pALS, and the same for non-bulbar pALS. The three facial metrics had different slopes in the Dutch-speaking cohort and English-speaking cohort for both bulbar and non-bulbar pALS. In addition to the three facial metrics, when bulbar onset pALS were considered, PD word count, PD closed class word ratio and RP max. jaw velocity up also had different slopes. Whereas, DDK HNR had different trajectories in the English-speaking cohort and Dutch-speaking cohort when non-bulbar pALS were considered.\"}"}
{"id": "kothare24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Longitudinal trajectory of metrics. 143 English-speaking pALS and 26 Dutch-speaking pALS. An asterisk (*) indicates that the longitudinal trajectories of the metric differ between the English-speaking and Dutch-speaking pALS cohorts.\\n\\n| Metric          | p-value of difference | pALS Cohort Intercept | \u00b1 standard error | Slope \u00b1 standard error |\\n|-----------------|-----------------------|-----------------------|------------------|------------------------|\\n| RP speaking duration | 0.7751                | English-speaking      | 30.10 \u00b1 9.52     | 0.1188 \u00b1 0.0881        |\\n|                 |                       | Dutch-speaking        | 29.12 \u00b1 8.90     | 0.1440 \u00b1 0.0826        |\\n| DDK cTV         | 0.9260                | English-speaking      | 0.06 \u00b1 0.01      | -1.96e-05 \u00b1 5.64e-05   |\\n|                 |                       | Dutch-speaking        | 0.07 \u00b1 0.01      | -2.48e-05 \u00b1 5.51e-05   |\\n| RP CTA          | 0.6379                | English-speaking      | 80.70 \u00b1 7.77     | -0.0971 \u00b1 0.0603       |\\n|                 |                       | Dutch-speaking        | 68.87 \u00b1 7.38     | -0.0688 \u00b1 0.0577       |\\n| PD word count   | 0.2219                | English-speaking      | 68.51 \u00b1 25.67    | 0.1653 \u00b1 0.1805        |\\n|                 |                       | Dutch-speaking        | 10.64 \u00b1 24.38    | 0.3858 \u00b1 0.1746        |\\n| SIT Max.        | < 0.0001*             | English-speaking      | 2.78 \u00b1 0.34      | -0.0002 \u00b1 0.0021       |\\n|                 |                       | Dutch-speaking        | 0.88 \u00b1 0.32      | 0.0117 \u00b1 0.0020        |\\n| SIT PPT         | 0.9688                | English-speaking      | 2.22 \u00b1 2.04      | 0.0112 \u00b1 0.0150        |\\n|                 |                       | Dutch-speaking        | 2.56 \u00b1 1.94      | 0.0106 \u00b1 0.0145        |\\n| RP Max.         | 0.7308                | English-speaking      | 1.64 \u00b1 0.07      | -2.69e-05 \u00b1 4.50e-04   |\\n|                 |                       | Dutch-speaking        | 1.59 \u00b1 0.07      | 1.28e-04 \u00b1 4.43e-04    |\\n| SIT HNR         | 0.5290                | English-speaking      | 10.12 \u00b1 1.42     | 0.0049 \u00b1 0.0091        |\\n|                 |                       | Dutch-speaking        | 11.03 \u00b1 1.34     | -0.0009 \u00b1 0.0088       |\\n| SIT Mean lip aperture | < 0.0001*         | English-speaking      | 0.78 \u00b1 0.10      | -0.0002 \u00b1 0.0006       |\\n|                 |                       | Dutch-speaking        | 0.10 \u00b1 0.10      | 0.0042 \u00b1 0.0006        |\\n| SIT Max.        | 0.0002*               | English-speaking      | 0.35 \u00b1 0.05      | -8.43e-05 \u00b1 2.82e-04   |\\n|                 |                       | Dutch-speaking        | 0.14 \u00b1 0.04      | 9.75e-04 \u00b1 2.73e-04    |\\n| PD Closed Class | 0.1873                | English-speaking      | 0.42 \u00b1 0.03      | 0.0001 \u00b1 0.0002        |\\n| Word Ratio      | 0.40                  | Dutch-speaking        | 0.40 \u00b1 0.03      | 0.0004 \u00b1 0.0002        |\\n| RP Mean F0      | 0.6174                | English-speaking      | 141.71 \u00b1 13.24   | 0.0723 \u00b1 0.0815        |\\n|                 |                       | Dutch-speaking        | 147.80 \u00b1 12.62   | 0.0316 \u00b1 0.0801        |\\n| SIT Max.        | 0.2334                | English-speaking      | 0.1 \u00b1 0.01       | -3.86e-05 \u00b1 0.0006     |\\n|                 |                       | Dutch-speaking        | 0.06 \u00b1 0.01      | 7.63e-05 \u00b1 0.0006      |\\n| DDK syllable count (syllables) | 0.4926             | English-speaking      | 62.46 \u00b1 23.29    | -0.0221 \u00b1 0.1509       |\\n|                 |                       | Dutch-speaking        | 56.53 \u00b1 21.88    | 0.0814 \u00b1 0.1447        |\\n| RP Max.         | 0.5221                | English-speaking      | 0.11 \u00b1 0.02      | 5.33e-06 \u00b1 1.10e-04    |\\n|                 |                       | Dutch-speaking        | 0.11 \u00b1 0.02      | 7.54e-05 \u00b1 1.08e-04    |\\n| 5. Discussion   |                       |                       |                  |                        |\\n| In this work, we set out to investigate whether speech-based objective digital biomarkers are consistent in remote tracking of ALS disease progression across languages. First, we looked at whether a selection of metrics that are useful in distinguishing English-speaking bulbar pALS from non-bulbar pALS show similar differences between Dutch-speaking bulbar pALS and non-bulbar pALS. Most of the speech acoustic and timing metrics show similar effect sizes in the English and Dutch cohorts. Facial metrics, however, do not show similar effect sizes except for maximum eye opening during sentence reading. Notably, reading passage canonical timing alignment \u2014 a number between 0% (non-alignment with a canonical production) and 100% (perfect alignment with a canonical production) as measured by the normalised inverse Levenshtein edit distance between words and silence boundaries \u2014 shows very similar effect sizes in English and Dutch. Interestingly, this measure has been found to be very responsive in tracking statistical and clinical change in pALS [17] even with small sample sizes [18]. Most metrics had similar test-retest reliability in the English-speaking and Dutch-speaking cohorts. For the metrics where test-retest reliability is lower in the Dutch cohort, it remains to be seen whether the gap is closed when a larger Dutch dataset is available. All speech metrics had similar slopes in the English and Dutch cohorts when growth curve models were fit to predict the average longitudinal trajectory of the metrics. Linguistic metrics and three of the six facial metrics also showed similar trajectories. Maximum eyebrow vertical displacement, mean lip aperture and maximum eye opening during sentence reading had steeper longitudinal trajectory slopes for the Dutch cohort than the English cohort. It is not clear why these three facial metrics should change differently in two cohorts with dissimilar linguistic backgrounds. One explanation is that the smaller sample size in the Dutch cohort makes this measure much noisier. Indeed, one limitation of this work is that the average time span of data collection for the Dutch-speaking cohort is much smaller than that in the English-speaking cohort. The Dutch-speaking data skews male. Future work with larger, more balanced sample sizes need to be conducted. This case study included languages from the same West Germanic language family [34]. Future work should test whether these observations are consistent when unrelated languages are considered. In conclusion, speech-based digital biomarkers in pALS show good consistency and great promise in tracking ALS disease progression in cohorts of patients speaking different languages. The impact of language on disease-related changes, if any, seems minimal for the majority of the biomarkers considered.\\n\\n6. Acknowledgements\\n\\nThis work was funded by the National Institutes of Health grant R42DC019877. We thank EverythingALS, the Peter Cohen Foundation and UMC Utrecht for participant recruitment.\\n\\n7. References\\n\\n[1] O. Hardiman, A. Al-Chalabi, A. Chio, E. M. Corr, G. Logroscino, W. Robberecht, P. J. Shaw, Z. Simmons, and L. H. Van Den Berg, \\\"Amyotrophic lateral sclerosis,\\\" Nature Reviews Disease Primers, vol. 3, no. 1, pp. 1\u201319, 2017.\\n\\n[2] A. Chio, G. Logroscino, O. Hardiman, R. Swingler, D. Mitchell, E. Beghi, B. G. Traynor, E. Consortium et al., \\\"Prognostic factors in als: a critical review,\\\" Amyotrophic lateral sclerosis, vol. 10, no. 5-6, pp. 310\u2013323, 2009.\"}"}
{"id": "kothare24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[3] J. R. Green, Y. Yunusova, M. S. Kuruvilla, J. Wang, G. L. Pattee, L. Synhorst, L. Zinman, and J. D. Berry, \u201cBulbar and speech assessment in ALS: Challenges and future directions,\u201d Orphanet Journal of Rare Diseases, vol. 14, no. 7-8, pp. 494\u2013500, 2013.\\n\\n[4] L. C. Wijesekera and P. Nigel Leigh, \u201cAmyotrophic lateral sclerosis (ALS) From Video-Based Analysis of Facial Movements: Detection of Amyotrophic Lateral Sclerosis (ALS) via Acoustic analysis,\u201d in Proc. Interspeech 2018, IEEE, 2018, pp. 150\u2013157.\\n\\n[5] L. J. Haverkamp, V. Appel, and S. H. Appel, \u201cNatural history of amyotrophic lateral sclerosis in a database population validation,\u201d Brain, vol. 20, no. 9, pp. 2791\u20132822, 1991.\\n\\n[6] A. Bandini, J. R. Green, J. Wang, T. F. Campbell, L. Zinman, and Y. Yunusova, \u201cRate of speech decline in individuals with amyotrophic lateral sclerosis,\u201d Journal of Speech, Language, and Hearing Research, vol. 65, no. 12, pp. 678\u2013679, 2021.\\n\\n[7] A. Bandini, J. R. Green, B. Taati, S. Orlandi, L. Zinman, and Y. Yunusova, \u201cReliability and validity of speech & pause timing-related speech biomarkers for remote monitoring of neurological and mental health,\u201d Journal of Communication Disorders, vol. 43, no. 1, pp. 132, 2020.\\n\\n[8] R. Norel, M. Pietrowicz, C. Agurto, S. Rishoni, and G. Cecchi, \u201cStudies of chinese speakers with dysarthria: intraindividual and incremental parsing. neural machine translation,\u201d in Proceedings of the Association for Computational Linguistics (ACL) Conference on Automatic Face & Gesture Recognition (FG 2018), 2018, pp. 377\u2013381.\\n\\n[9] C. Barnett, J. R. Green, R. Marzouqah, K. L. Stipancic, J. D. Berry, L. Zinman, \u201cSpeech in ALS: Longitudinal Changes in Lips and Jaw Movements and Vowel Acoustics,\u201d in Proc. Interspeech 2017, IEEE, 2017, pp. 688\u2013697.\\n\\n[10] M. Neumann, O. Roesler, J. Liscombe, H. Kothare, M. Eshghi, Y. Yunusova, K. P. Connaghan, B. J. Perry, M. F. Maffei, \u201cExploring Facial Metric Normalization for Within- and Between-Subject Comparisons in a Multimodal Agent Based Remote Health Assessment Integrating What Patients Say with What They Do,\u201d in NPJ Digital Medicine, vol. 6, no. 1, p. 15, 2023, pp. 678\u2013679.\\n\\n[11] H. Kothare, M. Neumann, J. Liscombe, J. Green, and V. Ramanarayanan, \u201cReliability and validity of speech & pause timing-related speech biomarkers for remote monitoring of neurological and mental health,\u201d Journal of Communication Disorders, vol. 43, no. 1, pp. 132, 2020.\\n\\n[12] L. E. Simmatis, J. Robin, M. J. Spilka, and Y. Yunusova, \u201cDenoising acoustic analysis,\u201d IEEE, 2020, pp. 4783\u20134787.\\n\\n[13] D. L. Guarin, B. Taati, A. Abrahao, L. Zinman, and Y. Yunusova, \u201cProbing speech speech and non-speech tasks,\u201d in 2018 13th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2018), 2018, pp. 377\u2013381.\\n\\n[14] Y. Yunusova, J. R. Green, M. J. Lindstrom, G. L. Pattee, and T. F. Campbell, \u201cSpeech biomarkers in rapid eye movement sleep behaviors: relationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[15] G. M. Stegmann, S. Hahn, J. Liss, J. Shefner, S. Rutkove, K. Sheline, \u201cSpeech biomarkers in rapid eye movement sleep behaviors: relationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[16] Y. Yunusova, J. R. Green, M. J. Lindstrom, L. J. Ball, G. L. Pattee, and L. Synhorst, \u201cReliability and validity of speech & pause timing-related speech biomarkers for remote monitoring of neurological and mental health,\u201d Journal of Communication Disorders, vol. 43, no. 1, pp. 132, 2020.\\n\\n[17] H. Kothare, M. Neumann, J. Liscombe, J. Green, and V. Ramanarayanan, \u201cRelationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[18] H. Kothare, M. Neumann, and V. Ramanarayanan, \u201cRelationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[19] A. Favaro, L. Moro-Vel\u00e1zquez, A. Butala, C. Motley, T. Cao, J. Montplaisir, J.-F. Gagnon, P. Du\u0161ek, A. Galbiati, S. Marelli, \u201cSpeech biomarkers in rapid eye movement sleep behaviors: relationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[20] J. Rusz, J. Hlavni\u010dka, M. Novotn\u00fd, T. Tykalov\u00e1, A. Pelletier, J. Kumm, L. Zinman, \u201cSpeech biomarkers in rapid eye movement sleep behaviors: relationship of interpretable biomarkers to represent language and speech,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 678\u2013679.\\n\\n[21] T. L. Whitehill, \u201cStudies of chinese speakers with dysarthria: intraindividual and incremental parsing. neural machine translation,\u201d in Proceedings of the Association for Computational Linguistics (ACL) Conference on Automatic Face & Gesture Recognition (FG 2018), 2018, pp. 377\u2013381.\\n\\n[22] V. Ramanarayanan, D. Pautler, L. Arbatti, A. Hosamath, M. Neu mann, H. Kothare, O. Roesler, J. Liscombe, A. Cornish, D. Habberstad, D. Suendermann-Oeft, V. Richter, D. Fox, D. Suendermann-Oeft, and I. Shoulman, \u201cWhen Words Speak Just as Loudly as Actions: Virtual Assistants as Telehealth Resources,\u201d in Frontiers in Neurology, vol. 12, no. 1, p. 132, 2020.\\n\\n[23] V. Ramanarayanan, \u201cMultimodal technologies for remote assessment of neurological and mental health,\u201d Journal of Medical and Jaw Movements and Vowel Acoustics,\u201d in Proc. Interspeech 2017, IEEE, 2017, pp. 688\u2013697.\\n\\n[24] P. Boersma, \u201cPraat, a system for doing phonetics by computer,\u201d Glot International, vol. 5, no. 9/10, pp. 341\u2013345, 2001.\\n\\n[25] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonnenberg, \u201cMontreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi,\u201d in Proc. Interspeech 2017, IEEE, 2017, pp. 688\u2013697.\\n\\n[26] Y. Kartynnik, A. Ablavatski, I. Grishchenko, and M. Grundmann, \u201cBlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,\u201d arXiv:1907.06724 [cs.CV] 16 Jul 2019. [Online]. Available: http://arxiv.org/abs/1907.06724\\n\\n[27] V. Bazarevsky, Y. Kartynnik, A. Vakunov, K. Raveendran, and M. Grundmann, \u201cBlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,\u201d arXiv:1907.05047 [cs.CV] 16 Jul 2019. [Online]. Available: http://arxiv.org/abs/1907.05047.\\n\\n[28] O. Roesler, H. Kothare, W. Burke, M. Neumann, J. Liscombe, A. Cornish, D. Habberstad, D. Suendermann-Oeft, and I. Shoulman, \u201cBlazeFace: Sub-millisecond Neural Face Detection on Mobile GPUs,\u201d arXiv:1907.05047 [cs.CV] 16 Jul 2019. [Online]. Available: http://arxiv.org/abs/1907.05047.\\n\\n[29] M. Honnibal and I. Montani, \u201cspacy 2: Natural language understanding with bloom embeddings, convolutional neural networks and incremental parsing. neural machine translation,\u201d in Proceedings of the Association for Computational Linguistics (ACL) Conference Companion Publication of the Proceedings of: International Society for CNS Clinical Trials and Methodology (ISCTM 2024) Spring Conference, Washington, D.C. 2024, p. 160\u2013165. [Online]. Available: https://doi.org/10.1145/3536220.3558071\\n\\n[30] D. Ienco and R. Meo, \u201cExploration and Reduction of the Feature Space by Hierarchical Clustering,\u201d in Proceedings of the 2008 SIAM International Conference on Data Mining, SIAM, 2008, pp. 577\u2013587.\\n\\n[31] J. H. Ward Jr, \u201cHierarchical grouping to optimize an objective function,\u201d Journal of the American Statistical Association, vol. 58, no. 301, pp. 236\u2013244, 1963.\\n\\n[32] G. V. Glass, B. McGaw, and M. L. Smith, \u201cMeta-analysis in social research,\u201d Sage Publications, 1981.\\n\\n[33] D. Von Rosen, \u201cThe growth curve model: a review,\u201d in Communica tions in Statistics-Theory and Methods, vol. 10, no. 6, pp. 498\u2013502, 1981.\\n\\n[34] B. Comrie et al., The World's Major Languages. Routledge London, 1987.\"}"}
