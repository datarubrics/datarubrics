{"id": "take24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information\\n\\nToward Environment-adaptive Dialogue Speech Synthesis\\n\\nOsamu Take, Shinnosuke Takamichi, Kentaro Seki, Yoshiaki Bando, Hiroshi Saruwatari\\n\\n1The University of Tokyo, Japan,\\n2Keio University, Japan\\n3National Institute of Advanced Industrial Science and Technology (AIST), Japan\\n\\nflymoons0325@g.ecc.u-tokyo.ac.jp\\n\\nAbstract\\n\\nThis paper presents SaSLaW, a spontaneous dialogue speech corpus containing synchronous recordings of what speakers speak, listen to, and watch. Humans consider the diverse environmental factors and then control the features of their utterances in face-to-face voice communications. Spoken dialogue systems capable of this adaptation to these audio environments enable natural and seamless communications. SaSLaW was developed to model human-speech adjustment for audio environments via first-person audio-visual perceptions in spontaneous dialogues. We propose the construction methodology of SaSLaW and display the analysis result of the corpus. We additionally conducted an experiment to develop text-to-speech models using SaSLaW and evaluate their performance of adaptations to audio environments. The results indicate that models incorporating hearing-audio data output more plausible speech tailored to diverse audio environments than the vanilla text-to-speech model.\\n\\nIndex Terms: speech corpus, spoken dialogue, speech chain, Lombard effect, entrainment\\n\\n1. Introduction\\n\\nText-to-speech (TTS) is an important technology for spoken dialogue systems (SDSs) such as conversational robots [1]. These systems are often implemented in conversational scenarios within real-world environments. Human-to-human voice communication in real environments often involves natural and intelligible speech tailored to surrounding factors such as background noise and their physical proximity. We call these environmental factors collectively as the audio environment.\\n\\nThe adaptation to audio environments by humans is based on the auditory and visual information they perceive [2, 3], which can be explained within the framework of the speech chain [4]. Reports have also indicated that different audio environments necessitate natural speech variations of conversational robots for humans [5]. Therefore, TTS incorporating audio environment inputs with the framework of speech chain is necessary for SDSs to achieve natural and seamless speech communication in dialogues. We refer to this dialogue TTS as environment-adaptive TTS (EA-TTS). Figure 1 illustrates the application of EA-TTS.\\n\\nDeep neural networks (DNNs) and prevailing large-scale corpora [6] enable TTS models to generate natural speech comparable to humans for read speech in quiet environments. However, an EA-TTS model cannot be constructed only with the speaker's clean speech recorded in quiet backgrounds. EA-TTS should require first-person recordings of what humans speak, hear, and see during dialogues in various audio environments. Corpora for such EA-TTS and their construction methods are yet to be established despite the prevalence of TTS corpora.\\n\\nFigure 1: Dialogue agent that generates speech using environment-adaptive TTS (EA-TTS). EA-TTS changes the speech style affected by interlocutor or environmental noise, just like us humans. This paper proposes a methodology of corpus construction to realize this and build open-source corpus.\\n\\nWe present a methodology to construct a spontaneous dialogue speech corpus containing synchronous recordings of egocentric audio-visual perceptions. Following this methodology, a novel speech corpus called SaSLaW is constructed and published [2]. We also construct EA-TTS models based on DNNs using SaSLaW and conduct a comparative evaluation.\\n\\n2. Related Work\\n\\n2.1. Emulating Adaptations to Audio Environments\\n\\nNoise environment. The Lombard effect [7] describes the involuntary voice raising of humans in noisy environments. Several methods have been proposed for mimicking the Lombard effect, including using signal-processing-inspired manipulations [8, 9] and neural TTS [10, 11, 12]. However, previous research [12] confirmed the degradation in the naturalness of manipulated speech by signal processing. These methods are also limited to the study of read speech and do not model the Lombard effect in spontaneous dialogue [13].\\n\\nToward listeners. Prosody of human speech is also affected by the physical proximity between talkers [14] and interlocutors' speech [15], a phenomenon known as entrainment [16]. Several methods construct neural TTS models with architectures for adapting to interlocutors' speech [17] and faces [18]. However, none of these methods address both spontaneous dialogue scenes and the use of egocentric perceptions.\\n\\n2.2. Corpora for Environment-adaptive TTS\\n\\nSeveral corpora [19, 20] have been constructed for TTS with modeling adaptations to noisy surroundings. These corpora primarily focus on modeling read-style Lombard speech. The construction of TTS corpora modeling adaptation to environmental noises in spontaneous conversations remains unexplored.\\n\\n1\u201cSo, what are you Speaking, Listening, and Watching?\u201d\\n2https://github.com/sarulab-speech/SaSLaW\"}"}
{"id": "take24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Corpus Style | Noise | See | Speak | IR  |\\n|--------------|-------|-----|-------|-----|\\n| TTS corpus   |       |     |       |     |\\n| SaSLaW (ours)| spon. | real| fp    | \u2713   |\\n| Hurricane [19]|       |     |       |     |\\n| CEJC [22]    | spon. | real| tp    | \u2713   |\\n| Guo et al. [21]| perf.| -   | tp    | \u2713   |\\n| Datasets not focusing on TTS |     |     |       |     |\\n| EgoCom [24]  | spon. | real| fp    | -   |\\n| EasyCom [23] | spon. | real| tp    | \u2713   |\\n\\nTable 1: Corpora comparison. \u201cSpon.\u201d and \u201cperf.\u201d are spontaneous and performative styles, respectively. \u201cfp\u201d and \u201ctp\u201d are first- and third-person views, respectively. \u201cIR\u201d indicates impulse responses between talkers, and \u2020 means near-real noise.\\n\\nFigure 2: Recording configuration during two-person conversation. Top illustrates the configuration of the noisy environment, and bottom illustrates participants\u2019 equipment.\\n\\nThere are TTS corpora for modeling dialogue scenes used with DNN-based TTS models [21, 22]. The CEJC corpus [22] contains spontaneous conversations in real environments, while capturing environmental noise and visual footage from a third-person perspective. The EasyCom [23] dataset contains participants\u2019 speech and egocentric videos during conversations in noisy environments but lacks the variety of audio environments. EgoCom [24] contains firsthand audio-visual experiences but lacks speech recordings with minimal external noise as it was primarily designed for recognition and understanding tasks.\\n\\n3. Corpus Construction Methodology\\n\\nWe describe the construction methodology of the spontaneous dialogue corpus SaSLaW, which includes first-person-perspective multi-modal information.\\n\\n3.1. Overview of SaSLaW\\n\\nSaSLaW captures adaptation to audio environments (noise, interlocutor) in spontaneous human speech communication. Achieving this involves recording scenes in which two participants engage in spontaneous dialogue while facing each other in a simulated noisy environment, mimicking real-world conditions. Figure 2 illustrates the recording configuration during a conversation between two participants. SaSLaW records what a participant speaks, listens to, and watches synchronously across two participants. Speech recordings can be utilized for constructing TTS models to generate natural speech incorporating human-like auditory and visual information.\\n\\n3.2. Recording Configuration and Procedure\\n\\nParticipant equipment. Two participants engage in conversation in a single indoor space, referred to as the recording room. They sat facing each other across a table. The distance between the two participants is set within 1.5 to 3 m and is not strictly controlled. Participants are equipped with a close-talking microphone, ear-mounted binaural microphone, and head-mounted camera on their heads, as illustrated in Figure 2. Each device corresponds to recording what they speak, hear, and see. The two microphones record at a sampling frequency of 44.1 kHz, while the head-mounted camera records at a frame rate of 30 fps. All six sensors record information synchronously. During the recording, variations in speech volume due to the audio environment are expected. To capture these variations, the gain for each participant\u2019s microphones is fixed throughout all recordings.\\n\\nEnvironmental noise. To simulate various real environments with noise, eight loudspeakers are positioned as shown in Figure 2, covering the area around the participants. Each loudspeaker plays a different segment of the same environmental noise, simulating diffusive environmental noise in the real environment. The real-environmental-noise data are derived from a subset of the DEMAND dataset [26]. The type and power of the noise played from loudspeakers are altered after a certain number of conversation recordings. Before recording the conversation, the ambient noise level in dB is measured at the center of the two participants using a noise meter.\\n\\nConversation content. The two participants are instructed on the theme and roles (e.g., sightseeing, the guide and tourist) and tasks. During the recording, the two participants engage in improvisational conversation consisting of five to eight turns, following provided instructions.\\n\\nAnnotation as TTS corpus. To use the SaSLaW for TTS use, we automatically segment close-talking microphone voices using pyannote.audio [27] into utterances and transcribe them into texts using whisper [28], followed by manual correction.\\n\\n3.3. Data Collection for Reproducible Evaluation\\n\\nIn subjective evaluations, evaluators should assess the plausibility within audio environments based on what they would listen to at the listener position while models output speech via sonic transmission, rather than synthetic speech itself. Therefore, we collect supplementary data following the previous evaluation methodology [25]. SaSLaW records impulse responses from talker to listener, positioned as shown in Figure 2, and ambient noise-only audio in the listener\u2019s position using the ear-mounted binaural microphone. Impulse responses are recorded for various distances between participants. Synthetic utterance samples are convolved with a certain impulse response and added with recorded noise-only audio. Then we acquire the evaluation samples simulating what listeners would hear.\"}"}
{"id": "take24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"One pair consisted of two male participants (named as spk01, spk02), while the other pair consisted of two female (named as spk05, spk06). The total utterance length of recorded speech was approximately 30 minutes on average.\\n\\n4.1. Purpose and Procedures of Corpus Analysis\\n\\nThis analysis examined how spontaneous speech changed depending on the sound pressure level of environmental noise and inter-speaker coordination. Note that we did not investigate the distance between participants.\\n\\nFirst, each conversation was assigned a label about environmental noise levels (env-label) such as \u201cnoisy\u201d (high environmental noise level), \u201cmoderate\u201d (moderate), or \u201cquiet\u201d (low). The label annotation was based on the sound pressure level of environmental noise measured in the conversation recording.\\n\\nNext, the root mean square of amplitudes (RMS), $F_1$ (first formant) frequency, and spectral tilt [29] were calculated for voiced frames of utterances as prosodic features. $F_0$ was computed and voiced frames were detected using harvest [30]. The $F_1$ frequency was computed through Praat [31]. Spectral tilt was computed following previous research [29], with a filter bank from $0.25$ to $8$ kHz. All the statistical tests below were conducted at a significance level of $p = 0.05$.\\n\\n4.2. Analysis Results and Discussion\\n\\nNoise.\\n\\nFigure 3 illustrates the distributions of RMS, $F_1$ frequencies and spectral tilts for each speaker. It is shown that the average $F_1$ frequency significantly increased from \u201cquiet\u201d to \u201cnoisy\u201d env-labels for all the speakers. The average of RMS showed a significant rise from \u201cquiet\u201d to \u201cnoisy,\u201d for all the speakers except for spk05. The average spectral tilt presented a significant increase for female speakers from \u201cquiet\u201d to \u201cnoisy,\u201d whereas a different trend was observed for male.\\n\\nThese increases corresponded to the result reported in the previous study [20]. Also, the result suggests that while some features share characteristics among speakers in adapting to varying levels of environmental noise, others do not exhibit such commonality. It indicates that solely relying on rule-based methods with signal processing makes it difficult to accurately simulate Lombard speech adapting to audio environments.\\n\\nInterlocutors.\\n\\nTable 2 shows the inter-speaker correlation coefficients of utterance RMS and $F_0$. The result indicates that in moderate and noisy environments, the target utterance features are significantly correlated with those of the last interlocutor\u2019s utterances. Adaptations to harsh listening environments may evoke this correlation enhancement from quiet to moderate.\"}"}
{"id": "take24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"through signal processing. While utterances manipulated in a signal-processing manner deviate in features from real Lombard speech, these utterances can effectively pretrain EA-TTS models due to their scalability. We assign each utterance an arbitrary noise signal at a varying level then increase its spectral tilt to enhance intelligibility within the corresponding noise. We use JSUT as the TTS corpus and DEMAND as the noise dataset.\\n\\n**Training conditions.** We split SaSLaW audio recordings into train and test sets for each speaker's data. There was no overlap in the environmental noise contained in the hearing audio between the sets. The test set covered all env-labels assigned to speech. Finally, we split the recordings of spk01 into 299/49 and spk06 into 443/64 train/test utterances. All three EA-TTS models were trained on a single NVIDIA GeForce RTX 4090 GPU. They were pre-trained for 900k (\u2264 three days) and fine-tuned on single-speaker recordings of SaSLaW for 100k steps (\u2264 12 hours).\\n\\n### 5.2. Objective Evaluation\\n\\nFor objective evaluation, we computed the prosodic features outlined in Section 4.1 for the synthetic speech. We investigated whether the distribution changed across different env-labels, consistent with the analysis presented in Figure 3.\\n\\nFigure 5 illustrates the prosodic feature distributions of synthetic speech. FS2 showed no significant differences across env-labels. For FS2-predsty and FS2-predsty-ptrn, the synthetic speech of spk01 showed a significant increase only for RMS from \u201cquiet\u201d to \u201cnoisy\u201d. For spk06, the synthesized speech exhibited a significant increase from \u201cquiet\u201d to \u201cnoisy\u201d for all features except between \u201cmoderate\u201d and \u201cnoisy\u201d for spectral tilt. These results indicate that the Env-to-style predictor enabled the generation of speech with characteristics adapted to audio environments.\\n\\n### 5.3. Subjective Evaluation\\n\\nWe conducted an AB preference test to compare the plausibility of the evaluation speech samples within surrounding noises. Each synthetic utterance was processed as described in Section 3.3 to serve as the evaluation utterance. Evaluators selected which utterance sounded more plausible in environmental noises through the listening experiment. The evaluators were provided with a combined criterion of naturalness and intelligibility as supplementary instruction. For each model-pair configuration, 72 evaluators were recruited and 720 responses were collected via Lancers. Figure 6 illustrates the env-label-wise preference scores for each model-pair and speaker.\\n\\nThe results indicate that for the \u201cnoisy\u201d label, both FS2-predsty and FS2-predsty-ptrn significantly outperformed FS2 in preference scores for both speakers, while FS2 significantly surpassed the other two for the \u201cquiet\u201d label. This result suggests that the EA-TTS models with the Env-to-style predictor successfully adapted to noisy surroundings. However, this adaptation to \u201cquiet\u201d environments degraded the plausibility of synthetic speech compared with FS2-synthesized speech, which had averaged prosodic features and gained intelligibility. Figure 6 shows that the preference scores of FS2-predsty-ptrn were equal to or significantly higher than those of FS2-predsty. This suggests that pseudo-data pre-training improved the performance.\\n\\n### 6. Conclusion\\n\\nWe introduced SaSLaW, a novel speech corpus for generative tasks with synchronized audio and visual first-person recordings. We described the methodology of constructing SaSLaW and analyzed the recordings to confirm human speech\u2019s adaptations to audio environments. The experimental results indicate that SaSLaW enables the construction of environment-adaptive TTS models by using the auditory perception of the target speaker as input, successfully producing plausible speech tailored to diverse audio environments. This work does not explore the analysis and modeling of speech adaptation to visual information, which can be investigated for further work.\\n\\n### 7. Acknowledgements\\n\\nPart of this work was supported by JSPS KAKENHI Grant Number 23H03418 and 22H03639, Moonshot R&D Grant. https://www.lancers.jp\"}"}
{"id": "take24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The authors also thank Aya Watanabe for her support in designing the figures for this paper.\\n\\n8. References\\n\\n[1] T. Kawahara, \u201cSpoken dialogue system for a human-like conversational robot ERICA,\u201d in Proc. International Workshop on Spoken Dialogue System Technology (IWSDS), 2019, pp. 65\u201375.\\n\\n[2] M. Cooke, S. King, M. Garnier, and V. Aubanel, \u201cThe listening talker: A review of human and algorithmic context-induced modifications of speech,\u201d Computer Speech & Language, vol. 28, no. 2, pp. 543\u2013571, 2014.\\n\\n[3] V. Hazan and R. Baker, \u201cAcoustic-phonetic characteristics of speech produced with communicative intent to counter adverse listening conditions.\u201d The Journal of the Acoustical Society of America, vol. 130, 4, pp. 2139\u201352, 2011.\\n\\n[4] P. B. Denes and E. Pinson, The speech chain. Macmillan, 1993.\\n\\n[5] P. Tuttosi, E. Hughson, A. Matsufuji, C. Zhang, and A. Lim, \u201cRead the room: Adapting a robot\u2019s voice to ambient and social contexts,\u201d in Proc. 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2023, pp. 3998\u20134005.\\n\\n[6] X. Tan, Neural Text-to-Speech Synthesis. Springer Nature, 2023.\\n\\n[7] E. Lombard, \u201cLe signe de l\u2019\u00e9l\u00e9vation de la voix,\u201d Annales des Maladies de L\u2019Oreille et du larynx, vol. 37, pp. 101\u2013119, 1911.\\n\\n[8] T.-C. Zorila, V. Kandia, and Y. Stylianou, \u201cSpeech-in-noise intelligibility improvement based on spectral shaping and dynamic range compression,\u201d in Proc. INTERSPEECH, 2012, pp. 635\u2013638.\\n\\n[9] T. V. Ngo, R. Kubo, and M. Akagi, \u201cMimicking Lombard effect: An analysis and reconstruction,\u201d IEICE Transactions on Information and Systems, vol. E103.D, no. 5, pp. 1108\u20131117, 2020.\\n\\n[10] B. Bollepalli, L. Juvela, and P. Alku, \u201cLombard speech synthesis using transfer learning in a Tacotron text-to-speech system,\u201d in Proc. INTERSPEECH, 2019, pp. 2833\u20132837.\\n\\n[11] S. Novitasari, S. Sakti, and S. Nakamura, \u201cA machine speech chain approach for dynamically adaptive Lombard TTS in static and dynamic noise environments,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, pp. 2673\u20132688, 2022.\\n\\n[12] T. Raitio, P. Petkov, J. Li, M. Shifas, A. Davis, and Y. Stylianou, \u201cVocal effort modeling in neural TTS for improving the intelligibility of synthetic speech in noise,\u201d in Proc. INTERSPEECH, 2022, pp. 1936\u20131940.\\n\\n[13] F. S. Laura Folk, \u201cThe Lombard effect in spontaneous dialog speech,\u201d in Proc. INTERSPEECH, 2011, pp. 2701\u20132704.\\n\\n[14] D. Pelegr\u00edn-Garc\u00eda, B. Smits, J. Brunskog, and C.-H. Jeong, \u201cVocal effort with changing talker-to-listener distance in different acoustic environments,\u201d The Journal of the Acoustical Society of America, vol. 129, no. 4, pp. 1981\u20131990, Apr. 2011.\\n\\n[15] R. Nishimura, N. Kitaoka, and S. Nakagawa, \u201cAnalysis of factors to make prosodic change in spoken dialog (feature articles; rhythm and timing),\u201d Journal of the Phonetic Society of Japan, vol. 13, no. 3, pp. 66\u201384, 2009.\\n\\n[16] R. Levitan, A. Gravano, L. Willson, B. \u02c7Stefan, J. Hirschberg, and A. Nenkova, \u201cAcoustic-prosodic entrainment and social behavior,\u201d in Proc. North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT). Association for Computational Linguistics, Jun. 2012, pp. 11\u201319.\\n\\n[17] J. Li, Y. Meng, X. Wu, Z. Wu, J. Jia, H. Meng, Q. Tian, Y. Wang, and Y. Wang, \u201cInferring speaking styles from multi-modal conversational context by multi-scale relational graph convolutional networks,\u201d in Proc. ACM International Conference on Multimedia, ser. MM \u201922, 2022, p. 5811\u20135820.\\n\\n[18] M. Zhou, Y. Bai, W. Zhang, T. Yao, T. Zhao, and T. Mei, \u201cVisual-aware text-to-speech*,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 1\u20135.\\n\\n[19] M. Cooke, C. Mayo, and C. Valentini-Botinhao, \u201cIntelligibility-enhancing speech modifications: the Hurricane Challenge,\u201d in Proc. INTERSPEECH, August 2013, pp. 1341\u20131345.\\n\\n[20] N. Alghamdi, S. Maddock, R. Marxer, J. Barker, and G. J. Brown, \u201cA corpus of audio-visual Lombard speech with frontal and profile views,\u201d The Journal of the Acoustical Society of America, vol. 143, no. 6, pp. EL523\u2013EL529, Jun. 2018.\\n\\n[21] H. Guo, S. Zhang, F. K. Soong, L. He, and L. Xie, \u201cConversational end-to-end TTS for voice agents,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 403\u2013409.\\n\\n[22] H. Koiso, H. Amatani, Y. Den, Y. Iseki, Y. Ishimoto, W. Kashino, Y. Kawabata, K. Nishikawa, Y. Tanaka, Y. Usuda, and Y. Watanabe, \u201cDesign and evaluation of the corpus of everyday Japanese conversation,\u201d in Proc. Language Resources and Evaluation Conference (LREC). European Language Resources Association, Jun. 2022, pp. 5587\u20135594.\\n\\n[23] J. Donley, V. Tourbabin, J.-S. Lee, M. Broyles, H. Jiang, J. Shen, M. Pantic, V. K. Ithapu, and R. Mehra, \u201cEasyCom: An augmented reality dataset to support algorithms for easy communication in noisy environments,\u201d arXiv preprint arXiv:2107.04174, 2021.\\n\\n[24] C. G. Northcutt, S. Zha, S. Lovegrove, and R. Newcombe, \u201cEgoCom: A multi-person multi-modal egocentric communications dataset,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 6, pp. 6783\u20136793, 2023.\\n\\n[25] J. Rennies, H. Schepker, C. Valentini-Botinhao, and M. Cooke, \u201cIntelligibility-enhancing speech modifications \u2014 The Hurricane Challenge 2.0,\u201d in Proc. INTERSPEECH, 2020, pp. 1341\u20131345.\\n\\n[26] J. Thiemann, N. Ito, and E. Vincent, \u201cDEMAND: a collection of multi-channel recordings of acoustic noise in diverse environments.\u201d Zenodo, Apr. 2018. [Online]. Available: https://doi.org/10.5281/zenodo.1227121\\n\\n[27] H. Bredin, \u201cpyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe,\u201d in Proc. INTERSPEECH, 2023, pp. 1983\u20131987.\\n\\n[28] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. International Conference on Machine Learning (ICML), ser. ICML\u201923, 2023, pp. 28 492\u201328 518.\\n\\n[29] Y. Sato and J. Villegas, \u201cSpectral tilt may have a smaller impact on the intelligibility of speech in noise,\u201d in Proc. IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2023, pp. 1\u20135.\\n\\n[30] M. Morise, \u201cHarvest: A high-performance fundamental frequency estimator from speech signals,\u201d in Proc. INTERSPEECH, 2017, pp. 2321\u20132325.\\n\\n[31] P. Boersma and D. Weenink, \u201cPraat: doing phonetics by computer (version 6.1.38),\u201d 2021. [Online]. Available: http://www.praat.org\\n\\n[32] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastSpeech 2: Fast and high-quality end-to-end text to speech,\u201d in Proc. International Conference on Learning Representations (ICLR), 2021.\\n\\n[33] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in Proc. International Conference on Neural Information Processing Systems (NeurIPS), ser. NIPS'20, 2020, pp. 17 022\u201317 033.\\n\\n[34] R. Sonobe, S. Takamichi, and H. Saruwatari, \u201cJSUT corpus: free large-scale Japanese speech corpus for end-to-end speech synthesis,\u201d arXiv preprint arXiv:1711.00354, 2017.\\n\\n[35] Y. Wang, D. Stanton, Y. Zhang, R. J. Skerry-Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \u201cStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in Proc. International Conference on Machine Learning (ICML), ser. Proceedings of Machine Learning Research, vol. 80, 2018, pp. 5167\u20135176.\"}"}
