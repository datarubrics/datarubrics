{"id": "rose22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper presents a new approach for end-to-end audio-visual multi-talker speech recognition. The approach, referred to here as the visual context attention model (VCAM), is important because it uses the available video information to assign decoded text to one of multiple visible faces. This essentially resolves the label ambiguity issue associated with most multi-talker modeling approaches which can decode multiple label strings but cannot assign the label strings to the correct speakers. This is implemented as a transformer-transducer based end-to-end model and evaluated using a two-speaker audio-visual overlapping speech dataset created from YouTube videos. It is shown in the paper that the VCAM model improves performance with respect to previously reported audio-only and audio-visual multi-talker ASR systems.\\n\\nIndex Terms \u2014 audio-visual speech recognition, multi-talker speech recognition\\n\\n1. INTRODUCTION\\nThis paper presents the multi-talker (M-T) visual context attention model (VCAM), an end-to-end (E2E) audio-visual (A/V) modeling approach for transcribing utterances in scenarios where there is overlapping speech from multiple talkers. The presence of overlapping speech in utterances arising from human-human interaction has been studied in several domains including meetings [1] and call center tasks [2]. In a study of interactions in an example call center domain, roughly 12% of the word occurrences in client\u2013operator interactions were found to correspond to overlapping speech. Furthermore, this study showed that dramatically higher WERs for both machine and human transcription were obtained in regions where talkers overlap. In general, multi-talker models attempt to improve speech recognition from multiple overlapping speakers by decoding transcriptions from each speaker, as opposed to improving speech recognition from a target speaker in the presence of background speech. The A/V approach presented here assumes that audio and video signals are available for overlapping talkers. Visual features, in the form of mouth tracks aligned with speech, are similar to those used in A/V end-to-end models for single-talker ASR [3, 4, 5]. Face tracking and video processing are performed offline in this work using tools described in [5].\\n\\nIn audio-only decoding of multiple transcriptions from overlapping speech, there is a basic ambiguity associated with assigning transcriptions to speakers [6, 7]. However, it is well known that human listeners use visual information to disambiguate utterances from multiple talkers in multi-party human-human interactions. This has motivated the VCAM approach presented here. The VCAM model incorporates the visual channel to assign each of the decoded transcriptions to the face associated with each of the overlapping speakers in the utterance. Assuming that a speaker's face serves as a proxy for speaker identity, this obviates the need to have a separate module for dealing with the ambiguity associated with assigning transcriptions to speakers.\\n\\nThe approach presented here builds on previous work in mask-based end-to-end A/V multi-talker modeling [2, 8]. The VCAM has the same mask-based structure as used in the audio-only multi-talker model in [2] where a conventional recurrent neural network transducer (RNN-T) architecture is extended to include a masking model for separation of encoded audio features. It also includes multiple label encoders to encode transcripts from overlapping speakers. This audio-only multi-talker model was extended in [8] to incorporate visual features from overlapping speakers with audio features. It was shown to significantly improve speaker disambiguation in ASR on overlapping speech relative to audio-only performance.\\n\\nThe input to the VCAM M-T model is an utterance containing overlapping speech along with mouth-tracks associated with all on-screen faces. A separate decoding pass is made for each available mouth track. For a given decoding pass at each audio frame, an attention weighted combination of encoded visual frames is input to a mask model with the encoded audio frame. Visual frames from a given mouth-track and audio frame are weighted according to their similarity to the audio frame. It will be shown in Section 2 that the attention maps produced by the model provide an indication when a given speaker is talking even during speaker overlap intervals.\\n\\nThere has been a great deal of recent work on audio-only end-to-end approaches to multi-talker ASR [9, 10, 11]. All of these approaches involve an extension of the single label encoder end-to-end model with a training procedure that aligns overlapping speech with transcriptions from multiple speakers. Further work has addressed the latency issues associated with multi-talker decoding [11]. An E2E A/V M-T approach has recently been applied to addressing the multi-speaker cocktail party effect [12]. There is also a large body of work on speech separation where the goal is to recover a target speech signal in the presence of background speech [13, 14, 6]. This includes approaches that fuse audio and visual features for speech separation in videos [15, 16, 17]. Explicit speech separation systems generally optimize criteria related to signal-to-background distortion and overall signal fidelity.\"}"}
{"id": "rose22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section introduces the end-to-end visual context attention model (VCAM) approach to multi-talker modeling. First, the audio-only and A/V M-T models presented in [2] and [8] are reviewed. Second, the VCAM A/V M-T approach is motivated and described.\\n\\n2.1. Mask based A/V multi-talker model\\n\\nA simplified block diagram of the audio-only multi-talker (M-T) model, originally presented in [2], is shown in Figure 1a. It was shown in [2] that the single label encoder RNNT can be extended to the multi-talker case by adding an LSTM masking model as shown in the figure. It is assumed in the figure that the audio input can contain up to M overlapping utterances. In training, it is assumed that a separate reference label sequence exists for each of the M overlapping utterances from distinct speakers. Multi-talker training is performed by separately aligning the overlapped audio frames to each of the M label sequences. A unique channel sequence index, \\\\(m\\\\), is appended to the encoded audio features for each label sequence before inputting the encoded audio to the masking model. This serves to disambiguate speech associated with label sequence \\\\(m\\\\) from competing speech. Separate RNNT losses are computed for each of the M label sequences, and the overall RNNT loss is the sum of channel specific RNNT losses.\\n\\n![Fig. 1. a) Audio-only multi-talker (M-T) RNNT model. b) Visual context attention model (VCAM) which also includes label encoder and joint network.](image)\\n\\nThe audio-only M-T model in Figure 1a was extended to an A/V M-T model by replacing the audio encoder with an A/V encoder accompanied by an attention network [8]. For each individual video frame, the attention network in [8] produces a weighted combination of the M video frames corresponding to each of the M label sequences. For both the audio-only and A/V models, separate RNNT losses are computed for each of the M label sequences. The overall RNNT loss is the sum of channel specific RNNT losses. All parameters for all models in [2] and [8] are trained using audio signals containing simulated overlapping speech utterances.\\n\\n2.2. Visual context attention M-T model (VCAM)\\n\\nThe VCAM multi-talker model is depicted for the M=2 speaker case by the block diagram in Figure 1b. This model is part of the larger end-to-end framework and two pass training procedure described in Section 2.1. However, there are several important aspects of this model that distinguish it from the M-T models discussed above. First, unlike the approach in [8] which computes attention weights frame-by-frame over mouth-tracks, attention weighting is performed over all video frames separately for each mouth-track. For each audio frame \\\\(x_{at}\\\\) at \\\\(t = 1, \\\\ldots, T\\\\) and for each mouth-track \\\\(m\\\\), the attention module generates an attention weighted visual context vector \\\\(v'_{mt}\\\\) from the encoded video features \\\\(v_{mt}\\\\). The second important aspect of the VCAM model is that it performs late stage integration of audio and visual features as opposed to simply concatenating audio and visual features at the input to and A/V encoder. This is thought to allow the model to accommodate some degree of asynchrony between the audio and video channels. Third, the time-varying channel-dependent attention weighted visual context vector in the VCAM is appended to the input of the masking model rather than a one-hot channel select vector that is appended to the input of the masking model for the M-T model in Figure 1a. It is shown below that the attention weighting over time that is performed in the VCAM model has the effect of providing an approximation of a speaker identity signal to the masking model.\\n\\nThe VCAM M-T model is implemented as follows. The audio features, \\\\(X_a = \\\\{x_{at}\\\\}_{t=1}^T\\\\), for a T length utterance are 240 dimensional vectors containing three stacked 80 dimensional mel-frequency filter-bank vectors extracted over 30 msec. frames. The encoded audio vectors, \\\\(A = \\\\{a_t\\\\}_{t=1}^T\\\\), are the \\\\(D_a=1024\\\\) dimensional output of a transformer-transducer (T-T) with 6 layers and 8 attention heads [19].\\n\\nThe input video frames, \\\\(X_v = \\\\{x_{v_{mt}}\\\\}_{t=1}^T, m=1, M\\\\), for each of M overlapping speakers in a T length utterance are \\\\(128 \\\\times 128 \\\\times 3\\\\) thumbnail images. Visual features are 512 dimensional vectors computed from the input video frames using a \\\\((2+1)D\\\\) convolutional neural network [20]. This convolutional network factorizes 3D convolution into a 2D spatial convolution and a 1D temporal convolution. It was found to achieve greater efficiencies than the 3D convolutional network used in [8] and [5] with no observed impact on WER. Encoded visual features, \\\\(V_m = \\\\{v_{mt}\\\\}_{t=1}^T, m=1, 2\\\\), are the \\\\(D_v=1024\\\\) dimensional output of a T-T with 4 layers and 4 attention heads.\\n\\nThe attention network for the VCAM depicted in Figure 1b computes a \\\\(T \\\\times T \\\\times M\\\\) dimensional tensor of attention weights based on the similarity between the encoded audio and visual feature vectors. The similarity between encoded audio at time \\\\(j\\\\) and encoded video for mouth-track \\\\(m\\\\) at time \\\\(i\\\\) is computed as the inner product \\\\(S_{mi} = V_m^T A_j\\\\).\\n\\nThe attention weights are the softmax similarity values:\\n\\n\\\\[\\n\\\\text{w}_{mi} = \\\\text{Softmax}(S_{mi})\\n\\\\]\\n\\nThe attention weighted visual context for the \\\\(m\\\\)th mouth-track, \\\\(V'_{m} = \\\\{v'_{mt}\\\\}_{t=1}^T\\\\), is obtained by attention weighting the encoded visual features across time:\\n\\n\\\\[\\nv'_{mj} = \\\\sum_{i=1}^{T} w_{mi} v_{mi}\\n\\\\]\\n\\nFinally, the encoded audio features and attention weighted encoded visual features are input to a T-T based mask encoder with 6 layers and 6 attention heads and the \\\\(1024\\\\) dimensional mask output is input to the joint network of the E2E network.\\n\\n2.3. Detecting active speakers from VCAM attention maps\\n\\nFigure 2 displays two attention maps, one for each of the two overlapping speakers\u2019 mouth-tracks in an example overlapped utterance. The first overlapping waveform begins at frame \\\\(t=0\\\\) and ends at frame \\\\(t=134\\\\) with the attention weights associated with the mouth-track for that waveform shown in the plot on the left. The second...\"}"}
{"id": "rose22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"waveform begins at frame $t=63$ and ends at $t=205$ with the attention weights for that mouth-track shown in the plot on the right. The figure illustrates the effect of the VCAM attention network. Both plots display the $T \\\\times T$ attention weights, $w_{m_{ij}}$, $i, j = 1, \\\\ldots, T$, from Equation 2 which are applied to the input encoded visual frames $v_{0j}$ and $v_{1j}$. The $i$th horizontal line in each plot represents the weights applied to the $j=1, \\\\ldots, T$ input visual frames at the $i$th audio frame.\\n\\nThere are two observations that can be made from these attention maps. First, when a given talker is speaking, the attention weights for that speaker's mouth-track are extremely sparse, indicating that the correct mouth-track is being selected by the attention weighting. Furthermore, this appears to be true even when there is overlapping speech (the interval between the frames where speaker 2 starts talking and speaker 1 ends). Second, the magnitude of attention weights for a given mouth-track become much smaller for those intervals where the associated talker is not speaking.\\n\\nThis implies that the attention module is effectively acting as a speaker detector, performing the role of implicit assignment of audio frames to the associated on-screen face. This raises the question as to whether the VCAM model could be used for detecting active speakers in overlapped utterances. The performance of active speaker detection (ASD) with this model is addressed in Section 4 for the overlapped speech dataset described in Section 3.\\n\\n3. EXPERIMENTAL STUDY\\n\\nThis section describes the experimental study for evaluating the performance of the A/V multi-talker models presented in Section 2. This study was performed using simulated overlapping speech data sets because we are not aware of any publicly available audio-visual overlapping speech corpora. We are aware of the publicly available simulated audio-only speech corpus described in [14] developed from LibriSpeech utterances [21]. However, the experimental work in recent audio-visual speech separation work that we are aware of, including work presented in [15, 16, 17], has been performed using simulated synthetic mixtures of single talker utterances. Audio-visual multi-talker ASR experiments described in [12] are based on simulated 2-talker mixtures created from randomly selected single talker utterances in the LRS2 corpus. The A/V multi-talker experiments described in Section 4 are performed using the simulated A/V overlapping speech training and test sets described in Section 3.1.\\n\\n3.1. Simulated audio-visual overlapping speech corpora\\n\\nA training set of simulated two-speaker overlapped utterances was created from a corpus of single speaker A/V YouTube utterances containing aligned face and mouth tracks. The methodology behind the collection of the A/V YouTube corpus can be found in [18, 22, 5, 8]. The overlapped audio waveform was created by taking two of the above single speaker utterances, offsetting one in time with respect to the other, and adding the two audio signals. The video portion consists of two mouth tracks where one of the mouth tracks has been offset to be aligned with the corresponding offset audio signal. However, shifting the video frames creates a situation where there are no video frames associated with a given speaker in those regions where that speaker is not speaking. To enforce that the faces of the two speakers are available throughout the entire utterance, video frames for non-speech regions were filled with forward-backward repetitions of video frames from that speaker. This provides video that is from the same speaker, but that is not synchronized with the audio.\\n\\nThere is a large amount of recent published work investigating the important issue of generalizing audio-only M-T approaches to scenarios with a larger number of speakers and more arbitrary turn-taking [23, 9, 24, 25]. However, we also found it important to maintain accuracy for the multi-talker models on both overlapping speech as well as single speaker utterances. To do this, overlapping speech training data was combined with single speaker utterances. Two single speaker scenarios were investigated. The first scenario assumes that, while a single talker is speaking, there is also an additional non-speaking on-screen face. To simulate this scenario for a given target utterance, a mouth-track from a different randomly chosen utterance is added, without the associated audio, to the target utterance (\\\"TwoFace\\\" scenario). A second scenario assumes that there is a single on-screen speaker with no additional on-screen faces. This is simulated by using blank thumbnails for the second mouth-track in the A/V utterance (\\\"OneFace\\\" scenario).\\n\\nThe offset used in shifting the audio signals was chosen to provide overlap intervals randomly selected with a uniform distribution between 1 and 5 seconds. Each overlapped speech utterance was stored with two reference transcriptions, two mouth tracks, and overlap interval start and end times. The resulting training corpus contains 20k hours of training data. Half of the training set consists of A/V overlapping speech utterances and half consists of TwoFace single speaker utterances described above.\\n\\nOverlapped and single speaker test sets were obtained from human transcribed utterances with aligned mouth tracks taken from YouTube videos. The process of forming overlapped utterances is the same as described above for the training set. The test sets all contain 3601 utterances with the overlapped test utterances ranging in length from 2.7 to 14.7 seconds, and the single speaker test utterances ranging in length from 2.5 to 8.0 seconds.\\n\\n3.2. AI Principles\\n\\nThe work presented in this paper abides by Google AI Principles [26]. By improving the robustness of speech recognition systems, we hope to increase the reach of ASR technology to a larger population of users, and use it to develop assistive technology. The data and models developed in this work are restricted to a...\"}"}
{"id": "rose22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. EXPERIMENTAL RESULTS\\n\\nResults are presented here for multi-talker end-to-end models presented in Section 2 evaluated on the overlapping and single speaker test sets described in Section 3. First, a WER comparison is made between the VCAM A/V M-T model presented in Section 2.2 and the audio-only and A/V M-T models described in Section 2.1. Second, active speaker detection performance using the VCAM model is evaluated on the overlapped test set. Third, the impact of using transformer-transducer (T-T) versus LSTM based encoders in M-T models is evaluated. Finally, the performance of the M-T models on the single speaker scenario described in Section 3 is presented.\\n\\nTable 1 provides a comparison between the WERs obtained for the A/V VCAM M-T model from Section 2.2, the audio-only and A/V M-T models described in Section 2.1, and the A/V audio-only single channel model. All M-T model WERs represent a dramatic reduction relative to the SingleChan A/V model WER when evaluated on the Overlap test set. WERs shown in the table for the A/V M-T models represent greater than 18% reduction compared to the audio-only M-T model. The VCAM model provides a 3% reduction in WER relative to the previous A/V M-T model.\\n\\nTable 1. Comparison of WERs for T-T based SingleChan and MultiTalker models on Single and Overlap test sets.\\n\\n| Model                   | Test Set | WER  |\\n|-------------------------|----------|------|\\n| SingleChan A/V          | Single   | 12.2 |\\n| SingleChan A/V          | Overlap  | 37.3 |\\n| MultiTalker Audio       | Overlap  | 17.8 |\\n| MultiTalker A/V         | Overlap  | 14.5 |\\n| MultiTalker VCAM A/V    | Overlap  | 14.1 |\\n\\nSection 2.3 discusses the use of the VCAM attention module for detecting active speech in overlapping and non-overlapping speech frames. Active speaker detection (ASD) performance is measured here for the mouth-tracks in the overlapped speech test set described in Section 3 using mean average precision (mAP). For each mouth-track in an overlapped utterance, active speech is detected for a given visual frame when the maximum attention weight for that frame, as depicted in Figure 2.2, exceeds a threshold. The mAP score is generated from a precision-recall curve by varying this threshold over a range and averaging the precision score over a range of recall values. Figure 3 shows the recall, or percentage of visual frames correctly detected as active speech, plotted separately for overlapping and non-overlapping frames. It is clear from the figure that recall for overlapped frames is at most 15% less than non-overlapped frames at a given detection threshold. A mAP score of 95.8% was obtained on this set where 27.7% of the frames correspond to overlapping speech. Of course, this task is far more constrained than other ASD tasks. However, it is important to note that this performance is obtained in the context of heavily overlapped speech.\\n\\nPrevious work on end-to-end multi-talker models was reported using LSTM based audio-only and A/V encoders [2, 8]. Since all end-to-end models implemented in this work are based on T-T encoders, the LSTM and T-T implementations of these models are compared here. Table 2 shows these WER comparisons for audio-only and A/V configurations of M-T models. In both cases, the T-T implementation reduced WERs with respect to the LSTM implementation by approximately 16 percent.\\n\\nTable 2. Comparison of WERs using LSTM and T-T encoders for audio-only (Audio) and audio-visual (A/V) multi-talker (MultiTalker) models on 2 talker overlapped speech (Overlap) test sets.\\n\\n| Model                   | Encoder | Test Set | WER  |\\n|-------------------------|---------|----------|------|\\n| MultiTalker Audio       | LSTM    | Overlap  | 21.4 |\\n| MultiTalker Audio       | T-T     | Overlap  | 17.8 |\\n| MultiTalker A/V         | LSTM    | Overlap  | 17.4 |\\n| MultiTalker A/V         | T-T     | Overlap  | 14.5 |\\n\\nWhile the results presented in Table 1 demonstrate that the VCAM M-T model is capable of dramatic reductions in WER relative to the single channel end-to-end model on overlapping speech, it is still necessary to evaluate the M-T on single speaker non-overlapping utterances. A comparison between WERs obtained for single channel and VCAM multi-talker models for the two single speaker A/V scenarios discussed in Section 3 is presented in Table 3. For the TwoFace single speaker scenario, the WER for the VCAM M-T model increases by 6.1% relative to the baseline single channel A/V model. For the OneFace scenario the WER for the VCAM represents a 9.8% increase relative to the single channel model. It is assumed the larger increase in WER observed for the OneFace scenario is at least partly due to the fact that there were no OneFace single speaker utterances in the training set. The goal of future work is to overcome the degradation associated with this and other unseen conditions by further augmenting the training set.\\n\\nTable 3. WER comparison of MultiTalker and SingleChan models on single speaker utterances under two scenarios for simulating second on-screen face: 1) Simulated second on-screen face (TwoFace), and 2) No second on-screen face (OneFace).\\n\\n| Audio-visual RNNT Model Performance (WER%) |\\n|------------------------------------------|\\n| A/V Model                                |\\n| OnScreenScenario | WER  |\\n|------------------|------|\\n| SingleChan       | N/A  | 12.2 |\\n| MultiTalker VCAM | TwoFace | 13.0 |\\n| MultiTalker VCAM | OneFace | 13.4 |\\n\\n5. SUMMARY AND CONCLUSIONS\\n\\nThis paper has presented the VCAM A/V end-to-end multi-talker ASR model. The method avoids the label ambiguity problem in M-T models in that it associates decoded transcriptions with a given on-screen face. The VCAM model was also shown to have the potential of performing end-to-end speaker diarization for overlapping speech along with multi-talker speech-to-text. This is enabled by an attention module that is able to associate speaker dependent mouth-tracks with one of multiple overlapping utterances. Experimental results showed that the VCAM A/V M-T model obtained lower WER than previously presented A/V M-T models on a 2 speaker simulated overlapping speech task. Furthermore, the performance of the VCAM model for single speaker audio-visual utterances represented only a small degradation with respect to single channel A/V models. Current work is directed towards evaluating VCAM performance on an A/V dataset collected from meetings with multiple participants using a 360 degree camera and array microphone.\\n\\n6. ACKNOWLEDGMENTS\\n\\nThe authors would like to thank Takaki Makino and Hank Liao for their contributions to A/V speech corpus development, Otavio Braga for work on efficient spatiotemporal convolutions for video analysis, and Basi Garcia for help with tools in model evaluation.\"}"}
{"id": "rose22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. REFERENCES\\n\\n[1] \u00d6. Cetin and E. Shriberg, \u201cAnalysis of overlaps in meetings by dialog factors, hot spots, speakers, and collection site: insights for automatic speech recognition,\u201d in InterSpeech, 2006.\\n\\n[2] Anshuman Tripathi, Han Lu, and Hasim Sak, \u201cEnd-to-end multi-talker overlapping speech recognition,\u201d in ICASSP 2020, pp. 6129\u20136133.\\n\\n[3] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, \u201cDeep audio-visual speech recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\\n\\n[4] S. Petridis, T. Stafylakis, P. Ma, G. Tzimiropoulos, and M. Pantic, \u201cAudio-visual speech recognition with a hybrid CTC/Attention architecture,\u201d in IEEE SLT, 2018.\\n\\n[5] T. Makino, H. Liao, Y. Assael, B. Shillingford, B. Garcia, O. Braga, and O. Siohan, \u201cRecurrent neural network transducer for audio-visual speech recognition,\u201d in 2019 IEEE ASRU Workshop, pp. 905\u2013912.\\n\\n[6] D. Yu, M. Kolb\u00e6k, Z. Tan, and J. Jensen, \u201cPermutation invariant training of deep models for speaker-independent multi-talker speech separation,\u201d in ICASSP 2017, pp. 241\u2013245.\\n\\n[7] Chao Weng, Dong Yu, Michael L. Seltzer, and Jasha Droppo, \u201cDeep neural networks for single-channel multi-talker speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 10, pp. 1670\u20131679, 2015.\\n\\n[8] Richard Rose, Olivier Siohan, Anshuman Tripathi, and Otavio Braga, \u201cEnd-to-end audio-visual speech recognition for overlapping speech,\u201d in InterSpeech, 2021.\\n\\n[9] Naoyuki Kanda, Xuankai Chang, Yashesh Gaur, Xiaofei Wang, Zhong Meng, Zhuo Chen, and Takuya Yoshioka, \u201cInvestigation of end-to-end speaker-attributed ASR for continuous multi-talker recordings,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 809\u2013816.\\n\\n[10] X. Chang, W. Zhang, Y. Qian, J. Le Roux, and S. Watanabe, \u201cEnd-to-end multi-speaker speech recognition with transformer,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 02 2020.\\n\\n[11] Liang Lu, Naoyuki Kanda, Jinyu Li, and Yifan Gong, \u201cStreaming end-to-end multi-talker speech recognition,\u201d IEEE Signal Processing Letters, vol. PP, 04 2021.\\n\\n[12] Yifei Wu, Chenda Li, Song Yang, Zhongqin Wu, and Yanmin Qian, \u201cAudio-visual multi-talker speech recognition in a cocktail party,\u201d in Proc. Interspeech 2021, pp. 3021\u20133025.\\n\\n[13] John R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe, \u201cDeep clustering: Discriminative embeddings for segmentation and separation,\u201d in ICASSP 2016 - IEEE International Conference on Acoustics, Speech and Signal Processing.\\n\\n[14] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, \u201cContinuous speech separation: Dataset and analysis,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7284\u20137288.\\n\\n[15] Ariel Ephrat, Inbar Mosseri, Oran Lang, Tali Dekel, Kevin Wilson, Avinatan Hassidim, William T. Freeman, and Michael Rubinstein, \u201cLooking to listen at the cocktail party,\u201d ACM Transactions on Graphics, vol. 37, no. 4, pp. 1\u201311, Aug 2018.\\n\\n[16] Jianwei Yu, Shi-Xiong Zhang, Jian Wu, Shahram Ghorbani, Bo Wu, Shiyin Kang, Shansong Liu, Xunying Liu, Helen Meng, and Dong Yu, \u201cAudio-visual recognition of overlapped speech for the LRS2 dataset,\u201d in ICASSP, 2020.\\n\\n[17] Guan-Lin Chao, William Chan, and Ian Lane, \u201cSpeaker-targeted audio-visual models for speech recognition in cocktail-party environments,\u201d in InterSpeech, 09 2016, pp. 2120\u20132124.\\n\\n[18] H. Liao, E. McDermott, and A. Senior, \u201cLarge scale deep neural network acoustic modeling with semi-supervised training data for YouTube video transcription,\u201d in 2013 IEEE Workshop on Automatic Speech Recognition and Understanding, 2013, pp. 368\u2013373.\\n\\n[19] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, \u201cTransformer transducer: A streamable speech recognition model with transformer encoders and RNN-T loss,\u201d in ICASSP 2020, pp. 7829\u20137833.\\n\\n[20] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri, \u201cA closer look at spatiotemporal convolutions for action recognition,\u201d in IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2018.\\n\\n[21] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP 2015, 2015, pp. 5206\u20135210.\\n\\n[22] Brendan Shillingford, Yannis Assael, Matthew W. Hoffman, Thomas Paine, C\u00b4\u0131an Hughes, Utsav Prabhu, Hank Liao, Hasim Sak, Kanishka Rao, Lorrayne Bennett, Marie Mulville, Ben Coppin, Ben Laurie, Andrew Senior, and Nando de Freitas, \u201cLarge-scale visual speech recognition,\u201d in InterSpeech, 2018.\\n\\n[23] Desh Raj, Liang Lu, Zhuo Chen, Yashesh Gaur, and Jinyu Li, \u201cContinuous streaming multi-talker ASR with dual-path transducers,\u201d ArXiv, vol. abs/2109.08555, 2021.\\n\\n[24] Ilya Sklyar, Anna Piunova, Xianrui Zheng, and Yulan Liu, \u201cMulti-turn RNN-T for streaming recognition of multi-party speech,\u201d ArXiv, vol. abs/2112.10200, 2021.\\n\\n[25] Thilo von Neumann, Keisuke Kinoshita, Christoph Boeddeker, Marc Delcroix, and Reinhold Haeb-Umbach, \u201cGraph-PIT: Generalized permutation invariant training for continuous separation of arbitrary numbers of speakers,\u201d arXiv preprint arXiv:2107.14446, 2021.\\n\\n[26] Google, \u201cArtificial intelligence at Google: Our principles,\u201d https://ai.google/principles/.\\n\\n[27] European Union Law, \u201cRegulation (EU) 2016/679 of the European Parliament and of the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and on the free movement of such data, and repealing directive 95/46/EC (General Data Protection Regulation),\u201d https://eurlex.europa.eu/legal-content/EN/TXT/?uri=CELEX.\\n\\n[28] Joseph Roth, Sourish Chaudhuri, Ondrej Klejch, Radhika Marvin, Andrew C. Gallagher, Liat Kaver, Sharadh Ramaswamy, Arkadiusz Stopczynski, Cordelia Schmid, Zhonghua Xi, and Caroline Pantofaru, \u201cAVA-ActiveSpeaker: An audio-visual dataset for active speaker detection,\u201d 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp. 3718\u20133722, 2019.\"}"}
