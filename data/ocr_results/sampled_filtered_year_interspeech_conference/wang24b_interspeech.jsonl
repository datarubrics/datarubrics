{"id": "wang24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d in Proc. of ICLR, 2021.\\n\\n[2] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. A. Kudinov, \u201cGrad-tts: A diffusion probabilistic model for text-to-speech,\u201d in Proc. of ICML, vol. 139, 2021, pp. 8599\u20138608.\\n\\n[3] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in Proc. of ICML, vol. 139, 2021, pp. 5530\u20135540.\\n\\n[4] Z. Ye, W. Xue, X. Tan, J. Chen, Q. Liu, and Y. Guo, \u201cComo-speech: One-step speech and singing voice synthesis via consistency model,\u201d in Proc. of ACM MM, 2023, pp. 1831\u20131839.\\n\\n[5] X. Tan, T. Qin, F. K. Soong, and T. Liu, \u201cA survey on neural speech synthesis,\u201d CoRR, vol. abs/2106.15561, 2021.\\n\\n[6] M. Chen, X. Tan, B. Li, Y. Liu, T. Qin, S. Zhao, and T. Liu, \u201cAdaspeech: Adaptive text to speech for custom voice,\u201d in Proc. of ICLR, 2021.\\n\\n[7] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and M. A. Ponti, \u201cYourtts: Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone,\u201d in Proc. of ICML, vol. 162, 2022, pp. 2709\u20132720.\\n\\n[8] W. Wang, Y. Song, and S. K. Jha, \u201cGeneralizable zero-shot speaker adaptive speech synthesis with disentangled representations,\u201d in Proc. of INTERSPEECH, 2023, pp. 4454\u20134458.\\n\\n[9] Z. Jiang, Y. Ren, Z. Ye, J. Liu, C. Zhang, Q. Yang, S. Ji, R. Huang, C. Wang, X. Yin, Z. Ma, and Z. Zhao, \u201cMega-tts: Zero-shot text-to-speech at scale with intrinsic inductive bias,\u201d CoRR, vol. abs/2306.03509, 2023.\\n\\n[10] W. Wang, Y. Song, and S. K. Jha, \u201cUSAT: A universal speaker-adaptive text-to-speech approach,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 32, pp. 2590\u20132604, 2024.\\n\\n[11] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d CoRR, vol. abs/2301.02111, 2023.\\n\\n[12] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibritts: A corpus derived from librispeech for text-to-speech,\u201d in Proc. of INTERSPEECH, 2019, pp. 1526\u20131530.\\n\\n[13] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. of ICASSP, 2015, pp. 5206\u20135210.\\n\\n[14] V. Christophe, Y. Junichi, M. Kirsten, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,\u201d in University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2016.\\n\\n[15] R. Ardila, M. Branson, K. Davis, M. Kohler, J. Meyer, M. Hennig, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proc. of LREC, 2020, pp. 4218\u20134222.\\n\\n[16] S. Ogun, V. Colotte, and E. Vincent, \u201cCan we use common voice to train a multi-speaker TTS system?\u201d in Proc. of IEEE SLT, 2022, pp. 900\u2013905.\\n\\n[17] \u201cCreative commons attribution 4.0 license (cc-by 4.0),\u201d https://creativecommons.org/licenses/by/4.0/.\\n\\n[18] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, M. Bacchiani, Y. Zhang, W. Han, and A. Bapna, \u201cLibritts-r: A restored multi-speaker text-to-speech corpus,\u201d CoRR, vol. abs/2305.18802, 2023.\\n\\n[19] \u201cCc0 1.0 universal public domain dedication,\u201d https://creativecommons.org/publicdomain/zero/1.0/.\\n\\n[20] C. Kim and R. M. Stern, \u201cRobust signal-to-noise ratio estimation based on waveform amplitude distribution analysis,\u201d in Proc. of INTERSPEECH, 2008.\\n\\n[21] E. Bakhturina, V. Lavrukhin, B. Ginsburg, and Y. Zhang, \u201cHi-fi multi-speaker english TTS dataset,\u201d in Proc. of INTERSPEECH, 2021, pp. 2776\u20132780.\\n\\n[22] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in Proc. of ICML, 2023.\\n\\n[23] Y. Zhang, E. Bakhturina, and B. Ginsburg, \u201cNemo (inverse) text normalization: From development to production,\u201d in Proc. of INTERSPEECH, 2021, pp. 4857\u20134859.\\n\\n[24] W. Hsu, B. Bolte, Y. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 29, pp. 3451\u20133460, 2021.\\n\\n[25] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-TDNN: emphasized channel attention, propagation and aggregation in TDNN based speaker verification,\u201d in Proc. of INTERSPEECH, 2020, pp. 3830\u20133834.\\n\\n[26] B. Kang, S. Xie, M. Rohrbach, Z. Yan, A. Gordo, J. Feng, and Y. Kalantidis, \u201cDecoupling representation and classifier for long-tailed recognition,\u201d in Proc. of ICLR, 2020.\\n\\n[27] D. Min, D. B. Lee, E. Yang, and S. J. Hwang, \u201cMeta-stylespeech: Multi-speaker adaptive text-to-speech generation,\u201d in Proc. of ICML, vol. 139, 2021, pp. 7748\u20137759.\\n\\n[28] S. \u00d6. Arik, J. Chen, K. Peng, W. Ping, and Y. Zhou, \u201cNeural voice cloning with a few samples,\u201d in Proc. of NeurIPS, 2018, pp. 10 040\u201310 050.\\n\\n[29] T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari, \u201cUTMOS: utokyo-sarulab system for voicemos challenge 2022,\u201d in Proc. of INTERSPEECH, 2022, pp. 4521\u20134525.\\n\\n[30] S. Huang, C. Lin, D. Liu, Y. Chen, and H. Lee, \u201cMeta-tts: Meta-learning for few-shot speaker adaptive text-to-speech,\u201d IEEE ACM Trans. Audio Speech Lang. Process., vol. 30, pp. 1558\u20131571, 2022.\\n\\n[31] J. Xue, Y. Deng, Y. Han, Y. Li, J. Sun, and J. Liang, \u201cECAPA-TDNN for multi-speaker text-to-speech synthesis,\u201d in Proc. of ISCSLP, 2022, pp. 230\u2013234.\\n\\n[32] W. Huang, E. Cooper, Y. Tsao, H. Wang, T. Toda, and J. Yamagishi, \u201cThe voicemos challenge 2022,\u201d in Proc. of INTERSPEECH, 2022, pp. 4536\u20134540.\\n\\n[33] D. Rekesh, S. Kriman, S. Majumdar, V. Noroozi, H. Huang, O. Hrinchuk, A. Kumar, and B. Ginsburg, \u201cFast conformer with linearly scalable attention for efficient speech recognition,\u201d CoRR, vol. abs/2305.05084, 2023.\\n\\n[34] N. R. Koluguri, T. Park, and B. Ginsburg, \u201cTitanet: Neural model for speaker representation with 1d depth-wise separable convolutions and global context,\u201d in Proc. of ICASSP, 2022.\\n\\n[35] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoxceleb: A large-scale speaker identification dataset,\u201d in Proc. of Interspeech, 2017.\\n\\n[36] D. Friedman and A. B. Dieng, \u201cThe vendi score: A diversity evaluation metric for machine learning,\u201d Transactions on Machine Learning Research, 2023.\\n\\n[37] R. Burgert, X. Li, A. Leite, K. Ranasinghe, and M. S. Ryoo, \u201cDiffusion illusions: Hiding images in plain sight,\u201d CoRR, vol. abs/2312.03817, 2023.\\n\\n[38] Y. Chen, B. Xu, Q. Wang, Y. Liu, and Z. Mao, \u201cBenchmarking large language models on controllable generation under diversified instructions,\u201d CoRR, vol. abs/2401.00690, 2024.\\n\\n[39] F. Wilcoxon, \u201cIndividual comparisons by ranking methods,\u201d in Breakthroughs in Statistics: Methodology and Distribution, 1992, pp. 196\u2013202.\\n\\n[40] M. Bernard and H. Titeux, \u201cPhonemizer: Text to phones transcription for multiple languages in python,\u201d Journal of Open Source Software, vol. 6, no. 68, p. 3958, 2021.\\n\\n[41] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in Proc. of ICLR, 2019.\"}"}
{"id": "wang24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech\\n\\nWenbin Wang1, Yang Song1, Sanjay Jha1\\n\\n1School of Computer Science and Engineering, University of New South Wales, Australia\\nwenbin.wang@unsw.edu.au, yang.song1@unsw.edu.au, sanjay.jha@unsw.edu.au\\n\\nAbstract\\nThis paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to address the limitations of current zero-shot speaker adaptive Text-to-Speech (TTS) systems that exhibit poor generalizability in adapting to speakers with accents. Compared to commonly used English corpora, such as LibriTTS and VCTK, GLOBE is unique in its inclusion of utterances from 23,519 speakers and covers 164 accents worldwide, along with detailed metadata for these speakers. Compared to its original corpus, i.e., Common Voice, GLOBE significantly improves the quality of the speech data through rigorous filtering and enhancement processes, while also populating all missing speaker metadata. The final curated GLOBE corpus includes 535 hours of speech data at a 24 kHz sampling rate. Our benchmark results indicate that the speaker adaptive TTS model trained on the GLOBE corpus can synthesize speech with better speaker similarity and comparable naturalness than that trained on other popular corpora. We will release GLOBE publicly after acceptance. The GLOBE dataset is available at https://globecorpus.github.io/.\\n\\nIndex Terms\\ndataset, text-to-speech, speaker adaptation\\n\\n1. Introduction\\nRecent advances in deep learning have significantly propelled TTS research forward. The latest generation of neural networks-based TTS models [1, 2, 3, 4] can now generate highly lifelike human speech. This advancement has shifted the TTS research focus toward more sophisticated and challenging tasks [5]. Among these emerging tasks, speaker adaptive TTS [5, 6], also known as voice cloning, especially in zero-shot scenarios [7, 8, 9], has emerged as an active area of interest. Zero-shot speaker adaptive TTS allows TTS models to swiftly adapt to new speaker voices, which are not included in the training dataset, using only seconds of speech samples. This technique significantly broadens TTS technology's acceptability.\\n\\nIn our previous works [8, 10], we found a significant challenge in current zero-shot speaker adaptive TTS research is models' limited generalizability to accented voices. Despite increasing model parameters and enlarging the training dataset, this challenge persists [11, 9]. Our analysis identifies one of the crucial factors contributing to this issue: the prevalent English TTS datasets contain a limited set of accents. For example, LibriTTS [12] and LibriSpeech [13] mainly consists of speakers with US English. The VCTK [14] dataset encompasses speakers with only 11 accents. The Common Voice dataset [15], which comprises more than 3,000 hours of speech and covers up to 337 accents, presents a potential solution. However, it also exhibits several undesirable characteristics for building TTS system [16]: 1) A significant number of speech samples contain noticeable background or electromagnetic noise; 2) Despite the audio file has a sample rate of 48 kHz, the actual signal bandwidth is limited; 3) Many samples feature mispronunciations or corrections where speakers repeat unfamiliar words upon realizing a mispronunciation; 4) Half of the speakers have missing metadata and the accent labels are confusing.\\n\\nTo address these issues, we introduce GLOBE, a high-quality English corpus with global accents, based on the Common Voice dataset. To construct this data, we remove low-quality, bandwidth-limited audio samples and re-align the utterance and text. Then, we manually cleaned the accent labels and populated missing speaker metadata through our prediction model. Compared to other popular English TTS datasets [14, 12, 15], the GLOBE dataset has its unique features:\\n\\n- **High Speech Quality:** The GLOBE dataset contains 535 hours of high-quality speech filtered from over 3,000 hours, with signal-to-noise ratio, signal bandwidth, and transcription accuracy. Our experimental results on zero-shot speaker adaptive TTS indicate that speech samples in GLOBE surpass those in VCTK and LibriTTS in terms of objective and subjective naturalness in mean opinion score evaluations.\\n\\n- **Global Accent Coverage:** With 23,519 speakers representing 164 different English accents from more than 50 countries, GLOBE offers unparalleled accent diversity. Our experiments demonstrate that such diversity significantly improves the generalizability of zero-shot speaker adaptive TTS models to different accents.\\n\\n- **Extra Speaker Information:** In addition to speech audio and corresponding text, GLOBE provides detailed metadata for all 23,519 speakers, including accent, age, and gender. This additional information enables future research of more personalized TTS models and mitigation of bias.\\n\\n2. Relevant English Multi-speaker Corpus\\nVCTK [14]. The VCTK dataset is a widely utilized corpus for developing TTS and voice cloning systems. It contains 44 hours of 48 kHz speech data from 109 English speakers and each speaker reads about 400 sentences from newspapers. Moreover, the VCTK dataset includes labels for each speaker's accent and gender, covering a total of 11 accents.\\n\\n1 In this paper, the \u201cCommon Voice\u201d dataset specifically refers to its Common Voice Corpus 14.0 English subset at https://commonvoice.mozilla.org/en/datasets. The original Common Voice dataset includes multiple languages and versions.\"}"}
{"id": "wang24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics on GLOBE and Relevant English Multi-speaker Corpus\\n\\n| Corpus       | Total Hours | Total Speakers | Sample Rate | Total Accent | Speaker Info                  | License        |\\n|--------------|-------------|----------------|-------------|--------------|-------------------------------|----------------|\\n| CSTR VCTK   | 44          | 109            | 48 kHz      | 11 Accent, Gender | CC BY 4.0                   | [17]           |\\n| LibriTTS    | 586         | 2,456          | 24 kHz      | -            | -                             | CC BY 4.0      | [17]           |\\n| LibriTTS-R  | 585         | 2,456          | 24 kHz      | -            | -                             | CC BY 4.0      | [17]           |\\n| Common V oice | 3,347     | 88,904         | 48 kHz      | 337 Accent, Age, Gender | CC-0 1.0             | [19]           |\\n| GLOBE        | 535         | 23,519         | 24 kHz      | 164 Accent, Age, Gender | CC-0 1.0             | [19]           |\\n\\nLibriTTS. The LibriTTS dataset is a well-known multi-speaker dataset for training speaker adaptive TTS systems. It derives from the LibriSpeech dataset [13] and includes 585 hours of audio recordings at a 24 kHz sampling rate, contributed by 2,456 speakers. This dataset specifically targets and mitigates various limitations in the original LibriSpeech collection, making it suitable for TTS system applications.\\n\\nLibriTTS-R. The LibriTTS-R corpus, an enhanced version of the LibriTTS dataset, significantly improves audio quality by incorporating speech restoration techniques. These enhancements support the training of high-quality TTS models. This corpus is the same as LibriTTS, retaining 585 hours of speech data from 2,456 speakers.\\n\\nCommon V oice. Common V oice is a dataset powered by the voices of volunteer contributors from all around the world. It contains 3,347 hours of audio from 88,904 speakers, recorded at a 48kHz sample rate. The Common V oice dataset is frequently utilized to build automatic speech recognition systems. Another study [16] demonstrates that despite Common V oice's extensive collection of audio, it is not suitable for building TTS models due to the prevalence of poor-quality audio.\\n\\nThe key information for these datasets, along with GLOBE, is summarized in Table 1.\\n\\n3. Data Processing Pipeline\\n\\n3.1. Speech Sample Pre-processing and Filtering\\n\\nDuring the initial construction phase of the GLOBE corpus, low-quality speech samples are identified and removed to prevent adverse impacts on the performance of TTS models when utilized as training data. The primary metric used for assessing speech sample quality is the signal-to-noise ratio (SNR), estimated through waveform amplitude distribution analysis [20], as a crucial indicator of audio clarity by quantifying the ratio of unwanted noise to identifiable speech. In line with [12], utterances with an SNR below 0 dB are excluded from the GLOBE corpus. Furthermore, the actual signal bandwidth is determined by identifying the highest frequency that is at least -50 dB below the power spectrogram's peak average, following the methodology outlined in [21]. Utterances with actual signal bandwidths below 12 kHz are excluded. Additionally, utterances containing more than 930 milliseconds of continuous internal silence, attributed to abnormal pauses or hesitations, are removed to further avoid negative impacts on the duration predictors within TTS models. The internal silence is detected by a voice activity detection (V AD) tool [21].\\n\\n3.2. Speech Text Alignment\\n\\nGiven the significantly higher word error rate of the Common V oice dataset compared to other popular TTS datasets [14, 12], which seriously impacts the intelligibility of synthesized speech if trained on these data [16], the next phase in the development of the GLOBE corpus focuses on improving transcription accuracy. Following [12], Whisper [22] is firstly utilized to transcribe all utterances and a weighted finite-state transducer-based system [23] is then employed to normalize the corresponding text from the original dataset and the transcribed texts to their spoken forms. Following that, the word-level edit distance between them is computed by a publicly available toolkit [3]. Utterances exhibiting an edit distance greater than 1 are eliminated. Furthermore, clips that have an edit distance of 1 due to consecutive word repetitions are also discarded, as this often results from the repeated pronunciation of unfamiliar words according to our analysis.\\n\\n3.3. Speaker Information Refinement and Speech Post-processing\\n\\nThe final development stage involves populating missing metadata for speakers including accent, age and gender, and applying speech sample post-processing. Initially, accents represented by fewer than five speakers or 10 utterances are merged with the most similar accent label or removed from the dataset, as we believe that such limited speech samples do not adequately capture the full scope of an accent's characteristics. Furthermore, meaningless accent labels, such as \\\"not bad\\\" and \\\"A'lo,\\\" are also removed along with their corresponding speakers. Subsequently, for each metadata class, i.e., accent, age and gender, a training dataset, along with label-balanced validation and test sets, are developed from the refined speech data and speaker labels. Each training set includes utterances from 11,000 speakers, while each validation and test set features at least 1,000 speakers. Utilizing these subsets, three speaker information prediction models with the same structure, as depicted in Figure 2, are developed to predict the speaker's accent, age and gender.\\n\\n3. The key information for these datasets, along with GLOBE, is summarized in Table 1.\"}"}
{"id": "wang24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gender, respectively. Due to the long-tail distribution of the speaker's accent label, as shown in Figure 1, the square-root sampling method [26] is employed during model training to mitigate the negative impact. These models finally achieve accuracies of 97.22%, 99.55%, and 99.95% for accent, age, and gender prediction, respectively, across the test sets. The models are then leveraged to populate the missing speaker metadata. After that, post-processing is applied to all utterances, which includes the elimination of leading and trailing silences based on VAD results and the further suppression of the background noise through a speech enhancement tool.\\n\\n4. Experiments\\n4.1. Experiments for Ground-Truth Speech Samples\\n4.1.1. Experimental setups\\nIn the first experiment, we evaluated the quality of ground-truth speech samples within GLOBE and other popular English multi-speaker datasets [14, 27, 18, 15]. To objectively evaluate audio quality, we randomly selected 10,000 samples from the full set of VCTK [14], the training set of GLOBE and Common Voice [15] and the \u201ctrain-clean\u201d subsets of LibriTTS [27] and LibriTTS-R [18]. These subsets were selected because they represent the highest audio quality available in each dataset. For subjective evaluations, particularly the mean opinion score, we randomly chose 120 samples from those used in the objective evaluation for each dataset. The following evaluation metrics were utilized:\\n\\n- **Naturalness Mean Opinion Score (NMOS).** To evaluate the naturalness of speech samples, following [18, 28], we employed the Mean Opinion Score. Participants were asked to rate the naturalness of each utterance using a five-point Likert Scale [27]. Each speech sample was rated by five distinct participants, and we calculated the average score along with a 95% confidence interval by the official tool [17] for each evaluated dataset.\\n\\n- **UTokyo-SaruLab Mean Opinion Score (UT-MOS) [29].** In line with prior studies [30, 31], predicted NMOS values were also provided for reference. The UT-MOS model was employed for NMOS prediction, which achieved state-of-the-art performance in 10 out of 16 metrics in the VoiceMOS Challenge [32].\\n\\n- **Word Error Rate (WER).** Following [18, 27], we employed the WER metric to measure the average misalignment in speech transcripts relative to the ground-truth text. A lower WER indicates more accurate alignment. Speech transcription was conducted using a pre-trained Conformer-based automatic speech recognition model [33].\\n\\n- **Speaker Embedding Cosine Similarity (SMCS).** Consistent with [18, 7], speaker embedding cosine similarity was used to evaluate the similarity between two speeches from the same speaker. The speaker embeddings were extracted using the TitaNet-L speaker verification model [34], which achieves the state-of-the-art equal error rate on the VoxCeleb1 [35].\\n\\n- **Speaker Embedding Vendi Score (SEVS).** Considering that the number of speakers in a dataset does not necessarily reflect its speaker or accent diversities, we introduced the speaker embedding-based Vendi Score [36] to evaluate the diversity of accents contained in each dataset. The Vendi Score is defined as the exponential of the Shannon entropy of the eigenvalues of a similarity matrix and has been used in both computer vision [37] and natural language processing research [38].\\n\\n4.1.2. Experimental results\\nTable 2 presents the experiment results for ground-truth speech samples. Concerning speech naturalness, all datasets, except for the Common Voice dataset, which recorded a lower NMOS of 3.54 due to prevalent low-quality audio, achieved similar NMOS scores. We also conducted the Mann\u2013Whitney\u2013Wilcoxon (MWW) test [39] to determine whether there are statistically significant differences in NMOS scores between any two datasets. It was found that the p-value between LibriTTS and GLOBE was $0.91 > 0.05$, indicating that the speech samples from GLOBE do not show a statistically significant difference from LibriTTS in naturalness. In contrast, the p-value between Common Voice and GLOBE is $4.3 e^{-6} < 0.05$, denoting that the naturalness of speech from GLOBE is statistically better than that of Common Voice. These findings are also corroborated by UT-MOS results. Regarding the WER, the VCTK dataset had the lowest WER, with LibriTTS, LibriTTS-R, and GLOBE showing comparably low WER levels, contrasting with Common Voice's higher WER. For SMCS, all datasets displayed similar scores, suggesting well-defined speaker characteristics within each dataset, given that TitaNet-L's threshold for determining utterances from the same speaker is 0.7. SEVS results indicate that LibriTTS and LibriTTS-R have wider speaker diversity than VCTK, with both Common Voice and GLOBE also showing improvements in speaker diversity compared to LibriTTS and LibriTTS-R. In summary, these results demonstrate that GLOBE achieves speech naturalness comparable to other popular TTS datasets by filtering out low-quality speech samples from Common Voice while maintaining richer speaker diversity compared to other datasets. We also visualized the speaker embedding distributions and their marginal distributions' kernel density estimates (KDE) of the GLOBE, VCTK, and LibriTTS datasets, as shown in Figure 3. To do this, we extracted 10,000 speaker embeddings from the...\"}"}
{"id": "wang24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation Results of the modified YourTTS Trained on Different Datasets.\\n\\n| Evaluation Corpus | WER(%) | UT-MOS | SMOS | SMCS |\\n|-------------------|--------|--------|------|------|\\n| LibriTTS test set | 9.5 \u00b1 0.5 | 4.8 \u00b1 0.2 | 9.1 \u00b1 0.2 | 8.6 \u00b1 0.1 |\\n| LibriTTS-R test set | 9.8 \u00b1 0.6 | 4.6 \u00b1 0.1 | 9.0 \u00b1 0.2 | 8.7 \u00b1 0.2 |\\n| Common Voice | 8.9 \u00b1 0.4 | 5.0 \u00b1 0.3 | 8.8 \u00b1 0.3 | 8.5 \u00b1 0.2 |\\n| GLOBE | 8.5 \u00b1 0.3 | 5.2 \u00b1 0.1 | 8.7 \u00b1 0.2 | 8.3 \u00b1 0.1 |\\n\\n- WER: Word Error Rate\\n- UT-MOS: User Test-MOS\\n- SMOS: Speech Mean Opinion Score\\n- SMCS: Speaker Mean Opinion Score\\n\\nGLOBE not only surpasses the popular TTS datasets like LibriTTS but also covers a broader range of datasets, enabling it to effectively adapt to various accents.\\n\\nIn terms of the Common Voice and GLOBE datasets, training was performed on the \\\"train-clean\\\" and \\\"train-other\\\" subsets. The model was adjusted to facilitate training with 24 kHz audio data to match the GLOBE statistics contained within the GLOBE corpus.\\n\\nTo investigate the influence of training datasets on the synthesized speech, we trained the modified YourTTS models on all datasets outlined in Section 2. For the VCTK dataset, training encompassed the \\\"train-clean\\\" and \\\"train-other\\\" subsets. In terms of the Common Voice and GLOBE datasets, training was performed on the \\\"train-clean\\\" and \\\"train-other\\\" subsets. The baseline model with three modifications: firstly, the language embedding was removed from the pre-trained speaker encoder with a trainable encoder, i.e., EPACA-TDNN; secondly, some model parameters were adjusted to facilitate training with 24 kHz audio data; thirdly, some model parameters were adjusted to avoid bias introduced by pre-trained models, we replaced the pre-trained speaker encoder with a trainable encoder, i.e., EPACA-TDNN.\\n\\nTo thoroughly assess the datasets' influence on model performance, we employed YourTTS, a widely used zero-shot speaker-adaptive TTS approach, as the baseline model with three modifications: firstly, the language embedding was removed from the pre-trained speaker encoder with a trainable encoder, i.e., EPACA-TDNN; secondly, some model parameters were adjusted to facilitate training with 24 kHz audio data; thirdly, some model parameters were adjusted to avoid bias introduced by pre-trained models, we replaced the pre-trained speaker encoder with a trainable encoder, i.e., EPACA-TDNN.\\n\\nWe trained the modified YourTTS models on all datasets outlined in Section 2.\"}"}
