{"id": "nafea24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] C. R. Carlson, \u201cMisogynistic Hate Speech and its Chilling Effect on Women\u2019s Free Expression during the 2016 U.S. Presidential Campaign,\u201d Journal of Hate Studies, vol. 14, no. 1, 2019.\\n\\n[2] Paul Mozur, \u201cA Genocide Incited on Facebook, With Posts From Myanmar\u2019s Military,\u201d The New York Times, 2018.\\n\\n[3] M. L. Williams, P. Burnap, A. Javed, H. Liu, and S. Ozalp, \u201cHate in the Machine: Anti-Black and Anti-Muslim Social Media Posts as Predictors of Offline Racially and Religiously Aggravated Crime,\u201d The British Journal of Criminology, 2019.\\n\\n[4] Z. Talat, \u201cAre You a Racist or Am I Seeing Things? Annotator Influence on Hate Speech Detection on Twitter,\u201d in Proceedings of the First Workshop on NLP and Computational Social Science. Austin, Texas: Association for Computational Linguistics, 2016.\\n\\n[5] T. Davidson, D. Warmsley, M. Macy, and I. Weber, \u201cAutomated Hate Speech Detection and the Problem of Offensive Language,\u201d in Proceedings of the International AAAI Conference on Web and Social Media, vol. 11, 2017.\\n\\n[6] D. Kiela, H. Firooz, A. Mohan, V. Goswami, A. Singh, P. Ringshadow, and D. Testuggine, \u201cThe Hateful Memes Challenge: Detecting Hate Speech in Multimodal Memes,\u201d arXiv:2005.04790 [cs], 2020.\\n\\n[7] J. Neitsch and O. Niebuhr, \u201cOn the role of prosody in the production and evaluation of German hate speech,\u201d in Speech Prosody 2020. ISCA, 2020.\\n\\n[8] B. Vidgen, H. Margetts, and A. Harris, How Much Online Abuse Is There? A Systematic Review of Evidence for the UK. London: The Alan Turing Institute, 2019.\\n\\n[9] B. W. Matthews, \u201cComparison of the predicted and observed secondary structure of T4 phage lysozyme,\u201d Biochimica Et Biophysica Acta, vol. 405, no. 2, 1975.\\n\\n[10] Z. Talat and D. Hovy, \u201cHateful Symbols or Hateful People? Predictive Features for Hate Speech Detection on Twitter,\u201d in Proceedings of the NAACL Student Research Workshop. San Diego, California: Association for Computational Linguistics, 2016.\\n\\n[11] F. M. Plaza-del-arco, D. Nozza, and D. Hovy, \u201cRespectful or Toxic? Using Zero-Shot Learning with Language Models to Detect Hate Speech,\u201d in The 7th Workshop on Online Abuse and Harms (WOAH). Toronto, Canada: Association for Computational Linguistics, 2023.\\n\\n[12] J. Neitsch and O. Niebuhr, \u201cAre Germans Better Haters Than Danes? Language-Specific Implicit Prosodies of Types of Hate Speech and How They Relate to Perceived Severity and Societal Rules,\u201d in Interspeech 2020. ISCA, 2020.\\n\\n[13] O. Niebuhr, \u201cProsody in hate speech perception: A step towards understanding the role of implicit prosody,\u201d in Speech Prosody 2022. ISCA, 2022.\\n\\n[14] C. Jacobs, N. C. Rakotonirina, E. A. Chimoto, B. A. Bassett, and H. Kamper, \u201cTowards hate speech detection in low-resource languages: Comparing ASR to acoustic word embeddings on Wolof and Swahili,\u201d in INTERSPEECH 2023. ISCA, 2023.\\n\\n[15] S. Ghosh, S. Lepcha, S. Sakshi, R. R. Shah, and S. Umesh, \u201cDeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances,\u201d in Interspeech 2022. ISCA, 2022.\\n\\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d in Proceedings of the 2019 Conference of the North. Minneapolis, Minnesota: Association for Computational Linguistics, 2019.\\n\\n[17] T. Davidson, D. Bhattacharya, and I. Weber, \u201cRacial Bias in Hate Speech and Abusive Language Detection Datasets,\u201d in Proceedings of the Third Workshop on Abusive Language Online. Florence, Italy: Association for Computational Linguistics, 2019.\\n\\n[18] E. Wulczyn, N. Thain, and L. Dixon, \u201cEx Machina: Personal Attacks Seen at Scale,\u201d in Proceedings of the 26th International Conference on World Wide Web. Perth Australia: International World Wide Web Conferences Steering Committee, 2017.\\n\\n[19] Z. Talat, T. Davidson, D. Warmsley, and I. Weber, \u201cUnderstanding Abuse: A Typology of Abusive Language Detection Subtasks,\u201d in Proceedings of the First Workshop on Abusive Language Online. Vancouver, BC, Canada: Association for Computational Linguistics, 2017.\\n\\n[20] P. Fortuna, M. Dominguez, L. Wanner, and Z. Talat, \u201cDirections for NLP practices applied to online hate speech detection,\u201d in Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, 2022.\\n\\n[21] J. L. Kirtz and Z. Talat, \u201cFutures for Research on Hate Speech in Online Social Media Platforms,\u201d 2023.\\n\\n[22] N. Thylstrup and Z. Talat, \u201cDetecting \u2018Dirt\u2019 and \u2018Toxicity\u2019: Re-thinking Content Moderation as Pollution Behaviour,\u201d SSRN Electronic Journal, 2020.\\n\\n[23] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust Speech Recognition via Large-Scale Weak Supervision,\u201d 2022.\\n\\n[24] W. Antoun, F. Baly, and H. Hajj, \u201cAraBERT: Transformer-based model for Arabic language understanding,\u201d in Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection, H. Al-Khalifa, W. Magdy, K. Darwish, T. Elsayed, and H. Mubarak, Eds. Marseille, France: European Language Resource Association, 2020.\\n\\n[25] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30. Curran Associates, Inc., 2017.\\n\\n[26] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. Balcan, and H. Lin, Eds., vol. 33. Curran Associates, Inc., 2020.\\n\\n[27] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, 2021.\\n\\n[28] S. R. Livingstone and F. A. Russo, \u201cThe ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english,\u201d PloS one, vol. 13, no. 5, p. e0196391, 2018.\\n\\n[29] J. Grosman, \u201cFine-tuned XLSR-53 large model for speech recognition in Arabic,\u201d https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-arabic, 2021.\\n\\n[30] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020.\\n\\n[31] N. Halabi, \u201cModern standard Arabic phonetics for speech synthesis,\u201d Ph.D. dissertation, University of Southampton, 2016.\\n\\n[32] H. Akoglu, \u201cUser\u2019s guide to correlation coefficients,\u201d Turkish Journal of Emergency Medicine, vol. 18, no. 3, 2018.\\n\\n[33] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, no. 4, 2008.\\n\\n[34] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, \u201cMELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, 2019.\"}"}
{"id": "nafea24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A\\\\textsc{RA}O\\\\textsc{FFENSE}: Detecting Offensive Speech Across Dialects in Arabic Media\\n\\nYoussef Nafea\\\\textsuperscript{1}, Shady Shehata\\\\textsuperscript{1,2}, Zeerak Talat\\\\textsuperscript{1}, Ahmed Aboeitta\\\\textsuperscript{1}, Ahmed Sharshar\\\\textsuperscript{1}, Preslav\\\\textsuperscript{1} Nakov\\n\\n\\\\textsuperscript{1}Mohamed Bin Zayed University of Artificial Intelligence, UAE\\n\\\\textsuperscript{2}Invertible AI\\n\\\\{youssef.nafea, shady.shehata\\\\}@mbzuai.ac.ae\\n\\nAbstract\\n\\nNatural language processing (NLP) has made efforts towards identifying toxicity and offensive content for the text and image modalities. Despite sharing similar concerns with text and images, such as increased access to online abuse using speech, speech offensiveness research trails behind. While NLP has primarily considered English language data, speech has emphasized under-represented languages such as Swahili and Wolof. In this work, we introduce \\\\textsc{RAOFFENSE}, a dataset of scripted media in Arabic dialects labelled for offensiveness. \\\\textsc{RAOFFENSE} contains 2146 instances, of which 475 are labelled as offensive, spanning 1.55 hours of audio. We assess the capabilities of speech models to detect offensive content and present a hard-to-beat multi-modal text and audio model which outperforms the baselines by 26+% in terms of the Matthews Correlation Coefficient. Our work thus presents the first benchmark for offensive speech detection in dialectical Arabic.\\n\\nIndex Terms\\n\\ncomputational paralinguistics, content moderation, offensive content\\n\\n1. Introduction\\n\\nHate speech and offensive language represent significant social issues as they can have inhibiting effects on public participation\\\\cite{1} and result in increases in tension\\\\cite{2} and physical violence between different groups\\\\cite{3}. For this reason, research in natural language processing (NLP) has extensively explored offensive language and hate speech detection in text\\\\cite{4,5} and images\\\\cite{6}. In contrast, the area has received scant attention within the voice communication domain, despite the fact that spoken communication can carry more information, e.g., through prosody\\\\cite{7}. However, as social media platforms make available mechanisms for communicating using speech and recordings, developing resources for detecting hateful and offensive speech is becoming increasingly necessary.\\n\\nWhile important, collecting data for offensive and hateful speech can be particularly difficult because (1) it is a relatively infrequent occurrence, accounting for only up to 5% of data on social media platforms\\\\cite{8} and (2) such content is often deeply personal and private. To address this research gap, we present a dataset for detecting offensive content in Arabic fictional televised media. We use fictional televised media, as it allows us to circumvent privacy risks to individuals and can therefore be made publicly available. The dataset consists of more than 2000 spoken segments totalling 1.55 hours of speech, of which 475 are labelled as offensive.\\n\\nWe find that using a unimodal approach to detecting offensiveness in Arabic can provide reasonable results; using language-specific models in a multimodal setup can greatly improve results. Indeed, our experiments show that our best multimodal setup outperforms the best unimodal baseline by 26% in terms of the Matthews Correlation Coefficient (MCC)\\\\cite{9}. We conclude that multimodal and language-specific approaches can improve over multimodal, language-invariant approaches, calling for the need for better language-specific resources.\\n\\n2. Related Work\\n\\nWithin NLP, prior work has extensively examined detecting offensive\\\\cite{5} and hateful speech\\\\cite{10}, using a variety of methods, including Support Vector Machines (SVM)\\\\cite{5} deep neural networks, e.g., Convolutional Neural Networks (CNN) and recently, fine-tuned large language models (LLMs)\\\\cite{11}. On the other hand, offensive language and hate speech detection has received less attention in speech. Here, we review three different strands of research on hate speech in the speech community.\\n\\nLinguistic Analyses\\n\\nPrior work, e.g.,\\\\cite{7,12,13} present linguistic analyses of the prosody of hate speech. For instance\\\\cite{7} analyzes the perceptions on personal and legal acceptability of hate speech in German. Presenting recordings of hate speech, they survey participants for whether the contents of the recordings are acceptable to them, individually, and whether there should be legal ramifications to such speech.\\n\\nThey find that there are significant correlations between prosodic features and whether participants believe the speech should have legal ramifications.\\n\\nKeyword Spotting\\n\\nWorking in a low-resource domain,\\\\cite{14} develop hateful keyword spotting systems for Swahili and Wolof. They present ASR-based systems using (1) Wav2vec XLS-R, and (2) a fully acoustic model using a multilingual correspondence auto-encoder recurrent neural network with Acoustic Word Embeddings. They find that while the latter underperforms in a sanitized test environment, it is more robust to domain shifts.\\n\\nEnd-to-End Classification\\n\\n\\\\cite{15} present a new dataset, DeToxy which consists of 5K toxic and 2M non-toxic statements in English. The data is sourced from publicly available speech datasets, which are re-annotated for toxicity, where more than 10% of the dataset is labelled as toxic by a text-based classifier. They present two baselines: a 2-step approach, which includes transcription of audio segments using Automatic Speech Recognition (ASR) and a BERT-based\\\\cite{16} text classifier; and an end-to-end approach using Wav2vec. In both approaches, they fine-tune Wav2vec on non-hateful data before fine-tuning their models on DeToxy, and find that their end-to-end approach outperforms the 2-step method as it captures semantic information that is lost in text through the speech signal.\"}"}
{"id": "nafea24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Dataset\\n\\nWe present our dataset, ARAOFFENSE, of offensive Speech in Arabic media across different dialects. ARAOFFENSE consists of segments from 28 movies and TV series across Egyptian, Gulf, Levantine, Iraqi, and Maghriby dialects in Arabic. We select media that are flagged for adult (16+) audiences and are available on YouTube either as a whole or as short clips. We then label dialogue turns in the clips for whether they are intended as offensive or not, resulting in a dataset containing 2146 labelled instances, of which 475 are offensive, spanning 1.55 hours of speech. In the following sections, we describe the data collection and annotation processes.\\n\\n3.1. Defining \\\"Offensiveness\\\"\\n\\nIn NLP, there have been significant discussions on the definition to use for categories such as \\\"offensive,\\\" \\\"toxic,\\\" and \\\"hate speech.\\\" In this work, we use the term \\\"offensive\\\" to mean \\\"speech that involves the use of language that is deemed inappropriate, disrespectful, or hurtful within a specific cultural or social setting, as well as speech that aims to insult, threaten, demean, or provoke others.\\\" Thus, we broadly follow the understanding of toxicity presented by [18], that \\\"offensive\\\" is content that can create a hostile environment.\\n\\n3.2. Data Collection\\n\\nWe collect our initial data sample of 2300 instances by collecting publicly available clips and episodes from Arabic television series and movies, henceforth media. We selected media that were rated for audiences 16 years or older, and marked as containing inappropriate language. We specifically chose to collect audio from movies and TV series to have high quality audio data from actors, correctly expressing emotions, which mimics real environments. Once identified, we then downloaded the media, along with any subtitles available, from YouTube. We then identified words that are frequently associated with offensive content, such as slurs and profanity, and extracted audio clips with an average length of 2.8 seconds. We then manually filtered segments with incoherent audio and audio with a large degree of noise, resulting in a sample of 2146 instances for annotation from 28 movies and TV series spanning Egyptian, Gulf, Levantine, Iraqi, and Maghriby Arabic dialects.\\n\\n3.3. Annotation Guidelines\\n\\nIn order to address the difficult challenge of obtaining consistency and agreement in annotating online abuse, we develop our annotation guidelines which present our definition of offensiveness (see Section 3.1) and the process for labelling data and dealing with uncertainty. In the instructions, we outline how they should address ambiguous cases by marking them for discussion during review sessions. We further instruct them to discard all samples with overlapping dialogue or background noise that rendered the dialogue difficult to understand, or entirely incomprehensible. During the annotation procedure, we held regular review sessions to resolve any disagreements and to specify the annotation guidelines further to ensure a consistent understanding of edge cases.\\n\\n3.4. Annotation Procedure\\n\\nThree of the authors were selected as annotators due to their fluency in Arabic and familiarity with different dialects of Arabic. Prior to the first round of annotation, the annotators reviewed the annotation guidelines and annotated a small batch of 50 instances separately, followed by a discussion with the remaining authors, to ensure a uniform understanding of the guidelines. The annotators were instructed to label each media clip as offensive or non-offensive using the definition provided in Section 3.1. All annotators then labelled the remaining dataset separately and met to discuss any instances of disagreement during review sessions. We include all samples in our dataset, where the annotators reached an agreement on the label as shown in Figure 1.\\n\\n3.5. Dataset Analysis\\n\\nWe collect 475 offensive and 1,671 non-offensive instances. Loudness levels, we find that annotators increasingly label content as offensive, when the decibel levels rise, with the two lowest levels consisting of less than 15% of offensive instance while data labeled for the highest loudness level collects almost twice as many offensive instances than non-offensive (see Table 1). We also find gendered disparities: 23% of the content from speakers labelled as male was labelled as offensive, in contrast to only 20% of content from speakers labelled as women. ARAOFFENSE is a highly imbalanced dataset just with 22% of the dataset labelled as offensive (see Table 1). Moreover, considering Figure 2, we note that there is also a large imbalance in terms of the length of speech segments where shorter sequences dominate the dataset. Upon inspection, the shorter sequences often contain just one sentence, and longer segments are infrequent due to the conversational nature of ARAOFFENSE.\\n\\n4. Methodology\\n\\nThe particular modelling choices made in developing automated systems for offensive language can have a strong impact on the affordances of content moderation systems [22]. Here, we...\"}"}
{"id": "nafea24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Loudness (dB) | Total\\n--- | ---\\n0-55 | 13 | 54 | 160 | 219 | 29 | 475\\n55-58 | 52 | 327 | 800 | 477 | 15 | 1,671\\n65-70 | |\\n\\nTable 1: Overview of decibel levels and offensiveness in ARAFFENSE.\\n\\nFigure 2: Audio duration distribution in the dataset.\\n\\nIn this work, we consider two approaches: a multi-modal approach combining text and audio and unimodal baselines. The multimodal models rely on state-of-the-art (SOTA) audio and text models as encoders, in order to capture audio features that describe the state and tone of speakers, and LLMs to provide embeddings for the words uttered. In our architecture (see Figure 3), we first use Whisper [23] large to extract transcripts from the audio, which are fed into AraBERT [24], a BERT model trained on Arabic texts, which functions as a text encoder. We further extract audio embeddings from an audio encoder, which are fused with the embeddings from the text encoder. For audio encoders, we examine (1) multilingual models; (2) models fine-tuned for emotion detection, and (3) models fine-tuned on Arabic speech data. We experiment with two fusion strategies: concatenation and multi-headed attention [25]. The fused audio-text embeddings are then provided to a classifier, which performs the binary offensiveness classification task.\\n\\n4.1.1. Audio Encoders\\nWe experiment with different pretrained audio encoders to allow capturing temporal, phonetic, and linguistic features in addition to prosodic information. Such information can be crucial for identifying offensiveness in speech, as it can relay information about the state of the speaker. Indeed, [15] find that speech models afford capturing prosodic information, which aids classification performance. Specifically, we use Wav2vec [26] a self-supervised CNN trained for ASR; HuBERT [27] a self-supervised Transformer-CNN model, trained for cluster prediction using span-masking; and Whisper, a sequence-to-sequence Transformer trained on multilingual speech data. Beyond the base Wav2vec-large model, we also conduct experiments with Wav2vec-Emotion 3 which is fine-tuned on RA VEDESS [28] as it can capture information about the emotional states of speakers. In addition, we used Wav2vec-Arabic [29] which is finetuned on Common Voice 6.1 [30] and Arabic Speech Data [31], as it is likely to have better representations of Arabic than multilingual models.\\n\\n4.1.2. Text Encoder\\nFor text encoding, we use AraBERTv2 [24] to construct text embeddings from the transcripts produced by Whisper. AraBERT has been trained using a range of data sources that include different dialects of Arabic, in addition to Modern Standard Arabic (MSA). AraBERT is therefore uniquely well-suited for our purposes, as our data reflects a range of Arabic dialects.\\n\\n4.1.3. Classifiers\\nThe final step in our approach is a binary classification task for offensive or not. We use two classical, simple classifiers: SVMs with polynomial and radial bias function kernels, respectively; and a random forest (RF) classifier. We choose these two algorithms as they have been shown to be useful across a range of tasks and are less resource intensive than complex deep learning algorithms like CNNs. Moreover, we use a single-layer neural network classifier with cross entropy loss applied to the outputs of the encoder models.\\n\\n4.2. Metrics\\nWe evaluate our models using the Matthews Correlation Coefficient (MCC) [9]. MCC measures the quality of binary classifications, with values in [-1,1], where -1 indicates complete disagreement between prediction and label, 0 indicates no better than random performance, and 1 indicates perfect performance. MCC is preferable to use over F1-measures, as the latter ignore true negatives and can therefore provide an unreliable measure of true performance. In contrast, MCC takes the entire confusion matrix into account to provide a balanced measure of performance, regardless of class imbalances.\\n\\n5. Experiments\\nIn our experiments, we examine the impact of different configurations of: (1) the audio encoder, (2) the fusion mechanism between text and audio embeddings, and (3) the binary classifier. We run every possible combination for baseline and multimodal models, and report the performance of the best performing configurations (see Table 2 for an overview) in Table 3.\\n\\n5.1. Baselines\\nWe select unimodal vanilla Wav2vec, HuBERT, and AraBERTv2, Wav2vec-Arabic models as encoders, and single layer neural networks as classifiers (see B1 to B5 in https://zenodo.org/records/6221127\\n\\n---\"}"}
{"id": "nafea24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Overview of models with their components.\\n\\n- **B1** is the random baseline.\\n- For our other baselines, **B2** to **B5**, we select Wav2vec and HuBERT as they have shown to be useful across speech tasks, and AraBERT as it has been pretrained on Arabic text.\\n\\n### 5.2. Multi-Modal Models\\n\\nWe experiment with combinations of audio encoders, fusion mechanisms, and classifiers for our experimental models (M1 to M7 in Table 2). For fusion mechanisms, we experiment with multi-headed attention and concatenation for the representations from each modality.\\n\\n### 5.3. Evaluation Setup\\n\\nWe split our dataset into 70% for training, 15% for validation, and 15% for test splits. We apply a data balancing approach, Synthetic Minority Over-sampling Technique (SMOTE), to upsample the minority class, as under-sampling techniques produced inferior results hence why they are not included in the paper. We perform hyper-parameter tuning for all classifiers using grid-searches over their respective hyper-parameters. We report parameter configurations for the best performing models in Section 6. We perform all experiments using a 24GB Quadro RTX 6000 GPU.\\n\\n### 6. Results & Discussion\\n\\nConsidering the MCC scores presented in Table 3, we find that our unimodal baseline method, at best, indicates a weak correlation (B5) between the label and predictions\u2014with the best performing model using Wav2vec-Arabic.\\n\\n### 5.2. Multi-Modal Models\\n\\nTurning to our experimental multimodal models, M1 through M8, we fuse the audio and text embeddings. Though the text embeddings are generated from the transcribed speech by Whisper, rather than gold standard transcription, we find that there is a noticeable improvement in model performance across all metrics. One exception here is M3, the worst of our experimental models in terms of MCC, which uses Whisper-large as a speech encoder, multi-headed attention to fuse embeddings, and SVM for classification. We believe that the lower performance may be caused by using Whisper as a basis for both audio and text embeddings, which may not capture new features introduced by another audio encoder. Our best performing model in terms of MCC, M8, uses Wav2vec-Arabic and a linear layer for classification. This result suggests that using language-specific audio encoders provides the most suitable encoding for offensive speech detection tasks.\\n\\n### 7. Conclusion\\n\\nIn this paper, we have introduced RAOFFENSE, a new dataset containing more than 2K samples annotated for detecting offensive speech in Arabic media, across different dialects. While our data is from scripted media, similarly to IEMOCAP and MELD, it allows us to share data that would otherwise be too sensitive to make publicly available. We explore 13 configurations for baseline models, and identify notable patterns for the task of detecting offensive speech. Firstly, we show that large performance gains can be achieved by relying on language-specific pretrained models, which outperform their multilingual counterparts. Secondly, we find that while using models from just one modality can provide reasonable results, using text and audio, even if the text is automatically transcribed by an imperfect ASR system can provide better results. Finally, we show that using attention to fuse embeddings tends to outperform concatenating embeddings from multiple modalities.\\n\\n### Table 3: Results for all model configurations on RAOFFENSE.\\n\\n| Model | MCC | Macro F1 | Weighted F1 |\\n|-------|-----|----------|-------------|\\n| B1    | 0.00| 0.50     | 0.65        |\\n| B2    | 0.10| 0.50     | 0.55        |\\n| B3    | 0.11| 0.55     | 0.64        |\\n| B4    | 0.14| 0.56     | 0.64        |\\n| B5    | 0.22| 0.61     | 0.71        |\\n| M1    | 0.31| 0.65     | 0.74        |\\n| M2    | 0.32| 0.66     | 0.74        |\\n| M3    | 0.27| 0.63     | 0.73        |\\n| M4    | 0.31| 0.64     | 0.71        |\\n| M5    | 0.34| 0.66     | 0.72        |\\n| M6    | 0.38| 0.69     | 0.77        |\\n| M7    | 0.40| 0.69     | 0.78        |\\n| M8    | 0.48| 0.74     | 0.81        |\\n\\nThe p-value for the pairwise comparison between the model predictions of B5 and M8 is 0.00023, indicating a statistically significant difference according to ANOVA's post-hoc analysis.\\n\\n---\\n\\nHyperparameters: lr = 0.0001, neurons = 512, epochs = 40.\"}"}
