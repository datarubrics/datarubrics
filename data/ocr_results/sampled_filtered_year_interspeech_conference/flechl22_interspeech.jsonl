{"id": "flechl22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] U.S. Dept. of Labor, \\\"The Health Insurance Portability and Accountability Act (HIPAA),\\\" http://purl.fdlp.gov/GPO/gpo10291, accessed: 2022-03-09.\\n\\n[2] S. Toshniwal, A. Kannan, C.-C. Chiu, Y. Wu, T. Sainath, and K. Livescu, \\\"A comparison of techniques for language model integration in encoder-decoder speech recognition,\\\" 2018, pp. 369\u2013375.\\n\\n[3] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \\\"Conformer: Convolution-augmented Transformer for Speech Recognition,\\\" arXiv:2005.08100, 2020.\\n\\n[4] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. J. Skerry-Ryan, R. A. Saurous, Y. Agiomyrgiannakis, and Y. Wu, \\\"Natural TTS synthesis by conditioning wavenet on mel spectrogram predictions,\\\" arXiv:1712.05884, 2017.\\n\\n[5] R. Prenger, R. Valle, and B. Catanzaro, \\\"WaveGlow: A flow-based generative network for speech synthesis,\\\" arXiv:1811.00002, 2018.\\n\\n[6] G. Yang, S. Yang, K. Liu, P. Fang, W. Chen, and L. Xie, \\\"Multi-band MelGAN: Faster waveform generation for high-quality text-to-speech,\\\" arXiv:2005.05106, 2020.\\n\\n[7] J. Kong, J. Kim, and J. Bae, \\\"HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,\\\" arXiv:2010.05646, 2020.\\n\\n[8] J. Kim, J. Kong, and J. Son, \\\"Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\\\" Proc. of the 38th International Conference of Machine Learning, PMLR 139, arXiv:2106.06103, 2021.\\n\\n[9] S. Mavandadi, T. N. Sainath, K. Hu, and Z. Wu, \\\"A Deliberation-Based Joint Acoustic and Text Decoder,\\\" in Proc. Interspeech 2021, 2021, pp. 2057\u20132061.\\n\\n[10] R. Zhao, J. Xue, J. Li, W. Wei, L. He, and Y. Gong, \\\"On Addressing Practical Challenges for RNN-Transducer,\\\" arXiv:2105.00858, 2021.\\n\\n[11] W. Chan, N. Jaitly, Q. V. Le, and O. Vinyals, \\\"Listen, attend and spell,\\\" arXiv:1508.01211, 2015.\\n\\n[12] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \\\"Attention Is All You Need,\\\" arXiv:1706.03762, 2017.\\n\\n[13] Q. Zhang, H. Lu, H. Sak, A. Tripathi, E. McDermott, S. Koo, and S. Kumar, \\\"Transformer Transducer: A Streamable Speech Recognition Model with Transformer Encoders and RNN-T Loss,\\\" arXiv:2002.02562, 2020.\\n\\n[14] M. Tang, D. Z. Hakkani-T\u00fcr, and A. G. T\u00fcr, \\\"Preserving privacy in spoken language databases,\\\" in Proc. of the International Workshop on Privacy and Security Issues in Data Mining, 2004.\\n\\n[15] \u00a8O. Uzuner, Y. Luo, and P. Szolovits, \\\"Evaluating the State-of-the-Art in Automatic De-identification,\\\" Journal of the American Medical Informatics Association, vol. 14, no. 5, pp. 550\u2013563, 2007.\\n\\n[16] S. M. Meystre, F. J. Friedlin, and B. R. South, \\\"Automatic de-identification of textual documents in the electronic health record: a review of recent research,\\\" BMC Med. Res. Methodol., vol. 10, p. 70, 2010.\\n\\n[17] F. Dernoncourt, J. Y. Lee, \u00a8O. Uzuner, and P. Szolovits, \\\"De-identification of patient notes with recurrent neural networks,\\\" Journal of the American Medical Informatics Association, vol. 24, p. 596606.\\n\\n[18] K. Khin, P. Burckhardt, and R. Padman, \\\"A deep learning architecture for de-identification of patient notes: Implementation and evaluation,\\\" in Proc. of the Workshop on Information Technologies and Systems, arXiv:1810.01570, 2018.\\n\\n[19] H. Zhu, J. Wang, G. Cheng, P. Zhang, and Y. Yan, \\\"Decoupled Federated Learning for ASR with Non-IID Data,\\\" in Proc. Interspeech 2022 (to appear), arXiv:2206.09102, 2022.\\n\\n[20] D. I. Adelani, A. Davody, T. Kleinbauer, and D. Klakow, \\\"Privacy Guarantees for De-Identifying Text Transformations,\\\" in Proc. Interspeech 2020, 2020, pp. 4666\u20134670.\\n\\n[21] Cerence Inc., \\\"Cerence TTS,\\\" https://www.cerence.com/cerence-products/cerence-drive/cerence-tts, accessed: 2022-03-09.\\n\\n[22] L. E. Shafey, H. Soltau, and I. Shafran, \\\"Joint Speech Recognition and Speaker Diarization via Sequence Transduction,\\\" in Proc. Interspeech 2019, 2019, pp. 396\u2013400.\\n\\n[23] T. Kudo and J. Richardson, \\\"SentencePiece: A simple and language independent subword tokenizer and detokenizer for Neural Text Processing,\\\" arXiv:1808.06226, 2018.\\n\\n[24] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \\\"SpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\\\" arXiv:1904.08779, 2019.\\n\\n[25] M. Abadi et al., \\\"TensorFlow: Large-scale machine learning on heterogeneous systems,\\\" software available from tensorflow.org.\"}"}
{"id": "flechl22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"End-to-end speech recognition modeling from de-identified data\\n\\nMartin Flechl, Shou-Chun Yin, Junho Park, Peter Skala\\nNuance Communications Inc., Burlington, USA\\nmartin.flechl@nuance.com\\n\\nAbstract\\nDe-identification of data used for automatic speech recognition modeling is a critical component in protecting privacy, especially in the medical domain. However, simply removing all personally identifiable information (PII) from end-to-end model training data leads to a significant performance degradation in particular for the recognition of names, dates, locations, and words from similar categories.\\n\\nWe propose and evaluate a two-step method for partially recovering this loss. First, PII is identified, and each occurrence is replaced with a random word sequence of the same category. Then, corresponding audio is produced via text-to-speech or by splicing together matching audio fragments extracted from the corpus. These artificial audio/label pairs, together with speaker turns from the original data without PII, are used to train models. We evaluate the performance of this method on in-house data of medical conversations and observe a recovery of almost the entire performance degradation in the general word error rate while still maintaining a strong diarization performance.\\n\\nOur main focus is the improvement of recall and precision in the recognition of PII-related words. Depending on the PII category, between 50%\u221290% of the performance degradation can be recovered using our proposed method.\\n\\nIndex Terms: speech recognition, ASR, end-to-end, de-identification, privacy, conformer, transducer, text-to-speech\\n\\n1. Introduction\\nTwo of the important ingredients to a performant automatic speech recognition (ASR) system are the quality and quantity of the training data. Special care is needed to avoid any mismatch between the data used for model training and for model application in the field, for example in terms of acoustic conditions, speaker variability and vocabulary usage. This is challenged by requirements to de-identify training data [1]: In its simplest form, this would imply to simply remove all PII from both the audio and label part of the training corpus. However, a model which does not see any person names, dates, or locations during training is likely not able to reliably recognize such entities in the field. While the impact on overall metrics such as word error rate (WER) might still be small such a system would nonetheless not be suited for most practical applications. For hybrid ASR models, the acoustic part of PII data (phonemes, with or without context) is usually covered by audio from a non-PII context and the problem can be efficiently dealt with by only manipulating text in order to make sure the PII tokens are adequately represented in the language model (LM). For end-to-end ASR modeling without LM fusion [2], however, in addition to text manipulation the arguably more demanding generation of audio is required in some form.\\n\\nWe address this problem by enriching the training data with words from PII categories (for example, names or dates) while minimizing the impact on the data in any other respect. Specifically, we first identify PII in the audio and text labels of the training data. Then, instead of just removing it, we replace each occurrence with a word sequence of the same category. For the text labels, this is straightforward. The challenge lies in providing audio corresponding to these inserted text labels. For this task, we compare different configurations employing text-to-speech (TTS) or matching fragments extracted from the corpus.\\n\\nUtterances which do not contain any PII to start with are combined with utterances de-identified and enriched in the described way to train ASR models. We use state-of-the-art end-to-end models based on the Conformer-Transducer architecture [3] to evaluate the performance degradation caused by removing all PII and how much of this loss can be recovered.\\n\\nIn the past few years, neural TTS models have managed to produce results comparable to human speech [4, 5, 6, 7, 8]. TTS has many applications related to ASR and is an obvious choice to generate audio counterparts for unpaired text for end-to-end ASR training [9]. Reusing audio snippets from a corpus (\u201csplicing\u201d) has previously been used in the context of domain adaptation as a cheap and scalable way to produce specific training data without additional data collection [10]. End-to-end ASR models have become the state-of-the-art in recent years in terms of reducing recognition error rates [11, 12, 13]. However, they also introduce a new set of challenges compared to hybrid ASR models. In this paper, we apply the established methods of TTS and splicing to reduce the negative impact of training data de-identification on end-to-end ASR models and evaluate the results of a few distinct strategies.\\n\\n2. De-identification\\nTo protect privacy and in some cases meet contractual or regulatory obligations, eliminating PII from collected data has become an important step across many fields. Methods for masking sensitive text elements and their impact on machine learning tasks have been now studied for almost twenty years for the general case [14] but also specifically in the healthcare domain [15]. Several methods have been suggested to de-identify health records [16] and patient notes [17, 18]. An alternative way of preserving privacy is via federated learning where sensitive data is exclusively processed on the client side [19].\\n\\nIn this paper, we deal with ASR for medical conversations which is arguably one of the most sensitive applications in this respect. In the following, de-identification is understood as the process of removing PII from speech, i.e., from spoken text (audio) or transcribed text. Examples are patient names or birth dates. Speaker anonymization, i.e., eliminating biometric attributes, is not subject of this paper.\\n\\nThe de-identification process starts with manual annotation of the transcribed text. PII is tagged and given one of roughly 30 labels. The audio interval containing the PII is found via forced alignment of the transcription to the audio. Then the PII is removed from the transcription and the waveform of the corresponding audio interval is replaced by silence. The result of\"}"}
{"id": "flechl22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Strategies for performance recovery\\n\\nThe workflow for the whole process is illustrated in Figure 1 and explained in the following.\\n\\n3.1. Surrogates\\n\\nSurrogates are generated in a pseudorandom and consistent manner: each PII phrase from a given category is replaced by a surrogate from the same category. This means that e.g., \u201cMichael\u201d may be replaced by \u201cJohn\u201d, \u201cNew York\u201d by \u201cLos Angeles\u201d, \u201c6/3 2021\u201d by \u201c7/4 2019\u201d and \u201cApril 7, 2020\u201d by \u201cfirst of May, 2021\u201d. This completes the treatment of the text labels of the corpus; two different methods to deal with the audio part are described in the following.\\n\\n3.2. Text-to-speech\\n\\nThe first method uses TTS to generate audio corresponding to the surrogate. Two different strategies are compared, as illustrated in Figure 2.\\n\\n- **TTS TOKEN**: Only the tokens of the surrogate are replaced by TTS. The complete speaker turn is then stitched together from parts of the original audio and TTS parts.\\n- **TTS TURN**: Any turn containing surrogates is entirely replaced by TTS. No stitching takes place; each turn is either entirely from the original audio or generated via TTS.\\n\\nFor the experimental studies presented in this paper, Cerence TTS is used to synthesize clean speech audio with one of eleven English speaker voices (four male, seven female) where we choose one randomly for each turn. Before the TTS audio snippet is concatenated with the original audio, it is processed in the following ways to smoothen the transition: the volume is adjusted to the same level as the original conversation, and silence is trimmed at the beginning and at the end of the audio snippet.\\n\\n3.3. Splicing using audio from the corpus\\n\\nThe second method is based on a strategy developed originally to adapt an existing model to a new domain without collecting additional data. The surrogate tokens are searched for in the corpus. They are then extracted using the token-level timing information obtained from a forced alignment of transcription and audio. Two different strategies are evaluated:\\n\\n- **speaker-dependent** (**SD**: We only use corpus audio from the speaker of that turn; if such a spoken word is unavailable then the parts of the turn containing PII tokens are dropped.\\n- **speaker-preferred** (**SP**: Same as **SD**, but with fallback to using audio from any other speaker if the required word cannot be found in the corpus for the speaker of that turn.\\n\\nThe complete turn is then spliced together from parts of the original audio and the extracted audio snippet, see Figure 3. In the speaker-preferred case, the final turn potentially consists of audio snippets from different speakers. In both cases acoustic conditions may differ (e.g., different level of background noise...\"}"}
{"id": "flechl22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"or distance to the microphone). The speaker-dependent strategy is more consistent and a priori more compatible with the desired modeling of the diarization. However, it has been argued that end-to-end models are particularly robust with respect to discontinuities at the transition between words. We address this question empirically by comparing to the speaker-preferred strategy. The latter has the advantage of a higher efficiency: if the search for a specific word is restricted to one speaker it is more likely it cannot be found in the corpus and hence the turn (or at least part of the turn) cannot be used for training. If more than one matching token is found then a random choice is made.\\n\\n4. Experiments\\n\\nThe training and test data, model architecture and other aspects of the training setup used to evaluate the performance of the different strategies are described in the following.\\n\\n4.1. Data\\n\\nThe training data consist of a subset of about 1000 hours of an in-house dataset containing medical conversations conducted in English covering about twenty different medical specialties. The recording device is carried by the doctor and hence the conversation typically consists of a mixture of near-field speech from a doctor and far-field speech from patient, caretakers, other doctors, and nurses. The audio is characterized by varying background noise and reverberation conditions. The test data are not de-identified but otherwise have the same characteristics and cover about 73 hours. The recordings are manually transcribed and PII sequences are annotated afterwards. In total, the annotated PII makes up 1.4% of the tokens in our training data and 2.1% of the total audio time. About 81% of all PII tokens are uttered by the doctor. For this reason the replaced audio mainly consists of near-field speech which is easier to generate artificially compared to far-field speech. Of all PII tokens, 45% are labeled as dates and 30% as patient or doctor names. The rest are numbers (medical records, phone numbers, age, etc) and names of places, hospitals or other organizations. Person names and dates are essentially entirely removed while numbers are only suppressed to a variable degree as they also occur in non-PII contexts.\\n\\n4.2. Experimental setup\\n\\nWe train end-to-end ASR models which in addition jointly predict diarization and auto-punctuation and are based on the Conformer-Transducer architecture. The inputs are pairs of wave files and sequences of word-pieces, created using a sentence piece model with a vocabulary size of 1024. The encoder extracts 64-dimensional Mel Frequency Cepstral Coefficients features from 32 ms windows in 10 ms steps. During training SpecAugment is applied. Two convolutional layers are used to subsample the features by a total factor of four, resulting in 40 ms frames. After a dense layer and a dropout layer, eighteen conformer blocks constitute the workhorse of the encoder. The macaron-style conformer blocks combine a feed-forward network, multi-head attention, a convolutional layer, another feed-forward network and layer normalization. After the conformer blocks, a feed-forward network with 512 units produces the encoder output. The prediction network is fed by previous model outputs and consists of a multi-head self-attention layer and three feed-forward network layers with 512 output units. The joint network concatenates the encoder and prediction network outputs and consists of two feed-forward network layers and a dropout layer. A softmax layer produces the final model output. In total, the model features 113 million trainable parameters. All models are trained on six Tesla V100 GPUs with TensorFlow for 80 epochs which is sufficient for the training to converge and takes about four days for the 1000 h training data subset used. Recognition performance is evaluated using an average of the last fifteen model checkpoints. Where we report results on continuing model training based on initializing with the final weights of another model training, this implies an additional training for fifteen epochs. We report the usual word error rate (WER) but also the word diarization error rate (WDER) which measures the fraction of words not assigned to the correct speaker (\\\"doctor\\\" or \\\"other\\\") to assess if any of the methods confuses the diarization performance of the model. However, the main focus is on tokens which have a significantly reduced frequency in the training corpus after de-identification if no recovery methods are applied. These tokens are either chosen by category (numbers, dates, and names) oragnostically directly based on their frequency ratio in the training corpus before and after de-identification, looking at ratio bands below 10% and between 10% - 20%.\\n\\nThe following models are trained:\\n\\n- baseline ID (B1): identified training data, i.e., no de-identification has taken place.\\n- baseline TURN (B2): all speaker turns with PII have been removed.\\n- baseline TOKEN (B3): all PII tokens have been removed (and the rest of each turn is kept).\\n- TTS TOKEN (T1): PII tokens have been replaced by surrogates, and for the audio part, PII tokens are generated using TTS.\\n- TTS TURN (T2): PII tokens have been replaced by surrogates, and for the audio part the whole turn is generated using TTS.\\n- spliced SD (S1): PII tokens have been replaced by surrogates, and for the audio part matching snippets from the same speaker are used if available; otherwise the turn is skipped.\\n- spliced SP (S2): PII tokens have been replaced by surrogates, and for the audio part matching snippets from preferably the same speaker (with fallback to any speaker) are used if available; otherwise the turn is skipped.\\n- spliced SDc (S3): same as spliced SD, but initializing the model with baseline TOKEN weights instead of starting from scratch.\\n- spliced SPc (S4): same as spliced SP, but initializing the model with baseline TOKEN weights instead of starting from scratch.\\n\\n4.3. Results\\n\\nThe resulting word error rates and word diarization error rates are shown in Table 1. B1 serves as a best-case baseline, with all information present; B2 and B3 constitute the worst-case baselines. The first thing to note is that the WER gap between upper and lower baselines is relatively small. This is not surprising considering that only about 1.4% of our tokens are PII-tagged.\"}"}
{"id": "flechl22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and hence the impact on general performance numbers is expected to be of that order of magnitude. Nonetheless it is desirable to close this gap. As can be seen, the best models working with de-identified data and our mitigation methods indeed do not show a significant difference in WER with respect to the best-case baseline.\\n\\nThe speaker-preferred splicing method gives significantly better results than the speaker-dependent one: the reason is the considerably larger percentage of successfully spliced turns of 66% for spliced SP compared to 17% for spliced SD. It is also worth noting that if the weights of the splicing models are initialized from a de-identified baseline model, results significantly improve. For TTS, the TOKEN method which maximizes the amount of original audio performs comparably to the TURN method which minimizes discontinuities in the audio. The WER and WDER are similar to the de-identification baseline.\\n\\nConcerning diarization, as expected de-identification does not directly impact the performance and WDER numbers do not significantly differ between the baselines. More importantly, the diarization performance does not suffer from either TTS or splicing methods. This provides evidence that our models are robust with respect to the artificial transitions which these methods add to the audio part of our training data. Note in particular that the diarization performance does not degrade for the speaker-preferred with respect to the speaker-dependent splicing method: even though most SP-spliced speaker turns contain parts uttered by two or more speakers, the model does not get confused. In our data, about two thirds of all uttered words are from the doctor. More precisely, the naive baseline of attributing all turns to the dominant speaker has a WDER of 32.3%.\\n\\nTable 1:\\n\\n| id  | model      | WER  | WDER |\\n|-----|------------|------|------|\\n| B1  | baseline ID | 11.8 | 3.7  |\\n| B2  | baseline TURN | 12.3 | 3.8  |\\n| B3  | baseline TOKEN | 12.1 | 3.7  |\\n| T1  | TTS TOKEN | 12.2 | 3.6  |\\n| T2  | TTS TURN | 12.2 | 3.8  |\\n| S1  | spliced SD | 12.4 | 3.9  |\\n| S2  | spliced SP | 12.0 | 3.8  |\\n| S3  | spliced SDc | 11.9 | 4.4  |\\n| S4  | spliced SPc | 11.8 | 4.0  |\\n\\nThe main goal of this paper is to improve the recognition of PII-like words. Table 2 shows corresponding results for the F1 scores of any numbers (e.g., two, ten, hundred), date-related words (months or days of the week) and common English first names where in all cases we exclude words that frequently occur with a different meaning (e.g., \u201cone\u201d from numbers, \u201cMay\u201d from months and names). We look at first names because they have a significant overlap between training and test data. Surnames and uncommon first names of the test set mostly do not occur in the training data even before de-identification: while this is a general challenge for ASR it is not a problem related to de-identification and hence not addressed here. In addition to these categories, we also aggregate words based on how strongly they are suppressed by de-identification.\\n\\nBoth de-identified baseline models (B2, B3) show a strong degradation, most noticeably for names where F1 scores drop from 77% to below 23%. The TTS and splicing models recover a significant portion of the F1 score drop. Typically, they have similar precision to the baseline ID model but slightly worse recall. The splicing models perform poorly with respect to names which is explained by the fact that the splicing efficiency is much lower for names compared to numbers, days of the week or months. This is not the case for TTS which performs best for names and in general tokens which are strongly suppressed in the training corpus due to de-identification. The recognition of numbers is decent even when training only with de-identified data since numbers occur also outside of PII contexts and are thus not entirely suppressed in the training data. However, note that even here the ($1 - F1$) error rate rises from 3% (B1) to 5% (B2, B3) after removing PII from the training data and our methods recover about half of that loss.\\n\\nTable 2:\\n\\n| id  | numbers | dates       | names       | r10 | r20 |\\n|-----|---------|-------------|-------------|-----|-----|\\n| B1  | 96.9    | 96.4        | 77.0        | 85.5| 86.9|\\n| B2  | 94.7    | 51.7        | 22.7        | 43.6| 74.6|\\n| B3  | 95.1    | 59.9        | 21.4        | 48.2| 75.5|\\n| T1  | 95.0    | 83.0        | 40.8        | 64.7| 79.6|\\n| T2  | 95.2    | 91.3        | 58.7        | 74.6| 82.3|\\n| S1  | 95.6    | 90.8        | 27.3        | 66.1| 76.9|\\n| S2  | 96.0    | 93.9        | 42.3        | 71.6| 80.0|\\n| S3  | 95.6    | 90.7        | 27.4        | 66.1| 76.9|\\n| S4  | 96.2    | 92.6        | 37.6        | 70.0| 80.6|\\n\\n5. Conclusions\\n\\nDe-identification of data used for ASR modeling poses a challenge for those tokens which become strongly suppressed in the training material, like names or dates. We propose various related strategies which are based on replacing PII tokens with surrogate tokens followed by generating corresponding audio via TTS or splicing and thereby enhancing the recognition performance of PII-related tokens compared to baseline methods. We evaluate the effectiveness of these strategies using a state-of-the-art conformer-transducer model architecture, comparing results to a best-case \u201cidentified data\u201d baseline and models trained with de-identified data without any mitigation as worst-case baseline.\\n\\nResults show that the best strategies can almost entirely recover the loss in general performance (WER) while maintaining a strong diarization performance (WDER). Most importantly, between 50%\u221290% of the recall and precision drop in recognizing PII-related tokens can be recovered. While the splicing models perform better in terms of WER recovery, the TTS models show the highest F1 scores for PII-like tokens like names or other words strongly suppressed by de-identification.\\n\\nFuture efforts will be based on trying to combine the splicing and TTS strategies, i.e., by using splicing if a matching word can be found in the corpus and otherwise falling back to TTS. We will also look for potential improvements by generating speech which more closely resembles the style surrounding the de-identified part of the data, both in terms of speaker characteristics (like accent, speaking rate, voice) and environmental conditions (like background noise and reverberation).\"}"}
