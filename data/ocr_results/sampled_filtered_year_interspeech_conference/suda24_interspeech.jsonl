{"id": "suda24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Who Finds This Voice Attractive?\\nA Large-Scale Experiment Using In-the-Wild Data\\nHitoshi Suda1, Aya Watanabe2,\u2020, Shinnosuke Takamichi2,3\\n\\n1 National Institute of Advanced Industrial Science and Technology (AIST), Japan\\n2 The University of Tokyo, Japan\\n3 Keio University, Japan\\nsuda.h@aist.go.jp, aya.watanabe@alumni.u-tokyo.ac.jp, shinnosuke.takamichi@ipc.i.u-tokyo.ac.jp\\n\\nAbstract\\nThis paper introduces CocoNut-Humoresque, an open-source large-scale speech likability corpus that includes speech segments and their per-listener likability scores. Evaluating voice likability is essential to designing preferable voices for speech systems, such as dialogue or announcement systems. In this study, we let 885 listeners rate 1800 speech segments of a wide range of speakers regarding their likability. When constructing the corpus, we also collected the multiple speaker attributes: genders, ages, and favorite YouTube videos. Therefore, the corpus enables the large-scale statistical analysis of voice likability regarding both speaker and listener factors. This paper describes the construction methodology and preliminary data analysis to reveal the gender and age biases in voice likability. In addition, the relationship between the likability and two acoustic features, the fundamental frequencies and the x-vectors of given utterances, is also investigated.\\n\\nIndex Terms: Voice likability, voice design, speech dataset, crowdsourcing\\n\\n1. Introduction\\nWith the advancement of speech synthesis, synthesized voices are widely used in various situations, such as voice assistants and announcements at public facilities[1]. Improvements in voice design technology, such as voice cloning and controlling techniques, have led to the enhancement of text-to-speech (TTS) systems and voice conversion techniques capable of a wide range of voice qualities[2, 3, 4]. Such techniques have been expected to customize the synthesized voice according to their purposes. For instance, if each user could customize the voice quality of dialogue systems, improving user experiences (UX) of a user-specific dialogue system would be possible, enabling more comfortable systems[5, 6]. Furthermore, in advertising and promoting some products, using attractive voices for the target customer segment can enhance the effectiveness of advertisements[7]. Based on these backgrounds, statistical analysis of attractive voices is necessary to design voices that listeners find attractive. Considering this, what kind of voices are attractive? Which listeners feel the attractiveness of the voice?\\n\\nMultiple studies focusing on voice likability have been reported. In a study evaluating simple voices such as monophthongs, the impact of fundamental frequency ($F_0$) is investigated, showing that specific ranges of $F_0$ are attractive[8]. The effects of the speech rate have also been studied by evaluating single-word voices[9]. These studies have only assessed short-duration voices and have not evaluated the influence of speaking styles, including accent and prosody. In a study investigating the likability of utterances by 800 speakers, a relationship between multiple acoustic features and likability was shown; however, a detailed discussion on the differences in ratings among listeners was not conducted[10]. In another study that examined evaluation methods, a sufficient discussion on the differences in evaluation among listeners also has not been achieved[11].\\n\\nAs a whole, we focus on two main unresolved points. First, these studies have mentioned the relationship with only basic acoustic features, and the relationship with features such as x-vectors[12], which can be utilized in TTS and voice conversion, has yet to be investigated. Next, in these studies, the number of listeners is minimal, making it impossible to discuss the tendencies of likability among diverse listeners according to listener attributes such as gender and age. Therefore, by conducting evaluation with a large number of listeners on a sufficient number of utterances, it becomes possible to discuss the tendencies of voice likability according to the attributes of the listeners. In addition, the utterances need meaningful sentences to extract speaker representations and evaluate them to consider their accents and prosody.\\n\\nIn this study, we collected subjective likability ratings for the utterances of meaningful sentences and constructed the CocoNut-Humoresque corpus. By employing a large-scale speech description corpus for evaluation, we gathered information on likability that considers not only primary attributes like \\\"male voice\\\" or \\\"high-pitched voice\\\" but also factors such as prosody, accent, speaking style, and the speaker's age. Additionally, this corpus includes the listener attributes, enabling the analysis of voice likability for each listener and providing insights for voice design. This paper describes the methodology for constructing this corpus and conducts a preliminary analysis to reveal differences in likability according to both the speaker and listener. The corpus is publicly available from https://github.com/sarulab-speech/Coco-Nut.\"}"}
{"id": "suda24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Visualization of the contents of the 80 evaluation subsets in the corpus. The columns show the speech segments, and the rows show the subsets.\\n\\nAlgorithm 1\\n\\nCreate a subset $S$ from the whole segments.\\n\\nRequire:\\n- Subset size $n$\\n- Set of the whole segments $D$\\n- The x-vector extractor $v$.\\n\\n1: $S \\\\leftarrow \\\\emptyset$\\n2: $x \\\\sim U(D)$ \\\\hspace{1cm} $x$ is randomly selected from $D$.\\n3: while $|S| + 1 \\\\leq n$ do\\n4: $S \\\\leftarrow S \\\\cup \\\\{x\\\\}$\\n5: $x \\\\leftarrow \\\\text{argmax}_{x \\\\in D \\\\setminus S} \\\\|v(x) - v(y)\\\\|_2$\\n6: end while\\n7: return $S$\\n\\n2.2. Corpus design\\n\\nFirst, we selected 1800 speech segments from Coco-Nut. In our corpus, 1200 segments were selected from the train set, 300 were selected from the validation set, and 300 were selected from the test set.\\n\\nSecond, we constructed 80 evaluation subsets with 30 segments. The first 60 subsets (group A) contain 20 segments, five segments, and five segments from the train, validation, and test sets, respectively. The following ten subsets (group B) contain 30 segments selected from the validation set. The last ten subsets (group C) contain 30 segments selected from the test set. Hence, each segment from the validation or test set is evaluated by listeners for group A and group B or C. Figure 1 shows the visualization of the included segments in the subsets. Each listener evaluated one subset selected randomly. We equally allocate the listeners to 80 subsets to evaluate the subsets as well-balanced. With the help of this subset construction method, the validation and test sets can be evaluated in both closed-listener and open-listener conditions when several systems (e.g., voice quality suggestion systems) are constructed.\\n\\nTo keep the diversity of the speakers in each subset, we selected the segments to maximize the diversity of speaker embeddings[14]. Algorithm 1 shows a pseudo-code of the algorithm for the subset construction. A segment is randomly selected from the whole segments first, and then the most unlike segment from all segments in the subset is chosen repeatedly.\\n\\nAs speaker embeddings, we used WavLM-based x-vectors[12, 15] extracted with microsoft/wavlm-base-plus-sv.\\n\\nAs a result, based on the descriptions in Coco-Nut, 1151 segments are male utterances, and 570 are female. The gender of the remaining 79 utterances cannot be derived from the descriptions. Note that this gender information is based on annotated descriptions in Coco-Nut, and the actual speakers' gender may differ from the perceived gender. Hence, in constructing this corpus, rather than considering the balance of perceived genders, we selected the segments only based on the diversity of x-vectors.\\n\\nIn this annotation process, we asked each listener about his/her gender, age, and three favorite YouTube videos. As for age, the listeners answered from the 10s, 20s, 30s, 40s, 50s, and over 59. As for gender and age, the listeners are allowed to answer as N/A. We also asked the listeners about the URLs of their favorite YouTube videos to reveal the relationship between their preference for videos and voice qualities.\\n\\nThe listeners scored the likability of voice qualities on a 6-point scale: 1\u2014dislike entirely, 2\u2014dislike, 3\u2014slightly dislike, 4\u2014slightly like, 5\u2014like, or 6\u2014completely like. To avoid a score concentration on a neutral score, we did not adopt a 5-point or 7-point scale. We asked the listeners to ignore the linguistic content and evaluate only the voice quality of the utterances.\\n\\nWe collected the listeners via an online crowdsourcing service. The listeners can understand Japanese, the Coco-Nut corpus's primary language. Each listener earned about 0.8 US dollars for his/her participation. The listeners were not allowed to join this evaluation twice or more.\\n\\n2.3. Listener attributes\\n\\nThe number of listeners was 885. Since the number of subsets is 80, each subset was assigned at least 11 listeners. Both male and female listeners were assigned to each of the subsets. Table 1 shows the number of listeners by gender and age. About 1\\n\\nhttps://huggingface.co/microsoft/wavlm-base-plus-sv\\n\\nTable 1: The number of listeners in each age and gender. \u201cN/A\u201d means that the listener chose \u201cno answer.\u201d\\n\\n| Age  | M     | F     | N/A | Total |\\n|------|-------|-------|-----|-------|\\n| 10s  | 7     | 1     | 0   | 8     |\\n| 20s  | 58    | 45    | 1   | 104   |\\n| 30s  | 137   | 97    | 1   | 235   |\\n| 40s  | 189   | 131   | 0   | 320   |\\n| 50s  | 93    | 64    | 0   | 157   |\\n| 60\u2013  | 41    | 15    | 0   | 56    |\\n| N/A  | 1     | 2     | 2   | 5     |\\n| Total| 526   | 355   | 4   | 885   |\\n\\nTable 2: The top five categories of listeners' favorite YouTube videos. Since some listeners did not answer their favorite videos or answered in an invalid format, the total number is less than the desired number.\\n\\n| Category      | Count |\\n|---------------|-------|\\n| Entertainment | 525   |\\n| Music         | 376   |\\n| People & Blogs| 361   |\\n| Gaming        | 249   |\\n| Howto & Style | 213   |\\n| Total         | 2377  |\\n\\n1https://huggingface.co/microsoft/wavlm-base-plus-sv\"}"}
{"id": "suda24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Analysis 1: Gender and age biases\\n\\nThis section compares distributions of the mean opinion scores (MOSs) of likability among different genders or ages to investigate gender and age biases in voice likability. In this section, a MOS is calculated for each segment, and then the distributions of the MOSs are compared.\\n\\n3.1. Gender biases\\n\\nIn this section, gender biases in voice likability are investigated. Distributions of MOSs are compared to clarify the effects of listeners' and speakers' genders. Figure 2 shows the violin plots of the results. As described in Section 2.2, the genders of speakers were estimated from the descriptions of the voice characteristics; therefore, in this paper, the genders of speakers mean the genders that the listeners sensed from the utterances.\\n\\nFirst, the effects of the listener's gender are investigated. With this investigation, we can observe the difference in scoring tendencies between males and females. By comparing Figure 2 (c) and (f), female listeners are confirmed to give lower scores to the voices than males. This difference is significant since the \\\\( p \\\\)-value based on Welch's \\\\( t \\\\) test is less than \\\\( 7 \\\\times 10^{-5} \\\\).\\n\\nIn addition, the \\\\( p \\\\)-value based on Bartlett's test was less than \\\\( 1.9 \\\\times 10^{-5} \\\\), and therefore, the variance of scores by female listeners was also significantly more extensive than those by male listeners.\\n\\nNext, the effects of the combination of listener's and speaker's genders are investigated. By comparing Figure 2 (a) and (b), for male listeners, the female utterances are more attractive than the male ones. The \\\\( p \\\\) value based on Welch's \\\\( t \\\\) test is lower than \\\\( 1.0 \\\\times 10^{-5} \\\\); therefore, this difference was significant. On the other hand, by comparing Figure 2 (d) and (e), for female listeners, no significant difference was observed in likability between the male and female utterances.\\n\\nIn conclusion, compared to female listeners, male listeners felt more attracted to female voices than male voices. However, gender biases in speakers were not observed in female listeners. That is, the female listeners rated the female utterances with lower scores than male listeners; hence, the female listeners gave lower scores in total. In addition, the female listeners gave more distributed scores than the male listeners.\\n\\n3.2. Age biases\\n\\nThe effects of the listener's age are also investigated to reveal the differences in scoring tendencies among them. In this investigation, the listeners were split into three groups: under 30, 30\u201349, and 50 or more. In addition, only voices in validation and test sets were counted because these samples have more listeners than those in train sets. By virtue of the corpus design, each utterance has at least 22 listeners. A MOS was calculated for each utterance and age group. If any age group has no listeners in an utterance, the utterance is eliminated.\\n\\nFigure 3 shows the violin plots of MOSs by age groups. The younger listeners gave significantly higher scores to the voices since the \\\\( p \\\\)-value between the under-30 and the whole group was less than 0.01 based on Welch's \\\\( t \\\\) test. The variance in MOSs by younger listeners is also significantly more extensive than that of the whole group.\\n\\nIn addition, the effects of the combination of the listener's gender and age are investigated. Figure 4 shows the violin plots. Based on the results, the discussed age biases are observed in both genders. Additionally, it can be concluded that the younger male listeners gave high likability scores, and the older female listeners gave low scores since the \\\\( p \\\\)-value is less than \\\\( 1.0 \\\\times 10^{-5} \\\\) based on Welch's \\\\( t \\\\) test.\\n\\n4. Analysis 2: Sample-by-sample analysis\\n\\nThis section investigates the voice likability sample by sample to reveal other likability factors than simple speaker attributes.\\n\\n4.1. Likable voices only for males or females\\n\\nTo reveal which voices are likable for males but not for females or vice versa, the MOSs given by male and female listeners...\"}"}
{"id": "suda24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mean opinion score (male listeners)\\n\\nmean opinion score (female listeners)\\n\\n\\\\[ r = 0.65 \\\\]\\n\\n(a)\\n\\n(b)\\n\\n60\\n100\\n200\\n300\\n400\\n500\\n600\\n\\n\\\\[ F_0 \\\\ [\\\\text{Hz}] \\\\]\\n\\nFigure 5:\\n\\nRelationship between scores by male and female listeners. The samples in the upper left part are likable, especially for female listeners, and vice versa. The points (a) and (b) represent the samples with the most divided opinions between male and female listeners. The \\\\( F_0 \\\\) means were calculated using Crepe's full model \\\\[16\\\\].\\n\\nIn this investigation, utterances in the train set were not counted because of the few listeners. Figure 5 shows a chart of this relationship. While a strong correlation between the male and female scores is observed, several samples appealed differently to male and female listeners. In Figure 5, the utterances with the most notable differences between male and female scores are pointed out as points (a) and (b). The utterance (a), a young male's acted speech, numbered 7230 in Coconut Nut, is likable, notably for female listeners. On the other hand, the utterance (b), a young girl's relaxed speech, numbered 6979 in Coconut Nut, is likable, notably for male listeners. As for utterance (b), while several male listeners rated it 6\u2014completely like, several female listeners rated it 1\u2014dislike entirely. Hence, there exist voices that are greatly preferred differently by men and women.\\n\\nAs indicated in Figure 5, the pitch of the voices is one of the critical factors in the differences in preferences. This result can be connected with the gender biases discussed in Section 3.1. However, as the figure shows, some utterances with low mean \\\\( F_0 \\\\) are preferable, especially for men, and vice versa. Therefore, the gender or the mean \\\\( F_0 \\\\) of a given utterance is not the only preference factor. In addition, the trend that women feel more attracted to male utterances with low \\\\( F_0 \\\\) was not confirmed, while some papers point out it \\\\[17, 18\\\\]. Hence, the attractiveness of the male voices to females can be affected not only by \\\\( F_0 \\\\) but also by other acoustic factors.\\n\\n4.2. Relationship between likability and x-vectors\\n\\nIn this section, the relationship between x-vectors and likability scores is investigated. By this investigation, we can determine whether the voice likability can be estimated from the speaker embeddings. As x-vectors, WavLM-based x-vectors were extracted with \\\\texttt{microsoft/wavlm-base-plus-sv}. By applying \\\\( t \\\\)-SNE and reducing dimensions, the x-vectors are visualized. Figure 6 shows the results. In this figure, the x-vectors formed clusters based on the speaker genders; the male utterances form the left cluster, and the female ones form the right. As the figure indicates, a likability trend exists in an x-vector space for both the mean and standard deviation. Voices in some areas are likable without high standard deviation, that is, without separating opinions. On the other hand, voices in some areas have separate opinions among listeners, such as the right end of the female cluster. These results indicate that the average voice likability or the variability in voice likability can be estimated using acoustic features such as \\\\( F_0 \\\\) and x-vectors of given utterances. The voices with high likability or high variability do not concentrate on one area; nonetheless, there are a few subspaces for such utterances.\\n\\nIn addition, by comparing Figure 6a and Figure 6b, some voices are confirmed to have nearly unified opinions regardless of their average likability scores. Therefore, we can derive a voice with a specific stable score for some applications, such as a voice design system.\\n\\n5. Conclusions\\n\\nThis paper introduces the Coconut Nut-Humoresque corpus, a large-scale speech likability corpus, to investigate voice likability. The number of evaluated utterances is 1800, and the number of participating listeners is 885. The evaluated utterances are natural Japanese utterances, and therefore, the voice likability can be investigated in in-the-wild situations. By preliminary corpus analysis, gender biases were observed in both listeners and speakers, and age biases of listeners were also observed. In short, male listeners gave higher scores to the female utterances than the male ones, and young male listeners gave higher scores than old female listeners. In addition, by sample-by-sample analysis, the likability trends were observed according to \\\\( F_0 \\\\) and x-vectors of utterances. However, voice likability is confirmed to be affected not only by these simple factors but also by other complicated factors.\"}"}
{"id": "suda24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. Acknowledgements\\nThis work was supported by JSPS KAKENHI Grant Number 23K20017, 21H04900, 22H03639, and 23H03418, and JST FOREST JPMJFR226V. This paper is based on results obtained from a project, JPNP20006, commissioned by the New Energy and Industrial Technology Development Organization (NEDO).\\n\\n7. References\\n\\n[1] P. Wagner, J. Beskow, S. Betz, J. Edlund, J. Gustafson, G. Eje Henter, S. Le Maguer, Z. Malisz, \u00c9. Sz\u00e9kely, C. T\u00e5nnander, and J. V\u00f6sse, \u201cSpeech synthesis evaluation \u2014 state-of-the-art assessment and suggestion for a novel research program,\u201d in Proc. 10th ISCA Workshop on Speech Synthesis (SSW 10), Sep. 2019.\\n\\n[2] S. \u00d6. Ar\u0131k, G. Diamos, A. Gibiansky, J. Miller, K. Peng, W. Ping, J. Raiman, and Y. Zhou, \u201cDeep voice 2: multi-speaker neural text-to-speech,\u201d in Proc. 31st International Conference on Neural Information Processing Systems, Dec. 2017, pp. 2966\u20132974.\\n\\n[3] J.-C. Chou and H.-Y. Lee, \u201cOne-shot voice conversion by separating speaker and content representations with instance normalization,\u201d in Proc. Interspeech 2019, Sep. 2019.\\n\\n[4] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lgel, and M. A. Ponti, \u201cYourTTS: Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone,\u201d in Proc. 39th International Conference on Machine Learning, vol. 162, 2022, pp. 2709\u20132720.\\n\\n[5] Q. Yu, T. Nguyen, S. Prakkamakul, and N. Salehi, \u201cI almost fell in love with a machine\u201d: Speaking with computers affects self-disclosure,\u201d in Extended Abstracts of the 2019 CHI Conference on Human Factors in Computing Systems, no. Paper LBW0255, May 2019, pp. 1\u20136.\\n\\n[6] S. Tolmeijer, N. Zierau, A. Janson, J. S. Wahdatehagh, J. M. M. Leimeister, and A. Bernstein, \u201cFemale by default? \u2014 exploring the effect of voice assistant gender and pitch on trait and trust attribution,\u201d in Extended Abstracts of the 2021 CHI Conference on Human Factors in Computing Systems, no. Article 455, May 2021, pp. 1\u20137.\\n\\n[7] F. Burkhardt, R. Huber, and A. Batliner, \u201cApplication of speaker classification in human machine dialog systems,\u201d in Speaker classification I: Fundamentals, features, and methods, C. M\u00fcller, Ed., Berlin, Heidelberg, 2007, pp. 174\u2013179.\\n\\n[8] B. Borkowska and B. Pawlowski, \u201cFemale voice frequency in the context of dominance and attractiveness perception,\u201d Animal Behaviour, vol. 82, no. 1, pp. 55\u201359, Jul. 2011.\\n\\n[9] C. Ferdenzi, S. Patel, I. Mehu-Blantar, M. Khidasheli, D. Sander, and S. Delplanque, \u201cVoice attractiveness: Influence of stimulus duration and type,\u201d Behavior Research Methods, vol. 45, no. 2, pp. 405\u2013413, Jun. 2013.\\n\\n[10] F. Burkhardt, B. Schuller, B. Weiss, and F. Weninger, \u201cWould you buy a car from me?\u201d \u2014 on the likability of telephone voices,\u201d in Proc. Interspeech 2011, Aug. 2011.\\n\\n[11] L. F. Gallardo, \u201cA paired-comparison listening test for collecting voice likability scores,\u201d in Proc. Speech Communication; 12. ITG Symposium, Oct. 2016, pp. 1\u20135.\\n\\n[12] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-vectors: Robust DNN embeddings for speaker recognition,\u201d in Proc. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2018, pp. 5329\u20135333.\\n\\n[13] A. Watanabe, S. Takamichi, Y. Saito, W. Nakata, D. Xin, and H. Saruwatari, \u201cCoco-Nut: Corpus of Japanese utterance and voice characteristics description for prompt-based control,\u201d in Proc. 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), Sep. 2023.\\n\\n[14] K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari, \u201cDiversity-based core-set selection for text-to-speech with linguistic and acoustic features,\u201d in Proc. 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2024, pp. 12351\u201312355.\\n\\n[15] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, \u201cWavLM: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, Oct. 2022.\\n\\n[16] J. W. Kim, J. Salamon, P. Li, and J. P. Bello, \u201cCrepe: A convolutional representation for pitch estimation,\u201d in Proc. 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Apr. 2018, pp. 161\u2013165.\\n\\n[17] D. R. Feinberg, B. C. Jones, A. C. Little, D. M. Burt, and D. I. Perrett, \u201cManipulations of fundamental and formant frequencies influence the attractiveness of human male voices,\u201d Animal Behaviour, vol. 69, no. 3, pp. 561\u2013568, Mar. 2005.\\n\\n[18] D. Riding, D. Lonsdale, and B. Brown, \u201cThe effects of average fundamental frequency and variance of fundamental frequency on male vocal attractiveness to women,\u201d Journal of Nonverbal Behavior, vol. 30, no. 2, pp. 55\u201361, Jun. 2006.\"}"}
