{"id": "yang22m_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Minimizing Sequential Confusion Error in Speech Command Recognition\\n\\nZhanheng Yang, Hang Lv, Xiong Wang, Ao Zhang, Lei Xie\\n\\nAudio, Speech and Language Processing Group (ASLP@NPU), School of Computer Science, Northwestern Polytechnical University, Xi'an, China\\n{zhhyang,zhangao1998}@mail.nwpu.edu.cn, hanglv@nwpu-aslp.org, xwang@npu-aslp.org, lxie@nwpu.edu.cn\\n\\nAbstract\\nSpeech command recognition (SCR) has been commonly used on resource constrained devices to achieve hands-free user experience. However, in real applications, confusion among commands with similar pronunciations often happens due to the limited capacity of small models deployed on edge devices, which drastically affects the user experience. In this paper, inspired by the advances of discriminative training in speech recognition, we propose a novel minimize sequential confusion error (MSCE) training criterion particularly for SCR, aiming to alleviate the command confusion problem. Specifically, we aim to improve the ability of discriminating the target command from other commands on the basis of MCE discriminative criteria. We define the likelihood of different commands through connectionist temporal classification (CTC). During training, we propose several strategies to use prior knowledge creating a confusing sequence set for similar-sounding command instead of creating the whole non-target command set, which can better save the training resources and effectively reduce command confusion errors. Specifically, we design and compare three different strategies for confusing set construction. By using our proposed method, we can relatively reduce the False Reject Rate (FRR) by 33.7% at 0.01 False Alarm Rate (FAR) and confusion errors by 18.28% on our collected speech command set.\\n\\nIndex Terms: Speech Command Recognition, Minimize Sequential Confusion Error, Discriminative training\\n\\n1. Introduction\\nSpeech command recognition (SCR), which detects pre-defined commands from consecutive audio streams, is widely deployed on edge devices such as smart household appliances, control panels and various Internet of Things (IoT) gadgets. Similar to keyword spotting (KWS), an SCR system usually runs locally on the resource-limited edge device, and thus needs to be small-footprint and prompt. But differently and more challengingly, an SCR system often needs to detect various commands ranging from a dozen to over one hundred depending on the device and application. Thus, besides improving detection accuracy and suppressing false alarm, handling confusions on different commands (especially those with similar pronunciations) is also vital to the user experience.\\n\\nHMM-based [1, 2, 3, 4, 5] approaches have dominated SCR for quite a long time, which usually use Gaussian Mixture Model (GMM) or Deep Neural Network (DNN) for acoustic modeling, and a Viterbi decoder is employed to search on the WFST decoding graph encoded by the command set to output the most likely command. Recently, some researchers also proposed to use a so-called end-to-end (E2E) neural model for SCR, which discards the decoding process in the HMM approach and directly maps the acoustic features to sub-word or word/command units for detection. The E2E approaches, such as those adopt convolution neural network (CNN) based models [6], usually have low computational consumption and simple system building procedure [6, 7, 8]. However, with the growth of command vocabulary, there is more variability in the length of commands. It is troublesome in context modeling for commands with different length, as fixed context (e.g. reception field in CNN) is often adopted. Apparently, it is also not flexible for new command expansion for customization purpose.\\n\\nIn practical applications, SCR systems have to face the confusion errors between the target command and the other competitive commands due to (partially) similar pronunciation among commands, for example, \\\"turn on the air conditioner\\\" and \\\"turn off the air conditioner\\\" will result in totally different actions. However, the common training criteria, such as cross-entropy (CE) and connectionist temporal classification (CTC) [9, 10], usually focus on optimizing the posterior probability of target frame or sequence only, which do not optimize the discrimination among different commands.\\n\\nIn this paper, we particularly address the command confusion problem by adopting discriminative training in SCR. Specifically, we propose a Minimize Sequential Confusion Error (MSCE) criterion for SCR, which is inspired by the MCE criterion and aims to improve the accuracy of command recognition and reduce confusion errors at the same time. The highlights of our proposed method are two-fold:\\n\\n\u2022 We introduce the CTC loss as a measure function, which could better define the likelihood of each command and thus improve the discriminative ability of the target command from all the other commands.\\n\\n\u2022 We design and compare three strategies for confusing set constructing, namely pronunciation similarity strategy (PSS), random selection strategy (RSS) and hybrid strategy (HS), according to the prior knowledge for each command to suppress. These strategies can reduce the training resource consumption notably.\\n\\nOur experiments show that, by using our proposed MSCE training criteria, we can relatively reduce FRR by 33.7% at 0.01 FAR and confusion errors by 18.28% on our collected evaluation set for SCR.\\n\\n2. Related work\\nSequence discriminative training criterion has been frequently used in speech recognition, which not only optimizes the accuracy of the target sequence but also suppresses the competitive non-target sequences at the same time. One of the most frequently-used criteria is Maximum Mutual Information (MMI) [11, 12]. The MMI criterion aims to enlarge the discrimination between the target sequence and the overall sequence set. The MMI family includes LF-MMI [11], Minimum\"}"}
{"id": "yang22m_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Word Error Rate (MWER) [13, 14] and Minimum Phone Error (MPE) [15, 16]. The MWER and MPE criteria introduce WER and PER in the optimization objective respectively as weight coefficient of the target sequence, which aims to suppress the WER/PER directly during training. Another frequently-used criterion is MCE [17], which optimizes the classification error directly instead of the probability distribution.\\n\\nIn the case of SCR, which can be considered as a typical classification task, we aim to improve the classification accuracy and minimize the command confusion. In this paper, we particularly modify the MCE criterion for speech command recognition with the purpose of command confusion minimization.\\n\\n3. Model Architecture\\n\\nIn this section, we introduce the overall framework of our system, as shown in Fig. 1. Specifically, we deploy a context imbalanced time-delay neural network (TDNN), design the modeling units and simplify the decoding process.\\n\\n3.1. Acoustic Model\\n\\nIn general, our acoustic model is established by multiple dilation TDNN-layer blocks as shown in Fig. 1(a). Specifically, each block is stacked by a dilation TDNN layer, a batch normalization layer and a dropout layer. We further improve the structure in the following two aspects: 1) To balance the receptive field and the model size, we deploy multi-scale (e.g. dilation rate=1,2,4) TDNN layers; 2) To keep low latency while maintaining good performance, we tune a few blocks in the middle of our model to only consider the left context. In this way, we achieve a low latency and small footprint model which is easy to deploy on edge devices.\\n\\n3.2. Modeling Units and Decoding\\n\\nConsidering the scalability of our model (e.g. custom commands) and the compatibility of varied lengths of commands, we use context-dependence (CD) phones as the modeling units. The number of distinct states within the HMM for each CD-phone is predefined as 5. Notably, since the phoneme coverage of command recognition task is limited, we use a decision tree to bound the similar HMM states together, which is beneficial to restrict the model size on edge devices.\\n\\nWe compose various knowledge sources including target commands information (G), lexicon (L), context decision tree (C) and HMM topologies (H) together for decoding. The final HCLG decoding graph could be presented as\\n\\n$$HCLG = \\\\min\\\\left(\\\\det\\\\left(H \\\\circ C \\\\circ L \\\\circ G\\\\right)\\\\right),$$\\n\\nwhere $\\\\min$ and $\\\\det$ represent minimization and determinization operations of WFST, respectively. The input label of the decoding graph is the bounded HMM state id and the output label is the word id in our setup.\\n\\nDuring decoding, the aim of SCR is to figure out the most likely command in one-pass decoding. So our decoder performs Viterbi decoding through token passing algorithm on the search graph. Beam pruning is applied in practice. Compared to the existing token passing algorithm, some modifications are made in order to further reduce the consumption of memory and speedup:\\n\\n- For SCR, we can obtain the length of each command in advance. So we use a fixed length array to record the output labels produced by decoding, instead of keeping a backward pointer for each token. In this way, we can get the recognized command directly from the array when triggered and avoid the procedure of generating lattice arcs and backtracing the whole decoding path. It helps to speedup and save memory.\\n- Without generating lattice, we just maintain two token lists dynamically for the current and next time steps. It helps to reduce the consumption of memory and time without sacrificing the performance.\\n\\nFinally, once one of the decoding paths reaches the final state of the graph, the average cost on each state through this path is computed to compare with the threshold. If it is greater than the threshold, the system will trigger. The overall inference process, including decoding process, is shown in Fig 1(b). In the next section we will introduce our discriminative training approach to suppress command confusion based on this model architecture.\\n\\n4. Minimize Sequential Confusion Error\\n\\nIn this section, we introduce our proposed MSCE criterion inspired by the MCE criterion. We will first give a quick review on the MCE criterion. Then we introduce the CTC loss as our sequence-level measure approach, and further define our proposed MSCE training criterion.\\n\\n4.1. Minimum Classification Error\\n\\nGenerally speaking, the MCE criterion [17] is defined in terms of discriminative functions for each category is optimized. Compared with the maximum likelihood estimation (MLE) method, the MCE method minimizes the classification error directly rather than learning the true data probability distributions. So it is more appropriate for enlarging the discrimination among different categories. We introduce a three-step procedure to derive the MCE criterion as follows.\\n\\nFirst, the form of the discriminative function is prescribed. For an input utterance $x^T$ with $T$ frames belonging to target category $\\\\kappa$, we define the discriminative function for the target classification as $g_\\\\kappa(x^T, \\\\Lambda)$, in which $\\\\Lambda$ denotes the entire set of system parameters. And we also deploy the same discriminative function for each non-target category, $\\\\tau$, in non-target set, $S_\\\\tau$, as $g_\\\\tau(x^T, \\\\Lambda)$ ($\\\\tau \\\\neq \\\\kappa$).\\n\\nThe second step is to define misclassification measure which allows us to embed the decision process into the overall minimum classification error formulation. The misclassification measure aims to distinguish the target category from all the other non-target categories for each input utterance $x^T$. A common used misclassification measure can be described as\\n\\n$$d_\\\\kappa(x^T, \\\\Lambda) = -g_\\\\kappa(x^T, \\\\Lambda) + \\\\sum_{\\\\tau \\\\in S_\\\\tau} g_\\\\tau(x^T, \\\\Lambda), \\\\quad (2)$$\\n\\nor\\n\\n$$d_\\\\kappa(x^T, \\\\Lambda) = -g_\\\\kappa(x^T, \\\\Lambda) \\\\sum_{\\\\tau \\\\in S_\\\\tau} g_\\\\tau(x^T, \\\\Lambda). \\\\quad (3)$$\\n\\nWordErrorRate(MWER)[13,14]andMinimumPhoneError(MPE)[15,16].TheMWERandMPEcriteriaintroduceWERandPERintheoptimizationobjectiverespectivelyasweightcoefficientofthetargetsequence,whichaimtosuppresstheWER/PERdirectlyduringtraining.Anotherfrequently-usedcriterionisMCE[17],whichoptimizestheclassificationerredirectlyinsteadoftheprobabilitydistribution.\\n\\nInthe case of SCR, whichcan be considered as a typicalclassification task, we aim to improve the classification accuracy and minimize the command confusion. In this paper, we particularly modify the MCE criterion for speech command recognition with the purpose of command confusion minimization.\\n\\n3. Model Architecture\\n\\nIn this section, we introduce the overall framework of our system, as shown in Fig. 1. Specifically, we deploy a context imbalanced time-delay neural network (TDNN), design the modeling units and simplify the decoding process.\\n\\n3.1. Acoustic Model\\n\\nIn general, our acoustic model is established by multiple dilation TDNN-layer blocks as shown in Fig. 1(a). Specifically, each block is stacked by a dilation TDNN layer, a batch normalization layer and a dropout layer. We further improve the structure in the following two aspects: 1) To balance the receptive field and the model size, we deploy multi-scale (e.g. dilation rate=1,2,4) TDNN layers; 2) To keep low latency while maintaining good performance, we tune a few blocks in the middle of our model to only consider the left context. In this way, we achieve a low latency and small footprint model which is easy to deploy on edge devices.\\n\\n3.2. Modeling Units and Decoding\\n\\nConsidering the scalability of our model (e.g. custom commands) and the compatibility of varied lengths of commands, we use context-dependence (CD) phones as the modeling units. The number of distinct states within the HMM for each CD-phone is predefined as 5. Notably, since the phoneme coverage of command recognition task is limited, we use a decision tree to bound the similar HMM states together, which is beneficial to restrict the model size on edge devices and improve the scalability of the model.\\n\\nWe compose various knowledge sources including target commands information (G), lexicon (L), context decision tree (C) and HMM topologies (H) together for decoding. The final HCLG decoding graph could be presented as\\n\\n$$HCLG = \\\\min\\\\left(\\\\det\\\\left(H \\\\circ C \\\\circ L \\\\circ G\\\\right)\\\\right),$$\\n\\nwhere $\\\\min$ and $\\\\det$ represent minimization and determinization operations of WFST, respectively. The input label of the decoding graph is the bounded HMM state id and the output label is the word id in our setup.\\n\\nDuring decoding, the aim of SCR is to figure out the most likely command in one-pass decoding. So our decoder performs Viterbi decoding through token passing algorithm on the search graph. Beam pruning is applied in practice. Compared to the existing token passing algorithm, some modifications are made in order to further reduce the consumption of memory and speedup:\\n\\n- For SCR, we can obtain the length of each command in advance. So we use a fixed length array to record the output labels produced by decoding, instead of keeping a backward pointer for each token. In this way, we can get the recognized command directly from the array when triggered and avoid the procedure of generating lattice arcs and backtracing the whole decoding path. It helps to speedup and save memory.\\n- Without generating lattice, we just maintain two token lists dynamically for the current and next time steps. It helps to reduce the consumption of memory and time without sacrificing the performance.\\n\\nFinally, once one of the decoding paths reaches the final state of the graph, the average cost on each state through this path is computed to compare with the threshold. If it is greater than the threshold, the system will trigger. The overall inference process, including decoding process, is shown in Fig 1(b). In the next section we will introduce our discriminative training approach to suppress command confusion based on this model architecture.\\n\\n4. Minimize Sequential Confusion Error\\n\\nIn this section, we introduce our proposed MSCE criterion inspired by the MCE criterion. We will first give a quick review on the MCE criterion. Then we introduce the CTC loss as our sequence-level measure approach, and further define our proposed MSCE training criterion.\\n\\n4.1. Minimum Classification Error\\n\\nGenerally speaking, the MCE criterion [17] is defined in terms of discriminative functions for each category is optimized. Compared with the maximum likelihood estimation (MLE) method, the MCE method minimizes the classification error directly rather than learning the true data probability distributions. So it is more appropriate for enlarging the discrimination among different categories. We introduce a three-step procedure to derive the MCE criterion as follows.\\n\\nFirst, the form of the discriminative function is prescribed. For an input utterance $x^T$ with $T$ frames belonging to target category $\\\\kappa$, we define the discriminative function for the target classification as $g_\\\\kappa(x^T, \\\\Lambda)$, in which $\\\\Lambda$ denotes the entire set of system parameters. And we also deploy the same discriminative function for each non-target category, $\\\\tau$, in non-target set, $S_\\\\tau$, as $g_\\\\tau(x^T, \\\\Lambda)$ ($\\\\tau \\\\neq \\\\kappa$).\\n\\nThe second step is to define misclassification measure which allows us to embed the decision process into the overall minimum classification error formulation. The misclassification measure aims to distinguish the target category from all the other non-target categories for each input utterance $x^T$. A common used misclassification measure can be described as\\n\\n$$d_\\\\kappa(x^T, \\\\Lambda) = -g_\\\\kappa(x^T, \\\\Lambda) + \\\\sum_{\\\\tau \\\\in S_\\\\tau} g_\\\\tau(x^T, \\\\Lambda), \\\\quad (2)$$\\n\\nor\\n\\n$$d_\\\\kappa(x^T, \\\\Lambda) = -g_\\\\kappa(x^T, \\\\Lambda) \\\\sum_{\\\\tau \\\\in S_\\\\tau} g_\\\\tau(x^T, \\\\Lambda). \\\\quad (3)$$\"}"}
{"id": "yang22m_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The larger the $d\\\\kappa$ value, the better the discriminative performance. Finally, we define a continuously differentiable loss function for the misclassification measure to optimize the parameters. It helps to eliminate the inconsistency between the scalar misclassification measure and the desired minimum error probability objective. We choose the most typical sigmoid function which is smoothed zero-one function suitable for gradient-based optimization algorithms:\\n\\n$$L(d\\\\kappa) = \\\\frac{1}{1 + e^{-\\\\xi(d\\\\kappa + \\\\alpha)}}, \\\\quad \\\\xi > 0,$$\\n\\nwhere $\\\\xi$ and $\\\\alpha$ are constants to control the steepness and translation of the function.\\n\\n4.2. MSCE for Speech Command Recognition\\n\\nInspired by the MCE discriminative criterion described above, we propose a Minimize Sequential Confusion Error (MSCE) criterion to deal with the command confusion problem in our SCR task. We define the measure function in advance, and then embed it into the misclassification measure. The model training process under MSCE criterion is shown in Fig 1(b).\\n\\nFirst, we regard the CTC loss function \\\\([9]\\\\) as the sequence-level measure function. The CTC loss could be described as\\n\\n$$L_{CTC}(l|x_T, \\\\Lambda) = \\\\sum_{\\\\pi \\\\in B} \\\\prod_{t=1}^{l_s} \\\\pi_t = l_s p(\\\\pi|x_t, \\\\Lambda) = \\\\left| l'_{s},\\\\right| \\\\sum_{s=1}^{\\\\alpha_t(s)} \\\\beta_t(s) y_{lt}.$$\\n\\nwhere $\\\\pi$ corresponds to all reasonable alignment paths of the target label sequence, $l_s$ denotes the element in $l$ and $t$ denotes the time step. So given the target label sequence, the CTC criterion maximizes the sum of likelihood of all the possible alignment paths of the target sequence by the forward-backward algorithm. Besides, the phone-level sequence of corresponding command is regarded as the CTC label sequence in our setup.\\n\\nAfter defining the sequence-level measure function for commands, we design the misclassification measure which aims to minimize the confusion errors. Equivalently, the measure aims to maximize the discrimination between the target command, $\\\\kappa$, and all confusing commands in the confusion set, $\\\\psi \\\\in S_\\\\psi$. So we define the misclassification measure for confusion errors in speech command recognition as\\n\\n$$d\\\\kappa(x_T, \\\\Lambda) = g_\\\\kappa \\\\sum_{\\\\psi \\\\in S_\\\\psi} g_\\\\psi,$$\\n\\nwhere the numerator part of the measure represents the sequence-level likelihood of the target command, while the denominator part describes the sequence-level likelihoods of the confusing commands. In this way, the better we minimize our sequence-level misclassification measure, the better the discrimination among commands.\\n\\nAfter embedding the sequence-level measure function, CTC, into the misclassification measure, we achieve the proposed MSCE criterion as:\\n\\n$$d\\\\kappa(x_T, \\\\Lambda) = L_{CTC}(\\\\kappa) \\\\sum_{\\\\psi \\\\in S_\\\\psi} L_{CTC}(\\\\psi).$$\\n\\nNotably, our proposed MSCE criterion is a continuously differentiable function which can be optimized by gradient-decent conveniently.\\n\\n4.3. Confusing Command Selection\\n\\nIn the training of MSCE criterion, it is not practical to include all non-target commands into the corresponding confusing set because CTC forward-backward algorithm needs to take up a large amount of GPU memory. Alternatively, we have to pick a finite number of non-target confusing commands to construct the corresponding confusing set. Here we propose three strategies.\\n\\n- **Pronunciation similarity strategy (PSS):** Since command confusion errors usually happen among commands which have similar pronunciation, we choose confusing commands according to the phone similarity. Specifically, we expand all commands to phone-level according to the lexicon at first. Then, for each target command, we compute its phone-level Levenshtein distance with each non-target command. At last, the top-$N$ most similar commands are selected to build the corresponding confusing set. Clearly, the confusing set of each target command is pre-defined before training.\\n\\n- **Random selection strategy (RSS):** Different from PSS, we randomly choose $N$ commands to construct the corresponding confusing set for each target command. Notably, the confusing set of each target command is generated dynamically during the training process.\\n\\n- **Hybrid strategy (HS):** This strategy combines both the characteristics of the two strategies described above. To be specific, we prepare two pools, one for top $N$ most similar commands with target command, the other for the whole commands set except for the target command. We randomly pick $i$ ($0 \\\\leq i \\\\leq N$) commands from one pool and $(N - i)$ commands from the other to construct the corresponding confusing set dynamically during the training process.\\n\\n4.4. Multi-Task Learning\\n\\nIn order to speed up the training process, we first train the network only with cross entropy (CE) criterion until it converges. Then we deploy our proposed MSCE criterion, using the CE criterion at the same time for regularization to reduce the overfitting tendency caused by sequence level training. The final loss function is described as\\n\\n$$L = \\\\beta L_{MSCE} + (1 - \\\\beta) L_{CE}.$$  \\n\\nwhere $L_{MSCE}$ denotes our proposed MSCE criterion loss function and $L_{CE}$ denotes the CE loss function used for regularization. Usually, we scale the MSCE loss function by a user-specified constant $\\\\beta = 0.8$ during training.\\n\\n5. Experiments\\n\\nIn this section, we introduce our corpus first, and then describe the experimental setup and results of our proposed methods. To the best of our knowledge, there does not exist any openly-sourced corpus which is designed for SCR task particularly with confusing commands under practical scenarios. Note that the Google speech commands corpus \\\\([18]\\\\) is an open dataset, but its commands are quite simple and there is no obvious pronunciation confusion among the commands. Hence in this study, we design a reasonable dataset for experimentation.\\n\\n5.1. Corpus Details\\n\\nThe corpus we used includes 41 Mandarin commands with different length (2-6 characters) for air conditioner control, including confusing commands with partial overlap in pronunciation.\"}"}
{"id": "yang22m_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The training set is recorded on smart phones in near-field quiet environment, containing approximately 95k speech utterances from 1744 different speakers, in which each command has 1800-2500 utterances. We use pyroomacoustics [19] to generate Room Impulse Response (RIR) for data augmentation, augmenting the original training set 10 times to 950k utterances by adding noise of various kinds. Besides, another 400k non-command utterances are added into the training set as negative samples, which come from diverse human voice (such as movies, lectures and news) and audio of environment noise.\\n\\nFor the development set, we randomly shuffle 50k utterances from the training set to construct it.\\n\\nThe evaluation set is recorded at different distances in several real rooms, with approximately 10k utterances from 55 speakers. Reading data are used as negative samples for computing False Alarm Rate (FAR), which are randomly shuffled from the AISHELL-2 corpus [20].\\n\\n5.2. Experimental Setups\\n\\nAll the experiments are conducted on the acoustic model with the same structure described in Section 2. The model includes 16 TDNN layer blocks, kernel size of each TDNN layer is 3, with 128 kernels. Dilation rate of each TDNN layer is [1, 2, 4, 4, 2, 1, 1, 2, 4, 4, 2, 1, 1, 2, 4, 4], respectively. The 40-dimensional mel-filterbanks are used as input features. The same search graph is also used for all the experiments, and we dynamically adjust the pruning thresholds to draw receiver operating characteristic (ROC) curves. ROC curves and confusion errors at specific FAR, which we defined as the number of misclassification samples whose decoding results are not negative, are reported in this paper. In order to ensure the reliability of the results and avoid random effects, all results are completed on the basis of model averaging.\\n\\n5.3. Comparison on Different Criteria\\n\\nWe first show the performance of different criteria, including CE, CTC, lattice-free MMI (LF-MMI) and our proposed MSCE criterion with HS strategy on ROC curves. The ROC curves are shown in Fig. 2(a). Compared with the baseline CE criterion, our proposed MSCE criterion is superior with 33.7% relative reduction at FRR at 0.01 FAR. Compared with the CTC criterion for ablation experiment, our proposed MSCE criterion is also superior. And we note that single CTC criterion does not always work well; it even hurts the model when the pruning threshold is relative tight at FAR under 0.01. We found that most of the errors are reject errors and it happen more for the evaluation utterances with low signal-to-noise ratio (SNR) in this case, because the peak posterior generated by CTC makes it prune the correct token prematurely. Compared with LF-MMI, a commonly used discriminative training criterion in speech recognition task and successfully used in keyword spotting as well [21], our proposed MSCE criterion is also superior with 12.8% relative reduction at FRR at 0.01 FAR.\\n\\n5.4. Discussion on MSCE Training\\n\\nWe also explore the strategies on confusing command selection for the denominator part of the MSCE criterion described in Section 3.3. For all different strategies, we set the hyperparameter $N = 4$. As shown in Fig. 2(b), HS is superior to RSS while RSS is superior to PSS. The result indicates that choosing commands from a whole command pool is essential as one of the sampling strategies, as it provides the proper class coverage, more in line with the definition of non-target command.\\n\\n5.5. Analysis on Confusion\\n\\nAnother important purpose of our proposed MSCE criterion is to alleviate the confusion error in SCR. So we particularly evaluate the relative reduction on confusion error compared to other training criteria on the evaluation set at several important operate points. The results are shown in Table 1. We can see that our proposed MSCE criterion achieves the best performance while reducing 18.28% confusion errors relatively. Meanwhile, CTC and LF-MMI can also get smaller gain of about 6% and 10% respectively. Again, HS performs best for confusion reduction among different selection strategies for choosing confusing commands. RSS and HS perform almost the same with a gain of about 18%, both of which are superior to PSS with gain of 14.19%. This means that a proper class coverage is of great impact. And HS is slightly superior to RSS with the gain of 18.28%, which proves that confusing commands with similar pronunciation can obtain a further gain on this basis.\\n\\n6. Conclusions\\n\\nIn this paper, we propose a novel MSCE training criterion for SCR, aiming to improve command recognition performance as well as alleviating confusion errors among different commands without additional model parameters and time consumption. The MSCE criterion adopts CTC as sequence-level probability function and creates a confusing sequence set for each command in order to suppress the likelihood of non-target sequences during training. We evaluate our method on a sizable dataset, showing 18.28% reduction in confusion errors and 33.7% reduction in FRR at 0.01 FAR over the CE training baseline. Our proposed MSCE criterion is also superior to LF-MMI \u2013 another popular discriminative training criterion.\"}"}
{"id": "yang22m_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. R. Rohlicek, W. Russell, S. Roukos, and H. Gish, \u201cContinuous hidden Markov modeling for speaker-independent word spotting,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1989, pp. 627\u2013630.\\n\\n[2] R. C. Rose and D. B. Paul, \u201cA hidden Markov model based keyword recognition system,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 1990, pp. 129\u2013132.\\n\\n[3] S. Panchapagesan, M. Sun, A. Khare, S. Matsoukas, A. Mandal, B. Hoffmeister, and S. Vitaladevuni, \u201cMulti-task learning and weighted cross-entropy for DNN-based keyword spotting,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2016, pp. 760\u2013764.\\n\\n[4] M. Sun, D. Snyder, Y. Gao, V. K. Nagaraja, M. Rodehorst, S. Panchapagesan, N. Strom, S. Matsoukas, and S. Vitaladevuni, \u201cCompressed time delay neural network for small-footprint keyword spotting,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2017, pp. 3607\u20133611.\\n\\n[5] M. Wu, S. Panchapagesan, M. Sun, J. Gu, R. Thomas, S. N. P. Vitaladevuni, B. Hoffmeister, and A. Mandal, \u201cMonophone-based background modeling for two-stage on-device wake word detection,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5494\u20135498.\\n\\n[6] T. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d 2015.\\n\\n[7] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2014, pp. 4087\u20134091.\\n\\n[8] B. Wei, M. Yang, T. Zhang, X. Tang, X. Huang, K. Kim, J. Lee, K. Cho, and S.-U. Park, \u201cEnd-to-end transformer-based open-vocabulary keyword spotting with location-guided local attention,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2021, pp. 361\u2013365.\\n\\n[9] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in ACM International Conference on Machine Learning (ICML), 2006, pp. 369\u2013376.\\n\\n[10] H. Yan, Q. He, and W. Xie, \u201cCRNN-CTC based Mandarin keyword spotting,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 7489\u20137493.\\n\\n[11] D. Povey, V. Peddinti, D. Galvez, P. Ghahremani, V. Manohar, X. Na, Y. Wang, and S. Khudanpur, \u201cPurely sequence-trained neural networks for ASR based on lattice-free MMI,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2016, pp. 2751\u20132755.\\n\\n[12] Z. Chen, Y. Qian, and K. Yu, \u201cSequence discriminative training for deep learning based acoustic keyword spotting,\u201d Elsevier Speech Communication, vol. 102, pp. 100\u2013111, 2018.\\n\\n[13] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, \u201cMinimum word error training of long short-term memory recurrent neural network language models for speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2016, pp. 5990\u20135994.\\n\\n[14] J. Guo, G. Tiwari, J. Droppo, M. Van Segbroeck, C.-W. Huang, A. Stolcke, and R. Maas, \u201cEfficient minimum word error rate training of RNN-Transducer for end-to-end speech recognition,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2020.\\n\\n[15] D. Povey and P. C. Woodland, \u201cMinimum phone error and I-smoothing for improved discriminative training,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), vol. 1, 2002, pp. I\u2013105.\\n\\n[16] S.-H. Liu, F.-H. Chu, S.-H. Lin, and B. Chen, \u201cInvestigating data selection for minimum phone error training of acoustic models,\u201d in IEEE International Conference on Multimedia and Expo (ICME), 2007, pp. 348\u2013351.\\n\\n[17] E. McDermott, T. J. Hazen, J. Le Roux, A. Nakamura, and S. Katagiri, \u201cDiscriminative training for large-vocabulary speech recognition using minimum classification error,\u201d IEEE Transactions on Audio, Speech, and Language Processing (TASLP), vol. 15, no. 1, pp. 203\u2013223, 2006.\\n\\n[18] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary speech recognition,\u201d arXiv preprint arXiv:1804.03209, 2018.\\n\\n[19] R. Scheibler, E. Bezzam, and I. Dokmani\u0107, \u201cPyroomacoustics: A python package for audio room simulation and array processing algorithms,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 351\u2013355.\\n\\n[20] J. Du, X. Na, X. Liu, and H. Bu, \u201cAishell-2: Transforming Mandarin ASR research into industrial scale,\u201d arXiv preprint arXiv:1808.10583, 2018.\\n\\n[21] Y. Wang, H. Lv, D. Povey, L. Xie, and S. Khudanpur, \u201cWake word detection with alignment-free lattice-free MMI,\u201d in ISCA Conference of the International Speech Communication Association (Interspeech), 2020.\"}"}
