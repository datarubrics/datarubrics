{"id": "aziz24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and Effective Baseline\\n\\nShiran Aziz, Yossi Adi, Shmuel Peleg\\nSchool of Computer Science and Engineering\\nThe Hebrew University of Jerusalem, Israel\\nshiran.aziz@mail.huji.ac.il\\n\\nAbstract\\nWith the popularity of cellular phones, events are often recorded by multiple devices from different locations and shared on social media. Several different recordings could be found for many events. Such recordings are usually noisy, where noise for each device is local and unrelated to others. This case of multiple microphones at unknown locations, capturing local, unrelated noise, was rarely treated in the literature. In this work we propose a simple and effective crowdsourced audio enhancement method to remove local noises at each input audio signal. Then, averaging all cleaned source signals gives an improved audio of the event. We demonstrate the effectiveness of our method using synthetic audio signals, together with real-world recordings. This simple approach can set a new baseline for crowdsourced audio enhancement for more sophisticated methods which we hope will be developed by the research community. Code, dataset, and models are available.\\n\\nIndex Terms: Audio enhancement, Time-frequency filtering, Crowdsourced denoising, User-Generated recordings\\n\\n1. Introduction\\nCellular phones are powerful multimedia devices, capable of quality recording of events around us. In particular, public events are often captured by multiple people from different locations. Many such user-generated recordings are also uploaded to social media, where several different recordings could be found for each event. In most cases user recordings have noisy audio signals, where noises are local to each device, and unrelated to each other due to the distance between users. Crowdsourced audio enhancement aims to use all available audio signals of an event, creating an audio signal that excludes the local noises at each input signal.\\n\\nUnlike more traditional single-channel and multi-channel denoising approaches [1, 2, 3, 4, 5, 6], in crowdsourced audio enhancement there is no prior definition of noise. Instead, noise is defined as a sound that is not common to most input audio. Hence, while local sounds will be removed, any global sounds that is present in all input signals will remain. For instance, consider several people shooting with their cell phones videos of a musical concert from different locations in the hall. The music coming from the main stage will be captured in all recordings, however the background noise will be unique to each of the recordings.\\n\\nThis work presents a straightforward method for crowdsourced audio enhancement. The method is based on filtering noisy space-time outliers from the input spectrograms considering both upper and lower thresholds. Specifically, we start by computing the Short-Time Fourier Transform (STFT) of all input signals. For each Time-Frequency (TF) cell we examine the magnitude values given to it by each input signal, and outlier values in each cell are removed. We define outliers as values which are substantially higher than the median magnitude value of the corresponding TF cell. The enhanced signal is constructed by averaging all amplitudes in each TF cell that are not considered as outliers. We evaluated the proposed method considering both synthetic and in-the-wild recordings. Results suggest that the proposed method significantly outperforms the baseline methods considering a diverse set of sources and background noises. The proposed method is simple and straightforward, requires no training, hence can serve as a foundational baseline for comparison with more sophisticated statistical techniques.\\n\\n2. Related Work\\nWhile much work has been done on audio enhancement using multi-channel microphone arrays [7], most papers are position-aware and address the case where the properties of the microphones and the relationship between them are known and constant [8, 9, 10].\\n\\nCombining user recordings should be position-agnostic, as we do not have any prior information on the relative position of the microphones. The authors in [11] were the first, to the best of our knowledge, to address crowdsourced audio enhancement from unrelated recordings. They proposed creating an improved audio signal, where the possible corruptions in each input signal can be missing frequencies or missing time periods.\\n\\nAnother relevant line of work is scene-agnostic multi-microphone speech processing. The authors in [12] proposed a deep learning based solution for speech dereverberation considering a varying number of microphone array at different positions. Some papers [13, 14] are focused on a setup where the target speaker is always closest to the microphone array. Unlike this approach we only have a single clean source, and we can not assume that one microphone has the cleanest recording of this source. Recently, [15] showed, in parallel to our work, a flexible multichannel speech enhancement, for a varying number of microphones at random positions inside a room. Though they show impressive results they focus on indoor speech recordings with relatively small distances between the microphones in the array. Unlike the crowdsourced speech enhancement task, these lines of work assume that all sources are captured by all microphones. Similarly, in Independent Component Analysis (ICA) [16] multi-channel speech separation is done by finding a linear representation of non-Gaussian data so that the components are as statistically independent as possible. Notice, ICA considers equal number of sources and microphones. Following such line of research the authors in [17] proposed the Full-rank spatial Covariance Analysis (FCA) method, while recently the authors in [18] proposed the fastFCA, which extends such re-\"}"}
{"id": "aziz24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"search direction and proposed a method for source separation for the undetermined case of more sources than microphones. It is assumed that microphones are far from each other such that all microphones at unknown locations. We consider different types of noises such as speech, environmental noises are different, and uncorrelated. Let $S$ be the source signal of the source signal and uncorrelated. Let $\u03b1$ be the amplitude of the frequency peak of signal $S$. The alignment process can be assumed to belong to the clean audio, and the amplitude at the corresponding peaks are estimated from the spectrogram: we select the signal with the maximum number of matched pair with signal $B$ as anchor signal, and normalize the others multiplicative constant between all corresponding pairs in the log-magnitude spectrogram we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude spectrograms $Y_{t, f}$, we perform the following: (i) given all input magnitude"}
{"id": "aziz24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Combining 5 synthetic noisy audio signals: Average SI-SNR of enhanced signal as a function of the SNR of the input signals, and 95% confidence interval on 100 experiments. Methods compared: (i) Mean: Using the mean of all signals. (ii) Median: Replacing the mean magnitude with the median magnitude in each TF cell. (iii) Max Elimination [19]: Removing the maximal magnitude in each TF cell. (iv) fastFCA: model the contribution of each source as a complex Gaussian distribution with zero mean. (v) Our Crowdsourced Enhancement, consistently having the best results.\\n\\nTable 1: signal: speech vs. Noise: speech / signal: Speech vs. Noises (DEMAND) / Music vs. Noise: Speech . Same as Fig. 1, but with PESQ and STOI as evaluation metric. The 95% confidence interval ranges between 0.05\u22120.18 and 0\u22120.01\u22120.02 respectively.\\n\\n| SNR | PESQ | STOI |\\n|-----|------|------|\\n| M   | MEAN | MEDIAN | FAST FCA | MAX E   |\\n| 10  | 1.07 | 1.05  | 1.26     | MEAN    |\\n| 9   | 1.16 | 1.06  | 1.34     | MEDIAN  |\\n| 8   | 1.10 | 1.18  | 1.20     | FAST    |\\n| 7   | 1.25 | 1.12  | 1.59     | FCA     |\\n| 6   | 1.61 | 1.22  | 2.18     | MAX E   |\\n| 5   | 0.54 | 0.60  | 0.39     | MEAN    |\\n| 4   | 0.69 | 0.64  | 0.59     | MEDIAN  |\\n| 3   | 0.60 | 0.73  | 0.45     | FAST    |\\n| 2   | 0.77 | 0.73  | 0.70     | FCA     |\\n| 1   | 0.86 | 0.78  | 0.82     | MAX E   |\\n| 0   | 1.10 | 1.09  | 1.21     | MEAN    |\\n| 9   | 1.29 | 1.11  | 1.62     | MEDIAN  |\\n| 8   | 1.23 | 1.33  | 1.31     | FAST    |\\n| 7   | 1.46 | 1.26  | 1.97     | FCA     |\\n| 6   | 1.98 | 1.44  | 2.69     | MAX E   |\\n| 5   | 0.64 | 0.69  | 0.52     | MEAN    |\\n| 4   | 0.78 | 0.73  | 0.70     | MEDIAN  |\\n| 3   | 0.71 | 0.80  | 0.57     | FAST    |\\n| 2   | 0.84 | 0.81  | 0.79     | FCA     |\\n| 1   | 0.91 | 0.85  | 0.90     | MAX E   |\\n| 0   | 1.23 | 1.24  | 1.51     | MEAN    |\\n| 9   | 1.64 | 1.33  | 2.26     | MEDIAN  |\\n| 8   | 1.58 | 1.56  | 1.59     | FAST    |\\n| 7   | 1.98 | 1.68  | 2.72     | FCA     |\\n| 6   | 2.68 | 2.00  | 3.34     | MAX E   |\\n| 5   | 0.77 | 0.81  | 0.69     | MEAN    |\\n| 4   | 0.87 | 0.84  | 0.82     | MEDIAN  |\\n| 3   | 0.84 | 0.86  | 0.71     | FAST    |\\n| 2   | 0.91 | 0.89  | 0.88     | FCA     |\\n| 1   | 0.95 | 0.92  | 0.92     | MAX E   |\\n| 0   | 1.35 | 1.41  | 1.80     | MEAN    |\\n| 9   | 1.91 | 1.53  | 2.64     | MEDIAN  |\\n| 8   | 1.78 | 1.64  | 1.78     | FAST    |\\n| 7   | 2.32 | 1.98  | 3.06     | FCA     |\\n| 6   | 3.02 | 2.35  | 3.50     | MAX E   |\\n| 5   | 0.82 | 0.86  | 0.76     | MEAN    |\\n| 4   | 0.90 | 0.88  | 0.86     | MEDIAN  |\\n| 3   | 0.87 | 0.86  | 0.75     | FAST    |\\n| 2   | 0.93 | 0.92  | 0.91     | FCA     |\\n| 1   | 0.96 | 0.94  | 0.94     | MAX E   |\\n| 0   | 1.54 | 2.32  | 2.15     | MEAN    |\\n| 9   | 2.22 | 1.79  | 2.98     | MEDIAN  |\\n| 8   | 2.00 | 1.70  | 1.96     | FAST    |\\n| 7   | 2.68 | 2.32  | 3.36     | FCA     |\\n| 6   | 3.33 | 2.71  | 3.79     | MAX E   |\\n| 5   | 0.86 | 0.89  | 0.83     | MEAN    |\\n| 4   | 0.93 | 0.91  | 0.90     | MEDIAN  |\\n| 3   | 0.90 | 0.85  | 0.78     | FAST    |\\n| 2   | 0.95 | 0.94  | 0.93     | FCA     |\\n| 1   | 0.97 | 0.95  | 0.96     | MAX E   |\\n\\nnoises, hammering, keyboard typing, dogs barking, etc. Speech data were obtained from the LibriSpeech corpus, while other types of noises were extracted from either DEMAND [23] or AudioSet [24].\\n\\nReal-world Recordings. We have collected real world user recordings of live music shows from YouTube. Multiple different clips were collected for each covered performance. As the clips were taken by independent users, we align and normalize all these recordings before processing. Overall, we collected \u223c300 video recordings from 4 different music shows.\\n\\n4.2. Baselines\\nWe evaluate the proposed method against four baselines. The first one, denoted as MEAN, is constructed by taking the average of all input audio signals. The second baseline, denoted as MEDIAN, is constructed by computing the STFT of all signals and of the average signal, and replacing the magnitude of the average signal in each TF cell with the median magnitude of all input signals in that TF cell [11]. Another baseline is the FAST FCA [18]. In the time-frequency domain, each source contribution is modeled as a zero-mean Gaussian random variable whose covariance represents the source's spatial properties. We used the implementation described in [18]. The last baseline is the Maximum Component Elimination [19], in which the magnitude of the average signal at each TF cell replaced by the average magnitude of all input signals in that TF cell after removing the maximal magnitude.\\n\\n4.3. Model Evaluation\\nTo assess the quality of the reconstructed audio in relation to the reference signal the Invariant Signal-to-Noise Ratio (SI-SNR) [25], PESQ [26, 27], and STOI [28] were used as an objective methods, while we use the MUlti Stimulus test with Hidden Reference and Anchor (MUSHRA) [29] test as a subjective one. We conducted a human listening test using a web platform [30], asking participants to rate the quality of recordings on a scale of 0 to 100 [31].\\n\\n4.4. Results\\nResults for the synthetic data can be seen on Figure 1 and Table 1 considering either music or speech as the source signal with various types of noises and SNR values. In all experiments we use k = 5 sources. Notice, as this is a synthetic dataset, we have the perfect alignment, hence we skip the alignment process in\"}"}
{"id": "aziz24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Three noisy inputs\\n\\nFive noisy inputs\\n\\nTen noisy inputs\\n\\nFigure 2: Average SI-SNR of enhanced signal, combining 3, 5, and 10 synthetic noisy audio signals. Source signal is music, and noise is speech. Max elimination [19], the best baseline, is compared with our Crowdsourced Enhancement. As expected, the benefit of our method over the baseline increases as more noisy signals are combined together.\\n\\nFigure 3: Same as Fig. 1, but with simulated packet loss, where each noisy input signals also has a randomly placed one second of silence. Max Elimination, the best baseline under additive noise, fails in this case.\\n\\nThis setting. We report the SI-SNR, STOI and PESQ metric between each of the methods against the clean target signal. Under each of the evaluated setups we extracted the enhanced signal using five synthesized noisy signals. Results suggest that the proposed method is significantly better than the evaluated baselines. This is more noticeable at low SNR values (e.g., -5, -10).\\n\\nInterestingly, when considering environmental noises from DEMAND, the gap between the proposed method and the evaluated baselines is smaller. In Figure 2 we compare our method to Max Elimination [19], considering different number of sources. Notice, the proposed method is superior to the Max Elimination method with an exception of three sources considering low SNR values. This implies that the proposed method can benefit from a large number of input sources.\\n\\nNext, we experiment with a packet loss setting, where we assume random parts of each input signals may be missing. We inject a low energy white Gaussian noise in the missing periods, to prevent numerical issues with fastFCA. To simulate that, we randomly erase one second from each input signal independently. Results are presented in Figure 3. Results suggest the proposed method is superior to the evaluated baselines under this setting as well. Interestingly, as we go to higher SNR values, the Max Elimination method converges towards the mean. This can be explained as the Max Elimination considers one element less than the mean method and for high SNR values it is often not a noisy element. Notice that the median method is not affected by the packet loss as it will ignore it anyway.\\n\\nWe perform subjective tests following the MUSHRA protocol [31], asking participants to rate the quality of recordings on a scale of 0 to 100. Obtained ratings: Max elimination [19], the closest prior art, got 48.4 \u00b1 2.9; our method got 67.4 \u00b1 2.6 (mean \u00b1 95% confidence interval). This suggests that the proposed method is superior to the evaluated baselines also considering subjective metrics. Code, datasets, models and audio examples are available at the following link: https://shiranaziz.github.io/crowdsourced_audio_enhancement/\\n\\n4.5. Recording from a Live Performance\\n\\nFinally, we evaluate the proposed approach on real recordings of live music shows collected from YouTube. As no ground truth is given when we enhance the crowdsourced recordings, the results can be examined on the website.\\n\\n5. Conclusions\\n\\nWe presented a simple and effective method for noise removal from crowdsourced recordings. The method examines individual time-frequency cells, and removes noisy input signals whose magnitude are outliers. The method can handle additive noise by removing outliers that are higher than the median signal, and can also handle silent moments (e.g., packet loss) by removing outliers lower than the median. We believe the development of simple and competitive baselines are crucial for constructing efficient solutions for real-world tasks. Although being simple, the proposed method improves over prior work, hence can be served as a new baseline for more complicated statistical methods which will be developed by the community in future work.\"}"}
{"id": "aziz24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Y. Ephraim and D. Malah, \u201cSpeech enhancement using a minimum-mean square error short-time spectral amplitude estimator,\u201d IEEE Transactions on Acoustics, Speech, and Signal Processing, vol. 32, no. 6, pp. 1109\u20131121, 1984.\\n\\n[2] A. A. Nugraha, A. Liutkus, and E. Vincent, \u201cMultichannel audio source separation with deep neural networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652\u20131664, 2016.\\n\\n[3] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov, \u201cA consolidated perspective on multimicrophone speech enhancement and source separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 25, no. 4, pp. 692\u2013730, 2017.\\n\\n[4] A. Defossez, G. Synnaeve, and Y. Adi, \u201cReal time speech enhancement in the waveform domain,\u201d arXiv preprint arXiv:2006.12847, 2020.\\n\\n[5] S. Araki, N. Ono, K. Kinoshita, and M. Delcroix, \u201cProjection back onto filtered observations for speech separation with distributed microphone array,\u201d in 2019 IEEE 8th International Workshop on Computational Advances in Multi-Sensor Adaptive Processing (CAMSAP). IEEE, 2019, pp. 291\u2013295.\\n\\n[6] S. E. Chazan, L. Wolf, E. Nachmani, and Y. Adi, \u201cSingle channel voice separation for unknown number of speakers under reverberant and noisy settings,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3730\u20133734.\\n\\n[7] J. Benesty, I. Cohen, and J. Chen, Fundamentals of signal enhancement and array signal processing. John Wiley & Sons, 2017.\\n\\n[8] E. Vincent, T. Virtanen, and S. Gannot, Audio source separation and speech enhancement. John Wiley & Sons, 2018.\\n\\n[9] P. Tzirakis, A. Kumar, and J. Donley, \u201cMulti-channel speech enhancement using graph neural networks,\u201d in ICASSP, 2021, pp. 3415\u20133419.\\n\\n[10] A. Pandey, B. Xu, A. Kumar, J. Donley, P. Calamia, and D. Wang, \u201cMultichannel speech enhancement without beamforming,\u201d in ICASSP, 2022, pp. 6502\u20136506.\\n\\n[11] M. Kim and P. Smaragdis, \u201cCollaborative audio enhancement using probabilistic latent component sharing,\u201d in ICASSP, 2013, pp. 896\u2013900.\\n\\n[12] Y. Yemini, E. Fetaya, H. Maron, and S. Gannot, \u201cScene-agnostic multi-microphone speech dereverberation,\u201d in Interspeech, 2021, pp. 1129\u20131133.\\n\\n[13] T. Yoshioka, X. Wang, and D. Wang, \u201cPicknet: Real-time channel selection for ad hoc microphone arrays,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 921\u2013925.\\n\\n[14] H. Taherian, S. E. Eskimez, T. Yoshioka, H. Wang, Z. Chen, and X. Huang, \u201cOne model to enhance them all: array geometry agnostic multi-channel personalized speech enhancement,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 271\u2013275.\\n\\n[15] A. Juki\u0107, J. Balam, and B. Ginsburg, \u201cFlexible multichannel speech enhancement for noise-robust frontend,\u201d in 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2023, pp. 1\u20135.\\n\\n[16] A. Hyv\u00e4rinen and E. Oja, \u201cIndependent component analysis: algorithms and applications,\u201d Neural Networks, vol. 13, no. 4-5, pp. 411\u2013430, 2000.\\n\\n[17] N. Q. Duong, E. Vincent, and R. Gribonval, \u201cUnder-determined reverberant audio source separation using a full-rank spatial covariance model,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 18, no. 7, pp. 1830\u20131840, 2010.\\n\\n[18] N. Ito, R. Ikeshita, H. Sawada, and T. Nakatani, \u201cA joint diagonalization based efficient approach to underdetermined blind audio source separation using the multichannel wiener filter,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1950\u20131965, 2021.\\n\\n[19] N. Stefanakis and A. Mouchtaris, \u201cMaximum component elimination in mixing of user generated audio recordings,\u201d in IEEE Int. Workshop on Multimedia Signal Processing (MMSP), 2017, pp. 1\u20136.\\n\\n[20] A. Wang, \u201cAn industrial strength audio search algorithm,\u201d in IS-MIR, 2003, pp. 7\u201313.\\n\\n[21] Z. Rafii, A. Liutkus, F.-R. St\u00f6ter, S. I. Mimilakis, and R. Bittner, \u201cThe MUSDB18 corpus for music separation,\u201d Dec. 2017. [Online]. Available: https://doi.org/10.5281/zenodo.1117372\\n\\n[22] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in ICASSP, 2015, pp. 5206\u20135210.\\n\\n[23] J. Thiemann, N. Ito, and E. Vincent, \u201cThe diverse environments multi-channel acoustic noise database (demand): A database of multichannel environmental noise recordings,\u201d in Int. Congress on Acoustics (ICA), 2013.\\n\\n[24] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in ICASSP, 2017, pp. 776\u2013780.\\n\\n[25] Y. Luo and N. Mesgarani, \u201cTasnet: time-domain audio separation network for real-time, single-channel speech separation,\u201d in ICASSP, 2018, pp. 696\u2013700.\\n\\n[26] A. W. Rix, J. G. Beerends, M. P. Hollier, and A. P. Hekstra, \u201cPerceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,\u201d in 2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221), vol. 2. IEEE, 2001, pp. 749\u2013752.\\n\\n[27] M. Wang, C. Boeddeker, R. G. Dantas, and ananda seelan, \u201cludlows/python-pesq: supporting for multiprocessing features,\u201d may 2022. [Online]. Available: https://doi.org/10.5281/zenodo.6549559\\n\\n[28] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, \u201cAn algorithm for intelligibility prediction of time\u2013frequency weighted noisy speech,\u201d IEEE Transactions on Audio, Speech, and Language Processing, vol. 19, no. 7, pp. 2125\u20132136, 2011.\\n\\n[29] R. B. ITU, \u201cMethod for the subjective assessment of intermediate quality level of coding systems,\u201d Recommendation ITU-R BS.1534, 2001.\\n\\n[30] M. Schoeffler, S. Bartoschek, F.-R. St\u00f6ter, M. Roess, S. Westphal, B. Edler, and J. Herre, \u201cwebmushra\u2014a comprehensive framework for web-based listening tests,\u201d Journal of Open Research Software, vol. 6, no. 1, 2018.\\n\\n[31] B. Series, \u201cMethod for the subjective assessment of intermediate quality level of audio systems,\u201d International Telecommunication Union Radiocommunication Assembly, 2014.\"}"}
