{"id": "lu22c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "lu22c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding\\n\\nYen-Ju Lu\u20201, Xuankai Chang\u20202, Chenda Li3, Wangyou Zhang3, Samuele Cornell4, Zhaoheng Ni5, Yoshiki Masuyama2,7, Brian Yan2, Robin Scheibler6, Zhong-Qiu Wang2, Yu Tsao1, Yanmin Qian3, Shinji Watanabe2\\n\\n1Academia Sinica, Taipei\\n2Carnegie Mellon University, USA\\n3Shanghai Jiao Tong University, Shanghai\\n4Universit\u00e0 Politecnica delle Marche, Italy\\n5Meta AI, USA\\n6LINE Corporation, Japan\\n7Tokyo Metropolitan University, Japan\\n\\nAbstract\\n\\nThis paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi-and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at https://github.com/ESPnet/ESPnet. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace.\\n\\nIndex Terms: speech enhancement, speech recognition, speech translation, spoken language understanding\\n\\n1. Introduction\\n\\nSpeech separation and enhancement (SSE) aims at extracting a target speech signal from noise, reverberation, and interfering speakers. It is essential to robust speech recognition [1, 2], assistive hearing [3], and robust speaker recognition [4]. SSE methods have benefited greatly from the recent development of deep learning (DL) approaches, and DL-based methods are now the de-facto standard. This opens up the possibility for end-to-end integration of SSE methods with many downstream speech processing back-end tasks [e.g., automatic speech recognition (ASR), keyword spotting, and speech translation (ST), to name a few]. In fact, many works have been recently exploring this research direction, mainly concerning ASR [5\u20138].\\n\\nTo accelerate research in SSE, ESPnet-SE toolkit [9] was developed and currently supports multiple state-of-the-art enhancement approaches and various corpora. In the meantime, there is currently a lot of effort and interest for robust ASR, ST, and spoken language understanding (SLU) in noisy and possibly distant speech scenarios [10, 11], such as ones encountered by smart-speaker devices. This motivated us to extend ESPnet-SE into ESPnet-SE++, which has a re-designed interface focused on modularity. This new interface allows for a seamless combination of different front-end SSE models with various downstream tasks such as ASR, SLU, ST, etc. As we showcase in Section 4 for ST and SLU, this easily allows to test and possibly fine-tune (as done for SLU) the same pre-trained front-end for multiple applications.\\n\\nThe contributions of ESPnet-SE++ are summarized below:\\n\\n\u2022 We significantly extend ESPnet-SE by providing new recipes for several enhancement corpora and challenges.\\n\u2022 In addition to new recipes, several state-of-the-art single-channel and multi-channel enhancement approaches have been added. Including unsupervised separation [12] and generative speech enhancement [13] approaches.\\n\u2022 A redesigned modular interface allows a flexible, \u201cplug-and-play\u201d combination of SE front-ends with different ESPnet back-end tasks such as ASR, SLU, ST, etc. As we showcase in Section 4 for ST and SLU, this easily allows to test and possibly fine-tune (as done for SLU) the same pre-trained front-end for multiple applications.\\n\u2022 We develop two multi-channel noisy-reverberant datasets derived from SLURP and Libri-Trans, respectively. We simulate potential applications of SLU and ST in a distant speech setting and showcase how tight integration of SE front-end with the back-end opens up promising research directions for other tasks besides ASR.\\n\\nAn extensive experimental evaluation is performed to showcase the flexibility of ESPnet-SE++. In particular, we conducted different experiments using four different datasets: CHiME-4, WSJ0-2mix, and the two purposely developed SLU and ST multi-channel datasets mentioned previously. Results show that speech enhancement can improve the performance in a wide variety of downstream tasks.\\n\\n2. Related Works\\n\\nIn this section, we briefly compare ESPnet-SE with other open-source deep learning-based speech enhancement and separation toolkits, such as nussl (North-western University Source Separation Library) [14], Onssen (An Open-source Speech Separation and Enhancement Library) [15], Asteroid (Audio source separation on Steroids) [16], and SpeechBrain [17]. While nussl provides several state-of-the-art SE methods, the data preparation and experiments of nussl and Onssen are not easily configurable from the command line [16]. On the other hand, Asteroid and SpeechBrain provide a whole pipeline from data preparation to enhancing and evaluating the testing speech.\"}"}
{"id": "lu22c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"recipes, while the other two operate in the complex time-frequency domain. In addition, there are also various SSE models, including (1) single-channel models: neural beamformer (MVDR, MWF), AuxIVA, *Neural Beamformer (MVDR, MWF)*), DCCRN [33], DC-CRN [34], SkiM [35], DPT-FSNet [36], iNeuBe [39, 40]. As mentioned above, ESPnet-SE++ adds several models on top of ESPnet-SE. For single-channel models, DCCRN [33] and DC-CRN [34] are added. To catch up with the latest development of deep learning-based multi-channel enhancement systems, we further added several multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), FaSNet [38], DC-CRN [34], and *DCCRN [33]*. Furthermore, ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n### 3.2. Models\\n\\n- ESPnet-SE covers a wide range of speech enhancement systems, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE provides various recipes for several SSE benchmarks: CHiME-4, REVERB, DIRHA, SMS-WSJ, LibriCSS, LibriMix, WHAM, and WHAMR. In addition, ESPnet-SE++ supports the latest state-of-the-art recipes and the latest enhancement/separation recipes, and ESPnet-SE++, is extended to support the latest state-of-the-art SE recipes but does not yet integrate multiple speech processing components with a front-end SSE module. This in-turn the data preparation steps for both enhancement and downstream tasks, including ASR, ST, and SLU. In addition, ESPnet-SE++ supports the native complex datatype built by PyTorch for complex time-frequency domain processing tasks into a single recipe. Although the application of SE can already be beneficial, especially in the multi-channel and multi-source, single-source and multi-source; (2) single-channel and multi-channel; (2) single-source and multi-source; (2) single-source and multi-source.\\n\\n- The training objective can vary a lot between different speech enhancement tasks, the joint-training of the front-end and back-end (e.g. ASR) techniques along with corresponding recipes but does not yet integrate multiple speech enhancement systems, we further added several multi-channel enhancement/separation models. In addition, various studies suggest that joint-optimization with downstream tasks can further boost the performance [10, 18, 19].\\n\\n### 3.3. Training Objectives\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n### 3.4. Single- and Multi-channel Speech Enhancement\\n\\nESPnet-SE++ owns 20 SSE recipes, with 24 different enhancement/separation models. In addition, there are also various SSE recipes in ESPnet-SE are not shown in the block of ESPnet-SE++.\\n\\n### 3.5. Features of ESPnet-SE++\\n\\n- In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into two different enhancement/separation models. In ESPnet-SE++, we add various classical and state-of-the-art art supervised SSE models, including (1) single-channel models: Deep Clustering [31], Deep Attractor Network [32], SVoice [37], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), DC-CRN [34], and *DCCRN [33]*.\\n\\n- ESPnet-SE++ supports multiple training objectives, including (3) generative enhancement model, CDiffuSE [13], and (2) multi-channel models: neural beamformer (MVDR, MWF), *Neural Beamformer (MVDR, MWF)*), MixIT [12].\\n\\n- The training objective can vary a lot between different speech enhancement systems, we further added several multi-channel enhancement/separation models. To make it more flexible, instead of relying on custom workarounds built on top of ESPnet-SE, we disentangle the training objective into"}
{"id": "lu22c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Results of single- and multi-channel speech enhancement approaches on CHiME-4.\\n\\n| Models                  | PESQ  | STOI  | SI-SNR (dB) |\\n|-------------------------|-------|-------|-------------|\\n| No processing           | 2.18  | 0.870 | 7.51        |\\n| Wang et al. [50]        | 3.68  | 0.986 | 22.00       |\\n| Single-channel Models   |       |       |             |\\n| Conv-TasNet (baseline)  | 2.58  | 0.892 | 11.57       |\\n| \u22c6 DCCRN                 | 2.59  | 0.895 | 12.57       |\\n| \u22c6 DC-CRN                | 2.43  | 0.880 | 11.59       |\\n| Multi-channel Models    |       |       |             |\\n| Neural Beamformer       | 2.61  | 0.954 | 13.98       |\\n| MVDR                    | 2.66  | 0.954 | 15.25       |\\n| \u22c6 wMPDR                 | 2.60  | 0.951 | 13.53       |\\n| \u22c6 wMPDR Souden          | 2.64  | 0.951 | 15.24       |\\n| WPD                     | 2.60  | 0.950 | 13.43       |\\n| WPD Souden              | 2.64  | 0.950 | 14.89       |\\n| \u22c6 SDW-MWF               | 2.43  | 0.922 | 11.87       |\\n| \u22c6 r1-MWF                | 2.67  | 0.953 | 15.08       |\\n| \u22c6 AuxIV A-ISS           | 2.49  | 0.900 | 10.34       |\\n| \u22c6 DC-CRN                | 2.95  | 0.948 | 17.04       |\\n| \u22c6 FaSNet                | 2.70  | 0.935 | 14.83       |\\n| \u22c6 iNeuBe (DNN)          | 3.24  | 0.969 | 19.52       |\\n\\n*The symbol \u22c6 denotes models newly-added to ESPnet-SE++.\\n\\nTable 2: Results of single-channel speech separation models on WSJ0-2mix.\\n\\n| Models                  | PESQ  | STOI  | SI-SNR (dB) |\\n|-------------------------|-------|-------|-------------|\\n| No processing           | 2.01  | 0.738 | 0.00        |\\n| Conv-TasNet (baseline)  | 3.25  | 0.953 | 15.94       |\\n| DPRNN-TasNet             | 3.47  | 0.968 | 17.91       |\\n| \u22c6 Deep Clustering        | 2.15  | 0.845 | 7.91        |\\n| \u22c6 Deep Attractor Network | 2.68  | 0.893 | 10.30       |\\n| \u22c6 DC-CRN                | 3.11  | 0.935 | 13.01       |\\n| \u22c6 SkiM                  | 3.47  | 0.966 | 18.45       |\\n| \u22c6 MixIT (Conv-TasNet)    | 3.00  | 0.938 | 13.50       |\\n\\n*The symbol \u22c6 denotes models newly-added to ESPnet-SE++.\\n\\nTable 3: WER for system combination of speech enhancement with speech recognition on CHiME-4 corpus.\\n\\n| Models                  | SIMU (%) | REAL (%) |\\n|-------------------------|----------|----------|\\n| No processing           | 19.7     | 18.0     |\\n| Single-channel Models   |          |          |\\n| Conv-TasNet             | 17.4     | 15.2     |\\n| DCCRN                   | 16.3     | 15.8     |\\n| Multi-channel Models    |          |          |\\n| FaSNet                  | 15.7     | 23.8     |\\n| Neural Beamformer       | 10.8     | 13.7     |\\n| iNeuBe (DNN)            | 9.0      | 35.8     |\\n\\nFor most speech enhancement models, S = 1. The decoder transforms the features into the target audios.\\n\\nAn interface holding together the encoder, separator and decoder has been designed. This modular design allows to explore many different architectural variations with less boilerplate code. Based on the unified framework, we also enrich the speech separation models in ESPnet-SE, including deep clustering [31], deep attractor network [32], DC-CRN [34], SkiM [35] and, MixIT [12]. The reproduced results on the WSJ0-2mix [31] benchmark are listed in Table 2.\\n\\n4. Combination tasks of ESPnet-SE++\\n\\nESPnet-SE++ allows for a tight and easy integration of SSE front-end processing techniques and back-end tasks as introduced in Sec. 3.1. To showcase this, we provide three combined recipes, CHiME-4, LT-S, and SLURP-S, with speech enhancement (SE) front-end subtask, followed by a back-end ASR/ST/SLU subtask. We performed several experiments with different techniques to assess if and how SE could improve the results of the back-end tasks even when multi-condition training is employed.\\n\\n4.1. Data Simulation\\n\\nWe created two multi-channel noisy-reverberant datasets based on SLURP and Libri-Trans (LT): SLURP-S and LT-S, where S stands for spatialized. We augmented the original SLURP and LT datasets using room impulse responses generated via Pyroomacoustics [53], and noises from FSD50k [54], SINS [55]. In detail, for each original utterance from SLURP and LT we simulate a smart-speaker scenario where the target signal is captured by a 4-microphone circular array with a diameter of 10 cm. For each utterance, we sample a room size from uniform distribution $U(10, 100) \\\\text{ m}^2$ and a reverberation time (T60) from $U(0.2, 0.6) \\\\text{ s}$, typical of most indoor settings [56].\"}"}
{"id": "lu22c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Room height is sampled from $U(2.5, 4)$ m. The target speech is contaminated by 1 up to 4 point-source localized noises from FSD50k and, in addition, also one diffuse noise audio sample from SINS. Diffuse noise is simulated using the technique outlined in [57]. The positions of the array, point-source noises and target are sampled randomly in the virtual room with the constraint of being at least 0.5 m apart from each other and the walls. The Signal-to-Noise ratio (SNR) for target versus point-source noises is sampled from $U(0, 15)$ dB, while for diffuse noise is sampled from $U(12, 35)$ dB.\\n\\nRegarding SLURP, since it already comprises noisy-reverberant examples, we use DNSMOS model [58] to select only a subset of sufficiently clean utterances (with estimated BAK $\\\\geq 3.2$) from training real, development and test, to use for SLURP-S. This process leaves 24497 utterances for training real, and 6387 and 6099 for development and test respectively. No utterance is discarded from training synthetic. The discarded utterances are kept but treated as single-channel examples with no oracle SE supervision, and are left for possible future work on semi/self-supervised joint SE+SLU. Instead, LT-S retains the same number of utterances of the original LT as it features only clean speech.\\n\\nFinally, in order to assess generalization to unseen noise conditions, we also generate, for both datasets, an additional test set with diffuse noise derived from QUT [59] instead of SINS.\\n\\n4.2. Combination of SE with ASR on CHiME-4\\n\\nIn Table 3 we report the results obtained by combining different SE front-ends with an ASR model on the CHiME-4 corpus [11]. CHiME-4 consists of both real and simulated noisy speech signals and is particularly suited for this analysis. We used a transformer encoder-decoder as the E2E ASR model that is pre-trained using the official ESPnet CHiME-4 recipe. The SE models are instead pre-trained using simulated data. The performance of these systems in terms of signal-based metrics is summarized in Sec. 3.4. After pre-training, the entire SE+ASR system is fine-tuned together with both SE and ASR losses. On the simulated data, all SE+ASR systems improve performance with respect to using solely the ASR model. This result confirms the effectiveness of jointly fine-tuning SE and ASR systems for robust speech recognition. Meanwhile, some models resulted in worse WERs on the real data. This is a commonly observed phenomenon when evaluating SE models on simulated and real data [19, 60], especially when they are not jointly optimized with the downstream ASR task. The neural beamformer achieved the best performance in both simulated and real sets.\\n\\n4.3. Combination of SE with ST on LT-S\\n\\nIn the left part of Table 4, we explore the combination of SE with a ST task performed on the simulated LT-S dataset outlined in Section 4.1. For the ST model, we used a conformer-encoder transformer-decoder as the back-end model. The ST model was trained on the original clean Libri-Trans dataset and achieved a BLEU score of 16.7 on the test set. The front-end SE models were pre-trained using both LT-S and SLURP-S datasets using the anechoic clean speech as target (thus performing joint de-noising and dereverberation). For the evaluation, we directly concatenate the pretrained SE models with the ST model in an end-to-end manner. Note that the whole system was not fine-tuned. We can see that the addition of the SE front-end generally improves the performance against the noisy speech even without fine-tuning. Among the SE models considered, the two iNeuBe models based on DNN $^2$ and multi-frame multi-channel Wiener filter (mfmcwf), bring the largest performance gain. In the single-channel case, DCCRN, as expected, outperforms Conv-Tasnet.\\n\\n4.4. Combination of SE with SLU on SLURP-S\\n\\nIn the right part of Table 4, we explore instead the combination of SE with a SLU back-end and report the intent classification accuracy. Regarding the SLU model, we used the conformer-encoder transformer-decoder trained with multi-condition training (clean and noisy-reverberant) on the SLURP-S corpus. After SLU back-end pre-training, we fine-tuned the SE and SLU systems together. The SE models are the same as used in the ST experiments in Sec. 4.3. Notably, we can see that in this instance, contrary to what is observed on CHiME-4 which is arguably less acoustically challenging than SLURP-S, Conv-TasNet and FaSNet lead to degraded performance. On the other hand, both DCCRN and iNeuBe are able to outperform the multi-condition SLU model. This confirms that models that rely on complex spectral mapping [40, 50] are more robust in challenging acoustical conditions.\\n\\n5. Conclusions\\n\\nIn this work we presented the ESPnet-SE++ toolkit. Built on top of ESPnet-SE, it includes new state-of-the-art models, losses, and recipes for speech enhancement corpora and challenges in various scenario. The toolkit interface has also been overhauled and improved with greater modularity. This allows a flexible combination of speech enhancement with ESPnet back-end tasks such as ASR, ST, and SLU. As an additional contribution, to showcase such integration of front-end and back-end components, we developed two multi-channel noisy-reverberant corpora based on SLURP and Libri-Trans. Experiments show that the use of SE front-end models improves both signal-based evaluation metrics and back-end tasks such as ASR, ST, and SLU, even when the back-end is trained on a multi-condition fashion. Future work could investigate further integration of different SE techniques with back-end tasks, including self-supervised pre-training via MixIT and generative enhancement models, with the goal of improving generalization to unseen acoustical conditions.\\n\\n6. References\\n\\n[1] Z. Wang et al., \u201cRank-1 constrained multichannel Wiener fil-\\nter for speech recognition in noisy environments,\u201d Computer\\nSpeech & Lang., vol. 49, pp. 37\u201351, 2018.\"}"}
