{"id": "choi24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSelf-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.\\n\\nIndex Terms: self-supervised learning, model analysis, lexical semantics, phonological distance\\n\\n1. Introduction\\n\\nSelf-supervised speech models (S3Ms) have become the de facto standard for solving various downstream speech tasks [1\u20135]. Analysis studies suggest that S3Ms encode various linguistic properties related to: phonetics [6\u201310], phonology [11], morphology [6, 12, 13], syntax [6, 12\u201315], and semantics [6, 12, 13, 16]. However, most existing literature either focuses only on phonetic analysis [7\u20139] or studies the existence of various linguistic properties [6, 12, 14]. We still do not understand if the S3M representations are better at encoding phonetic or semantic properties, or are equally good at both. To answer that, we design a fine-grained analysis framework to study the word-level representations. Specifically, we measure the similarity between word pairs, namely, near-homophones and synonyms. For example, consider the two word pairs: (dog, dig) and (dog, puppy). While \u201cpuppy\u201d is semantically similar to \u201cdog,\u201d \u201cdig\u201d is more phonetically similar to \u201cdog\u201d while bearing no semantic similarity.\\n\\nOur experimental results (Section 3) show that while semantically similar word pairs are closer than random word pairs, phonetically similar word pairs are significantly closer across all layers. This behavior is repeatedly observed in various S3Ms, differing in pre-training objective, model size, and the language of the pre-training data. Next, we seek to verify this implication on downstream task performance. Previous studies show that S3Ms perform exceptionally well in semantic tasks such as intent classification (IC) [4, 17, 18]. For instance, the frozen S3M model accuracy for the Fluent Speech Commands (FSC) dataset [19] is over 99% [4], making the task seem already solved. However, we question whether these high scores are indicative of word meaning encoded within S3M representations. To verify, we develop a simple baseline that uses only the word identity information. A simple decision tree classifier trained on word identity performs better than frozen S3M representations on two IC datasets. Thus, we conclude that S3M\u2019s impressive performance on these IC datasets does not indicate their semantic capabilities.\\n\\nTo summarize, in this work, we (i) contribute a carefully curated dataset of word pairs that distinguishes meaning and pronunciation (Section 2), (ii) study various S3Ms and find that word-level S3M representations are more phonetic than semantic (Section 3), and (iii) critically study widely used intent classification tasks and conclude that the S3M performance on these tasks is not necessarily indicative of semantic knowledge (Section 4).\\n\\n2. Experimental Setup\\n\\n2.1. Extracting synonyms and near homophones\\n\\nTo extract synonyms, we use WordNet [20], an English lexical database. For simplicity, we regard all the words in synsets (cognitive synonyms) as synonyms. In addition, to extract synonyms in different languages, we also use Open Multilingual Wordnet v1.4 (OMW) [21]. We use NLTK [22] to access both WordNet and OMW. For example, there are 16 synsets of the word \u201cstudy\u201d. Examples are synset learn.v.04 with words \\\\{learn, study, read, take\\\\}, or synset report.n.01 with \\\\{report, study, written report\\\\}.\\n\\nFor near homophones, we first phonemicize each word using the CMU pronouncing dictionary [23] for English and Epitran [24] for other languages. Then we use the Levenshtein distance [25, 26] to measure the phonetic distance between two phonemicized word pairs, divided by the longest of the two pairs to normalize the distance within the range 0 to 1. Further, to yield a sensible threshold for defining the near homophones, we measure the phonetic distance between random word pairs in the LibriSpeech dataset. We found that the top 0.1% of the random word pairs have phonetic distance equal to or smaller than 0.4. Hence, we define near homophones as having normalized Levenshtein distance $d \\\\leq 0.4$. Example word pairs can be found in Table 1.\\n\\n2.2. Datasets\\n\\nFor English analysis, we use both dev-clean and test-clean subsets of the LibriSpeech dataset [27], similar to [6, 12]. We also use existing LibriSpeech alignments [18] to extract timestamp information for each word. Alignments are generated by the\"}"}
{"id": "choi24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Example word pairs and their normalized Levenshtein distances.\\n\\n| Distance range | Example word pairs and their distances |\\n|----------------|---------------------------------------|\\n| 0.0 \u2264 d \u2264 0.2 | [oysters, stirs (0.20)] [dripping, drilling (0.17)] |\\n| 0.2 < d \u2264 0.4 | [mind, mound (0.25)] [bread, braids (0.4)] |\\n| 0.4 < d \u2264 0.6 | [socks, saxon (0.5)] [caffeine, patio (0.6)] |\\n| 0.6 < d \u2264 0.8 | [morally, sausage (0.67)] [spring, constrain (0.75)] |\\n| 0.8 < d \u2264 1.0 | [euclid, jack (0.83)] [cherry, shrank (1.0)] |\\n\\nMontreal Forced Aligner (MFA) [28]. To strengthen the statistical significance of our findings, we use statistical bootstrapping [29]. We randomly choose 10K word utterances and repeat the same experiment five times. Then, we report the averaged value across five experiments with its 95% confidence interval.\\n\\nFor crosslingual analysis, we use the Multilingual Spoken Words dataset (MSW) [30], which contains 1-second sliced words based on the Common Voice dataset [31]. However, as OMW and Epitran do not cover all languages, we choose the intersection between the supported languages: English, Chinese, Italian, Spanish, Indonesian, Polish, and Swedish. Similarly for OMW, we randomly choose 2K word utterances per language, a total of 14K utterances, and use bootstrapping with five repeats.\\n\\n2.3. Sampling word pairs\\n\\nIn addition to Near homophone and Synonym pairs described in Section 2.1, we sample three more types of word pairs for comparison. (i) Random pairs are generated by pairing the sampled list of words with itself after shuffling, used as a lower bound for similarity. (ii) Same word pairs are generated using all the unique words in the sampled set and pairing those with instances of the same word, used as an upper bound for similarity. (iii) Same speaker pairs are generated by pairing unique words of the same speaker.\\n\\nFor crosslingual analysis, we exclude the Same speaker condition, as OMW does not offer speaker information. We define Same word as above, but for Near homophone and Synonym pairs, we restrict the languages in each pair. One word is always English, and the paired other always comes from non-English languages. The restriction exists to avoid the English-English pair. Also, data for other languages is too small to generate a non-English-non-English pair.\\n\\n2.4. Extracting S3M representations\\n\\nWe use the layer-wise features of wav2vec2.0-Base, -Large, HuBERT-Base, and -Large. To test the impact of different datasets, we additionally test XLS-R-300M [32] and WavLM-Large [3], which have similar number of parameters as wav2vec2.0-Large and HuBERT-Large. XLS-R-300M is pre-trained on speech from multiple languages, while the others are pre-trained on English only. As all the models' pre-training losses contain cosine similarity, we use it as the similarity metric.\\n\\nTo extract word-level representations, we experiment with two types of slicing. Feature slicing inputs the entire utterance into the model and slices the relevant time span from the resulting representation. This method is commonly used in the analysis literature [6, 12, 13]. Audio slicing inputs only the specific word segment, in order to remove the contextual information surrounding the word [9, 10]. We also test multiple types of pooling to obtain a single vector representation for each word segment. Mean pooling across the temporal dimension is the most common approach [4, 6]. Center pooling chooses the temporally middle frame, which has been previously observed to retain word identity information [13]. Centroid pooling measures the cosine similarity between all the frames and returns one specific frame that is most similar to others.\\n\\n3. Findings\\n\\nIn this section, we analyze the S3M representations by comparing the cosine similarities of various word pairs. First, we study the effect of different slicing and pooling techniques (Sections 3.1 and 3.2). Next, we perform a comparative study of different S3Ms in our English (Section 3.3) and crosslingual (Section 3.4) word pairs. Finally, we study the effect of speaker variability in our datasets (Section 3.5). We provide all the code for reproducibility.\\n\\n3.1. Feature slicing squashes representations\\n\\nFigure 1: Cosine similarity between paired word representations of HuBERT-large on the LibriSpeech dev-clean and test-clean subsets. We compare audio slicing (top) vs. feature slicing (bottom). Normalized similarity curves (subtracting the Random baseline, right) are included to visualize the differences more clearly.\\n\\nWe first compare audio and feature slicing in Figure 1. Each figure shows the average cosine similarity of five types of word pairs, as described in Section 2.3. Comparing the unnormalized plots in Figure 1, we can immediately observe that feature slicing, unlike audio slicing, makes absolute values of similarities similar for all the word pairs, i.e., squashing the representations. We suspect it is due to the transformer architecture mixing the surrounding contextual information of the whole utterance so that the resulting representations become similar. Considering that previous work successfully extracted word information from the feature-sliced representations [13], we emphasize that the existence of information is not equivalent to how representations are distanced from each other. Even if the representations are squashed, the information is likely stored within the representation in a nontrivial manner, requiring additional learnable modules to extract the information.\\n\\n1https://github.com/juice500ml/phonetic_semantic_probing\"}"}
{"id": "choi24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Excluding the case where the confidence intervals overlap, the word similarities are ordered near-homophones (green circle) > synonyms (orange X) > random (blue dotted line). This finding implies that the distance between the HuBERT-large representations encodes more phonetics than semantics across all the layers. In order to exclusively focus on lexical semantics, we conduct the remaining analyses with audio slicing. The audio-sliced representations do not differentiate between word senses (multiple meanings of a single word) that require contextual information. Additionally, Figure 1 underlines the importance of having a baseline cosine similarity. Although the similarity values are close to 1.0, their high values are due to the anisotropic nature of transformer embeddings [33], i.e., representations crowded in a specific direction. From now on, we will only report scores normalized by subtracting random (denoted Norm.) as they present more interpretable layer-wise trends while preserving relative comparisons between different types of word pairs.\\n\\n3.2. Effects of different pooling methods\\n\\nWe compare normalized similarities of different pooling methods: mean pooling (Figure 1), center pooling, and centroid pooling (Figure 2). We observe that (i) centroid pooling and mean pooling retain more semantic content (synonyms) than center pooling, whereas (ii) center pooling preserves more word identity information (same-words) than both centroid pooling and mean pooling. These differences suggest that the temporal center (center pooling) is not necessarily at the center of the representation space (centroid pooling). Also, each framewise representation within a single word may model different aspects of speech. These implications urge future work on the intra-differences of word representations. We use the mean pooling for the remainder of our analysis.\\n\\n3.3. S3Ms encode more phonetic than semantic content\\n\\nIn Figure 3, we observe a clear dominance of near-homophones similarity over synonyms similarity in all the layers and all the models. This indicates that S3Ms encode phonetically similar word pairs more closely than semantically similar pairs. Additionally, the layer-wise trends are similar for synonyms, near-homophones, and same-words. This finding is corroborated by previous work studying word identity, phonetics, syntactic, and semantic content within S3M layers [13]. Unlike pre-trained text models where the linguistic properties follow a hierarchical structure [34], the same S3M layers are best at encoding both phonetic and semantic linguistic properties. Additionally, we see that the speaker information diminishes for the later layers of all S3Ms. Also, the layer-wise trends align with the previous findings [12,13,35] that HuBERT-Large and WavLM-Large maintain phonetic information until the last layer, whereas wav2vec2.0-Large encodes most information in the middle.\\n\\n3.4. Crosslingual word pairs follow similar trends\\n\\nWe conduct a similar analysis using the MSW dataset in Figure 4 and found a similar trend to the monolingual LibriSpeech dataset in Figure 3. Surprisingly, there is a non-negligible similarity between synonyms across different languages, both on cross-lingually trained XLS-R-300M and English-only WavLM-Large. Nevertheless, similarly to previous observations, synonyms pairs are consistently much farther than near-homophones pairs. As the paired words are always in different languages, the above results suggest that S3Ms could possess limited translation abilities.\"}"}
{"id": "choi24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To observe whether speaker variability affects the analysis, we consider the five speakers with the most utterances: speaker ID = (5142, 2412, 6313, 1580, 2277). We use all samples from each speaker and calculate the similarity score per speaker. In Figure 5, we report the mean and 95% confidence intervals for similarity scores across five speakers. Comparing Figures 1 and 5, we observe that the results are nearly identical, although the variance is higher for random and synonyms word pairs.\\n\\n4. Is Word Identity Sufficient for Intent Classification?\\n\\nAs shown in Section 3, S3M representations encode phonetics more than semantics in their distances. However, the downstream task performance on semantic tasks such as intent classification [19, 36, 37] is known to have extremely high accuracies for certain datasets [4, 17, 18]. Based on our findings, we question whether the word identity itself could suffice for these tasks. To test this hypothesis, we represent each utterance as a bag of words (BoW), thus stripping away any word meaning information. For example, the utterances \u201cblue sea is blue\u201d and \u201cred sea\u201d become \\\\([2, 1, 1, 0]\\\\) and \\\\([0, 1, 0, 1]\\\\). Then, we compare the performance of S3Ms with BoW to verify whether S3Ms exceed our word identity baseline.\\n\\n4.1. Settings\\n\\nThe intent classification (IC) task takes the speech utterance as input and classifies it according to the speaker\u2019s intent. For example, the Fluent Speech Commands (FSC) dataset contains 30K utterances with 31 unique intents, such as action:change language, object:newspaper, or location:bathroom. Similarly, the Snips Smartlights (SNIPS) [36,37] dataset has 1660 utterances with 6 intents, such as SwitchLightOn or SetLightColor.\\n\\nWe train a decision tree for BoW and a single fully-connected layer for S3Ms. We use the original train/val/test splits and compare the test accuracy. Additionally, we use a more challenging split, introduced by [17]. Speaker split (SPK) removes the overlap of speakers between the splits, and utterance split (UTT) minimizes the overlap of n-grams.\\n\\n4.2. Results\\n\\nIn Figure 6, we see that BoW almost always outperforms or is comparable to S3Ms, suggesting that high accuracy on these tasks [4] cannot conclusively indicate the semantic capabilities of S3Ms. Moreover, BoW achieves 100% accuracy on the original and SPK splits of FSC, implying that these evaluation settings are inadequate for measuring semantic capability.\\n\\n5. Related Work\\n\\nComparative studies on phonetic and semantic information were previously conducted on non-S3M speech models [39\u201341]. We primarily focus on analyzing and comparing the layer-wise representations of S3Ms. Recent literature on layer-wise analysis often focuses on measuring the downstream task performance using probing [10, 14], such as phoneme classification for phonology [10]. Canonical correlation analysis (CCA) is also often used [6,12,13] to measure correlation with linguistically meaningful information, such as word identity [12, 13]. However, both use learnable modules to verify whether the linguistic information is encoded rather than directly observing the representations via distances. Finally, frozen S3M representations are widely used after k-means clustering [7, 8, 38]. The cluster indices for each frame are now sometimes called semantic tokens [42]. As the distances between representations determine the clusters, we believe that analyses like ours can provide further insight when using discrete tokens derived from S3Ms.\\n\\n6. Conclusion\\n\\nWe analyzed various S3Ms using synonyms and near-homophone word pairs. We conclude that phonetics dominates semantics within all layers of S3Ms. Hence, we are further motivated to look in detail at existing semantic benchmarks for S3Ms. We introduce a bag of words baseline that fully preserves the word identity but removes the word meaning. We show that existing intent classification baselines can be improved by having an additional baseline.\"}"}
{"id": "choi24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgements\\n\\nThis work is based on the results obtained from the project, Programs for Bridging the gap between R&D and the IDeal society (society 5.0) and Generating Economic and social value (BRIDGE)/Practical Global Research in the AI \u00d7 Robotics Services, implemented by the Cabinet Office, Government of Japan. This work used the Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\\n\\n8. References\\n\\n[1] A. Baevski, Y. Zhou, A. Mohamed et al., \u201cwav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations,\u201d Proc. NeurIPS, 2020.\\n[2] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai et al., \u201cHuBERT: Self-Supervised Speech Representation Learning by Masked Prediction of Hidden Units,\u201d IEEE/ACM TASLP, 2021.\\n[3] S. Chen, C. Wang, Z. Chen et al., \u201cWavLM: Large-Scale Self-Supervised Pre-Training for Full Stack Speech Processing,\u201d IEEE JSTSP, 2022.\\n[4] S. wen Yang, P.-H. Chi, Y.-S. Chuang et al., \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Interspeech, 2021.\\n[5] J. Shi, D. Berrebbi, W. Chen et al., \u201cML-SUPERB: Multilingual Speech Universal PERformance Benchmark,\u201d in Proc. Interspeech, 2023.\\n[6] A. Pasad, J.-C. Chou, and K. Livescu, \u201cLayer-wise Analysis of a Self-supervised Speech Representation Model,\u201d in Proc. ASRU, 2021.\\n[7] D. Wells, H. Tang, and K. Richmond, \u201cPhonetic Analysis of Self-supervised Representations of English Speech,\u201d in Proc. Interspeech, 2022.\\n[8] B. M. Abdullah, M. M. Shaik, B. M \u00a8obius et al., \u201cAn Information-Theoretic Analysis of Self-supervised Discrete Representations of Speech,\u201d in Proc. Interspeech, 2023.\\n[9] K. Choi and E. J. Yeo, \u201cOpening the Black Box of wav2vec Feature Encoder,\u201d arXiv preprint arXiv:2210.15386, 2022.\\n[10] K. Choi, J.-w. Jung, and S. Watanabe, \u201cUnderstanding Probe Behaviors through Variational Bounds of Mutual Information,\u201d Proc. ICASSP, 2024.\\n[11] K. Martin, J. Gauthier, C. Breiss et al., \u201cProbing Self-supervised Speech Models for Phonetic and Phonemic Information: A Case Study in Aspiration,\u201d in Proc. Interspeech, 2023.\\n[12] A. Pasad, B. Shi, and K. Livescu, \u201cComparative Layer-Wise Analysis of Self-Supervised Speech Models,\u201d in Proc. ICASSP, 2023.\\n[13] A. Pasad, C.-M. Chien, S. Settle et al., \u201cWhat Do Self-Supervised Speech Models Know About Words?\u201d TACL, 2024.\\n[14] G. Shen, A. Alishahi, A. Bisazza et al., \u201cWave to Syntax: Probing spoken language models for syntax,\u201d in Proc. Interspeech, 2023.\\n[15] H. Mohebbi, G. Chrupala, W. H. Zuidema et al., \u201cHomophone Disambiguation Reveals Patterns of Context Mixing in Speech Transformers,\u201d in Proc. EMNLP, 2023.\\n[16] T. Ashihara, T. Moriya, K. Matsuura et al., \u201cSpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?\u201d in Proc. Interspeech, 2023.\\n[17] S. Arora, A. Ostapenko, V. Viswanathan et al., \u201cRethinking End-to-End Evaluation of Decomposable Tasks: A Case Study on Spoken Language Understanding,\u201d in Proc. Interspeech, 2021.\\n[18] L. Lugosch, M. Ravanelli, P. Ignoto et al., \u201cSpeech Model Pre-Training for End-to-End Spoken Language Understanding,\u201d in Proc. Interspeech, 2019.\\n[19] Y. Qian, X. Bianv, Y. Shi et al., \u201cSpeech-language Pre-training for End-to-end Spoken Language Understanding,\u201d in Proc. ICASSP, 2021.\\n[20] C. Fellbaum, \u201cWordnet,\u201d in Theory and Applications of Ontology: Computer Applications. Springer, 2010, pp. 231\u2013243.\\n[21] F. Bond and K. Paik, \u201cA Survey of WordNets and their Licenses,\u201d in Proc. Global WordNet Conference, 2012, pp. 64\u201371.\\n[22] S. Bird, \u201cNLTK: The Natural Language Toolkit,\u201d in Proc. ACL, 2006.\\n[23] R. Weide et al., \u201cThe CMU Pronouncing Dictionary,\u201d Release 0.7b, http://www.speech.cs.cmu.edu/cgi-bin/cmudict, 2014.\\n[24] D. R. Mortensen, S. Dalmia, and P. Littell, \u201cEpitran: Precision G2P for many languages,\u201d in Proc. LREC, 2018.\\n[25] V. I. Levenshtein et al., \u201cBinary Codes Capable of Correcting Deletions, Insertions and Reversals,\u201d in Soviet physics doklady, vol. 10, no. 8, 1966, pp. 707\u2013710.\\n[26] J. Nerbonne and W. Heeringa, \u201cMeasuring Dialect Distance Phonetically,\u201d in Computational Phonology: Third Meeting of the ACL Special Interest Group in Computational Phonology, 1997.\\n[27] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015.\\n[28] M. McAuliffe, M. Socolof, S. Mihuc et al., \u201cMontreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi,\u201d in Proc. Interspeech, 2017.\\n[29] B. Efron and R. J. Tibshirani, An Introduction to the Bootstrap. CRC Press, 1994.\\n[30] M. Mazumder, S. Chitlangia, C. Banbury et al., \u201cMultilingual Spoken Words Corpus,\u201d in NeurIPS Track on Datasets and Benchmarks, 2021.\\n[31] R. Ardila, M. Branson, K. Davis et al., \u201cCommon Voice: A Massively-Multilingual Speech Corpus,\u201d in Proc. LREC, 2020.\\n[32] A. Babu, C. Wang, A. Tjandra et al., \u201cXLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,\u201d in Proc. Interspeech, 2022.\\n[33] K. Ethayarajh, \u201cHow Contextual are Contextualized Word Representations? Comparing the Geometry of BERT, ELMo, and GPT-2 Embeddings,\u201d in Proc. EMNLP, 2019.\\n[34] J. Devlin, M.-W. Chang, K. Lee et al., \u201cBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding,\u201d in Proc. NAACL-HLT, 2019.\\n[35] R. Sanabria, H. Tang, and S. Goldwater, \u201cAnalyzing Acoustic Word Embeddings from Pre-trained Self-supervised Speech Models,\u201d in Proc. ICASSP, 2023.\\n[36] A. Saade, J. Dureau, D. Leroy et al., \u201cSpoken Language Understanding on the Edge,\u201d in Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing, NeurIPS, 2019.\\n[37] A. Coucke, A. Saade, A. Ball et al., \u201cSnips Voice Platform: an embedded Spoken Language Understanding system for private-by-design voice interfaces,\u201d in Privacy in Machine Learning and Artificial Intelligence Workshop, ICML, 2018.\\n[38] X. Chang, B. Yan, K. Choi et al., \u201cExploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study,\u201d Proc. ICASSP, 2024.\\n[39] G. Chen and Y. Cao, \u201cA Reality Check and a Practical Baseline for Semantic Speech Embedding,\u201d in Proc. ICASSP, 2023.\\n[40] Y.-C. Chen, S.-F. Huang, C.-H. Shen et al., \u201cPhonetic-and-Semantic Embedding of Spoken Words with Applications in Spoken Content Retrieval,\u201d in Proc. SLT, 2018.\\n[41] E. Dunbar, M. Bernard, N. Hamilakis et al., \u201cThe Zero Resource Speech Challenge 2021: Spoken Language Modelling,\u201d in Proc. Interspeech 2021, 2021.\\n[42] Z. Borsos, R. Marinier, D. Vincent et al., \u201cAudioLM: a Language Modeling Approach to Audio Generation,\u201d IEEE/ACM TASLP, 2023.\"}"}
