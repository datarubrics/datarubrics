{"id": "maciejewski24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] E. C. Cherry, \u201cSome experiments on the recognition of speech, with one and with two ears,\u201d The Journal of the Acoustical Society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.\\n\\n[2] S. Pascual et al., \u201cSpeech enhancement, separation, recognition and speaker diarization in the wild,\u201d in Proc. ISCA Interspeech, 2022, pp. 6167\u20136171.\\n\\n[3] H. Phan et al., \u201cReal time speech enhancement in the wave domain,\u201d in Proc. ISCA Interspeech, 2017, pp. 3642\u20133646.\\n\\n[4] A. D\u00e9fossez et al., \u201cGPU-accelerated guided source separation for multichannel audio books,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2020, pp. 3111\u20133115.\\n\\n[5] B. J. Borgstr\u00f6m and M. S. Brandstein, \u201cSpeech enhancement via attention masking network (SEAMNET): An end-to-end system for joint suppression of noise and reverberation,\u201d IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 29, pp. 1700\u20131704, 2020.\\n\\n[6] X. Xiao et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[7] Z.-Q. Wang et al., \u201cDeep beamforming networks for multi-channel speech recognition for unsegmented recordings,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2020, pp. 7124\u20137128.\\n\\n[8] Z. Zhang et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[9] C. Subakan et al., \u201cOn training targets for supervised speech separation,\u201d in Proc. IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 31, pp. 3800\u20133813, 2023.\\n\\n[10] Y. Wang et al., \u201cAttentive encoder-decoder model for speech enhancement,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2016, pp. 5745\u20135749.\\n\\n[11] D. Raj et al., \u201cMontreal forced aligner: Trainable text-removal, speaker diarization benchmark, and recipe,\u201d 2021, arXiv:2106.04624.\\n\\n[12] S. Watanabe et al., \u201cAll-neural beamformer for continuous speech recognition evaluation,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2020, pp. 7284\u20137288.\\n\\n[13] S. Cornell et al., \u201cThe CHiME-7 DASR Challenge: Distant meeting transcription with multiple devices in diverse scenarios,\u201d in Proc. ISCA Interspeech, 2020, pp. 1\u20137.\\n\\n[14] J. Carletta et al., \u201cThe second DIHARD diarization challenge: Task, dataset, and baselines,\u201d in Proc. ISCA Interspeech, 2019, pp. 978\u2013982.\\n\\n[15] A. Janin et al., \u201cAnnotating egocentric video,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition, 2022, pp. 18 995\u201319 012.\\n\\n[16] Y. Fu et al., \u201cThe fifth \u2018CHiME\u2019 speech separation and recognition evaluation,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 5206\u20135210.\\n\\n[17] F. Yu et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[18] J. Barker et al., \u201cSpeechBrain: A general-purpose speech processing system,\u201d 2022, arXiv:2305.13516.\\n\\n[19] M. V. Segbroeck et al., \u201cScale-lagged speaker diarization,\u201d in Proc. 8th International Workshop on Speech Processing in Everyday Environments (CHiME 2020), 2020, pp. 1476\u20131480.\\n\\n[20] T. Liu et al., \u201cThe rich transcription 2006 spring meeting announcement,\u201d in Proc. ISCA Interspeech, 2006, pp. 309\u2013322.\\n\\n[21] J. S. Chung et al., \u201cGPU-accelerated guided source separation for multichannel audio books,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2020, pp. 7114\u20137118.\\n\\n[22] E. Z. Xu et al., \u201cDeep neural end-to-end overlapped speech detection,\u201d in Proc. ISCA Interspeech, 2021, pp. 905\u2013911.\\n\\n[23] K. Grauman et al., \u201cSequence-level multi-frame neural beamforming for joint suppression of noise and reverberation,\u201d IEEE/ACM Transactions on Audio, Speech and Language Processing, vol. 33, 2020, pp. 12 449\u201312 460.\\n\\n[24] J. W. Du Bois et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[25] P. \u02d9Zelasko et al., \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Adv. Neural Information Processing Systems 36, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds. NeurIPS, 2023, pp. 1\u20135.\\n\\n[26] N. Ryant et al., \u201cThe AMI meeting corpus: A preannouncement,\u201d in Proc. 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2020), 2020, pp. 299\u2013303.\\n\\n[27] L. Brandschain et al., \u201cThe CHiME-6 Challenge: Tackling multispeaker and multidevice meeting transcription with multiple devices in diverse scenarios,\u201d in Proc. ISCA Interspeech, 2022, pp. 1476\u20131480.\\n\\n[28] M. McAuliffe et al., \u201cMixer-6 speech LDC2013S03,\u201d Philadelphia: Linguistic Data Consortium, 2013.\\n\\n[29] V. Pratap et al., \u201cThe second DIHARD diarization challenge: Task, dataset, and baselines,\u201d in Proc. ISCA Interspeech, 2019, pp. 978\u2013982.\\n\\n[30] A. Baevski et al., \u201cSpeechBrain: A general-purpose speech processing system,\u201d 2022, arXiv:2212.04356.\\n\\n[31] M. Ravanelli et al., \u201cThe ICSI meeting corpus,\u201d in Proc. ISCA Interspeech, 2022, pp. 1476\u20131480.\\n\\n[32] H. Bredin et al., \u201cpyannote.audio 2.1 speaker diarization pipeline: principles, benchmark, and recipe,\u201d 2021, arXiv:2106.04624.\\n\\n[33] H. Bredin et al., \u201cEnd-to-end speaker segmentation for speaker diarization,\u201d in Proc. ISCA Interspeech, 2023, arXiv:2305.13516.\\n\\n[34] H. Bredin and A. Laurent, \u201cEnd-to-end speaker segmentation for speaker diarization,\u201d in Proc. ISCA Interspeech, 2023, arXiv:2305.13516.\\n\\n[35] F. Landini et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[36] L. Bullock et al., \u201cOverlap-aware diarization: Resegmentation using the modern deep learning ecosystem,\u201d 2021, arXiv:2110.12561.\\n\\n[37] A. Plaquet and H. Bredin, \u201cPowerset multi-class cross entropy loss for neural speaker diarization,\u201d in Proc. ISCA Interspeech, 2022, pp. 6032\u20136036.\\n\\n[38] A. Radford et al., \u201cLanguage model pretraining with normalized information divergence objectives,\u201d 2019, arXiv:1910.13461.\\n\\n[39] D. Raj et al., \u201cSpot the conversation: Speaker diarisation in egocentric video,\u201d in Proc. 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2020), 2020, pp. 1\u20137.\\n\\n[40] V. Panayotov et al., \u201cLibriSpeech: An ASR corpus based on public domain audio books,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing, 2015, pp. 4159\u20134163.\\n\\n[41] Z. Chen et al., \u201cOn word error rate definitions and their performance on real-world data,\u201d Computer Speech Language, vol. 64, 2021, pp. 101254.\\n\\n[42] J. G. Fiscus et al., \u201cOn word error rate definitions and their performance on real-world data,\u201d Computer Speech Language, vol. 64, 2021, pp. 101254.\\n\\n[43] T. von Neumann et al., \u201cOn word error rate definitions and their performance on real-world data,\u201d Computer Speech Language, vol. 64, 2021, pp. 101254.\\n\\n[44] \u2014\u2014, \u201cMeetEval: A toolkit for computation of word error rates for meeting transcription systems,\u201d in CHiME Workshop, 2023.\"}"}
{"id": "maciejewski24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluating the Santa Barbara Corpus: Challenges of the Breadth of Conversational Spoken Language\\nMatthew Maciejewski1\u2217, Dominik Klement3\u2217, Ruizhe Huang2, Matthew Wiesner1, Sanjeev Khudanpur1,2\\n1HLTCOE and 2CLSP, Johns Hopkins University, USA\\n3Speech@FIT, Brno University of Technology, Czechia\\nmatt@mmaciejewski.com, xkleme15@stud.fit.vutbr.cz, {ruizhe,wiesner,khudanpur}@jhu.edu\\n\\nAbstract\\nAs speech technology has matured, there has been a push towards systems that can process conversational speech, reflecting the so-called \u201ccocktail party problem,\u201d which includes not only more challenging acoustic conditions, but also necessitates solutions to new problems, such as identifying who spoke when and processing multiple concurrent streams of speech. Such problems have been approached primarily via corpora comprising business meetings and dinner parties, overlooking the broad range of conversational dynamics and speaker demographics that fall under the category of multi-talker speech.\\n\\nTo this end, we introduce the use of the Santa Barbara Corpus of Spoken American English for evaluation of speech technology\u2014including preparing the corpus and annotations for automatic processing, demonstrating the failure of state-of-the-art systems to withstand the heterogeneity of conditions, and highlighting the situations where standard methods struggle to perform at all.\\n\\nIndex Terms: conversational speech, diarization, speech recognition\\n\\n1. Introduction\\nSolving the cocktail party problem\u2014the ability to recognize speech when one or more conversations are taking place, often in reverberant and noisy environments\u2014has long been considered among the ultimate goals of the recognition of speech [1]. A variety of techniques have partially enabled this ability in speech processing systems. For instance, speech enhancement [2\u20135] and beamforming [6\u20138] have been used to suppress the noise and reverberation present in far-field recordings, and speech separation [9, 10] and overlap-aware [11] or speaker-attributed automatic speech recognition (ASR) systems [12,13] have been developed to accommodate overlapping speech.\\n\\nThese techniques are often evaluated on recordings of business meetings [14\u201317] or dinner parties [18,19]. While work on such corpora has driven progress in multi-talker speech technology, they do not reflect the full range of human interactions or acoustic environments present in naturally occurring multi-talker audio. Recently released corpora for benchmarking audio-visual diarization [20\u201323] include a broader range of human interactions, but lack transcription for ASR.\\n\\nOne existing resource that explicitly aims to capture the full range of naturally occurring spoken interaction is the Santa Barbara Corpus of Spoken American English (SBCSAE) [24], which was originally collected for linguistic analysis of everyday speech from American English speakers of all \u201cages, occupations, genders, and ethnic and social backgrounds.\u201d\\n\\n**To address the shortcomings in evaluation of conversational speech technology, we repurpose the Santa Barbara corpus for evaluation of multi-talker speech technology, including speaker diarization and speaker-attributed automatic speech recognition. This involved processing of transcript files, realignment of segment boundaries, detection of anonymized regions of speech, and more. In this work, we describe our efforts in detail and also demonstrate and analyze the inconsistent performance of standard systems in these heterogeneous conditions. All corpus processing scripts and results will be released and integrated into the Lhotse [25] audio preparation library for ease of use.**\\n\\n1.1. Related Work\\nRecent work in diarization has focused on creating data resources that cover the breadth of natural human interactions. For instance, The DIHARD [26] data contains some interviews, e.g., from the Mixer 6 corpus [27], as well as restaurant conversations and meetings. Recent corpora such as the AVA-VD [22] and VoxConverse [21] datasets consist of movies and celebrity interviews and likely do not reflect everyday speech. Similar to SBCSAE is the MSDWILD [20] corpus, which used targeted web-scrapping to collect videos of everyday conversations. SBCSAE, however, has nearly double the amount of speech with many (>5) speakers per recording.\\n\\nAlmost all publicly available multi-talker ASR corpora consist of meeting or dinner scenarios. While the recent CHiME-7 DASR challenge [13] focused on multi-domain speaker-attributed ASR systems, the domains were all \u201cseen\u201d in training and limited to the CHiME [18], DipCo [19], and Mixer 6 [27], corpora which cover dinner parties and two-person interviews. SBCSAE, which comes with full transcription, can be used for evaluation of multi-talker and speaker-attributed ASR systems in more heterogeneous environments. The Ego4d-VD set [23] has some similarities, though contains only 5-minute ego-centric multimodal clips of social interactions. SBCSAE, however, contains longer (\u223c20 minute) recordings of all kinds of speech, not just social interactions, recorded in stereo.\\n\\n2. Corpus Overview\\nThe Santa Barbara corpus comprises 60 recordings (24 hours) of naturally-occurring speech, recorded from 1987\u20131996 with stereo microphones (though 4 recordings are monaural and 4 have a silent channel), released at 22.05 kHz.\\n\\nBecause the corpus was constructed for linguists to study the speech of American English speakers of all backgrounds, the available transcripts are at the level of intonation units and are very detailed. Annotation includes full transcripts with...\"}"}
{"id": "maciejewski24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"utterance-level time marks, speaker labels, overlap labels, vocal style markings, inhalations/exhalations, laughter and other vocalization, and some non-speech sounds. However, as the corpus was originally intended for human viewing, there are a number of formatting issues that complicate automatic processing, including inconsistent tab and space character delimiting both across and within files as well as other typographical inconsistencies. In single-speaker regions, the utterance time marks were not precise, only placing boundaries between utterances. However, the excess silence can be reliably removed by resegmenting via forced alignment with the transcripts.\\n\\nSBCSAE also comes with metadata about participants\u2019 gender, age, hometown, current state, education level, years of education, occupation, and ethnicity, and also recording information such as the recording location/room, type of conversation, and conversation summary. To protect the participants\u2019 privacy, personally-identifying information, such as name or address, had been redacted via transcript anonymization and low-pass filtering of the audio prior to release. Additionally, for some speakers, some or all metadata is missing.\\n\\nFigure 1: Characterizing the speech in the SBCSAE by according to conversation type, the number of speakers, and the percent of overlapped speech. Each point is a recording.\\n\\nFigure 1 characterizes the styles of speech present in SBCSAE, each with unique challenges. Conversations with family, friends, romantic partners, doctors, and work colleagues feature prominently, containing between 2-10 speakers and potentially large amounts of overlapping speech. There are a number of task-oriented conversations involving game-play, cooking, veterinary clinic work, and even witness preparation. These involve groups where speech is more formal and less overlapped. Finally many recordings feature \u201clecture\u201d scenarios with predominantly a single speaker (e.g. a lecturer, tour guide, or pastor), and many additional participants who speak only briefly (e.g. to ask or answer a question). Counter-intuitively, these recordings often contain the largest number of speakers.\\n\\n3. Corpus Preparation\\n\\n3.1. Transcript Processing and Correction\\n\\nFigure 2: Example of SBCSAE transcript file, demonstrating annotation level and style.\\n\\nAn example selection from a transcript file is shown in Figure 2. Transcripts with this level of detail are valuable, but require processing to be usable for evaluation. Additionally, typographical errors can be problematic. For example, the highly-prevalent space/tab character errors lead to significant errors of tracking speakers due to difficulties in delineating the time mark entries, optionally-empty speaker attribution entry, and text transcription entry. For ASR purposes, we removed all special characters and annotations except for code-switching marks, punctuation, laughter, and undecipherable utterances. The results were then extensively checked for processing errors, and many annotation errors were corrected manually.\\n\\nWe made manual corrections to address inconsistent annotation: we annotated all unmarked instances of code-switching; we created\u2014to the best of our ability\u2014new, consistent, dummy speaker labels for speakers who were assigned a collective speaker label such as \u201caudience\u201d. In one recording (SBC021), the high number of people, low volume, and short utterance length made this speaker re-annotation difficult, so we instead gave each new speaker-unlabeled utterance a new dummy label. This likely overestimates the speaker count, but better represents the dynamics of that recording. This recording should be treated carefully in diarization evaluation.\\n\\n3.2. Anonymization Filter Detection\\n\\nAs part of the release of the Santa Barbara corpus, personally-identifying information of the participants was anonymized. This was done via text replacement in the transcripts (along with a marker) as well as filtering of speech energy above 400 Hz to maintain pitch information while removing formants necessary to recognize the words spoken. While the corpus documentation refers to \u201cfilter list files (*.flt)\u201d that contain the filtered regions, we were unable to find evidence of these files having been released among the UCSB, TalkBank, and LDC copies of the corpus. As a result, we developed a manually-tuned algorithm to detect these filtered regions based on a significant reduction of energy in high-frequency parts of the spectrum. The spectra and algorithm results were manually checked and all errors found were corrected. The code and results have been released, including a file in the .uem format used in diarization scoring packages for denoting scored regions.\\n\\n3.3. Alignment\\n\\nAs noted in Section 2, the provided segments often include excess silence in single-speaker regions, which makes them unsuitable for evaluating speaker diarization and voice activity detection (VAD) systems; even ASR systems typically remove silence regions as a preprocessing step. We improved the ground-truth segments by re-aligning the original audio with the transcripts. Because overlapped regions are well-annotated and are challenging for forced alignment (FA) models, we only re-aligned single-speaker segments. We produced two sets of alignments: alignments designed to tightly match speech activity for diarization purposes, and ASR alignments aiming to never clip spoken words.\\n\\nOur pipeline consists of three FA models: an HMM-GMM-based Montreal Forced Aligner (MFA) [28], torchaudio *-CTC-based MMS_FA [29] and WAV2VEC2_ASR_BASE_960H, both built on top of Wav2Vec2 [30]. We aligned audio on a per-segment basis for each model separately. Certain annotations corresponding to laughter, yell, or unknown words, are either deleted or mapped to \u201c*\u201d in the *-CTC aligners. As an orthogonal resegmentation procedure, we used the pyannote-3.1 diarization system to create a VAD-based segmentation. We computed the intersection over union (IoU) between all per-segment pairs of FA outputs and defined that two models agree if the IoU is positive. If at least one system disagreed with the others, we...\"}"}
{"id": "maciejewski24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"left the segment for manual inspection. To obtain a per-segment matching score, we computed a weighted average of IoU scores. We assigned more weight to agreement between more dissimilar systems. If the IoU was too low or any systems do not overlap, the segments were manually inspected. Afterwards, VAD was used to confirm the presence of speech. Conflicting results were then manually inspected. The union of all alignment intervals was used as a tentative updated segment boundary. We finally applied the updated boundaries to segments that were less than half their original duration.\\n\\nFor diarization alignments, we aimed to remove as much silence as possible. We used the SpeechBrain CRDNN VAD system \\\\cite{31} in union with the pyannote diarization system, as pyannote sometimes drops large consecutive chunks of speech. We also relaxed the silence realignment condition to 10%, which in combination with VAD union allowed us to remove almost five times more silence, totalling 1 hour for the ASR and \\\\( \\\\sim 5 \\\\) hours for the diarization segmentations.\\n\\n### 4. Experimental Configuration\\n\\n#### 4.1. Systems\\n\\n##### 4.1.1. Diarization Systems\\n\\nWe evaluated performance of state-of-the-art diarization systems on the SBCSAE using a traditional cascaded, speaker embedding clustering system and a modern end-to-end neural-based system. For the cascaded system, we used the Diarizer 4 package, based on the AMI \\\\cite{14} recipe. The pipeline uses pyannote-2.0's VAD and overlap detection \\\\cite{32\u201334} along with Brno University of Technology's implementation of x-vector extraction and VBx \\\\cite{35} with overlap \\\\cite{36}. We did not fine-tune any model parameters in order evaluate to what extent systems can generalize to heterogeneous conditions.\\n\\nFor the neural system, we used pyannote-3.1 \\\\cite{37}, for its performance record and prevalence of use both in research and production. This system is based on powerset-classification neural diarization \\\\cite{37}, with an additional clustering component to stitch the windowed sections used to accommodate the memory requirements of neural models. For heterogeneity experiments, we used this system with all default settings. For tuning experiments, we focused on parameters which affect the number of unique speakers the model produces, evaluating both using a fixed number of speakers as well as varying the minimum number of embeddings required for a speaker cluster.\\n\\n##### 4.1.2. Speech Recognition Systems\\n\\nFor ASR, we sought to test robustness to challenging conditions and multi-talker overlapped speech transcription. We selected OpenAI Whisper Large-v3 \\\\cite{38}, a single-talker ASR system pre-trained on vast amount of data. We use Guided Source Separation (GSS) \\\\cite{39} pre-processing to help Whisper cope with overlapped speech and enhance the audio, even though the dataset does not contain more than two channels that GSS would benefit from. As an alternative, we use the SURT 2.0 large \\\\cite{11} model, pre-trained on single-talker dataset LibriSpeech \\\\cite{40} and multi-talker simulated dataset LibriCSS \\\\cite{41}, to test performance of an ASR system trained to transcribe overlapped speech.\\n\\nWe first split the long-form recordings into groups according to the re-aligned ASR supervisions (transcription units, such as parts of sentences). Then, we ensured that each group contains at most 20 re-aligned supervisions, which limits the duration of a single group to at most 1 minute as well as the maximum number of speaker changes, and restricts the MIMO WER combinatorial space.\\n\\n#### 4.2. Evaluation Metrics\\n\\n##### 4.2.1. Diarization Metrics\\n\\nFor evaluation, we used both Diarization Error Rate (DER) \\\\cite{42} and Jaccard Error Rate (JER) \\\\cite{26}, as they have complementary downsides. The salient difference is that while DER is in some sense the amount of errors divided by the total amount of speech, JER is roughly the per-speaker detection error averaged across all reference speakers. This means that DER tends to under-emphasize errors of rare speakers due to contributing little overall speech, while JER does not penalize hallucinating extra speakers which do not correspond to a reference speaker.\\n\\n##### 4.2.2. Speech Recognition Metrics\\n\\nWe evaluated speaker-agnostic ASR performance using MIMO-WER \\\\cite{43} as implemented in the MeetEval toolkit \\\\cite{44}, as it enables evaluating a single hypothesis stream against multiple overlapping references. For speaker-attributed ASR we used cpWER \\\\cite{12}.\\n\\n![Figure 3: DER/JER comparison on a per-sample basis. The baseline diarization results are reflected in Figure 3. There is a wide range of performances by both systems, with a floor of around 20%, which is not unreasonable for generally far-field recordings, and a ceiling of around 75% DER. As some recordings contain over 20 unique speakers, it is unsurprising that JER tops out above 99%. It also matches expectations that recordings with lower DER but higher JER have more speakers in them. Interestingly, there is one recording where JER is much lower than DER. The systems consistently overestimate the number of speakers in it, which aligns with the primary potential downside of JER.\\n\\nIt is also encouraging to see that there are recordings where both DER and JER are high, which indicates truly bad performance rather than artifacts of the metrics. The worst samples for pyannote are dominated by speech by the elderly and teenagers, likely due to a lack of age range in the training data. The x-vector system performs better on these samples, likely due to the broad range of training data in the x-vector system. However, it tends to do very poorly on heavily-overlapped recordings, suggesting the overlap-aware VBx method does not do as well as a neural model at recovering overlapping speech.\"}"}
{"id": "maciejewski24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additionally, we explored the speaker-counting aspect of diarization through the pyannote model, which both has default behavior of automatically detecting the number of speakers via its clustering method as well as the ability to cluster to a fixed number of speakers. We swept this parameter to find the best-performing \u2018oracle\u2019 number of speakers for each recording. Comparisons of the oracle, detected, and ground-truth number of speakers are shown in Figure 4.\\n\\nThe primary takeaway from this figure is that there is no systemic bias between these quantities\u2014which would be seen if the points tended to lie on one side or another of the lines. One interesting result is that the most systemic bias is that pyannote tends to detect a larger number of speakers than is optimal for DER, but not JER. This suggests that pyannote does a reasonable job of detecting speakers, and is not optimizing the DER metric, which can incentivize dropping rare speakers.\\n\\nFinally, Figure 5 reflects analysis we did with respect to the min_cluster_size parameter of pyannote, which helps prevent the model from over-clustering, but also provides a floor to the amount of speech the model is capable of detecting as a unique speaker. Similar to the prior experiment, we found the per-sample \u2018oracle\u2019 optimal setting.\\n\\nIn many cases, decreasing this parameter can lead to dramatic reductions in JER (up to 33% absolute), suggesting that the clustering backend does drop speakers that the end-to-end model found. However, decreasing this parameter reduces overall performance, suggesting it is already tuned optimally for this dataset (from below), even for the speaker-emphasizing JER metric. For DER, we unsurprisingly saw no gains by reducing this parameter and still saw an increased error rate in aggregate.\\n\\n5.2. Speech Recognition Results\\n\\nTable 1 reports the performance of Whisper and SURT Large. While the SURT model is able to detect and separate overlapped speech to an extent, it often produces incoherent sentences even on clean recordings with little overlap. This may be due to training on synthetically created mixtures from LibriSpeech [40] that do not reflect real-world conditions. We therefore performed 4-fold cross-validation, fine-tuning (FT) the SURT model on each chunk of \u223c5 hours of speech. However, fine-tuning does not help, likely because SBCSAE contains such heterogeneous recordings. While Whisper was not trained to perform multispeaker ASR, and often fails to transcribe non-dominant speech in overlapped regions, it significantly outperforms the SURT model likely thanks to robustness from training on much larger amounts of data. The MIMO-WER ranges from 5.99% on mostly single speaker recordings to 42.31% on recordings with significant overlapped speech. Examining Table 1, we see that both models perform considerably better on groups with low overlap. This suggests that pre-training models on colossal amounts of single speaker data leads to superior performance, but does not solve ubiquitous problems in spontaneous speech.\\n\\nSBCSAE also enables evaluation of speaker-attributed ASR. We first evaluated Whisper on speaker-labeled supervisions using oracle diarization, and later used GSS to assist Whisper and potentially enhance the target speaker on overlapped speech. As Figure 6 shows, GSS decreased the maximum cpWER by 11.2% absolute. The improvement mainly comes from a single recording (SBC019) that contains two less-correlated channels, which GSS benefits from as opposed to highly-correlated stereo recordings prevalent in the data. Furthermore, the violin plot displays a slight cpWER mass shift towards lower error rate values, demonstrating that GSS results in small improvements on other recordings as well. Finally, we evaluated Whisper + GSS with a pyannote diarization system. The last violin plot in Figure 6 shows 25.48% relative cpWER degradation when real a diarization systems is used compared to Whisper + GSS with oracle diarization. The long distribution tail shows how diarization can negatively affect cpWER, which on SBC011 increased from 21.61% to 109.34% due to failure to distinguish between voices of the elderly.\"}"}
