{"id": "fara22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression.\\n\\nSalvatore Fara, Stefano Goria, Emilia Molimpakis, Nicholas Cummins\\n\\n1 Thymia, London, UK\\n2 Institute of Psychiatry, Psychology & Neuroscience (IoPPN), King's College London, London, UK\\n\\n{salvatore,stefano,emilia,nick}@thymia.ai, nick.cummins@kcl.ac.uk\\n\\nAbstract\\n\\nEmbedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multimodal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes.\\n\\nIndex Terms\\n\\n: depression, computational paralinguistics, cognitive games, n-Back, symptom measurements\\n\\n1. Introduction\\n\\nMajor depressive disorders (MDDs) are a constantly growing economic and societal problem [1]. The harsh collateral social side-effects of COVID-19 (social isolation, employment loss, bereavement, grief) have further exacerbated this already substantial problem [2, 3]. MDDs are also one of the leading causes of disability worldwide, accompanied by high socio-economic costs [4]. As a result, accurately identifying, treating and thereby reducing the prevalence of MDDs is a major public health goal and challenge. However, globally, demand for mental health support greatly outstrips supply. Therefore, advances in digital health tools and phenotyping technologies that can support clinicians in this effort are crucial to ensure better and more widespread access to high-quality mental health support services and treatment.\\n\\nSpeech has great potential to be a source of such phenotypes and to provide unique preventative and predictive information about depression [5, 6, 7, 8]. However, current research in this space is not without its limitations. These become apparent when we consider a) the datasets used, b) core assumptions regarding effects of depression severity, and c) additional modalities collected alongside speech.\\n\\nIf we look at machine learning approaches that have arguably dominated speech-depression research over the last ten years, the majority of these works use the Audio/Visual Emotion Challenge (AVEC) datasets [9, 10]. Whilst these investigations have undoubtedly advanced our knowledge in modelling depressed speech, their continuous use in pseudo-competition settings raises concerns relating to Goodhart's Law [11] and overfitting [12]. Therefore, in order for the speech-depression community to continue to be driven forward, it is imperative that new databases are (additionally) used henceforth.\\n\\nAnother critical issue in speech-depression research is that the majority of works assume depression severity is in, and of itself, the most dominant and important measurable effect in speech. However, depression has a highly heterogeneous clinical profile; it may be the case therefore, that speech alterations observed within depression could be more strongly (and more explainably) associated with subsets of core depressive symptoms, rather than severity as an absolute measure. Preliminary works exploring this conjecture have indeed demonstrated that speaking rate measures are more strongly correlated with mood and psychomotor retardation measurements than with overall depression severity [13, 14]. These findings, however, are from small sample sizes and should be regarded as preliminary only.\\n\\nFinally, no research to date has considered placing depression assessment via speech - whether with respect to depressive severity or depressive symptoms - alongside assessments of depression from other modalities; other than vision [15, 16, 17]. For example, to the best of the authors' knowledge, there is no research examining how speech alterations may be complemented with reaction times and error rates in more classic neuropsychology protocols [18, 19]. If we are to attempt to understand how the speech signal may be related to depressive symptomatology, it makes sense to examine it alongside and within the context of such classic protocols, leveraging complementary information to gain a better understanding of this relationship.\\n\\nIn response to the limitations described above, this paper presents a set of preliminary analyses conducted on a novel, large, online multimodal dataset collected by Thymia Limited (henceforth Thymia). In the following sections, we detail how for the first time, we combined speech elicitation tasks with the n-Back Task, an experimental psychology protocol targeting Working Memory [18, 20, 21]. Presented analyses include a set of machine learning experiments that highlight the complementary information contained within the speech and n-Back features when predicting the presence or absence of depression. Importantly, we additionally present a set of experiments that highlight the association between different speech and n-Back markers at the symptom level of depression, allowing us an initial insight into how speech and the n-Back Task may be used in tandem to better target different core depressive symptoms.\"}"}
{"id": "fara22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Data Collection\\n\\nAs part of its core mission, Thymia [22] (a London based mental health tech startup) is actively collecting multimodal \u2013 video, speech and behavioural \u2013 data to develop models targeting remote assessment and monitoring of depression. In order to maximise access to the studies, the data collection needs to run on a browser, operating system and device-agnostic platform.\\n\\n2.1. Thymia Research Platform\\n\\nIn the past few years, several platforms for online studies have gained popularity [23], but to the best of our knowledge none offer the level of flexibility and security required for our intended task. Thymia, therefore, developed and implemented our own research platform.\\n\\nThe Thymia Research platform allows the hosting of complex, remote, one-off or longitudinal multimodal studies where detailed, informed consent and demographic data can be gathered, questionnaires can be completed, and gamified activities can be assigned to participants on a schedule. During these activities, data from the device\u2019s camera, keyboard, mouse/trackpad and/or touch screen can be streamed to a secure backend. Throughout the experiment protocols, when launching activities that require media recording, participants are reminded that their camera and/or microphone will be switched on for recording and they are free to opt out.\\n\\n2.2. Online Study Setup\\n\\nThe dataset used in this work is part of a larger online study 2 running on the Thymia Research platform. It consists of demographic and psychiatric questionnaires, speech eliciting tasks and gamified experimental protocols targeting visual processing, attention, psychomotor response and working memory.\\n\\nParticipants were pre-screened and split into two groups: a patient and a healthy age- and gender-matched control group. Both groups consisted of adult, native English speakers, aged 18 to 75 (evenly split across age groups), with normal or corrected-to-normal vision, no hearing, language or speech impairments and - for the control group - no prior history of psychiatric illness. The patient group participants must have had a formal MDD diagnosis by a GP, clinical psychologist or psychiatrist at least two months prior to participating.\\n\\nA study session is completed via the participant\u2019s laptop or smart device without any researcher supervision. The session includes standardised questionnaires to gather information about demographics and mood, including the Patient Health Questionnaire - 8 (PHQ-8) [24], a well-established depression scale used commonly in research which aims to assess a number of core depressive symptoms, including fatigue, working memory impairment, anhedonia and low mood. Participants also completed speech eliciting tasks, including an Image Description Task, and short point-and-click (or screen tapping) tasks measuring reaction times, accuracy and error rates, including an implementation of the classical n-Back task [19].\\n\\n2.2.1. Image Description Task\\n\\nIn the Image Description Task, participants are encouraged to describe what they see in the image whilst keeping their camera and microphone on; while performing the task participants can see their own camera feed as feedback and reminder of the fact that they are being recorded. The image itself is a rich, animated illustration depicting a caf\u00e9 environment filled with people at different tables (Figure 1).\\n\\n2.2.2. n-Back Task\\n\\nPerformance in the n-Back Task offers insights into working memory dysfunction in depression [18, 21]. The implementation used as part of this study is presented as a card memory game. A rounded edged card (as in a deck of cards) with a single digit number or letter in the middle of it appears in the centre of a blank screen for a short interval; this is then followed by a blank screen; followed by another card containing another number or letter; followed by a blank screen etc. The participant must tap/click the screen when the current number or letter matches the number or letter that appeared \\\\(n\\\\) cards back. There are two difficulty levels based on the value of \\\\(n\\\\).\\n\\nThe different difficulty levels are based on progressive cognitive loading. In the first block, \\\\(n\\\\) equals one (1) (i.e. the current target card must match the card one card back); in the second block \\\\(n\\\\) increases to two (2) (i.e. the current target card matches the card appearing two cards before it). Each participant saw three practice blocks for each n-Back load, followed by 6 experiment blocks for each n-Back load. Match number and position were pseudo-randomised across blocks and block order was counterbalanced across participants.\"}"}
{"id": "fara22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Sociodemographic, Depression (Low: PHQ-8 < 10, High: PHQ-8 \u2265 10) and Activity distributions in the experimental data.\\n\\n| Partition | #Participants | Age Mean | Age SD | #Low | #High | Total Activity Time |\\n|-----------|---------------|----------|--------|------|-------|---------------------|\\n| Male      | 387           | 34.94    | 12.64  | 501  | 274   | 10:19:36            |\\n| Female    | 388           |          |        |      |       | 17:13:01            |\\n| Test      | 97            | 35.39    | 13.24  | 125  | 69    | 2:21:33             |\\n\\n3. Dataset\\n\\nOur experimental dataset consists of 969 participants who performed a range of activities within a single session on the Thymia Research Platform using their own personal devices. The presented analysis focuses on two specific data modalities gathered through the platform: audio data from the Image Description Task, and behavioural data from the n-Back Task.\\n\\nWe performed a stratified split of the dataset into training and test sets (80/20 %), keeping the same proportions of genders, age groups and PHQ-8 distribution (Table 1).\\n\\n3.1. Quality Controls\\n\\nGiven the real-life nature of the dataset, each participant using their own personal device, we performed a number of quality controls on the different data modalities.\\n\\nFor the audio recordings, we implemented an automated audio quality pipeline to flag the audio files as having good or bad quality based on two criteria: (i) the quality of extracted audio features, and (ii) the presence of speech activity. Out of 917 files, the audio quality pipeline detected 51 bad quality audio files that were rejected from subsequent analysis. Through manual inspection of these 51 files, we confirmed that they were all cases of either high environmental noise, microphone turned-off or malfunctioning, or participant not speaking. A principal component projection of the extracted features of each audio file in the dataset, confirms the difference between the distributions of good and bad audio files (Figure 2).\\n\\nFor the n-Back Task, we confirmed that the various metrics of performance are within expected ranges and vary with n-Back load as well as known covariates such as age; noting a general decrease in n-Back performance with age [25].\\n\\n4. Experimental Settings\\n\\nWe investigated the impact of combining multiple data modalities, namely audio data from the Image Description Task and behavioural data from the n-Back Task, in a binary PHQ-8 classification paradigm (PHQ-8 < 10 vs PHQ-8 \u2265 10). Details on the features and the models used are provided in the following.\\n\\n4.1. Speech Features\\n\\nThe audio recordings of the Image Description Task were processed to extract a range of acoustic and linguistic features. We extracted 88 acoustic features as defined in the extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) [26] using OPEN SMILE [27]. In addition, we extracted a curated set of 28 features describing speech rate, pitch, voice quality and formant properties, using the Parselmouth package [28] as a Python interface to Praat [29]. To quantify the linguistic content, we first transcribed the files using Amazon Transcribe [30]. We then used the spaCy library [31] to extract 25 linguistic features describing speech-rate, pause-rate and part-of-speech usage.\\n\\n4.2. n-Back Features\\n\\nFor each session, the data collected from the n-Back Task consists of a sequence of clicks and a corresponding sequence of targets and non-targets. From these sequences, we calculated standard features that quantify the performance in the task, namely precision, recall, false-positive rate, as well as reaction time. These features were calculated separately for the two n-Back loads (1-Back and 2-Back), yielding 8 n-Back features.\\n\\n4.3. Models\\n\\nAll models, training and calibration procedures are implemented using the scikit-learn package [32].\\n\\n4.3.1. Speech and n-Back Models\\n\\nFirst, we investigated five unimodal models: four speech models and an n-Back model. The speech models are; (i) an eGeMAPS model; (ii) a Praat model; (iii) a Linguistic model; and (iv) a Speech model, which is the early fusion model of the 141 acoustic and linguistic features. All models consist of a binary Random Forest classifier with input features from their specific feature representations. All models receive three additional features as input, namely age, gender, and device setup. This last categorical feature encodes the personal device setup on which the session was performed (i.e. Laptop+Trackpad, Laptop+Mouse, Mobile/Tablet, Desktop). Standard rescaling is applied to all numerical features, while gender is encoded as binary and device setup is one-hot encoded. The unimodal models are fitted and calibrated on the training set using a cross-validated random parameter search with 100 iterations and 10 cross-validation folds. The hyperparameters tuned were #Trees, Max. Depth, Min. #Samples/Split and Max. Rel. #Features.\"}"}
{"id": "fara22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Cross-validated model performances when classifying low (PHQ-8 < 10) versus high (PHQ-8 \u2265 10) depression on the training set\\n\\n| Model     | ROC-AUC  | Mean | SD   |\\n|-----------|----------|------|------|\\n| eGeMAPS   | 0.620    | 0.032|\\n| Praat     | 0.607    | 0.056|\\n| Linguistic| 0.625    | 0.045|\\n| n-Back    | 0.619    | 0.074|\\n| Speech    | 0.631    | 0.024|\\n| Multimodal| 0.652    | 0.037|\\n\\n4.3.2. Multimodal Model\\n\\nWe combined the Speech and n-Back unimodal models into a voting ensemble to create a multimodal model. The predictions of this model are given by a soft-voting rule, whereby the multimodal prediction is given by the most likely class label after averaging the predicted class probabilities across the unimodal models.\\n\\n4.3.3. Feature Analysis\\n\\nTo gain further insights into the performance of our multimodal system, we ran a set of linear regression analyses on our training set. The aim of this testing was to establish the importance of different eGeMAPs, Praat, Linguistic and n-Back features when predicting either individual items (questions) within the PHQ-8 scale, or predicting overall depression severity as given by the overall PHQ-8 score. We modelled each feature separately, and included age, gender and personal device setup (encoded as dummy variables) as covariates. We ranked feature importance in predicting each item, or the overall score using the $R^2$ value of each model.\\n\\nFigure 3: ROC curves calculated on the hold-out test set for the Speech, n-Back and Multimodal models.\\n\\nTable 3: Top features for predicting either a single PHQ-8 item or total PHQ-8 score\\n\\n| Item | Feature                  | $R^2$ | Beta |\\n|------|--------------------------|-------|------|\\n| #1   | Loudness Peaks Per Sec. (eGeMAPS) | .051  | -.054|\\n| #2   | Loudness Peaks Per Sec. (eGeMAPS) | .039  | -.049|\\n| #3   | 1-Back Reaction Time      | .034  | .089 |\\n| #4   | 1-Back False Positive     | .049  | .096 |\\n| #5   | CV Spectral Flux Voiced (eGeMAPS) | .052  | .001 |\\n| #6   | Syllable Rate (Praat)     | .042  | .001 |\\n| #7   | Loudness Peaks Per Sec.   | .034  | -.073|\\n| #8   | overall n-Back Precision  | .033  | .055 |\\n|      | Total Loudness Peaks Per Sec | .049  | -.322|\\n\\n5. Results and Discussion\\n\\nWe use the area under the ROC curve (ROC-AUC) as the performance metric to compare the models. We see that the performance of the unimodal models on the training set is well-above chance level on average (Table 2). This performance demonstrates that both speech and n-Back features contain predictive information about the corresponding PHQ-8 score. In addition, the average performance of the Multimodal model is higher than all unimodal models (Table 2), suggesting that the two modalities contain complementary PHQ-8 information.\\n\\nTo validate these results, we tested the Speech, n-Back and Multimodal models on the test set, which was not touched during model training and hyperparameter calibration. The performance of all models on the test set is qualitatively similar to the training set, with the Multimodal model outperforming the unimodal ones (Figure 3).\\n\\nOur linear regression analysis offers insights into why the fusion of our Speech and n-Back features improves results. When comparing the top-ranking features for each PHQ-8 item (Table 3), we can see that the items relating to anhedonia (Item #1), depressed mood (Item #2), change in appetite (Item #5), feelings of worthlessness (Item #6) and problems concentrating (Item #7) all returned a speech feature as the top-ranked feature. While the top feature for the somatic (Item #3 and #4) and psychomotor (Item #8) items were from the n-Back. Interestingly, n-Back features are ranked in the top three positions for the somatic and psychomotor items highlighting the strength of this task in capturing these changes.\\n\\n6. Conclusion\\n\\nDigital phenotyping offers the chance to aid depression diagnosis and management by providing objective information based on cognitive, physiological and behavioural cues. The results presented in this paper demonstrate that speech features, and metrics derived from n-Back Task performance offer complementary information when predicting depression. This is shown in two ways: by the increase in predictive performance when fusing both modalities, and feature space analysis, indicating that features from the different modalities strongly align to different items within the PHQ-8 domain. To the best of the authors' knowledge, this is the first time such a result has been shown. Future work will focus on repeating the fusion results in more complex models and exploring how the addition of facial information changes the feature space dynamics.\"}"}
{"id": "fara22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] OECD, \u201cPromoting mental health in europe: Why and how?\u201d in Health at a Glance: Europe 2018. OECD iLibrary, 2018, pp. 19\u201343.\\n\\n[2] C. Moreno, T. Wykes, S. Galderisi, M. Nordentoft, N. Crossley, N. Jones, M. Cannon, C. U. Correll, L. Byrne, S. Carr et al., \u201cHow mental health care should change as a consequence of the covid-19 pandemic,\u201d The Lancet Psychiatry, vol. 7, no. 9, pp. 813\u2013824, 2020.\\n\\n[3] Office for National Statistics, \u201cCoronavirus and depression in adults, Great Britain: January to March 2021,\u201d 2021. [Online]. Available: shorturl.at/msFO9\\n\\n[4] Deloitte Research, \u201cMental health and employers: Refreshing the case for investment,\u201d 2020. [Online]. Available: shorturl.at/imwzY\\n\\n[5] N. Cummins, S. Scherer, J. Krajewski, S. Schnieder, J. Epps, and T. F. Quatieri, \u201cA review of depression and suicide risk assessment using speech analysis,\u201d Speech communication, vol. 71, pp. 10\u201349, 2015.\\n\\n[6] D. M. Low, K. H. Bentley, and S. S. Ghosh, \u201cAutomated assessment of psychiatric disorders using speech: A systematic review,\u201d Laryngoscope Investigative Otolaryngology, vol. 5, no. 1, pp. 96\u2013116, 2020.\\n\\n[7] M. Yamamoto, A. Takamiya, K. Sawada, M. Yoshimura, M. Kitzazawa, K.-c. Liang, T. Fujita, M. Mimura, and T. Kishimoto, \u201cUsing speech recognition technology to investigate the association between timing-related speech features and depression severity,\u201d PloS one, vol. 15, no. 9, p. e0238726, 2020.\\n\\n[8] A. Abbas, C. Sauder, V. Yadav, V. Koesmahargyo, A. Aghjayan, S. Marecki, M. Evans, and I. R. Galatzer-Levy, \u201cRemote digital measurement of facial and vocal markers of major depressive disorder severity and treatment response: a pilot study,\u201d Frontiers in Digital Health, vol. 3, p. 610006, 2021.\\n\\n[9] M. Valstar, B. Schuller, K. Smith, F. Eyben, B. Jiang, S. Bilalakhia, S. Scherer, R. Cowie, and M. Pantic, \u201cA VEC 2013 \u2013 The Continuous Audio/Visual Emotion and Depression Recognition Challenge,\u201d in Proc. the 3rd Audio/visual Emotion Challenge. Barcelona, Spain: ACM, 2013, pp. 3\u201310.\\n\\n[10] F. Ringeval, B. Schuller, M. Valstar, J. Gratch, R. Cowie, S. Scherer, S. Mozgai, N. Cummins, M. Schmitt, and M. Pantic, \u201cA VEC 2017: Real-Life Depression, and Affect Recognition Workshop and Challenge,\u201d in Proc. the 7th Audio/Visual Emotion Challenge. Mountain View, California, USA: ACM, 2017, p. 3\u20139.\\n\\n[11] J. Miller, S. Milli, and M. Hardt, \u201cStrategic classification is causal modeling in disguise,\u201d in Proc. 37th International Conference on Machine Learning. Vienna, Austria: PMLR, 2020, pp. 6917\u20136926.\\n\\n[12] D. M. Hawkins, \u201cThe problem of overfitting,\u201d Journal of Chemical Information and Computer Sciences, vol. 44, no. 1, pp. 1\u201312, 2004.\\n\\n[13] R. Horwitz, T. F. Quatieri, B. S. Helfer, B. Yu, J. R. Williamson, and J. Mundt, \u201cOn the relative importance of vocal source, system, and prosody in human depression,\u201d in 2013 IEEE International Conference on Body Sensor Networks. Las Vegas, NV , USA: IEEE, 2013, pp. 1\u20136.\\n\\n[14] A. C. Trevino, T. F. Quatieri, and N. Malyska, \u201cPhonologically-based biomarkers for major depressive disorder,\u201d EURASIP Journal on Advances in Signal Processing, vol. 2011, no. 1, pp. 1\u201318, 2011.\\n\\n[15] J. F. Cohn, N. Cummins, J. Epps, R. Goecke, J. Joshi, and S. Scherer, \u201cMultimodal assessment of depression from behavioral signals,\u201d in The Handbook of Multimodal-Multisensor Interfaces: Signal Processing, Architectures, and Detection of Emotion and Cognition-Volume 2. Morgan & Claypool, 2018, pp. 375\u2013417.\\n\\n[16] J. M. Girard and J. F. Cohn, \u201cAutomated audiovisual depression analysis,\u201d Current opinion in psychology, vol. 4, pp. 75\u201379, 2015.\\n\\n[17] A. Pampouchidou, P. G. Simos, K. Marias, F. Meriaudeau, F. Yang, M. Pediaditis, and M. Tsiknakis, \u201cAutomatic Assessment of Depression Based on Visual Cues: A Systematic Review,\u201d IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 445\u2013470, 2017.\\n\\n[18] S. Nikolin, Y. Y. Tan, A. Schwaab, A. Moffa, C. K. Loo, and D. Martin, \u201cAn investigation of working memory deficits in depression using the n-back task: A systematic review and meta-analysis,\u201d Journal of Affective Disorders, vol. 284, pp. 1\u20138, 2021.\\n\\n[19] A. M. Owen, K. M. McMillan, A. R. Laird, and E. Bullmore, \u201cN-back working memory paradigm: A meta-analysis of normative functional neuroimaging studies,\u201d Human brain mapping, vol. 25, no. 1, pp. 46\u201359, 2005.\\n\\n[20] K. M. Lukasik, O. Waris, A. Soveri, M. Lehtonen, and M. Laine, \u201cThe Relationship of Anxiety and Stress With Working Memory Performance in a Large Non-depressed Sample,\u201d Frontiers in Psychology, vol. 10, 2019. [Online]. Available: https://doi.org/10.3389/fpsyg.2019.00004\\n\\n[21] E. Rose and K. Ebmeier, \u201cPattern of impaired working memory during major depression,\u201d Journal of Affective Disorders, vol. 90, no. 2, pp. 149\u2013161, 2006.\\n\\n[22] Thymia. [Online]. Available: https://thymia.ai\\n\\n[23] A. Woods, C. Velasco, C. Levitan, X. Wan, and C. Spence, \u201cConducting perception research over the internet: a tutorial review,\u201d PeerJ, vol. 3:e1058, 2015. [Online]. Available: https://doi.org/10.7717/peerj.1058\\n\\n[24] K. Kroenke, T. W. Strine, R. L. Spitzer, J. B. Williams, J. T. Berry, and A. H. Mokdad, \u201cThe phq-8 as a measure of current depression in the general population,\u201d Journal of Affective Disorders, vol. 114, no. 1-3, pp. 163\u2013173, 2009.\\n\\n[25] P. D. Gajewski, E. Hanisch, M. Falkenstein, S. Th\u00f6nes, and E. Wascher, \u201cWhat does the n-back task measure as we get older? relations between working-memory measures and other cognitive functions across the lifespan,\u201d Frontiers in Psychology, vol. 9, 2018. [Online]. Available: https://doi.org/10.3389/fpsyg.2018.02208\\n\\n[26] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr\u00e9, C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan et al., \u201cThe Geneva minimalistic acoustic parameter set (GeMAPS) for voice research and affective computing,\u201d IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2015.\\n\\n[27] F. Eyben, M. W\u00f6llmer, and B. Schuller, \u201cOpensmile: the munich versatile and fast open-source audio feature extractor,\u201d in Proc. 18th ACM international conference on Multimedia. Firenze Italy: ACM, 2010, pp. 1459\u20131462.\\n\\n[28] Y. Jadoul, B. Thompson, and B. de Boer, \u201cIntroducing Parselmouth: A Python interface to Praat,\u201d Journal of Phonetics, vol. 71, pp. 1\u201315, 2018.\\n\\n[29] P. Boersma, \u201cPraat, a system for doing phonetics by computer,\u201d Glot. Int., vol. 5, no. 9, pp. 341\u2013345, 2001.\\n\\n[30] Amazon transcribe. [Online]. Available: https://aws.amazon.com/ja/jp/transcribe/\\n\\n[31] M. Honnibal and I. Montani, \u201cspaCy 2: Natural language understanding with Bloom embeddings, convolutional neural networks and incremental parsing,\u201d 2017. [Online]. Available: https://spacy.io/\\n\\n[32] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, B. Thirion, O. Grisel, M. Blondel, P. Prettenhofer, R. Weiss, V. Dubourg, J. Vanderplas, A. Passos, D. Cournapeau, M. Brucher, M. Perrot, and E. Duchesnay, \u201cScikit-learn: Machine Learning in Python,\u201d Journal of Machine Learning Research, vol. 12, pp. 2825\u20132830, 2011.\"}"}
