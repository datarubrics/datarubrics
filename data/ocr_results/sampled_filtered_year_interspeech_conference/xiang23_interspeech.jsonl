{"id": "xiang23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] R. C. Streijl, S. Winkler, and D. S. Hands, \\\"Mean opinion score (mos) revisited: Methods and applications, limitations and alternatives,\\\" Multimedia Syst., vol. 22, no. 2, p. 213\u2013227, mar 2016. [Online]. Available: https://doi.org/10.1007/s00530-014-0446-1\\n\\n[2] A. Rix, J. Beerends, M. Hollier, and A. Hekstra, \\\"Perceptual evaluation of speech quality (pesq)-a new method for speech quality assessment of telephone networks and codecs,\\\" in 2001 IEEE International Conference on Acoustics, Speech, and Signal Processing. Proceedings (Cat. No.01CH37221), vol. 2, 2001, pp. 749\u2013752 vol.2.\\n\\n[3] A. W. Rix, M. P. Hollier, J. G. Beerends, and A. P. Hekstra, \\\"Pesq-the new itu standard for end-to-end speech quality assessment,\\\" Journal of the Audio Engineering Society, September 2000.\\n\\n[4] N. R. French and J. C. Steinberg, \\\"Factors governing the intelligibility of speech sounds,\\\" The Journal of the Acoustical Society of America, vol. 19, no. 1, pp. 90\u2013119, 1947.\\n\\n[5] H. J. M. Steeneken and T. Houtgast, \\\"A physical method for measuring speech-transmission quality,\\\" The Journal of the Acoustical Society of America, vol. 67, no. 1, pp. 318\u2013326, 1980. [Online]. Available: https://doi.org/10.1121/1.384464\\n\\n[6] H. J. M. Steeneken and T. Houtgast, \\\"Some applications of the Speech Transmission Index (STI) in auditoria,\\\" p. 11433, Feb. 1982.\\n\\n[7] J. Jensen and C. H. Taal, \\\"An algorithm for predicting the intelligibility of speech masked by modulated noise maskers,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 24, no. 11, pp. 2009\u20132022, 2016.\\n\\n[8] J. Ma, Y. Hu, and P. C. Loizou, \\\"Objective measures for predicting speech intelligibility in noisy conditions based on new band-importance functions,\\\" The Journal of the Acoustical Society of America, vol. 125, no. 5, pp. 3387\u20133405, 2009.\\n\\n[9] Sound system equipment - Part 16: Objective rating of speech intelligibility by speech transmission index, IEC 60268-16:2020, 2020.\\n\\n[10] T. Houtgast and H. J. M. Steeneken, \\\"The modulation transfer function in room acoustics as a predictor of speech intelligibility,\\\" Journal of the Acoustical Society of America, vol. 54, pp. 557\u2013557, 1973.\\n\\n[11] P. Seetharaman, G. J. Mysore, P. Smaragdis, and B. Pardo, \\\"Blind estimation of the speech transmission index for speech quality prediction,\\\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 591\u2013595.\\n\\n[12] S. Duangpummet, J. Karnjana, W. Kongprawechnon, and M. Unoki, \\\"A robust method for blindly estimating speech transmission index using convolutional neural network with temporal amplitude envelope,\\\" in 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC), 2019, pp. 1208\u20131214.\\n\\n[13] \u2014\u2014, \\\"Blind estimation of speech transmission index and room acoustic parameters based on the extended model of room impulse response,\\\" Applied Acoustics, vol. 185, p. 108372, 2022.\\n\\n[14] P. S. L\u00f3pez, P. Callens, and M. Cernak, \\\"A universal deep room acoustics estimator,\\\" in 2021 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2021, pp. 356\u2013360.\\n\\n[15] T.-Y. Lin, P. Doll\u00e1r, R. Girshick, K. He, B. Hariharan, and S. Belongie, \\\"Feature pyramid networks for object detection,\\\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017, pp. 2117\u20132125.\\n\\n[16] M. R. Schroeder, \\\"Modulation transfer functions: Definition and measurement,\\\" Acta Acustica united with Acustica, vol. 49, no. 3, pp. 179\u2013182, 1981.\\n\\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \\\"Attention is all you need,\\\" Advances in neural information processing systems, vol. 30, 2017.\\n\\n[18] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, \\\"Image quality assessment: from error visibility to structural similarity,\\\" IEEE transactions on image processing, vol. 13, no. 4, pp. 600\u2013612, 2004.\\n\\n[19] L. Liebel and M. K\u00f6rner, \\\"Auxiliary tasks in multi-task learning,\\\" arXiv preprint arXiv:1805.06334, 2018.\\n\\n[20] C. Warren, \\\"Echothief impulse response library,\\\" www.echothief.com/downloads/, accessed: 2022-06-06.\\n\\n[21] J. S. Garofolo, \\\"Timit acoustic phonetic continuous speech corpus,\\\" Linguistic Data Consortium, 1993.\\n\\n[22] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. Fiscus, and D. S. Pallett, \\\"Darpa timit acoustic-phonetic continuous speech corpus cd-rom. nist speech disc 1-1.1,\\\" NASA STI/Recon technical report n, vol. 93, p. 27403, 1993.\\n\\n[23] A. Mesaros, T. Heittola, and T. Virtanen, \\\"Tut database for acoustic scene classification and sound event detection,\\\" in 2016 24th European Signal Processing Conference (EUSIPCO). IEEE, 2016, pp. 1128\u20131132.\\n\\n[24] \u2014\u2014, \\\"Tut database for acoustic scene classification and sound event detection,\\\" in 2016 24th European Signal Processing Conference (EUSIPCO). IEEE, 2016, pp. 1128\u20131132.\"}"}
{"id": "xiang23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Estimate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network\\n\\nBajian Xiang\u2020, Hongkun Liu\u2020, Zedong Wu, Su Shen, Xiangdong Zhang\\nYousonic Technology, Beijing, China\\n{bajian.xiang, hongkun.liu, zedong.wu, su.shen, xiangdong.zhang}@yousonic.tech\\n\\nAbstract\\nThe Speech Transmission Index (STI) is a crucial metric for evaluating speech intelligibility, but its standard measurement method is too complicated for real-time applications. Though recently proposed deep learning based STI estimation schemes can effectively address the problem, existing methods still fall short of covering all possible STI scenarios. This paper presents eSTImate: an end-to-end deep learning system for real-time STI blind estimation that integrates the tasks of STI estimation and speech enhancement through a feature pyramid auxiliary learning architecture and incorporates multi-head attention mechanisms. The proposed model demonstrates the performance of state-of-the-art, achieving a low mean absolute error of 0.016 and root mean square error of 0.021 on the constructed dataset that covers the whole range of STI, highlighting its potential to provide accurate and consistent real-time STI estimation across diverse real-world scenarios.\\n\\nIndex Terms: speech transmission index estimation, speech enhancement, deep neural networks, auxiliary learning\\n\\n1. Introduction\\nAs speech signal propagates, it is more or less modified by acoustic factors in the surrounding environment, such as ambient noise and reverberation characteristics, resulting in the degradation of speech quality. To assess the speech intelligibility at the listener's end, a range of evaluation methods have been suggested. Subjective evaluations, including MOS [1], involve human listeners' opinions about the speech quality. These methods remain as valuable tools in a wide range of contexts despite limitations like individual differences. Objective evaluations utilize algorithms and mathematical models to evaluate speech quality, including PESQ [2, 3], AI [4], STI [5, 6] and STOI [7]. Notably, the Speech Transmission Index (STI) has been demonstrated to exhibit strong correlation with speech intelligibility [8].\\n\\nSTI is quantified on a scale of 0 to 1, with higher values indicating better speech intelligibility. To provide a more specific and standardized evaluation, STI is divided into standard categories defined by five grades: bad, poor, fair, good, and excellent [9]. The differentiation criteria for these grades are set at 0.30, 0.45, 0.60, and 0.75, respectively.\\n\\nThe standard direct measurement method of STI employs 7-octave bands and 14 modulation frequencies to sequentially generate modulation test signals. This approach typically takes the handheld devices about 15 minutes for complete measurement and makes it difficult to be adopted in practice. As an alternative, the indirect method derives STI from the Room Impulse Response (RIR), offering a simpler way of computation [10]. However, the acquisition of the the RIR as a computational condition during real-time measurement is exceedingly challenging in practical scenario.\\n\\nDue to recent advancements in neural networks, novel deep-learning-based algorithms have been developed for STI measurement with the potential to overcome the limitations of traditional measurement methods. Seetharaman et al. proposed a fully Convolutional Neural Network (CNN) to estimate the STI value directly from the input PCM audio [11]. While this network exhibited comparable performance to human in discriminating STI conditions, the range of STI values predicted by the model was constrained by the distribution of the underlying dataset, with a lower bound about 0.60. Additionally, Duangpummet et al. proposed a scheme that incorporates the Temporal Amplitude Envelope (TAE) into a CNN [12]. Despite the broadened range of measurements, a reduction in accuracy is observable when compared to the previous study. Afterwards, the authors employed an extended RIR model into a similar network structure to enable the simultaneous prediction of the STI and 5 other room acoustic parameters, yielding improved accuracy compared with their previous method [13]. To date, Lopez et al. have presented a deep Convolutional Recurrent Neural Network (CRNN) for the blind estimation of the STI and 5 other room acoustic parameters, achieving a competitive result in STI in terms of both accuracy and estimation range [14].\\n\\nHowever, the current methods are still unable to effectively distinguish between the lowest grades of STI, namely bad and poor scenario, as they fail to provide an accurate measurement for STI values below 0.30. Moreover, predicting STI simultaneously with other room parameters may not be a practical approach, as STI is not solely related to RIR but also affected by noise. Typically, estimating STI with other room parameters may impede real-time STI prediction, for STI is greater in temporal volatility than other room parameters.\\n\\nThis paper presents a novel auxiliary learning framework that integrates the tasks of blind STI estimation and Speech Enhancement (SE) through a Feature Pyramid Network (FPN) architecture [15]. To make better use of the correlations between speech sequences, a multi-head attention mechanism is incorporated into the network. To meet all possible needs of real measurement scenarios, a dataset covering the full range of STI is constructed. The proposed network exhibits SOTA performance, as evidenced by its achievement of a low Mean Absolute Error (MAE) of 0.016 and Root Mean Square Error (RMSE) of 0.021 on the dataset we built, showing the potential to estimate precise and consistent real-time STI estimation across diverse real-world scenarios.\"}"}
{"id": "xiang23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Problem formulation\\nIn the time domain, an observed speech signal $y(t)$ captured by a microphone is a combination of the anechoic speech signal $x(t)$, the RIR $h(t)$, and the ambient noise $n(t)$, mathematically presented as\\n\\n$$y(t) = x(t) * h(t) + n(t),$$\\n\\nwhere $*$ denotes convolution operation. Typically, the standard indirect methods require all these 3 components to calculate STI. In this process, signal-to-noise ratio (SNR) is initially calculated for each octave band as\\n\\n$$\\\\rho_k = 10 \\\\times \\\\log_{10} \\\\frac{\\\\int_{0}^{\\\\infty} x^2_k(t) dt}{\\\\int_{0}^{\\\\infty} n^2_k(t) dt},$$\\n\\nwhere $\\\\rho_k$, $x_k$ and $n_k$ are the SNR, original signal and ambient noise in octave band $k$, respectively. Afterwards, the Schroeder Method [16] is used to calculate the Modulation Transfer Function (MTF) with adjustment for noise in each octave band $k$ and modulation frequency $f_m$, as\\n\\n$$m_k(f_m) = \\\\frac{|\\\\int_{0}^{\\\\infty} h^2_k(t) e^{-j2\\\\pi f_m t} dt|}{\\\\int_{0}^{\\\\infty} h^2_k(t) dt \\\\left[1 + 10^{-\\\\rho_k/10}\\\\right]^{-1}},$$\\n\\nwhere $h_k(t)$ is the RIR in octave band $k$. Then the effective signal-to-noise ratio $N_{k,f_m}$ is calculated using MTF as\\n\\n$$N_{k,f_m} = 10 \\\\times \\\\log_{10} \\\\left|m_k(f_m)\\\\right|,$$\\n\\nwhere $N_{k,f_m}$ should be limited between $-15$ and $+15$. Next, by averaging over the 14 modulation frequencies, the Modulation Transfer Index (MTI) can be calculated as\\n\\n$$M_k = \\\\frac{1}{14} \\\\sum_{m=1}^{14} N_{k,f_m} + 15,\"\"}"}
{"id": "xiang23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"attention module, and the resulting features are added to $C$ via residual connections and layer normalization. The features processed by the attention module are then mapped to the target domain of STI via a 5-layer fully connected network to obtain the STI prediction for each frame. The first 4 layers of the fully connected network contain linear layers with a halving feature dimension, ReLU activation, and batch normalization, with the features eventually reduced to $64$. Dropout is also added to prevent overfitting. The last layer maps the feature directly to the STI prediction.\\n\\n3.3. Speech enhancement decoding\\n\\nThe purpose of introducing speech enhancement as an auxiliary task is to help the network better understand the differences between clean and noisy speech during training. Specifically, during backpropagation, the speech enhancement auxiliary task adjusts the shared encoder to improve the overall generalization performance of the model. It should be noted that in the actual use of the eSTImate framework, we discard the speech enhancement decoder module and only use encoding to improve the system's prediction speed.\\n\\nThe speech enhancement decoder is designed to extract high-level semantic information from the encoder and perform bottom-up sampling. It first reduces the channel dimension of the lowest-level feature map obtained by down-sampling from the encoder. Then, it adds the up-sampled and involved lowest-level feature map to the corresponding high-level feature map to generate a new set of feature maps. Finally, all the feature maps generated during the up-sampling process are fused and up-sampled to the same size as the input spectrogram, forming the final output of estimated clean speech $\\\\hat{S} = (\\\\hat{s}_1, \\\\hat{s}_2, \\\\cdots, \\\\hat{s}_n)$.\\n\\n3.4. Training objective\\n\\nThe Mean Square Error (MSE) between the predicted STI and the ground truth STI, as well as the Structural Similarity Index (SSIM) \\\\([18]\\\\) between the predicted clean spectrogram and the ground truth clean spectrogram, are used as the training objectives, as\\n\\n$$L = \\\\alpha \\\\times L_{MSE} (\\\\hat{STI}, STI) + \\\\beta \\\\times L_{SSIM} (\\\\hat{S}, S) \\\\quad (7)$$\\n\\nwhere $\\\\alpha$ and $\\\\beta$ are weights for the MSE and SSIM losses, respectively. The MSE loss measures the average squared difference between the predicted and true STI values, while the SSIM loss assesses the structural similarity between the predicted and true clean spectrograms, taking into account luminance, contrast, and structural information.\\n\\nDuring the training process, an automatically weighted loss technique for multi-task learning framework is used to guide the selection of $\\\\alpha$ and $\\\\beta$ \\\\([19]\\\\).\\n\\n4. Experiments\\n\\n4.1. Dataset\\n\\nThe dataset built for training the proposed eSTImate system consists of 592,928 synthesized single-channel noisy audio samples, with a sampling rate of 16 kHz. Each utterance has a duration of 20 s to 30 s and is accompanied by its corresponding real-time STI ground truth value, as well as a clean audio aligned with the noisy audio. The proposed dataset covers the entire range of STI values from 0 to 1, and the distribution is visualized in Figure 2.\\n\\nA total of 108 real-world RIRs are used in our experiments. Among them, 32 RIRs were recorded in different offices in China using a standard recording method, while the remaining 76 RIRs were obtained from the EchoThief Impulse Response Library \\\\([20]\\\\). The selected RIRs cover a range of Reverberation Time (T60) from 0.4 to 1.0, with 18 RIRs at each 0.1 interval, measured using the Dirac tool from Acoustics Engineering. For each interval, we randomly selected 2 RIRs to generate test data.\\n\\nThe TIMIT dataset \\\\([21]\\\\) was used as the clean speech corpus, which contains recordings of 630 individuals from 8 major dialect regions in the United States each speaking 10 standard sentences \\\\([22]\\\\). The noise used was primarily sourced from the TUT Acoustic Scenes dataset \\\\([23]\\\\), which consists of audio recordings of various street environments, including 6 different categories such as cars, braking sounds, and pedestrians \\\\([24]\\\\). Additionally, we added pink noise and babble noise to our noise library. For each reverb speech, we added 3 types of noise randomly, with each type of noise added at 3 different SNR levels of 0dB, 6dB, and 20dB, respectively.\\n\\n4.2. Experimental Details\\n\\nIn training, the batch size is 7. The initial learning rate is $10^{-5}$, halved every 10 epochs. The inference time for each 4 s frame is 0.1 s and 0.047 s respectively with AMD Ryzen5 5600H CPU and 24G GeForce RTX 3090 GPU.\\n\\n4.3. STI evaluation\\n\\nThe performance of the proposed eSTImate algorithm was evaluated on a test set with STI ground truth values ranging from 0.21 to 0.95. Data analysis was conducted at intervals of 0.01, and the results are shown in Figure 3. The red dashed line in the figure represents the ideal prediction, while the blue solid line represents the mean prediction of the eSTImate model on the test set, and the blue shaded area represents the standard deviation of the prediction. The results demonstrate a strong fitting performance with very small prediction errors, achieving an RMSE of 0.021 and an MAE of 0.016.\\n\\nThe model shows the closest prediction results to the ground truth values within the range of STI values from 0.3 to 0.8, with a slight overestimation in the range of 0.2 to 0.3 and underestimation above 0.8. This prediction pattern is strongly related to the distribution of our dataset, where there are relatively few samples with STI values below 0.3 and above 0.8, causing the model's predictions for these ranges to be less accurate. Although there is a slight deviation in the prediction results at both ends of the range, it has little impact on the STI classification task in practical applications. To demonstrate this, we tested the model's classification accuracy of the five standard\"}"}
{"id": "xiang23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4. Ablation study\\n\\nOn the basis of only using Resnet50 as backbone, we explored the influences of FPN structure, Multi-head Attention Block (MAB), and Automatic Weight Parameter Learning (AWPL) on STI estimation and noise-free spectrogram prediction respectively. The results are shown in Table 1 and Figure 4.\\n\\nBefore the introduction of AWPL, we initially set the weight parameters $\\\\alpha = 1$ and $\\\\beta = 0$ for FPN-based models. The results indicate that incorporating the FPN structure and auxiliary task led to a notable improvement on the STI prediction task, with MAE and RMSE enhanced by 14% and 16%, respectively.\\n\\nAfter introducing the attention module, the STI prediction metrics remain relatively unchanged. Figure 4(c) shows that this is due to a relatively larger bias in the predictions and a reduction in variance, indicating an improvement in the model's generalization ability. In addition, the performance of the auxiliary speech enhancement task improved, suggesting that the preset parameters might have favored the auxiliary task.\\n\\nWith the introduction of AWPL, the performance of STI estimation is further improved, resulting in a 29% and 25% increase in MAE and RMSE, respectively, compared to the baseline. The model still maintains a small variance and further reduces the prediction bias for STI values less than 0.3 and greater than 0.8, as shown in Figure 4(d), achieving the best overall performance in the ablation experiment.\\n\\n4.5. Framework comparison\\n\\nWe conducted a comparison of the proposed eSTImate method with previous methods as described in Section 1. Table 2 presents the results, where RMSE is the primary evaluation metric. It should be noted that the STI distribution range differs in previous studies due to the use of different datasets and data generation methods, and the validation range is also listed in the comparison table to measure the coverage and overall capabilities of the models.\\n\\nIt is evident that previous STI framework cannot distinguish between the Poor and Bad levels in the 5-level standard STI classification, while our proposed model can effectively differentiate between all the 5 levels and achieves the lowest RMSE. The results demonstrate that the proposed eSTImate outperforms the state-of-the-art method by 36%, not only covering a wider range of STI predictions but also achieving smaller deviations.\"}"}
