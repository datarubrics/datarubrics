{"id": "afonja24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Performant ASR Models for Medical Entities in Accented Speech\\nTejumade Afonja\u20201, Tobi Olatunji\u20202,\u2217, Sewade Ogun3,\u2217, Naome A. Etori4,\u2217, Abraham Owodunni2,\u2217, Moshood Yekini5,\u2217\\n\\n1CISPA Helmholtz Center for Information Security\\n2Intron Health\\n3Universit\u00e9 de Lorraine, CNRS, Inria, LORIA, F-54000 Nancy, France\\n4University of Minnesota - Twin Cities, USA\\n5African Masters of Machine Intelligence, AIMS/AMMI\\n6AI Saturdays Lagos\\n\\n\u2020Equal contribution.\\n\\nAbstract\\nRecent strides in automatic speech recognition (ASR) have accelerated their application in the medical domain where their performance on accented medical named entities (NE) such as drug names, diagnoses, and lab results, is largely unknown. We rigorously evaluate multiple ASR models on a clinical English dataset of 93 African accents. Our analysis reveals that despite some models achieving low overall word error rates (WER), errors in clinical entities are higher, potentially posing substantial risks to patient safety. To empirically demonstrate this, we extract clinical entities from transcripts, develop a novel algorithm to align ASR predictions with these entities, and compute medical NE Recall, medical WER, and character error rate. Our results show that fine-tuning on accented clinical speech improves medical WER by a wide margin (25-34 % relative), improving their practical applicability in healthcare environments.\\n\\nIndex Terms: speech recognition, medical documentation, medical named-entity recognition, African-accented speech\\n\\n1. Introduction\\nIn recent years, significant advances have been made in accented speech recognition with state-of-the-art (SOTA) automatic speech recognition (ASR) models proficiently transcribing diverse linguistic interactions [1, 2, 3]. However, the effectiveness of these models in clinical or medical settings, where nuanced communication is paramount, remains a challenge [4]. This becomes particularly evident when clinicians with non-western accents document critical medical information using ASR technology. While these SOTA models achieve low word error rates (WER) on general speech, they commonly struggle with accurately transcribing clinical named entities (NE), e.g., see Table 1. The domain-specific nature of clinical documentation introduces a vulnerability that could have severe consequences for patient well-being \u2013 minor inaccuracies in essential elements like drug names, diagnoses, lab results, and lesion measurements (for example, writing renal instead of adrenal, or hyper instead of hypo) could potentially risk patient safety and expose clinicians to avoidable litigation [5]. To empirically expose this problem, we analyze the performance of several SOTA open-source and commercial ASR models on medical NEs (MNEs). Our investigation reveals that current SOTA general-purpose multilingual ASR models while excelling in cross-domain scenarios, exhibit sub-optimal Recall rates for MNEs in accented speech. This limitation diminishes the practical utility of these models in healthcare settings, underscoring the need for specialized solutions.\\n\\n1. We use the terms 'clinical' and 'medical' interchangeably to encompass all aspects related to the practice of medicine and patient care.\\n\\nOur contributions are as follows:\\n1. We benchmark 19 open-source and commercial ASR models on African accented clinical speech highlighting the deficiencies of existing architectures in accurately recognizing accented MNEs.\\n2. We introduce metrics for evaluating medical NER performance in the context of accented speech, including medical named entity Recall, medical WER (M-WER), and medical character error rate (M-CER).\\n3. We develop a novel fuzzy string matching algorithm to better align ASR-predicted noisy NEs to ground truth NEs for more nuanced analysis.\\n4. We demonstrate that supervised fine-tuning substantially enhances accented medical NER, making ASR models more applicable and reliable in real-world clinical scenarios.\\n\\n2. Related Work\\nRecently, authors in [6] highlighted the challenges faced by popular ASR models in recognizing African named entities like persons, locations, and organizations from accented speech. They improved entity WER through data augmentation techniques. In the medical domain, the authors in [7] relied on large language models for correcting medical ASR transcription errors. The work of [8] also used a sequence-to-sequence model to correct clinical ASR errors. Accurately detecting and classifying medical named entities from text has been explored in [9, 10] where [9] identified five key MNEs, and employed deep learning and multi-task learning approaches to extract crucial information from clinical narratives. Also, the authors in [10] developed an ensemble of deep contextual models trained on clinical corpora from PubMed to enhance clinical NE recognition. The work of [11] separately benchmarked clinical speech recognition and entity extraction. Additionally, a production-scalable BiLSTM-CNN-Char framework with pre-trained embedding was designed by [12], which was shown to achieve better performance in speed and prediction compared to the SOTA models and commercial clinical NE recognition solutions. The authors in [13] proposed a clinical task-specific prompting framework that adopts entity definitions, annotation guidelines and samples, and error analysis-based instructions. However, research benchmarking SOTA ASR models on accented medical NE transcription or recognition is still lacking.\\n\\n3. Approach\\nWe investigated this problem by evaluating 19 open-source and commercial ASR systems on a dataset of African-accented clinical speech. A schematic is shown in Figure 1. The dataset,\"}"}
{"id": "afonja24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Predicted sentences from selected ASR models compared to the reference sentence.\\n\\n| Model                  | Sentence                                      |\\n|------------------------|-----------------------------------------------|\\n| Reference sentence     | lungs clear but dim scattered rhonchi nonproductive cough |\\n| Xlsr-53-en             | lungs clear but dim scattered rhonchi non-productive hub |\\n| Whisper-medium         | non-scler but dim-scattered rhonchi non-productive hub |\\n| GCP [Medical]          | lungs clear but dim scattered rhonchi nonproductive |\\n| AWS [Medical] (Primary Care) | lungs clear but deems scattered rhonchi nonproductive |\\n| Whisper-medium-clinical| lungs clear but dim scattered rhonchi nonproductive cough |\\n\\nFigure 1: Methodology: Ground truth transcripts are passed to a commercial medical NER model, and audios are passed through multiple ASR models. Predicted medical entities are extracted using the MedTextAlign algorithm. Metrics are computed over silver NEs.\\n\\n3.1. Data\\nFor this analysis, we leveraged AfriSpeech-200 [4], a 200-hour Pan-African accented English speech corpus for clinical and general domain ASR with 120 accents, and over 2,300 unique speakers from over 10 African countries. The dataset statistics and NE categories are shown in Table 2. Our evaluation focuses on the clinical domain test subset. After filtering out texts lacking sufficient medical context, we retained a total of 2,844 samples, encompassing 93 different accents.\\n\\nTable 2: Dataset splits showing the number of speakers, the number of clips, speech duration, and medical named entity category counts in train/dev/test splits.\\n\\n| Item                                      | Train          | Dev           | Test           |\\n|-------------------------------------------|----------------|---------------|----------------|\\n| Number of speakers                        | 1466           | 247           | 750            |\\n| Duration (in hours)                       | 173.4          | 8.74          | 18.77          |\\n| Number of accents                         | 71             | 45            | 108            |\\n| Number of clips/speaker                   | 39.56          | 13.08         | 8.46           |\\n| Number of speakers/accent                 | 20.65          | 5.49          | 6.94           |\\n| # clinical domain clips (61.80 %)         | 36318          | 1824          | 3623           |\\n| # general domain clips (38.20 %)          | 21682          | 1407          | 2723           |\\n| Medical named entity category count       |                |               |                |\\n| Medication (MED)                          | 4164           | 132           | 276            |\\n| Medical condition (COND)                  | 18804          | 901           | 1414           |\\n| Anatomy (ANA)                             | 13650          | 645           | 927            |\\n| Test treatment procedure (TTP)            | 10713          | 428           | 893            |\\n| Protected health information (PHI)        | 3449           | 105           | 253            |\\n\\n3.2. ASR Models\\nWe evaluated several open-source and commercial (general-purpose and medical) ASR systems covering multiple SOTA ASR architectures shown in Table 4. In addition, we selected two models for fine-tuning based on the model performance reported in [4] and our computational constraints.\\n\\n3.3. Named Entity Extraction\\n3.3.1. Extracting Ground Truth Entities\\nSince the dataset was not annotated with MNEs, we leveraged a commercially available medical NER model, Amazon Comprehend Medical [9], to automatically extract medical NEs from ground truth transcripts. This service has been publicly benchmarked against other NER systems by GigaOm [3] and [12], and has good accuracy in predicting multiple medical NE categories. We call these silver annotations as these are not human annotations.\\n\\n3.3.2. Selected Named Entities\\nWe focused on five key medical named entity categories: medication (MED), medical condition (COND), anatomy (ANA), test treatment procedure (TTP), and protected health information (PHI). These categories cover a wide range of entities, including medication names, dosages, diagnoses, signs, symptoms, and protected health information such as names, addresses, ID numbers, etc. The distribution of entities across these categories for each dataset split is detailed in Table 2.\\n\\n3.3.3. MedTextAlign: Extracting Predicted Named Entities\\nTo evaluate predicted MNEs, a naive method is to use an NER model to identify the ASR-predicted MNEs. However, given that the ASR predictions are noisy, often having different lengths and spellings than ground-truth NEs, and single-word to multi-word entity mismatches exist, these issues pose a challenge for most NER models, making them inadequate. For example, \u201canalgesic properties\u201d is misspelled as \u201canagesic propatis\u201d, \u201cdigoxin\u201d wrongly transcribed as \u201cdikod sin\u201d, and \u201cspironolactone\u201d as \u201cspiro no lactone\u201d. An alignment algorithm was thus needed to better match ground-truth MNEs. Therefore, we developed MedTextAlign, a solution that uses a fuzzy string-matching algorithm to better align the predicted to the ground truth (silver) MNEs.\\n\\nMedTextAlign first tokenizes the predicted transcript, creates a candidate list of unigrams, bigrams, and trigrams from the predicted transcript, then leverages a fuzzy string matching algorithm to compare with each MNE from the ground truth transcript to find the closest match. The fuzzy match, akin to measuring the longest common character subsequence (e.g., in ROUGE-L [14]), produces a score between 0 and 1 for each string pair, with 1 indicating a perfect match, enabling effective\\n\\n2 Amazon Comprehend Medical at https://aws.amazon.com/comprehend/medical/\\n3 GigaOm Clinical NLP Benchmark at https://gigaom.com/report/healthcare-natural-language-processing/\\n4 We used the python SequenceMatcher ratio() method at https://docs.python.org/3/library/difflib.html and set the cut-off threshold to 0.5.\"}"}
{"id": "afonja24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"unlike quinidine, disopyramide does not increase the plasma concentration of digoxin in patients. Unlike quinidine, disopyramide does not increase the plasma concentration of digoxin in patients except for ketamine, the following agents have no analgesic properties and do not cause paralysis or muscle relaxation except for ketamine, because all agents have no analgesic properties and do not cause paralysis or muscle relaxation.\\n\\nTable 4: Performance evaluation of benchmarked models on AfriSpeech-200 clinical domain test dataset. We report WER comparing transcript and prediction, alongside specific metrics including the medical WER (M-WER), the medical CER (M-CER), and the Recall for the different entities, medication (MED), anatomy (ANA), medical condition (COND), test treatment procedure (TTP), and protected health information (PHI).\\n\\n| Model                                      | WER | M-WER | M-CER | MED  | ANA  | COND | TTP  | PHI  |\\n|--------------------------------------------|-----|-------|-------|------|------|------|------|------|\\n| Pretrained                                 | 0.902 | 0.944 | 0.504 | 0.019 | 0.069 | 0.063 | 0.056 | 0.034 |\\n| Wavlm-libri-clean-100h-large               | 0.784 | 0.852 | 0.307 | 0.029 | 0.177 | 0.142 | 0.110 | 0.043 |\\n| Hubert-large-ls960-ft                     | 0.712 | 0.758 | 0.279 | 0.070 | 0.282 | 0.258 | 0.168 | 0.069 |\\n| Hubert-xlarge-ls960-ft                    | 0.722 | 0.770 | 0.275 | 0.067 | 0.284 | 0.262 | 0.166 | 0.075 |\\n| Wav2vec2-large-robust-ft-swbd-300h        | 0.907 | 0.919 | 0.367 | 0.040 | 0.129 | 0.139 | 0.112 | 0.051 |\\n| Wav2vec2-large-960h                       | 0.796 | 0.846 | 0.345 | 0.032 | 0.189 | 0.171 | 0.109 | 0.049 |\\n| Wav2vec2-large-960h-lv60-self             | 0.694 | 0.753 | 0.277 | 0.064 | 0.309 | 0.254 | 0.173 | 0.087 |\\n| Wav2vec2-xls-r-1b-english                 | 0.666 | 0.729 | 0.266 | 0.081 | 0.251 | 0.249 | 0.227 | 0.138 |\\n| Wav2vec2-large-xlsr-53-english            | 0.646 | 0.710 | 0.272 | 0.072 | 0.261 | 0.256 | 0.201 | 0.095 |\\n| Whisper-small-en                          | 0.486 | 0.566 | 0.225 | 0.215 | 0.571 | 0.536 | 0.475 | 0.300 |\\n| Whisper-small                             | 0.451 | 0.567 | 0.216 | 0.235 | 0.566 | 0.541 | 0.486 | 0.301 |\\n| Whisper-medium-en                         | 0.415 | 0.504 | 0.188 | 0.330 | 0.636 | 0.601 | 0.532 | 0.300 |\\n| Whisper-medium                            | 0.392 | 0.487 | 0.174 | 0.343 | 0.680 | 0.627 | 0.568 | 0.335 |\\n| Whisper-large                             | 0.373 | 0.454 | 0.154 | 0.425 | 0.717 | 0.667 | 0.597 | 0.331 |\\n| Commercial                                 |       |       |       |      |      |      |      |      |\\n| Azure                                      | 0.442 | 0.491 | 0.216 | 0.611 | 0.660 | 0.623 | 0.515 | 0.261 |\\n| AWS                                        | 0.540 | 0.660 | 0.249 | 0.212 | 0.523 | 0.485 | 0.382 | 0.246 |\\n| AWS [Medical] (Primary Care)              | 0.516 | 0.553 | 0.218 | 0.572 | 0.644 | 0.567 | 0.494 | 0.204 |\\n| GCP                                        | 0.622 | 0.634 | 0.391 | 0.386 | 0.425 | 0.380 | 0.332 | 0.177 |\\n| GCP [Medical]                              | 0.527 | 0.434 | 0.211 | 0.568 | 0.701 | 0.565 | 0.513 | 0.184 |\\n| Fine-tuned on AfriSpeech-200               |       |       |       |      |      |      |      |      |\\n| Wav2vec2-large-xlsr-53-english-general     | 0.473 | 0.680 | 0.235 | 0.144 | 0.294 | 0.300 | 0.297 | 0.300 |\\n| Wav2vec2-large-xlsr-53-english-both       | 0.308 | 0.467 | 0.135 | 0.451 | 0.658 | 0.576 | 0.567 | 0.356 |\\n| Wav2vec2-large-xlsr-53-english-clinical   | 0.307 | 0.465 | 0.133 | 0.496 | 0.689 | 0.584 | 0.588 | 0.291 |\\n| Whisper-medium-general                    | 0.532 | 0.711 | 0.347 | 0.114 | 0.314 | 0.279 | 0.282 | 0.325 |\\n| Whisper-medium-clinical                   | 0.264 | 0.388 | 0.136 | 0.659 | 0.806 | 0.712 | 0.706 | 0.405 |\\n| Whisper-medium-both                       | 0.241 | 0.365 | 0.118 | 0.731 | 0.822 | 0.725 | 0.726 | 0.490 |\\n\\n3.4. Evaluation Metrics\\n\\nGiven the challenges with ASR alignment, conventional ASR or NER metrics fail to effectively measure the model\u2019s ability to transcribe medical entities. Consequently, to comprehensively assess the performance of these ASR models, we opted for a broad range of metrics that cover various evaluation dimensions.\\n\\n1. Recall: An information retrieval metric that computes the proportion of recovered correct (exact match) entities in the prediction. Precision and F1 score were not computed because they are overly sensitive to ASR noise or errors. Higher Recall is better.\\n\\n2. Word error rate (WER): a word-level metric that evaluates insertions, deletions, and substitutions in the predicted sequence. Lower is better.\\n\\n3. Medical WER (M-WER): WER computed between the ground truth MNEs and their aligned MNEs in the prediction alone. This isolates WER on MNEs of interest while ignoring all other words. All ground truth MNEs in each sample are concatenated with intervening spaces. The MNEs recovered by MedTextAlign are also concatenated in the same way. WER is then calculated between resulting sequences. Lower is better.\\n\\n4. Medical CER (M-CER): Similar to medical WER, but at the character level. M-CER measures the severity of ASR mis-spellings. Lower is better.\\n\\nMatching of nearly correct spellings in the transcript, e.g., matching wrongly spelled \u201cquinidan\u201d or \u201cdisopyramid\u201d (see Table 3). Although not perfect, this strategy proved to be very effective. In Table 3, we underline approximate entity matches and put in bold-face exact matches given by the Wavlm-libri-clean-100h-base model.\"}"}
{"id": "afonja24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\n4.1. Benchmarking\\nWe compared SOTA open-source pre-trained ASR models: Whisper [1], Wav2vec2 [3], XLSR [15], Hubert [2], WavLM [16], alongside commercial clinical and non-clinical ASR systems; Azure [17], AWS [18], and GCP [19]. For all open-source pre-trained ASR models, we refer readers to read their respective papers for details on pretraining corpora, model architecture, and hyperparameters. In addition, we used the Hugging Face transformer library [20] for inference. For each model, we show results on the AfriSpeech clinical domain test set in Table 4.\\n\\n4.2. Fine-tuning\\nFor the fine-tuning experiments, we fine-tuned the ASR models on three domains: (1) general domain (21,682 clips), (2) clinical domain (36,318 clips), and (3) both domains (58,000 clips). We fine-tuned the models using each domain's training set and tested on the clinical domain test set to investigate the effect of out-of-domain accented data on model performance. Additionally, based on the benchmark results in Table 4 and GPU memory constraints, two top performing open-source model architectures, Whisper-medium [1] and Wav2vec-large-xlsr-53 (XLSR-53) [21], were selected for fine-tuning. XLSR-53 (378.9 M parameters) is an encoder-decoder architecture with a convolution-based feature extractor pre-trained using a self-supervised objective. Whisper-medium (789.9 M parameters) is a decoder-only multi-task architecture trained on over 680,000 hours of multilingual and multitask data using a weak supervision objective.\\nEach model was fine-tuned using mixed-precision training, with the AdamW optimizer [22], a batch size of 16 for 10 epochs, using a linear learning rate decay after a warmup over the first 10% of iterations. Learning rates of $10^{-4}$ and $2 \\\\times 10^{-4}$ were used for the XLSR-53 and Whisper models respectively. The XLSR-53 models were trained on a single Tesla T4 GPU with 16GB GPU memory while Whisper was trained on a RTX8000 GPU with 48GB GPU memory. In general, fine-tuning took between 24-48 hours for each model.\\n\\n5. Results and Discussion\\nBenchmarking results on 19 open-source and commercial ASR systems, as well as our fine-tuning experiments, are presented in Table 4.\\n\\n5.1. Large multilingual models with web-scale training data generalize better\\nThe overarching trend favors ASR models like Whisper [1] that were trained on vast amounts of multilingual web-scale speech data. Their data diversity and pretraining objective confer better generalization capabilities to accented speech and the clinical domain, as evidenced by their lower WER and higher Recall on MNEs, outperforming ASR models trained on monolingual data by a wide margin.\\n\\n5.2. WER vs medical WER\\nAs consistently observed across all pre-trained model families, M-WER was relatively worse overall by 4-51% than WER, empirically validating the performance gap on medical NEs. The only exception was GCP [Medical] where its M-WER was better, demonstrating a trade-off in domain-specific fine-tuning.\\n\\n5.3. Relative Performance across Entity Categories\\nAlthough Whisper-large outperformed other open- and closed-source models on WER, its MNE Recall was still poor overall with 42% for medications (MED), 33% for protected health information (PHI), 59% for test treatment procedure (TTP), and 67% for medical conditions (COND), falling far below its practical applicability in real-world clinical scenarios [23] due to the extent of required editing. Also, its 71% Recall for anatomy (ANA) may have resulted from the relative abundance of body parts like leg, brain, heart, liver, etc., in web-scale text.\\n\\n5.4. Medical CER: Exact vs Approximate Match\\nAs seen in Table 3, medical WER sometimes unfairly penalized even the most minuscule ASR errors, e.g., quinidan vs quinidine, especially with multi-word entities, e.g., \u201cmuscle relaxation\u201d vs \u201cmozul relaxition\u201d, treating minor and severe ASR errors alike, a phenomenon that was not investigated in most prior works [7]. M-CER is complimentary in this regard, allowing us to better evaluate the severity of ASR misspellings. Also, lower M-CER helps to select the better of two ASR models with comparable WERs, like GCP [Medical] and AWS [Medical].\\n\\n5.5. Fine-tuning Results on General vs Clinical Domain\\nThe fine-tuned models significantly improved on MNE Recall, WER, medical WER, and medical CER, as the models were better adapted to accented speech in the clinical domain. However, this is not a silver bullet. Our results show that fine-tuning the ASR models on the general domain accented speech alone, in fact, worsens the WER, M-WER, M-CER, and Recall on clinical speech. XLSR-53 fine-tuned on the clinical subset reduced WER by 35%, M-WER by 31%, and M-CER by 43% relative to the general domain. Fine-tuning Whisper-medium on both domains yielded the best results overall, improving WER by 54%, M-WER by 48%, and M-CER by 65% relative to finetuning on the general domain only, suggesting that the ASR models still benefit from exposure to general-domain accented speech.\\n\\n6. Limitations\\nASR models, while beneficial, can risk patient safety and expose clinicians to liability through minor errors like mistranscribed drug names, doses, or diagnoses. Verification steps and spell-checkers can be integrated into the workflow to mitigate potential errors. Using automatically generated named entities instead of human annotation also introduces errors in entity identification. Automated systems can serve as initial annotation agents, with their outputs refined by domain experts. Lastly, the AfriSpeech-200 dataset often includes medical abbreviations (e.g., \u201cPt\u201d for \u201cPatient\u201d), therefore, transcripts should be normalized for more accurate benchmarking.\\n\\n7. Conclusion\\nThis work highlights a noticeable disparity between general and medical WER for many SOTA ASR models, pointing to challenges in accurately recognizing accented medical named entities. Fine-tuning these models with domain-specific data was beneficial in addressing some of these issues, indicating that tailored fine-tuning can enhance ASR performance in healthcare.\"}"}
{"id": "afonja24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Acknowledgements\\n\\nWe appreciate the invaluable support from Intron Health for contributing the dataset and compute for experiments. Tejumade Afonja is partially supported by ELSA \u2013 European Lighthouse on Secure and Safe AI funded by the European Union under grant agreement No. 101070617. We appreciate the support provided by the BioRAMP researchers, whose collaboration and insights have been fundamental to our research.\\n\\n9. References\\n\\n[1] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[2] W.-N. Hsu, B. Bolte, Y. -H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHuBERT: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[3] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[4] T. Olatunji, T. Afonja, A. Yadavalli, C. C. Emezue, S. Singh, B. F. Dossou, J. Osuchukwu, S. Osei, A. L. Tonja, N. Etori et al., \u201cAfriSpeech-200: Pan-African accented speech dataset for clinical and general domain ASR,\u201d Transactions of the Association for Computational Linguistics, vol. 11, pp. 1669\u20131685, 2023.\\n\\n[5] S. Ajami, \u201cUse of speech-to-text technology for documentation by healthcare providers,\u201d The National medical journal of India, vol. 29, no. 3, p. 148, 2016.\\n\\n[6] T. Olatunji, T. Afonja, B. F. P. Dossou, A. L. Tonja, C. C. Emezue, A. M. Rufai, and S. Singh, \u201cAfriNames: Most ASR Models \u2018Butcher\u2019 African Names,\u201d in Proc. Interspeech, 2023, pp. 5077\u20135081.\\n\\n[7] A. Adedeji, S. Joshi, and B. Doohan, \u201cThe sound of healthcare: Improving medical transcription ASR accuracy with large language models,\u201d 2024.\\n\\n[8] Y. Jiang and C. Poellabauer, \u201cA sequence-to-sequence based error correction model for medical automatic speech recognition,\u201d in International Conference on Bioinformatics and Biomedicine (BIBM). IEEE, 2021, pp. 3029\u20133035.\\n\\n[9] P. Bhatia, B. Celikkaya, M. Khalilia, and S. Senthivel, \u201cComprehend Medical: A named entity recognition and relationship extraction web service,\u201d in International Conference On Machine Learning And Applications (ICMLA). IEEE, 2019, pp. 1844\u20131851.\\n\\n[10] Y. Zhou, C. Ju, J. H. Caufield, K. Shih, C. Chen, Y. Sun, K.-W. Chang, P. Ping, and W. Wang, \u201cClinical named entity recognition using contextualized token representations,\u201d arXiv preprint arXiv:2106.12608, 2021.\\n\\n[11] H. Suominen, L. Zhou, L. Hanlen, G. Ferraro et al., \u201cBenchmarking clinical speech recognition and information extraction: new data, methods, and evaluations,\u201d JMIR medical informatics, vol. 3, no. 2, p. e4321, 2015.\\n\\n[12] V. Kocaman and D. Talby, \u201cAccurate clinical and biomedical named entity recognition at scale,\u201d Software Impacts, vol. 13, p. 100373, 2022.\\n\\n[13] Y. Hu, Q. Chen, J. Du, X. Peng, V. K. Keloth, X. Zuo, Y. Zhou, Z. Li, X. Jiang, Z. Lu et al., \u201cImproving large language models for clinical named entity recognition via prompt engineering,\u201d Journal of the American Medical Informatics Association, p. ocad259, 2024.\\n\\n[14] C.-Y. Lin, \u201cROUGE: A package for automatic evaluation of summaries,\u201d in Text Summarization Branches Out. Barcelona, Spain: Association for Computational Linguistics, Jul. 2004, pp. 74\u201381. [Online]. Available: https://aclanthology.org/W04-1013\\n\\n[15] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. M. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised cross-lingual speech representation learning at scale,\u201d in Proc. Interspeech, 2022.\\n\\n[16] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavLM: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\\n\\n[17] \u201cCloud computing services: Microsoft Azure. Cloud Computing Services | Microsoft Azure. (n.d.).\u201d https://azure.microsoft.com/, accessed: 2024-03-01.\\n\\n[18] \u201cCloud Computing Services - Amazon Web Services (AWS),\u201d http://aws.amazon.com, accessed: 2024-03-01.\\n\\n[19] \u201cCloud Computing Services | Google Cloud,\u201d https://cloud.google.com/, accessed: 2024-03-01.\\n\\n[20] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. L. Scao, S. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: https://www.aclweb.org/anthology/2020.emnlp-demos.6\\n\\n[21] J. Grosman, \u201cFine-tuned XLSR-53 large model for speech recognition in English,\u201d https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-english, 2021.\\n\\n[22] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in International Conference on Learning Representations, 2018.\\n\\n[23] E. Luchies, M. Spruit, and M. Askari, \u201cSpeech technology in Dutch health care: A qualitative study.\u201d in HEALTHINF, 2018, pp. 339\u2013348.\"}"}
