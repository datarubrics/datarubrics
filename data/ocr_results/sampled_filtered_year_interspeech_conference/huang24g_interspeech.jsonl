{"id": "huang24g_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, and A. Zisserman, \\\"Spot the conversation: Speaker diarisation in the wild,\\\" in Inter-speech, 2020.\\n\\n[2] Q. Wang, C. Downey, L. Wan, P. A. Mansfield, and I. L. Moreno, \\\"Speaker diarization with lstm,\\\" in ICASSP, 2018.\\n\\n[3] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \\\"Speaker recognition for multi-speaker conversations using x-vectors,\\\" in ICASSP, 2019.\\n\\n[4] K. Hoover, S. Chaudhuri, C. Pantofaru, M. Slaney, and I. Sturdy, \\\"Putting a face to the voice: Fusing audio and visual signals across a video to determine speakers,\\\" arXiv preprint arXiv:1706.00079, 2017.\\n\\n[5] W. Xie, A. Nagrani, J. S. Chung, and A. Zisserman, \\\"Utterance-level aggregation for speaker recognition in the wild,\\\" in ICASSP, 2019.\\n\\n[6] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \\\"X-vectors: Robust dnn embeddings for speaker recognition,\\\" in ICASSP, 2018.\\n\\n[7] T. Ochiai, M. Delcroix, K. Kinoshita, A. Ogawa, and T. Nakatani, \\\"Multimodal speakerbeam: Single channel target speech extraction with audio-visual speaker clues,\\\" in Interspeech, 2019.\\n\\n[8] Z. Pan, X. Qian, and H. Li, \\\"Speaker extraction with co-speech gestures cue,\\\" IEEE Signal Processing Letters, pp. 1467\u20131471, 2022.\\n\\n[9] X. Qian, A. Brutti, O. Lanz, M. Omologo, and A. Cavallaro, \\\"Audio-visual tracking of concurrent speakers,\\\" IEEE Transactions on Multimedia, p. 942\u2013954, 2022.\\n\\n[10] X. Qian, M. Madhavi, Z. Pan, J. Wang, and H. Li, \\\"Multi-target doa estimation with an audio-visual fusion mechanism,\\\" in ICASSP, 2021.\\n\\n[11] C. Schymura, T. Ochiai, M. Delcroix, K. Kinoshita, T. Nakatani, S. Araki, and D. Kolossa, \\\"A dynamic stream weight backprop kalman filter for audiovisual speaker tracking,\\\" in ICASSP, 2020.\\n\\n[12] X. Zheng, C. Zhang, and P. Woodland, \\\"Tandem multitask training of speaker diarisation and speech recognition for meeting transcription,\\\" arXiv preprint arXiv:2207.03852, 2022.\\n\\n[13] T. Liu, R. K. Das, K. A. Lee, and H. Li, \\\"Mfa: Tdnn with multi-scale frequency-channel attention for text-independent speaker verification with short utterances,\\\" in ICASSP, 2022.\\n\\n[14] F. Yu, S. Zhang, Y. Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo, Z. Yan, B. Ma, X. Xu, and H. Bu, \\\"M2met: The icassp 2022 multi-channel multi-party meeting transcription challenge,\\\" in ICASSP, 2022.\\n\\n[15] M. Li, L. Zhang, H. Ji, and R. J. Radke, \\\"Keep meeting summaries on topic: Abstractive multi-modal meeting summarization,\\\" in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2190\u20132196.\\n\\n[16] S. Li and N. Fujii, \\\"Estimating gaze points from facial landmarks by a remote spherical camera,\\\" in ICPR, 2021.\\n\\n[17] F. Patrona, A. Iosifidis, A. Tefas, N. Nikolaidis, and I. Pitas, \\\"Visual voice activity detection in the wild,\\\" IEEE Transactions on Multimedia, pp. 967\u2013977, 2016.\\n\\n[18] T. Afouras, A. Owens, J. S. Chung, and A. Zisserman, \\\"Self-supervised learning of audio-visual objects from video,\\\" in ECCV, 2020.\\n\\n[19] J. S. Chung, \\\"Naver at activitynet challenge 2019\u2013task b active speaker detection (ava),\\\" arXiv preprint arXiv:1906.10555, 2019.\\n\\n[20] J. S. Chung and A. Zisserman, \\\"Out of time: automated lip sync in the wild,\\\" in ACCV 2016 Workshops, 2017.\\n\\n[21] B. Pouthier, L. Pilati, L. K. Gudupudi, C. Bouveyron, and F. Precioso, \\\"Active speaker detection as a multi-objective optimization with uncertainty-based multimodal fusion,\\\" in Interspeech, 2021.\\n\\n[22] J. Le\u00f3n-Alc\u00e1zar, F. Heilbron, A. Thabet, and B. Ghanem, \\\"Maas: Multi-modal assignation for active speaker detection,\\\" in ICCV, 2021.\\n\\n[23] Y. -H. Zhang, J. Xiao, S. Yang, and S. Shan, \\\"Multi-task learning for audio-visual active speaker detection,\\\" The ActivityNet Large-Scale Activity Recognition Challenge, pp. 1\u20134, 2019.\\n\\n[24] J. L. Alcazar, F. Caba, L. Mai, F. Perazzi, J.-Y. Lee, P. Arbelaez, and B. Ghanem, \\\"Active speakers in context,\\\" in CVPR, 2020.\\n\\n[25] Y. Zhang, S. Liang, S. Yang, X. Liu, Z. Wu, S. Shan, and X. Chen, \\\"Unicon: Unified context network for robust active speaker detection,\\\" in ACM MM, 2021.\\n\\n[26] J. Roth, S. Chaudhuri, O. Klejch, R. Marvin, A. Gallagher, L. Kaver, S. Ramaswamy, A. Stopczynski, C. Schmid, Z. Xi et al., \\\"Ava active speaker: An audio-visual dataset for active speaker detection,\\\" in ICASSP, 2020.\\n\\n[27] J. H. DiBiase, \\\"A high-accuracy, low-latency technique for talker localization in reverberant environments using microphone arrays.\\\" Brown University, 2000.\\n\\n[28] C. Close, \\\"A doubly equidistant projection of the sphere,\\\" The Geographical Journal, pp. 144\u2013145, 1934.\\n\\n[29] H. Purwins, B. Li, T. Virtanen, J. Schl\u00fcter, S.-Y. Chang, and T. Sainath, \\\"Deep learning for audio signal processing,\\\" IEEE Journal of Selected Topics in Signal Processing, pp. 206\u2013219, 2019.\\n\\n[30] J. Liao, H. Duan, K. Feng, W. Zhao, Y. Yang, and L. Chen, \\\"A light weight model for active speaker detection,\\\" in CVPR, 2023.\\n\\n[31] K. He, X. Zhang, S. Ren, and J. Sun, \\\"Deep residual learning for image recognition,\\\" in CVPR, 2016.\\n\\n[32] J. Liao, H. Duan, W. Zhao, Y. Yang, and L. Chen, \\\"A light weight model for video shot occlusion detection,\\\" in ICASSP, 2022.\\n\\n[33] Z. Qiu, T. Yao, and T. Mei, \\\"Learning spatio-temporal representation with pseudo-3d residual networks,\\\" in ICCV, 2017.\\n\\n[34] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, \\\"A closer look at spatiotemporal convolutions for action recognition,\\\" in CVPR, 2018.\\n\\n[35] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk, and Y. Bengio, \\\"Learning phrase representations using rnn encoder\u2013decoder for statistical machine translation,\\\" in EMNLP, 2014.\\n\\n[36] R. Tao, Z. Pan, R. K. Das, X. Qian, M. Z. Shou, and H. Li, \\\"Is someone speaking? exploring long-term temporal features for audio-visual active speaker detection,\\\" in ACM MM, 2021.\\n\\n[37] O. Kopuklu, M. Taseska, and G. Rigoll, \\\"How to design a three-stage architecture for audio-visual active speaker detection in the wild,\\\" in ICCV, 2021.\\n\\n[38] K. Min, S. Roy, S. Tripathi, T. Guha, and S. Majumdar, \\\"Learning long-term spatial-temporal graphs for active speaker detection,\\\" in ECCV, 2022.\\n\\n[39] V. Rozgic, K. J. Han, P. G. Georgiou, and S. Narayanan, \\\"Multimodal speaker segmentation and identification in presence of overlapped speech segments,\\\" Journal of Multimedia, p. 322, 2010.\"}"}
{"id": "huang24g_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Active Speaker Detection in Fisheye Meeting Scenes with Scene Spatial Spectrums\\n\\nXinghao Huang\u2217, Weiwei Jiang\u2217, Long Rao, Wei Xu, Wenqing Cheng\\n\\nHubei Key Laboratory of Smart Internet Technology, School of Electronic Information and Communication, Huazhong University of Science and Technology, Wuhan, China\\n\\n{huangxinghao,jiangweiwei,rao2000,xuwei,chengwq}@hust.edu.cn\\n\\nAbstract\\n\\nActive Speaker Detection (ASD) plays a crucial role in scene understanding tasks by determining whether an on-screen person in a given scene is speaking. In this work, to address the ASD in the context of multi-party roundtable meetings, we propose a novel approach that incorporates the fusion of spatial information of the scenes. To leverage the multiple data sources of the scenes, our method involves generating audio spatial spectrum heatmaps from the multi-channel audio and integrating them with the panoramic images. Additionally, we propose the novel FisheyeMeeting dataset, which combines fisheye panoramic video recordings with multi-channel audio captured from a six-channel circular microphone array. By enabling the multi-modal model to capture audio-visual cues in multi-party meeting scenes, our approach achieves an impressive 89.11% mAP on the FisheyeMeeting dataset. Notably, this outperforms the current SOTA methods by a significant 2.3% mAP improvement.\\n\\nIndex Terms: active speaker detection, meeting scenes, spatial spectrums\\n\\n1. Introduction\\n\\nActive Speaker Detection (ASD) is to determine whether a candidate in a given scene is speaking, which is valuable for various multi-modal speech processing tasks, such as speaker diarization[1, 2], speaker recognition[3, 4, 5, 6], target speaker extraction[7, 8], and speaker tracking[9, 10, 11].\\n\\nIn multi-party roundtable meetings, the ASD task plays a crucial role in identifying different speakers[12, 13]. This capability is highly valuable for tasks such as recording meeting content[14], generating meeting summaries[15], and facilitating subsequent analysis. The ASD system can assign timestamps to each speaker's utterances, providing insights into their contributions and level of engagement. Therefore, the development of an ASD solution tailored for multi-party roundtable meeting scenes holds significant importance.\\n\\nCurrently, roundtable meeting recordings primarily rely on fisheye cameras[16] and built-in microphones. Due to the seating arrangement where participants gather around a table, a single fisheye camera can capture the complete audio-visual information of the roundtable meeting scene, including relative positions, facial expressions, body gestures, and conversations. However, traditional methods typically focus on lip movements[17] synchronized with corresponding audio[18, 19, *], considering them as the primary modality for speaker detection[21, 22, 23]. In the context of multi-party meetings, this approach faces challenges when participants lower heads, turn heads, or engage in simultaneous conversations, resulting in unclear lip movements and audio overlap. Moreover, traditional ASD methods often rely on facial sequences and single-channel audio[24, 25], failing to leverage the spatial information available in multi-party meeting scenes where participants are seated around a table with fixed positions.\\n\\nBased on the mentioned limitations, compared to the application of ASD in other scenes, there are two main challenges:\\n\\n(1) Current ASD methods are not effectively designed to handle the challenges posed by multi-party meeting scenes. They fail to fully exploit the abundant spatial information available in multi-party meetings, including the relative positions of participants.\\n\\n(2) Lack of suitable datasets: The existing mainstream ASD datasets, AVA-ActiveSpeaker[26], primarily consist of audio-visual data from movie clips. These data typically feature clear speech and involve a small number of individuals (1-3) engaged in dialogues. There is a notable absence of datasets specifically tailored for multi-party meetings that utilize fisheye images.\\n\\nTo tackle the first challenge, we except to leverage multi-channel audio to generate audio spatial spectrum features that correspond to the spatial information of the scene. These spatial spectrum features are then incorporated with scene features into the audio-visual features to enable the model to capture contextual information from the scene. This fusion of multimodal features facilitates active speaker detection in multi-party roundtable meeting scenes. To address the second challenge, we employ a fisheye camera and a six-channel circular microphone array to collect a large-scale ASD dataset in multi-party roundtable meeting scenes. The dataset consists of fisheye audio-visual data and multi-channel audio data from multi-party meeting segments. The number of participants in the scenes ranges from 7 to 11. It contains scenes with low-volume discussions and simultaneous speech by multiple candidates, with various noise commonly encountered in real-world settings, overcoming the limitations of existing datasets. The main contributions of this paper can be summarized as follows:\\n\\n1. To the best of our knowledge, we are the first to solve the ASD task in multi-party roundtable meeting scenes.\\n\\n2. We propose a method that incorporates scene features and spatial spectrum features. By combining with corresponding face sequences and audio, the model effectively understands and handles the ASD task in this specific scene. Our results demonstrate significant improvements compared to the state-of-the-art models of ASD, validating the superiority of scene spatial spectrum features in the ASD task of multi-party roundtable meeting scenes.\"}"}
{"id": "huang24g_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: The overview framework of our method. It consists of feature encoder and corresponding decoder. The scene images and the generated corresponding spatial spectrum heatmaps are combined to form a new scene spatial spectrum heatmaps, which are then used as input to the model. The feature encoder comprises the Audio Feature Encoder, Visual Feature Encoder, and Scene Feature Encoder, each designed to capture the unique characteristics of the respective modalities. After the fusion process, the fused features are inputted into the decoder, which generates the final prediction results.\\n\\n3. We propose an ASD dataset called FisheyeMeeting, which includes audio-visual data and corresponding multi-channel audio data, specifically designed to address the lack of ASD datasets in multi-party roundtable meeting scenes.\\n\\n2. Our Method\\n\\nIn this section, we provide a detailed description of the generation of the scene spatial spectrum heatmaps and our proposed ASD method that integrates audio-visual features and scene spatial spectrum features, as shown in Figure 1. Our method consists of feature encoder and corresponding decoder. The cropped face tracks, corresponding audio, and scene spatial spectrum heatmaps serve as inputs. Finally, we predict whether the current candidate is speaking and generate binary classification result for each face frame.\\n\\n2.1. Scene spatial spectrum heatmaps generation\\n\\nDue to the inherent spatial information provided by fisheye images in multi-party roundtable meetings, we aim to integrate the information with the ASD task. Taking into account the continuity of speech and the sudden nature of noise in such meetings, we propose a method to calculate the average power of audio from different directions over a specific time duration. The suddenness of noise may result in a higher power at a specific moment but exhibit a noticeable difference in average power over a given time duration compared to the continuous speech. We associate the average power from each direction with the 360-degree fisheye scene image and overlay it onto the scene image. This process results in a new scene spatial spectrum heatmap that indicates the direction of sound sources in the 360-degree space, leveraging the rich spatial information provided by fisheye images in multi-party roundtable meetings.\\n\\nWe employ sliding window to generate spatial spectrum heatmaps that correspond to scene images. Firstly, we extract multi-channel audio segments with a duration of $\\\\tau$ centered around the current scene frame's timestamp $T$. Subsequently, we modify the central timestamp to $T + 1/\\\\text{FPS}$, where FPS denotes the video frame rate to another segment. This process is repeated iteratively. The extracted multi-channel audio segments are then used to calculate the spatial power using the SRP-PHAT [27] algorithm. The generated spatial spectrum heatmap depicts the power for different directions. The directions with higher power are represented with warmer colors. To align the spatial spectrum heatmap with the fisheye scene image, we apply equidistant projection [28], which is commonly used in fisheye cameras, to project the spatial spectrum heatmap onto a plane, as depicted in Figure 2.\\n\\n2.2. Feature encoder\\n\\nIn order to maintain the integrity and effectiveness of modal feature, we apply different feature encoders tailored to each of the three modalities, taking into account their unique characteristics, as shown in Figure 3.\\n\\n\u2022 Audio Feature Encoder: We extract 13-dimensional MFCC [29] from the original audio segments, which are then combined with temporal information to form the two-dimensional feature map, serves as the input to the audio feature encoder. We split the 2D convolution into two 1D convolutions [30], extracting information separately from the MFCC dimension and the temporal dimension. Finally, we perform global average pooling in the MFCC dimension to obtain candidate audio features, denoted as $F_a$. The structure of the audio feature encoder is illustrated in Figure 3a.\\n\\n\u2022 Scene Feature Encoder: The scene feature encoder captures the spatial information within each frame of the scene and encodes the temporal sequence of scene frames into embed-\"}"}
{"id": "huang24g_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The architecture of the feature encoders. (a) the structure of the Audio Feature Encoder. (b) the structure of the Scene Feature Encoder. (c) the structure of the Visual Feature Encoder.\\n\\nDue to the higher information content present in scene spatial spectrum heatmaps compared to facial images, we opt for the utilization of a more powerful feature extraction approach. We combine 3D CNN and ResNet18\\\\cite{31}, to extract the corresponding scene spatial spectrum features denoted as \\\\( F_p \\\\). The structure of the scene feature encoder is illustrated in Figure 3b.\\n\\n\u2022 Visual Feature Encoder: The visual feature encoder captures the information pertaining to each face frame and encodes the face sequences into sequential embeddings. We replace part of the 3D convolutional layer with R(2+1)D\\\\cite{32, 33, 34}, where an extra ReLU between the 2D and 1D convolutions increases the non-linearity in the network. Finally, we apply global max pooling in the spatial dimension to obtain the visual features \\\\( F_v \\\\) for the candidate face sequences. The structure of the visual feature encoder is illustrated in Figure 3c.\\n\\n2.3. Embedding decoder\\n\\nTo integrate the features from three modalities, we concatenate them to obtain a joint feature called \\\\( F_{avp} \\\\). This joint representation is then fed into our lightweight decoder. We employ a bidirectional Gated Recurrent Unit (GRU)\\\\cite{35} to model the temporal context of the multi-modal feature \\\\( F_{avp} \\\\). Then we pass the resulting sequence through FC layers to predict whether a candidate is speaking.\\n\\n3. Experiments\\n\\nIn this section, we introduce the dataset and provide a detailed description of our experimental setup to evaluate the effectiveness of our method.\\n\\n3.1. Dataset\\n\\nThe current mainstream dataset, A V A-Speaker\\\\cite{26}, selects audio-visual data from multiple movie clips, which are represented as ordinary 2D images. However, there is a lack of large-scale datasets in the ASD domain that provide publicly available fisheye panoramic images of multi-party roundtable meetings. To address this gap, we propose a dataset called FisheyeMeeting.\\n\\nThe FisheyeMeeting dataset captures meeting images using a fisheye panoramic camera in meeting scenes. This provides a more comprehensive view of the scene, and also presents a greater challenge for speaker detection due to the image distortion. The dataset also includes corresponding multi-channel audio captured by the circular multi-channel microphone array. We collect a total of 42 hours of meeting scene videos. In the paper, we carefully select 6 hours of continuous data, included a substantial number of participants, minimal instances of facial occlusions, high-quality images, and stable images without any noticeable shaking. The dataset consists of 256,000 frames with visible faces, which are divided into training set and validation set. The number of candidates present in the observed scene consistently exceeds seven. We randomly segment the full video into clips of 5 to 15 seconds in length.\\n\\nWe use Scale-invariant Face Detector (S3FD) for face detection on selected video segments and connect the detected face bounding boxes with high Intersection over Union (IOU) across consecutive frames to form face trajectories. Multiple annotators with basic understanding of the ASD task annotate the segments, and cross-check and correct the annotations. The annotation requirements are standardized as follows: when a speaker is present, the video is defined as visible, even if the speaker's mouth is barely visible. Occasionally, speakers cover their mouths with their hands or turn their faces sideways, but as long as the speaker's head is visible in the video, it is considered visible. The annotation requires accurate boundary annotation for active segments with a precision of 0.1 second, and segments are divided into two parts when pauses exceed 0.5 second.\\n\\n3.2. Implementation details\\n\\nThe experiment is implemented using the Adam optimizer. The initial learning rate is set to \\\\( 10^{-4} \\\\), and it is reduced by 5% for each epoch. The face images are cropped to a size of 112\u00d7112 pixels and underwent visual augmentations such as cropping, rotation, and color jittering. The audio data, on the other hand, undergo a similar overlapping noise addition process as \\\\cite{36}. From the audio, a two-dimensional feature map is extracted, consisting of 13-dimensional MFCCs along with temporal information. To generate the spatial spectrum heatmaps, we set the value of \\\\( \\\\tau \\\\) to 0.4. Both the input scene images and spatial spectrum heatmaps have a size of 224\u00d7224 pixels. The spatial spectrum heatmap is mapped to the gray space and replace the corresponding scene image's alpha channel in the RGBA space. The result image is then converted to the RGB space, forming a new scene spatial spectrum heatmap.\\n\\n3.3. Evaluation metric\\n\\nFor the FisheyeMeeting dataset, we employ the standard evaluation method used in the ASD task. We calculate the mean average precision (mAP) to evaluate the final prediction results on the validation set of the FisheyeMeeting.\\n\\n3.4. Loss function\\n\\nThe ASD task is a frame-level prediction and classification task. We project the output of the decoder using a fully connected layer and apply a softmax operation to obtain the ASD label sequence. We then compare the predicted label sequence with the ground truth label sequence using the cross-entropy loss.\\n\\n4. Results and Analysis\\n\\nIn this section, we compare the performance of our proposed model, utilizing scene spatial spectrum features, with SOTA methods of ASD on the FisheyeMeeting dataset. Furthermore, we explore improvements of our model architecture under different test conditions and provide an analysis of the results.\"}"}
{"id": "huang24g_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Comparison with other methods\\n\\nTable 1: Comparison with the state of the arts of ASD task, on the FisheyeMeeting validation set in terms of mAP.\\n\\n| Method                      | mAP (%) |\\n|-----------------------------|---------|\\n| ASDNet (ICCV\u201921) [37]       | 82.06   |\\n| TalkNet (MM\u201921) [36]        | 81.88   |\\n| Light-ASD (CVPR\u201923) [30]    | 86.81   |\\n| SPELL (ECCV\u201922) [38]        | 86.22   |\\n| Ours                        | 89.11   |\\n\\nWe conduct experiments by training representative ASD methods on the FisheyeMeeting dataset and obtain final results. ASDNet [37] is the first to propose a phased design scheme specifically for the ASD task. TalkNet [36] validates the use of long-term context in the ASD task for the first time. Light-ASD [30] introduces a lightweight and effective ASD architecture. SPELL [38] proposes a graph-based approach for ASD.\\n\\nThe experimental results are displayed in Table 1. It is noteworthy that upon incorporating scene spatial spectrum features, our method exhibits a significant improvement of 2.3% mAP to the prominent approach, Light-ASD, on the val set of FisheyeMeeting. It is evident that the model has learned rich spatial features corresponding to the candidates in the multi-party roundtable meeting scenes.\\n\\nFigure 4: ASD Results of Our Method on FisheyeMeeting val set. The results demonstrate our method\u2019s ability to accurately detect active speakers in various scenes. The green box denotes the active speaker. The red box denotes the inactive speaker. From the figure, it is evident that with the aid of scene spatial spectrum features, our method can accurately detect all active speakers in meeting scenes, even in cases of multiple speakers or incomplete facial information.\\n\\n4.2. Ablation study\\n\\nIn this section, we analyze the performance of each component of our proposed method on the FisheyeMeeting dataset. The results are presented in Table 2 and Table 3 respectively.\\n\\nWe first analyze the accuracy of SRP-PHAT for sound source localization [39]. After calculating the spatial spectrums for each frame of the scene, we align the spatial spectrum heatmap with the scene image using equidistant projection. Subsequently, we calculate the mean power within each face bounding box as the score for the respective entity, which is employed to calculate the overall mAP. The results indicate that, although multi-channel audio data can be applied for sound source localization solely, the presence of noise and the interference caused by multiple speakers in the meeting scenes make it challenging to achieve desirable outcomes.\\n\\nTable 2: Impact of scene images and spatial spectrum heatmaps. SSH represents spatial spectrum heatmaps. SI represents scene images.\\n\\n| Method                      | mAP (%) |\\n|-----------------------------|---------|\\n| SRP-PHAT [39]               | 54.59   |\\n| Light-ASD [30]              | 86.81   |\\n| Ours (with SSH)             | 85.37   |\\n| Ours (with SI)              | 87.53   |\\n| Ours (with SSH and SI)      | 89.11   |\\n\\nWe also conduct an analysis of the respective contributions of the scene images and spatial spectrum heatmaps in our proposed method. We find when considering only the spatial spectrum heatmaps as input, our method exhibits lower mAP compared to Light-ASD. This discrepancy can be attributed to the inability of the model to establish accurate correspondences between candidates and the spatial spectrums, resulting in the spatial spectrum features being treated as spurious noise, thereby diminishing mAP. Conversely, when solely relying on the scene images, the network is able to acquire spatial positional information among candidates, thereby gets a modest improvement in precision. The combination of the scene images and spatial spectrum heatmaps evidently provides a more comprehensive and accurate information of the corresponding scene, thereby resulting in the optimal performance.\\n\\nTable 3: Impact of scene feature encoder. It only measures the parameters of the encoder, and FLOPS required for processing one scene image in conjunction with its corresponding spatial spectrums.\\n\\n| Method                      | Params(M) | FLOPs(G) | mAP (%) |\\n|-----------------------------|-----------|----------|---------|\\n| 2D + 1D                     | 2.25      | 1.19     | 87.23   |\\n| 3D                          | 3.24      | 1.57     | 88.97   |\\n| 3D + ResNet18               | 11.32     | 0.70     | 89.11   |\\n\\nWe finally analyze the performance of different scene feature encoders, as shown in Table 3. We find the encoder with the largest number of parameters (3D + ResNet18), achieves the best results. This aligns with our expectation: due to the significantly increased information content in scene spatial spectrum heatmaps compared to face images, encoders with larger parameter sizes possess stronger representational capabilities, enabling them to better extract and utilize contextual information.\\n\\n5. Conclusion\\n\\nIn this paper, we propose an ASD dataset captured by a fisheye panoramic camera in meeting scenes. The dataset consists of over 2.5 million frames of multi-party meeting 360-degree panoramic images, accompanied by corresponding multi-channel audio data. We also propose a multi-modal fusion ASD approach specifically designed for multi-party meeting scenes, incorporating audio-visual and scene spatial spectrum modalities. We establish a foundation for conducting ASD tasks in complex multi-party meeting environments and explore novel solutions for addressing ASD challenges in such settings. We hope our work will find broader applications in other speech-related tasks within multi-party meeting scenes in the future.\\n\\nThe source code will be released.\\n\\n1https://github.com/myaff1/ASD-meeting\"}"}
