{"id": "kim22h_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nKeyword spotting is the task of detecting a keyword in streaming audio. Conventional keyword spotting targets predefined keywords classification, but there is growing attention in few-shot (query-by-example) keyword spotting, e.g., $N$-way classification given $M$-shot support samples. Moreover, in real-world scenarios, there can be utterances from unexpected categories (open-set) which need to be rejected rather than classified as one of the $N$ classes. Combining the two needs, we tackle few-shot open-set keyword spotting with a new benchmark setting, named split GSC. We propose episode-known dummy prototypes based on metric learning to detect an open-set better and introduce a simple and powerful approach, Dummy Prototypical Networks (D-ProtoNets). Our D-ProtoNets shows clear margins compared to recent few-shot open-set recognition (FSOSR) approaches in the suggested split GSC. We also verify our method on a standard benchmark, mini ImageNet, and D-ProtoNets shows the state-of-the-art open-set detection rate in FSOSR.\\n\\nIndex Terms: Few-shot learning, Open-set Recognition, Keyword Spotting, Dummy Prototype, Prototypical Networks\\n\\n1. Introduction\\nKeyword spotting (KWS) detects keywords like \\\"Hey, Google\\\" and \\\"Hey, Siri\\\" in streaming audio. KWS systems usually target edge devices such as mobile phones and smart speakers, and previous studies have concentrated on better network designs in terms of detection rate [1, 2] and computational cost [3, 4] while targeting multi-keyword classifications with Google speech commands dataset (GSC) version 1 and 2 [5].\\n\\nRecently, there has been growing attention in query-by-example (few-shot) keyword spotting systems [6, 7, 8, 9]. Few-shot learning (FSL) absorbs knowledge from a training dataset and leverages the knowledge to adapt to evaluation tasks of unseen categories using only a few labeled (support) samples. However, on top of FSL, real-world scenarios naturally meet utterances of unexpected categories without support examples, and neural networks tend to be over-confident [10] and can misjudge those unexpected samples by one of the FSL classes. Thus, there is a need to detect those unseen open-set classes (Open-Set Recognition (OSR) [11]). This work introduces OSR to few-shot keyword spotting that is a more challenging setting, few-shot open-set recognition (FSOSR) [12].\\n\\nFigure 1 shows example episodes of FSL and FSOSR while using an FSL method, Prototypical Networks (ProtoNets) [13]. In an $N$-way $M$-shot episode, the goal of FSL is correctly classifying $N$ classes that are unseen during training but known using $M$ support samples for each. FSL does not consider open-set classes out of the $N$ classes. On the other hand, FSOSR needs to distinguish an unknown open-set from the known classes while still performing FSL. FSOSR is more challenging than conventional OSR because an open-set changes over episodes based on the choice of $N$ classes (Figure 1 bottom).\\n\\nThus, a desirable FSOSR method needs to adapt to the varying open-set. We predict episode-specific (episode-known) dummies based on support examples in each episode and classify an open-set as the dummies. Using the episode-known dummies, we propose Dummy Prototypical Networks (D-ProtoNets).\\n\\nFor few-shot open-set keyword spotting (FSOS-KWS), we introduce a benchmark setting named split GSC, a subset of GSC ver2. Our D-ProtoNets achieves state-of-the-art (SOTA) performance in split GSC. We also verify D-ProtoNets on mini ImageNet [14], a widely used FSL benchmark, and D-ProtoNets is better in detecting open-set than other baselines.\\n\\n2. Related Works\\nThe literature on few-shot learning and open-set recognition are vast, thus we focus on the most relevant works.\\n\\nFew-Shot Learning.\\nFew-shot learning has three popular branches, adaptation, hallucination, and metric learning methods. The adaptation methods [15] make a model easy to fine-tune in the low-shot regime, and the hallucination methods [16] augment training examples for data starved classes. Our approach aligns with the last one, metric-based learning [13, 14], which learns a metric space in which distance metrics can classify samples. Especially our method is designed on top of Prototypical Networks (ProtoNets) [13]. Recently, FEAT [17] shows that it is helpful to make support samples task-specific using a set-to-set function, Transformer [18], in FSL. Also, some approaches have addressed few-shot KWS [7, 8, 9], but to the best of our knowledge, this is the first work introducing few-shot...\"}"}
{"id": "kim22h_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Dummy Prototypical Network. \\\\( f_\\\\phi(\\\\cdot) \\\\) and \\\\( g_\\\\phi(\\\\cdot) \\\\) are the encoder and the dummy generator, respectively. 'K' and 'U' stand for 'Known' and 'Unknown', respectively. For a support set \\\\( S \\\\) and a query set \\\\( Q \\\\), we consider \\\\( \\\\phi \\\\) and \\\\( W_g \\\\) are learnable. We get an average of the support samples of each class, \\\\( n \\\\) is the number of classes.\\n\\nOur work is in line with metric-learning-based approaches and is based on the representative prototypes, \\\\( \\\\Phi \\\\), and detects open-set using transformation consistency. \\\\( \\\\Phi \\\\) is the set of prototypes and \\\\( \\\\Phi' \\\\) is the set of pseudo-open-set classes without any support examples. We denote a support set and a query set as \\\\( S \\\\) and \\\\( Q \\\\), respectively.\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\n3. Method\\n\\n3.1. Preliminaries\\n\\nFSOSR episodes, each of which has \\\\( N \\\\) support examples for each class and \\\\( M \\\\) queries from both \\\\( N \\\\) known and \\\\( M \\\\) unknown open-set classes.\\n\\n\\\\[ M \\\\quad \\\\text{support samples and} \\\\quad N \\\\quad \\\\text{queries for each class.} \\\\]\\n\\n\\\\[ \\\\text{At inference time, all classes of} \\\\quad N \\\\quad \\\\text{known classes are present in the training set.} \\\\]\\n\\nWe consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\n\\\\[ \\\\text{During training, a model learns from} \\\\quad y \\\\quad \\\\text{where} \\\\quad y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\quad \\\\text{and} \\\\quad y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D. \\\\]\\n\\nWe assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with \\\\( y_{\\\\text{train}} \\\\).\\n\\nDuring training, a model learns from \\\\( y \\\\) where \\\\( y_{\\\\text{train}} = \\\\{x_i \\\\}_{i=1}^N \\\\) and \\\\( y_{\\\\text{unseen}} = \\\\{x_i \\\\}_{i=N+1}^D \\\\). We assume \\\\( x_i = x_j \\\\) for \\\\( i \\\\neq j \\\\), and \\\\( y_i = 1 \\\\) for \\\\( i \\\\leq N \\\\), and \\\\( y_i = 0 \\\\) for \\\\( N < i \\\\leq D \\\\). Our work focuses on the \\\\( m \\\\) pseudo-open-set classes formed by \\\\( n \\\\) known classes, \\\\( N \\\\) support samples, and \\\\( M \\\\) unseen, and again an episode consists of \\\\( N \\\\)-way \\\\( M \\\\)-shot episode. ProtoNets gets \\\\( n \\\\) support samples and \\\\( M \\\\) queries for each class. At inference time, all classes of \\\\( N \\\\) known classes are present in the training set. We consider \\\\( S \\\\) and \\\\( Q \\\\) are composed of labeled samples, \\\\( y_{\\\\text{train}} \\\\), and \\\\( y_{\\\\text{evaluation}} \\\\), respectively, and \\\\( y_{\\\\text{evaluation}} \\\\) that do not overlap with"}
{"id": "kim22h_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"an augmented prototype set \\\\{c_1, \\\\ldots, c_N, c_d\\\\}. We set the labels of open-set queries to the \\\\(N+1\\\\)-th label \\\\(y_d\\\\) which corresponds to \\\\(c_d\\\\). However, we observed that the unnormalized distance metric does not always hold for detecting open-set and shows poor AU-ROC with Conv4-64 in splitGSC and our training details.\\n\\n\\\\[\\n\\\\text{arg max}_y L(x, y) \\\\text{ respectively. We balance the two losses by } \\\\lambda Q \\\\text{ as below:}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe use Gumbel distribution of \\\\(\\\\mu\\\\) and \\\\(\\\\gamma > 0\\\\) to replace the non-differentiable sample, \\\\(\\\\text{arg max}_y L(x, y)\\\\). We can naively choose the most probable dummy for an input \\\\(x\\\\), \\\\(\\\\text{arg max}_y L(x, y)\\\\), to let the dummy easily reduce its loss, i.e., \\\\(\\\\log \\\\mathbb{E}_{y \\\\sim \\\\text{Gumbel}(\\\\mu)}[\\\\exp(\\\\lambda Q(y | x))]\\\\), with \\\\(\\\\gamma\\\\) as a default.\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\n\\\\[\\n\\\\text{loss} = L^{ce}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q(y_d | x)}}{\\\\text{arg max}_y L(x, y)}\\n\\\\]\\n\\nWe expand D-ProtoNets with multi-dummies by \\\\(\\\\gamma\\\\) times. We balance the two losses by \\\\(\\\\lambda Q\\\\) as below:\\n\\n\\\\[\\n\\\\text{loss} = L_\\\\text{CE}(x, y) - \\\\gamma \\\\sum_{d=1}^{N+1} \\\\log \\\\frac{e^{\\\\lambda Q"}
{"id": "kim22h_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: miniImageNet: 5-way {1, 5}-shot FSOSR results. The numbers are mean (std) over 5 trials (%). *SnaTCHer shows better FSL accuracies than ours, especially for 1-shot settings. We interpret that their additional set-to-set transformation for support examples makes the extreme low support examples regime more robust by using the relation between a few transformations for all the queries. Our D-ProtoNets can be used with a complementary concept like FEAT. Erasing undesirable instance discrepancy is highly important in FSOS-KWS. We also suggest future research, erasing undesirable instance discrepancy, for FSOS-KWS.\\n\\nTable 3 shows the comparison between D-ProtoNets and various baselines. Our dummy prototypes improve vanilla Prototypical Networks (ProtoNets). By adding dummies, there are considerable improvements of more than 10% in AUROC. We could get further improvements through introducing Gumbel softmax and ther improvements through introducing Gumbel softmax and softmax temperature \\\\( \\\\tau \\\\) works best. D-ProtoNets use softmax temperature \\\\( \\\\tau = 16 \\\\) for FEAT and \\\\( \\\\tau = 3 \\\\) for SnaTCHer. The top row implies the best settings. D-ProtoNets use dummy prototypes to improve further. At inference time, the transformations for all the queries of D-ProtoNets+FEAT are relaxed by RFN. RFN module operates its input x (RFN) \\\\([35,36]\\\\), to reduce undesirable instance discrepancy in ground objects rather than backgrounds in images. Motivated by [34], we use an explicit normalization along the frequency-axis, named Relaxed instance Frequency-wise Normalization \\\\( \\\\text{IFN} \\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis, and LN is layer normalization \\\\([38]\\\\) for relaxing the effect of IFN. Here we use the normalization \\\\( \\\\text{LN} \\\\) along frequency-axis, and LN is layer normalization \\\\([37]\\\\) along frequency-axis,"}
{"id": "kim22h_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] R. Tang and J. Lin, \u201cDeep residual learning for small-footprint keyword spotting,\u201d in ICASSP. IEEE, 2018, pp. 5484\u20135488.\\n\\n[2] M. Lee, J. Lee, H. J. Jang, B. Kim, W. Chang, and K. Hwang, \u201cOrthogonality constrained multi-head attention for keyword spotting,\u201d in ASRU. IEEE, 2019, pp. 86\u201392.\\n\\n[3] M. Xu and X. Zhang, \u201cDepthwise separable convolutional resnet with squeeze-and-excitation blocks for small-footprint keyword spotting,\u201d in INTERSPEECH. ISCA, 2020, pp. 2547\u20132551.\\n\\n[4] B. Kim, S. Chang, J. Lee, and D. Sung, \u201cBroadcasted Residual Learning for Efficient Keyword Spotting,\u201d in Proc. Interspeech 2021, 2021, pp. 4538\u20134542.\\n\\n[5] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary speech recognition,\u201d arXiv preprint arXiv:1804.03209, 2018.\\n\\n[6] B. Kim, M. Lee, J. Lee, Y. Kim, and K. Hwang, \u201cQuery-by-example on-device keyword spotting,\u201d in ASRU. IEEE, 2019, pp. 532\u2013538.\\n\\n[7] Y. Chen, T. Ko, L. Shang, X. Chen, X. Jiang, and Q. Li, \u201cAn investigation of few-shot learning in spoken term classification,\u201d in INTERSPEECH. ISCA, 2020, pp. 2582\u20132586.\\n\\n[8] A. Parnami and M. Lee, \u201cFew-shot keyword spotting with prototypical networks,\u201d CoRR, vol. abs/2007.14463, 2020.\\n\\n[9] J. Huang, W. Gharbieh, H. S. Shim, and E. Kim, \u201cQuery-by-example keyword spotting system using multi-head attention and soft-triple loss,\u201d in ICASSP. IEEE, 2021, pp. 6858\u20136862.\\n\\n[10] A. M. Nguyen, J. Yosinski, and J. Clune, \u201cDeep neural networks are easily fooled: High confidence predictions for unrecognizable images,\u201d in CVPR. IEEE Computer Society, 2015, pp. 427\u2013436.\\n\\n[11] W. J. Scheirer, A. de Rezende Rocha, A. Sapkota, and T. E. Boult, \u201cToward open set recognition,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 35, no. 7, pp. 1757\u20131772, 2013.\\n\\n[12] B. Liu, H. Kang, H. Li, G. Hua, and N. Vasconcelos, \u201cFew-shot open-set recognition using meta-learning,\u201d in CVPR. Computer Vision Foundation / IEEE, 2020, pp. 8795\u20138804.\\n\\n[13] J. Snell, K. Swersky, and R. S. Zemel, \u201cPrototypical networks for few-shot learning,\u201d in NIPS, 2017, pp. 4077\u20134087.\\n\\n[14] O. Vinyals, C. Blundell, T. Lillicrap, K. Kavukcuoglu, and D. Wierstra, \u201cMatching networks for one shot learning,\u201d in NIPS, 2016, pp. 3630\u20133638.\\n\\n[15] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in ICML, ser. Proceedings of Machine Learning Research, vol. 70. PMLR, 2017, pp. 1126\u20131135.\\n\\n[16] B. Hariharan and R. B. Girshick, \u201cLow-shot visual recognition by shrinking and hallucinating features,\u201d in ICCV. IEEE Computer Society, 2017, pp. 3037\u20133046.\\n\\n[17] H. Ye, H. Hu, D. Zhan, and F. Sha, \u201cFew-shot learning via embedding adaptation with set-to-set functions,\u201d in CVPR. Computer Vision Foundation / IEEE, 2020, pp. 8805\u20138814.\\n\\n[18] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in NIPS, 2017, pp. 5998\u20136008.\\n\\n[19] R. Yoshihashi, W. Shao, R. Kawakami, S. You, M. Iida, and T. Naemura, \u201cClassification-reconstruction learning for open-set recognition,\u201d in CVPR. Computer Vision Foundation / IEEE, 2019, pp. 4016\u20134025.\\n\\n[20] Z. Ge, S. Demyanov, and R. Garnavi, \u201cGenerative openmax for multi-class open set classification,\u201d in BMVC. BMV A Press, 2017.\\n\\n[21] P. Perera, V. I. Morariu, R. Jain, V. Manjunatha, C. Wigington, V. Ordonez, and V. M. Patel, \u201cGenerative-discriminative feature representations for open-set recognition,\u201d in CVPR. Computer Vision Foundation / IEEE, 2020, pp. 11 811\u201311 820.\\n\\n[22] D. Zhou, H. Ye, and D. Zhan, \u201cLearning placeholders for open-set recognition,\u201d in CVPR. Computer Vision Foundation / IEEE, 2021, pp. 4401\u20134410.\\n\\n[23] V. Verma, A. Lamb, C. Beckham, A. Najafi, I. Mitliagkas, D. Lopez-Paz, and Y. Bengio, \u201cManifold mixup: Better representations by interpolating hidden states,\u201d in ICML, ser. Proceedings of Machine Learning Research, vol. 97. PMLR, 2019, pp. 6438\u20136447.\\n\\n[24] M. Jeong, S. Choi, and C. Kim, \u201cFew-shot open-set recognition by transformation consistency,\u201d in CVPR. Computer Vision Foundation / IEEE, 2021, pp. 12 566\u201312 575.\\n\\n[25] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. Salakhutdinov, and A. J. Smola, \u201cDeep sets,\u201d in NIPS, 2017, pp. 3391\u20133401.\\n\\n[26] E. Jang, S. Gu, and B. Poole, \u201cCategorical reparameterization with gumbel-softmax,\u201d in ICLR (Poster). OpenReview.net, 2017.\\n\\n[27] B. N. Oreshkin, P. R. Lopez, and A. Lacoste, \u201cTADAM: task-dependent adaptive metric for improved few-shot learning,\u201d in NeurIPS, 2018, pp. 719\u2013729.\\n\\n[28] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in ICLR (Poster), 2015.\\n\\n[29] A. Bendale and T. E. Boult, \u201cTowards open set deep networks,\u201d in CVPR. IEEE Computer Society, 2016, pp. 1563\u20131572.\\n\\n[30] L. Neal, M. L. Olson, X. Z. Fern, W. Wong, and F. Li, \u201cOpen set learning with counterfactual images,\u201d in ECCV (6), ser. Lecture Notes in Computer Science, vol. 11210. Springer, 2018, pp. 620\u2013635.\\n\\n[31] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang, A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei, \u201cImageNet Large Scale Visual Recognition Challenge,\u201d International Journal of Computer Vision (IJCV), vol. 115, no. 3, pp. 211\u2013252, 2015.\\n\\n[32] S. Ravi and H. Larochelle, \u201cOptimization as a model for few-shot learning,\u201d in ICLR. OpenReview.net, 2017.\\n\\n[33] K. Lee, S. Maji, A. Ravichandran, and S. Soatto, \u201cMeta-learning with differentiable convex optimization,\u201d in CVPR. Computer Vision Foundation / IEEE, 2019, pp. 10 657\u201310 665.\\n\\n[34] X. Luo, L. Wei, L. Wen, J. Yang, L. Xie, Z. Xu, and Q. Tian, \u201cRectifying the shortcut learning of background: Shared object concentration for few-shot image recognition,\u201d CoRR, vol. abs/2107.07746, 2021.\\n\\n[35] B. Kim, S. Yang, J. Kim, and S. Chang, \u201cDomain generalization on efficient acoustic scene classification using residual normalization,\u201d in Proceedings of the 6th Detection and Classification of Acoustic Scenes and Events 2021 Workshop (DCASE2021), Barcelona, Spain, November 2021, pp. 21\u201325.\\n\\n[36] B. Kim, S. Yang, J. Kim, H. Park, J.-T. Lee, and S. Chang, \u201cTowards robust domain generalization in 2d neural audio processing,\u201d 2021.\\n\\n[37] D. Ulyanov, A. Vedaldi, and V. S. Lempitsky, \u201cInstance normalization: The missing ingredient for fast stylization,\u201d CoRR, vol. abs/1607.08022, 2016.\\n\\n[38] L. J. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d CoRR, vol. abs/1607.06450, 2016.\\n\\n[39] B. Kim, S. Yang, J. Kim, and S. Chang, \u201cQTI submission to DCASE 2021: Residual normalization for device-imbalanced acoustic scene classification with efficient design,\u201d DCASE2021 Challenge, Tech. Rep., June 2021.\"}"}
