{"id": "tan22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., \u201cNatural tts synthesis by conditioning wavenet on mel spectrogram predictions,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4779\u20134783.\\n\\n[2] C. Yu, H. Lu, N. Hu, M. Yu, C. Weng, K. Xu, P. Liu, D. Tuo, S. Kang, G. Lei et al., \u201cDurian: Duration informed attention network for speech synthesis.\u201d in INTERSPEECH, 2020, pp. 2027\u20132031.\\n\\n[3] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d arXiv preprint arXiv:2006.04558, 2020.\\n\\n[4] T. Li, S. Yang, L. Xue, and L. Xie, \u201cControllable emotion transfer for end-to-end speech synthesis,\u201d in 2021 12th International Symposium on Chinese Spoken Language Processing (ISCSLP). IEEE, 2021, pp. 1\u20135.\\n\\n[5] D. Tan, L. Deng, Y. T. Yeung, X. Jiang, X. Chen, and T. Lee, \u201cEditspeech: A text based speech editing system using partial inference and bidirectional fusion,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021, pp. 626\u2013633.\\n\\n[6] Z. Jin, G. J. Mysore, S. Diverdi, J. Lu, and A. Finkelstein, \u201cV oco: Text-based insertion and replacement in audio narration,\u201d ACM Transactions on Graphics (TOG), vol. 36, no. 4, pp. 1\u201313, 2017.\\n\\n[7] J. Cong, S. Yang, L. Xie, G. Yu, and G. Wan, \u201cData efficient voice cloning from noisy samples with domain adversarial training,\u201d arXiv preprint arXiv:2008.04265, 2020.\\n\\n[8] C. Zhang, Y. Ren, X. Tan, J. Liu, K. Zhang, T. Qin, S. Zhao, and T.-Y. Liu, \u201cDenoispeech: Denoising text to speech with frame-level noise modeling,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 7063\u20137067.\\n\\n[9] W.-N. Hsu, Y. Zhang, R. J. Weiss, Y.-A. Chung, Y. Wang, Y. Wu, and J. Glass, \u201cDisentangling correlated speaker and noise for speech synthesis via data augmentation and adversarial factorization,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5901\u20135905.\\n\\n[10] R. Giri, U. Isik, and A. Krishnaswamy, \u201cAttention wave-u-net for speech enhancement,\u201d in 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2019, pp. 249\u2013253.\\n\\n[11] T.-A. Hsieh, H.-M. Wang, X. Lu, and Y. Tsao, \u201cWavecrn: An efficient convolutional recurrent neural network for end-to-end speech enhancement,\u201d IEEE Signal Processing Letters, vol. 27, pp. 2149\u20132153, 2020.\\n\\n[12] D. S. Williamson and D. Wang, \u201cTime-frequency masking in the complex domain for speech dereverberation and denoising,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 25, no. 7, pp. 1492\u20131501, 2017.\\n\\n[13] J. Su, Z. Jin, and A. Finkelstein, \u201cAcoustic matching by embedding impulse responses,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 426\u2013430.\\n\\n[14] Z.-Q. Wang and D. Wang, \u201cDeep learning based target cancellation for speech dereverberation,\u201d IEEE/ACM transactions on audio, speech, and language processing, vol. 28, pp. 941\u2013950, 2020.\\n\\n[15] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, \u201cGeneralized end-to-end loss for speaker verification,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4879\u20134883.\\n\\n[16] I. Szoke, M. Skacl, L. Mosner, J. Paliesek, and J. Cernocky, \u201cBuilding and evaluation of a real room impulse response dataset,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 4, pp. 863\u2013876, 2019.\\n\\n[17] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibritts: A corpus derived from librispeech for text-to-speech,\u201d arXiv preprint arXiv:1904.02882, 2019.\\n\\n[18] D. Tan, H. Huang, G. Zhang, and T. Lee, \u201cCuhk-ee voice cloning system for icassp 2021 m2voc challenge,\u201d arXiv preprint arXiv:2103.04699, 2021.\\n\\n[19] Y. Jia, Y. Zhang, R. J. Weiss, Q. Wang, J. Shen, F. Ren, Z. Chen, P. Nguyen, R. Pang, I. L. Moreno et al., \u201cTransfer learning from speaker verification to multispeaker text-to-speech synthesis,\u201d arXiv preprint arXiv:1806.04558, 2018.\\n\\n[20] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d arXiv preprint arXiv:2010.05646, 2020.\\n\\n[21] J. Yamagishi, C. Veaux, K. MacDonald et al., \u201cCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92),\u201d 2019.\\n\\n[22] K. Kinoshita, M. Delcroix, T. Yoshioka, T. Nakatani, E. Habets, R. Haeb-Umbach, V. Leutnant, A. Sehr, W. Kellermann, R. Maas et al., \u201cThe reverb challenge: A common evaluation framework for dereverberation and recognition of reverberant speech,\u201d in 2013 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics. IEEE, 2013, pp. 1\u20134.\\n\\n[23] M. Jeub, M. Schafer, and P. Vary, \u201cA binaural room impulse response database for the evaluation of dereverberation algorithms,\u201d in 2009 16th International Conference on Digital Signal Processing. IEEE, 2009, pp. 1\u20135.\\n\\n[24] L. Van der Maaten and G. Hinton, \u201cVisualizing data using t-sne.\u201d Journal of machine learning research, vol. 9, no. 11, 2008.\"}"}
{"id": "tan22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Environment Aware Text-to-Speech Synthesis\\n\\nDaxin Tan, Guangyan Zhang, Tan Lee\\nDepartment of Electronic Engineering, The Chinese University of Hong Kong, Hong Kong\\n{daxintan, gyzhang}@link.cuhk.edu.hk, tanlee@ee.cuhk.edu.hk\\n\\nAbstract\\nThis study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration.\\n\\nIndex Terms: speech synthesis, acoustic environment, disentanglement, de-reverberation\\n\\n1. Introduction\\nWith the advancement of deep learning, neural network based text-to-speech synthesis (TTS) systems have attained great successes [1\u20133]. They produce high-quality synthesized speech comparable to human speech in terms of intelligibility, naturalness and speaking style [4]. In most cases, TTS systems are trained with a large amount of clean speech with coherent audio quality. Naturally the training objective is to generate speech with similar acoustic characteristics to the speech for training.\\n\\nDespite being widely accepted, this paradigm is found to pose various practical limitations that hinder the practical deployment of TTS systems. On one hand, collecting large amount of clean speech is generally difficult and costly, as professional arrangement of recording in well-controlled acoustic environment like soundproof studio is required. In contrast, realistic speech audios captured casually in daily-life environments, e.g., meeting room, play room, are much easier to access and accumulate. These speech signals are unexceptionally contaminated by background noise and reverberation. Being able to utilize speech data from heterogeneous sources is highly desirable for efficiently and maximally exploiting available data resources for TTS system development.\\n\\nOn the other hand, it is an appealing function that a TTS system is able to synthesize speech to fit a designated acoustic environment. An example scenario is when a user needs to insert a speech segment into or substitute certain part of a recorded audio in order to change its speech content. The inserted or substituting segment can be synthesized from a given TTS system [5, 6]. The newly synthesized speech is required to carry the acoustic environment characteristics that are consistent with the original recording. In the application of voice cloning, it is common that only reverberant and/or noisy speech are available for the target speaker, while the goal is to generate clean speech of this speaker [7, 8]. If \u201cclean\u201d is considered as one of the acoustic environment, this task can be formulated as generating speech to fit an acoustic environment that is different from the original one.\\n\\nIn the present study, we propose a method to utilize diverse-environment data for system development and extend the usage of TTS system. The main idea is to model acoustic environment of speech recording as a factor of data variability and incorporate it as a condition into the speech generation process. Specifically, environment in this study refers only to the room reverberation, while noise will be discussed in the future. In reality, speech data from a specific speaker are usually recorded in a limited range of environments, the speaker factor and the environment factor tend to be highly correlated. In the extreme case, speech from each speaker is associated with one single environment. Concerning this issue, the disentanglement between speaker and environment factor of speech is also tackled in this work.\\n\\nModelling of environment-related factor in speech audio was investigated in previous studies [7\u201313]. In [9], disentanglement of speaker and noise was achieved through data augmentation and factorization. In [7], domain adversarial training was applied. Both studies were focused on making good use of data sources and aimed to generate clean speech for target speakers when only noisy speech data are available. In [12\u201314], the main goal was to reduce reverberation in speech. The task can be regarded as signal conversion from reverberant environment to \u201cclean\u201d environment. In [13], the problem of acoustic matching, i.e., conversion among different indoor acoustic environments, was investigated. These conversion processes operate typically in the waveform-to-waveform manner. In our proposed approach, acoustic environment is considered as a factor of variation in neural TTS.\\n\\nWe design a TTS model that leverages both clean and reverberant speech data. The model is trained to synthesize speech for target speakers under designated acoustic environments. Two extractor modules trained with specially designed datasets are used to extract embeddings that represent speaker and environment respectively. The TTS module generates speech by conditioning on the two embeddings. With speaker and environment information provided by reference utterances, the synthesized speech is made to carry the desired speaker characteristics and environment attributes.\\n\\n2. The proposed system\\nAn overview of our proposed system is shown in Figure 1. The system contains a speaker embedding extractor, an environment embedding extractor and a TTS module.\\n\\n2.1. Speaker embedding extractor\\nThe speaker embedding extractor is developed based on a speaker verification (SV) system. The SV system is trained with\\n\\n1. https://daxintan-cuhk.github.io/Environment-Aware-TTS/\"}"}
{"id": "tan22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An overview of the proposed system.\\n\\nThe generalized end-to-end loss as proposed in [15]. The objective is to make the embeddings extracted from utterances of the same speaker be close to each other, and enlarge the distance among the embeddings from different speakers.\\n\\nIn each training batch, \\\\( s \\\\) speakers are selected and \\\\( u \\\\) utterances are sampled from each of the speakers. For each utterance, a speech segment of 80-frames length is cropped from random location of the utterance and is used as the input of extractor. The extractor model consists of LSTM layers followed by a linear layer. The output of extractor is L2-normalized to be the speaker embedding of this utterance. Let \\\\( E_{ij} \\\\) denote the speaker embedding extracted from utterance \\\\( j \\\\) of speaker \\\\( i \\\\), where \\\\( 1 \\\\leq i \\\\leq s \\\\) and \\\\( 1 \\\\leq j \\\\leq u \\\\). For speaker \\\\( i \\\\), the centroid of speaker embedding can be calculated in two different ways: (1) derived from all utterances of speaker \\\\( i \\\\), denoted as \\\\( C_i \\\\); (2) derived in a similar way to (1) but with utterance \\\\( j \\\\) excluded, denoted as \\\\( C_i,(-j)_s \\\\). The speaker embedding \\\\( E_{ij} \\\\) is compared with the centroid of all speakers to construct the similarity matrix, which is then utilized to construct the softmax loss. The centroids, similarity matrix and loss function are defined as follows.\\n\\nCentroid:\\n\\\\[\\nC_i = \\\\frac{1}{u} \\\\sum_{1 \\\\leq m \\\\leq u} E_{im}, \\\\quad C_i,(-j)_s = \\\\frac{1}{u-1} \\\\sum_{1 \\\\leq m \\\\leq u, m \\\\neq j} E_{im}\\n\\\\]\\n\\nSimilarity matrix of speaker embeddings:\\n\\\\[\\nS_{ij,k} = \\\\begin{cases} \\n  w \\\\cdot \\\\cos(E_{ij}, C_i,(-j)_s) + b, & k = i \\\\\\\\\\n  w \\\\cdot \\\\cos(E_{ij}, C_k) + b, & k \\\\neq i \\n\\\\end{cases}\\n\\\\]\\n\\nSoftmax loss for speaker embedding extractor:\\n\\\\[\\nL(E_{ij}) = -\\\\log \\\\exp(S_{ij,i}) \\\\sum_{k=1}^{s} \\\\exp(S_{ij,k}), \\\\quad L(E_i) = \\\\sum_{1 \\\\leq i \\\\leq s} \\\\sum_{1 \\\\leq j \\\\leq u} L(E_{ij})\\n\\\\]\\n\\nThe environment embedding extractor derives an environment embedding from a speech utterance. In the present study we assume that environment-specific information can be represented by the measured room impulse response (RIR). To train the extractor module, we purposely construct a speech dataset such that each type of environment is associated with multiple speakers. We use the clean speech utterances from 247 speakers in the \\\"train-clean-100\\\" subset of LibriTTS dataset [17]. 2,325 kinds of measured RIRs from the BUT ReverbDB dataset [16] are used to represent different environments. For each environment condition, reverberant speech from multiple speakers are generated by convolving the clean speech with the respective RIR. The environment embedding extractor is trained in a similar way to the speaker embedding extractor, i.e., to force embeddings from the same environment to be close to each other and enlarge the distance among those from different environments.\\n\\nIn each training batch, we select \\\\( e \\\\) environments and sample \\\\( u \\\\) utterances from each environment. Each utterance-level environment embedding \\\\( E_{ij} \\\\), where \\\\( 1 \\\\leq i \\\\leq e \\\\) and \\\\( 1 \\\\leq j \\\\leq u \\\\), is compared to the centroids of all environments to obtain the similarity matrix and softmax loss, as illustrated in Figure 2.\\n\\nCentroid:\\n\\\\[\\nC_i_e = \\\\frac{1}{u} \\\\sum_{1 \\\\leq m \\\\leq u} E_{im}, \\\\quad C_i,(-j)_e = \\\\frac{1}{u-1} \\\\sum_{1 \\\\leq m \\\\leq u, m \\\\neq j} E_{im}\\n\\\\]\\n\\nSimilarity matrix of environment embeddings:\\n\\\\[\\nS_{ij,k} = \\\\begin{cases} \\n  w \\\\cdot \\\\cos(E_{ij}, C_i,(-j)_e) + b, & k = i \\\\\\\\\\n  w \\\\cdot \\\\cos(E_{ij}, C_k) + b, & k \\\\neq i \\n\\\\end{cases}\\n\\\\]\\n\\nSoftmax loss for environment embedding extractor:\\n\\\\[\\nL(E_{ij}) = -\\\\log \\\\exp(S_{ij,i}) \\\\sum_{k=1}^{e} \\\\exp(S_{ij,k}), \\\\quad L(E_e) = \\\\sum_{1 \\\\leq i \\\\leq e} \\\\sum_{1 \\\\leq j \\\\leq u} L(E_{ij})\\n\\\\]\\n\\nThe environment embedding extractor is trained to minimize the loss \\\\( L(E_e) \\\\).\\n\\n2.3. TTS module\\n\\nThe TTS module is designed as a combination of the Tacotron2 [1] and the DurIAN [2] architectures. It is similar to the one proposed and evaluated in our previous work on voice cloning [18], except that the embedding extractor modules replace the look-up tables. The TTS module comprises an encoder, a duration predictor, a length regulator, a Prenet and a decoder. The duration predictor is used to determine phone-level duration from input text. The input text and phone-level duration are taken up by the encoder and the length regulator. A speaker embedding and an environment embedding are generated by the two extractors as described in Section 2.1 and 2.2. Both embeddings are broadcasted and concatenated with the encoder output to form the input to the decoder, which is similar to the method in [19]. The Prenet and the decoder generate the mel-spectrogram of output speech, which is converted into speech waveform using the HiFi-GAN vocoder [20]. The L2-norm loss between the generated spectrogram and the ground-truth one is minimized to train the TTS module.\\n\\nThe loss for the overall proposed system is as follows:\"}"}
{"id": "tan22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Experimental setup\\n\\n3.1. Speech dataset\\nThe TTS module is trained with a specially constructed dataset in which the speaker factor and the environment factor are fully entangled. That is, each speaker is associated with a specific environment. Clean speech data of multiple speakers are obtained from the VCTK dataset [21]. Measured RIRs representing different environments are obtained from the REVERB challenge database [22] and the Aachen impulse response database [23]. 108 speakers and 108 environments are randomly selected to form 108 unique pairs of speaker-environment combination, and 100 utterances are sampled to represent each combination. It should be noted that, \u201cclean\u201d is regarded as one of the environments, for which clean speech is used directly in TTS training. For other environments, clean speech utterances from the respective speaker are convolved with the given RIR to generate training speech.\\n\\n95% of the speaker-environment pairs are selected, and 95% utterances in these combinations are used as training data. The training data for TTS module do not have overlap with those for training the two embedding extractors, in terms of speaker identity and environment type.\\n\\n3.2. System configuration\\nThe audio data used a sampling rate of 22,050 Hz. Short-time Fourier transform (STFT) is carried out with Hann window of 50 ms long and frame hop of 12.5 ms. Mel-spectrogram is computed from the magnitudes of STFT coefficients using 80-channel mel-scale filterbank from 0 to 8,000 Hz, followed by log dynamic range compression. The mel-spectrograms are used for the training of the speaker embedding extractor, environment embedding extractor and the TTS module.\\n\\nIn the training of speaker (or environment) embedding extractor, each batch involves 64 speakers (or environments), and 10 utterances are sampled from the speech data of each speaker (or environment). From each utterance, a mel-spectrogram with 80-frame length is cropped for training. Thus total of 640 mel-spectrograms are included in each batch. Each of the embedding extractors contains three layers of 256-dimension unidirectional LSTM layers and one linear layer. The speaker (or environment) embedding is a 256-dimension vector.\\n\\n3.3. Baseline system\\nThe baseline system consists of a speaker embedding extractor, an environment embedding extractor and a TTS module. These modules are trained jointly as described with the same dataset in Section 3.1. The loss for the overall baseline system comprises three parts: speaker classification loss for speaker embedding extractor, environment classification loss for environment embedding extractor, and the speech reconstruction loss for the TTS module. The hyperparameter values used are the same as in the proposed system. The difference of the baseline system and proposed system is that, in baseline system, the speaker and environment embedding extractor is not trained with the objective mentioned in Section 2.1 and 2.2, but only two classification against speaker and environment are adopted for disentanglement supervision.\\n\\nThe loss for the overall baseline system is as follows:\\n\\n\\\\[\\nL = L_{\\\\text{cls}}(E_s, \\\\text{speaker}) + L_{\\\\text{cls}}(E_e, \\\\text{environment}) + L_{\\\\text{recon}}(\\\\text{mel|} E_s, E_e, \\\\text{text})\\n\\\\]\\n\\n4. Results and discussion\\n\\n4.1. Visualization of extracted embeddings\\nTo figure out whether the speaker embedding and environment embedding model and control corresponding factor of the speech, we first synthesize utterances conditioned on the speaker embedding and environment embedding that are extracted from two different utterances. Then we utilize the speaker embedding extractor and environment embedding extractor, mentioned in Section 2.1 and 2.2, to derive speaker embedding and environment embedding respectively from the synthesized utterance. T-SNE visualization [24] is carried out on these two embedding along with the speaker and environment label. The result is shown in Figure 3. It is noted from Figure 3 (a) and (b) that speaker embeddings from the same speakers tends to cluster together, and speaker embeddings concerning the same environment are mixed with each other. This suggests that the extracted speaker embeddings capture speaker-related information and not environment-related one. From Figure 3 (c) and (d), it can be concluded that the extracted environment embeddings are able to capture the environment-related information but not speaker-related one in the speech.\\n\\nTable 1: Mel-cepstral distortion (MCD) between synthesized and ground-truth speech\\n\\n| Speaker | Environment | Combination | MCD | Proposed | Baseline |\\n|---------|-------------|-------------|-----|----------|----------|\\n| seen    | seen        | seen        | 6.899 | 6.668    |\\n| seen    | unseen      | unseen      | 7.312 | 7.605    |\\n| unseen  | unseen      | unseen      | 7.442 | 7.464    |\\n\\nFigure 3: The t-SNE visualization of speaker embedding and environment embedding. (a) speaker embedding with speaker label. (b) speaker embedding with environment label. (c) environment embedding with environment label. (d) environment embedding with speaker label.\"}"}
{"id": "tan22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Embedding classification against speaker and environment of proposed and baseline system\\n\\n| Combination     | System   | Classification accuracy |\\n|-----------------|----------|------------------------|\\n|                 | Top-1    | Top-5                  |\\n|                 | Top-1    | Top-5                  |\\n| seen            | Proposed | $51.0\\\\%$ $71.2\\\\%$     |\\n|                 | Baseline | $43.8\\\\%$ $67.3\\\\%$      |\\n| unseen          | Proposed | $16.8\\\\%$ $31.2\\\\%$      |\\n|                 | Baseline | $4.8\\\\%$ $13.6\\\\%$       |\\n\\nTable 3: MOS with 95% confidence interval on speaker similarity and environment similarity between synthesized speech and reference speech\\n\\n| MOS  | Proposed     | Baseline |\\n|------|--------------|----------|\\n|      | speaker similarity |         |\\n| seen | $3.88 \\\\pm 0.09$ | $3.76 \\\\pm 0.10$ |\\n|      | environment similarity |         |\\n| seen | $3.74 \\\\pm 0.10$ | $3.47 \\\\pm 0.11$ |\\n|      | speaker similarity |         |\\n| unseen| $3.18 \\\\pm 0.10$ | $1.98 \\\\pm 0.11$ |\\n|      | environment similarity |         |\\n| unseen| $3.25 \\\\pm 0.10$ | $2.33 \\\\pm 0.11$ |\\n\\n4.2. Objective evaluation\\n\\n4.2.1. Mel-cepstrum distortion\\n\\nObjective evaluation on synthesized speech utterances is carried out using the metric of mel-cepstral distortion (MCD). The following three cases are investigated. In all cases, the ground-truth speech is obtained by convolving the clean speech from the respective speaker with the selected RIR.\\n\\n1. The synthesized speech is generated with speaker-environment combinations covered in the training dataset (i.e., \u201cseen combinations\u201d). The MCD result can reflect the system\u2019s general performance in speech generation;\\n2. The synthesized speech is generated with new speaker-environment combinations that are not covered by the training dataset (i.e., \u201cunseen combinations\u201d). More precisely, both the speaker identity and the environment type are included in the training data but their combination is not. The MCD result can reflect the system\u2019s efficacy of disentanglement of speaker and environment factors;\\n3. The synthesized speech is generated with both the speaker identity and the environment type that are not included in system training (i.e., \u201cunseen speaker\u201d and \u201cunseen environment\u201d). The MCD result can reflect the generalization capacity of the systems.\\n\\nThe results of objective evaluation are given in Table 1. The proposed system is not as good as the baseline system in generating speech on the seen speaker-environment combinations. However, for unseen speaker-environment combination, the proposed system can attain significant lower MCD than the baseline system, indicating that it is able to disentangle the speaker and environment factors and model them independently. The proposed system performs slightly better than the baseline system for unseen speaker and unseen environment, indicating that the proposed system has slightly better generalization capacity for speaker and environment.\\n\\n4.2.2. Classification of embeddings\\n\\nTwo classifiers are trained on speaker embeddings and environment embeddings respectively. The embeddings extracted from natural speech are used as the input for the training of classifiers, while the ground-truth speaker or environment labels are used as training output for classification.\\n\\nWith the trained classifiers, two kinds of synthesized speech are examined. The first one is the same as Case 1 in Section 4.2.1, i.e., with seen speaker-environment combinations. The second one is the same as Case 2, i.e., unseen combination of seen speaker and seen environment. The trained speaker and environment extractors are used to derive the embeddings from synthesized utterances. The embeddings are used as the input of the corresponding classifier to predict the speaker identity or environment type. The classification results are shown as in Table 2. For the seen combinations of speaker and environment, the proposed system exhibit better accuracy on embedding classification against speaker and environment than the baseline system. For unseen combinations of speaker and environment, the proposed system performs significantly better than the baseline system and demonstrates much better disentanglement capacity of speaker and environment factor.\\n\\n4.3. Subjective evaluation\\n\\nSubjective evaluation is performed on two scenarios. The first scenario is speech generation with seen speaker-environment combinations. In this scenario, a natural reference utterance is used for extracting speaker and environment embeddings, and both embeddings are used in speech generation. Two versions of synthesized utterances are obtained from the proposed and baseline systems respectively. Participants of the evaluation are required to listen to the audio samples and rate the speaker similarity and the environment similarity between each synthesized utterance and the reference one.\\n\\nThe second scenario is speech generation with new combination of seen speaker and seen environment. Two reference utterances are involved. One is used for extracting speaker embedding and the other one for extracting environment embedding. The two embeddings are then used to produce two synthesized utterances with the proposed and the baseline systems. The listeners are required to rate the speaker or environment similarity between each synthesized utterance and the respective reference speech.\\n\\nIn both scenarios, the ratings on speaker and environment similarity range from 1 (\u201ccompletely different\u201d) to 5 (\u201cexactly the same\u201d). 15 listeners participated in the listening test and each of them rated 32 samples. The result is shown as in Table 3. It can be seen that the proposed system performs better than the baseline system, in terms of both speaker and environment similarity in both scenarios. The proposed system demonstrates better disentanglement capacity and generation performance.\\n\\n5. Conclusion\\n\\nIn this paper, we propose an environment-aware TTS system, which is capable of generating speech that carries designated speaker timbre and environment attribute. Characterization and disentanglement of speaker and environment factors in speech are carried out in the system design. Both objective and subjective evaluation results have shown that our proposed system is not only able to model the speaker and environment in speech respectively, but also performs better than baseline system in terms of the speaker and environment similarity of the generated speech. Our system can be further used in text-based speech editing, robust voice cloning and speech de-reverberation.\\n\\n6. Acknowledgements\\n\\nThis research is partially supported by a Knowledge Transfer Project Fund (Ref: KPF20QEP26) from the Chinese University of Hong Kong.\"}"}
