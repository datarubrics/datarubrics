{"id": "veliche24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] R. Tatman, \u201cGender and dialect bias in YouTube\u2019s automatic captions,\u201d in EthNLP@EACL, 2017. [Online]. Available: https://api.semanticscholar.org/CorpusID:13997424\\n\\n[2] N. Mehrabi, F. Morstatter, N. Saxena, K. Lerman, and A. Galstyan, \u201cA survey on bias and fairness in machine learning,\u201d ACM Comput. Surv., vol. 54, no. 6.\\n\\n[3] P. Dheram, M. Ramakrishnan, A. Raju, I.-F. Chen, B. King, K. Powell, M. Saboowala, K. Shetty, and A. Stolcke, \u201cToward fairness in speech recognition: Discovery and mitigation of performance disparities,\u201d in Interspeech 2022, 2022.\\n\\n[4] A. Koenecke, A. Nam, E. Lake, J. Nudell, M. Quartey, Z. Mengkapsha, C. Toups, J. Rickford, D. Jurafsky, and S. Goel, \u201cRacial disparities in automated speech recognition,\u201d Proceedings of the National Academy of Sciences, vol. 117, p. 201915768, 03 2020.\\n\\n[5] M. Riviere, J. Copet, and G. Synnaeve, \u201cASR4REAL: An extended benchmark for speech models,\u201d 10 2021.\\n\\n[6] S. Feng, O. Kudina, B. Halpern, and O. Scharenborg, \u201cQuantifying bias in automatic speech recognition,\u201d 03 2021.\\n\\n[7] T. Kendall and C. Farrington, \u201cThe corpus of regional African American language,\u201d Version, vol. 6, p. 1, 2018.\\n\\n[8] \u201cStanford linguistics, voices of California,\u201d http://web.stanford.edu/dept/linguistics/VoCal/.\\n\\n[9] J. P. Bajorek, \u201cA voice recognition still has significant race and gender biases,\u201d pp. Harvard Business Review, [Online]. Available: https://hbr.org/2019/05/voice\u2013recognition\u2013still\u2013has\u2013significant\u2013race\u2013and\u2013gender\u2013biases/, 5 2019.\\n\\n[10] Z. M. C. Heldreth, M. Lahav, J. Sublewski, and E. Tuennerman, \u201c\\\"I don't think these devices are very culturally sensitive.\\\" - The impact of errors on African Americans in automated speech recognition,\u201d 2021.\\n\\n[11] R. Tatman and C. Kasten, \u201cEffects of talker dialect, gender & race on accuracy of Bing speech and YouTube automatic captions,\u201d in Interspeech, 2017. [Online]. Available: https://api.semanticscholar.org/CorpusID:38523141\\n\\n[12] J. L. Martin and K. Tang, \u201cUnderstanding Racial Disparities in Automatic Speech Recognition: The Case of Habitual 'be',' in Proc. Interspeech 2020, 2020, pp. 626\u2013630.\\n\\n[13] I.-E. Veliche and P. Fung, \u201cImproving fairness and robustness in end-to-end speech recognition through unsupervised clustering,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[14] L. Sar\u0131, M. Hasegawa-Johnson, and C. D. Yoo, \u201cCounterfactually fair automatic speech recognition,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3515\u20133525, 2021.\\n\\n[15] M. Kusner, J. Loftus, C. Russell, and R. Silva, \u201cCounterfactual fairness,\u201d in Proceedings of the 31st International Conference on Neural Information Processing Systems, ser. NIPS\u201917. Red Hook, NY , USA: Curran Associates Inc., 2017, p. 4069\u20134079.\\n\\n[16] J. Meyer, L. Rauchenstein, J. D. Eisenberg, and N. Howell, \u201cArtie bias corpus: An open dataset for detecting demographic bias in speech applications,\u201d in Proceedings of the Twelfth Language Resources and Evaluation Conference. Marseille, France: European Language Resources Association, May 2020, pp. 6462\u20136468. [Online]. Available: https://aclanthology.org/2020.lrec-1.796\\n\\n[17] C. Liu, M. Picheny, L. Sar\u0131, P. Chitkara, A. Xiao, X. Zhang, M. Chou, A. Alvarado, C. Hazirbas, and Y. Saraf, \u201cTowards measuring fairness in speech recognition: Casual conversations dataset transcriptions,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6162\u20136166.\\n\\n[18] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, \u201cThe ICSI meeting corpus,\u201d in 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP \u201903)., vol. 1, 2003, pp. I\u2013I.\\n\\n[19] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d ArXiv, vol. abs/1211.3711, 2012. [Online]. Available: https://api.semanticscholar.org/CorpusID:17194112\\n\\n[20] Y. Shi, Y. Wang, C. Wu, C.-F. Yeh, J. Chan, F. Zhang, D. Le, and M. Seltzer, \u201cEmformer: Efficient memory transformer based acoustic model for low latency streaming speech recognition,\u201d in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6783\u20136787.\\n\\n[21] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d 2022. [Online]. Available: https://arxiv.org/abs/2212.04356\\n\\n[22] Z. Liu, I.-E. Veliche, and F. Peng, \u201cModel-based approach for measuring the fairness in ASR,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 6532\u20136536.\\n\\n[23] B. Efron and R. J. Tibshirani, \u201cAn introduction to the bootstrap,\u201d in CRC Press, 1994.\\n\\n[24] C. D. M. Sharon Goldwater, Dan Jurafsky, \u201cWhich words are hard to recognize? prosodic, lexical, and disfluency factors that increase speech recognition error rates.\u201d Speech Communication, 52 (3), pp.181. ff10.1016/j.specom.2009.10.001ff. ffhal-00608401f, 2010.\"}"}
{"id": "veliche24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards measuring fairness in speech recognition: Fair-Speech dataset\\n\\nIrina-Elena Veliche, Zhuangqun Huang, Vineeth Ayyat Kochaniyan, Fuchun Peng, Ozlem Kalinli, Michael L. Seltzer\\n\\nMeta AI, USA\\nive@meta.com, teddyhuang@meta.com, vineeth.ayyat@gmail.com, fuchunpeng@meta.com, okalinli@meta.com, mikeseltzer@meta.com\\n\\nAbstract\\nThe current public datasets for speech recognition (ASR) tend not to focus specifically on the fairness aspect, such as performance across different demographic groups. This paper introduces a novel dataset, Fair-Speech, a publicly released corpus to help researchers evaluate their ASR models for accuracy across a diverse set of self-reported demographic information, such as age, gender, ethnicity, geographic variation and whether the participants consider themselves native English speakers. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the United States, who were paid to record and submit audios of themselves saying voice commands. We also provide ASR baselines, including on models trained on transcribed and untranscribed social media videos and open source models.\\n\\nIndex Terms: fairness, speech recognition, dataset, age, gender, ethnicity, geographic location, English accent\\n\\n1. Introduction\\nThe performance of current speech recognition (ASR) systems has improved significantly over the last few years, with the emergence of new modeling techniques and considerable amounts of training data. However, most of the improvements are targeted for overall word error rate (WER). The evaluation sets being used tend to lack information associated with the demographic characteristics, such as ethnicity, geographic variation and whether utterances come from native or non-native English speakers. Also, most numbers are reported in aggregate, without giving a more clear picture of the potential gaps between different demographic groups. While there have been many studies showing that ASR systems do not perform equally well for all demographic and accent groups [1, 2, 3, 4, 5, 6], the number of open sourced datasets that can be used for evaluation of such characteristics is limited.\\n\\nIn this paper we introduce a new ASR dataset, Fair-Speech. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the U.S. who were paid to record and submit audio of themselves saying commands. They self-identified their demographic information, such as age, gender, ethnicity, geographic location and whether they consider themselves native English speakers, together with their first language.\\n\\nThe verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases \u2014 music, capture, utilities, notification control, messaging, calling, and dictation \u2014 that can support researchers who are building or have models in those areas. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Some examples of prompts were asking how they would search for a song or make plans with friends, including deciding where to meet. Providing broad prompts to guide the speakers is better than simply asking participants to read text prompts, since that tends to make the audios sound less natural: people would make different kinds of pauses than in natural speech and entities might also not be pronounced properly, if the participants are not familiar with them.\\n\\nOur dataset includes the audio and transcription of participants\u2019 utterances, together with their self-identified labels across the different demographic categories. The intent is for this to be used for evaluating the performance of existing ASR models. The data user agreement prevents a user from developing models that predicts the value of those labels, but one may measure the performance of different models as a function of those labels.\\n\\nBy releasing this dataset, we hope to further motivate the AI community to continue improving the fairness of speech recognition models, which will help everyone have a better experience using applications with ASR.\\n\\n2. Previous work on ASR Fairness\\nAs voice recognition systems have become more integrated into daily lives, especially through the use of voice assistants, there has been a considerable amount of research showing that those systems exhibit biases when it comes to the performance of the ASR models. For example, [4] studied the ability of different ASR systems to transcribe structure interviews of black and white speakers, finding that all of them exhibited substantial racial disparities. The study was done on Corpus of regional African American language [7], a collection of socio-linguistic interviews with dozens of black individuals who speak African American Vernacular English, and also on Voices of California [8], which is a compilation of interviews recorded in both rural and urban areas of California. Prior studies also showed disparities across accents and socio-economic status of the speakers in [5], race and gender bias in [9, 10, 11, 12], regional and non-native accent [6].\\n\\nThere is also some recent work on how some of these demographic biases can be mitigated, such as in [3, 13, 14, 15]. However, some are using in-house datasets, which are difficult to use for comparison.\\n\\nThere are a number of open sourced datasets that can be used for measure the fairness of ASR systems across different demographic groups. Apart from the two mentioned above [7, 8], there is also the Artie Bias corpus [16], a curated set of the Mozilla Common Voice corpus, which contains demographic tags for age, gender, accent. Casual conversations dataset [17] has associated tags for gender, age and skin tone, while the ICSI meeting corpus [18] has associated information on gender, age,\"}"}
{"id": "veliche24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Fair-Speech dataset aims to provide data recorded in a free speech manner from a more diverse set of speakers, where participants self-identified across the different demographic categories.\\n\\n3. Corpus contents\\nThe verbal commands included in this dataset are categorized into seven domains, primarily serving voice assistant use cases\u2014music, capture, utilities, notification control, messaging, calling, and dictation. In response to prompts that relate to each of these domains, dataset participants provided their own audio commands. Our dataset includes the audio and transcription of participants' utterances. The audio is mobile collected. The intention of this dataset is to be used as an evaluation tool, to uncover gaps or biases in ASR models.\\n\\nThis dataset was constructed with the recordings of paid participants who have explicitly provided their consent for their recordings to be used in research, together with the associated demographic information. This ensures that the dataset aligns with the ethical standards e.g. for data collection, respects the privacy and autonomy of the participants, but also promotes transparency and other key ethical considerations in responsible data collection practices.\\n\\nTable 1 shows the per-category distribution of the entire dataset, in terms of number of unique speakers and number of utterances for each demographic sub-group. For age we have a fairly good representation across 18 - 65 groups, with a larger percentage of utterances in the 31 - 45 bucket. The 66+ bucket had too few speakers and utterances, so we chose to not include it here. For gender distribution the percentage of utterances is more balanced. Since we didn't have a significant number of utterances from people who identified as non-binary, we chose to not include them, to not show skewed results. In terms of ethnicity, we have a fairly good representation across multiple categories. The two categories where we have less representation are Native Hawaiian or other Pacific Islander, with 3.6% of total entries and Middle Eastern or North African, with 2.4%. They also have less number of speakers than the other categories. In terms of geographic variation, a bit more than half of the utterances are from people who earn less than $50k per year and there are only 7% from people who earn more than $100k, with only 50 speakers in that sub-group. In terms of linguistic variation, we split the utterances based on native language, whether it's English or not. There is about 80% of the data coming from people whose native language is English. For the other bucket, we provide the first language in the dataset as well. After English, most utterances are from Spanish and Mandarin speakers, with other languages represented in smaller percentages as well.\\n\\n3.1. Breakdown by gender for demographic categories\\nIn Figure 1 we show the breakdowns by gender for each of the demographic categories. While for some categories there is a good balance between the different genders, for example in the 18 - 22 sub-group, for some there are many more utterances coming from male than from female speakers, such as in the Black or African American sub-group, or the other way around, such as in the Asian, South Asian or Asian-American sub-group. This is important to take into account when analyzing results. Therefore, in section 5.1 we'll take into account confounding factors and speaker variability when showing WER gaps between different sub-groups.\\n\\n| Age          | #speakers | #utterances |\\n|--------------|-----------|-------------|\\n| 18 - 22      | 84        | 3846        |\\n| 23 - 30      | 95        | 4168        |\\n| 31 - 45      | 285       | 12770       |\\n| 46 - 65      | 129       | 5687        |\\n\\n| Gender | #speakers | #utterances |\\n|--------|-----------|-------------|\\n| Female | 321       | 14422       |\\n| Male   | 272       | 12049       |\\n\\n| Ethnicity | #speakers | #utterances |\\n|-----------|-----------|-------------|\\n| Asian, South Asian or Asian American | 82     | 3854        |\\n| Black or African American | 180    | 7807        |\\n| Hispanic, Latino or Spanish | 63     | 2814        |\\n| Middle Eastern or North African | 17     | 749         |\\n| Native American, American Indian, or Alaska Native | 105    | 4632        |\\n| Native Hawaiian or Other Pacific Islander | 22     | 969         |\\n| White | 124       | 5646        |\\n\\n| Geographic variation | #speakers | #utterances |\\n|----------------------|-----------|-------------|\\n| Low (US <= $50k)    | 335       | 14780       |\\n| Medium (US = $50k-$100k) | 217    | 9824        |\\n| Affluent (US = $100k+) | 41      | 1867        |\\n\\n| Linguistic variation | #speakers | #utterances |\\n|----------------------|-----------|-------------|\\n| L1                   | 482       | 21528       |\\n| L2                   | 111       | 4943        |\\n\\n3.2. Data transcription\\nTo produce transcription useful for ASR evaluation, all data was verbatim transcribed. Colloquial words were kept as spoken, as well as repeated words. Numbers were spelled out as spoken and entities were transcribed with capital letter. All the other text is lower-cased, without punctuation. Since this is a voice command dataset, each audio contains utterances from a single speaker. The length of the utterances vary. Each recording lasts an average of 7.36 seconds. The maximum length of an utterance is around 1 minute.\\n\\nHigh-quality annotations were achieved through a multi-pass human transcription, resolution, curation, and scoring. The multi-pass annotation by external vendors ran until a three-way agreement. The 10% without agreement was resolved by internal linguistic engineers. Data curation started from a side by side audit comparing annotations with ASR hypotheses coming from models with different sizes. The 650 rows with low confidence were further audited by cultural-specific linguistic vendors. All audited data was reviewed by multiple internal teams for cross-functional validation. Finally, 200 random rows were scored by an internal linguistic expert to obtain the annotation WER (1.47%).\\n\\n4. Speech recognition experiments\\nTo provide some ASR baselines on the ASR fairness dataset, we trained a series of recurrent neural network transducer (RNN-T) [19] models and also used open-sourced models.\\n\\n\u2022 1. Video model, supervised only data: an RNN-T model with an emformer encoder [20], LSTM predictor and a joiner, having approximately 50 million parameters in total. The input feature stride is 6. Encoder network has 13 emformer layers, each with embedded dimension of 480, 4 attention heads, FFN size of 2048. Prediction network is an LSTM layer with 256 hidden units and dropout 0.3. Joint network has 1024 hidden units and a softmax layer of 4096 units for blank and wordpieces. The model was trained on 29.8K hours of English video data that is completely de-identified before transcription. It contains a diverse range of speakers, accents, native language and education level.\"}"}
{"id": "veliche24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We apply distortion and additive noise to the speed-perturbed data. This results in a total of 148.9K hours of training data.\\n\\n- 2. Video model, semi-supervised: a streaming emformer model trained on over 2 million hours of social media videos. 29.8K hours are manually transcribed as described above and the rest is unlabeled data, decoded by larger teacher models. The model has approximately 290M parameters.\\n\\n- 3. Whisper [21]: transformer based model, trained on 1 million hours of weakly labeled audio and 4 million hours of pseudo-labeled audio. We evaluate on both the large-v2 model, which has 1550M parameters and small, which has 244M parameters.\\n\\n5. Results\\n\\nWe computed WER on the Fair-Speech dataset using the four models described above. We also noted the relative gap between the groups with the lowest and highest WER in each category.\\n\\nAs can be seen in Table 2, there are gaps across all demographic groups. Some of the key observations:\\n\\n- The data used in training can have a significant impact on the WER, particularly when it comes to bias between different demographic groups. The relative gap across all the demographic categories for all evaluated models is in double digits for most of the categories. For Whisper models the gap is larger than 40% across all dimensions.\\n\\n- Adding more data in training can significantly improve the performance of the model, such as when semi-supervised data was added for the video models. Interestingly however, for geographic variation, even though the WER improves with more data, the relative gap becomes wider. The linguistic variation difference almost disappears when more data is added in the training, making the dataset more diverse. Also for ethnicity, while the supervised model has some large gaps between different groups, after adding semi-supervised data the gaps decrease and the group with the highest WER actually changes.\\n\\n- As expected, having a larger model improves the accuracy across all the categories, as can be seen with the two Whisper models. However, the relative gap in the linguistic variation sub-group is actually increasing for the larger model.\\n\\n- Data might not be enough to achieve a fair model. All the models shown here were trained on more than 1 million hours of data. However, they exhibit significant gaps across each of the demographic sub-groups. Thus, new modeling techniques are needed to focus on improving the performance for all people. Also, during evaluation, a particular focus needs to be given to demographic breakdowns in addition to overall model accuracy.\\n\\nThere can be many nuances when interpreting these WER.\"}"}
{"id": "veliche24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Evaluation results on the fairness dataset.\\n\\n| Category          | Whisper - small WER | Whisper - large-v2 WER | Video, supervised WER | Video, semi-supervised WER |\\n|-------------------|----------------------|-------------------------|----------------------|---------------------------|\\n| Low (US <= $50k) | 55.73%               | 61.2%                   | 4.13%                | 3.94%                     |\\n| Medium (US = $50k-$100k) | 6.52%               | 9.52%                   | 7.91%                | 8.85%                     |\\n| Affluent (US = $100k+) | 13.3%               | 11.46%                  | 5.3%                 | 5.16%                     |\\n| Hispanic, Latino or Spanish, White | 6.74%               | 6.74%                   | 4.93%                | 4.95%                     |\\n| Hispanic, Latino or Spanish, Middle Eastern or North African | 4.95%               | 4.95%                   | 5.84%                | 5.75%                     |\\n| Hispanic, Latino or Spanish, Black or African American | 4.91%               | 4.91%                   | 5.05%                | 5.08%                     |\\n| Hispanic, Latino or Spanish, Asian, South Asian or Asian American | 4.95%               | 4.95%                   | 5.08%                | 5.08%                     |\\n| Hispanic, Latino or Spanish, Black or African American | 4.95%               | 4.95%                   | 5.08%                | 5.08%                     |\\n| Hispanic, Latino or Spanish, Asian, South Asian or Asian American | 4.95%               | 4.95%                   | 5.08%                | 5.08%                     |\\n\\nWe hope that they will help in evaluating and improving the fairness of speech recognition models.\\n\\nIn this paper, we introduced a new ASR dataset, Fair-Speech, which is used for fairness evaluation when developing speech recognition models. We also run baseline analysis on different models and found that there are statistically significant gaps in WER among different subgroups of the factor (age, gender etc.) as the fixed effect and speaker label as a random effect.\\n\\nWe use confounding factors. Thus, we use a model-based approach to measure fairness, using mixed-effects Poisson regression.[12] found that this is due to phonological, phonetic or linguistic theory, that establishes that women tend to speak a different language, rather than the grammatical or lexical characteristics. This is in line with previous research on racial disparities. In this paper, we also found statistically significant differences between the Black or African American subgroup and all the other subgroups.\\n\\nWhen analyzing the results, we employed a model-based approach to analyze WER results using model-based approach.\\n\\nAnalyze WER results using model-based approach.\\n\\nFor this analysis, we used the video semi-supervised approach to measure fairness, using mixed-effects Poisson regression with the demographic group we focus on (age, gender etc.) as the fixed effect and speaker label as a random effect. We hope that they will help in evaluating and improving the fairness of speech recognition models.\\n\\nWe analyze WER results using model-based approach. We hope that they will help in evaluating and improving the fairness of speech recognition models.\\n\\nWhen analyzing the results, we employed a model-based approach to analyze WER results using model-based approach.\\n\\nAnalyze WER results using model-based approach.\"}"}
