{"id": "talkar22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "talkar22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speech Acoustics in Mild Cognitive Impairment and Parkinson\u2019s Disease With and Without Concurrent Drawing Tasks\\n\\nTanya Talkar 1,2, Christina Manxhari 3, James R. Williamson 1, Kara M. Smith 3, Thomas F. Quatieri 1,2\\n\\n1 Human Health and Performance Systems, MIT Lincoln Laboratory, Lexington, MA, USA\\n2 Speech and Hearing Bioscience and Technology, Harvard University, Boston, MA, USA\\n3 Department of Neurology and NeuroNexus Institute, University of Massachusetts Chan Medical School, Worcester, MA, USA\\n\\nttalkar@g.harvard.edu, christina.manxhari@umassmed.edu, jrw@ll.mit.edu, kara.smith@umassmemorial.org, quatieri@ll.mit.edu\\n\\nAbstract\\nParkinson\u2019s disease (PD) is characterized by motor dysfunction; however, non-motor symptoms such as cognitive decline also have a dramatic impact on quality of life. Current assessments to diagnose cognitive impairment take many hours and require high clinician involvement. Thus, there is a need to develop new tools leading to quick and accurate determination of cognitive impairment to allow for appropriate, timely interventions. In this paper, individuals with PD, designated as either having no cognitive impairment (NCI) or mild cognitive impairment (MCI), undergo a speech-based protocol, involving reading or listing items within a category, performed either with or without a concurrent drawing task. From the speech recordings, we extract motor coordination-based features, derived from correlations across acoustic features representative of speech production subsystems. The correlation-based features are utilized in Gaussian mixture models to discriminate between individuals designated NCI or MCI in both the single and dual task paradigms. Features derived from the laryngeal and respiratory subsystems, in particular, discriminate between these two groups with AUCs > 0.80. These results suggest that cognitive impairment can be detected using speech from both single and dual task paradigms, and that cognitive impairment may manifest as differences in vocal fold vibration stability.\\n\\nIndex Terms: speech protocol, dual task, acoustic analysis, parkinson\u2019s disease, motor coordination, machine learning\\n\\n1. Introduction\\nNearly 1 million individuals in the United States have been diagnosed with Parkinson\u2019s Disease (PD), and approximately 60,000 Americans are diagnosed every year [1]. PD is a neurodegenerative disorder that causes tremor, bradykinesia, rigidity, and gait and balance impairment [1, 2, 3, 4]. However, individuals with PD may also be affected by many non-motor symptoms as the disease progresses, including the introduction of cognitive impairment (CI) which can range from mild cognitive impairment (MCI) to dementia [5, 6, 7]. Current assessment tools to detect and monitor CI in PD have substantial limitations. Brief screening assessments like the Montreal Cognitive Assessment (MoCA) are not sensitive to early, mild deficits or to change over time, while detailed neuropsychological testing is impractical as it requires several hours of testing and access to a trained neuropsychologist [8]. There is a need to develop more sensitive cognitive assessment tools that can be administered quickly and easily. In particular, it is critical to detect MCI since these individuals are at high risk of progressing to dementia and may be more responsive to therapies earlier in their course [9]. Such tools would enable clinicians to detect CI earlier and monitor an individual\u2019s cognitive status over time.\\n\\nPD has wide-ranging impacts on the speech production system that can be assessed through acoustic features. Motor speech deficits arise from basal ganglia dysfunction in PD [10]. Impairments in PD range across articulatory, laryngeal, and respiratory realms, manifesting as decreased amplitude and velocity of lip and jaw movements, decreased quality of voice, monopitch and monoloudness, and a reduced number of words per breath group [11, 12, 13, 14, 15]. The basal ganglia dysfunction in PD may also lead to differences in coordination within and across all speech production subsystems, which could be observable through acoustic measures.\\n\\nAcoustic markers have helped predict MCI, typically through MoCA scores. Previous work with correlation-based features, a proxy for articulatory coordination, as well as phoneme-based features resulted in a correlation of 0.52 between actual and predicted MoCA scores in individuals with PD [16]. Machine learning models consisting of features derived from reading disfluency errors resulted in a correlation of 0.64 [17]. While these studies have shown promise in detecting PD-MCI, it is important to extend the analysis to assess speech acoustic performance through cognitively challenging speech tasks which can allow for more robust screening [18]. Additionally, due to the limitations of the MoCA described above, in this study, we apply a more stringent categorization of no CI (NCI) or MCI based on an extensive cognitive battery.\\n\\nIn PD, motor task performance worsens when an additional motor modality is added as a dual task (e.g. speech and drawing simultaneously). Much of dual task research focuses on evaluation of performance during walking, with a simultaneous cognitive or speech task [19, 20]. One of the only studies focused on acoustic feature changes, Whitfield et al. (2019) investigated acoustic feature changes in individuals with PD for reading and spontaneous speech tasks performed in isolation (single task) versus simultaneously drawing circles counter-clockwise (dual task). With this low-demand drawing task, there was little change to acoustic features [21]. To our knowledge, however, there have been no studies evaluating the effect of CI on dual task performance.\"}"}
{"id": "talkar22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mance or the relative cognitive difficulty of a task on speech acoustics. Due to the additional cognitive burden's possible effect on motor movements, we aim to assess the effect of varying dual tasks on speech production subsystems relative to the effect of single task paradigms in individuals with NCI or MCI.\\n\\nIn this paper, we first introduce the design of a protocol involving speech and drawing tasks of varying cognitive difficulty for the purpose of screening for MCI. We implement the protocol and develop models to predict cognitive status (MCI vs. no cognitive impairment) using motor coordination-based features. We identify acoustic features associated with PD-MCI during single task and dual task conditions, interpret their physiological implications, and assess differences in model performance utilizing features from the single vs. dual task paradigms.\\n\\n2. Methods\\n\\n2.1. Participants\\n\\nParticipants were recruited at UMass Chan Medical School (UMCMS)/UMass Memorial Healthcare under a larger study characterizing speech in individuals with PD. All participants were individuals older than 18, with a disease duration \u2265 2 years and no diagnosis of dementia. Participants were excluded if they had a neurological disorder other than PD, were non-English speakers, had unintelligible speech due to effects of PD, or had other voice, speech, or swallowing disorders. Eighteen participants with PD participated in the protocol described in this paper. Clinical assessments included the Movement Disorders Society - Unified PD Rating Scale (MDS-UPDRS) and the Geriatric Depression Scale. Cognitive assessment included the MoCA and a validated battery of neuropsychological tests representing all cognitive domains, which was utilized in a consensus conference to classify participants as either No Cognitive Impairment (NCI) or Cognitive Impairment No Dementia [8]. For the purposes of this study, CIND was equivalent to MCI. One subject was excluded from further analysis due to an indeterminate ruling from the neuropsychological testing. This resulted in nine individuals designated as NCI (2 Females; age 67.19 \u00b1 7.28 years) and eight individuals designated as MCI (4 Females; age 69.19 \u00b1 7.93 years), for a total of seventeen participants. The experiment was approved by the UMCMS Institutional Review Board, and all participants signed an informed consent form prior to participation in the study.\\n\\n2.2. Protocol\\n\\nTable 1: Protocol Tasks: Reading\\n\\n| Single | Dual |\\n|--------|------|\\n| Read story | Draw ten circles counter-clockwise & read sentence |\\n\\nTable 2: Protocol Tasks: Listing\\n\\n| Single | Dual |\\n|--------|------|\\n| List words starting with \\\"B\\\" | Copy triangle shape & list words starting with \\\"T\\\" alternating with types of animals |\\n| Copy triangle shape & list words starting with \\\"C\\\" | List words starting with \\\"T\\\" alternating with types of trees, plants, or flowers |\\n| Draw circles counter-clockwise & list words starting with \\\"C\\\" alternating with types of fruits or vegetables | Draw circles counter-clockwise & list words starting with \\\"C\\\" |\\n\\nAll participants participated in a set of 15 tasks, presented in one of three random orderings. For the purposes of this analysis, we focused on recordings of the 10 tasks which included speech tasks of reading or listing words. The reading tasks are shown in Table 1 and the listing tasks are described in Table 2. Each listing task was timed to last one minute. All experimental sessions were conducted in a quiet clinic exam room at UMCMS. During the experimental session, voice recordings (48000 Hz) and handwriting data (62.5 Hz) were collected using a custom MATLAB script run on a Microsoft Surface Book Pro with the built-in microphone.\\n\\n2.3. Acoustic low-level feature extraction\\n\\nAcoustic features and their delta time series were extracted from each recording to represent three speech production subsystems: articulatory, laryngeal, and respiratory. Formants (vocal tract resonances) and mel-frequency cepstral coefficients (MFCCs) were used to represent the articulatory subsystem. MFCCs were extracted at 200 Hz using the Praat software [22]. The first three formants (F1-F3) were extracted using the Kalman-based autoregressive moving average (KARMA) software, which derives vocal tract resonances using an energy-based voice detector [23].\\n\\nFundamental frequency (F0), harmonic-to-noise ratio (HNR), cepstral peak prominence (CPP), and creak were selected as features to represent movements in the laryngeal subsystem and voice quality [24]. F0 and HNR were extracted using the Praat software [22, 25] at 1000 Hz and 100 Hz respectively. CPP and creak were extracted at 100 Hz with custom MATLAB scripts [26, 27]. To represent the speech envelope, a reflection of the laryngeal and respiratory subsystem, intensity was extracted at 100 Hz using a custom MATLAB script. This software provides a smooth contour of amplitude peaks based on an iterative time-domain signal envelope estimation, capturing both the contributions of the respiratory system and resonance-harmonics interaction to the amplitude modulation of the speech envelope [28].\\n\\n2.4. High-level correlation-based features\\n\\nProxy measures of coordination within and across the underlying motor systems were calculated using multivariate auto- and cross-correlations of the low-level acoustic features and their delta time series [29, 30]. Channel-delay correlation matrices, expanding the dimensionality of the low-level time series, are constructed using the acoustic time series. The correlation matrices represent coupling strengths across the acoustic time series at multiple relative time delays, which are a reflection of the time-delay embedding space of these features. The process and delay scales used are described in further detail in [29].\\n\\nTo capture the interaction within and across speech production subsystems, correlations were calculated for individual and\"}"}
{"id": "talkar22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 3: Acoustic feature combinations used for correlation analysis. Analysis also included the deltas of these features.\\nThe subsystem of the base features is provided in parentheses: Articulatory (A), Laryngeal (L), and Respiratory (R).\\n\\n| Base Features       | Cross Subsystem Correlations                          |\\n|---------------------|-------------------------------------------------------|\\n| Formants (A)        | F0 x Formants                                        |\\n| MFCCs (A)           | F0 x Envelope                                        |\\n| F0 (L)              |Envelope x Formants                                    |\\n| HNR (L)             |F0 x Envelope x Formants                              |\\n| CPP (L)             |F0 x CPP                                              |\\n| Creak (L)           |F0 x HNR                                              |\\n| Envelope (R)        |F0 x HNR x CPP                                        |\\n|                     | F0 x HNR x CPP x envelope                            |\\n|                     | HNR x CPP x envelope                                  |\\n|                     | CPP x envelope                                        |\\n\\nCombinations of representative low-level features for single and dual tasks. Feature combinations (e.g. F0 correlated with formants) were calculated by concatenating the feature vectors. When correlated with F0, other features were interpolated to 1000 Hz using spline interpolation. There were a total of 34 feature and delta-feature combinations used, shown in Table 3. This captured correlations, as a proxy for motor coordination, within and across the three speech production subsystems.\\n\\nThe eigenvalues from all resulting correlation matrices were extracted in rank-order (from largest to smallest). Each correlation matrix with $n$ channel inputs resulted in $15 \\\\times n$ eigenvalues. For example, correlations of F0 x formants resulted in 60 eigenvalues, due to four input channels. Eigenvalues from each delay scale for a single task and feature combination were concatenated to form a final feature vector with $n \\\\times 15 \\\\times 4$ elements, which was then input into a classifier.\\n\\n### Figure 1: Gaussian Mixture Model (GMM) architecture used to discriminate between participants designed as NCI or MCI.\\n\\n#### 2.5. Bootstrapping classification\\nClassification of participants into NCI and MCI categories was done using a nested bootstrapping methodology to generate GMMs for each correlation feature, focusing on a subset of tasks [31]. This is described in Figure 1. The full feature matrix entered into the model creation pipeline was an aggregation of all of the feature rows across participants and the particular tasks focused on (e.g. single task list tasks). The GMMs were generated using an ensemble of ten GMMs created over four iterations with the training data of the bootstrap iteration. NCI and MCI GMMs were generated using a supervised adaptation technique [32]. The model output score, used to determine performance metrics, was the ratio of the log of the sum likelihood of the participant belonging to the MCI GMMs over the log of the sum likelihood of the participant belonging to the NCI GMMs. The model output scores were used to determine the area under the receiver operating characteristic (ROC) curve (AUC), which is found by plotting the true positive rate against the false positive rate at various threshold scores.\\n\\nIn the outer bootstrap iteration, a randomized balanced set of the data belonging to one NCI participant and one MCI participant was held out as a test set, corresponding to multiple rows. In the inner loop, a set of 100 bootstrap iterations was run holding out a validation set of one NCI and one MCI individual on each iteration, and performing principal component analysis (PCA) on the remaining data to generate GMMs. The number of principal components (PCs) yielding the highest average AUC across the iterations was chosen to reduce the dimensionality of the full training set and generate the NCI and MCI GMMs. The AUC for the outer bootstrap iteration was calculated using the test set, using the same PCA transform as the training set. The final reported AUC was the average AUC calculated across 100 bootstrap iterations. The AUC distributions from each model were compared using Wilcoxon rank-sum tests to determine whether significant differences existed between the performance on single or dual tasks, with p-values corrected for multiple comparisons using the Benjamini-Hochberg comparison with a false discovery rate of 25%.\\n\\nFused models, fusing across all of the acoustic features, were created by leaving out the data from one randomly selected NCI participant and one randomly selected MCI participant as above. With the remaining training set, 34 models were generated, one for each feature, following the same methodology as described above, to generate the NCI and MCI GMM ensembles. For each feature model, the model score was evaluated on the held out test data. The final score for each test data row was the average of the model scores across all feature models. The final scores determined the AUC for the iteration. This was repeated for a total of 100 bootstrap iterations and the final reported AUC was the average across all iterations. This helped determine whether the features provided complimentary information to boost performance for a particular task set.\\n\\n### 3. Results\\n#### 3.1. Classification\\n\\n| Subsystem Feature           | Single | Dual   | Dual - Single AUC |\\n|-----------------------------|--------|--------|-------------------|\\n| Laryngeal x Respiratory     |        |        |                   |\\n| Envelope x CPP             | 0.69   | 0.80   | 0.11              |\\n| Envelope x HNR x CPP       | 0.67   | 0.75   | 0.08              |\\n| Laryngeal F0 x HNR x CPP   | 0.75   | 0.71   | 0.04              |\\n| F0 x CPP                   | 0.81   | 0.67   | 0.14              |\\n| delta-F0 x delta-CPP       | 0.74   | 0.62   | 0.12              |\\n| All Fused (All Features)   | 0.67   | 0.74   |                   |\\n\\nComparisons were computed for two different combinations of tasks: (1) single modality list tasks vs dual modality list tasks and (2) single modality reading tasks vs dual modality reading tasks. Table 4 shows the features that resulted in AUCs >0.70 in either the single modality or dual modality case.\"}"}
{"id": "talkar22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for the list tasks, the p-value from the Wilcoxon rank-sum test, as well as the AUC resulting from the fused models. An AUC of 0.74 was achieved by fusing across all of the features in the dual modality case. AUCs \u2265 0.80 were achieved with the correlation of envelope x CPP from the dual modality tasks, and the correlation of F0 x CPP from the single modality tasks.\\n\\nTable 5 shows the two features that resulted in AUCs > 0.70 in either the single or dual modality case for the reading tasks. The features with significant differences between single vs. dual modality AUCs are bolded. Fewer features discriminated as well between NCI and MCI individuals in the reading tasks as compared to the list-based tasks. An AUC > 0.80 was achieved with CPP extracted from the single modality reading tasks. Table 5:\\n\\n| Subsystem | Feature | Single | Dual |\\n|-----------|---------|--------|------|\\n| Laryngeal | CPP     | 0.84   | 0.44 |\\n| Articulatory | delta-MFCC | 0.66   | 0.73 |\\n\\n4. Discussion\\n\\nIn this paper, we describe a speech and drawing protocol, focusing on single and dual tasks, and analytical methodology to determine the cognitive status of individuals with PD. GMMs generated from correlation features derived from acoustic time series representative of the laryngeal speech production subsystem were able to discriminate between NCI individuals and MCI individuals with AUCs > 0.80. This is the first study to our knowledge to assess the cognitive status of individuals with PD during a variety of speech tasks simultaneously with drawing tasks of increasing difficulty. This work provides insight into the type of speech and drawing tasks which can be used to predict cognitive status in these individuals, and provide evidence for the use of correlation features, representing motor coordination of underlying speech production subsystems, in discriminating between NCI and MCI. It also highlights that fusing acoustic features derived from a list-generation speech task appears to lead to better discrimination between NCI and MCI individuals as compared to fusing features from a reading task, especially when combined with a simultaneous drawing task. Altogether, the results show promise in the use of this approach to augment existing clinical determinations of cognitive status, perhaps leading to an at-home protocol which could be used for tracking of status in between clinical visits.\\n\\nAnalysis of the top performing features in both the listing- and reading-based tasks highlighted differences present in the laryngeal and respiratory speech subsystems. Specifically, an AUC of 0.80 was achieved from the correlation of speech envelope and CPP derived from the dual modality listing tasks, an AUC of 0.81 was achieved from the correlation of F0 and CPP derived from the single modality listing tasks, and an AUC of 0.84 was achieved from the correlation of CPP derived from the single modality reading task. The presence of CPP in many of these features further implies that cognitive impairment may manifest in the ability to regulate vocal fold vibrations, and differences also arise from the interactions between subglottal pressure and vocal fold openings and closings, as well as pitch control. The only articulatory-based feature that discriminated with an AUC > 0.70 was delta-MFCC from the dual modality reading task. This perhaps indicates that while articulatory differences may exist between individuals with PD and healthy controls, as seen in [11, 12, 13, 14], the variation in articulatory deficits may be more subtle when detecting CI with these tasks. This may also be implied by the degradation of performance when all features are combined for fusion. It will be important to validate these findings and interpretations in a larger group of individuals, and also compare to healthy controls to determine if the differences are localized to the laryngeal and respiratory speech subsystems in PD. Future work will also incorporate spontaneous speech to compare performance across different tasks and assess how cognitive impairment manifests in the speech production subsystems in this context.\\n\\nIn this particular analysis, we were not able to make a definitive assessment of whether dual modality list-based and reading-based tasks appeared to perform better than their single modality equivalent tasks, as the top performing features appeared to have success in both cases. However, fusion across all of the features did imply that dual modality listing tasks (AUC = 0.74) did perform better than single modality listing tasks (AUC = 0.67), and showed that single and dual modality reading tasks performed below chance (AUC < 0.50). This suggests that the cognitive load of list-generation helps differentiate between NCI and MCI individuals. We did, however, expect larger differences to exist between performance on the single and dual task performance. In this paper, we did not control for the number of words uttered during the minute in the list-generation task, which may affect acoustic performance if those in the MCI group produced fewer words overall in both single and dual task paradigms. In addition, there may be compensatory processes which were more active in the MCI group because they found the dual task harder, and therefore their acoustic performance improved relative to the single task, compared to the NCI group which may have found the task much easier. Future work will incorporate additional speech production based features, such as phoneme-based features, which may help further elucidate the effect of the dual task on both groups.\\n\\nIt is also important to analyze motor coordination patterns in single and dual task paradigms for the drawing data collected from the tablet. We have found in previous work that coordination-based analysis of handwriting and drawing discriminated between children with and without autism spectrum disorder [33]. Further analysis with the current dataset and future participants will aim to look at the discriminative ability of features derived from the drawing trajectory data, as well as commonalities that may exist between speech motor coordination patterns and hand drawing patterns in the protocol.\\n\\nFinally, we aim to analyze longitudinal changes with continued assessments, moving to an at-home collection. This would allow for more frequent assessments to better detect and monitor cognitive impairment in PD. In addition, the protocol and analysis presented here could be applied to other neurological disorders in which motor speech and cognitive function are also impaired to help detangle these deficits.\\n\\n5. Acknowledgements\\n\\nTT was supported by NIH-NIDCD under Grant F31DC019509. KS was supported by NIH-NIDCD under Grant K23DC016656. We thank Cyrus Zabetian MD and Brenna Cholerton PhD for contributing their expertise to consensus conferences.\"}"}
