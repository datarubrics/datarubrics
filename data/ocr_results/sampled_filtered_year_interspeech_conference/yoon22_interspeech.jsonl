{"id": "yoon22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nNon-verbal expressions of politicians are important in election. In particular, the emotional intensity of politician revealed in a debate can be strongly linked to voters' evaluation. This paper proposes a multimodal deep-learning model for predicting the perceived emotional intensity of a candidate, which utilizes voice, face, and gesture to capture the comprehensive information of one's emotional intensity revealed in a debate. We collect a dataset of political debate videos from the 2020 Democratic presidential primaries in the USA, and train the proposed model with randomly sampled clips from the debate videos. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls.\\n\\n1. Introduction\\nNon-verbal expressions of politicians are important in election. Scholars have shown that non-verbal signals such as facial expressions, eye blink, and vocal pitch can affect voters' attitudes and evaluations on political leaders [1, 2, 3, 4, 5, 6, 7, 8]. For instance, Sullivan et al [3] showed that both American and French voters respond positively to the positive emotional displays of political leaders. Boussalis and Coan [4] found that an angry face can positively affect voters' attitudes toward politicians in televised debates.\\n\\nThe emotional intensity of politicians revealed in videos, e.g., debates or news, is an important factor that can affect the decision making process of voters [6, 7, 8, 9]. For example, Klofstad et al [8] showed that the voters prefer female politicians with lower pitched voices. Dietrich et al found that the Congresswomen who show high emotional intensity while talking about 'women issues' receive positive evaluations from women [9].\\n\\nWhile prior work mostly used the vocal pitch of politicians in measuring the emotional intensity for understanding the voting behavior, however, relatively little attention has been paid to utilize multiple modalities such as facial expression, gesture, and vocal expression in modeling the emotional intensity. Instead of simply calculating the pitch of a person's voice to measure the intensity, we consider diverse types of emotional intensity by capturing multiple factors including facial expression [10], body movements [11], etc. In this way, our work goes one step further by comprehensively quantifying and measuring the perceived emotional intensity with a computational approach and investigating how it can affect voting behavior.\\n\\n* Corresponding authors.\\n\\nTo this end, we propose a multimodal deep-learning model that takes non-verbal features as inputs and predicts the perceived emotional intensity of a candidate revealed in a video clip. Inspired by prior work [10, 11, 9], we focus on non-verbal features such as vocal, facial, and gestural features to train the proposed model. As a case study, we collected a dataset of political debate videos of the 2020 Democratic presidential primaries in the USA. Note that the televised political debates can be representative data for capturing and analyzing the non-verbal behaviors of politicians. We then recruit annotators to label the perceived emotional intensity of each candidate who is shown in a random subset of short clips, which are used to train the proposed model. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively related with the changes in candidates' favorability in public polls.\\n\\n2. Debate Data\\nIn this section, we introduce a debate data used in our study. We first describe the collected televised debate videos, and then introduce (i) how to annotate emotional intensity of the collected videos, and (ii) how to extract non-verbal features.\\n\\n2.1. Data Collection\\nWe collected videos for each of the 11 debates from the source of the televised debate. Each debate is sponsored by a news channel or a TV station such as NBC, ABC, and CBS, who made high-quality videos available for download. We use the maximum quality available (generally 720p or higher) for better fine-grained feature extraction. The total length of the collected videos is 34 hours, 25 minutes, and 58 seconds with each debate lasting 2-2.5 hours on average. Note that we use the timestamped transcripts of each debate with speaker information.\\n\\n2.2. Data Annotation\\nTo build a training set for the emotional intensity prediction task, we first randomly sampled 40 short video clips (i.e., 5 secs long) for each candidate. We then recruited eight college students to annotate the emotional intensity score for the given video clips. The annotators are asked to rate the emotional intensity score for the given short video clips on a scale of 1 to 5 (1 being the lowest and 5 being the highest). As a result, we obtain 920 short video clips with emotional intensity scores (i.e., 40 video clips \\\\times 23 candidates). The mean and standard deviation of the annotated (perceived) emotional intensity score were 0.55 (95% C.I. 0.54 - 0.56) and 0.17, respectively. The inter-rater reliability for emotional intensity annotation was ex-\"}"}
{"id": "yoon22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1 shows the average annotated intensity score of 23 candidates. As illustrated in Figure 1, Kirsten Gillibrand shows the highest emotional intensity, whereas Michael Bloomberg shows the lowest. Note that we rescaled the emotional intensity score to 0\u20131.\\n\\n2.3. Non-verbal Feature Extraction\\n\\n2.3.1. Vocal Features\\n\\nTo extract vocal features from a given video clip, we employ Librosa, a well-known open-source toolkit for audio processing. We first split the audio waveform of each video clip into seconds (i.e., one segment = one second). In each second, we then extract 13 Mel-Frequency Cepstral Coefficients (MFCCs) features and obtain flattened 572-dimensional vectors. Hence, our final vocal features have a shape of 5 (seconds) \u00d7 572 per sample.\\n\\n2.3.2. Facial Features\\n\\nFor facial feature extraction, we utilize dlib [12], a popular open-source software for computer vision tasks such as face recognition and verification. We extract 68 facial landmarks (i.e., x and y coordinates) for each frame (1 FPS) in the video clip, and obtain 136-dimensional vectors by concatenating each x and y coordinates. If a face is not detected in a frame, we replace it with zero vectors. Our final facial features have a shape of 5 (seconds) \u00d7 136 per sample.\\n\\n2.3.3. Gestural Features\\n\\nWe use OpenPose [13], a state-of-the-art open-source pose estimation software, to extract gesture information. As most of the movements of candidates come from the upper part of the body, we consider 13 key points from nose, neck, shoulders, elbows, wrists, eyes and ears. Similar to the facial landmark extraction process, we extract 13 body key points for each frame in the video clip. We then obtain 26-dimensional vectors by concatenating each x and y coordinates. Our final gestural features have a shape of 5 (seconds) \u00d7 26 per sample.\\n\\n3. Emotional Intensity Prediction\\n\\nIn this section, we first introduce the problem statement that describes the objective of the proposed model. We then present the overall architecture of the proposed model to predict perceived emotional intensity in the given video clip.\\n\\n3.1. Problem Statement\\n\\nThe goal of the proposed model is to predict perceived emotional intensity by learning low-level non-verbal features including vocal, facial, and gestural features. More specifically, we define this problem as a regression problem that predicts emotional intensity score ranging between 0\u20131 for a given short video clip. Suppose we have a set of video clips $C = \\\\{c_n\\\\} | C| n = 1$ and each clip can be represented as $c_n = (X_{nm} \\\\in \\\\mathbb{R}^{t \\\\times d_m}, X_{nf} \\\\in \\\\mathbb{R}^{t \\\\times d_f}, X_{nb} \\\\in \\\\mathbb{R}^{t \\\\times d_b})$ where $X_{nm}$, $X_{nf}$, $X_{nb}$, $d_m$, $d_f$, $d_b$, and $t$ represent the vocal features, the facial features, the gestural features, the dimension of vocal features, the dimension of facial features, the dimension of gestural features, and the length of sequences, respectively. By learning a set of clips $C$, the proposed model can predict the perceived emotional intensity score of the given clip by learning latent features of candidates' non-verbal behaviors in debates.\\n\\nFigure 2 illustrates the overall architecture of the proposed model. To generate acoustic and visual representations from the low-level non-verbal feature vectors, the model employs two encoders, acoustic and visual encoders, respectively. The...\"}"}
{"id": "yoon22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"acoustic encoder takes vocal features as input to generate acoustic representation as follows:\\n\\n\\\\[ U_a = \\\\text{Dropout}(\\\\text{AcousticEncoder}(X^{nm})) \\\\]  \\n\\nwhere AcousticEncoder(\u00b7), Dropout(\u00b7), \\\\( X^{nm} \\\\), and \\\\( U_a \\\\) denote the acoustic encoder, dropout, vocal features, and acoustic representation, respectively.\\n\\nOn the other hand, the visual encoder concatenates the facial features and the gestural features, and employs the stacked LSTM to make visual representation, \\\\( U_v \\\\), calculated as follows:\\n\\n\\\\[ U_v = \\\\text{Dropout}(\\\\text{VisualEncoder}(X^{nf} \\\\oplus X^{nb})) \\\\]  \\n\\nwhere VisualEncoder(\u00b7), Dropout(\u00b7), \\\\( X^{nf} \\\\), \\\\( X^{nb} \\\\), and \\\\( U_v \\\\) denote the visual encoder, dropout, facial features, gestural features, and visual representation, respectively.\\n\\nNote that both acoustic and visual encoders apply the same architecture (i.e., stacked LSTM).\\n\\nFinally, the intensity prediction layer colligates acoustic and visual representations to generate a multimodal representation and predict a perceived emotional intensity for the given clip as follows:\\n\\n\\\\[ Z = \\\\text{Dropout}(\\\\text{Fuse}(U_a \\\\oplus U_v)) \\\\]  \\n\\n\\\\[ \\\\hat{Y} = \\\\text{Sigmoid}(\\\\text{Fpred}(Z)) \\\\]  \\n\\nwhere Fuse(\u00b7), Fpred(\u00b7), Sigmoid(\u00b7), and \\\\( Z \\\\) denote the fully connected layer for fusion, fully connected layer for intensity prediction, sigmoid activation function, and multimodal representation, respectively.\\n\\nThe emotional intensity prediction task is defined as a regression problem, and the mean squared error [15] and Adam [16] as the loss function and optimizer, respectively.\\n\\n### 4. Experiments\\n\\n#### 4.1. Experimental Settings\\n\\nWe split the dataset into the train and test sets with a 8:2 ratio. We also set the number of units, dropout rate, batch size, epochs, and learning rate to 8, 0.25, 128, 300, and 0.0002, respectively. Note that all weights are randomly initialized.\\n\\n#### 4.2. Baseline Methods\\n\\nTo evaluate the overall performance of the proposed model, we compare with the following five methods: (i) Support Vector Machine ([17] SVM), (ii) K-Nearest Neighbors ([18] KNN), (iii) Random Forest ([19] RF), (iv) Early Fusion LSTM ([EF-LSTM]), and (v) Late Fusion LSTM ([LF-LSTM]). For SVM, KNN, and RF, we aggregate all features by averaging and concatenating vectors. For EF-LSTM, we concatenate the vocal, facial, and gestural features, and pass them to the LSTM layer followed by a fully connected layer with sigmoid activation function. Lastly, for LF-LSTM, we concatenate the outputs of three different LSTMs (i.e., vocal/facial/gestural features), and add a fully connected layer with sigmoid activation function to predict emotional intensity.\\n\\n#### 4.3. Experimental Results\\n\\n#### 4.3.1. Overall Performance\\n\\nTable 1 shows the root mean squared error (RMSE) and the mean absolute percentage error (MAPE) of the baseline models and the proposed model. As shown in Table 1, the proposed model shows high emotional intensity performance (i.e., 0.12 of RMSE and 0.20 of MAPE), which outperforms other baseline models. This indicates that the proposed fusion method using the stacked LSTM can capture distinct indicators in predicting emotional intensity. Among the baselines, we find that RF shows the higher performance than the traditional machine-learning methods. This is because RF randomly selects subsets of high dimensional input features (i.e., vocal + facial + gestural features) via the bagging process to avoid overfitting. We also find that LF-LSTM achieves higher performance than EF-LSTM. This suggests that aggregating features at the decision-level (i.e., late fusion) helps the model capture important signals for emotional intensity prediction. We provide the prediction results of the proposed model available at: [https://dsail-skku.github.io/INTERSPEECH2022/](https://dsail-skku.github.io/INTERSPEECH2022/).\\n\\n| Model         | RMSE  | MAPE  |\\n|---------------|-------|-------|\\n| SVM           | 0.15  | 0.24  |\\n| RF            | 0.13  | 0.23  |\\n| KNN           | 0.15  | 0.23  |\\n| EF-LSTM       | 0.15  | 0.26  |\\n| LF-LSTM       | 0.14  | 0.22  |\\n| The Proposed Model | 0.12  | 0.20  |\\n\\n#### 4.3.2. Analysis on Different Modalities\\n\\nTo analyze the importance of each input modality for predicting emotional intensity, we conduct a performance analysis on the unimodal and bimodal models. For the unimodal model, we use a stacked LSTM layer to generate unimodal representation followed by the intensity prediction layer in Figure 2. As shown in Table 2, the model trained with the vocal features achieves higher performances (0.15 of RMSE and 0.25 of MAPE) than the models trained with the facial and the gestural features. That is, the candidates' vocal characteristics are more useful than their faces and gestures in predicting perceived emotional intensity. For the bimodal model, we first concatenate two different input features, and feed them into a stacked LSTM layer followed by the intensity prediction layer. Here, most of the bimodal models show better performance than unimodal models, implying that considering multiple modalities except V+F improves the performance. It is worth to note the performance improvement by the F+G model, which reveals that the candidates' facial and gestural features play a complementary role in predicting emotional intensity.\\n\\n### 5. Case Study: 2020 Democratic Party Presidential Primaries in the USA\\n\\nWe now investigate whether there is a relation between the perceived emotional intensity and the changes in candidates' favorability. We first define the debate performance (i.e., net favorability) on politicians. We next apply the proposed model...\"}"}
{"id": "yoon22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Performance comparisons on unimodal, bimodal, and trimodal emotional intensity prediction models. V, F, and G denote vocal, facial, and gestural features, respectively.\\n\\n| Type   | Modality | RMSE | MAPE |\\n|--------|----------|------|------|\\n| Unimodal | V        | 0.15 | 0.25 |\\n|         | F        | 0.17 | 0.30 |\\n|         | G        | 0.16 | 0.26 |\\n| Bimodal | V + F    | 0.15 | 0.26 |\\n|         | V + G    | 0.14 | 0.24 |\\n|         | F + G    | 0.14 | 0.23 |\\n| Trimodal| V + F + G| 0.12 | 0.20 |\\n\\nTo the rest (i.e., not in our training set) of the videos in 2020 Democratic Party presidential primaries in the USA, to extract the emotional intensity of each candidate. Finally, we calculate the linear correlation between the perceived emotional intensity and the net favorability.\\n\\n5.1. Net Favorability\\nTo measure debate performance (i.e., voting behavior) on politicians, we leverage the before and after polls of FiveThirtyEight and Morning Consult for obtaining favorability and unfavorability ratings [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30]. Note that FiveThirtyEight claims that the \u201cpoll is based on a nationally-representative probability sample of adults age 18 or older. Questions presented in this document were only asked of those who are likely to vote in the Democratic primary or caucus (n=3,360).\u201d We calculate the performance metric as the net change in the (favorability-unfavorability percentages) for before and after the debate, which we shorten to the name as net favorability. Change in favorability provides an equal baseline for all candidates and thus is expected to be more representative of debate skill rather than an accumulation of historic events compared to other metrics.\\n\\nFormally, we compute the net favorability as follows:\\n\\n\\\\[ N_{ij} = \\\\text{Before Favorability} - \\\\text{Before Unfavorability} \\\\]  \\n\\\\[ N'_{ij} = \\\\text{After Favorability} - \\\\text{After Unfavorability} \\\\]  \\n\\\\[ P_{ij} = N'_{ij} - N_{ij} \\\\]\\n\\nwhere \\\\( i \\\\) denotes debate number, \\\\( j \\\\) denotes candidate appearing in one or more debate, and \\\\( (7) \\\\) represents the net favorability.\\n\\n5.2. Emotional Intensity Extraction\\nTo extract the emotional intensity of each candidate from the videos the test set, we first segment video clips where only one candidate appears and speaks (i.e., over 15h). We next apply the proposed model to predict the emotional intensity score every 5-second time window. For example, if a video clip is 8 seconds long, our model predicts emotional intensity during 1 to 5, 2 to 6, 3 to 7, and 4 to 8 seconds, respectively. We finally calculate the average score of the predicted emotional intensities for the video clip.\\n\\n5.3. Emotional Intensity and Net Favorability\\nWe investigate whether there is a relation between a candidate's emotional intensity and his/her net favorability. To this end, we first calculate the mean and the standard deviation (std.) of emotional intensity for each candidate in 11 different debates. Note that a candidate can appear in one or more debates. We then measure the Pearson correlation coefficient between mean and std. of emotional intensity and net favorability. As a result, we find no significant relation between the mean emotional intensity and the net favorability (\\\\( r = -0.08, p = 0.39 \\\\)), implying that a candidate's mean emotional intensity does not affect performance. However, as shown in Figure 3, we find that there is a positive correlation between std. of emotional intensity and the net favorability (\\\\( r = 0.21, p < 0.05 \\\\)), implying the more candidate shows dynamics during his/her speech, the more positive changes in his/her favorability can happen.\\n\\nFigure 3: Scatter plot for standard deviation of emotional intensity and net favorability.\\n\\n6. Conclusion\\nIn this paper, we proposed a multimodal deep-learning model that uses non-verbal features as inputs and predicts the perceived emotional intensity. To this end, we collected a novel multimodal debate dataset from the 2020 Democratic Party presidential primaries in the USA. To train our proposed model, we extracted non-verbal features including vocal, facial, and gestural features. Our model achieved high emotional intensity prediction performance that outperforms other baseline methods. Our case study on the 2020 Democratic Party presidential primaries in the USA shows that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls. We believe the proposed model is useful in understanding non-verbal communications of political leaders. Furthermore, the model can be used as a research tool to extract the perceived emotional intensity from videos.\\n\\n7. Acknowledgements\\nThis research was supported by the framework of international cooperation program managed by the National Research Foundation of Korea (NRF-2020K2A9A2A11103842) and the National Research Foundation (NRF) of Korea Grant funded by the Korean Government (MSIT) (No. 2021R1A4A3022102).\"}"}
{"id": "yoon22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] R. D. Masters, D. G. Sullivan, J. T. Lanzetta, G. J. McHugo, and B. G. Englis, \\\"The facial displays of leaders: Toward an ethology of human politics,\\\" *Journal of Social and Biological Structures*, vol. 9, no. 4, pp. 319\u2013343, 1986.\\n\\n[2] R. D. Masters and D. G. Sullivan, \\\"Nonverbal displays and political leadership in France and the United States,\\\" *Political Behavior*, vol. 11, no. 2, pp. 123\u2013156, 1989.\\n\\n[3] D. G. Sullivan, \\\"Emotional responses to the nonverbal behavior of French and American political leaders,\\\" *Political Behavior*, vol. 18, no. 3, pp. 311\u2013325, 1996.\\n\\n[4] C. Boussalis and T. G. Coan, \\\"Facing the electorate: Computational approaches to the study of nonverbal communication and voter impression formation,\\\" *Political Communication*, vol. 38, no. 1-2, pp. 75\u201397, 2021.\\n\\n[5] C. Boussalis, T. G. Coan, M. R. Holman, and S. M\u00fcller, \\\"Gender, candidate emotional expression, and voter reactions during televised debates,\\\" *American Political Science Review*, vol. 115, no. 4, pp. 1242\u20131257, 2021.\\n\\n[6] R. C. Anderson and C. A. Klofstad, \\\"Preference for leaders with masculine voices holds in the case of feminine leadership roles,\\\" *PloS one*, vol. 7, no. 12, p. e51216, 2012.\\n\\n[7] R. C. Anderson, C. A. Klofstad, W. J. Mayew, and M. Venkatachalam, \\\"Vocal fry may undermine the success of young women in the labor market,\\\" *PloS one*, vol. 9, no. 5, p. e97506, 2014.\\n\\n[8] C. A. Klofstad, R. C. Anderson, and S. Peters, \\\"Sounds like a winner: Voice pitch influences perception of leadership capacity in both men and women,\\\" *Proceedings of the Royal Society B: Biological Sciences*, vol. 279, no. 1738, pp. 2698\u20132704, 2012.\\n\\n[9] B. J. Dietrich, M. Hayes, and D. Z. O'brien, \\\"Pitch perfect: Vocal pitch and the emotional intensity of congressional speech,\\\" *American Political Science Review*, vol. 113, no. 4, pp. 941\u2013962, 2019.\\n\\n[10] S. Nowicki Jr and J. Carton, \\\"The measurement of emotional intensity from facial expressions,\\\" *The Journal of Social Psychology*, vol. 133, no. 5, pp. 749\u2013750, 1993.\\n\\n[11] M. Sun, Y. Mou, H. Xie, M. Xia, M. Wong, and X. Ma, \\\"Estimating emotional intensity from body poses for human-robot interaction,\\\" 2018 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 2018.\\n\\n[12] D. E. King, \\\"Dlib-ml: A machine learning toolkit,\\\" *Journal of Machine Learning Research*, vol. 10, p. 1755\u20131758, Dec. 2009.\\n\\n[13] Z. Cao, T. Simon, S.-E. Wei, and Y. Sheikh, \\\"Real-time multi-person 2D pose estimation using part affinity fields,\\\" in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2017, pp. 7291\u20137299.\\n\\n[14] S. Hochreiter and J. Schmidhuber, \\\"Long short-term memory,\\\" *Neural Computation*, vol. 9, no. 8, pp. 1735\u20131780, 1997.\\n\\n[15] I. Goodfellow, Y. Bengio, and A. Courville, *Deep learning*. MIT press, 2016.\\n\\n[16] D. P. Kingma and J. Ba, \\\"Adam: A method for stochastic optimization,\\\" Computing Research Repository (CoRR), vol. abs/1412.6980, 2015.\\n\\n[17] B. E. Boser, I. M. Guyon, and V. N. Vapnik, \\\"A training algorithm for optimal margin classifiers,\\\" in *Proceedings of the Fifth Annual Workshop on Computational Learning Theory*, 1992, pp. 144\u2013152.\\n\\n[18] J. M. Keller, M. R. Gray, and J. A. Givens, \\\"A fuzzy k-nearest neighbor algorithm,\\\" *IEEE Transactions on Systems, Man, and Cybernetics*, no. 4, pp. 580\u2013585, 1985.\\n\\n[19] L. Breiman, \\\"Random forests,\\\" *Machine Learning*, vol. 45, no. 1, pp. 5\u201332, 2001.\\n\\n[20] J. Wolfe, \\\"A final look at who won and lost the first democratic debates,\\\" Jul. 2019. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-poll/\\n\\n[21] \u2014\u2014, \\\"Who won the third democratic debate?\\\" Sep. 2019. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-september-poll/\\n\\n[22] \u2014\u2014, \\\"Who won the fourth democratic debate?\\\" Oct. 2019. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-october-poll/\\n\\n[23] \u2014\u2014, \\\"Who won the fifth democratic debate?\\\" Nov. 2019. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-november-poll/\\n\\n[24] \u2014\u2014, \\\"Who won the december democratic debate?\\\" Dec. 2019. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-december-poll/\\n\\n[25] \u2014\u2014, \\\"Who won the january democratic debate?\\\" Jan. 2020. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-january-poll/\\n\\n[26] \u2014\u2014, \\\"Who won the new hampshire democratic primary debate?\\\" Feb. 2020. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-first-february-poll/\\n\\n[27] \u2014\u2014, \\\"Who won the south carolina democratic debate?\\\" Feb. 2020. [Online]. Available: https://projects.fivethirtyeight.com/democratic-debate-south-carolina-poll/\\n\\n[28] \u2014\u2014, \\\"Who won the biden-sanders debate?\\\" Mar. 2020. [Online]. Available: https://projects.fivethirtyeight.com/biden-sanders-debate-poll/\\n\\n[29] N. Silver, \\\"Polls since the second debate show kamala harris slipping,\\\" Aug. 2019. [Online]. Available: https://fivethirtyeight.com/features/polls-since-the-second-debate-show-kamala-harris-slipping/\\n\\n[30] E. Yokley, \\\"Bloomberg loses ground following debate debut in las vegas,\\\" Feb. 2020. [Online]. Available: https://morningconsult.com/2020/02/21/michael-bloomberg-polling-post-debate-las-vegas/\"}"}
