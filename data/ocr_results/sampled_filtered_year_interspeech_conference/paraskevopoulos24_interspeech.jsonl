{"id": "paraskevopoulos24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acknowledgements\\n\\nFunded by the European Union. PREMIERE Project (No. 101061303). Views and opinions expressed are however those of the authors only and do not necessarily reflect those of the European Union.\\n\\nReferences\\n\\n[1] A. Graves and N. Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in Proceedings of the 31st International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, E. P. Xing and T. Jebara, Eds., vol. 32, no. 2. Beijing, China: PMLR, 22\u201324 Jun 2014, pp. 1764\u20131772. [Online]. Available: https://proceedings.mlr.press/v32/graves14.html\\n\\n[2] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised Cross-Lingual Representation Learning for Speech Recognition,\u201d in Proc. Interspeech 2021, 2021, pp. 2426\u20132430.\\n\\n[3] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[4] J. Kahn, A. Lee, and A. Hannun, \u201cSelf-training for end-to-end speech recognition,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7084\u20137088.\\n\\n[5] A. Ragni, K. M. Knill, S. P. Rath, and M. J. Gales, \u201cData augmentation for low resource languages,\u201d in INTERSPEECH 2014: 15th annual conference of the international speech communication association. International Speech Communication Association (ISCA), 2014, pp. 810\u2013814.\\n\\n[6] S. Singh, F. Hou, and R. Wang, \u201cA Novel Self-training Approach for Low-resource Speech Recognition,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 1588\u20131592.\\n\\n[7] S. Gandhi, P. von Platen, and A. M. Rush, \u201cDistil-whisper: Robust knowledge distillation via large-scale pseudo labelling,\u201d arXiv preprint arXiv:2311.00430, 2023.\\n\\n[8] A. Clifton, S. Reddy, Y. Yu, A. Pappu, R. Rezapour, H. Bonab, M. Eskevich, G. Jones, J. Karlgren, B. Carterette et al., \u201c100,000 podcasts: A spoken english document corpus,\u201d in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 5903\u20135917.\\n\\n[9] E. Garmash, E. Tanaka, A. Clifton, J. Correia, S. Jat, W. Zhu, R. Jones, and J. Karlgren, \u201cCem mil podcasts: A spoken portuguese document corpus for multi-modal, multi-lingual and multi-dialect information access research,\u201d in International Conference of the Cross-Language Evaluation Forum for European Languages. Springer, 2023, pp. 48\u201359.\\n\\n[10] D. Galvez, G. Diamos, J. Torres, K. Achorn, J. Cer\u00f3n, A. Gopi, D. Kanter, M. Lam, M. Mazumder, and V. Janapa Reddi, \u201cThe people\u2019s speech: A large-scale diverse english speech recognition dataset for commercial usage,\u201d in Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks, J. Vanschoren and S. Yeung, Eds., vol. 1. Curran, 2021. [Online]. Available: https://datasets-benchmarks-proceedings.neurips.cc/paper/files/paper/2021/file/202cb962ac59075b964b07152d234b70-Paper-round1.pdf\\n\\n[11] C. Guoguo, C. Shuzhou, W. Guanbo, and et al., \u201cGigaspeech: An evolving, multi-domain asr corpus with 10,000 hours of transcribed audio,\u201d in Proc. Interspeech 2021, 2021.\\n\\n[12] M. Bain, J. Huh, T. Han, and A. Zisserman, \u201cWhisperX: Time-Accurate Speech Transcription of Long-Form Audio,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 4489\u20134493.\\n\\n[13] A. Plaquet and H. Bredin, \u201cPowerset multi-class cross entropy loss for neural speaker diarization,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 3222\u20133226.\\n\\n[14] H. Bredin, \u201cpyannote.audio 2.1 speaker diarization pipeline: principle, benchmark, and recipe,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 1983\u20131987.\\n\\n[15] J. Grosman, \u201cFine-tuned XLSR-53 large model for speech recognition in Greek,\u201d https://huggingface.co/jonatasgrosman/wav2vec2-large-xlsr-53-greek, 2021.\\n\\n[16] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020, pp. 4211\u20134215.\\n\\n[17] K. Park and T. Mulc, \u201cCSS10: A Collection of Single Speaker Speech Datasets for 10 Languages,\u201d in Proc. Interspeech 2019, 2019, pp. 1566\u20131570.\\n\\n[18] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 798\u2013805.\\n\\n[19] V. Digalakis, D. Oikonomidis, D. Pratsolis, N. Tsourakis, C. Vosnidis, N. Chatzichrisafis, and V. Diakoloukas, \u201cLarge vocabulary continuous speech recognition in greek: corpus and an automatic dictation system,\u201d in Proc. 8th European Conference on Speech Communication and Technology (Eurospeech 2003), 2003, pp. 1565\u20131568.\\n\\n[20] G. Paraskevopoulos, T. Kouzelis, G. Rouvalis, A. Katsamanis, V. Katsouros, and A. Potamianos, \u201cSample-efficient unsupervised domain adaptation of speech recognition systems: A case study for modern greek,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 32, pp. 286\u2013299, 2024.\\n\\n[21] V. Manohar, D. Povey, and S. Khudanpur, \u201cJhu kaldi system for arabic mgb-3 asr challenge using diarization, audio-transcript alignment and transfer learning,\u201d in 2017 IEEE automatic speech recognition and understanding workshop (ASRU). IEEE, 2017, pp. 346\u2013352.\\n\\n[22] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding. IEEE Signal Processing Society, 2011.\"}"}
{"id": "paraskevopoulos24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data\\n\\nGeorgios Paraskevopoulos1, Chara Tsoukala1, Athanasios Katsamanis1, Vassilis Katsouros1\\n\\n1Institute for Speech and Language Processing, Athena Research Center, Athens, Greece\\n\\n{g.paraskevopoulos,chara.tsoukala,nkatsam,vsk}@athenarc.gr\\n\\nAbstract\\n\\nThe development of speech technologies for languages with limited digital representation poses significant challenges, primarily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages.\\n\\nIndex Terms: speech recognition, low-resource languages, whisper, weak supervision\\n\\n1. Introduction\\n\\nSpeech technologies, specifically Automatic Speech Recognition (ASR), have evolved in recent years transitioning towards end-to-end systems [1, 2]. The good performance achieved by neural ASR systems has accelerated research and given rise to innovative industry applications.\\n\\nBuilding upon these advancements, the field has seen significant progress in developing more efficient and effective end-to-end systems. However, this evolution has been mostly focused on languages with abundant digital resources, primarily English, leading to a stark imbalance in technology accessibility and performance across languages. The introduction of massively multilingual speech recognition models [2, 3] has begun to address this disparity, offering a pathway to include digitally underrepresented languages. Despite these efforts, there remains an urgent need for creating and expanding new corpora specifically tailored to these languages, but current efforts are costly and fragmented.\\n\\nIn addressing the challenge of developing comprehensive and dynamically updated corpora for the data-intensive demands of contemporary neural models, self-training [4] emerges as a cost-effective strategy. This method has gained more relevance with current advancements in model performance, improving its effectiveness for augmenting language data resources. Notably, self-training has been effectively utilized for low-resource languages, as demonstrated by Ragni et al. [5] within a hybrid DNN-HMM framework and by Singh et al. [6] through an iterative pseudo-labeling scheme. The development of Distil-Whisper has further validated the potential and efficiency of self-training techniques [7], where efficient versions of Whisper have been crafted using pseudo-labeled corpora with minimal impact on model performance.\\n\\nTo address the scarcity of data, podcasts can be used to create diverse and extensive speech corpora. In 2020, Clifton et al. [8] released the Spotify Dataset, which consists of 100,000 episodes and nearly 60,000 hours of English speech. This dataset was later augmented to include Portuguese podcasts [9]. Despite its initial value, the Spotify Dataset has ceased maintenance and is no longer accessible. Among other significant contributions to the field, the People's speech dataset [10] and Gigaspeech [11] have been developed using podcasts, yet both predominantly focus on English. To the best of our knowledge, there are no podcast corpora tailored to low-resource languages. The potential benefits of establishing such resources are substantial, as, combined with self-training and pseudo-labeling, such corpora can be used to train larger speech models.\\n\\nIn this work, we assemble a podcast corpus for Modern Greek (Greek Podcast Corpus; GPC), spanning 16 distinct domains to facilitate multi-domain evaluation. The WhisperX pipeline [12] is used for corpus segmentation and transcription, in conjunction with Whisper large-v3 [3], a cutting-edge, massively multilingual ASR model. We collect an untranscribed pre-training corpus with 3124 hours of speech, and a transcribed ASR corpus divided into training, validation, and test sets, with 623, 443, and 13 hours of speech respectively. The training, validation, and test splits are evenly stratified across the 16 domains. Additionally, we identify two single-speaker podcasts with high-quality audio, suitable for TTS training purposes. To gauge the efficacy of self-training in enhancing ASR capabilities, we fine-tune Whisper-small and Whisper-medium on the newly constructed ASR corpus using varying quantities of speech. Our evaluation includes standard corpora, not included in the fine-tuning set. Furthermore, we conduct a thorough analysis across different domains using the podcast test set. The results demonstrate the positive impact of weakly-supervised fine-tuning on Greek ASR performance, with observed improvements scaling with model size and training data quantity.\\n\\nOur contributions are a) the creation of a large, multi-domain podcast corpus for Modern Greek, which can be easily expanded to thousands of speech hours, and b) extensive multi-domain and out-of-training evaluation demonstrating the effectiveness of utilizing weakly supervised corpora. The trained model checkpoints, as well as recipes to recreate the corpora and models, are publicly available1 for research purposes, align-\\n\\n1https://github.com/georgepar/greek_podcasts_asr\"}"}
{"id": "paraskevopoulos24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Analysis of the collected podcasts in the GPC pre-training corpus per category.\\n\\n| Domain      | Total hours | #podcasts | #episodes |\\n|-------------|-------------|-----------|-----------|\\n| Arts        | 368         | 24        | 1977      |\\n| Business    | 151         | 12        | 539       |\\n| Comedy      | 249         | 23        | 667       |\\n| Education   | 262         | 35        | 1003      |\\n| HealthFitness | 165       | 20        | 899       |\\n| History     | 126         | 12        | 666       |\\n| KidsFamily  | 60          | 8         | 214       |\\n| Leisure     | 243         | 10        | 669       |\\n| Music       | 217         | 10        | 2974      |\\n| News        | 176         | 10        | 2374      |\\n| Science     | 114         | 8         | 147       |\\n| SocietyCulture | 275      | 34        | 1159      |\\n| Sports      | 258         | 23        | 873       |\\n| Technology  | 151         | 8         | 1037      |\\n| TrueCrime   | 122         | 7         | 230       |\\n| TVFilm      | 187         | 11        | 409       |\\n| **Total**   | **3124**    | **255**   | **15837** |\\n\\ning with the AI Act (Art. 2, Par. 6) Note, we do not redistribute the original data, rather we provide scripts for corpus reproduction.\\n\\n2. Background: The WhisperX pipeline\\n\\nThe collected podcasts are available in long-form audio format, but current ASR pipelines require segmented and aligned audio for training. In the case of Whisper, the model can handle samples with maximum duration $T_0 = 30$ seconds. To segment and transcribe the collected speech corpus we utilize the WhisperX pipeline \\\\[12\\\\]. This pipeline yields a list of speech segments of roughly 30-second duration, paired with the obtained transcription. The pipeline includes four steps:\\n\\n1. Voice Activity Detection (VAD): The input audio is split into segments that contain speech (active), and inactive segments that contain silence, music, noise, etc. The pyannote VAD model \\\\[13, 14\\\\] is chosen for this step.\\n\\n2. Cut and Merge: Following VAD, segments containing active speech are further processed to fit the input constraints of the Whisper model. Longer segments are divided into smaller parts at points of low VAD scores. Additionally, adjacent segments of very short duration are combined until they reach a maximum length of 30 seconds.\\n\\n3. Transcription: The core of the pipeline leverages the whisper-large-v3 model \\\\[15\\\\], trained on a vast dataset comprising 1 million hours of weakly labeled audio and an additional 4 million hours of pseudo-labeled audio. This model, consisting of 1550M parameters, yields a $10^{-20\\\\%}$ improvement over the previous version.\\n\\n4. Alignment: The final step involves obtaining word-level timestamps, using a phoneme-level model for forced alignment. We use a version of XSLR-53 \\\\[15\\\\] which has been fine-tuned on the Greek parts of Common Voice (CV) \\\\[16\\\\]. Bain et al. \\\\[12\\\\] have demonstrated this to yield more accurate and robust timestamps than Whisper.\\n\\n3. The Greek Podcast Corpus\\n\\n3.1. Data collection and preprocessing\\n\\nThe data collection consists of two steps and is built using available open-source tools. First, we build a web crawler to collect RSS feeds of relevant podcasts. RSS (Really Simple Syndication) is a web feed format used to publish frequently updated content, such as blog entries, news headlines, or podcasts, in a standardized, machine-readable format.\\n\\nAfter collecting the RSS feeds, the audio can be downloaded using an open-source \u201cpodcatching\u201d tool. After the audio is downloaded, we convert it to WAV format, single-channel, at 16 kHz sampling rate. At the time of writing, this process has yielded 3124 hours of audio, which can be readily used as a pre-training corpus. Table 1 breaks down the collected corpus statistics per category.\\n\\nAdditionally, during data collection we manually identified 3 podcasts by 2 speakers (24 and 123 hours respectively), with clean, single-speaker speech, that may be useful for training TTS models. We include them in the GPC corpus for future use by the research community.\\n\\n3.2. Subcorpora and splits\\n\\nTo construct the ASR corpus, we categorize the podcasts and exclude those categories that either have only a minimal number of podcasts or predominantly feature speech in \u201cpurist\u201d Greek, which is an archaic variant of Modern Greek. This process results in a selection of 16 diverse categories. Following this, we randomly select 50 hours from each category to assemble a stratified and varied ASR corpus totaling 800 hours of audio, denoted as GPC-50 for the rest of this paper. Within this corpus, we designate a test split and a validation split, allocating 1 hour per category (for a total of 16 hours) and 15 minutes per category (for a total of 4 hours), respectively. The rest of the audio is allocated to the GPC-50 training set. We also create smaller training subsets from the GPC-50 training set by sampling 20, 10, 5, and 2 hours from each category, resulting in the GPC-20, GPC-10, GPC-5, and GPC-2 subsets. These contain total audio durations of 320, 160, 80, and 32 hours, respectively. Fig. 1 illustrates the created subsets.\\n\\n5 We collect RSS feeds from https://podcastaddict.com/?lang=el. This pipeline can be run for any of the 27 languages included in this website.\"}"}
{"id": "paraskevopoulos24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Automatic Transcription\\n\\nGPC-50 is transcribed and segmented utilizing the WhisperX pipeline. Following this, we implement two post-processing procedures. First, we eliminate segments where Whisper produces hallucinations, commonly occurring during music sections not filtered out by VAD. These can be readily identified through the \\\"\u03a5\u03c0\u03cc\u03c4\u03b9\u03c4\u03bb\u03bf\u03b9 AUTHORIZED\\\" (trans. \\\"AUTHORIZED subtitles\\\") caption. Subsequently, due to the phoneme-level aligner\u2019s monolingual design, segments containing code-switched English-Greek phrases may present inaccurate timestamps and were consequently removed. This issue mainly arises when English text appears at the beginning or end of a transcript, for instance, \\\"\u03b1\u03ba\u03bf\u03bb\u03bf\u03c5\u03b8\u03ae\u03c3\u03c4\u03b5 \u03bc\u03b5 \u03c3\u03c4\u03bf Instagram\\\" (trans. \\\"follow me on Instagram\\\"). We observe an approximate 20% reduction in data size due to the inactive segment filtering by VAD and our post-processing. This leaves 623,3813 hours of useable training, validation, and test data respectively.\\n\\n4. Experimental setup\\n\\n4.1. Experimental details\\n\\nGiven our limited computing budget, we fine-tune whisper-small and whisper-medium (with 244 M and 769 M parameters respectively) on GPC for speech recognition for 4 epochs. We perform fine-tuning for different amounts of training data on GPC-{50,20,10,5,2} training splits. For training, we use a linear warmup schedule of 500 steps, the batch size is set to 16 samples per device and accumulate gradients every 4 training steps. The learning rate is set to $10^{-5}$ and we set the maximum generation length to 225. We use 16-bit floating point arithmetic and the standard \\\"sdpa\\\" attention implementation during training. Fine-tuning runs on 1 RTX 3090 GPU with 24 GB VRAM for all scenarios except for fine-tuning whisper-medium on GPC-20 and GPC-50, where we use 2 RTX 3090 GPUs and halve the gradient accumulation steps to maintain an equivalent effective batch size across experiments. During decoding, we normalize the reference and transcriptions and report the Word Error Rate (WER) over the samples of the respective test sets.\\n\\n4.2. Datasets\\n\\nTo assess the impact of continued training on weakly supervised data, we evaluate the fine-tuned models using the standard test sets of four diverse speech corpora in Modern Greek. These corpora are used exclusively for evaluation purposes. Although we do not utilize these corpora for fine-tuning, we cannot confirm whether they have been encountered during Whisper\u2019s pre-training phase. The four benchmark corpora are the following:\\n\\n- **Fleurs** [18] is a multilingual speech corpus targeted for few-shot evaluation of speech models. It contains 3000 sentences by English Wikipedia, read by male and female participants, across 104 languages, resulting in 1400 hours of parallel speech. The Greek subset of Fleurs contains a total of 13 hours of speech, with the test set containing 2 hours of speech.\\n\\n- **Common Voice (CV)** [16] version 11.0, accessed on April 27, 2022 is a crowdsourced, multilingual corpus of dictated speech, developed by Mozilla through a web or iPhone app. Contributors read prompts sourced from public domains, e.g., books and Wikipedia, limited to 15 words. The dataset, comprising 18 hours of validated speech from 325 contributors aged 19 to 59 with male and female voices, is split into the standard train, development, and test sections. The CV test set contains 2 hours of speech.\\n\\n### Table 2: Evaluation on the GPC-50 test set.\\n\\n| Dataset Model | Finetuning corpus | WER   |\\n|---------------|-------------------|-------|\\n| small GPC-50  |                   | 12.81 |\\n| medium GPC-50 |                   | 9.95  |\\n| large-v2      |                   |       |\\n| large-v3      |                   |       |\\n\\n### Table 3: Evaluation on standard out-of-training Greek corpora.\\n\\n| Dataset Model | Finetuning corpus | WER   |\\n|---------------|-------------------|-------|\\n| small GPC-50  |                   | 16.4  |\\n| medium GPC-50 |                   | 14.03 |\\n| large-v2      |                   |       |\\n| large-v3      |                   |       |\\n\\n- **LG** [19] is a pioneering corpus for Large Vocabulary Continuous Speech Recognition (LVCSR) in Greek, featuring 33136 newscast utterances (72 hours of speech) from 125 speakers (55 males, 70 females) associated with the well-known \\\"Eleftherotypia\\\" newspaper in Greece. These recordings, captured under diverse acoustic settings, include sessions in a soundproof room, a quiet room, and an office environment. On average, utterances last 7.8 seconds. The dataset\u2019s transcriptions are rich, including speech and non-speech sounds (e.g., \u2018cough\u2019), Greek words in lowercase with stress marks, and numbers spelled out. We remove non-speech events from the transcriptions during evaluation. The LG test set contains 9 hours.\\n\\n- **HParl** [20] is a corpus for Modern Greek, collected from the recorded proceedings of the Greek parliament. The corpus contains 50 parliamentary sessions with 120 hours of speech from 387 speakers, and includes transcripts recorded in real-time by secretaries. The audio is segmented using an iterative alignment algorithm [21], implemented using Kaldi [22]. The average duration of the resulting segments is 4.6 seconds. The acoustic conditions of HParl are challenging since large reverberation is present, due to the recording conditions in the parliament chamber. The HParl test set contains 11 hours of speech.\\n\\n- **Logotypografia (LG)** [19] is a pioneering corpus for Large Vocabulary Continuous Speech Recognition (LVCSR) in Greek, featuring 33136 newscast utterances (72 hours of speech) from 125 speakers (55 males, 70 females) associated with the well-known \u201cEleftherotypia\u201d newspaper in Greece. These recordings, captured under diverse acoustic settings, include sessions in a soundproof room, a quiet room, and an office environment. On average, utterances last 7.8 seconds. The dataset\u2019s transcriptions are rich, including speech and non-speech sounds (e.g., \u2018cough\u2019), Greek words in lowercase with stress marks, and numbers spelled out. We remove non-speech events from the transcriptions during evaluation. The LG test set contains 9 hours.\\n\\n- **HParl** [20] is a corpus for Modern Greek, collected from the recorded proceedings of the Greek parliament. The corpus contains 50 parliamentary sessions with 120 hours of speech from 387 speakers, and includes transcripts recorded in real-time by secretaries. The audio is segmented using an iterative alignment algorithm [21], implemented using Kaldi [22]. The average duration of the resulting segments is 4.6 seconds. The acoustic conditions of HParl are challenging since large reverberation is present, due to the recording conditions in the parliament chamber. The HParl test set contains 11 hours of speech.\"}"}
{"id": "paraskevopoulos24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Results\\n\\nOur experiments revolve around three main research questions:\\n\\ni) Does fine-tuning on pseudo-labeled corpora help with unseen datasets\\n\\nii) What is the ASR performance across domains, and\\n\\niii) How does performance scale with increasing data size.\\n\\nIn Table 2, we present the evaluation of whisper-small, medium, large-v2 on the test set of the GPC-50 corpus. We assess the out-of-the-box Whisper models and the versions of Whisper small and medium fine-tuned on the GPC-50 training set. We observe a significant reduction in WER for the fine-tuned versions, with approximately 4x and 2x absolute improvements for small and medium, respectively. Both fine-tuned versions outperform whisper-large-v2.\\n\\nAnswering the first research question, we evaluate the fine-tuned models across four standard corpora, with results displayed in Table 3. We assess all Whisper variants from Table 2, including the large-v3 version. Once again, fine-tuning results in significant, albeit more moderate, improvements compared to the pre-trained versions of Whisper-small and Whisper-medium. In the cases of Fleurs, CV, and LG, the fine-tuned models perform competitively with their larger counterparts, even surpassing them in the CV dataset. For HP, we note that large-v3 exhibits a substantial performance advantage over the other variants, possibly due to its expanded training set, which may include speech with similar acoustic characteristics and/or employs reverberation-based augmentation.\\n\\nIn Fig. 2, we evaluate the different Whisper versions across the 16 domains in the GPC-50 test set. We observe that the fine-tuned versions consistently outperform whisper-large-v2 across all domains. Furthermore, we note variations in difficulty across domains. For instance, \\\"Comedy,\\\" which features laughter and conversational speech, appears to be more challenging. Conversely, \\\"Education\\\" and \\\"True Crime,\\\" characterized by slow, clear speech from a single speaker and better recording conditions, result in a WER in the range of 4\u20135.\\n\\nFinally, we fine-tune the small and medium variants of whisper on varying amounts of training speech and evaluate on the GPC-50 test set. The results can be seen in Fig. 3. We verify that the performance scales with increasing the training corpus size, surpassing the large-v2 variant when we use the GPC-5 subset with 60 hours of useable speech.\\n\\n6. Conclusions\\n\\nIn this paper, we explored the impact of pseudo-transcriptions for training speech recognition models for low-resource languages. For this, we collected an evolving corpus for Modern Greek from podcasts. The Greek Podcast Corpus has been transcribed with the WhisperX pipeline using the state-of-the-art large-v3 model. Our evaluation aligns with current trends, showing that weak supervision can be a cost-effective strategy to create large corpora and train competitive models. This is especially relevant in the case of low-resource languages, where the amount of available data is the limiting factor. In the future, we want to expand the GPC with more data, while also creating a gold split for reliable evaluation. Furthermore, we want to explore effective fine-tuning strategies and explore additional applications (e.g., speech-to-text translation and summarization).\"}"}
