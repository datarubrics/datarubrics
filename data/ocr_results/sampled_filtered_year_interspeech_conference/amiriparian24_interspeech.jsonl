{"id": "amiriparian24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets\\n\\nShahin Amiriparian1, Filip Packa\u00b4n2, Maurice Gerczuk2, Bj\u00a8orn W. Schuller1,2,3\\n\\n1 CHI \u2013 Chair of Health Informatics, MRI, TU Munich, Germany\\n2 Chair of Embedded Intelligence for Health Care & Wellbeing, University of Augsburg, Germany\\n3 GLAM \u2013 Group on Language, Audio, & Music, Imperial College, UK\\n\\nshahin.amiriparian@tum.de\\n\\nAbstract\\n\\nFoundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++:\\n\\nhttps://huggingface.co/amiriparian/ExHuBERT.\\n\\nIndex Terms\\n\\n: affective computing, speech emotion recognition, transformers, deep learning\\n\\n1. Introduction\\n\\nSpeech Emotion Recognition (SER) has a rich research history going back to the 1970s (first patents) [1] and 1990s (first research papers) [2] and while it has reaped the benefits of deep learning, a core issue remains to this day: While there are many available databases of emotional speech, most of them only contain comparatively few samples or speakers, hindering effective single-corpus training of large neural networks [3]. As a response to this circumstance, cross- and multi-corpus SER has established itself as a highly important research direction [4]. Due to the fact that databases in the field often differ significantly in recording settings, nature of speech and emotion portrayal (acted, elicited, natural), language, and other factors, successful approaches have employed special strategies and architectural considerations such as domain adaptation [5] or adapter transfer learning [6] to achieve satisfactory performance. Other efforts have gone towards making deep learning models more robust against distortions, noise, and other variations in the speech signal [7, 8].\\n\\nHowever, the recent paradigm shift in general deep learning towards large, Transformer-based models has already impacted the field [9]. The Transformer architecture's inherent capability of learning arbitrary structural information from high-dimensional data combined with the exploitation of huge amounts of unlabeled data through self- and unsupervised learning has significantly reduced the need for human-annotated corpora for training powerful and transferable models [10, 11]. Specifically for the fields of speech recognition and analysis, pre-trained Transformers such as Wav2Vec2.0 (W2V2) [12] or HuBERT [13] have shown considerable generalization capabilities. By exploiting self-supervision on large amounts of unlabeled speech data, these models learn to effectively capture the structure of spoken language. For a wide range of downstream tasks, pre-trained transformers provide competitive performance as feature extractors [14, 15] or through finetuning, e.g., for SER and speaker identification [16]. Wagner et al. [9] finetune wav2vec and HuBERT models on MSP-Podcast [17] and show that their best models provide state-of-the-art performance on a number of SER corpora. They further trace the models' efficacy to a number of beneficial characteristics induced by both pre-training and the transformer architecture itself, such as implicit modeling of linguistic information and a general resilience against speaker, gender, or domain variations.\\n\\nWhile these and other works have convincingly made the argument for large, pre-trained Transformer models in SER, none have investigated whether their generalisability and robustness can enable effective learning of a single, transferable model on a heterogenous set of databases without the need for domain or corpus adaptation strategies. In the present study, we aim to fill this gap by evaluating the capability of large audio transformers to learn salient features for SER by multi-corpus finetuning. For this purpose, we build on the work of [6], introducing EmoSet++, integrating 37 SER corpora spanning 15 languages. We then fine-tune HuBERT on the assembled corpus and compare its transfer learning performance to other large pre-trained models on 6 additional SER databases. Finally, we introduce ExHuBERT, which integrates EmoSet++ finetuning with Backbone Block Expansion (BBE) \u2013 a technique recently introduced in LLAMA Pro [18] \u2013 to deliver a state-of-the-art model and training strategy for emotion recognition.\\n\\n2. EmoSet++\\n\\nWe introduce EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus. It extends EmoSet [6] and integrates 37 unique emotion datasets with 150,907 speech recordings and a cumulative length of 119.5 hours. Most of the datasets that we have included comprise common languages such as English, German, or Mandarin, alongside rarer ones like Persian or Urdu. For all corpora, we create speaker-independent splits. To facilitate training, all distinct dataset labels (comprising 106 emotion classes) are mapped to six classes, representing combinations of low/high arousal and negative/neutral/positive valence. The mapping is based on Russel's Circumplex of Affect [19]. The dataset splits were derived through three methods: adopting from the collected dataset, manual construction for speaker independence, or simply dividing it into 10% partitions for both testing and validation in cases where speaker...\"}"}
{"id": "amiriparian24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Distribution of speech sample durations within Emoset++. Predominantly, the samples fall within the range of 0 to 5 seconds in length.\\n\\n3. Proposed Approach\\n\\nWe propose an enhanced version of HuBERT [13] through fine-tuning on Emoset++ and incorporating Backbone Block Expansion (BBE), denoted as ExHuBERT. Specifically, we fine-tune the encoder part of the HuBERT architecture on 37 diverse emotion datasets, which span various languages and cultural backgrounds (cf. Section 3.1). After the weights are updated, we conduct backbone extension (cf. Section 3.2), and in the final step, we evaluate the performance of the backbone extended HuBERT (ExHuBERT) on 6 unseen test emotion datasets (cf. Section 4).\\n\\n3.1. Fine-Tuning HuBERT on Emoset++\\n\\nOur system is built upon the Transformer architecture HuBERT [13], which has shown promising results in SER [9, 28, 29]. A simple linear layer is added on top for the classification of the 6 mapped arousal valence classes. Fine-tuning is conducted in a round-robin fashion, ensuring each dataset contributes equally to the model. Upon achieving a state where our model spans multiple domains and languages, we utilize its transfer learning and generalization capabilities within ExHuBERT.\\n\\n3.2. Backbone Block Expansion\\n\\nAfter the fine-tuning process, we duplicate each encoder layer along with its weights. This augmentation results in an expanded version of HuBERT (ExHuBERT), featuring a total of 48 layers. The newly added layers are inserted after the original ones, accompanied by a skip connection to maintain the original layer's behavior. To stabilize the training process after layer duplication, we add a Zero Linear Layer (ZLL) at the end of each duplicated layer. The ZLL comprises initialized zero weights, ensuring that the output of the copied layers initiates from zero. This technique plays a crucial role in training by preventing unknown outputs from destabilizing the training process [18]. Furthermore, we freeze the original layers to preserve their encoded knowledge, permitting only the copied layers to undergo training. These steps guarantee that the HuBERT model with BBE behaves identically to the HuBERT model without BBE during the initial stages. A high-level overview of ExHuBERT is depicted in Figure 2.\\n\\n4. Experiments and Results\\n\\nWe split the experiments into two main parts: i) selection of the suitable audio Transformer for the BBE and fine-tuning of the chosen Transformer on Emoset++ (cf. Section 4.1), and ii) conducting BBE on the fine-tuned Transformer and evaluating its performance on unseen emotion datasets (cf. Section 4.2).\\n\\n4.1. Selection of the Suitable Audio Transformer for BBE\\n\\nTo choose the optimal architecture for BBE, we evaluate 6 state-of-the-art Transformers, including W2V2 XLS-R (300 million and 1 billion), Whisper (Medium and Large v3), and HuBERT (Large and XLarge) on all 26 emotion corpora of Emoset [6]. We selected two variants of each architecture to compare their parameter impact, ensuring that variants of the same size had approximately equal parameter counts. Each Transformer is initialized with pre-trained weights obtained from huggingface.co. Additionally, we add a simple linear layer on top of each Transformer, with an output size of 6, corresponding to the mapped classes. For the evaluation metric, we use Unweighted Average Recall (UAR) due to its effectiveness in assessing the overall classification performance across all classes without bias towards any under- or oversampled class. We fine-tune all Transformer models in a round-robin fashion, sequentially passing each dataset forward and backward through the model with one batch in each step. For W2V2 and HuBERT variants, we use raw audio waveforms resampled to 16 kHz as inputs, while we feed Whisper with log Mel spectrograms (with either 80 or 128 bins) obtained from waveforms. We freeze the CNN encoder during the entire training process for W2V2 and HuBERT. This step is unnecessary for Whisper. We conclude the experimentation phase after 3k steps using AdamW optimisation with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$, $\\\\epsilon = 1 \\\\times 10^{-8}$, and a learning rate of $1 \\\\times 10^{-5}$. The performance of these audio Transformers on the unseen test partition of eight commonly used emotion datasets is provided in Table 1.\\n\\nOur results demonstrate that the HuBERT Large model outperforms others, achieving an average UAR of 62.7% over all eight datasets, followed by W2V2 XLS-R 300 M with 57.1% UAR. The worst-performing models are both variants of Whisper, each achieving 42.0% and 41.1% UAR, respectively. We consequently settle on HuBERT Large as the base model for fine-tuning on the extended Emoset++ and transfer learning through BBE.\\n\\nSubsequently, we test the impact of using both 83.7% of Emoset++ and the full Emoset++ for fine-tuning the selected HuBERT model for speech emotion recognition, aiming to evaluate the impact of dataset size on model performance and generalization. Training on Emoset++ leads to substantial performance gains for all databases, compared to the original Emoset, raising the average UAR over the 8 databases from 1. https://huggingface.co/facebook/wav2vec2-xls-r-300m 2. https://huggingface.co/facebook/wav2vec2-xls-r-1b 3. https://huggingface.co/openai/whisper-medium 4. https://huggingface.co/openai/whisper-large-v3 5. https://huggingface.co/facebook/hubert-large-ls960-ft 6. https://huggingface.co/facebook/hubert-xlarge-ls960-ft 7. Results on all datasets are provided here: https://huggingface.co/amiriparian/ExHuBERT/blob/main/supp-mat.pdf\"}"}
{"id": "amiriparian24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance comparison of the applied Transformers on eight common speech emotion datasets. Our proposed fine-tuning of HuBERT Large on EMOS++ demonstrates superior performance over all other Transformers. The best results without fine-tuning on EMOS++ are bolded, and the best overall results (including fine-tuning on EMOS++) are bolded and lightly shaded.\\n\\n62.7% to 64.9% when only adding 5 training corpora, and to 69.7% with the full set. All of the evaluated databases benefit from adding more corpora to EMOS++. However, EmoFilm and Mandarin Emotional Speech stop seeing gains after adding the first 5 new datasets.\\n\\n4.2. ExHuBERT Comparison\\n\\nWe compare the transfer learning capabilities of our enhanced version of HuBERT (ExHuBERT Large EMOS++) against other state-of-the-art Transformer models on a set of six unseen SER corpora, including Athens Emotional States Inventory (AESI) [30], Audio, Speech, and Vision Processing Lab Emotional Sound database (ASVP-ESD) [31], JL-Corpus [32], MLEnd8, Synthesized Database of Basic Emotions (SyntAct) [33], and Variably Intense Vocalizations of Affect and Emotion Corpus (VIV AE) [34]. Specifically, we chose two variants of W2V2 \u2013 W2V2 XLS-R and the emotion fine-tuned W2V2 model by Wagner et al. [9] \u2013 and HuBERT Large LS960 to evaluate the impact of model architectures and the efficacy of EMOS++ fine-tuning. Furthermore, we ablate the performance gains achieved through EMOS++ fine-tuning from those due to block expansion by additionally expanding the LibriSpeech pre-trained HuBERT model (ExHuBERT Large LS960). Finally, we increase the number of trainable parameters of our ExHuBERT model by (1) unfreezing the original HuBERT layers (ExHuBERT Large Non-Frozen EMOS++) and (2) tripling each original layer during block expansion (ExHuBERT XLarge EMOS++). Table 2 shows the results and the number of trainable parameters for each of the evaluated models on six databases external to EMOS++.\"}"}
{"id": "amiriparian24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Training curves on the six external SER corpora, displaying validation UAR.\\n\\nTable 2: Performance comparison of the transfer learning capabilities of our proposed backbone extended HuBERT (ExHuBERT), and its variations, against state-of-the-art Transformers across six emotion datasets. None of these datasets were utilized in the fine-tuning process of ExHuBERT++. For each model, we also provide the average UAR over all datasets and the number of trainable parameters. The best results are bolded and lightly shaded.\\n\\nInconsistent improvements across the six SER corpora, with a slightly lower average UAR of 61.3% compared to fine-tuning without BBE. Overall, BBE seems to have a positive effect on accuracy when the domain shift between source and target databases is rather small, e.g., from one SER corpus to another.\\n\\nWe further analyze how fast the different models converge during training on an unseen corpus in Figure 3, which shows the validation UARs over 10,000 training steps. Analogous to the test set results, ExHuBERT++ pre-training increases overall performance and further achieves faster convergence on all databases compared to models without SER pre-training and the MSP-Podcast fine-tuned W2V2. For AESI, BBE helps the ExHuBERT model to converge even earlier.\\n\\nTo conclude, we look at the results achieved with the larger versions of ExHuBERT, which double the amount of trainable parameters. Unfreezing the original HuBERT layers after BBE degrades performance even from simple fine-tuning of HuBERT on each of the target corpora, highlighting the importance of keeping the weights of the original model fixed for transfer learning. Expanding each block in ExHuBERT by adding a second copy of the respective layer suffers from substantial overfitting, achieving the worst overall UAR of all evaluated Transformer models.\\n\\nIn summary, the validation of our approach on both databases contained within ExHuBERT++ and external corpora shows that (1) multi-corpus pre-training on ExHuBERT++ leads to substantially increased SER performance on seen and unseen corpora, and (2) the addition BBE in ExHuBERT further helps with generalization to new datasets.\\n\\nFor running all of our machine learning experiments, we utilized one RTX-3090 GPU with 24 GB memory, and needed a total time of 313 hours: 121 h for stage 1 (pre-selection of Transformers), 87 h for ExHuBERT++ fine-tuning, 17 h for ExHuBERT++ testing, and 88 h for the second stage.\\n\\n5. Conclusions\\nWe have proposed a novel twofold approach for SER by (i) collecting ExHuBERT++, a comprehensive multi-cultural and multilingual corpus comprising 37 emotion datasets, and (ii) introducing an enhanced version of HuBERT, denoted as ExHuBERT, achieved through fine-tuning on ExHuBERT++ and incorporating backbone extension. To find the suitable Transformer for BBE, we first selected six versions of state-of-the-art audio Transformers and analyzed their performance on ExHuBERT++ [6] and then fine-tuned the best-performing Transformer (which was HuBERT Large) on ExHuBERT++. In the subsequent phase, we applied BBE to the fine-tuned HuBERT Large model and compared its performance with other Transformers on six previously unseen emotion datasets. The experimental results underscore the effectiveness of our proposed approach, demonstrating its capacity to generalize across diverse datasets and establish new benchmarks for a variety of emotion recognition tasks. Lastly, we have uploaded ExHuBERT on huggingface.co. Fine-tuning and deploying ExHuBERT may be computationally demanding, potentially limiting its use in resource-constrained environments.\\n\\nFor future work, we aim to include MSP-Podcast dataset [17] in ExHuBERT++ and enhance ExHuBERT for continuous recognition of arousal, valence, and dominance.\\n\\n6. Acknowledgements\\nThis work was supported by MDSI \u2013 Munich Data Science Institute as well as MCML \u2013 Munich Center of Machine Learning.\\n\\n9 https://huggingface.co/amiriparian/ExHuBERT\"}"}
{"id": "amiriparian24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. D. Williamson, Speech analyzer for analyzing pitch or frequency perturbations in individual speech pattern to determine the emotional state of the person, US Patent 4,093,821, Jun. 1978.\\n\\n[2] F. Dellaert, T. Polzin, and A. Waibel, \u201cRecognizing emotion in speech,\u201d in Proc. ICSLP, IEEE, vol. 3, 1996, pp. 1970\u20131973.\\n\\n[3] B. W. Schuller, \u201cSpeech emotion recognition: Two decades in a nutshell, benchmarks, and ongoing trends,\u201d Communications of the ACM, vol. 61, no. 5, pp. 90\u201399, Apr. 2018.\\n\\n[4] S. Zhang, R. Liu, X. Tao, and X. Zhao, \u201cDeep cross-corpus speech emotion recognition: Recent advances and perspectives,\u201d Frontiers in Neurorobotics, vol. 15, 2021.\\n\\n[5] S. Latif, R. Rana, S. Khalifa, R. Jurdak, and B. Schuller, \u201cSelf-supervised adversarial domain adaptation for cross-corpus and cross-language speech emotion recognition,\u201d IEEE Transactions on Affective Computing, vol. 14, no. 3, pp. 1912\u20131926, Jul. 2023.\\n\\n[6] M. Gerczuk, S. Amiriparian, S. Ottl, and B. Schuller, \u201cEmoNet: A Transfer Learning Framework for Multi-Corpus Speech Emotion Recognition,\u201d IEEE Transactions on Affective Computing, vol. 13, 2022.\\n\\n[7] C. Oates, A. Triantafyllopoulos, I. Steiner, and B. W. Schuller, \u201cRobust speech emotion recognition under different encoding conditions,\u201d in Proc. INTERSPEECH, ISCA, Sep. 2019, pp. 3935\u20133939.\\n\\n[8] A. Triantafyllopoulos, G. Keren, J. Wagner, I. Steiner, and B. W. Schuller, \u201cTowards robust speech emotion recognition using deep residual networks for speech enhancement,\u201d in Proc. INTERSPEECH, ISCA, Sep. 2019, pp. 1691\u20131695.\\n\\n[9] J. Wagner, A. Triantafyllopoulos, H. Wierstorf, M. Schmitt, F. Burkhardt, F. Eyben, and B. W. Schuller, \u201cDawn of the transformer era in speech emotion recognition: Closing the valence gap,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 9, pp. 10 745\u201310 759, Sep. 2023.\\n\\n[10] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Proc. NeurIPS, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., vol. 30, Curran Associates, Inc., 2017.\\n\\n[11] L. Ericsson, H. Gouk, C. C. Loy, and T. M. Hospedales, \u201cSelf-supervised representation learning: Introduction, advances, and challenges,\u201d IEEE Signal Processing Magazine, vol. 39, no. 3, pp. 42\u201362, May 2022.\\n\\n[12] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Proc. NeurIPS, vol. 33, Curran Associates, Inc., 2020, pp. 12 449\u201312 460.\\n\\n[13] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[14] S.-w. Yang, P.-H. Chi, Y.-S. Chuang, et al., \u201cSUPERB: Speech processing universal PERformance benchmark,\u201d in Proc. INTERSPEECH, ISCA, Aug. 2021, pp. 1194\u20131198.\\n\\n[15] L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion recognition from speech using wav2vec 2.0 embeddings,\u201d Interspeech 2021, pp. 3400\u20133404, Aug. 2021.\\n\\n[16] Y. Wang, A. Boumadane, and A. Heba, \u201cA Fine-tuned Wav2vec 2.0/HuBERT Benchmark For Speech Emotion Recognition, Speaker Verification and Spoken Language Understanding,\u201d Oct. 2022. arXiv: 2111.02735 [cs, eess].\\n\\n[17] R. Lotfian and C. Busso, \u201cBuilding naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,\u201d IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471\u2013483, Oct. 2019.\\n\\n[18] C. Wu, Y. Gan, Y. Ge, Z. Lu, J. Wang, Y. Feng, P. Luo, and Y. Shan, \u201cLLaMA pro: Progressive LLaMA with block expansion,\u201d Jan. 2024. arXiv: 2401.02415 [cs].\\n\\n[19] J. A. Russell, \u201cA circumplex model of affect.,\u201d Journal of personality and social psychology, vol. 39, no. 6, p. 1161, 1980.\\n\\n[20] F. Burkhardt, A. Paeschke, M. Rolfes, W. F. Sendlmeier, and B. Weiss, \u201cA database of German emotional speech,\u201d in Proc. INTERSPEECH, ISCA, Sep. 2005, pp. 1517\u20131520.\\n\\n[21] E. Parada-Cabaleiro, G. Costantini, A. Batliner, M. Schmitt, and B. W. Schuller, \u201cDEMoS: An Italian emotional speech corpus,\u201d Language Resources and Evaluation, vol. 54, no. 2, pp. 341\u2013383, Jun. 2020.\\n\\n[22] E. Parada-Cabaleiro, G. Costantini, A. Batliner, A. Baird, and B. Schuller, \u201cCategorical vs Dimensional Perception of Italian Emotional Speech,\u201d in Proc. INTERSPEECH, ISCA, Sep. 2018, pp. 3638\u20133642.\\n\\n[23] A. Dhall, R. Goecke, J. Joshi, K. Sikka, and T. Gedeon, \u201cEmotion Recognition In The Wild Challenge 2014: Baseline, Data and Protocol,\u201d in Proc. ICMI, New York, NY, USA: Association for Computing Machinery, Nov. 2014, pp. 461\u2013466, ISBN: 978-1-4503-2885-2.\\n\\n[24] O. Martin, I. Kotsia, B. Macq, and I. Pitas, \u201cThe eNTERFACE\u201905 Audio-Visual Emotion Database,\u201d in Proc. ICDEW, Apr. 2006.\\n\\n[25] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, no. 4, pp. 335\u2013359, Dec. 2008.\\n\\n[26] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, \u201cMELD: A Multimodal Multi-Party Dataset for Emotion Recognition in Conversations,\u201d Jun. 2019. arXiv: 1810.02508 [cs].\\n\\n[27] T. L. Nwe, S. W. Foo, and L. C. De Silva, \u201cSpeech emotion recognition using hidden Markov models,\u201d Speech Communication, vol. 41, no. 4, pp. 603\u2013623, Nov. 2003.\\n\\n[28] M. A. Pastor, D. Ribas, A. Ortega, A. Miguel, and E. Lleida, \u201cCross-corpus speech emotion recognition with hubert self-supervised representation,\u201d in IberSPEECH, ISCA, 2022, pp. 76\u201380.\\n\\n[29] Y. Wang, A. Boumadane, and A. Heba, \u201cA fine-tuned wav2vec 2.0/hubert benchmark for speech emotion recognition, speaker verification and spoken language understanding,\u201d arXiv preprint arXiv:2111.02735, 2021.\\n\\n[30] T. Chaspari, C. Soldatos, and P. Maragos, \u201cThe development of the Athens Emotional States Inventory (AESI): Collection, validation and automatic processing of emotionally loaded sentences,\u201d The World Journal of Biological Psychiatry: The Official Journal of the World Federation of Societies of Biological Psychiatry, vol. 16, no. 5, pp. 312\u2013322, 2015.\\n\\n[31] T. T. L. Dejoli, Q. He, and W. Xie, \u201cAudio,Speech and Vision Processing Lab Emotional Sound database (ASVP-ESD),\u201d May 2021.\\n\\n[32] J. James, L. Tian, and C. Inez Watson, \u201cAn Open Source Emotional Speech Corpus for Human Robot Interaction Applications,\u201d in Proc. INTERSPEECH, 2018, pp. 2768\u20132772.\\n\\n[33] F. Burkhardt, F. Eyben, and B. Schuller, \u201cSyntAct: A Synthesized Database of Basic Emotions,\u201d in Proc. DCLRL, J. S\u00f8lev\u00e5 and C. Lignos, Eds., Marseille, France: European Language Resources Association, Jun. 2022, pp. 1\u20139.\\n\\n[34] N. Holz, P. Larrouy-Maestri, and D. Poeppel, \u201cThe variably intense vocalizations of affect and emotion (VIV AE) corpus prompts new perspective on nonspeech perception,\u201d Emotion, vol. 22, no. 1, pp. 213\u2013225, 2022.\"}"}
