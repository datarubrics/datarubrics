{"id": "ma24c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks\\n\\nMin Ma1, Yuma Koizumi 2, Shigeki Karita 2, Heiga Zen 2, Jason Riesa 1, Haruko Ishikawa 2, Michiel Bacchiani 2\\n\\nGoogle DeepMind, USA\\nGoogle DeepMind, Japan\\n{minm, koizumiyuma, heigazen}@google.com\\n\\nAbstract\\n\\nThis paper introduces FLEURS-R, a speech restoration applied version of the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS) corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher. The aim of FLEURS-R is to advance speech technology in more languages and catalyze research including text-to-speech (TTS) and other speech generation tasks in low-resource languages. Comprehensive evaluations with the restored speech and TTS baseline models trained from the new corpus show that the new corpus obtained significantly improved speech quality while maintaining the semantic contents of the speech. The corpus is publicly released via Hugging Face.\\n\\nIndex Terms: Multilingual speech corpus, speech generative models, speech restoration, text-to-speech.\\n\\n1. Introduction\\n\\nThere have been rapid development in the speech generation area in the past few years. Models such as denoising diffusion probabilistic models (DDPMs) [1, 2], neural audio codec [3, 4], and large language models (LLMs) [5\u20137] have been successfully applied to speech generation tasks. As these speech generation tasks can be viewed as a sequence-to-sequence generative task, the progress in generative models in different areas can be introduced to improve the performance. Now naturally sounding synthetic speech with an arbitrary speaker's voice can be synthesized with a small amount of speech data [8]. There are models which support controlling their speaking styles and/or voice characteristics via natural language-based prompts [9,10].\\n\\nAlthough there have been great advancements in the modeling side, the progress in the data side is relatively slow. As these new generative models are language agnostic and can be pre-trained from a large quantity of speech-only or text-only data (e.g., 100k hours) [5, 11], the required amount of speech-text paired data is getting smaller [7]. This nature is highly relevant for developing multilingual speech generation models, especially for low-resource languages.\\n\\nThe FLEURS [12] corpus covers 102 languages, which spans over 17 language families and 27 unique writing systems. It was designed to enable speech technology in more languages and catalyze research in low-resource speech understanding. However, as all recordings are kept as they-are, either from quiet or noisy environment, making it less ideal for speech generation tasks, where models are requested to produce high-quality speech.\\n\\nRecently, Koizumi et al. introduced LibriTTS-R [13], which is a speech restoration applied version of the LibriTTS corpus [14]. As it offers speech signals in higher sampling rate, less noise, and less reverberation, neural end-to-end TTS models trained with LibriTTS-R achieved better subjective naturalness than those with the original LibriTTS corpus [13].\\n\\nThis paper introduces FLEURS-R, a speech-restoration applied version of the FLEURS corpus. It keeps the same properties as the original FLEURS corpus with improved audio quality, i.e., less noise and reverberation with higher sampling rate (24 kHz). Table 1 compares FLEURS-R with existing common public multilingual TTS corpora. The key properties of FLEURS-R that are:\\n\\n\u2022 Containing N-way parallel speech and text in 102 languages; the improved speech quality makes it a better choice for speech generation tasks, including TTS, speech-to-speech translation (S2ST) and voice conversion (VC).\\n\u2022 Highly multilingual (102 languages) where 80% languages are low-resource. It helps catalyze speech generation research in multilingual, cross-lingual and low-resource settings.\\n\\n2. Speech Restoration Pipeline\\n\\n2.1. Speech Restoration Model\\n\\nWe restored the FLEURS speech samples using the same methodology employed to create the LibriTTS-R corpus [13]. LibriTTS-R was created by applying a speech restoration model Miipher [19] to the LibriTTS corpus [14]. Miipher extracts acoustic features from noisy speech using w2v-BERT [20]. The system then employs DF-Conformer [21] to convert these noisy acoustic features into clean ones while using speaker and text conditioning features extracted by speaker-encoder [22] and PnG-BERT [23]. Finally, the WaveFit [24] neural vocoder generates a clean speech waveform from the predicted features. Since FLEURS is a multilingual corpus and Miipher supports only English [19], we made several updates to the Miipher model structure to accommodate this difference. First, we replaced the acoustic feature extractor from w2v-BERT [20] to the Universal Speech Model (USM) [25]. Unlike w2v-BERT [20], which was trained on English speech samples, the USM was pre-trained on a massive dataset of 12 million hours of speech spanning over 300 languages. We used a non-fine-tuned USM encoder to preserve the speaker's acoustic characteristics in the extracted features. Specifically, we used the 2-billion parameter \\\"pre-trained\\\" model [25]. In self-supervised learning (SSL) speech feature extraction, it is known that deeper layers tend to lose detailed and local acoustic information [26]; therefore, we used the intermediate feature from the 13th of 32th layers based on preliminary experiments.\\n\\n1. https://huggingface.co/datasets/google/fleurs-r\"}"}
{"id": "ma24c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"## Table 1: Comparison among FLEURS-R and other common public speech corpora.\\n\\n| Dataset                  | #Locales | Total Duration | Domains | Speech Type | Sampling Rate | License          | Parallel speech |\\n|-------------------------|----------|----------------|---------|-------------|---------------|------------------|-----------------|\\n| MLS [15]                | 8        | 50.5k hours    | Audiobook | Read        | 16 kHz        | CC-BY-4.0        | No              |\\n| CML-TTS [17]            | 7        | 3.2k hours     | Audiobook | Read        | 24 kHz        | CC-BY-4.0        | No              |\\n| M-AILabs speech datasets| 9        | 1k hours       | Audiobook | Read        | 16 kHz        | BSD 3-Clause License | No              |\\n| BC2013 [18]             | 4        | 0.3k hours     | Audiobook | Read        | 44.1 kHz      | Non-commercial   | No              |\\n| LibriTTS-R [13]         | 1        | 0.6k hours     | Audiobook | Read        | 24 kHz        | CC-BY-4.0        | No              |\\n| FLEURS [12]             | 102      | 1.4k hours     | Wikipedia | Read        | 16 kHz        | CC-BY-4.0        | Yes             |\\n| FLEURS-R (this work)    | 102      | 1.3k hours     | Wikipedia | Read        | 24 kHz        | CC-BY-4.0        | Yes             |\\n\\nFurthermore, preliminary experiments indicated that the USM features retained both acoustic details; neither text nor speaker conditioning improved the reconstruction accuracy. Consequently, both speaker encoder [22] and PnG-BERT text encoder [23] were removed from the new Miipher network architecture.\\n\\n### 2.2. Data Processing Pipeline\\n\\nFirst we applied the new Miipher model-based speech restoration to the complete set in the original FLEURS corpus. Thanks to Miipher's audio super-resolution capability, the sampling rate of speech samples in the FLEURS-R samples was increased from 16 kHz to 24 kHz. Note that FLEURS-R maintains the same constituent samples as the original FLEURS corpus except the audio quality.\\n\\nDue to possible errors caused in the Miipher speech restoration process, some restored samples may exhibit signal processing artifacts. To identify successfully restored samples, we performed ASR-based filtering. The list of the rejected samples will also be published. Note that all experiments in Sec. 3 including TTS model training were conducted with the rejected samples.\\n\\n### 3. Evaluations\\n\\nWe conducted ASR-based intelligibility evaluations, automatic subjective naturalness evaluations, and TTS model training experiments with the new FLEURS-R corpus. Some demo samples from each experiment are available as a supplementary material to honor double-blind review.\\n\\n#### 3.1. ASR Evaluation\\n\\nTo validate the consistency of semantic contents between original FLEURS and new FLEURS-R corpora, we conducted ASR evaluations over all 102 languages. We used the Maestro-U [27] grapheme ASR model which performed reasonably well in terms of character error rates (CERs) in most of these 102 languages.\\n\\nTable 2 shows the language-specific CERs. The abbreviations in the first row denote regions; Western European (WE), Eastern European (EE), Central-Asia, Middle-East and North-Africa (CMN), Sub-Saharan Africa (SSA), South-Asia (SA), South-East Asia (SEA), and Chinese, Japanese and Korean (CJK). Please refer to [12] for the individual locale codes. It can be seen from the table that the average CERs over all locales for FLEURS and FLEURS-R were approximately equal (9.67% and 9.74%). This suggests the speech restoration process maintained the semantic contents in the original speech in most languages.\\n\\n32% languages got improved CERs, especially in Xhosa (xh), Umbundu (umb), Macedonian (mk), Tamil (ta), Turkish (tr), Hebrew (he) and Armenian (hy). The gains mostly come from the reduced substitution error rates, likely because enhanced speech quality makes the acoustically similar words more distinguishable. Other locales maintained or only saw small regressions in CERs. The exceptional locales were Nepali (ne), Punjabi (pa), Indonesian (id), Latvian (lv), and Czech (cs). Such degradation was from higher substitution and deletion error rates. On most locales, insertion error rates were generally reduced, indicating that Miipher reduced the environment noises of speech recordings. Three locales, Sorani-Kurdish (ckb), Sindhi (sd) and Cantonese (yue) observed high CERs, though their ASR baseline CERs on FLEURS are already high. We provide samples from the two groups (most improved and most regressed) in the supplementary materials.\\n\\n#### 3.2. Speech Naturalness Evaluation\\n\\nWhile the subjective 5-point Mean Opinion Score (MOS) is a standard evaluation method to assess the naturalness, it poses\"}"}
{"id": "ma24c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3 shows that SQuId MOS in FLEURS-R were generally higher than FLEURS across all languages. On average, FLEURS-R had a 0.2 point improvement over FLEURS in the SQuId MOS. Although improvements in the SQuId MOS were observed in almost all languages except Irish (ga). Languages spoken in South Asia exhibited large gains in the SQuId MOS. Since SQuId MOS model trained on 16 kHz speech, the 24 kHz speech generated by TTS model built on FLEURS-R was resampled to 16 kHz before scoring, therefore, the actual gains in naturalness from training TTS models on FLEURS-R would be larger than the SQuId MOS score differences indicate.\\n\\nWe also investigated whether these score improvements were independent of utterance duration. As shown in Figure 1, the speech restored by Miipher is consistently better than original FLEURS speech in naturalness, in term of SQuId MOS. The restoration gains are especially significant on shorter utterances.\\n\\n3.3. TTS Evaluations\\n\\n3.3.1. Model Configurations\\n\\nWe adopted the model configuration from Virtuoso 2 [29] to build a TTS baseline. It is a non-autoregressive TTS model, which uses UTF-8 byte as input representation. It aims to build a robust model supporting high and low resource languages via self-supervised and semi-supervised learning from speech-only, text-only, and speech-text pair datasets. Its speech encoder and shared encoder were composed of 6 and 18 Conformer [30] layers, respectively, with a hidden dimension of 768. The feed-forward text encoder consisted of 12 Conformer layers with a hidden dimension of 768. The semantic feature decoder in this model comprised 6 lightweight convolutional layers. The model was conditioned on both speaker and language IDs during training, allowing both speaker and language control at inference. To capture intra-speaker prosodic variations that occur in natural speech, this model has a global variational autoencoder (VAE) over input speech, which can be used to add prosodic diversity at the inference stage. Please refer to [29] for details.\\n\\nWe trained multi-speaker Virtuoso 2 models on either FLEURS to produce speech of 16 kHz sample rate, or trained on FLEURS-R to generate speech of 24 kHz sample rate. The same hyper-parameters were used between two models for consistent comparisons.\\n\\n3.3.2. Speech Naturalness Evaluation\\n\\nWe evaluated the naturalness of speech synthesized by the Virtuoso models by the same SQuId model. Table 4 indicates that the TTS model trained by FLEURS-R produced more naturally-sounding speech, with an overall score of 3.89 compared to 3.79 for that model trained by FLEURS. In the same manner, since the speech predicted by TTS model trained on FLEURS-R has to be resampled from 24 kHz to 16 kHz before scoring, the actual naturalness of the synthesized speech should surpass than what the rating of 3.89 suggests. The most significant improvements were observed in Khmer (km), Burmese (my), Mandarin (cmn), and several South Asian languages, including Oriya (or), Hindi (hi), and Tamil (ta). We hypothesize the gains might due to shared acoustic-prosodic properties among Southeastern Asian languages, and Southern Asian languages.\\n\\n3.3.3. ASR Evaluation on Synthesized Speech\\n\\nTo evaluate the intelligibility of the synthesized speeches, we reused the same pre-existing ASR model to compute CERs on them. As shown in Table 5, the overall CERs remained consistent between two models. This suggests that the TTS models respectively trained on the restored / original corpus, could produce high-quality speech.\"}"}
{"id": "ma24c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: The 5-scale SQuId MOS in naturalness for synthetic speech generated by the Virtuoso models trained on the FLEURS (top) and the FLEURS-R (bottom) corpora in all 102 languages.\\n\\n| Language | WEast | bs | ca | hr | da | nl | en | fi | fr | gl | de | el | hu | is | ga |\\n|----------|-------|----|----|----|----|----|----|----|----|----|----|----|----|----|----|\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.99  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.93  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.98  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.89  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.88  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.95  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.85  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.79  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.71  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.85  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.75  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.99  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.06  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.89  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.99  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.00  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 2.95  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.04  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.10  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.90  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.10  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.94  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.05  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.85  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.77  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.13  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.03  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.08  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.47  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.87  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.85  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.79  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.60  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.07  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.91  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.73  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.93  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.97  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.87  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.91  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.78  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.85  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.76  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.98  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.02  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.47  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.73  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.93  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.97  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.02  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.47  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.02  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.87  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.08  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.66  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.48  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.52  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.64  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.81  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.87  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.59  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.29  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.00  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.04  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.01  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.13  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.03  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.08  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.91  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.73  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.93  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.97  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.96  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 4.01  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.86  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.76  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.90  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.77  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.83  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.77  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.80  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n|          | 3.62  |    |    |    |    |    |    |    |    |    |    |    |    |    |    |\\n"}
{"id": "ma24c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] N. Chen, Y. Zhang, H. Zen, R. J. Weiss, M. Norouzi, and W. Chan, \u201cWaveGrad: Estimating gradients for waveform generation,\u201d in Proc. ICLR, 2021.\\n\\n[2] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, and M. Kudinov, \u201cGrad-TTS: A diffusion probabilistic model for text-to-speech,\u201d in Proc. ICML, 2021.\\n\\n[3] N. Zeghidour, A. Luebs, A. Omran, J. Skoglund, and M. Tagliasacchi, \u201cSoundStream: An end-to-end neural audio codec,\u201d IEEE/ACM Trans. ASLP, vol. 30, pp. 495\u2013507, 2021.\\n\\n[4] A. D\u00e9fossez, J. Copet, G. Synnaeve, and Y. Adi, \u201cHigh fidelity neural audio compression,\u201d Trans. MLR, 2022.\\n\\n[5] Z. Borsos, R. Marinier, D. Vincent, E. Kharitonov, O. Pietquin, M. Sharifi, D. Roblek, O. Teboul, D. Grangier, M. Tagliasacchi et al., \u201cAudioLM: a language modeling approach to audio generation,\u201d IEEE/ACM Trans. ASLP, 2023.\\n\\n[6] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv:2301.02111, 2023.\\n\\n[7] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, \u201cSpeak, read and prompt: High-fidelity text-to-speech with minimal supervision,\u201d Trans. ACL, vol. 11, pp. 1703\u20131718, 2023.\\n\\n[8] E. Casanova, J. Weber, C. D. Shulby, A. C. Junior, E. G\u00f6lge, and M. A. Ponti, \u201cYourTTS: Towards zero-shot multi-speaker TTS and zero-shot voice conversion for everyone,\u201d in Proc. ICML, 2022, pp. 2709\u20132720.\\n\\n[9] Z. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, \u201cPromptTTS: Controllable text-to-speech with text descriptions,\u201d in Proc. ICASSP, 2023, pp. 1\u20135.\\n\\n[10] D. Yang, S. Liu, R. Huang, C. Weng, and H. Meng, \u201cInstructTTS: Modeling expressive TTS in discrete latent space with natural language style prompt,\u201d arXiv:2301.13662, 2023.\\n\\n[11] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov et al., \u201cAudioPaLM: A large language model that can speak and listen,\u201d arXiv:2306.12925, 2023.\\n\\n[12] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, \u201cFLEURS: Few-shot learning evaluation of universal representations of speech,\u201d in Proc. SLT. IEEE, 2023, pp. 798\u2013805.\\n\\n[13] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, M. Bacchiani, Y. Zhang, W. Han, and A. Bapna, \u201cLibriTTS-R: A restored multi-speaker text-to-speech corpus,\u201d arXiv:2305.18802, 2023.\\n\\n[14] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from LibriSpeech for text-to-speech,\u201d in Proc. Interspeech, 2019, pp. 1526\u20131530.\\n\\n[15] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMLS: A large-scale multilingual dataset for speech research,\u201d in Proc. Interspeech, 2020.\\n\\n[16] \u201ccreative commons attribution 4.0 license (CC-BY 4.0).\u201d [Online]. Available: https://creativecommons.org/licenses/by/4.0/\\n\\n[17] F. S. Oliveira, E. Casanova, A. C. J\u00fanior, A. S. Soares et al., \u201cCML-TTS: A multilingual dataset for speech synthesis in low-resource languages,\u201d arXiv:2306.10097, 2023.\\n\\n[18] K. Prahallad, A. Vadapalli, N. Elluru, G. Mantena, B. Pulugundla, P. Bhaskararao, H. A. Murthy, S. King, V. Karaiskos, and A. W. Black, \u201cThe Blizzard Challenge 2013 \u2013 Indian language task,\u201d in Blizzard Challenge workshop, vol. 2013, 2013.\\n\\n[19] Y. Koizumi, H. Zen, S. Karita, S. Wisdom, H. Erdogan, J. R. Hershey, L. Jones, and M. Bacchiani, \u201cDF-Conformer: Integrated architecture of Conv-TasNet and Conformer using linear complexity self-attention for speech enhancement,\u201d in WASPAA, 2021.\\n\\n[20] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cw2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,\u201d in Proc. IEEE ASRU, 2021.\\n\\n[21] Y. Koizumi, S. Karita, S. Wisdom, H. Erdogan, J. R. Hershey, L. Jones, and M. Bacchiani, \u201cDF-Conformer: Integrated architecture of Conv-TasNet and Conformer using linear complexity self-attention for speech enhancement,\u201d in WASPAA, 2021.\\n\\n[22] Q. Wang, Y. Yu, J. Pelecanos, Y. Huang, and I. L. Moreno, \u201cAttentive temporal pooling for Conformer-based streaming language identification in long-form speech,\u201d in Odyssey, 2022.\\n\\n[23] Y. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu, \u201cPnG BERT: Augmented BERT on phonemes and graphemes for neural TTS,\u201d in Proc. Interspeech, 2021.\\n\\n[24] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, \u201cWaveFit: An iterative and non-autoregressive neural vocoder based on fixed-point iteration,\u201d in Proc. SLT, 2023.\\n\\n[25] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang, Z. Meng, K. Hu, A. Rosenberg, R. Prabhavalkar, D. S. Park, P. Haghani, J. Riesa, G. Perng, H. Soltau, T. Strohman, B. Ramabhadran, T. Sainath, P. Moreno, C.-C. Chiu, J. Schalkwyk, F. Beaufays, and Y. Wu, \u201cGoogle USM: Scaling automatic speech recognition beyond 100 languages,\u201d arXiv:2303.01037, 2023.\\n\\n[26] A. Pasad, J.-C. Chou, and K. Livescu, \u201cLayer-wise analysis of a self-supervised speech representation model,\u201d in ASRU, 2021.\\n\\n[27] Z. Chen, A. Bapna, A. Rosenberg, Y. Zhang, B. Ramabhadran, P. Moreno, and N. Chen, \u201cMaestro-U: Leveraging joint speech-text representation learning for zero supervised speech asr,\u201d in Proc. SLT. IEEE, 2023, pp. 68\u201375.\\n\\n[28] T. Sellam, A. Bapna, J. Camp et al., \u201cSQuId: Measuring speech naturalness in many languages,\u201d arXiv:2210.06324, 2022.\\n\\n[29] T. Saeki, G. Wang, N. Morioka, I. Elias, K. Kastner, A. Rosenberg, B. Ramabhadran, H. Zen, F. Beaufays, and H. Shemtov, \u201cExtending multilingual speech synthesis to 100+ languages without transcribed data,\u201d in Proc. ICASSP, 2024.\\n\\n[30] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d Proc. Interspeech, 2020.\"}"}
