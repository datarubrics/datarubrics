{"id": "yang22g_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ASR Error Correction with Constrained Decoding on Operation Prediction\\n\\nJingyuan Yang\u2020, Rongjun Li\u2020, Wei Peng*\\n\\nArtificial Intelligence Application Research Center, Huawei Technologies\\nyangjingyuan2@huawei.com, lirongjun3@huawei.com, peng.wei1@huawei.com\\n\\nAbstract\\n\\nError correction techniques remain effective to refine outputs from automatic speech recognition (ASR) models. Existing end-to-end error correction methods based on an encoder-decoder architecture process all tokens in the decoding phase, creating undesirable latency. In this paper, we propose an ASR error correction method utilizing the predictions of correction operations. More specifically, we construct a predictor between the encoder and the decoder to learn if a token should be kept (\u201cK\u201d), deleted (\u201cD\u201d), or changed (\u201cC\u201d) to restrict decoding to only part of the input sequence embeddings (the \u201cC\u201d tokens) for fast inference. Experiments on three public datasets demonstrate the effectiveness of the proposed approach in reducing the latency of the decoding process in ASR correction. It enhances the inference speed by at least three times (3.4 and 5.7 times) while maintaining the same level of accuracy (with WER reductions of 0.53% and 1.69% respectively) for our two proposed models compared to a solid encoder-decoder baseline. In the meantime, we produce and release a benchmark dataset contributing to the ASR error correction community to foster research along this line.\\n\\nIndex Terms: automatic speech recognition, error correction, sequence to sequence, constrained decoder\\n\\n1. Introduction\\n\\nAutomatic speech recognition (ASR) models play critical roles in pipeline-based contemporary deep learning systems (i.e., Voice Assistant and Speech Translation) by transforming sequences of audio signals to sequences of word tokens for downstream tasks [1, 2]. Despite its comprehensive application to industrial scenarios, ASR is ill-performed in real-world cases, whereas out-of-domain speech and noisy data prevail. For example, Table 1 shows several common ASR errors, including an entity error with a reference data \u201cto orlando\u201d wrongly recognized as \u201ctewel ando\u201d, leading to problems in downstream tasks.\\n\\nError correction (EC) techniques remain effective means to refine outputs from ASR models. These EC approaches belong to a cascade framework and an end-to-end sequence to sequence framework. The cascade framework consists of a sequence labeling method to identify the sequence of interests, followed by a rule-based value error recovery module [3]. A cascading EC framework requires costly manual transcriptions to all positive and negative tokens; therefore, scaling up a cascading EC approach is challenging. An end-to-end framework treats EC of ASR as a machine translation method between problematic sequences of tokens and reference sequences [4, 5, 6, 7]. Along this line, Transformer-based ASR error correction methods learn a mapping between the outputs of ASR and the related ground-truth transcriptions [8]. Pretrained language models, i.e., BERT [9] and BART [10], are used as encoders to represent input sequences to boost ASR error correction [11], in which a Bart-initialized model is regarded to significantly outperform that from BERT [7]. The end-to-end ASR error correction framework becomes the mainstream as it achieves the state-of-the-art (SOTA) WER and requires fewer efforts in preparing training data than the cascading framework. Existing transformer-based ASR EC models process all input embeddings equally in the decoding phase, creating undesirable latency. In this paper, we propose an ASR error correction method utilizing the predictions of correction operations to constrain the decoding and reduce the inference latency. More specifically, we design a predictor between the encoder and the decoder to classify if a token should be kept (\u201cK\u201d), deleted (\u201cD\u201d) or changed (\u201cC\u201d) to restrict decoding to only part of the input sequence embeddings (the \u201cC\u201d tokens) for fast inference. On the other hand, there is a lack of publicly available benchmark datasets for ASR error correction as researchers in this area majorly report the experimental results on the in-house data. Such data deficiency in the public domain has inhibited the advancement of the ASR error correction techniques. The contributions of our work are two folds:\\n\\n\u2022 To our best knowledge, we are the first to construct an operation predictor to perform constrained decoding to enhance the inference speed of an ASR error correction approach. Experiments on three datasets demonstrate the effectiveness of the proposed method in enhancing the inference speed by at least three times while maintaining the same level of accuracy (WER) compared to a solid encoder-decoder baseline.\\n\\n\u2022 To address the lack of public benchmark datasets for ASR error correction, we apply internal ASR and Text-to-Speech (TTS) engines to a single-turn dialogue dataset TOP [12]. In this way, we produce an ASR error correction dataset contributing to the community to foster research along this line. The code and datasets1 are made publicly available.\\n\\n1https://github.com/yangjingyuan/ConstDecoder\\n\\nTable 1: Typical ASR errors include: 1) the grammatical error due to unclear voice signal (in red fonts); 2) the similar sound error created by phonetically confusing words (in blue); 3) the lack of domain-specific terms causing the entity error (in olive fonts) ; 4) the insertion error in cyan, and the delete error in orange\\n\\n| Type | Example |\\n|------|---------|\\n| Reference | cheapest airfare from tacoma to orlando. |\\n| ASR   | cheaper stair from tacoma tewel ando. |\\n|       | will it get * hotter in hext. |\\n|       | will it get a hotter in *. |\\n\\nThe code and datasets are made publicly available.\"}"}
{"id": "yang22g_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The overall architecture of the proposed method. The decoder takes the encoded hidden representation over the \\\"C\\\" positions, the attended context vector, and the prediction from the last time step as inputs to produce the corrected tokens. In particular, when multiple \\\"C\\\" tags are present, they are decoded in parallel.\\n\\n2. Related Work\\nASR correction has been jointly trained with natural language understanding in a multi-task setting [2]. To address the issue associated with semantic-only post-correction, phonetic features are included in the correction model to tackle homophonic errors commonly identified in Chinese ASR [13]. Along this direction, [5] propose a variant transformer model encoding both the word and phoneme sequence of an entity to fix name entity errors. A similar research [14] utilizes a dual encoder in an end-to-end framework to handle an input word sequence and corresponding phoneme sequence. More works [11, 7] have emerged to boost the accuracy of error correction. It was not until recently that researchers commenced exploring means to reduce decoding latency associated with sequence-to-sequence ASR correction models. LaserTagger [15] converts the sequence generation task to a text editing one to enhance inference speed. However, LaserTagger cannot handle the out-of-vocabulary problem as it relies on a predefined vocabulary. Although there is an attempt leveraging non-autoregressive models in ASR correction, the study [16] is built on a large internal Mandarin speech corpus. It is argued that an error correction method using non-autoregressive models suffer from the issue of over-correction, resulting in mis-replacement of normal words. A direct comparison is not applicable before a full-scale release of code and data. [17] propose a semi-autoregressive model coupled with an LSTM for generation, achieving SOTA accuracy and inference speed in the utterance restoration. Inspired by [17], we design an end-to-end ASR error correction method using an operation predictor to restrict decoding to only desirable parts of the input sequence embeddings to reduce inference latency.\\n\\n3. Approach\\nThe task of ASR error correction can be treated as a functional mapping $f: X \\\\rightarrow T$ between $X$ and $T$, where $X$ denotes text outputs from ASR containing errors, and $T$ is the ground-truth reference text. Unlike other sequence generation tasks, which may have radically different inputs and outputs, it is observed that most contents of $X$ and $T$ are identical. The intuition here is to only apply a mapping ($f$) between the desirable part of $X$ and the counterparts in $T$. We design an end-to-end architecture consisting of an encoder, a decoder, and an operation predictor to constrain the decoding process to the desired part of the input sequence, shown in Figure 1. The encoder produces sequence embeddings from the word embeddings of the ASR output texts and related positional embeddings. The operation predictor is a fully-connected layer trained to identify the above-mentioned \\\"K\\\", \\\"D\\\" and \\\"C\\\" input sequence embeddings. The model can reduce inference latency significantly by restricting the decoding process to the \\\"C\\\" tokens.\\n\\n3.1. Preprocessing phase\\nThe preprocessing step is similar to that mentioned in [17]: (1) Insert dummy tokens between very adjacent token in $X$; (2) Apply WordPiece tokenizer [18] to $X$ and $T$ to produce corresponding token sequences $X = \\\\{\\\\omega_x^1, \\\\omega_x^2, ..., \\\\omega_x^n\\\\}$, $T = \\\\{\\\\omega_t^1, \\\\omega_t^2, ..., \\\\omega_t^m\\\\}$; (3) Construct training labels for the operation predictor and the decoder. We calculate the longest common subsequence (LCS) between $X$ and $T$, and apply LCS to align $X$ and $T$ recurrently. The aligned tokens are labeled as \\\"KEEP\\\" (\\\"K\\\") with the rest tagged as \\\"DELETE\\\" (\\\"D\\\") or \\\"CHANGE\\\" (\\\"C\\\") with the positions of the chunks of \\\"C\\\" tokens recorded. The operation training labels are defined as $D = \\\\{d_1, d_2, ..., d_n\\\\}$. The decoding sequence for \\\"C\\\" commencing at $k$th position can be denoted as $Y_k = \\\\{\\\\omega_y^1,k, \\\\omega_y^2,k, ..., \\\\omega_y^{o,k}\\\\}$.\\n\\n3.2. Model architecture\\nWe use BERT as an encoder to obtain the hidden representations of the model inputs $X$:\\n\\n$$E_x = \\\\text{BERT Encoder}(\\\\text{WE}(X) + \\\\text{PE}(X))$$\\n\\nwhere $\\\\text{WE}$ and $\\\\text{PE}$ are related word and position embedding functions respectively. $E_x = \\\\{e_x^1, e_x^2, ..., e_x^n\\\\}$ is the encoded sequence.\"}"}
{"id": "yang22g_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"representation of X, and \\\\( e_x^i \\\\) is the \\\\( i \\\\)th encoded token representation of X. Next, one fully-connected layer with softmax is designed to be the operation predictor:\\n\\n\\\\[\\np(\\\\hat{d}_i | \\\\omega_x^i) = \\\\text{softmax}(W_d e_x^i + b_d).\\n\\\\]\\n\\n(2)\\n\\nHere, \\\\( \\\\hat{d}_i \\\\) is the predicted operation. \\\\( W_d \\\\) and \\\\( b_d \\\\) are the learnable parameters. At last, the operation loss is defined as negative log-likelihood:\\n\\n\\\\[\\n\\\\text{loss}_{oper} = - \\\\sum_i \\\\log(p(\\\\hat{d}_i | \\\\omega_x^i)).\\n\\\\]\\n\\n(3)\\n\\nTwo kinds of constrained decoders are implemented, one is LSTM-based and the other is transformer-based.\\n\\n- LSTM-based decoder\\n\\n\\\\[\\ns_{k+1} = \\\\text{LSTM}(\\\\text{WE}(\\\\omega_y^t,k), s_k^t).\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( \\\\omega_y^t,k \\\\) and \\\\( s_k^t \\\\) are the decoder input and the LSTM hidden state at time step \\\\( t \\\\) in terms of the \\\\( k \\\\)th change operation. The initial values of \\\\( \\\\omega_y^0,k \\\\) and \\\\( s_0^k \\\\) are \\\\(<\\\\text{bos}>\\\\) and \\\\( e_x \\\\) respectively. \\\\( \\\\text{WE} \\\\) is the same word embedding function as in equation 1. Moreover, corresponding context vector is calculated based on the additive attention [19] mechanism:\\n\\n\\\\[\\nc_{k+1} = \\\\text{Attn}(Q, K, V),\\n\\\\]\\n\\n(5)\\n\\n\\\\[\\nQ = s_{k+1}, K = E_x, V = E_x.\\n\\\\]\\n\\n(6)\\n\\nwhere the LSTM hidden state \\\\( s_{k+1} \\\\) works as query (\\\"Q\\\"), and the encoded model input \\\\( E_x \\\\) works as key (\\\"K\\\") and value (\\\"V\\\") in corresponding attention calculation. Finally, the context vector \\\\( c_{k+1} \\\\) and the LSTM hidden state \\\\( s_{k+1} \\\\) are fused to predict the target text.\\n\\n\\\\[\\np(\\\\hat{\\\\omega}_y^{t+1}, k) = \\\\text{softmax}(W_y [c_{k+1} \\\\oplus s_{k+1}] + b_y)\\n\\\\]\\n\\n(7)\\n\\nwhere \\\\( \\\\oplus \\\\) indicates a concatenate function, \\\\( W_y \\\\) and \\\\( b_y \\\\) are the learnable parameters.\\n\\n- Transformer decoder\\n\\nFor the \\\\( k \\\\)th change position, the transformer-based decoder inputs at timestep \\\\( t \\\\) are calculated as follows:\\n\\n\\\\[\\nE_y^{t,k} = \\\\text{W}_p[(\\\\text{WE}(\\\\omega_y^t,k) + \\\\text{PE}(\\\\omega_y^t,k)) \\\\oplus e_x^k].\\n\\\\]\\n\\n(8)\\n\\nwhere \\\\( \\\\text{WE} \\\\) and \\\\( \\\\text{PE} \\\\) are the same word and position embedding functions as in equation 1. \\\\( e_x^k \\\\) is the encoded token representation mentioned before. \\\\( \\\\text{W}_p \\\\) is the trainable parameters. Then, a standard transformer decoder [20] is employed to fuse all relevant information:\\n\\n\\\\[\\nh_{k+1} = \\\\text{TransformerDecoder}(Q, K, V),\\n\\\\]\\n\\n(9)\\n\\n\\\\[\\nQ = E_y^{t,k}, K = E_x, V = E_x.\\n\\\\]\\n\\n(10)\\n\\nwhere the decoder input \\\\( E_y^{t,k} \\\\) works as query (\\\"Q\\\"), and encoded model input \\\\( E_x \\\\) works as key (\\\"K\\\") and value (\\\"V\\\") in transformer decoder. \\\\( h_{k+1} \\\\) is the fused vector. Finally, the target output is predicted as follows:\\n\\n\\\\[\\np(\\\\hat{\\\\omega}_y^{t+1}, k) = \\\\text{softmax}(W_y h_{k+1} + b_y)\\n\\\\]\\n\\n(11)\\n\\nwhere \\\\( W_y \\\\) and \\\\( b_y \\\\) are trained parameters. Both LSTM-based and transformer-based generation loss are defined as:\\n\\n\\\\[\\n\\\\text{loss}_{dec} = - \\\\sum_k \\\\sum_t \\\\log(p(\\\\hat{\\\\omega}_y^{t,k})).\\n\\\\]\\n\\n(12)\\n\\nThe total loss is calculated as follows:\\n\\n\\\\[\\n\\\\text{loss} = \\\\alpha \\\\text{loss}_{oper} + \\\\text{loss}_{dec}.\\n\\\\]\\n\\n(13)\\n\\nwhere \\\\( \\\\alpha \\\\) is the hyperparameter.\\n\\n4. Experiments\\n\\nIn order to evaluate the effectiveness of our proposed model, three datasets leveraging different ASR engines are utilized.\\n\\nTable 2: Statistics of the utilized datasets\\n\\n| Name | Train | Valid | Test |\\n|------|-------|-------|------|\\n| ATIS | 3867  | 967   | 800  |\\n| SNIPS| 13084 | 700   | 700  |\\n| TOP  | 31279 | 4462  | 9042 |\\n\\nFigure 2: A comparison of ASR error type distributions for three benchmark datasets.\\n\\n- ATIS [21] is a public benchmark dataset containing user voice records of flight information. [22] used a TTS engine (Amazon Polly) to convert the text to voice, adding ambient noise to simulate real-life data. Finally, a LAS [23] ASR engine was used to convert voice data into transcribed texts.\\n- SNIPS [24] is also a benchmark dataset for speaking language understanding study. [1] used a commercial TTS to synthesize audio data from texts. Kaldi [4] is used to transcribe audio to texts.\\n- TOP [12] is the dataset proposed by Facebook focusing on the topics of navigation and events. We use our own commercial TTS and ASR engine to produce the dataset.\\n\\nFigure 2 shows a distribution of errors in our sampled datasets. It is noted that they are significantly different, enabling a comprehensive evaluation of model effectiveness. We compare the propose method with the following three baselines\\n\\n- Original: refers to the WER of the original datasets when no error correction applies;\\n- SC BART [7]: a pretrained encoder-decoder transformer language model, which has reached SOTA results on ASR error correction tasks;\\n- distillBART [25]: a distilled version of BART-large, which has a 12-layer encoder and a single-layer decoder. The application of distillation followed by finetuning on the three datasets to enhance the inference speed.\\n\\n2https://aws.amazon.com/polly/\\n3www.pacdv.com/sounds/ambience_sounds.html\\n4https://github.com/kaldi-asr/kaldi\"}"}
{"id": "yang22g_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: The experimental results of our method against other SOTA methods in WER on three benchmark datasets. ConstDecoder lstm and ConstDecoder trans refer to our proposed constrained decoder using LSTM and transformer respectively.\\n\\n| Method       | ATIS | SNIPS | TOP | Average |\\n|--------------|------|-------|-----|---------|\\n| Original     | 30.65| 45.73 | 13.45| 29.94   |\\n| SC BART [7]  | 21.47| 30.35 | 7.30 | 19.70   |\\n| distilBART [25] | 26.51| 33.28 | 8.25 | 22.68   |\\n| ConstDecoder lstm | 22.44| 31.75 | 10.02| 21.40   |\\n| vs. Original | -8.21| -13.98| -3.43| -8.54   |\\n| ConstDecoder trans | 21.74| 30.98 | 7.99 | 20.23   |\\n| vs. SC BART  | +0.97| +1.40 | +2.72| +1.69   |\\n| vs. distilBART | -4.07| -1.53 | +1.77| -1.28   |\\n\\nTable 4: The inference time cost in milliseconds (ms) against a SOTA transformer-based error correction model (SC BART) and a distilled version of it (distilBART) on three benchmark datasets.\\n\\n| Method       | ATIS | SNIPS | TOP | Average |\\n|--------------|------|-------|-----|---------|\\n| SC BART      | 90.30| 75.30 | 75.08| 80.22   |\\n| distilBART   | 45.55| 41.55 | 40.08| 42.39   |\\n| ConstDecoder lstm | 14.31| 15.06 | 12.53| 13.96   |\\n| vs. SC BART  | 6.30 | 5.00  | 6.00 | 5.70    |\\n| vs. distilBART | 3.20| 2.80  | 3.20 | 3.00    |\\n| ConstDecoder trans | 25.61| 26.66 | 18.43| 23.56   |\\n| vs. SC BART  | 3.50 | 2.80  | 4.10 | 3.40    |\\n| vs. distilBART | 1.80| 1.60  | 2.20 | 1.80    |\\n\\n4.2. Results analysis\\n\\nTables 3-4 show the experimental results in terms of WER and inference speed of the proposed method against a SOTA transformer baseline (SC BART) on three benchmark datasets. Both SC BART and our method can reduce WER for the original datasets significantly. SC BART performs slightly better than our method (0.53%) in WER, possibly ascribing to: 1) It utilizes a more robust pretrained language model and; 2) It engages a fully autoregressive model. However, our method leads in inference speed, achieving at least three times faster decoding time on average for the three involved benchmark datasets. Applying model distillation techniques to BART-large followed by finetuning can enhance its inference speed moderately to 42.39 milliseconds but produce a worse WER (2.44% lower than that of ConstDecoder trans).\\n\\nIt can be observed that a transformer is a preferable option to LSTM for implementing the decoder in our method when WER is a major concern. Furthermore, our method achieves consistent results in all three benchmark datasets produced by different ASR techniques, demonstrating a reasonable level of robustness.\\n\\nTable 5: The effectiveness of the corrected ASR transcribed texts for different error types. For example, out of 61 grammatical errors in the original samples, 45 are corrected, resulting in a correction ratio (Cor. Ratio) of 73.77%.\\n\\n| Error Type | Example (ASR vs. Correction) | (Cor. Ratio) |\\n|------------|-----------------------------|--------------|\\n| grammatical | ASR: which flight sleep chicago on april | Cor.: which flights leave chicago on april | 73.77% |\\n| similar sound | ASR: sport events in roll e this weekend | Cor.: sporting events in raleigh this weekend | 32.08% |\\n| insertion | ASR: arrive in indian indianapolis | Cor.: arrive in indianapolis | 16.67% |\\n| entity | ASR: from mil to orlando | Cor.: from milwaukee to orlando | 10.20% |\\n| delete | ASR: most of the movie | Cor.: what are the movie schedules | 3.17% |\\n\\nTable 5 illustrates the result of a sampled analysis for the correction ratio presented in three datasets by randomly sampling one hundred cases from each dataset. It can be observed that the proposed method can address grammatical errors (73.77%) and issues associated with similar sounds (32.08%) effectively. A big gap is identified in addressing the entity, insertion, and deletion errors. A further look into the negative cases discloses that entity errors may be caused by a lack of knowledge handling the out-of-vocabulary (OOV) problem. It is worth noting that integrating a memory module to capture rare entities incrementally during error correction may be the solution, and we leave this as future work.\\n\\n5. Conclusions\\n\\nIn this paper, we illustrate an end-to-end ASR error correction method leveraging constrained decoding on operation prediction. Experiments on three benchmark datasets demonstrate the effectiveness of the proposed models in boosting inference speed by at least three times (3.4 and 5.7 times) while maintaining the same level of accuracy (with WER reductions of 0.53% and 1.69% respectively) compared to a SOTA transformer-based baseline. In the meantime, we apply internal ASR and TTS engines to a single-turn dialogue dataset, turning it into a publicly available benchmark dataset for ASR error correction. Future work will investigate the roles of incremental learning mechanisms in practice-oriented ASR error correction approaches to handle issues associated with the OOV problem mentioned above.\"}"}
{"id": "yang22g_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] C. Huang and Y. Chen, \u201cLearning asr-robust contextualized embeddings for spoken language understanding,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pp. 8009\u20138013.\\n\\n[2] Y. Weng, S. S. Miryala, C. Khatri, R. Wang, H. Zheng, P. Molino, M. Namazifar, A. Papangelis, H. Williams, F. Bell, and G. T\u00fcr, \u201cJoint contextual modeling for ASR correction and language understanding,\u201d in 2020 IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2020, Barcelona, Spain, May 4-8, 2020, pp. 6349\u20136353.\\n\\n[3] C. Liu, S. Zhu, L. Chen, and K. Yu, \u201cRobust spoken language understanding with rl-based value error recovery,\u201d in Natural Language Processing and Chinese Computing - 9th CCF International Conference, NLPCC 2020, Zhengzhou, China, October 14-18, 2020, Proceedings, Part I, ser. Lecture Notes in Computer Science, vol. 12430, pp. 78\u201390.\\n\\n[4] J. Guo, T. N. Sainath, and R. J. Weiss, \u201cA spelling correction model for end-to-end speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP 2019, Brighton, United Kingdom, May 12-17, 2019, pp. 5651\u20135655.\\n\\n[5] H. Wang, S. Dong, Y. Liu, J. Logan, A. K. Agrawal, and Y. Liu, \u201cASR error correction with augmented transformer for entity retrieval,\u201d in Interspeech 2020, 21st Annual Conference of the International Speech Communication Association, Virtual Event, Shanghai, China, 25-29 October 2020, pp. 1550\u20131554.\\n\\n[6] D. Bekal, A. Shenoy, M. Sunkara, S. Bodapati, and K. Kirchhoff, \u201cRemember the context! ASR slot error correction through memorization,\u201d in IEEE Automatic Speech Recognition and Understanding Workshop, ASRU 2021, Cartagena, Colombia, December 13-17, 2021, pp. 236\u2013243.\\n\\n[7] Y. Zhao, X. Yang, J. Wang, Y. Gao, C. Yan, and Y. Zhou, \u201cBART based semantic correction for mandarin automatic speech recognition system,\u201d in Interspeech 2021, 22nd Annual Conference of the International Speech Communication Association, Brno, Czechia, 30 August - 3 September 2021, pp. 2017\u20132021.\\n\\n[8] S. Zhang, M. Lei, and Z. Yan, \u201cInvestigation of transformer based spelling correction model for ctc-based end-to-end mandarin speech recognition,\u201d in Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019, pp. 2180\u20132184.\\n\\n[9] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT: pre-training of deep bidirectional transformers for language understanding,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers), pp. 4171\u20134186.\\n\\n[10] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBART: denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\u201d in Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pp. 7871\u20137880.\\n\\n[11] W. Li, H. Di, L. Wang, K. Ouchi, and J. Lu, \u201cBoost transformer with BERT and copying mechanism for ASR error correction,\u201d in International Joint Conference on Neural Networks, IJCNN 2021, Shenzhen, China, July 18-22, 2021, pp. 1\u20136.\\n\\n[12] S. Gupta, R. Shah, M. Mohit, A. Kumar, and M. Lewis, \u201cSemantic parsing for task oriented dialog using hierarchical representations,\u201d in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, Brussels, Belgium, October 31 - November 4, 2018, pp. 2787\u20132792.\\n\\n[13] Y. Chen, C. Cheng, C. Chen, M. Sung, and Y. Yeh, \u201cIntegrated semantic and phonetic post-correction for chinese speech recognition,\u201d in Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing, ROCLING 2021, Taoyuan, Taiwan, October 15-16, 2021. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP), 2021, pp. 95\u2013102.\"}"}
