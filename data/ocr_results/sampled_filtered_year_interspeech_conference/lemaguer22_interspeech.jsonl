{"id": "lemaguer22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Back to the Future: Extending the Blizzard Challenge 2013\\n\\nS\u00e9bastien Le Maguer1, Simon King2, Naomi Harte1\\n\\n1Sigmedia Lab, ADAPT Centre, School of Engineering, Trinity College Dublin, Ireland\\n2The Centre for Speech Technology Research, University of Edinburgh, UK\\nlemagues@tcd.ie, Simon.King@ed.ac.uk, nharte@tcd.ie\\n\\nAbstract\\n\\nNowadays, speech synthesis technology is synonymous with the use of Deep Learning. To understand more about how synthesis systems have progressed with the advent of Deep Learning requires open-sourced speech resources that connect past and present technologies. This would allow direct comparisons. This paper presents such a resource by extending the 2013 edition of the Blizzard Challenge. Using this extension, we compare top-tier systems from the past to modern technologies in a controlled setting. From this edition, we selected the best representative of each historical synthesis technology, to which we added four systems representing combinations of modern acoustic models and neural vocoders. A large scale subjective evaluation was conducted to evaluate naturalness.\\n\\nOur results show that, as expected, modern technologies generate more natural synthetic speech. However, these systems are still not perceived to be as natural as the human voice. Crucially, we also observed that the Mean Opinion Score (MOS) of the historical systems dropped a full MOS point from their scores in the original edition. This demonstrates the relative nature of MOS: it should generally not be reported as an absolute value despite its origin as an absolute category rating.\\n\\nIndex Terms: speech synthesis evaluation, Blizzard Challenge, reproducibility\\n\\n1. Introduction\\n\\nAll the submissions to the most recent edition of the Blizzard Challenge [1] were based on Deep Learning. This illustrates how ubiquitous this type of model has become in speech synthesis, which is the result of two breakthroughs in the last 6 years. The first [2] was the development of neural vocoders, capable of rendering a signal almost indistinguishable from human speech. The second [3] was the development of Sequence-To-Sequence (Seq2Seq) architectures which alleviated the need for expertly crafted features.\\n\\nAt the same time, large scale open-source datasets such as LibriTTS [4] or CSS10 [5] have been developed to train these models. The availability of these corpora, combined with the democratization of open-source implementations (e.g. [6, 7, 8, 9]) of modern speech technologies, has enabled researchers to craft high-quality synthetic speech more easily. However, as a result, much of what was learned from using previous technologies on older datasets has become overshadowed. In order to understand which issues Deep Learning is able to address in comparison to previous technologies, we should not dismiss those past research efforts. In other words, we need anchors which connect past and present technologies.\\n\\nA first attempt to address this was proposed by Cooper et al. [10]. The primary goal of their study was to produce a dataset for training Deep Learning-based objective quality prediction systems. In order to generate this dataset, the authors conducted a large scale evaluation using samples from multiple editions of the Blizzard Challenge [11] and voice conversion challenge [12]. Because the speakers in these corpora vary between editions, and because multiple genres (e.g. news, novel or conversational) of sentences were evaluated at once, listeners had to assess a highly heterogeneous set of samples. Therefore, it is difficult to pinpoint how the listeners rated a specific sample. In addition to these issues, the measurement used in this study is the Mean Opinion Score (MOS).\\n\\nThe MOS is the result obtained after conducting an Absolute Category Rating (ACR) evaluation whose protocol is described in ITU-T recommendation P.800 [13]. Several studies [14, 15, 16, 17, 18, 19] have already addressed the limitations of the MOS for various domains including speech synthesis. In speech synthesis, the main problem of the MOS is that the use of a 5-point Likert scale leads to a compression of the results [16]. Consequently, some important differences are likely to be missed. Unlike the recommendation for the MUltiple Stimuli with Hidden Reference and Anchor (MUSHRA) [20], P.800 does not impose the use of a hidden reference or anchors. Although many evaluations of speech synthesis include the natural voice as one of the \u201csystems\u201d being evaluated, MOS differs from MUSHRA in that the listener is not aware of the presence of this reference. Therefore, as [15] points out, an additional problem is the interpretation of the MOS of a synthetic system.\\n\\nNonetheless, while research on speech synthesis evaluation remains active [21, 22, 23], the ACR protocol remains stubbornly predominant in the face of alternatives such as the MUSHRA [20]. The most likely reason behind the popularity of the ACR is convenience: it is easy to implement and is scalable for large subjective evaluation campaigns with several systems to evaluate. Unfortunately, this apparent simplicity also leads researchers to neglect the adequate design of their evaluation as demonstrated by Wester et al. [24] in a meta-analysis of the Interspeech 2014 submissions.\\n\\nTo analyze the evolution of synthetic speech and what each technique improves, we need to develop better approaches. Towards this goal, we have developed a dataset dedicated to the evaluation of synthetic speech, whose core property is maintaining a connection between past and present technologies. The Blizzard Challenge has run every year since 2005 and provides the ideal source of material to build such a dataset. The current study extends the 2013 edition. We selected the top-tier submissions to the original challenge to which we added modern speech synthesis by training four modern open-source systems using Seq2Seq acoustic models and neural vocoders on the same data. We then conducted a subjective evaluation with native English listeners. The resulting dataset, freely available for research purposes [1], includes the models and the configurations to reproduce them as well as the subjective evaluation results.\\n\\n1https://github.com/sigmedia/bc 2013 extension\\n\\n18-22 September 2022, Incheon, Korea\\n\\nCopyright \u00a9 2022 ISCA 2378 10.21437/Interspeech.2022-10633\"}"}
{"id": "lemaguer22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In addition to creating this valuable dataset, we use it to answer two critical questions about the evaluation of naturalness:\\n\\n\u2022 How do past top-tier systems compare to present technologies? (Section 3.2 and Section 3.3)\\n\u2022 How reliable are the Mean Opinion Scores associated with synthesis systems? (Section 3.4)\\n\\n2. Extending the Blizzard Challenge 2013\\n\\nThe Blizzard Challenge organizers have made numerous corpora freely available for research purposes. Among these corpora, the one provided in 2013 [25] has multiple advantages. It is in English and based on audiobooks with a more controlled prosody than the corpora in 2016\u20132018. This corpus is also the jointly most used (2011 and 2013 each have around 550 licenses issued).\\n\\n2.1. Training dataset and selected systems\\n\\nThe 2013 edition is composed of 2 tasks for English [25]: 2013-EH1 and 2013-EH2. Both tasks used data from a single speaker: a female American professional narrator and actor. The data comprises the speaker reading audiobooks of various literary genres. The organizers provided an unsegmented dataset of 300h of speech for 2013-EH1, whereas for 2013-EH2 there was a sentence-level segmented subset of about 20h of speech with corresponding transcriptions. For 2013-EH1, most audio files are only available in MP3 compressed form. For 2013-EH2, the audio files are noncompressed WAV files sampled at 44.1 kHz.\\n\\nWe chose to use 2013-EH2 because uncompressed audio was available and a segmentation into sentences is already provided. The resulting corpus comprises 9733 utterances whose durations vary between 0.5 s and 35.3 s.\\n\\n2.1.1. Selected Systems\\n\\nTen systems were evaluated during the original 2013-EH2 task. These submissions belonged to three speech synthesis families: unit selection; statistical parametric HMM-based; and hybrid.\\n\\nOur choice of submissions to include in the current work is determined by three criteria: exclude systems already rated with a low MOS; one representative system per family; avoid additional variables in our analysis.\\n\\nFollowing these criteria, system K is selected as the representative of the hybrid synthesis family. For HMM synthesis, systems I and C are the available candidates. Given that system C is the HTS baseline for which the implementation is freely available, and was widely used by the community at the time, we select system C as the HMM synthesis system. Finally, for the unit selection family, the candidates are systems L and N. The sampling rate used by system L is 44.1 kHz. But, because the sampling rate used by systems K, C and N is 16 kHz, we exclude system L to satisfy the third criterion. For the same reason, we downsampled the natural speech to 16 kHz.\\n\\n2.1.2. Test Utterances\\n\\nClark et al. [22] showed that the context of the utterances used during an evaluation influences its outcome. During the original listening test in 2013, sentence-sized and paragraph-sized speech samples were evaluated. However, the protocol developed to evaluate the paragraphs is not widely used by the speech synthesis community. Because of this, we restrict our experiments to sentences. Two types of sentence were available: novel sentences taken from audiobooks read by the same speaker, but not in the training set; news sentences extracted from a newspaper.\\n\\nWe are focusing on the evaluation of naturalness of these two types of sentences, and this difference is expected to influence the ratings. Novel sentences are more expressive than news sentences. We do not have samples of the news sentences for the natural voice.\\n\\nOur extension of the Blizzard Challenge 2013 included seven systems. The established Blizzard Challenge protocol requires that listeners rate only one sample per system for each section. To satisfy this constraint, given that we are evaluating fewer systems than in 2013, we had to select, per section, a subset of 8 sentences among the 11 test sentences previously used. In the analysis section, we therefore only analyze the ratings for the same subset of utterances from the original challenge.\\n\\n2.2. Extension with Modern Systems\\n\\nWe built four additional systems, covering the combinations of two acoustic modelling architectures with two neural vocoders, to represent modern technologies. For the acoustic models, we used Tacotron [3] as implemented by [9] and FastPitch [27, 28]. Tacotron is the original Sequence-To-Sequence architecture. It uses an attention mechanism both to learn phone-spectrogram alignment during training and to predict duration during inference. FastPitch, on the other hand, is fully-supervised regarding phone-spectrogram alignment during training, and performs explicit duration prediction during inference. Externally-provided F0 provides additional supervision and is likewise explicitly predicted during inference. FastPitch is therefore significantly faster to train, taking one day compared to Tacotron's three and a half weeks (on a single NVIDIA GeForce RTX 2080 Ti).\\n\\nAs the corpus contains utterances with long pauses which can derail the training of attention, we used guided-attention for Tacotron. This, and FastPitch, both require a phone-spectrogram alignment, which we obtained using Montreal Forced Aligner (MFA) [29].\\n\\nThe neural vocoders we selected are WaveNet [2, 6] and Parallel WaveGAN [30, 7]. WaveNet is the original neural vocoder and is therefore an important milestone in the evolution of speech synthesis. While the quality produced by WaveNet is known to be outstanding, training and inference are slow due to its auto-regressive architecture. Parallel WaveGAN uses the Generative Adversarial Network (GAN) paradigm, with a generator based on components developed for WaveNet. The main difference with WaveNet is that Parallel WaveGAN is not auto-regressive and the loss function is a multi-resolution STFT. This allows Parallel WaveGAN to be trained faster; it is also fast at inference time. We use the following identifiers for each system: FastPitch/WaveNet (F-N), FastPitch/Parallel WaveGAN (F-G), Tacotron/WaveNet (T-N) and Tacotron/Parallel WaveGAN (T-G).\\n\\nThe input to these models is a phonetic sequence, obtained using MaryTTS [31], enriched by appending elementary prosodic information (consistent with what was standard in 2013). For each vowel, we provide the lexical stress of the containing syllable. We associate the punctuation type to each pause label, if available.\\n\\nWhen training these systems, operating at 16 kHz is a necessary compromise in order to compare to the selected historical systems. Therefore, we selected these implementations as each of them provides a training recipe for either the Blizzard Challenge...\"}"}
{"id": "lemaguer22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In addition, the acoustic parameters are those proposed for Parallel WaveGAN: a mel-spectrogram using 80 filters with cutoff frequencies from 50 Hz for the lower bound and 7600 kHz for the upper bound.\\n\\nWe experimented with multiple normalization strategies which have been proposed for training neural vocoders [6, 8, 7, 9]. The results of these experiments showed that using a z-score normalization of the mel-spectrograms led to more stable results for both neural vocoders.\\n\\n2.3. Recruitment and evaluation protocol\\n\\nFor the current study, we are only concerned with evaluation of naturalness by native listeners. In 2013, these listeners (labelled EE in [25]) were recruited and the evaluation was run in-person at the University of Edinburgh\u2019s perceptual testing laboratory. Here, we recruited listeners via Prolific [32] and they performed the evaluation online. We pre-screened participants to balance their sex and English accent (UK/US). The total duration of the evaluation for each listener was calibrated to take typically 20 minutes and no more than 30 minutes, as recommended by the ITU-T [13]. Each participant was paid \u00a33.75, a rate suggested as good remuneration by Prolific.\\n\\nWe wished to deviate as little as possible from the subjective evaluation protocol used in 2013 to enable meaningful comparisons. Besides evaluating fewer samples, we split the post-test questionnaire used in the original challenge into pre- and post-test questionnaires. For the pre-test questionnaire, the listeners were asked about their age, accent (identified as \u201cdialect\u201d in the questionnaire), and listening conditions. For the post-test questionnaire, the listeners were asked about their familiarity with speech technologies. The questions were the same as those used in the original edition.\\n\\n3. Results and analysis\\n\\n3.1. Listeners\\n\\n68 native speakers of English participated in the listening test. Even though it was explicitly stated that listeners should use headphones, 8 of them indicated that they did not. While their responses are included in the released dataset, we removed them for the analysis presented in this paper.\\n\\nOf the 60 remaining listeners, 30 are native British English speakers, 29 native US English speakers and one of an unknown accent. This last listener is excluded from the analysis. Therefore, our analysis is based on 59 listeners\u2019 responses of which 28 are from females and 31 from males.\\n\\n3.2. How do modern systems compare to past top-tier ones?\\n\\nThe overall results are presented in a standard boxplot (Figure 1) and a summary table (Table 1). As observed in [15], comparing the average MOS provides little information about the statistical significance between the systems. Thus as recommended by [15], we conducted pairwise Wilcoxon signed-rank tests between systems\u2019 MOS to test the ordering significance.\\n\\nTable 2 presents the result of this test. In all the figures and tables, we use the suffix \u201c-E\u201d to identify the new results from our listening test of the historical systems.\\n\\nIf we focus on the results for the extension (rows without overlay in Table 1 and green boxes in Figure 1), we can see the following trends. First of all, the natural voice (A-16) is considered the best with a median MOS of 5. Next come A, K, T-N, F-N, T-G, F-G, N, C, K-E, N-E, C-E.\\n\\nThe Seq2Seq systems with median MOS varying from 4 for Tacotron/WaveNet to 3 for FastPitch/WaveGAN. Finally, the historical systems are rated as the least natural: the median MOS for K, N and C are varying from 3 to 2.\\n\\nThe results presented in table 2 confirm the previous ordering with the following additional information: F-G differs significantly from T-N only for the news sentences; systems N and C do not differ significantly from each other.\\n\\nAs expected, modern systems produce more natural-sounding synthetic speech than all 2013 state-of-the-art technologies. The difference between F-G and T-N/F-N is also consistent with the results reported in [27, 30]. Our results also\"}"}
{"id": "lemaguer22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Significance results of a pairwise Wilcoxon signed rank test with Bonferroni correction for our listening test. Each cell marked with \u25a0 indicates that the two systems are considered different with a p-value < 0.01 for both novel and news sentences; with \u25a0 only for news sentences; with \u25a0 only for novel sentences. We do not have any samples of news sentences for the natural voice, so the column A-16 has only been computed using novel sentences.\\n\\nshow that, despite this improvement, the latest technologies still do not typically generate speech that is considered as natural as recorded human speech.\\n\\n3.3. Investigating the influence of each balanced dimension\\n\\nTable 2 shows that the pattern is stable across the two types of sentences with the only discrepancy being that, for news sentences, F-G is considered less natural than T-N. This difference is likely due to the absence of samples of news sentences for the natural voice; the natural voice being rated significantly higher for the novel sentences. As the difference in required expressivity could also play a role, a more dedicated evaluation would be required to explore this. In a similar way, we compared significant differences across the sex of the listeners. The results showed that F-G does not differ significantly from the other Seq2Seq systems and K does not differ significantly from C for male listeners. The data showed this could not be attributed to the listeners' declared familiarity with Text-To-Speech (TTS). This difference for males is potentially due to additional factors e.g. linguistic skills or other demographic factors, but our listener tests were not designed specifically to investigate gender differences and thus this result should not be over-interpreted [33]. Finally, when comparing the significance patterns regarding the accent of the listeners, we did not observe any differences between British and American listeners.\\n\\nOur results are consistent with concerns over MOS already raised by several studies [15, 24, 19]. At 60 listeners, we can adequately rank the systems for naturalness, but the scores do not permit further analysis of the differences between systems. Even when controlling the balance of factors such as sex and accent of English, a typical ACR is not sufficient to give insights into how these factors influence perception. Alternative protocols, such as MUSHRA [20] and Prosody-MOS [23], may reveal more but new protocols are clearly needed, if we wish to properly evaluate modern synthesis systems.\\n\\n3.4. How reliable is MOS?\\n\\nWhen comparing the results of the submissions from the original 2013 edition, we observe the following. From Table 1, we see that downsampling the natural recorded speech does not significantly impact the perception of its naturalness, which is reassuring: recorded speech should indeed be rated as perfectly natural regardless of its bandwidth (within reasonable limits). However from the same table, the most striking point is the drop in MOS (both the average and the median) for all the historical systems, when comparing their scores from the 2013 listening test with the current one. Specifically, system K average MOS drops from 3.81 to 2.62; system N average MOS from 3.22 to 2.00 and system C average MOS from 2.88 to 1.96. Exactly the same stimuli were presented to listeners in both listening tests, yet the MOS scores given by listeners to each and every one of these systems dropped by a full point.\\n\\nDespite this drop in the absolute scores, the ranking of systems is maintained. System K is still considered as more natural than C and N with these two systems no longer found to be significantly different. In other words, the scores have been compressed. This is likely to be the consequence of the presence of the neural voices in this listening test. It might also be due to a change in the listener population, who are likely to be more exposed to higher-quality synthetic speech than the 2013 listeners. The fact remains that MOS is not absolute. Researchers should never report an isolated MOS out of context and must be cautious about their conclusions when using this metric.\\n\\nOur results have immediate implications for MOS prediction models. Their central tenet is that MOS is an absolute category rating. We have shown that MOS is highly sensitive to which systems are included in a particular listening test: it is in fact partially or perhaps entirely a relative score. It is certainly not valid to simply pool MOS obtained from multiple listening tests, to form a training set for a MOS prediction model.\\n\\n4. Conclusions\\n\\nBy extending the Blizzard Challenge 2013, we created a dataset connecting past and present synthesis technologies. This dataset uses open-source tools and other resources that are free for research purposes. Using this dataset, we investigated the evolution of MOS. As expected, we showed that modern synthesis technologies are perceived as more natural. Yet we also found that the natural voice remains distinguishable from synthetic voices. Crucially, we observed that the MOS of the historical systems dropped a full point compared to the original 2013 listening test. This demonstrates the relative nature of MOS despite its origin as an absolute category rating. An ACR is deceptively easy to conduct: the difficulty lies in the interpretation and reliability of the results.\\n\\nFurther replication studies are necessary to determine the extent to which we can rely on the results of the highly subjective evaluation protocols used in our field. This also applies to other protocols such as the MUSHRA [20] even if it is a hybrid rating/ranking task which removes the temptation to quote absolute scores. In the meantime, authors should not draw conclusions about the performance of their system solely based on its MOS value.\\n\\n5. Acknowledgements\\n\\nThe authors would like to thank Rob Clark, Rasmus Dall, Casilia Valentini-Botinhao, Erica Cooper, Xing Wang and Junichi Yamagishi for the fruitful discussions on the topic presented in this study. This research was conducted with the financial support of Irish Research Council (IRC) under Grant Agreement No. 208222/15425 at the ADAPT SFI Research Centre at Trinity College Dublin. ADAPT, the SFI Research Centre for AI-Driven Digital Content Technology, is funded by Science Foundation Ireland through the SFI Research Centers Programme under Grant No. 13/RC/2106_P2.\"}"}
{"id": "lemaguer22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Z.-H. Ling, X. Zhou, and S. King, \u201cThe blizzard challenge 2021,\u201d in The Blizzard Challenge Workshop, 2021, http://www.festvox.org/blizzard/bc2021/BC21_aling_zhou_king.pdf.\\n\\n[2] A. Van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d CoRR, vol. abs/1609.03499, 2016. [Online]. Available: http://arxiv.org/abs/1609.03499\\n\\n[3] Y. Wang, R. J. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio, Q. V. Le, Y. Agiomyrgiannakis, R. Clark, and R. A. Saurous, \u201cTacotron: A fully end-to-end text-to-speech synthesis model,\u201d CoRR, 2017. [Online]. Available: http://arxiv.org/abs/1703.10135\\n\\n[4] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A Corpus Derived from LibriSpeech for Text-to-Speech,\u201d in Proc. Interspeech 2019, 2019, pp. 1526\u20131530.\\n\\n[5] K. Park and T. Mulc, \u201cCSS10: A Collection of Single Speaker Speech Datasets for 10 Languages,\u201d in Proc. Interspeech 2019, 2019, pp. 1566\u20131570.\\n\\n[6] R. Yamamoto. (2020) WaveNET. [Online]. Available: https://github.com/r9y9/wavenet\\n\\n[7] T. Hayashi. (2021) Parallel WaveGAN. [Online]. Available: https://github.com/kan-bayashi/ParallelWaveGAN/commit/6d4411b65f9487de5ec49dabf029dc107f23192d\\n\\n[8] NVIDIA Group. (2020) waveglow. [Online]. Available: https://github.com/NVIDIA/waveglow/commit/8afb643df59265016af6bd255c7516309d675168\\n\\n[9] C. Valentini-Botinhao, A. Govender, and O. McCarthy. (2020) Tacotron + WaveRNN. [Online]. Available: https://github.com/cassiavb/Tacotron/commit/946408f8cd7b5fe9c53931c631267ba2a723910d\\n\\n[10] E. Cooper and J. Yamagishi, \u201cHow do Voices from Past Speech Synthesis Challenges Compare Today?\u201d in ISCA Speech Synthesis Workshop (SSW 11), 2021, pp. 183\u2013188.\\n\\n[11] A. W. Black and K. Tokuda, \u201cThe blizzard challenge - 2005: evaluating corpus-based speech synthesis on common datasets,\u201d in INTERSPEECH 2005 - Eurospeech, 9th European Conference on Speech Communication and Technology, Lisbon, Portugal, September 4-8, 2005, 2005, pp. 77\u201380.\\n\\n[12] B. Sisman, J. Yamagishi, S. King, and H. Li, \u201cAn overview of voice conversion and its challenges: From statistical modeling to deep learning,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, p. 132\u2013157, 2021.\\n\\n[13] ITU, \u201cMethods for subjective determination of transmission quality,\u201d International Telecommunication Union (ITU-P), Geneva, ITU-T Recommendation P.800, 1996.\\n\\n[14] M. Viswanathan and M. Viswanathan, \u201cMeasuring speech quality for text-to-speech systems: development and assessment of a modified mean opinion score (MOS) scale,\u201d Computer Speech & Language, vol. 19, no. 1, pp. 55\u201383, 2005.\\n\\n[15] R. A. Clark, M. Podsiadlo, M. Fraser, C. Mayo, and S. King, \u201cStatistical analysis of the blizzard challenge 2007 listening test results,\u201d in The Blizzard Challenge Workshop, 2007, http://festvox.org/blizzard/bc2007/blizzard2007/full_papers/blz3003.pdf.\\n\\n[16] S. Zielinski, F. Rumsey, and S. Bech, \u201cOn Some Biases Encountered in Modern Audio Quality Listening Tests-A Review,\u201d Journal of the Audio Engineering Society (JAES), vol. 56, no. 6, pp. 427\u2013451, Jun 2008.\\n\\n[17] R. C. Streijl, S. Winkler, and D. S. Hands, \u201cMean opinion score (MOS) revisited: methods and applications, limitations and alternatives,\u201d Multimedia Systems, vol. 22, no. 2, pp. 213\u2013227, 2016.\\n\\n[18] A. Rosenberg and B. Ramabhadran, \u201cBias and statistical significance in evaluating speech synthesis with mean opinion scores,\u201d in Proc. Interspeech 2017, 2017, pp. 3976\u20133980. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2017-479\\n\\n[19] S. Shirali-Shahreza and G. Penn, \u201cMOS Naturalness and the Quest for Human-Like Speech,\u201d IEEE Spoken Language Technology Workshop (SLT), Dec 2018.\\n\\n[20] ITU-T, \u201cMethod for the subjective assessment of intermediate sound quality (MUSHRA),\u201d International Telecommunication Union (ITU-R), Tech. Rep. P.1534-1, 2001.\\n\\n[21] P. Wagner, J. Beskow, S. Betz, J. Edlund, J. Gustafson, G. E. Henter, S. L. Maguer, Z. Malisz, \u00b4E. Sz\u00b4ekely, C. T\u02daannander et al., \u201cSpeech Synthesis Evaluation\u2014State-of-the-Art Assessment and Suggestion for a Novel Research Program,\u201d in Speech Synthesis Workshop (SSW), 2019, pp. 105\u2013110.\\n\\n[22] R. Clark, H. Silen, T. Kenter, and R. Leith, \u201cEvaluating Long-form Text-to-Speech: Comparing the Ratings of Sentences and Paragraphs,\u201d in Proceedings of the Speech Synthesis Workshop (SSW), 2019, pp. 99\u2013104.\\n\\n[23] J. O\u2019Mahony, P. Oplustil-Gallegos, C. Lai, and S. King, \u201cFactors Affecting the Evaluation of Synthetic Speech in Context,\u201d in Proc. 11th ISCA Speech Synthesis Workshop (SSW 11), 2021, pp. 148\u2013153.\\n\\n[24] M. Wester, C. Valentini-Botinhao, and G. E. Henter, \u201cAre we using enough listeners? no! - an empirically-supported critique of interspeech 2014 TTS evaluations,\u201d in Annual Conference of the International Speech Communication Association (Interspeech), 2015, pp. 3476\u20133480.\\n\\n[25] S. King and V. Karaiskos, \u201cThe blizzard challenge 2013,\u201d in The Blizzard Challenge Workshop, 2013, http://festvox.org/blizzard/bc2013/summary Blizzard2013.pdf.\\n\\n[26] F. Hinterleitner, G. Neitzel, S. M \u00a8oller, and C. Norrenbrock, \u201cAn evaluation protocol for the subjective assessment of text-to-speech in audiobook reading tasks,\u201d in The Blizzard Challenge Workshop, 2011. [Online]. Available: http://festvox.org/blizzard/bc2011/DeutcheTelekomBlizzard2011.pdf\\n\\n[27] A. \u0141a \u00b4ncucki, \u201cFastpitch: Parallel Text-to-Speech with Pitch Prediction,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, Jun. 2021, pp. 6588\u20136592.\\n\\n[28] NVIDIA Group. (2021) FastPitch. [Online]. Available: https://github.com/NVIDIA/DeepLearningExamples/commit/6a642837c471c596aab7edf204384f66e9483ab2\\n\\n[29] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, \u201cMontreal forced aligner: Trainable text-speech alignment using kaldi.\u201d in International Conference on Speech Communication and Technology (Interspeech), 2017, pp. 498\u2013502.\\n\\n[30] R. Yamamoto, E. Song, and J.-M. Kim, \u201cParallel Wavegan: A Fast Waveform Generation Model Based on Generative Adversarial Networks with Multi-Resolution Spectrogram,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, May 2020, pp. 6199\u20136203.\\n\\n[31] I. Steiner and S. L. Maguer, \u201cCreating New Language and Voice Components for the Updated MaryTTS Text-to-Speech Synthesis Platform,\u201d in International Conference on Language Resources and Evaluation (LREC), 2018.\\n\\n[32] \u201cProlific\u00b7 Quickly find research participants you can trust.\u201d [Online]. Available: https://www.prolific.co\\n\\n[33] E. Gaudrain, J. Undurraga, N. Grimault, and D. \u00b8Baskent, \u201cComments on \u201cdifferences in common psychoacoustical tasks by sex, menstrual cycle, and race\u201d by d. mcfadden et al., 2018, and methodological pitfalls in human population research,\u201d 2020.\"}"}
