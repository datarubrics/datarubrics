{"id": "hai24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DreamVoice: Text-Guided Voice Conversion\\n\\nJiarui Hai1,\u2020, Karan Thakkar1,\u2020, Helin Wang1, Zengyi Qin2,3, Mounya Elhilali1,\u22c6\\n\\n1Electrical and Computer Engineering, Johns Hopkins University, Baltimore, MD, USA\\n2Massachusetts Institute of Technology, Cambridge, MA, USA\\n3MyShell.ai, USA\\n\\njhai2@jhu.edu, kthakkar2@jhu.edu, hwang258@jhu.edu, qinzy@mit.edu, mounya@jhu.edu\\n\\nAbstract\\n\\nGenerative voice technologies are rapidly evolving, offering opportunities for more personalized and inclusive experiences. Traditional one-shot voice conversion (VC) requires a target recording during inference, limiting ease of usage in generating desired voice timbres. Text-guided generation offers an intuitive solution to convert voices to desired \u201cDreamVoices\u201d according to the users\u2019 needs. Our paper presents two major contributions to VC technology: (1) DreamVoiceDB, a robust dataset of voice timbre annotations for 900 speakers from VCTK and LibriTTS. (2) Two text-guided VC methods: DreamVC, an end-to-end diffusion-based text-guided VC model; and DreamVG, a versatile text-to-voice generation plugin that can be combined with any one-shot VC models. The experimental results demonstrate that our proposed methods trained on the DreamVoiceDB dataset generate voice timbres accurately aligned with the text prompt and achieve high-quality VC.\\n\\nIndex Terms: voice conversion, voice timbre, prompt, diffusion probabilistic models\\n\\n1. Introduction\\n\\nThe emergence of augmented reality devices and accessible virtual environments marks a significant technological shift [1, 2]. This transformation underscores the need to develop tools that enhance user experience in these virtual spaces, ensuring safety and engagement. Therefore, highlighting the need for more intuitive interaction methods with such technologies. Imagine a world where a user (source) can effortlessly modify their voice to suit their digital persona (target), from specifying \u201ca young male voice with a dark tone and smooth texture\u201d to customizing auditory experiences like making \u201cplayer A\u2019s voice less harsh.\u201d This technology holds particular significance for individuals with gender dysphoria or speech impairments, offering them avenues for expression that were previously inaccessible.\\n\\nAn essential aspect of VC is providing the model with a robust and accessible representation of the target voice during inference or training. Traditionally, one-shot VC models rely on the availability of target recording to extract pre-trained speaker embeddings [3, 4, 5, 6] during inference. However, the accessibility of the target recording or embeddings is not always feasible for all applications. Recently, there has been a shift towards using text-guided control or conditioning for generative audio tasks like expressive Text-to-Speech [7, 8], Text-to-Audio [9] and Style Transfer [10]. Ultimately, the shift towards text-guided control, while offering scalability and flexibility, hinges critically on the quality of text annotations.\\n\\n\u2020 Indicates equal contribution.\\n\\n\u22c6 This work was supported in part by ONR N00014-23-1-2050 and N00014-23-1-2086.\\n\\nPrevious research studies [10, 7, 8] have attempted to annotate voice timbre, also recognized as tone or color of voice, using text-based methods. However, these datasets often face limitations such as small scale, synthetic markings, restricted access, or opaque collection strategies. PromptTTS++ [7] employs a keyword-based marking strategy on a subset of speakers in the LibriTTS dataset [11]. However, the annotation details are not clearly stated and the annotated data has not been released to the best of our knowledge. Promptspeaker [8] uses an semi-synthetic Mandarin dataset, of which only the internal subset of 74 speakers contains detailed timbre annotations and the remaining data merely contains gender or age information, based on the actual ground truth labels, rather than the perceived timbre. PromptVC [10] uses an internal Mandarin dataset that only contains six speakers and lacks detailed documentation on the control of the text annotations of style and timbre. Hence, creating a comprehensive, meticulously detailed, and open-source dataset of text annotations will play a pivotal role in advancing text-based voice control and conditioning.\\n\\nThis study explores the task of text-guided voice generation and conversion, unlike [10, 7] that use text-guidance for speech content control and generation. Our main contributions are summarized as follows:\\n\\n1. We release DreamVoiceDB, an extensive, open-source voice timbre dataset of 900 speakers sampled from LibriTTS-R [11] and VCTK [12] dataset, annotated by speech and language experts for high-quality research applications.\\n2. We propose two text-guided voice generation and conversion models: DreamVC, an innovative text-guided voice timbre conversion model using Diffusion Probabilistic Models (DPM) [13] and Classifier-Free Guidance (CFG) [14] for effective condition controllability; DreamVG, a light and plug-and-play text-to-voice generation plugin model compatible with one-shot VC models. DreamVG also uses DPM with CFG to generate speaker embeddings.\\n3. We experimentally demonstrate that the proposed dataset and models can achieve high-quality voice conversion with timbres that are precisely aligned with the text description.\\n\\n2. DreamVoiceDB: Voice Timbre Dataset\\n\\nVoice timbre emerges from a combination of factors, including age, gender, physical properties of the vocal tract and vocal cord, and perceptual characteristics. To accurately capture the rich characteristics of timbre we followed a comprehensive three-stage process as summarized in Figure 1.\\n\\nIn the first stage, 900 speakers were sampled from existing multi-speaker datasets LibriTTS-R [11] and VCTK [12]. Further details and more information can be found on the official website: https://research.myshell.ai/dreamvoice.\"}"}
{"id": "hai24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 1: Expert Consultation and Keyword Selection\\n\\nStage 2: Survey Design and Data Collection\\n\\nStage 3: Analysis and Label Alignment\\n\\nWith Reference Audio\\n\\nWithout Reference Audio\\n\\nFigure 1: Schematic diagram of DreamVoiceDB survey method.\\n\\nFollowing this, an expert voice actor guided us through the selection of keywords that best represent voice timbre. A total of 10 keywords were split into two categories based on their level of subjectivity. The first category focuses on basic, more objective timbre aspects like age, gender, brightness, and roughness, while the second encompasses subjective characteristics such as perceived strength, warmth, and authority. In addition to these keywords, we extended our inquiry to include the perceived voice's suitability of voice-related professions such as Storytelling and Client Interaction, linking qualities that make each voice distinct and memorable.\\n\\nThe second stage involved combining the expert's knowledge deeply into our survey methodology. Questions were designed with reference audio examples to facilitate objective category annotations based on relative comparison. Assessment of subtler attributes like brightness and roughness was conducted using a Likert scale defined based on expert knowledge. The second category responses were measured using a binary scale for all the keywords in that category in compliance with their subjective nature. Following the survey design and release, a test run was conducted to recruit the best annotators. A total of 8 expert annotators, comprising 4 females and males, were selected for the study. Annotator's expertise spanned speech-related fields such as speech and language pathology, speech and accent coaching, singing coaching, and transcription work. All 900 speakers were annotated once by each expert annotator.\\n\\nLastly, a comprehensive procedure was employed after data collection to align and investigate the identified keywords, prioritizing those based on their respective agreement scores. Keywords that garnered unanimous consensus among annotators were seamlessly integrated into the dataset. Conversely, keywords exhibiting moderate agreement levels were subjected to rigorous reassessment based on their agreement distribution and combination of manual self-reported scores. This process significantly augmented the dataset's precision and authenticity, keeping in mind the richness and diversity of the dataset. Following keyword annotations, we used OpenAI's GPT4 [15] API to generate approximately 50 natural language descriptors for each speaker depending on the combinations of the keywords. The prompts generated included all leave-one-out and leave-many-out combinations to cover a wide range of practical inputs. Further details about the keyword distribution and analysis code are available.\\n\\n3. Method\\n\\n3.1. General Voice Conversion Pipeline\\n\\nVoice conversion models work by taking the content of the source speech, mixing it with the target speaker's timbre, and then generating the converted voice. Recent VC methods often use latent features extracted from large pre-trained Speech Language Models (SLMs), which contain limited speaker information but rich content information [18], as the content embedding for the source speaker. In the case of one-shot VC, a pre-trained speaker verification model, trained on datasets with a large number of speakers, is utilized to extract the speaker embedding of the target speaker. The content embedding and the speaker embedding are then used to condition a VC model to synthesize speech with the content of the source speaker and the voice timbre of the target speaker. Researchers have deployed discriminative [4] and generative models for this task including GANs [19, 20] and Diffusion models [21, 22]. While GANs [23] are fragile in convergence and relatively hard to train, Diffusion [24] models provide a more stable alternative for training generative VC models. In addition, recent diffusion have shown promising performance in both diversity and quality on various text-guided generation tasks.\\n\\n3.2. Diffusion Models and Classifier-free Guidance\\n\\nDiffusion Probabilistic Models (DPMs) are characterized by a two-fold process: a forward process and a backward process. The forward process operates by incrementally introducing Gaussian noise into the data according to schedule $\\\\beta_1, \\\\ldots, \\\\beta_T$.\\n\\n$$q(x_{1:T} | x_0) := \\\\sum_{t=1}^{T} \\\\mathcal{N}(x_t | x_{t-1}, \\\\beta_t)$$\\n\\n$$q(x_t | x_{t-1}) := \\\\mathcal{N}(x_t; \\\\bar{\\\\alpha}_t x_{t-1}, \\\\tilde{\\\\beta}_t I)$$\\n\\nEquivalently:\\n\\n$$x_t := \\\\bar{\\\\alpha}_t x_0 + \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} \\\\epsilon,$$\\n\\nwhere $\\\\epsilon \\\\sim \\\\mathcal{N}(0, I)$\\n\\nwhere $\\\\alpha_t := 1 - \\\\beta_t$ and $\\\\bar{\\\\alpha}_t := \\\\frac{Q}{s \\\\sum \\\\alpha_s}$. The backward process is essential for iteratively recovering information, thereby enabling the generation of new data from random Gaussian noise. The key parameter in this process is $\\\\beta_t$, representing the noise variance at each timestep. When $\\\\beta_t$ is small, the reverse step aligns with a Gaussian distribution, facilitating the gradual denoising of the data.\\n\\n$$p_\\\\theta(x_0: T) := p(x_T) \\\\prod_{t=1}^{T} p_\\\\theta(x_{t-1} | x_t)$$\\n\\n$$p_\\\\theta(x_{t-1} | x_t) := \\\\mathcal{N}(x_{t-1}; \\\\tilde{\\\\mu}_t, \\\\tilde{\\\\beta}_t I)$$\\n\\nwhere variance $\\\\tilde{\\\\beta}_t$ can be calculated from the forward process posteriors:\\n\\n$$\\\\tilde{\\\\beta}_t := 1 - \\\\bar{\\\\alpha}_t - \\\\frac{1}{\\\\bar{\\\\alpha}_t} \\\\beta_t$$\\n\\nFollowing method proposed in [25] which has shown improvements in audio generation [26], we apply a fixed noisy schedule to $\\\\beta_t$ and $\\\\alpha_t$ and use velocity $v_t$ instead of noise $\\\\epsilon$ as the neural network's prediction target:\\n\\n$$v_t := \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\epsilon - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} x_0$$\\n\\nAccording to (4) and (7), the backward process is then performed by the following functions:\\n\\n$$x_0 := \\\\sqrt{\\\\bar{\\\\alpha}_t} x_t - \\\\sqrt{1 - \\\\bar{\\\\alpha}_t} v_t$$\\n\\n\"}"}
{"id": "hai24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Overview of the (a) DreamVC, (b) DreamVG, and (c) Plugin Strategy. Modules in blue are pre-trained models and remain frozen during training, while modules in yellow are trained. Green blocks represent the source speaker information while red blocks represent the target speaker information. Purple blocks correspond to the converted speech. Dashed lines represent skip connections.\\n\\nLM represents the Language Model. KV represents Cross-Attention \\\\cite{16} and FiLM represents Feature-wise Linear Modulation layers \\\\cite{17} used for fusing Text Prompt and diffusion step $t$ respectively. SDE solver is the stochastic differential equations for the diffusion sampling. Text Prompt is the text description about the desired target voice.\\n\\n$\\\\mu_t := \\\\sqrt{\\\\bar{\\\\alpha}_t - 1} \\\\beta_t \\\\bar{\\\\alpha}_t x_0 + \\\\sqrt{\\\\alpha_t (1 - \\\\bar{\\\\alpha}_t - 1)} x_t$ (9)\\n\\nCFG \\\\cite{14} is increasingly being adopted to steer the sampling process in diffusion models. This technique modifies the model output $v$ during sampling, as described by the equation:\\n\\n$v_{\\\\text{cfg}} = v_{\\\\text{neg}} + w (v_{\\\\text{pos}} - v_{\\\\text{neg}})$ (10)\\n\\nwhere $w$ represents the guidance scale, and $v_{\\\\text{pos}}$ and $v_{\\\\text{neg}}$ denote the model outputs under positive and negative conditions, respectively. And $v_{\\\\text{cfg}}$ is the classifier-free guided velocity.\\n\\nTo further refine this process, a rescaling method proposed in \\\\cite{25} is applied to $v_{\\\\text{cfg}}$ to enhance its effectiveness and mitigate over-exposure when $w$ is large.\\n\\n$v_{\\\\text{re}} = v_{\\\\text{cfg}} \\\\cdot \\\\text{std}(v_{\\\\text{pos}}) / \\\\text{std}(v_{\\\\text{cfg}})$ (11)\\n\\n$v'_{\\\\text{cfg}} = \\\\phi \\\\cdot v_{\\\\text{re}} + (1 - \\\\phi) \\\\cdot v_{\\\\text{cfg}}$ (12)\\n\\nHere, $\\\\phi$ is a hyperparameter used to control the strength of the rescale adjustment. $v'_{\\\\text{cfg}}$ is the rescaled CFG velocity used for diffusion sampling.\\n\\n3.3. DreamVC: Text-to-Voice Conversion Model\\n\\nThe DreamVC model leverages a text-guided process to modify the timbre of the source speech based on the given text prompt. The model is based on a conditional diffusion model that uses speech content and text prompt as dual conditions to guide the generation of the output as detailed in Figure 2(a). The output mel-spectrogram is converted to waveform using the pre-trained neural vocoder. This model distinguishes itself from DiffVC \\\\cite{21} in several key aspects: it eschews the use of an average-voice encoder and instead uses a pre-trained SLM for disentangling voice and content, integrates cross-attention layers to merge text prompts effectively, and employs the CFG to control the impact of conditions.\\n\\n3.4. DreamVG: Text-to-Voice Generation Plugin\\n\\nHowever, as an end-to-end text-guided voice conversion model based on the diffusion model, DreamVC faces limitations in real-world applications due to its drawbacks such as slow inference speed, high memory usage, expensive training, and difficulty in reproducing a desirable voice that was once generated. To address these issues, we introduce DreamVG, an alternative model that adopts a plug-and-use strategy. DreamVG efficiently generates latent speaker embeddings from text prompts using a conditional diffusion model, enhancing its practicality and application scope, as illustrated in Figure 2(b). This module can act as a replacement for any one-shot VC model that uses latent speaker embeddings to generate the target voice, as shown in Figure 2(c). The plugin method of DreamVG boosts the functionality of pre-trained one-shot voice conversion models, rendering it a flexible solution to enable text guidance.\\n\\n4. Experiments\\n\\n4.1. Experimental Settings\\n\\nFor this study, we used the recordings of 900 speakers from VCTK \\\\cite{12} and LibriTTS-R \\\\cite{11} datasets and their text prompts from the proposed annotated dataset DreamVoiceDB as mentioned in Section 2 for training, and used speakers from LibriTTS-R dev set for validation and test. All the audio files were sampled at 24KHz for synthesis and 16KHz for content and speaker embedding extraction.\\n\\nThe U-Net model \\\\cite{27} used in DreamVC has 3 downsampling and 3 upsampling blocks configured with 128, 256, and 512 channels respectively, and each of the 4 blocks in the middle has a cross-attention layer, totaling 103.2M parameters. The pre-trained T5 base \\\\cite{28}, noted for its excellence in various generative tasks, is used to process text prompts. The pre-trained ContentVec \\\\cite{18} is used for content embedding extraction and a\"}"}
{"id": "hai24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of Objective scores: Word Error Rate (WER), Phoneme Error Rate (PER), Relative Inference Speed (RIS), and Mean Opinion Scores (MOS) with their 95% confidence intervals (CI): Q-Quality, N-Naturalness, C-Prompt-Voice-Consistency.\\n\\n| Method               | WER  | PER  | RIS  | MOS-Q  | MOS-N  | MOS-C  |\\n|----------------------|------|------|------|--------|--------|--------|\\n| Ground-Truth         | 4.32 | 4.26 |      |        |        |        |\\n| FreeVC               | 6.37 | 5.79 |      | 4.09   | 3.98   |        |\\n| ReDiffVC             | 3.45 | 2.66 |      | 3.67   | 3.76   |        |\\n| DreamVC              | 4.10 | 8.08 | 1.00 | 3.62   | 3.61   |        |\\n| DreamVG+FreeVC       | 7.58 | 10.05| 2.71 | 3.90   | 3.85   |        |\\n| DreamVG+ReDiffVC     | 5.11 | 8.65 | 1.08 | 3.80   | 3.76   |        |\\n\\npre-trained BigVGAN [29] is employed as the neural vocoder. Based on the configuration of the BigVGAN vocoder, the mel-spectrogram has 100 mel-spectrograms, a window size of 1024, and a hop size of 256. Embeddings extracted from ContenVec are duplicated by hard mapping to match the sample rate of mel-spectrogram. The diffusion steps and inference steps for the default DreamVC are 1000 and 50 respectively, and the corresponding variance $\\\\beta$ is set from 0.0001 to 0.02. During sampling, we found the value of guidance scale $w$ as 3, a rescaling factor $\\\\phi$ of 0.7, and setting an unconditional prompt as the negative condition can lead to better generation quality.\\n\\nThe neural network in DreamVG has three blocks configured with 128, 256, and 256 channels, where each block has a cross-attention layer, totaling 26.2M parameters. Similar to DreamVC, we use the T5 base model to generate the prompt embeddings. We adopted the speaker verification model commonly applied in one-shot VC models [22, 30] as the model output for DreamVG. The diffusion steps and inference steps for the default DreamVG are 1000 and 100 respectively, and the corresponding variance $\\\\beta$ is set from 0.0001 to 0.02. During sampling, we use a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [22] is one of the state-of-the-art one-shot voice conversion models that utilizes the VITS [31] architecture enhanced by GAN training. (2) ReDiffVC is a variation of DreamVC designed for one-shot voice conversion, which replaces cross-attention blocks with self-attention blocks. ReDiffVC incorporates the one-shot speaker embedding by adding it to the diffusion step embedding. Additionally, it employs CFG with a guidance scale $w$ of 3 and a rescaling factor $\\\\phi$ of 0.7. The DreamVG is integrated with two pre-trained one-shot VC models to facilitate text-guided control in VC: (1) FreeVC [2"}
{"id": "hai24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. Hamad and B. Jia, \u201cHow virtual reality technology has changed our lives: an overview of the current and potential applications and limitations,\u201d *International journal of environmental research and public health*, vol. 19, no. 18, p. 11278, 2022.\\n\\n[2] I. Hupont Torres, V. Charisi, G. de Prato, K. Pogorzelska, S. Schade, A. Kotsev, M. Sobolewski, N. Duch Brown, E. Calza, C. Dunker et al., \u201cNext generation virtual worlds: Societal, technological, economic and policy challenges for the eu,\u201d Joint Research Centre (Seville site), Tech. Rep., 2023.\\n\\n[3] B. Sisman, J. Yamagishi, S. King, and H. Li, \u201cAn overview of voice conversion and its challenges: From statistical modeling to deep learning,\u201d *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, vol. 29, pp. 132\u2013157, 2020.\\n\\n[4] K. Qian, Y. Zhang, S. Chang, X. Yang, and M. Hasegawa-Johnson, \u201cAutovc: Zero-shot voice style transfer with only autoencoder loss,\u201d in *International Conference on Machine Learning*. PMLR, 2019, pp. 5210\u20135219.\\n\\n[5] K. Qian, Z. Jin, M. Hasegawa-Johnson, and G. J. Mysore, \u201cF0-consistent many-to-many non-parallel voice conversion via conditional autoencoder,\u201d in *ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2020, pp. 6284\u20136288.\\n\\n[6] S. Nercessian, \u201cImproved zero-shot voice conversion using explicit conditioning signals.\u201d in *INTERSPEECH*, 2020, pp. 4711\u20134715.\\n\\n[7] R. Shimizu, R. Yamamoto, M. Kawamura, Y. Shirahata, T. Komatsu, K. Tachibana et al., \u201cPrompttts++: Controlling speaker identity in prompt-based text-to-speech using natural language descriptions,\u201d *arXiv preprint arXiv:2309.08140*, 2023.\\n\\n[8] Y. Zhang, G. Liu, Y. Lei, Y. Chen, H. Yin, L. Xie, and Z. Li, \u201cPromptspeaker: Speaker generation based on text descriptions,\u201d in *2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU)*. IEEE, 2023, pp. 1\u20137.\\n\\n[9] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, \u201cMake-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,\u201d in *International Conference on Machine Learning*. PMLR, 2023, pp. 13 916\u201313 932.\\n\\n[10] J. Yao, Y. Yang, Y. Lei, Z. Ning, Y. Hu, Y. Pan, J. Yin, H. Zhou, H. Lu, and L. Xie, \u201cPromptvc: Flexible stylistic voice conversion in latent space driven by natural language prompts,\u201d *arXiv preprint arXiv:2309.09262*, 2023.\\n\\n[11] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, M. Bacchiani, Y. Zhang, W. Han, and A. Bapna, \u201cLibritts-r: A restored multi-speaker text-to-speech corpus,\u201d *arXiv preprint arXiv:2305.18802*, 2023.\\n\\n[12] J. Yamagishi, C. Veaux, and K. MacDonald, \u201cCstr vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit (version 0.92),\u201d 2019. [Online]. Available: https://api.semanticscholar.org/CorpusID:213060286\\n\\n[13] J. Ho, A. Jain, and P. Abbeel, \u201cDenoising diffusion probabilistic models,\u201d *Advances in neural information processing systems*, vol. 33, pp. 6840\u20136851, 2020.\\n\\n[14] J. Ho and T. Salimans, \u201cClassifier-free diffusion guidance,\u201d in *NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications*, 2021.\\n\\n[15] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida, J. Altenschmidt, S. Altman, S. Anadkat et al., \u201cGpt-4 technical report,\u201d *arXiv preprint arXiv:2303.08774*, 2023.\\n\\n[16] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d *Advances in neural information processing systems*, vol. 30, 2017.\\n\\n[17] E. Perez, F. Strub, H. De Vries, V. Dumoulin, and A. Courville, \u201cFilm: Visual reasoning with a general conditioning layer,\u201d in *Proceedings of the AAAI conference on artificial intelligence*, vol. 32, no. 1, 2018.\\n\\n[18] K. Qian, Y. Zhang, H. Gao, J. Ni, C.-I. Lai, D. Cox, M. Hasegawa-Johnson, and S. Chang, \u201cContentvec: An improved self-supervised speech representation by disentangling speakers,\u201d in *International Conference on Machine Learning*. PMLR, 2022, pp. 18 003\u201318 017.\\n\\n[19] C.-C. Hsu, H.-T. Hwang, Y.-C. Wu, Y. Tsao, and H.-M. Wang, \u201cVoice conversion from unaligned corpora using variational autoencoding wasserstein generative adversarial networks,\u201d *arXiv preprint arXiv:1704.00849*, 2017.\\n\\n[20] H. Kameoka, T. Kaneko, K. Tanaka, and N. Hojo, \u201cStargan-vc: Non-parallel many-to-many voice conversion using star generative adversarial networks,\u201d in *2018 IEEE Spoken Language Technology Workshop (SLT)*. IEEE, 2018, pp. 266\u2013273.\\n\\n[21] V. Popov, I. Ovsk, V. Gogoryan, T. Sadekova, M. S. Kudinov, and J. Wei, \u201cDiffusion-based voice conversion with fast maximum likelihood sampling scheme,\u201d in *International Conference on Learning Representations*, 2021.\\n\\n[22] J. Li, W. Tu, and L. Xiao, \u201cFreevc: Towards high-quality text-free one-shot voice conversion,\u201d in *ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2023, pp. 1\u20135.\\n\\n[23] D. Saxena and J. Cao, \u201cGenerative adversarial networks (gans) challenges, solutions, and future directions,\u201d *ACM Computing Surveys (CSUR)*, vol. 54, no. 3, pp. 1\u201342, 2021.\\n\\n[24] L. Yang, Z. Zhang, Y. Song, S. Hong, R. Xu, Y. Zhao, W. Zhang, B. Cui, and M.-H. Yang, \u201cDiffusion models: A comprehensive survey of methods and applications,\u201d *ACM Computing Surveys*, vol. 56, no. 4, pp. 1\u201339, 2023.\\n\\n[25] S. Lin, B. Liu, J. Li, and X. Yang, \u201cCommon diffusion noise schedules and sample steps are flawed,\u201d in *Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision*, 2024, pp. 5404\u20135411.\\n\\n[26] J. Hai, H. Wang, D. Yang, K. Thakkar, N. Dehak, and M. Elhilali, \u201cDpm-tse: A diffusion probabilistic model for target sound extraction,\u201d in *ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2024, pp. 1196\u20131200.\\n\\n[27] O. Ronneberger, P. Fischer, and T. Brox, \u201cU-net: Convolutional networks for biomedical image segmentation,\u201d in *Medical image computing and computer-assisted intervention\u2013MICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III*. Springer, 2015, pp. 234\u2013241.\\n\\n[28] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d *Journal of machine learning research*, vol. 21, no. 140, pp. 1\u201367, 2020.\\n\\n[29] S.-g. Lee, W. Ping, B. Ginsburg, B. Catanzaro, and S. Yoon, \u201cBigvgan: A universal neural vocoder with large-scale training,\u201d in *The Eleventh International Conference on Learning Representations*, 2022.\\n\\n[30] S. Liu, Y. Cao, D. Wang, X. Wu, X. Liu, and H. Meng, \u201cAny-to-many voice conversion with location-relative sequence-to-sequence modeling,\u201d *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, vol. 29, pp. 1717\u20131728, 2021.\\n\\n[31] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in *International Conference on Machine Learning*. PMLR, 2021, pp. 5530\u20135540.\\n\\n[32] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in *International Conference on Machine Learning*. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[33] X. Li, S. Dalmia, J. Li, M. Lee, P. Littell, J. Yao, A. Anastasopoulos, D. R. Mortensen, G. Neubig, A. W. Black et al., \u201cUniversal phone recognition with a multilingual allophone system,\u201d in *ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2020, pp. 8249\u20138253.\"}"}
