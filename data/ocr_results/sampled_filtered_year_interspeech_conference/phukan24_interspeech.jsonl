{"id": "phukan24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] P. Yang, X. Wang, X. Duan, H. Chen, R. Hou, C. Jin, and W. Zhu, \u201cAvqa: A dataset for audio-visual question answering on videos,\u201d in Proceedings of the 30th ACM International Conference on Multimedia, 2022, pp. 3480\u20133491.\\n\\n[2] G. Li, Y. Wei, Y. Tian, C. Xu, J.-R. Wen, and D. Hu, \u201cLearning to answer questions in dynamic audio-visual scenarios,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 19 108\u201319 118.\\n\\n[3] H. Yun, Y. Yu, W. Yang, K. Lee, and G. Kim, \u201cPano-avqa: Grounded audio-visual question answering on 360deg videos,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2031\u20132041.\\n\\n[4] G. Li, W. Hou, and D. Hu, \u201cProgressive spatio-temporal perception for audio-visual question answering,\u201d in Proceedings of the 31st ACM International Conference on Multimedia, 2023, pp. 7808\u20137816.\\n\\n[5] S. Changpinyo, L. Xue, M. Yarom, A. Thapliyal, I. Szpektor, J. Amelot, X. Chen, and R. Soricut, \u201cMaXM: Towards multilingual visual question answering,\u201d in Findings of the Association for Computational Linguistics: EMNLP 2023, H. Bouamor, J. Pino, and K. Bali, Eds. Singapore: Association for Computational Linguistics, Dec. 2023, pp. 2667\u20132682. [Online]. Available: https://aclanthology.org/2023.findings-emnlp.176\\n\\n[6] S. R. Behera, P. B. Reddy, A. M. Tripathi, M. B. Rathod, and T. Karavadi, \u201cTowards Multi-Lingual Audio Question Answering,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 356\u2013360.\\n\\n[7] Z. Tong, Y. Song, J. Wang, and L. Wang, \u201cVideomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training,\u201d Advances in neural information processing systems, vol. 35, pp. 10 078\u201310 093, 2022.\\n\\n[8] Y. Gong, Y.-A. Chung, and J. Glass, \u201cAST: Audio Spectrogram Transformer,\u201d in Proc. Interspeech 2021, 2021, pp. 571\u2013575.\\n\\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d arXiv preprint arXiv:1810.04805, 2018.\\n\\n[10] S. Antol, A. Agrawal, J. Lu, M. Mitchell, D. Batra, C. L. Zitnick, and D. Parikh, \u201cVqa: Visual question answering,\u201d in Proceedings of the IEEE international conference on computer vision, 2015, pp. 2425\u20132433.\\n\\n[11] Y. Lin, Y. Xie, D. Chen, Y. Xu, C. Zhu, and L. Yuan, \u201cRevive: Regional visual representation matters in knowledge-based visual question answering,\u201d Advances in Neural Information Processing Systems, vol. 35, pp. 10 560\u201310 571, 2022.\\n\\n[12] S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, \u201cClotho-avqa: A crowdsourced dataset for audio question answering,\u201d in 2022 30th European Signal Processing Conference (EUSIPCO). IEEE, 2022, pp. 1140\u20131144.\\n\\n[13] H. M. Fayek and J. Johnson, \u201cTemporal reasoning via audio question answering,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2283\u20132294, 2020.\\n\\n[14] Z. Li, D. Guo, J. Zhou, J. Zhang, and M. Wang, \u201cObject-aware adaptive-positivity learning for audio-visual question answering,\u201d arXiv preprint arXiv:2312.12816, 2023.\\n\\n[15] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for automatic evaluation of machine translation,\u201d in Proceedings of the 40th annual meeting of the Association for Computational Linguistics, 2002, pp. 311\u2013318.\\n\\n[16] C.-Y. Lin, \u201cRouge: A package for automatic evaluation of summaries,\u201d in Text summarization branches out, 2004, pp. 74\u201381.\\n\\n[17] S. Banerjee and A. Lavie, \u201cMeteor: an automatic metric for mt evaluation with high levels of correlation with human judgments,\u201d Proceedings of ACL-WMT, pp. 65\u201372.\\n\\n[18] K. He, X. Chen, S. Xie, Y. Li, P. Doll\u00e1r, and R. Girshick, \u201cMasked autoencoders are scalable vision learners,\u201d in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 16 000\u201316 009.\\n\\n[19] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d arXiv preprint arXiv:2010.11929, 2020.\\n\\n[20] T. Khandelwal and R. K. Das, \u201cA Multi-Task Learning Framework for Sound Event Detection using High-level Acoustic Characteristics of Sounds,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 1214\u20131218.\\n\\n[21] M. Tomar, A. Tiwari, T. Saha, and S. Saha, \u201cYour tone speaks louder than your face! modality order infused multi-modal sarcasm detection,\u201d in Proceedings of the 31st ACM International Conference on Multimedia, 2023, pp. 3926\u20133933.\\n\\n[22] S. Zaiem, Y. Kemiche, T. Parcollet, S. Essid, and M. Ravanelli, \u201cSpeech Self-Supervised Representation Benchmarking: Are We Doing it Right?\u201d in Proc. INTERSPEECH 2023, 2023, pp. 2873\u20132877.\\n\\n[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.\"}"}
{"id": "phukan24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Multilingual Audio-Visual Question Answering\\n\\nOrchid Chetia Phukan*1, Priyabrata Mallick*2, Swarup Ranjan Behera*2, Aalekhya Satya Narayani2, Arun Balaji Buduru1, Rajesh Sharma1,3\\n\\n1IIIT-Delhi, India, 2Reliance Jio AICoE, Hyderabad, India 3University of Tartu, Estonia\\n\\n*equal contribution\\norchidp@iiitd.ac.in, priyabrata.mallick@ril.com, swarup.behera@ril.com\\n\\nAbstract\\n\\nIn this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely, MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.\\n\\nIndex Terms: audio-visual question answering, multilingual audio-visual question answering, cross-modal task\\n\\n1. Introduction\\n\\nHumans naturally integrate visual and auditory stimuli, allowing for a holistic understanding of their environment. Audio-Visual Question Answering (AVQA) aims to emulate innate multimodal cognitive system of humans in machines. In a formal context, when provided with a video stream, AVQA endeavors to address natural language queries by fusing details from both auditory and visual modalities. Picture a video showing a calm beach scene, and an AVQA system is asked, \u201cWhat caused the sudden stir?\u201d To answer well, the system needs to blend what it sees - maybe waves crashing or people reacting - with what it hears, like the roar of an approaching storm or the cries of seagulls flying off. This shows how important it is for AVQA to understand how audio and visuals connect for accurate responses. This field of investigation has become increasingly prominent in recent years [1, 2, 3, 4]. However, developments in AVQA primarily concentrated on English, thus restricting its accessibility to a more advantaged segment of the global community.\\n\\nOne could contend that the prevalence of English in the field is largely attributed to the existence of AVQA benchmarks designed specifically in English [1, 2]. Creating these benchmarks takes a lot of resources, mainly because human annotators need to carefully collect and check the questions and answers for each videos. In this paper, we address the goal of extending AVQA to languages other than English by introducing two machine-translated multilingual AVQA datasets thus preventing extra human annotation effort. These datasets are generated by creating question-answer pairs in multiple languages based on two existing benchmark AVQA datasets: MUSIC-AVQA [2] and AVQA [1]. Our work follows a recent lineage of works that uses translation for data generation for multilingual Question-Answering (QA) systems [5, 6]. Furthermore, we propose, MERA (Multilingual Audio-Visual Question Answering Framework) and it makes use of state-of-the-art (SOTA) video (VideoMAE [7]), audio (AST [8]), and textual (multilingual BERT [9]) foundation models for effective multilingual AVQA. In summary, the main contributions are as follows.\\n\\n\u2022 We present two multilingual AVQA datasets - m-MUSIC-AVQA and m-AVQA - in eight diverse languages: English (en), French (fr), Hindi (hi), German (de), Spanish (es), Italian (it), Dutch (nl), and Portuguese (pt).\\n\\n\u2022 We propose MERA framework for multilingual AVQA and suite of models namely, MERA-C, MERA-L, and MERA-T. MERA-C, MERA-L, MERA-T uses CNN, LSTM, and transformer networks respectively.\\n\\n\u2022 Extensive experiments with the proposed models show the usefulness of the newly created multilingual AVQA datasets and the effectiveness of the models towards providing correct answers. MERA-C showed the best performance in most instances for different languages and question types.\\n\\n\u2022 Furthermore, we show that weighted-ensemble of the models output probabilities leads to improvements across all the languages and different question types compared to the individual models.\\n\\nThe dataset and code can be accessed at 1https://github.com/swarupbehera/mAVQA.\\n\\n2. Related Work\\n\\nHere, we present a brief overview of past research encompassing various QA types and their efforts towards extending it to multilingual settings. We only focus on the multimodal QA (Visual Question Answering (VQA), Audio Question Answering (AQA), and AVQA) types that requires grounding natural language queries based on other modalities.\\n\\n2.1. Visual Question Answering\\n\\nVQA generally involves training a system to comprehend the content of an image and respond to questions about it using natural language. Initially, Antol et al. [10] presented a dataset comprising 204721 images sourced from the MS COCO dataset and an additional abstract scene dataset containing 50000 scenes. This dataset includes 760,000 questions and approximately 10 million corresponding answers. Furthermore, researchers proposed REVIVE [11] for effective VQA which uses a region-based approach on the input image that performs better in comparison to whole image-based and sliding window approaches.\"}"}
{"id": "phukan24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparative overview of AVQA datasets. Sound type (ST) - O represents Object sounds. Visual Scene Type (VST), Number of videos (#Video). Number of question-answer pairs (#QA pairs).\\n\\n| Dataset          | ST | VST         | #Video | #QA pairs |\\n|------------------|----|-------------|--------|-----------|\\n| MUSIC-A VQA      | O  | Music      | 9.3k   | 45.9K     |\\n| A VQA            | O  | Real-life  | 57.0K  | 57.3K     |\\n| Pano-A VQA       | O  | Panoramic  | 5.4K   | 51.7K     |\\n\\n2.2. Audio Question Answering\\n\\nAQA is a task where a system interprets audio signals and natural language questions to produce a desired natural language output. AQA has seen substantial growth due to the availability of high-quality datasets, for example, ClothoAQA [12] which consisted of 1991 audio files with 'yes' or 'no' and single-word question-answer types. They proposed LSTM-based models for benchmarking clothoAQA. Fayek et al. [13] introduced the DAQA dataset for AQA consisting natural sound events. They then presented a novel framework called Multiple Auxiliary Controllers for Linear Modulation (MALiMo), which builds upon the Feature-wise Linear Modulation (FiLM) model. However, these studies were for English AQA and Behera et al. [6] led the first step towards multilingual AQA by proposing AQA dataset for eight languages and a BiLSTM-based framework.\\n\\n2.3. Audio-Visual Question Answering\\n\\nYun et al. [3] proposed an innovative method for answering spatial and audio-visual questions in 360\u00b0 videos, aiming for a comprehensive semantic understanding of omnidirectional surroundings. Yang et al. [1] introduced A VQA, a real-life audio-visual dataset, and proposed a Hierarchical Audio-Visual Fusion (HA VF) model for the same. Furthermore, Li et al. [2] proposed MUSIC-A VQA dataset and developed a novel spatiotemporal grounding model to tackle complex comprehension and reasoning tasks involving both audio and visual modalities. Researchers also propose an object-aware approach that explicitly forms relation between the objects, sounds, and questions for improved A VQA on MUSIC-A VQA [14]. However, all these studies work only in English and A VQA in multilingual settings hasn't been explored yet. In this study, we work towards this direction.\\n\\n3. Multilingual A VQA Datasets\\n\\nHere, in this section, we discuss the datasets curated in our study for multilingual A VQA. Acquiring high-quality labeled data remains a key challenge in multilingual A VQA, as in many other machine learning tasks. To address this, we introduce m-MUSIC-A VQA and m-A VQA datasets designed for multilingual A VQA. It is noteworthy to mention the existence of three datasets for A VQA: MUSIC-A VQA [2], A VQA [1], and Pano-A VQA [3]. We present the details of the datasets, including sound type, visual scene type, number of videos, and number of question-answer pairs in Table 1. It's crucial to emphasize that Pano-A VQA isn't publicly accessible. Therefore, we focus on the other two datasets for this study. For comprehensive statistical illustrations of the MUSIC-A VQA and A VQA datasets, including the distribution of video categories and the distributions of question-answer pairs, please refer to the respective papers.\\n\\nTable 2: Analysis of question types and top five frequent answer candidates across MUSIC-AVQA and AVQA Datasets. Question types (QT). Total number of questions (#Q). Number of questions for top 5 answers (#Q5). Top 5 classes of answers.\\n\\n| QT      | #Q   | #Q5 | Top 5 Classes       |\\n|---------|------|-----|---------------------|\\n| Existential      | 4990 | 4990 | no, yes             |\\n| Location       | 4615 | 2539 | yes, no, left, right, middle |\\n| Counting       | 6351 | 6225 | zero, one, two, three, four |\\n| Comparative     | 5545 | 5545 | no, yes             |\\n| Temporal       | 4131 | 2080 | simultaneously, right, left, middle, violin |\\n\\n| QT      | #Q   | #Q5 | Top 5 Classes       |\\n|---------|------|-----|---------------------|\\n| Which   | 22005 | 6707 | dog, bird, cat, chicken, cattle |\\n| Come from | 16282 | 4217 | train, aircraft, sound of wind, motorcycle, helicopter |\\n| Happening | 11346 | 1310 | rope skipping, skiing, rowing, riding, machine gun fire |\\n| Where   | 6805 | 1539 | at sea, field, eabed, highway, aquatic |\\n| Why     | 256  | 198  | i'm hungry, decompression, roller coaster ride, frightened, motorcycle |\\n| Before next | 150  | 4    | volcanic explosion, setting off fireworks, tornado, sharpen the knife, set off firecrackers |\\n| When    | 64   | 21   | evening, chicken, train, lion, sound of wind |\\n| Used for | 57   | 12   | decompression, train, dog, turkey, protect your eyes |\\n\\nIn Table 2, we provide an analysis of the question types along with the top five frequent candidates for each type across both datasets.\\n\\nTo create multilingual datasets from MUSIC-A VQA and A VQA, we translated questions and answers from these datasets into seven additional languages using Google's machine translation API.\\n\\nTo create multilingual datasets from MUSIC-A VQA and A VQA, we translated questions and answers from these datasets into seven additional languages using Google's machine translation API. We also experimented with several other open-source machine translation tools, but their translation accuracy fell short of Google's. Evaluation with standard metrics such as BLEU [15], ROUGE [16], and METEOR [17] confirmed the reliability of our translations. The selected languages include French (fr), Hindi (hi), German (de), Spanish (es), Italian (it), Dutch (nl), and Portuguese (pt). Furthermore, human verification and refinement were conducted on the translated question-answer pairs to ensure accuracy. We followed the multilingual AQA work by Behera et al. [6] for the selection of the languages for our study and in addition, they also reported that translation through Google's machine translation gave the best results. Examples of m-MUSIC-A VQA entries are provided in Figure 2.\\n\\n2. https://cloud.google.com/translate\"}"}
{"id": "phukan24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"4. Methodology\\n\\nHere, we discuss various components of the proposed framework, MERA, shown in Figure 1. First, we discuss the foundation models for extracting meaningful representations from different modalities followed by the suite of models proposed. We leverage the foundation models based on the SOTA performance in respective modalities for various tasks. We use VideoMAE, Audio Spectrogram Transformer (AST), multilingual BERT (mBERT) for extracting video, audio, and text representations respectively.\\n\\n4.1. Foundation Models\\n\\nVideoMAE [7]: It is a revolutionary representation learning model for video trained in a self-supervised manner inspired by Image MAE [18] that involves masked video pre-training. It is built upon simple ViT [19] backbones. VideoMAE showed superior performance in comparison to contrastive self-supervised pre-trained models in various downstream video tasks.\\n\\nAST [8]: It is the first convolution-free fully attention-based model for audio classification. It uses pre-trained ViT [19] as the backbone architecture and further fine-tuned on AudioSet. AST showed SOTA performance for various audio classification tasks such as on ESC-50, speech commands, and so on.\\n\\nmBERT [9]: It was pre-trained on 102 languages in a self-supervised fashion. It makes use of two different pretext objective for pre-training: masked language modeling (MLM) and next-sentence prediction (NSP). The model through these objectives learns the inner representation of the languages in training set and later on can be used for various downstream tasks in multiple languages.\\n\\nWe use VideoMAE 3, AST 4, and mBERT 5, openly available in Hugginface. We resample the audios to 16 Khz before passing to AST. Like previous works such as [6] focusing on building multilingual QA systems that uses language identifier, MERA uses mBERT for extraction of text representations which doesn't require a language identifier for each incoming question and eliminates the necessity of relying on the accuracy of the language identification model and the need of different models for extracting representations for different languages thus preventing computational overhead.\\n\\n4.2. Suite of Models\\n\\nWe select the modeling networks as they are commonly used across various related tasks [14, 20, 21, 22]. For MERA-L, extracted representations from the foundation models are passed to the LSTM layers with hidden size of 60. We use an individual...\"}"}
{"id": "phukan24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation Scores of Models on different languages; L, C, T, ENS stands for MERA-L, MERA-C, MERA-T, and Ensemble respectively; A stands average; Values in blue represents the highest score in a question type for a particular language and blue in A represents the highest score after averaging across all the languages for a question type; The scores are accuracy scores as used by [2] and are in %\\n\\n| Language | Existential | Comparative | Counting | Location | Temporal |\\n|----------|-------------|-------------|----------|----------|----------|\\n| en       | 79.09       | 81.50       | 80.60    | 80.70    | 54.08    |\\n|          |             |             |          |          | 60.00    |\\n|          |             |             |          |          | 61.43    |\\n|          |             |             |          |          | 60.35    |\\n|          |             |             |          |          | 55.72    |\\n|          |             |             |          |          | 59.18    |\\n|          |             |             |          |          | 51.64    |\\n|          |             |             |          |          | 57.29    |\\n| fr       | 80.80       | 80.60       | 80.20    | 81.50    | 59.46    |\\n|          |             |             |          |          | 60.26    |\\n|          |             |             |          |          | 59.19    |\\n|          |             |             |          |          | 61.25    |\\n|          |             |             |          |          | 58.47    |\\n|          |             |             |          |          | 57.84    |\\n| hi       | 81.20       | 80.10       | 80.80    | 81.40    | 57.93    |\\n|          |             |             |          |          | 62.06    |\\n|          |             |             |          |          | 60.89    |\\n|          |             |             |          |          | 63.31    |\\n|          |             |             |          |          | 59.18    |\\n| de       | 79.89       | 77.38       | 80.60    | 80.10    | 58.20    |\\n|          |             |             |          |          | 62.24    |\\n|          |             |             |          |          | 59.91    |\\n|          |             |             |          |          | 63.58    |\\n|          |             |             |          |          | 59.65    |\\n|          |             |             |          |          | 60.28    |\\n| es       | 78.79       | 81.10       | 80.60    | 81.30    | 58.29    |\\n|          |             |             |          |          | 60.98    |\\n|          |             |             |          |          | 59.01    |\\n|          |             |             |          |          | 61.34    |\\n|          |             |             |          |          | 58.79    |\\n|          |             |             |          |          | 61.38    |\\n|          |             |             |          |          | 55.72    |\\n|          |             |             |          |          | 61.30    |\\n| it       | 80.40       | 77.28       | 79.89    | 81.00    | 60.00    |\\n|          |             |             |          |          | 63.22    |\\n|          |             |             |          |          | 59.46    |\\n|          |             |             |          |          | 63.31    |\\n|          |             |             |          |          | 60.12    |\\n|          |             |             |          |          | 62.79    |\\n|          |             |             |          |          | 55.96    |\\n|          |             |             |          |          | 60.36    |\\n|          |             |             |          |          | 36.56    |\\n|          |             |             |          |          | 49.73    |\\n|          |             |             |          |          | 31.71    |\\n|          |             |             |          |          | 49.62    |\\n|          |             |             |          |          | 41.14    |\\n|          |             |             |          |          | 54.36    |\\n| nl       | 80.80       | 80.60       | 78.69    | 81.10    | 55.96    |\\n|          |             |             |          |          | 60.26    |\\n|          |             |             |          |          | 53.27    |\\n|          |             |             |          |          | 64.12    |\\n|          |             |             |          |          | 57.45    |\\n|          |             |             |          |          | 57.84    |\\n|          |             |             |          |          | 56.20    |\\n|          |             |             |          |          | 59.26    |\\n|          |             |             |          |          | 38.18    |\\n|          |             |             |          |          | 49.40    |\\n|          |             |             |          |          | 33.76    |\\n|          |             |             |          |          | 49.83    |\\n|          |             |             |          |          | 39.19    |\\n|          |             |             |          |          | 52.66    |\\n|          |             |             |          |          | 32.88    |\\n| pt       | 78.49       | 80.60       | 80.60    | 80.60    | 60.00    |\\n|          |             |             |          |          | 59.82    |\\n|          |             |             |          |          | 59.55    |\\n|          |             |             |          |          | 60.17    |\\n|          |             |             |          |          | 56.82    |\\n|          |             |             |          |          | 60.12    |\\n|          |             |             |          |          | 55.80    |\\n|          |             |             |          |          | 58.47    |\\n|          |             |             |          |          | 38.40    |\\n|          |             |             |          |          | 48.22    |\\n|          |             |             |          |          | 32.68    |\\n|          |             |             |          |          | 42.23    |\\n|          |             |             |          |          | 51.94    |\\n|          |             |             |          |          | 32.76    |\\n|          |             |             |          |          | 51.21    |\\n|          |             |             |          |          | 41.42    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n|          |             |             |          |          | 32.21    |\\n|          |             |             |          |          | 53.72    |\\n|          |             |             |          |          | 32.40    |\\n|          |             |             |          |          | 52.22    |\\n"}
