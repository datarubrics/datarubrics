{"id": "leung24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A qualitative study exploring the effect of com-\\nmembration with partially intelligible speech,\"\\nM. S. Hawley, \"A qualitative study exploring the...\"}"}
{"id": "leung24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training Data Augmentation for Dysarthric Automatic Speech Recognition\\nby Text-to-Dysarthric-Speech Synthesis\\nWing-Zin Leung, Mattias Cross, Anton Ragni, Stefan Goetze\\n1 Speech and Hearing (SPandH), Dept. of Computer Science, The University of Sheffield, UK\\n{WLeung5, MCross2, a.ragni, s.goetze}@sheffield.ac.uk\\n\\nAbstract\\nAutomatic speech recognition (ASR) research has achieved impressive performance in recent years and has significant potential for enabling access for people with dysarthria (PwD) in augmentative and alternative communication (AAC) and home environment systems. However, progress in dysarthric ASR (DASR) has been limited by high variability in dysarthric speech and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using text-to-dysarthic-speech (TTDS) synthesis for finetuning large ASR models is effective for DASR. Specifically, diffusion-based text-to-speech (TTS) models can produce speech samples similar to dysarthric speech that can be used as additional training data for fine-tuning ASR foundation models, in this case Whisper. Results show improved synthesis metrics and ASR performance for the proposed multi-speaker diffusion-based TTDS data augmentation for ASR fine-tuning compared to current DASR baselines.\\n\\nIndex Terms: Dysarthric speech recognition, diffusion, text-to-speech synthesis, data augmentation\\n\\n1. Introduction\\nDysarthria is a type of motor speech disorder (MSD) that reflects abnormalities in motor movements required for speech production [1]. The psychosocial impact [2] and restrictions on functioning and participation [3] for PwD are well documented [4, 5]. DASR has important implications for AAC devices and home environmental control systems [6, 7]. Although the accuracy of ASR systems for typical speech has improved significantly [8, 9], there are challenges inherent with DASR. Due to high inter- and intra-speaker variability in dysarthric speech and limited public availability of dysarthric data, generic ASR models usually do not generalise well to dysarthric speakers [10, 11]. Baseline ASR models [12], typically trained on larger amounts of typical speech data, can be adapted to domains with limited data availability, such as dysarthric speech recordings [13]. Model adaptation approaches show improved performance [14, 15] and deep learning in combination with data augmentation techniques to address data sparsity [16] have achieved state-of-the-art (SotA) performance for DASR.\\n\\nTransformer-based models have not been adequately explored for dysarthric speech as such architectures require a significant amount of training data that is not publicly available. [16] implements a spatial convolutional neural network (CNN) with multi-head Transformers (pre-trained on control speaker data) to recognise visual representations of whole-word dysarthric speech, and [17] use dysarthric and typical speech corpora with data augmentation techniques to implement two-step parameter adjustments to train a dysarthric Transformer model. A recurrent neural network (RNN)-Transducer model has been trained on the Euphonia dataset [18], which contains over 1400 hours of audio data, however, the dataset is not publicly available. Representations from foundation ASR models have been used as input features for DASR systems (e.g. Wav2Vec2.0 [19] and WavLM [20]), but foundation models have not yet been adapted for DASR.\\n\\nData augmentation techniques have been widely studied for typical speech tasks [21], but data augmentation for DASR requires further research [22]. Spectro-temporal differences between typical and dysarthric speech (e.g. speaking rate) have influenced approaches, such as vocal tract length perturbation (VTLP) [23], tempo-stretching [24] and speed perturbation [25]. Although slower speaking rates and modifications to the spectral envelope can be modelled, perceptual dysarthric speech characteristics (e.g. articulatory imprecision or voice quality [26]) are not captured. Subsequently, generative adversarial networks (GANs) have been applied to speed-perturbed typical speech for speech synthesis [22] and voice conversion (VC) [27]. Also, Transformer-based systems have been implemented for TTDS synthesis [28, 29]. Recently, diffusion probabilistic modelling (DPM) has been applied to a VC task for dysarthric data augmentation [30]. The results demonstrate improved word error rate (WER) performance of DASR systems using augmented data, and subjective evaluations by human expert listeners show that severity characteristics of dysarthric speech are captured in the synthesis. This paper proposes (i) to create DASR training data by DPM, training Grad-TTS [31] from scratch on dysarthric data to (ii) analyse the use of additional augmented data only to finetune large ASR models (here Whisper), i.e. without matched control speaker data. The remainder of the paper is structured as follows: Section 2 describes the TTDS system and the ASR model adaptation. Experiments are described in Section 3 and their results are presented in Section 4. Section 5 concludes the paper.\\n\\n2. Methodology\\n2.1. Dysarthric Speech Synthesis\\nTo synthesise dysarthric mel-spectrogram data $X$ for DASR augmentation, we train the Grad-TTS [31] model.\\n\\n$$X = \\\\text{Grad-TTS}(y, s)$$\\n\\nGrad-TTS code adapted from https://github.com/huawei-noah/Speech-Backbones. Implementation available at https://github.com/WingZLeung/TTDS.\"}"}
{"id": "leung24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"T\\n\\nbeing the identity matrix, and\\n\\nis defined as the reverse of a forward diffusion process from\\n\\n\\\\(X\\\\) Glow-TTS \\\\[32\\\\]. The sampling process from\\n\\nhas the same transformer architecture and training objective as\\n\\n\\\\(X\\\\) an estimate of a training-distribution sample matrix\\n\\n\\\\(\u03b8\\\\) parameters\\n\\n\\\\(X\\\\) to\\n\\n\\\\(X\\\\) of the noisy mel-spectrogram matrix\\n\\na diffusion process (cf. (4)) to denoise the initial distribution\\n\\non the Grad-TTS text-encoder output\\n\\nof frames. This starts by forming initial distributions centred\\n\\n\\\\(F\\\\) with\\n\\n\\\\(\u03b8\\\\) parameters\\n\\n\\\\(X\\\\) starts from structured data\\n\\nexplores reduced\\n\\n\\\\(\u03b2\\\\) starts at\\n\\n\\\\(\u03b2\\\\) are\\n\\n\\\\(X\\\\) to\\n\\n\\\\(X\\\\) denoting\\n\\n\\\\(\u03b2\\\\) are\\n\\n\\\\(X\\\\) are\\n\\n\\\\(\u03b2\\\\) starts from\\n\\n\\\\(\u03b2\\\\) are\\n\\n\\\\(X\\\\) is necessary.\\n\\n\\\\(X\\\\) to\\n\\n\\\\(X\\\\) of dysarthric data for a speaker identity\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identitys\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment the limited avail-\\nable dysarthric speech data for a speaker identity\\n\\n\\\\(E\\\\).\\n\\n\\\\(X\\\\) from scratch on dysarthric data to augment"}
{"id": "leung24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Text-to-Speech Synthesis\\n\\nFor dysarthric speech synthesis, we train the Grad-TTS models (cf. Section 2.1) using TORGO dysarthric from scratch. Previous implementations of DPM TTDS pre-train the synthesis model on age-gender-matched typical speech and fine-tune on dysarthric speech data [30, 40]. The Grad-TTS models require training and validation data for a given speaker to train a speaker embedding $s_E$ in (1). TORGO does not have pre-defined data splits. Therefore, data splits were created for TTS training by pairing array and head microphones (of the same utterance) and then randomly splitting utterances into train, validation and test data splits in an 80%, 10%, 10% ratio per speaker. A systematic approach was considered, e.g. considering the distribution of single/multi-word utterances, and distribution of utterances across splits. However, analysis in [38] shows there are 951-969 unique utterances (across 16,158 recordings), and not all dysarthric speakers completed recordings of all utterances. Once the TORGO dysarthric TTDS models are trained, the transcripts for all splits are input to the trained models to synthesise additional training data for LOSO [38] ASR model adaptation. As Grad-TTS has not yet been adequately explored with dysarthric speech, we investigate $\\\\beta_T$ hyperparameter values in (3). Additionally, in the interest of using as little dysarthric data as possible, we investigate three conditions:\\n\\n(a) An all-speaker (ASp) model is trained using the data of all dysarthric speakers (i.e. the TORGO dysarthric data). The ASp model is trained using the training and validation splits of the entire dataset, and used to synthesise data for all speakers.\\n\\n(b) Single-speaker (SSp) models are trained using a single dysarthric speaker's data (i.e. a model is trained using one speaker's training and validation data, and this model is used to synthesise data for the same speaker).\\n\\n(c) Dysarthria-severity-group speaker (DSpG) models: the TORGO dysarthric speakers are partitioned into two groups by dysarthria severity rating on the FDA (cf. Table 1). Severe and mod-severe speakers (i.e. F01, M01, M02, M04, M05) form Group $G_1$, and mild and moderate speakers (i.e. F03, F04, M03) Group $G_2$. The DSpG $G_1$ model is trained on $G_1$ speakers' training and validation data, and used to synthesise data for these speakers.\\n\\nAlthough control speaker data is not used to train models for dysarthric speech synthesis, ASp and SSp TORGO control TTS models were trained in the same manner to compare evaluation metrics. Finally, Grad-TTS models are optimised on hyperparameters of learning rate, epochs and batch size.\\n\\n3.3. Dysarthric ASR\\n\\nThe Whisper ASR model is finetuned on a composition of real (TORGO) data and synthetic data (created by TTDS in Section 2.1). A cumulative ratio of additional synthetic training data for data augmentation (from 0-100% in 10% increments) is implemented. The TORGO dysarthric ASR models are trained using the LOSO methodology [38] to create speaker independent (SI) models: for a given target speaker, the data of the remaining speakers are used to train the model, which is tested on the given target speaker's data (and therefore the target speaker's data is not seen by the ASR model). Values for learning rate, warm-up, epochs, and batch size hyperparameters are optimised during training.\\n\\n3.4. Evaluation metrics\\n\\nTo evaluate the quality of the synthesised dysarthric speech, the mean cepstral distortion (MCD) is used as an objective metric, and subjective evaluation by a human expert listener was conducted. The performance of dysarthric ASR systems are measured by WER.\\n\\n3.4.1. Mean Cepstral Distortion (MCD)\\n\\nThe MCD is defined as the Euclidian distance between the synthesised and reference mel spectra, and is computed by alignment with dynamic time warping (DTW) [41]. The MCD has been shown to have correlation to subjective test results in speech synthesis analysis [42], although this has not been adequately investigated with dysarthric speech.\\n\\n3.4.2. Subjective Evaluation\\n\\nSubjective evaluation by expert listeners (SLTs) has been used to measure the presence and severity of dysarthric speech characteristics in synthesised speech [30]. For this study, an SLT with $>10$ years of experience assessing and diagnosing speech disorders perceptually evaluated the synthesised data. For every dysarthric speaker, 20 audio samples from the TORGO database and 20 synthetic audio samples were randomly selected. The selected audio samples were presented to the SLT individually in random order. Every audio sample was rated on overall dysarthria severity on a 5-point scale (between 0 for none and 4 for severe) [1]. The mean scores were calculated for TORGO and synthetic audio per speaker, to allow comparison of the presence and severity of dysarthric speech characteristics.\\n\\n3.4.3. Word Error Rate (WER)\\n\\nInference was performed with the pretrained baseline Whisper model, and adapted dysarthric Whisper models on a given target speaker (adapted models were trained on the remaining speakers in a LOSO methodology). Transcripts were processed with Whisper's English text normalizer, and WER calculated between the processed reference and hypothesis transcripts. Average (Avg.) WER is calculated as the average of single-speaker WER scores, and severity group averages calculated as the average scores of speakers in the group (to allow direct comparison to similar studies [38]). The overall (Ovl.) WER score, commonly used to assess ASR for typical speech was also calculated by computing the WER score for transcripts across all speakers.\\n\\n4. Results\\n\\n4.1. Text to Speech Synthesis\\n\\nMCD results for the TORGO control (C) and TORGO dysarthric (D) TTS models are shown in Table 2.\\n\\n| $\\\\beta_T$   | C ASp | C SSp | D ASp | D DSpG | D SSp |\\n|------------|-------|-------|-------|--------|-------|\\n| $\\\\beta_T$ = 10 | 6.61  | 6.62  | 6.71  | 6.81   |       |\\n| $\\\\beta_T$ = 20 | 6.75  | 7.92  | 6.98  | 7.49   | 7.80  |\\n\\nComparing MCD values for TTDS models with $\\\\beta_T = 10$ and $\\\\beta_T = 20$ in (3), the $\\\\beta_T = 10$ models show lower MCD for\"}"}
{"id": "leung24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"control (C) and dysarthric (D) groups for all conditions ASp, SSp & DSpG (as defined in Section 3.2). Results indicate a slight tendency that more data leads to better (lower) MCD, however, for this the MCD difference is minor.\\n\\nAn SLT conducted a subjective evaluation (cf. Section 3.4.2) of TORGO data and data synthesised from the best model (i.e. D ASp), and the averaged ratings for dysarthria severity, their difference as well as the MCD metrics are displayed in Table 3. The ratings show that dysarthric speech characteristics are present in the synthesised samples, but there are differences in the level of severity. The Kendall's Tau coefficient between average dysarthria severity scores and severity group MCD scores is $-0.67$, indicating a strong negative association between the two ranked variables.\\n\\nTable 3: Subjective evaluation of dysarthic data.\\n\\n| Severity   | Severe | Mod.-Sev. | Moderate | Mild     | Average | Overall |\\n|------------|--------|-----------|----------|----------|---------|---------|\\n| TORGO ref. | 3.28   | 2.45      | 1.70     | 0.275    | 2.16     | 0.54    |\\n| TORGO syn. | 2.63   | 2.55      | 1.35     | 1.05     | 2.05     | 0.88    |\\n| Difference | 0.65   | -0.10     | 0.35     | -0.78    | 0.11     | 0.66    |\\n| MCD        | 5.72   | 7.09      | 5.88     | 6.44     | 6.15     | 5.04    |\\n\\n4.2. ASR model adaptation\\n\\n4.2.1. Whisper baseline performance\\n\\nThe pretrained Whisper models (without any finetuning) are used for inference on the TORGO control (C) and TORGO dysarthric (D) data to establish baseline performance in the following. As a LOSO approach is used for TORGO ASR model adaptation, inference is performed on the whole dataset. Table 4 shows the performance in WER for the Whisper-medium (WM) and Whisper-large (WL) baseline models.\\n\\nTable 4: WER in % for the Whisper medium (WM) and large (WL) baseline models. C denotes control and D dysarthric.\\n\\n| Severity   | Sev. M.-Sev. | Mod. | Mild | Avg. | Ovl. |\\n|------------|--------------|------|------|------|------|\\n| WM         |              | 17.93| 13.69| 10   |      |\\n| WL         |              | 12.31| 11.92| 8.79 | 9.71 |\\n| D WM       | 123.37       | 168.83| 45  | 79   | 91.23|\\n| D WL       | 126.20       | 187.72| 32  | 38   | 93.21|\\n| Difference | 0.65         | -0.10| 0.35| -0.78|      |\\n| MCD        | 5.72         | 7.09 | 5.88| 6.44 | 6.15 |\\n\\nThe WM and WL models for the TORGO control (non-dysarthric) data achieve overall WERs of 13.69 and 11.92%, respectively, and results for dysarthric speech shows much higher WER. The WL model has a relatively lower overall WER score on dysarthric speakers but a higher WER in average over speakers due to relatively poorer performance for severe and moderate-severe dysarthric speakers.\\n\\n4.2.2. Whisper model adaptation\\n\\nThe Whisper medium (WM) and large (WL) models are fine-tuned on a composition of real data (TORGO) and an increasing percentage of synthetic data synthesised by the best TTDS model, i.e. D ASp with $\\\\beta_T = 10$. Table 5 shows the results of the WM adaptation, since WM is the smaller model and showed better performance than WL for more severe dysarthric data. Adaptation using only real dysarthric data (i.e. no synthetic data) achieves an overall WER score of 56.14%, i.e. performances significantly better than the baseline in Table 4. Synthetic data further improves performance, with the best performance achieved using 100% additional synthesised training data, i.e. the training speakers in LOSO adaptation, reducing overall and average WER to 20.45% and 20.70%, respectively.\\n\\nPerformance gains can be observed in particular for severe and moderate-severe dysarthric speech. The WL model also performed best with 100% additional synthesised data (not explicitly shown here), and the trend is maintained when the Whisper models are additionally trained with SpecAugment. Table 6 compares the WER performance of the WM and WL with 100% additional synthetic data (with and without SpecAugment) to recent SotA benchmarks on the TORGO ASR task. The proposed Grad-TTS augmented Whisper model adaptation outperforms all baseline models on the same task. The WL with SpecAugment shows best performance overall.\\n\\nTable 6: TORGO WER performance in comparison to benchmarks. +denotes SpecAugment.\\n\\n| Severity   | Sev. M.-Sev. | Mod. | Mild | Avg. | Ovl. |\\n|------------|--------------|------|------|------|------|\\n| LF-MMI     | -            | -    | -    | -    | -    |\\n| FS2 & D-HMM| 55.88        | 49.60| 36.80| 12.60| 39.20|\\n| FMLLR-DNN  | 43.29        | 44.05| 35.93| 11.65| 34.55|\\n| SD-CTL     | 68.24        | 33.15| 22.84| 10.35| 30.76|\\n| GTTS & WM  | 28.49        | 17.87| 26.14| 3.82 | 20.70|\\n| GTTS & WM+ | 25.82        | 14.95| 5.26 | 4.28 | 19.16|\\n| GTTS & WL  | 46.39        | 18.29| 24.27| 3.03 | 25.84|\\n| GTTS & WL+ | 23.30        | 13.98| 3.27 | 2.57 | 16.93|\\n\\n5. Conclusion\\n\\nThis work showed that it is possible to train Grad-TTS from scratch without matched control data to synthesise samples with dysarthric speech characteristics. A $\\\\beta_T = 10$ schedule improves sample quality for the typical and dysarthric speech data used. The results show that Whisper can be finetuned for SotA DASR on TORGO, and that data augmentation is beneficial. The amount of synthesised data required is dependent on the severity of the dysarthric speaker.\\n\\n6. References\\n\\n[1] J. R. Duffy, Motor Speech Disorders. Elsevier, 2019.\\n[2] M. Walshe, Clinical cases in dysarthria. Milton Park, Abingdon, Oxon New York, NY: Routledge, 2022.\\n[3] A. D. Page and K. M. Yorkston, \\\"Communicative participation in dysarthria: Perspectives for management,\\\" Brain sciences, vol. 12, no. 4, p. 420, 2022.\"}"}
