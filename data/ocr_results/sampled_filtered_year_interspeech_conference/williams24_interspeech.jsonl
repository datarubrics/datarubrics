{"id": "williams24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"also by Good Systems, a research grand challenge at the University of Texas at Austin. The authors would like to thank Prof. Gopal Ramchurn, Prof. Joel Fischer, Prof. Sharon Strover, Dr. Liz Dowthwaite, and Dr. Anna-Maria Piskopani for their helpful feedback during this work.\\n\\n[1] C. Sprenger, F. Eippert, J. Finsterbusch, U. Bingel, M. Rose, and M. Granot and I. Weissman-Fogel, \u201cThe Effect of Post-Surgical Autopsy on Anesthetic Quality,\u201d Anesthesiology, vol. 124, no. 4, pp. 813\u2013820, 2016.\\n\\n[2] M. Granot and I. Weissman-Fogel, \u201cThe Effect of Post-Surgical Autopsy on Anesthetic Quality,\u201d Anesthesiology, vol. 124, no. 4, pp. 813\u2013820, 2016.\\n\\n[3] J. Nesbitt, S. Moxham, L. Williams et al., \u201cImproving Pain Assessment and Management in People with Cognitive Impairment,\u201d Pain, vol. 156, no. 12, pp. 2321\u20132328, 2015.\\n\\n[4] A. K. Cook, C. A. Niven, and M. G. Downs, \u201cAssessing the Pain Sensory and Emotional Response to a Noxious Stimulus,\u201d Journal of Evaluation in Clinical Practice, vol. 26, no. 3, pp. 1048\u20131053, 2020.\\n\\n[5] J. Liu, L. L. Chen, S. Shen, J. Mao, M. Lopes, S. Liu, and M. Carroll, \u201cAutomatic Detection of Pain Using Machine Learning,\u201d Science, vol. 361, no. 6399, pp. 953\u2013957, 2018.\\n\\n[6] T. Sakulchit, B. Kuzeljevic, and R. D. Goldman, \u201cEvaluation of Digital Biomarkers in Healthcare,\u201d JMIR Formative Evaluation, vol. 7, p. e45355, 2023.\\n\\n[7] S. C. Ahluwalia, K. F. Giannitrapani, S. K. Dobscha, R. Cromer, J. L \u00a8otsch and A. Ultsch, \u201cMachine Learning in Pain Research,\u201d Journal of Evaluation in Clinical Practice, vol. 26, no. 3, pp. 1048\u20131053, 2020.\\n\\n[8] R. S. Stojancic, A. Subramaniam, C. Vuong, K. Utkarsh, N. C. Olsen, and M. Carroll, \u201cAutomatic Detection of Pain Using Machine Learning,\u201d Journal of Evaluation in Clinical Practice, vol. 26, no. 3, pp. 1048\u20131053, 2020.\\n\\n[9] R. F. Rojas, C. Joseph, G. Bargshady, and K.-L. Ou, \u201cEmpirical Evaluation of the Pain Level from Speech,\u201d in Proc. Speech Prosody 2016, VDE, 2018, pp. 1\u20135.\\n\\n[10] D. Powell, \u201cWalk, talk, think, see and feel: harnessing the power of digital biomarkers to improve healthcare,\u201d Frontiers in Neuroinformatics, vol. 12, 2018.\\n\\n[11] Y. Oshrat, A. Bloch, A. Lerner, A. Cohen, M. Avigal, and M. S. Salekin, G. Zamzmi, J. Hausmann, D. Goldgof, R. Kasturi, and R. Fergus, S. Vishwanathan, and R. Garnett, Eds., \u201cAssessing Autonomic Function from Electrodermal Activity and Heart Rate Variability During Cold-Pressor Test and Emotional Challenge,\u201d in Advances in Neural Information Processing Systems 30, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds., 2017, pp. 313\u2013318.\\n\\n[12] F.-S. Tsai, Y.-M. Weng, C.-J. Ng, and C.-C. Lee, \u201cEmbedding of Pain Categorization in Mammographic Images,\u201d European Journal of Pain, vol. 22, no. 11, pp. 1019\u20131022, 2012.\\n\\n[13] Z. Ren, N. Cummins, J. Han, S. Schnieder, J. Krajewski, and Y. Liu, K. Wang, L. Wei, J. Chen, Y. Zhan, D. Tao, \u201cPain prediction in people with neurological disorders using fNIRS,\u201d Data in Brief, vol. 35, p. 106796, 2023.\\n\\n[14] C. Fr \u00a8ojd, C. Lampic, G. Larsson, G. Birgeg \u00a8ard, and L. v. Ess \u00b4en, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to Pain,\u201d C. B\u00a8uchel, \u201cAttention Modulates Spinal Cord Responses to"}
{"id": "williams24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Predicting Acute Pain Levels Implicitly from Vocal Features\\n\\nJennifer Williams1, Eike Schneiders2, Henry Card1, Tina Seabrooke1, Beatrice Pakenham-Walsh1, Tayyaba Azim1, Lucy Valls-Reed1, Ganesh Vigneswaran1, John Robert Bautista3, Rohan Chandra4, Arya Farahi4\\n\\n1University of Southampton, UK; 2University of Nottingham, UK; 3University of Missouri-Columbia, USA; 4University of Texas at Austin, USA\\n\\nj.williams@soton.ac.uk\\n\\nAbstract\\n\\nEvaluating pain in speech represents a critical challenge in high-stakes clinical scenarios, from analgesia delivery to emergency triage. Clinicians have predominantly relied on direct verbal communication of pain which is difficult for patients with communication barriers, such as those affected by stroke, autism, and learning difficulties. Many previous efforts have focused on multimodal data which does not suit all clinical applications. Our work is the first to collect a new English speech dataset wherein we have induced acute pain in adults using a cold pressor task protocol and recorded subjects reading sentences out loud. We report pain discrimination performance as F1 scores from binary (pain vs. no pain) and three-class (mild, moderate, severe) prediction tasks, and support our results with explainable feature analysis. Our work is a step towards providing medical decision support for pain evaluation from speech to improve care across diverse and remote healthcare settings.\\n\\nIndex Terms: speech paralinguistics, health, medical conversations, pain assessment, speaker states\\n\\n1. Introduction\\n\\nThe perception and management of pain, which encompasses neurological, psychological, and cognitive dimensions, poses a significant challenge in the medical field. Pain perception transcends mere sensory inputs, integrating emotional and cognitive aspects that markedly influence its intensity and character [1]. Moreover, the complexity of pain perception is exacerbated by neuroplastic changes following surgical interventions, highlighting the necessity to understand pain modulation mechanisms for efficacious management [2]. Although patients frequently possess the capability to express and explicitly communicate their pain verbally, this task becomes particularly challenging for individuals unable to do so, such as those affected by stroke [3], learning difficulties [4], autism [5], and young children [6]. For these patients, the identification of objective, non-invasive indicators of pain is crucial to ensure the provision of timely and appropriate care. Even when a patient is able to communicate with the clinician, their self-reported pain ratings reflect individual, subjective experiences of pain, making them difficult to quantify and assess reliably [7].\\n\\nHistorically, machine learning models have provided valuable insights into pain assessment across various contexts [8], utilising methodologies such as electromyography (EMG), electroencephalogram (EEG), skin conductance, and electrocardiography (ECG) to predict pain levels and outcomes. Recent advancements have even leveraged wearable technologies (e.g. Apple Watch) for continuous monitoring [9], facilitating early interventions in conditions like sickle cell disease. The development of personalized pain assessment offers a novel and relatively untapped opportunity. With the advent of wearable technologies, speech, replete with digital biomarkers and distinct 'Digital Biomarker Fingerprints,' can provide unique insights into individual health states [10]. This approach to healthcare, which leverages the ubiquitous nature of audio data, holds the promise of identifying personalized health characteristics through these digital fingerprints. The feasibility of this method has been demonstrated, albeit preliminarily, in recent work [11, 12, 13], which used audio analysis for assessing different types of pain in various languages.\\n\\nA critical step toward trustworthy pain decision support tools requires understanding which kinds of acoustic features to exploit in order to improve the accuracy of pain assessment. This knowledge has wide application in the medical domain [14], from analgesia administration to medication titration. Healthcare professionals, through established rapport and ongoing patient interactions, are adept at discerning pain through verbal and nonverbal cues. However, recent shifts towards telephonic and video consultations have diminished the efficacy of these cues, underscoring the need for a reliable, distance-compatible decision-support tool. Our exploration of pain assessment through speech processing contributes to this enduring problem, promising to bridge the gap in patient care, particularly in distance-led (e.g. telephone/video conference) health settings.\\n\\nOne of the main research gaps that this paper addresses is that there are no existing English speech datasets available for acute pain assessment in adults that are labeled with clinically-relevant pain scales [15]. In this paper, we make the following contributions:\\n\\n\u2022 Present a new English speech-only pain dataset from acute pain inducement and identify functional acoustic features that correlate with reported pain.\\n\u2022 Utilize these features with discriminative machine learning to predict pain levels in two tasks: presence/absence of pain and mild/moderate/severe pain.\\n\u2022 Show how different functional features contribute to pain level assessment.\\n\\n2. Related Work\\n\\nPrevious work on pain assessment has spanned multiple disciplines and input modalities including functional near-infrared spectroscopy (fNIRS) [16], and electroencephalography (EEG) data [17]. It has also been viewed from the standpoint of 'affective computing' [18]. However, individuals express pain differently and features beyond clear affect must be considered, including speech rate, breathiness, mispronunciation, restarts, etc. Reliable detection of pain from key acoustic features remains unsolved. More recently, there was a renewed call for...\"}"}
{"id": "williams24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"research efforts to develop AI tools that support clinicians with pain assessment from bioacoustic markers in speech \\\\[19\\\\] and our work takes a step forward towards that goal.\\n\\nAutomated pain recognition has invited two streams of research: pain detection which treats the problem as a binary classification task, and pain intensity assessment as a multi-class classification or regression task. Another research dimension in this domain is the use of unimodal input \\\\[13, 20\\\\] versus multiple modalities (audio, video, physiological) \\\\[21\\\\]. Although speech and sound have proven very effective for analysing the expression of emotions at an early stage of human development (e.g., crying infants \\\\[22\\\\]), there remains much to be explored for adult pain assessment, especially for real-time applications. With increasing use of telehealth delivery and emergency calling, there is a growing need for speech-based research on pain assessment.\\n\\nThe first to report findings of bioacoustic markers of pain from the speech signal was \\\\[11\\\\]. They developed a proof-of-concept solution to detect the presence or absence of pain using speech from interviews of adult hospital patients suffering various acute and chronic conditions. While their sample size was quite small (400 speech instances), they derived two new types of features from low-level descriptors (LLDs) and reported promising results using support vector machines on a binary prediction task. Further work from \\\\[12\\\\] classified pain assessment for adults from speech using a combination of prosodic features, mel frequency cepstral coefficients (MFCCs) and deep neural network bottleneck features. Their best reported balanced accuracy was 74.2\\\\% on a binary task (mild/severe) and 54.2\\\\% on a three-class task (mild/moderate/severe).\\n\\nSimilar to our work in this paper, \\\\[13\\\\] collected a German speech pain dataset based on a cold pressor task to induce acute pain. However, it was an uncontrolled collection and limited only to German speech, making explainability difficult and generalisation across languages uncertain. They reported best performance on three-class pain prediction with 42.7\\\\% unweighted average recall, which is only slightly better than chance given the imbalance of pain levels in their dataset. They used acoustic features, MFCCs, and deep spectrum VGG16 features \\\\[23\\\\]. Our work addresses a similar problem, but using clinically-relevant pain levels with our newly collected English speech pain dataset where acute pain was induced by a cold pressor task. Our work includes binary pain detection as well as three-class assessment with promising results using functional acoustic features. Our work also contributes new understanding towards explainability about how different acoustic features influence pain classification.\\n\\n### 3. Data Collection\\n\\nWe followed a protocol common to the discipline of psychology to induce acute pain in adult human subjects using very cold water (0-4\u00b0C) in a Cold Pressor Task (CPT) \\\\[24, 25\\\\] while eliciting and recording speech. Pain level ground truth was assumed, as in clinical contexts, that subjects reported pain accurately according to their experience and individual perspectives.\\n\\n#### 3.1. Recruitment and Compensation\\n\\nParticipants were 15 undergraduate (12 female, 3 male, mean age: 18.73, sd: 0.96) Psychology students from the United Kingdom who participated in return for partial course credit. Participants were excluded from the study if they reported: high blood pressure; a heart or circulation problem; dysthymia; a cardiovascular disorder; history of Raynaud\u2019s syndrome, fainting, seizures, or frostbite; an open cut, sore, or bone fracture on or near to either hand; a neurological disorder; diabetes; epilepsy; or pregnancy. Participants were required to be aged between 18 and 35 years. Participants were provided with details of the study procedure, purpose, and their rights, including the right to withdraw from the study or to end each trial by removing their hand from the water.\\n\\n#### 3.2. Pain Inducement: The Cold Pressor Task\\n\\nThe CPT is a commonly used task for the induction of discomfort to mild pain \\\\[26, 27\\\\]. Participants were asked to immerse their hand into cold water (0-4\u00b0C) while performing a speaking task (Section 3.3). To provide a control condition and collect data for the no-pain condition, we included a warm water (34-37\u00b0C) condition. To minimize risk to the participants, and in accordance with the ethical guidelines, participants immersed their hand for a maximum duration of up to three minutes \u2013 or until they withdrew their hand from the water \\\\[26\\\\]. To prevent the buildup of microclimate within the water tanks, the water was circulated with a water pump (5.8 L/h).\\n\\nParticipants were randomly assigned to one of four groups, varying the order of which hands, left (L) or right (R), was placed into the cold (C) or warm (W) water. The four experimental groups were: (1) LC-LW-RC-RW, (2) LW-LC-RW-RC, (3) RC-RW-LC-LW, and (4) RW-RC-LW-LC. Prior to the first trial, the participant's hand temperatures were recorded using an infrared thermometer, providing a baseline to ensure that their hand temperatures could be brought back to their specific baseline following the final trial. Participants submerged their entire hand in the water, with the palm facing upwards. After every instance of cold water exposure, participants placed that same hand into a nearby warm water bath to bring their hand up to baseline temperature to minimize discomfort and to proceed with the next trial. Following the experiment, participants were debriefed and provided with a take-home debriefing sheet.\\n\\n#### 3.3. Speech Elicitation and Collection\\n\\nDuring the CPT protocol for both warm and cold water immersion (water tubs were side-by-side to minimize movement around the room), participants read aloud sentences from a randomized selection of Harvard Sentences \\\\[28\\\\] that were presented on 50\u201d screen approximately 2 meters away. Every sixth sentence required them to say a pain statement to register their pain level on a 1-10 scale, as follows: \u201cOn a scale from 1\u201310, the pain I feel right now is...\u201d The order of the sentences were randomized per subject and manually advanced by the research team, to allow speakers to read the sentences at their own pace. Speech was collected from two microphones: (1) R\u00f8de Wireless PRO close-talking mic with lapel placed approximately 10\u201d from each speaker\u2019s chin (primary), and (2) Blue Yeti desktop microphone placed approximately 20\u201d from the speaker. All audio was collected as one utterance per wave file, as 16-bit mono PCM 16 kHz. The data collection room was approximately 10x10\u2019. In total, from the 15 participants, we collected 2...\"}"}
{"id": "williams24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Due to the variability across genders and very small sample of male utterances, in our experiments (Section 5.3) we report results for female-only and mixed gender. We used data from the primary (R\u00f8de) lapel mic.\\n\\n3.4. Ethics\\nAutomatic pain discrimination has the potential to be misused outside of the medical domain (e.g., using pain levels for torture or abuse) while it can also provide decision support tools to legitimate health professionals. Our data collection and research activities aligned with a Responsible Research and Innovation (RRI) approach called the AREA (Anticipate, Reflect, Engage, Act) framework [29] to guide our ethics application and to determine whether this work should be undertaken. We followed the ethical guidelines of our institution precisely and we adhere to all applicable regulations regarding data sharing.\\n\\nFigure 1: Distribution of utterances labeled for pain in the binary and three-class discrimination tasks, indicating distribution of male and female utterances.\\n\\n4. Speech Data Pre-Processing\\nAll 1,947 utterances were manually reviewed by the authors for quality control. We removed 225 utterances wherein: a speaker's words were cut off, other speakers were present in the audio, noise interference was present including noise from outside of the recording room, from the CPT equipment, the speaker bumping the microphone against the water tubs or the subject shifting in an audible manner.\\n\\nThe pain statements (Section 3.3) were used to label utterances by manually extrapolating the reported pain level backwards for the previous five utterances, allowing us to label every utterance with the subject's self-reported pain level. If the pain statement was not available due to our quality control procedure, then we removed the previous sentences that remained unlabeled. For pain levels wherein subjects reported two pain levels (e.g., \u201cbetween 5 and 6\u201d or \u201c8.75\u201d) we rounded up to the nearest integer. Likewise, if a subject reported a pain of 0, this was re-labeled as 1 since we used a pain scale of 1-10.\\n\\nAll recordings were trimmed using voice activity detection (VAD) to remove any leading and trailing silence using the Python webrtcvad toolkit with the lowest aggressiveness setting and we removed 14 utterances where the duration was less than 1.5s. Our final dataset contained 1,690 utterances.\\n\\n5. Pain Discrimination Experiments\\nFrom Figure 1, we adjusted the reported pain levels in our dataset to align with two discriminative tasks: presence or absence of pain (\u201cbinary task\u201d) and mild, moderate, severe pain (\u201cthree-class task\u201d). For the binary pain labels, we considered 1-3 as no pain and 4-10 as pain. For the three-class problem, we treated 1-3 as mild, 4-6 as moderate, and 7-10 as severe [15].\\n\\n5.1. Acoustic Features\\nWe extracted 6,373 functional acoustic features using the Python openSMILE toolkit from the ComParE2016 feature set, which is known for being used in a variety of non-semantic and paralinguistic acoustic events, including previous work on pain assessment [13]. We applied standard normal scaling (mean 0, unit variance), followed by principal components analysis (PCA), and used a Scree Test to determine that the optimal number of components was 45 in both tasks.\\n\\n5.2. Classifiers and Hyper-parameters\\nTo establish the first baseline on this dataset, we explored several machine learning algorithms with hyper-parameter tuning. Using the Python Scikit-Learn toolkit, we selected support vector machines (SVM), which have previously shown to be useful for this task, Logistic Regression (Logit), and multi-layer perceptron (MLP). To select hyper-parameters, we split our dataset into 70/30 train/test and performed gridsearch on the training set with stratified 10-fold cross-validation (stratified by label not speaker). For SVM, we explored different kernels (linear, radial basis function, and polynomial), as well as values for the regularization parameter $C = \\\\{0.001, 0.1, \\\\ldots, 1\\\\}$, with $\\\\gamma = 1/n_{\\\\text{features}}$. For Logit, we translated the labels into integers (0 and 1 for binary, and 0, 1, 2 for three-class) and explored values for the inverse regularization strength $C = \\\\{0.001, 0.1, \\\\ldots, 1\\\\}$. For MLP, we explored different hidden layer sizes of 1-6 layers using 256 nodes per layer, and activation function set to ReLU [33]. We explored different learning rate initializations of $\\\\text{lr} = \\\\{0.1, 0.01, 0.001\\\\}$ with constant, adaptive and inverse scaling, as well as $\\\\alpha = \\\\{0.0001, 0.001, 0.01\\\\}$, and early stopping. All other parameters were set to default values. We found the best parameters by optimizing for F1 score with micro averaging. The best hyperparameters, feature names, and example mel frequency spectrograms are available.\\n\\n5.3. Results\\nTable 1 presents performance as F1 score with micro averaging highlighting best performance on 10-fold cross validation and held-out test. We identified the best hyper-parameters from cross-validation and applied that to our test set. The SVM with polynomial kernel performs similarly to MLP in both tasks, including female-only and mixed gender utterances, with SVM slightly better on cross-validation. As a baseline, we show a random classifier (representing random guessing). We further investigated how different speakers affect classifier performance using hold-one-out speakers in a separate train/test split, and\\n\\n7https://rhoposit.github.io/interspeech2024\"}"}
{"id": "williams24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"found that these models do not generalize well to unseen speakers, which should be explored in future work. This could be due to the size or imbalance of the dataset, and further motivates new directions toward personalized pain assessment.\\n\\n|                | Binary | Three-Class |\\n|----------------|--------|-------------|\\n|                | Female | Mixed       |\\n| SVM - CV       | 80.0 \u00b13.8 | 80.3 \u00b11.7 |\\n|                | 69.9 \u00b14.3 | 70.9 \u00b14.6 |\\n| SVM - Test     | 78.0   | 73.8        |\\n| Logit - CV     | 71.8 \u00b13.4 | 73.8 \u00b15.5 |\\n|                | 66.7 \u00b13.5 | 66.4 \u00b12.8 |\\n| Logit - Test   | 72.1   | 70.6        |\\n| MLP - CV       | 79.3 \u00b12.5 | 78.9 \u00b13.0 |\\n|                | 68.3 \u00b12.7 | 68.3 \u00b14.9 |\\n| MLP - Test     | 78.3   | 72.8        |\\n| Random         | 50.0   | 50.0        |\\n|                | 33.3   | 33.3        |\\n\\n### Feature Analysis\\n\\nExplainability is an important aspect of any automated technique that may influence or contribute to clinician decision-making. Of the 45 features identified through PCA, 22 were spectral descriptors, 16 were energy descriptors, and 7 were voicing descriptors. Here, we explore how the different features contribute to classifier decisions using SHAP values [34]. We used the best-performing SVM model for each task from Section 5.3 and fit a Kernel Explainer to a stratified background model of 260 utterances for each task. We report the top 9 features for female-only speakers in the binary (Figure 2) and three-class (Figure 3) assessments. From both figures, we identify key features that contribute to classifier decisions:\\n\\n1. PC1\\n   - audspecRasta\\n   - lengthL1norm\\n   - sma\\n   - flatness\\n2. PC2\\n   - pcm\\n   - fftMag\\n   - spectralRollOff75.0\\n   - sma\\n   - de\\n   - stddev\\n3. PC3\\n   - audSpec\\n   - Rfilt\\n   - sma[1]\\n   - lpc1\\n4. PC4\\n   - audSpec\\n   - Rfilt\\n   - sma[3]\\n   - lpgain\\n\\nFeature PC1 is a feature corresponding to the relative spectral transform (RASTA) [35] applied to the auditory spectrum, which uses a band-pass filter on the energy in each frequency subband to smooth over noise variations and can simulate human audition. PC2 is a spectral roll off point, which can be linked to voice/unvoiced speech and breathiness. PC3 and PC4 represent mel frequency spectrum Perceptual Linear Prediction (PLP) cepstral coefficients and are known to be used in speech emotion recognition tasks [36]. Explanations of which features are most useful for pain assessment can contribute to developing more trustworthy tools for clinicians to adopt in practice, as we have demonstrated in this crucial first step.\\n\\n### Discussion and Future Work\\n\\nWe have presented a framework from which we collected and curated a new English speech dataset based on induced acute pain in adults. We identified key acoustic features that can discriminate well for pain detection and pain assessment. We showed that our SVM and MLP classifiers performed well on both tasks. Although our results are not directly comparable to previous work due to differences in underlying data, we achieved overall better performance than what has been previously reported for unimodal pain assessment from speech. The limitations of our dataset (number of speakers and gender imbalance) necessitated reporting separate results for female speakers and our classifiers did not generalize well to unseen speakers. To address this limitation, we have begun collecting a larger dataset of 50 participants with closer gender balance, which will be made publicly available for academic use. Our efforts will allow deeper exploration of automatic pain assessment, including more variety of speech features such as prosody and learning directly from spectrograms. We are interested in developing techniques that can more finely assess pain levels beyond a three-class granularity. The goal that we are working toward is to develop low-cost decision support tools for health professionals who stand to benefit from knowing that pain-related information can be found in the speech signal. Such tools have wide implications for the future of medicine, including personalized health monitoring, remote health conversations, and mitigation of bias when delivering analgesia.\"}"}
