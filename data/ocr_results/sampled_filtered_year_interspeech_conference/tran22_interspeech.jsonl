{"id": "tran22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] Y. Wang, R. Skerry-Ryan, D. Stanton, Y. Wu, R. J. Weiss, N. Jaitly, Z. Yang, Y. Xiao, Z. Chen, S. Bengio et al., \u201cTacotron: Towards end-to-end speech synthesis,\u201d arXiv preprint arXiv:1703.10135, 2017.\\n\\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., \u201cNatural tts synthesis by conditioning wavenet on mel spectrogram predictions,\u201d in 2018 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 4779\u20134783.\\n\\n[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech: Fast, robust and controllable text to speech,\u201d Advances in Neural Information Processing Systems, vol. 32, 2019.\\n\\n[4] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d arXiv preprint arXiv:2006.04558, 2020.\\n\\n[5] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-tts: A generative flow for text-to-speech via monotonic alignment search,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 8067\u20138077, 2020.\\n\\n[6] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWavenet: A generative model for raw audio,\u201d arXiv preprint arXiv:1609.03499, 2016.\\n\\n[7] R. Prenger, R. Valle, and B. Catanzaro, \u201cWaveglow: A flow-based generative network for speech synthesis,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 3617\u20133621.\\n\\n[8] J.-M. Valin and J. Skoglund, \u201cLpcnet: Improving neural speech synthesis through linear prediction,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5891\u20135895.\\n\\n[9] N. Kalchbrenner, E. Elsen, K. Simonyan, S. Noury, N. Casagrande, E. Lockhart, F. Stimberg, A. Oord, S. Dieleman, and K. Kavukcuoglu, \u201cEfficient neural audio synthesis,\u201d in International Conference on Machine Learning. PMLR, 2018, pp. 2410\u20132419.\\n\\n[10] R. Yamamoto, E. Song, and J.-M. Kim, \u201cParallel wavegan: A fast waveform generation model based on generative adversarial networks with multi-resolution spectrogram,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6199\u20136203.\\n\\n[11] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 17 022\u201317 033, 2020.\\n\\n[12] J.-H. Kim, S.-H. Lee, J.-H. Lee, and S.-W. Lee, \u201cFre-gan: Adversarial frequency-consistent audio synthesis,\u201d arXiv preprint arXiv:2106.02297, 2021.\\n\\n[13] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 5530\u20135540.\\n\\n[14] N. Ellinas, G. Vamvoukakis, K. Markopoulos, A. Chalamandaris, G. Maniati, P. Kakoulidis, S. Raptis, J. S. Sung, H. Park, and P. Tsiakoulis, \u201cHigh quality streaming speech synthesis with low, sentence-length-independent latency,\u201d arXiv preprint arXiv:2111.09052, 2021.\\n\\n[15] T. Capes, P. Coles, A. Conkie, L. Golipour, A. Hadjitarkhani, Q. Hu, N. Huddleston, M. Hunt, J. Li, M. Neeracher et al., \u201cSiri on-device deep learning-guided unit selection text-to-speech system.\u201d in Interspeech, 2017, pp. 4011\u20134015.\\n\\n[16] C. Wu, Z. Xiu, Y. Shi, O. Kalinli, C. Fuegen, T. Koehler, and Q. He, \u201cTransformer-based acoustic modeling for streaming speech synthesis,\u201d in Proc. Interspeech, 2021, pp. 146\u2013150.\\n\\n[17] T. Q. Nguyen, \u201cNear-perfect-reconstruction pseudo-qmf banks,\u201d IEEE Transactions on signal processing, vol. 42, no. 1, pp. 65\u201376, 1994.\\n\\n[18] C. Yu, H. Lu, N. Hu, M. Yu, C. Weng, K. Xu, P. Liu, D. Tuo, S. Kang, G. Lei et al., \u201cDurian: Duration informed attention network for multimodal synthesis,\u201d arXiv preprint arXiv:1909.01700, 2019.\\n\\n[19] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d arXiv preprint arXiv:1711.05101, 2017.\\n\\n[20] J. Kong, J. Son, and J. Kim, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 5530\u20135540.\"}"}
{"id": "tran22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis\\n\\nTho Tran1, The Chuong Chu1, Hoang Vu1, Trung Bui2, and Steven Q. H. Truong1\\n\\n1 VinBrain, Hanoi, Vietnam\\n2 Independent Researcher, USA\\n\\n{v.thotran,v.chuongchu, v.vuhoang, v.brain01}@vinbrain.net, bhtrung@gmail.com\\n\\nAbstract\\n\\nIn recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments.\\n\\nIndex Terms: End-to-End Text-to-Speech, real-time, streaming, TTS, VITS, HiFi-GAN, customer perceived latency\\n\\n1. Introduction\\n\\nIn recent years, the neural text-to-speech (TTS) system has become popular in research and production. Applying neural networks to speech synthesis reduces the complexity of traditional TTS systems and produces high-quality speech that can be indistinguishable from natural speech.\\n\\nIn most cases, neural TTS system pipelines consist of two stages generative modeling, excluding text analysis modules such as text normalization and grapheme-to-phoneme (G2P) conversion. The first stage is acoustic modeling, which usually generates intermediate speech representations such as mel-spectrograms [1, 2, 3, 4, 5] from the normalized text, and the next stage is called vocoding, which synthesizes the audio waveform conditioned on the intermediate speech representations [6, 7, 8, 9, 10, 11, 12]. Most studies use autoregressive neural networks in acoustic modeling and waveform synthesizing, showing high-fidelity results comparable to human speech. However, their sequential generative process makes slow inference speed, given that the mel-spectrogram sequence normally has a length of hundreds to thousands of frames and robustness (word skipping and repeating) issues with a long sequence or out of vocabulary. Recently, non-autoregressive methods have been designed to address these limitations by generating spectrograms or waveform audios in parallel. However, the 2-stage speech synthesis has several disadvantages. Two models in this pipeline require sequential training from acoustic model to vocoder or fine-tuning the vocoder with the mel-spectrogram output of the acoustic model. As a result, it is time-consuming to train these two components in this pipeline to produce good audio quality. Therefore, the parallel end-to-end TTS architectures have attracted to improving those restrictions of the 2-stage TTS. A parallel end-to-end TTS model such as VITS [13] can generate more natural-sounding audio than the SOTA two-stage pipelines like Tacotron2 [2] + HiFi-GAN [11] or GlowTTS [5] + HiFi-GAN [11]. At the same time, it takes only one training progress.\\n\\nAlthough VITS [13] can produce human-like sound quality with low latency on GPU, it still has high latency if the input is a long sequence and the synthesis process is running on the CPU. Some streaming methods are proposed to resolve this bottleneck, such as [14, 15, 16]. However, their pipelines are 2-stage TTS, and both the acoustic model and vocoder are auto-regressive models. In [14], the proposed system, of which the pipeline consists of a variant Tacotron [1] for the acoustic model and LPCNet [8] for the vocoder, can synthesize real-time human-like speech on CPU. The acoustic model takes the entire long text sequence for this approach and produces the output frames sequentially. After that, the output frames are accumulated in a fixed-length buffer. Subsequently, audio samples are synthesized sequentially by LPCNet vocoder with input is the buffer containing the acoustic frames. Therefore, the user can start listening while the next audio segment is being synthesized. Due to the autoregressive nature, those systems face several problems: difficulty in controlling the speed of the synthesized speech and robustness (word skipping and repeating) issues. Nevertheless, the non-autoregressive models do not have these problems with long sequence input such as [3, 4].\\n\\nIn this paper, we tackle the drawbacks of these approaches by proposing a method facilitating the parallel TTS system for real-time non-GPU applications. From [11, 12, 13], we adopt a new architecture for the decoder of VITS to reduce latency when processing on frame-rate length. At the same time, we present a streaming inference method for our model with much lower latency than the normal inference procedure of VITS on CPU. Our experiments are implemented on our in-house Vietnamese dataset namely VBSF001 with a professional female voice. We release this dataset. The paper structure is as follows. Section 2 describes our proposed method, including the model architecture, the training losses, and the streaming inference method. Our experiments and results are detailed in Section 3. The final section presents conclusions.\\n\\n1https://huggingface.co/datasets/thotnd/VBSF001\"}"}
{"id": "tran22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Methods\\n\\n2.1. Model Architecture\\n\\nOur architecture is based on VITS [13], a parallel end-to-end TTS. The model consists of a posterior encoder, prior encoder, decoder, discriminator, and the duration predictor. The posterior encoder and the prior encoder are similar to VITS [13]. The decoder, duration predictor, and discriminators are modified to reduce the latency time while maintaining the audio quality. The posterior encoder and discriminators are only used for training, not for inference.\\n\\n2.1.1. Posterior encoder and Prior encoder\\n\\nThe posterior encoder takes the linear-scale log magnitude spectrograms and returns latent variables. The prior encoder consists of a text encoder, a linear projection, and a normalizing flow. The text encoder is a transformer-based encoder that converts input characters into hidden representations. The text encoder is followed by a linear projection layer that produces the mean and variance used for constructing the prior distribution. The details of those modules are described in VITS [13].\\n\\n2.1.2. Decoder\\n\\nThe original VITS model uses HiFi-GAN V1 generator architecture in [11] for the decoder, generating the high-quality waveform. However, the number of parameters in HiFi-GAN V1 is approximately 14M, which leads to high latency when running on CPU devices. Thus, we proposed a multi-band decoder architecture to synthesize the high-fidelity waveform in parallel, as shown in Figure 1b. The decoder transforms the latent variables into four frequency sub-band waveforms, including two high-frequency and two low-frequency bands. The synthetic sub-band waveforms are summed to the full-band waveform by the synthesis filter of the PQMF filter [17]. Specifically, the decoder consists of 2 upsampling blocks, each containing an upsampling layer followed by a multi-receptive field fusion (MRF) module proposed in [11] and a leaky-relu activation.\\n\\n2.1.3. Discriminator\\n\\nWe use the resolution-wise discriminators in [12], consisting of the resolution-wise multi-period discriminator (RPD) and the resolution-wise multi-scale discriminator (RSD). The RPD captures the periodic audio patterns with five sub-discriminators, each of which processes specific periodic parts of the input audio. The RSD observes consecutive patterns of the audio by three sub-discriminators. Each sub-discriminator operates on different input scales: raw audio, 2x downsampled audio, and 4x downsampled audio. The Discrete Wavelet Transform (DWT) is applied to downsampling audio to replace the average pooling to avoid losing information about high frequencies.\\n\\n2.1.4. Duration Predictor\\n\\nThe duration predictor takes the hidden representation which produced by the text encoder and estimates the distribution of character duration. The duration predictor consists of two convolutional layers with ReLU activation, layer normalization, and dropout followed by a projection layer. The architecture hyperparameters of the duration predictor are the same as those of GlowTTS [5].\\n\\n2.2. Training Losses\\n\\n2.2.1. Adversarial Training\\n\\nDuring adversarial training, the generated output from the decoder \\\\( G \\\\) and the ground truth waveform \\\\( y \\\\) are distinguished by multiple discriminators \\\\( D \\\\). In this work, two loss functions: the least-squares loss and the additional feature-matching loss are employed for adversarial training and training the generator respectively.\\n\\n\\\\[\\nL_{\\\\text{adv}}(D) = E_{y,z}(D(y) - 1)^2 + D(G(z))^2 \\\\tag{1}\\n\\\\]\\n\\n\\\\[\\nL_{\\\\text{adv}}(G) = E_z[(D(G(z)) - 1)^2] \\\\tag{2}\\n\\\\]\\n\\n\\\\[\\nL_{\\\\text{fm}}(G) = E_{y,z} \\\\left( T \\\\sum_{l=1}^{N_l} ||D_l(y) - D_l(G(z))||_1 \\\\right) \\\\tag{3}\\n\\\\]\\n\\nwhere \\\\( T \\\\) denotes the total number of layers in the discriminator and \\\\( D_l \\\\) outputs the feature map of the \\\\( l \\\\)th layer of the discriminator with \\\\( N_l \\\\) number of features.\\n\\n2.2.2. Variational Inference\\n\\nThe proposed architecture in [13] is structured as a conditional VAE with the training objective of maximizing the variational lower bound (VLB) of the intractable log-likelihood of the data \\\\( p_\\\\theta(x|c) \\\\):\\n\\n\\\\[\\n\\\\log p_\\\\theta(x|c) \\\\geq E_{q_\\\\phi(z|x)}[\\\\log p_\\\\theta(x|z) - \\\\log q_\\\\phi(z|x) p_\\\\theta(z|c)] \\\\tag{4}\\n\\\\]\\n\\nwhere \\\\( c \\\\) is characters or phonemes representations, \\\\( z \\\\) is the latent variables, \\\\( p_\\\\theta(z|c) \\\\) is the conditional prior distribution of the \\\\( z \\\\) given \\\\( c \\\\), \\\\( q_\\\\phi(z|x) \\\\) is the approximate posterior distribution of \\\\( z \\\\), and \\\\( p_\\\\theta(x|z) \\\\) is the likelihood function of a data point \\\\( x \\\\). The objective is then interpreted as minimizing the sum of reconstruction loss \\\\( -\\\\log p_\\\\theta(x|z) \\\\) and the KL divergence \\\\( L_{\\\\text{kl}} = \\\\log q_\\\\phi(z|x) - \\\\log p_\\\\theta(z|c) \\\\).\\n\\nIn this work, during the training process of the generator, the abovementioned reconstruction loss applied in [13] is replaced by the multi-resolution STFT reconstruction loss in both full-band and sub-band scales introduced in [10].\\n\\nMulti-resolution STFT Loss\\n\\nFor a single STFT loss, we minimize the spectral convergence \\\\( L_{\\\\text{sc}} \\\\) and log STFT magnitude \\\\( L_{\\\\text{mag}} \\\\) between the ground truth waveform \\\\( y \\\\) and the synthesized audio from decoder \\\\( G \\\\):\\n\\n\\\\[\\nL_{\\\\text{sc}}(G) = \\\\| \\\\text{STFT}(y) - \\\\text{STFT}(G(z)) \\\\|_F \\\\tag{5}\\n\\\\]\\n\\n\\\\[\\nL_{\\\\text{mag}}(G) = 1/N \\\\| \\\\log |\\\\text{STFT}(y)| - \\\\log |\\\\text{STFT}(G(z))| \\\\|_1 \\\\tag{6}\\n\\\\]\\n\\nwhere \\\\( \\\\| . \\\\|_F \\\\) and \\\\( \\\\| . \\\\|_1 \\\\) represent the Frobenius and \\\\( L_1 \\\\) norms, respectively. \\\\( |\\\\text{STFT}(.)| \\\\) indicates the STFT magnitude and \\\\( N \\\\) is the number of elements in the magnitude. For the multi-resolution STFT loss function, there are \\\\( M \\\\) single STFT losses with different analysis parameters (FFT size, window size and hop size). The \\\\( M \\\\) losses are then averaged:\\n\\n\\\\[\\nL_{\\\\text{mr}}_{\\\\text{stft}}(G) = E_{z,y} \\\\left[ \\\\frac{1}{M} \\\\sum_{m=1}^{M} (L_{\\\\text{m}}_{\\\\text{sc}}(G) + L_{\\\\text{m}}_{\\\\text{mag}}(G)) \\\\right] \\\\tag{7}\\n\\\\]\\n\\nAnd the final multi-resolution STFT loss function for our model becomes:\\n\\n\\\\[\\nL_{\\\\text{mr}}_{\\\\text{stft}}(G) = \\\\frac{1}{2} (L_{\\\\text{full}}_{\\\\text{fmr}}_{\\\\text{stft}}(G) + L_{\\\\text{sub}}_{\\\\text{smr}}_{\\\\text{stft}}(G)) \\\\tag{8}\\n\\\\]\"}"}
{"id": "tran22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.2.3. Duration Loss\\nFor the duration training, we use the method from [5], thus the same duration loss is applied of which the objective is to minimize the Mean Square Error (MSE) between the output of the duration predictor $\\\\hat{d}$ and the duration label $d$ from an alignment matrix produced by an algorithm called \u201cMonotonic Alignment Search\u201d in [5]:\\n\\n$$L_{dur} = \\\\text{MSE}(\\\\hat{d}, d)$$\\n\\n(9)\\n\\n2.2.4. Final Loss\\nFinally, with the combination of VAE and GAN training, the final objective loss for training our model is a weighted sum of the abovementioned losses:\\n\\n$$L_{vae} = L_{kl} + L_{dur} + \\\\lambda_{fm} L_{fm} + \\\\lambda_{mr} L_{mr} + L_{adv}(G)$$\\n\\n(10)\\n\\nwhere we set $\\\\lambda_{fm} = 4$ and $\\\\lambda_{mr} = 45$\\n\\n2.3. Streaming inference\\nWhile generating the speech with the VITS [13], the prior encoder (text encoder, linear projection, and normalizing flow) combines with the stochastic duration predictor to generate the latent variables as same as the acoustic frame sequence from the input phonemes. Then the latent variables are fed into the decoder, which returns the speech audio. This process has a very low latency on GPU due to the efficient parallel computation.\\n\\nFor a longer input sequence, the normalizing flow and the decoder must process with an enormous frame length, which takes a very long time to generate speech on CPUs, thus raising the high latency from when the synthesis process starts until the final audible speech is synthesized. To tackle this problem, we propose a streaming method for our non-autoregressive model, as shown in Figure 1a.\\n\\nFirstly, the text encoder transforms a sequence of characters from input texts into hidden representations. Then, the duration predictor produces the distributions of the character duration, which takes the output of the text encoder. The acoustic frames are estimated based on the output of linear projection and the distribution of character duration. Since the length of the input of the above modules is equivalent to the character-level sequence length, which is relatively short, for the above modules, we implement non-streaming while ensuring efficient computation and providing good prosody quality for longer utterances. Moreover, we chunk the entire acoustic frames into the fixed segments passed sequentially to the normalizing flow and the decoder to minimize the latency. Although the normalizing flow is trained with full acoustic frame sequences, it is still possible to process short sequences due to its nature of being fully convolutional. Our experiment shows that the output audio quality is still comparable to non-streaming system. Since the decoder is trained with the input as short segments of the latent variables and the output as short chunks of audio, it can easily adapt to our streaming system. To reduce the boundary effect, the last 16 frames of the previous segment and the first 16 frames of the following segment are concatenated to the current segment, respectively. Each acoustic segment has 32 frames corresponding to 8192 audio samples, and the acoustic receptive field has the size of 64 consecutive frames corresponding to 16384 audio samples. The audio segment is generated by passing the acoustic receptive field to the normalizing flow, followed by the decoder. Finally, the synthesized audio will be removed at both ends by an interval of 4096 samples corresponding to 16 frames added in the acoustic receptive field. This streaming inference method dramatically reduces the customer perceived latency (CPL) as long as the CPU can run the process faster than real-time audio.\\n\\n3. Experiments and results\\n3.1. Data\\nWe use an in-house Vietnamese dataset, namely VBSF001, with a professional Southern female accent for our experiments. The dataset consists of about 7000 utterances, approximately 10 hours of audio data. The speech data is sampled at 22kHz. We split the dataset into a training set (6800 utterances), a validation set (100 utterances), and a test set (100 utterances). Similar to VITS [13], the linear spectrogram is used as input of the posterior encoder, produced by short-time Fourier transform (STFT) with 256 of hop size, 1024 of window size, and 1024 points of Fourier transform. For the text data, a rule-based TTS front-end module normalizes it. The input to our model is normalized characters sequence and punctuations.\"}"}
{"id": "tran22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our model is a variant of VITS \\\\cite{13} architecture. The hyperparameters in the prior and posterior encoder are the same as the original model. With the discriminator, the configuration is described in detail in FreGAN \\\\cite{12}. We have some modifications about the decoder and the losses. For the decoder depicted in Section 3.1, the multi-band decoder upsamples the latent variables with two upsampling blocks to match the temporal resolution of sub-band audio. As a result, similarly to HiFi-GAN \\\\cite{11}, we set the kernel size of transposed convolutions to \\\\([16, 16]\\\\) and dilation rates of MRF to \\\\([\\\\[1, 1\\\\], \\\\[3, 1\\\\], \\\\[5, 1\\\\]]\\\\) x 3. Moreover, the pre-convolution layer is a 1D-convolution layer of which the kernel size is 7, and the number of output channels is 256. After applying the last convolution layer, the number of channels of the output is 4, corresponding to 4 sub-band waveforms. For the multi-band processing, we follow the method in \\\\cite{18}. The pseudo quadrature mirror filter \\\\cite{17} (PQMF) is adopted, and we choose the finite impulse response (FIR) analysis/synthesis filter order of 63 for uniformly spaced 4-band implementations. The multi-resolution STFT loss is computed by the average of six different STFT losses, consisting of three for full-band and three for sub-band with different STFT analysis parameters (FFT size, window size, hop size) as follow: \\\\((1024, 600, 120), (2048, 1200, 240), (512, 240, 50), (384, 150, 30), (683, 300, 60), (171, 60, 10)\\\\).\\n\\nThe network parameters are trained with the AdamW\\\\cite{19} optimizer with $\\\\beta_1 = 0.8$, $\\\\beta_2 = 0.99$, and weight decay $\\\\lambda = 0.01$. An $0.999^{1/8}$ factor schedules the learning rate decay in every epoch with an initial learning rate of $2 \\\\times 10^{-4}$. We use mixed precision training on 2 NVIDIA V100 GPUs. The batch size is set to 64, and the model is trained up to 500k steps. In addition, we also use the public implementations \\\\cite{2} to train the VITS model for our Vietnamese dataset as a baseline to compare with our methods in terms of quality audio, synthesis speed, and latency.\\n\\n### 3.3. Latency and Quality\\n\\nWe compare the audio quality of our non-streaming and streaming methods and the baseline VITS by the MOS test on 100 samples of the evaluation set. Each audio is listened to by 20 testers, who are all Vietnamese native speakers from different regions. We measured the synthesis speed and the latency time on an Intel i5-8300H CPU at 2.3GHz with 20 randomly selected samples from our dataset.\\n\\nThe MOS result with 95% confidence intervals is shown in Table 1. Our model in the non-streaming mode achieves a similar MOS to that of VITS while significantly reducing the model size and inference time compared to VITS. It can synthesize speech 4.37 times faster than VITS on CPU; the details of the synthesis speed is shown in Table 2. Our audio samples are available on the demo website \\\\cite{3}.\\n\\nFor the streaming mode, our method still maintains audio quality on the MOS test with a slight reduction of 0.10 compared to the VITS. To demonstrate our streaming method's effectiveness in synthesizing high-quality speech on CPU with low latency, we measure the customer perceived latency (CPL) as the time interval from a TTS request received to when the first audio buffer is played (lower CPL is better). When the streaming mode is on, our method reduces the CPL 26.6 times and 6.64 times compared to the original VITS and when our model is in non-streaming mode, respectively. The CPL of VITS, VITS with our streaming inference method, our model (non-streaming), and our model (streaming) are depicted in Figure 2.\\n\\n### Table 1: Comparison of the MOS with 95% confidence intervals and the number of parameters in the inference procedure.\\n\\n| Model            | MOS (CI)       | Parameters |\\n|------------------|----------------|------------|\\n| Ground Truth     | 4.56 (\u00b1 0.032) | -          |\\n| VITS             | 4.45 (\u00b1 0.026) | 29.1M      |\\n| Ours (non-streaming) | 4.43 (\u00b1 0.034) | 18.3M      |\\n| VITS (streaming) | 4.36 (\u00b1 0.031) | -          |\\n| Ours (streaming) | 4.35 (\u00b1 0.028) | -          |\\n\\n### Table 2: Comparison of the synthesis speed. Speed of n kHz means that the model can generate n \u00d7 1000 raw audio samples per second. The numbers in () mean the speed compared to real-time.\\n\\n| Model                        | Speed (kHz) | Speed compared to real-time |\\n|------------------------------|-------------|-----------------------------|\\n| VITS                         | 36.75 (\u00d7 1.67) |                             |\\n| Ours (non-streaming)         | 161.03 (\u00d7 7.30) |                             |\\n\\n### Figure 2: The customer perceived latency (CPL) evaluation with increase audio length. A logarithmic scale is used for the latency (y-axis) for a better visualization of the large discrepancies between the various systems shown.\\n\\nThis work presents a variant of parallel end-to-end VITS, which synthesizes high-fidelity audio with lower latency than the original VITS. Our variation of VITS has adopted the multi-band processing to the decoder to reduce the upsampling steps and replaced the original discriminators with the discriminators in Fre-GAN to avoid losing information about high frequencies. We introduced an effective streaming inference method for the non-autoregressive TTS. Combining the proposed model architecture and the streaming inference method is an efficient approach for real-time applications on low-resource devices. The customer perceived latency reduced 26 times compared to VITS, while the MOS only decreases by 0.10.\"}"}
