{"id": "vinnikov24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"NOTSOFAR-1 Challenge:\\nNew Datasets, Baseline, and Tasks for Distant Meeting Transcription\\nAlon Vinnikov, Amir Ivry, Aviv Hurvitz, Igor Abramovski, Sharon Koubi, Ilya Gurvich, Shai Pe'er, Xiong Xiao, Benjamin Martinez Elizalde, Naoyuki Kanda, Xiaofei Wang, Shalev Shaer, Stav Yagev, Yossi Asher, Sunit Sivasankaran, Yifan Gong, Min Tang, Huaming Wang, Eyal Krupka\\n\\nMicrosoft notsofarchallenge@microsoft.com\\n\\nAbstract\\nWe introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (NOTSOFAR) Challenge, datasets, and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in meeting scenarios, with single-channel and known-geometry multi-channel tracks, using a single device. We launch two new datasets: First, a benchmark dataset of 280 English meetings, averaging 6 minutes each, capturing a broad spectrum of acoustic and conversational patterns across 30 rooms with 4-8 attendees. Second, a 1000-hour simulated training dataset, synthesized for real-world generalization, incorporating 15,000 real acoustic transfer functions. The NOTSOFAR-1 Challenge aims to advance research in the field of DASR, providing key resources to unlock the potential of data-driven methods, which we believe are currently constrained by the absence of comprehensive high-quality training and benchmark datasets.\\n\\nIndex Terms: speech recognition, speaker diarization, speech separation, multi-channel speech processing\\n\\n1. Introduction\\nConversational speech recognition remains a formidable challenge, particularly in meeting scenarios where distant array devices are commonly employed for capturing speech. The distance between speakers and microphones brings about substantial acoustic complexities including reverberation, variations in speaker distances, volumes and noises. These complexities are further amplified by the ever-changing positions of speakers and environmental conditions, altering the acoustic transfer functions (ATFs) [1] between speakers and microphones. Compounding these acoustic challenges are the dynamic aspects of conversations, including speech overlap, interruptions, rapid changes in speakers, and non-verbal vocalizations.\\n\\nWith the emergence of Large Language Models (LLMs), conversational speech recognition has been infused with unprecedented practical importance. LLMs fed by speech recognition engines are redefining user-experience with capabilities ranging from meeting summary, note taking and sentiment analysis, to personalized and context-aware responses to user queries. The foundation of these innovative features, particularly in meeting scenarios, lies in distant speaker diarization and automatic speech recognition (DASR).\\n\\nThe field of conversational speech recognition has seen significant advancements thanks to numerous datasets such as AMI [2], ICSI [3], Sheffield Wargames [4], ASpiRE [5], DIRHA [6], VOiCES [7], DiPCo [8], Ego4D [9], AliMeeting [10], LibriCSS [11], and challenges such as CHiME [12, 13, 14, 15, 16, 17, 18]. However, they suffer from various limitations, particularly with respect to advancing DASR. The three most-similar corpora to our work, targeting general meeting scenarios with unsegmented recordings from a single device, are LibriCSS, AMI, and AliMeetings.\\n\\nLibriCSS makes a significant step forward by including unsegmented semi-real meetings and proposing to measure word error rate (WER) directly rather than signal-based metrics for front-end processing. They also provide matching fully simulated training data for speech separation front-end. However, since their evaluation data has been obtained by playing LibriSpeech utterances via loudspeakers, it does not account for natural conversational dynamics or real-world acoustic complexities, such as time-varying ATFs. AMI makes an impressive contribution collecting 100 hours of real English meeting recordings with close-talk annotations. However, its recordings are confined to three rooms, and its development and evaluation sets comprise only 18 and 16 sessions with a maximum of four attendees. This small sample size and limited acoustic diversity lead to uncertainty in predicting real-world performance.\\n\\nAliMeetings collects 240 real Mandarin meeting recordings involving more than 481 speakers. Yet, its test set consists of only 20 sessions with a maximum of four attendees, still facing the same small-sample issue as AMI. In addition to these limitations, neither AMI nor AliMeetings include a matching simulated training set, which hinders the application of data-driven front-ends.\\n\\nAddressing these limitations, our main contributions are as follows:\\n\\n\u2022 A meeting dataset for benchmarking and training: We release a dataset comprising 280 distinct meetings, carefully designed for benchmarking purposes. Meetings were recorded across a variety of rooms, capturing an unprecedented spectrum of real-world scenarios, with up to 8 attendees per meeting. We emphasize transcription accuracy and machine-bias prevention with a multi-judge annotation process. Meetings include detailed metadata for error analysis, documenting acoustic and conversational aspects. Recordings employ both single-channel and multi-channel devices, reflecting the range of commercial recording equipment.\\n\\n\u2022 A matched simulated training dataset: For training speech separation and enhancement methods, we also release a simulated dataset of approximately 1000 hours, utilizing 15,000 real ATFs. The mixing process is designed to mirror real-world patterns with up to three speakers per mixture, and features high-quality clean speech.\\n\\n\u2022 An open-source baseline system: We offer an implementation complete with inference, training, data handling, and...\"}"}
{"id": "vinnikov24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"evaluation code. It is entirely written in Python and includes\\ncontinuous speech separation (CSS), ASR, and diarization\\nmodules \\\\[19, 20\\\\].\\n\\n\\\\- NOTSOFAR Challenge\\n\\nThese resources outlined above are launched as part of a challenge to address fundamental\\nquestions in the field, focusing on single-device DASR with\\nsingle-channel and known-geometry multi-channel tracks.\\n\\n2. Tracks, Metrics and Scientific Goals\\n\\nThe challenge features two main tracks centered on the use of\\na single device: the single-channel (SC) track and the known-\\ngeometry multi-channel (MC) track, with participants able to\\nsubmit entries to either one or both. The use of a single de-\\nvice and fixed, known geometry in the multi-channel track\\nare key distinctions from CHiME-8's DASR challenge\\n3. This\\naligns with typical conference room setups, sidesteps multi-\\ndevice technical complexities, and enables array-geometry-\\nspecific solutions. Participants receive unsegmented record-\\nings from each meeting and are asked to generate time-marked,\\nspeaker-attributed transcripts. In line with the notations in \\\\[18\\\\],\\neach meeting is annotated with a set of segments, denoted as\\n\\\\[r_i = (\\\\Delta, s, v)\\\\], where \\\\(\\\\Delta = (t_s, t_e)\\\\) are start and end times,\\n\\\\(s \\\\in \\\\mathbb{Z}^+\\\\) is the speaker label, and \\\\(v \\\\in \\\\Sigma^*\\\\) is the transcript\\nfor the segment, with vocabulary \\\\(\\\\Sigma\\\\). A meeting's reference is\\nthen fully defined by \\\\(R = \\\\{r_1, \\\\ldots, r_N\\\\}\\\\). The participants'\\ntask is to estimate this reference through a generated hypoth-\\nthesis \\\\(H = \\\\{h_1, \\\\ldots, h_N\\\\}\\\\). Note that the estimated speaker labels\\nare not required to be identical to the reference labels.\\n\\nSystems will be ranked using the time-constrained\\nminimum-permutation word-error rate (tcpWER), aligned with\\nCHiME-8's DASR track. This metric accounts for speaker di-\\narization errors, word errors, and temporal errors. Given the\\nsensitivity of tcpWER to diarization mistakes, where misat-\\ntributed segments incur penalties even with correct word recog-\\nnition, we also employ its speaker-agnostic counterpart, the\\ntime-constrained optimal reference combination word-error rate\\n(tcorcWER) \\\\[21\\\\], as a complementary metric.\\n\\nWe aim to address fundamental questions in the field of\\nDASR: How do multi-channel geometry-specific algorithms\\n(MC track) perform in contrast to geometry-agnostic algorithms\\n(CHiME-8's DASR\\n4), and how do they compare to single-\\nchannel approaches (SC track)? Can the introduced simulated\\ntraining dataset lead to data-driven front-end solutions that gen-\\neralize well to realistic acoustic scenarios? Finally, how can\\nthe available supervision signals be utilized to improve algo-\\nrithms? Namely, separated speech components from the sim-\\nulated dataset, along with close-talk recordings, transcriptions,\\nand speaker labels from the recorded meeting dataset.\\n\\n3. Training and Benchmarking Datasets\\n\\n3.1. NOTSOFAR Recorded Meeting Dataset\\n\\nTable 1 provides an overview of the recorded meeting dataset,\\nfeaturing 280 unique meetings with authentic multi-participant\\nEnglish conversations. Meetings were recorded in 30 varied\\nconference rooms, averaging 6 minutes in duration. To increase\\nacoustic diversity, each meeting was captured with several de-\\nvices, each positioned differently. This setup involved around 5\\nsingle-channel (SC) devices, each producing a single internally\\nprocessed stream, and 4 multi-channel (MC) devices, each pro-\\nducing 7 raw streams from a geometry of one central and six\\nsurrounding microphones. A recording from a single device\\nduring one meeting is referred to as a \\\"session\\\". Overall, the\\ndataset contains around 150 hours of SC data and 110 hours of\\nMC data across all sessions. The evaluation set is entirely dis-\\njoint from the training and development sets, with no overlap\\nin speakers or rooms. This dataset was designed with unique\\ncharacteristics for benchmarking DASR, as detailed next.\\n\\nCapturing the full spectrum of real-world complexities:\\nTo fully evaluate the accuracy of a DASR system, it is essential\\nto test it not only in typical scenarios but also in rare events at\\nor beyond the borders of its standard operational envelope. In\\nline with this, the recordings encompass a wide array of acous-\\ntic situations, including speakers at varying distances and vol-\\numes, and different types of noises at multiple levels. Impor-\\ntantly, the dataset includes acoustic situations that sometimes\\nsubstantially and frequently modify the ATF between the active\\nspeaker and the device. These range from in-seat movement,\\nwalking, standing or sitting, to speaking near a whiteboard, and\\nentering or leaving the room. Conversational dynamics include\\noverlapping speech from multiple speakers, interruptions, rapid\\nspeaker change, laughter, coughing, and fillers. The meetings\\nspan 30 different rooms of various dimensions, layouts, con-\\nstruction materials and acoustic characteristics. Attendees for\\neach meeting typically vary between 4-8 adults, involving 22\\nunique speakers in the development and training sets, and a dis-\\ntinct group of 10 speakers in the evaluation set, while ensuring\\nbalanced gender representation.\\n\\nMetadata for error analysis:\\nTo facilitate error analysis\\neach meeting is accompanied by metadata, including activities\\nand acoustic events. Some meetings are purposefully domi-\\nnated by rare events which are documented in the metadata.\\nFor example, meetings tagged as extra noisy, or having ex-\\ntra overlapping speech are useful for testing algorithms under\\nthose conditions in an isolated manner. Meetings where speak-\\ners are in motion or are speaking near sound-reflective white-\\nboards while turning their heads are useful to test the robustness\\nof an algorithm to changes in the ATF.\\n\\nPrioritizing number of meetings over total duration:\\nSpeech datasets often highlight the duration of their recordings\\nin hours, yet the utility of a benchmark is determined by the dis-\\ntribution it spans and the sample size it provides. Recognizing\\nthat prolonged recordings from the same meeting offer dimin-\\nishing returns in terms of yielding new, independent informa-\\ntion, our approach focuses on maximizing the diversity of the\\ndataset by including a larger number of distinct meetings. Our\\ndataset consists of approximately 280 relatively short meetings,\\neach lasting roughly six minutes. Since each meeting offers a\\nunique mix of acoustics, speaker identities, and conversational\"}"}
{"id": "vinnikov24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dynamics, we obtain a highly diverse sample. This allows us to assume each meeting is approximately an independent and identically distributed (i.i.d.) element, facilitating the computation of meaningful confidence intervals, a vital aspect of algorithm comparison. Our baseline code features a simple method for calculating confidence intervals, both in absolute terms and relative to the baseline algorithm. Notably, the narrowness of these confidence intervals is determined by the number of meetings rather than the total recorded hours.\\n\\nBias-free transcription process: Bias in transcription can arise when annotators are influenced by automated systems, producing transcriptions that inadvertently favor a particular system. Consider a machine-aided process where a machine first transcribes the recordings, and a human conducts a second pass, deciding to accept some words and correct others. Due to the inherent ambiguities of speech, even in close-talk recordings, and especially in situations with heated discussions, overlapping speech, and rapid speaker turns, some erroneous machine suggestions may seem reasonable and hence may be accepted by the human annotator. While this process is relatively efficient, it perpetuates errors of the specific transcription algorithm used. The implications of this bias are profound: competing algorithms will be penalized for deviating from the transcription algorithm, potentially concealing the progress of new algorithms. This is particularly problematic when improvements are incremental, as is often the case in research. Since even subtler forms of machine aid can introduce bias, our transcription process for the development and evaluation sets relies on close-talk recordings and is conducted entirely without human access to machine-generated transcriptions [22].\\n\\nMulti-judge annotation for accurate segmentation and transcription: To ensure high-quality ground-truth transcription and segmentation, attendees wore close-talk microphones. Spectral subtraction between close-talk channels was used as pre-processing for an automatic segmentation algorithm, tuned to never miss the target speaker. Human annotators then corrected any segmentation errors, and additional throat microphones were employed for quality control. For most meetings, transcription was carried out independently by two professional transcribers, with a third judge resolving non-consensus cases. About 50 meetings in the training set involved only two transcribers, with the second making the final decision.\\n\\nDistinguishing single-channel from single-microphone: Speech recognition systems often deal with audio from conference room devices. These devices are typically equipped with microphone arrays and employ proprietary on-device processing involving echo cancellation, de-reverberation, beamforming, and noise suppression, yielding a single stream output that we refer to in this paper as \u201csingle-channel\u201d. This significantly differs from the single-microphone audio commonly used for evaluation. To bridge the gap to real-world applications, our recordings include a variety of commercially available single-channel devices, and the NOTSOFAR-1 single-channel track is dedicated to evaluating performance with these devices.\\n\\nOriented toward far-field: The primary challenge in far-field speech transcription lies in creating an algorithm that is robust to the acoustic complexities introduced by the distance between microphone and speaker, as well as the surrounding environmental conditions. Some aspects of our dataset are focused on far-field evaluation. Namely, we steer away from challenges related to different languages, accents, and domain-specific jargon. The recordings are exclusively in English, spoken by native or near-native speakers who were instructed accordingly.\\n\\nA multitude of topics and interaction styles: Most meetings featured semi-professional topics, in which participants role-played as professionals discussing a work-related issue. For example, a cruise ship company planning an event, or users complaining about IT problems. Some meetings featured non-work-related topics, such as favorite TV shows, or debating whether to raise kids as vegetarians. We expect this role-playing to create a range of conversational dynamics akin to those found in actual meetings in terms of speech overlap, turn-taking dynamics, speech disfluencies, interruptions and so on.\\n\\n3.2. NOTSOFAR Simulated Dataset\\n\\nThe simulated training set consists of about 1000 hours simulated with the same microphone-array geometry of one central and six surrounding microphones, matching the multi-channel devices in the NOTSOFAR meeting dataset. It provides separated speech and noise components as supervision signals for training data-driven speech separation and enhancement methods. It is designed to foster generalization to realistic settings by identifying and closing major train-test gaps prevalent in simulated datasets in prior works.\\n\\nReal-room acoustic transfer functions: Simulated datasets for speech separation typically rely on the Image Method [23] to generate simulated ATFs, which are then convolved with clean speech to further simulate in-room utterances. Despite advancements in simulation techniques, simulated ATFs do not take into account many elements present in the real-world, such as: the room's layout and the unique reflective properties of its surfaces, the dependency of the reflection coefficients on frequency and angle of incidence, the reflections from adjacent speakers, the directivity of the speech source, and the acoustic effects of the specific microphone array casing. The limitations of the Image Method in this context have been explored in several studies, e.g., [24, 25, 26]. To address these shortcomings, our dataset features ATFs recorded in actual conference rooms, in an acoustic setup arranged to closely replicate authentic meeting environments. We collected a total of 15,000 real ATFs, measured in various positions and rooms by multiple devices sharing the same geometry. The ATFs were reconstructed from chirps emitted by a mouth simulator.\\n\\nMOS-filtered clean speech: For clean speech we use the Librivox corpus [27], which contains recordings of varying quality. Many of these recordings are of excellent speech quality, but others exhibit poor quality in the form of speech distortion, background noise and reverberation. This quality variance compromises simulation authenticity, since the ATF convolution operation assumes true clean speech. Hence, we follow the work of [28] and choose the upper quartile with respect to the subjective mean-opinion score (MOS) as our clean speech corpus, amounting to 500 hours of speech.\\n\\nAugmented clean speech for realistic overlap patterns: We observe that the Librivox clean speech corpus, comprising recordings of individuals reading audiobooks, typically consists of rather continuous segments of speech lasting several seconds, without significant pauses. This makes it harder to achieve a mixture reflecting realistic overlap patterns which include short interruptions and rapid speaker turns. We therefore apply a signal processing algorithm that detects drops in speech power to identify short pauses in speech and subsequently insert random-length silence breaks at these points. By augmenting clean speech in this manner, we facilitate a more realistic simulation of conversational dynamics during the mixing stage.\\n\\nMixing up to three speakers: During the mixing process we first convolve the augmented clean speech utterances with...\"}"}
{"id": "vinnikov24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"different ATFs selected from the same room. The resulting individual utterances are roughly 40 seconds long. We then apply a small random shift before summing utterances from 3 distinct speakers. This strategy results in mixtures that average 50 seconds in length, possessing two attractive characteristics. First, we qualitatively observe that segments of 2-10 seconds, as commonly utilized in speech separation training, display complex overlaps and rapid speaker turns involving up to three speakers, mirroring real-world patterns. Second, these segments tend to be more challenging, serving as a form of hard sample mining, which tends to benefit data-driven methods. Finally, transient and stationary noises recorded in real rooms with the matching geometry are added to the mixture.\\n\\nSeparating direct and early reflections from late reverberations: Previous studies have highlighted that speech signals captured by a distant microphone are generally smeared by reverberation, which severely degrades automatic ASR performance [29]. Motivated by this, as well as internal experimentation findings, we have designed the supervision signals in this training set to facilitate not only speech separation training but also dereverberation. We assume a room impulse response (RIR) consists of three parts: a direct-path response, early reflections, and late reverberations. The recorded ATFs (now viewed as RIRs) are then divided into two components: direct and early reflections, and late reverberations. This division is accomplished by applying a smoothed-out cutoff at the 50-millisecond mark. The mixed signal at time $t$, with $i$ indexing speakers, is then represented as follows:\\n\\n$$mixture(t) = \\\\sum_{i} h_{spk_{direct+early}}(t) + h_{spk_{reverb}}(t) + noise(t).$$\\n\\nThe individual ground truth (GT) components are provided as supervision signals in our dataset. The NOTSOFAR-1 baseline model employs the direct and early reflections component as target labels for its speech separation module.\\n\\nLimitations: A notable limitation of the simulated training set is the static nature of the ATFs, which, despite being recorded in real rooms, remain fixed throughout each simulated utterance. This fails to mirror common real-world situations where the ATF varies in time as the speaker or environment moves. To evaluate an algorithm's robustness against time-varying ATFs, one can leverage the metadata provided in the NOTSOFAR meeting dataset. Furthermore, we hope that the training portion of these real recordings, which includes instances of time-varying ATFs, will contribute significantly to improving an algorithm's robustness to such variations.\\n\\n4. Baseline System\\n\\nThe baseline for NOTSOFAR-1, inspired by [19, 20], consists of three steps: continuous speech separation (CSS), ASR, and speaker diarization.\\n\\n**CSS:** The objective of CSS is to receive an input audio stream, consisting of either 1 or 7 channels, and convert it into a set of overlap-free streams. In our case, we generate $N=3$ distinct speech streams, which can support up to 3 speakers overlapping at the same time. We follow the conventional CSS framework, first training a speech separation network with permutation invariant training (PIT) and mask-based L1 loss, which takes fixed-length audio segments as input and outputs $N$ speech masks and one noise mask. The masks are in short-time Fourier transform domain. For the network architecture we select the Conformer model from [30].\\n\\nIn the inference phase, the network is applied in a block-wise streaming fashion to overlapping fixed-length segments. Since the order of the $N$ speech mask outputs may not be consistent across segments, we align every pair of adjacent segments. To estimate the best order, we consider all permutations and select the one with the lowest mean absolute error (MAE) between the masks, calculated over the frames shared by the two segments. After stitching the $N$ speech masks and one noise mask over time, we proceed to generate each of the $N$ output streams. For the single-channel variant, this consists of multiplying the input mixture by the target speech mask. For the multi-channel variant, we rely on mask-based minimum variance distortionless response (MVDR) beamforming [31]. As part of this scheme, the spatial covariance matrices (SCMs) of the target and interference signals are computed, where the interference signal is defined as the sum of all non-target speakers and the background noise.\\n\\n**ASR:** For automatic speech recognition (ASR), we employ Whisper \\\"large-v3\\\" [32] with word-level timestamps. ASR is applied independently to each audio stream produced by CSS.\\n\\n**Speaker Diarization:** In this step, every ASR-transcribed word is assigned a speaker label, such as 'spk0', 'spk1'. Utilizing the NeMo toolkit [33], we extract multi-scale speaker embedding vectors from six windows centered on each word, with sizes uniformly spaced from 0.5 to 3 seconds. These embeddings then undergo offline clustering via normalized maximum eigengap-based spectral clustering (NME-SC), where the affinity matrix is the average of the matrices across all scales.\\n\\nIn table 2 we report the performance achieved by both variants of our proposed baseline system on the development set. We include analysis based on a selection of hashtags from our metadata, providing insights into how different conditions affect system performance. Additionally, we evaluate the performance of close-talk recordings processed through the ASR and diarization modules, thereby quantifying the gap in performance relative to far-field processing achieved with the CSS module.\\n\\n5. Conclusions\\n\\nIn this work, we introduced the NOTSOFAR-1 Challenge, datasets, and baseline system. The challenge is specifically designed to evaluate DASR amid real-world complexities. To this end, we developed a benchmarking dataset with a large volume of distinct meetings, enabling reliable performance analysis. Annotations were created based on the agreement of multiple human listeners, providing high quality ground-truth annotations. We also provided a 1000-hour simulated multi-talker training dataset, crafted to leverage known microphone array geometry, and mitigate the mismatch between training and testing conditions. These contributions collectively aim to stimulate further research and innovation, and to push the boundaries of what is scientifically possible in the field of DASR.\"}"}
{"id": "vinnikov24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We express our profound gratitude to Hadas Eilon Carmi, Ross Cutler, Prem Premkumar, Solomiya Branets, and the IC3-AI team at Microsoft for their invaluable assistance.\\n\\n7. References\\n\\n[1] J. Chung and D. Blaser, \u201cTransfer function method of measuring in-duct acoustic properties. i. theory,\u201d The Journal of the Acoustical Society of America, vol. 68, no. 3, pp. 907\u2013913, 1980.\\n\\n[2] W. Kraaij, T. Hain, M. Lincoln, and W. Post, \u201cThe AMI meeting corpus,\u201d in Proc. International Conference on Methods and Techniques in Behavioral Research, 2005.\\n\\n[3] R. Dhillon, S. Bhagat, H. Carvey, and E. Shriberg, \u201cMeeting recorder project: Dialog act labeling guide,\u201d ICSI Technical Report TR-04, Tech. Rep., 2004.\\n\\n[4] C. Fox, Y. Liu, E. Zwyssig, and T. Hain, \u201cThe Sheffield wargames corpus,\u201d in Proc. Interspeech. ISCA, 2013.\\n\\n[5] M. Harper, \u201cThe automatic speech recognition in reverberant environments (ASpIRE) challenge,\u201d in Proc. 2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU). IEEE, 2015, pp. 547\u2013554.\\n\\n[6] L. Cristoforetti, M. Ravanelli, M. Omologo, A. Sosi, A. Abad, M. Hagm\u00fcller, P. Maragos et al., \u201cThe DIRHA simulated corpus,\u201d in Proc. LREC, 2014, pp. 2629\u20132634.\\n\\n[7] C. Richey, M. A. Barrios, Z. Armstrong, C. Bartels, H. Franco, M. Graciarena, A. Lawson, M. K. Nandwana, A. Stauffer, J. van Hout et al., \u201cVoices obscured in complex environmental settings (voices) corpus,\u201d arXiv preprint arXiv:1804.05053, 2018.\\n\\n[8] M. Van Segbroeck, A. Zaid, K. Kutsenko, C. Huerta, T. Nguyen, X. Luo, B. Hoffmeister, J. Trmal, M. Omologo, and R. Maas, \u201cDiPCo\u2013dinner party corpus,\u201d arXiv preprint arXiv:1909.13447, 2019.\\n\\n[9] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar, J. Hamburger, H. Jiang, M. Liu, X. Liu et al., \u201cEgo4d: Around the world in 3,000 hours of egocentric video,\u201d in Proc. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 18995\u201319012.\\n\\n[10] F. Yu, S. Zhang, Y. Fu, L. Xie, S. Zheng, Z. Du, W. Huang, P. Guo, Z. Yan, B. Ma et al., \u201cM2MeT: The ICASSP 2022 multi-channel multi-party meeting transcription challenge,\u201d in Proc. ICASSP. IEEE, 2022, pp. 6167\u20136171.\\n\\n[11] Z. Chen, T. Yoshioka, L. Lu, T. Zhou, Z. Meng, Y. Luo, J. Wu, X. Xiao, and J. Li, \u201cContinuous speech separation: Dataset and analysis,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7284\u20137288.\\n\\n[12] N. Ma, J. Barker, H. Christensen, and P. Green, \u201cA hearing-inspired approach for distant-microphone speech recognition in the presence of multiple sources,\u201d Computer Speech & Language, vol. 27, no. 3, pp. 820\u2013836, 2013.\\n\\n[13] E. Vincent, J. Barker, S. Watanabe, J. Le Roux, F. Nesta, and M. Matassoni, \u201cThe second \u2018CHiME\u2019 speech separation and recognition challenge: Datasets, tasks and baselines,\u201d in Proc. ICASSP. IEEE, 2013, pp. 126\u2013130.\\n\\n[14] J. Barker, R. Marxer, E. Vincent, and S. Watanabe, \u201cThe third \u2018CHiME\u2019 speech separation and recognition challenge: Analysis and outcomes,\u201d Computer Speech & Language, vol. 46, pp. 605\u2013626, 2017.\\n\\n[15] E. Vincent, S. Watanabe, J. Barker, and R. Marxer, \u201cThe 4th CHiME speech separation and recognition challenge,\u201d URL: http://spandh.dcs.shef.ac.uk/chime-challenge {Last Accessed on 1 August, 2018}, 2016.\\n\\n[16] J. Barker, S. Watanabe, E. Vincent, and J. Trmal, \u201cThe fifth \u2018CHiME\u2019 speech separation and recognition challenge: dataset, task and baselines,\u201d arXiv preprint arXiv:1803.10609, 2018.\\n\\n[17] S. Watanabe, M. Mandel, J. Barker, E. Vincent, A. Arora, X. Chang, S. Khudanpur, V. Manohar, D. Povey, D. Raj et al., \u201cCHiME-6 challenge: Tackling multispeaker speech recognition for unsegmented recordings,\u201d arXiv preprint arXiv:2004.09249, 2020.\\n\\n[18] S. Cornell, M. Wiesner, S. Watanabe, D. Raj, X. Chang, P. Garcia, Y. Masuyama, Z.-Q. Wang, S. Squartini, and S. Khudanpur, \u201cThe CHiME-7 DASR challenge: Distant meeting transcription with multiple devices in diverse scenarios,\u201d arXiv preprint arXiv:2306.13734, 2023.\\n\\n[19] T. Yoshioka, I. Abramovski, C. Aksoylar, Z. Chen, M. David, D. Dimitriadis, Y. Gong, I. Gurvich, X. Huang, Y. Huang et al., \u201cAdvances in online audio-visual meeting transcription,\u201d in Proc. 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 276\u2013283.\\n\\n[20] D. Raj, P. Denisov, Z. Chen, H. Erdogan, Z. Huang, M. He, S. Watanabe, J. Du, T. Yoshioka, Y. Luo et al., \u201cIntegration of speech separation, diarization, and recognition for multi-speaker meetings: System description, comparison, and analysis,\u201d in Proc. 2021 IEEE spoken language technology workshop (SLT). IEEE, 2021, pp. 897\u2013904.\\n\\n[21] T. von Neumann, C. Boeddeker, M. Delcroix, and R. Haeb-Umbach, \u201cMeeteval: A toolkit for computation of word error rates for meeting transcription systems,\u201d 2024.\\n\\n[22] M. Levit, Y. Huang, S. Chang, and Y. Gong, \u201cDon\u2019t count on ASR to transcribe for you: Breaking bias with two crowds.\u201d in Proc. Interspeech, 2017, pp. 3941\u20133945.\\n\\n[23] J. B. Allen and D. A. Berkley, \u201cImage method for efficiently simulating small-room acoustics,\u201d The Journal of the Acoustical Society of America, vol. 65, no. 4, pp. 943\u2013950, 1979.\\n\\n[24] L. Savioja and U. P. Svensson, \u201cOverview of geometrical room acoustic modeling techniques,\u201d The Journal of the Acoustical Society of America, vol. 138, no. 2, pp. 708\u2013730, 2015.\\n\\n[25] F. Brinkmann, L. Asp\u00f6ck, D. Ackermann, S. Lepa, M. Vorl\u00e4nder, and S. Weinzierl, \u201cA round robin on room acoustical simulation and auralization,\u201d The Journal of the Acoustical Society of America, vol. 145, no. 4, pp. 2746\u20132760, 2019.\\n\\n[26] E. De Sena, N. Antonello, M. Moonen, and T. Van Waterschoot, \u201cOn the modeling of rectangular geometries in room acoustic simulations,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 4, pp. 774\u2013786, 2015.\"}"}
