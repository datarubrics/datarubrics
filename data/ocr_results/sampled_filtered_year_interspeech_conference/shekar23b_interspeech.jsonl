{"id": "shekar23b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer\\n\\nRam C.M.C Shekar\u00b9, Mu Yang\u00b9, Kevin Hirschi\u00b2, Stephen Looney\u00b3, Okim Kang\u00b2, John Hansen\u00b9\\n\\n\u00b9Center for Robust Speech Systems (CRSS), University of Texas at Dallas, TX, USA\\n\u00b2Northern Arizona University, AZ, USA, \u00b3Pennsylvania State University, PA, USA\\n\\nAbstract\\n\\nAutomatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners.\\n\\nIndex Terms: Wav2vec 2.0, Transformer, goodness of pronunciation, phoneme, prosody, suprasegmental.\\n\\n1. Introduction\\n\\nRecent advancements in acoustic modeling and automatic speech recognition (ASR) techniques have allowed for the development of CAPT tools. CAPT is aimed at self-directed language learning and automatic mispronunciation detection [1, 2, 3]. This facilitates non-native (L2) speakers to learn foreign-spoken (L1) languages. Most advancement efforts focus on scoring phoneme-level pronunciation quality [4, 5, 6, 7, 8, 9, 10]. The major focus is on providing diagnosis on phonetic-level errors (phoneme substitution, deletion, insertion) [11, 12, 13, 14, 15]. Recently, the importance of assessing prosodic-level features (e.g. lexical stress, intonation) has increased substantially [16]. L2 speech is influenced by suprasegmental and temporal differences from their first language, lexical stress and speech rate, hypothesized to account for large variations in L2 speech are believed to play a key role in influencing L2 speech intelligibility [17], which is different from L1 speech intelligibility, typically assessed in the presence of noise [18].\\n\\nInternational Teaching Assistants (ITAs) at North American universities have often faced communication difficulties due to differences in their speech [19, 20]. Two important prosodic components, speech rate [21, 22, 23, 24] and pause units [24, 25, 26] are hypothesized to be related to intelligibility and perceived accentedness [21]. However, [24] suggests that the relationship between speech rate and perception of L2 speech is curvilinear, implying an optimal rate may neither be too high or too low and it may lie in between. Pause patterns, different from L1 speakers, negatively impact perceptions of fluency [25] and ITA effectiveness [26]. Lexical stress deviations also impact several dimensions of listener judgments. L2 speakers' speech rate is typically considered a strong predictor of perceived fluency [22, 23]. However, results from [25] indicate that perception of L2 speech and speech rate is curvilinear in that L2 speech that is spoken too quickly or too slowly inhibits comprehension. Nevertheless, the complex relationship between speech rate, lexical stress, other important prosodic factors, and intelligibility still remains an ongoing research question. In our study, an ITA dataset is prepared by collecting data from international students who serve as teaching assistants (TAs) in North American universities. The dataset is subjectively rated for intelligibility and accentedness. Most existing approaches in L2 speech analysis focus on assessing pronunciation. Pronunciation quality can be modeled at multiple levels: phonetic, word, and utterance, and multiple factors such as prosody, stress, lexical stress, etc. These factors are typically modeled separately [27, 28, 29, 30, 31, 32]. However, many multi-level scores on phonetic, word, and utterance-level features can be correlated. Recent advancements in machine learning have allowed us to learn a more comprehensive representation. In our task, a transformer-based model that is trained on multiple aspects of pronunciation simultaneously is leveraged to study the relationship between L2 intelligibility and multi-level aspects of pronunciation. Furthermore, in this study, we also consider both human transcripts and transcripts generated by ASR, in order to (i) simulate conditions that exist in several instantaneous assessment tools, and (ii) assess the robustness of our L2 speech intelligibility assessment framework. Additionally, the phonetic-level MDD solution is also leveraged to study the relationship between L2 speech intelligibility and the lowest-level phonetic error characteristics.\\n\\n2. Related work\\n\\n2.1. GOPT Overview\\n\\nConventional methods like GOP [4, 5, 7, 8, 10] have been extensively studied. Recent advancements in transformer-based models have enabled advanced GOP-based approaches that can effectively capture the relationship between phonemes and...\"}"}
{"id": "shekar23b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Assessment of L2 Speech Intelligibility using GOPT and Wav2Vec2.0 models.\\n\\nDevelopment of Self-attention-based models [32, 33] resulted in GOPT (Goodness of Pronunciation Transformer), which is a recently developed pronunciation assessment model that is based on Goodness of Pronunciation (GOP) features and a Transformer self-attention architecture [34]. Multi-level labels (one phoneme-level, three word-level, and five utterance-level accuracy, prosody, and fluency) are used to achieve multi-aspect and multi-grained supervision for GOPT training. GOPT is a state-of-the-art model that jointly predicts multi-aspect scores of pronunciation assessment with different granularities. This contrasts with several conventional approaches which also independently incorporate different aspects of pronunciation markers. GOPT learns the correlation between utterance-level tokens and phoneme-level tokens through the attention mechanism.\\n\\n2.2. Wav2vec2.0 Overview\\n\\nWav2vec 2.0 [35] is a pre-trained method that uses a feature encoder, a context network, and a quantization module, to learn a contrastive learning objective. Wav2vec 2.0 has achieved state-of-the-art results on phone recognition for CAPT. Wav2vec 2.0's latent representation also renders rich phonetic information [36].\\n\\n3. Experimental Setup\\n\\nIn this work, we make use of the international teaching assistance (ITA) dataset and leverage state-of-the-art GOP-based Transformer and Wav2vec 2.0 for characterizing utterance, word and phoneme level pronunciation. The proposed framework for the assessment of L2 intelligibility using human/machine annotations and multi-level pronunciation properties is shown in Fig. 1.\\n\\n### 3.1. ITA Dataset\\n\\nFor this study, 54 adult learners, with diverse L1 backgrounds and English-speaking abilities, produced a total of 76 conversational in-class speech recordings, with an average length of 6.06 minutes. The ITA recordings can be broadly categorized based on the level of non-native speech proficiency and intelligibility: (i) 'L2' (Low intelligibility) and (ii) 'L2-High' (High intelligibility). Along with the speech utterances, some parts of ITA recordings are supplemented by both human and machine annotations. Human annotations include transcripts referred to as human speech recognition (HSR) and prosodic annotations, whereas at the machine level ASR transcripts are generated. 'L2-High' has HSR transcripts available whereas some portions of the 'L2' speech has both HSR transcripts and prosody annotations. Only a few utterances have only HSR transcripts. Whisper-ASR [37] is used to obtain ASR transcription for utterances that have missing HSR transcripts. 'L2' level has several combinations of supplementary human and/or machine annotations for every portion of the ITA recordings. The number of speakers and utterances for every portion of the ITA recordings as described in Table 1.\\n\\n| Annotation/Scoring | # Speakers | # Utterances |\\n|--------------------|------------|-------------|\\n| Human              | 15         | 887         |\\n| Machine            | 15         | 5012        |\\n| HSR                | -          | 15          |\\n| ASR                | 57         | 4839        |\\n| HSR+ Prosody       | -          | 57          |\\n| Prosody ASR        | 57         | 4839        |\\n| L2-High            | 19         | 438         |\\n\\n### 3.1.1. Human labeled Transcripts and Prosody Ratings\\n\\nFifteen trained linguists rated the speech based on accentedness and intelligibility. Speech rate, measured in syllables per second, silent pause and filled pause durations were recorded and omitted from syllable count. Additionally, lexical stress scores were calibrated using polysyllabic words extracted from speech event transcripts scored against the CMU Pronouncing Dictionary [38]. A lexical stress score indicates the duration, intensity, or pitch emphasis of the stressed syllable. Furthermore, the accentedness and intelligibility of the speakers were also rated by the linguistic listener pool.\\n\\n### 3.1.2. Whisper ASR Transcripts Generation\\n\\nAlthough the ITA dataset is nearly labeled and carefully compiled, transcribers have generally reported difficulty in reliably deciphering the content. Therefore, additionally Whisper ASR network [37] is considered to provide auxiliary transcription, and assess the robustness of intelligibility scoring. Whisper [37] performs end-to-end ASR on 30-second audio chunks. The Whisper architecture is based on an encoder-decoder Transformer.\\n\\n### 3.2. Acoustic Model and GOP Feature Extraction\\n\\nGoodness-of-pronunciation (GOP), an early DNN-based method proposed to characterize MDD, focuses on evaluating\"}"}
{"id": "shekar23b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"phonetic errors. In our study, a Kaldi-based ASR acoustic\\nmodel, which is based on the factorized time-delay neural\\nnetwork (TDNN-F), is trained with Librispeech [39]. GOP fea-\\ntures for the ITA dataset are extracted using Kaldi Librispeech\\nS5 recipe. GOP extraction involves processing the audio\\nsample along with its corresponding canonical transcription\\nwithin the acoustic module to obtain a sequence of frame-level\\nphonetic posterior probabilities, these are then force-aligned at\\nthe phoneme level and converted to 84-dimensional goodness\\nof pronunciation (GOP) features.\\n\\n3.3. GOPT Inference and Scoring\\nThe GOPT is trained using the 84-dim GOP feature as in-\\nput. GOPT is trained on Speechocean762 [40], a free open-\\nsource dataset designed for pronunciation assessment. Spee-\\nchocean762 is multi-labeled and provides rich label informa-\\ntion at every level. During GOPT training, the mean squared\\nerror (MSE) as the loss is computed at the utterance, word and\\nphoneme levels and averaged.\\n\\n3.4. MDD using Wav2vec 2.0\\nAlong with the GOPT, the MDD evaluation is considered an\\nauxiliary machine annotation/score, used for the characteriza-\\ntion of the reliability of the pronounced phonemes and aspects\\nof L2 speech. More recently, MDD has been achieved via end-\\nto-end phoneme recognition [41]. The implemented model ap-\\npplies the momentum pseudo labeling technique to Wav2vec 2.0\\nfine-tuning, to leverage the unlabeled L2 speech for improved\\nphoneme recognition performance [41]. MDD information is\\ncomputed using the phoneme scorer (JiWER) that interprets the\\nphoneme predictions from Wav2Vec2.0 [42].\\n\\n4. Results\\nAll the utterances are transcripted using either HSR or ASR\\ntechniques. All the utterances from the ITA-Recordings, includ-\\ning both 'L2' and 'L2-High' portions, are assessed using the\\nmulti-granular and multi-level GOPT. Now, some portions may\\nhave additional prosodic annotations, and MDD information.\\nIn summary, for every utterance of the ITA dataset, GOPT pro-\\nduces five utterance-level scores (accuracy, fluency, complete-\\nness, prosody, and total score), three world-level scores (accu-\\nracy, stress, and total score) and a phoneme level score. Addi-\\ntionally, annotations/scores are available based on the portions\\nin ITA-Recording. The prosodic annotations are: (i) articulation\\nrate, (ii) lexical stress score, and (iii) silent pause duration and\\nMDD information is: (i) PER: phoneme error rate (ii) MER:\\nmatch error rate and (iii) IL: information loss. The intelligibil-\\nity scores are characterized by (i) intelligibility at phrase level,\\n(ii) intelligibility as a total number of words understood, and\\n(iii) average intelligibility for the entire duration.\\n\\n4.1. Multicollinearity Test and Regression Analysis\\nThe ITA dataset was tested for multicollinearity and it showed\\nthat there was no significant correlation between the predictors.\\nFor all variations of the ITA dataset, a simple linear regression,\\nrandom forest regressor, and XGBoost regressor were applied\\nand the results are as shown in Table 2. Random forest regres-\\nsor was found to be the best-performing model based on mean\\nsquare error and R2 metrics as highlighted in Table 2.\\n\\n| Annotation/Scoring | Regression | MSE    | R2     |\\n|--------------------|------------|--------|--------|\\n| Human              | Linear     | 0.0761 | 0.0213 |\\n|                    | Random Forest | 5.24E-09 | 1.0000 |\\n|                    | XGBoost    | 6.46E-08 | 1.0000 |\\n| L2 ASR             | Linear     | 0.0547 | 0.0352 |\\n|                    | Random Forest | 1.88E-07 | 1.0000 |\\n|                    | XGBoost    | 6.78E-05 | 0.9987 |\\n| HSR+ Prosody       | Linear     | 0.0472 | 0.1384 |\\n|                    | Random Forest | 8.28E-10 | 1.0000 |\\n|                    | XGBoost    | 1.02E-06 | 1.0000 |\\n| Prosody ASR        | Linear     | 0.031357 | 0.4680 |\\n|                    | Random Forest | 9.30E-29 | 1.0000 |\\n|                    | XGBoost    | 2.77E-09 | 1.0000 |\\n| ASR + MDD Prosody  | Linear     | 0.046829 | 0.1708 |\\n|                    | Random Forest | 1.15E-08 | 1.0000 |\\n|                    | XGBoost    | 2.18E-07 | 1.0000 |\\n| Prosody ASR + MDD  | Linear     | 0.0249 | 0.5772 |\\n|                    | Random Forest | 9.22E-29 | 1.0000 |\\n|                    | XGBoost    | 5.50E-10 | 1.0000 |\\n| L2-High HSR        | Linear     | 0.0761 | 0.0210 |\\n|                    | Random Forest | 3.18E-10 | 1.0000 |\\n|                    | XGBoost    | 6.07E-08 | 0.9999 |\\n\\n4.2. Characterization of L2 Intelligibility using Feature Im-\\nportance\\nRandom forest regressor was found to be the best-performing\\nmodel and they have been used for assessing feature impor-\\ntance. The feature importance results for all combinations of\\nannotations/scoring for the ITA dataset are described in Fig. 2.\\nIn Fig. 2 A), a comparative analysis of L2 intelligibility pre-\\ndictors for 'L2-High:HSR', 'L2:HSR', and 'L2:ASR' config-\\nurations reveal that only utterance level scores of GOPT are\\nsignificant and other word level and phonetic predictors are\\nnot dominant predictors. Fluency is the top predictor for 'L2-\\nHigh:HSR', prosody is the top predictor for 'L2:HSR' and in-\\nterestingly, accuracy is the top predictor for 'L2:ASR' con-\\nfiguration. In Fig. 2 B), four configurations for L2 speech,\\nnamely: 'HSR + Prosody', 'ASR + Prosody', 'ASR + MDD',\\n'ASR + Prosody + MDD' are considered for comparative anal-\\nysis of non-native speech intelligibility predictors. As earlier,\\namong GOPT scores, only utterance level scores are signifi-\\ncant and comparing 'HSR + Prosody' based model with 'ASR\\n+ Prosody' suggests that human-labeled prosodic predictors\\nare significant for L2 intelligibility. Among GOPT utterance\\nscores, only prosody and total scores contribute towards pre-\\ndicting L2 intelligibility for 'HSR + Prosody' configuration and\\nWhisper-ASR generated transcripts seems to degrade the re-\\nliability of GOPT-generated utterance-level scores. Also, the\\nimportant prosodic features such as 'articulation rate', 'lexical\\nstress' and 'silent pause duration' remain the most important\\nfeatures which influence human labeled intelligibility scores.\\nAmong 'ASR + MDD' and 'ASR + MDD + Prosody' con-\\nfigurations, human prosodic information seems to play a very\\ndominant role in predicting L2 intelligibility. In the absence of\\nany human labels and relying on Whisper-ASR generated tran-\\nscripts and the MDD information, namely: information loss,\\nphoneme error rate and match error rates were significantly in-\\nfluential for predicting L2 intelligibility characteristics. Addi-\\ntionally, all utterance-level GOPT predictors also contributed\\ntowards predicting L2 intelligibility. However, we observe that\\nbetween GOPT and MDD scores, the phoneme-level MDD\\nscores played a dominant role in influencing L2 intelligibility.\"}"}
{"id": "shekar23b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Discussion\\n\\nPreviously it has been found that lexical stress and speech rate independently relate to L2 speech intelligibility. In our assessment, prosodic features were found to outweigh the multi-level granular features of pronunciation significantly, especially when human-labeled prosodic features were available. Among all scores, only the utterance level scores, specifically prosody, completeness, total score, and others were dominant in the absence of any prosodic features. Furthermore, a comparative analysis of phoneme-level MDD diagnostic features was found to be more influential than GOPT scores, despite the availability of multi-level scoring. Analysis of features showed interesting aspects of L2 intelligibility: (i) Human or ASR transcription does not significantly influence the factors that impact L2 speech intelligibility; (ii) Apart from transcription, human-rated prosodic factors are dominant in influencing L2 intelligibility; (iii) MDD is still a major factor for assessing L2 intelligibility; (iv) Among automatic/machine learning based predictors, lower phonetic level features are more important compared to multi-level GOPT features, however, human transcribed prosodic features are more relevant compared to lower level phonetic transcription.\\n\\n6. Conclusion\\n\\nThis study has considered a framework to assess L2 speech intelligibility by considering several aspects of pronunciation: (i) lower phonetic level pronunciation-based MDD solution; (ii) Multi-level granular pronunciation assessment tool; (iii) Transcription robustness: Human vs ASR; (iv) Human rated prosodic labels. Most existing approaches use MDD solutions for characterizing L2 pronunciation. However, our study shows that an automatic assessment of L2 speech intelligibility can be carried out reliably, irrespective of human or automatic speech transcription. Human-rated prosodic predictors are the dominant factors in the assessment of L2 speech intelligibility. Also, there is no significant difference in the dominant predictors for L2 speech intelligibility when HSR transcripts are compared to ASR generate versions. However, MDD predictors do become highly dominant in characterizing L2 speech intelligibility. Furthermore, MDD features are still dominant in the presence of multi-level GOPT features. The findings from this analysis provide direction for the development of robust systems for characterizing L2 speech intelligibility. Furthermore, with the increasing demand for instantaneous L2 speech assessment tools, the results of this experimental framework underscore the need to develop more accurate tools to estimate human-rated prosodic features and MDD solutions.\\n\\n7. Acknowledgements\\n\\nThis study is supported by NSF EAGER CISE Project 2140415, and partially by the University of Texas at Dallas from the Distinguished University Chair in Telecommunications Engineering held by J. H. L. Hansen.\"}"}
{"id": "shekar23b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] M. Eskenazi, \u201cAn overview of spoken language technology for education,\u201d Speech Communication, 2009.\\n\\n[2] K. Zechner, D. Higgins, X. Xi et al., \u201cAutomatic scoring of non-native spontaneous speech in tests of spoken English,\u201d Speech Communication, 2009.\\n\\n[3] S. M. Witt, \u201cAutomatic error detection in pronunciation training: Where we are and where we need to go,\u201d in International Symposium on Automatic Detection of Errors in Pronunciation Training, 2012.\\n\\n[4] S. M. Witt and S. J. Young, \u201cPhone-level pronunciation scoring and assessment for interactive language learning,\u201d Speech Communication, 2000.\\n\\n[5] F. Zhang, C. Huang, F. K. Soong et al., \u201cAutomatic mispronunciation detection for Mandarin,\u201d in Proc. ICASSP, 2008.\\n\\n[6] D. Luo, Y. Qiao, N. Minematsu et al., \u201cAnalysis and utilization of multilingual speaker adaptation technique for learners\u2019 pronunciation evaluation,\u201d in Proc. Interspeech, 2009.\\n\\n[7] Y.-B. Wang and L.-S. Lee, \u201cImproved approaches of modeling and detecting error patterns with empirical analysis for computer-aided pronunciation training,\u201d in Proc. ICASSP, 2012.\\n\\n[8] W. Hu, Y. Qian, F. K. Soong et al., \u201cImproved mispronunciation detection with deep neural network trained acoustic models and transfer learning based logistic regression classifiers,\u201d Speech Communication, 2015.\\n\\n[9] J. Shi, N. Huo, and Q. Jin, \u201cContext-aware goodness of pronunciation for computer-assisted pronunciation training,\u201d arXiv preprint arXiv:2008.08647, 2020.\\n\\n[10] J. van Doremalen, C. Cucchiarini, and H. Strik, \u201cUsing non-native error patterns to improve pronunciation verification,\u201d in Proc. Interspeech, 2010.\\n\\n[11] S. Sudhakara, M. K. Ramanathi, C. Yarra et al., \u201cAn improved goodness of pronunciation (gop) measure for pronunciation evaluation with DNN-HMM system considering HMM transition probabilities,\u201d in Proc. Interspeech, 2019.\\n\\n[12] W. Li, S. M. Siniscalchi, N. F. Chen et al., \u201cImproving non-native mispronunciation detection and enriching diagnostic feedback with DNN-based speech attribute modeling,\u201d in Proc. ICASSP, 2016.\\n\\n[13] M. Wu, K. Li, W.-K. Leung et al., \u201cTransformer-based end-to-end mispronunciation detection and diagnosis,\u201d in Proc. Interspeech, 2021.\\n\\n[14] L. Peng, K. Fu, B. Lin et al., \u201cA study on fine-tuning wav2vec2.0 model for the task of mispronunciation detection and diagnosis,\u201d in Proc. Interspeech, 2021.\\n\\n[15] Y. Feng, G. Fu, Q. Chen et al., \u201cSed-mdd: Towards sentence-dependent end-to-end mispronunciation detection and diagnosis,\u201d in Proc. ICASSP, 2020.\\n\\n[16] D. Korzekwa, R. Barra-Chicote, S. Zaporowski et al., \u201cDetection of Lexical Stress Errors in Non-Native (L2) English with Data Augmentation and Attention,\u201d in Proc. Interspeech, 2021.\\n\\n[17] O. Kang, \u201cLinguistic analysis of speaking features distinguishing general English exams at CEFR levels,\u201d Research Notes, 2013.\\n\\n[18] N. Mamun, M. S. Zilany, J. H. L. Hansen et al., \u201cAn intrusive method for estimating speech intelligibility from noisy and distorted signals,\u201d The Journal of the Acoustical Society of America, 2021.\\n\\n[19] M. Inglis, \u201cThe communicator style measure applied to non-native speaking teaching assistants,\u201d International Journal of Intercultural Relations, 1993.\\n\\n[20] J. M. Levis, \u201cChanging contexts and shifting paradigms in pronunciation teaching,\u201d TESOL Quarterly, 2005.\\n\\n[21] O. Kang, R. I. Thomson, and M. Moran, \u201cEmpirical approaches to measuring the intelligibility of different varieties of English in predicting listener comprehension,\u201d Language Learning, 2018.\\n\\n[22] P. Trofimovich and W. Baker, \u201cLearning second language suprasegmentals: Effect of L2 experience on prosody and fluency characteristics of L2 speech,\u201d Studies in Second Language Acquisition, 2006.\\n\\n[23] M. J. Munro and T. M. Derwing, \u201cThe effects of speaking rate on the comprehensibility of native and foreign-accented speech,\u201d Canadian Acoustics, 1996.\\n\\n[24] P. Tavakoli, \u201cPausing patterns: Differences between L2 learners and native speakers,\u201d ELT Journal, 2011.\\n\\n[25] J. Kahng, \u201cThe effect of pause location on perceived fluency,\u201d Applied Psycholinguistics, 2018.\\n\\n[26] A. E. Tyler, A. A. Jefferies, and C. E. Davies, \u201cThe effect of discourse structuring devices on listener perceptions of coherence in non-native university teacher\u2019s spoken discourse,\u201d World Englishes, 1988.\\n\\n[27] C. Cucchiarini, H. Strik, and L. Boves, \u201cQuantitative assessment of second language learners\u2019 fluency by means of automatic speech recognition technology,\u201d The Journal of the Acoustical Society of America, 2000.\\n\\n[28] P. C. Bagshaw, \u201cAutomatic prosodic analysis for computer-aided pronunciation teaching,\u201d Ph.D. dissertation, University of Edinburgh PhD thesis, 1994.\\n\\n[29] J. Tepperman and S. Narayanan, \u201cAutomatic syllable stress detection using prosodic features for pronunciation evaluation of language learners,\u201d in Proc. ICASSP, 2005.\\n\\n[30] J. P. Arias, N. B. Yoma, and H. Vivanco, \u201cAutomatic intonation assessment for computer-aided language learning,\u201d Speech Communication, 2010.\\n\\n[31] K. Li, X. Wu, and H. Meng, \u201cIntonation classification for L2 English speech using multi-distribution deep neural networks,\u201d Computer Speech & Language, 2017.\\n\\n[32] A. Vaswani, N. Shazeer, N. Parmar et al., \u201cAttention is all you need,\u201d Proc. NeurIPS, 2017.\\n\\n[33] B. Lin, L. Wang, X. Feng et al., \u201cAutomatic scoring at multi-granularity for L2 pronunciation,\u201d in Proc. Interspeech, 2020.\\n\\n[34] Y. Gong, Z. Chen, I.-H. Chu et al., \u201cTransformer-based multi-aspect multi-granularity non-native English speaker pronunciation assessment,\u201d in Proc. ICASSP, 2022.\\n\\n[35] A. Baevski, Y. Zhou, A. Mohamed et al., \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Proc. NeurIPS, 2020.\\n\\n[36] T.-A. Hsieh, C. Yu, S.-W. Fu et al., \u201cImproving Perceptual Quality by Phone-Fortified Perceptual Loss Using Wasserstein Distance for Speech Enhancement,\u201d in Proc. Interspeech, 2021.\\n\\n[37] A. Radford, J. W. Kim, T. Xu et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\\n\\n[38] J. Kominek and A. W. Black, \u201cThe CMU ARCTIC speech databases,\u201d in Fifth ISCA Workshop on Speech Synthesis, 2004.\\n\\n[39] V. Panayotov, G. Chen, D. Povey et al., in Proc. ICASSP, 2015.\\n\\n[40] J. Zhang, Z. Zhang, Y. Wang et al., \u201cSPEECOCEAN762: An open-source non-native English speech corpus for pronunciation assessment,\u201d in Proc. Interspeech, 2021.\\n\\n[41] M. Yang, K. Hirschi, S. D. Looney et al., \u201cImproving Mispronunciation Detection with Wav2vec2-based Momentum Pseudo-Labeling for Accentedness and Intelligibility Assessment,\u201d in Proc. Interspeech, 2022.\\n\\n[42] A. C. Morris, V. Maier, and P. Green, \u201cFrom WER and RIL to MER and WIL: Improved evaluation measures for connected speech recognition,\u201d in Proc. Interspeech, 2004.\"}"}
