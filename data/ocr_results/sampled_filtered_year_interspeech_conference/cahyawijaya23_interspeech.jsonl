{"id": "cahyawijaya23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] R. Beale and C. Peter, \u201cThe role of affect and emotion in HCI,\u201d in Affect and Emotion in Human-Computer Interaction. Springer Berlin Heidelberg, 2008, pp. 1\u201311.\\n\\n[2] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz, and J. Taylor, \u201cEmotion recognition in human-computer interaction,\u201d IEEE Signal Processing Magazine, vol. 18, no. 1, pp. 32\u201380, 2001.\\n\\n[3] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIEMOCAP: interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, no. 4, pp. 335\u2013359, Nov. 2008.\\n\\n[4] S. R. Livingstone and F. A. Russo, \u201cThe ryerson audio-visual database of emotional speech and song (RAVDESS): A dynamic, multimodal set of facial and vocal expressions in north american english,\u201d PLOS ONE, vol. 13, no. 5, May 2018.\\n\\n[5] A. Zadeh, P. P. Liang, S. Poria, P. Vij, E. Cambria, and L.-P. Morency, \u201cMulti-attention recurrent network for human communication comprehension,\u201d in 32nd AAAI, 2018.\\n\\n[6] S. Poria, D. Hazarika, N. Majumder, G. Naik, E. Cambria, and R. Mihalcea, \u201cMELD: A multimodal multi-party dataset for emotion recognition in conversations,\u201d in Proc. 57th ACL. ACL, Jul. 2019, pp. 527\u2013536.\\n\\n[7] E. Kim, D. Bryant, D. Srikanth, and A. Howard, \u201cAge bias in emotion detection: An analysis of facial emotion recognition performance on young, middle-aged, and older adults,\u201d in Proc. 2021 AAAI/ACM Conference on AI, Ethics, and Society, ser. AIES \u201921. New York, NY , USA: Association for Computing Machinery, 2021, p. 638\u2013644.\\n\\n[8] D. Rouzet, A. C. Sanchez, T. Renault, and O. Roehn, \u201cFiscal challenges and inclusive growth in ageing societies,\u201d no. 27, 2019.\\n\\n[9] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma, \u201cCREMA-d: Crowd-sourced emotional multimodal actors dataset,\u201d IEEE Transactions on Affective Computing, vol. 5, no. 4, pp. 377\u2013390, Oct. 2014.\\n\\n[10] K. Ma, X. Wang, X. Yang, M. Zhang, J. M. Girard, and L.-P. Morency, \u201cElderreact: A multimodal dataset for recognizing emotional response in aging adults,\u201d in 2019 International Conference on Multimodal Interaction, ser. ICMI \u201919. New York, NY , USA: Association for Computing Machinery, 2019, p. 349\u2013357.\\n\\n[11] M. K. Pichora-Fuller and K. Dupuis, \u201cToronto emotional speech set (TESS),\u201d 2020.\\n\\n[12] M. C. Lee, S. C. Yeh, S. Y. Chiu, and J. W. Chang, \u201cA deep convolutional neural network based virtual elderly companion agent,\u201d in Proceedings of the 8th ACM on Multimedia Systems Conference. ACM, Jun. 2017.\\n\\n[13] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cWav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS\u201920. Red Hook, NY , USA: Curran Associates Inc., 2020.\\n\\n[14] J. Martinez-Miranda and A. Aldea, \u201cEmotions in human and artificial intelligence,\u201d Computers in Human Behavior, vol. 21, no. 2, pp. 323\u2013341, Mar. 2005.\\n\\n[15] D. Bertero and P. Fung, \u201cMultimodal deep neural nets for detecting humor in TV sitcoms,\u201d in 2016 IEEE SLT Workshop, 2016.\\n\\n[16] L. Cen, M. Dong, H. L. Z. L. Yu, and P. Ch, \u201cMachine learning methods in the application of speech emotion recognition,\u201d in Application of Machine Learning. InTech, Feb. 2010.\\n\\n[17] D. Bertero and P. Fung, \u201cA first look into a convolutional neural network for speech emotion detection,\u201d in IEEE ICASSP, 2017.\\n\\n[18] W. Dai, S. Cahyawijaya, Z. Liu, and P. Fung, \u201cMultimodal end-to-end sparse model for emotion recognition,\u201d in Proceedings of the NAACL-HLT 2021. Online: ACL, Jun. 2021, pp. 5305\u20135316.\\n\\n[19] S. Latif, H. S. Ali, M. Usama, R. Rana, B. Schuller, and J. Qadir, \u201cAI-based emotion recognition: Promise, peril, and prescriptions for prosocial path,\u201d 2022.\\n\\n[20] S. Hareli, K. Kafetsios, and U. Hess, \u201cA cross-cultural study on emotion expression and the learning of social norms,\u201d Frontiers in Psychology, vol. 6, Oct. 2015.\\n\\n[21] N. Lim, \u201cCultural differences in emotion: differences in emotional arousal level between the east and the west,\u201d Integrative Medicine Research, vol. 5, no. 2, pp. 105\u2013109, Jun. 2016.\\n\\n[22] I. Iosifov, O. Iosifova, O. Romanovskyi, V. Sokolov, and I. Sukailo, \u201cTransferability evaluation of speech emotion recognition between different languages,\u201d in Advances in Computer Science for Engineering and Education. Springer International Publishing, 2022, pp. 413\u2013426.\\n\\n[23] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d 2021.\\n\\n[24] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,\u201d in Proc. Interspeech 2022, 2022.\\n\\n[25] Z. Fan, M. Li, S. Zhou, and B. Xu, \u201cExploring wav2vec 2.0 on Speaker Verification and Language Identification,\u201d in Proc. Interspeech 2021, 2021, pp. 1509\u20131513.\\n\\n[26] L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion Recognition from Speech Using wav2vec 2.0 Embeddings,\u201d in Proc. Interspeech 2021, 2021, pp. 3400\u20133404.\\n\\n[27] H. Lovenia, S. Cahyawijaya, G. Winata, P. Xu, Y. Xu, Z. Liu, R. Frieske, T. Yu, W. Dai, E. J. Barezi, Q. Chen, X. Ma, B. Shi, and P. Fung, \u201cASCEND: A spontaneous Chinese-English dataset for code-switching in multi-turn conversation,\u201d in Proc. 13th LREC. ELRA, Jun. 2022, pp. 7259\u20137268.\\n\\n[28] S. Cahyawijaya, H. Lovenia, A. F. Aji, G. I. Winata, B. Wilie, R. Mahendra, C. Wibisono, A. Romadhony, K. Vincentio, F. Koto, J. Santoso, D. Moeljadi et al., \u201cNusacrowd: Open source initiative for Indonesian NLP resources,\u201d 2022.\\n\\n[29] W. Dai, S. Cahyawijaya, T. Yu, E. J. Barezi, P. Xu, C. T. Yiu, R. Frieske, H. Lovenia, G. Winata, Q. Chen, X. Ma, B. Shi, and P. Fung, \u201cCI-AVSR: A Cantonese audio-visual speech dataset for in-car command recognition,\u201d in Proc. 13th LREC. ELRA, 2022.\\n\\n[30] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cEmotional voice conversion: Theory, databases and Esd,\u201d Speech Communication, vol. 137, pp. 1\u201318, 2022.\\n\\n[31] G. I. Winata, S. Cahyawijaya, Z. Liu, Z. Lin, A. Madotto, P. Xu, and P. Fung, \u201cLearning Fast Adaptation on Cross-Accented Speech Recognition,\u201d in Proc. Interspeech 2020, 2020.\\n\\n[32] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson, \u201cXTREME: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation,\u201d in Proc. 37th ICML, ser. Proceedings of Machine Learning Research, H. D. III and A. Singh, Eds., vol. 119. PMLR, 13\u201318 Jul 2020.\\n\\n[33] G. I. Winata, A. F. Aji, S. Cahyawijaya, R. Mahendra, F. Koto, A. Romadhony, K. Kurniawan, D. Moeljadi, R. E. Prasojo, P. Fung, T. Baldwin, J. H. Lau, R. Sennrich, and S. Ruder, \u201cNusax: Multilingual parallel sentiment dataset for 10 Indonesian local languages,\u201d 2022.\\n\\n[34] W. Dai, S. Cahyawijaya, T. Yu, E. J. Barezi, P. Xu, C. T. Yiu, R. Frieske, H. Lovenia, G. Winata, Q. Chen, X. Ma, B. Shi, and P. Fung, \u201cCI-AVSR: A Cantonese audio-visual speech dataset for in-car command recognition,\u201d in Proc. 13th LREC. ELRA, 2022.\\n\\n[35] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cEmotional voice conversion: Theory, databases and Esd,\u201d Speech Communication, vol. 137, pp. 1\u201318, 2022.\\n\\n[36] G. I. Winata, A. F. Aji, S. Cahyawijaya, R. Mahendra, F. Koto, A. Romadhony, K. Kurniawan, D. Moeljadi, R. E. Prasojo, P. Fung, T. Baldwin, J. H. Lau, R. Sennrich, and S. Ruder, \u201cNusax: Multilingual parallel sentiment dataset for 10 Indonesian local languages,\u201d 2022.\\n\\n[37] S.-W. Huang, C.-T. Lin, S.-P. Chen, Y.-Y. Wu, P.-H. Hsu, and S.-H. Lai, \u201cAuggan: Cross domain adaptation with gan-based data augmentation,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\n[38] J. Singh, B. McCann, N. S. Keskar, C. Xiong, and R. Socher, \u201cXlda: Cross-lingual data augmentation for natural language inference and question answering,\u201d 2019.\\n\\n[39] L. McInnes, J. Healy, N. Saul, and L. Grossberger, \u201cUmap: Uniform manifold approximation and projection,\u201d The Journal of Open Source Software, vol. 3, no. 29, p. 861, 2018.\\n\\n[40] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in ICLR, 2019.\\n\\n[41] C. Fu, T. Dissanayake, K. Hosoda, T. Maekawa, and H. Ishiguro, \u201cSimilarity of speech emotion in different languages revealed by a neural network with attention,\u201d in 2020 IEEE 14th International Conference on Semantic Computing (ICSC). IEEE, Feb. 2020.\"}"}
{"id": "cahyawijaya23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Cross-Lingual Cross-Age Group Adaptation\\nfor Low-Resource Elderly Speech Emotion Recognition\\nSamuel Cahyawijaya\u2217, Holy Lovenia\u2217, Willy Chung\u2217, Rita Frieske, Zihan Liu, Pascale Fung\\n\\nThe Hong Kong University of Science and Technology\\n{scahyawijaya, hlovenia, whcchung}@connect.ust.hk\\n\\nAbstract\\nSpeech emotion recognition plays a crucial role in human-computer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages\u2014English, Mandarin Chinese, and Cantonese; and 2 different age groups\u2014adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability. We release publicly release our code at https://github.com/HLTCHKUST/elderly_ser.\\n\\nIndex Terms: speech emotion recognition, cross-lingual adaptation, cross-age adaptation, elderly, low-resource language\\n\\n1. Introduction\\nUnderstanding human emotion is a crucial step towards better human-computer interaction [1, 2]. Most studies on speech emotion recognition are centered on young-adult people, mainly originating from English-speaking countries [3, 4, 5, 6]. This demographic bias causes existing commercial emotion recognition systems to inaccurately perceive emotion in the elderly [7]. Despite the fast-growing elderly demographic in many countries [8], only a few studies with a fairly limited amount of data work on emotion recognition for elderly [9, 10, 11], especially from non-English-speaking countries [12].\\n\\nTo cope with the limited resource problem in the elderly emotion recognition, in this work, we study the prospect of transferring emotion recognition ability over various age groups and languages through the utilization of multilingual pre-trained speech models, e.g., Wav2Vec 2.0 [13]. Specifically, we aim to understand the transferability of emotion recognition ability using only speech modality between two languages and two age groups, i.e., English-speaking adults, English-speaking elderly, Mandarin-speaking adults, and Mandarin-speaking elderly. To do this, we develop BiMotion, a bi-lingual bi-age-group speech emotion recognition benchmark that covers 6 adult and elderly speech emotion recognition datasets from English and Mandarin Chinese. Additionally, we analyze the effect of language distance on the transferability of emotion recognition ability from Mandarin and English using YueMotion, a newly-constructed Cantonese speech emotion recognition dataset.\\n\\nWe analyze the transferability of emotion recognition ability in three ways, i.e., 1) cross-group inference using only the source group data for training to better understand the features distinction between each group, 2) cross-group data augmentation to better understand the transferability across different groups, and 3) feature-space projection to better understand the effect of transferability across different language distance.\\n\\nThrough a series of experiments, we conclude that:\\n\u2022 Very distinct speech features are needed for recognizing emotion across different language and age groups, which makes cross-lingual inference an ill-suited method for transferring speech emotion recognition ability.\\n\u2022 Despite the different important speech features across groups, data augmentation across different groups is still beneficial as it helps the model to better regularize yielding higher evaluation performance.\\n\u2022 Language distance plays a crucial role in cross-lingual transferability, as more similar languages tend to share more similar speech characteristics, thus allowing better generalization to the target language.\\n\\n2. Related Work\\n2.1. Emotion Recognition\\nVarious efforts have explored emotion recognition solutions for around two decades ago through different modalities [14, 15,\"}"}
{"id": "cahyawijaya23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of datasets covered in BiMotion.\\n\\n| Dataset       | Language | Age Group | #Train | #Valid | #Test |\\n|---------------|----------|-----------|--------|--------|-------|\\n| CREMA-D       | English  | Elderly   | 150    | 42     | 300   |\\n|               |          | Adults    | 5000   | 750    | 1200  |\\n| CSED          | Mandarin | Elderly   | 200    | 52     | 400   |\\n| ElderReact    | English  | Elderly   | 615    | 355    | 353   |\\n| ESD           | Mandarin | Adults    | 15000  | 1000   | 1500  |\\n|               | English  | Adults    | 15000  | 1000   | 1500  |\\n| IEMOCAP       | English  | Adults    | 7500   | 1039   | 1500  |\\n| TESS          | English  | Elderly   | 699    | 200    | 500   |\\n|               |          | Adults    | 700    | 201    | 500   |\\n\\n16, 17, 18, 19]. Most studies focus on methods that can estimate the subject emotion effectively through a specific modality while some others focus on combining multiple modalities to better estimate the subject emotion. The transferability of emotion recognition across different cultures and language groups has also been previously studied [20, 21, 22] showing that there are significant differences across languages and cultures. Nevertheless, it is only evaluated on a small-scale model that owns limited world knowledge. In this work, we extend this analysis to pre-trained speech models and further explore the transferability to cover an extremely low-resource group, i.e., the elderly in the low-resource language group.\\n\\n2.2. Pre-trained Speech Model\\n\\nLarge pre-trained speech models achieve state-of-the-art performance for various speech tasks in recent years [13, 23, 24]. These models have been trained in an unsupervised manner on large-scale multilingual speech corpora. The representation of these models has been shown to be effective, achieving state-of-the-art performance in various downstream tasks including automatic speech recognition, speaker verification, language identification, and emotion recognition [25, 26, 27, 28, 29]. In this work, we adopt the XLSR Wav2Vec 2.0 model [13, 24], which has been pre-trained on large-scale multilingual speech corpora. The multilingual representation learned by the model will be crucial for evaluating the transferability of emotion recognition ability across different languages.\\n\\n3. Benchmark and Dataset\\n\\n3.1. BiMotion Benchmark\\n\\nTo test the transferability of emotion recognition ability across different languages and age groups, we develop a bilingual speech emotion recognition benchmark namely BiMotion, that covers two languages, i.e., English and Mandarin, and two age groups, i.e., Elderly (age \u2265 60 y.o) and Adults (age 20-59). We construct our benchmark from a collection of six publicly available datasets, i.e., CREMA-D [9], CSED [12], ElderReact [10], ESD [30], IEMOCAP [3], and TESS [11]. For datasets with an official split, i.e., ESD and ElderReact, we use the provided dataset splits. For the other datasets, we generate a new dataset split since there is no official split provided. The statistics of the datasets in BiMotion are shown in Table 1.\\n\\n3.2. YueMotion Dataset\\n\\nTo strengthen our transferability analysis, we further introduce a new speech emotion recognition dataset covering adults in Cantonese named YueMotion. The utterances in YueMotion are recorded by 11 speakers (4 males and 7 females) each with a personal recording device. During the recording session, each speaker is asked to say out loud 10 pre-defined sentences used in daily conversations with 6 different emotions, i.e., happiness, sadness, neutral, disgust, fear, and anger. The YueMotion dataset is used to further verify our hypothesis regarding the transferability of emotion recognition ability. The detailed statistics of YueMotion dataset are shown in Table 2.\\n\\nTable 2: Statistics of the YueMotion dataset.\\n\\n| Age Group | Gender | #Train | #Valid | #Test | #All |\\n|-----------|--------|--------|--------|-------|------|\\n| Adults    | Male   | 120    | 36     | 84    | 240  |\\n|           | Female | 210    | 63     | 147   | 420  |\\n|           | Total  | 330    | 99     | 231   | 660  |\\n\\n4. Methodology\\n\\nWe employ two methods to evaluate the transferability of emotion recognition ability, i.e., cross-group data augmentation and cross-group inference. The detail of each method is explained in the following subsection.\\n\\n4.1. Cross-Group Inference\\n\\nWe define a set of language $L = \\\\{ l_1, l_2, \\\\ldots, l_n \\\\}$ and a set of age group $A = \\\\{ a_1, a_2, \\\\ldots, a_m \\\\}$. We define training and evaluation data from a language $l_i$ and age group $a_j$ as $X_{train}^{l_i,a_j}$ and $X_{test}^{l_i,a_j}$, respectively. To understand the transferability from the source group to the target group, we explore a method called cross-group inference. Following prior works on zero-shot accent/language adaptation [31, 32, 33], we explore cross-lingual and cross-age inference settings in our experiment. Specifically, given a training dataset $X_{train}^{l_i,a_j}$, we select three out-of-group test datasets: 1) datasets that are not in $l_i$ language ($X_{test}^{\\\\neg l_i,a_j}$), 2) datasets that are not in $a_j$ age-group ($X_{test}^{l_i,\\\\neg a_j}$), and 3) datasets that are not in $l_i$ language and $a_j$ age-group $X_{test}^{\\\\neg l_i,\\\\neg a_j}$. We conduct cross-group inference by training the models on a specific group training set and evaluating them on the three out-of-group test sets. With this method, we can analyze the effect of cross-group information on a specific language and age group.\\n\\n4.2. Cross-Group Data Augmentation\\n\\nPrior works [34, 35] have shown that cross-domain and cross-lingual data augmentation method effectively improves the model performance in textual and speech modalities, especially on low-resource languages and domains. We adopt the data augmentation technique to the speech emotion recognition task by utilizing cross-lingual and cross-age data augmentation. Our cross-group data augmentation injects a training dataset $X_{train}^{l_i,a_j}$ from a language group $l_i$ and an age group $a_j$ with data from other language and age groups producing a new augmented dataset. Given $X_{train}^{l_i,a_j}$, there are three different data augmentation settings possible, i.e., cross-lingual data augmentation on the same age group resulting in $X_{train}^{L,a_j}$, cross-age data augmentation on the same language resulting in $X_{train}^{l_i,A}$, and cross-lingual cross-age data augmentation resulting in $X_{train}^{L,A}$. We then fine-tune the model on the augmented dataset and evaluate it on $X_{test}^{l_i,a_j}$, the test set with a language $l_i$ and age group $a_j$. To enhance readability, we use colored underlines to mark the term cross-group data augmentation and cross-group inference onwards.\"}"}
{"id": "cahyawijaya23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Evaluation results of cross-lingual and cross-age groups inference on BiMotion. Cross-all denotes cross-lingual and cross-age groups, while the result for the Mandarin-Elderly, while the result for the Mandarin-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross data in cross-aug data cross-cross in cross-data cross-cross data in cross-cross data in cross-aug data cross-cross data in cross-data cross-cross data in cross-cross data.\\n\\nAs the test set, and explore Cantonese elderly and Cantonese adults data on the YueMotion ferability experiment on the YueMotion dataset. We use the English, Mandarin, and Cantonese.\\n\\nFor our experiment, we utilize the XLSR-53 Wav2Vec 2.0 [13, 24] model which is pre-trained in 53 languages including English. To evaluate the transferability of emotion recognition ability, we conduct an extensive analysis in the BiMotion benchmark.\\n\\nWe explore two different cross-group transfer methods, i.e., cross-group data-augmentation (\u00a74.2) and cross-group inference (\u00a74.1). Specifically, we fine-tune the pre-trained XLSR-53 model using the in-group and the out-of-group cross-group data.\\n\\nGiven a fine-tuned emotion recognition model $\\\\theta$, we extract the speech representation by taking the high-dimensional features before the last linear classification layer. By using weighted $\\\\theta$, we can approximate the effect of dimensional space with UMAP [36] for visualization. With this method, we can analyze the influence of cross-language and age-group.\\n\\nThrough this method, we can analyze the influence of cross-language and age-group augmentation on a specific language and age group. Despite having different characteristics, there are some differences in speech features used to recognize emotion between different languages and different age groups, which makes transfer through cross-group inference suitable for speech emotion recognition.\\n\\nTable 3: Evaluation results on XLinI, where $X_L$ is an abbreviation for the specific language and an age-group. We use the model checkpoint from https://huggingface.co/facebook/wav2vec2-large-xlsr-53\\n\\n| Group          | Mandarin-Elderly | Mandarin-Adults | Cantonese-Elderly | Cantonese-Adults |\\n|----------------|------------------|-----------------|-------------------|------------------|\\n| Baseline       | 74.84 (l=eng)    | 75.47 (l=eng)   | 66.96 (l=zho)     | 67.34 (l=zho)    |\\n| Cross-all      | 97.00 (l=eng)    | 74.13 (l=eng)   | 97.40 (l=zho)     | 97.00 (l=zho)    |\\n| Cross-age      | 70.28 (l=eng)    | 74.13 (l=eng)   | 52.21 (l=zho)     | 54.84 (l=zho)    |\\n\\n5.1. Baseline Models\\n\\nWe use the same hyperparameters in all experiments. For fine-tuning the model we use the following settings with a simple in-group training baseline where given a specific language and age-group.\\n\\n$$X_{trnL,A} \\\\rightarrow X_{test}$$\\n\\nwhere $X_{trnL,A}$ is the training data, i.e., $X_{trnL,A}$ and evaluated on the test set of the corresponding language and age-group. $X_{test}$ is the test data, i.e., $X_{test}$ and $X_{test}$.\\n\\n5.2. Training Settings\\n\\nHyperparameters for fine-tuning the model are set as follows: learning rate of 5e-5, a dropout rate of 0.1, a number of epochs 50, weight decay of 1e-5, batch size 2, and early stopping of 5 epochs.\\n\\n5.3. Evaluation Metrics\\n\\nTo measure the performance of cross-group transferability on YueMotion, we utilize the training datasets from BiMotion and compare the effect of cross-group transferability on YueMotion, cross-group data-augmentation (\u00a74.2) and cross-group inference (\u00a74.1).\\n\\nFor our evaluation, we use the model checkpoint from https://huggingface.co/facebook/wav2vec2-large-xlsr-53\\n\\n6. Results and Discussion\\n\\nBased on our experiments, we utilize the training datasets from BiMotion and compare the effectiveness of the cross-group transferability on YueMotion, cross-group data-augmentation (\u00a74.2) and cross-group inference (\u00a74.1). Specifically, we fine-tune the pre-trained XLSR-53 model using the in-group and the out-of-group cross-group data.\\n\\n6.1. Transferability via Cross-Group Inference\\n\\nTo further strengthen our analysis, we conduct the transferability via cross-group data augmentation results in Table 4, the average of the F1 scores for each class.\\n\\n$$\\\\frac{1}{n} \\\\sum_{i=1}^{n} F1_{i}$$\\n\\nwhere $n$ is the number of groups and $F1_{i}$ is the F1 score for the $i^{th}$ group.\\n\\nTable 4: Evaluation results of cross-lingual and cross-age groups inference on BiMotion. Cross-all denotes cross-lingual and cross-age groups, while the result for the Mandarin-Elderly, while the result for the Mandarin-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross in cross-data cross-cross data in cross-data cross-cross data in cross-cross data in cross-cross data.\\n\\n| Group          | Mandarin-Elderly | Mandarin-Adults | Cantonese-Elderly | Cantonese-Adults |\\n|----------------|------------------|-----------------|-------------------|------------------|\\n| Baseline       | 57.54 (l=eng)    | 27.69 (l=eng)   | 27.34 (l=zho)     | 32.42 (l=zho)    |\\n| Cross-all      | 69.85 (l=eng)    | 28.71 (l=eng)   | 69.53 (l=zho)     | 70.28 (l=zho)    |\\n| Cross-age      | 55.12 (l=eng)    | 22.66 (l=eng)   | 52.21 (l=zho)     | 54.84 (l=zho)    |\\n\\n6.2. Transferability via Cross-Group Data Augmentation\\n\\nWe compare the performance of cross-group data augmentation results in Table 4, the best weighted F1-score for each group. While for the English-Elderly, the result for the English-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross in cross-data cross-cross data in cross-data cross-cross data in cross-cross data.\\n\\nDespite having different characteristics, there are some differences in speech features used to recognize emotion between different languages and different age groups, which makes transfer through cross-group inference suitable for speech emotion recognition.\\n\\nBased on our cross-group data augmentation results in Table 4, the best weighted F1-score for each group is shown. While for the English-Elderly, the result for the English-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross in cross-data cross-cross data in cross-data cross-cross data in cross-cross data.\\n\\nBy framing the problem as multilabel, the occurrence of each emotion becomes sparse, thus leading to an imbalanced label distribution. To consider the imbalance distribution, we use binary F1-score to compute per-label evaluation weighted by the number of samples in that class.\\n\\nFor our cross-group data augmentation results in Table 4, the best weighted F1-score for each group is shown. While for the English-Elderly, the result for the English-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross in cross-data cross-cross data in cross-data cross-cross data in cross-cross data.\\n\\nDespite having different characteristics, there are some differences in speech features used to recognize emotion between different languages and different age groups, which makes transfer through cross-group inference suitable for speech emotion recognition.\\n\\nBased on our cross-group data augmentation results in Table 4, the best weighted F1-score for each group is shown. While for the English-Elderly, the result for the English-Adults remains the same, which is probably due to the limited data and cross-aug datasets, i.e., cross-cross points to the training coverage, which will determine augmentation in cross-data cross-cross in cross-data cross-cross in cross-data cross-cross data in cross-data cross-cross data in cross-cross data.\"}"}
{"id": "cahyawijaya23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Cross-group data augmentation on the Cantonese-Adults data. Blue, red, and olive regions represent the density plot of the training data for happy, sad, and neutral emotions, respectively.\\n\\nFor cross-group data augmentation, the effect is apparent with the only improvement coming from the English-Adults group, while the performance in other groups decreases. We conjecture this is due to the huge linguistic differences between English and Mandarin [21, 38]. Further analysis regarding the effect of language distance is discussed in \u00a76.3. Furthermore, a combination of cross-lingual and cross-age data augmentation tends to improve the performance even higher. Specifically, the performance on English-Elderly and English-Adults increases by \u223c13% and \u223c17% weighted F1-score, respectively. The performance on Mandarin-Adults group improves marginally by \u223c0.4% weighted F1-score. On the other hand, the performance of Mandarin-Elderly decreases by \u223c5% weighted F1-score compared to the in-group training baseline. This might be caused by distributional shift due to the large amount of augmentation from other groups with respect to the amount of data in the Mandarin-Elderly group.\\n\\nWe further analyze the cross-group behavior further through feature-space projection using the model trained on all BiMotion training data. As shown in Figure 3, cross-age data augmentation improves the training coverage of the model resulting in a better generalization on unseen evaluation data, despite the speech feature differences among different age groups. This shows the potential of leveraging cross-age data augmentation for modeling low-resource groups such as the elderly.\\n\\n6.3. Effect of Language Distance on Transferability\\n\\nWe further analyze the effect of language distance on cross-lingual data augmentation. Specifically, we explore the effect of cross-lingual data augmentation from Mandarin and English to Cantonese. To balance the amount of data between English and Mandarin, we only utilize the ESD dataset [30] which contains 15K training, 1K validation, and 1.5K utterances for each English and Mandarin. For the Cantonese dataset, we utilize the newly constructed speech emotion dataset, YueMotion. As shown in Table 5, the performance on the Cantonese data increases when the model is augmented with Mandarin data and decreases when the model is augmented with English. This follows the linguistic similarity of languages; Mandarin and Cantonese are more similar as both come from the same language family, i.e., Sino-Tibetan, compared to English that comes from the Indo-European language family. Similar pattern is also observed in the Mandarin test data when the Cantonese-Mandarin trained model outperforms the Cantonese-Mandarin-English model by \u223c3% weighted F1-score.\\n\\nWe also analyze the cross-lingual behavior further through feature-space projection with the Cantonese-Mandarin-English trained model. As shown in Figure 2, the original Cantonese training data cannot cover all the test data. When Mandarin data is added, the training coverage improved which covers more test samples for each label. When English data is added, the training coverage is improved, but not without shifting the training distribution, e.g., the out-of-distribution on the sad label, causing the model to perform worse on the Cantonese evaluation data.\\n\\n7. Conclusion\\n\\nWe investigate the transferability of speech emotion recognition ability to achieve a better generalization for the elderly in low-resource languages. We construct an English-Mandarin speech emotion benchmark for adults and the elderly, namely BiMotion, from six publicly available datasets. We also construct, YueMotion, a low-resource speech emotion dataset for adults in Cantonese to analyze the effect of language distance in cross-lingual data augmentation. Based on the experiments, we conclude: 1) significantly distinct speech features are necessary to recognize emotion across different language and age groups, 2) although the speech features may vary across different groups, cross-group data augmentation is still beneficial to better generalize the model, and 3) language distance substantially affects the effectiveness of cross-lingual transferability. Our analysis lays the groundwork for developing more effective speech emotion recognition models for low-resource groups, e.g., the elderly in low-resource languages.\\n\\nTable 5: The effect of cross-lingual transferability in YueMotion.\\n\\n| Training Data | Test Data |\\n|---------------|-----------|\\n| English only  | 45.33     |\\n| Mandarin only | -         |\\n| English + Mandarin | 47.59 |\\n| English + Mandarin + Cantonese | 51.60 |\\n| Mandarin + English | 43.59 |\\n| Mandarin + Cantonese | 92.61 |\\n| Mandarin + Cantonese + English | - |\\n\\nWe also utilize the newly constructed speech emotion dataset, YueMotion. As shown in Table 5, the performance on the Cantonese data increases when the model is augmented with Mandarin data and decreases when the model is augmented with English. This follows the linguistic similarity of languages; Mandarin and Cantonese are more similar as both come from the same language family, i.e., Sino-Tibetan, compared to English that comes from the Indo-European language family. Similar pattern is also observed in the Mandarin test data when the Cantonese-Mandarin trained model outperforms the Cantonese-Mandarin-English model by \u223c3% weighted F1-score.\\n\\nWe also analyze the cross-lingual behavior further through feature-space projection with the Cantonese-Mandarin-English trained model. As shown in Figure 2, the original Cantonese training data cannot cover all the test data. When Mandarin data is added, the training coverage improved which covers more test samples for each label. When English data is added, the training coverage is improved, but not without shifting the training distribution, e.g., the out-of-distribution on the sad label, causing the model to perform worse on the Cantonese evaluation data.\"}"}
