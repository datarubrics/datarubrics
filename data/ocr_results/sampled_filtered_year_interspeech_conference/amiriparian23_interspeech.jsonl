{"id": "amiriparian23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speech-Based Classification of Defensive Communication: A Novel Dataset and Results\\nShahin Amiriparian1, Lukas Christ1, Regina Kushtanova2,3, Maurice Gerczuk1, Alexandra Teynor2, Bjorn W. Schuller1,3\\n\\n1 Chair of Embedded Intelligence for Health Care and Wellbeing, University of Augsburg, Germany\\n2 Institute for Agile Software Development, Augsburg University of Applied Sciences, Germany\\n3 GLAM \u2013 Group on Language, Audio, & Music, Imperial College London, UK\\n\\nshahin.amiriparian@uni-a.de\\n\\nAbstract\\nDefensive communication is known to have detrimental effects on the quality of social interactions. Hence, recognising and reducing defensive behaviour is crucial to improving professional and personal communication. We introduce DefComm-DB, a novel multimodal dataset comprising video recordings in which one of the following types of defensive communication is present: (i) verbally attacking the conversation partner, (ii) withdrawing from the communication, (iii) making oneself greater, and (iv) making oneself smaller. Subsequently, we present a machine learning approach for the automatic classification of DefComm-DB. In particular, we utilise wav2vec2, autoencoders, a pre-trained CNN and openSMILE for feature extraction from the audio modality. For the text stream, we apply ELECTRA and SBERT. On the unseen test set, our models achieve an Unweighted Average Recall of 49.4 % and 52.2 % for the audio and text modalities, respectively, showing the feasibility of the introduced challenge.\\n\\nIndex Terms: speech processing, defensive communication, computational paralinguistics, Transformers\\n\\n1. Introduction\\nEffective communication plays an essential role in various aspects of human life, including personal and professional development, relationships, and overall well-being. Positive interpersonal communication helps to establish understanding and trust between individuals, making interaction more harmonious [1]. However, due to its complexity, interpersonal communication can fail in many ways. Contributing factors can include cultural and linguistic differences, societal norms, roles and values [2], stress level, mental health concerns, and personal circumstances [3]. In addition, various communication flaws can cause messages to be misinterpreted and expressed in an ineffective manner [4].\\n\\nOne particular form of behaviour in communication is defensiveness. It is often characterised by hostile, aggressive, or passive-aggressive behaviour [5] and can manifest as verbal and nonverbal responses [5] and actions directed toward reducing anxiety [6]. According to Stamp et al. [7], defensive behaviour comprises three parts: a self-perceived flaw that one refuses to acknowledge, sensitivity to that flaw, and a perceived or actual attack by another individual focused on that flaw [7]. This type of behaviour can reduce the effectiveness of communication [8] and cause defensive or aggressive responses in return which can lead to a detrimental cycle of poor communication [5]. Furthermore, defensiveness can negatively affect the quality and satisfaction of relationships and romantic partnerships by increasing the number of conflicts [9, 10]. Moreover, it has been identified as one of the key factors in marriage breakdown [9]. In the workplace, it can have a negative impact on the quality of leader-member interaction which is associated with higher levels of burnout and lower job satisfaction [11]. It is thus important to understand the dynamics of defensive communication and be able to quickly identify when it emerges and steer conversations constructively. By detecting defensive behaviour, individuals can navigate conflicts, resolve them, improve communication skills, and interact better in society. Therefore, it is desirable to have systems in place that can assist in this process.\\n\\nSo far, prior studies on this topic are non-computational and mainly questionnaire-based. Further, they have primarily focused on defence mechanisms, a phenomenon that underlies defensiveness. Various tools have been developed to study defensive behaviours that a person may not be aware of, based on Freud's idea that an observer can reliably determine them [12]. Such tools include the Lerner Defence Scale [13] based on the Rorschach inkblot test and a method developed by Cooper et al. [14] that measures 15 defences from three categories. Following the hierarchical organisation of defence mechanisms proposed by Vaillant et al. [15], Perry et al. [16] developed the Defense Mechanisms Rating Scale [16] and its computerised Q-sort version Q-sort based Defense Mechanisms Rating Scale (DMRS-Q) [17], which provide quantitative and qualitative scores reflecting a person's defensive functioning.\\n\\nTo the best of our knowledge, there are no approaches for automatic classification of defensiveness and defence mechanisms in communication. Other studies which are mainly from the field of computational paralinguistics [18, 19] explore each individual's character traits such as Big Five, insecurity, social anxiety, and feeling threatened that can contribute to defensive behaviour [7].\\n\\nTo this end, in a first attempt to automatic defence mechanism classification, we introduce a novel dataset for defensive communication (cf. Section 2) and provide machine learning solutions for its automatic classification (cf. Section 3). Applications of our research include but are not limited to the fields of conflict resolution [20], communication training [21], and psychology [22].\\n\\n2. Dataset\\nWe have collected a novel Multimodal Defensive Communication Database, denoted as DefComm-DB, from YouTube. It comprises genuine non-acted dialogues between English-speaking individuals in 'real-world' settings that feature one of the defensive behaviours outlined in Birkenbihl's model of communication failures [21]:\\n\\n1. Attacking the conversation partner (class Attack): videos that depict individuals actively attacking verbally, blaming the other person, or shifting the other person's attention to themselves...\"}"}
{"id": "amiriparian23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Withdrawing from the communication (class Flight): videos where people refuse to respond, withdraw from the conversation or change the topic or focus.\\n\\n3. Making oneself greater (class Greater): videos that depict individuals boasting, self-justifying in an aggressive manner, denying accusations, exhibiting a sense of dominance or superiority, or expressing indignation.\\n\\n4. Making oneself smaller (class Smaller): videos that display individuals engaging in self-deprecation, self-blame, exhibiting a sense of guilt, apologising, and expressing feelings of vulnerability or worthlessness.\\n\\nInitially, we collected 431 data points. Next, we carefully checked the collected videos to ensure that they aligned with Birkenbihl's model [21] and that no samples were repeated. After this evaluation, a final set of 261 data points was created. Key statistics on the dataset are provided in Table 1.\\n\\nDEFCOMM-DB features a variety of video topics, including interviews with celebrities and professional athletes, political debates, legal trials, TV shows, and video footage obtained by paparazzi, among others. The situations, number of participants, gender, age, and ethnicity vary from scene to scene. From each video, we retrieve audio, visual, and textual modalities. In this paper, we focus on the audio modality and the speech transcriptions. We have made DEFCOMM-DB available for academic researchers.\\n\\n2.1. Search Parameters\\nWe based our search for suitable YouTube videos on the assumption that defensive behaviour can be identified by certain verbal and non-verbal signs. Verbal indicators of defensive communication may include making excuses, denying responsibility [23], shifting blame, interrupting others [10], and using loud, fast, aggressive, or monotonous and evaluative speech [24]. Non-verbal cues of defensiveness may include closed body posture, avoidance of eye contact, specific head positioning, and facial expressions that convey hostility or disinterest [5]. To find these cues, we look for situations that could potentially involve conflict. Examples of search queries included but were not limited to: \u201cbad interviews\u201d, \u201cheated sports interviews\u201d, \u201ccelebrities freak out\u201d, \u201cemotional court speeches\u201d, \u201cheated debates\u201d, \u201cbragging\u201d, \u201cshaming\u201d, \u201crefusal to answer\u201d, and \u201cpainful conversations.\u201d\\n\\n2.2. Annotation\\nTo build the gold standard, the video recordings were annotated by 11 participants (5 f and 6 m) with a mean age of 25 years (\u00b14.9 years). Among them, four are AI researchers, one master's student with a background in psychology, and six are undergraduates studying social science and psychology (2), computer science (2), economics (1), and management (1). Prior to the annotation process, the participants were provided with verbal and written instructions outlining the purpose and goals of the study, a detailed description of the four classes that needed to be labelled, examples of videos featuring targeted reactions, and technical information about the labelling process. The annotators then assigned each data point to one of the following labels: Attack, Smaller, Greater, and Flight. Additionally, they could select if the video does not have any of the targeted reactions and leave a comment if they noticed some issues.\\n\\nTable 1: Statistics on DEFCOMM-DB: number of video clips, mean duration (\u00b5), standard deviation (\u03c3), minimum, maximum, and total duration of collected videos per class.\\n\\n| Label     | # video clips | \u00b5 [s] | \u03c3 [s] | min [s] | max [s] | \u03a3 duration [s] |\\n|-----------|---------------|-------|-------|---------|---------|----------------|\\n| Attack    | 112           | 8     | 9     | 2       | 46      | 949            |\\n| Flight    | 57            | 9     | 8     | 2       | 62      | 494            |\\n| Greater   | 45            | 9     | 6     | 2       | 25      | 416            |\\n| Smaller   | 47            | 12    | 8     | 3       | 49      | 556            |\\n| Total     | 261           | 9     | 8     | 2       | 62      | 2415           |\\n\\n2.2.1. Inter-Annotator Agreement\\nA large degree of subjectivity may be involved in the annotation process, even if it is based on clear instructions. Therefore, to ensure the reliability of the dataset, it is necessary to calculate the level of agreement between annotators. To do this, we use Krippendorf's alpha (\u03b1) [25]. For our collected annotations, we obtain an \u03b1 value of 0.63, indicating the difficulty of the labelling and substantial confusion between annotators. As shown in Figure 1a, there is a more prominent tendency for confusion between the classes Greater and Attack, as well as between Smaller and Flight. Despite this confusion, it can be noted that overall, the annotators tend to label samples in a similar manner. In Figure 1b, we also provide the agreement between all annotators. Upon completion of the annotation process, videos are assigned labels based on the majority agreement among participants.\\n\\nFigure 1: Co-occurrence matrix of annotators' labels per class (a), confusion matrix showing agreement among annotators (b).\"}"}
{"id": "amiriparian23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"communicating defensively in the respective situation such that every data point corresponds to exactly one person.\\n\\n3.2. Feature Extraction\\n\\nWe utilise a set of open-source deep learning methods for representation learning and feature extraction from the audio (cf. Sections 3.2.1 to 3.2.4) and text (cf. Sections 3.2.5 and 3.2.6) modalities of the recordings in D\\\\textsubscript{EF}C\\\\textsubscript{OMM}-DB.\\n\\n3.2.1. wav2vec2\\n\\nWav2Vec 2.0 \\\\cite{28} is a Transformer model pretrained for automatic speech recognition which has successfully been applied to speech emotion recognition \\\\cite{29}. We utilise a base version with 12 layers and about 95 M parameters, trained on 960 hours of data from the Librispeech \\\\cite{30} dataset. We extract 768-dimensional embeddings from the audio files by passing them to the pretrained model and averaging the resulting representations of its final layer.\\n\\n3.2.2. auDeep\\n\\nAU\\\\textsubscript{D}EEP \\\\cite{31, 32} is employed for unsupervised representation learning from audio data. We extract Mel-spectrograms (128 Mels and Hanning window of width 80 ms with 50\\\\% overlap) from raw waveforms in the dataset, followed by clipping power levels below four given thresholds \\\\{-30, -45, -60, -75\\\\} dB to eliminate background noise. Next, distinct Recurrent Neural Network (RNN) autoencoders (2 hidden Gated Recurrent Unit (GRU) layers each with 256 units, unidirectional encoder, bidirectional decoder) are trained for 64 epochs with a batch size of 16, a learning rate of 0.001 and a keep probability of 80\\\\% on each of these sets of spectrograms, and the learnt representations of each spectrogram are extracted as feature vectors of size 1024. Finally, these feature vectors are concatenated to form the final feature vector of size 4096.\\n\\n3.2.3. DeepSpectrum\\n\\nAdditionally, D\\\\textsubscript{EEP}\\\\textsubscript{SPECTRUM} is applied to obtain deep audio representations utilising pre-trained Convolutional Neural Networks (CNNs) \\\\cite{33}. First, audio signals are transformed into Mel-spectrogram plots (\\\\textit{viridis} colour map, 64 Mels and Hanning window of width 32 ms with 50\\\\% overlap). Using a sliding window of 1 s and overlap of 500 ms, the generated spectrograms are then forwarded through a D\\\\textsubscript{ENSE}\\\\textsubscript{NET\\\\textsubscript{121}} architecture (weights pre-trained on ImageNet), and the activations of the penultimate fully connected layer of the network are extracted, resulting in a 1024-dimensional feature set.\\n\\n3.2.4. eGeMAPS\\n\\nThe expert-designed extended Geneva Minimalistic Acoustic Parameter Set (eGeMAPS) feature set \\\\cite{34} comprises 88 speech-related paralinguistic features and has been successfully applied in speech and affective computing tasks before \\\\cite{35, 36}. We extract the eGeMAPS features for every 2 s audio segment via OPEN\\\\textsubscript{SMILE} \\\\cite{37} using the standard configuration with a frame size of 1 s and a step size of 500 ms.\\n\\n3.2.5. ELECTRA\\n\\nWe select a pretrained ELECTRA model \\\\cite{38}, which has been used successfully in sentiment analysis \\\\cite{38} and emotion recognition \\\\cite{39}. Specifically, we use the base version with 12 layers to generate 768-dimensional sentence embeddings by extracting the final layer's representation of the special CLS token.\\n\\n3.2.6. Semantic Similarity\\n\\nTo obtain more interpretable textual features, we opt for 2 reference sentences for each of the 4 classes: \u201cI am attacking you\u201d and \u201cYou are not as great as you think\u201d for the class Attack, \u201cI am ending this conversation\u201d and \u201cI can not compete with you\u201d for the class Flight, \u201cI am superior to you\u201d and \u201cI am as great as you, if not greater\u201d for the class Greater, and \u201cI surrender to you\u201d and \u201cPlease do not hurt me\u201d for the class Smaller. These sentences were inspired by Birkenbihl's definition of defensive communication \\\\cite{21}. We then utilise 4 pretrained sentence embedding models (all-mpnet-base-v2, all-distilroberta-v1, all-MiniLM-L12-v2, MiniLM-L6-H384-uncased) from the SBERT framework \\\\cite{40} to compute cosine distances between the representations of all sentences in a video and these 8 reference sentences. Each video is represented by a 32-dimensional feature vector computed through a weighted sum of sentence cosine distances, with weights based on sentence token count.\\n\\n4. Experimental Settings\\n\\nPartitioning:\\n\\nTo reduce model bias toward specific speakers, we implement a speaker-independent partitioning strategy and divide the data into three training, development, and test sets. We further ensure that each set contains an approximately equal number of data points for each class (cf. Table 2).\\n\\nModel training:\\n\\nWe use Support Vector Machines (SVMs) with UAR as the evaluation metric. During the training, we optimise each SVM model on the development data split using the grid search hyperparameter optimisation and test the best-performing models on the unseen test partition. Specifically, we search for the optimal value of SVM's cost parameter (C) in the range of between \\\\(10^{-7}\\\\) and \\\\(10^{1}\\\\) on a logarithmic scale. Additionally, we test linear, Radial Basis Function (RBF), Sigmoid, and polynomial kernels. For the polynomial kernel, we further search for the optimal degree within the range of 2, 3, 4, and 5. Regarding the features extracted per segment, a majority voting over all segment-level predictions is conducted to obtain the class prediction for a video. Since the dataset has a slight imbalance in class distribution, we further apply the Synthetic Minority Over-sampling Technique (SMOTE) \\\\cite{41} with its default parameters (random state=seed of the current experiment, k neighbors=5) to the extracted features to address this issue. Every experiment is repeated with five fixed seeds.\\n\\nModel fusion:\\n\\nSubsequently, we apply a simple late fusion approach to the trained models, where each model's class prediction is weighted according to the model's UAR on the development set. We experiment with the late fusion of both the audio-based models only and all models.\"}"}
{"id": "amiriparian23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Overall and class-wise performance of the SVM models trained on clip-level features. Every experiment is repeated with\\nAll audio & text models\\n\\n| Method                | Audio Modality | Text Modality | Model Fusion | Overall \\\\[% UAR | Attack \\\\[% Recall | Flight \\\\[% Recall | Greater \\\\[% Recall |\\n|-----------------------|----------------|---------------|--------------|-----------------|------------------|-------------------|-------------------|\\n| eGeMAPS               | 54             | 65            | 60           | 70.9 \u00b1 3.7      | 88.0 \u00b1 1.5       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.7       |\\n| DS (D2v-base-960h)   | 51             | 60            | 58           | 65.0 \u00b1 3.5      | 88.0 \u00b1 3.7       | 46.8 \u00b1 4.0       | 72.9 \u00b1 4.0       |\\n| w2v-base-960h        | 58             | 60            | 58           | 62.0 \u00b1 4.2      | 85.0 \u00b1 3.8       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.8       |\\n| PECTRUM              | 55             | 65            | 60           | 68.0 \u00b1 3.9      | 88.0 \u00b1 3.7       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.7       |\\n| ENSE (fuse)          | 49             | 60            | 58           | 64.0 \u00b1 4.1      | 88.0 \u00b1 3.7       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.7       |\\n| ELECTRA              | 52             | 60            | 58           | 66.0 \u00b1 4.3      | 88.0 \u00b1 3.7       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.7       |\\n| Semantic Similarity  | 52             | 60            | 58           | 65.0 \u00b1 4.2      | 88.0 \u00b1 3.7       | 46.8 \u00b1 3.8       | 72.9 \u00b1 3.7       |\\n\\nResults\\n\\nIn the presented study, we investigated the feasibility of using machine learning to automatically detect four forms of defensive communication based on a popular model proposed by Gibb [5], should be adopted and compared. Furthermore, high-quality subtitles were obtained from the text-based models to the late fusion has a performance of this modality to be on par with the semantic similarity model at 52.0 % UAR. Additionally, per-class recalls reveal that the ELEC-0) 36, 7) 26, 0) 33, 7) 36, 2) 46, 7) 33, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7) 26, 3) 30, 7) 46, 0) 42, 5) 34, 7) 33, 0) 50, 7)"}
{"id": "amiriparian23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"interpersonal speech,\\\" Frontiers in Robotics and AI, vol. 6, p. 116, 2019.\\n\\n[2] R. D. Jenifer and G. Raman, \u201cCross-cultural communication barriers in the workplace,\u201d International Journal of Management, vol. 6, no. 1, pp. 348\u2013351, 2015.\\n\\n[3] S. S. King, \u201cThe relationship between stress and communication in the organizational context,\u201d Communication Studies, vol. 37, no. 1, pp. 27\u201335, 1986.\\n\\n[4] J. W. Pfeiffer, \u201cConditions that hinder effective communication,\u201d The Pfeiffer Library, vol. 6, no. 2, pp. 400\u2013456, 1998.\\n\\n[5] J. R. Gibb, \u201cDefensive communication,\u201d Journal of Communication, vol. 11, no. 3, pp. 141\u2013148, 1961.\\n\\n[6] P. Cramer, Protecting the self: Defense mechanisms in action. Guilford Press, 2006.\\n\\n[7] G. H. Stamp, A. L. Vangelisti, and J. A. Daly, \u201cThe creation of defensiveness in social interaction,\u201d Communication Quarterly, vol. 40, no. 2, pp. 177\u2013190, 1992.\\n\\n[8] L. Tickle-Degnen and R. Rosenthal, \u201cThe nature of rapport and its nonverbal correlates,\u201d Psychological Inquiry, vol. 1, no. 4, pp. 285\u2013293, 1990.\\n\\n[9] J. M. Gottman, \u201cA theory of marital dissolution and stability.\u201d Journal of family psychology, vol. 7, no. 1, p. 57, 1993.\\n\\n[10] J. A. Becker, B. Ellevold, and G. H. Stamp, \u201cThe creation of defensiveness in social interaction ii: A model of defensive communication among romantic couples,\u201d Communication Monographs, vol. 75, no. 1, pp. 86\u2013110, 2008.\\n\\n[11] J. A. Becker, J. R. Halbesleben, and H. Dan O\u2019Hair, \u201cDefensive communication and burnout in the workplace: The mediating role of leader\u2013member exchange,\u201d Communication Research Reports, vol. 22, no. 2, pp. 143\u2013150, 2005.\\n\\n[12] J. C. Perry and F. F. Lanni, \u201cObserver-rated measures of defense mechanisms,\u201d Journal of personality, vol. 66, no. 6, pp. 993\u20131024, 1998.\\n\\n[13] P. Lerner and H. Lerner, \u201cRorschach assessment of primitive defenses in borderline personality structure,\u201d Borderline phenomena and the Rorschach test, pp. 257\u2013274, 1980.\\n\\n[14] S. H. Cooper, J. C. Perry, and D. Arnow, \u201cAn empirical approach to the study of defense mechanisms: I. reliability and preliminary validity of the rorschach defense scales,\u201d Journal of Personality Assessment, vol. 52, no. 2, pp. 187\u2013203, 1988.\\n\\n[15] G. E. Vaillant, \u201cEgo mechanisms of defense and personality psychopathology.\u201d Journal of abnormal psychology, vol. 103, no. 1, p. 44, 1994.\\n\\n[16] J. C. Perry and M. Henry, \u201cStudying defense mechanisms in psychotherapy using the defense mechanism rating scales,\u201d Defense mechanisms: Theoretical, research and clinical perspectives, vol. 136, pp. 165\u2013186, 2004.\\n\\n[17] M. Di Giuseppe and J. C. Perry, \u201cThe hierarchy of defense mechanisms: Assessing defensive functioning with the defense mechanisms rating scales q-sort,\u201d Frontiers in Psychology, vol. 12, 2021.\\n\\n[18] B. Schuller, S. Steidl, A. Batliner, E. N\u00f6th, A. Vinciarelli, F. Burkhardt, R. van Son, F. Weninger, F. Eyben, T. Bocklet, G. Mohammadi, and B. Weiss, \u201cThe INTERSPEECH 2012 speaker trait challenge,\u201d in Proc. Interspeech, 2012, pp. 254\u2013257.\\n\\n[19] L. Batrinca, B. Lepri, N. Mana, and F. Pianesi, \u201cMultimodal recognition of personality traits in human-computer collaborative tasks,\u201d in Proc. ICMI, 2012, pp. 39\u201346.\\n\\n[20] M. Askari, S. B. M. Noah, S. A. B. Hassan, and M. B. Baba, \u201cComparison the effects of communication and conflict resolution skills training on marital satisfaction,\u201d International Journal of Psychological Studies, vol. 4, no. 1, p. 182, 2012.\\n\\n[21] V. F. Birkenbihl, Kommunikationstraining: zwischenmenschliche Beziehungen erfolgreich gestalten. mvg Verlag, 2013.\\n\\n[22] J. Parker and E. Coiera, \u201cImproving clinical communication: a view from psychology,\u201d Journal of the American Medical Informatics Association, vol. 7, no. 5, pp. 453\u2013461, 2000.\\n\\n[23] M. L. Friedlander and G. S. Schwartz, \u201cToward a theory of strategic self-presentation in counseling and psychotherapy.\u201d Journal of Counseling Psychology, vol. 32, no. 4, p. 483, 1985.\\n\\n[24] J. M. Civikly, R. Wayne Pace, and R. M. Krause, \u201cInterviewer and client behaviors in supportive and defensive interviews,\u201d Annals of the International Communication Association, vol. 1, no. 1, pp. 347\u2013361, 1977.\\n\\n[25] K. Krippendorff, Content analysis: An introduction to its methodology. Sage publications, 2018.\\n\\n[26] H. Bredin and A. Laurent, \u201cEnd-to-end speaker segmentation for overlap-aware resegmentation,\u201d in Proc. Interspeech, Brno, Czech Republic, August 2021.\\n\\n[27] O. Guhr, A.-K. Schumann, F. Bahrmann, and H. J. B\u00f6ehme, \u201cFull-stop: Multilingual deep models for punctuation prediction,\u201d in Proc. SwissText. Winterthur, Switzerland: CEUR, June 2021.\\n\\n[28] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[29] L. Pepino, P. Riera, and L. Ferrer, \u201cEmotion Recognition from Speech Using wav2vec 2.0 Embeddings,\u201d in Proc. Interspeech, 2021, pp. 3400\u20133404.\\n\\n[30] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in Proc. ICASSP. IEEE, 2015, pp. 5206\u20135210.\\n\\n[31] S. Amiriparian, M. Freitag, N. Cummins, and B. Schuller, \u201cSequence to Sequence Autoencoders for Unsupervised Representation Learning from Audio,\u201d in Proc. of DCASE 2017. Munich, Germany: IEEE, November 2017, pp. 17\u201321.\\n\\n[32] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and B. Schuller, \u201cauDeep: Unsupervised Learning of Representations from Audio,\u201d JMLR, vol. 18, pp. 1\u20135, April 2018.\\n\\n[33] S. Amiriparian, M. Gerczuk, S. Ottl, N. Cummins, M. Freitag, S. Pugachevskiy, and B. Schuller, \u201cSnore Sound Classification Using Image-based Deep Spectrum Features,\u201d in Proc. Interspeech. Stockholm, Sweden: ISCA, Aug. 2017, pp. 3512\u20133516.\\n\\n[34] F. Eyben, K. R. Scherer, B. W. Schuller, J. Sundberg, E. Andr\u00e9, C. Busso, L. Y. Devillers, J. Epps, P. Laukka, S. S. Narayanan et al., \u201cThe geneva minimalistic acoustic parameter set (gemaps) for voice research and affective computing,\u201d IEEE Transactions on Affective Computing, vol. 7, no. 2, pp. 190\u2013202, 2015.\\n\\n[35] J. Wagner, D. Schiller, A. Seiderer, and E. Andr\u00e9, \u201cDeep Learning in Paralinguistic Recognition Tasks: Are Hand-crafted Features Still Relevant?\u201d in Proc. Interspeech, 2018, pp. 147\u2013151.\\n\\n[36] S. Amiriparian, \u201cDeep representation learning techniques for audio signal processing,\u201d Ph.D. dissertation, Technische Universit\u00e4t M\u00fcnchen, 2019.\\n\\n[37] F. Eyben, M. W\u00f6llmer, and B. Schuller, \u201cOpensmile: the munich versatile and fast open-source audio feature extractor,\u201d in Proc. ACM Multimedia. Firenze, Italy: ACM, 2010, pp. 1459\u20131462.\\n\\n[38] K. Clark, M. Luong, Q. V. Le, and C. D. Manning, \u201cELECTRA: pre-training text encoders as discriminators rather than generators,\u201d in Proc. ICLR, Addis Ababa, Ethiopia, 2020.\\n\\n[39] L. Christ, S. Amiriparian, M. Milling, I. Aslan, and B. W. Schuller, \u201cAutomatic emotion modelling in written stories,\u201d arXiv preprint arXiv:2212.11382, 2022.\\n\\n[40] N. Reimers and I. Gurevych, \u201cSentence-BERT: Sentence embeddings using Siamese BERT-networks,\u201d in Proc. EMNLP. Hong Kong, China: ACL, Nov. 2019, pp. 3982\u20133992.\\n\\n[41] N. V. Chawla, K. W. Bowyer, L. O. Hall, and W. P. Kegelmeyer, \u201cSmote: synthetic minority over-sampling technique,\u201d Journal of artificial intelligence research, vol. 16, pp. 321\u2013357, 2002.\\n\\n[42] S. Amiriparian, T. H\u00fcbner, V. Karas, M. Gerczuk, S. Ottl, and B. W. Schuller, \u201cDeepspectrumlite: A power-efficient transfer learning framework for embedded speech and audio processing from decentralized data,\u201d Frontiers in AI, vol. 5, 2022.\"}"}
