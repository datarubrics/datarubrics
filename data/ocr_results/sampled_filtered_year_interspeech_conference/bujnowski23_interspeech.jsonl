{"id": "bujnowski23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Select language, modality or put on a mask!\\n\\nExperiments with multimodal emotion recognition\\n\\nPawe\u0142 Bujnowski1, Bart\u0142omiej Ku\u017ama1, Bart\u0142omiej Paziewski1, Jacek Rutkowski1, Joanna Marhula1, Zuzanna Bordzicka1, Piotr Andruszkiewicz1,2\\n\\n1Samsung Research Poland, Warsaw\\n2Warsaw University of Technology, Warsaw\\n\\n{p.bujnowski,b.kuzma,b.paziewski,j.rutkowski2,j.marhula,z.bordzicka,p.andruszki2}@samsung.com\\n\\nAbstract\\n\\nWe propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality.\\n\\nOur second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks.\\n\\nIndex Terms: multimodal emotion recognition, multilingual models, masked faces\\n\\n1. Introduction\\n\\nMultimodal Emotion Recognition (MER) has recently been explored more extensively as many annotated video datasets have been made publicly available [1, 2]. The majority of research focuses on models that can effectively process signals of various modalities [3]. Some of them reduce computations to deal with rich input effectively, e.g. by the use sparse modules [4, 5].\\n\\nAlthough many methods have been presented so far, there is still a lack of extensive research on the influence of video, audio and text signals on the recognition of emotions analyzed both jointly and separately. Our motivation is to understand MER by studying the real impact of single modalities and their combinations for various emotions. We would like to present our average results as well as compare them with on-the-spot recordings of emotional scenes.\\n\\nAs a part of our study, we also investigate the effects of adding noise to the model input. We have decided to use a natural noise in the form of partly covered face, for example, by mask-wearing or covering the head with a scarf. Mask-wearing is more common in some collectivistic countries (e.g. South Korea, Thailand, the United Arab Emirates, Mexico), but also has become more popular worldwide due to the spread of Covid-19 [6]. Even though the pandemic has receded, masks have not disappeared completely from the public space. In our experiment, we show that mask-wearing has an impact on three modalities in a standard MER system: apart from the image, the audio and text signals are also affected, which leads to lower MER accuracy. In comparison with other research on emotion recognition for masked faces [7, 8], our approach includes all 3 signals (video, audio and text) and uses real recordings of actors in masks.\\n\\nDiscussion of our MER models can engage many conference participants who can also record emotional scenes themselves (with or without masks) or select one of sample videos for analysis. Importantly, our MER system supports not only English but also German and Korean, which increases the cultural scope of the research and the significance of our experiments. We also plan to increase the number of languages in the future.\\n\\n2. System architecture\\n\\nOur demonstration MER system is built of a few components: API with signal recording, automatic speech recognition (ASR), multimodal deep-learning model for emotion recognition, API showing detection results. Figure 1 shows the architecture. Our API is a tool adapted to recording people (both image and speech) in MP4 video format. It can also upload any movies in supported formats. The app selects 10 to 15 frames per second from the video and pushes them to the MER model. In parallel, the system separates the audio file using the FFMPEG library, https://ffmpeg.org, transforming sampling rate to 16kHz and leaving 1 channel. The audio file is sent as an input to the ASR model. Depending on the language, we use various ASR Pytorch libraries: for English - \u201cShinji Watanabe/gigaspeech asr ... bpe5000\u201d model from the ESPnet library [9], for German - \u201cspeechbrain/asr-crdnn-commonvoice-de\u201d model, https://speechbrain.github.io, and for Korean - the commercial ASR from https://cloud.google.com. As a MER model we have modified the multimodal end-to-end sparse architecture from [4] by improving threshold mechanism, changing training hyperparameters and fixing some minor errors in the code. We have also adapted a text pretrained transformer to various languages...\"}"}
{"id": "bujnowski23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"languages: we use BERT-multimodal uncased model for German and Korean, and Albert XLarge for English from https://huggingface.co library.\\n\\nWith a series of images, a speech WAV file and an ASR text output, the model is able to run with various multimodal signal setups - from unique modalities to all their combinations. Subsequently, in a short time (\u223c1 s), the system presents radar charts for 7 emotion categories, expressing the confidence of the model for selected modality groups (see Figure 2).\\n\\n3. Empirical results\\n\\n3.1. Multilingual and multimodal models\\n\\nThe first experiment explores the impact of signal combinations in the model input on MER results. Depending on the language, we use various datasets. For English we combined CMU-MOSEI [2] and IEMOCAP [1] data to build a more complex dataset than any of them alone. For a better quality of the MOSEI corpus, we selected videos that were annotated with the same emotion by at least 2 judges. From IEMOCAP we picked emotion videos with the same labels as in MOSEI. The whole collection includes about 10k video scenes (\u223c17h) with 7 emotions: neutral, happiness, anger, sadness, fear, disgust and surprise. For German and Korean we used an internal collection of multimodal data from open license movies that were cut into short videos and annotated by 3 judges. We received approx. 3k videos for German (\u223c4.3h with 160 actors) and 1.2k for Korean (\u223c2.5h with 140 actors).\\n\\nWe divided data into 3 parts: train, valid and test, respecting the ratio: 70%, 15%, 15%. For each language we trained separate models with various combinations of modalities. We run the fully end-to-end model up to 30 epochs to optimize the scores. Our results are presented in the left part of Table 1.\\n\\nFor all languages the video signal appeared the most important. For English combining video with audio gives the highest F1, whereas for German and Korean single modality video model achieves the best results. Possibly the dominance of text modality in the other studies, like [4], was caused by a low number of video frames used or too short model training.\\n\\nTable 1:\\n\\n| SIGN | EN-NM | EN-M | DE | KO |\\n|------|-------|------|----|----|\\n| V    | 66.8  | 56.8 | 63.7 | 27.0 | 16.3 |\\n| A    | 54.6  | 49.6 | 55.8 | 27.4 | 24.0 |\\n| T    | 62.5  | 46.7 | 56.5 | 43.8 | 40.8 |\\n| V A  | 68.9  | 55.5 | 62.7 | 28.5 | 23.8 |\\n| VT   | 65.8  | 49.1 | 60.0 | 48.7 | 41.2 |\\n| AT   | 63.8  | 48.8 | 55.7 | 42.5 | 41.0 |\\n| V AT | 67.4  | 49.7 | 56.6 | 43.7 | 41.7 |\\n\\n3.2. Experiments with people wearing masks\\n\\nThe second experiment focuses on the effect of mask-wearing on MER. We collected about 400 English recordings in masks and 200 without a mask from 14 actors for the 7 emotions. We evaluated models for English from Section 3.1 trained on data with uncovered faces. In this study, we used ASR text output (instead of human transcription) as a more realistic input for the...\"}"}
