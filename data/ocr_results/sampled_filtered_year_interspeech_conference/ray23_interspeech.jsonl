{"id": "ray23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compositional Generalization in Spoken Language Understanding\\n\\nAvik Ray1, Yilin Shen2, Hongxia Jin3\\n\\n1Amazon Alexa, USA\\n2, 3Samsung Research America, USA\\navik@utexas.edu, yilin.shen@samsung.com, hongxia.jin@samsung.com\\n\\nAbstract\\n\\nState-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model.\\n\\nIndex Terms: spoken language understanding, compositional generalization\\n\\n1. Introduction\\n\\nSpoken language understanding is an important component of task-oriented dialog systems powering today's voice controlled AI agents, and chat bots. Intent classification and slot tagging are two main sub-tasks in SLU [1, 2, 3, 4, 5]. Human language is inherently compositional [6], and humans possess the ability to understand infinite new utterances by focusing on relevant informative sub-parts of the utterance which were learned previously [7]. In this work, we consider informative sub-parts of an utterance containing slots. For example, humans can understand slot value \\\"boston\\\" is of the slot type/label B-to-city from the utterance \\\"show flights to boston\\\". Similarly, from another utterance \\\"find flights from atlanta\\\", they can learn \\\"atlanta\\\" has the slot type B-from-city. If presented with a new utterance \\\"show flights from atlanta to boston\\\", humans can still infer the correct slot labels of \\\"atlanta\\\" and \\\"boston\\\", even though they have never seen these slots appear together before in the same utterance. Despite their success, current state-of-the-art SLU models [8], based on pre-trained language models [9, 10], struggle to perform such simple compositional generalization for slot tagging, as we demonstrate in this work.\\n\\nIn this paper, we investigate two main aspects of slot compositionality; (a) identifying a novel combination of slot types in an utterance which was never seen during training, and (b) identifying more number of slot types per utterance than any training utterance, which we refer to as length generalization. It is vitally important for SLU models to perform both these types of compositional generalization due to the following reasons. Firstly, in domains with a large number of slots (e.g. airline reservation), it can be both time-consuming and expensive to collect and annotate training utterances corresponding to each possible combination of slots. Secondly, for resource constrained cold-start skill developers [11], it is cheaper and easier to annotate a small number of short utterances (with just one or two slots) for training, than longer utterances with many slots which the SLU model may encounter after deployment. Building compositional SLU models which can generalize well under both these settings is vital for both scalable development, and reliability of future AI agents.\\n\\nDue to a lack of compositional objective during training, existing SLU models fail to learn the correct dependence of slot words on the relevant informative words that convey their meaning. Instead, they often rely on spurious slot correlations to make their decision. When these models encounter an utterance with a novel combination of slots unseen during training, they fail to exploit this learned correlation, hence do not generalize. When the models encounter longer utterances with many slots per utterance, than they have seen during training, they often fail due to poor quality slot representations under a longer sentence context. While some techniques have been proposed to improve compositionality of sequence-to-sequence models in small synthetic datasets [12, 13, 14, 15], these are computationally expensive to train, and hard to scale on real-world datasets.\\n\\nMain contributions:\\n\\nIn this work, we improve compositional generalization of SLU models by using explicit compositional objectives during training, and develop novel data augmentation techniques to address both types of compositional generalization in benchmark SLU datasets. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms state-of-the-art BERT SLU model.\"}"}
{"id": "ray23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"technique that helps generalization to longer utterances. Our proposed methods are practical and enable SLU models to scale to large real-world datasets. Our main contributions are:\\n\\n1. We create the first compositional splits of benchmark SLU datasets (ATIS, and SNIPS). These splits can be used as a new benchmark to evaluate compositional generalization properties of SLU models.\\n\\n2. We explore two types of slot compositional generalization, novel slot combination, and length generalization, and conduct in-depth analysis to investigate why existing SLU models perform poorly in both cases.\\n\\n3. We propose a new compositional loss that improves compositionality of SLU models to utterances with unseen slot combinations, and a new paired training technique that improves length generalization of SLU models.\\n\\n4. We show that our new compositional SLU model can achieve significant (up to 5%) improvement in slot tagging F1 score on our new compositional splits.\\n\\n2. Compositional Benchmark Datasets\\n\\nIn this section, we propose our method to create compositional splits of benchmark SLU datasets.\\n\\nIn order to systematically evaluate compositional generalization of SLU models, we start with two benchmark SLU datasets. The first benchmark dataset ATIS [16] contains utterances related to airline reservation. We consider the data split from [17, 1] containing 4,978 training, and 893 test utterances in the standard split (T_{train}, T_{test}). We also use the second benchmark dataset SNIPS [18] containing various utterances in entertainment, weather, and restaurant domains. In the standard split SNIPS has 13,784 training, and 700 test utterances. For each dataset, we create two compositional train/test splits by selecting a subset of utterances from their standard train/test splits.\\n\\nA) Novel Slot Combination Split:\\n\\nHuman can easily identify a slot type in isolation just by focusing on most informative words which are used to describe a slot, even when presented with an utterance having a new combination of two or more slot types that were not seen during learning (training) phase (also referred as systematicity in cognitive science [7]). To test this aspect of compositional generalization, we create a train/test split where none of the combination (or set) of slot types present in a test utterance appear during training. We generate this split (referred as novel slot combination) using the following steps: (a) We remove from standard training set all utterances which have a combination of slot types that appear in the standard test set. We do not remove utterances with a single slot since these are fundamental examples from which the model learns the true semantic meaning of such slots. (b) In order to better separate compositional generalization with OOV generalization [19, 20], we replace any OOV slot values with a randomly selected slot value (but of the same slot label) from the training set to generate the final test set. Figure 1 shows example utterances from our novel slot combination split of ATIS dataset.\\n\\nB) Length Generalization Split:\\n\\nSequence based neural network models are inherently poor at generalization to longer sequences than what it observed during training [21, 22, 23]. Note that, the informativeness of an utterance directly depends on the number of slots present in the utterance, but it does not necessarily depend on actual length of the utterance. Hence, to test length generalization we consider the number of slots in the utterance to generate the split. Increasing the number of slots in an utterance also naturally increases its length. We create compositional train/test splits to test length generalization as follows: (a) From the standard training set, we only select utterances which have number of slots less than or equal to a fixed integer (we use k = 2 in our experiments) (b) We also remove from the test set utterances with slot combinations in the training set and substitute OOV slot values as before. We test if an SLU model has the ability to identify slots when number of slots in the utterance can be much larger than that observed during training. Figure 1 shows an example utterance from our ATIS length generalization split. Table 1 reports the split sizes.\\n\\n3. Our Method\\n\\nIn this section, first we describe the baseline SLU model which we consider in our experiments, and explore why they have poor compositionality. Later, we propose new techniques to improve compositional generalization of SLU models. We use the following notations: Let x = (x_1, ..., x_n) denote an input utterance, where words/tokens x_i \u2208 V, the vocabulary. Each input token x_i is annotated by a slot label y_i \u2208 Y, the slot vocabulary. We consider slot labels in the standard IOB format, where label 'O' denotes the word/token that does not belong to any slot.\\n\\n3.1. Baseline SLU Model Analysis\\n\\nLarge pre-trained language models have been shown to be successful in most natural language understanding tasks. Our baseline SLU model is based on one such model BERT [9], which also achieves state-of-the-art on benchmark SLU datasets. Our model is similar to the implementation in [8]. We train the model jointly on intent classification and slot tagging tasks using the objective:\\n\\nL = L_{intent} + \\\\lambda L_{slot},\\n\\nwhere L_{intent} is the intent classification loss, L_{slot} is the slot tagging loss, and \\\\lambda is a hyper-parameter to balance the losses.\\n\\nKey issue: Although the BERT baseline model can achieve SOTA on standard splits of benchmark ATIS and SNIPS datasets, we observe that it often suffers significant drop in slot tagging performance on our compositional splits. Recall that in BERT, the self-attention layer of the transformer computes the attention distribution for each attention head h as follows:\\n\\nP_h = \\\\text{Softmax}\\\\left(\\\\frac{1}{\\\\sqrt{d}} \\\\text{HW}_h \\\\text{Q}_h (\\\\text{HW}_h \\\\text{K}_h)^T\\\\right) \\\\tag{1}\\n\\nwhere \\\\text{W}_Q_h, \\\\text{W}_K_h are the query, and key projection matrices of the h-th attention head, H is the output hidden layer vectors of previous layer, and d is the head dimension. By plotting this attention head distribution we can observe how much information each token contributes to the final slot label output logit. A human identifies a slot type by focusing on the surrounding most informative words in the utterance that help to convey the semantic meaning of the slot. One may expect that the final transformer layer in SLU model performs the same by providing higher attention weights to informative words corresponding to a slot value. However, we observe that this is not always the case. For example, as shown in Figure 2 in utterance \\\"play rock from the eighties\\\", in order to identify the slot value \\\"rock\\\", the BERT SLU model gives more attention to a different slot \\\"eighties\\\", than informative context word \\\"play\\\". This could indicate that BERT SLU model often learns spurious correlations among context and slot words. Therefore, when such slots appear in an utterance with a new combination of slot not seen during training, the BERT model may fail to identify them. This results in poor slot tagging performance of BERT SLU model in our compositional splits (Section 4).\"}"}
{"id": "ray23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Our Compositional SLU models\\n\\nWe now describe two main techniques that we show can improve compositional generalization of SLU models.\\n\\n1. New Compositional Loss to Improve Slot Compositionality:\\n   We develop a new compositional loss which reduces spurious slot correlation and encourages the SLU model to focus its attention on informative context words. Intuitively, if two words have different slot labels, they should be identifiable based on a disjoint set of words. For example, for the utterance \\\"play rock from the eighties\\\" (Figure 2), to identify slot label for the word \\\"rock\\\" it is sufficient to focus on context words $S_{\\\\text{rock}} = \\\\{\\\\text{play}, \\\\text{rock}\\\\}$. To identify slot label for word \\\"eighties\\\" it is sufficient to focus on words $S_{\\\\text{eighties}} = \\\\{\\\\text{play}, \\\\text{rock}, \\\\text{from}, \\\\text{eighties}\\\\}$. Note that these two sets of words are different. Our compositional loss is a sum of two loss functions as follows:\\n\\n$$L_{\\\\text{slot-pair}} = \\\\frac{1}{N_1} \\\\sum_h \\\\sum_{i,j} \\\\mathbb{1}(y_i \\\\neq y_j) \\\\text{KL}(P_{ih_i}, P_{jh_j}) \\\\quad (2)$$\\n\\n$$L_{\\\\text{non-deg}} = \\\\frac{1}{N_2} \\\\sum_h \\\\sum_{i} \\\\mathbb{1}(y_i \\\\neq O) \\\\text{KL}(P_{ih_i}, 1_i) \\\\quad (3)$$\\n\\nwhere $P_{ih_i}$ is the attention probability distribution corresponding to the token $x_i$, head $h$ of the final transformer layer, $1_i$ is the indicator distribution over all input tokens with 1 at position $i$, and 0 elsewhere, and $N_1, N_2$ are normalizing constants. The slot pair loss $L_{\\\\text{slot-pair}}$ encourages the attention distribution for two slot words $x_i, x_j$ with different slot labels $y_i, y_j$ to focus on a disjoint set of context words. The second non-degenerate loss $L_{\\\\text{non-deg}}$ prevents the slot pair loss to converge to a degenerate solution where each token mainly focuses on itself. The final compositional loss for training our SLU model is given by:\\n\\n$$L = L_{\\\\text{intent}} + \\\\lambda_1 L_{\\\\text{slot}} - \\\\lambda_2 L_{\\\\text{slot-pair}} - \\\\lambda_3 L_{\\\\text{non-deg}} \\\\quad (4)$$\\n\\nwhere $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3$ are hyper-parameters.\\n\\n2. Paired Training to Improve Length Generalization:\\n   The compositional loss enables the SLU model to better generalize to utterances with new combination of slots not seen during training (Section 4). However, the model still perform poorly in length generalization splits. Length generalization has been shown to be particularly difficult for both sequence generation [21, 24], and multimodal tasks [25]. We hypothesize that the model fails to generate good hidden state representations when presented an utterance with many slots, greater than those learned at training. To mitigate this problem, we develop an effective data augmentation approach we refer as paired training. Previously, a data augmentation approach GECA has been used to improve compositional generalization in sequence-to-sequence models [21]. However, their performance on length generalization remained poor since it only replaces words/phrases in existing training sentences, and does not necessarily produce very long sentences. In our approach, we randomly select two distinct training utterances of the same intent but a disjoint combination of slots, and concatenate them with a period separator to form a new training sample. This exposes the model both to longer sequences, as well as new combination of slots not present in the original training set, resulting in better length generalization. Note that, neuro-symbolic approaches have been shown to perform effective length generalization in seq-to-seq models [14, 15]. However, these models are difficult to train, and they do not scale to real-world datasets which don't follow strict grammar rules. Model based data augmentation approach has also been explored for improving robustness of SLU models [26]. However, this requires additional NL template information for each intent/slot which is difficult to obtain for large domains (e.g. ATIS).\\n\\n4. Experiments\\n\\n4.1. Settings and baselines\\n\\nWe train SLU models jointly for intent classification, and slot tagging tasks. Our evaluation metrics are slot tagging F1 score, and intent accuracy. Incorporating dependency parse information is known to improve compositional generalization of neural networks [27, 28]. We test an advanced baseline model (BERT SLU + parse tree) which modifies the original attention scores in the final transformer layer with a weight inversely dependent on token distance on dependency tree. Intuitively, tokens which are further away in the dependency tree are assumed to be less informative, and given lower attention scores. We also test a third baseline (BERT SLU + relative pos emb) which incorporates relative position embedding in BERT's attention computation [29]. In seq-to-seq tasks, it has been shown that relative position embedding helps in length generalization [30].\\n\\nParameters: Our baseline and compositional SLU models are fine-tuned from BERT model bert-base-uncased [9]. We use hyper-parameters: batch size 32, learning rate $\\\\in \\\\{10^{-5}, 5 \\\\times 10^{-5}\\\\}$, number of training steps $\\\\in \\\\{4K, 5K, 6K\\\\}, \\\\lambda_1 = 1$. For compositional models we use $\\\\lambda_2 = 0$, $\\\\lambda_3 = 0.1$.\\n\\n4.2. Results\\n\\nResults on novel slot combination split:\\n\\nFirst, we compare the performance of SLU models on the novel slot combination splits, where the test utterances have a distinct combination of slot types which doesn't appear together in training. Table 1 presents the results (averaged over 5 runs with different seeds). The first row corresponds to the performance of BERT SLU model when trained on the full standard training set $T_{\\\\text{train}}$. This acts as an upper bound for the model performance. When the models are trained on the smaller compositional training set, the performance of the baseline models drop since they do not generalize well to the test sets. Observe that, in SNIPS compositional test set, the baseline performance drops about 3% F1 score. The intent accuracy also drop around 1%. BERT combined with dependency parse tree, fails to improve slot tagging performance, but it improves the intent accuracy. BERT with...\"}"}
{"id": "ray23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Performance on Our Compositional Splits. The (train, test) sizes of Novel Slot Combination Split are (1229, 496) in ATIS and (1939, 600) in SNIPS. Sizes in Length Generalization Split are (1494, 163) in ATIS and (7107, 253) in SNIPS.\\n\\nIn this work, we demonstrate that SOTA SLU models based on pre-trained language models have poor generalization when: (a) novel slot combinations are removed. This indicates they have a similar effect on generalization to novel slot combination. We further propose a new paired training data augmentation technique which greatly improves performance to a compositional SLU model. However, as we discussed in Section 1, in real world cold-start settings this is often not the case which necessitates our compositional SLU model.\\n\\nIn Table 3, we compare the performance of our compositional SLU model. The two bottom rows in Table 1 show a slightly better performance. For ATIS, we observe a significant effect in length generalization.\\n\\nNext, we investigate Slot Error Analysis in Length Generalization Split. We further propose a new paired training data augmentation technique which greatly improves performance to a compositional SLU model. The two bottom rows in Table 1 show a slightly better performance. For ATIS, we observe a significant effect in length generalization.\\n\\nFinally, we perform an ablation study to better understand the contribution of each component of our compositional SLU model. The two bottom rows in Table 1 show a slightly better performance. For ATIS, we observe a significant effect in length generalization.\\n\\nIn Table 3, we compare the performance of our compositional SLU model. The two bottom rows in Table 1 show a slightly better performance. For ATIS, we observe a significant effect in length generalization.\\n\\nIn our future work, we want to further explore the impact of OOV slot values on compositionality.\"}"}
{"id": "ray23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] D. Hakkani-T\u00fcr, G. T\u00fcr, A. Celikyilmaz, Y.-N. Chen, J. Gao, L. Deng, and Y.-Y. Wang, \\\"Multi-domain joint semantic frame parsing using bi-directional RNN-LSTM,\\\" in INTERSPEECH, 2016, pp. 715\u2013719.\\n\\n[2] B. Liu and I. Lane, \\\"Attention-based recurrent neural network models for joint intent detection and slot filling,\\\" in Interspeech 2016, 2016, pp. 685\u2013689.\\n\\n[3] Y. Kim, S. Lee, and K. Stratos, \\\"ONENET: joint domain, intent, slot prediction for spoken language understanding,\\\" in 2017 IEEE Automatic Speech Recognition and Understanding Workshop, 2017, pp. 547\u2013553.\\n\\n[4] C.-W. Goo, G. Gao, Y.-K. Hsu, C.-L. Huo, T.-C. Chen, K.-W. Hsu, and Y.-N. Chen, \\\"Slot-gated modeling for joint slot filling and intent prediction,\\\" in Proc. of the 16th NAACL-HLT, 2018.\\n\\n[5] Y. Wang, Y. Shen, and H. Jin, \\\"A bi-model based RNN semantic frame parsing model for intent detection and slot filling,\\\" in Proc. of the 2018 NAACL-HLT, Volume 2 (Short Papers), 2018, pp. 309\u2013314.\\n\\n[6] N. Chomsky, \\\"Syntactic structures,\\\" Mouton, 1957.\\n\\n[7] J. A. Fodor and Z. W. Pylyshyn, \\\"Connectionism and cognitive architecture: A critical analysis,\\\" Cognition, vol. 28, no. 1-2, pp. 3\u201371, 1988.\\n\\n[8] Q. Chen, Z. Zhuo, and W. Wang, \\\"BERT for joint intent classification and slot filling,\\\" CoRR, vol. abs/1902.10909, 2019.\\n\\n[9] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \\\"BERT: pre-training of deep bidirectional transformers for language understanding,\\\" in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short Papers). Association for Computational Linguistics, 2019, pp. 4171\u20134186.\\n\\n[10] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \\\"Roberta: A robustly optimized BERT pretraining approach,\\\" CoRR, vol. abs/1907.11692, 2019. [Online]. Available: http://arxiv.org/abs/1907.11692\\n\\n[11] Y. Shen, A. Ray, A. Patel, and H. Jin, \\\"CRUISE: cold-start new skill development via iterative utterance generation,\\\" in Proceedings of ACL 2018, System Demonstrations. Association for Computational Linguistics, 2018, pp. 105\u2013110.\\n\\n[12] B. M. Lake, \\\"Compositional generalization through meta sequence-to-sequence learning,\\\" in Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, 2019, pp. 9788\u20139798.\\n\\n[13] J. Gordon, D. Lopez-Paz, M. Baroni, and D. Bouchacourt, \\\"Permutation equivariant models for compositional generalization in language,\\\" in 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020.\\n\\n[14] M. I. Nye, A. Solar-Lezama, J. Tenenbaum, and B. M. Lake, \\\"Learning compositional rules via neural program synthesis,\\\" in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\n[15] X. Chen, C. Liang, A. W. Yu, D. Song, and D. Zhou, \\\"Compositional generalization via neural-symbolic stack machines,\\\" in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\n[16] D. A. Dahl, M. Bates, M. Brown, W. M. Fisher, K. Hunicke-Smith, D. S. Pallett, C. Pao, A. I. Rudnicky, and E. Shriberg, \\\"Expanding the scope of the ATIS task: The ATIS-3 corpus,\\\" in Human Language Technology, Proceedings of a Workshop held at Plainsboro, New Jerey, USA. Morgan Kaufmann, 1994.\\n\\n[17] Y. He and S. J. Young, \\\"Semantic processing using the hidden vector state model,\\\" Comput. Speech Lang., vol. 19, no. 1, pp. 85\u2013106, 2005.\\n\\n[18] A. Coucke, A. Saade, A. Ball, T. Bluche, A. Caulier, D. Leroy, C. Doumouro, T. Gisselbrecht, F. Caltagirone, T. Lavril, M. Primet, and J. Dureau, \\\"Snips voice platform: an embedded spoken language understanding system for private-by-design voice interfaces,\\\" CoRR, vol. abs/1805.10190, 2018.\\n\\n[19] A. Ray, Y. Shen, and H. Jin, \\\"Iterative delexicalization for improved spoken language understanding,\\\" in Interspeech 2019, 20th Annual Conference of the International Speech Communication Association, Graz, Austria, 15-19 September 2019. ISCA, 2019, pp. 1183\u20131187.\\n\\n[20] Y. Yan, K. He, H. Xu, S. Liu, F. Meng, M. Hu, and W. Xu, \\\"Adversarial semantic decoupling for recognizing open-vocabulary slots,\\\" in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020. Association for Computational Linguistics, 2020, pp. 6070\u20136075.\\n\\n[21] B. M. Lake and M. Baroni, \\\"Generalization without systematicity: On the compositional skills of sequence-to-sequence recurrent networks,\\\" in Proceedings of the 35th International Conference on Machine Learning, ICML 2018, Stockholmsm\u00e4ssan, Stockholm, Sweden, July 10-15, 2018, ser. Proceedings of Machine Learning Research, vol. 80, 2018, pp. 2879\u20132888.\\n\\n[22] L. Ruis, J. Andreas, M. Baroni, D. Bouchacourt, and B. M. Lake, \\\"A benchmark for systematic generalization in grounded language understanding,\\\" in Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, 2020.\\n\\n[23] D. Hupkes, V. Dankers, M. Mul, and E. Bruni, \\\"Compositionality decomposed: How do neural networks generalise?\\\" J. Artif. Intell. Res., vol. 67, pp. 757\u2013795, 2020.\\n\\n[24] E. Aky\u00fcrek, A. F. Aky\u00fcrek, and J. Andreas, \\\"Learning to recombine and resample data for compositional generalization,\\\" in 9th International Conference on Learning Representations, ICLR 2021. OpenReview.net, 2021.\\n\\n[25] T. Gao, Q. Huang, and R. J. Mooney, \\\"Systematic generalization on gscan with language conditioned embedding,\\\" in Proceedings of AACL/IJCNLP 2020. Association for Computational Linguistics, 2020, pp. 491\u2013503.\\n\\n[26] Z. Zhao, S. Zhu, and K. Yu, \\\"Data augmentation with atomic templates for spoken language understanding,\\\" in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019, Hong Kong, China, November 3-7, 2019. Association for Computational Linguistics, 2019, pp. 3635\u20133641.\\n\\n[27] V. Cirik, T. Berg-Kirkpatrick, and L. Morency, \\\"Using syntax to ground referring expressions in natural images,\\\" in Proc. of the 32nd AAAI Conference on Artificial Intelligence, (AAAI-18), the 30th innovative Applications of Artificial Intelligence (IAAI-18), and the 8th AAAI Symposium on Educational Advances in Artificial Intelligence (EAAI-18). AAAI Press, 2018, pp. 6756\u20136764.\\n\\n[28] Y. Kuo, B. Katz, and A. Barbu, \\\"Compositional networks enable systematic generalization for grounded language understanding,\\\" in Findings of the Association for Computational Linguistics: EMNLP 2021. Association for Computational Linguistics, 2021, pp. 216\u2013226.\\n\\n[29] Z. Huang, D. Liang, P. Xu, and B. Xiang, \\\"Improve transformer models with better relative position embeddings,\\\" in Findings of the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20 November 2020, ser. Findings of ACL, vol. EMNLP 2020, 2020, pp. 3327\u20133335.\\n\\n[30] S. Onta\u00f1\u00f3n, J. Ainslie, Z. Fisher, and V. Cvicek, \\\"Making transformers solve compositional tasks,\\\" in Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL. Association for Computational Linguistics, 2022, pp. 3591\u20133607.\"}"}
