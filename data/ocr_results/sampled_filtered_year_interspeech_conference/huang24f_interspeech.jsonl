{"id": "huang24f_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation\\n\\nRuizhe Huang\\\\(^1\\\\), Mahsa Yarmohammadi\\\\(^1\\\\), Sanjeev Khudanpur\\\\(^1\\\\), Daniel Povey\\\\(^2\\\\)\\n\\n\\\\(^1\\\\)Johns Hopkins University, USA\\n\\\\(^2\\\\)Xiaomi Corp., China\\n\\n\\\\{ruizhe,mahsa,khudanpur\\\\}@jhu.edu\\n\\nAbstract\\n\\nExisting research suggests that automatic speech recognition (ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60\\\\% and 25\\\\% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines.\\n\\nIndex Terms: speech recognition, contextual biasing, data augmentation\\n\\n1. Introduction\\n\\nHuman speech recognition does not occur in isolation. In addition to acoustic cues, we often rely on various contextual resources, such as semantic or visual context or speaker's background knowledge, to aid in understanding and interpreting spoken content. In particular, these contextual cues play a significant role in recognizing rare words and named entities. End-to-end (E2E) automatic speech recognition (ASR) has emerged as the dominant solution of ASR, due to its simplicity of modeling and impressive performance. However, a conventional E2E ASR model takes merely acoustics features as input and outputs the corresponding text transcription.\\n\\nRecently, various contextual biasing techniques (contextual ASR) have been proposed to improve standard E2E ASR models, including [1, 2, 3] for connectionist temporal classification (CTC) models, [4, 5, 6, 7] for attention-based encoder-decoder (LAS) models, [8, 9, 10, 11, 12, 13, 14, 15, 16, 17] for transducer models and more recently [18, 19, 20, 21] for (or with) large language models (LLMs). Following the majority of prior research, we define the context to be lists of biasing words, which are usually rare words in the model's training data. Other types of context, such as visual contexts [22], date-time and location [23] are out of scope of this paper.\\n\\nIn general, contextual ASR can be achieved either in a shallow way or deep way (or a hybrid of the two). In shallow biasing, the internal representations of the E2E models are unchanged. The contextual biasing most likely happens only during the decoding process, where the contexts are used to guide the beam search, e.g., shallow fusion [1, 24, 25, 26, 27], spelling correction [28]. Shallow fusion is considered as a simple yet robust baseline, as it can be easily integrated into beam search to provide moderate improvement in recognizing rare words. In deep (neural) biasing, the context is injected into the E2E model to edit the internal representations of the models and make a potentially new output distribution, e.g., with cross-attention over the biasing lists [4, 11]. Neural biasing has been reported outperforming shallow fusion [2, 4, 9, 10, 11, 12], due to the specialized parameters trained to accommodate contexts. However, many existing work [7, 13, 14, 15, 16, 17] do not directly compare their neural biasing approaches with shallow fusion. They only report the gains over non-contextual ASR baselines.\\n\\nThis paper proposes two techniques that can improve neural biasing across various ASR models. First, we inject contexts into earlier layers of the encoder as opposed to only the last layer. Although this idea has been explored in some existing work [29, 15], the specific layer and the number of layers to be integrated with contexts remain unclear. Other work [2, 14] use the outputs from intermediate encoder layers as the queries for contexts lookup, but the resulting contextual embedding is still integrated with the encoder's final output at the last layer. While some work [3, 9] raise concerns about runtime latency associated with neural biasing, we report decoding runtime in our experiments and find that the overhead of early context injection is negligible when the biasing lists are of size 500. For larger biasing lists, light-weight algorithms may be applied to shorten the biasing lists, which can be a future work.\\n\\nSecondly, during training, we propose to perturb the reference transcription with alternative, similar-sounding spellings of the rare words. For example, we opt to replace the word \\\"Klein\\\" with a random alternative spelling \\\"Klane\\\" in both the transcription and contexts. Hence, the end-to-end trained model is forced to rely on the contexts to make correct predictions. Alternative spellings has been explored in [1, 3, 9, 10, 26, 30, 31, 32]. Among them, [1, 3, 26, 30, 31] use alternative spelling during decoding to improve the recall of rare or out-of-vocabulary words. Closely related to our work are [9, 10, 32] where the alternative spellings are used as data augmentation during training. However, their neural biasing architecture is very different from ours. [32] is based on CLAS architecture [4] and tries to better distinguish phonetically confusable phrases. [9, 10] use a contextual predictor (\\\"PLM\\\") which is implemented by a prefix tree, instead of cross attention mechanism, in their transducer model. Thus, their encoders are not context-aware, although transducers are \\\"encoder-heavy\\\" models. Note that [1, 3, 26, 30, 31, 32] also propose algorithms or models to generate alternative spellings, e.g., a grapheme-to-grapheme (G2G) model [26], which may further benefit our approach. In this work, we simply use less than 200 hand-crafted linguistic rules (e.g., \\\"ein\\\"\u2194\\\"ane\\\", \\\"s\\\"\u2194\\\"z\\\") that cover all 26 English letters and show this already goes a long way.\"}"}
{"id": "huang24f_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Despite the simplicity of our techniques, we achieve the new state-of-the-art results on LibriSpeech [33] in the contextual ASR setup [10]. Furthermore, we demonstrate promising improvements over shallow fusion with neural biasing on two public datasets, SPGISpeech [34] and ConEC [35], where the latter uses real-world contexts rather than synthesized contexts for training. Our implementation and experiment results are available in the ConEC repository: https://github.com/huangruizhe/ConEC.\\n\\nIn this section, we review conventional, non-contextual ASR models, taking transducers as an example. Then we describe the integration of neural contextual biasing by cross attention, which was recently applied cross attention to bias transducer models, as in [11, 12]. Applied cross attention to bias the attention-based models with cross attention. Later, [8, 10] applied cross attention to bias transducer models, which was first proposed in [36] to learn the transducer model's mechanism to the transducer models.\\n\\n2.1. Transducer ASR Model\\n\\nTransducer model has been widely used in ASR due to its effectiveness and streaming nature. It has three components. The encoder serves the role of an acoustic model. The predictor's embedding, which is the idea of a CTC model. The transducer model, on the other hand, explicitly imposes dependencies between the output word pieces in the vocabulary, which is the idea of a CTC model. The transducer model has been widely used in ASR due to its effectiveness and streaming nature. It has three components.\\n\\n2.2. Neural Biasing with Cross Attention\\n\\nCross Attention\\n\\nThere can be many layers of the transformation. The output of the transducer model is of the shape \\\\( W \\\\mid X_t, U \\\\), where \\\\( W \\\\) is the size of the word pieces vocabulary. From this output, the probability of the ground-truth transcription \\\\( \\\\bar{W} \\\\) can be computed efficiently via dynamic programming, which will be maximized during training.\\n\\nThe above context encoder and the cross-attention biasing adapter can be trained in an end-to-end fashion together with the transducer model. In [8, 11], all parameters are trained from scratch. In [12], the transducer's parameters are pretrained and then frozen. Only a small number of the rest of the parameters (yellow modules in Fig. 1) are updated. The benefit is that the neural contextual biasing module (right) is frozen. Only the yellow modules are trained. The gray modules come context-aware. Modified embeddings \\\\( \\\\hat{X} \\\\) are fed to the next encoder layers or the joiner to make context-aware predictions. This is illustrated in Figure 1.\\n\\nEncoder\\n\\nThe encoder serves the role of an acoustic model. It takes the feature sequence \\\\( h_{enc} = (h_{1enc}, \\\\ldots, h_{Tenc}) \\\\) from the given speech feature sequence \\\\( X \\\\). The encoder can take up most of the parameters (e.g., more than 90% parameters). From this output, the probability of the ground-truth transcription \\\\( \\\\bar{W} \\\\) can be computed efficiently via dynamic programming, which will be maximized during training.\\n\\nSoftmax\\n\\nThe joiner takes the embeddings \\\\( \\\\hat{h} = (\\\\hat{h}_t, \\\\ldots, \\\\hat{h}_u) \\\\) of length \\\\( T \\\\) from the encoder, one may produce a single input sequence, where \\\\( w_1, \\\\ldots, w_N \\\\) is the size of the word pieces vocabulary. From this output, the probability of the ground-truth transcription \\\\( \\\\bar{W} \\\\) can be computed efficiently via dynamic programming, which will be maximized during training.\\n\\nJoiner\\n\\nThe joiner \\\\( h_{join} \\\\) is implemented by a recurrent neural network (RNN) or more recently Conformer [37] architecture. Encoder can take up most of the parameters (e.g., more than 90% parameters). From this output, the probability of the ground-truth transcription \\\\( \\\\bar{W} \\\\) can be computed efficiently via dynamic programming, which will be maximized during training.\\n\\nPredictor\\n\\nThere can be many layers of the transformation. The output of the transducer model is of the shape \\\\( W | X_t, U \\\\), where \\\\( W \\\\) is the size of the word pieces vocabulary. From this output, the probability of the ground-truth transcription \\\\( \\\\bar{W} \\\\) can be computed efficiently via dynamic programming, which will be maximized during training.\\n\\nCross Attention\\n\\nThe transducer model has been widely used in ASR due to its effectiveness and streaming nature. It has three components. The encoder serves the role of an acoustic model. The predictor's embedding, which is the idea of a CTC model. The transducer model, on the other hand, explicitly imposes dependencies between the output word pieces in the vocabulary, which is the idea of a CTC model. The transducer model has been widely used in ASR due to its effectiveness and streaming nature. It has three components.\"}"}
{"id": "huang24f_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Proposed Approaches\\n\\n3.1. Early Context Injection\\n\\nIn [2, 4, 5, 8, 11, 12, 14, 17], cross-attention neural biasing architectures are used as described in section 2.2. All of them inject contexts only into the encoder's final output. The drawback is that the contextualization of a frame \\\\( \\\\hat{h}_{\\\\text{enc}}^t \\\\) has no impact on the other frames, as they are edited essentially independently from one another. On the other hand, if contexts are injected to earlier encoder layers (e.g., at \\\\( h_{\\\\text{enc}}^{i,t} \\\\) in Figure 1), it may have far-reaching impacts on the model's internal states via its self-attention mechanism.\\n\\nOne might argue that cross-attention context lookup can introduce significant computation overhead, hence they perform it only once at the last encoder layer. However, as we will show later in Section 4, this overhead is negligible. In fact, context injection can be viewed as simply adding an extra layer to the encoder. For ASR, the input or intermediate sequences usually have several hundred frames. Thus, each layer of the encoder, e.g., a conformer [37], needs to do self-attention over several hundred items. This is a comparable computation to contexts lookup if we have several hundred biasing words to attend to.\\n\\nThe predictor of the transducer is usually implemented by a simple LSTM network or even a stateless n-gram feed-forward network [38]. Thus, we only bias the predictor's final output.\\n\\n3.2. Text Perturbation with Alternative Spellings\\n\\nWhen transcribing unfamiliar names of people, locations or products, humans tend to choose the most familiar or phonetically similar option. Given a reference guide, they will likely refer to it for guidance. Moreover, when the contents in the reference guide change, they adapt accordingly. Text perturbation follows this idea to train the model to optimally use the contextual information. While acoustic data augmentation (e.g., [39]) is a common practice for ASR, text perturbation has not been widely adopted yet.\\n\\nOn the other hand, we observe that ASR models can overfit the training data. The word error rates on the training data is so low that the end-to-end training of neural biasing modules may not have enough chances to learn to attend to the contexts. On LibriSpeech and SPGISpeech (Section 4.1), the training data distribution and word error rates are listed in Table 1. Even without contexts, there is only 29.38% and 4.42% utterances in the training data containing at least one mis-recognized rare word (defined in Section 4.1). This implies that only these utterances may potentially benefit from the contexts containing ground-truth rare words during training. After random text perturbation (details can be found in Section 4.1), the percentage of such mis-recognized utterances containing rare words increases to 90.13% and 36.37% of all utterances.\\n\\nIn our experiments, we apply hand-crafted linguistic rules to obtain similar-sounding spelling alternatives. We replace the \\\"maximal\\\" matched pattern with its counterpart (e.g., \\\"lee\\\" \u2192 \\\"li\\\" although pattern \\\"e\\\" \u2194 \\\"a\\\" is also a match). For some languages, e.g., Chinese, it is even easier to obtain spelling alternatives by pronunciation dictionary lookup for characters.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\nWe use LibriSpeech [33] and SPGISpeech [34] to train contextual ASR models. The orthographically normalized version of SPGISpeech is used. Dataset statistics can be found in Table 1. We follow the same setup as in [10, 17] to generate artificial biasing lists during training. More specifically, we define rare words to be the words beyond top 5k/3k most frequent words for LibriSpeech/SPGISpeech. Due to the long tail nature, these words has poor ASR performance compared to the common words (Table 2 row 1). Thus, we provide the ASR model with such words, as well as some distractors, as the context, aiming to enhance the model's ability to recognize these words. For each utterance, we include all rare words from its reference transcription into its context. We also add 100 distractors sampled from all the rare words in the training vocabulary. We observe no gains when adding more distractors.\\n\\nFor evaluation, we use the test sets in LibriSpeech and SPGISpeech. The LibriSpeech contexts are predefined in [10], and we define the SPGISpeech contexts similarly. We also use ConEC [35], which consists of earnings calls in Earnings-21 [40] and their real-world supplementary materials including presentation slides, earnings news release, a list of meeting participants' names and affiliations. We follow the evaluation metrics in [10, 17] to compute overall word error rate (WER) and the WER for common words and rare words (U-WER, B-WER). For ConEC, we also report the WERs for named entities.\\n\\nFor text perturbation, we randomly perturb the spellings of rare words with a given probability (0.2 for both datasets). For SPGISpeech, we also randomly (with probability of 0.8) discard utterances containing no rare words during training.\"}"}
{"id": "huang24f_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Contextual ASR on LibriSpeech. Each cell is formatted as WER (U-WER / B-WER).\\n\\n\\\\( N \\\\) is the number of distractors added to the biasing words list. NO (no biasing), SF (shallow fusion), NB (neural biasing), TP (text perturbation), \\\\( @\\\\text{Layers}: 9,15 \\\\) means to which layers contexts are injected. \u2020: This row shares the same results as there is no context for \u201cno biasing\u201d, so \\\\( N \\\\) is irrelevant.\\n\\n|         | \\\\( N=100 \\\\) | \\\\( N=500 \\\\) | \\\\( N=1000 \\\\) |\\n|---------|-------------|-------------|-------------|\\n| test-clean |            |             |             |\\n| test-other |            |             |             |\\n| NO       | 2.17 (1.25/9.65) | 5.22 (3.32/21.83) | \u2020\u2020 |\\n| SF       | 1.49 (1.18/3.98) | 4.01 (3.27/10.50) | 1.59 (1.26/4.22) | 4.11 (3.32/11.05) | 1.63 (1.31/4.27) | 4.26 (3.47/11.21) |\\n| NB       | 1.72 (1.17/6.03) | 4.13 (3.18/12.47) | 1.72 (1.19/6.06) | 4.33 (3.24/13.85) | 1.78 (1.19/6.60) | 4.34 (3.19/14.41) |\\n| +@Layers: 9,15 | 1.52 (1.13/4.65) | 3.69 (3.06/9.16) | 1.71 (1.21/5.75) | 4.00 (3.16/11.44) | 1.86 (1.28/6.58) | 4.38 (3.35/13.46) |\\n| TP       | 1.24 (1.11/2.29) | 3.32 (3.08/5.46) | 1.50 (1.24/3.56) | 3.69 (3.18/8.19) | 1.74 (1.33/5.10) | 4.24 (3.49/10.84) |\\n\\nPromptASR [15] 1.73 ( - / - ) 4.07 ( - / - ) 2.0 ( - / - ) 4.45 ( - / - ) 2.13 ( - / - ) 4.67 ( - / - )\\n\\nGuided attn [17] 2.2 (1.8/5.1) 5.4 (4.7/12.2) n/a n/a 2.4 (1.9/6.4) 6.0 (5.0/15.3)\\n\\n4.3. Results\\n\\nThe WER results for LibriSpeech are reported in Table 2. When there is no context or biasing, the rare word WER can be as high as 21.83%. Shallow fusion with biasing words provides significant improvement on this dataset, which is nearly 50% relative WER reduction. Note that shallow fusion is insensitive to the size \\\\( N \\\\) of the biasing words lists. The neural biasing results from two recently published papers [15, 17] and our vanilla neural biasing implementation do not outperform our shallow fusion baseline, even though their \u201cno biasing\u201d results (not shown here) are very close to ours. When contexts are injected to both the 9th and 15th layers, we see improvements over the vanilla neural biasing. When we further perturb the reference transcriptions, we achieve the best neural biasing results, with 60% and 25% B-WER reduction over no biasing and shallow fusion, without degrading U-WER. Also note that neural biasing is more sensitive to the biasing list size \\\\( N \\\\) compared to shallow fusion.\\n\\nOn SPGISpeech (Table 3), with a test set of 100 hours long, our techniques again outperform the baselines. On ConEC, our methods surpass the baseline except for one entity class.\\n\\nTable 3: Contextual ASR on SPGISpeech and ConEC\\n\\n|         | \\\\( N=100 \\\\) | \\\\( N=500 \\\\) |\\n|---------|-------------|-------------|\\n| NO       | 2.10 (1.81/6.60) | 1.85 (1.73/3.57) |\\n| SF       | 1.78 (1.68/3.24) | 1.82 (1.69/3.76) |\\n| +@Layers: 9,15 | 1.71 (1.65/2.56) | 1.79 (1.68/3.35) |\\n\\nConEC WER\\n\\n|         | PERSON | PRODUCT | ORG |\\n|---------|--------|---------|-----|\\n| NO       | 10.41 (8.71/26.02) | 45.9 | 24.25 | 29.54 |\\n| SF       | 10.29 (8.70/24.84) | 39.82 | 21.86 | 26.09 |\\n| NB       | 10.66 (8.93/26.44) | 41.38 | 24.85 | 27.46 |\\n| +@Layers: 9,15 | 10.40 (8.76/24.61) | 35.72 | 25.48 | 25.70 |\\n\\nNext, we explore which layers are optimal for integrating the contexts (Table 4). Text perturbation is disabled here. It appears that the combination of layers 9 and 15 yields the best performance. We also measure the wall-clock time to decode LibriSpeech test-other (of 5.3 hours) with a beam size of 4 for beam search. It takes about 3.5 minutes on one NVIDIA Tesla V100 GPU, even when we inject contexts to 4 encoder layers. As a reference, it takes 2.6 minutes for a non-contextual transducer model to decode the same data.\\n\\nFinally, we search for the best probability for perturbing each rare word (Table 5). When the probability takes 0.2, the contextual ASR model has a balanced WER performance for common and rare words.\\n\\nTable 4: Contextual ASR model has a balanced WER performance for common and rare words.\\n\\n|         | \\\\( N=100 \\\\) | \\\\( N=500 \\\\) |\\n|---------|-------------|-------------|\\n| test-clean |            |             |             |\\n| test-other |            |             |             |\\n| NO       | 2.17 (1.25/9.65) | 5.22 (3.32/21.83) | 0.1 1.60 (1.27/4.25) | 3.89 (3.38/8.34) |\\n| 0.2      | 1.50 (1.24/3.56) | 3.69 (3.18/8.19) |\\n| 0.4      | 1.65 (1.40/3.66) | 3.81 (3.45/6.99) |\\n\\nTable 5: The impact of the probability for text perturbation on LibriSpeech (\\\\( N=500 \\\\), @Layers: 9,15)\\n\\n7. Conclusion\\n\\nIn this paper, we apply two techniques to improve cross attention based neural biasing for contextual ASR. First, we inject contexts into intermediate encoder layers in addition to the last layer. Second, during training, we replace the rare words with their similar-sounding alternative spellings in both the reference transcription and contexts. The techniques yield significant improvement in recognizing rare words and named entities on three datasets, including a real-world contextual ASR test set. Future work may explore using advanced alternative spellings generators, shortening the biasing lists or reducing the sensitivity of contextual ASR models to the distractors.\\n\\n6. Acknowledgements\\n\\nThe authors would like to acknowledge the helpful discussion and support from Jing Liu, Mingzhi Yu, Grant Strimel, and Ariya Rastrow. This work was supported by a fellowship from JHU + Amazon Initiative for Interactive AI (AI2AI).\"}"}
{"id": "huang24f_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. D. Fox and N. Delworth, \u201cImproving contextual recognition of rare words with an alternate spelling prediction model,\u201d INTERSPEECH, 2022.\\n\\n[2] S. Dingliwal, M. Sunkara, S. Ronanki, J. J. Farris, K. Kirchhoff et al., \u201cPersonalization of CTC speech recognition models,\u201d SLT, 2023.\\n\\n[3] Z. Lei, E. Pusateri, S. Han, L. Liu, M. Xu et al., \u201cPersonalization of CTC-based end-to-end speech recognition using pronunciation-driven subword tokenization,\u201d ICASSP, 2024.\\n\\n[4] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and D. Zhao, \u201cDeep context: End-to-end contextual speech recognition,\u201d SLT, 2018.\\n\\n[5] A. Bruguier, R. Prabhavalkar, G. Pundak, and T. N. Sainath, \u201cPhoebe: Pronunciation-aware contextualization for end-to-end speech recognition,\u201d ICASSP, 2019.\\n\\n[6] C. Huber, J. Hussain, S. St\u00fcker, and A. H. Waibel, \u201cInstant one-shot word-learning for context-specific neural sequence-to-sequence speech recognition,\u201d ASRU, 2021.\\n\\n[7] Z. Zhang and P. Zhou, \u201cEnd-to-end contextual asr based on posterior distribution adaptation for hybrid ctc/attention system,\u201d ArXiv, vol. abs/2202.09003, 2022.\\n\\n[8] M. Jain, G. Keren, J. Mahadeokar, and Y. Saraf, \u201cContextual RNN-T for open domain ASR,\u201d INTERSPEECH, 2020.\\n\\n[9] D. Le, G. Keren, J. Chan, J. Mahadeokar, C. Fuegen et al., \u201cDeep shallow fusion for RNN-T personalization,\u201d SLT, 2020.\\n\\n[10] D. Le, M. Jain, G. Keren, S. Kim, Y. Shi et al., \u201cContextualized streaming end-to-end speech recognition with trie-based deep bilog and shallow fusion,\u201d INTERSPEECH, 2021.\\n\\n[11] F.-J. Chang, J. Liu, M. H. Radfar, A. Mouchtaris, M. Omologo et al., \u201cContext-aware transformer transducer for speech recognition,\u201d ASRU, 2021.\\n\\n[12] K. M. Sathyendra, T. Muniyappa, F.-J. Chang, J. Liu, J. Su et al., \u201cContextual adapters for personalized speech recognition in neural transducers,\u201d ICASSP, 2022.\\n\\n[13] G. Sun, C. Zhang, and P. C. Woodland, \u201cMinimising biasing word errors for contextual ASR with the tree-constrained pointer generator,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, 2022.\\n\\n[14] R. Pandey, R. Ren, Q. Luo, J. Liu, A. Rastrow et al., \u201cProcter: Pronunciation-aware contextual adapter for personalized speech recognition in neural transducers,\u201d ICASSP, 2023.\\n\\n[15] X. Yang, W. Kang, Z. Yao, Y. Yang, L. Guo et al., \u201cPromptASR for contextualized ASR with controllable style,\u201d ICASSP, 2024.\\n\\n[16] H. Futami, E. Tsunoo, Y. Kashiwagi, H. Ogawa, S. Arora et al., \u201cPhoneme-aware encoding for prefix-tree-based contextual ASR,\u201d ICASSP, 2024.\\n\\n[17] J. Tang, K. Kim, S. Shon, F. Wu, P. Sridhar et al., \u201cImproving ASR contextual biasing with guided attention,\u201d ICASSP, 2024.\\n\\n[18] Z. Chen, H. Huang, A. Andrusenko, O. Hrinchuk, K. C. Puvvada et al., \u201cSALM: Speech-augmented language model with in-context learning for speech recognition and translation,\u201d ICASSP, 2024.\\n\\n[19] C. Sun, Z. Ahmed, Y. Ma, Z. Liu, Y. Pang et al., \u201cContextual biasing of named-entities with large language models,\u201d ICASSP, 2024.\\n\\n[20] E. Lakomkin, C. Wu, Y. Fathullah, O. Kalinli, M. L. Seltzer et al., \u201cEnd-to-end speech recognition contextualization with large language models,\u201d ICASSP, 2024.\\n\\n[21] K. Everson, Y. Gu, C.-H. H. Yang, P. G. Shivakumar, G.-T. Lin et al., \u201cTowards ASR robust spoken language understanding through in-context learning with word confusion networks,\u201d ICASSP, 2024.\\n\\n[22] P. Pramanick and C. Sarkar, \u201cCan visual context improve automatic speech recognition for an embodied agent?\u201d EMNLP, 2022.\\n\\n[23] S. N. Ray, S. Mitra, R. Bilgi, and S. Garimella, \u201cImproving RNN-T ASR performance with date-time and location awareness,\u201d ArXiv, vol. abs/2106.06183, 2021.\\n\\n[24] D. Zhao, T. N. Sainath, D. Rybach, P. Rondon, D. Bhatia et al., \u201cShallow-fusion end-to-end contextual biasing,\u201d INTERSPEECH, 2019.\\n\\n[25] W. Wang, Z. Wu, D. Caseiro, T. Munkhdalai, K. C. Sim et al., \u201cContextual biasing with the Knuth-Morris-Pratt matching algorithm,\u201d ArXiv, vol. abs/2310.00178, 2023.\\n\\n[26] D. Le, T. Koehler, C. Fuegen, and M. L. Seltzer, \u201cG2G: TTS-driven pronunciation learning for graphemic hybrid ASR,\u201d ICASSP, 2019.\\n\\n[27] I. Williams, A. Kannan, P. S. Aleksic, D. Rybach, and T. N. Sainath, \u201cContextual speech recognition in end-to-end neural network systems using beam search,\u201d INTERSPEECH, 2018.\\n\\n[28] X. Wang, Y. Liu, J. Li, V. Miljanic, S. Zhao et al., \u201cTowards contextual spelling correction for customization of end-to-end speech recognition systems,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, 2022.\\n\\n[29] Z. Wu, T. Munkhdalai, P. Rondon, G. Pundak, K. C. Sim et al., \u201cDual-Mode NAM: Effective top-k context injection for end-to-end ASR,\u201d INTERSPEECH, 2023.\\n\\n[30] R. Huang, O. Abdel-Hamid, X. Li, and G. Evermann, \u201cClass LM and word mapping for contextual biasing in end-to-end ASR,\u201d in INTERSPEECH, 2020.\\n\\n[31] G. Chen, O. Yilmaz, J. Trmal, D. Povey, and S. Khudanpur, \u201cUsing proxies for oov keywords in the keyword search task,\u201d ASRU, 2013.\\n\\n[32] U. Alon, G. Pundak, and T. N. Sainath, \u201cContextual speech recognition with difficult negative training examples,\u201d ICASSP, 2019.\\n\\n[33] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibriSpeech: An ASR corpus based on public domain audio books,\u201d ICASSP, 2015.\\n\\n[34] P. K. O'Neill, V. Lavrukhin, S. Majumdar, V. Noroozi, Y. Zhang et al., \u201cSPGISpeech: 5,000 hours of transcribed financial audio for fully formatted end-to-end speech recognition,\u201d INTERSPEECH, 2021.\\n\\n[35] R. Huang, M. Yarmohammadi, J. Trmal, J. Liu, D. Raj et al., \u201cConEC: Earnings call dataset with real-world contexts for benchmarking contextual speech recognition,\u201d LREC, 2024.\\n\\n[36] A. Graves, \u201cSequence transduction with recurrent neural networks,\u201d ICML Workshop on Representation Learning, 2012.\\n\\n[37] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d INTERSPEECH, 2020.\\n\\n[38] M. R. Ghodsi, X. Liu, J. A. Apfel, R. Cabrera, and E. Weinstein, \u201cRnn-transducer with stateless prediction network,\u201d ICASSP, 2020.\\n\\n[39] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph et al., \u201cSpecAugment: A simple data augmentation method for automatic speech recognition,\u201d in INTERSPEECH, 2019.\\n\\n[40] M. Rio, N. Delworth, R. Westerman, M. Huang, N. Bhandari et al., \u201cEarnings-21: A practical benchmark for ASR in the wild,\u201d INTERSPEECH, 2021.\\n\\n[41] Z. Yao, L. Guo, X. Yang, W. Kang, F. Kuang et al., \u201cZipformer: A faster and better encoder for automatic speech recognition,\u201d ICLR, 2024.\\n\\n[42] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d NAACL, 2019.\"}"}
