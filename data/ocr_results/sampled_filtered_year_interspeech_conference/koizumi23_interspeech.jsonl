{"id": "koizumi23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. van den Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, \u201cWaveNet: A generative model for raw audio,\u201d arXiv:1609.03499, 2016.\\n\\n[2] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan, R. A. Saurous, Y. Agiomvrgiannakis, and Y. Wu, \u201cNatural TTS synthesis by conditioning WaveNet on mel spectrogram predictions,\u201d in Proc. ICASSP, 2018.\\n\\n[3] Y. Jia, H. Zen, J. Shen, Y. Zhang, and Y. Wu, \u201cPnG BERT: Augmented BERT on phonemes and graphemes for neural TTS,\u201d in Proc. Interspeech, 2021.\\n\\n[4] I. Elias, H. Zen, J. Shen, Y. Zhang, Y. Jia, R. J. Weiss, and Y. Wu, \u201cParallel Tacotron: Non-autoregressive and controllable TTS,\u201d in Proc. ICASSP, 2021.\\n\\n[5] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastSpeech: Fast, robust and controllable text to speech,\u201d in Proc. NeurIPS, 2019.\\n\\n[6] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastSpeech 2: Fast and high-quality end-to-end text to speech,\u201d in Proc. Int. Conf. Learn. Represent. (ICLR), 2021.\\n\\n[7] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in Proc. NeurIPS, 2020.\\n\\n[8] Y. Koizumi, K. Yatabe, H. Zen, and M. Bacchiani, \u201cWaveFit: An iterative and non-autoregressive neural vocoder based on fixed-point iteration,\u201d in Proc. IEEE SLT, 2023.\\n\\n[9] T. Hayashi, R. Yamamoto, K. Inoue, T. Yoshimura, S. Watanabe, T. Toda, K. Takeda, Y. Zhang, and X. Tan, \u201cESPnet-TTS: Unified, reproducible, and integratable open source end-to-end text-to-speech toolkit,\u201d in Proc. ICASSP, 2020.\\n\\n[10] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. De Mori, and Y. Bengio, \u201cSpeechBrain: A general-purpose speech toolkit,\u201d arXiv:2106.04624, 2021.\\n\\n[11] J. Shen, Y. Jia, M. Chrzanowski, Y. Zhang, I. Elias, H. Zen, and Y. Wu, \u201cNon-attentive Tacotron: Robust and controllable neural TTS synthesis including unsupervised duration modeling,\u201d arXiv:2010.04301, 2020.\\n\\n[12] S. Maiti and M. I. Mandel, \u201cParametric resynthesis with neural vocoders,\u201d in Proc. IEEE WASPAA, 2019.\\n\\n[13] \u2014\u2014, \u201cSpeaker independence of neural vocoders and their effect on parametric resynthesis speech enhancement,\u201d in Proc. ICASSP, 2020.\\n\\n[14] T. Saeki, S. Takamichi, T. Nakamura, N. Tanji, and H. Saruwatari, \u201cSelfRemaster: Self-supervised speech restoration with analysis-by-synthesis approach using channel modeling,\u201d in Proc. Interspeech, 2022.\\n\\n[15] J. Su, Z. Jin, and A. Finkelstein, \u201cHiFi-GAN: High-fidelity denoising and dereverberation based on speech deep features in adversarial networks,\u201d in Proc. Interspeech, 2020.\\n\\n[16] \u2014\u2014, \u201cHiFi-GAN-2: Studio-quality speech enhancement via generative adversarial networks conditioned on acoustic features,\u201d in Proc. IEEE WASPAA, 2021.\\n\\n[17] H. Liu, X. Liu, Q. Kong, Q. Tian, Y. Zhao, D. Wang, C. Huang, and Y. Wang, \u201cVoiceFixer: A unified framework for high-fidelity speech restoration,\u201d in Proc Interspeech, 2022.\\n\\n[18] J. Serr `a, S. Pascual, J. Pons, R. O. Araz, and D. Scaini, \u201cUniversal speech enhancement with score-based diffusion,\u201d arXiv:2206.03065, 2022.\\n\\n[19] H. Zen, R. Clark, R. J. Weiss, V. Dang, Y. Jia, Y. Wu, Y. Zhang, and Z. Chen, \u201cLibriTTS: A corpus derived from LibriSpeech for text-to-speech,\u201d in Proc. Interspeech, 2019.\\n\\n[20] Y. Koizumi, H. Zen, S. Karita, Y. Ding, K. Yatabe, N. Morioka, Y. Zhang, W. Han, A. Bapna, and M. Bacchiani, \u201cMiipher: A robust speech restoration model integrating self-supervised speech and text representations,\u201d arXiv:2303.01664, 2023.\\n\\n[21] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cw2v-BERT: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,\u201d in Proc. IEEE ASRU, 2021.\\n\\n[22] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015.\\n\\n[23] K. Ito and L. Johnson, \u201cThe lj speech dataset,\u201d https://keithito.com/LJ-Speech-Dataset/, 2017.\\n\\n[24] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-tts: A generative flow for text-to-speech via monotonic alignment search,\u201d in Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2020.\\n\\n[25] R. Valle, K. J. Shih, R. Prenger, and B. Catanzaro, \u201cFlowtron: an autoregressive flow-based generative network for text-to-speech synthesis,\u201d in Proc. Int. Conf. Learn. Represent. (ICLR), 2021.\\n\\n[26] E. Casanova, J. Weber, C. Shulby, A. C. Junior, E. G \u00a8olge, and M. Antonelli Ponti, \u201cYourTTS: Towards zero-shot multispeaker TTS and zero-shot voice conversion for everyone,\u201d arXiv:2112.02418, 2021.\\n\\n[27] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li, L. He, S. Zhao, and F. Wei, \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv:2301.02111, 2023.\\n\\n[28] E. Kharitonov, D. Vincent, Z. Borsos, R. Marinier, S. Girgin, O. Pietquin, M. Sharifi, M. Tagliasacchi, and N. Zeghidour, \u201cSpeak, read and prompt: High-fidelity text-to-speech with minimal supervision,\u201d arXiv:2302.03540, 2023.\\n\\n[29] Y. Koizumi, S. Karita, S. Wisdom, H. Erdogan, J. R. Hershey, L. Jones, and M. Bacchiani, \u201cDF-Conformer: Integrated architecture of Conv-TasNet and Conformer using linear complexity self-attention for speech enhancement,\u201d in Proc. IEEE WASPAA, 2021.\\n\\n[30] Z. Borsos, M. Sharifi, and M. Tagliasacchi, \u201cSpeechPainter: Text-conditioned speech inpainting,\u201d in Proc. Interspeech, 2022.\\n\\n[31] S. Wang, A. Mesaros, T. Heittola, and T. Virtanen, \u201cA curated dataset of urban scenes for audio-visual scene analysis,\u201d in Proc. ICASSP, 2021.\\n\\n[32] J. B. Allen and D. A. Berkley, \u201cImage method for efficiently simulating small-room acoustics,\u201d J. Acoust. Soc. Am., 1979.\\n\\n[33] Y. Zhang, J. Qin, D. S. Park, W. Han, C.-C. Chiu, R. Pang, Q. V. Le, and Y. Wu, \u201cPushing the limits of semi-supervised learning for automatic speech recognition,\u201d in Proc. NeurIPS SAS 2020 Workshop, 2020.\\n\\n[34] Y. Jia, Y. Zhang, R. Weiss, Q. Wang, J. Shen, F. Ren, z. Chen, P. Nguyen, R. Pang, I. Lopez Moreno, and Y. Wu, \u201cTransfer learning from speaker verification to multispeaker text-to-speech synthesis,\u201d in Proc. NeurIPS, 2018.\\n\\n[35] Y. Chen, Y. Assael, B. Shillingford, D. Budden, S. Reed, H. Zen, Q. Wang, L. C. Cobo, A. Trask, B. Laurie, C. Gulcehre, A. van den Oord, O. Vinyals, and N. de Freitas, \u201cSample efficient adaptive text-to-speech,\u201d in Proc. ICLR, 2019.\\n\\n[36] N. Kalchbrenner, W. Elsen, K. Simonyan, S. Noury, N. Casagrande, W. Lockhart, F. Stimberg, A. van den Oord, S. Dieleman, and K. Kavukcuoglu, \u201cEfficient neural audio synthesis,\u201d in Proc. ICML, 2018.\\n\\n[37] D. P. Kingma and J. L. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2015.\"}"}
{"id": "koizumi23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThis paper introduces a new speech dataset called \u201cLibriTTS-R\u201d designed for text-to-speech (TTS) use. It is derived by applying speech restoration to the LibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The constituent samples of LibriTTS-R are identical to those of LibriTTS, with only the sound quality improved. Experimental results show that the LibriTTS-R ground-truth samples showed significantly improved sound quality compared to those in LibriTTS. In addition, neural end-to-end TTS trained with LibriTTS-R achieved speech naturalness on par with that of the ground-truth samples. The corpus is freely available for download from http://www.openslr.org/141/.\\n\\nIndex Terms\\n: Text-to-speech, dataset, speech restoration\\n\\n1. Introduction\\nText-to-speech (TTS) technologies have been rapidly advanced along with the development of deep learning [1\u20136]. With studio-quality recorded speech data, one can train acoustic models [2, 3] and high-fidelity neural vocoders [7, 8]. These have enabled us to synthesize speech in a reading style almost as natural as human speech. In addition, many implementations of the latest TTS models have been published [9,10], and the gateway to TTS research is certainly widening.\\n\\nOne of the remaining barriers to develop high-quality TTS systems is the lack of large and high-quality public dataset. Training of high-quality TTS models requires a large amount of studio-quality data. In several TTS papers, over 100 hours of studio-recorded data have been used [3, 8, 11]. Unfortunately, such studio-recorded datasets are not publicly available, and thus reproducing their results is difficult for others.\\n\\nAt the same time, speech restoration (SR) has advanced using speech generative models [12\u201318]. These state-of-the-art models can convert reverberated lecture and historical speech to studio-recorded quality [16\u201318]. Inspired by these results, we came up with an idea that the above-mentioned barrier can be removed by applying SR to public datasets.\\n\\nWith this paper, we publish LibriTTS-R, a quality-improved version of LibriTTS [19]. LibriTTS is a non-restrictive license multi-speaker TTS corpus consisting of 585 hours of speech data from 2,456 speakers and the corresponding texts. We cleaned LibriTTS by applying a text-informed SR model, Mi- phier [20], that uses w2v-BERT [21] feature cleaner and Wave-Fit neural vocoder [8]. By subjective experiments, we show that the speech naturalness of a TTS model trained with LibriTTS-R is greatly improved from that trained with LibriTTS, and is comparable with that of the ground-truth.\\n\\nLibriTTS-R is publicly available at http://www.openslr.org/141/, with the same non-restrictive license. Audio samples of the ground-truth and TTS generated samples are available at our demo page 1.\\n\\n2. The LibriTTS corpus\\nThe LibriTTS corpus is one of the largest multi-speaker speech datasets designed for TTS use. This dataset consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The audio and text materials are derived from the LibriSpeech corpus [22], which has been used for training and evaluating automatic speech recognition systems. Since the original LibriSpeech corpus has several undesired properties for TTS including sampling rate and text normalization issues, the samples in LibriTTS were re-derived from the original materials (MP3 from LibriVox and texts from Project Gutenberg).\\n\\nOne issue is that the LibriTTS sound quality is not on par with smaller scale but higher quality TTS datasets such as LJspeech [23]. The quality of the TTS output is highly affected by that of the speech samples used in model training. Therefore, the quality of the generated samples of a TTS model trained on LibriTTS doesn't match those of the ground-truth samples [24, 25]. For example, Glow-TTS achieved 3.45 mean-opinion-score (MOS) on LibriTTS where the speech obtained from the ground-truth mel-spectrograms by a vocoder was 4.22 [24]. Note that MOSs on the LJspeech for generated and ground-truth were 4.01 and 4.19, respectively [24]. The results suggest that the quality of speech samples in LibriTTS are inadequate for training of high-quality TTS models.\\n\\n3. Data processing pipeline\\nAlthough noisy TTS datasets are useful for advanced TTS model training [26\u201328], access to large scale high-quality datasets is as equally important for advancing TTS techniques. To provide a public large-scale and high-quality TTS dataset, we apply a SR model to LibriTTS.\\n\\n3.1. Speech restoration model overview\\nOne critical requirement of SR models for the purpose of cleaning datasets is robustness. If the SR model generates a large number of samples with artifacts, it will adversely impact the subsequent TTS model training. Therefore, for our purposes, we need to reduce as much as possible the number of samples that fail to be recovered.\\n\\n1https://google.github.io/df-conformer/librittsr/\"}"}
{"id": "koizumi23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To satisfy this requirement, we use a text-informed para-\\nmetric re-synthesis-based SR model, Miipher [20], as shown in \\nFig. 1. In this model, first, w2v-BERT features are extracted \\nby w2v-BERT [21] from the noisy waveform. Then, a DF-\\nConformer [29]-based feature-cleaner predicts the w2v-BERT \\nfeatures of the clean waveform. Finally, the restored waveform \\nis synthesized using a WaveFit-5 neural vocoder [8].\\n\\nThe reason for selecting Miipher is that it adresses two par-\\nticularly difficult to restore degradation patterns observed in \\nLibriTTS samples. The first degradation is phoneme masking. \\nSpeech signals are sometimes masked by noise and/or rever-\\nerberation, resulting in speech that is difficult to discriminate \\nfrom noise without additional information. The second degra-\\ndation is phoneme deletion. Important frequency parts of some \\nphonemes could be missing from the signal due to non-linear \\naudio processing and/or down-sampling. To address these prob-\\nlems, Miipher introduced two techniques. (i) for the input fea-\\nture, it uses w2v-BERT [21] features instead of log-mel spectro-\\ngram used in a conventional SR model [17], and (ii) to use lin-\\nguistic features conditioning extracted by PnG-BERT [3] from \\nthe transcript corresponding to the noisy speech. Since w2v-\\nBERT is trained on large amounts of degraded speech samples \\nand it improves ASR performance, we expect its effectiveness \\non making SR models robust against speech degradation. In \\naddition, the use of text information improving speech inpaint-\\ning performance [30], we consider that it also improves speech \\nrestoration performance. For the detail, please see the original \\npaper [20].\\n\\nWe trained a Miipher model with a proprietary dataset that \\ncontains 2,680 hours of noisy and studio-quality speech pairs. \\nThe target speech dataset contains 670 hours of studio-recorded \\nAustralian, British, Indian, Nigerian, and North American En-\\nglish at 24 kHz sampling. For the noise dataset, we used \\nthe TAU Urban Audio-Visual Scenes 2021 dataset [31], inter-\\nnally collected noise snippets that simulate conditions like cafe, \\nkitchen, and cars, and noise sources. The noisy utterances were \\ngenerated by mixing randomly selected speech and noise sam-\\nples from these datasets with signal-to-noise ratio (SNR) from \\n5 dB to 30 dB. In addition, we augmented the noisy dataset with \\n4 patterns depending on the presence or absence of reverbera-\\ntion and codec artifacts. A room impulse response (RIR) for \\neach sample was generated by a stochastic RIR generator using \\nthe image method [32]. For simulating codec artifacts, we ran-\\ndomly applied one of MP3, Vorbis, A-law, Adaptive Multi-Rate \\nWideband (AMR-WB), and OPUS with a random bit-rate. The \\nwaveform.\\n\\nTable 1: MOS and SxS test results on the ground-truth samples \\nwith their 95% confidence intervals. A positive SxS score indi-\\ncates that LibriTTS-R was preferred.\\n\\n| Split      | MOS (\u2191) | SxS (\u2191) |\\n|------------|---------|---------|\\n| LibriTTS   | 4.36 \u00b1 0.08 | 4.41 \u00b1 0.07 |\\n| LibriTTS-R | 4.40 \u00b1 0.07 | 4.42 \u00b1 0.08 |\\n\\nWe first pre-trained the feature-cleaner and WaveFit neural vocoder 150k and 1M steps, respectively, where WaveFit \\nwas trained to reconstruct waveform from clean w2v-BERT \\nfeatures. Then, we fine-tuned the WaveFit neural vocoder \\n350k steps using cleaned w2v-BERT features by the pre-trained \\nfeature-cleaner.\\n\\nFirst, we calculated PnG-BERT [3] features from a transcript \\nand a speaker embedding using the speaker encoder described \\nin [20] from the original 24 kHz sampling waveform. Here, for \\nspeech samples with waveform lengths shorter than 2 seconds, \\nthe speaker embedding was calculated after repeating them to \\nget a pseudo longer waveform. Since the w2v-BERT [21] model \\nwas trained on 16 kHz waveforms, we applied down-sampling \\nto the LibriTTS sample for calculating w2v-BERT features. Fi-\\n\\nFinally, we synthesized restored 24 kHz sampling waveform using \\nWaveFit [8].\\n\\n4. Experiments\\n\\n4.1. Subjective experiments for ground-truth samples\\n\\n4.1.1. Experimental setups\\n\\nWe first compared the quality of ground-truth speech samples \\nin LibriTTS-R with those in LibriTTS. We evaluated the sound \\nquality using \u201ctest-clean\u201d and \u201ctest-other\u201d subsets. We ran-\\ndomly selected 620 samples from each subset. Since the \u201ctrain-\\n*\u201d and \u201cdev-*\u201d subsets are also divided into \u201cclean\u201d and \u201cother\u201d \\naccording to the same word-error-rate (WER)-based criteria, the \\nsound quality of the entire dataset can be predicted by evaluat-\\ning the sound quality of these two subsets.\\n\\nTo evaluate subjective quality, we rated speech quality \\nthrough mean-opinion-score (MOS) and side-by-side (SxS) \\npreference tests. We asked to rate the naturalness in MOS test, \\nand \u201cwhich sound quality is better?\u201d in SxS test. The scale \\nof MOS was a 5-point scale (1: Bad, 2: Poor, 3: Fair, 4: Good, \\n5: Excellent) with rating increments of 0.5, and that of SxS was \\na 7-point scale (-3 to 3). Test stimuli were randomly chosen and \\neach stimulus was evaluated by one subject. Each subject was \\nallowed to evaluate up to six stimuli, that is, over 100 subjects \\nparticipated in this experiment to evaluate 640 samples in each \\ncondition. The subjects were paid native English speakers in \\nthe United States. They were requested to use headphones in a \\nquiet room. Audio samples are available in our demo page \\n1.\\n\\n4.1.2. Results\\n\\nTable 1 shows the MOS and SxS test results. In terms of speech \\nnaturalness, LibriTTS achieved high MOSs: 4.36 and 3.94 on \\ntest-clean and test-other, respectively. Although LibriTTS-R\\nachieved better MOSs than LibriTTS in both splits, the differ-\\n\\n...\"}"}
{"id": "koizumi23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2: Log-mel spectrograms of ground-truth waveforms from (top) LibriTTS and (bottom) LibriTTS-R. The left two and right two examples are from \u201ctest-clean\u201d and \u201ctest-other\u201d splits, respectively.\\n\\nFigure 2 shows the 128-dim log-mel spectrogram of speech samples from LibriTTS and LibriTTS-R. We can see that the LibriTTS samples are degraded by a variety of factors even if these are from the test-clean split: from left to right, it can be considered that speech samples were degraded by downsampling, environmental noise, reverberation, and non-linear speech enhancement, respectively. As we can see spectrograms of LibriTTS-R samples, the SR model restored these speech samples into high-quality ones. This could be the reason of the significant differences in the SxS tests.\\n\\nNote that we have found a few examples that LibriTTS speech sample achieved a better score in SxS comparison. By listening these examples, two of 640 LibriTTS-R speech samples were distorted due to the failure of SR. Since it is difficult to manually check all samples, we have not checked all speech samples in LibriTTS-R. Therefore, the samples in training splits may also contain a small number of distorted samples.\\n\\n4.2. Subjective experiments for TTS generated samples\\n4.2.1. Experimental setups\\nWe trained multi-speaker TTS models with the same architecture and the same hyper-parameters using either the LibriTTS or LibriTTS-R corpus. The TTS model was build by concatenating the following acoustic model and neural vocoder without joint fine-tuning.\\n\\n**Acoustic model:** We used a duration unsupervised Non-Attentive Tacotron (NAT) with a fine-grained variational autoencoder (FV AE) [11]. We used the same hyper-parameters and training parameters listed in the original paper [11]. We trained this model for 150k steps with a batch size of 1,024.\\n\\n**Neural vocoder:** We used a WaveRNN [36] which consisted of a single long short-term memory layer with 512 hidden units, 5 convolutional layers with 512 channels as the conditioning stack to process the mel-spectrogram features, and a 10-component mixture of logistic distributions as its output layer. The learning rate was linearly increased to $10^{-4}$ in the first 100 steps then exponentially decayed to $10^{-6}$ from 200k to 300k steps. We trained this model using the Adam optimizer [37] for 500k steps with a batch size of 512.\\n\\nThe TTS model was trained on two types of training datasets: Train-460 and Train-960. Train-460 consists of the \u201ctrain-clean-100\u201d and \u201ctrain-clean-360\u201d subsets, and Train-960 indicates using \u201ctrain-other-500\u201d in addition to Train-460. For the test sentences, we randomly selected 620 evaluation sentences from the test-clean split. We synthesized waveforms with 6 speakers (three female and three male) those are used in the LibriTTS baseline experiments [19]. The female and male reader IDs were (19, 103, 1841) and (204, 1121, 5717), respectively. To evaluate subjective quality, we rated speech naturalness through MOS and side-by-side (SxS) preference tests. The listening test setting was the same as Sec. 4.1 Audio samples of generated speech are available in our demo page 1.\"}"}
{"id": "koizumi23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Log-mel spectrograms of TTS generated waveforms where the multi-speaker TTS model was trained on (top) LibriTTS and (bottom) LibriTTS-R, respectively. The input text was \u201cThe Free State Hotel served as barracks\u201d.\\n\\nTable 2: MOSs for the baseline multi-speaker TTS model outputs with their 95% confidence intervals.\\n\\n| Training dataset | Speaker ID | MOS (\u00b195% CI) |\\n|------------------|------------|---------------|\\n| LibriTTS Train-460 | 19         | 2.38 \u00b1 0.11   |\\n|                  | 204        | 1.84 \u00b1 0.14   |\\n| LibriTTS Train-960 | 19         | 2.51 \u00b1 0.10   |\\n|                  | 204        | 2.20 \u00b1 0.12   |\\n| LibriTTS-R Train-460 | 19       | 3.88 \u00b1 0.09   |\\n|                  | 204       | 3.67 \u00b1 0.09   |\\n| LibriTTS-R Train-960 | 19       | 4.31 \u00b1 0.08   |\\n|                  | 204       | 4.23 \u00b1 0.07   |\\n\\nTable 3: SxS test results on the baseline multi-speaker TTS model outputs with their 95% confidence intervals. A positive score indicates that training on LibriTTS-R was preferred.\\n\\n| Speaker ID | Training dataset | Score (\u00b195% CI) |\\n|------------|------------------|-----------------|\\n| 19         | LibriTTS Train-460 | 2.38 \u00b1 0.11   |\\n|            | LibriTTS Train-960 | 2.51 \u00b1 0.10   |\\n| 204        | LibriTTS Train-460 | 1.84 \u00b1 0.14   |\\n|            | LibriTTS Train-960 | 2.20 \u00b1 0.12   |\\n\\n4.2.2. Results\\n\\nTable 2 shows the MOS results. In all speaker IDs except for ID 19, the TTS model using LibriTTS-R Train-960 as the training dataset achieved the highest MOSs. For Speaker ID 19, the model using LibriTTS-R Train-460 achieved the highest MOS, which was not significantly different from that using LibriTTS-R Train-960. In other speaker IDs, MOSs of LibriTTS-R Train-960 were significantly better than that of LibriTTS-R Train-460. This trend was not observed in LibriTTS, rather in some cases, MOS was decreased by using LibriTTS Train-960. The reason for this degradation might be because that the \u201ctrain-other-500\u201d split contains a lot of degraded speech samples. This result suggests that the use of LibriTTS \u201ctrain-other-500\u201d split rather degrades the output quality of the TTS. In contrast, speech samples in LibriTTS-R \u201ctrain-other-500\u201d split are restored to high-quality speech samples, and resulting in that enables us to use a large amount of high-quality training data and improved the naturalness of the TTS outputs. In addition, the TTS model trained on LibriTTS-R Train-960 achieved MOSs on a par with human spoken speech samples in LibriTTS, effects of a few distorted speech samples in the training can be considered as not significant.\\n\\nTable 3 shows the SxS results. We observed that the use of LibriTTS-R also improve not only naturalness but also the sound quality of TTS outputs. Figure 3 shows 128-dim log-mel spectrograms of TTS outputs. We can see the harmonic structure is broken in the ID 5717 output of the TTS model trained on LibriTTS (top right). The presence of such a sample could be the reason for the lower naturalness scores on the MOS test. Also, from ID 103 and 1121 examples, we can observe background noise in the output of TTS model trained on LibriTTS. Such background noise does not exist in the outputs of TTS model trained on LibriTTS-R. From these results, we conclude that the LibriTTS-R corpus is a better TTS corpus than the LibriTTS corpus, and enables us to train a high-quality TTS model.\\n\\n5. Conclusions\\n\\nThis paper introduced LibriTTS-R, a sound quality improved version of LibriTTS [19]. We cleaned speech samples in the LibriTTS corpus by applying an SR model [20]. By subjective experiments, we show that the speech naturalness of a TTS model trained with LibriTTS-R is improved from that trained with LibriTTS, and is comparable with that of the ground-truth. This corpus is released online, and it is freely available for download from http://www.openslr.org/141/. We hope that the release of this corpus accelerates TTS research.\"}"}
