{"id": "netzorg24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgements\\n\\nThis work was supported by the UC Noyce Initiative, Society of Hellman Fellows, NSF, NIH/NIDCD and the Schwab Innovation Fund.\\n\\n8. References\\n\\n[1] V. A. Trinh, P. Ghahremani, B. King, J. Droppo, A. Stolcke, and R. Maas, \\\"Reducing geographic disparities in automatic speech recognition via elastic weight consolidation,\\\" in Interspeech 2022. ISCA, Sep. 2022. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2022-11063\\n\\n[2] T. Feng, R. Hebbar, N. Mehlman, X. Shi, A. Kommineni, and S. Narayanan, \\\"A review of speech-centric trustworthy machine learning: Privacy, safety, and fairness,\\\" APSIPA Transactions on Signal and Information Processing, vol. 12, no. 3, 2023. [Online]. Available: http://dx.doi.org/10.1561/116.00000084\\n\\n[3] Z. Ju, Y. Wang, K. Shen, X. Tan, D. Xin, D. Yang, Y. Liu, Y. Leng, K. Song, S. Tang, Z. Wu, T. Qin, X.-Y. Li, W. Ye, S. Zhang, J. Bian, L. He, J. Li, and S. Zhao, \\\"Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models,\\\" 2024.\\n\\n[4] A. Vyas, B. Shi, M. Le, A. Tjandra, Y.-C. Wu, B. Guo, J. Zhang, X. Zhang, R. Adkins, W. Ngan, J. Wang, I. Cruz, B. Akula, A. Akinyemi, B. Ellis, R. Moritz, Y. Yungster, A. Rakotoarison, L. Tan, C. Summers, C. Wood, J. Lane, M. Williamson, and W.-N. Hsu, \\\"Audiobox: Unified audio generation with natural language prompts,\\\" 2023.\\n\\n[5] N. Lavan, \\\"The time course of person perception from voices: A behavioral study,\\\" Psychological Science, vol. 34, no. 7, pp. 771\u2013783, 2023.\\n\\n[6] E. J. Bush, B. I. Krueger, M. Cody, J. D. Clapp, and V. D. Novak, \\\"Considerations for voice and communication training software for transgender and nonbinary people,\\\" Journal of Voice, 2022. [Online]. Available: https://www.sciencedirect.com/science/article/pii/S0892199722000686\\n\\n[7] C. Brennan, A. C. Hannah, J. Romick, J. W. Lewon, and C. Meyers, \\\"Differences and similarities in the perception of voice gender for individuals who are or are not members of the lgbt+ community,\\\" Journal of Voice, 2022.\\n\\n[8] R. K. Adler, S. Hirsch, and J. Pickering, Voice and communication therapy for the transgender/gender diverse client: A comprehensive clinical guide. Plural Publishing, 2018.\\n\\n[9] A. Z. Huff, \\\"Modern responses to traditional pitfalls in gender affirming behavioral voice modification,\\\" Otolaryngologic Clinics of North America, vol. 55, no. 4, pp. 727\u2013738, 2022.\\n\\n[10] J. Lian, C. Zhang, and D. Yu, \\\"Robust disentangled variational speech representation learning for zero-shot voice conversion,\\\" in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 6572\u20136576.\\n\\n[11] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \\\"X-vectors: Robust dnn embeddings for speaker recognition,\\\" in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018, pp. 5329\u20135333.\\n\\n[12] N. Dawalatabad, M. Ravanelli, F. Grondin, J. Thienpondt, B. Desplanques, and H. Na, \\\"Ecapa-tdnn embeddings for speaker diarization,\\\" in Interspeech 2021. ISCA, Aug. 2021. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2021-941\\n\\n[13] M. Panariello, F. Nespoli, M. Todisco, and N. Evans, \\\"Speaker anonymization using neural audio codec language models,\\\" in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 4725\u20134729.\\n\\n[14] F. Fang, X. Wang, J. Yamagishi, I. Echizen, M. Todisco, N. Evans, and J.-F. Bonastre, \\\"Speaker anonymization using x-vector and neural waveform models,\\\" arXiv preprint arXiv:1905.13561, 2019.\\n\\n[15] Z. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, \\\"Prompttts: Controllable text-to-speech with text descriptions,\\\" 2022.\\n\\n[16] R. Netzorg, B. Yu, A. Guzman, P. Wu, L. McNulty, and G. K. Anumanchipalli, \\\"Towards an interpretable representation of speaker identity via perceptual voice qualities,\\\" in ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2024, pp. 12 391\u201312 395.\\n\\n[17] M. Hope and J. Lilley, \\\"Gender expansive listeners utilize a non-binary, multidimensional conception of gender to inform voice gender perception,\\\" Brain and language, vol. 224, p. 105049, 2022.\\n\\n[18] D. V. Dolquist and B. Munson, \\\"Clinical focus: The development and description of a palette of transmasculine voices,\\\" American journal of speech-language pathology, vol. 33, no. 3, pp. 1113\u20131126, 2024.\\n\\n[19] M. Hope, C. Ward, and J. Lilley, \\\"Nonbinary american english speakers encode gender in vowel acoustics,\\\" 2023.\\n\\n[20] B. M. Merritt, Perceptual representation of speaker gender. Indiana University, 2022.\\n\\n[21] D. Markova, L. Richer, M. Pangelinan, D. H. Schwartz, G. Leonard, M. Perron, G. B. Pike, S. Veillette, M. M. Chakravarty, Z. Pausova et al., \\\"Age-and sex-related variations in vocal-tract morphology and voice acoustics during adolescence,\\\" Hormones and behavior, vol. 81, pp. 84\u201396, 2016.\\n\\n[22] R. D. Kent, \\\"Vocal tract acoustics,\\\" Journal of Voice, vol. 7, no. 2, pp. 97\u2013117, 1993.\\n\\n[23] G. B. Kempster, B. R. Gerratt, K. V. Abbott, J. Barkmeier-Kraemer, and R. E. Hillman, \\\"Consensus auditory-perceptual evaluation of voice: Development of a standardized clinical protocol,\\\" American Journal of Speech-Language Pathology, vol. 18, no. 2, pp. 124\u2013132, 2009.\\n\\n[24] B. Desplanques, J. Thienpondt, and K. Demuynck, \\\"Ecapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\\\" in Interspeech 2020. ISCA, Oct. 2020. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2020-2650\\n\\n[25] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, J.-C. Chou, S.-L. Yeh, S.-W. Fu, C.-F. Liao, E. Rastorgueva, F. Grondin, W. Aris, H. Na, Y. Gao, R. D. Mori, and Y. Bengio, \\\"SpeechBrain: A general-purpose speech toolkit,\\\" 2021, arXiv:2106.04624.\\n\\n[26] E. Harper, S. Majumdar, O. Kuchaiev, L. Jason, Y. Zhang, E. Bakhturina, V. Noroozi, S. Subramanian, K. Nithin, H. Jocelyn, F. Jia, J. Balam, X. Yang, M. Livne, Y. Dong, S. Naren, and B. Ginsburg, \\\"Nemo: a toolkit for conversational ai and large language models,\\\" 2024. [Online]. Available: https://nvidia.github.io/NeMo/\\n\\n[27] S. Ellis, S. Goetze, and H. Christensen, \\\"Moving towards non-binary gender identification via analysis of system errors in binary gender classification,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[28] J. Yamagishi, C. Veaux, and K. MacDonald, \\\"CSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92).\\\" University of Edinburgh. The Centre for Speech Technology Research (CSTR), 2019.\\n\\n[29] X. Xiang, S. Wang, H. Huang, Y. Qian, and K. Yu, \\\"Margin matters: Towards more discriminative deep neural network embeddings for speaker recognition,\\\" in 2019 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC). IEEE, 2019, pp. 1652\u20131656.\\n\\n[30] A. Nagrani, J. S. Chung, and A. Zisserman, \\\"Voxceleb: A large-scale speaker identification dataset,\\\" in Interspeech 2017. ISCA, Aug. 2017. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2017-950\\n\\n[31] J. Gill-Peterson, A Short History of Trans Misogyny. Verso Books, 2024.\"}"}
{"id": "netzorg24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As experts in voice modification, trans-feminine gender-affirming voice teachers have unique perspectives on voice that confound current understandings of speaker identity. To demonstrate this, we present the Versatile Voice Dataset (VVD), a collection of three speakers modifying their voices along gendered axes. The VVD illustrates that current approaches in speaker modeling, based on categorical notions of gender and a static understanding of vocal texture, fail to account for the flexibility of the vocal tract. Utilizing publicly-available speaker embeddings, we demonstrate that gender classification systems are highly sensitive to voice modification, and speaker verification systems fail to identify voices as coming from the same speaker as voice modification becomes more drastic. As one path towards moving beyond categorical and static notions of speaker identity, we propose modeling individual qualities of vocal texture such as pitch, resonance, and weight.\\n\\nIndex Terms: speaker identity, voice modification, speech perception\\n\\n1. Introduction\\n\\nSpeech systems are brittle. Historically, unseen speakers, accent dialect variation, and different microphone conditions can greatly affect the performance of speech systems across a variety of different tasks [1, 2]. In recent years, with the advent of large models, this brittleness has been reduced. Large foundation models can now robustly capture differences between speakers, environments, and recording devices [3, 4]. As accuracy across diverse tasks improves, the question naturally arises of whether these current models are sufficiently robust to the variability of the human voice.\\n\\nThere are few in the world more aware of the variability of voice than trans and gender-diverse individuals. In their daily lives, trans and gender-diverse individuals regularly engage with the fact that listeners come to conclusions about the gender of an individual's voice in less than a second [5], often resulting in gender judgements that override other aspects of presentation. Since feminizing hormone replacement therapy does not affect voice, trans women, for example, seeking to feminize their voices must either undergo surgical procedures or behavioral modification. As such, whether out of personal interest or necessity, trans women experiment daily with the production and perception of voice, becoming all too familiar with the nuances of vocal identity.\\n\\nDespite its importance, gender-affirming voice modification is a notoriously difficult task within the trans and gender-diverse community [6], with experienced clinicians being rare [7] and gender-affirming voice training only just starting to be studied by the scientific community [8]. Due to these limitations, many trans individuals seek training from informal online sources, such as tutorial videos or private coaches [6], which we will group together as the Informal Trans Voice Training Community (IVTC). Spanning the Internet, the IVTC is a broad community across Discord servers, YouTube channels, and Reddit forums. These sources primarily consist of private individuals, many of whom are trans themselves, teaching techniques they developed or adapted from other sources for their own voice training, often without institutional or formal training [9]. Each subcommunity uses similar language to describe perceptual qualities of voice, such as resonance and weight, but lack of formalized research leads to disagreement upon the precise importance and definitions of these qualities.\\n\\nExploring perspectives from the Informal Trans Voice Community, we bring together three gender-affirming voice teachers from different subcommunities of the IVTC to record and analyze audio clips demonstrating the versatility of voice, creating the Versatile Voice Dataset (VVD). With the VVD, we further demonstrate how current approaches to speaker modeling fall short when drawing inferences from diverse voices produced by a single speaker.\\n\\nWith training, the human voice is incredibly flexible, and the limits of intra-speaker vocal flexibility are unknown. The diversity of single speaker voices we present in this work pose a thrilling challenge and new direction for the speech community. By centering the experiences of the transgender and gender-diverse community, we have the opportunity to model and understand the full capabilities of the human voice.\\n\\nOur contributions are the following:\\n1. The creation of the Versatile Voice Dataset, a collection of 3 trans-feminine voice teachers reading the 6 CAPE-V sentences in 27 different configurations of pitch, resonance, and weight.\\n2. A perceptual study of the VVD, where we explore how well non-expert and expert listeners can perform speaker verification on novel and diverse voices.\\n3. An evaluation of current speaker embeddings on gender and speaker identity modeling, and possible next steps via modeling vocal texture.\\n\\n2. Related Work\\n\\n2.1. Speaker Identity Modeling\\n\\nModeling speaker identity is a fundamental task in speech processing, with applications to downstream tasks like voice conversion [10], speaker recognition [11], diarization [12], and anonymization [13, 14]. While there is much work to produce new voices and manipulate speaker or style embeddings in voice synthesis and modification [3, 15], voices...\"}"}
{"id": "netzorg24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"often treated as unique identifiers of speakers, with intra-\\nspeaker modification contained to emotive speech [3]. De-\\nscriptions of speaker identity are often broad, demographic\\ncategories such as 'Male/Female', 'Child/Adult/Elderly', or\\n'Healthy/Pathological'. There has been some recent work at-\\ntempting to produce speaker embeddings that describe voices\\nalong perceptual axes [16], but this prior work is based on cis-\\ngender voices, which omits a large swath of perceptual diversity.\\n\\n2.2. Trans and Gender-Expansive Speech\\n\\nUnderstanding the similarities and differences between trans,\\ngender-expansive and cis-normative speech has gained interest\\nin recent years. Prior work has noted that trans and gender-\\nexpansive individuals tend to use more diverse vocabulary and\\nvaried scalar ratings when describing the gender of an individ-\\nual's voice [7, 17]. These differences in perception are primar-\\nily attributed to trans and gender-expansive individuals' use of\\nvoice to present their gender identities.\\n\\nDatasets that focus on exploring the acoustic properties of\\ntrans and gender-expansive speech are only just starting to be\\ndeveloped. A Palette of Voices for Transmasculine Individu-\\nals [18] and the Mid-Atlantic Gender-Expansive Speech Cor-\\npus [19] are, to the best of our knowledge, the only two pub-\\nlicly available datasets of trans and gender-expansive speech\\ncurrently available. While more exist, they are usually regional\\nand privately held to protect the identity of trans and gender-\\nexpansive individuals [20]. Analyses of trans and gender-\\nexpansive datasets focus on exploring inter-speaker acoustic\\nvariability across acoustic measures that correlate with gender,\\nsuch as pitch or formants.\\n\\nWhile prior datasets are designed to explore inter-speaker\\nvariability, we are primarily interested in exploring intra-\\nspeaker variability, as seen through the lens of gender-affirming\\nvoice modification, in order to overcome the limitations of ex-\\nisting speaker identity models.\\n\\n3. Versatile Voice Dataset\\n\\nIn this section, we present the Versatile Voice Dataset (VVD),\\nthe first public dataset of speakers drastically modifying the per-\\nceived speaker identity of their own voice. We first define the\\nterms pitch, resonance, and weight as voice teachers within the\\nITVC understand them, and then describe the collection process\\nfor the VVD. Samples from the VVD are provided online\\n1\\n\\n3.1. Pitch, Resonance, and Weight\\n\\nAs these qualities have been found to be extremely important\\nin gender perception [21], pitch, resonance and weight are the\\nmost commonly used concepts in the ITVC to teach behavioral\\nvoice modification. Connecting the concepts to acoustic and\\nphysical phenomenon, pitch is understood as the fundamental\\nfrequency, resonance as the acoustic qualities of the overall vo-\\ncal tract shape (pharyngeal, oral, and nasal) [22], and weight as\\nthe sound quality associated with the vocal fold vibratory mass,\\nwhich correlates with the closed quotient [9].\\n\\nWhile pitch, resonance, and weight as concepts are mostly\\nagreed upon across the ITVC, there is more ambiguity in these\\nterms when it comes to precise definitions and perceptual de-\\nscriptions, especially for weight. Speakers 001 and 003 describe\\nweight as a \u201cbuzzy\u201d sound, whereas 002 describes it as more of\\na \u201crumble\u201d, only sounding buzzy under high adduction, high\\nresonance configurations. These disagreements serve as excit-\\ning stepping stones for future perceptual studies.\\n\\nIt is important to note that many teachers in the ITVC high-\\nlight different aspects of voice than those typically emphasized\\nin speech language pathology. Rather than an emphasis on oral\\ncavity resonance and pitch as in speech language pathology [8],\\nmany in the ITVC choose to focus on pharyngeal resonance,\\nand use the concept of weight to balance pitch modification [9].\\n\\n3.2. Data Collection\\n\\nAlong self-interpreted configurations of low, medium, and high\\nfor pitch, resonance, and weight, 3 trans-feminine gender-\\naffirming voice teachers produced up to 27 voices. With each\\nvoice configuration, the voice teacher read the six CAPE-V sen-\\ntences aloud [23], for a total of 14.5 minutes of speech. Sum-\\nmary of speaker information is provided in Table 1.\\n\\nAs the ITVC is broad, the three voice teachers had not in-\\nteracted before the collection of this data and had not heard each\\nothers' voices. The ambiguous instructions of low, medium, and\\nhigh configurations led to each teacher interpreting the terms\\nand task slightly differently. Speaker 001, for example, speaks\\nwith a yawn-like voice in the low-low-low configuration, while\\nSpeaker 002 and 003 speak in more natural-sounding voices.\\n\\nSimilarly, different interpretations of weight lead to voice vari-\\nation, reflected in Speaker 002 omitting med-high-high and\\nhigh-high-high configurations to avoid vocal strain. Without an\\nexternal and consistent measurement of pitch, resonance, and\\nweight, these discrepancies lead to difficulties in drawing con-\\nclusions on inter-speaker comparisons such as naturalness of\\nspeech across configurations.\\n\\nStandardizing the sampling rate to 16kHz is the only\\npre-processing performed on the VVD clips. No other pre-\\nprocessing was performed in order to preserve each speaker's\\ninterpretation of pitch, resonance, and weight. As all audio clips\\nwere provided by authors of this paper, no ethics approval was\\nnecessary.\\n\\n4. Limitations of Current Speaker\\n\\nEmbeddings\\n\\nHere, we explore how intra-speaker variation in the VVD af-\\nects state-of-the-art ECAPA-TDNN speaker embeddings [24]\\nfrom SpeechBrain [25] and NeMo [26] on two tasks in speaker\\nmodeling: Gender Classification and Speaker Verification.\\n\\n4.1. Gender Classification\\n\\nIn many speech models, gender information is used as a proxy\\nfor vocal texture, converting the high-dimensional space of vo-\\ncal acoustics into a small number of categories. The utility of\\nthese categories is based on prior work on gendered differences\\nin speech [9] and the ability of gender classification models\\nto accurately infer gender categories from speech [27]. Using\\nScikit-Learn\u2019s default parameter settings, Random Forest mod-\\neling predicting speaker gender in VCTK [28] from either NeMo\\nor SpeechBrain ECAPA-TDNN embeddings achieve high accu-\\nracies of 99.1% and 97.0% accuracy on the test split of VCTK,\\nrespectively.\\n\\nUsing gender as a proxy, however, begins to lose its pre-\\ndictive power with more vocal diversity. Visualizing the gender\\nclassifiers' predictions over the VVD in Figure 1, we see that\\nthe predictions across voices span almost the entire range of 0\\n(Female) to 1 (Male) for both models and all speakers.\"}"}
{"id": "netzorg24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Speaker | Pronouns | Accent | Voices Produced | Microphone\\n--- | --- | --- | --- | ---\\n001 | She/Her | Californian American | 27 | AT2020\\n002 | Any | Australian | 25 | RZ19-03450100\\n003 | She/Her | Californian American | 27 | AKG P120\\n\\nTable 1: Speaker Information for the Three Speakers in the Versatile Voice Dataset.\\n\\n![Figure 1](image_url)\\n\\n**Figure 1:** Predicted gender identity by NeMo's ECAPA-TDNN model compared with predicted gender identity by SpeechBrain's ECAPA-TDNN model across all speakers.\\n\\n| Pitch | Resonance | NeMo | SpeechBrain |\\n|---|---|---|---|\\n| low | low | 0.61 | 0.61 |\\n| med | 0.60 | 0.46 |\\n| high | 0.51 | 0.38 |\\n\\n| Pitch | Resonance | NeMo | SpeechBrain |\\n|---|---|---|---|\\n| low | low | 0.58 | 0.56 |\\n| med | 0.50 | 0.41 |\\n| high | 0.40 | 0.29 |\\n\\nTable 2: Probability of a given voice being Male averaged across configurations of Pitch and Resonance. Std. Error \u2264 0.02 for all entries, as estimated via bootstrap.\\n\\nFurthermore, differences in predicted gender are not arbitrary, but the result of the deliberate modifications made by the three speakers. Shown in Table 2, pitch and resonance levels directly influence the gender prediction of the model, with both models predicting more feminine voices as both pitch and resonance increase. While current speaker embeddings do implicitly encode high-level perceptual qualities such as pitch and resonance, multiple configurations produce similar predictions, revealing an ambiguity between vocal configuration and binary gender.\\n\\nThese exercises illustrate how, by summarizing vocal diversity with a small number of identity-based categories, gender obscures the richer acoustic space that affects voice perception. While helpful for modeling many voices, categorical notions of gender can result in unreliable and possibly harmful classifications to those who deviate from prototypical voices. Knowing that a voice is a woman's, for example, does not accurately identify the unique texture of that voice. Moreover, systems based on categorical gender can be manipulated by deliberate behavioral changes to voice, which we explore further in Section 4.2.\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification on VVD is remarkably higher than typical test performance, with EER on VVD being 21.52% for NeMo and 29.00% for SpeechBrain. This is not merely due to a distribution shift, as analysis on the test set of VCTK \\\\[28\\\\], which was not used in either of the models' training, achieves similar a EER to the EER achieved on VoxCeleb1.\\n\\n#### 4.2.2. Error Analysis and Difficulty\\n\\nThe high-level EER results make it clear that speaker verification systems perform worse on the VVD; however, these results do not indicate how the systems misclassify voices, or how they compare to human listeners. We explore these questions here.\\n\\nA unique quality of the VVD is the ability to directly measure the distance between voices, and then compare the performance of both automated speaker verification systems and human identification across said distance. Converting the levels of low, medium, and high to 0, 1, 2, and then taking the L1 distance between two audio clips' three dimensional vectors, we map all pairs of voices onto a discrete space distance ranging between 0 and 6. For example, the distance between low-low-low and high-high-high would be a distance of 6.\\n\\nWe perform two perceptual experiments with non-expert and expert listeners. As noted in Section 3.2, the three voice teachers were unfamiliar with each others' voices. Therefore, for expert classifications, we asked each voice teacher (e.g. 002)...\\n\\n### 4.2. Speaker Verification\\n\\n#### 4.2.1. Equal Error Rate\\n\\nThe goal of speaker verification is to determine whether or not two voice clips are created by the same speaker. Many of these methods are trained on cross-entropy loss alongside that encourage inter-class diversity and intra-class compactness, such as additive angular margin loss (AAM-Softmax) \\\\[29\\\\]. Coupled with architecture design, state-of-the-art models like the ECAPA-TDNN models from SpeechBrain and NeMo are able to achieve low Equal Error Rates (EERs) on test sets, achieving EERs on VoxCeleb1 \\\\[30\\\\] of 0.80% and 0.92%, respectively, being able to identify clips as originating from different speakers with remarkable accuracy.\\n\\nThis modeling approach to speaker verification fundamentally assumes that a speaker's vocal texture is static, which is violated by the speakers in the VVD. Illustrated in Table 3, the EER of speaker verification"}
{"id": "netzorg24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Accuracy (\u00b11 Std. Error) of Speaker Verification across L1 Voice Distance over voice clips produced by pairs of speakers.\\n\\nto classify whether or not given pairs of clips from the other teachers (e.g. 001, 003) belonged to the same speaker. For non-expert listeners, we asked six workers with Master's Qualification on Amazon Mechanical Turk to perform the same task. Both non-experts and experts were given the same 200 tasks and instructions, whereby the clips in each task contained voices from a maximum of two speakers, and were balanced such that random guessing would result in a performance of 50% accuracy. To measure classification accuracy for the SpeechBrain and NeMo ECAPA-TDNN models, we use both libraries' default threshold for classification evaluated on all possible pairs. Averaging classification across all pairs and speakers, the achieved accuracies are: 59% for non-experts, 63% for experts, 78.47% for SpeechBrain, and 77.89% for NeMo. For human labelers, even those that are aware of the flexibility the vocal tract, speaker verification across diverse voices is a challenging task. Reporting the results split across pairs and distance in Figure 2, a variety of interesting trends emerge. Across pairs coming from both different and same speakers, non-expert and model accuracies correlate. For pairs coming from different speakers, we see that accuracy increases as voices become distinct; for pairs coming from the same speaker, we see the opposite trend where accuracies trend towards 0 as L1-Distance increases.\\n\\nWhile low same-speaker performance is expected as voices become distinct, due to the fundamental ignorance of speech modification by the considered models, the expert vs. non-expert performance comparison sheds light on the perception of voice. We see that, across both types of pairs, even experts struggle to tell if two voice clips are produced by the same speaker, performing at random when voices are most distinct. Despite non-experts being informed that a total of two speakers produced all voice clips in this task, non-expert performance diverges with that of experts, consistently rating voices with an L1-Distance greater than 3 as belonging to different speakers. Additional prompting might increase non-expert same-speaker performance, but non-experts' assumptions of voice flexibility are similar to that of models: the flawed assumption that vocal texture is fixed.\\n\\n5. Next Steps: Modeling Vocal Texture\\n\\nJust as gender-diversity cannot be limited to a single label of \\\"cisgender\\\", \\\"non-binary\\\", or \\\"transgender\\\" (as notions of gender differ across cultures and time [31]), categories of \\\"man\\\", \\\"woman\\\", and \\\"non-binary\\\" do not capture the full complexity of voice. Binary or categorical gender information, while helpful for defining a broad distribution over possible voices, results in large amounts of ambiguity concerning the specific texture that makes a voice unique. Moving away from a categorical notion of gender, models that center high-level perceptual qualities of voice, such as pitch, resonance, and weight, offer an opportunity to explicitly map the perceptual space of vocal texture. Providing intra-speaker labels of perceptual qualities, the VVD makes it possible to measure the performance of vocal texture models, such as the PQ-Representation [16], which explicitly model resonance and weight. Evaluating the PQ-Representation's ability to rank intra-speaker variation of resonance and weight, we see in Table 4 that the PQ-Representation is unable to accurately rank the resonance of clips, performing slightly below random, and is only able to rank weight marginally better than ranking via Praat's harmonics-to-noise ratio (HNR) estimation. While prior work shows that the PQ-Representation accurately fits typical voices, the diversity of voices in the VVD leads to worse performance.\\n\\nTo encourage further modeling of vocal texture, we propose the three dimensional vector of averages of F0, averages of the first through third formants, and averages of the HNR as a baseline measurement of the VVD's vocal qualities, achieving initial ranking accuracies of 91.7%, 71.5%, and 62.3% for pitch, resonance, and weight respectively.\\n\\n6. Conclusion\\n\\nThe Versatile Voice Dataset demonstrates that there is a section of the population for whom static and categorical models of speaker identity will fail, but the shortcomings of these models provide a novel opportunity to advance the study of speech. Currently, we are unaware of the limits of behavioral voice modification, what a model of vocal tract flexibility would entail, or how to learn modification-robust speaker embeddings. We are excited for the countless research questions behavioral voice modification poses to the speech community, and the future insights that may emerge through collaborations with those often outside of the traditional academic milieu.\\n\\n|             | Resonance | Weight |\\n|-------------|-----------|--------|\\n| PQ-Representation | 49.5%    | 62.5%  |\\n| Avg. Formant-HNR   | 71.5%    | 62.3%  |\\n\\nTable 4: Accuracy of correctly ranking pairs of audio clips with different resonance or weight levels, as measured by the PQ-Representation and avg. formant and HNR. Mean Pitch ranking as provided by Praat achieves 91.7% accuracy.\"}"}
