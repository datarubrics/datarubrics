{"id": "li22j_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TALCS: AN OPEN-SOURCE MANDARIN-ENGLISH CODE-SWITCHING CORPUS AND A SPEECH RECOGNITION BASELINE\\n\\nChengfei Li, Shuhao Deng, Yaoping Wang, Guangjing Wang, Yaguang Gong, Changbin Chen, Jinfeng Bai\\n\\nAbstract\\n\\nFor code-switching (CS) automatic speech recognition (ASR) is identified as one of the most challenging tasks for ASR systems, either for hybrid or pure deep neural network (DNN) architectures. Large industrial datasets are often inaccessible, used to train speech recognition is that a large number of labeled data are required. This is because there are no within-language code-switching utterances. There is increasing research interest in developing techniques to handle code-switching speech recognition.\\n\\nOne of the main reasons for the success of ASR technology is due to the development of machine learning and neural network methods to reduce the amount of training data required. However, such a CS system simply by merging two systems or end-to-end ASR systems as most of the off-the-shelf systems are monolingual and cannot handle code-switching data.\\n\\nThe People's Speech corpus [3] provides a large dataset for Chinese ASR, which has greatly promoted the progress of Mandarin speech recognition and English ASR benefit from these large datasets of speech recognition, which has greatly promoted the progress of ASR.\\n\\nBut the shortage of corresponding code-switching data has also led to a shortage of CS open source data. In China, two training datasets are divided into three parts: training set, development set and test set. This corpus is freely available for download and it is also the known largest open-source CS corpus in the world. This corpus is freely available for download.\\n\\nWe introduce the recording procedure, including the largest speech recognition, Code-Switching (CS) \u2014 a baseline system, including ESPnet [8] and WENETSPEECH [5]. Using these two popular speech recognition toolkits to make a baseline system, including ESPnet [8] and WENETSPEECH [5].\\n\\nIn this paper, we introduce a new corpus of Mandarin speech recognition and English CS. We use the open source CS dataset to make a realistic ASR system. We introduce the recording procedure, including the People's Speech corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet [8] and WENETSPEECH [5].\\n\\nThe People's Speech corpus contains roughly 587 hours of speech sampled at 16 kHz, 100+ hours weakly labeled speech, and about 10000 hours of open source data and a speech recognition dataset in the world. This corpus is freely available under the permissive license. And the People's Speech corpus is derived from the largest open-source Mandarin ASR dataset in the world. This corpus is freely available for download and it is also the known largest open-source CS corpus in the world. This corpus is freely available for download.\\n\\nIn this paper, we introduce a new corpus of ASR and CS. We use the People's Speech corpus to make a realistic ASR system. We introduce the recording procedure, including the People's Speech corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet [8] and WENETSPEECH [5].\\n\\nThe People's Speech corpus contains roughly 587 hours of speech sampled at 16 kHz, 100+ hours weakly labeled speech, and about 10000 hours of open source data and a speech recognition dataset in the world. The People's Speech corpus is freely available under the permissive license. And the People's Speech corpus is derived from the largest open-source Mandarin ASR dataset in the world. This corpus is freely available for download and it is also the known largest open-source CS corpus in the world. This corpus is freely available for download.\\n\\nThe TALCS corpus is one of the most representative scenes of CS code-switching data. TALCS is one of the most well-known open-source CS datasets. Several CS datasets have been released recently, such as AISHELL-1 and AISHELL-2, which provide the largest Mandarin code-switching open source automatic speech recognition (CS-ASR) [1]. However, CS-ASR is identified as one of the most challenging tasks due to the development of machine learning and neural network methods to reduce the amount of training data required. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\\n\\nIn addition to the shortage of corresponding code-switching data, there are also lack of data because there are no within-language code-switching utterances. There is increasing research interest in developing techniques to handle code-switching speech recognition. This is because there are no within-language code-switching utterances. There is increasing research interest in developing techniques to handle code-switching speech recognition. However, it is not an easy thing for CS due to the phenomenon where speakers alternate between different languages. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\\n\\nIn this paper, we introduce a new corpus of ASR and CS. We use the People's Speech corpus to make a realistic ASR system. We introduce the recording procedure, including the People's Speech corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet [8] and WENETSPEECH [5].\\n\\nThe People's Speech corpus contains roughly 587 hours of speech sampled at 16 kHz, 100+ hours weakly labeled speech, and about 10000 hours of open source data and a speech recognition dataset in the world. This corpus is freely available for download and it is also the known largest open-source CS corpus in the world. This corpus is freely available for download. We provide the largest open-source Mandarin ASR dataset in the world.\\n\\nThe TALCS corpus is one of the most representative scenes of CS code-switching data. TALCS is one of the most well-known open-source CS datasets. Several CS datasets have been released recently, such as AISHELL-1 and AISHELL-2, which provide the largest Mandarin code-switching open source automatic speech recognition (CS-ASR) [1]. However, CS-ASR is identified as one of the most challenging tasks due to the phenomenon where speakers alternate between different languages. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\\n\\nIn addition to the shortage of corresponding code-switching data, there are also lack of data because there are no within-language code-switching utterances. There is increasing research interest in developing techniques to handle code-switching speech recognition. However, it is not an easy thing for CS due to the phenomenon where speakers alternate between different languages. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\\n\\nIn this paper, we introduce a new corpus of ASR and CS. We use the People's Speech corpus to make a realistic ASR system. We introduce the recording procedure, including the People's Speech corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet [8] and WENETSPEECH [5].\\n\\nThe People's Speech corpus contains roughly 587 hours of speech sampled at 16 kHz, 100+ hours weakly labeled speech, and about 10000 hours of open source data and a speech recognition dataset in the world. This corpus is freely available for download and it is also the known largest open-source CS corpus in the world. This corpus is freely available for download. We provide the largest open-source Mandarin ASR dataset in the world.\\n\\nThe TALCS corpus is one of the most representative scenes of CS code-switching data. TALCS is one of the most well-known open-source CS datasets. Several CS datasets have been released recently, such as AISHELL-1 and AISHELL-2, which provide the largest Mandarin code-switching open source automatic speech recognition (CS-ASR) [1]. However, CS-ASR is identified as one of the most challenging tasks due to the phenomenon where speakers alternate between different languages. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\\n\\nIn addition to the shortage of corresponding code-switching data, there are also lack of data because there are no within-language code-switching utterances. There is increasing research interest in developing techniques to handle code-switching speech recognition. However, it is not an easy thing for CS due to the phenomenon where speakers alternate between different languages. However, such a CS system simply by merging two systems or end-to-end ASR systems cannot handle code-switching data.\"}"}
{"id": "li22j_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The Mixture Error Rate (MER) performance of the two speech recognition toolkits is compared in TALCS corpus. The experimental results imply that the quality of audio recordings and transcriptions are promising and the baseline system is workable.\\n\\nThis paper is organized as below. Section 2 presents the CS-ASR related work. Section 3 describes the structure of the TALCS corpus, and presents the recording procedure. In Section 4, we describe the process we used to build the CS-ASR system. Finally, in Section 5, we summarize our work and look forward to the future work.\\n\\n2. Related work\\n\\nIn ASR community, the sequence-to-sequence acoustic modeling has attracted great attention. In the past ten years, deep neural network (DNN) has gradually replaced Gaussian mixture model (GMM) for acoustic modeling, and the hybrid ASR system composed of an acoustic model, language model and lexicon model has achieved convincing performance. Recently, end-to-end (E2E) ASR models such as Connectionist Temporal Classification (CTC) \\\\[11\\\\], the recurrent neural network transducer (RNN-T) \\\\[12\\\\], transformer \\\\[13\\\\] or convformer transducer \\\\[14\\\\], attention-based encoder-decoder models \\\\[15\\\\] have gained popularity and achieved state-of-the-art performance in accuracy and latency.\\n\\nIn contrast to conventional hybrid ASR systems, they jointly learn acoustic and language modeling in a single neural network that is E2E trained from labeled data. For example, the recently proposed Transformer-based end-to-end ASR architectures use deeper encoder-decoder architecture with feedforward layers and multi-head attention for sequence modeling, and comes with the advantages of parallel computation and capturing long contexts without recurrence.\\n\\nCS occurs commonly in everyday conversations in multilingual societies. For example, Mandarin and English are often mixed in conversations in some countries of Southeast Asia, such as Malaysia and Singapore, while Cantonese and English are mixed in colloquial Cantonese in Hong Kong \\\\[2\\\\]. In addition, the CS phenomenon occurs more frequently in the workplace. To handle CS speech, there have been many studies in acoustic modeling \\\\[16\\\\], language modeling \\\\[17, 18\\\\], and ASR systems \\\\[19-21\\\\]. And Mandarin-English CS-ASR is typically a low-resource task due to the scarce acoustic and text resources that contain CS. In CS-ASR, to solve the data scarcity problem during acoustic modeling, one solution is to simply merge two different monolingual datasets into one CS dataset for the training of CS-ASR model, an other effective solution is to adapt two well-trained high-resource language acoustic model to the target low-resource domain using transfer learning. These solutions can alleviate the problem of data scarcity to some extent, but the performance of CS-ASR model will reach the bottleneck and cannot be further improved because its training data does not fully match the situation in the actual application scenario.\\n\\nIn order to further improve the performance of CS-ASR, large-scale training data that can match the actual application scenario is still crucial. In order to alleviate the scarcity of CS open source data and promote the progress of CS-ASR technology, we release TALCS corpus, a 580+ hours mono-channel Mandarin-English code-switching speech corpus designed for various code-switching speech processing task. And in the next section, we will introduce the recording procedure and subsequent processing of TALCS corpus in detail.\\n\\n3. Corpus description\\n\\nTALCS corpus is a subset of the TAL-ASR corpus (https://ai.100tal.com/dataset), which is a 580+ hours mono-channel Mandarin-English code-switching speech corpus designed for various code-switching speech processing task.\\n\\n3.1. Corpus profile\\n\\nTALCS corpus comes from online one-to-one teaching scene, in which teachers and students come from different regions of China. And all of the speech utterances of TALCS corpus are recorded by the personal computer microphone. During the course of online one-to-one class, the dialogue between teachers and students is natural, not read aloud, and this means that TALCS corpus is a typical corpus derived from real scenes rather than deliberately produced, which is very conducive to CS-ASR in real scenes. In the online one-to-one course of English teaching, teachers and students mainly use Mandarin for dialogue, and their English pronunciation is relatively standard. TALCS corpus is derived from an online one-to-one course, and teachers and students do not communicate directly face-to-face, so the speech utterances of teachers and students can be recorded through different microphones. For this reason, TALCS corpus has no audio mixing between teachers and students. Considering the privacy of students and corresponding legal and ethical issues, TALCS corpus only includes the speech utterances of teachers.\\n\\nGenerally speaking, Mandarin-English code-switching can be divided into two categories: one is intra-sentence mixing, and another is inter-sentence mixing. As shown in Figure 1, TALCS corpus covers both two cases. Figure 1 (a) shows an example of intra-sentence mixing in TALCS corpus, and it means what is the distinguish between \\\"below\\\" and \\\"under\\\". And we can see that the CS phenomenon occurs three times in this short sentence, which is a typical English-Mandarin-English example in TALCS corpus. Since TALCS corpus comes from online one-to-one English teaching scenes,\"}"}
{"id": "li22j_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Teachers need to explain the meaning of each word or distinguish between different words to students, and there are many examples of such intra-sentence mixing. Figure 1(b) shows an example of inter-sentence mixing in TALCS corpus, and it means let\u2019s look at some examples, \u201cthe ice is too shin...\u201d In online one-to-one English teaching scenes, teachers whose mother tongue is Mandarin often explain some English sentences to students to help them understand some commonly used English expressions, such as tense, active voice, passive voice and so on. Therefore, the phenomenon of inter-sentence mixing often occurs in the online English teaching scene. TALCS corpus contains a large number of intra-sentence mixed audio and inter-sentence mixed audio, and it can well match the CS phenomenon in daily life, which is conducive to the training of CS-ASR model and promote the development of CS-ASR.\\n\\nThe distance between the teacher and the microphone acquisition device is about 20 cm to 30 cm. The TALCS corpus chooses high fidelity microphone audio data and resample it to 16 kHz, 16-bit WAV format, which is the mainstream setup for commercial ASR systems. There are more than 100 teachers in the recording. For the TALCS corpus, the gender is balanced. All teachers are between the ages of 22 and 28 and come from different regions of China. Our English teachers, whose mother tongue is Chinese, have rich experience in English teaching, and their English pronunciation is quite standard. Thus, these teachers can easily switch between English pronunciation and Chinese pronunciation. In addition, the teachers will also be required to pass some grade examinations, such as CET-4 and CET-6, to ensure pronunciation standards. Therefore, the quality of TALCS corpus is reliable. For the entire TALCS corpus, in order to conduct ASR experiments conveniently, we divide it into three parts: training set, development set and test set. The details are presented in Section 3.2 and Section 3.3.\\n\\n3.2 Corpus Transcription\\n\\nTALCS corpus covers the knowledge points of middle school English, such as grammar, reading comprehension and writing. The corpus including 370K sentences with a length ranging from 0.3 seconds to 30 seconds. Because the audio data comes from one-to-one online course, the environment background noise of corpus is relatively low, and teachers\u2019 lecture place are generally in dormitories, families and classrooms. Raw texts are manually filtered to eliminate improper contents involving sensitive political issues, user privacy, pornography, violence, etc. Symbols such as \u201c<\u201d, \u201c>\u201d, \u201c[\u201d, \u201c]\u201d, \u201c\u02dc\u201d, \u201c/\u201d, \u201c\\\\\u201d, \u201c=\u201d, etc., are removed. There are no spaces between Chinese characters, and there are spaces between English words. All English words and letters are capitalized and all text files are encoded in UTF-8.\\n\\nIn order to ensure the high quality of TALCS corpus and enable it to be effectively applied to CS-ASR tasks, the label acquisition of TALCS corpus includes two stages: data annotation stage and quality checking stage. In data annotate stage, due to the particularity and complexity of code-switching, there is more demanding for data annotators. In order to ensure the quality of annotating, we have looked for data annotator who have passed CET-4 to annotate the TALCS corpus. And we annotate the audio data of the whole online class and the average duration of each class is about 2 hours. The data annotator divides the 2-hour audio data into many small sentences according to semantics, and transcribes each small sentence at the same time. Considering the complexity of semantics and different understanding of semantics for different data annotator, only one data annotator is assigned to each class for annotation. It is required to transcribe all the sentences that can be heard clearly, and abandon the sentences that have obvious mispronunciations. And text normalization (TN) is carefully applied towards English words, letter, numbers, name are:\\n\\n- 1, 2, 3 are normalized to \u201d\u4e00\u201d, \u201d\u4e8c\u201d, \u201d\u4e09\u201d.\\n- All the English words contained are capitalized with spaces between words.\\n- English letters such as \u201dING\u201d, \u201dED\u201d are presented in uppercase and also with spaces between letters.\\n\\nIn quality checking stage, TALCS corpus is divided into many parts. Data quality inspectors are asked to check the speech data and transcription of each part at a rate of 20%. Utterances with inconsistent raw text and transcription are marked. If the marked sentences exceed the specified proportion of 10%, the annotating team was asked to reannotate all the data of this part. In addition, data quality inspectors are also asked to check the corpus contents involving student sensitive political issues, such as name, school and so on.\\n\\nTable 1: Data structure of TALCS corpus\\n\\n| Subset   | Number of sentences | Duration (h) |\\n|----------|---------------------|--------------|\\n| Training set | 350000               | 555.9        |\\n| Development set | 5000               | 8.0          |\\n| Test set | 15000               | 23.6         |\\n| Total    | 370000              | 587.5        |\\n\\nAs mentioned above, TALCS corpus comes from the online one-to-one English teaching scene of TAL Education Group. We annotate the audio data of the whole class and the average duration of each class is about 2 hours. In the online one-to-one English teaching course, the teachers need to use...\"}"}
{"id": "li22j_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"their mother tongue, that is Mandarin, to explain the relevant knowledge of English. Therefore, not all of the sentences ... tasks. 6. Acknowledgement This work was supported by National Key R&D Program of China, under Grant No. 2020AAA0104500.\\n\\nFollowing this parameter setting, we train two end ... 3 time masks\\n\\nfeatures and the convolution sub\\n\\nmodel is Adam. The SpecAugment with 2 frequency masks ... = 30)\\n\\nlogarithmic linear combination of cross entropy loss (\u03bb ... encoder unit is set to 2048. The CNN kernel is set to 15 in Conformer\\n\\nTransformer decoder. Both in Conformer encoder and\\n\\nTransformer decoder. In our CS\\n\\nexperiments in a conformer model, which uses multi\\n\\nhead number and head adim is\\n\\n512, respectively\\n\\nT = 40) are applied on the input\\n\\nsampled speech signal, and employ\\n\\nattention mechanism to capture the global information of\\n\\nspeech signal, and employ\\n\\ndistributed lexicon.\\n\\nFigure 2 shows the number\\n\\nsequences in a conformer model, which uses multi\\n\\nhead number and head adim is\\n\\n512, respectively\\n\\nT = 40) are applied on the input\\n\\nwindow length is set to 30ms\\n\\nA\\n\\n4.1. Experiments\\n\\nIn this section, using TALCS corpus, we will conduct CS\\n\\nASR experiments in two popular speech recognition toolkits, including ESPnet and Wenet\\n\\nThe Kaldi toolkit [22] is employed to extract 80\\ndimensional Filter banks and 3\\n\\ntime masks\\n\\nand CTC loss (\u03bb\\n\\n13/1.\\n\\nThe TALCS corpus contains 3670 commonly used\\n\\nMandarin characters and 20 English words and letters. The\\n\\ndistribution of 20 Mandarin characters and 20 English words\\n\\nphenomenon of CS in daily life.\\n\\nconsistent with the proportion of Mandarin\\n\\nMandarin\\n\\n13/1.\\n\\nSince the Mandarin\\n\\nMandarin\\n\\nphenomenon of CS in daily life.\\n\\nrelated.\\n\\nThe TALCS corpus contains 370000 sentences with a total duration of 587 hours from mor\\n\\nTALCS corpus, which include\\n\\ndevelopment set a\\n\\nTALCS corpus was\\n\\nor letters that appear most frequently in TALCS corpus. The\\n\\ndistribution of 20 Mandarin characters and 20 English words\\n\\nTALCS corpus to generate\\n\\nour lexicon and Byte Pair Encoding\\n\\n-toolkit and the training set of th\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\nof about 8 hours, and the test\\n\\nof about 556 hours, and the\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\n24 hours.\\n\\nsentences with a total duration of about 8 hours, and the test\\n\\ncorpus. And T\\n\\nTALCS corpus, which include\\n\\ndevelopment set a\\n\\nTALCS corpus was\\n\\nor letters that appear most frequently in TALCS corpus. The\\n\\ndistribution of 20 Mandarin characters and 20 English words\\n\\nTALCS corpus to generate\\n\\nour lexicon and Byte Pair Encoding\\n\\n-toolkit and the training set of th\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\nof about 8 hours, and the test\\n\\nof about 556 hours, and the\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\n24 hours.\\n\\nsentences with a total duration of about 8 hours, and the test\\n\\ncorpus. And T\\n\\nTALCS corpus, which include\\n\\ndevelopment set a\\n\\nTALCS corpus was\\n\\nor letters that appear most frequently in TALCS corpus. The\\n\\ndistribution of 20 Mandarin characters and 20 English words\\n\\nTALCS corpus to generate\\n\\nour lexicon and Byte Pair Encoding\\n\\n-toolkit and the training set of th\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\nof about 8 hours, and the test\\n\\nof about 556 hours, and the\\n\\ntraining set consists of 350000 sentences with a total duration\\n\\n24 hours.\"}"}
{"id": "li22j_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. **References**\\n\\n[1] N. T. Vu et al., \u201cA first speech recognition system for Mandarin-English code-switch conversational speech,\u201d 2012 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2012, pp. 4889-4892.\\n\\n[2] H. Bu, J. Du, X. Na, B. Wu and H. Zheng, \u201cAISHELL-1: An open-source Mandarin speech corpus and a speech recognition baseline,\u201d 2017 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (OCOCOSDA), 2017, pp. 1-5.\\n\\n[3] Zhang, B., Lv, H., Guo, P., Shao, Q., Yang, C., Xie, L., Xu, X., Bu, H., Chen, X., Zeng, C., Wu, D., & Peng, Z. (2021). WenetSpeech: A 10000+ Hours Multi-domain Mandarin Corpus for Speech Recognition. ArXiv, abs/2110.03370.\\n\\n[4] V. Panayotov, G. Chen, D. Povey and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2015, pp. 5206-5210.\\n\\n[5] Galvez, D., Diamos, G.F., Ciro, J.C., Cer'on, J.F., Achorn, K., Gopi, A., Kanter, D., Lam, M., Mazumder, M., & Reddi, V.J. (2021). The People's Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage. ArXiv, abs/2111.09344.\\n\\n[6] G. Lee, T.-N. Ho, E.-S. Chng and H. Li, \u201cA review of the mandarin-english code-switching corpus: SEAME,\u201d 2017 International Conference on Asian Language Processing (IALP), 2017, pp. 210-213.\\n\\n[7] Zhang, H., Xu, H., Pham, V.T., Huang, H., & Siong, C.E. (2020). Monolingual Data Selection Analysis for English-Mandarin Hybrid Code-Switching Speech Recognition. INTERSPEECH.\\n\\n[8] Watanabe, S., Hori, T., Karita, S., Hayashi, T., Nishitoba, J., Unno, Y., Yalta, N., Heymann, J., Wiesner, M., Chen, N., Renduchintala, A., & Ochiai, T. (2018). ESPnet: End-to-End Speech Processing Toolkit. ArXiv, abs/1804.00015.\\n\\n[9] Yao, Z., Wu, D., Wang, X., Zhang, B., Yu, F., Yang, C., Peng, Z., Chen, X., Xie, L., & Lei, X. (2021). WeNet: Production Oriented Streaming and Non-Streaming End-to-End Speech Recognition Toolkit. Interspeech.\\n\\n[10] G. Hinton et al., \u201cDeep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,\u201d in IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82-97, Nov. 2012.\\n\\n[11] A. Graves and N. Jaitly, \u201cTowards end-to-end speech recognition with recurrent neural networks,\u201d in International conference on machine learning. PMLR, 2014, pp. 1764-1772.\\n\\n[12] Graves, A. (2012). Sequence Transduction with Recurrent Neural Networks. ArXiv, abs/1211.3711.\\n\\n[13] Vaswani, A., Shazeer, N.M., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., & Polosukhin, I. (2017). Attention is All you Need. ArXiv, abs/1706.03762.\\n\\n[14] Bello, I., Zoph, B., Vaswani, A., Shlens, J., & Le, Q.V. (2019). Attention Augmented Convolutional Networks. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), 3285-3294.\\n\\n[15] Gulati, A., Qin, J., Chiu, C., Parmar, N., Zhang, Y., Yu, J., Han, W., Wang, S., Zhang, Z., Wu, Y., & Pang, R. (2020). Conformer: Convolution-augmented Transformer for Speech Recognition. ArXiv, abs/2005.08100.\\n\\n[16] E. Y\u0131lmaz, H. van den Heuvel, and D. van Leeuwen, \u201cInvestigating bilingual deep neural networks for automatic recognition of code-switching Frisian speech,\u201d Procedia Computer Science, vol. 81, pp. 159-166, 2016.\\n\\n[17] H. Adel, N. T. Vu, K. Kirchhoff, D. Telaar, and T. Schultz, \\\"Syntactic and semantic features for code-switching factored language models,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 23, no. 3, pp. 431-440, 2015.\\n\\n[18] Winata, G.I., Madotto, A., Wu, C., & Fung, P. (2018). Learn to Code-Switch: Data Augmentation using Copy Mechanism on Language Modeling. ArXiv, abs/1810.10254.\\n\\n[19] H. Seki, S. Watanabe, T. Hori, J. Le Roux, and J. R. Hershey, \u201cAn end-to-end language-tracking speech recognizer for mixed-language speech,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4919-4923.\\n\\n[20] Luo, N., Jiang, D., Zhao, S., Gong, C., Zou, W., & Li, X. (2018). Towards End-to-End Code-Switching Speech Recognition. ArXiv, abs/1810.13091.\\n\\n[21] Zhang, Shiliang et al. \u201cTowards Language-Universal Mandarin-English Speech Recognition.\u201d INTERSPEECH (2019).\\n\\n[22] Povey, D., Ghoshal, A.K., Boulianne, G., Burget, L., Glembek, O., Goel, N.K., Hannemann, M., Motl\u00edcek, P., Qian, Y., Schwarz, P., Silovsk\u00fd, J., Stemmer, G., & Vesel\u00fd, K. (2011). The Kaldi Speech Recognition Toolkit.\\n\\n[23] Shi, X., Feng, Q., & Xie, L. (2020). The ASRU 2019 Mandarin-English Code-Switching Speech Recognition Challenge: Open Datasets, Tracks, Methods and Results. ArXiv, abs/2007.05916.\"}"}
