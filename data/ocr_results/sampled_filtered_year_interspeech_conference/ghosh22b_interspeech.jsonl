{"id": "ghosh22b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] P. Fortuna, J. Soler, and L. Wanner, \\\"Toxic, hateful, offensive or abusive? what are we really classification? an empirical analysis of hate speech datasets,\\\" in Proceedings of the 12th language resources and evaluation conference, 2020, pp. 6786\u20136794.\\n\\n[2] Y. Xie, R. Liang, Z. Liang, C. Huang, C. Zou, and B. Schuller, \\\"Speech emotion classification using attention-based LSTM,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 11, pp. 1675\u20131685, 2019.\\n\\n[3] P. Tzirakis, J. Zhang, and B. W. Schuller, \\\"End-to-end speech emotion recognition using deep neural networks,\\\" in IEEE ICASSP 2018, pp. 5089\u20135093.\\n\\n[4] E. Lakomkin, M. A. Zamani, C. Weber, S. Magg, and S. Wermter, \\\"Incorporating end-to-end speech recognition models for sentiment analysis,\\\" in IEEE ICRA 2019, pp. 7976\u20137982.\\n\\n[5] R. Guermazi, M. Hammami, and A. B. Hamadou, \\\"Using a semi-automatic keyword dictionary for improving violent web site filtering,\\\" in 2007 Third International IEEE Conference on Signal-Image Technologies and Internet-Based System. IEEE, 2007, pp. 337\u2013344.\\n\\n[6] N. Djuric, J. Zhou, R. Morris, M. Grbovic, V. Radosavljevic, and N. Bhamidipati, \\\"Hate speech detection with comment embeddings,\\\" in Proceedings of the 24th international conference on world wide web, 2015, pp. 29\u201330.\\n\\n[7] P. Badjatiya, S. Gupta, M. Gupta, and V. Varma, \\\"Deep learning for hate speech detection in tweets,\\\" in Proceedings of the 26th international conference on World Wide Web companion, 2017, pp. 759\u2013760.\\n\\n[8] T. Davidson, D. Warmsley, M. Macy, and I. Weber, \\\"Automated hate speech detection and the problem of offensive language,\\\" in ICWSM 2017, vol. 11, 2017.\\n\\n[9] A.-M. Founta, C. Djouvas, D. Chatzakou, I. Leontiadis, J. Blackburn, G. Stringhini, A. Vakali, M. Sirivianos, and N. Kourtellis, \\\"Large scale crowdsourcing and characterization of twitter abusive behavior,\\\" 2018.\\n\\n[10] J. A. Leite, D. Silva, K. Bontcheva, and C. Scarton, \\\"Toxic language detection in social media for Brazilian Portuguese: New dataset and multilingual analysis,\\\" in ACL-IJCNLP 2020, Suzhou, China, pp. 914\u2013924.\\n\\n[11] M. Yousefi and D. Emmanouilidou, \\\"Audio-based toxic language classification using self-attentive convolutional neural network,\\\" in EUSIPCO 2021, pp. 11\u201315.\\n\\n[12] C. S. Wu and U. Bhandary, \\\"Detection of hate speech in videos using machine learning,\\\" in 2020 International Conference on Computational Science and Computational Intelligence (CSCI), 2020, pp. 585\u2013590.\\n\\n[13] A. Rana and S. Jha, \\\"Emotion based hate speech detection using multimodal learning,\\\" arXiv preprint arXiv:2202.06218, 2022.\\n\\n[14] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \\\"wav2vec 2.0: A framework for self-supervised learning of speech representations,\\\" 2020.\\n\\n[15] S. Ling and Y. Liu, \\\"Decoar 2.0: Deep contextualized acoustic representations with vector quantization,\\\" arXiv preprint arXiv:2012.06659, 2020.\\n\\n[16] A. T. Liu, S.-w. Yang, P.-H. Chi, P.-c. Hsu, and H.-y. Lee, \\\"Mockingjay: Unsupervised speech representation learning with deep bidirectional transformer encoders,\\\" in IEEE ICASSP 2020.\\n\\n[17] I. Cohn, I. Laish, G. Beryozkin, G. Li, I. Shafran, I. Szpektor, T. Hartman, A. Hassidim, and Y. Matias, \\\"Audio de-identification-a new entity recognition task,\\\" in NAACL 2019, Minneapolis, Minnesota, pp. 197\u2013204.\\n\\n[18] H. Yadav, S. Ghosh, Y. Yu, and R. R. Shah, \\\"End-to-end named entity recognition from English speech,\\\" in Interspeech 2021, 2021, pp. 4268\u20134272.\\n\\n[19] Z. Lu, L. Cao, Y. Zhang, C.-C. Chiu, and J. Fan, \\\"Speech sentiment analysis via pre-trained features from end-to-end ASR models,\\\" in IEEE ICASSP 2020, pp. 7149\u20137153.\\n\\n[20] P. Fortuna and S. Nunes, \\\"A survey on automatic detection of hate speech in text,\\\" ACM Computing Surveys (CSUR), vol. 51, no. 4, pp. 1\u201330, 2018.\\n\\n[21] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \\\"BERT: pre-training of deep bidirectional transformers for language understanding,\\\" CoRR, vol. abs/1810.04805, 2018. [Online]. Available: http://arxiv.org/abs/1810.04805\\n\\n[22] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \\\"Librispeech: an ASR corpus based on public domain audio books,\\\" in IEEE ICASSP 2015, pp. 5206\u20135210.\\n\\n[23] J. S. Garofolo, \\\"Timit acoustic phonetic continuous speech corpus,\\\" Linguistic Data Consortium, 1993, 1993.\\n\\n[24] F. Hernandez, V. Nguyen, S. Ghannay, N. Tomashenko, and Y. Est\u00e9e, \\\"Ted-lium 3: Twice as much data and corpus repartition for experiments on speaker adaptation,\\\" in International conference on speech and computer. Springer, 2018, pp. 198\u2013208.\\n\\n[25] M. Z. Boito, W. N. Havard, M. Garnerin, \u00b4E. L. Ferrand, and L. Besacier, \\\"Mass: A large and clean multilingual corpus of sentence-aligned spoken utterances extracted from the bible,\\\" arXiv preprint arXiv:1907.12895, 2019.\\n\\n[26] M. S. Grover, P. Bamdev, Y. Kumar, M. Hama, and R. R. Shah, \\\"audino: A modern annotation tool for audio and speech,\\\" CoRR, vol. abs/2006.05236, 2020.\\n\\n[27] M. Sap, D. Card, S. Gabriel, Y. Choi, and N. A. Smith, \\\"The risk of racial bias in hate speech detection,\\\" in ACL 2019, pp. 1668\u20131678.\\n\\n[28] T. Garg, S. Masud, T. Suresh, and T. Chakraborty, \\\"Handling bias in toxic speech detection: A survey,\\\" arXiv preprint arXiv:2202.00126, 2022.\\n\\n[29] W.-N. Hsu, A. Sriram, A. Baevski, T. Likhomanenko, Q. Xu, V. Pratap, J. Kahn, A. Lee, R. Collobert, G. Synnaeve, and M. Auli, \\\"Robust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,\\\" in Interspeech 2021, 2021, pp. 721\u2013725.\\n\\n[30] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \\\"Common voice: A massively-multilingual speech corpus,\\\" arXiv preprint arXiv:1912.06670, 2019.\\n\\n[31] J. Godfrey and E. Holliman, \\\"Switchboard-1 release 2 ldc97s62,\\\" LDC, 1993.\\n\\n[32] C. Cieri, D. Graff, O. Kimball, D. Miller, and K. Walker, \\\"Fisher English training speech part 1 transcripts,\\\" Philadelphia: Linguistic Data Consortium, 2004.\\n\\n[33] P. Malik, A. Aggrawal, and D. K. Vishwakarma, \\\"Toxic speech detection using traditional machine learning models and Bert and FastText embedding with deep neural networks,\\\" in IEEE ICCMC 2021, pp. 1254\u20131259.\\n\\n[34] Z. Zhao, Z. Zhang, and F. Hopfgartner, \\\"A comparative study of using pre-trained language models for toxic comment classification,\\\" in Companion Proceedings of the Web Conference, 2021, pp. 500\u2013507.\\n\\n[35] D. Borkan, L. Dixon, J. Sorensen, N. Thain, and L. Vasserman, \\\"Nuanced metrics for measuring unintended bias with real data for text classification,\\\" CoRR, vol. abs/1903.04561, 2019.\\n\\n[36] P. Flandrin, G. Rilling, and P. Goncalves, \\\"Empirical mode decomposition as a filter bank,\\\" IEEE signal processing letters, vol. 11, no. 2, pp. 112\u2013114, 2004.\\n\\n[37] J. Shah, Y. K. Singla, C. Chen, and R. R. Shah, \\\"What all do audio transformer models hear? probing acoustic representations for language delivery and its structure,\\\" arXiv preprint arXiv:2101.00387, 2021.\"}"}
{"id": "ghosh22b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances\\n\\nSreyan Ghosh1, Samden Lepcha3, S Sakshi4, Rajiv Ratn Shah2, S. Umesh1\\n\\n1 Speech Lab, Department of Electrical Engineering, IIT Madras, Chennai, India\\n2 MIDAS Labs, IIIT-Delhi, India\\n3 TEG Analytics, Bangalore, India,\\n4 Cisco Systems, Bangalore, India\\n\\ngsreyan@gmail.com, sam.lepcha@outlook.com, ssakshi1397@gmail.com, rajivratn@iiitd.ac.in, umeshs@ee.iitm.ac.in\\n\\nAbstract\\nToxic speech is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text and written conversations with very limited work on toxicity detection from spoken utterances or using the modality of speech. In this paper, we introduce a new dataset DeToxy, the first publicly available toxicity annotated dataset for the English language. DeToxy is sourced from various openly available speech databases and consists of over 2 million utterances. We believe that our dataset would act as a benchmark for the relatively new and unexplored Spoken Language Processing (SLP) task of detecting toxicity from spoken utterances and boost further research in this space. Finally, we also provide strong unimodal baselines for our dataset and compare traditional two-step cascade and End-to-End (E2E) approaches. Our experiments show that in the case of spoken utterances, text-based approaches are largely dependent on gold human-annotated transcripts for their performance and also suffer from the problem of keyword bias. However, the presence of speech files in DeToxy helps facilitate the development of E2E speech models which alleviate both the above-stated problems by better capturing speech clues.\\n\\nIndex Terms\\nSpeech Toxicity Analysis, End-to-End, 2-step, Multimodal\\n\\n1. Introduction\\nSocial network platforms are generally meant to share positive, constructive, and insightful content. However, in recent times, people often get exposed to objectionable content like threats, identity attacks, hate speech, insults, obscene texts, offensive remarks, or bullying. Though previous research on these topics has often focused on them individually, in this work we denote such content as \\\"toxic speech\\\", and consider it as a super-set of all the above sub-types [1].\\n\\nWith the rise of different forms of content available online beyond just written text, i.e., audio and video, it is crucial that we devise efficient content moderation systems for these forms of shared media. However, most prior work in literature and available datasets focus primarily on the modality of conversational text, with other modalities of conversation ignored at large. Thus, to alleviate this problem, in this paper, we propose a new dataset DeToxy, for the relatively new and unexplored SLP task of toxicity classification in spoken utterances. This remains a crucial problem to solve for interactive intelligent systems, with broad applications in the field of content moderation in online audio/video content, gaming, customer service, etc.\\n\\nDeToxy is a large-scale multimodal dataset with both speech and text cues, manually annotated for toxicity detection in spoken utterances, with potential applications in building unimodal or multimodal models for conversational utterance-level Speech Toxicity Classification (STC) and E2E speech systems which better capture semantics of the utterance.\\n\\nThe key challenge in toxicity classification in spoken speech is to learn good representations that capture toxicity signals from raw speech signals which are inherent in a rich set of acoustic and linguistic content, including semantic meaning, speaker characteristics, emotion, tone, and possibly even sentiment information while remaining invariant under different speakers, acoustic conditions, and other natural speech variations. However, different from Speech Emotion Recognition (SER) and similar to Speech Sentiment Recognition (SSER), toxicity classification from spoken utterances involves the system capturing semantic and syntactic properties of the utterance beyond just acoustic and prosodic cues which makes this task all the more difficult.\\n\\nTraditional approaches for various speech downstream tasks involving spoken utterance classification employ low-level acoustic features, such as band-energies, filter banks, and MFCC features [2], or raw waveform [3]. We acknowledge the fact that models trained on these low-level features can easily overfit to noise or signals irrelevant to the task. One way to remove variations in speech is to transcribe the audio into text and use text features to predict toxicity as done with SSER by [4]. Nonetheless, toxicity signals in the speech, like tone and emotion, can be lost in the transcription. Thus, it is essential for any system to learn speech representations that make high-level information from speech signals available to solve downstream SLP tasks. Inspired by this, we provide both 2-step cascade and E2E baselines for STC on DeToxy and analyze the advantages and disadvantages in both, concluding why the modality of speech is crucial for the task of STC and makes DeToxy an important contribution to the speech community. Additionally, we show how prosodies and linguistic cues in natural speech aid our E2E model, which outperforms the 2-step methodology using less than 10% of the total amount of labeled data available to our 2-step system.\"}"}
{"id": "ghosh22b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"audio and videos. Another notable mention is the work done by authors of [13], where they employ speech representations learned on a downstream SER task, as a second modality of input to the final model they learn for hate speech detection.\\n\\nOn the other hand, the most commonly explored SLP tasks include Automatic Speech Recognition (ASR), SER, speaker verification, speaker identification, speech separation, speech enhancement, Named Entity Recognition (NER), phoneme recognition, etc, and the most recently explored speech sentiment classification which is very related to our task. All of these tasks are well studied with a lot of open-source datasets available online. With recent advancements in Natural Language Processing (NLP) and SLP, a lot of the systems achieving state-of-the-art in these tasks leverage E2E multi-layer neural networks like CNNs or Transformers, including self-supervised learning (SSL) and semi-supervised techniques which have shown to help models in the low-resource data paradigm and proven to learn powerful speech representations [14, 15, 16].\\n\\nWith downstream SLP tasks like speech sentiment classification and NER requiring an understanding of the contents and semantics of the spoken utterance, people have employed both 2-step [17] and E2E methodologies [18, 19]. Some of these self-supervised methodologies, originally invented for pushing the performance of state-of-the-art ASR systems, have also shown success in other downstream tasks [16].\\n\\n3. DeToxy and DeToxy-B\\n\\nIn this paper, we present DeToxy, the first annotated dataset for toxic speech detection in the English language. Our dataset is a subset of various open-source datasets, detailed statistics about which can be found in Table 1. We also present DeToxy-B, a balanced version of the dataset, curated from the original larger version taking into consideration auxiliary factors like trigger terms and utterance sentiment labels.\\n\\n3.1. Dataset Collection and Annotation\\n\\nFor annotation, we define toxicity as rude, disrespectful, or otherwise likely to make someone leave a discussion. This definition is reminiscent of the wealth of literature on toxic speech classification [8, 9, 20]. For the initial version of our dataset, we primarily focus on openly available speech databases with or without text transcripts available. For obtaining transcripts of datasets for which transcripts were not available, we use pre-trained wav2vec-2.0 [14] to obtain the transcriptions from a similar domain. In the process of consolidation of our dataset, primarily, after an empirical analysis of the transcripts of a majority of the open-source datasets available online, we found that most of them did not contain toxic utterances. Thus, we follow a 2-step approach to annotate our dataset. First, we train a textual toxicity classifier using BERT [21] and use that to select datasets that had at least 10% of their total number of utterances flagged as toxic by the model. Some datasets which did not fit this criterion were LibriSpeech [22], TIMIT [23], TED-LIUM [24] and MASS [25]. Next, all these utterances from all datasets filtered through step 1 were manually annotated by 3 professional annotators using audino [26] taking both the textual transcript and speech files into account. Finally, we do a simple majority voting among the 3 annotations to determine the final toxicity class of each utterance. The Cohen's Kappa Score for inter-annotator agreement is 0.76. Table 1 and Table 2 show detailed descriptions of DeToxy and DeToxy-B datasets respectively.\\n\\n3.2. Sampling DeToxy-B and Test-Trigger Datasets\\n\\nFor DeToxy-B, we keep all the toxic utterances from DeToxy and sample thrice the number of toxic utterances as our non-toxic utterances. We acknowledge the fact that sentiment is a strong controlling factor for various speech cues, and over-sampling any one of the sentiments might lead our E2E model to overfit the same. Thus, we don't sample at random but use sentiment labels to sample non-toxic utterances so that the final DeToxy-B dataset has an equal distribution of sentiments for its non-toxic utterances. These sentiment labels were annotated by a single annotator only for the purpose of constructing DeToxy-B and thus we don't include these labels in our final release of the dataset. In this paper, we evaluate all our proposed baselines on only DeToxy-B, with a Training, Development and Test stratified split of 70:15:15.\\n\\nFinally, we also collate an explicit test set with just non-toxic utterances where each utterance consisted of at least one trigger term which we call the Test-Trigger (Test-T)1. We do this to evaluate how our baselines perform against not getting biased to trigger terms, a long-standing problem in toxicity classification [27, 28]. Our Test-T dataset is sampled as a subset from the DeToxy-B Validation and Test sets.\\n\\n4. Baselines\\n\\n4.1. Problem Formulation\\n\\nThe task of toxicity detection from spoken utterances involves assigning a probability score, denoting the toxicity, to each utterance fed to the model. Formally put, if $X = \\\\{x_1, \\\\cdots, x_i, \\\\cdots, x_N\\\\}$ are the set of input utterances from the dataset $D$ with $N$ utterances, then $Y = \\\\{y_1, \\\\cdots, y_i, \\\\cdots, y_N\\\\}$ are their corresponding labels where $y_i \\\\in \\\\{0, 1\\\\}$. In the next 2 sections, we describe in detail our proposed 2-step and E2E baselines for DeToxy, the different components involved, and the training procedure involved in each step.\\n\\n4.2. 2-step Baselines\\n\\nOur 2-step approach consists of two primary components, an ASR component that produces transcriptions from spoken utterances, and a Sequence Classification Model for classifying the toxicity of the output transcript. For our ASR model, we resort to using wav2vec-2.0. Wav2vec-2.0 is built on the transformer architecture and has a Convolutional Neural Network (CNN) feature encoder layer, which encodes raw audio into speech frames. Wav2vec-2.0 follows the pre-training and fine-tuning paradigm wherein we first pre-train the model using a large unlabeled audio corpus in a self-supervised fashion solving a contrastive task as follows:\\n\\n$$L_m = -\\\\text{log} \\\\exp \\\\left( \\\\frac{\\\\text{sim} (c_t, q_t)}{\\\\kappa} \\\\right) \\\\sum_{\\\\tilde{q} \\\\sim Q} \\\\exp \\\\left( \\\\frac{\\\\text{sim} (c_t, \\\\tilde{q})}{\\\\kappa} \\\\right)$$\\n\\nwhere $L_m$ is the InfoNCE loss that wav2vec-2.0 tries to minimize, $c_t$ is the context representation of a masked frame obtained from the last transformer layer, $q_t$ is the quantized representation of obtained through a product quantization based quantization module and $\\\\tilde{q}$. Precisely, for every masked input frame which is input from the feature encoder module to the transformer-based context encoder, wav2vec-2.0 tries to identify the true quantized representation among a set of distractors. For more implementation-specific details about wav2vec-2.0, we refer our readers to [14]. For our 2-step approach, we\\n\\n1The list of trigger terms were obtained from https://hatebase.org/.\"}"}
{"id": "ghosh22b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 1: Statistics for the DeToxy dataset\\n\\n| Dataset            | # Utterances | # Toxic | # Non-Toxic | # Sp | # Positive | # Neutral | # Negative | TL (hh:mm:ss) | Source            |\\n|--------------------|--------------|---------|-------------|------|------------|-----------|------------|----------------|------------------|\\n| CMU-MOSEI          | 44,977       | 217     | 44,760      | 1,000| \u2713          | \u2713         | \u2713          | 1:44:25        | YouTube Videos    |\\n| CMU-MOSI           | 2,199        | 67      | 2,132       | 98   | \u2713          | \u2717         | \u2713          | 0:18:03        | Opinion Videos    |\\n| Common Voice       | 1,584,219    | 2,888   | 1,581,331   | 66,173| \u2713          | \u2717         | \u2717          | 2181:00:00      | Crowd-Sourced      |\\n| IEMOCAP            | 10,087       | 274     | 9,813       | 10   | \u2713          | \u2713         | \u2717          | 11:28:12        | Scripted Sessions |\\n| LJ Speech          | 13,100       | 40      | 13,060      | 1    | \u2713          | \u2717         | \u2717          | 23:55:17        | Audio Books       |\\n| MELD               | 13,708       | 142     | 13,566      | 304  | \u2713          | \u2713         | \u2713          | 12:02:44        | T.V. Shows        |\\n| MSP-Improv         | 8,348        | 129     | 8,219       | 13   | \u2713          | \u2713         | \u2717          | 8:25:41         | Improvisations    |\\n| MSP-Podcast        | 73,042       | 692     | 72,350      | 1,273| \u2713          | \u2713         | \u2717          | 113:41:00       | Podcasts          |\\n| Social-IQ          | 12,024       | 122     | 11,902      |      | \u2713          | \u2717         | \u2717          | 20:39:00        | YouTube Videos    |\\n| Switchboard        | 259,890      | 456     | 259,434     | 400  | \u2713          | \u2717         | \u2717          | 260:00:00       | Telephonic Conv.  |\\n| VCTK               | 44,583       | 50      | 44,533      | 110  | \u2713          | \u2717         | \u2717          | 70:22:28        | Newspaper Articles |\\n| **Total**          | **2,336,177**| **5,077**| **2,331,100**|      |            |            |            | **2770:04:15**  |                  |\\n\\nThe * symbol signifies that the numbers reported are approximate.\\n\\n### Table 2: Statistics for the DeToxy-B dataset\\n\\nIn this table, we include the sentiment label statistics annotated by us for sampling DeToxy-B. Additionally, \u201cSp\u201d and \u201cTL\u201d refers to the number of speakers and total length respectively.\\n\\n#### Post SSL pre-training, the model is then fine-tuned by mini-\\n#### minimizing the Connectionist Temporal Classification (CTC) objec-\\n#### tive function on labeled speech data. We do not use a language\\n#### model in our final setup to decode our utterances during infer-\\n#### ence, as we did not find significant differences in Word Error\\n#### Rate (WER) with it in our experiments, and also to keep the\\n#### setup simple.\\n\\nBoth self-supervised and supervised training paradigms for\\nASR tend to get biased to the domain from where the data\\noriginates [29] and therefore we make careful data considera-\\n#### tions for pre-training and fine-tuning our model. Most works\\n#### in the literature report result on a single benchmark dataset and\\n#### so we resort to multiple dataset pre-training but single dataset\\n#### fine-tuning. We choose to pre-train our model using a combi-\\n#### nation of Libri-Light, Common Voice (CV) [30], SwitchBoard\\n#### (SWBD) [31], and Fisher [32]. These 4 datasets cover a di-\\n#### verse set of domains and are well-suited to our downstream\\n#### ASR fine-tuning needs without creating a reasonable domain\\n#### shift [29]. For downstream ASR fine-tuning, we resort to 3 dif-\\n#### ferent dataset setups as follows: 1) To fit our model to the con-\\n#### versational domain, we fine-tune it on 300 hours of SWBD, 2)\\n#### for read speech (e.g. Audio Books) we resort to 960 hours of\\n#### LibriSpeech fine-tuning and finally 3) We also fine-tune on CV\\n#### which comprises of a majority of DeToxy.\\n\\nFor our text-based sequence-classification model, which\\nis the second step of our 2-step approach, we employ the\\nBERT BASE architecture from the transformer family. Since its\\ninception, transformers have achieved SOTA on sequence clas-\\nsification tasks including toxicity or hate-speech classification\\n[33, 34]. Formally put, we tokenize each word in the sen-\\ntence and feed it as input through the transformer archite-\\ncure. Next, we utilize the hidden state embedding\\n$e$ corresponding to the [CLS] token, where $e \\\\in \\\\mathbb{R}^{768}$, as the ag-\\ngregate representation of words in the transcript of the utter-\\nance. This embedding $e$ is now fed through a final classification\\nlayer which learns a parameterized function $f(e)$ and outputs $f(e) = h \\\\in \\\\mathbb{R}^n$ where $n = 2$. Finally, we pass the logits obtained through a softmax activation function to get the prob-\\nability distribution $p$ of toxicity for each sentence. For train-\\ning our model we minimize the Cross Entropy (C.E.) loss as\\n\\n$$C.E. = -\\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\sum_{j=1}^{M} y_{ij} \\\\log(p_{ij})$$\\n\\nwhere $y_{ij}$ corresponds to the $j$th gold annotated class from the $i$th training\\ninstance in our dataset and $p_{ij}$ is the probability score given\\nby the model that the $i$th instance belongs to the $j$th class. Dur-\\ning inference, we simply perform the $\\\\text{argmax}(p_i)$ to find the\\nfinal class for each utterance. We fine-tune BERT BASE on a pub-\\nlicly available large-scale hate-speech classification dataset [35] or on the relatively smaller gold toxicity annotations from the\\nDeToxy-B dataset.\\n\\n### 4.3. End-to-End Baselines\\n\\nFor our E2E Baselines, we choose to experiment with different\\nfeature extractors, where we build baselines employing either\\nlow-level feature extractors (eg. Filter-Banks) or high-level fea-\\ntures from contextual feature encoder models learned through\\nSSL on unlabelled speech data. In the next 2 sub-sections, we\\nbriefly describe both our approaches.\"}"}
{"id": "ghosh22b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3.1. Filter-Bank Baseline\\n\\nFor our Filter-Bank (F-Bank) [36] based E2E approach, given an audio input sequence $x$, we extract log-compressed mel-scaled F-Banks with a window size of 25 ms and a hop size of 10 ms to obtain $T$ frames. After this, we employ GlobalAveragePooling over all the $T$ frames $[h_1, \\\\ldots, h_T]$, where $h_t$ corresponds to the frame at time-step $t$, to obtain a single embedding $e \\\\in \\\\mathbb{R}^{80}$ for each utterance. This embedding is then fed into a toxicity classification decoder, consisting of a single fully connected layer post which we employ the softmax activation function to the output of this layer to obtain the probability of toxicity for each utterance.\\n\\n4.3.2. wav2vec-2.0 Baseline\\n\\nFor a fair comparison with our 2-step approach, we employ the same wav2vec-2.0 architecture employed in the 2-step procedure for our E2E system and experiment with only the BASE architecture. We follow the same pre-training methodology as our 2-step baseline ASR system, but this time we fine-tune the parameters of our model on DeToxy-B, solving a downstream toxicity classification task, and train in an E2E fashion. During downstream training, we either freeze or keep all the parameters of wav2vec-2.0 trainable. In both cases, the CNN feature extractor remains frozen by default.\\n\\nTo fine-tune our model on the downstream task, we employ the same pooling strategy and prediction-head as our F-Bank approach, except now we take the embeddings from the last layer of our ASR encoder and obtain a single embedding $e \\\\in \\\\mathbb{R}^{768}$ for each utterance by passing it through a GlobalAveragePooling layer which pools over all the time-steps.\\n\\nBoth these models are trained using Cross-Entropy Loss and similar to our 2-step procedure, during inference, we take the argmax of the output of the softmax function as the final class for each utterance.\\n\\n5. Experimental Setup\\n\\nWe use the PyTorch Framework for building and evaluating all our Deep Learning models along with the Transformer implementations, pre-trained models, and specific tokenizers are taken from the HuggingFace library. We train all our models with Adam optimizer in batched mode with a batch size of 8 and a learning rate of $1 \\\\times 10^{-4}$ for a maximum of 100 epochs with an early-stopping of 5 epochs. We make all our code and data publicly available on GitHub.\\n\\n6. Results and Result Analysis\\n\\nTable 3 provides the macro-averaged $F_1$ scores obtained for variations of both our 2-step and E2E baselines on the Development, Test and Test-T splits of DeToxy-B post training on the Train split. The category section of the 2-step system is noted as \\\"Transcript Source - Sequence Classifier Data Source\\\", where Gold is gold transcripts and LS, CV, and SWBD stand for transcripts obtained from fine-tuning wav2vec-2.0 on LibriSpeech, Common Voice, and SwitchBoard respectively. CC[35] and DB stand for BERT BASE fine-tuned on Civil Comments or DeToxy-B gold text transcripts respectively. Beyond frozen and unfrozen wav2vec-2.0 E2E setups, based on findings by [37], we also test our E2E model by taking embeddings from the 9th layer which contains maximum semantic information.\\n\\n### Table 3: Experimental Results of our 2-step and E2E baselines on DeToxy-B.\\n\\n| System Category | Dev     | Test    | Test-T   |\\n|-----------------|---------|---------|----------|\\n| 2-step Gold-CC  | 0.787   | 0.801   | 0.258    |\\n| 2-step Gold-DB  | 0.921   | 0.934   | 0.492    |\\n| 2-step LS-CC    | 0.672   | 0.682   | 0.281    |\\n| 2-step LS-DB    | 0.742   | 0.744   | 0.484    |\\n| 2-step CV-CC    | 0.659   | 0.674   | 0.270    |\\n| 2-step CV-DB    | 0.505   | 0.724   | 0.474    |\\n| 2-step SWBD-CC  | 0.672   | 0.682   | 0.270    |\\n| 2-step SWBD-DB  | 0.726   | 0.737   | 0.478    |\\n| F-Bank          | 0.610   | 0.620   | 0.491    |\\n| wav2vec-2.0 Freezed | 0.448   | 0.457   | 0.497    |\\n| wav2vec-2.0 Unfreezed | 0.877   | 0.869   | 0.510    |\\n| wav2vec-2.0 (9) Unfreezed | 0.897   | 0.877   | 0.540    |\\n\\nAs we see in Table 3, our E2E system clearly outperforms our 2-step system setup when gold transcripts are not available to the text sequence classifier. We hypothesize the true reason to be that ASR models don't generalize across domains and generally fail to perform well when trained on one domain and inferred on another. This is also very evident in our case where we get an average WER of 33%, 43% and 26.9% on LS, CV and SWBD respectively. 2-step systems trained on CC consistently under-performs, which reveals that text sequence classifiers models trained to classify toxicity are not robust to changes in the domain.\\n\\nOur unfrozen wav2vec-2.0 E2E setup with representations taken from the 9th layer outperforms all our baselines where gold transcripts were not used to train our sequence classification model, which is also the ideal case in the real world. This proves that semantic information of the utterance is crucial to toxicity classification from spoken speech. This setup also outperforms other baselines on Test-T, in which very evidently all our baselines struggled on, especially our 2-step baselines trained on CC.\\n\\n7. Conclusion\\n\\nIn this paper, we introduce the first publicly available human-annotated dataset DeToxy for the task of toxic speech classification and propose two strong baselines for this task. Future work includes expanding the dataset using more naturally spoken utterances, exploring different modalities, and developing better neural architectures to solve this problem. Additionally, we would like to explore the factors and features learned by our E2E system that explain its superiority over the 2-step system.\"}"}
