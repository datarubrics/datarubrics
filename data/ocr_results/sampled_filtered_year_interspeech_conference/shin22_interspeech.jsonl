{"id": "shin22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] G. Chen, C. Parada, and G. Heigold, \u201cSmall-footprint keyword spotting using deep neural networks,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2014.\\n\\n[2] T. N. Sainath and C. Parada, \u201cConvolutional neural networks for small-footprint keyword spotting,\u201d in INTERSPEECH, 2015.\\n\\n[3] X. Chen, S. Yin, D. Song, P. Ouyang, L. Liu, and S. Wei, \u201cSmall-footprint keyword spotting with graph convolutional network,\u201d in IEEE Automatic Speech Recognition and Understanding Workshop, 2019.\\n\\n[4] G. Chen, C. Parada, and T. N. Sainath, \u201cQuery-by-example keyword spotting using long short-term memory networks,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2015.\\n\\n[5] L. Lugosch, S. Myer, and V. S. Tomar, \u201cDonut: Ctc-based query-by-example keyword spotting,\u201d in NeurIPS Workshop on Interpretability and Robustness in Audio, Speech, and Language, 2018.\\n\\n[6] J. Huang, W. Gharbieh, H. S. Shim, and E. Kim, \u201cQuery-by-example keyword spotting system using multi-head attention and softtriple loss,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2021.\\n\\n[7] N. Sacchi, A. Nanchen, M. Jaggi, and M. Cernak, \u201cOpen-vocabulary keyword spotting with audio and text embeddings,\u201d in INTERSPEECH, 2019.\\n\\n[8] B. Yusuf, A. Gok, B. Gundogdu, and M. Saraclar, \u201cEnd-to-end open vocabulary keyword search,\u201d in INTERSPEECH, 2021.\\n\\n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, 2017.\\n\\n[10] L. Momeni, T. Afouras, T. Stafylakis, S. Albanie, and A. Zisserman, \u201cSeeing wake words: Audio-visual keyword spotting,\u201d in Proceedings of the British Machine Vision Conference, 2020.\\n\\n[11] J. Lee, S.-W. Chung, S. Kim, H.-G. Kang, and K. Sohn, \u201cLooking into your speech: Learning cross-modal affinity for audio-visual speech separation,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\n[12] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \u201cFocal loss for dense object detection,\u201d in Proceedings of the International Conference on Computer Vision, 2017.\\n\\n[13] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2015.\\n\\n[14] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the International Conference on Machine Learning, 2006.\\n\\n[15] J. Park, Kyubyong & Kim, \u201cg2p,\u201d https://github.com/Kyubyong/g2p, 2019.\\n\\n[16] Y. Qian, M. Yin, Y. You, and K. Yu, \u201cMulti-task joint-learning of deep neural networks for robust speech recognition,\u201d in IEEE Automatic Speech Recognition and Understanding Workshop, 2015.\\n\\n[17] S. Mahto, H. Yamamoto, and T. Koshinaka, \u201ci-vector transformation using a novel discriminative denoising autoencoder for noise-robust speaker recognition,\u201d in INTERSPEECH, 2017.\\n\\n[18] H. Tachibana, K. Uenoyama, and S. Aihara, \u201cEfficiently trainable text-to-speech system based on deep convolutional networks with guided attention,\u201d in Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing, 2018.\\n\\n[19] J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania, \u201cCalibrating deep neural networks using focal loss,\u201d in Advances in Neural Information Processing Systems, 2020.\\n\\n[20] M. McAuliffe, M. Socolof, S. Mihuc, M. Wagner, and M. Sonderegger, \u201cMontreal forced aligner: Trainable text-speech alignment using kaldi,\u201d in INTERSPEECH, 2017.\\n\\n[21] C. K. Reddy, E. Beyrami, J. Pool, R. Cutler, S. Srinivasan, and J. Gehrke, \u201cA scalable noisy speech dataset and online subjective test framework,\u201d in INTERSPEECH, 2019.\\n\\n[22] V. I. Levenshtein et al., \u201cBinary codes capable of correcting deletions, insertions, and reversals,\u201d in Soviet physics doklady, vol. 10, no. 8. Soviet Union, 1966, pp. 707\u2013710.\\n\\n[23] P. Warden, \u201cSpeech commands: A dataset for limited-vocabulary speech recognition,\u201d arXiv preprint arXiv:1804.03209, 2018.\\n\\n[24] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim, and S. Ha, \u201cTemporal convolution for real-time keyword spotting on mobile devices,\u201d in INTERSPEECH, 2019.\\n\\n[25] B. Kim, M. Lee, J. Lee, Y. Kim, and K. Hwang, \u201cQuery-by-example on-device keyword spotting,\u201d in IEEE Automatic Speech Recognition and Understanding Workshop, 2019.\"}"}
{"id": "shin22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting\\n\\nHyeon-Kyeong Shin1, 2 Hyewon Han2 Doyeon Kim2 Soo-Whan Chung2 Hong-Goo Kang2\\n\\n1 Naver Corporation, South Korea\\n2 Dept. of Electrical and Electronic Engineering, Yonsei University, South Korea\\n\\nAbstract\\nIn this paper, we propose a novel end-to-end user-defined keyword spotting method that utilizes linguistically corresponding patterns between speech and text sequences. Unlike previous approaches requiring speech keyword enrollment, our method compares input queries with an enrolled text keyword sequence. To place the audio and text representations within a common latent space, we adopt an attention-based cross-modal matching approach that is trained in an end-to-end manner with monotonic matching loss and keyword classification loss. We also utilize a de-noising loss for the acoustic embedding network to improve robustness in noisy environments. Additionally, we introduce the LibriPhrase dataset, a new short-phrase dataset based on LibriSpeech for efficiently training keyword spotting models. Our proposed method achieves competitive results on various evaluation sets compared to other single-modal and cross-modal baselines.\\n\\nIndex Terms\\nuser-defined keyword spotting, open-vocabulary, audio-text correspondence detection\\n\\n1. Introduction\\nKeyword spotting (KWS) is the task of identifying enrolled keywords within spoken utterances. It is highly challenging because it involves not only detecting keywords accurately, but also rejecting other words reliably. KWS systems have significantly improved due to the advent of deep learning algorithms, which have demonstrated superior performance in speech analysis and detection. There are two major approaches toward deep learning-based KWS: one is a keyword-filler strategy that detects phonetic sequences on input streams [1, 2, 3], and the other is a query-by-example (QbyE) method [4, 5, 6] that matches input queries to enrolled examples.\\n\\nWith increasing demand of user-friendly services in smart speakers and assistants, it has become more important to use user-defined keywords for KWS. However, detecting user-defined keywords is a more challenging task because it involves dealing with arbitrary phrases which may not have been included in their training data. Most of user-defined keyword spotting (UDKWS) methods are designed with QbyE approaches, where the methods detect similarities between a given audio query and pre-enrolled audio examples [4, 5, 6]. However, since QbyE approaches require voice keyword enrollment, their performance highly depends on the consistency of the audio recording session. In general, it is not easy to maintain this consistency, so it is difficult to generalize QbyE approaches for a wide variety of users and environments. For example, each user has different vocal characteristics. And they may enroll the keyword in various recording environments, where input speech can easily be distorted by environmental conditions and background noise.\\n\\nTo address this problem, we introduce the idea of cross-modal approaches with text keyword enrollment instead of speech signals. We propose an end-to-end UDKWS strategy that utilizes matching criteria between text embeddings and audio embedding queries. The text modality has only linguistic representations, regardless of users' voices or recording environments. Therefore, this cross-modal approach can achieve compliant performance results without keyword enrollment issues. We build a cross-modal correspondence detector (CMCD), using several sub-modules. We design two-stream networks to reliably embed linguistic representations of speech and text sequences within a common latent space. Since the audio-text joint latent space places linguistically similar embeddings close to each other [7, 8], it is possible to distinguish keywords from other speech inputs. Based on these representations, our proposed method decides whether the input speech contains a keyword or not, by using a cross-attention mechanism [9, 10, 11].\\n\\nWe additionally propose a monotonic matching loss and a de-noising loss to serve as training criteria for our approach. The monotonic matching loss enforces correspondence between modalities on the affinity matrix to build robust embeddings, and the de-noising loss helps induce consistent audio representations even if there is noise within the input speech. We also replace the commonly used binary classification loss with focal loss [12] for more effective learning in imbalanced training sets. Finally, we introduce the LibriPhrase dataset, which contains short phrases (at most 4 words) from the LibriSpeech dataset [13]. Since keywords tend to be composed of relatively short sequences of words, LibriPhrase provides more efficient training compared to using a long speech corpus on the open-vocabulary keyword spotting and keyphrase detection tasks.\\n\\n2. User-defined Keyword Spotting\\nIn this section, we explain several baseline models that use single-modal or cross-modal approaches for user-defined keyword spotting (UDKWS). Most previous works for UDKWS utilize query-by-example (QbyE) methods, which use only audio signals as an input. QbyE methods enroll reference keyword speech and compare it with new input speech queries. For example, variable-length audio signals with a fixed-length embedding using an LSTM network is represented in [4]. There have been other similar KWS methods using QbyE approaches [6], but their performance degrades for short phrases or non-word inputs due to their word-level training criteria. In [14], connectionist temporal classification (CTC) loss, widely used in speech recognition, has been used to overcome the open-vocabulary problem. An acoustic network trained with the CTC criterion first estimates phonetic sequences, after which a detector determines whether the input speech contains a keyword or not.\\n\\nSome studies have explored the feasibility of cross-modal approaches that use not only spoken keywords, but also corresponding information from other modalities, e.g., video and...\"}"}
{"id": "shin22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Overall architecture of the proposed model, CMCD. Red arrows indicate each training loss.\\n\\nIn [7], an audio-text keyword spotting approach is proposed by computing both audio and phonetic embeddings independently, then mapping them to word-level representations with triplet loss to extract linguistic information and place them in the same latent space. Also, [11] showed that the word-level embeddings extracted from the enrolled text query are used to compute the probability by multiplying with the frame-wise audio embedding.\\n\\nOur approach is different from the aforementioned methods because we consider the similarity of the entire text and audio sequences instead of handling word-level embeddings explicitly. In [10], an audio-visual keyword spotting approach is proposed, where an audio-text model and a visual-text model are pre-trained and then combined into an integrated audio-visual system during the fine-tuning step. The audio-visual model computes a similarity map between video and audio embeddings and utilizes the similarity map for the keyword detector. This approach is similar to ours in the sense that it computes similarity map patterns and utilizes these patterns in the keyword recognition stage. However, this method is difficult to be used in embedded systems due to the large model size.\\n\\n3. Proposed Method\\n\\nIn this section, we describe our proposed model, a cross-modal correspondence detector (CMCD). CMCD consists of three sub-blocks: audio and text encoders, a pattern extraction module, and a pattern discriminator. The overall architecture is illustrated in Figure 1.\\n\\n3.1. Architecture\\n\\nAudio/Text encoders. Encoders embed audio and text inputs within a joint latent space. The audio encoder consists of two 1-D convolution layers with a kernel size of 5 and two gated recurrent units (GRUs). The input for the audio encoder is 40-dimensional mel-filterbank coefficients, extracted every 10ms with a 25ms frame length. For computational efficiency, the first convolutional layer reduces the number of frames by skipping consecutive frames with stride 2. The text encoder consists of a pre-trained grapheme-to-phoneme (G2P) model [15] followed by a fully-connected layer. It looks up the CMU pronouncing dictionary to cover a large number of English vocabulary words. In the case of out-of-vocabulary (OOV) words, it estimates the phoneme sequences using a neural network. We denote audio and text embeddings by $E_a \\\\in \\\\mathbb{R}^{T_a \\\\times m}$ and $E_t \\\\in \\\\mathbb{R}^{T_t \\\\times m}$, respectively, where $T_a$, $T_t$, and $m$ are the lengths of audio and text embeddings, and the output embedding dimension which is set to 128 in this work.\\n\\nPattern extractor. Motivated by [9], the pattern extractor is based on a cross attention mechanism [10, 11] between acoustic and phonetic representations to obtain temporal correlation patterns. The acoustic embedding $E_a$ is fed into the network as a key $K$ and a value $V$, and the text embedding $E_t$ is used as a query $Q$: \\n\\n$$\\\\text{Attn} = \\\\text{softmax}(QK^T\\\\sqrt{d_k}) \\\\times V = A(Q,K) \\\\times V. \\\\quad (1)$$\\n\\nIn Equation (1), the affinity matrix $A$ represents the temporal correlation between the audio and text embeddings. If the $Q$ and $K$ represent the same linguistic content, the $A$ displays a monotonic pattern; if they represent different content, $A$ displays an obscure pattern. The output of the pattern extractor is the attention matrix $\\\\text{Attn}$, which contains information about the audio and text agreement.\\n\\nPattern discriminator. The pattern discriminator decides whether audio and text inputs have the same keyword (positive) or not (negative). A single GRU layer with $m$ dimension takes the attention matrix as an input, and the output of the last frame is fed into a fully-connected layer with a sigmoid function.\\n\\n3.2. Training criterion\\n\\nOur training objective consists of de-noising loss ($L_{DN}$), monotonic matching loss ($L_{MM}$), and detection loss ($L_D$):\\n\\n$$L_{total} = \\\\lambda_1 L_{DN} + \\\\lambda_2 L_{MM} + L_D, \\\\quad (2)$$\\n\\nwhere $\\\\lambda_1$ and $\\\\lambda_2$ are weighting factors which are set to 0.5, 0.3 in this work.\\n\\nDe-noising loss. Motivated by [16, 17], we use a de-noising loss to obtain robust performance even for noisy speech queries. By constructing the acoustic encoder as a Siamese network, we minimize the mean square error (MSE) between clean and noisy embeddings as follows:\\n\\n$$L_{DN} = ||E_{clean}^A - E_{noisy}^A||^2, \\\\quad (3)$$\\n\\nFigure 2: Examples of the targets in the monotonic matching loss when audio keyword is \u201cHello world\u201d; i) Positive case: (a) Full matching; ii) Negative case: (b) Non-matching, (c) Partial matching (front), (d) Partial matching (back).\"}"}
{"id": "shin22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $E_{\\\\text{clean}}$ and $E_{\\\\text{noisy}}$ are the acoustic embeddings of clean and noisy speech, respectively.\\n\\nMonotonic matching loss. As noted above, the affinity matrix $A$ in the pattern extractor represents the correspondence between audio and text inputs. We utilize a monotonic matching loss (MML) to strengthen the patterns depicted in $A$ by increasing its similarity with manually built patterns. As illustrated in Figure 2, we provide three different target affinity patterns $M_i$ depending on the different cases, and minimize the distance between the affinity matrix and a given target pattern during training as shown below:\\n\\n$$L_{\\\\text{MM}} = \\\\| A - M_i \\\\|^2,$$\\n\\nwhere $i \\\\in \\\\{f, n, pf\\\\}$,\\n\\n$$M_f = \\\\exp\\\\left\\\\{ - \\\\left( \\\\frac{t_{\\\\text{a}}}{T_{\\\\text{a}}} - \\\\frac{t_{\\\\text{t}}}{T_{\\\\text{t}}} \\\\right)^2 \\\\right\\\\} g^2 \\\\sum,$$\\n\\n$$M_n = \\\\left| X \\\\right| \\\\sum,$$\\n\\nwhere $X \\\\sim N(\\\\mu, \\\\sigma^2)$,\\n\\n$$M_{pf} = \\\\begin{cases} M_f, & t_{\\\\text{t}} \\\\leq K \\\\\\\\ M_n, & t_{\\\\text{t}} > K \\\\end{cases},$$\\n\\nwhere $T_{\\\\text{a}}, T_{\\\\text{t}}$ are the lengths of audio and text embeddings, and $g$ is set to 0.2. $K$ is a boundary of matching text. $X$ is the normal distributed random noise. $M_i$ is normalized into summation $\\\\Sigma$, to be matched with the range of affinity matrices.\\n\\nThe full matching case corresponds to when the input audio and text represent the same keyword; i.e. when there is a strong correspondence between the audio and text embeddings. Here, the pattern on the affinity matrix displays large matching values on the diagonal. Inspired by [18], we provide an explicit guidance using a Gaussian diagonal pattern $M_f$ (Equation (5) and Figure 2(a)). The non-matching case corresponds to when the audio and text represent different keywords; the target pattern is a scattered matrix using normalized random noise $M_n$ (Equation (6) and Figure 2(b)). We also train the affinity matrix on cases where the text is partially matched, the partial matching case. The target matrix $M_{pf}$ consists of both monotonic and random noise components if the front parts of the sequences match (Figure 2(c)). If only the rear parts match, we consider it as the non-matching case $M_n$ (Figure 2(d)), because it should be not recognized anymore if the front part is wrong even if the rear part is matched in the keyword spotting.\\n\\nDetection loss. The detection loss $L_D$ aims to teach the model to detect keywords based on the cross-attention matrix. Binary cross-entropy (BCE) loss is commonly used for this objective. We use BCE loss in the early stages of training, and replace it with focal loss [12] in later stages. Focal loss shows impressive results when the training data are not well structured (e.g. imbalanced dataset [12] or miscalibration [19]). Since we train our network using a general corpus and the number of positive pairs is much smaller than the number of negative pairs, we find that it is advantageous to use the focal loss for training efficiency.\\n\\n4. Experiments\\n\\n4.1. Experimental settings\\n\\nDatasets. For training and evaluation, we built a new phrase-unit corpus from the LibriSpeech dataset [13]. We call it as LibriPhrase, and its creation process is motivated by the data processing in [5]. The training set of LibriPhrase is generated from the train-clean-100 and train-clean-360 datasets of LibriSpeech. LibriSpeech does not provide alignment annotations for audio and text pairs, so we use the Montreal Forced Aligner [20] to determine word-level time stamps in the audio. Then, we split full-length utterances into shorter phrases between 1 to 4 words long. The training set contains 200k phrases of each length, for a total of 800k phrases.\\n\\nFor environmental robustness, we add babble noises of the MS-SNSD dataset [21] to audio samples in LibriPhrase during the training, where the SNR is randomly set between 5 to 15dB. For the evaluation, we generate 4,391 / 2,605 / 467 / 56 episodes of each length consisting of positive and negative audio phrases for enrolled keywords (anchor) using the train-others-500 subset. Each episode contains 3 positive and 3 negative audio-text pairs. There are two types of negative sets: easy negatives ($L_P^E$) and hard negatives ($L_P^H$). We compute the Levenshtein distances [22] between the anchors and negatives to determine whether a negative sample is easy or hard. Table 1 shows examples of easy and hard negatives in LibriPhrase.\\n\\nIn the evaluation phase, we use two more datasets, as well as LibriPhrase. From the Google Speech Commands V1 dataset (G) [23], we choose 10 different short words, following the evaluation data setup in [24]. The Qualcomm Keyword Speech dataset (Q) [25] consists of the following 4 commands: 'Hey android', 'Hey snapdragon', 'Hi galaxy', and 'Hi lumina'.\\n\\n4.2. Experimental results\\n\\nWe compare performances with conventional approaches trained on the same corpus. We also analyze the impact of each training objective by an ablation study under the same settings. Comparison with baselines. All the conventional meth-\"}"}
{"id": "shin22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Experimental results of UDKWS methods in various datasets. G: Google Commands V1, Q: Qualcomm Keyword Speech dataset, LP E: LibriPhrase-Easy, LP H: LibriPhrase-Hard. (I): single-modal approach, (II): cross-modal approach.\\n\\n| Method          | EER (%)  | AUC (%) |\\n|-----------------|----------|---------|\\n| (I) CTC [5]     | 31.65    | 18.23   |\\n| (I) Attention [6] | 14.75    | 49.13   |\\n| (II) Triplet [7] | 35.60    | 38.72   |\\n| (II) Proposed   | 27.25    | 12.15   |\\n\\nTable 3: EER(%) results for the ablation study.\\n\\n| Loss | G | Q | LP E | LP H |\\n|------|---|---|------|------|\\n| LD   | \u2713 | - | 27.75| 13.54| 8.20 |\\n| LD + DN + MM | \u2713 | \u2713 | 27.65| 13.02| 8.37 |\\n|      | \u2713 | \u2713 | 27.25| 12.15| 8.42 |\\n\\nFigure 4: Qualitative results of ablation study. Visualization of affinity matrices (x-axis: phoneme sequence, y-axis: acoustic frames) for different text keywords when given audio keyword is \u201ci mean to\u201d. 1st row: LD, 2nd row: LD + DN + MM.\\n\\nOur proposed model outperforms the conventional methods in the LP H and Q dataset. These datasets have negative phrases that include phonetically similar words to the enrolled keywords. It is difficult to estimate the phonetic information from the Q dataset because it includes the proper noun at the end of the phrase, e.g., \u2018snapdragon\u2019 and \u2018lumina\u2019, but our method still outperforms the conventional methods. On the other hand, the attention-based QbyE method shows powerful performance on G dataset, which consists of frequently used words such as \u2018on\u2019, \u2018off\u2019, \u2018one\u2019 and \u2018two\u2019 as keywords. This method has an advantage when the keyword is included in the training set due to its similarity scoring measurement mechanism. However, when the keyword is not familiar as in Q and LibriPhrase, it shows degraded performances. Consequently, our proposed method shows slightly degraded performance compared to the attention-based QbyE method [6] that is able to learn the characteristics of the keywords from a classifier in the training stage. However, except for the G dataset, our proposed method outperforms the baselines on all the datasets in terms of AUC score and EER.\\n\\nAblation study. We analyze the effectiveness of each training loss described in Section 3.2. Table 3 summarizes the EER results for all the evaluation datasets. Compared to the results only using detection loss, LD, de-noising loss DN improves the overall performance for the G dataset which has environmental distortions. However, it has negative impact to the LP E dataset that consists of relatively clean audio data. In addition, the monotonic matching loss, MM, improves overall performance to most of the datasets. In Figure 4, we visualize the affinity matrix to show the impact of monotonic matching loss to various types of matching cases. It is clear that the monotonic matching loss helps build more coherent patterns in matching areas, but scattered patterns in non-matching areas.\\n\\nAnalysis on the length of keywords. Figure 5 depicts the detection performance graphs on the LibriPhrase evaluation dataset. It shows the change in performance according to the number of words included in a keyword phrase. Our proposed method shows robust detection performance regardless of the length of keywords. It shows the best scores on most of sub-sets, while other methods are more vulnerable on shorter keywords. Especially, the performance of attention-based QbyE method [6] significantly degrades when there is a single word in the keyword.\\n\\n5. Conclusion\\nIn this work, we proposed a cross-modal correspondence detector (CMCD), an end-to-end user-defined keyword spotting (UDKWS) system using an agreement between audio and text keyword. CMCD uses a cross attention mechanism to generate and discriminate meaningful patterns between audio and text embeddings. We introduced various training strategies for the effective learning, especially using monotonic matching loss. We also presented LibriPhrase, a new short-phrase dataset for training the KWS systems in real-world settings. Experiments showed that CMCD outperformed or showed competitive performance compared to the conventional methods on various datasets.\\n\\nAcknowledgements. This research was sponsored by Naver Corporation.\"}"}
