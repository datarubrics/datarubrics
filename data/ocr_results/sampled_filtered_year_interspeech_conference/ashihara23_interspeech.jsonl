{"id": "ashihara23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nSelf-supervised learning (SSL) for speech representation has been successfully applied in various downstream tasks, such as speech and speaker recognition. More recently, speech SSL models have also been shown to be beneficial in advancing spoken language understanding tasks, implying that the SSL models have the potential to learn not only acoustic but also linguistic information. In this paper, we aim to clarify if speech SSL techniques can well capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a speech version of the General Language Understanding Evaluation (GLUE) benchmark. Since GLUE comprises a variety of natural language understanding tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines, suggesting that they can acquire a certain amount of general linguistic knowledge from just unlabeled speech data.\\n\\nIndex Terms\\nself-supervised learning, speech representation, linguistic knowledge, natural language processing\\n\\n1. Introduction\\nSelf-supervised learning (SSL) has become a prominent technique to leverage a large amount of unlabeled data in an unsupervised fashion. For the speech community, various SSL methods have been proposed and accuracy has dramatically improved, especially in automatic speech recognition (ASR) tasks under low-resource conditions. Subsequent studies have demonstrated success with task-generalizability, i.e., performance improvement in a wide range of tasks such as speaker recognition, emotion recognition, and speech enhancement. These positive results likely reflect the ability of the SSL model to learn a wide range of speech information (e.g., phonemes and speaker characteristics) from only speech data without any labels.\\n\\nMore recently, speech SSL models have also been utilized in spoken language understanding (SLU) tasks, e.g., named entity recognition and sentiment analysis, and these universal models have demonstrated superiority over conventional approaches. Their success can be naturally attributed to the speech information captured by SSL as described above.\\n\\nHowever, since performing SLU tasks requires natural language processing (NLP) ability, the benefit of SSL in these tasks may also imply that speech SSL models can latently capture linguistic characteristics, such as semantics and syntax, from speech signals in addition to acoustic characteristics.\\n\\nThe above implications are supported by other studies. Previous studies have presented and utilized the zero-resource benchmark to evaluate the spoken language models, which are language models trained using the discrete acoustic units obtained by clustering the speech SSL output. Benchmark results have shown that the unit-based language models are feasible, indicating that self-supervised representation seemingly retains some multi-level information such as phonetics, lexicography, syntax, and semantics. The other work comprehensively analyzed the speech representation layer-wise and demonstrated that SSL models capture some word and semantic information in the middle layers. While the aforementioned papers shed light on the language properties acquired by the speech SSL, further investigation is needed because, for example, it is important to align the representations across speech and text modalities for a unified multimodal SSL model.\\n\\nIn particular, motivated by the above efforts, we aim to elucidate if linguistic information captured by speech SSL models is enough to solve practical and diverse natural language understanding (NLU) tasks. Moreover, we would like to compare the linguistic capabilities of not only speech SSL models but also text-based ones such as BERT and identify their main differences to confirm if text data is still required to represent the linguistic information.\\n\\nIn this paper, for the purpose of exploiting the linguistic knowledge learned via SSL, we apply a probing task, which is a popular assessment method, to self-supervised speech representations. Specifically, we introduce the speech version of the General Language Understanding Evaluation (GLUE) benchmark, called SpeechGLUE, and evaluate speech SSL models extensively, in a fair comparison to NLP SSL models. While there is a large body of NLU probing tasks and benchmarks, we adopt GLUE which is relatively basic among the existing NLU benchmarks.\\n\\nSince GLUE is designed to cover a diverse range of NLU tasks, SpeechGLUE, a collection of NLU tasks that convert input text to speech, is intended to evaluate the general-purpose NLU knowledge within the speech SSL models. For the conversion, we adopt text-to-speech (TTS) systems, which allow for the realization of tasks that assess purely linguistic knowledge by constraining acoustic conditions such as variations in speakers, speaking styles and recording settings.\\n\\nWe base our implementation of SpeechGLUE on the S3PRL toolkit developed for SUPERB, which facilitates comparisons with various speech SSL models.\\n\\nFrom published experiments, speech SSL models lag behind NLP SSL models in performance, especially in the task of judging whether a sentence is linguistically acceptable. However, strong speech SSL models, such as WavLM, perform substantially better than chance level or baselines.\\n\\nNote that our approach can be applied to any probing task of NLP.\\n\\nWe release SpeechGLUE for reproducibility and comparison with successive SSL techniques at https://github.com/ashi-ta/speechGLUE.\"}"}
{"id": "ashihara23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Brief summary of GLUE. MCC, PCC, and SCC denote Matthews, Pearson, and Spearman correlation coefficients, respectively. For details on this benchmark, see the original paper of \\\\[22\\\\].\\n\\n| Corpus                      | Task                        | Metrics                  | Labels                      |\\n|----------------------------|-----------------------------|---------------------------|-----------------------------|\\n| Single-sentence tasks      | CoLA                        | acceptability (grammaticality) | unacceptable / acceptable |\\n|                            | SST2                        | sentiment analysis accuracy | positive / negative        |\\n| Similarity and paraphrase tasks using sentence pairs | MRPC                        | semantic equivalence (paraphrase) accuracy & F1 | equivalent / not equivalent |\\n|                            | QQP                         | semantic equivalence (paraphrase) accuracy & F1 | duplicate / not duplicate |\\n|                            | STS-B                       | sentence similarity PCC & SCC | similarity score (1\u20135)     |\\n| Natural language inference (NLI) tasks using sentence pairs | MNLI-m                      | NLI (in-domain) accuracy entailment / contradiction / | MNLI-mm |\\n|                            | QNLI                        | NLI (cross-domain) neutral QNLI |\\n|                            | RTE                         | NLI accuracy entailment / not entailment RTE |\\n|                            | WNLI                        | NLI (coreference) accuracy entailment / not entailment WNLI |\\n\\n(e.g., log-mel filterbank output) and achieve close to the performance of the NLP SSL models, especially in sentence similarity and natural language inference (NLI) tasks. The experiments confirm that SSL models can capture enough linguistic information to tackle purely NLU tasks. By releasing the SpeechGLUE task, we hope to not only clarify the linguistic capabilities of current SSL models, but also to allow future models to be assessed in terms of their improvements in these tasks. Indeed, we believe that SLU tasks, which require capturing fine linguistic information, will be more and more important in future speech processing research.\\n\\n2. Related work\\nThere are several speech benchmarks related to the current work. ASR-GLUE \\\\[25\\\\] is a collection of human speech recordings based on selected sentences intended for some of the GLUE tasks. This benchmark includes only development and test sets to evaluate the negative impact of ASR error propagating to the backend NLU system. Therefore, we cannot train downstream models on this dataset, making it unsuitable for our purpose. Other datasets such as \\\\[6\\\\], \\\\[26\\\\] have been helpful in benchmarking speech SSL models. While they are designed to evaluate the generalizability through diverse speech processing tasks, SpeechGLUE, a collection of purely NLU tasks based on GLUE, aims to delve into the linguistic properties.\\n\\n3. Method\\nIn this section, we briefly explain GLUE \\\\[22\\\\] tasks in Section 3.1 and then, introduce SpeechGLUE in Section 3.2.\\n\\n3.1. GLUE\\nThe original GLUE benchmark contains 9 tasks divided into 3 categories:\\n1) the Corpus of Linguistic Acceptability (CoLA) \\\\[27\\\\] and the Stanford Sentiment Treebank (SST-2) \\\\[28\\\\] for single-sentence tasks,\\n2) the Microsoft Research Paraphrase Corpus (MRPC) \\\\[29\\\\], Quora Question Pairs (QQP) \\\\[30\\\\] and the Semantic Textual Similarity Benchmark (STS-B) \\\\[31\\\\] for similarity and paraphrase tasks, and\\n3) Multi-Genre NLI (MNLI) \\\\[32\\\\], Question-answering NLI (QNLI) \\\\[33\\\\], Recognizing Textual Entailment (RTE) \\\\[34\\\\] \u2013 \\\\[37\\\\] and Winograd NLI (WNLI) \\\\[38\\\\] for NLI tasks. An overview of each task is given in Table 1.\\n\\n3.2. SpeechGLUE\\nAs described in Section 3.1, the GLUE benchmark was originally designed to assess NLU systems. Since our objective is to evaluate speech SSL models in terms of NLP capability, the text sentences must be converted into corresponding speech utterances. In this work, we applied a single-speaker TTS system to investigate purely linguistic knowledge of speech SSL by suppressing acoustic variabilities such as speaker, speaking style, and recording noise. Recent neural-based TTS systems can generate high-quality speech in terms of naturalness and intelligibility \\\\[39\\\\]. Moreover, the conversion cost via TTS is lower than rerecording the text by humans.\\n\\nThe overall system is summarized in Figure 1. Since SpeechGLUE is essentially a counterpart of GLUE, speech and NLP SSL models are fairly comparable except for the [SEP] token utilized in NLP SSL models. [SEP] token is a special separation token that indicates where to split pairs in the examples (e.g., pairs of question-answer). For speech SSL models, this study simply employs a white noise signal of 50 ms as an alternative.\\n\\nTo incorporate SSL upstream models, we utilize the pretrained model as a feature extractor and do not update the parameters during the training on GLUE/SpeechGLUE tasks in order to verify just the linguistic representation captured only by SSL.\\n\\n4. Experimental Setup\\n4.1. SpeechGLUE dataset\\nFor the GLUE benchmark itself, we utilized the publicly available dataset \\\\[3\\\\] provided by Hugging Face. To generate the speech, we adopted the VITS \\\\[40\\\\] model \\\\[4\\\\] trained by LJSpeech \\\\[5\\\\] using the ESPnet toolkit \\\\[41\\\\]. Because the VITS model was trained with a sampling frequency of 22050 Hz, we resampled the output data to 16000 Hz to match the sampling frequency assumed by the SSL models. The dataset after applying TTS is summarized in Table 2. Note that the original number of examples in the test set of QQP and in the training set of SST2 were 390965 and 67349, but the sizes were reduced to 390963 and 67347 through the execution of TTS. This is because some samples were deemed impractical as they included a huge number of digits or only null text, which could not be synthesized into speech. In addition, the original text samples were altered with the ESPnet-based text normalization such as by removing symbols (e.g., quotation marks) and by translating Latin abbreviations (e.g., \\\"i.e.\\\") into English (e.g., \\\"that is\\\"). Thus, the word sequence of the GLUE benchmark was also transformed for a fair comparison; nevertheless, no significant degradation was noted in our preliminary GLUE experiment.\\n\\n4.2. Upstream\\nTo explore the linguistic ability of speech SSL models, we utilized the multiple baselines and SSL methods with speech and text modalities summarized in Table 3. In this table, the upstream components were divided into three sections: the baselines, the speech SSL models, and the NLP SSL models. For the...\"}"}
{"id": "ashihara23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2: Summary of the data size in SpeechGLUE. The tasks with underline indicate the relatively high-resource tasks. Note that the number of examples in some tasks is decreased from the original number as noted in Section 4.2.1.\\n\\n| Task        | Input Unlabeled data (#hours or #words) | #hours (#examples) |\\n|-------------|----------------------------------------|--------------------|\\n| MNLI-m      | 111.0 / 332.9 (104743)                 | 66.5 (67347)       |\\n| MNLI-mm     | 5.8 / 17.8 (5463)                      | 3.4 (1821)         |\\n| QQP         | 21.0 / 11.1 (9832)                     | 111.0 / 332.9 (104743) |\\n| MRPC        | 399.3 / 404.0 (363846)                 | 44.4 / 44.9 (40430) |\\n| SST2        | 6.3 (8551)                            | 4.1                |\\n| CoLA        | 6.6 / 6.6 (5749)                       | 4.1                |\\n| WNLI        | 1.2 / 0.5 (635)                        | 4.1                |\\n| QQP         | 4.1                                    | 4.1                |\\n\\nTable 3: Overview of upstream models.\\n\\n| Upstreams        | #Params | Input | Unlabeled data | (text - waveform) | (w/o SSL) |\\n|------------------|---------|-------|----------------|-------------------|-----------|\\n| BERT             | 110M    | 0.01M | text           | -waveform         | -w/o SSL  |\\n| English Wikipedia | 340M    | 94M   | waveform       | -waveform         | -w/o SSL  |\\n| VoxPopuli        | 125M    | 0.01M | text           | -waveform         | -w/o SSL  |\\n| English Wikipedia | 315M    | 94M   | waveform       | -waveform         | -w/o SSL  |\\n| VoxLingua107     | 315M    | 94M   | waveform       | -waveform         | -w/o SSL  |\\n| BABEL            | 94M     | 315M  | waveform       | -waveform         | -w/o SSL  |\\n| BookCorpus       | 315M    | 94M   | waveform       | -waveform         | -w/o SSL  |\\n| CommonVoice      | 94M     | 315M  | waveform       | -waveform         | -w/o SSL  |\\n\\nFor optimization, we adopted Adam with a learning rate of 1e-5 and a batch size of 32. Two types of total training steps were used depending on the amount of training data: 50k steps for low-resource tasks and 150k steps for high-resource tasks. The models were updated during training unlike upstream models, which acted as feature extractors. The architecture of the feature encoder was identical to that of an existing SSL study, with the exceptions stated in Section 4.3.1. The decoder layer and the output of the decoder were connected only to the SSL models, which acted as feature extractors. The architecture of the feature encoder was basically identical regardless of the upstream model and tasks, except STS-B which used a mean squared error loss.\\n\\nThe downstream model structure is sectioned into two parts: an encoder and a classifier/regressor. The encoder contained the subword, was always set to zero; because the embedding information was acquired rather than to achieve state-of-the-art performance on the GLUE benchmark. Additionally, there were seemingly no clear differences in performance tendency across models, i.e., tasks underlined in Table 2 showed no clear performance difference.\\n\\nFor the baseline models listed in the first section in Table 2, we adopted three types of upstream feature extractors: w/o SSL, and the encoder of all SSL models with and without multiple layers (i.e., FBANK and phoneme). We also evaluated a model architecture, the encoder of all SSL models with a combination of four SSL approaches and two SSL methods, and WavLM. The architecture of the feature encoder was identical to that of an existing SSL study. The encoder architecture of the feature encoder was identical to that of an existing SSL study. The robustness of w/o SSL and B to random initialization was tested. The downstream model, as motivated by an existing study, was straightforwardly connected to the backend of the upstream models. The parameters of downstream models followed by a 256-dim linear layer with tanh function, mean-pooling across whole sequences, and a final linear layer for classification or regression task. With respect to the last linear layer, the number of classes was 2 except for MNLI and STS-B to perform the regression task. We applied a dropout with probability of 0.1 to the output of the linear layer, the number of classes was 2 except for MNLI and STS-B to perform the regression task. We applied a dropout with probability of 0.1 to the output of the linear layer.\"}"}
{"id": "ashihara23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4: Evaluation result for each model and each task on the development set of SpeechGLUE and GLUE. Acc, MCC, PCC and SCC denote accuracy, Matthews, Pearson and Spearman correlation coefficients, respectively. The resulting score with bold font (underline) indicates the highest score among the speech (NLP) SSL models.\\n\\n| Task         | Baselines | WavLM (0.3B) | HuBERT (21B) | WavLM LARGE | HuBERT LARGE | BERT LARGE | WavLM B | XLS-R (0.3B) |\\n|--------------|-----------|---------------|--------------|-------------|--------------|------------|---------|--------------|\\n| CoLA         | 65.0 (53.5) | 64.1 (20.4)   | 64.8 (10.1)  | 70.8 (53.8) | 71.2 (58.7)  | 76.1 (67.1) | 80.0 (69.2) | 78.6 (69.8)  |\\n| SST-2        | 56.2 (40.1) | 57.6 (40.7)   | 58.3 (40.7)  | 63.2 (35.5) | 65.0 (35.5)  | 69.4 (37.7) | 74.6 (37.1) | 74.3 (37.1)  |\\n| MRPC         | 77.8 (73.0) | 76.0 (71.7)   | 77.2 (71.8)  | 78.6 (71.8) | 79.5 (71.8)  | 81.0 (72.8) | 82.8 (72.8) | 82.8 (72.8)  |\\n| QQP          | 55.0 (44.8) | 53.1 (39.9)   | 54.0 (39.7)  | 55.0 (44.8) | 57.9 (44.8)  | 59.1 (44.8) | 60.7 (44.8) | 61.2 (44.8)  |\\n| QNLI         | 81.0 (76.7) | 80.4 (76.7)   | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7) | 81.0 (76.7)  |\\n| WNLI         | 74.3 (63.3) | 72.8 (61.4)   | 75.7 (63.3)  | 77.2 (65.1) | 76.3 (65.1)  | 77.8 (66.1) | 79.5 (66.1) | 79.5 (66.1)  |\\n| RTE          | 49.2 (44.8) | 47.6 (44.8)   | 49.2 (44.8)  | 49.2 (44.8) | 49.2 (44.8)  | 50.5 (44.8) | 50.5 (44.8) | 50.5 (44.8)  |\\n| MNLI-m / -mm | 56.7 (49.0) | 56.3 (49.0)   | 56.3 (49.0)  | 59.0 (49.0) | 59.0 (49.0)  | 60.1 (49.0) | 60.1 (49.0) | 60.1 (49.0)  |\\n| Natural language inference (NLI) | | | | | | | | |\\n| RTE          | 49.2 (44.8) | 47.6 (44.8)   | 49.2 (44.8)  | 49.2 (44.8) | 49.2 (44.8)  | 50.5 (44.8) | 50.5 (44.8) | 50.5 (44.8)  |\\n| QQP          | 55.0 (44.8) | 53.1 (39.9)   | 54.0 (39.7)  | 55.0 (44.8) | 57.9 (44.8)  | 59.1 (44.8) | 60.7 (44.8) | 61.2 (44.8)  |\\n| QNLI         | 81.0 (76.7) | 80.4 (76.7)   | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7) | 81.0 (76.7)  |\\n| WNLI         | 74.3 (63.3) | 72.8 (61.4)   | 75.7 (63.3)  | 77.2 (65.1) | 76.3 (65.1)  | 77.8 (66.1) | 79.5 (66.1) | 79.5 (66.1)  |\\n| RTE          | 49.2 (44.8) | 47.6 (44.8)   | 49.2 (44.8)  | 49.2 (44.8) | 49.2 (44.8)  | 50.5 (44.8) | 50.5 (44.8) | 50.5 (44.8)  |\\n| MNLI-m / -mm | 56.7 (49.0) | 56.3 (49.0)   | 56.3 (49.0)  | 59.0 (49.0) | 59.0 (49.0)  | 60.1 (49.0) | 60.1 (49.0) | 60.1 (49.0)  |\\n| Natural language inference (NLI) | | | | | | | | |\\n| RTE          | 49.2 (44.8) | 47.6 (44.8)   | 49.2 (44.8)  | 49.2 (44.8) | 49.2 (44.8)  | 50.5 (44.8) | 50.5 (44.8) | 50.5 (44.8)  |\\n| QQP          | 55.0 (44.8) | 53.1 (39.9)   | 54.0 (39.7)  | 55.0 (44.8) | 57.9 (44.8)  | 59.1 (44.8) | 60.7 (44.8) | 61.2 (44.8)  |\\n| QNLI         | 81.0 (76.7) | 80.4 (76.7)   | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7)  | 81.0 (76.7) | 81.0 (76.7) | 81.0 (76.7)  |\\n| WNLI         | 74.3 (63.3) | 72.8 (61.4)   | 75.7 (63.3)  | 77.2 (65.1) | 76.3 (65.1)  | 77.8 (66.1) | 79.5 (66.1) | 79.5 (66.1)  |\\n| RTE          | 49.2 (44.8) | 47.6 (44.8)   | 49.2 (44.8)  | 49.2 (44.8) | 49.2 (44.8)  | 50.5 (44.8) | 50.5 (44.8) | 50.5 (44.8)  |\\n| MNLI-m / -mm | 56.7 (49.0) | 56.3 (49.0)   | 56.3 (49.0)  | 59.0 (49.0) | 59.0 (49.0)  | 60.1 (49.0) | 60.1 (49.0) | 60.1 (49.0)  |\\n\\nThe experimental results of SpeechGLUE and GLUE are shown in Table 4. The best scores are highlighted in bold, and the ones in parentheses are the results of the upstream model. Note that, in calculating the average score, we exclude the results of the chance rate (w/o SSL). The choice of the model for each task was determined based on the performance on the development set. The performance on the test set was consistently better than on the development set, indicating the feasibility of the approach. The choice of the model for each task was determined based on the performance on the development set. The performance on the test set was consistently better than on the development set, indicating the feasibility of the approach.\"}"}
{"id": "ashihara23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A. van den Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d in NeurIPS, 2018.\\n\\nR. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, A. Y. Ng, P. Denoyer, and Y. Bengio, \u201cGroundup: A new dataset for measuring language understanding,\u201d in Proceedings of the Fourth International Conference on Learning Representations (ICLR), 2016.\\n\\nS. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan, \u201cThe third IEEE PASCAL challenge on textual entailment,\u201d in Proceedings of the Third International Conference on Learning Representations (ICLR), 2015.\\n\\nW. B. Dolan and C. Brockett, \u201cAutomatically constructing a conversational agent\u2019s dialogue history,\u201d in Proceedings of the 2006 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2006.\\n\\nJ. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder for joint speech and language modeling,\u201d in Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018.\\n\\nT. A. Nguyen, B. Sagot, and E. Dupoux, \u201cAre discrete units necessary for spoken language modeling?\u201d in Proceedings of the Second Conference on Artificial Intelligence and Natural Language Processing (IWP), 2005.\\n\\nA. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \u201cA broad-coverage challenge corpus for sentence understanding through inference,\u201d in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\\n\\nS. Shon, A. Pasad, F. Wu, P. Brusco, Y. Artzi, K. Livescu, and A. Godin, \u201cLayer-wise analysis of a language model,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.\\n\\nA. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d arXiv preprint arXiv:2010.11929, 2020.\\n\\nH.-S. Tsai, H.-J. Chang, W.-C. Huang, Z. Huang, K. Lakhotia, D. Giampiccolo, B. Magnini, I. Dagan, and B. Dolan, \u201cThe third IEEE PASCAL challenge on textual entailment,\u201d in Proceedings of the Third International Conference on Learning Representations (ICLR), 2015.\\n\\nR. van der Goot, M. M\u00fcller-Eberstein, and B. Plank, \u201cFrustrated learning: A new dataset for measuring language understanding,\u201d in Proceedings of the Fourth International Conference on Learning Representations (ICLR), 2016.\\n\\nP. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \u201cSQuAD: 100,000+ questions for machine comprehension of text,\u201d in Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2016.\\n\\nS. Shon, A. Pasad, F. Wu, P. Brusco, Y. Artzi, K. Livescu, and A. Godin, \u201cLayer-wise analysis of a language model,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2019.\\n\\nA. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d arXiv preprint arXiv:2010.11929, 2020.\\n\\nH. J. Levesque, E. Davis, and L. Morgenstern, \u201cThe Winograd S Honourability Test,\u201d in Proceedings of the Third Conference on Empirical Methods in Natural Language Processing (EMNLP), 2007.\\n\\nE. Dunbar, M. Bernard, N. Hamilakis, T. A. Nguyen, L. Bentivogli, P. Clark, I. Dagan, and D. Giampiccolo, \u201cThe TAC KBP 2017 evaluation corpus for knowledge distillation of self-supervised speech models,\u201d in Proceedings of the First Conference on Natural Language Understanding (NLU), 2017.\\n\\nJ. Shor, A. Jansen, R. Maor, O. Lang, O. Tuval, F. de Chauvel, S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y. Ueda, S. Kumar, K. Ganesan, Y. Peng, S. Arora, Y. Higuchi, Y."}
