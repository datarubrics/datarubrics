{"id": "ding23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stable Speech Emotion Recognition with Head-k-Pooling Loss\\nChaoyue Ding1\u2217, Jiakui Li1\u2217, Daoming Zong1\u2217, Baoxiang Li1\u2020, Tianhao Zhang1,2, Qunyan Zhou1\\n\\nAbstract\\nSpeech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability.\\n\\nIndex Terms: speech emotion recognition, self-attention, temporal convolution, negative sampling\\n\\n1. Introduction\\nEmotion, as one of the basic paralinguistic information, conveys the user's intention and status, which helps the speech interaction system to improve user experience. Speech emotion recognition (SER), aiming to identify human emotions from speech, has been an active research field for decades [1, 2, 3]. It is widely used in numerous applications, such as intelligent robots, automated call centers, and distance education [4, 5]. Recently, deep learning methods have garnered increasing attention due to their powerful representation capabilities [6, 7, 8]. Particularly, convolutional neural network (CNN)-based methods have seen substantial improvements in SER over traditional methods, as the inductive biases inherent to CNNs, such as spatial locality and translation equivariance, are believed to be helpful for learning sound speech representations [9, 10]. Earlier works use fixed-scale convolutional kernels to extract emotion-related representations, but ignores the variation in the expression of emotions at different scales [11, 12, 13]. Later, work on multi-scale CNNs was proposed to address this issue, such as Light-SERNet [9] and deep-CNN [14]. They proposed to aggregate the information of multi-scale features extracted by multiple branches of convolutional kernels with varying sizes. Despite their modest success, these methods still suffer from either emphasizing too much on local relationship modeling or ignoring segment-level label mismatch.\\n\\nFor the first point, notice that most existing CNN-based\\n\\nmethods can directly capture the local relationship between adjacent feature frames, but lack the ability to model the long-term dependencies between temporally distant frames, which may also be valuable for SER recognition [15, 16]. Recent approaches also explore the self-attention mechanism [17] to capture long-term relationships in speech for emotion detection [4, 18, 19]. Since self-attention can relate any two temporal spans of the feature input, it comes naturally to integrate self-attention and CNNs to model global and local temporal dependencies for SER. For the second point, given an utterance, current SER methods usually divide it into multiple segments using fixed windows and hop lengths, then use the same label to supervise each segment for training. However, the true emotions of these segments may not match the original utterance label. This is because the degree of human emotional intensity is time-varying. Emotional variability can be represented by two main dimensions: valence (a continuum from negative to positive) and arousal (a continuum that varies from low to high) [20]. Depending on the speaker's individual pronunciation habits (e.g., variation in intonation and pitch), the valence and arousal value of an utterance may not be maintained at the same level over time. This means, as shown in Fig. 1, a Chinese Mandarin utterance labeled as happy may also contain some neutral segments. With this in mind, training all segments with the original utterance label will introduce noise data and...\"}"}
{"id": "ding23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"may lead to model performance degradation. In this work, we propose a global-local relation fusion network, named GLRF, for speech emotion recognition. The core component of GLRF is its inherent global-local relation-aware (GLRA) block, where one branch specializes in local context modeling (by temporal convolution) while another branch specializes in long-distance relationship modeling (by multi-head self-attention). Besides, to reduce error propagation caused by misallocation of segment labels, we design a novel head-$k$-pooling loss for GLRF training, which enables the model to pick the top $k$% confident segments to teach itself. More specifically, we automatically select the top $k$% segments as decision-makers according to the posterior probability of the current emotion category output by the model, and these decision-makers are trained using the same label of utterance. The remaining segments are treated as assistants. Under the current emotion category, the gradients for those assistants are set to zero, as it is uncertain whether these assistants have the same emotion category as the decision-makers, and they may present a neutral affective state. When performing negative sampling, these assistants are only judged to be negative samples of categories that differ greatly from the valence and arousal value of the current emotion category. As an example shown in Fig. 1, for an assistant segment of happy, we just assume that its class is unlikely to be fear, sad, or angry and do not regard it as a negative sample of the neutral class. To summarize, our main contributions are as follows:\\n\\n- We propose a GLRF, a novel architecture for speech emotion recognition, which can efficiently capture both long-term and short-term temporal dependencies in speech.\\n- We customize a head-$k$-pooling loss to enhance the training of GLRF. This loss helps the model select the top $k$% of segments with the highest confidence based on posterior probabilities, reducing errors caused by inaccurate segment label assignments and minimizing the propagation of errors in the training process.\\n- We introduced ST-EMO, the largest (153 hours) Chinese Mandarin speech emotion dataset for advancing SER research. In particular, ST-EMO covers some in-car driving scenarios.\\n\\n2. Method\\n\\nModel Pipeline. Fig. 2 illustrates the overall architecture of the proposed GLRF. Given a long-form utterance, we first split it into several 2s segments by a sliding window of 1.8s overlap. We then extract the mel-frequency cepstral coefficients (MFCCs) features per segment as input. During training, we send each segment to GLRF and obtain the segment-level confidence scores for all emotion categories by learning multiple binary classifiers for each class vs. all other classes (i.e., one-vs-rest). We then select segments with the top $k$% confidence and train them with the same label as the corresponding utterance via multiple binary cross-entropy losses. Below we detail the core ingredient of GLRF, namely the global-local relation-aware (GLRA) block, and head-$k$-pooling loss tailored for SER.\\n\\n2.1. Global-Local Relation-Aware Block\\n\\nGlobal-Local Relation Module. Formally, GLRF takes a set of MFCCs features $X \\\\in \\\\mathbb{R}^{N \\\\times F \\\\times T}$ as input, where $N$ is the number of segments, $T$ is the number of frames per segment, and $F$ denotes the feature dimension. To begin with, $X$ is first temporally convolved by a 1D convolution with a kernel size of 3, followed by batch normalization and ReLU activation, yielding the feature map of $F_0 \\\\in \\\\mathbb{R}^{N \\\\times C_0 \\\\times T}$. $F_0$ is then sent into the GLRA block, which can be decomposed into global and local relation-aware branches (see Fig. 2). The global branch models global contexts by a normal multi-head self-attention module, while the local branch captures local contexts by a composition of temporal convolutions and linear layers. Both branches use residual connections. Such a dual-stream approach places attention and convolution modules in parallel, encouraging them to have different views on the speech from global and local perspectives, so that the architecture can benefit from specialization and achieve higher efficiency.\\n\\nGlobal-Local Fusion Module. Later, we fuse information from two branches via gates. To determine the contribution of the information flow, gates need to integrate information from two branches. This can be achieved through an element-wise summation. Then we apply global average pooling to generate channel-wise statistics as $s$. Further, we compress $s$ into a compact feature $z$ via a simple fully-connected (FC) layer for efficiency. Next, $z$ is distributed to calculate the attention scores of each information flow. Finally, we multiply the normalized attention scores by the results from two branches. After a GLRA block, we get $F_1 \\\\in \\\\mathbb{R}^{N \\\\times C_1 \\\\times T}$. We stack multiple GLRA blocks and perform max-pooling between two GLRA blocks to halve the size of the temporal dimension $T$. The output of the final GLRA block is fed into an FC layer for scoring each segment.\"}"}
{"id": "ding23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ments, we choose to use those most certain segments, referred to as decision-makers, to guide the model. Formally, let $N$ denote the number of segmented segments for an utterance. $C$ is the cardinality of emotion classes, and $z_{ij}$ is the output logits of the classifier for the $i$-th class in the $j$-th segment. For an utterance labeled as emotion class $i$, we first select the $K$ segments based on the posterior probabilities by:\\n\\n$$\\\\{\\\\ell_j\\\\}_{K \\\\cdot j=1} = \\\\text{rank}((\\\\text{sigmoid}(z_{1i}), \\\\ldots, \\\\text{sigmoid}(z_{N_i})))$$\\n\\nWhere $\\\\text{rank}(k)$ is an operator that returns the indices of the top $k\\\\%$ elements ($k \\\\in [10, 100]$, $K = \\\\lceil k\\\\% \\\\cdot N \\\\rceil$), the $\\\\ell_j$ indexes the selected segment. Then the head-k-pooling loss for a single utterance can be defined as:\\n\\n$$L = \\\\frac{1}{K} \\\\sum_{j=1}^{C} \\\\sum_{i} \\\\left[ y_{\\\\ell_ji} \\\\log(1+e^{-z_{\\\\ell_ji}}) + (1-y_{\\\\ell_ji}) \\\\log(1+e^{z_{\\\\ell_ji}}) \\\\right]$$\\n\\nNegative Sampling. We perform negative sampling in unselected segments. Two situations need to be carefully considered: 1) if the emotional state of an utterance is labeled as neutral, it can be used as a negative sample for any other category; and 2) if the current label of an utterance is non-neutral, it can appear in the negative sample pool excluding current class and neutral class. For example, segments labeled as happy in an utterance are unlikely to be fear, sad or angry, but some of them may be neutral.\\n\\n3. Experiments\\n3.1. Datasets and Evaluation Protocols\\nWe evaluate our model on the Interactive Emotional Dyadic Motion Capture database (IEMOCAP) \\\\[21\\\\] and our collected Chinese Mandarin dataset ST-EMO. Specifically, IEMOCAP contains 12 hours of English audiovisual data divided into five sections, each with scripted and improvised scenes recorded by a male and a female professional actor. Scripted parts are performed for predetermined emotions, while improvised parts are closer to natural speech. Following previous works \\\\[2, 9, 22\\\\], we first merge the two classes of excited and happy as they are close in the valence and arousal (VA) domain \\\\[23\\\\], then select four types of emotions (i.e., angry, happy, sad, and neutral) for experiments, and evaluate on the full (scripted+improvised) dataset.\\n\\nST-EMO comprises 153 hours of audio data, which are divided into five emotion classes (i.e., angry, happy, sad, neutral, and fear), and were recorded by 50 male actors and 50 female actresses in the text-independent, text-dependent, and improvised scenario, respectively. Speaker age ranges from 20 to 41. ST-EMO was collected in several small rooms with no background noise, using Android smartphones (48kHz, 16-bit) fixed at a distance of 25cm directly in front of the actors/actresses. The duration of all utterances in ST-EMO ranges between 1.0 and 17.2 seconds, with an average duration of 5.6 seconds, and over 91.2% of them range between 3 and 10 seconds. Text-independent and text-dependent scripts are relevant to everyday life, which resembles the SMP2020-EWECT \\\\[24\\\\] and CPED \\\\[25\\\\] datasets. Statistically, the text-independent section consists of 100 scripts, each of which was performed by all actors/actresses with five distinct emotions, e.g., saying the script \\\"\u539f\u4f86\u662f\u6709\u4f60\u9001\u4f60\u56de\u4f86\\\" with five distinct emotions. The improvised section includes 100 in-car scenarios, with each emotion corresponding to 100 scripts, and Table 1:\\n\\n|            | Angry | Happy | Neutral | Sad | Fear | All   |\\n|------------|-------|-------|---------|-----|------|-------|\\n| Independent| 9,132 | 5,818 | 9,644   | 8,760| 7,048| 40,402|\\n| Dependent  | 9,711 | 7,323 | 9,587   | 9,134| 7,402| 43,157|\\n| Improvisation | 1,978 | 1,781 | 1,980   | 1,641| 1,800| 9,180 |\\n| Full Dataset | 20,821| 14,922| 21,211  | 19,535| 16,250| 92,739|\\n\\nTable 2: Performance comparison between GLRF and its counterparts in terms of WA (%), UA (%), and Micro-F1 (%).\\n\\n|            | WA      | UA      | Micro-F1 |\\n|------------|---------|---------|----------|\\n| Deep-CNN   | 64.59   | 65.35   | 64.43    |\\n| L-SERNet   | 68.21   | 68.10   | 68.15    |\\n| GLAM       | 69.74   | 71.03   | 69.70    |\\n| DRN-MHSA   | 66.67   | 67.56   | 66.73    |\\n| AA-CNN     | 70.38   | 71.71   | 70.13    |\\n| GLRF       | 72.81   | 73.39   | 72.92    |\\n\\nAfter collecting the raw data, we denoised the data. For each piece of data, we consulted four experts to judge its emotion category. The data is considered valid only when the categories given by at least two experts are consistent with the original label of the data. By this means, we got a total of 92,739 pieces of data. Three commonly-used metrics are adopted for assessment: weighted accuracy (WA), unweighted accuracy (UA), and micro-F1 score. The difference between WA and UA is that UA considers label imbalance in computing accuracy, while UA does not. In the testing phase, the final prediction is obtained by averaging the predictions over all segments in an utterance. In addition, to gain a better understanding of the proposed loss, we develop a new metric, called pure accuracy, to measure the smoothness of model predictions. Under the criterion of pure accuracy, a model's prediction is deemed correct only if it satisfies one of the following conditions:\\n\\ni) for a test utterance labeled as non-neutral, the model must hit the current label at least once across all segments while allowing only the neutral class to be present in the prediction results except the current class; ii) for a test utterance labeled as neutral, the model's prediction for all segments must be neutral. Denote by $N_f$ the number of all utterances, $N_c$ the number of utterances identified as correct, and we have $\\\\text{Pure-Acc} = \\\\left(\\\\frac{N_c}{N_f}\\\\right) \\\\cdot 100\\\\%$.\\n\\n3.2. Baselines and Implementation Details\\nWe take several strong SER baselines into account for model comparison, including Deep-CNN \\\\[14\\\\], L-SERNet \\\\[9\\\\], GLAM \\\\[23\\\\], DRN-MHSA \\\\[4\\\\], and AA-CNN \\\\[19\\\\]. It is noteworthy that GLRF exhibits a significant departure from the above methods. While certain methods like AA-CNN and GLAM also use the attention mechanism or gMLP \\\\[26\\\\] to capture long-distance relationships, their attention is built on top of multiple CNN layers. In contrast, GLRF runs the attention and\"}"}
{"id": "ding23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our experiments, we apply the 5-fold cross-validation to ensure reliability on IEMOCAP. We randomly select 80% of the data for training and the remaining 20% for testing, following [14, 19, 27]. We split the ST-EMO dataset into training and test sets in a ratio of 8:2 with no speaker overlap. All compared methods are run on the same data split while using their suggested hyperparameter configurations. Furthermore, we fine-tune all models to achieve their optimal performance. During training, each utterance is split into 2s segments with 1.8s overlap, and during testing, each utterance is divided into 2s segments with 1.6s overlap. The MFCCs features of each segment are extracted as input. Finally, the scores of all segments within the same utterance are averaged to yield the prediction result.\\n\\n3.3. Main Results\\n\\nTable 2 illustrates that GLRF outperforms the state-of-the-art methods by a substantial margin in all the listed metrics. Notably, GLRF exhibits a 2.43% absolute improvement in W A on IEMOCAP compared to the current leading method, AA-CNN, and a 2.8% absolute improvement in W A on ST-EMO. These results fully demonstrate the superiority and effectiveness of our model. For an in-depth analysis, we show the confusion matrices derived from our model and AA-CNN in Fig. 3, to clarify the model's predictions for specific emotion categories. It is evident that the model's precision has exhibited the most significant enhancement in the three emotional categories of neutral, angry, and fear compared to AA-CNN, with a remarkable surge of 3.64%, 3.22%, and 3.61%, respectively.\\n\\nPure accuracy is an SER evaluation protocol that takes the predictions for each segment into account and is particularly sensitive to sharp predictions (i.e., those with significant differences in the valence and arousal domain). Higher pure accuracy indicates smoother and more stable model predictions. As illustrated in Fig. 4, our model outperforms other methods in pure accuracy, implying that GLRF is more suitable for streaming deployment. In streaming SER scenarios, the model is required to provide real-time predictions for each segment. Models with higher pure accuracy tend to avoid the jump of predictions between different classes, leading to a better user experience.\\n\\n3.4. Ablation Study\\n\\nThis section provides ablation studies to shed light on the effect of each component of GLRF, as well as the proposed loss. Impact of $k$ in Loss Function. Table 3 presents the results of GLRF trained on a range of $k$ values. Note that $k$ in the head-$k$-pooling loss function denotes the percentage of segments that contribute to the loss calculation (a.k.a. decision makers). As $k$ approaches 100, the head-$k$-pooling loss function degenerates into the conventional cross-entropy loss. The table shows that setting $k = 70$ yields the best performance, so we use it by default in our experiments. Increasing $k$ beyond this value leads to a decrease in pure accuracy.\\n\\nEffectiveness of Key Components. To elucidate the influence of individual components in the GLRF, we conduct a comprehensive component analysis by iteratively replacing each component with one from the full GLRF and evaluating the resultant performance, as depicted in Table 4. Specifically, setting (1) entails the substitution of summation for the fusion manner in GLRA; setting (2) involves the replacement of global and local branches with two convolutions (conv3 $\\\\times$ 3 and conv5 $\\\\times$ 5) in GLRA; setting (3) corresponds to the substitution of head-$k$-pooling loss with cross-entropy loss. Overall, the model performance exhibits a considerable decline as each component is replaced, verifying the efficacy of the proposed components.\\n\\n4. Conclusion\\n\\nIn this work, we present GLRF, a novel architecture for speech emotion recognition. It combines the strengths of convolution and self-attention, performing local context modeling and long-distance relationship modeling in parallel while capturing both globality and locality. Furthermore, we tailor a head-$k$-pooling loss to facilitate training on SER tasks, enabling the model to teach itself by picking the most confident top $k$% segments. Besides, we introduce ST-EMO, the largest Chinese Mandarin speech emotion dataset for SER research. Experimental results on two speech emotion benchmarks demonstrate the superiority and stability of our model.\"}"}
{"id": "ding23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] K. Han, D. Yu, and I. Tashev, \u201cSpeech emotion recognition using deep neural network and extreme learning machine,\u201d in Proc. INTERSPEECH, 2014, pp. 223\u2013227.\\n\\n[2] A. Satt, S. Rozenberg, R. Hoory et al., \u201cEfficient emotion recognition from speech using deep learning on spectrograms.\u201d in Proc. INTERSPEECH, 2017, pp. 1089\u20131093.\\n\\n[3] J. Santoso, T. Yamada, K. Ishizuka, T. Hashimoto, and S. Makino, \u201cPerformance improvement of speech emotion recognition by neutral speech detection using autoencoder and intermediate representation,\u201d in Proc. INTERSPEECH, 2022, pp. 4700\u20134704.\\n\\n[4] R. Li, Z. Wu, J. Jia, S. Zhao, and H. Meng, \u201cDilated residual network with multi-head self-attention for speech emotion recognition,\u201d in Proc. ICASSP, 2019, pp. 6675\u20136679.\\n\\n[5] M. B. Akc\u00b8ay and K. O \u02d8guz, \u201cSpeech emotion recognition: Emotional models, databases, features, preprocessing methods, supporting modalities, and classifiers,\u201d Speech Communication, vol. 116, pp. 56\u201376, 2020.\\n\\n[6] D. Dai, Z. Wu, R. Li, X. Wu, J. Jia, and H. Meng, \u201cLearning discriminative features from spectrograms using center loss for speech emotion recognition,\u201d in Proc. ICASSP, 2019, pp. 7405\u20137409.\\n\\n[7] X. Li, W. Wang, X. Hu, and J. Yang, \u201cSelective kernel networks,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 510\u2013519.\\n\\n[8] B. T. Atmaja, A. Sasou, and M. Akagi, \u201cSurvey on bimodal speech emotion recognition from acoustic and linguistic information fusion,\u201d Speech Communication, vol. 140, pp. 11\u201328, 2022.\\n\\n[9] A. Aftab, A. Morsali, S. Ghaemmaghami, and B. Champagne, \u201cLIGHT-SERNET: A lightweight fully convolutional neural network for speech emotion recognition,\u201d in Proc. ICASSP, 2022, pp. 6912\u20136916.\\n\\n[10] G. Trigeorgis, F. Ringeval, R. Brueckner, E. Marchi, M. A. Nicolaou, B. Schuller, and S. Zafeiriou, \u201cAdieu features? end-to-end speech emotion recognition using a deep convolutional recurrent network,\u201d in Proc. ICASSP, 2016, pp. 5200\u20135204.\\n\\n[11] D. Bertero and P. Fung, \u201cA first look into a convolutional neural network for speech emotion detection,\u201d in Proc. ICASSP, 2017, pp. 5115\u20135119.\\n\\n[12] J. Zhao, X. Mao, and L. Chen, \u201cSpeech emotion recognition using deep 1d & 2d cnn lstm networks,\u201d Biomedical Signal Processing and Control, vol. 47, pp. 312\u2013323, 2019.\\n\\n[13] K. Chauhan, K. K. Sharma, and T. Varma, \u201cSpeech emotion recognition using convolution neural networks,\u201d in International Conference on Artificial Intelligence and Smart Systems, 2021, pp. 1176\u20131181.\\n\\n[14] D. Issa, M. F. Demirci, and A. Yazici, \u201cSpeech emotion recognition with deep convolutional neural networks,\u201d Biomedical Signal Processing and Control, vol. 59, p. 101894, 2020.\\n\\n[15] Z. Wu, Z. Liu, J. Lin, Y. Lin, and S. Han, \u201cLite transformer with long-short range attention,\u201d arXiv preprint arXiv:2004.11886, 2020.\\n\\n[16] R. Dai, S. Das, K. Kahatapitiya, M. S. Ryoo, and F. Br \u00b4emond, \u201cMS-TCT: Multi-scale temporal convtransformer for a lightweight model based on separable convolution for speech emotion recognition detection,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 20 041\u201320 051.\\n\\n[17] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, 2017, p. 5998\u20136008.\\n\\n[18] M. Xu, F. Zhang, and S. U. Khan, \u201cImprove accuracy of speech emotion recognition with attention head fusion,\u201d in Annual Computing and Communication Workshop and Conference, 2020, pp. 1058\u20131064.\\n\\n[19] M. Xu, F. Zhang, X. Cui, and W. Zhang, \u201cSpeech emotion recognition with multiscale area attention and data augmentation,\u201d in Proc. ICASSP, 2021, pp. 6319\u20136323.\\n\\n[20] P. E. Bestelmeyer, S. A. Kotz, and P. Belin, \u201cEffects of emotional valence and arousal on the voice perception network,\u201d Social Cognitve and Affective Neuroscience, vol. 12, pp. 1351\u20131358, 2017.\\n\\n[21] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIEMOCAP: Interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, pp. 335\u2013359, 2008.\\n\\n[22] Z. Aldeneh and E. M. Provost, \u201cUsing regional saliency for speech emotion recognition,\u201d in Proc. ICASSP, 2017, pp. 2741\u20132745.\\n\\n[23] W. Zhu and X. Li, \u201cSpeech emotion recognition with global-aware fusion on multi-scale feature representation,\u201d in Proc. ICASSP, 2022, pp. 6437\u20136441.\\n\\n[24] \u201cThe evaluation of weibo emotion classification technology, SMP2020-EWECT,\u201d The Ninth China National Conference on Social Media Processing, 2020. [Online]. Available: https://smp2020ewect.github.io/\\n\\n[25] Y. Chen, W. Fan, X. Xing, J. Pang, M. Huang, W. Han, Q. Tie, and X. Xu, \u201cCPED: A large-scale chinese personalized and emotional dialogue dataset for conversational AI,\u201d arXiv preprint arXiv:2205.14727, 2022.\\n\\n[26] H. Liu, Z. Dai, D. So, and Q. V. Le, \u201cPay attention to MLPs,\u201d in Advances in Neural Information Processing Systems, 2021, pp. 9204\u20139215.\\n\\n[27] Y. Liu, H. Sun, W. Guan, Y. Xia, and Z. Zhao, \u201cDiscriminative feature representation based on cascaded attention network with adversarial joint loss for speech emotion recognition,\u201d in Proc. INTERSPEECH, 2022, pp. 4750\u20134754.\"}"}
