{"id": "tran23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAutomatic voice pathology detection is promising for non-invasive screening and early intervention using sound signals. Nevertheless, existing methods are susceptible to covariate shifts due to background noises, human voice variations, and data selection biases leading to severe performance degradation in real-world scenarios. Hence, we propose a non-invasive framework that contrastively learns personalization from sound waves as a pre-train and predicts latent-spaced profile features through semi-supervised learning. It allows all subjects from various distributions (e.g., regionality, gender, age) to benefit from personalized predictions for robust voice pathology in a privacy-fulfilled manner. We extensively evaluate the framework on four real-world respiratory illnesses datasets, including Coswara, COUGHVID, ICBHI, and our private dataset - ASound under multiple covariate shift settings (i.e., cross-dataset), improving up to 4.12% in overall performance.\\n\\nIndex Terms: covariate shift, robust voice pathology detection\\n\\n1. Introduction\\n\\nThe advent of sound and speech technology has opened many new possibilities in voice pathology detection, such as chronic diseases and infectious respiratory conditions (e.g., COVID-19) [1, 2, 3]. Conventional methods for respiration monitoring, such as thoracic impedance pneumography [4], or capnography [5], are either too invasive or inconvenient. Recent studies show the use of artificial intelligence (AI) based pathology detection for cost-effective and non-invasive screening and monitoring of a wide spectrum of diseases. Nevertheless, such models are susceptible to reliability issues in real-world scenarios as trained models tend to learn mixed characteristics linked to personal information under various categories rather than downstream features, thereby leading to poor performance in action.\\n\\nFigure 1 illustrates covariate shift in a cross-dataset validation scenario, where a classification model is trained to achieve acceptable performances on one dataset (source) and then fails when being tested on similar (target) datasets. Indeed, human sounds have distinctive characteristics associated with disease-related information which can be exploited as latent profile features for better learning and reliability.\\n\\nIn this work, we propose an end-to-end AI-based Voice Pathology Detection framework with two aims: (1) learning latent profile characteristics in sound waves, and (2) maintaining the robustness of AI systems due to the distribution shift issue. A transformer-based model is pre-trained to learn personalized features through masked contrastive learning with negatives sampled from other users. Then, this model is used to extract latent profile embedding for each subject based on all samples they provided. Additionally, the model can be self-trained to enhance the embedding and support users with insufficient samples, i.e., new patients. The profile embedding is used in finetuning with a traditional classification model in an end-to-end manner. Our proposed approach fulfills privacy compliance by learning latent space features through anonymized sound data without utilizing additional personal information.\\n\\nOur main contributions are listed as follows:\\n\\n\u2022 To our best knowledge, it is the first work to propose a personalization strategy within a unified deep-learning framework to mitigate covariate shifts in the domain of sound data without touching sensitive meta information.\\n\\n\u2022 Introduce a novel personalized encoding method using each data subject\u2019s sound samples to build their profiles based on pre-training with contrastive learning and can be applied to all users through semi-supervised learning.\\n\\n\u2022 Evaluate our methods with comprehensive experiments on multiple real-world datasets, both publicly available, such as Coswara, COUGHVID, ICBHI, and our private datasets, namely ASound. The results demonstrate the efficacy of our proposed framework.\\n\\n\u2022 Source codes, reproducible baselines, and our new dataset are made publicly available at https://github.com/ReML-AI/RoPADet for future research and benchmarking purposes.\\n\\n2. Related Work\\n\\nCovariate shift refers to possible changes in the distribution of the input variables present in the training and the testing data.\"}"}
{"id": "tran23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"i.e., $P_{\\\\text{train}}(X,y) \\\\neq P_{\\\\text{test}}(X,y)$. The continuous nature of sound data brings a variety of noisy aspects unique to each user, which the first or second iteration's results typically yield the best performances on downstream tasks for RoPADet.\\n\\nThis helps learn a discriminative model that distinguishes time step with distractors sampled from audios of other users. For instance, in [13], the authors leveraged other samples of additional samples of the same user as the user embedding. In COVID-19, the intuition is when audio samples are collected from different areas (e.g., a different country), but can be useful for early detection in another region. Therefore, our framework is robust to covariate shift and privacy-preserving prevention of the pandemic in another region. Thus, we obtained a profile extractor for COVID-19 progression prediction and recovery trend prediction mechanisms, covariate issues can be addressed, for example, disentangle them from illness-related features. With the profiling, RoPADet, that leverages pre-training to strengthen base signals, regardless of the characteristics as a pre-train and predicting profile embeddings for unseen human sounds through semi-supervised self-training. These embeddings capture individual respiratory-related characteristics and sounds.\\n\\n3.1. Latent profiling for personalization\\n\\nFor COVID-19, the intuition is grouping and extracting profile features from multiple samples of the same subject, invariant to the target task and without using sensitive data such as gender or maturity. Covariate shift is a reoccurring problem and it has been studied in multiple works regarding the applicability of personalization in multiple areas with personalization. In COVID-19, the intuition is grouping and extracting profile features from multiple samples of the same subject, invariant to the target task and without using sensitive data such as gender or maturity. Personalization is a well-established and complex topic, where different areas with personalization. In COVID-19, the intuition is grouping and extracting profile features from multiple samples of the same subject, invariant to the target task and without using sensitive data such as gender or maturity. Personalization is a well-established and complex topic, with multiple applications, such as in personalized speech recognition [10] or hate speech detection [11]. It also has a multitude of medical applications [12]. A typical personalized machine learning method leverages additional cues, such as sensitive metadata related to each user. Another direction utilizes a personalized profile extractor that enables learning latent characteristics from intrinsic medical conditions, gender, and age, leading to robust sound classification.\\n\\nFor COVID-19, we investigated profile information that captures each sub-characteristic and base signals, regardless of the characteristics as a pre-train and predicting profile embeddings for unseen human sounds through semi-supervised self-training. These embeddings capture individual respiratory-related characteristics and sounds. To our best knowledge, we are the first to introduce a personalized profile extractor for sounds, named RoPADet, that leverages pre-training to strengthen base signals, regardless of the characteristics as a pre-train and predicting profile embeddings for unseen human sounds through semi-supervised self-training. These embeddings capture individual respiratory-related characteristics and sounds.\\n\\nFormally, define\\n\\n\\\\[ D = \\\\{ P_{\\\\text{train}}(X,y) \\\\} \\\\]\\n\\nas the dataset of samples with label $y$.\\n\\nAfter the profile pre-training phase on $D$, we evaluate the model $\\\\theta$ on $P_{\\\\text{test}}(X,y)$ and obtain the final RoPADet.\\n\\n\\\\[ \\\\text{Output} \\\\quad \\\\hat{f}(D) \\\\]\\n\\nInput\\n\\n\\\\[ \\\\text{feat} = \\\\{ \\\\text{pre-train} \\\\} \\\\]\\n\\nAlgorithm 1\\n\\n```\\n1: // Initialization\\n2: W \u2190 \u22122\\n3: \\\\( \\\\theta \\\\) \u2190 random initialization\\n4: // Pre-train with masked contrastive learning (MCL)\\n5: for all user $u_i$ do\\n6: \\\\( f \\\\) \u2190 \u22121D-CNN+Transformer architecture of RoPADet\\n7: \\\\( \\\\theta \\\\) \u2190 \u2212profile\\n8: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n9: \\\\( \\\\eta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n10: for $\\\\text{iter} = 1$ to $\\\\text{MaxIter}$ do\\n11: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n12: \\\\( D_{\\\\text{pred}} \\\\) \u2190 \u2212\\\\{\\\\}\\n13: \\\\( D_{\\\\text{NP}} \\\\) \u2190 \u2212\\\\{\\\\}\\n14: \\\\( D_{\\\\text{NP}} \\\\) \u2190 \u2212\\\\{\\\\}\\n15: \\\\( D_{\\\\text{NP}} \\\\) \u2190 \u2212\\\\{\\\\}\\n16: \\\\( D_{\\\\text{NP}} \\\\) \u2190 \u2212\\\\{\\\\}\\n17: // Train student\\n18: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n19: Loss \u2190 \u2212\\\\( \\\\theta \\\\)\\n20: // Update profile extractor\\n21: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n22: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n23: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n24: // Update teacher\\n25: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n26: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n27: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n28: // Fine-tune\\n29: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n30: Loss \u2190 \u2212\\\\( \\\\theta \\\\)\\n31: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n32: \\\\( \\\\theta \\\\) \u2190 \u2212\\\\( \\\\theta \\\\)\\n```\\n\\nreturn $\\\\theta$\"}"}
{"id": "tran23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Downstream task training\\n\\nTo learn the downstream task $T$ with the advantage of the personalized profile information, a downstream feature extractor $f_{\\\\theta_{\\\\text{feat}}}$ is leveraged with profile features:\\n\\n$$\\\\hat{y}_i = W(f_{\\\\theta_{\\\\text{feat}}}(X_i) \\\\oplus \\\\text{pred}_u), u_i = u(2)$$\\n\\nwhere $W$ denotes a linear classifier head and $\\\\oplus$ denotes the concatenate operation. $f_{\\\\theta_{\\\\text{profile}}}$ used to compute $\\\\text{pred}_u$ is obtained from the previous pre-training and self-training stages and is frozen in this stage. This is correspondence with RoPADet.\\n\\nFinally, the model is trained in an end-to-end manner through back-propagation via a cross-entropy loss function:\\n\\n$$\\\\text{Loss} = \\\\sum_i y_i \\\\log(\\\\sigma(\\\\hat{y}_i)) + (1 - y_i) \\\\log(1 - \\\\sigma(\\\\hat{y}_i))$$\\n\\nAlgorithm 1 describes the overall training pipeline. As listed in Algorithm 1, from line 5 to 7, we apply a pre-training method that learns discriminative features between samples through contrastive learning. The self-train stage is carried out for learning pseudo profiles, as in line 8 to line 23. The pre-trained or self-trained model, act as the profile extractor, has the same architecture as the conventional model for downstream task, excluding the downstream classification layers. We perform an average pooling operation on the features extracted from samples provided for personalization and used this as the profile feature. The profile feature is concatenated with the classification features before input to a downstream classifier, to perform end-to-end training, demonstrated in line 28-31.\\n\\n3.3. Model architecture\\n\\nFollowing recent advancements, we adapt a transformer-based model for downstream feature learning branch and neural profile extractor of RoPADet, as depicted in Figure 2. Unlike prior works that directly input raw waveform, a spectrum representation of waveform is extracted and fed into front-end 1-D temporal CNN layers before input through the transformer encoder blocks. Our method views each timestep that constitutes frequency signals and nearby information as tokens for the self-attention mechanism. While previous works input waveform, or patches of spectrogram, ours is an efficient way of combining these signals w.r.t the nature of sound data. Moreover, we can leverage the masked pre-training task of transformers as auxiliary task for self-training.\\n\\nTable 1: Performances when train on ASound-P (with profile information) and test on ASound-P and ASound-NP (without profile information).\\n\\n| Exp Method | Profile-learning | Profile-inference | AUC on ASound-P | AUC on ASound-NP |\\n|------------|-----------------|------------------|----------------|-----------------|\\n| #1 CNN-based | \u2717              | \u2717               | 0.6783         | 0.3714          |\\n| #2 Transformer | \u2717              | \u2717               | 0.8561         | 0.3822          |\\n| #3 RoPADet | \u2713              | \u2717               | 0.8693         | 0.3858          |\\n| #4 RoPADet | \u2713              | \u2713               | 0.8699         | 0.3964          |\\n\\nTable 2: Performances of proposed methods on ICBHI dataset (official 60-40 split).\\n\\n$\\\\uparrow$ means higher number is better.\\n\\n| Exp Method | Additional training data | Persona- | Sens. | Spec. | ICBHI | \u2191 |\\n|------------|--------------------------|----------|-------|-------|-------|---|\\n| #1 SoTA#1 | \u2713                         | \u2717        | 57.3  | 30.0  | 85.6  |   |\\n| #2 SoTA#2 | \u2713                         | \u2717        | 57.55 | 39.15 | 75.95 | \u2b06 |\\n| #3 SoTA#3 | \u2713                         | \u2717        | 58.29 | 37.24 | 79.34 |   |\\n| #4 SoTA#4 | \u2717                         | \u2717        | 53.90 | 36.36 | 71.44 |   |\\n| #5 SoTA#5 | \u2717                         | \u2717        | 54.74 | 33.84 | 75.35 |   |\\n| #6 RoPADet| \u2717                         | \u2713        | 58.86 | 40.79 | 76.93 | \u2b06 |\\n\\n4. Results and Discussions\\n\\n4.1. Datasets\\n\\nWe extensively conduct experiments on 3 real-world respiratory sound datasets: COUGHVID ($n=7379$) [21], Coswara ($n=4306$) [22], and ICBHI ($n=6898$) [23]. Moreover, we collected a crowdsourced dataset, named ASound ($n=4495$), for respiratory illness detection recorded using mobile phones without profile information. Motivated by reliability issues, we conducted a second collection phase, in which if consent is given, we assigned each participant a unique anonymized identifier for personalization. Two additional variants are composed for pre-training and self-training investigations: ASound-P includes samples with profile information ($n=570$ by 117 users), and ASound-NP without profile information ($n=1651$). We further...\"}"}
{"id": "tran23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Performances when train on ASound, Coswara, and COUGHVID. Trained models are then evaluated on each in-domain or out-domain test sets. We focus on a comparison between our proposed model with or without personalization. Here we denote results with subscript $L$, $M$, and $H$ as in the scenario where the effect of covariate shift is low, medium or high, consecutively. The notation $\\\\uparrow$ indicates that a higher number is better, whereas $\\\\downarrow$ indicates that a lower number is better.\\n\\n| Exp | Train Dataset | Method | Test on ASound | Test on Coswara | Test on COUGHVID |\\n|-----|---------------|--------|----------------|-----------------|------------------|\\n| #1  | ASound        | Transformer-based | 0.9673          | 0.0420          | 0.7258 0.1722 0.5156 0.1215 |\\n| #2  | RoPADet       | $L$    | 0.9687          | 0.0450          | 0.7386 0.1631 0.5309 |\\n|     |               | $M$    | $L$            | 0.1390          | 0.8102 0.1342 |\\n| #3  | ASound        | Transformer-based | 0.7619          | 0.1720          | 0.8081 0.1912 |\\n| #4  | RoPADet       | $L$    | 0.7691          | 0.1390          | 0.8102 0.1342 |\\n|     |               | $H$    |                |                | 0.5015 |\\n| #5  | COUGHVID      | Transformer-based | 0.4987          | 0.2819          | 0.6078 0.2414 |\\n| #6  | RoPADet       | $H$    | 0.5157          | 0.2804          | 0.6173 |\\n\\nTable 4: Performances when train on ASound-P (obtained from experiments in Table 1), and then evaluate (AUC scores) on each sub-group of users that contributed to ASound-NP.\\n\\n| Exp | Method | Age Group | Variance between Gender Group |\\n|-----|--------|-----------|-------------------------------|\\n|     | Transformer-based | $0^{-18}$ | $\\\\uparrow$                     |\\n| #1  | Transformer-based | $18^{-23}$ | $\\\\uparrow$                     |\\n| #2  | RoPADet     | $23^{-28}$ | $\\\\uparrow$                     |\\n|     | RoPADet     | $28^{-33}$ | $\\\\uparrow$                     |\\n| #3  | RoPADet     | $33^{-100}$ | $\\\\uparrow$                     |\\n\\nNote: $\\\\uparrow$ indicates that a higher number is better, whereas $\\\\downarrow$ indicates that a lower number is better.\\n\\nvalidate our personalization strategy on ICBHI as profile information is provided (multiple sound samples per patient).\\n\\n4.2. Setup\\nEvaluation metrics. Standard evaluation metrics such as precision, recall, F1, AUC are all reported for future benchmarking purposes. Among these metrics, AUC score is our main focused metric, to be consistent with previous works and for evaluation on imbalanced datasets; ICBHI metric is used in the ICBHI challenge (i.e., the average of the sensitivity and the specificity); Brier score [24] is used to evaluate covariate shift in cross-dataset scenarios.\\n\\nExperimental settings. Our proposed framework is implemented using fairseq [25]. All experiments are carried out on an Ubuntu Server (20.04 LTS) with 2 RTX 3090 GPUs. For all settings, we pre-trained our model on combined training sets. For finetuning, models are trained for 100 epochs with AdamW optimizer, a learning rate of 1e-4, and a batch size of 64. Unless otherwise specified, all scores are reported based on 5-fold cross-validation (on ASound, Coswara, and COUGHVID) or the average of 5 random seeds (on ICBHI).\\n\\n4.3. Results\\n4.3.1. Performances with personalization\\nIn this experiment, we evaluate the effectiveness of personalization strategy through pre-training for profile learning and self-training for profile inference on ASound-P (with profile information) and ASound-NP (with no profile information). Results in Table 1 indicate that while pre-trained profile extractor improves performance on data with, but maintains same performance on data without sufficient profile labels over Transformer-based model that share same architecture but without personalization. When we apply semi-supervised learning through self-training, we observe a large improvement for ASound-NP, up to 2.5% over the baseline CNN. This proves the usefulness of our proposed personalization approach through pre-training and self-training. Moreover, results on ICBHI dataset demonstrated in Table 2 where RoPADet with personalization advance the SoTA for solutions without additional training data by 4.12% in ICBHI score. It also achieved a gap of 0.5% compared to solutions using extra training data, without additional training data and fewer parameters. This also shows the advantages of our framework.\\n\\n4.3.2. Performances under covariate shift scenarios\\nNext, we compare models with or without personalization in real-world scenarios where the profile information may not be available. We first train a separate model for each dataset: ASound, Coswara, and COUGHVID. Then, models are evaluated on each test set of the 3 datasets. Results shown in Table 3 demonstrate consistent improvements in cross-dataset evaluations, up to 1.7% in case of train on COUGHVID and then test on ASound, as shown in Exp#5 and Exp#6.\\n\\n4.3.3. Fair performance evaluation across sub-groups\\nTo ensure fair classification for each sub-group, we evaluate models on each sub-group of users that contributed samples to ASound-NP. Sub-groups are constructed based on the user's age or gender. The results illustrated in Table 4 show that our proposed personalization framework improves performances across all sub-groups and allows for fair classification results between groups with low variances.\\n\\n5. Conclusion\\nIn this study, we proposed RoPADet, a neural network personalization approach with good improvements through pre-training and self-training. We focused on the problem of covariate shift, which is a significant problem leading to a detrimental performance in AI systems. These consequences might decrease the system's trustworthiness and safety, especially in the healthcare domain. Our personalization technique can be utilized for a variety of patient-related healthcare tasks. It can be applied even if the user is new to the system (i.e., no profile is provided), and as the user continues to use the system, it becomes more adaptable and reliable. Our approach not only helps improve the system's performance but also takes into account users' safety and privacy and aids in gaining their belief. Extensive experiments on various benchmark datasets and tasks from real-world settings confirmed the effectiveness and generalizability of the proposed approach. Future works may be interested in more advanced methods for fusing user profiles and classification features or in weighting user samples, such as exploiting temporal orders.\"}"}
{"id": "tran23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. Imran, I. Posokhova, H. N. Qureshi, U. Masood, M. S. Riaz, K. Ali, C. N. John, M. I. Hussain, and M. Nabeel, \u201cAI4COVID-19: AI enabled preliminary diagnosis for COVID-19 from cough samples via an app,\u201d Informatics in Medicine Unlocked, vol. 20, p. 100378, 2020.\\n\\n[2] G. T. Frost, G. Theron, and T. Niesler, \u201cTB or not TB? Acoustic cough analysis for tuberculosis classification,\u201d in Proc. Inter-speech 2022, 2022, pp. 2448\u20132452.\\n\\n[3] P. A. P\u00b4erez-Toro, P. Klumpp, A. Hernandez, T. Arias, P. Lillo, A. Slachevsky, A. M. Garc\u00b4\u0131a, M. Schuster, A. K. Maier, E. Noeth, and J. R. Orozco-Arroyave, \u201cAlzheimer\u2019s Detection from English to Spanish Using Acoustic and Linguistic Embeddings,\u201d in Proc. Interspeech 2022, 2022, pp. 2483\u20132487.\\n\\n[4] J. H. Yoo, H. Jeong, J. Lee, and T.-M. Chung, \u201cFederated learning: Issues in medical application,\u201d in Future Data and Security Engineering. Springer International Publishing, 2021, pp. 3\u201322.\\n\\n[5] A. F. Pacela, \u201cImpedance pneumography\u2014a survey of instrumentation techniques,\u201d Medical and biological engineering, vol. 4, no. 1, pp. 1\u201315, 1966.\\n\\n[6] H. Coppock, L. Jones, I. Kiskin, and B. Schuller, \u201cCOVID-19 detection from audio: seven grains of salt,\u201d The Lancet Digital Health, vol. 3, no. 9, pp. e537\u2013e538, Sep. 2021.\\n\\n[7] J. Han, T. Xia, D. Spathis et al., \u201cSounds of COVID-19: exploring realistic performance of audio-based digital testing,\u201d NPJ digital medicine, vol. 5, no. 1, pp. 1\u20139, 2022.\\n\\n[8] M. Roberts, D. Driggs, M. Thorpe, J. Gilbey, M. Ursprung, A. I. Aviles-Rivero, C. Etmann, C. McCague, L. Beer, J. R. Weir-McCall, Z. Teng, E. Gkrania-Klotsas, A. Ruggiero, A. Korhonen, E. Jefferson, E. Ako, G. Langs, G. Gozaliasl, G. Yang, H. Prosch, J. Preller, J. Stanczuk, J. Tang, J. Hofmanninger, J. Babar, L. E. S \u00b4anchez, M. Thillai, P. M. Gonzalez, P. Teare, X. Zhu, M. Patel, C. Cafolla, H. Azadbakht, J. Jacob, J. Lowe, K. Zhang, K. Bradley, M. Wassin, M. Holzer, K. Ji, M. D. Oreet, T. Ai, N. Walton, P. Lio, S. Stranks, T. Shadbahr, W. Lin, Y . Zha, Z. Niu, J. H. F. Rudd, E. Sala, and C.-B. S. and, \u201cCommon pitfalls and recommendations for using machine learning to detect and prognosticate for COVID-19 using chest radiographs and CT scans,\u201d Nature Machine Intelligence, vol. 3, no. 3, pp. 199\u2013217, Mar. 2021.\\n\\n[9] M. Mohammadamini, D. Matrouf, J.-F. Bonastre, S. Dowerah, R. Serizel, and D. Jouvet, \u201cBarlow Twins self-supervised learning for robust speaker recognition,\u201d in Proc. Interspeech 2022, 2022, pp. 4033\u20134037.\\n\\n[10] I. McGraw, R. Prabhavalkar, R. Alvarez, M. G. Arenas, K. Rao, D. Rybach, O. Alsharif, H. Sak, A. Gruenstein, F. Beaufays et al., \u201cPersonalized speech recognition on mobile devices,\u201d in 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2016, pp. 5955\u20135959.\\n\\n[11] M. R. Awal, R. Cao, R. K.-W. Lee, and S. Mitrovi \u00b4c, \u201cAngrybert: Joint learning target and emotion for hate speech detection,\u201d in PAKDD. Springer, 2021, pp. 701\u2013713.\\n\\n[12] T. Golany and K. Radinsky, \u201cPGANs: Personalized generative adversarial networks for ecg synthesis to improve patient-specific deep ecg classification,\u201d AAAI, vol. 33, no. 01, pp. 557\u2013564, Jul. 2019.\\n\\n[13] A. Sivaraman, S. Kim, and M. Kim, \u201cPersonalized Speech Enhancement Through Self-Supervised Data Augmentation and Purification,\u201d in Proc. Interspeech 2021, 2021, pp. 2676\u20132680.\\n\\n[14] T. Dang, J. Han, T. Xia et al., \u201cExploring Longitudinal Cough, Breath, and Voice Data for COVID-19 Disease Progression Prediction via Sequential Deep Learning: Model Development and Validation,\u201d Journal of Medical Internet Research, vol. 24, 02 2022.\\n\\n[15] S. Liu, A. Mallol-Ragolta, E. Parada-Cabaleiro, K. Qian, X. Jing, A. Kathan, B. Hu, and B. W. Schuller, \u201cAudio self-supervised learning: A survey,\u201d Patterns, vol. 3, no. 12, p. 100616, 2022.\\n\\n[16] T. Vu, M.-T. Luong, Q. Le, G. Simon, and M. Iyyer, \u201cSTraTA: Self-training with task augmentation for better few-shot learning,\u201d in Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. Online and Punta Cana, Dominican Republic: Association for Computational Linguistics, Nov. 2021, pp. 5715\u20135731.\\n\\n[17] L. Pham, D. Ngo, K. Tran, T. Hoang, A. Schindler, and I. McLoughlin, \u201cAn Ensemble of Deep Learning Frameworks for Predicting Respiratory Anomalies,\u201d in 2022 44th Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), 2022, pp. 4595\u20134598.\\n\\n[18] I. Moummad and N. Farrugia, \u201cSupervised Contrastive Learning for Respiratory Sound Classification,\u201d arXiv preprint arXiv:2210.16192, 2022.\\n\\n[19] T. Nguyen and F. Pernkopf, \u201cLung sound classification using co-tuning and stochastic normalization,\u201d IEEE Transactions on Biomedical Engineering, vol. 69, no. 9, pp. 2872\u20132882, 2022.\\n\\n[20] J. Li, J. Yuan, H. Wang, S. Liu, Q. Guo, Y . Ma, Y . Li, L. Zhao, and G. Wang, \u201cLungAttn: advanced lung sound classification using attention mechanism with dual TQWT and triple STFT spectrogram,\u201d Physiological Measurement, vol. 42, no. 10, p. 105006, oct 2021.\\n\\n[21] L. Orlandic, T. Teijeiro, and D. Atienza, \u201cThe COUGHVID crowdsourcing dataset, a corpus for the study of large-scale cough analysis algorithms,\u201d Scientific Data, vol. 8, no. 1, Jun. 2021.\\n\\n[22] N. Sharma, P. Krishnan, R. Kumar, S. Ramoji, S. R. Chetupalli, N. R., P. K. Ghosh, and S. Ganapathy, \u201cCoswara \u2014 A Database of Breathing, Cough, and Voice Sounds for COVID-19 Diagnosis,\u201d in Proc. Interspeech 2020, 2020, pp. 4811\u20134815.\\n\\n[23] B. M. Rocha, D. Filos, L. Mendes, G. Serbes, S. Ulukaya, Y . P. Kahya, N. Jakovljevic, T. L. Turukalo, I. M. V ogiatzis, E. Perantonis, E. Kaimakamis, P. Natsiavas, A. Oliveira, C. J \u00b4acome, A. Marques, N. Maglaveras, R. P. Paiva, I. Chouvarda, and P. de Carvalho, \u201cAn open access database for the evaluation of respiratory sound classification algorithms,\u201d Physiological Measurement, vol. 40, no. 3, p. 035001, Mar. 2019.\\n\\n[24] G. W. Brier, \u201cVerification of forecasts expressed in terms of probability,\u201d Monthly Weather Review, vol. 78, no. 1, pp. 1\u20133, Jan. 1950.\\n\\n[25] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grangier, and M. Auli, \u201cfairseq: A fast, extensible toolkit for sequence modeling,\u201d in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations). Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 48\u201353.\"}"}
