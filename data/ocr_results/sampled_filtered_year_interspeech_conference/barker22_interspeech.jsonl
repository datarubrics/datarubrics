{"id": "barker22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nThis paper reports on the design and outcomes of the 1st Clarity Prediction Challenge (CPC1) for predicting the intelligibility of hearing aid processed signals heard by individuals with a hearing impairment. The challenge was designed to promote the development of new intelligibility measures suitable for use in developing hearing aid algorithms. Participants were supplied with listening test data compromising 7233 responses from 27 individuals. Data was split between training and test sets in a manner that fostered a machine learning approach and allowed both closed-set (known listeners) and open-set (unseen listener/unseen system) evaluation. The paper provides a description of the challenge design including the datasets, the hearing aid algorithms applied, the listeners and the perceptual tests. The challenge attracted submissions from 15 systems. The results are reviewed and the paper summarises, compares and contrasts approaches.\\n\\nIndex Terms: speech-in-noise, speech intelligibility, hearing aid, hearing loss, machine learning\\n\\n1. Introduction\\nNew approaches to hearing aid (HA) signal processing are emerging that use machine learning (ML) techniques (e.g., [1]). These approaches may have the potential to revolutionise hearing aid effectiveness for speech-in-noise listening, a situation for which current aids often provide little benefit. However, to fully exploit ML for HAs requires better objective speech intelligibility measures, i.e., to act as targets for optimisation and automatic evaluation. This paper presents the first Clarity Prediction Challenge (CPC1), the first open signal processing competition to directly address this need. Competitors were tasked with designing intelligibility predictors for listeners with a hearing impairment (HI). The challenge ran between November 2021 and April 2022 and below the challenge design is outlined and initial outcomes presented.\\n\\nWhile speech intelligibility prediction is not new, the requirements for HAs go beyond what most traditional approaches provide. For example, many intelligibility measures have been developed for the telecommunications industry, evolving out of the speech transmission index [2]. Measures focus on modelling the impact of additive or convolutive channel noise (a key concern in the telecoms industry), but are insufficient for modelling the highly non-linear processing performed by HAs. More recent methods, notably short-time objective intelligibility (STOI) [3], have been designed using time-frequency weighting. These have typically been developed using young adults with so-called \u2018normal hearing\u2019, however, and nearly all lack approaches for including hearing loss.\\n\\nCPC1 was designed with the development of individualised measures and hearing impairment at its core. Entrants were supplied with speech-in-noise signals that have been processed by a range of experimental HAs. This audio has also been evaluated by a panel of listeners with hearing impairment. CPC1 competitors were tasked to predict the intelligibility of a specific sentence as heard by a listener with quantified hearing characteristics (audiograms and other standard test results.) This provides a high degree of challenge for speech intelligibility algorithms which, to score well, need to produce good predictions over a wide range of signal and listener types.\\n\\nWhile intelligibility prediction is a growing area, with much new work emerging (e.g., [4, 5, 6, 7, 8, 9, 10]), there are a lack of common datasets and tasks. So a further motivation for CPC1 has been to benchmark recent approaches. The challenge has attracted input from some of the major groups working in this area, and so represents a useful snapshot of the state-of-the-art.\\n\\nThe paper is set out as follows. Section 2 describes the materials including the signals and listener data from which the challenge is constructed. Section 3 describes the challenge tasks and rules, and briefly outlines the baseline system that was provided to entrants. The challenge submissions are reviewed in Section 4 with results presented in Section 5. The paper concludes in Section 6 with initial findings and future directions.\\n\\n2. Materials\\nEntrants were challenged to predict the intelligibility of speech-in-noise signals processed by HAs for given listeners. The key factors outlined below are: the signals that were processed; the HA systems; the listener characteristics; and the intelligibility measurements.\\n\\n2.1. The Signals\\nA 1,500 subset of the 10,000 speech-in-noise scenes generated for the first Clarity Enhancement Challenge (CEC1) [1] were used. Target utterances are 7 to 10 word studio-recorded sentences [11] and the interferers are recordings of common domestic noises such as washing machines. The scenes are simulated first by convolving the source signals with Binaural Room Impulse Responses, which are created in a geometric room...\"}"}
{"id": "barker22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acoustic model [12] using the OlHead-HRTF database [13] (which has recordings for behind-the-ear HA devices with three microphones). As outlined in [1]: room dimensions and materials; and listener, target and noise locations, are randomised. The level of the interferer signal was adjusted to obtain a specific speech-weighted better ear SNR at the front HA microphone. The SNRs range from -6 to 6 dB chosen on the basis of pilot testing with 13 unaided hearing-impaired listeners. The reverberated speech and noise signals were then summed. The interferer always preceded the onset of the target speech by 2 s and continues for 1 s after the target offset.\\n\\n2.2. The Hearing Aid Systems\\nThe signals have been processed with 10 different HA algorithms from the Clarity Enhancement Challenge (CEC1) [1]. In that challenge, entrants had been tasked with producing HA systems that maximise the intelligibility of speech-in-noise scenes for listeners with known audiograms. Algorithms were allowed to run offline with no constraint on computational cost, however, they had to use causal signal processing with a maximum allowed algorithmic latency of 5 ms.\\n\\nThe HA systems varied considerably, with different approaches to: single channel source separation; multichannel beamforming, and signal amplification. (Full details appear in the Proceedings of the Clarity-2021 Workshop [14]). The algorithms also varied considerably in effectiveness, both in terms of objective measures and listening test outcomes.\\n\\n2.3. The Listeners and Listening Tests\\nA panel of hearing-impaired listeners was recruited each characterised by bilateral pure-tone audiograms measured at [250, 500, 1000, 2000, 3000, 4000, 6000, 8000] Hz. Only listeners who had a hearing loss of no larger than 80 dB HL in more than two bands were included. Exclusion criteria included: use of hearing intervention other than acoustic hearing aids; diagnosis of Meniere's disease or hyperacusis, or of severe tinnitus. Hearing loss severity, defined as the average loss in dB HL between 2 and 8 kHz inclusive, was mild (15\u201335 dB) for 1 listener, moderate (35\u201356 dB) for 9 listeners and severe (>56 dB) for 17 listeners with a range of 35 dB to 76 dB.\\n\\nThe listening tests were conducted using the project's Listen@Home software running on Lenovo 10e Chromebook tablets with Sennheiser PC-8 headsets. Participants used these in quiet rooms in their own homes. The software presented the signals in blocks (one HA algorithm per block) and listeners were asked to repeat what they heard. The voice recordings were first converted to text using the Google Cloud Speech-to-Text API and then a team of transcribers validated the Speech-to-Text outputs and corrected errors. Ethical approval was obtained from Nottingham Audiology Services and NHS UK (IRAS Project ID: 276060).\\n\\n27 listeners completed the tests, providing a total of 7233 responses. An intelligibility score was computed for each listener's response to a sentence. The transcription of the listener's response was aligned with the ground-truth text and the number of correctly identified words was counted. The percentage words correct was used as a proxy for sentence intelligibility.\\n\\nThe variable effectiveness of the systems, the different levels of hearing impairment, and SNR range of the source material, provided a wide spread in intelligibility scores and a challenging prediction task.\\n\\n1. https://cloud.google.com/speech-to-text\\n\\n3. Challenge Tasks and Baseline\\nParticipants were asked to predict the intelligibility scores using either \u2018intrusive\u2019 or \u2018non-intrusive\u2019 systems. The distinction is explained in Figure 1. Intrusive systems, such as the challenge baseline, use representations of the noise-free reference speech signal as an additional input. This is compared to the hearing aid outputs and the distance is mapped onto an intelligibility score. Note, in ML approaches (such as many of the systems submitted to the challenge), separate feature extraction, distance measure and mapping functions may be combined into a complex non-linear model such as a deep neural network with many trained parameters. Non-intrusive systems attempt to estimate intelligibility directly from the HA output signal itself. Generally, this is a harder task and non-intrusive systems are expected to have worse scores.\\n\\n![Figure 1: Examples of intrusive vs. non-intrusive models.](image-url)\"}"}
{"id": "barker22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The parameters of the logistic model were learnt from the training data using a least-mean-square error fit. Note, although billed as a 'baseline', this is a sophisticated approach that is expected to produce scores that are not easy to beat. Raw baseline predictions are shown in Figure 2.\\n\\n### 4. Submissions\\n\\nA total of 15 systems were submitted originating from 9 separate teams, with most evaluating their systems for both the open and closed sets. Systems have been categorised as intrusive or non-intrusive with a roughly even split between the two types (see column 'Intr' of Table 1).\\n\\n#### 4.1. Modelling Hearing Loss\\n\\nHearing loss (HL) was modelled in a variety of ways. Many systems used the MSBG model from the baseline (E16, E19, E23, E29, E31, E32, E33, E36). Of these E19 adapted the model by adding noise at the audiogram thresholds to ensure inaudible signal components could not contribute to intelligibility. System E36 employed MSBG indirectly by using the baseline system predictions as one of several inputs to a more complex deep-learning fusion system. In contrast, System E02 and E38 modelled hearing loss using an auditory peripheral model that simulated spike activity in the auditory nerves [17]; their model could be tuned for a given audiogram. Systems E06 and E34 were built on top of the hearing-aid speech perception index (HASPI), which has an internal HL model [18].\\n\\nSystems E22 and E30 were unusual in that they contained no explicit HL model. E30 used standard better-ear STOI between the HA output and the reference, but then introduced listener individuality by combining STOI (and other features) with the listener ID in a final regression stage (possible only in the closed-set track). E22 applied an existing non-intrusive model directly to the HA outputs and modelled listener variability in the closed-set track by fitting different intelligibility mapping functions for each listener.\\n\\n#### 4.2. Binaural modelling\\n\\nAll entrants had to handle the binaural nature of the challenge. Listening tests were based on the outputs of binaural HA algorithms responding to spatialised scenes, often with spaced target and interferer locations. Further, the listeners may have had different degrees of hearing loss in each ear. This aspect was challenging because binaural cues in the audio might have been degraded by the HA processing. Unsurprisingly, no entrant used monaural processing by averaging signals or using only one HA output.\\n\\nThe most common strategy, employed in the baseline and by entrants E02, E06, E19, E22, E29, E32, E38, mimicked the 'better ear' processing that humans listeners employ. In this strategy, intelligibility scores are made independently for each ear and the overall intelligibility is formed from the better of the two scores. At one extreme this can be performed for the entire signal (e.g., E30). Alternatively, better ear decisions can be made based on short temporal windows and then integrated over time (e.g., E32). The static nature of the scenes probably favoured the former, simpler approach.\\n\\nAlternatively (or in addition), left and right ears outputs can be combined at the signal level - mimicking human binaural unmasking. The equalization-cancellation (EC) model of binaural processing [19] uses cross-correlation to detect and cancel the effects of noise interference. System E19 used EC processing for frequencies up to 1,500 Hz and better ear processing above 1,500 Hz. The baseline system used an EC approach in combination with better ear processing. E02 and E38 used spiking neural models and a form of EC processing via spike alignment. Most systems chose not to employ traditional binaural unmasking, however. In those that did (e.g., the baseline, E19), the binaural information was found to be too weak to be useful and intelligibility scores were dominated by the better-ear effect.\\n\\nOther entrants performed binaural processing via data-driven information fusion at various levels. For example, E16 and E33 used a linear layer to fuse frame-level intelligibility decisions. At the other extreme, E23 and E35 concatenated windows from the left and right HA signals, from which latent features were extracted considering maximum mutual information.\\n\\n#### 4.3. Intrusive systems\\n\\nIntrusive systems traditionally operate by comparing the corrupted signal with that of a clean reference signal using a perceptual-motivated distance measure. Problems arise with HA processing because algorithms can introduce non-linear distortion to the signal to increase intelligibility (e.g., frequency shifting). So the appropriate distance measure is not clear, especially if the characteristics of the HA processor are unknown. Nevertheless, techniques like MBSTOI used in baseline, which measure distance using within-band envelope correlations, can produce reasonable scores over a range of operating conditions.\\n\\nFor the systems submitted, E19 used the Binaural Speech Intelligibility model (BSIM) [20] for representing the signals and performed the comparison using the Speech Transmission Index [2] (similar to STOI). E02 used a neural spiking representation and measured mutual information for the comparison. E30 had multiple intrusive and non-intrusive components with the intrusive measure being based on STOI.\\n\\nOther systems used more abstract representations and/or non-linear regressions to compare the signals. E31 used a single Convolutional Neural Network (CNN) to jointly extract features from the within channel envelopes of the reference and degraded signals. Whereas E32 passed both signals separately through an end-to-end speech recognition system and compared the latent representations formed at various stages in the model. E31 and E36 also used speech recognition modelling to extract deep representations of the signal. Whereas, E30 took the simpler approach of using the prompt text directly (also considered intrusive) to judge the predictability of the sentence, as listeners would find high probability sentences more intelligible.\"}"}
{"id": "barker22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Evaluation of 15 submitted systems plus baseline for RMS prediction error (RMSE) and ground-truth vs prediction correlation (Corr). Results are shown for closed set (Track 1) and open set (Track 2). \u2018Intr\u2019 Yes indicates an intrusive system. \u2018Prior\u2019 is a system blindly always guessing the mean of the training data intelligibility.\\n\\n| Entrant | Intr. | RMSE \u2193 | Corr \u2191 | Prior RMSE \u2193 | Prior Corr \u2191 |\\n|---------|-------|--------|--------|--------------|--------------|\\n| E30 [22] | Yes  | 22.5 \u00b1 0.5 | 0.79 | \u2013 | \u2013 |\\n| E32 [23] | Yes  | 23.1 \u00b1 0.5 | 0.77 | 23.5 \u00b1 0.9 | 0.76 |\\n| E29 [24] | No   | 23.3 \u00b1 0.5 | 0.77 | 24.6 \u00b1 1.0 | 0.73 |\\n| E36 [25] | Yes  | 24.0 \u00b1 0.5 | 0.76 | 29.2 \u00b1 1.2 | 0.60 |\\n| E33 [26] | No   | 24.1 \u00b1 0.5 | 0.75 | 28.9 \u00b1 1.1 | 0.65 |\\n| E16 [26] | No   | 24.7 \u00b1 0.5 | 0.74 | 30.7 \u00b1 1.2 | 0.59 |\\n| E22 [27] | No   | 25.9 \u00b1 0.5 | 0.70 | 32.1 \u00b1 1.2 | 0.54 |\\n| E19 [28] | Yes  | 27.5 \u00b1 0.6 | 0.66 | 28.1 \u00b1 1.1 | 0.63 |\\n| Base. [1] | Yes  | 28.5 \u00b1 0.6 | 0.62 | 36.5 \u00b1 1.4 | 0.53 |\\n| E06 [29] | No   | 32.0 \u00b1 0.7 | 0.50 | \u2013 | \u2013 |\\n| E34 [29] | No   | 33.4 \u00b1 0.7 | 0.43 | \u2013 | \u2013 |\\n| E35 [30] | No   | 35.4 \u00b1 0.7 | 0.25 | 35.7 \u00b1 1.4 | 0.22 |\\n| E31 [31] | Yes  | 37.2 \u00b1 0.7 | 0.41 | 28.3 \u00b1 1.1 | 0.67 |\\n| E23 [32] | No   | 41.5 \u00b1 0.7 | 0.07 | 43.7 \u00b1 1.5 | 0.05 |\\n| E02 [33] | Yes  | \u2013 | \u2013 | 35.2 \u00b1 1.4 | 0.38 |\\n| E38 [33] | Yes  | \u2013 | \u2013 | 49.7 \u00b1 1.5 | 0.30 |\\n\\n4. Non-intrusive systems\\nThe challenge also attracted diverse non-intrusive approaches. Some directly trained systems that tried to learn the speech intelligibility (SI) scores from the degraded signals directly. Whereas others built statistical models of intelligible speech (e.g., training a speech recogniser) and then looked at confidence measures when the system was presented with degraded speech. For example, E29 used confidence measures extracted from an end-to-end speech recognition system, which was first trained on a large speech dataset and then adapted using the MSBG processed signals from the CPC1 training set. A similar approach using automatic speech recognition (ASR) confidence measures was used in E22. E35 used contrastive predictive coding and vector quantization to extract features from a deep learning based SI predictor trained on the CPC1 training set [21]. Whereas, as an example of the direct approach, E16 attempted to learn the SI score from the hearing loss model outputs using a CNN-LSTM architecture with an attention layer. Unsupervised learning was used on MSBG outputs to form a suitable latent representation of the hearing impaired signals. E06 and E34 attempted to build a network trained to predict HASPI scores from the degraded signal only.\\n\\n5. Results\\nResults are summarised in Table 1 showing: the RMS prediction error with one standard error, and also the correlation between predicted and ground-truth scores. Systems are ranked by the Track 1 RMSE performance (best first). The table also includes the RMSE score achieved when simply guessing the mean of the training data intelligibility for all signals (\u2018Prior\u2019).\\n\\nThe very best systems had an RMSE of 22.5 (closed) and 23.5 (open) and correlations of 0.79 and 0.76, respectively. This was significantly better than the baseline system. It was also notable that the best approaches are quite close in performance despite the diversity of the approaches taken. It was expected from the outset that it would not be possible for systems to achieve very low RMSE scores: there will be some aspects of the listener responses that are simply not predictable from the information provided, e.g., momentary distractions or loss of concentration leading to spurious responses etc.\\n\\nAs expected nearly all systems scored higher on the closed-set challenge than on the open-set, other than system E31. The baseline system scored 28.5 (closed) and 36.5 (open). The performance drop was larger for most of the submitted systems, but unlike some entries, the baseline system had an identical configuration in both tracks. For the closed-set task, the baseline was quite competitive with 8 submissions performing better but 5 performing worse, whereas in the open-track 11 out of 13 systems outperformed the baseline. Note, surprisingly two submissions in each track have performed poorer than the prior system, which guessed the training-set mean speech intelligibility score for every case. One of these was a new approach using spike activity that did not use individual listener characteristics, and so still has potential for improvement [33].\\n\\nWhile the challenge quantified hearing abilities, this data appears to have been less useful for predicting speech intelligibility than expected. Given that the listeners had control over the volume level on the tablet and the HA processors already had amplification stages, this probably meant that audibility was not crucial to the listening task, and hence the audiogram was less useful than anticipated. Only a small number of entrants used the other hearing measures (a suprathreshold metric and results from hearing and HA use questionnaires). Nevertheless, the best system in Track 1 did use the listener ID, which would act as a proxy to hearing acuity. In addition, it captured other non-acoustic factors from the listening tests (e.g., how quickly subjects gave up when the task got difficult).\\n\\nThe ranking of the intrusive systems was on average higher than that of the non-intrusive systems, however, the best non-intrusive system was able to come very close to the top performance, i.e., 23.3 vs. 22.5 (closed) and 24.6 vs. 23.5 (open). This was surprising given the large amount of extra information that was provided by access to the reference speech signal.\\n\\n6. Conclusions\\nThis paper outlined the first ever open-challenge for speech intelligibility prediction for signals processed by hearing aids. The best competitors made significant improvements on the baseline model. For the closed track, the best system used a CNN+LSTM with a wide range of input data in addition to the audio from the HA. The best open track model used latent and other representations from a DNN-ASR as inputs to an SI model. The best intrusive methods performed only slightly better than non-intrusive methods. This bodes well for future Clarity Enhancement Challenges to improve HA processing, because accurate non-intrusive models allow optimization of more non-linear ML approaches. The next Prediction Challenge (CPC2) in 2023 will include new data that will reduce the overfitting of models to a single database of listening tests.\\n\\n7. Acknowledgements\\nClarity is funded by UKRI (EP/S031448/1, EP/S031308/1, EP/S031324/1 and EP/S030298/1). We thank Amazon, the Hearing Industry Research Consortium and the Royal National Institute for the Deaf (RNID) for their support.\"}"}
{"id": "barker22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] S. Graetzer, J. Barker, T. J. Cox, M. Akeroyd, J. F. Culling, G. Naylor, E. Porter, and R. V. Mu\u00f1oz, \u201cClarity-2021 challenges: Machine learning challenges for advancing hearing aid processing,\u201d in *Proceedings of Interspeech 2021*, Aug. 2021, pp. 686\u2013690.\\n\\n[2] T. Houtgast and H. Steeneken, \u201cA physical method for measuring speech-transmission quality,\u201d *The Journal of the Acoustical Society of America*, vol. 67, pp. 318\u2013326, 1980.\\n\\n[3] C. H. Taal, R. C. Hendriks, R. Heusdens, and J. Jensen, \u201cAn algorithm for intelligibility prediction of time\u2013frequency weighted noisy speech,\u201d *IEEE Transactions on Audio, Speech, and Language Processing*, vol. 19, no. 7, pp. 2125\u20132136, 2011.\\n\\n[4] K. Yamamoto, T. Irino, T. Matsui, S. Araki, K. Kinoshita, and T. Nakatani, \u201cPredicting speech intelligibility using a gammachirp envelope distortion index based on the signal-to-distortion ratio.\u201d in *INTERSPEECH*, 2017, pp. 2949\u20132953.\\n\\n[5] C. S\u00f8rensen et al., \u201cNon-intrusive intelligibility prediction using a codebook-based approach,\u201d in *EUSIPCO*. IEEE, 2017, pp. 216\u2013220.\\n\\n[6] C. Spille et al., \u201cPredicting speech intelligibility with deep neural networks,\u201d *Computer Speech & Language*, vol. 48, pp. 51\u201366, 2018.\\n\\n[7] K. Arai, S. Araki, A. Ogawa, K. Kinoshita, T. Nakatani, K. Yamamoto, and T. Irino, \u201cPredicting speech intelligibility of enhanced speech using phone accuracy of DNN-based asr system.\u201d in *Interspeech*, 2019, pp. 4275\u20134279.\\n\\n[8] M. Karbasi, S. Bleeck, and D. Kolossa, \u201cNon-intrusive speech intelligibility prediction using automatic speech recognition derived measures,\u201d *arXiv preprint arXiv:2010.08574*, 2020.\\n\\n[9] R. E. Zezario et al., \u201cSTOI-Net: A deep learning based non-intrusive speech intelligibility assessment model,\u201d in *APSIPA ASC*. IEEE, 2020, pp. 482\u2013486.\\n\\n[10] A. M. C. Martinez, C. Spille, J. Ro\u00dfbach, B. Kollmeier, and B. T. Meyer, \u201cPrediction of speech intelligibility with DNN-based performance measures,\u201d *Computer Speech & Language*, p. 101329, 2021.\\n\\n[11] S. Graetzer, M. A. A. and Jon Barker, T. J. Cox, J. F. Culling, G. Naylor, and E. P. and Rhoddy Viveros-Mu\u00f1oz, \u201cDataset of British English speech recordings for psychoacoustics and speech processing research: The Clarity speech corpus,\u201d *Data in Brief*, vol. 41, no. 107951, Apr. 2022.\\n\\n[12] D. Schr\u00f6der and M. Vorlander, \u201cRAVEN: A real-time framework for the auralization of interactive virtual environments,\u201d in *Forum Acusticum*, Denmark: Aalborg, 2021, pp. 1541\u20131546.\\n\\n[13] F. Denk, S. M. Ernst, J. Heeren, S. D. Ewert, and B. Kollmeier, \u201cThe Oldenburg Hearing Device (OlHeaD) HRTF Database,\u201d University of Oldenburg, Tech. Rep., 2018.\\n\\n[14] Proc. ISCA Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2021), Virtual Workshop, Sep. 2021. [Online]. Available: https://claritychallenge.github.io/clarity2021-workshop/\\n\\n[15] Y. Nejime and B. C. Moore, \u201cSimulation of the effect of threshold elevation and loudness recruitment combined with reduced frequency selectivity on the intelligibility of speech in noise,\u201d *The Journal of the Acoustical Society of America*, vol. 102, no. 1, pp. 603\u2013615, 1997.\\n\\n[16] A. H. Andersen, J. M. de Haan, Z. H. Tan, and J. Jensen, \u201cRefinement and validation of the binaural short time objective intelligibility measure for spatially diverse conditions,\u201d *Speech Communication*, vol. 102, pp. 1\u201313, 2018.\\n\\n[17] I. C. Bruce, Y. Erfani, and M. S. Zilany, \u201cA phenomenological model of the synapse between the inner hair cell and auditory nerve: Implications of limited neurotransmitter release sites,\u201d *Hearing Research*, vol. 360, pp. 40\u201354, 2018.\\n\\n[18] J. M. Kates and K. H. Arehart, \u201cThe hearing-aid speech perception index (HASPI),\u201d *Speech Communication*, vol. 65, pp. 75\u201393, 2014.\\n\\n[19] N. I. Durlach, \u201cEqualization and cancellation theory,\u201d in *Foundations of Modern Auditory Theory*, J. V. Tobias, Ed., 1972, vol. 2, pp. 371\u2013463.\\n\\n[20] C. F. Hauth, S. C. Berning, B. Kollmeier, and T. Brand, \u201cModelling binaural unmasking of speech using a blind binaural processing stage,\u201d *Trends in Hearing*, vol. 24, 2020.\\n\\n[21] A. F. McKinney and B. Cauchi, \u201cNon-intrusive binaural speech intelligibility prediction from discrete latent representations,\u201d *Signal Processing Letters*, vol. to appear, 2022.\\n\\n[22] M. Huckvale and G. Hilkhuysen, \u201cELO-SPHERES intelligibility prediction model for the Clarity Prediction Challenge 2022,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[23] Z. Tu, N. Ma, and J. Barker, \u201cExploiting hidden representations from a DNN-based speech recogniser for speech intelligibility prediction in hearing-impaired listeners,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[24] \u2014\u2014, \u201cUnsupervised uncertainty measures of automatic speech recognition for non-intrusive speech intelligibility prediction,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[25] N. Kamo, K. Arai, A. Ogawa, S. Araki, T. Nakatani, K. Kinoshita, M. Delcroix, T. Ochiai, and T. Irino, \u201cConformer-based fusion of text, audio, and listener characteristics for predicting speech intelligibility of hearing aid users,\u201d in *Proceedings of the 2nd Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2022)*, Online, Jun. 2022.\\n\\n[26] R. E. Zezario, F. Chen, C.-S. Fuh, H.-M. Wang, and Y. Tsao, \u201cMbi-net: A non-intrusive multi-branched speech intelligibility prediction model for hearing aids,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[27] J. Ro\u00dfbach, R. Huber, S. Ro\u00dfges, C. F. Hauth, T. Biberger, T. Brand, B. T. Meyer, and J. Rennies, \u201cSpeech intelligibility prediction for hearing-impaired listeners with the LEAP model,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[28] S. Ro\u00dfges, J. R. C. F. Hauth, T. Biberger, B. T. Meyer, R. Huber, J. Rennies, and T. Brand, \u201cSpeech intelligibility prediction using the bBSIM-STI model - technical report contribution E019,\u201d in *Proceedings of the 2nd Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2022)*, Online, Jun. 2022.\\n\\n[29] G. Close, S. Hollands, S. Goetze, and T. Hain, \u201cNon-intrusive speech intelligibility metric prediction for hearing impaired individuals,\u201d in *Proceedings of Interspeech 2022*, Incheon, South Korea, Sep. 2022.\\n\\n[30] A. F. McKinney and B. Cauchi, \u201cNon-intrusive prediction of speech intelligibility for the first Clarity Prediction Challenge (CPC1),\u201d in *Proceedings of the 2nd Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2022)*, Online, Jun. 2022.\\n\\n[31] C. O. Mawalim, B. A. Titalim, and M. Unoki, \u201cCPC1 E031 system description,\u201d in *Proceedings of the 2nd Clarity Workshop on Machine Learning Challenges for Hearing Aids (Clarity-2022)*, Online, Jun. 2022.\"}"}
