{"id": "lennes23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] D. R. Ladd, *Intonational phonology*. Cambridge: Cambridge University Press, 1996.\\n\\n[2] J. Leather, \\\"Speaker normalization in perception of lexical tone,\\\" *Journal of Phonetics*, vol. 11, pp. 373\u2013382, 1983.\\n\\n[3] C. B. Moore and A. Jongman, \\\"Speaker normalization in the perception of Mandarin Chinese tones,\\\" *The Journal of the Acoustical Society of America*, vol. 102, pp. 1864\u20131877, 1997.\\n\\n[4] E. Couper-Kuhlen, \\\"The prosody of repetition: on quoting and mimicry,\\\" in *Prosody in Conversation*, E. Couper-Kuhlen and M. Selting, Eds. Cambridge: Cambridge University Press, 1996.\\n\\n[5] I. Mennen, \\\"Second language acquisition of pitch range in German learners of English,\\\" *Studies in Second Language Acquisition*, vol. 36, no. 2, pp. 303\u2013329, 2014. [Online]. Available: https://www.jstor.org/stable/26328942\\n\\n[6] M. Lennes, M. Stevanovic, D. Aalto, and P. Palo, \\\"Comparing pitch distributions using Praat and R,\\\" *Phonetician*, no. 111-112, pp. 35\u201353, 2015.\\n\\n[7] M. Lennes, D. Aalto, and P. Palo, \\\"Puheen perustaajuusjakaumat: Alustavia tuloksia,\\\" in *Fonetiikan p\u00e4iv\u00e4t 2008. XXV Fonetiikan p\u00e4ivill\u00e4 Tampereen yliopistossa 11.-12.1.2008 pidetyt esitelm\u00e4t*, ser. Tampere Studies in Language, Translation and Culture, Series B 3, M. O\u2019Dell and T. Nieminen, Eds. Tampere: Tampere University Press, 2009, pp. 147\u2013155. [Online]. Available: https://urn.fi/urn:isbn:978-951-44-7580-1\\n\\n[8] R. S. Moore, \\\"Comparison of children\u2019s and adults\u2019 vocal ranges and preferred tessituras in singing familiar songs,\\\" *Bulletin of the Council for Research in Music Education*, vol. 107, pp. 13\u201322, 1991. [Online]. Available: http://www.jstor.org/stable/40318417\\n\\n[9] J. T. Eichhorn, R. D. Kent, D. Austin, and H. K. Vorperian, \\\"Effects of aging on vocal fundamental frequency and vowel formants in men and women,\\\" *Journal of Voice*, vol. 32, no. 5, pp. 644.e1\u2013644.e9, 2018.\\n\\n[10] G. Saggio and G. Costantini, \\\"Worldwide healthy adult voice baseline parameters: a comprehensive review,\\\" *Journal of Voice*, vol. 36, no. 5, pp. 637\u2013649, 2022.\\n\\n[11] M. Nishio and S. Niimi, \\\"Changes in speaking fundamental frequency characteristics with aging,\\\" *Folia Phoniatr Logop*, vol. 60, pp. 120\u2013127, 2008.\\n\\n[12] L. Albuquerque, C. Oliveira, A. Teixeira, P. Sa-Couto, and D. Figueiredo, \\\"A comprehensive analysis of age and gender effects in European Portuguese oral vowels,\\\" *Journal of Voice*, vol. 37, no. 1, pp. 143.e13\u2013143.e29, 2023.\\n\\n[13] E. T. Stathopoulos, J. E. Huber, and J. E. Sussman, \\\"Changes in acoustic characteristics of the voice across the life span: Measures from individuals 4\u201393 years of age,\\\" *Journal of Speech, Language, and Hearing Research*, vol. 54, no. 4, pp. 1011\u20131021, 2011.\\n\\n[14] S. Deliyski and D. A. Xue, \\\"Effects of aging on selected acoustic voice parameters: Preliminary normative data and educational implications,\\\" *Educational Gerontology*, vol. 27, no. 2, pp. 159\u2013168, 2001.\\n\\n[15] University of Helsinki, \\\"Donate Speech Corpus, version 1.0,\\\" 2022. [Online]. Available: http://urn.fi/urn:nbn:fi:lb-2020090321\\n\\n[16] S. Amiriparian, J. Han, M. Schmitt, A. Baird, A. Mallol-Ragolta, M. Milling, M. Gerczuk, and B. Schuller, \\\"Synchronization in interpersonal speech,\\\" *Frontiers in Robotics and Artificial Intelligence*, vol. 6, no. 116, 2019.\\n\\n[17] K. Lind\u00e9n, T. Jauhiainen, M. Lennes, M. Kurimo, A. Rossi, T. Kurki, and O. Pitkanen, \\\"Donate Speech: Collecting and sharing a large-scale speech database for social sciences, humanities and artificial intelligence research and innovation,\\\" in *CLARIN: The Infrastructure for Language Resources*, A. W. Darja Fi\u0161er, Ed. Berlin: de Gruyter, 2022, pp. 481\u2013510.\\n\\n[18] P. Boersma and D. Weenink. (2022) Praat: doing phonetics by computer (Version 6.3.02). [Computer program]. Available: https://www.praat.org/. Retrieved on 29.11.2022.\\n\\n[19] P. Boersma, \\\"Accurate short-term analysis of the fundamental frequency and the harmonics-to-noise ratio of a sampled sound,\\\" *Proceedings of the Institute of Phonetic Sciences*, vol. 17, pp. 97\u2013110, 1993.\\n\\n[20] Posit Software, PBC, \\\"RStudio 2022.12.0 build 353,\\\" [Computer program], 2022, available: https://posit.co/downloads/.\"}"}
{"id": "lennes23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pitch distributions in a very large corpus of spontaneous Finnish speech\\n\\nMietta Lennes1, Minnaleena Toivola\\n\\n1University of Helsinki, Finland\\nmietta.lennes@helsinki.fi, minnaleena.toivola@helsinki.fi\\n\\nAbstract\\nSpeakers differ in the pitch range they use in their speech. In order to analyze the functional aspects of pitch, the typical pitch range of each individual is needed as reference. However, systematically collected pitch data from a sufficiently large corpus have not been previously available. We analyze the pitch distributions of individual speakers in a subset of the Donate Speech Corpus, collected from speakers of Finnish in 2020\u20132021. We report pitch analysis results based on samples from 8197 speakers and 1475 hours of speech. We compare the results obtained from male and female speakers in different age groups.\\n\\nIndex Terms\\npitch range, voice range, prosody, age-related pitch variation, Finnish\\n\\n1. Introduction\\nThe pitch range that an individual is able to comfortably produce is largely restricted by the physiological properties of the person in question. People can only control the length and modes of vibration of their vocal folds during phonation up to a certain degree, although the vocal range may be slightly extended by practicing.\\n\\nIn speech, pitch is an important psychoacoustic cue used by the speaker-listeners for chunking and organizing the acoustic speech signal and for monitoring interaction during conversation. The changes in pitch contribute to the perception of prosodic properties such as intonation, stress and accent, but they also serve paralinguistic functions [1]. In tone languages, specific pitch patterns can be used for distinguishing lexical meanings. However, pitch perception is relative: what counts as high or low pitch varies by speaker [2, 3, 4].\\n\\nIn cross-linguistic comparisons, it is common belief that there are differences in pitch variation between languages [5]. However, studies have shown large differences in the methods used to calculate pitch variation as well as in the numbers of speakers. It remains unclear whether the pitch range used in speech would tend to change for a given individual when speaking different languages. It is more likely that the differences between languages tend to occur in the actual pitch patterns used by the speakers.\\n\\n1.1. The individual pitch range as reference\\nPrevious studies of both speech and singing suggest that individual speakers and singers tend to prefer a certain vocal range. In music, the most comfortable vocal range in singing is referred to as the singer's tessitura.\\n\\nPitch can be estimated from the speech signal by applying algorithms that aim to detect the fundamental frequency (\\\\( f_0 \\\\)). During voiced sounds, the \\\\( f_0 \\\\) tends to reflect the glottal frequency in phonation. A general difficulty in pitch detection is that the researcher needs to provide the algorithm with reasonable minimum and maximum pitch values. The accuracy of the detected values tends to suffer in case the boundaries are not optimal for the speaker in question. However, it is difficult to guess the optimal values without manually inspecting the data. This has slowed down the analysis process of many researchers even though a fully accurate pitch analysis is not always required.\\n\\nIn our previous work [6], we showed that the pitch distributions of individual speakers are relatively similar in shape if the measured pitch data are transformed into semitone scale and the speaker-specific statistical pitch mode is used as the reference point. We also presented a method for estimating the typical pitch range by locating the pitch mode in the density function of the pitch data. In an earlier study of Finnish, Russian and Dutch spontaneous speech [7], following a similar approach, no evident differences were observed in the shapes of the mode-referred pitch distributions between speakers of different languages.\\n\\nAccording to a study by Moore [8], children and adults tend to favour the lower half of the vocal range that they are able to use in singing. When people are allowed to start singing alone and to choose the key of the song, they tend to select a range where the lowest note of the song is about five semitones above the lower boundary of their vocal range. The highest quarter of the vocal range is rarely used by singers.\\n\\n1.2. Pitch across the life span\\nLanguage-specific studies of the differences in pitch between women and men by age have found evidence that pitch decreases after adolescence for physiological reasons. In addition, the pitch of women's voices starts decreasing by the time of their menopause, whereas men's pitch tends to rise at older age. [9]\\n\\nIn a literature review, Saggio and Constantini [10] reported the \\\\( f_0 \\\\) means and standard deviations for homogeneous groups of women and men, aged 20\u201340 years, from twenty countries. Baseline means showed an \\\\( f_0 \\\\) ranging from 202\u2013267 Hz for females and 113\u2013154 Hz for males. The effect of age on \\\\( f_0 \\\\) has been assessed in numerous studies. The \\\\( f_0 \\\\) in women tends to decrease with age [11, 9, 12], although an increase in \\\\( f_0 \\\\) at age 80 has also been observed [13]. There is also uncertainty about age-related changes in \\\\( f_0 \\\\) in men. A slight increase in the \\\\( f_0 \\\\) of older men has been reported [11, 13, 9], whereas another study reported a decrease in \\\\( f_0 \\\\) for men over 60 [14].\\n\\nIn previous studies of the normal voice pitch of both male and female speakers, the results have been partly contradictory.\"}"}
{"id": "lennes23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"especially regarding age-related changes. The number of participants per age group has often been limited. In some studies, the pitch analysis has been performed on vowel segments in continuous speech, whereas others have measured pitch in sustained vowel productions. There is also variation in the specific methods of selecting and collecting the pitch data points and in the choice of statistical metrics used for reporting the results.\\n\\n2. Material and Methods\\n\\nFor the present study, we used a subset of the Donate Speech Corpus [15], a very large speech corpus collected during the Donate Speech campaign in Finland starting from June 2020. The corpus contains Finnish speech samples from thousands of volunteers who recorded their speech via a dedicated mobile or desktop app. The speech was elicited under different themes and individual tasks where the speakers were encouraged to talk about a video or image, or specific areas in their daily lives (pets, favourite piece of clothing, feelings under the covid-19 pandemic, etc.). Since the speech recordings mainly consist of monologues, where the speaker is talking to the app and usually alone, the corpus is well suited for studying the speakers' individual preferences. For instance, potential pitch synchronization effects that may occur between speakers [16] can be excluded.\\n\\nIn the Donate Speech app, the speakers were also requested to provide some background information about themselves, including gender, age group, native language, area of residence, dialectal background, education level, profession, etc. When designing the background questions, any potentially identifying details were avoided. The speakers were allowed to skip these questions, and they may also have provided false details. Therefore, the speaker metadata are only partial and not fully reliable. For a more detailed description of the Donate Speech campaign and the design principles of the corpus, see Lind\u00e9n et al. [17].\\n\\nThe first version of the complete Donate Speech Corpus [15] contains about 3200 hours of spontaneous speech. For further inspection, we selected the recordings in the data packages 1\u201312 from the year 2020. These packages contained a total of about 1475 hours of speech from 8390 different client ID\u2019s, which we consider as roughly corresponding to the number of individual speakers. However, several speakers may have donated their speech by using the same mobile device or computer, and thus the number should not be considered as an exact one.\\n\\nIn all the audio files and within entire recordings, pitch was analyzed at 20 ms intervals by using a script written for Praat [18], applying the default autocorrelation method for pitch detection [19] in the Praat program. During the first analysis pass, the minimum pitch parameter was set at 50 Hz and the maximum pitch at 600 Hz, the rest of the analysis parameters remaining at their default values. These settings would hardly be ideal for any individual speaker, since the algorithm would probably tend to smooth out fast pitch changes and it might also pick pitch candidates that are an octave too high. However, this method was used in order to try and \u201cbootstrap\u201d the speaker-specific modal pitch range, i.e., the span within which a majority of pitch values tend to fall for each speaker [6].\\n\\nAll the defined pitch values were gathered into data files along with the name of the original audio file. The full set of pitch data was then analysed in RStudio [20] and combined with the background details that had been provided by the speakers.\\n\\n![Figure 1](image1.png)\\n\\n**Figure 1:** The pitch distributions of female (red) and male (blue) speakers (N=60) in spoken Finnish. Pitch was detected by applying the pitch floor frequency of 50 Hz and the ceiling frequency of 600 Hz for all speakers.\\n\\n![Figure 2](image2.png)\\n\\n**Figure 2:** Distributions of pitch values as referred to the speaker-specific pitch modes of female (red) and male (blue) speakers (N=60). The same pitch floor (50 Hz) and ceiling (600 Hz) frequencies were used in pitch detection for all speakers.\"}"}
{"id": "lennes23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: The distributions of the individually calculated pitch modes of female and male speakers.\\n\\nAnd high boundaries of the modal range as the pitch floor and ceiling parameters (again, keeping the rest of the parameters at their default values). For all speakers, the pitch floor and ceiling values were set at five semitones below each individual's pitch mode and at ten semitones above the mode. The goal of the second analysis pass was to reach a higher resolution of the pitch values analyzed within the speaker's typical speaking voice range. However, for the purposes of the present study, the quality of the results was not assessed in detail, and it is possible that the principles for automatically setting the individual analysis parameters could be improved.\\n\\nDue to the original data collection method of the Donate Speech Corpus, some client ID's were supplied with some contradictory background details. Since it was impossible to ascertain whether several speakers might have donated under the same ID, we decided to exclude the problematic client ID's from further comparison. We only selected those client ID's that had provided their gender as either \u201cmale\u201d or \u201cfemale\u201d, i.e., excluding empty and contradictory answers as well as the options \u201cother\u201d or \u201cdo not wish to answer\u201d. We also excluded client ID's that were associated with multiple age groups, and client ID's with either none or contradictory information about native language. In order to ensure reliable pitch distributions, we also excluded client ID's with less than 300 pitch values. Finally, we were left with pitch data from 8197 unique client ID's, which we will refer to as \u201cspeakers\u201d.\\n\\n3. Results\\n\\nA boxplot of the individually calculated pitch modes of the female and male speakers in each age group is shown in figure 3. For further comparison, table 1 provides the medians of the individual pitch modes and means, expressed in Hertz (Hz) and semitone (ST) scales, and the medians of individual standard deviations of pitch in ST, grouped by self-reported gender and age.\\n\\nThe pitch modes of the male speakers in the age bracket of 11\u201320 years exhibit a great deal of variation, which probably reflects the fact that speakers undergo their puberty at those ages. The typical pitch of female speakers apparently does not change as radically as the pitch of males after childhood, although the figure suggests a downward trend for women until they reach their fifties or sixties. For the female speakers in this data set, the lowest pitch modes were observed in the age group of 61\u201370 years.\\n\\nThe changes with age are also broadly in line with previous studies that have found a lowering of voice pitch with age, but our results suggest more specific tendencies for typical pitch changes in certain age groups. For men over 20 years of age, the lowest typical pitches apparently occur in the 41\u201350 age group, although the differences with the previous age group or with the next one are not large. The pitch modes for male speakers exhibit a gradual increase in the elder age groups.\\n\\nThe differences between the medians of the typical pitches of male and female speakers are particularly interesting.\"}"}
{"id": "lennes23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1: Medians of the speaker-specific means, standard deviations and statistical modes of voice pitch, grouped by self-reported gender and age. The medians of the means and modes are expressed in both Hertz scale and in semitones with reference to 100 Hz.\\n\\n| Age Group | Female Mean (Hz) | Female Mode (Hz) | Male Mean (Hz) | Male Mode (Hz) | Female Mean (ST) | Female Mode (ST) | Male Mean (ST) | Male Mode (ST) | Difference (ST) |\\n|-----------|-----------------|------------------|---------------|---------------|------------------|------------------|---------------|---------------|-----------------|\\n| 1-10      | 203             | 177              | 228           | 201           | 1155             | 101              | 101           | 99            | 16              |\\n| 11-20     | 212             | 187              | 231           | 204           | 116              | 106              | 106           | 97            | 69              |\\n| 21-30     | 231             | 207              | 252           | 222           | 112              | 99               | 99            | 93            | 4                |\\n| 31-40     | 241             | 217              | 264           | 239           | 112              | 99               | 99            | 96            | 3                |\\n| 41-50     | 251             | 227              | 275           | 252           | 119              | 103              | 103           | 97            | 2                |\\n| 51-60     | 261             | 237              | 296           | 273           | 120              | 105              | 105           | 98            | 2                |\\n| 61-70     | 271             | 247              | 313           | 290           | 125              | 110              | 110           | 103           | 7                |\\n| 71-80     | 281             | 257              | 334           | 311           | 130              | 115              | 115           | 107           | 8                |\\n| 81-90     | 291             | 267              | 357           | 334           | 135              | 120              | 120           | 112           | 8                |\\n| 91-100    | 301             | 277              | 380           | 357           | 140              | 125              | 125           | 117           | 8                |\\n\\nThere are some general limitations to the pitch detection method that may result in individual erroneous values, for instance in the case of creaky voice where regular periodicity might not be evident in the speech signal. Thus, the pitch parameter can be very sensitive to extreme pitch values and there are some issues related to the metadata erroneously reported by the speakers, and it is even possible that some of the recordings be some issues related to the metadata erroneously reported by the speakers, and it is even possible that some of the recordings.\\n\\nIn the present analysis workflow may be some issues related to the metadata erroneously reported by the speakers, and it is even possible that some of the recordings. Nevertheless, we believe that the majority of the data are representative of the speakers, and it is even possible that some of the recordings. However, since the pitch mean distributions resulting from the present analysis workflow may be some issues related to the metadata erroneously reported by the speakers, and it is even possible that some of the recordings.\\n\\nIn previous research, the result needs to be confirmed. For both female and male adult speakers, these age groups are very low in this data set. The pitch means of male and female speakers in the age of 61-70 years. However, it is to be noted that the current data do provide some reference points of the pitch levels that can be very sensitive to extreme pitch values and there are some issues related to the metadata erroneously reported by the speakers, and it is even possible that some of the recordings.\\n\\nIn this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set, the typical pitch of their speech. In this data set"}
