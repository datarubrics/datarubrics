{"id": "demopoulos24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism\\n\\nCarly Demopoulos, Linnea Lampinen, Cris Preciado, Hardik Kothare, & Vikram Ramanarayanan\\n\\n1 University of California, San Francisco\\n2 Modality.AI, Inc., San Francisco\\ncarly.demopoulos@ucsf.edu\\n\\nAbstract\\n\\nImpairments in nonverbal communication are a defining feature of autism spectrum disorder (ASD) and can manifest as difficulty with, or even complete lack of, communication of emotional states via production of facial affect or vocal affect. The purpose of this study was to evaluate psychometric properties of a novel multimodal dialog based Affect Production Task (APT) in children and adolescents (ages 8-17) with a diagnosis of autism (N=72) or neurotypical controls (N=37). Participants completed activities designed to quantify objective facial and vocal affect production ability using audiovisual capture. Criterion, ecological, and discriminant validity were assessed. Psychometric performance across task conditions, age, sex, and race/ethnicity also was examined.\\n\\nResults of this initial psychometric evaluation suggest that the APT is a valid measure of affect production abilities in children and adolescents, and that psychometric performance is invariant to age, sex, or race/ethnicity.\\n\\nIndex Terms: facial affect, vocal affect, autism, assessment, psychometric properties, Affect Production Task\\n\\n1. Introduction\\n\\n1.1. Background\\n\\nLarge scale efforts have been undertaken to address the challenge of precise measurement of social communication skills in autism spectrum disorder (ASD). From these efforts, several limitations were identified in even the best of available measures (e.g., subjectivity, high training burden, inadequate coverage of symptom domains, etc.) [1], [2]. Further, these measures collapse many symptom areas into general domains, whereas, social communication is complex, involving a range of related but distinct skills. As such, overly broad measures of social communication abilities are inadequate for quantifying these skills or being sensitive to their change over time. These limitations have led to a call for development of objective measures of social communication skills [3], [4] to address the unmet assessment needs of researchers and clinicians.\\n\\nA particular gap in measurement of social communication is in the domain of nonverbal communication. While the literature on language/verbal communication impairment in autism is vast and documents the significant functional impact of deficits in these skills, an important aspect of the success in generating this large body of literature is the availability of objective, performance-based measures of verbal communication skills. In contrast, objective, standardized, performance-based measures of nonverbal communication are scarce, and typically require laborious processing burden as described in the section to follow. This underscores the need to validate measures of nonverbal communication that can be feasibly employed to sensitively measure specific nonverbal skills in individuals with a range of functional abilities and capture changes in these skills over time. Only then can we begin to understand the impact of these specific deficits and develop empirically supported treatments.\\n\\n1.2. Prior research on measurement of affect production\\n\\nPrior affect production research in ASD has largely relied on parent report [5] or rater classification of emotion via a coding system (e.g., Facial Affect Coding System (FACS) [6]\u2013[9], AFFEX [10], Maximally Discriminative Movement Coding System (MAX) [11]) using tasks designed to naturally elicit emotional responses. While parent report measures are an important component of comprehensive assessment, they are vulnerable to the limitations of subjective report, such as underrepresenting measured abilities [12] or capturing nonspecific effects [13], [14]. Likewise, paradigms that spontaneously elicit emotion have methodological limitations (i.e., individual variability in the type and degree of emotional response cannot be controlled for). Thus, these paradigms cannot isolate affect production ability from emotional response. Further, emotion classification coding systems are training and labor intensive and require establishment and maintenance of reliability between raters.\\n\\nTo address these issues, the use of digital phenotyping has been proposed to increase precision of measurement for quantifying a range of behaviors associated with autism, including nonverbal communication [4]. This digital phenotyping technology has been successfully applied to study facial affect production in children with autism via spontaneous emotional response [15], [16] and via a prompted facial affect production task of the Janssen Autism Knowledge Engine (JAKE) research assessment system, which uses FACET automated facial expression analysis software [16]. Because the JAKE assessment system was developed to assess a range of autism symptoms, its depth of assessment in each symptom domain is necessarily limited. Consequently, the JAKE affect production task is unimodal, assessing only facial affect production in isolation, whereas nonverbal communication impairment in ASD may manifest as impaired integration of verbal and nonverbal communication [17]. Likewise, studies of automated classification of vocal affect produced by individuals with ASD have been unimodal and/or have utilized an emotion elicitation design [18], [19], or have limited emotional specificity (i.e., positive, negative, neutral) [20], which is inadequate for capturing the nature of abnormal affect production in this population. In a systematic review of studies...\"}"}
{"id": "demopoulos24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"reporting automated emotion recognition systems in autism, several limitations were identified including unimodal assessment of usually facial affect only, small and largely neurotypical samples with few females, use of spontaneous emotion elicitation paradigms which confound individual differences in emotional responsivity, inadequate measures of \\\"ground truth\\\", and reliance on physiological signals for recognition of emotional states, when in fact, physiological signals carry information about emotional intensity, but not emotional classification. We have developed a novel Affect Production Task (APT) to address these limitations in prior approaches to the measurement of affect production in ASD.\\n\\nIn prior work we have demonstrated the predictive validity of the APT in distinguishing between groups of typically developing and autistic youth [22]. Here, we further explore psychometric properties, including criterion, ecological, and discriminant validity, as well as psychometric performance across age, sex, race/ethnicity, and task conditions.\\n\\n2. Methods\\n\\n2.1. Participants\\n\\nParticipants were 109 children and adolescents ages 8-17 years who were diagnosed with autism spectrum disorder (ASD; N=72) or were typically developing controls (TDC; N=37). Sex assigned at birth was evenly distributed for the TDC group (18 male, 19 female) and was more balanced than population prevalence for the ASD group (44 male, 28 female).\\n\\n2.2. Procedures\\n\\nASD diagnoses were confirmed according to DSM-5 criteria informed by use of gold standard diagnostic measures, including the Autism Diagnostic Observation Schedule-2nd Edition (ADOS-2) and the Autism Diagnostic Interview-Revised (ADI-R) [23]. Participants were administered the APT as part of a neuropsychological research battery for ongoing studies of speech and voice in autism via a multimodal dialog platform [34, 35], wherein a virtual agent engages participants in a sequence of affect production subtasks as described below.\\n\\n2.2.1 Affect Production Task\\n\\nThe APT interactive session asks the participants to produce one of four emotions (i.e., happy, sad, angry, afraid) through each of the subtasks described below. The session begins with a speaker test, background noise check and a microphone test. The participant is asked to ensure that their face is fully visible with no face coverings or shadows obscuring their face. The virtual dialogue agent then welcomes the participant to the session and prompts neutral facial and vocal expression 3 times to establish a baseline. There is an option for participants to repeat any incorrect trial (e.g., not ready, unusable response).\\n\\nAffect production is assessed in 3 task conditions. The first is the Noncontextual Monosyllabic Condition in which the participant is asked to say the word \\\"oh\\\" in a way that communicates the specified emotion (i.e., happy, sad, angry, afraid) by the way their face looks and their voice sounds. The purpose of this task is to assess the ability of the participant to produce a monosyllabic utterance that conveys a specified emotion. The monosyllabic utterance is used to minimize expressive language/speech demands. In the Contextual Monosyllabic Condition, the participant listens to a brief illustrated emotional narrative before being asked to say the word \\\"oh\\\" in a way that communicates the specified emotion by the way their face looks and their voice sounds. The purpose of this task condition is to provide context to help those who may not understand the concept of named emotions in isolation. Finally, in the Noncontextual Sentence Length Condition, the participant is asked to say the emotionally neutral sentence \\\"I'll be right back\\\" in a way that communicates the specified emotion by the way their face looks and their voice sounds.\\n\\n2.2.2 Computation of Facial and Vocal Metrics\\n\\nMediaPipe Face Detection based on BlazeFace [25] is used to determine frame-wise x and y coordinates of the face. Facial landmarks are then generated by the Google MediaPipe Face Mesh algorithm [26], 14 of which are key landmarks in the computation of facial movements (Figure 1). All facial metrics derived from these landmarks are measured in pixels and then normalized by dividing them by the inter-caruncular distance (i.e., the distance between the right eye left corner and the left eye right corner; shown in red in Figure 1) to account for cross-participant positional variability relative to the camera [27]. Individual variability in inter-caruncular distance has not been found to impact data analysis in our pilot sample, as outlier analysis has only identified outliers for individual responses (mainly due to noncompliance, distraction, etc.); no outliers were identified on the participant level. Facial metrics are defined in Table 1.\\n\\nSpeech data were collected at a sampling rate of 48kHz. Praat [28] was used to extract spectral metrics, energy metrics, and duration metrics (Table 2).\\n\\nHuman ratings of affect production ability\\n\\nTwo human raters classified each participant's responses as happy, sad, angry, afraid, or neutral, blinded to the prompted emotion for each response. Facial and vocal responses were classified separately, so that raters only had access to one modality (face or voice) when performing ratings. Human raters had intact affect recognition abilities, determined by scoring in the average range or higher on the Diagnostic Analysis of Nonverbal Accuracy-2 (DANVA-2) [24], a standardized, norm-referenced facial and vocal affect recognition task. While only four emotions were prompted (happy, sad, angry, afraid), raters had a five-response choice option (inclusion of \\\"neutral\\\") to allow them to identify flat affect/failure to convey any emotion. Raters were not instructed on how to classify affect via use of a facial affect coding system or any other operationalized set of stimuli.\\n\\nThe sentence \\\"I'll be right back\\\" was selected for its emotionally neutral semantic context in an effort to parallel the stimulus used in a standardized facial and vocal affect recognition task, the Diagnostic Analysis of Nonverbal Accuracy-2 (DANVA-2) [24]. The DANVA-2 vocal affect emotion. The monosyllabic utterance is used to minimize expressive language/speech demands.\"}"}
{"id": "demopoulos24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"imposed criteria developed based upon other task demands. Instead, we aimed to classify affective expression based on the subjective impression of the rater in order to most closely approximate the experience of one's affect being interpreted in daily life. The agreement between the human rater's classification of affect and the emotion that was prompted was used as an index of affect production ability, as this measures the degree to which the participant was able to effectively communicate the intended emotion via facial/vocal expression.\\n\\nEach human rater's percent accuracy for all responses in each condition was calculated and averaged separately for facial and vocal affect. The averages were then averaged across raters to derive affect production ability scores for each participant.\\n\\nTable 1. Glossary of the 12 objective facial metrics\\n\\n| Metric              | Construct                                |\\n|---------------------|------------------------------------------|\\n| Lip aperture        | Average lip opening                       |\\n| Lip width           | Average lip width                         |\\n| MSA                 | Average total MSA                         |\\n| MSA mean symmetry ratio | Symmetry of the mouth                 |\\n| Velocity, acceleration & jerk | Speed & rates of change in speed & acceleration |\\n| Eye opening         | Average opening of eyes                   |\\n| VDE                 | Average vertical eyebrow displacement    |\\n\\nTable 2. Glossary of the 17 objective vocal metrics\\n\\n| Metric              | Construct                                |\\n|---------------------|------------------------------------------|\\n| Shimmer [%]         | Perturbation of the amplitude domain      |\\n| SNR [dB]            | Ratio of signal power to noise power     |\\n| Intensity [dB]      | Energy of a sound over an area           |\\n| Articulation intensity [dB] | Intensity of speech in an utterance  |\\n| F1 [Hz]             | Vocal pitch, with typical ranges of values for different sexes and ages |\\n| Jitter [%]          | Frequency variation from cycle to cycle  |\\n| Formant frequencies [Hz] | Distinctive frequency components of acoustic signal produced by speech; characterizes vowel quality |\\n| F2 slope [Hz/s]     | Rate of vocal tract shape change for vowel |\\n| Speaking duration [s] | Total duration of the utterance         |\\n| Pause time [%]      | % of time utterance is paused            |\\n| Articulation duration [s] | Amount of voiced time |\\n| Timepoint [s] max F0 & min F0 | Latency of maximum and minimum pitch across the utterance |\\n| CBP prominence [dB] | Robust measure of voice quality; lower values indicate greater levels of dysphonia |\\n| Harmonics SNR [dB] | Perceived hoarseness, breathiness or roughness (lower = more hoarse) |\\n\\n2.3. Analysis of validity\\n\\n2.3.1 Criterion-related validity\\n\\nCriterion-related validity was assessed via the prediction of facial and vocal affect production ability from the objective facial and vocal metrics, respectively. The combined participant sample was used for these analyses in order to capture a range of impaired and intact affect production abilities to be compared against objective metrics. Stepwise linear regression analyses were performed for each task condition to examine the prediction of affect production ability from the objective automated metrics. All analyses were performed separately for facial and vocal affect production. Participants were excluded from these analyses if missing data exceeded 20% for objective metrics (e.g., missing data for entire task conditions). For those included, missing values were imputed with mean replacement.\\n\\n2.3.2 Ecological validity\\n\\nEcological validity was assessed via nonparametric correlation analyses between affect production ability and clinician rating of ASD symptom severity on the Childhood Autism Rating Scale-2nd Edition (CARS-2) [29], as well as parent report of facial and vocal expression on the ADI-R [30]. These analyses were performed with the ASD group only as the TDC group was not evaluated on these autism diagnostic tools.\\n\\n2.3.3 Discriminant validity\\n\\nDiscriminant validity was assessed via hierarchical linear regression analyses to quantify the relative contribution of objective metrics to the prediction of human rated affect production ability after controlling for performance on measures of facial and vocal sensorimotor control and affect recognition. Sensorimotor control of face and voice was assessed via rater accuracy for imitated facial expressions (i.e., if a rater identified an expression as happy, and the participant was imitating the actor's portrayal of happy, this indicates adequate sensorimotor imitation). Facial and vocal affect recognition were indexed by averaging the age-scaled scores for adult and child conditions of the DANVA-2 faces and paralanguage subtests, respectively [24]. Associations also were examined between affect production ability and nonsocial ASD characteristics as rated by parents on the RBS-R [31], [32], using the five-factor scoring [33]. Analyses were performed separately for face and voice for all analyses. Discriminant validity analyses were performed on the ASD sample to avoid inflation of measures of association due to group effects.\\n\\n2.4. Psychometric properties across sex, age, and race/ethnicity, and task demands\\n\\nAnalyses for validity measures were performed controlling for age and separately for participants assigned male versus female sex at birth and for participants in different racial/ethnic groups to evaluate whether APT performance was invariant to age, sex, and race/ethnicity. Repeated measures ANOVA was performed across all participants to examine within-subject effects of task condition to determine if different task demands resulted in different affect production ability scores within-individual or interacted with group membership.\\n\\n3. Results\\n\\n3.1 Criterion validity\\n\\nObjective automated facial and vocal metrics significantly predicted affect production ability. Specifically, stepwise linear regression analyses indicated that facial metrics predicted 60% of the variance in rater accuracy for both noncontextual production tasks, and 32% of the variance for the contextual production task. Objective vocal metrics predicted 41% of the variance in rater accuracy for the noncontextual monosyllabic production task, and 58% of the variance for the noncontextual sentence-length task and the contextual production task.\"}"}
{"id": "demopoulos24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significantly associated with lower affect production ability, quantified by accuracy of human rater classifications for facial (r = -0.391, p = 0.013, N = 40) and vocal production (r = -0.434, p = 0.007, N = 38). Scatterplots also show correspondence between APT performance and parent ratings of facial and vocal expression on the ADI-R.\\n\\nThe distribution of ratings shows a trend toward lower rater accuracy corresponding to higher (more severe) ADI-R rating (Figure 2).\\n\\nFigure 2: Scatterplots of Affect Production Ability and Parent Report of Facial and Vocal Expression\\n\\n3.3. Discriminant validity\\n\\nThe linear combination of raw APT metrics contributed additional, statistically significant proportions of the variance in human rater accuracy for all production tasks, after controlling for affect recognition and sensorimotor control. Specifically, APT metrics contributed an additional 33% variance for facial monosyllabic production, 27% for facial sentence-length, 42% for facial contextual monosyllabic, 57% for vocal monosyllabic production, 72% for vocal sentence-length, 30% for vocal contextual monosyllabic.\\n\\nCorrelation coefficients were low across all RBS-R scales and facial and vocal affect production ability scores (\u2264 0.203).\\n\\n3.4. Psychometric properties across sex, age, and race/ethnicity, and task demands\\n\\nAssociations between CARS-2 scores and affect production ability for facial (r = -0.438, p = 0.005) and vocal affect (r = -0.593, p < 0.001) remained strong after controlling for age. With regard to effects of sex, associations between CARS-2 and APT facial and vocal affect production were run separately for participants assigned male versus female sex at birth. These associations remained strong in the split sample, with r = -0.436 (facial affect; N = 22) and r = -0.394 (vocal affect; N = 21) for male participants. Likewise, for females, r = -0.313 for facial affect (N = 18) and r = -0.485 for vocal affect (N = 17).\\n\\nANOVA results indicate that neither facial nor vocal affect production differed significantly between racial or ethnic groups. Analyses were not sufficiently powered to examine interaction effects of diagnostic group on differential impact of race or ethnicity on task performance.\\n\\nFinally, repeated measures ANOVA indicated that, for facial affect production, there were no significant interaction effects of group and task condition; however, the ASD group performed significantly more poorly than the TDC group across all conditions. For vocal affect production, there was a significant effect of task (F(2,88) = 4.184, p = 0.018), but the interaction between group and task was not statistically significant. Post hoc contrasts indicated that the TDC group performed significantly better for the Contextual Monosyllabic Condition relative to their performance in the other conditions.\\n\\n4. Discussion and Conclusions\\n\\nThis study assessed psychometric properties of the APT, a novel automated measure of affect production. Building on prior work demonstrating predictive validity of the APT automated metrics in classifying ASD versus TDC groups [22], this study investigated the criterion, ecological, and discriminant validity of the APT, as well as the effects of age, sex, race, ethnicity and task demands on psychometric performance. Investigation of criterion validity revealed that the automated facial and vocal metrics captured during the APT predicted the effectiveness of the participant's affect production (i.e., how well the intended emotion was communicated to human raters). This successful prediction was demonstrated in the absence of any weighting or other optimization of metrics, suggesting that machine learning can be applied to develop an APT affect recognition algorithm from a large sample of APT data. This would allow for automation of human ratings, which would save time and allow for standardization and collection of normative data on APT performance.\\n\\nAssessment of ecological and discriminant validity indicated that APT performance was significantly associated with severity of overall ASD symptoms on the CARS-2, but was not associated with nonsocial ASD symptoms measured on the RBS-R. Further, scatterplots of APT performance and parent reported facial and vocal expression on the ADI-R indicated that when parents rated these as more abnormal, affect production ability scores were lowest. Further, objective metrics successfully predicted affect production ability even after controlling for sensorimotor control of face and voice and affect recognition abilities. Taken together, these findings suggest that the APT captures skills in affective communication above and beyond what can be attributed to basic sensorimotor control or emotional understanding more broadly.\\n\\nFinally, evidence of ecological validity was preserved after covarying age and across sexes. Affect production abilities did not differ across racial and ethnic groups. Likewise, for facial affect production, the TDC group outperformed the ASD group across all task conditions, but group differences did not significantly differ across tasks. In contrast, for vocal affect production there was a significant effect of task on performance; however, this was driven mainly by the TDC group showing an advantage on the contextual as opposed to noncontextual tasks, whereas the ASD group did not show the same benefit from provision of narrative context in producing affective expressions. These findings suggest that ability to incorporate situational context into affect production may explain some of the difficulty in nonverbal communication that is a defining feature of autism.\\n\\nNotably, sample size and inability to include TDC participants in analyses involving ASD diagnostic measures limit the extent of the analyses we were able to perform. Future research in large sample of individuals with and without ASD a broader age range are necessary to establish validity and reliability of the APT.\\n\\nIn summary, preliminary evaluation of the psychometric properties suggests that the standardized task structure of the APT is effective in capturing affect production abilities in children and adolescents across age, sex, race, ethnicity, and task demands. This suggests that the APT is accessible to individuals of different demographic and verbal abilities. Further, unweighted automated metrics of facial and vocal features successfully predicted affect production abilities, suggesting potential for further automation and standardization of the APT.\"}"}
{"id": "demopoulos24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n1. S. Faja et al., \u201cEvaluation of clinical assessments of social abilities for use in autism clinical trials,\u201d Journal of Speech, Language, and Hearing Research, pp. 1\u20138, 2024.\\n\\n2. K. S. L. Lam and M. G. Aman, \u201cThe repetitive behavior rating scale,\u201d Journal of Autism Developmental Disorders, vol. 31, no. 1, pp. 55\u201368, 2001.\\n\\n3. J. Bodfish and M. Lewis, \u201cRepetitive Behavior in Autism,\u201d Journal of Autism Developmental Disorders, vol. 29, no. 2, pp. 111\u2013125, 1999.\\n\\n4. J. Bodfish, F. Symons, and M. Lewis, \u201cThe Repetitive Behavior Interview,\u201d Western Carolina Center Research Reports, vol. 24, no. 5, pp. 659\u2013664, 2010.\\n\\n5. E. Schopler, M. E. Van Bourgondien, G. J. Wellman, and S. Risi, \u201cChildhood Autism Rating Scale, Second Edition (CARS 2).\u201d Western Psychological Services, Torrance, CA, 2007.\\n\\n6. J. Autism Dev. Disord. 1987, vol. 26, no. 6, pp. 836\u2013840, 1987.\\n\\n7. J. Autism Dev. Disord. 1990, vol. 22, no. 8, pp. 836.e2, 2018, doi: 10.1007/s10803-018-3756-1.\\n\\n8. J. Autism Dev. Disord. 2019, vol. 26, no. 1, pp. 1\u20132, 2007, doi: 10.1007/s10803-006-9134-1.\\n\\n9. J. Autism Dev. Disord. 2017, vol. 49, no. 1, pp. 294\u2013301, 2016, doi: 10.1002/aur.2927.\\n\\n10. J. Autism Dev. Disord. 2018, vol. 57, no. 11, pp. 828\u2013839, 2018, doi: 10.1007/s10803-018-3757-0.\\n\\n11. J. Autism Dev. Disord. 2019, vol. 57, no. 1, pp. 87\u201390, 2019, doi: 10.1007/s10803-018-3758-z.\\n\\n12. J. Autism Dev. Disord. 2019, vol. 29, no. 1, pp. 1981\u20131986, 2019, doi: 10.1007/s10803-018-3759-y.\\n\\n13. J. Autism Dev. Disord. 2019, vol. 59, no. 3, pp. 475\u2013485, 2019, doi: 10.1007/s10803-018-3760-9.\\n\\n14. J. Autism Dev. Disord. 2019, vol. 64, no. 5, pp. 474\u2013485, 2019, doi: 10.1007/s10803-018-3761-8.\\n\\n15. J. Autism Dev. Disord. 2020, vol. 100, 1990, doi: 10.1016/S0304-306X(90)90030-X.\\n\\n16. J. Autism Dev. Disord. 2021, vol. 60, 2018, doi: 10.1016/j.autres.2021.02.018.\\n\\n17. J. Autism Dev. Disord. 2022, vol. 117, no. 1, pp. 1\u20136, 2022, doi: 10.1016/j.autres.2021.117206.\\n\\n18. J. Autism Dev. Disord. 2023, vol. 11, no. 5, pp. 981\u2013996, 2023, doi: 10.1007/s10803-022-04851-5.\\n\\n19. J. Autism Dev. Disord. 2024, vol. 12, no. 4, pp. 1\u201310, 2024, doi: 10.1007/s10803-023-04852-4.\\n\\n20. J. Autism Dev. Disord. 2025, vol. 13, no. 5, pp. 1\u201310, 2025, doi: 10.1007/s10803-024-04853-3.\\n\\n21. J. Autism Dev. Disord. 2026, vol. 14, no. 6, pp. 1\u201310, 2026, doi: 10.1007/s10803-025-04854-2.\\n\\n22. J. Autism Dev. Disord. 2027, vol. 15, no. 7, pp. 1\u201310, 2027, doi: 10.1007/s10803-026-04855-1.\\n\\n23. J. Autism Dev. Disord. 2028, vol. 16, no. 8, pp. 1\u201310, 2028, doi: 10.1007/s10803-027-04856-0.\\n\\n24. J. Autism Dev. Disord. 2029, vol. 17, no. 9, pp. 1\u201310, 2029, doi: 10.1007/s10803-028-04857-9.\\n\\n25. J. Autism Dev. Disord. 2030, vol. 18, no. 10, pp. 1\u201310, 2030, doi: 10.1007/s10803-029-04858-8.\\n\\n26. J. Autism Dev. Disord. 2031, vol. 19, no. 11, pp. 1\u201310, 2031, doi: 10.1007/s10803-030-04859-7.\\n\\n27. J. Autism Dev. Disord. 2032, vol. 20, no. 12, pp. 1\u201310, 2032, doi: 10.1007/s10803-031-04860-6.\\n\\n28. J. Autism Dev. Disord. 2033, vol. 21, no. 13, pp. 1\u201310, 2033, doi: 10.1007/s10803-032-04861-5.\\n\\n29. J. Autism Dev. Disord. 2034, vol. 22, no. 14, pp. 1\u201310, 2034, doi: 10.1007/s10803-033-04862-4.\\n\\n30. J. Autism Dev. Disord. 2035, vol. 23, no. 15, pp. 1\u201310, 2035, doi: 10.1007/s10803-034-04863-3.\\n\\n31. J. Autism Dev. Disord. 2036, vol. 24, no. 16, pp. 1\u201310, 2036, doi: 10.1007/s10803-035-04864-2.\\n\\n32. J. Autism Dev. Disord. 2037, vol. 25, no. 17, pp. 1\u201310, 2037, doi: 10.1007/s10803-036-04865-1.\\n\\n33. J. Autism Dev. Disord. 2038, vol. 26, no. 18, pp. 1\u201310, 2038, doi: 10.1007/s10803-037-04866-0.\\n\\n34. J. Autism Dev. Disord. 2039, vol. 27, no. 19, pp. 1\u201310, 2039, doi: 10.1007/s10803-038-04867-9.\"}"}
