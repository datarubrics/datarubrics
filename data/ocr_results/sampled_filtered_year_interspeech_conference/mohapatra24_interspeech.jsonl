{"id": "mohapatra24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] E. Yairi and N. Ambrose, \\\"Epidemiology of stuttering: 21st century advances,\\\" Journal of fluency disorders, vol. 38, no. 2, 2013.\\n\\n[2] A. Kirkland, J. Gustafson, and E. Sz\u00e9kely, \\\"Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence,\\\" in 24th International Speech Communication Association, Interspeech 2023, Dublin, Ireland, Aug 20 2023-Aug 24 2023. International Speech Communication Association, 2023, pp. 5217\u20135221.\\n\\n[3] L. Clark, B. R. Cowan, A. Roper, S. Lindsay, and O. Sheers, \\\"Speech diversity and speech interfaces: Considering an inclusive future through stammering,\\\" in Proceedings of the 2nd Conference on Conversational User Interfaces, 2020.\\n\\n[4] P. Mohapatra, S. Preejith, and M. Sivaprakasam, \\\"A novel sensor for wrist based optical heart rate monitor,\\\" in 2017 IEEE international instrumentation and measurement technology conference (I2MTC). IEEE, 2017.\\n\\n[5] O. Shonibare, X. Tong, and V. Ravichandran, \\\"Enhancing asr for stuttered speech with limited data using detect and pass,\\\" arXiv preprint arXiv:2202.05396, 2022.\\n\\n[6] T. Kourkounakis, A. Hajavi, and A. Etemad, \\\"Detecting multiple speech disfluencies using a deep residual network with bidirectional long short-term memory,\\\" in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\n\\n[7] T. Kourkounakis and A. Hajavi et. al., \\\"Fluentnet: End-to-end detection of stuttered speech disfluencies with deep learning,\\\" IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29, 2021.\\n\\n[8] S. A. Sheikh, M. Sahidullah, F. Hirsch, and S. Ouni, \\\"Machine learning for stuttering identification: Review, challenges and future directions,\\\" Neurocomputing, 2022.\\n\\n[9] P. Howell, S. Davis, and J. Bartrip, \\\"The university college london archive of stuttered speech (uclass),\\\" 2009.\\n\\n[10] F. Rudzicz, A. K. Namasivayam, and T. Wolff, \\\"The torgo database of acoustic and articulatory speech from speakers with dysarthria,\\\" Language Resources and Evaluation, vol. 46, 2012.\\n\\n[11] N. B. Ratner and B. MacWhinney, \\\"Fluency bank: A new resource for fluency research and practice,\\\" Journal of fluency disorders, vol. 56, 2018.\\n\\n[12] I. Esmaili, N. J. Dabanloo, and M. Vali, \\\"An automatic prolongation detection approach in continuous speech with robustness against speaking rate variations,\\\" Journal of medical signals and sensors, vol. 7, no. 1, 2017.\\n\\n[13] C. Lea, V. Mitra, A. et al. Joshi, S. Kajarekar, and J. P. Bigham, \\\"Sep-28k: A dataset for stuttering event detection from podcasts with people who stutter,\\\" in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021.\\n\\n[14] A. Romana and K. Koishida, \\\"Toward a multimodal approach for disfluency detection and categorization,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\\n\\n[15] A. Romana, K. Koishida, and E. M. Provost, \\\"Automatic disfluency detection from untranscribed speech,\\\" arXiv preprint arXiv:2311.00867, 2023.\\n\\n[16] S. A. Sheikh, M. Sahidullah, F. Hirsch, and S. Ouni, \\\"Stutter-net: Stuttering detection using time delay neural network,\\\" in 2021 29th European Signal Processing Conference (EUSIPCO). IEEE, 2021.\\n\\n[17] P. Mohapatra, A. Pandey, B. Islam, and Q. Zhu, \\\"Speech disfluency detection with contextual representation and data distillation,\\\" in Proceedings of the 1st ACM International Workshop on Intelligent Acoustic Systems and Applications, 2022.\\n\\n[18] P. Mohapatra, B. Islam, M. T. Islam, R. Jiao, and Q. Zhu, \\\"Efficient stuttering event detection using siamese networks,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\\n\\n[19] L. S. Chee, O. C. Ai, M. Hariharan, and S. Yaacob, \\\"Mfcc based recognition of repetitions and prolongations in stuttered speech using k-nn and lda,\\\" in 2009 IEEE student conference on research and development (SCOReD). IEEE, 2009.\\n\\n[20] J. C. Rocholl, V. Zayats, D. D. Walker, N. B. Murad, A. Schneider, and D. J. Liebling, \\\"Disfluency detection with unlabeled data and small bert models,\\\" arXiv preprint arXiv:2104.10769, 2021.\\n\\n[21] P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, \\\"Auto-avsr: Audio-visual speech recognition with automatic labels,\\\" in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023.\\n\\n[22] M. Sadeghi, S. Leglaive, X. Alameda-Pineda, L. Girin, and R. Hohraud, \\\"Audio-visual speech enhancement using conditional variational auto-encoders,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, 2020.\\n\\n[23] D. Dadebayev, W. W. Goh, and E. X. Tan, \\\"Eeg-based emotion recognition: Review of commercial eeg devices and machine learning techniques,\\\" Journal of King Saud University-Computer and Information Sciences, vol. 34, no. 7, 2022.\\n\\n[24] X. Zhang and Y. Li, \\\"A dual attention-based modality-collaborative fusion network for emotion recognition.\\\" INTERSPEECH, 2023.\\n\\n[25] S. Ghosh, U. Tyagi, S. Kumar, M. Suri, and R. R. Shah, \\\"A novel multimodal dynamic fusion network for disfluency detection in spoken utterances,\\\" arXiv preprint arXiv:2211.14700, 2022.\\n\\n[26] B. Chen, Q. Cao, M. Hou, Z. Zhang, G. Lu, and D. Zhang, \\\"Multimodal emotion recognition with temporal and semantic consistency,\\\" IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, 2021.\\n\\n[27] M. Ma, J. Ren, L. Zhao, S. Tulyakov, C. Wu, and X. Peng, \\\"Smil: Multimodal learning with severely missing modality,\\\" in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 3, 2021.\\n\\n[28] L. Tran, X. Liu, J. Zhou, and R. Jin, \\\"Missing modalities imputation via cascaded residual autoencoder,\\\" in Proceedings of the IEEE conference on computer vision and pattern recognition, 2017.\\n\\n[29] P. Mohapatra, A. Pandey, Y. Sui, and Q. Zhu, \\\"Person identification with wearable sensing using missing feature encoding and multi-stage modality fusion,\\\" in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023.\\n\\n[30] Q. Gao, D. W. et al. and Joshua David Amason, S. Yuan, C. Tao, R. Henao, M. Hadziahmetovic, L. Carin, and M. Pajic, \\\"Gradient importance learning for incomplete observations,\\\" in International Conference on Learning Representations, 2022.\\n\\n[31] S. P. Bayerl, G. R. et al. and Shammur Absar Chowdhury, T. Ciulli, M. Danieli, K. Riedhammer, and G. Riccardi, \\\"What can Speech and Language Tell us About the Working Alliance in Psychotherapy,\\\" in Proc. Interspeech 2022, 2022.\\n\\n[32] P. Mohapatra, A. Pandey, Y. Sui, and Q. Zhu, \\\"Effect of attention and self-supervised speech embeddings on non-semantic speech tasks,\\\" in Proceedings of the 31st ACM International Conference on Multimedia, 2023.\"}"}
{"id": "mohapatra24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audio-visual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples.\\n\\nIndex Terms: speech disfluency; multimodal learning.\\n\\n1. Introduction\\n\\nSpeech disfluency, encompassing hesitations, repetitions, or prolongations, affects about 1% of the global population [1]. Disfluent speech impacts communication, interpersonal skills, social connections [2], and access to voice-assisted technologies [3]. Thus, advancing research in disfluency is critical for enhancing accessibility and inclusivity in technology [4]. Speech disfluency research particularly faces the challenge of lack of public datasets for benchmarking due to high annotation cost, the clinical nature of the task, and the usage of proprietary datasets [5, 6]. Certain studies tackle this scarcity by artificially introducing disfluencies into typical speech [7]. However, this method may oversimplify the nuances of naturally occurring disfluency and might not effectively translate to real-world contexts [8]. UCLASS [9], TORGO [10], and FluenctBank [11] are some of the disfluency-specific speech corpora collected from monologues and interviews with people who stutter ranging from children to dysarthric patients, but they lack labels for disfluencies. Past researchers have conducted private annotations [6, 12] for these databanks, adhering to an inconsistent taxonomy in the process. To mitigate these shortcomings, Lea et. al [13] have released an annotated dataset, SEP28k, and also provided labels for FluencyBank audio clips with five different classes of disfluencies. All these datasets primarily consist of unimodal audio data, with a few works exploring manual text transcriptions as an additional modality [14, 15].\\n\\nMost existing works in disfluency detection also heavily rely on audio data, employing handcrafted features [13, 16], pretrained language model embeddings [17], or custom disfluency foundation models with unlabeled atypical speech [18]. These are often paired with classifiers like statistical models [19], support vector machines [8], or deep neural networks [13, 17, 18, 16]. Some recent works incorporate manual text transcriptions [20] as an additional modality or use off-the-shelf foundation models for untranscribed audio [15, 14] to improve disfluency detection. Despite outperforming the audio-only features, audio-text multimodal studies recognize the limited expressiveness of disfluency markers through text and the inherent errors in transcribing disfluent speech.\\n\\nIn this work, we offer a novel perspective in enhancing disfluency detection by incorporating visual cues. The first challenge in developing audio-visual multimodal frameworks for disfluency detection is the lack of paired annotated audio-visual disfluency datasets. Second, due to various practical issues such as sensor malfunctions, resource limitations, environment constraints (poor lighting, obstructions, etc.), and privacy concerns, it is common to have missing modalities, particularly video, during inference. Finally, unlike mainstream speech enhancement tasks, the paradigms of audio-visual multimodal learning may not seamlessly extend to the assessment of paralinguistic tasks in atypical speech. Hence, designing multimodal learning frameworks tailored to disfluent speech, resilient to modality-missingness, is crucial to improving disfluency detection.\\n\\nOur Approach.\\n\\nTo design a novel multimodal fusion framework resilient to arbitrary missingness of video modality, we first curate a custom audio-visual dataset leveraging meta-data [13] and raw datasets [11] from past works. Our architecture features a weight-sharing encoder for both modalities, allowing it to operate on samples where one of the modalities is missing during training or inference, as shown in Figure 1. (I). We utilize a temporal decimator to project the densely sampled audio input's feature embedding from a pretrained foundation model to the same vector space as the features extracted from the full facial region of the speaker in video samples. Subsequently, both modalities share a learnable encoder, which learns the corresponding temporal context. The encoder then collapses it before incorporating the dynamic scaling factors for the modalities and feeding to a classifier head. Our empirical results show that commonly-used strategies like concatenating features in a lower dimension for fusion and using cropped lip regions as typically used for speech recognition tasks [21, 22] do not work very well in this case. Hence, we also demonstrate other fusion strategies when the availability of both modalities is always guaranteed, as shown in Figure 1. (II, III).\\n\\nIn summary, our contributions include: (1) a 3.3-hour paired and annotated audio-visual disfluency dataset with...\"}"}
{"id": "mohapatra24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: Illustration of our multimodal learning framework for speech disfluency detection. (I) Unified Modality Fusion Network resilient to missing video modalities, (II) Modality-specific early fusion, and (III) Modality-specific late fusion.\\n\\nRelated Works. Emotion understanding is another paralinguistic task where multimodal learning is conducted beyond audio modality, through visual cues, physiological signals, hand gestures, text, etc. [23, 24], utilizing extensive datasets for modality-dependent fusion [25], cross-modal interaction structures [26], and collaborative learning [24]. General multimodal learning for addressing modality-missingness applies reconstruction in latent space [27], statistical imputation [28], encoding missingness [29], or reinforcement learning-based policy design to weigh incomplete input vectors [30] dynamically. In contrast, our work introduces a unified encoder scheme featuring weight-sharing and additive fusion for a small-scale end-to-end trainable disfluency detection task.\\n\\n2. Approach\\n\\n2.1. Dataset Curation\\n\\nThe lack of multimodal disfluency detection frameworks from an audio-visual perspective is largely due to scarce public paired and annotated audio-visual datasets. We take efforts to curate an audio-visual dataset by leveraging meta-data and open-source databases from past works [13, 11] and adhere to their taxonomic recommendations. FluencyBank contains video recordings from 32 individuals with disfluent speech in the English language. Lea et. al [13] released an annotated dataset of 3-second audio segments for the FluencyBank database containing 4,144 audio clips. They recruit 3 annotators to systematically label each audio segment as one of the five disfluencies \u2013 Blocks (Bl), Word repetition (WP), Sound repetition (SnD), Interjection (Intrj), and Prolongation (Pro) or as fluent speech. One of the challenges with this annotation schema is the inter-annotator disagreement and past works have highlighted its detrimental impact on performance [17]. To address this, we conduct a distillation of the dataset by only retaining samples with a majority vote similar to our previous work [17] and constructing a corpus for FluencyBank containing 4,004 annotated audio clips. Next, we leverage the natural temporal alignment of audio and video databases from FluencyBank and segment the available videos based on the meta-data for the start and end times of a continuous recording from the audio annotations to construct a paired multimodal dataset for FluencyBank. Note that although the FluencyBank corpus originally also offered text transcriptions, past works [14] have highlighted its temporal misalignment and inaccuracies in capturing the frame-level disfluencies to a degree that can be leveraged effectively for overall disfluency categorization. Hence, we focus only on the audio-visual multimodal dataset in this work. All the audio data are sampled at 44.1kHz and the video data at 30 frames per second. The summary of the amount of data available for each type of speech disfluency and their descriptions are given in Table 1. To facilitate benchmarking for multimodal speech disfluency research, we release this curated audio-visual annotated dataset of approximately 3.3 hours along with the training and evaluation splits for reproducibility.\\n\\n2.2. Preprocessing\\n\\nAudio Embeddings. Past works [17, 31, 32] consistently highlight the effectiveness of features extracted from large-scale pretrained networks over traditional acoustic features. We leverage large-scale foundation models trained on 1000s of hours of typical speech data to extract meaningful features. In this case, we utilize the base architecture of wav2vec 2.0 [33] to support a small-scale end-to-end training with limited labeled data. The 3-second audio data is resampled to 16kHz and fed to 12 frozen layers of transformers in wav2vec 2.0. In Section 3.2 we also show that one can easily replace this extractor as suitable for their task. The audio input, $x_a \\\\in \\\\mathbb{R}^{1 \\\\times T_a}$ where $T_a$ is the temporal length of each sample, is transformed to $w_a \\\\in \\\\mathbb{R}^{C_a \\\\times F_a \\\\times T_a}$. Audio signal is sampled at a much higher rate than video modality. To facilitate projection to...\"}"}
{"id": "mohapatra24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"a common latent space eventually, we implement an embedding decimator, $F_{\\\\text{decim}} : R^{c_a \\\\times f_a \\\\times t_{\\\\text{decim}}} \\\\rightarrow R^{c_a \\\\times f_a \\\\times t_{\\\\text{decim}}}$, where\\n\\n$$F_{\\\\text{decim}} = \\\\text{decimation}.$$ \\n\\nDifferent from most multimodal frameworks, we propose a pipeline of data transformation is identical to the unified fusion (Figure 1. (III)).\\n\\nTwo dedicated encoders are employed, where $G_a$ and $G_v$ are the audio and video encoders respectively. However, instead of a shared encoder, it contains two separate encoders, $G_a$ and $G_v$, and then feeding the result's normalized version to the feature fusion transformer encoder layer [35] realized using three input streams.\\n\\nFor both modalities, we use the temporal mean pooling after the fusion (in case of early fusion schema, the output of $G_r$ is employed for feature summarization). Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization.\\n\\nThe intuition for this design is that the relevant temporal dependencies of each modality can be learned independently.\\n\\nThe overall design is tolerant of the missingness of one of the modalities due to this shared-weights-based fusion. It can support the parallel availability of modalities during both the training and inference. As a result, we also propose two variants of multimodal learning.\\n\\nWe evaluate the effectiveness of our multimodal disfluency detection approach, including variants DA V-united [35], DA V-early [25], and DA V-late [25], against state-of-the-art multimodal and unimodal methods.\\n\\nThe output of this missingness-resilient audio-visual disfluency detection is achieved using a multi-head (16 heads in our case) attention.\\n\\nThe representative attention is given to the classifier head, $y_r \\\\in \\\\{0, 1\\\\}$, along with the fluent and disfluent speech segments, we optimize the function\\n\\n$$L = -\\\\sum_{i=1}^{m} y_i \\\\cdot \\\\log y_i + (1 - y_i) \\\\cdot \\\\log (1 - y_i),$$\\n\\nwhere $y_i$ is the one-hot form of the label, $m = \\\\frac{n}{2}$ is the size of a batch, and $n$ is the total number of frames in the video segment.\\n\\nTo extract meaningful video features from the full facial region, we leverage 3D CNNs with a ResNet-18 encoder which transforms end visual speech recognition tasks. This pipeline consists of the fluent and disfluent speech segments, we optimize the function $L = -\\\\sum_{i=1}^{m} y_i \\\\cdot \\\\log y_i + (1 - y_i) \\\\cdot \\\\log (1 - y_i)$, where $y_i$ is the one-hot form of the label, $m = \\\\frac{n}{2}$ is the size of a batch, and $n$ is the total number of frames in the video segment. To extract meaningful video features from the full facial region, we leverage 3D CNNs with a ResNet-18 encoder which transforms end visual speech recognition tasks. This pipeline consists of\\n\\nTo extract meaningful video features from the full facial region, we leverage 3D CNNs with a ResNet-18 encoder which transforms end visual speech recognition tasks. This pipeline consists of\\n\\n$F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the multi-headed attention, a temporal mean pooling is employed, where $F_a \\\\in R^{f_a \\\\times t}$ and $F_v \\\\in R^{f_v \\\\times t \\\\times v}$. After this stage a vision encoder, $P = F_{\\\\text{pool}} \\\\times F_V$, is employed for feature summarization. Following the"}
{"id": "mohapatra24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and ... used acoustic-only modality for disfluency detection.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nAuto-A VSR [21]. We leverage a lip-reading-focused audio-visual stream with a wav2vec 2.0 BASE feature extractor with 2D CNN layers, a 2D ResNet34 backbone, and develop a unified weight-sharing multimodal design that models a multimodal input space for subsequent analysis. No transcribed audio input is used for the model. Visual features are extracted from the 18% less paired ROI [34], and audio features are from the base versions of Hubert Audio-only and wav2vec2.0 DA V-unified, respectively, compared to full face at the Video-only model.\\n\\nAuto-A VSR [21]. We leverage a state-of-the-art audio-text-based visual speech classification model, Auto-A VSR [21], which presents itself distinctly through facial gestures allowing for the visual data to partially in a multimodal framework, surpassing the commonly used acoustic-only modality for disfluency detection.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\\n\\nTable 2: Comparison of three variants of our multimodal approach (DAV-early, DAV-late, DAV-unified) and four unimodal and multimodal baselines on balanced accuracy and F1-score across five disfluency tasks. The best results are highlighted in bold.\\n\\nFigure 2: We examine the impact of various audio-visual features on performance of our DAV-unified approach on varying availability of visual modality during inference.\"}"}
