{"id": "lehecka24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives\\n\\nJan Lehecka, Josef V. Psutka, Lubos Smidl, Pavel Ircing, Josef Psutka\\nDepartment of Cybernetics, University of West Bohemia Pilsen, Czech Republic\\n{jlehecka,psutka,j,smidl,ircing,psutka}@kky.zcu.cz\\n\\nAbstract\\nIn this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-language sentences. Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage. Our results suggest that monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers. We also performed the same experiments on the public CommonVoice dataset to verify our results. We are contributing to the research community by releasing our pre-trained models to the public.\\n\\nIndex Terms: speech recognition, bilingual models, trilingual models, oral history archives\\n\\n1. Introduction\\nOne of the most valuable possessions mankind has is the heritage from previous generations in the form of historical documents and recordings. This heritage preserves the memory of humanity which should not be forgotten. Since collections of historical documents and recordings can grow huge in size, one of the key challenges of our age is to preserve and curate this heritage and make it as accessible and searchable to the public and researchers as possible in order to enable advanced analyzing, studying, and learning of new lessons from our history.\\n\\nIn this paper, we focus mainly on one very important and unique oral history archive: MALACH. It is an audiovisual archive initially collected in the 1990s to preserve the authentic memories of Holocaust survivors. This archive stores vast and extremely valuable testimonies from our recent history recorded through audiovisual interviews. Today, these interviews are stored in the Visual History Archive (VHA) at the Shoah Foundation Institute (SFI) at the University of Southern California (USC)\\\\footnote{https://vha.usc.edu}, along with other interviews with witnesses to the history of the entire 20th century (more than 56k interviews). The Holocaust part of the archive contains testimonies in 32 languages of the personal memories of people who survived the World War II Holocaust. In this paper, we denote this archive and derived datasets as MALACH after the name of the first project that began in 2001 and laid the foundations for work on this unique archive.\\\\footnote{https://malach.umiacs.umd.edu}\\n\\nThe natural way how to process large oral history archives to make the content more accessible is to transcribe the speech using an Automatic Speech Recognition (ASR) system. Although ASR systems have improved rapidly in the last years \\\\cite{1, 2, 3}, automatically transcribing interviews from the MALACH is still a challenge since the interviews contain spontaneous speech full of disfluencies, emotional excitements, mixed-language sentences, and heavy accents, and are often influenced by the high age of speakers (the average age of all speakers at the time of recording was about 75 years) \\\\cite{4, 5}.\\n\\nThe large number of mixed-language sentences, mainly the German phrases present in interviews across all other languages\\\\footnote{E.g. a sentence from an English interview: \u201cI said yes, Herr Lagerf\u00a8uhrer, ich habe gehalten Hemdentashen, so he give me two more.\u201d}, together with the natural multilingualism of this archive (32 languages with many non-native speakers) motivated us to study multilingual aspects of this archive and try to answer some interesting questions: Would for example adding some German speech data into the training process of the English ASR model solve the problem of transcribing mixed-language sentences? More generally, would the bilingual pre-trained models be more suitable for this task than monolingual models? And how about trilingual models \u2013 could they be even more suitable? Ultimately, would a large-scale massively multilingual ASR model transcribe the interviews better than a set of per-language monolingual ASR models? How much will results from these approaches differ?\\n\\nTo answer these questions, in this work, we present a comparative analysis over a large set of experiments with different language combinations in both pre-training and fine-tuning of ASR systems based on Wav2Vec 2.0 models \\\\cite{1} while simplifying the problem only to 3 languages: English, German, and Czech. Since some of the MALACH datasets are not publically available, we fine-tuned, evaluated, and compared our models also on another well-known and publicly available dataset \u2013 CommonVoice \\\\cite{6} \u2013 to see if our findings are applicable also to other multilingual datasets than oral history archives.\\n\\n2. Related work\\nThe original MALACH project took place between 2001 and 2006. The WER of the ASR systems developed within the project reached 39.40% for English \\\\cite{7} and 38.57% for Czech \\\\cite{8} by the end of the project in 2006. German part was not processed with ASR at that time. Even after the project finished, the efficiency of the ASR systems has continuously improved using new approaches, so in 2011 the WER of 27.11% was achieved for Czech recordings \\\\cite{9}. New training methods based on DNN brought further improvement of WER (21.70% for English \\\\cite{10} and 19.11% for Czech \\\\cite{11}). The best WER without using end-to-end approaches reached 17.85% for English and 14.65% for Czech \\\\cite{4}. After the introduction of end-to-end Transformer-based audio models, \\\\cite{12} reported a significant improvement for the Czech dataset (WER=10.48%) and...\"}"}
{"id": "lehecka24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for English (WER=13.5%). The last state-of-the-art results we are aware of are 12.88% for English, 17.08% for German, and 8.43% for Czech reported in [5].\\n\\nAll recent works on MALACH datasets (and a majority of other ASR datasets) use either monolingual models or large massively multilingual models (pre-trained on 100+ languages). However, no attention has been paid to bilingual and trilingual ASR models in the literature so far.\\n\\n2.1. Wav2Vec 2.0 model\\n\\nAfter the introduction of the Transformer architecture [14], a new era of AI began. Not long after that, Transformer-based models also established a new paradigm in the task of automatic speech recognition by introducing Wav2Vec 2.0 [1]. It is a Transformer encoder pre-trained to reconstruct the corrupted signals. The raw audio signal input is processed by the model into a sequence of frame-level contextualized speech representations encoding individual audio frames within their context. The training of Wav2Vec ASR models typically consists of two main phases: self-supervised pre-training and supervised fine-tuning. When fine-tuning, the model is supplemented with the final Connectionist Temporal Classification (CTC) layer [15], in which the most probable sequence of text tokens (i.e., the predicted transcription) is decoded.\\n\\n3. Selected languages\\n\\nTo simplify the experiments, we chose 3 languages well-represented in both fine-tuning datasets we were working with (see Sec. 5): English, German, and Czech. They all belong to the Indo-European language family, Czech belongs to the Balto-Slavic branch while English and German are Germanic languages with higher mutual lexical similarity. We chose these languages for several reasons: (1) German is a language whose phrases intertwine throughout the MALACH archive, so it should be included; (2) English is the most represented and studied part of both datasets; and (3) Czech is well-represented in both datasets while it is an example of language from a completely different language branch. This language selection allows us to test various language combinations and see whether lexical similarity will be somehow reflected in results from multilingual models. We didn't include more languages to keep the number of possible language combinations reasonably small for experimenting and the results interpretable.\\n\\n4. Pre-trained models\\n\\nWe started from the Wav2Vec-base English model [1] pre-trained from 50k hours from Libri-light dataset [16]. We used the model as a base for English monolingual ASR models. To ensure comparable results of our experiments, we adopted the exact same pre-training setup for all other models we pre-trained from scratch, and scaled pre-training data for the other two languages to the same amount. For Czech and German, we collected 50k hours of speech from public sources, mainly from the VoxPopuli dataset [17] and a mix of self-crawled publicly available podcasts and audiobooks.\\n\\nThe scheme of pre-training our models is depicted in Fig. 1. First, we pre-trained German and Czech monolingual Wav2Vec-base models from 50k hours of speech data each. Then, we prepared a mixture of all pairs of languages to train bilingual models from 100k hours each. Finally, we combine data from all three languages to pre-train a trilingual model from 150k hours of the equally balanced trilingual dataset. We experimented with three monolingual, three bilingual, and one trilingual model. Except for the monolingual English model, we pre-trained all models from scratch using the exact same setting as used for the English model, i.e., we trained the base model for 400 thousand steps with a batch size of about 1.6 hours. We used Fairseq tool for pre-training all models. Pre-training of each model took approx. five days on a machine with eight NVIDIA A100 GPUs. We are releasing all newly pre-trained models publicly to the research community.\\n\\n4.1. Large-scale multilingual models\\n\\nTo compare our Wav2Vec models also with large-scale multilingual models, we selected Wav2Vec-XLS-R-300M [2] and Whisper [3]. Wav2Vec-XLS-R-300M is a popular model pre-trained by Meta AI on 128 languages and approximately 436 thousand hours of unlabeled speech data. We experimented with the 300M variant, which has more than $3 \\\\times$ more parameters than the Wav2Vec-base model. Whisper is another popular model trained by OpenAI on 99 languages from 680,000 hours of multilingual and multitask labeled data. It is an encoder-decoder model already fine-tuned on multilingual ASR tasks by the authors, so it can also be used as a zero-shot speech recognizer without fine-tuning. We experimented with three sizes of the Whisper model: base, small, and large.\\n\\n5. Fine-tuning datasets\\n\\nWe were experimenting with two multilingual datasets: CommonVoice and MALACH. For both datasets and all selected languages, we cleaned all transcripts by removing non-speech events and punctuation and mapping texts into lowercase. The data statistics are shown in Tab. 1.\\n\\n5https://github.com/pytorch/fairseq\\n6https://huggingface.co/fav-kky\"}"}
{"id": "lehecka24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1: Fine-tuning datasets\\n\\nWe show the number of hours, words in transcripts (in thousands), and the average length of train/dev/test segments of audio (in seconds). Note that these statistics are from balanced trilingual datasets we used in this paper; the full datasets contain more data.\\n\\n| Language Pair | English | German | Czech |\\n|---------------|---------|--------|-------|\\n|               | # hours | # words | avg-len |\\n| English       | 25.0    | 161     | 5.7    |\\n| German        | 27.2    | 157     | 6.0    |\\n| Czech         | 27.0    | 153     | 5.9    |\\n| Malach        | 85.0    | 669     | 26.8   |\\n|               | 9.2     | 73      | 24.0   |\\n|               | 4.3     | 36      | 5.3    |\\n\\n#### 5.1. CommonVoice\\n\\nThe CommonVoice is a crowdsourced dataset collected by Mozilla [6]. We used corpus version 16.0 containing 19,673 validated hours in 120 languages. English portion contains 1,718 hours, German 912, and Czech 26 hours of training data. Since a mixture of these datasets is highly unbalanced and every mixed-language fine-tuning we ran with the full datasets ended in favor of English ASR with poor performance for the other two languages, we decided to balance the dataset equally and use only a randomly selected subset with 25 hours of training data per language. With this significant reduction of English and German training data, we ensure equal importance and fair conditions for all languages during the training. The development and test splits were used completely without any change.\\n\\n#### 5.2. MALACH\\n\\nThe full MALACH archive is a monumental collection with over 100,000 hours of interviews in 32 languages. About half of the archive is in English, although most English interviews are given by non-native speakers. The annotated English part contains 375 hours of speech, the German part almost 2,000 hours, and the Czech part 130 hours. Similarly to the CommonVoice dataset, we decided to balance this dataset and use an equal amount of training data for each language. The smallest training split is in the Czech part with about 87 hours of training data, so we randomly selected a subset with 85 hours of training data per language. We used full development and test splits without any additional changes. The test part had no speaker overlaps with train or development parts in all languages.\\n\\nFor English and Czech, we used datasets released under the Linguistic Data Consortium (LDC) \u2013 English [18] and Czech [19]. We adopted the same train-dev-test splits as in [4] and segmented train and development parts using time labels from the annotations into segments not exceeding 30 s, which is a reasonable limit of input examples during training due to GPU memory limits. The test parts for these two languages were already cleaned and contained only selected shorter segments (usually covering the maximum length of a single speaker's utterance without overlaps). As we found in [12], the Czech MALACH transcripts contain a mix of formal and colloquial Czech, causing a mismatch between train and test data, so we converted all Czech training transcripts into formal Czech to close the gap.\\n\\nFor German, we adopted the same data splitting and preprocessing as in [5] with additional removing of the non-speech token `ah` from transcripts as we observed inconsistent annotations of this token. We used the full unsegmented test split without any further segmentation or filtration, so the recordings in the German test dataset are much longer and less clean than test data from other languages.\\n\\n### 6. Experiments\\n\\nWe fine-tuned all Wav2Vec models with the same setting as the base model in [1], i.e., we trained for 80 thousand steps with a batch size of about 26 minutes per step, and the learning rate warmed up over the first 8,000 steps to a maximum value of \\\\(2 \\\\times 10^{-5}\\\\), where it was held for the next 32,000 steps, and finally decayed exponentially to zero. The weights of the feature encoder were frozen for the first 10,000 steps of the fine-tuning.\\n\\nWhisper models were fine-tuned differently because we observed some overfitting tendencies. For each model size, dataset, and language, we ran 4 different fine-tunings with learning rates set to \\\\(1 \\\\times 10^{-5}\\\\), \\\\(5 \\\\times 10^{-5}\\\\), \\\\(1 \\\\times 10^{-4}\\\\), and \\\\(5 \\\\times 10^{-4}\\\\).\\n\\nWe trained all models for 10 epochs with a batch size of 32 and measured the error rate after each epoch on the development dataset. Finally, we chose a checkpoint with the lowest error rate for evaluation. We fine-tuned Wav2Vec models with Fairseq and Whisper with the Transformers library [7], both on a machine with eight NVIDIA A100 GPUs. The training took approx. 12 hours (Wav2Vec-base), 30 hours (Wav2Vec-XLS-R), resp. less than 3 hours (Whisper).\\n\\nWe compared models in terms of word error rate (WER). Since all transcripts were in lowercase and cleaned from punctuation, our fine-tuned models cannot predict punctuation or upper-cased characters, so we did not consider casing and punctuation differences with the reference as errors.\\n\\nOur results are tabulated in Tab. 2. In the first part of the table (rows 1-12), we are comparing monolingual Wav2Vec models with bilingual models in all possible language combinations, and with the trilingual model. We fine-tuned each pre-trained model on all combinations of languages that were possible. For monolingual fine-tuning (i.e. we used data from one language only during the fine-tuning), we aggregated results into a single row and separated individual fine-tuning languages by a slash in the language columns. For example, on the 8th row, we took a trilingual pre-trained model (pre-training languages were CS+EN+DE), fine-tuned 6 different ASR models, one per each language (CS/EN/DE) and each dataset (CommonVoice, MALACH), and evaluated each model on corresponding test data. On the contrary, on the 12th row, we took the same pre-trained trilingual model and fine-tuned 2 different ASR models (one per dataset) from a mixture of 3 languages in the fine-tuning data (denoted as CS+EN+DE). In other words, we denote the joining of datasets for multilingual training by \"+\" (the resulting model is one multilingual model per dataset), and a set of language-specific monolingual training runs by \"/\" resulting in one monolingual model per each language and dataset.\\n\\nIn the second part of Tab. 2 (rows 13-19), we present results using large-scale multilingual models. It is important to consider individual models' sizes and fairly compare only models of similar sizes, so we also included the column with the number of trainable parameters in the table. We fine-tuned all 4 models (Wav2Vec-XLS-R and three sizes of Whisper) on all languages and datasets. The zero-shot performance of the Whisper models is reported on rows 14, 16, and 18.\\n\\n7https://github.com/huggingface/transformers\"}"}
{"id": "lehecka24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2: WER [%] for monolingual and various multilingual ASR models evaluated on relevant test splits of two multilingual datasets: CommonVoice and MALACH.\\n\\nWe use ISO 639-1 codes to denote individual languages: CS=Czech, EN=English, and DE=German.\\n\\n| # of Pre-training | Fine-tuning | CommonVoice | MALACH |\\n|-------------------|-------------|-------------|--------|\\n| Model type        | langs       | CS/EN/DE CS/EN/DE | CS/EN/DE |\\n| Wav2Vec-base 95M  | 1           | CS/EN/DE     | 11.36  34.07 17.23 | 11.19  19.89 20.55 |\\n| (bilingual) CS+EN | 2           | CS/EN       | 15.08  37.24 - - | 13.62  21.93 - |\\n| CS+DE             | 3           | CS/DE      | 14.99  36.66 - - | 14.89  23.12 - |\\n| CS+DE             | 4           | CS/DE      | 14.97  - 21.16 13.16 | - 24.49 |\\n| EN+DE             | 5           | EN/DE      | - 36.71 20.16 - 21.07 | 23.39 |\\n| EN+DE             | 6           | - 36.14 20.73 - 22.53 | 24.70 |\\n| Wav2Vec-base 95M  | 7           | CS+EN+DE    | 16.52  38.70 21.72 14.19 21.66 24.87 |\\n| (trilingual) CS+EN | 8           | CS/EN      | 16.80  38.16 - 15.66 22.35 - |\\n| CS+DE             | 9           | CS/DE      | 16.47  - 22.11 15.49 - 26.56 |\\n| EN+DE             | 10          | - 38.09 22.30 - 22.74 26.41 |\\n| CS+EN+DE          | 11          | 16.97 38.35 22.86 16.82 24.29 28.49 |\\n| Wav2Vec-XLS-R 315M| 12          | CS/EN/DE   | 11.37 | 26.96 14.27 12.46 17.23 20.11 |\\n| Whisper-base 74M  | 13          | CS/EN/DE   | - | 70.62 25.35 28.66 58.39 25.01 34.03 |\\n| Whisper-small 244M| 14          | CS/EN/DE   | - | 38.03 17.07 15.10 35.69 20.11 26.11 |\\n| Whisper-large 1,550M| 15        | CS/EN/DE   | - | 14.23 11.67 7.29 21.65 16.97 21.59 |\\n| CS+EN+DE          | 16          | 15.14 13.59 8.95 10.19 27.10 19.59 |\\n\\n**Discussion**\\n\\nOur results suggest that adding more languages into the pre-training phase while keeping the model at the same size did not bring any improvement for either dataset. On the contrary, we observed a trend in WER increasing when adding more languages. We plotted this interesting trend in Fig. 2, where for each dataset and each language, we compared the monolingual model (row 1 in Tab. 2), the average WER scored by bilingual models (rows 2, 4, and 6) and the trilingual model fine-tuned on a single language (row 8). This suggests that even when our dataset is multilingual and contains a lot of mixed-language sentences, the best we can do (when we want to keep the model reasonably small for production) is to train the monolingual model on each language separately from scratch.\\n\\nIt is worth noting that our best results in Tab. 2 are not state-of-the-art results. We are just comparing different models under the same conditions. Significantly better results could be scored with monolingual models when using more training data and a language model in the CTC decoder [13, 5].\\n\\nAs for adding more languages into fine-tuning (assuming we already have a pre-trained multilingual model), we also did not observe any significant improvements. For the CommonVoice dataset, the results are moreover the same no matter whether we fine-tune a multilingual model or more monolingual models. For the MALACH dataset, we observed even an increase of WER after adding more languages into fine-tuning (compare e.g. rows 2 vs. 3, 4 vs. 5, 6 vs. 7, or 8-12 in Tab. 2).\\n\\nLarge-scale multilingual models could outperform monolingual Wav2Vec-base models, but only at the cost of a many times higher number of parameters, and thus higher computational complexity leading to higher price and carbon footprint for each decoded word. If we compare models with similar sizes as Wav2Vec-base (Whisper-base and -small), we can, again, observe the superiority of monolingual models. In several cases, the Whisper model tended to hallucinate after the fine-tuning, leading to higher WER than its zero-shot performance. The Whisper models scored significantly better results on English and German CommonVoice datasets, which is probably due to the presence of the full datasets in the training process.\\n\\n**8. Conclusions**\\n\\nIn this paper, we have presented a comparative analysis over a large set of experiments with different language combinations in both pre-training and fine-tuning of ASR systems based on Wav2Vec 2.0 models. We evaluated our models on two multilingual datasets and three languages. Our results suggest that monolingual Wav2vec models are, in most cases, superior to multilingual models. Only large-scale multilingual models, many times larger, can outperform monolingual Wav2Vec models, but only at the cost of much higher decoding complexity and carbon footprint for each transcribed word.\"}"}
{"id": "lehecka24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"9. Acknowledgements\\n\\nThis research was supported by the Ministry of the Interior of the Czech Republic, project No. VJ01010108 and by the Ministry of Education, Youth and Sports of the Czech Republic through the e-INFRA CZ (ID: 90254).\\n\\n10. References\\n\\n[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cWav2Vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12449\u201312460, 2020.\\n\\n[2] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino, A. Baevski, A. Conneau, and M. Auli, \u201cXLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale,\u201d in Proc. Interspeech 2022, 2022, pp. 2278\u20132282.\\n\\n[3] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28492\u201328518.\\n\\n[4] J. V. Psutka, A. Pra\u017e\u00e1k, and J. Van\u011bk, \u201cRecognition of heavily accented and emotional speech of English and Czech Holocaust survivors using various DNN architectures,\u201d in Speech and Computer, A. Karpov and R. Potapova, Eds. Cham: Springer International Publishing, 2021, pp. 553\u2013564.\\n\\n[5] J. Lehecka, J. \u0160vec, J. V. Psutka, and P. Ircing, \u201cTransformer-based Speech Recognition Models for Oral History Archives in English, German, and Czech,\u201d in Proceedings of INTERSPEECH 2023, 2023, pp. 201\u2013205.\\n\\n[6] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), 2020, pp. 4211\u20134215.\\n\\n[7] W. Byrne, D. Doermann, M. Franz, S. Gustman, J. Haji\u010d, D. Oard, M. Picheny, J. Psutka, B. Ramabhadran, D. Soergel, T. Ward, and Wei-Jing Zhu, \u201cAutomatic recognition of spontaneous speech for access to multilingual oral history archives,\u201d IEEE Transactions on Speech and Audio Processing, vol. 12, no. 4, pp. 420\u2013435, 2004.\\n\\n[8] J. Psutka, P. Ircing, J. V. Psutka, J. Haji\u010d, W. Byrne, and J. M\u00edrovsk\u00fd, \u201cAutomatic transcription of Czech, Russian and Slovak spontaneous speech in the MALACH project,\u201d in Eurospeech 2005. ISCA, 2005, pp. 1349\u20131352.\\n\\n[9] J. \u0160vec, J. Psutka, L. \u0160m\u00eddl, and J. Trmal, \u201cA relevance score estimation for spoken term detection based on RNN-generated pronunciation embeddings,\u201d in Interspeech 2017, 2017, pp. 2934\u20132938.\\n\\n[10] M. Picheny, Z. T\u00fcske, B. Kingsbury, K. Audhkhasi, X. Cui, and G. Saon, \u201cChallenging the boundaries of speech recognition: The MALACH corpus,\u201d in Interspeech 2019, 2019, pp. 326\u2013330.\\n\\n[11] J. \u0160vec, J. Psutka, L. \u0160m\u00eddl, and J. Trmal, \u201cA relevance score estimation for spoken term detection based on RNN-generated pronunciation embeddings,\u201d in Interspeech 2017, 2017, pp. 2934\u20132938.\\n\\n[12] J. Lehecka, J. V. Psutka, and J. Psutka, \u201cTransformer-based automatic speech recognition of formal and colloquial Czech in MALACH project,\u201d in Text, Speech, and Dialogue, P. Sojka, A. Hor\u00e1k, I. Kope\u010dek, and K. Pala, Eds. Cham: Springer International Publishing, 2022, pp. 301\u2013312.\\n\\n[13] M. Picheny, Q. Yang, D. Zhang, and L. Zhang, \u201cThe MALACH Corpus: Results with End-to-End Architectures and Pretraining,\u201d in Proc. INTERSPEECH 2023, 2023, pp. 5097\u20135101.\\n\\n[14] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, 2017, pp. 5998\u20136008.\\n\\n[15] A. Graves, S. Fernandez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural nets,\u201d in ICML \u201906: Proceedings of the International Conference on Machine Learning, 2006.\\n\\n[16] J. Kahn, M. Riviere, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar\u00e9, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., \u201cLibri-light: A benchmark for asr with limited or no supervision,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7669\u20137673.\\n\\n[17] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). Online: Association for Computational Linguistics, Aug. 2021, pp. 993\u20131003. [Online]. Available: https://aclanthology.org/2021.acl-long.80\\n\\n[18] B. Ramabhadran, S. Gustman, W. Byrne, J. Haji\u010d, D. Oard, J. S. Olsson, M. Picheny, and J. Psutka, \u201cUSC-SFI MALACH Interviews and Transcripts English LDC2012S05,\u201d Philadelphia: Linguistic Data Consortium, https://catalog.ldc.upenn.edu/LDC2012s05, 2012.\\n\\n[19] J. Psutka, V. Radov\u00e1, P. Ircing, J. Matou\u0161ek, and L. M\u00fcller, \u201cUSC-SFI MALACH Interviews and Transcripts Czech LDC2014S04,\u201d Philadelphia: Linguistic Data Consortium, https://catalog.ldc.upenn.edu/LDC2014S04, 2014.\"}"}
