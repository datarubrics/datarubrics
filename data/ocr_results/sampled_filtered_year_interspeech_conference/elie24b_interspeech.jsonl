{"id": "elie24b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper studies changes in articulatory configurations across genders and periods using an inversion from acoustic to articulatory parameters. From a diachronic corpus based on French media archives spanning 60 years from 1955 to 2015, automatic transcription and forced alignment allowed extracting the central frame of each vowel. More than one million frames were obtained from over a thousand speakers across gender and age categories. Their formants were used from these vocalic frames to fit the parameters of Maeda\u2019s articulatory model. Evaluations of the quality of these processes are provided. We focus here on two parameters of Maeda\u2019s model linked to total vocal tract length: the relative position of the larynx (higher for females) and the lips protrusion (more protruded for males). Implications for voice quality across genders are discussed. The effect across periods seems gender independent; thus, the assertion that females lowered their pitch with time is not supported.\"}"}
{"id": "elie24b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Methods\\n\\n2.1. Dataset\\n\\nThe study is based on a corpus of TV and radio samples obtained from the French National Institute of Audiovisual (INA). This corpus contains speech samples of 1025 speakers broadcast during four time periods: 1955-56, 1975-76, 1995-96, and 2015-16. Speakers are spread across genders and age categories (20-35, 36-50, 51-65, and over 65 y.o.) according to Table 1. An initial target of 30 speakers for each of the 32 period/gender/age categories was set; it was not met for some categories, especially in the oldest periods, for females, and for the oldest age groups, due to a particularly important under-representation of these demographic categories in the media in the 20th century (that continues today). Selected speech samples present a high acoustic quality for media archives, mostly free of background noises and music. This was made possible by applying the selection method proposed by [27], which uses the source separation algorithms proposed by [28] to evaluate the relative speech-to-noise ratio, to localize and discard the most noisy excerpts (noise being either music or other noises). A target of at least 3 minutes of speech by speaker was set. The complete corpus represents about 111 hours of speech without pauses. A complete description of the corpus construction can be found in [26, 27].\\n\\nTable 1: Distribution of the 1025 speakers in categories of Period and Gender (Female / Male) for each age group (lines).\\n\\n| Period     | 1955-56 | 1975-76 | 1995-96 | 2015-16 |\\n|------------|---------|---------|---------|---------|\\n| Gender     | F       | M       | F       | M       | F       | M       | F       | M       |\\n| 20-35      | 18      | 41      | 18      | 18      | 30      | 29      | 31      | 31      |\\n| 36-50      | 21      | 72      | 23      | 42      | 33      | 45      | 30      | 53      |\\n| 51-65      | 18      | 51      | 27      | 37      | 29      | 48      | 29      | 49      |\\n| >65        | 19      | 15      | 18      | 25      | 29      | 34      | 31      | 31      |\\n\\nTo identify vocalic segments, the corpus was automatically transcribed using Whisper [29]. The quality of the transcription was estimated on a subset of 81 segments (about 20 minutes) randomly selected from all periods, gender, and age categories. Four people have manually transcribed the segments without overlap. An evaluation of the word error rate (WER) and of the phone error rate (PER) was done (Table 2). Whisper reaches a 12.8% WER on the manually transcribed subset.\\n\\nTable 2: Results of the WER and PER (hits, substitutions, deletions, insertions), obtained on the subset after applying the text normalization. Details of the PER for vowels only are also specified.\\n\\n| Edit types | H | S | D | I |\\n|------------|---|---|---|---|\\n| Error rates (%) |   |   |   |   |\\n| Words       | 4124 | 197 | 316 | 82 | 12.8 |\\n| Phones      | 13828 | 169 | 801 | 205 | 7.9 |\\n| Oral vowels | 5186 | 89  | 386 | 79  | 9.8 |\\n\\nThe PER was obtained by running the Montreal Forced Aligner (MFA) [30] with its French dictionary and aligner on Whisper's predictions and on the ground truth, dropping to 7.9%. The 12 oral vowels of French were selected for the last PER in Table 2. 5.4% of the vowels in the evaluation subset were removed in the Whisper transcription. This shows a reduced effect of Whisper smoothing its transcripts. The vowels most frequently deleted are /\u0153/ (46.7% deletion), /\u00f8/ (44.6%), /@/ (8.6%) of the vowels in the evaluation subset were removed in the Whisper transcription. This shows a reduced effect of Whisper smoothing its transcripts. The vowels most frequently deleted are /\u0153/ (46.7% deletion), /\u00f8/ (44.6%), /@/ (8.6%). 25% of the deleted phones are derived from the French word for hesitation, \\\"euh\\\" (pronounced /\u0153/ or /\u00f8/) followed by the function words \\\"et\\\" (6.5%) and \\\"de\\\" (4.7%). This leads to 1,139,818 segmented vowels once those unvoiced or longer than 200ms were removed (they represented about 20% of the complete vowel set). The central 25ms frame of each vowel was then targeted for the acoustical analysis.\\n\\n2.2. Acoustic feature extraction\\n\\nThe first four formants (F\u2081, F\u2082, F\u2083, F\u2084) were estimated thanks to Praat [31] Burg algorithm. To estimate formants, the method proposed in [32] was applied, which consists of a systematic variation of the frequency ceiling by speaker and phone to find an optimal ceiling \u2013 that minimizes the formant variation within each category. We estimated each speaker's Vocal Tract Length (V TL) from the formant obtained on each vowel by applying the equations 1 and 4 from [33] (cf. Eqs. 1& 2); the mean of V TL estimations for all vowels of a speaker was kept as the speaker's estimated V TL.\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\phi &= 0.089 \\\\\\\\\\nF_1 &= 1 + 0.102 F_2^3 + 0.121 F_3^5 + 0.669 F_4^7 \\\\quad (1) \\\\\\\\\\nV TL &= c_4 \\\\phi \\\\quad (2)\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\(F_1, F_2, F_3, F_4\\\\) are the first four formant values, and \\\\(c = 34000\\\\) cm/s is the speed of sound. These procedures were applied to the raw recordings of the corpus, and then only the values from the central frame of each vowel were kept.\\n\\n2.3. Acoustic-to-articulatory inversion\\n\\nWe performed acoustic-to-articulatory inversion to estimate the articulatory parameters of speakers. The articulatory parameters are the seven parameters of Maeda's articulatory model [25]. The Maeda model generates midsagittal shapes of the vocal tract using seven independent articulatory parameters corresponding to the principal components that explain most of the observed variance in articulatory data. These parameters are expressed in standard deviation around their mean value. These seven parameters are associated with the position of the jaw (JAW), the anteroposterior position and height of the tongue dorsum (TD position and TD height), the vertical position of the tongue tip (TT position), the height of the lower lip (LL), the lip protrusion (LP), and the larynx height (LH), as shown in Figure 1.\\n\\nThe inversion process was based on formant values: the selected inverted articulatory parameters minimized the difference between generated and observed formant frequencies. For each vowel realization of each speaker, we solved the following\"}"}
{"id": "elie24b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"optimization problem:\\n\\n\\\\[ x = \\\\text{argmin} \\\\hat{x} \\\\left| \\\\left| f_{\\\\text{obs}} - f_{\\\\text{gen}}(\\\\hat{x}) \\\\right| \\\\right|^2 \\\\], (3)\\n\\nwhere \\\\( f_{\\\\text{obs}} \\\\) contains the first four observed formants (produced by the speakers, and estimated from the central frame of each vowel), and \\\\( f_{\\\\text{gen}}(\\\\hat{x}) \\\\) contains the first four formants computed from the midsagittal shapes of the vocal tract generated by the vector \\\\( \\\\hat{x} \\\\) of Maeda's parameters.\\n\\nThe problem of articulatory inversion is a well-known ill-posed problem: an infinity of articulatory configurations can generate the same formant frequency pattern [34]. As such, the solution of Eq. (3) is not unique. Some authors have included temporal constraints to regularize the problem, e.g., smooth formant trajectories [35,36], a solution unavailable because we are trying to invert vowels individually. Instead, we kept the solutions that minimize the dispersion in the articulatory space for realizations of the same vowel of a given speaker.\\n\\nFor that purpose, we proceeded as follows. The Maeda model was first adapted to the speaker's average \\\\( V_{TL} \\\\) (estimated as described above) by modifying the model's size correction factor such that the length of the vocal tract of the neutral configuration (i.e. the null vector) is that of the speaker. Then, for each of the \\\\( N \\\\) realizations of a vowel by a speaker, we ran 20 optimizations using the Nelder-Mead algorithm [37] initialized at random, resulting in a set of \\\\( 20N \\\\) solution vectors \\\\( x \\\\). We then computed the weighted mean solution vector \\\\( \\\\bar{x} \\\\) of these solutions, where the weights were computed as the log of the final cost function, as follows:\\n\\n\\\\[ \\\\bar{x} = \\\\frac{1}{20N} \\\\sum_{i=1}^{20N} x_i \\\\left( -\\\\log(\\\\left| \\\\left| f_i(\\\\hat{x}) - f_{\\\\text{gen}}(x_i) \\\\right| \\\\right|^2) \\\\right) \\\\] (4)\\n\\nThen, for each realization \\\\( n \\\\) of the \\\\( N \\\\) vowels, among the set of 20 solutions corresponding to \\\\( n \\\\), we chose the closest solution to the mean solution \\\\( \\\\bar{x} \\\\) in the articulatory space, namely with the lowest Euclidean distance.\\n\\nWe concentrated here only on two parameters, the larynx height \\\\( LH \\\\) and the lip protrusion \\\\( LP \\\\), because they directly affect the overall length of the vocal tract, and thus its resonances. They both are used with known effect in animal behaviors to signal size and related behavioral tendencies \u2013 for example submissive/aggressive behavior respectively linked with smile or protrusion (see the comment on the acoustic origin of smile in [13]), or the lowered larynx of deer used for body size exaggeration during the mating season [38]. These parameters, estimated from an inversion procedure to fit Maeda's model, indicate required changes at both ends of the vocal tract to fit the observed formants, and not directly the vocal tract length but only a modification of the average size.\\n\\n2.4. Statistical analysis process\\n\\nBased on these two predicted articulatory parameters, \\\\( LH \\\\) and \\\\( LP \\\\), used as dependent variables, we fit linear mixed models using R's \\\\textit{lme4} library [39], with the following fixed factors: the speakers' \\\\textit{Age} (note that the four age categories used to select the speakers are not used at the model fitting stage), the speakers' \\\\textit{Gender} (Female/Male) and the time \\\\textit{Period} (1955-56, 1975-76, 1995-96, 2015-16). The 1025 Speakers and the 12 oral \\\\textit{Vowels} were treated as nested random effects (with vowels nested within each specific speaker). All double and triple interactions between the fixed factors were included in the model. A model simplification procedure was then applied to each model, using the \\\\textit{step()} function [40]. The minimal adequate models after these simplification steps are described in Eq. 5, where \\\\( DV \\\\) is the dependent variable, either \\\\( LH \\\\) or the \\\\( LP \\\\), explained by \\\\textit{Age}, \\\\textit{Gender} (and their interaction) plus \\\\textit{Period}, and the two random factors \\\\textit{Vowel} nested in \\\\textit{Speaker}.\\n\\n\\\\[ DV \\\\sim \\\\text{Age} \\\\times \\\\text{Gender} + \\\\text{Period} + (1 | \\\\text{Speaker/Vowel}) \\\\] (5)\\n\\nTable 3 details the ANOVA table for both models. The random factors had an important role but for the \\\\textit{Speaker} factor for the larynx height.\\n\\nTable 3: Analysis of Deviance Table (Type II Wald chisquare tests) for the models based on Larynx Height (top) and Lip protrusion (bottom), reporting for each factor and the interaction their estimated \\\\( \\\\chi^2 \\\\), degree of freedom and associated probability.\\n\\n| Factor       | ChiSq | Df | Pr(\u00bfChiSq) |\\n|--------------|-------|----|------------|\\n| Age          | 4.16  | 1  | < 0.05     |\\n| Gender       | 455.59| 1  | < 0.001    |\\n| Period       | 57.06 | 3  | < 0.001    |\\n| Age:Gender   | 21.28 | 1  | < 0.001    |\\n\\n| Factor       | ChiSq | Df | Pr(\u00bfChiSq) |\\n|--------------|-------|----|------------|\\n| Age          | 0.03  | 1  | 0.852      |\\n| Gender       | 145.82| 1  | < 0.001    |\\n| Period       | 54.09 | 3  | < 0.001    |\\n| Age:Gender   | 4.43  | 1  | < 0.05     |\\n\\n3. Results\\n\\n3.1. Model of larynx height\\n\\nLet's recall the Maeda's model parameters of interest \\\\( LH \\\\) and \\\\( LP \\\\) are in arbitrary unit (result of a PCA output), and varying between -3 and +3. The model fit on the estimation of larynx height showed a significant change over age, dependent on the speaker's age (see Fig. 2, top panel): the effect is reverse for each gender, with a decreasing value for females with age (i.e., the larynx tends to be lower with older age), while \\\\( LH \\\\) rises with age for males. The main difference observed on the \\\\( LH \\\\) parameter is related to \\\\textit{Gender}, with females tending to raise their larynx (mean value for \\\\( LH = 0.37 \\\\)), while males slightly tend to lower it (\\\\( LH = -0.03 \\\\)). The mean \\\\( LH \\\\) position also changed across the time period (\\\\( LP \\\\) decreases as depicted in the bottom panel of Fig. 2), with lower predicted \\\\( LH \\\\) values from the 1955's until 2015. Importantly, this change is not dependent on gender and thus applies to both female and male speakers. The lowering of the larynx across the period is about -0.07 cm for each period, which corresponds to a lengthening of less than 0.5% on the vocal tract for 60 years \u2013 or a really small amount of change (considering that the maximal change possible with the model amount to more than 10% of the total tract length).\\n\\n3.2. Model of lip protrusion\\n\\nThe values fitted for \\\\( LP \\\\) also change according to the speaker \\\\textit{Age}, across gender (Fig. 3. But for this parameter, males tend to produce more protrusion (\\\\( LP = 0.04 \\\\)) while females tend to have more stretched lips (\\\\( LP = 0.14 \\\\)). The change with \\\\textit{Age} is also opposed across \\\\textit{Genders} and tends to neutralize gender differences. Lip protrusion also differs across periods (without link to gender), with a tendency to more protrusions in the two more recent periods (\\\\( LP \\\\approx 0 \\\\)), compared to 1955 and 1975.\"}"}
{"id": "elie24b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Effects on the LH parameter of (top) speaker Age (x-axis) and Gender (curves), and (bottom) period (x-axis).\\n\\n\\\\[ \\\\text{LP} \\\\approx -0.1 \\\\]. Here also, the size of these changes is very limited, with the model also predicting changes of less than 1% of the total vocal tract length.\\n\\nFigure 3: Effects of speaker Age (x-axis) and Gender (curves) on the LP parameter.\\n\\n4. Discussion and Conclusions\\nWe applied an acoustic inversion technique to estimate articulatory parameters from a large corpus of media speech excerpts.\\n\\nThe methods, based on the automatic processing of large quantities of spontaneous speech, proved efficient: the automatic transcription tools do a robust job of extracting most of the speech, and the presented evaluation showed that, word deletions by Whisper applies mostly to hesitations and repetitions.\\n\\nThe quality in terms of phonetic alignment is better than at the lexical level.\\n\\nRegarding the question raised for this dataset \u2013 do females in French media have vocal characteristics that have changed over this time period? \u2013 the answer is contrasted. We observed changes in terms of adjustment to the vocal tract, and the most important differences reported here are related to gender, with females tending to raise their larynx and stretch their lips, while males tend to lower their larynx and protrude their lips (note that we are talking of the predictions of an inversion model, that reflect at best modest articulatory changes, one shall not exaggerate these articulatory differences). Such gender-specific differences are reminiscent of the strategies described by Ohala in relation to the Frequency Code [13]. However, these gender differences do not change over time periods, but they change with the speakers\u2019 age: older speakers tend to show fewer gender-specific differences. Over periods, the evolution is not gender-specific but exists and goes towards lower resonances (lower larynx, more protrusion).\\n\\nSo have females lowered resonances over periods? Yes, but males also. The analysis of this corpus does not support the fact that females only have lowered their voices over time \u2013 at least for the population represented in this corpus (composed of individuals invited in media and referenced in the INA archives, so who reached some fame: almost all of them have a Wikipedia page, for example). If a change over periods occurred, it applies to both genders, and it may be interpreted as a change in the situational setting where speakers were recorded, typically with a microphone farther away from the mouth in older periods. The distance to the microphone relate to the use of a more projected voice, requesting a higher vocal effort [41], that tends to raise the fundamental frequency and the first formant [42,43]. A closer microphone and softer voices may explain the lower pitch in a more recent broadcast. This does not constitute a change in voice for a category of the population, but rather a stylistic change, as already noted for journalists by, e.g., [44]. The more important presence of older females (and males) over 60 years of age in more recent periods (a fact confirmed during the documentation work made to create this corpus) may also be part of the explanation, as female voices deepen with age. Finally, it is also possible that the pressure for a lower voice is felt by certain categories of the female population (typically those having political or economic responsibilities), and a split of the speaker across their occupation may be worth further investigation.\\n\\n5. Acknowledgements\\nThis work has been partially funded by the French National Research Agency (GEM project - ANR-19-CE38-0012).\\n\\n6. References\\n[1] U. Tawilapakul, \u201cFace threatening and speaker presuppositions: The case of feminine polite particles in Thai,\u201d Journal of Pragmatics, vol. 195, p. 69\u201390, 2022.\\n[2] L. Rebollo Couto and C. R. Lopes, Eds., As formas de tratamento em portugu\u00eas e em espanhol: varia\u00e7\u00e3o, mudan\u00e7a e fun\u00e7\u00f5es conversacionais. Niter\u00f3i, RJ: Editora da UFF, 2011.\\n[3] D. G. Childers and K. Wu, \u201cGender recognition from speech.\u201d\"}"}
{"id": "elie24b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
