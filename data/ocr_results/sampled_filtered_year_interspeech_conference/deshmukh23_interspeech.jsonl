{"id": "deshmukh23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudioCaps: Generating Captions for Audios in The Wild,\u201d in NAACL-HLT, 2019.\\n\\n[2] K. Drossos, S. Lipping, and T. Virtanen, \u201cClotho: an audio captioning dataset,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 736\u2013740.\\n\\n[3] I. Mart\u00edn-Morat\u00f3 and A. Mesaros, \u201cWhat is the ground truth? reliability of multi-annotator data for audio tagging,\u201d in 2021 29th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 76\u201380.\\n\\n[4] A. S. Koepke, A.-M. Oncescu, J. Henriques, Z. Akata, and S. Albanie, \u201cAudio retrieval with natural language queries: A benchmark study,\u201d IEEE Transactions on Multimedia, 2022.\\n\\n[5] G. Chechik, E. Ie, M. Rehn, S. Bengio, and D. Lyon, \u201cLarge-scale content-based audio retrieval from text queries,\u201d in Proceedings of the 1st ACM International Conference on Multimedia Information Retrieval, ser. MIR \u201908. New York, NY, USA: Association for Computing Machinery, 2008, p. 105\u2013112. [Online]. Available: https://doi.org/10.1145/1460096.1460115\\n\\n[6] S. Ikawa and K. Kashino, \u201cAcoustic event search with an onomatopoeic query: measuring distance between onomatopoeic words and sounds,\u201d in DCASE, 2018.\\n\\n[7] B. Elizalde, S. Zarar, and B. Raj, \u201cCross modal audio search and retrieval with joint embeddings based on text and audio,\u201d in ICASSP 2019 - 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 4095\u20134099.\\n\\n[8] B. M. Elizalde, \u201cNever-ending learning of sounds,\u201d Ph.D. dissertation, Carnegie Mellon University Pittsburgh, PA, 2020.\\n\\n[9] A.-M. Oncescu, A. S. Koepke, J. F. Henriques, Z. Akata, and S. Albanie, \u201cAudio Retrieval with Natural Language Queries,\u201d in Proc. Interspeech 2021, 2021, pp. 2411\u20132415.\\n\\n[10] H. Xie, S. Lipping, and T. Virtanen, \u201cDcase 2022 challenge task 6b: Language-based audio retrieval,\u201d arXiv preprint arXiv:2206.06108, 2022.\\n\\n[11] X. Mei, X. Liu, J. Sun, M. D. Plumbley, and W. Wang, \u201cOn metric learning for audio-text cross-modal retrieval,\u201d arXiv preprint arXiv:2203.15537, 2022.\\n\\n[12] S. Hershey, S. Chaudhuri, D. P. W. Ellis, J. F. Gemmeke, A. Jansen, C. Moore, M. Plakal, D. Platt, R. A. Saurous, B. Seybold, M. Slaney, R. Weiss, and K. Wilson, \u201cCnn architectures for large-scale audio classification,\u201d in International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017. [Online]. Available: https://arxiv.org/abs/1609.09430\\n\\n[13] Q. Kong, Y. Cao, T. Iqbal, Y. Wang, W. Wang, and M. D. Plumbley, \u201cPanns: Large-scale pretrained audio neural networks for audio pattern recognition,\u201d IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 28, p. 2880\u20132894, jan 2020. [Online]. Available: https://doi.org/10.1109/TASLP.2020.3030497\\n\\n[14] K. Chen, X. Du, B. Zhu, Z. Ma, T. Berg-Kirkpatrick, and S. Dubnov, \u201cHts-at: A hierarchical token-semantic audio transformer for sound classification and detection,\u201d in ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 646\u2013650.\\n\\n[15] Y. Gong, C.-I. Lai, Y.-A. Chung, and J. Glass, \u201cSsast: Self-supervised audio spectrogram transformer,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 10, 2022, pp. 10 699\u201310 709.\\n\\n[16] Y. Gong, Y.-A. Chung, and J. Glass, \u201cAST: Audio Spectrogram Transformer,\u201d in Proc. Interspeech 2021, 2021, pp. 571\u2013575.\\n\\n[17] B. Elizalde, S. Deshmukh, M. A. Ismail, and H. Wang, \u201cClap: Learning audio concepts from natural language supervision,\u201d arXiv preprint arXiv:2206.04769, 2022.\\n\\n[18] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[19] S. Schneider, A. Baevski, R. Collobert, and M. Auli, \u201cwav2vec: Unsupervised Pre-Training for Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 3465\u20133469. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2019-1873\\n\\n[20] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\\n\\n[21] S. Deshmukh, B. Raj, and R. Singh, \u201cImproving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks,\u201d in Proc. Interspeech 2021, 2021, pp. 596\u2013600.\\n\\n[22] \u2014\u2014, \u201cMulti-task learning for interpretable weakly labelled sound event detection,\u201d arXiv preprint arXiv:2008.07085, 2020.\\n\\n[23] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 2613\u20132617. [Online]. Available: http://dx.doi.org/10.21437/Interspeech.2019-2680\\n\\n[24] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in ICLR (Poster), 2015. [Online]. Available: http://arxiv.org/abs/1412.6980\\n\\n[25] T. L. de Gail and D. Kicinski, \u201cTake it easy: Relaxing contrastive ranking loss with CIDEr,\u201d DCASE2022 Challenge, Tech. Rep., July 2022.\\n\\n[26] . Benno Weck1, . Miguel P\u00b4erez Fern\u00b4andez1, H. Kirchhoff1, and X. Serra2, \u201cAligning audio and text embeddings for the language-based audio retrieval task of the DCASE challenge 2022,\u201d DCASE2022 Challenge, Tech. Rep., July 2022.\\n\\n[27] X. Xu, Z. Xie, M. Wu, and K. Yu, \u201cThe SJTU system for DCASE2022 challenge task 6: Audio captioning with audio-text retrieval pre-training,\u201d DCASE2022 Challenge, Tech. Rep., July 2022.\\n\\n[28] Yusong Wu1, 2, Tianyu Zhang1, 2, and Ke Chen3, \u201cText-to-audio retrieval via large-scale contrastive training,\u201d DCASE2022 Challenge, Tech. Rep., July 2022.\"}"}
{"id": "deshmukh23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Audio Retrieval with WavText5K and CLAP Training\\n\\nSoham Deshmukh, Benjamin Elizalde, Huaming Wang\\nMicrosoft\\n\\nAbstract\\nText-based audio retrieval takes a natural language query to retrieve relevant audio files in a database. Most retrieval models are trained, optimized, and evaluated on a single dataset. In this paper, we quantify the effect of adding training data using three datasets and the effect on performance by evaluating the same model on two datasets. For our study, first, we introduce a new collection of about 5000 audio-text pairs called WavText5K. We qualitatively show how WavText5K differs from audio-text datasets and quantitatively show its effectiveness for retrieval. Our results show that adding more audio-text pairs does not necessarily improve performance. Second, we compare two effective audio encoders: CNN and audio transformers. We propose an architecture that demonstrates that utilizing both encoders improves the individual model\u2019s performance. Overall, using WavText5K and the proposed encoder combination outperforms the benchmark for AudioCaps and Clotho by 6% and 23%.\\n\\n1. Introduction\\nThe audio-retrieval technology has numerous applications in search engines, anomaly detection, and audio and video editing. We will use audio-retrieval to refer to both text-audio and audio-text retrieval in this work. The early works in audio-retrieval focused on using audio tags or events [5, 6, 7, 8]. With the availability of audio captioning datasets [1, 2, 4], the audio-retrieval task was expanded to include natural language descriptions as queries [9, 10]. The difficulty of building a high recall audio-retrieval system is evident from the metrics in the first audio retrieval Challenge at IEEE DCASE 2022 Task 6B [10].\\n\\nThe machine learning approach to audio-retrieval [4, 11, 9] consists of an audio encoder and a text encoder that learns a joint multimodal space. In literature, best results [11] are obtained by training independent audio-retrieval models for each target dataset. The direction of training on larger audio-text pairs and its impact on audio-retrieval performance is not explored. Therefore, in this work, we evaluate and quantify the effect of using multiple datasets in training without optimizing for a target evaluation dataset. We show that adding more training datasets does not necessarily improve audio-retrieval performance. We hypothesize that learning alignment between acoustic information and descriptions is difficult from complex audio scenes containing the occurrence of multiple audio events and interactions, leading to a drop in performance. So we introduce a new collection of audio-text pairs called WavText5K consisting of audio recordings and descriptions focused on isolated audio events. We describe what is isolated audio events and how these are different from the existing datasets in Section 2.2. From model architecture, the type of audio-encoder has a significant impact on audio-retrieval performance. We explore and compare two main families of audio encoders: CNN [12, 13] and Transformers [14, 15]. Recent literature has shown audio transformers [13, 16, 15, 14] work well on a variety of downstream tasks. However, the transformer models can intake limited input patches and tokens. This is unfavorable for training audio-retrieval models which have to learn temporal dependencies over 20-30 seconds of audio clips.\\n\\nOur three main contributions are:\\n\u2022 We introduce a new 5000 audio-text pairs collection called WavText5K focused on isolated audio events and their descriptions.\\n\u2022 We quantify and show that adding more training data does not necessarily improve audio retrieval performance.\\n\u2022 Our proposed architecture combines two well-established audio encoders \u2013a CNN and an audio transformer\u2013 outperforming benchmark performance. Hence, answering two questions unknown a priori. 1) Would combining a CNN and a transformer be redundant, or complement each other? 2) If they complement each other, what is the performance gain?\\n\\n2. WavText5K\\nIn this section, we introduce WavText5K and describe the unique sources of audio-text pairs, how the descriptions of the audio content differ from other datasets, and a quantitative analysis of the data.\\n\\n2.1. Dataset collection\\nWavText5K is available online 1 and was sourced from two websites that have not been used in any other collection to the best of our knowledge: BigSoundBank 2 and SoundBible 3. In Table 2, we compare WavText5K against the common datasets used for audio retrieval, particularly in the first audio retrieval Challenge at IEEE DCASE 2022 [10]. AudioCaps [1] is sourced from YouTube videos, Clotho [2] is sourced from freesound.org, MACS [3] is sourced from audio recorded in European locations, and SoundDescs [4] is sourced from the BBC website. WavText5K is derived from two sound effects libraries that are royalty-free and free to download. The BigSoundBank website consists of sound effects in WAV, BFW, AIFF, MP3, OGG format with audio title and audio descriptions available. The SoundBible website consists of sound effects in WAV or MP3 with audio titles, descriptions. BigSoundBank has other metadata available like channels, conditions, sound type, bit depth etc, which we did not collect. The sampling rate of audio files varies, so we resampled all audio to 44.1 kHz. While collecting the audio, we encountered empty audio files, incorrect down-\\n\\n1 https://github.com/microsoft/WavText5K\\n2 https://bigsoundbank.com\\n3 https://soundbible.com\"}"}
{"id": "deshmukh23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1:\\nThe captions of the first three datasets come from curated processes, SoundDescs' are obtained automatically from descriptions provided with the data, and WavText5K uses free-form descriptions provided by the uploader of the audio recording.\\n\\n2.2. Description of isolated events\\n\\nSource Description\\n\\nAudioCaps Screeching and light banging with a distant crow calling.\\n\\nClotho A crow crows loudly as a person is heard imitating the sound.\\n\\nMACS a crow is screaming in the background kids are yelling then another bird is screaming.\\n\\nSoundDescs Birds - Birds, Madumbalai National Park, early morning with close-up partridge calls, warblers, crow-pheasants and house crow\\n\\nWavText5K A single crow crying in the middle of the night\\n\\nTable 2:\\nVariability of captions.\\n\\nWavText5K has audio-text pairs with natural language descriptions that focus on isolated events rather than complex acoustic content. In Table 2, we exemplify the variability in the complexity of captions across the datasets in this study. The captions in Clotho contain multiple sound events like \u201ccrow crows and a person imitating the crow\u201d and the same in MACS \u201ca crow screaming with kids yelling in the background along with another bird\u201d. SoundDescs has larger captions with multiple events in longer-duration audio recordings. In contrast to this, the WavText5K caption focuses on \u201ccrow crying\u201d and provides a description of where and when (\u201cthe middle of the night\u201d) the event is happening. We show such audio-text pairs help in improving audio-retrieval performance in Section 5.2.\\n\\nWavText5K has free-form descriptions provided by the uploader of the audio recording. The captions of Clotho or AudioCaps were designed primarily for audio captioning with curated annotation processes. SoundDescs was designed for audio retrieval benchmarks and is considerably more varied in duration and audio content than the previous two. However, the text descriptions are of mixed quality, since they are obtained automatically from descriptions provided with the original audio. MACS had a curated process to annotate descriptions but was not designed for any of these tasks. Due to the lack of captioned datasets MACS has become popular.\\n\\n2.3. Data analysis\\n\\nWe provide a breakdown of the statistics of the datasets of this study in Table 1. WavText5K consists of 4525 audios, 4348 descriptions, and 4525 audio titles. The titles can be used together with the descriptions to form captions (see Section 5.4). The number of audios in WavText5K is comparable to MACS and Clotho. AudioCaps has the largest number of audios with 55,512 and SoundDescs has 32,979. The average number of words in WavText5K descriptions (12.5 words) is comparable to other datasets. The average audio duration (20.27 seconds) is longer than datasets like AudioCaps and comparable to Clotho.\\n\\nTo exemplify some of the audio events encountered in WavText5K, we plot the most common words in Figure 1. We obtained the words from the descriptions, which are filtered to remove filler words. We can see that materials like \u201cmetal\u201d and \u201cwater\u201d are the two most common terms.\\n\\n3. Audio-retrieval with contrastive learning\\n\\nWe utilize the CLAP model [17] which jointly trains audio and text encoder to learn a common multimodal space using contrastive learning. The trained audio and text encoder are then later used to retrieve files for audio-text and text-audio retrieval. The proposed architecture is shown in Figure 2.\\n\\n3.1. CLAP Training\\n\\nLet the training data be $D = \\\\{(a_i, t_i)\\\\}_{i=1}^N$. Let $f(a)$ be the audio encoder and $g(t)$ be the text encoder which are learnable embedding functions. Here, the audio encoder $f(a)$ first converts the raw audio into a log Mel spectrogram followed by a learnable embedding function. For a batch size of $b$:\\n\\n$$x_a = \\\\{f(a_i)\\\\}_{i=1}^b; x_t = \\\\{g(t_i)\\\\}_{i=1}^b$$\\n\\nwhere $x_a \\\\in \\\\mathbb{R}^{b \\\\times v}$ are the audio representations of dimension $v$, and $x_t \\\\in \\\\mathbb{R}^{b \\\\times u}$ are the text representations of dimension $u$. The audio and text representation are brought into a common multimodal space of dimension $d$ by independent linear projection layers $l_a(a)$, $l_t(t)$. This results in:\\n\\n$$\\\\hat{x}_a = l_a(x_a); \\\\hat{x}_t = l_t(x_t)$$\\n\\nwhere $\\\\hat{x}_a \\\\in \\\\mathbb{R}^{b \\\\times d}$ and $\\\\hat{x}_t \\\\in \\\\mathbb{R}^{b \\\\times d}$. Once both audio and text embeddings are in common embedding space, we can compare their similarity as:\\n\\n$$C = \\\\tau(\\\\hat{x}_a \\\\cdot \\\\hat{x}_t^\\\\top)$$\\n\\nTo exemplify some of the audio events encountered in WavText5K, we plot the most common words in Figure 1. We obtained the words from the descriptions, which are filtered to remove filler words. We can see that materials like \u201cmetal\u201d and \u201cwater\u201d are the two most common terms.\"}"}
{"id": "deshmukh23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The model is trained on audio-text pairs using CLAP. At testing/retrieval time, the trained encoders match the audio query to descriptions (audio-text retrieval) or text query to audio files (text-audio retrieval) in the database. Where $\\\\tau$ is a temperature parameter and the similarity matrix $C$ has $b$ correct pairs in the diagonal. To learn the embedding functions and projection layers we use symmetric cross-entropy loss ($L$):\\n\\n$$L = 0.5(\\\\ell_{\\\\text{text}}(C) + \\\\ell_{\\\\text{audio}}(C))$$\\n\\n(4)\\n\\nwhere $\\\\ell_k = \\\\frac{1}{N} \\\\sum_{i=0}^{N} \\\\log \\\\text{diag}(\\\\text{softmax}(C))$ along text and audio axis respectively.\\n\\n3.2. Audio Retrieval\\n\\nAfter CLAP training, the model is used for audio-retrieval as shown in Figure 2. The audio file(s) are embedded by audio encoder $f_t$ and the descriptions by text encoder $g_t$. This is followed by independently projecting ($l_t, l_a$) the embeddings into a common multimodal space and computing the similarity matrix between the audio and text embeddings. This similarity matrix is represented as $C$ in Figure 2. For audio-text retrieval, top-$N$ descriptions are computed by picking the descriptions corresponding to the top $N$ values in similarity matrix $C$. Similarly, for text-audio retrieval, top-$N$ audios are computed by picking the audios corresponding to the top $N$ values in $C$.\\n\\n3.3. Audio and Text encoder\\n\\nThe CLAP model [17] used PANN's CNN14 [13] as the audio encoder and BERT as the text encoder. There have been recent advances in audio transformer models [16, 15, 14] which show comparable or better performance than CNN models. However, the transformer models can take limited input patches and tokens. For example, the HTSAT is trained with 10 seconds audio clips. This is unfavorable for audio-text pair training where the audio concepts and complex descriptions have temporal dependencies which evolve over 20-30 seconds of audio. On the other hand, the HTSAT (0.47 mAP) is better than CNN14 (0.38 mAP) in understanding sound events. So instead, we propose using the combination of CNN14 and HTSAT as the encoder:\\n\\n$$f(a) = \\\\text{Concat}(\\\\text{CNN14}(a), \\\\text{HTSAT}(a))$$\\n\\n(5)\\n\\nIn section 5.3, we show that the CNN14 and HTSAT complement each other and improve audio-retrieval performance. We leave the investigation of unified models like Wav2Vec [18, 19] and deep fusion for future work. For text encoder, we use RoBERTa [20] instead of BERT which is a more robust text encoder. We leave dynamic methods of combining audio embeddings like attention mechanisms [21, 22] for future work.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\nWe use Clotho [2] and AudioCaps [1] for training the baseline and their test sets for audio-retrieval model evaluation. We also use MACS [3], SoundDescs [4] and WavText5K in the training dataset for experiments in Table 3. For WavText5K, we use 4348 audio-text pairs which have both description and title.\\n\\n4.2. Experimental setups\\n\\nThe audio files are resampled to 44.1 kHz and represented by log Mel spectrogram. The log Mel spectrogram is constructed with a hop size of 320, a window size of 1024, and 64 Mel bins in the range of 50-8000 Hz. Each audio file is randomly truncated to 20 secs for CNN14 and 10 secs for HTSAT. We use SpecAugment [23] for augmenting audio files. The captions were not augmented or altered. The audio-text pairs are randomly sampled to form batches during training. The projection dimension for CLAP is set to be 1024 and the temperature $\\\\tau$ is initialised to 0.007. We use Adam Optimiser [24] with the learning rate of $10^{-4}$ which is reduced by a factor of 0.1 every 20 epochs for a total of 45 epochs. The model is trained on 8 GPUs with a batch size of 128.\\n\\n5. Results and Discussion\\n\\nIn this section, we show how WavText5K helps to improve audio-retrieval performance. We evaluate and quantify the effect of training an audio retrieval system on multiple datasets. We show how our proposed architecture combining encoders outperforms the benchmark.\\n\\n5.1. Training with multiple datasets\\n\\nOur baseline is a model trained on AudioCaps and Clotho and the test set of either AudioCaps or Clotho are used to evaluate the model's performance for audio to text and text to audio retrieval. As noted in [4], the Clotho dataset is particularly more challenging than AudioCaps due to its varied audio content distributed in 10-30 seconds audio files. This is unlike AudioCaps which is limited in temporal dependencies to 10 seconds.\\n\\nIn Table 3 we show that adding more data (audio-text pairs) to training does not necessarily improve performance. We used CNN14 as an audio encoder and kept settings constant across experiments. Adding MACS [3] (row 2 and row 6 in Table 3), adds 17k audio-text pairs and leads to a drop in both audio-text and text-audio retrieval performance. SoundDescs has 33k recordings consisting of complex acoustic scenes and detailed descriptions. The model is not able to utilize and learn from SoundDescs audio-text pairs, as evident from row 4 and row 8 in Table 3. Authors in [25, 26, 27] reported similar conclusions with self-collected audio-text pairs and did not provide numerical evidence to understand the effect in performance.\\n\\n5.2. WavText5K improves performance\\n\\nWe hypothesize that learning alignment between acoustic information and descriptions is difficult from complex audio scenes...\"}"}
{"id": "deshmukh23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Experiments of training with different datasets: AC (AudioCaps), Cl (Clotho), MACS, SD (SoundDescs), WT5K (WavText5K).\\n\\nAll experiments use the same setting for training CLAP model with CNN14 audio encoder. R is Recall and mAP is mean Average Precision.\\n\\n| Training dataset | Retr. dataset | mAP@10 | R@1 | R@5 | R@10 | R@50 |\\n|------------------|---------------|--------|-----|-----|------|------|\\n| AC               | AC            | 46.57  | 33.42| 68.00| 79.95| 96.42|\\n| Cl               | Cl            | 25.85  | 16.48| 39.58| 52.46| 82.00|\\n| AC, Cl, MACS     | AC            | 45.03  | 31.37| 66.60| 79.32| 95.19|\\n| Cl               | Cl            | 23.85  | 14.39| 35.73| 49.14| 80.03|\\n| AC, Cl, SD       | AC            | 44.42  | 30.91| 65.10| 79.10| 94.71|\\n| Cl               | Cl            | 24.74  | 15.79| 36.78| 49.93| 80.75|\\n| AC, Cl, WT5K     | AC            | 45.28  | 33.07| 67.30| 80.30| 95.74|\\n| Cl               | Cl            | 24.55  | 14.79| 37.60| 48.52| 80.10|\\n| AC, Cl, SD       | Cl            | 24.34  | 14.20| 35.23| 48.72| 79.90|\\n| Cl               | Cl            | 25.65  | 16.02| 38.68| 51.43| 82.68|\\n| AC, Cl, WT5K     | Cl            | 25.85  | 16.48| 39.58| 52.46| 82.00|\\n\\nTable 4: The CNN and Transformer audio encoder complement each other and outperform the literature benchmark [11].\\n\\n| Training dataset | Retr. dataset | mAP@10 | R@1 | R@5 | R@10 | R@50 |\\n|------------------|---------------|--------|-----|-----|------|------|\\n| Benchmark [11]   | AC            | 33.90  | 69.70| 82.60|      |      |\\n| Cl               | Cl            | 14.40  | 36.60| 49.90|      |      |\\n| AC, Cl, WT5K     | AC            | 46.33  | 34.07| 66.90| 79.81| 95.36|\\n| Cl               | Cl            | 22.62  | 14.24| 36.11| 49.29| 82.47|\\n| CNN14-HTSAT     | AC            | 49.45  | 34.69| 70.22| 82.00| 97.28|\\n| Cl               | Cl            | 27.12  | 16.75| 41.09| 54.07| 83.79|\\n\\nTable 5: Constructing the caption with the title and the description of WavText5K results in better retrieval performance.\\n\\n| Training dataset | Retr. dataset | mAP@10 | R@1 | R@5 | R@10 |\\n|------------------|---------------|--------|-----|-----|------|\\n| Desc.            | AC            | 49.54  | 34.77| 30.48| 40.16|\\n| Desc., Title     | AC            | 49.45  | 34.69| 30.81| 41.91|\\n| Desc.            | Cl            | 26.15  | 16.19| 12.77| 19.14|\\n| Desc., Title     | Cl            | 27.12  | 16.75| 13.65| 20.00|\\n\\n5.3. Audio encoder architecture\\n\\nIn Table 4, we show that CNN and the audio transformer complement each other and improve audio retrieval performance. CNN is the most common encoder in the audio retrieval literature, followed by audio transformers [28, 27]. However, they have not been combined into one architecture. Other authors have combined up to four encoders [25], but have not outperformed benchmarks like our architecture. In row 3 of Table 4, the audio retrieval model with transformer-based audio encoder HTSAT performs better than the CNN14 model on AudioCaps evaluation. However, the CNN-based audio encoder (row 6) performs better on Clotho evaluation where the recording is 20-30 seconds in length and more complex than AudioCaps. By combining CNN14 and HTSAT, the audio retrieval models' performance increases on all metrics for both the evaluation datasets. Compared with benchmark [11] on row 1, our proposed combination leads to an improvement on text-audio R@1 by 2.3% and 16.3% on AudioCaps and Clotho dataset respectively. Similarly, for audio-text R@1, we see an improvement of 6.4% and 23.5% on AudioCaps and Clotho respectively.\\n\\n5.4. Caption construction in WavText5K\\n\\nTitles in WavText5K have additional information which can be combined with the descriptions to improve audio-retrieval performance (See Table 5). Overall we suggest using the title and descriptions for training models. We performed an ablation study to understand the effect of caption construction using \\\"{title}\\\" and \\\"{description}\\\" in addition to AudioCaps and Clotho to train the model CNN14+HTSAT. On Clotho, using \\\"{title}\\\" provides about 3.5% and 4.5% improvement on T-A and A-T R@1 metrics against using only \\\"{description}\\\" as caption. On AudioCaps, we observed mixed results where performance decreases by 0.2% on T-A retrieval and increases on A-T retrieval by 4.4%.\\n\\n6. Conclusion\\n\\nWe analyzed the effect of training audio-retrieval system on multiple datasets and quantified its effect on audio-retrieval performance on two publicly available datasets. We found that adding more audio-text pairs or training datasets does not necessarily improve audio-retrieval performance. We introduced a collection of 5000 audio-text pairs called WavText5K which focuses on isolated audio events and their descriptions. We demonstrate quantitatively how adding WavText5K to training data improves audio-text and text-audio retrieval performance, unlike other datasets like SoundDescs and MACS. Our analysis of audio encoder architecture shows that CNN and Transformer models complement each other, and their combination achieves benchmark performance on both audio-retrieval evaluation datasets. Further exploration of the relation between the quality of audio-text pairs and its effect on learning audio representations can offer additional insights into making text-based retrieval systems better.\"}"}
