{"id": "lin24f_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgement\\n\\nThis work is supported by the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation) under Germany's Excellence Strategy (University Allowance, EXC 2077, University of Bremen, Germany). The research is also supported by National Natural Science Foundation of China (Grant No. 62271432); Internal Project of Shenzhen Research Institute of Big Data (Grant No. T00120220002).\\n\\nWe thank all the individuals who took part in these experiments, and extend our thanks to Xinyi Chen and Duo Ma for their valuable assistance in collecting the data.\\n\\n8. References\\n\\n[1] E. C. Cherry, \\\"Some experiments on the recognition of speech, with one and with two ears,\\\" Journal of the Acoustical Society of America, vol. 25, pp. 975\u2013979, 1953.\\n\\n[2] S. Cai, H. Zhu, T. Schultz, and H. Li, \\\"EEG-based Auditory Attention Detection in Cocktail Party Environment,\\\" APSIPA Transactions on Signal and Information Processing, vol. 12, no. 3, p. e22, 2023.\\n\\n[3] J. A. O'Sullivan, A. J. Power, N. Mesgarani, S. Rajaram, J. J. Foxe, B. G. Shinn-Cunningham, M. Slaney, S. A. Shamma, and E. C. Lalor, \\\"Attentional selection in a cocktail party environment can be decoded from single-trial EEG,\\\" Cerebral Cortex, vol. 25, no. 7, pp. 1697\u20131706, 2015.\\n\\n[4] S. Cai, P. Li, E. Su, Q. Liu, and L. Xie, \\\"A neural-inspired architecture for EEG-based auditory attention detection,\\\" IEEE Transactions on Human-Machine Systems, vol. 52, no. 4, pp. 668\u2013676, 2022.\\n\\n[5] S. Geirnaert, S. Vandecappelle, E. Alickovic, A. de Cheveign\u00e9, E. C. Lalor, B. T. Meyer, S. Miran, T. Francart, and A. Bertrand, \\\"Electroencephalography-based auditory attention decoding: Toward neurosteered hearing devices,\\\" IEEE Signal Processing Magazine, vol. 38, pp. 89\u2013102, 2020.\\n\\n[6] S. A. Fuglsang, T. Dau, and J. Hjortkj\u00e6r, \\\"Noise-robust cortical tracking of attended speech in real-world acoustic scenes,\\\" NeuroImage, vol. 156, pp. 435\u2013444, 2017.\\n\\n[7] N. Das, W. Biesmans, A. Bertrand, and T. Francart, \\\"The effect of head-related filtering and ear-specific decoding bias on auditory attention detection,\\\" Journal of Neural Engineering, vol. 13, no. 5, p. 056014, 2016.\\n\\n[8] P. Li et al., \\\"ESAA: An EEG-Speech Auditory Attention Detection Database,\\\" in 2022 25th Conference of the Oriental CO-COCOSDA International Committee for the Co-ordination and Standardisation of Speech Databases and Assessment Techniques (O-COCOSDA). IEEE, 2022.\\n\\n[9] Y. Zhang, H. Ruan, Z. Yuan, H. Du, X. Gao, and J. Lu, \\\"A learnable spatial mapping for decoding the directional focus of auditory attention using EEG,\\\" in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, Conference Proceedings, pp. 1\u20135.\\n\\n[10] S. Vandecappelle, L. Deckers, N. Das, A. H. Ansari, A. Bertrand, and T. Francart, \\\"EEG-based detection of the locus of auditory attention with convolutional neural networks,\\\" Elife, vol. 10, p. e56481, 2021.\\n\\n[11] E. Su, S. Cai, L. Xie, H. Li, and T. Schultz, \\\"STAnet: a spatiotemporal attention network for decoding auditory spatial attention from EEG,\\\" IEEE Transactions on Biomedical Engineering, vol. 69, no. 7, pp. 2233\u20132242, 2022.\\n\\n[12] R. Wang, S. Cai, and H. Li, \\\"EEG-based auditory attention detection with spatiotemporal graph and graph convolutional network,\\\" INTERSPEECH 2023, 2023.\\n\\n[13] Chinese Proficiency Test, \\\"HSK,\\\" http://www.chinesetest.cn/godownload.do, 2020, accessed: 25 June 2022.\\n\\n[14] F. Mattioli, C. Porcaro, and G. Baldassarre, \\\"A 1D CNN for high accuracy classification and transfer learning in motor imagery EEG-based brain-computer interface,\\\" Journal of Neural Engineering, vol. 18, no. 6, p. 066053, 2022.\\n\\n[15] F. Hassan, S. F. Hussain, and S. M. Qaisar, \\\"Epileptic seizure detection using a hybrid 1D CNN-machine learning approach from EEG data,\\\" Journal of Healthcare Engineering, 2022.\\n\\n[16] S. Cai, E. Su, L. Xie, and H. Li, \\\"EEG-Based Auditory Attention Detection via Frequency and Channel Neural Attention,\\\" IEEE Transactions on Human-Machine Systems, vol. PP, pp. 1\u201311, 2021.\\n\\n[17] A. Gramfort, M. Luessi, E. Larson, D. A. Engemann, D. Strohmeier, C. Brodbeck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, and M. S. Hamalainen, \\\"MEG and EEG data analysis with MNE-Python,\\\" Frontiers in Neuroscience, vol. 7, no. 267, pp. 1\u201313, 2013.\"}"}
{"id": "lin24f_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nRecent studies have demonstrated the feasibility of localizing an attended sound source from electroencephalography (EEG) signals in a cocktail party scenario. This is referred to as EEG-enabled Auditory Spatial Attention Detection (ASAD). Despite the promise, there is a lack of ASAD datasets. Most existing ASAD datasets are recorded from two speaking locations. To bridge this gap, we introduce a new Auditory Spatial Attention (ASA) dataset, featuring multiple speaking locations of sound sources. The new dataset is designed to challenge and refine deep neural network solutions in real-world applications. Furthermore, we build a channel attention convolutional neural network (CA-CNN) as a reference model for ASA, that serves as a competitive benchmark for future studies.\\n\\nIndex Terms\\nAuditory spatial attention, EEG, channel attention, cocktail party problem\\n\\n1. Introduction\\nThe \\\"cocktail party effect\\\" refers to human\u2019s ability to selectively attend to a single speaker in a multi-speaker acoustic environment, a capability significantly diminished in those with hearing impairments [1]. Conventional hearing aids often fall short in amplifying the attended sound source in cocktail party scenarios [2]. Recent advancements in neuroscience have revealed the potential to directly detect human auditory attention from EEG signals [3, 4], opening avenues for brain-controlled hearing aids [5].\\n\\nAuditory attention detection algorithms can be categorized into two main paradigms: stimulus-reconstruction and auditory spatial attention detection (ASAD). The former correlates acoustic stimulus representations with EEG signals to identify the attended speaker, while the latter, i.e. ASAD, relies solely on EEG signals to determine the location of the attended sound source. ASAD is advantageous in practice because it eliminates the need for clean audio signals and operates accurately on low latency settings [2]. These characteristics make ASAD particularly suitable for real-time attention detection systems, especially in complex acoustic scenarios where hearing aid users shift their focus between different locations. Therefore, our study focuses on EEG-enabled ASAD tasks.\\n\\nDespite recent advancements, there are limited publicly available EEG databases specifically designed for research on selective auditory attention. Fuglsang et al. [6] introduced the DTU database, which consists of 64-channel EEG signals recorded from 18 subjects with normal hearing. The participants focused on one of two competing speakers in each trial, with stimuli presented at 60\u00b0 to the left and right of the subjects. Each trial follows multiple-choice questions and the answers indicate whether the subjects were compliant in the auditory attention task [6]. Independently, KUL database [7] and ESAA database [8] were introduced, involving stimuli presented from \u00b190\u00b0 using dichotic or head-related transfer function filtering (HRTF) techniques. It is worth noting that the above three widely used datasets are recorded from two speaking locations of sound sources. Recently, the NJU dataset was released [9]. It has a setup of 14 loudspeakers in an array, presenting a more realistic scenario. 32-channel EEG data were recorded from 21 subjects of normal hearing. However, the subjects listened to each trial twice, one to the left and another to the right, which could have an adverse impact on the attention levels [3]. Therefore, we propose a new Auditory Spatial Attention (ASA) dataset that contains 10 competing speaking locations, i.e., \u00b190\u00b0, \u00b160\u00b0, \u00b145\u00b0, \u00b130\u00b0, and \u00b15\u00b0. Moreover, behavioral indices, such as answers to multiple-choice questions and self-rated attention scores, are incorporated into the dataset, that allows for the study of the correlation between the level of attention and the resulting EEG signals.\\n\\nGiven the nonlinear characteristics and low signal-to-noise ratio of EEG signals, deep neural networks (DNNs) have been employed for the ASAD task to uncover hidden features. Among them, the convolutional neural network (CNN) stands out as a well-adopted method for EEG-enabled ASAD, which is referred to as \\\"CNN-KUL\\\" hereafter. In [10], a basic single-layer 2D CNN showed an average accuracy of around 81% with short decision windows (1-2 seconds) in the KUL dataset. Furthermore, a spatial-temporal attention network (STAnet) [11] was proposed and achieved remarkable results within 1-second decision windows (90.1% accuracy in the KUL dataset). More recently, graph convolution networks (GCNs) have surfaced as an alternative to CNNs for ASAD tasks [12]. However, one should note that these models are designed for subject-dependent conditions, while subject-independent solutions are more desirable.\\n\\nBuilding on the success of the previous studies, we introduce a novel channel attention convolutional neural network (CA-CNN) tailored for EEG-enabled ASAD tasks. CA-CNN serves as the reference model for the proposed ASA dataset, establishing a benchmark for future studies by the ASA user community. The CA-CNN model exhibits exceptional performance and robustness under subject-dependent conditions over the prior approaches.\\n\\nIn the rest of this paper, we first describe the design of the ASA dataset. We then present the performance evaluation of the proposed CA-CNN model on the ASA dataset and discuss...\"}"}
{"id": "lin24f_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Description of the ASA Dataset\\n\\n2.1. Participants\\nTwenty participants (9 male and 11 female) with normal auditory function were recruited from the university in this study. The male participants displayed a mean height of 175.6 \u00b1 6.31 cm and a mean weight of 67.3 \u00b1 12.73 kg, while the female participants exhibited a mean height of 163.5 \u00b1 4.29 cm and a mean weight of 54.0 \u00b1 6.99 kg. Prior to their participation, all volunteers provided written informed consent following ethical guidelines. Approval of all experimental procedures was granted by the ethics committee of the Chinese University of Hong Kong (Shenzhen).\\n\\n2.2. Auditory stimuli\\nIn this study, all participants, whose native language is Mandarin, were exposed to 40 short Chinese stories as auditory stimuli. Each auditory stimulus ranges from approximately one to one and a half minutes, narrated by a male or a female professional speaker [13]. In line with previous ASAD experimental prototypes [3, 6, 7, 8], each story was accompanied by three multiple-choice questions, with each question offering four possible answers to assess comprehension.\\n\\nDuring each trial, two different auditory streams were played in different spatial directions of the listening subject using HRTF filtering. Participants were instructed to concentrate on a single Chinese story presented among the two competing stimuli. The auditory stimuli were delivered at a sample rate of 44.1 kHz.\\n\\n2.3. Data acquisition\\nParticipants in the study were seated in a soundproof chamber and instructed to wear wired headphones for the auditory stimuli presentation. EEG signals were recorded using the BrainAmp system at a sampling rate of 500 Hz. Specifically, a 64-channel Ag/AgCl electrode cap developed by Easycap was used, adhering to the electrode placement standards of the international 10/20 system. To ensure synchronization between the auditory stimuli and EEG responses, a Python-scripted automated playback and EEG-marking system was implemented.\\n\\n2.4. ASAD task design\\nAs outlined in Table 1, each participant engaged in a total of 20 trials. The trials were designed to systematically vary the spatial orientation of two auditory stimuli to each participant. Specifically, the orientation angles between the stimuli were adjusted sequentially every four trials, following a predetermined sequence: \u00b190\u00b0, \u00b160\u00b0, \u00b145\u00b0, \u00b130\u00b0, and \u00b15\u00b0. Here, a negative angle (\"-\") indicates leftward orientation, while a positive angle (\"+\") denotes rightward orientation. For instance, an orientation of -90\u00b0 represents an extreme leftward position.\\n\\nParticipants were instructed to focus on the auditory stimulus emanating from either the left or right side in each trial. The localization of the auditory stimulus was balanced over subjects for the attended side throughout the experiment. Additionally, the narratives delivered by male and female speakers varied across trials, maintaining an equal gender distribution among speakers.\\n\\nDuring each trial, participants were instructed to keep their gaze fixed on the centrally positioned crosshair, minimizing eye blinking frequency. Each trial lasted approximately one to one and a half minutes, resulting in an EEG data collection time of around 24 minutes per participant. Consequently, the cumulative EEG data gathered from all 20 participants amounted to approximately 480 minutes or 8 hours.\\n\\n2.5. Behavioral indices\\nFollowing each trial, participants were required to answer questions related to the presented stimuli. Additionally, they were required to self-assess their attention levels, using a scale ranging from -2 to 2. These assessments not only served to maintain participants' engagement and motivation toward concentrating on the task but also offered insights into evaluating their attention status. Initially, our dataset incorporated data from 24 participants. However, data from three subjects who didn't follow the instructions were discarded. Additionally, one subject was excluded from analyses because he/she failed to correctly answer 2/3 of the questions [3, 8]. As a result, our refined dataset comprised data from 20 subjects.\\n\\nTo explore the relationship between the percentage of questions answered correctly and self-assessed attention levels, we calculated the Pearson correlation coefficient, denoted as $r$. The results indicate a statistically significant correlation ($r = 0.27, p < 0.001$), suggesting a positive association between the question accuracy and the participants' self-assessed attention levels [3, 8].\\n\\nTable 1: Experiment design for a random subject. Trials are numbered according to the order in which they were presented to the subject. The localization of the auditory stimulus was balanced over subjects for the attended side. L=Left, R=Right, F= Female, M=Male\\n\\n| Trial | Attended Side | Spatial locations | Gender of left speaker | Gender of right speaker |\\n|-------|---------------|-------------------|------------------------|------------------------|\\n| 1     | L             | -90\u00b0, +90\u00b0        | F                      | M                      |\\n| 2     | R             | -90\u00b0, +90\u00b0        | M                      | F                      |\\n| 3     | R             | -90\u00b0, +90\u00b0        | F                      | M                      |\\n| 4     | L             | -90\u00b0, +90\u00b0        | M                      | F                      |\\n| 5     | L             | -60\u00b0, +60\u00b0        | M                      | F                      |\\n| 6     | R             | -60\u00b0, +60\u00b0        | M                      | F                      |\\n| 7     | L             | -60\u00b0, +60\u00b0        | F                      | M                      |\\n| 8     | R             | -60\u00b0, +60\u00b0        | F                      | M                      |\\n| 9     | L             | -45\u00b0, +45\u00b0        | M                      | F                      |\\n| 10    | R             | -45\u00b0, +45\u00b0        | M                      | F                      |\\n| 11    | R             | -45\u00b0, +45\u00b0        | F                      | M                      |\\n| 12    | L             | -45\u00b0, +45\u00b0        | F                      | M                      |\\n| 13    | L             | -30\u00b0, +30\u00b0        | M                      | F                      |\\n| 14    | R             | -30\u00b0, +30\u00b0        | F                      | M                      |\\n| 15    | L             | -30\u00b0, +30\u00b0        | F                      | M                      |\\n| 16    | R             | -30\u00b0, +30\u00b0        | M                      | F                      |\\n| 17    | L             | -5\u00b0, +5\u00b0          | M                      | F                      |\\n| 18    | R             | -5\u00b0, +5\u00b0          | M                      | F                      |\\n| 19    | R             | -5\u00b0, +5\u00b0          | F                      | M                      |\\n| 20    | L             | -5\u00b0, +5\u00b0          | F                      | M                      |\"}"}
{"id": "lin24f_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. 1D-CNN module\\n\\nEEG signals are dynamic time-series data that contain valuable temporal information. Recently, 1D-CNN has proven effective in capturing these temporal patterns within EEG signals in several applications, such as motor imagery [14] and seizure detection [15]. Inspired by this, we adopt a 3-layer 1D-CNN module to extract temporal representations from EEG signals in our CA-CNN architecture.\\n\\nThe CA-CNN takes $E \\\\in \\\\mathbb{R}^{L \\\\times C}$, a segment of EEG signals as the input, where $L$ denotes the number of samples within the segment, and $C$ is the number of EEG channels. The first two layers are identical, each consisting of 16 filters of size 3, followed by a batch normalization (BN) layer and an average pooling layer with a pool size of 2. The third layer employs 32 filters of size 3, followed by a BN layer. Throughout these layers, the leaky ReLU is employed as the activation function. The resulting feature, denoted as $E_{f} \\\\in \\\\mathbb{R}^{L/4 \\\\times C_f}$, is extracted after the 1D-CNN module, where $C_f = 32$.\\n\\n3.2. Channel attention module\\n\\nEEG signals contain information originating from diverse channels, and the patterns within EEG responses to auditory stimuli can vary widely among individuals. The incorporation of attention mechanisms into the analysis of EEG signals offers several advantages, enhancing model performance and adaptability through the dynamic emphasis or de-emphasis of specific channels based on their respective significance [11, 16].\\n\\nA channel attention module is employed in our study to extract spatial information from EEG signals better. This approach can be expressed as follows:\\n\\n$$W = \\\\text{FC} \\\\sigma(\\\\text{FC} \\\\zeta(\\\\text{GAP}(E_f)))$$\\n\\n$$E_{ca} = W \\\\otimes E_f$$\\n\\nHere, $W \\\\in \\\\mathbb{R}^{C_f}$ represents the channel attention weights. FC refers to a fully-connected (fc) layer with a Sigmoid activation function, while FC denotes a fc layer with leaky ReLU function. GAP stands for global average pooling, and $\\\\otimes$ represents element-wise multiplication.\\n\\n3.3. Classifier\\n\\nTaking $E_{ca} \\\\in \\\\mathbb{R}^{L/4 \\\\times C_f}$ as input, the classifier integrates a global average pooling layer to condense the feature maps, and a fc layer with 2 units utilizing Softmax activation for predicting attended sound source. Training is performed iteratively using the binary cross-entropy loss function and the Adam optimizer:\\n\\n$$L = -\\\\frac{1}{L} \\\\sum_{l=1}^{L} y_l \\\\cdot \\\\log p_l + (1 - y_l) \\\\cdot \\\\log(1 - p_l)$$\\n\\nwhere $y_l$ represents the true label and $p_l$ the predicted probability for the $l$-th decision window.\\n\\n4. Experiments\\n\\n4.1. Data preprocessing\\n\\nThe EEG data were preprocessed using MNE-Python[17]. Initially, a high-pass FIR filter with a 1 Hz cutoff frequency was utilized to eliminate electrode drift. Subsequently, the EEG data were re-referenced to the average of all scalp channels. Following previous AAD studies [16, 2], we used an FIR low-pass filter to bandpass the EEG data to a frequency range of 1-50 Hz. The data were then resampled to a frequency of 128 Hz. For analysis, we utilized a sliding decision window with a 50% overlap rate to segment the EEG data, denoted as a \u201cdecision window.\u201d Considering the approximate 1-second lag in human attention shifting, the decision window length was set to 1 second in this study. Normalization was applied to each decision window to minimize trial-related biases.\\n\\n4.2. Training and evaluation\\n\\nWe evaluated the proposed models under subject-independent conditions using a 5-fold cross-validation approach [9]. The average results across all subjects were reported as the model\u2019s performance. Moreover, to prevent potential biased outcomes resulting from data leakage, we ensure that EEG responses from different subjects to the same speech stimulus were not present in both the training and testing datasets during data splitting.\\n\\nAs illustrated in Fig. 2, the EEG data from each trial were segmented into five consecutive folds. Each fold comprised EEG data organized in a continuous time-series across all subjects. Notably, we also discarded potential overlaps, referred to as \u201crepeated segments,\u201d where the tail end of a training window might overlap with the test set.\\n\\nThe model implementation utilized TensorFlow version 2.13.0, employing the Adam optimizer with a learning rate of.\"}"}
{"id": "lin24f_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To prevent overfitting, early stopping was applied, monitoring accuracy with a minimum required improvement of 0.01 to sustain training. The patience parameter was set to 8 epochs, and an automatic mode selection was employed. The training process was limited to a maximum of 100 epochs.\\n\\n5. Results and Discussion\\n\\n5.1. Effect of channel attention\\n\\nTo assess the effectiveness of channel attention, we conduct an ablation analysis, comparing the performance of 1D-CNN with CA-CNN. Specifically, 1D-CNN refers to CA-CNN without the channel attention module. As shown in Fig. 3, the CA-CNN not only achieves a superior average accuracy of 87.20% but also exhibits enhanced robustness across subjects, as indicated by a lower standard deviation (SD) of 5.56% and a competitive minimum accuracy of 79.29%. In contrast, the 1D-CNN shows an average accuracy of 80.97%, accompanied by a higher SD of 8.62% and a less competitive minimum accuracy of 69.55%. The findings underscore the crucial role of channel attention for a more robust and discriminative representation of EEG for ASAD tasks, showcasing its potential to enhance the model\u2019s sensitivity to individual auditory attention patterns.\\n\\n5.2. Comparative analysis\\n\\nWe conducted a comprehensive evaluation of various models, including GCN [12], STAnet [11], the CNN-KUL model [10], and our proposed CA-CNN, assessing the ASAD accuracy under the subject-independent scenario in ASA dataset. Each model is comparable in size, with parameter counts as follows: GCN with 3510, STAnet with 3929 parameters, CNN-KUL with 5487, and CA-CNN with 6834. This ensures a fair comparison of their ASAD performance in our study.\\n\\nAs depicted in Fig. 4, GCN achieves an average accuracy of 56.59% (SD: 7.30%), STAnet exhibits an average accuracy of 63.63% (SD: 7.17%), CNN-KUL attains an average accuracy of 77.76% (SD: 9.13%), and CA-CNN significantly outperforms all models (paired $t$-test, $p < 0.001$), achieving a remarkable accuracy of 87.20% (SD: 5.56%). Notably, GCN, STAnet, and CNN-KUL were initially evaluated on scenarios with only two competing speaker directions. The performance decline observed in all models under this more challenging experimental setup emphasizes the robustness of our CA-CNN model. It also underscores the necessity for additional ASAD datasets that encompass more realistic scenarios, as the existing models experience limitations in such conditions. The incorporation of expanded datasets is crucial to fully exploit the practical application potential of DNNs.\\n\\n6. Conclusion\\n\\nWe introduced a new ASA dataset, encompassing auditory stimuli featuring spatial variations across multiple angles ranging from -90\u00b0 to 90\u00b0. This dataset serves as a valuable complement to existing data repositories in the field of EEG-enabled ASAD tasks. Additionally, we have developed a reference model, i.e. CA-CNN, for detecting auditory spatial attention. Through comparative analysis and experiments, we show that CA-CNN consistently outperforms the state-of-the-art models, particularly under subject-dependent conditions. The ASA database and the source code of the reference model are to be publicly released for research purposes.\"}"}
