{"id": "mitsui22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Y. Zhang, S. Sun, M. Galley, Y.-C. Chen, C. Brockett, X. Gao, J. Gao, J. Liu, and B. Dolan, \u201cDialoGPT: Large-scale generative pre-training for conversational response generation,\u201d in Proc. ACL, online, Jul. 2020, pp. 270\u2013278.\\n\\n[2] R. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y. Du et al., \u201cLaMDA: Language models for dialog applications,\u201d arXiv preprint arXiv:2201.08239, Jan. 2022.\\n\\n[3] J. Shen, R. Pang, R. J. Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y. Zhang, Y. Wang, R. Skerrv-Ryan et al., \u201cNatural TTS synthesis by conditioning WaveNet on mel spectrogram predictions,\u201d in Proc. ICASSP, Calgary, Canada, May 2018, pp. 4779\u20134783.\\n\\n[4] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in Proc. ICML, online, Jul. 2021, pp. 5530\u20135540.\\n\\n[5] S. Kita and S. Ide, \u201cNodding, aizuchi, and final particles in Japanese conversation: How conversation reflects the ideology of communication and social relationships,\u201d Journal of Pragmatics, vol. 39, no. 7, pp. 1242\u20131254, 2007.\\n\\n[6] A. Gravano, \u02c7S. Be \u02c7nu\u02c7s, R. Levitan, and J. Hirschberg, \u201cThree ToBI-based measures of prosodic entrainment and their correlations with speaker engagement,\u201d in Proc. SLT, California, U.S.A., Dec. 2014, pp. 578\u2013583.\\n\\n[7] R. Levitan and J. B. Hirschberg, \u201cMeasuring acoustic-prosodic entrainment with respect to multiple levels and dimensions,\u201d in Proc. INTERSPEECH, Florence, Italy, Aug. 2011, pp. 3081\u20133084.\\n\\n[8] R. Street, \u201cSpeech convergence and speech evaluation in fact-finding interviews,\u201d Human Communication Research, vol. 11, no. 2, pp. 139\u2013169, 1984.\\n\\n[9] H. Mori, T. Satake, M. Nakamura, and H. Kasuya, \u201cConstructing a spoken dialogue corpus for studying paralinguistic information in expressive conversation and analyzing its statistical/acoustic characteristics,\u201d Speech Communication, vol. 53, no. 1, pp. 36\u201350, Jan. 2011.\\n\\n[10] M. Yokoyama, T. Nagata, and H. Mori, \u201cEffects of dimensional input on paralinguistic information perceived from synthesized dialogue speech with neural network,\u201d in Proc. INTERSPEECH, Hyderabad, India, Sep. 2018, pp. 3053\u20133056.\\n\\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional Transformers for language understanding,\u201d in Proc. NAACL-HLT, Minneapolis, U.S.A., Jun. 2019, pp. 4171\u20134186.\\n\\n[12] H. Guo, S. Zhang, F. K. Soong, L. He, and L. Xie, \u201cConversational end-to-end TTS for voice agents,\u201d in Proc. SLT, online, Jan. 2021, pp. 403\u2013409.\\n\\n[13] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \u201cStyle Tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in Proc. ICML, Stockholm, Sweden, Jul. 2018, pp. 5180\u20135189.\\n\\n[14] J. Cong, S. Yang, N. Hu, G. Li, L. Xie, and D. Su, \u201cControllable context-aware conversational speech synthesis,\u201d in Proc. INTERSPEECH, online, Sep. 2021, pp. 4658\u20134662.\\n\\n[15] D. P. Kingma and M. Welling, \u201cAuto-encoding variational Bayes,\u201d in Proc. ICLR, Banff, Canada, Apr. 2014.\\n\\n[16] J. Kim, S. Kim, J. Kong, and S. Yoon, \u201cGlow-TTS: A generative flow for text-to-speech via monotonic alignment search,\u201d in Proc. NeurIPS, vol. 33, online, Dec. 2020, pp. 8067\u20138077.\\n\\n[17] W.-N. Hsu, Y. Zhang, R. J. Weiss, H. Zen, Y. Wu, Y. Wang, Y. Cao, Y. Jia, Z. Chen, J. Shen et al., \u201cHierarchical generative modeling for controllable speech synthesis,\u201d in Proc. ICLR, New Orleans, U.S.A., May 2019.\\n\\n[18] I. Loshchilov and F. Hutter, \u201cDecoupled weight decay regularization,\u201d in Proc. ICLR, New Orleans, U.S.A., May 2019.\\n\\n[19] S. R. Bowman, L. Vilnis, O. Vinyals, A. M. Dai, R. Jozefowicz, and S. Bengio, \u201cGenerating sentences from a continuous space,\u201d in Proc. CoNLL, Berlin, Germany, Aug. 2016, pp. 10\u201321.\\n\\n[20] N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov, \u201cDropout: a simple way to prevent neural networks from overfitting,\u201d The journal of machine learning research, vol. 15, no. 1, pp. 1929\u20131958, Jun. 2014.\\n\\n[21] R. Kubichek, \u201cMel-cepstral distance measure for objective speech quality assessment,\u201d in Proc. IEEE PACRIM, Victoria, Canada, May 1993, pp. 125\u2013128.\\n\\n[22] D. J. Berndt and J. Clifford, \u201cUsing dynamic time warping to find patterns in time series,\u201d in Proc. KDD Workshop, vol. 10, Seattle, U.S.A., 1994, pp. 359\u2013370.\"}"}
{"id": "mitsui22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue\\n\\nKentaro Mitsui, Tianyu Zhao, Kei Sawada, Yukiya Hono, Yoshihiko Nankaku, Keiichi Tokuda\\n\\n1 rinn Co., Ltd., Japan, 2 Nagoya Institute of Technology, Japan\\n\\n{ kemits,tianyuz,keisawada }@rinna.co.jp, {hono,nankaku,tokuda}@sp.nitech.ac.jp\\n\\nAbstract\\n\\nThe recent text-to-speech (TTS) has achieved quality comparable to that of humans; however, its application in spoken dialogue has not been widely studied. This study aims to realize a TTS that closely resembles human dialogue. First, we record and transcribe actual spontaneous dialogues. Then, the proposed dialogue TTS is trained in two stages: first stage, variational autoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMV AE)-VITS is trained, which introduces an utterance-level latent variable into variational inference with adversarial learning for end-to-end text-to-speech (VITS), a recently proposed end-to-end TTS model. A style encoder that extracts a latent speaking style representation from speech is trained jointly with TTS. In the second stage, a style predictor is trained to predict the speaking style to be synthesized from dialogue history. During inference, by passing the speaking style representation predicted by the style predictor to VAE/GMV AE-VITS, speech can be synthesized in a style appropriate to the context of the dialogue. Subjective evaluation results demonstrate that the proposed method outperforms the original VITS in terms of dialogue-level naturalness.\\n\\nIndex Terms: end-to-end TTS, spontaneous dialogue, speaking style, variational autoencoder, BERT\\n\\n1. Introduction\\n\\nDialogue is a conversation between two or more people. In recent years, the development of natural language processing has greatly improved the quality of text-based dialogue generation resulting in human-computer or computer-computer dialogue [1, 2]. On the other hand, speech is essential for human dialogue. Therefore, TTS has an important role in facilitating communication between humans and computers.\\n\\nThe development of deep learning has resulted in synthesizing speech in a quality comparable to that of humans [3, 4]. However, dialogue speech often has characteristics that are different from those of the recited speech. First, while recited speech has transcript beforehand, dialogue speech is a spontaneous speech. Therefore, dialogue speech is more difficult to model than recited speech because of repetition, fillers, prolongation, and breaths. Second, dialogues are frequently accompanied by backchannels, also known as aizuchi [5] in Japanese, and laughter. These factors transcribed in the same way can be uttered in various styles. Thus, it is necessary to appropriately model the one-to-many relationship between text and speech. Finally, several factors of speech such as pitch [6], energy [7], and speech rate [8] can be in sync with the dialogue partner, which is called entrainment [7]. Considering these features, TTS can resemble more natural human-human dialogue.\\n\\nSeveral studies have focused on conversational TTS. Yokoyama et al. used Utsunomiya University spoken dialogue database [9] to control paralinguistic information [10]. They utilized paralinguistic information tags and did not consider dialogue history. Guo et al. used the bidirectional encoder representations from Transformers (BERT) [11] to compute encodings of current text and chat history and fed them to the encoder of the acoustic model to improve the naturalness of the synthetic speech [12]. Cong et al. considered the acoustic information of the previous utterance as well as the linguistic information by predicting the Global Style Token [13] of the current utterance from the mel-spectrogram of the previous utterance [14]. These two studies used predefined transcript to record spoken dialogue, which may differ from actual dialogue without transcript, in terms of the frequency of the spontaneous behaviors and the presence or absence of backchannels.\\n\\nIn this study, we record a free-form dialogue on a given topic without preparing a transcript to achieve more human-like dialogue speech synthesis. Of the three aforementioned features of spontaneous dialogue, (1) we use VITS [4], an end-to-end TTS which robustly estimates alignment between text and speech by monotonic alignment search (MAS) and blank tokens. (2) We incorporate an utterance-level latent variable into VITS to facilitate the modeling of one-to-many relationship between text and speech. Following the framework of VAE [15], we propose two methods: VAE-VITS that assumes a standard normal distribution for the prior distribution of the latent variable, and GMV AE-VITS, which assumes a Gaussian Mixture Model (GMM) for the prior. Furthermore, by sharing the latent space among speakers, training is encouraged to make similar speaking styles between speakers close in the latent space. (3) We introduce a style predictor that predicts the speaking style of current speech based on dialogue history to realize an entrainment that is close to actual dialogue. Speech sequences in dialogue history are difficult to handle directly because their length is extremely long. Therefore, we adopt a two-stage training framework: first, VAE/GMV AE-VITS is trained using a single utterance and then style predictor is trained using a sequence of style representations extracted from past utterances.\\n\\n2. Spontaneous dialogue corpus\\n\\nTo record speech that is close to actual human-human conversation, the following method is used for speech recording and post-processing. First, two or more speakers are given a topic and asked to talk freely and their voices are recorded on independent channels. Automatic speech recognition (ASR) automatically transcribes the recorded speech. Transcripts are then manually modified and given time information (start and end time of each utterance) to produce the final transcript with time information. Using this time information, the audio file is split to obtain utterance-level speech. Although it is time-consuming to transcribe and assign time information to free dialogue, the use of ASR can greatly reduce the burden of post-processing. In addition, the lack of predefined transcript allows the speakers to produce more spontaneous speech which contains repetition, fillers, prolongation, and also backchannels. Speech data...\"}"}
{"id": "mitsui22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Let the recorded speech enable us to model the style of dialogue speech and speaker ID of each utterance, respectively. The purpose of this study is to synthesize the speech sequence of dialogue, each of which has consistent style in dialogue speech. By assuming GMM for the distribution of speaking styles during a dialogue, we model the transition of speaking styles using the style predictor. Hereafter, we replace the subscript $u$ with the index $n$ in the dialogue for simplicity.\\n\\n![Diagaram of the proposed method](image)\\n\\nThe proposed method introduces an utterance-level latent variable $z_n$ to represent the speaking style of each utterance. $z_n$ is given as follows:\\n\\n$$z_n = \\\\text{Style Encoder}(x_n, \\\\mathcal{S})$$\\n\\nwhere $x_n$ is a monotonically aligned feature vector $f_n$, $s_n$ is a speaker embedding, and $\\\\mathcal{S}$ is a character embedding.\\n\\nThe purpose of this study is to synthesize the speech sequence of dialogue, each of which has consistent style in dialogue speech. By assuming GMM for the distribution of speaking styles during a dialogue, we model the transition of speaking styles using the style predictor.\\n\\nThe first training stage models the utterance-level relationship between phoneme sequence $y_n$ and speaker ID $s_n$, and it is difficult to model them directly when $y_n$ is an extremely long time series. In this case, the posterior distribution of $z_n$ can be trained more stably than fully attention-based models and $c_n$ can be trained more stably than fully attention-based models.\\n\\nThe second stage of training is to train the style predictor, which predicts the density $q(z_n|\\\\theta)$ and the variance $\\\\sigma(z_n|\\\\theta)$ of $z_n$. The following objective function is used to train the style predictor:\\n\\n$$\\\\log p(z_1, \\\\ldots, z_n|\\\\theta) = \\\\sum_{n=1}^{N} \\\\log p(z_n|\\\\theta) + \\\\sum_{n=1}^{N} \\\\log p(x_n|z_n, \\\\theta)$$\\n\\nwhere $p(x_n|z_n, \\\\theta)$ is the probability density function of $x_n$ given $z_n$ and $\\\\theta$ is the parameter of the model.\\n\\nThe objective function is the product of the objective functions of the first and second stages, which is the sum of the log-likelihood and the KL divergence between the prior and posterior distributions.\\n\\nThe ELBO of the log-likelihood is given by:\\n\\n$$\\\\mathbb{E}_{q(z_n)} \\\\left[ \\\\log p(x_n|z_n, \\\\theta) \\\\right] - \\\\sum_{n=1}^{N} \\\\log q(z_n|\\\\theta)$$\\n\\nwhere $q(z_n|\\\\theta)$ is the prior distribution of $z_n$ and $p(x_n|z_n, \\\\theta)$ is the probability density function of $x_n$ given $z_n$ and $\\\\theta$ is the parameter of the model.\\n\\nThe ELBO can be maximized by the following update rule:\\n\\n$$\\\\theta \\\\leftarrow \\\\arg \\\\max_{\\\\theta} \\\\mathbb{E}_{q(z_n)} \\\\left[ \\\\log p(x_n|z_n, \\\\theta) \\\\right] - \\\\sum_{n=1}^{N} \\\\log q(z_n|\\\\theta)$$\\n\\nThe ELBO is the sum of the log-likelihood and the KL divergence between the prior and posterior distributions. The log-likelihood is the log-probability of the observed data given the model parameters. The KL divergence is a measure of the difference between two probability distributions. The update rule maximizes the ELBO, which is the sum of the log-likelihood and the KL divergence between the prior and posterior distributions.\\n\\nThe proposed method can be trained by maximizing the evidence lower bound (ELBO) of the following log-likelihood (for $M$ Monte Carlo sampling):\\n\\n$$\\\\mathbb{E}_{q(z_n)} \\\\left[ \\\\log p(x_n|z_n, \\\\theta) \\\\right] - \\\\sum_{n=1}^{N} \\\\log q(z_n|\\\\theta)$$\\n\\nwhere $q(z_n|\\\\theta)$ is the prior distribution of $z_n$ and $p(x_n|z_n, \\\\theta)$ is the probability density function of $x_n$ given $z_n$ and $\\\\theta$ is the parameter of the model.\\n\\nThe objective function is the sum of the log-likelihood and the KL divergence between the prior and posterior distributions. The ELBO can be used to train the model by maximizing the log-probability of the observed data given the model parameters.\\n\\nThe purpose of this study is to synthesize the speech sequence of dialogue, each of which has consistent style in dialogue speech. By assuming GMM for the distribution of speaking styles during a dialogue, we model the transition of speaking styles using the style predictor.\\n\\nThe first training stage models the utterance-level relationship between phoneme sequence $y_n$ and speaker ID $s_n$, and it is difficult to model them directly when $y_n$ is an extremely long time series.\"}"}
{"id": "mitsui22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) Style Predictor\\n\\n(b) Sentence Encoding\\n\\n(c) Context Encoding\\n\\nPre-trained BERT\\n\\nFigure 2: Conceptual diagram of (a) style predictor, (b) sentence encoding, and (c) context encoding.\\n\\n- **Style Predictor**: Takes two sequences as inputs, the style encoder of trained BERT and the other sequence is to use the style encoder of trained BERT and the other is to use the style encoder of trained BERT and the other is to use the style encoder of trained BERT.\\n\\n- **Sentence Encoding**: Computes 1024-dimensional sentence encoding.\\n\\n- **Context Encoding**: Computes 1024-dimensional sentence encoding.\\n\\n**4.1. Experimental conditions**\\n\\n- **Datasets**:\\n  - **Japanese dialogues**: Between two females, who can see each other's faces through glass and can hear each other's voice with a headphone, were recorded in isolated soundproof chambers.\\n  - **Recorded data**: Consisted of 18,385 utterances.\\n  - **Data collection**: 15,880 utterances were used in training, development, and evaluation set, respectively.\\n  - **Data creation**: 3 dialogues (1,362 utterances) were used as training set.\\n  - **Data transcription**: 5 dialogues (1,362 utterances) were used as training set.\\n  - **Data transcription**: 5 dialogues (1,362 utterances) were used as training set.\\n\\n- **4.1.2. Model and training details**\\n\\n- **Model and training details**:\\n  - **Model**: VAE/GMV AE-VITS\\n  - **Training details**:\\n    - **Batch size**: 32\\n    - **Sequence length**: 30\\n    - **KL weights**: 0\\n    - **Learning rate**: 8\\n    - **Optimization**: AdamW optimizer\\n\\n**4.2. Results**\\n\\n**4.2.1. Objective evaluation of style predictor**\\n\\n**4.2.2. Objective evaluation of overall system**\\n\\n**Table 1**: RMSE between the style vector predicted using the style predictor trained in the previous study and target style vectors.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.70| 9.50| 0.50|\\n| S               | 7.04| 8.17| 0.41|\\n| C               | 7.06| 8.18| 0.44|\\n| S+C             | 7.51| 9.18| 0.47|\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nWe trained three models: the original VITS [4], which does not explicitly consider speaking styles, and the proposed VAE/GMV AE-VITS described in section 3.1. The hyperparameters of VITS were set to be the same as in the previous study. Following GMV AE-Tacotron [17], the utterance encoder of VITS and GMV AE-VITS was composed of two 1D-convolutional layers with 512 filters, two unidirectional LSTM layers with 256 cells at each direction, and a mean pooling layer followed by a linear projection layer. The number of Monte Carlo samples for computing the VAE/GMV AE-VITS was set to 1 and the dimension of the latent classes was set to 10.\\n\\nWe conducted an objective evaluation to compare the performance of the proposed methods with baseline VITS. For both VAE and GMV AE, the KL annealing [19] was introduced for training and the temperature of VAE/GMV AE-VITS was set to 1 and the dimension of the latent classes was set to 10.\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nWe trained BERT [11] from scratch on approximately 400 GB of Japanese text and used it to compute 1024-dimensional sentence encoding.\\n\\n**4.3. Experiments**\\n\\n**4.3.1. Objective evaluation of overall system**\\n\\n**4.3.2. Objective evaluation of style predictor**\\n\\n**Table 1**: RMSE between the style vector predicted using the style predictor trained in the previous study and target style vectors.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.70| 9.50| 0.50|\\n| S               | 7.04| 8.17| 0.41|\\n| C               | 7.06| 8.18| 0.44|\\n| S+C             | 7.51| 9.18| 0.47|\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nNone was significantly smaller than C and target style vectors is presented in Table 1. The RMSE of the current utterance was set to 1 and the dimension of the latent classes was set to 10.\\n\\nWe trained BERT [11] from scratch on approximately 400 GB of Japanese text and used it to compute 1024-dimensional sentence encoding.\\n\\n**4.3.1. Objective evaluation of overall system**\\n\\n**4.3.2. Objective evaluation of style predictor**\\n\\n**Table 1**: RMSE between the style vector predicted using the style predictor trained in the previous study and target style vectors.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.70| 9.50| 0.50|\\n| S               | 7.04| 8.17| 0.41|\\n| C               | 7.06| 8.18| 0.44|\\n| S+C             | 7.51| 9.18| 0.47|\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nNone was significantly smaller than C and target style vectors is presented in Table 1. The RMSE of the current utterance was set to 1 and the dimension of the latent classes was set to 10.\\n\\nWe trained BERT [11] from scratch on approximately 400 GB of Japanese text and used it to compute 1024-dimensional sentence encoding.\\n\\n**4.3.1. Objective evaluation of overall system**\\n\\n**4.3.2. Objective evaluation of style predictor**\\n\\n**Table 1**: RMSE between the style vector predicted using the style predictor trained in the previous study and target style vectors.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.70| 9.50| 0.50|\\n| S               | 7.04| 8.17| 0.41|\\n| C               | 7.06| 8.18| 0.44|\\n| S+C             | 7.51| 9.18| 0.47|\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nNone was significantly smaller than C and target style vectors is presented in Table 1. The RMSE of the current utterance was set to 1 and the dimension of the latent classes was set to 10.\\n\\nWe trained BERT [11] from scratch on approximately 400 GB of Japanese text and used it to compute 1024-dimensional sentence encoding.\\n\\n**4.3.1. Objective evaluation of overall system**\\n\\n**4.3.2. Objective evaluation of style predictor**\\n\\n**Table 1**: RMSE between the style vector predicted using the style predictor trained in the previous study and target style vectors.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.70| 9.50| 0.50|\\n| S               | 7.04| 8.17| 0.41|\\n| C               | 7.06| 8.18| 0.44|\\n| S+C             | 7.51| 9.18| 0.47|\\n\\n**Table 2**: Objective evaluation results.\\n\\n| Method          | MCD | MSD | DUR |\\n|-----------------|-----|-----|-----|\\n| None            | 7.54| 9.22| 0.50|\\n| S               | GMV AE-predicted 7.51 9.18 0.47 |\\n| C               | GMV AE-oracle 7.04 8.17 0.41 |\\n| S+C             | GMV AE-predicted 7.54 9.22 0.50 |\\n\\nNone was significantly smaller than C and target style vectors is presented in Table 1. The RMSE of the current utterance was set to 1 and the dimension of the latent classes was set to 10.\\n\\nWe trained BERT [11] from scratch on approximately 400 GB of Japanese text and used it to compute 1024-dimensional sentence encoding.\"}"}
{"id": "mitsui22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Analysis of latent space learned by GMVAE-VITS in terms of (a) prior distribution, (b) posterior distribution, (c) average duration, (d) average pitch, and (e) average loudness.\\n\\nTable 3: Results of MOS evaluation on utterance-level and dialogue-level naturalness with 95% confidence intervals.\\n\\n| Method          | Utterance | Dialogue |\\n|-----------------|-----------|----------|\\n| VITS            | 3.38 \u00b1 0.14 | 3.34 \u00b1 0.12 |\\n| GMVAE-oracle    | 3.51 \u00b1 0.12 | 3.59 \u00b1 0.12 |\\n| GMVAE-predicted | 3.56 \u00b1 0.12 | 3.53 \u00b1 0.11 |\\n\\nThe results are presented in Table 2. Both VAE/GMV AE-oracle showed significant improvement in MCD and MSD compared to baseline VITS. DUR was also slightly improved, suggesting that \\\\( z_u \\\\) represents duration-related features as well as acoustic features. VAE/GMV AE-predicted also showed improvement in MCD and MSD relative to baseline VITS. This indicates that the style predictor was able to predict speaking styles that are close to those of target speech. The performance of GMVAE-VITS was slightly better than VAE-VITS for both oracle and predicted. This is probably because the richer prior of GMVAE-VITS could represent the various speaking styles of dialogue speech more appropriately. In the next section, we further compare the proposed GMVAE-VITS with baseline VITS.\\n\\n4.2.3. Subjective evaluation of overall system\\n\\nWe conducted two mean opinion score (MOS) tests to evaluate the subjective quality of the synthetic speech. In the utterance-level evaluation, raters were presented only one utterance and asked to evaluate its naturalness. In the dialogue-level evaluation, raters were presented a short dialogue consisting of 6 utterances (approximately 10\u201320 sec) and asked to evaluate its naturalness as a spoken dialogue (whether natural entrainment occurred, whether the speaking style was suitable for the context, etc.). We used ground truth timing of each utterance to construct dialogue samples because our spontaneous dialogue corpus contained numerous overlaps and simply playing the synthesized speech alternatively resulted in unnatural dialogue. For dialogue-level evaluation, we also computed text-speech alignment over recorded speech using MAS and used the alignment information for synthesis to align speech length with the original one. The evaluation was conducted on a 5-point scale from 1 (bad) to 5 (excellent). Thirty raters participated in the evaluation, and each rater evaluated thirty speech samples. The results are presented in Table 3. With regard to utterance-level naturalness, although the scores of GMVAE-oracle/predicted were slightly higher than VITS, there was no significant difference between them. Though VITS does not utilize explicit style representation, the synthetic speech was evaluated as natural because various speaking styles exist that sound natural when heard as a single utterance. Regarding dialogue-level naturalness, the score of GMVAE-oracle was significantly higher than VITS (\\\\( p = 0.003 \\\\) in Student's t-test), confirming that using appropriate speech styles contributed to the naturalness of dialogue. Furthermore, GMVAE-predicted also achieved a significantly higher score than VITS (\\\\( p = 0.021 \\\\)), indicating that style predictor was able to predict the appropriate speaking style when heard as a dialogue.\\n\\n4.2.4. Analysis of latent space\\n\\nFig. 3 (a) and (b) illustrates the prior and posterior distribution of trained GMVAE-VITS, respectively, where dimensionality reduction was applied using principal component analysis. We observed that the latent representations of the two speakers were mixed, indicating that the learned latent space was speaker-independent. This is because the speaker embedding was explicitly used, allowing the latent variable \\\\( z_u \\\\) to represent only speaker-independent speaking styles. We also synthesized all texts in the evaluation set with different speaker IDs and latent classes and described the average duration, pitch of voiced segments, and loudness of synthetic speech in Fig. 3 (c), (d), and (e). We confirmed that each latent class had different characteristics and they were common across speakers. With these characteristics, the learned prior distribution can be applied to modify speaking style to the desired one.\\n\\n5. Conclusions\\n\\nIn this study, we aimed to synthesize spoken dialogue that is close to human spontaneous dialogue and proposed (1) recording and transcription of free-form dialogues without transcripts, (2) VAE/GMV AE-VITS to model various speaking styles, and (3) a style predictor that predicts speaking styles using linguistic and acoustic features from past dialogues. The combination of GMVAE-VITS and the style predictor achieved higher naturalness than conventional VITS in a dialogue-level evaluation. The latent space acquired by GMVAE-VITS was speaker-independent and had different characteristics for each latent class. This study assumed that transcriptions of past utterances and timing of each utterance were available; however, actual applications will require estimating these as well. Future work will include introducing a mechanism to automatically estimate them and unifying the proposed two-stage training framework into a single end-to-end training framework.\"}"}
