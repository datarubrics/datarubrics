{"id": "takamichi22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yi Ren, Chenxu Hu, Xu Tan, Tao Qin, Sheng Zhao, Zhou Zhao, and Tie-Yan Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d in International Conference on Learning Representations, 2021.\\n\\nJaehyeon Kim, Jungil Kong, and Juhee Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Marina Meila and Tong Zhang, Eds. 2021, vol. 139 of Proceedings of Machine Learning Research, pp. 5530\u20135540, PMLR.\\n\\nY. Saito, S. Takamichi, and H. Saruwatari, \u201cStatistical parametric speech synthesis incorporating generative adversarial networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 26, no. 1, pp. 84\u201396, Jan. 2018.\\n\\nMiko\u0142aj Binkowski, Jeff Donahue, Sander Dieleman, Aidan Clark, Erich Elsen, Norman Casagrande, Luis C Cobo, and Karen Simonyan, \u201cHigh fidelity speech synthesis with adversarial networks,\u201d in International Conference on Learning Representations, 2019.\\n\\nNanxin Chen, Yu Zhang, Heiga Zen, Ron J. Weiss, Mohammad Norouzi, Najim Dehak, and William Chan, \u201cWaveGrad 2: Iterative Refinement for Text-to-Speech Synthesis,\u201d in Proc. Interspeech 2021, 2021, pp. 3765\u20133769.\\n\\nZhifeng Kong, Wei Ping, Jiaji Huang, Kexin Zhao, and Bryan Catanzaro, \u201cDiffWave: A versatile diffusion model for audio synthesis,\u201d in International Conference on Learning Representations, 2021.\\n\\nHaohan Guo, Shaofei Zhang, Frank K. Soong, Lei He, and Lei Xie, \u201cConversational end-to-end tts for voice agents,\u201d in 2021 IEEE Spoken Language Technology Workshop (SLT), 2021, pp. 403\u2013409.\\n\\nTuomo Raitio, Ramya Rasipuram, and Dan Castellani, \u201cControllable neural text-to-speech synthesis using intuitive prosodic features,\u201d arXiv preprint arXiv:2009.06775, 2020.\\n\\nXiaolian Zhu, Shan Yang, Geng Yang, and Lei Xie, \u201cControlling emotion strength with relative attribute for end-to-end speech synthesis,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2019, pp. 192\u2013199.\\n\\nKing Simon, Crumlish Jane, Martin Amy, and Wihlborg Lovisa, \u201cThe blizzard challenge 2018,\u201d in Proc. Blizzard Challenge workshop. Hyderabad, India, 2018.\\n\\nGuanghui Xu, Wei Song, Zhengchen Zhang, Chao Zhang, Xiaodong He, and Bowen Zhou, \u201cImproving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis,\u201d in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6079\u20136083.\\n\\nJunjie Pan, Lin Wu, Xiang Yin, Pengfei Wu, Chenchang Xu, and Zejun Ma, \u201cA chapter-wise understanding system for text-to-speech in Chinese novels,\u201d in ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021, pp. 6069\u20136073.\\n\\nNakata Wataru, Koriyama Tomoki, Takamichi Shinnosuke, Tanji Naoko, Ijima Yusuke, Masumura Ryo, and Saruwatari Hiroshi, \u201cAudiobook Speech Synthesis Conditioned by Cross-Sentence Context-Aware Word Embeddings,\u201d in Proc. 11th ISCA Speech Synthesis Workshop (SSW 11), 2021, pp. 211\u2013215.\\n\\nEva Sz\u00e9kely, Joao Paulo Cabral, Mohamed Abou-Zleikha, Peter Cahill, and Julie Carson-Berndsen, \u201cEvaluating expressive speech synthesis from audiobook corpora for conversational phrases,\u201d in Proceedings of the Eighth International Conference on Language Resources and Evaluation (LREC'12). Istanbul, Turkey, May 2012, pp. 3335\u20133339, European Language Resources Association (ELRA).\\n\\nEva Sz\u00e9kely, Tam\u00e1s G\u00e1bor Csap\u00f3, B\u00e9lint TOTH, P\u00e9ter Mihajlik, and Julie Carson-Berndsen, \u201cSynthesizing expressive speech from amateur audiobook recordings,\u201d in 2012 IEEE Spoken Language Technology Workshop (SLT), 2012, pp. 297\u2013302.\\n\\nSunghee Jung and Hoirin Kim, \u201cPitchtron: Towards audiobook generation from ordinary people\u2019s voices,\u201d 2020.\\n\\nLudwig K\u00fcrzinger, Dominik Winkelbauer, Lujun Li, Tobias Watzel, and Gerhard Rigoll, \u201cCtc-segmentation of large corpora for German end-to-end speech recognition,\u201d in Speech and Computer, Alexey Karpov and Rodmonga Potapova, Eds., Cham, 2020, pp. 267\u2013278, Springer International Publishing.\\n\\nYi Luo and Nima Mesgarani, \u201cConv-tasnet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256\u20131266, 2019.\\n\\nRomain Hennequin, Anis Khlif, Felix Voituret, and Manuel Moussallam, \u201cSpleeter: A fast and state-of-the-art music source separation tool with pre-trained models,\u201d Late-Breaking/Demo ISMIR 2019, November 2019, Deezer Research.\\n\\nKing Simon and Karaiskos Vasilis, \u201cThe blizzard challenge 2013,\u201d in Proc. Blizzard Challenge workshop. Barcelona, Catalonia, Spain, 2013.\\n\\nHeiga Zen, Viet Dang, Rob Clark, Yu Zhang, Ron J. Weiss, Ye Jia, Zhifeng Chen, and Yonghui Wu, \u201cLibriTTS: A corpus derived from LibriSpeech for text-to-speech,\u201d 2019.\\n\\nAghilas Sini, Damien Lolive, Ga\u00eblle Vidal, Marie Tahon, and \u00calisabeth Delais-Roussarie, \u201cSynPaFlex-corpus: An expressive French audiobooks corpus dedicated to expressive speech synthesis.,\u201d in Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018). Miyazaki, Japan, May 2018, European Language Resources Association (ELRA).\\n\\nAdriana Stan, Oliver Watts, Yoshitaka Mamiya, Mircea Giurgiu, Robert A. J. Clark, Junichi Yamagishi, and Simon King, \u201cTUNDRA: a multilingual corpus of found data for TTS research created with light supervision,\u201d in Proceedings of the 14th Conference of the International Speech Communication Association (INTERSPEECH), Lyon, France, 2013, pp. 2331\u20132335.\\n\\nShinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, Adithya Renduchintala, and Tsubasa Ochiai, \u201cESPnet: End-to-end speech processing toolkit,\u201d in Proceedings of Interspeech, 2018, pp. 2207\u20132211.\\n\\nKikuo Maekawa, Hanae Koiso, Sadaoki Furui, and Hitoshi Ishihara, \u201cSpontaneous speech corpus of Japanese.,\u201d in Proc. LREC, 2000, pp. 947\u2013952.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems, I. Guyon, U. V. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, Eds. 2017, vol. 30, Curran Associates, Inc.\\n\\nAkinobu Lee and Tatsuya Kawahara, \u201cRecent development of open-source speech recognition engine julius,\u201d in Proceedings : APSIPA ASC 2009 : Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference. oct 2009, pp. 131\u2013137, Asia-Pacific Signal and Information Processing Association, 2009 Annual Summit and Conference, International Organizing Committee.\\n\\nIulia Turc, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova, \u201cWell-read students learn better: On the importance of pre-training compact models,\u201d 2019.\\n\\nJungil Kong, Jaehyeon Kim, and Jaekyoung Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in Advances in Neural Information Processing Systems, H. Larochelle, M. Ranzato, R. Hadsell, M. F. Balcan, and H. Lin, Eds. 2020, vol. 33, pp. 17022\u201317033, Curran Associates, Inc.\"}"}
{"id": "takamichi22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we construct a Japanese audiobook speech corpus called \u201cJ-MAC\u201d for speech synthesis research. With the success of reading-style speech synthesis, the research target is shifting to tasks that use complicated contexts. Audiobook speech synthesis is a good example that requires cross-sentence expressiveness, etc. Unlike reading-style speech, speaker-specific expressiveness in audiobook speech also becomes the context. To enhance this research, we propose a method of constructing a corpus from audiobooks read by professional speakers. From many audiobooks and their texts, our method can automatically extract and refine the data without any language dependency. Specifically, we use vocal-instrumental separation to extract clean data, connectionist temporal classification to roughly align text and audio, and voice activity detection to refine the alignment. J-MAC is open-sourced in our project page. We also conduct audiobook speech synthesis evaluations, and the results give insights into audiobook speech synthesis.\\n\\nIndex Terms: speech synthesis, audiobook, speech corpus\\n\\n1. Introduction\\n\\nDue to the development of deep learning, significant progress has been made on text-to-speech synthesis. Self and source-target attention [1, 2], duration intervention [3, 4], adversarial training [5, 6], and score matching [7, 8] have enabled us to synthesize high-fidelity (near-human quality) reading-style speech. In response to these developments, the input for speech synthesis is shifting from simple textual context to more complicated contexts, e.g., dialogue [9] and emotion [10, 11].\\n\\nAudiobook speech synthesis requires the use of complicated contexts [12, 13, 14]. To synthesize natural speech as an audiobook, we need to consider several contexts, which is not considered in a simple text reading task. These are, for example, cross-sentence contexts [13, 15, 14], style [16], and expressiveness [12]. The speaker who reads the book is also an important context. For example, amateur speaker\u2019s voices tend to be less expressive than professional speakers\u2019 voices [17, 18]. Even if different professional speakers read the same book, their language understanding and voice expression differ. Therefore, a corpus consisting of multiple professional speakers\u2019 voice should be constructed and open-sourced to accurately evaluate the performance of audiobook speech synthesis. However, it is very time-consuming to create such a corpus from scratch.\\n\\nIn this paper, we propose a method of constructing a multi-speaker audiobook corpus from audiobook products, and then construct a Japanese speech corpus \u201cJ-MAC\u201d (Japanese Multi-speaker Audiobook Corpus) by using the proposed method. The method involves using source separation and connectionist temporal classification (CTC) [19] for estimating data cleanliness. Iterative text-audio alignment and voice activity detection (VAD)-based refinement improve the text-audio fitting. We conduct audiobook speech synthesis evaluation using J-MAC and discuss what problems should be solved in the future. The contributions of this study are as follows:\\n\\n\u2022 We construct a new audiobook corpus that includes multiple professional speakers for Japanese audiobook speech synthesis. Our corpus is open-sourced in our project page.\\n\\n\u2022 The evaluation results give insights into audiobook speech synthesis, e.g., 1) improving a synthesis method enhances naturalness of synthetic speech regardless of the speaker, and 2) the effects of the synthesis method, speaker, and book on naturalness are strongly entangled.\\n\\n2. Corpus construction\\n\\nWe describe the steps to build the corpus, i.e., data collection, data cleansing, and alignment.\\n\\n2.1. Data collection\\n\\nWe first collect audiobooks read by professional speakers. We set the following two conditions; the audiobook must have\\n\\n\u2022 Reference text: We choose out-of-copyright books (novels) available on the Web and search the audiobook versions of these books. Another way of doing this is choosing the audiobooks and transcribing text by automatic speech recognition (ASR). However, we did not choose this another way because novels have many named entities that are difficult to transcribe with current ASR.\\n\\n\u2022 Another version by a different speaker: Even if the same book is read by different speakers, each speaker expresses it differently. Therefore, we prioritize the collection of audiobooks by different speakers over different versions by a single speaker.\\n\\nSince neighboring sentences and hierarchical information help in conditioning audiobook speech synthesis models [13, 15, 14], we create structured texts from the reference text. An example is below. The text has levels, i.e., chapter, paragraph, style, and sentence. \u201cStyle\u201d means narrative or spoken (i.e., character-acting) sentences. The timings of each sentence are retrieved using the method described in the following subsection.\\n\\n```yaml\\nkumo.yaml (The beginning of \u201cThe Spider\u2019s Thread\u201d)\\nchapt000: # chapter index(000-)\\nparag000: # paragraph index (000-)\\nstyle000: # style index (000-)\\ntime:\\n- 0.96 # start time [sec]\\n- 3.32 # end time [sec]\\nsent: \u201cIt happened one day.\u201d # sentence\\ntime: ...\\n```\\n\\n2.2. SNR-based cleansing\\n\\nAn audiobook does not always have clean data. We classify the data into \u201cclean\u201d, \u201cmusic-inserted\u201d, and \u201cmusic-overlapped,\u201d as shown in Figure 1.\\n\\n1. https://sites.google.com/site/shinnosuketakamichi/research-topics/j-mac_corpus\\n\\n2. We did not evaluate the recording quality because we used commercial audiobook products that seem to have good recording quality.\\n\\nWe choose out-of-copyright books (novels) available on the Web and search the audiobook versions of these books. Another way of doing this is choosing the audiobooks and transcribing text by automatic speech recognition (ASR). However, we did not choose this another way because novels have many named entities that are difficult to transcribe with current ASR.\\n\\nSince neighboring sentences and hierarchical information help in conditioning audiobook speech synthesis models, we create structured texts from the reference text. An example is below. The text has levels, i.e., chapter, paragraph, style, and sentence. \u201cStyle\u201d means narrative or spoken (i.e., character-acting) sentences. The timings of each sentence are retrieved using the method described in the following subsection.\\n\\n```yaml\\nkumo.yaml (The beginning of \u201cThe Spider\u2019s Thread\u201d)\\nchapt000: # chapter index(000-)\\nparag000: # paragraph index (000-)\\nstyle000: # style index (000-)\\ntime:\\n- 0.96 # start time [sec]\\n- 3.32 # end time [sec]\\nsent: \u201cIt happened one day.\u201d # sentence\\ntime: ...\\n```\\n\\n1. https://sites.google.com/site/shinnosuketakamichi/research-topics/j-mac_corpus\\n\\nWe did not evaluate the recording quality because we used commercial audiobook products that seem to have good recording quality.\"}"}
{"id": "takamichi22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following two processes are carried out. However, the accuracy of the alignment is not sufficient, so the alignment of such long audio tends to fail. Therefore, we re-align the long audio by dynamically splitting it using short audio, the alignment of such long audio tends to fail.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing. Audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nWe use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nWe retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nWe retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nWe retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs. We use CTC segmentation \\\\[19\\\\] using pre-trained end-to-end alignment models \\\\[23\\\\]. It uses CTC segmentation aligns the sentence timings in the audio given a ground-truth text. Using an end-to-end model enables alignment without language-dependent preprocessing.\\n\\nAfter SNR-based cleansing, we have pairs of structured texts and audio. We retrieve sentence-level alignments from these pairs."}
{"id": "takamichi22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4: Distribution of audiobooks per book (left) and speaker (right). Resulting corpus includes audiobooks read by different speakers and speaker who read multiple books.\\n\\nTable 2: Comparison of open-sourced audiobook corpora. Duration in \\\"hours.\\\"\\n\\n| Lang. | Corpus      | Duration | #speakers | Professional | Parallel |\\n|-------|-------------|----------|-----------|--------------|----------|\\n| En    | Blizzard2018 [12] | 6.5      | 1         | Yes          | No       |\\n| En    | Blizzard2013 [22] | 300      | 1         | Yes          | No       |\\n| En    | LibriTTS [23] | 585      | 2,456     | No           | Yes      |\\n| Fr    | SynPaFlex [24] | 87       | 1         | No           | No       |\\n| Multi | TUNDRA [25] | 60       | 14        | No           | No       |\\n| Ja    | J-KAC [15] | 9        | 1         | Yes          | No       |\\n| Ja    | J-MAC (ours) | 32       | 39        | Yes          | Yes      |\\n\\n3.2. Evaluation in background music estimation\\n\\nWe used a pre-trained model of Spleeter 9 for deep learning-based vocal-instrumental separation. We used py-webrtcvad 10 for signal processing-based VAD. Figure 5 shows the results of SNR calculation. The SNR ranged from 0 dB (i.e., voices and instrumentals are similar in volume.) to 100 dB (very clean). The threshold was determined manually because it was difficult to determine it automatically due to its gentle variation. The \\\"Clean\\\" audiobooks proceeded to segmentation, as described in the next subsection.\\n\\n3.3. Evaluation in segmentation\\n\\nWe used an recurrent neural network (RNN)-based CTC alignment model provided by ESPnet [26]. The model was trained using the CSJ corpus [27]. The CTC score improvement by realignment converged until three iterations. The 5-best sentences on the CTC scores were used to split audio in realignment. Figure 6 shows the CTC scores. The realignment significantly improved the CTC scores around $-4.0$ to $-0.2$ and contributed to selecting more well-aligned audiobooks. Even after realignment, the CTC scores of certain audiobooks were significantly lower (<$-4.0$). We examined these audiobooks and found that they were translations of Japanese books into English. Although the text and audio do not basically correspond, some named entities do (e.g., readings of the main character's name in Japanese text is represented in English speech). The interesting finding is that these alignments did not fail, and CTC scores are obtained.\\n\\nFigure 7 shows histograms of timing shift by VAD. Most of the aligned timings were shifted, suggesting that CTC alignment alone can result in overlapping in the speech segment; our correction using VAD avoids this problem.\\n\\n3.4. Evaluation in speech synthesis\\n\\nFinally, we evaluated our corpus on an audiobook speech synthesis task using multiple synthesis methods, audiobooks, and\\n\\n[9] https://github.com/deezer/spleeter\\n[10] https://github.com/wiseman/py-webrtcvad\\n\\nTo train on a multi-speaker corpus, speaker lookup embedding was used.\\n\\nWe downsampled audio data to 22.05 kHz, segmented them into a sentence level, and split them into training and validation sets in advance. The sizes of the training and validation sets were 14,043 and 100 samples, respectively.\\n\\nJulius [29] was used to obtain alignments between utterances and phoneme sequences. The generated melspectrogram configurations were 80 dimensions, with frame length of 1,024 samples and frame shift of 256 samples.\\n\\nWe used the pre-trained BERT model trained using Japanese Wikipedia data. The model configuration is equivalent to BERT-tiny [30]. However, we did not perform pre-trained distillation. The BERT weights are unfreezed upon training except for the embedding layer as not all vocabulary appear on the training set.\\n\\nFor optimization, we used the Adam optimizer with $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.99$. Learning rate scheduling was applied in the same manner as in a previous study [28] with 4,000 warmup steps, and the batch size was 64. For the loss function, we used the mean square error of each output from a variance adapter and the melspectrogram of the model output and decoder output (i.e. output before going through Postnet). The\"}"}
{"id": "takamichi22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I have no name. I am a cat. I have no name.\\n\\nEncoder\\nCurr\\n. sent.\\nMel\\n-\\nspectrogram\\nBERT\\n\\nI\\nhave\\nno\\nname\\nI\\nhave\\nno\\nname\\n\\nDecoder\\n&\\nPostnet\\nVariance\\nadaptor\\nSingleSentence\\nSpeaker ID\\nSpeaker\\nembedding\\n\\n3.29 / 3.41\\n3.30 / 3.45\\n0.009\\n3.01 / 3.14\\n3.64 / 3.73\\n3.45 / 3.29\\n3.21 / 3.24\\np\\n0.000\\n2.97 / 3.08\\nAve.\\n0.000\\n3.20 / 3.18\\n0.048\\n3.24 / 3.35\\n2.97 / 3.19\\n3.08 / 3.00\\n3.53 / 3.00\\n3.24 / 3.30\\n0.041\\n3.35 / 3.29\\n3.15 / 3.21\\n3.42 / 3.52\\n3.48 / 3.27\\n3.33 / 3.38\\n3.36 / 3.35\\n3.25 / 3.16\\n3.23 / 3.57\\n\\nThese results, we can provide the following insights.\\n\\n260 listeners participated to the MOS test and each of books (all were developed from the same source (Aozora project). The test set had no overlap from the training and validation sets, but all the sets belonged to the same domain (instantiation, educational, and search purpose only).\\n\\nThe amount of training data for the low-scoring books is more than those for \u201cm1\u201d and \u201cm3\u201d (1 audiobook, respectively), so we cannot say that the reason for this result is the amount of training data alone.\\n\\nConsidering multiple sentences slightly improves naturalness (vs. 3.45 by 3.43). As mentioned above, the domains of these books are different (\u2018akazukin\u2019, \u2018donguri\u2019, and \u2018tsuchigami,\u2019 which are labeled with 2361 book1\u2013\u201cbook4,\u201d respectively) selected from the J-KAC corpus. The test set was four audiobooks (\u201cakai\u201d, \u201cmomiji\u201d, \u201cakazukin\u201d, \u201cdonguri\u201d, and \u201ctsuchigami,\u201d which are labeled with \u201cbook1\u201d\u2013\u201cbook4,\u201d respectively) selected from the J-KAC corpus. The test set was four audiobooks (\u201cakai\u201d, \u201cmomiji\u201d, \u201cakazukin\u201d, \u201cdonguri\u201d, and \u201ctsuchigami,\u201d which are labeled with \u201cbook1\u201d\u2013\u201cbook4,\u201d respectively).\\n\\nThe disentanglement between synthesis methods, speakers, and books, and the same extent regardless of speaker, and that there is an entanglement between synthesis methods, speakers, and books, and the interaction among each factor, except for the method * speaker. These results also indicate interaction of two or three factors. Gray box indicates worst values. \u201cAve.\u201d rows and columns indicate average of \u201cSingleSentence\u201d and \u201cMultiSentences.\u201d Gray boxes are worst values.\\n\\nWe conducted a mean opinion score (MOS) evaluation on the naturalness of audiobook speech. We explored the effect of the naturalness of audiobook speech. We evaluated the corpus in terms of naturalness of audiobook speech. We explored the effect of the naturalness of audiobook speech. We evaluated the corpus in terms of naturalness of audiobook speech. We explored the effect of the naturalness of audiobook speech. We evaluated the corpus in terms of naturalness of audiobook speech. We explored the effect of the naturalness of audiobook speech. We evaluated the corpus in terms of naturalness of audiobook speech. We explored the effect of the naturalness of audiobook speech. We evaluated the corpus in terms of naturalness of audiobook speech.\\n\\nWe used HiFiGAN [31] with the weights distributed on official code implementation. Specifically, we used the UNIVERSE code implementation. We used HiFiGAN [31] with the weights distributed on official code implementation.\\n\\nThe HiFiGAN is an open-sourced library for generating high-quality speech by mel spectrogram predictions.\\n\\nWe did not perform finetuning of the HiFiGAN Encoder. We did not perform finetuning of the HiFiGAN Encoder.\\n\\nTable 4 shows the results of the analysis of variance (ANOVA). Statistical significance was observed in each of synthesis methods, speakers, and books, and the interaction of the three factors. Specifically, we used the UNIVERSE code implementation.\\n\\nTable 4:\\n\\n| Factor                        | p-value |\\n|-------------------------------|---------|\\n| method * speaker              |         |\\n| method * book                 |         |\\n| speaker * book                |         |\\n\\nAcknowledgements: This work is also supported by JSPS KAKENHI 19H01116, 21H04900, and JST, Moonshot R&D Program (Grant Number JPMJPS2011).\"}"}
