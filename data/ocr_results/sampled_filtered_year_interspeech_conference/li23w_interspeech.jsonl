{"id": "li23w_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Few-shot Class-incremental Audio Classification Using Stochastic Classifier\\n\\nYanxiong Li, Wenchang Cao, Jialong Li, Wei Xie, Qianhua He\\nSchool of Electronic and Information Engineering, South China University of Technology, Guangzhou, China\\neeyxli@scut.edu.cn, eewenchangcao@mail.scut.edu.cn\\n\\nAbstract\\nIt is generally assumed that the number of classes is fixed in current audio classification methods, and the model can recognize pre-given classes only. When new classes emerge, the model needs to be retrained with adequate samples of all classes. If new classes continually emerge, these methods will not work well and even be infeasible. In this study, we propose a method for few-shot class-incremental audio classification, which continually recognizes new classes and remembers old ones. The proposed model consists of an embedding extractor and a stochastic classifier. The former is trained in base session and frozen in incremental sessions, while the latter is incrementally expanded in all sessions. Two datasets (NS-100 and LS-100) are built by choosing samples from audio corpora of NSynth and LibriSpeech, respectively.\\n\\nResults show that our method exceeds four baseline ones in average accuracy and performance dropping rate.\\n\\nCode is at https://github.com/vinceasvp/meta-sc.\\n\\nIndex Terms: few-shot learning, incremental learning, audio classification, stochastic classifier, prototype vector\\n\\n1. Introduction\\nAudio classification aims to recognize different types of sounds in environments, which is a hot topic with wide applications, such as multimedia analysis [1], audio captioning [2], traffic surveillance [3], bio-acoustic monitoring [4], automatic assisted driving [5], and smart home [6].\\n\\nSome works were done on audio classification in recent years [7]\u2013[11]. In these works, it was generally assumed that the vocabulary of audio classes was fixed and pre-known. Hence, the trained model can recognize the pre-given classes only. To recognize new classes, the model needs to be retrained with abundant labeled samples of all classes. Retraining the model is quite time-consuming and laborious. If the samples of old classes are unavailable due to data privacy and memory shortage, finetuning the model with samples of new classes will make the model quickly forget old ones (catastrophic forgetting problem [12]). However, in many applications, the vocabulary of audio classes dynamically expands. For example, users of intelligent audio devices (e.g., smart speakers) often want to add new classes, such as abnormal sounds, or audio wake-up words.\\n\\nTo reduce the requirement for the amount of training samples, the methods of few-shot sound classification (recognition or detection) were proposed [13]\u2013[18]. In these methods, the model can recognize new classes from a few training samples [19], [20], but they cannot remember old classes. To continually recognize new classes and remember old ones, the incremental learning [21], [22] based methods for audio classification (or detection) were proposed [23]\u2013[27]. Although these methods can recognize new and old classes, they still have shortcomings. For instance, abundant samples of new classes are required to update the model. This requirement is difficult to meet in practice, since the labeled samples of some new classes are few.\\n\\nRecently, a learning paradigm, Few-Shot Class-Incremental Learning (FSCIL) was proposed [28], [29], which aims to continually expand the model for recognizing new classes with few training samples and meanwhile remembering old ones. The FSCIL faces two main problems: overfitting to new classes because of very few samples of new classes, and catastrophic forgetting of old classes due to absence of samples of old classes in incremental sessions. To address these two problems, a decoupled learning strategy was proposed, in which the feature extractor and the classifier (two major components of the model) were trained independently [28]\u2013[34]. Wang et al. applied the Dynamical Few-Shot Learning (DFSL) [35] to audio classification, which is the latest work related to the Few-shot Class-incremental Audio Classification (FCAC). Although these works above have merits, they still need to be improved for tackling the two problems faced by the FSCIL and FCAC. For example, their classifier consists of prototypes (a prototype represents a class) and each prototype is a deterministic vector. Each deterministic prototype vector might not represent the new class effectively, since it is learnt from few training samples of one new class. Hence, the classifier consisting of deterministic prototype vectors (called deterministic classifier here) might not obtain discriminative decision boundaries over all classes, and would not perform well for audio classification.\\n\\nInspired by the success of Stochastic Classifier (SC) in computer vision [36], we propose a FCAC method using a SC here. The SC is trained for generating prototypes and each prototype is sampled from a distribution which is determined by mean vectors and variance vectors. Each prototype is a mean vector with a margin (variance vector). Hence, many prototypes around the mean vector can be obtained and at least one of these prototypes is expected to represent the new class well. As is done in prior works, the embedding extractor is frozen in incremental sessions after training in base session, while the SC is dynamically expanded in incremental sessions for continually recognizing new classes. Two datasets, named NS-100 and LS-100 are constructed by choosing samples from two public audio corpora of the NSynth and LibriSpeech, respectively. To reproduce our experiments, the construction details (such as metadata, explanations) of these two datasets are introduced at https://github.com/vinceasvp/meta-sc. Results indicate that our method outperforms four state-of-the-art methods in terms of Average Accuracy (AA) and Performance Dropping rate (PD).\\n\\nIn conclusion, the main contributions of this study are as follows. First, we design a SC which is expanded in incremental sessions and is adopted to generate prototypes for continually recognizing new classes. Second, we propose a FCAC method whose effectiveness is evaluated on two audio datasets.\\n\\n2. Method\\nIn this section, we describe our method in detail, including problem definition, the framework of our method, and SC.\"}"}
{"id": "li23w_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Training EE in supervised way\\n\\nExtracting embeddings by EE\\n\\nComputing mean and variance vectors of classifier. For a given input sample $x$, the prototypes is used to calculate the class score for a specific class $C_i$. The class score is proportional to the cosine similarity between the embedding $f_{\\\\theta}(x)$ and the mean vector $\\\\mu_\\\\theta$ of the classifier:\\n\\n$$\\\\cos(f_{\\\\theta}(x), \\\\mu_\\\\theta) = \\\\frac{x \\\\cdot \\\\omega}{\\\\|x\\\\| \\\\|\\\\omega\\\\|}$$\\n\\n(5)\\n\\nand the $\\\\omega$ stands for the weight of SC; and $\\\\mu_\\\\theta$ and $\\\\sigma_\\\\theta$ stands for the mean vector and variance vector of the classifier. For a given input sample $x$, the class score of the SC is proportional to the cosine similarity between the embedding $f_{\\\\theta}(x)$ and the mean vector $\\\\mu_\\\\theta$ of the classifier:\\n\\n$$\\\\cos(f_{\\\\theta}(x), \\\\mu_\\\\theta) = \\\\frac{x \\\\cdot \\\\omega}{\\\\|x\\\\| \\\\|\\\\omega\\\\|}$$\\n\\n(4)\\n\\nwhere $\\\\omega = \\\\theta \\\\times \\\\mathbb{1} - \\\\theta \\\\times \\\\mathbb{0}$.\\n\\nIn the base session, we first train an EE in a supervised way episodically trained on each batch using the loss:\\n\\n$$\\\\ell(\\\\theta (\\\\cdot)) = -\\\\log p_{\\\\theta}(y | x)$$\\n\\nwhere $p_{\\\\theta}(y | x)$ is the probability of classifying the input sample $x$ and its corresponding label $y$.\\n\\nAs shown in Figure 1, the framework of our method includes the training and testing datasets.\\n\\n**Base session (Session 0) and Incremental session (Sessions 1 to $M$)***\\n\\nThe framework of our method consists of two kinds of sessions: base and incremental sessions.\\n\\nEE: Embedding Extractor.\\n\\nTrained stochastic classifier $f_{\\\\theta}$ which has abundant samples. Then, the EE is frozen for recognizing new samples.\\n\\nEach batch consists of a support set and a query set. The SC is trained on each batch using the loss:\\n\\n$$\\\\ell(\\\\theta (\\\\cdot)) = -\\\\log p_{\\\\theta}(y | x)$$\\n\\nwhere $p_{\\\\theta}(y | x)$ is the probability of classifying the input sample $x$ and its corresponding label $y$.\\n\\nIn incremental session, the SC is expanded (trained) to recognize new classes. In this study, cosine similarity between the embeddings and prototypes is used to calculate the class score for a specific class $C_i$. The prototype of a deterministic classifier $f_{\\\\theta}(\\\\cdot)$ can be correctly classified to class $C_i$ if the region where $f_{\\\\theta}(\\\\cdot)$ is similar to $\\\\mu_{C_i}$ is large. Figure 2 (a) shows the embedding space of a deterministic classifier.\\n\\nUsing a SC trained on the training dataset $\\\\mathcal{D}_0$ composed of adequate samples per class for model training. In contrast, each $\\\\mathcal{D}_m$ is a small-scale dataset, and is composed of $\\\\mathcal{D}_0$ and $\\\\cdots$ $\\\\mathcal{D}_{m-1}$.\\n\\nThe dataset in different sessions do not have the same type of classes, namely $\\\\cup_{m=1}^{\\\\infty} \\\\mathcal{D}_m = \\\\{e\\\\}$, respectively.\\n\\nThe training and testing datasets of various sessions are denoted by $\\\\mathcal{D}_m$ (session $0$) and $\\\\mathcal{D}_m$ (session $m$), respectively.\\n\\nThe training and testing dataset of current and all prior sessions, namely $\\\\mathcal{L}_m$, have the same type of classes, namely $\\\\{e\\\\}$, respectively.\\n\\nThe SC includes $m$ incremental sessions, namely one base session and $m-1$ incremental sessions.\\n\\nThere are three steps in the base and incremental sessions. There are three steps in the base session:\\n\\n1. Training EE\\n2. Training SC\\n3. Producing prototypes\\n\\nIn the base session, we first train an EE in a supervised way episodically trained on each batch using the loss:\\n\\n$$\\\\ell(\\\\theta (\\\\cdot)) = -\\\\log p_{\\\\theta}(y | x)$$\\n\\nwhere $p_{\\\\theta}(y | x)$ is the probability of classifying the input sample $x$ and its corresponding label $y$.\\n\\nNext, we use the backbone of a ResNet18 as the EE here. For extracting embedding in audio and visual processing tasks [38], [39], we use the backbone of a ResNet [37] for extracting embedding from Log Mel-spectra of samples. Inspired by the success of ResNet [37] for extracting embedding in audio and visual processing tasks [38], [39], we use the backbone of a ResNet [37] for extracting embedding from Log Mel-spectra of samples. Inspired by the success of ResNet [37] for extracting embedding in audio and visual processing tasks [38], [39], we use the backbone of a ResNet [37] for extracting embedding from Log Mel-spectra of samples.\"}"}
{"id": "li23w_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( \\\\hat{\\\\mu} = \\\\mu + \\\\mathcal{N}(0, 1) \\\\odot \\\\sigma \\\\) is used to sample the prototypes.\\n\\nAs shown in Figure 2 (b), classification boundary turns into a margin determined by \\\\( f_{\\\\mathcal{U}} \\\\) and \\\\( f_{\\\\mathcal{V}} \\\\). When \\\\( f_{\\\\mathcal{E}}(x) \\\\) locates in this margin, we can only have an inexplicit prediction that \\\\( f_{\\\\mathcal{E}}(x) \\\\) maybe belongs to class \\\\( j \\\\), which means more powerful punishment is put into effect in next training iteration and enforce the EE to produce more informative embeddings in base session. Moreover, the SC allows distinguishing between the epistemic uncertainty and the aleatoric uncertainty [45].\\n\\nBenefiting from the distinguishing feature, new class will have high epistemic uncertainty due to scarcity of training samples. Then, the problem of overfitting in new class will be alleviated.\\n\\nFigure 2: The classification boundary between two classes in (a) deterministic classifier, (b) stochastic classifier. The margin in (b) leads to more discriminative classification boundaries.\\n\\n3. Experiments\\n\\n3.1. Experimental datasets\\n\\nExperiments are done on the datasets chosen from two audio corpora, including NSynth [41] and LibriSpeech [42]. These two audio corpora can be publicly accessed and are widely used in prior works. There are 306,043 audio snippets in the NSynth, and each audio snippet is of four seconds to represent one type of musical instruments. There are 1,006 musical instruments (classes). The LibriSpeech is a speech corpus of approximately 1,000 hours of audiobooks uttered by 2,484 speakers (classes). The datasets chosen from the NSynth and LibriSpeech, are labeled as NS-100 and LS-100, respectively. These datasets are split into two parts without overlaps of classes, namely \\\\( D_0 \\\\) and \\\\( D_m \\\\) (\\\\( 1 \\\\leq i \\\\leq (M-1) \\\\)). \\\\( D_0 \\\\) (or \\\\( D_m \\\\)) is composed of training dataset \\\\( D_{\\\\mathbb{B}} \\\\) (or \\\\( D_{\\\\mathbb{A}} \\\\)) and testing dataset \\\\( D_{\\\\mathbb{B}} \\\\) (or \\\\( D_{\\\\mathbb{A}} \\\\)). In each episodic training step, \\\\( N \\\\times K \\\\) samples are randomly chosen from \\\\( N \\\\) classes (\\\\( K \\\\) samples per class) in the training dataset to generate support set, and then \\\\( N \\\\times K \\\\) different samples of the same \\\\( N \\\\) classes (\\\\( K \\\\) samples per class) are randomly chosen from the training dataset to construct the query set. The selections of classes and samples per class are repeated until all classes and samples in the training dataset are chosen once. The samples in various batches are different to each other. In testing step, all testing datasets are input to the EE and classifier as a whole. Table 1 lists the detailed information of NS-100/LS-100.\\n\\n| Parameters | NS-100 | LS-100 |\\n|------------|--------|--------|\\n| #Classes   | 55/60  | 55/60  |\\n| #Samples   | 11000/300000 | 5500/6000 |\\n| Length (hours) | 12.23/16.66 | 1.52/3.33 |\\n| #Samples/class | 200/500 | 100/100 |\\n\\n#Classes: number of samples per class. Numbers of the left and right of the slash are corresponding values of NS-100 and LS-100, respectively.\\n\\n3.2. Experimental setup\\n\\nAccuracy is defined as the number of correctly classified samples divided by total number of samples involved in classification. It is used to measure the performance of various methods in each session. Both AA and PD are used to assess overall performance of different methods, and defined by\\n\\n\\\\[\\nAA = \\\\frac{1}{\\\\mathcal{N}} \\\\sum_{i=1}^{\\\\mathcal{N}} A_{i}^{\\\\mathbb{B}} - A_{i}^{G}\\n\\\\]\\n\\n(6)\\n\\n\\\\[\\nPD = A^{\\\\mathbb{B}} - A^{G}\\n\\\\]\\n\\n(7)\\n\\nwhere \\\\( A_m \\\\) stands for the accuracy in session \\\\( m \\\\). Frame length and frame overlapping are 25 ms and 10 ms, respectively. Dimensions of log Mel-spectrum and prototypes are 128 and 512, respectively. The coefficient \\\\( \\\\lambda \\\\) is experimentally set to 0.9 and 0.6 for LS-100 and NS-100, respectively.\\n\\n3.3. Experimental results\\n\\nIn the first experiment, we assess the effectiveness of our method with two classifiers on LS-100. The results obtained by our method with stochastic or deterministic classifiers are listed in Table 2. When SC is used, our method obtains higher AA scores for the classes of Base, Incremental and All (Base + Incremental). That is, SC outperforms deterministic classifier when they are adopted in our method for audio classification on LS-100.\\n\\nTable 2: Average accuracy scores obtained by our method with deterministic classifier and stochastic classifier on LS-100.\\n\\n| Classes  | Deterministic classifier | Stochastic classifier |\\n|----------|--------------------------|-----------------------|\\n| Base     | 91.88%                   | 92.13%                |\\n| Incremental | 74.52%              | 77.41%                |\\n| All      | 87.37%                   | 88.39%                |\\n\\nIn the second experiment, we discuss the influence of the values of \\\\( N \\\\) and \\\\( K \\\\) on the performance of our method. The accuracy of the last session obtained by our method on LS-100 is given in Figure 3, from which three observations can be obtained. First, when \\\\((N, K)\\\\) is equal to \\\\((20, 20)\\\\), our method obtains the highest accuracy score of 88.48%. Second, for the same number of ways, the larger the number of shots, the higher the accuracy scores. It is probably that with the increase of shots, the model acquires more information about new classes and obtains higher accuracy scores. Third, for the same number of shots, when the number of ways is equal to 20, our method obtains the highest accuracy score (except 5-way 1-shot). When the value of \\\\( N \\\\) deviates from 20, accuracy scores decrease. The possible reasons are as follows. When the number of ways decreases, the number of incremental sessions will increase and the old classes are more likely to be forgotten. When the number of ways increases, the number of new classes in one incremental session will increase and the confusions between new classes in each session is more likely to increase.\\n\\nFigure 3: Accuracy of the last session obtained by our method on LS-100.\"}"}
{"id": "li23w_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Results obtained by different methods on LS-100. Incr. denotes incremental classes.\\n\\n| Methods | Session | Base | Incr. | All |\\n|---------|---------|------|-------|-----|\\n| Finetune | 0 | 92.73 | 92.73 | 91.72 |\\n| iCaRL | 1 | 92.34 | 92.34 | 91.67 |\\n| DFSL | 2 | 89.53 | 77.74 | 86.53 |\\n| CEC | 3 | 74.99 | 92.48 | 87.85 |\\n\\nThe AA and PD under the same conditions. Second, we designed a graph model is designed to propagate the contextual information between classifiers for updating prototypes.\\n\\nIn this paper, we discussed the FCAC problem, and tried to address it using a SC. Based on the description of our method and experimental evaluations, we can draw two conclusions.\\n\\nFirst, our method outperforms state-of-the-art methods in both AA and PD. The advantage of our method over all baseline methods. These methods are marked as Finetune [43], iCaRL [44], DFSL [35], and CEC [32]. In the Finetune method, data retention and knowledge distillation are used to overfit the new classes and forget the old ones. In the iCaRL method, data retention and knowledge distillation are used to overfit the new classes and forget the old ones. In the CEC method, continually evolving classifiers are adopted for identifying new classes and used for realizing FCAC. In the CEC method, continually evolving classifiers are adopted for identifying new classes and used for realizing FCAC. In the Finetune method, data retention and knowledge distillation are used to overfit the new classes and forget the old ones. In the CEC method, continually evolving classifiers are adopted for identifying new classes and used for realizing FCAC. In the CEC method, continually evolving classifiers are adopted for identifying new classes and used for realizing FCAC.\\n\\nConclusions\\n\\n5. Acknowledgements\\n\\nThis work was supported by the national natural science foundation of China (62111530145, 61771200), international scientific research collaboration project of Guangdong (2021A0505030003), and Guangdong basic and applied basic research foundation (2021A1515011454, 2022A1515011687).\"}"}
{"id": "li23w_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] Y. Li, Y. Zhang, X. Li, M. Liu, W. Wang, and J. Yang, \u201cAcoustic event diarization in TV/movie audios using deep embedding and feature alignment,\u201d IEEE Computational Intelligence Magazine, vol. 17, no. 2, pp. 29-48, 2022.\\n\\n[2] E. Koh, F. Saki, Y. Guo, C. Hung, and E. Visser, \u201cIncremental classifier and representation learning,\u201d in Proc. of IEEE ICASSP, 2022, vol. 58055, pp. 33999-34025, 2019.\\n\\n[3] Y. Li, X. Li, Y. Zhang, M. Liu, and W. Wang, \u201cAnomalous sound captioning dataset,\u201d in Proc. IEEE ICASSP, 2021, pp. 2544-2553.\\n\\n[4] A. Terenzi, N. Ortolani, I. Nolasco, E. Benetos, and S. Cecchi, \u201cOverview and evaluation of sound event localization and detection in DCASE 2019,\u201d Proc. IEEE MultiMedia, vol. 5, pp. 101-108, 2022.\\n\\n[5] D. Barchiesi, D. Giannoulis, D. Stowell, and M.D. Plumbley, \u201cComparison of feature extraction methods for sound-based classification of honey bee activity,\u201d IEEE/ACM TASLP, vol. 10, pp. 684-694, 2012.\\n\\n[6] Y. Zeng, Y. Li, Z. Zhou, R. Wang, and D. Lu, \u201cDomestic activities recognition,\u201d in Proc. of IEEE ICASSP, 2020, vol. 32, no. 3, pp. 112-122, 2022.\\n\\n[7] T. Iqbal, Y. Cao, Q. Kong, M. D. Plumbley, and W. Wang, \u201cTransient sound events using attentional similarity for few-shot classification with attentional graph neural networks,\u201d in Proc. of IEEE ICASSP, 2019, pp. 112-122, 2022.\\n\\n[8] A. Politis, A. Mesaros, S. Adavanne, T. Heittola, and T. Virtanen, \u201cTask incremental learning using a few examples,\u201d in Proc. of AAAI, 2020, pp. 12180-12189.\\n\\n[9] K. Choi, M. Kersner, J. Morton, and B. Chang, \u201cTemporal interference,\u201d in Proc. IEEE WASPAA, 2020, pp. 308-312.\\n\\n[10] M. Mohaimenuzzaman, C. Bergmeir, and B. Meyer, \u201cPruning vs knowledge distillation for on-device audio classification,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[11] Y. Li, M. Liu, K. Drossos, and T. Virtanen, \u201cSound event detection with few data,\u201d in Proc. of IEEE ICASSP, 2020, vol. 33, pp. 1-15.\\n\\n[12] H. Shin, J. Kwon Lee, J. Kim, and J. Kim, \u201cContinual learning with static memory for audio classification without catastrophic forgetting,\u201d in Proc. of IEEE/CVF CVPR, 2020, vol. 45, no. 3, pp. 2945-2951, 2023.\\n\\n[13] S. Zhang, Y. Qin, K. Sun, and Y. Lin, \u201cFew-shot audio classification from audio recordings using multi-scale dilated depthwise separable convolutional network,\u201d in Proc. of NeurIPS, 2019.\\n\\n[14] S. Chou, K. Cheng, J. R. Jang, and Y. Yang, \u201cLearning to match of out-of-distribution data for audio classification,\u201d in Proc. of IEEE ICASSP, 2018, pp. 770-774.\\n\\n[15] J. Pons, J. Serra, and X. Serra, \u201cTraining neural audio classifiers for audio surveillance of roads,\u201d in Proc. IEEE/CVF CVPR, 2017, pp. 4078-4088.\\n\\n[16] Y. Wang, N. J. Bryan, M. Cartwright, J. Pablo Bello, and J. Pe\u00f1a, \u201cFew-shot sound recognition,\u201d in Proc. of NeurIPS \u00e0 2020, vol. 33, pp. 1068-1077.\\n\\n[17] Y. Wang, D. V. Anderson, \u201cHybrid attention-based deep generative replay,\u201d in Proc. of IEEE/CVF CVPR, 2021, pp. 6960-6970.\\n\\n[18] D. Yang, H. Wang, Y. Zou, Z. Ye, and W. Wang, \u201cA mutual learning algorithm for sound event detection using deep audio representation and a BLSTM network,\u201d in Proc. of IEEE ICASSP, 2020, pp. 1-8. doi: 10.1109/MMUL.2022.3208923.\\n\\n[19] J. Snell, K. Swersky, and R. Zemel, \u201cPrototypical networks for few-shot learning,\u201d in Proc. of ICML, 2017, pp. 5206-5210.\\n\\n[20] C. Finn, P. Abbeel, and S. Levine, \u201cModel-agnostic meta-learning for fast adaptation of deep networks,\u201d in Proc. of ICML, 2017, vol. 70, pp. 1-15.\\n\\n[21] H.J. Chen, A.C. Cheng, D.C. Juan, W. Wei, and M. Sun, \u201cAdaptive aggregation networks for class-incremental learning,\u201d in Proc. of NeurIPS, 2017, pp. 2990-2999.\\n\\n[22] Y. Liu, B. Schiele, and Q. Sun, \u201cAdaptive aggregation networks for class-incremental learning,\u201d in Proc. of NeurIPS, 2017, pp. 2990-2999.\\n\\n[23] E. Koh, F. Saki, Y. Guo, C. Hung, and E. Visser, \u201cIncremental classifier and representation learning,\u201d in Proc. of IEEE CVPR, 2017, pp. 5533-5542.\\n\\n[24] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, \u201cFew-shot sound event detection using deep audio representation and a BLSTM network,\u201d in Proc. of IEEE/CVF CVPR, 2019, pp. 16-20.\\n\\n[25] M.A. Hussain, C.-L. Lee, and T.-H. Tsai, \u201cAn efficient incremental learning algorithm for sound event detection,\u201d in Proc. of IEEE ICASSP, 2019, pp. 16-34, 2015.\\n\\n[26] S. Karam, S.-J. Ruan, and Q.M.u. Haq, \u201cTask incremental learning without forgetting,\u201d in Proc. of AAAI, 2020, pp. 897-906.\\n\\n[27] M.A. Hussain, C.-L. Lee, and T.-H. Tsai, \u201cAn efficient incremental learning algorithm for sound classification,\u201d in Proc. IEEE Consumer Electronics Magazine, vol. 10, pp. 4367-4375, 2018.\\n\\n[28] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, \u201cFew-shot sound event detection using deep audio representation and a BLSTM network,\u201d in Proc. of IEEE/CVF CVPR, 2019, pp. 16-20.\\n\\n[29] A. Ayub, and A.R. Wagner, \u201cCognitively-inspired model for sound event diarization in TV/movie audios using deep embedding and feature alignment,\u201d IEEE Computational Intelligence Magazine, vol. 17, no. 2, pp. 29-48, 2022.\\n\\n[30] S. Gidaris, and N. Komodakis, \u201cDynamic few-shot visual learning with dynamic support network for few-shot class incremental learning,\u201d in Proc. of IEEE/CVF CVPR, 2020.\\n\\n[31] P. Mazumder, P. Singh, and P. Rai, \u201cFew-shot lifelong learning,\u201d in Proc. of IEEE/CVF CVPR, 2020, vol. 33, pp. 5138-5146.\\n\\n[32] A. Cheraghian, S. Rahman, P. Fang, S. K. Roy, L. Petersson, and S. Dubreuil, \u201cDynamic support network for few-shot class incremental learning,\u201d in Proc. of IEEE/CVF CVPR, 2020.\\n\\n[33] C. Zhang, N. Song, G. Lin, Y. Zheng, P. Pan, and Y. Xu, \u201cFew-shot continual learning for audio classification,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[34] B. Yang, M. Lin, Y. Zhang, B. Liu, X. Liang, R. Ji, and Q. Ye, \u201cFew-shot continual learning for audio classification,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[35] Y. Wang, N. J. Bryan, M. Cartwright, J. Pablo Bello, and J. Pe\u00f1a, \u201cFew-shot sound recognition,\u201d in Proc. of NeurIPS, 2017, pp. 1-13.\\n\\n[36] Z. Lu, Y. Yang, X. Zhu, C. Liu, Y. -Z. Song and T. Xiang, \u201cDynamic support network for few-shot class incremental learning,\u201d in Proc. of IEEE/CVF CVPR, 2020.\\n\\n[37] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image classification,\u201d in Proc. of IEEE ICASSP, 2016, pp. 6707, 2022.\\n\\n[38] X. Shi, E. Cooper, and J. Yamagishi, \u201cUse of speaker recognition for class-incremental learning,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[39] R. Qian, T. Meng, B. Gong, M.H. Yang, H. Wang, S. Belongie, \u201ciCaRL: Incremental classifier and representation learning,\u201d in Proc. of NeurIPS, 2017, pp. 2990-2999.\\n\\n[40] D. Kingma, and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Proc. of ICLR, 2015.\\n\\n[41] J. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. of IEEE/CVF CVPR, 2017, pp. 5533-5542.\\n\\n[42] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C.H. Lampert, \u201ciCaRL: Incremental classifier and representation learning,\u201d in Proc. of NeurIPS, 2017, pp. 2990-2999.\\n\\n[43] P. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa, \u201ciCaRL: Incremental classifier and representation learning,\u201d in Proc. of NeurIPS, 2017, pp. 2990-2999.\\n\\n[44] L.V. Jospin, H. Laga, F. Boussaid, et al., \u201cHands-on Bayesian deep generative replay,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[45] J. Engel, C. Resnick, A. Roberts, S. Dieleman, M. Norouzi, D. Eck, V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in Proc. of IEEE/CVF CVPR, 2017, pp. 2990-2999.\\n\\n[46] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, \u201cFew-shot sound event detection using deep audio representation and a BLSTM network,\u201d in Proc. of IEEE/CVF CVPR, 2019, pp. 16-20.\\n\\n[47] D. Ma, C. I. Tang, and C. Mascolo, \u201cImproving feature generalizability with multitask learning in class incremental learning,\u201d in Proc. of IEEE ICASSP, 2022, pp. 3649-3653.\\n\\n[48] S. Karam, S.-J. Ruan, and Q.M.u. Haq, \u201cTask incremental learning without forgetting,\u201d in Proc. of IEEE/CVF CVPR, 2021.\\n\\n[49] D. Barchiesi, D. Giannoulis, D. Stowell, and M.D. Plumbley, \u201cComparison of feature extraction methods for sound-based classification of honey bee activity,\u201d IEEE/ACM TASLP, vol. 30, pp. 112-122, 2022.\"}"}
