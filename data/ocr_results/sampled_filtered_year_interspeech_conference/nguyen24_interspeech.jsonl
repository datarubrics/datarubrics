{"id": "nguyen24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models\\n\\nMinh Nguyen1,2,\u2217, Franck Dernoncourt3, Seunghyun Yoon3, Hanieh Deilamsalehy3, Hao Tan3, Ryan Rossi3, Quan Hung Tran3, Trung Bui3, Thien Huu Nguyen1\\n\\n1Department of Computer Science, University of Oregon, USA\\n2AWS AI Labs, USA\\n3Adobe Research, USA\\n\\nminhnv@cs.uoregon.edu, {franck.dernoncourt,syoon,deilamsa,hatan,ryrossi,quanthdhcn,bui}@adobe.com, thien@cs.uoregon.edu\\n\\nAbstract\\n\\nWe introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in speech recognition, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a precision of 80.3%, setting a new benchmark for SpeakerID. The data and code are publicly available here: https://github.com/adobe-research/speaker-identification\\n\\nIndex Terms: speaker identification, dialogue transcripts, pre-trained language models\\n\\n1. Introduction\\n\\nThe rapid expansion of dialogue-centric content across various media platforms, including television programs, online meetings, and radio podcasts, has significantly heightened user interest in accessing and exploring these rich sources of information and entertainment. In response to this growing demand, leading archival platforms and organizations such as YouTube, France's National Audiovisual Institute, and the British Broadcasting Corporation have dedicated considerable efforts towards the efficient storage and indexing of such content, facilitating its retrieval [1, 2, 3]. Within this context, the challenge of accurately identifying speakers within dialogues\u2014a process known as Speaker Identification (SpeakerID)\u2014has emerged as a pivotal area of research. SpeakerID involves the task of recognizing and distinguishing between the voices of different speakers within an audio or video segment, aiming to assign the correct speaker names to each spoken segment. This process is crucial for enhancing the accessibility and searchability of multimedia content, enabling users to find segments featuring specific speakers. As a result, the development of effective SpeakerID systems has attracted significant research efforts, as evidenced by a body of work [4, 5, 6, 7, 8], striving to overcome the challenges associated with this complex task.\\n\\nHistorically, SpeakerID research has predominantly focused on multimodal approaches, relying on both video/images and audio. Recognizing these limitations, some researchers have shifted towards text-based SpeakerID, leveraging dialogue transcripts to identify speaker names, where a model needs to identify names for speakers in a given dialogue transcript [4, 9, 10, 11]. An example for the task is presented in Table 1. In this example, the task involves analyzing the dialogue to identify where speakers introduce themselves or are mentioned by name, and then attributing those segments of dialogue to the correct individuals. The shift to the text-based SpeakerID has been bolstered by advances in speech recognition technologies [12, 13, 14, 15] and the emergence of pre-trained language models (PLMs) [16, 17], making the task increasingly viable. However, a notable gap remains: to our knowledge, no existing text-based SpeakerID research has utilized deep learning techniques or PLMs, and the scarcity of large-scale training datasets has further hindered model development.\\n\\nTo address such issues, we first propose a simple method to automatically obtain a high-quality large-scale training data for text-based SpeakerID from the popular MediaSum corpus [18], which contains transcripts for 463.6K media interviews from the National Public Radio (NPR) and the Cable News Network (CNN). In addition to the transcripts, each interview comes with the information of the involved speakers as shown in Figure 1. However, it is impossible for a text-based SpeakerID model to produce full names for the speakers (e.g., \\\"ALISYN CAMEROTA\\\"); the model needs to assign variants of the names (e.g., \\\"Alisyn\\\") mentioned in the transcript to the speakers. As such, we propose to perform text matching to find mentions of speaker names in a transcript and use such names as labels for the task.\\n\\nFurthermore, we propose novel transformer-based models for text-based SpeakerID. Our key observation is that speakers are often around when their names are mentioned during a meeting, i.e., it is often the case that we can assign a person name to the previous, current, or the next speaker. As such, we propose to represent the three possible speakers using their spoken sentences closest to the current sentence/utterance mentioning the name.\"}"}
{"id": "nguyen24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"New resurgence of the Taliban forces and instability of Afghan forces motivating this move but many more questions remain this morning. CNN's Joe Johns is live for us at the White House with details. What have you learned, Joe?\\n\\nAlisyn, the administration going through the status quo in Afghanistan. The president expected to announce a plan that is essentially the old plan.\\n\\nFigure 1: An example in the MediaSum dataset. In the SpeakerID setting, the speakers are not provided with their names at test time but their speaker identities such as \\\"speaker1\\\", \\\"speaker2\\\" produced by a speaker diarization system. A model performing SpeakerID needs to recover the actual names for the speakers based on the transcript.\\n\\nTo conduct experiments, we randomly sample a portion of our synthetic data generated from MediaSum and randomly split the data into train/dev/test sets. Experimental results show that our best model achieves a great performance with a precision of 80.3% on the test set, demonstrating the quality of our proposed dataset and models for text-based SpeakerID. For the sake of simplicity, we will refer to text-based Speaker Identification as simply \\\"SpeakerID\\\" from this point forward.\\n\\n2. Methodology\\n\\n2.1. Problem Definition\\n\\nWe formalize the text-based speaker identification (SpeakerID) as follows. Given a dialogue transcript with anonymized speaker identities (e.g., \\\"speaker1\\\", \\\"speaker2\\\") and person names (e.g., \\\"Paul Erickson\\\") for each sentence in the transcript, find the actual names for each anonymized speaker identity. Here, the transcript can be obtained for the dialogue by a speech-to-text system such as Fairseq S2T [15], the speaker identities for each sentence can be produced by a speaker diarization system such as SOND [20], the person names can be detected by a named entity recognition (NER) such as Trankit [21]. In this work, we assume such information is available for our SpeakerID models.\\n\\n2.2. Data Collection\\n\\nMediaSum Dataset: MediaSum is a large-scale dialogue summarization dataset that was created by [18]. It contains 463.6K interview-summary pairs from diverse news sources such as National Public Radio (NPR) [1] and Cable News Network (CNN) [2]. The interviews cover a wide range of topics/domains, including politics, entertainment, sports, and technologies. In addition, each utterance/sentence in the transcript is tagged with the information of the speaker, including their names, titles, and affiliations. An example of the MediaSum transcripts is shown in Figure 1.\\n\\nData processing: Given a MediaSum example, we perform the following steps to obtain training examples for SpeakerID:\\n\\n- Step 1: Detect person names in the transcript.\\n- Step 2: Anonymize the speaker information by replacing their actual names with speaker identities such as \\\"speaker1\\\", \\\"speaker2\\\".\\n- Step 3: Map the detected person names to the speaker identities via text matching. The names that do not match any speaker are assigned to a special speaker identity \\\"null\\\". In this process, we use the state-of-the-art NER model from Trankit [21], which achieves the state-of-the-art NER performance of 92.5 F1 on CoNLL English test set, to find spans for named entities in each sentence. Entities with the tag \\\"PERSON\\\" are considered as person names. The start and end tokens for each person name are then stored for the example. For the text matching between the person names and speaker actual names, we employ the Levenshtein Distance [22] to perform the fuzzy text matching. In particular, Levenshtein Distance is a method for measuring the similarity between two strings of characters. The method computes the minimum changes (i.e., insertions, deletions, or substitutions of individual characters) needed to transform one string into the other. The similarity between the two strings can be measured as:\\n\\n$$\\\\theta = \\\\frac{l_{\\\\text{sum}} - d}{l_{\\\\text{sum}}}$$\\n\\nwhere $l_{\\\\text{sum}}$ is the total length of the two strings and $d$ is the computed Levenshtein distance. As names of the speakers can vary slightly in the transcript (e.g., missing last name), we find out that names with the similarity of at least 0.8 can be effectively considered the same.\\n\\n2.3. Proposed Models\\n\\n2.3.1. Single-Name Model\\n\\nWe present our first model design for SpeakerID in Figure 2. In this model, we focus on a given person name $n$. For each occurrence of $n$, we identify the sentence or utterance $w_{\\\\text{cur}}$ in which $n$ appears. This sentence is produced by a speaker, denoted as $s_{\\\\text{cur}}$ (i.e., current speaker). We also consider the immediate dialogue context by identifying the sentences preceding and following $w_{\\\\text{cur}}$, labeled as $w_{\\\\text{prev}}$ and $w_{\\\\text{next}}$, along with their corresponding speakers $s_{\\\\text{prev}}$ and $s_{\\\\text{next}}$ (previous and next speakers), respectively. This contextual framing is essential...\"}"}
{"id": "nguyen24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Overview of our proposed single-name model for SpeakerID.\\n\\nTo construct a comprehensive input sequence, we concatenate $w_{prev}$, $w_{cur}$, and $w_{next}$ into a single sequence $w$. In cases where either $w_{prev}$ or $w_{next}$ is missing (e.g., if $w_{cur}$ is at the beginning or end of a dialogue), we introduce padding sentences to maintain consistency in sequence formation. This concatenated sequence represents a broader dialogue context that encapsulates not just the mentioned name but also the surrounding conversational flow.\\n\\nUpon forming this input sequence, we process it through a pretrained language model (PLM), such as RoBERTa [17], renowned for its ability to derive deep contextualized representations of text. By passing $w$ through the PLM, we extract the last-layer subword representations, which capture nuanced semantic and syntactic features of the text. To obtain word-level representations from these subwords, we average the subword representations for each word. Subsequently, to represent each sentence within our concatenated sequence, we compute the average of its word representations, yielding distinct vectors that encapsulate the essence of each sentence.\\n\\nGiven that these sentences originate from three different speakers, we posit that their vectors contain unique semantic signatures reflective of each speaker's communicative style or content. Thus, we treat these sentence vectors as proxies for the speakers themselves, assigning them as $r_{prev}$, $r_{cur}$, and $r_{next}$ for the previous, current, and next speakers, respectively.\\n\\nFor the person name $n$, its representation $r_n$ is derived by averaging the word representations within the span of $n$. This name vector, encapsulating the linguistic context of the name within the dialogue, is then paired with each speaker vector ($r_{prev}$, $r_{cur}$, $r_{next}$). These pairs form the basis for predicting the association between $n$ and the potential speakers.\\n\\nEach of these concatenated pair vectors is inputted into a feed-forward neural network culminating in a sigmoid output layer, which outputs probability scores $p_{prev}$, $p_{cur}$, $p_{next}$ representing the likelihood of the name $n$ being associated with the previous, current, and next speakers, respectively. The model's learning objective is to minimize the standard cross-entropy loss between these predicted probabilities and the true speaker identities. This training process fine-tunes the model to discern the subtle cues within the dialogue that indicate speaker identities, thereby enhancing its ability to accurately attribute names to the correct speakers within complex conversational contexts.\\n\\nFigure 3: Overview of our proposed multi-name model for SpeakerID.\\n\\n2.3.2. Multi-Name Model\\n\\nIn scenarios where a sentence includes multiple names, our observations indicate that these names generally correspond to distinct speaker identities. This includes the possibility of a \\\"null\\\" speaker identity, which is used to represent instances where the speaker's identity may not be directly linked to any mentioned names within the dialogue. To address this complexity, we introduce a sophisticated model designed to simultaneously predict speaker identities for multiple names within a single sentence. This approach is particularly useful in dialogues where multiple individuals are referenced, necessitating a nuanced understanding of speaker identity.\\n\\nConsider a sentence that mentions $K$ person names, represented by the vectors $r_1, r_2, \\\\ldots, r_K$. In our model, each of these name representations is treated as a node within a fully connected graph $G$. The edges of this graph serve to represent the similarity between pairs of names, suggesting that names with higher similarity scores might share or be closely related to specific speaker identities.\\n\\nTo quantify the similarity between any two names, we employ the cosine similarity measure. Specifically, the weight of the edge between any two nodes (names) $i$ and $j$ in the graph is:\\n\\n$$\\\\alpha_{ij} = \\\\text{softmax} \\\\left( \\\\frac{r_i^T r_j}{P_j r_i^T r_j} \\\\right)$$\\n\\nThis formula essentially normalizes the cosine similarities between a name $i$ and all other names $j$, ensuring that the edge weights are comparable across the graph and facilitating a probabilistic interpretation of name similarity.\\n\\nUpon establishing the graph structure with calculated edge weights, we proceed to employ a Graph Convolutional Network (GCN) [19] to refine the representations of each name. The GCN operates over $L$ layers, where each layer enhances the name representations by aggregating information from connected nodes (i.e., other names) weighted by their similarities. The operation at each layer $l$ is defined as:\\n\\n$$h_l(i) = \\\\text{ReLU} \\\\left( \\\\sum_{j=1}^{K} \\\\alpha_{ij} W_l h_{l-1}(j) + b_l \\\\right)$$\\n\\nThis formula aggregates information from connected nodes (i.e., other names) weighted by their similarities. The operation at each layer $l$ is defined as:\\n\\n$$h_l(i) = \\\\text{ReLU} \\\\left( \\\\sum_{j=1}^{K} \\\\alpha_{ij} W_l h_{l-1}(j) + b_l \\\\right)$$\"}"}
{"id": "nguyen24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this equation, $W_l$ and $b_l$ represent the learnable weight matrix and bias for the $l$th layer of the GCN, respectively, and $h_0^i$ is the initial input representation for the name $i$. This method allows the model to iteratively refine the representation of each name by incorporating contextual information from other names within the same sentence, effectively capturing the relational dynamics between mentioned individuals. Ultimately, the enhanced name representations can be paired with speaker representations, as detailed in a previous section, to accurately predict the corresponding speaker identities. This innovative approach leverages the power of GCN to understand and model the complex interrelations between multiple names mentioned in dialogues, offering a promising avenue for advancing the accuracy of speaker identification in rich, multimodal content.\\n\\n2.3.3. Inference\\nAt test time, there might be multiple names assigned to the same speaker identity. To make final predictions for the names, we simply select the name with the highest probability score.\\n\\n3. Experiments\\n3.1. Dataset\\nWe randomly sample 200 meetings from the MediaSum dataset [18] in English language to create a SpeakerID dataset for experimental purpose. We randomly split the resulting dataset into train/dev/test with a ratio of 8/1/1. Statistics for the experimental dataset is shown in Table 2.\\n\\n| Dataset | #meetings | #sents | #names | #speakers |\\n|---------|-----------|--------|--------|-----------|\\n| Train   | 160       | 17,440 | 5,170  | 962       |\\n| Dev     | 21        | 1,719  | 570    | 118       |\\n| Test    | 19        | 1,562  | 429    | 106       |\\n\\nTable 2: Statistics of the dataset sampled for experiments. \\n\\n#meetings, #sents, #names, #speakers respectively denote the numbers of meetings, sentences, names, and speakers in the dataset.\\n\\n3.2. Hyper-parameters\\nWe tune and select our hyper-parameters on the development set of the dataset. In particular, the models use RoBERTa large version as the PLM and are trained using the Adam optimizer with a learning rate of $1 \\\\times 10^{-5}$ and a batch size of 16. All feed-forward networks have hidden vector sizes of 400, and there are 2 layers for the GCN. To implement the models, Pytorch version 1.7.1 [23] and Huggingface Transformers version 3.5.1 (Apache 2.0 license) [24] are used. The Trankit library version 1.0 (Apache 2.0 license) [21] is used to preprocess the data and perform named entity recognition. The model's performance is evaluated over three runs with different random seeds. Experiments are conducted on a single Tesla V100-SXM2 GPU with 32GB memory operated by Ubuntu 20.04.4 LTS.\\n\\n3.3. Evaluation Metrics\\nWe measure the performance of the models by calculating the number of speakers that the models can successfully found their names in the transcripts. We then compute the precision, recall, and F1 scores for the models accordingly.\\n\\n| Models         | Precision | Recall | F1    |\\n|----------------|-----------|--------|-------|\\n| Single-name    | 80.3%     | 50.0%  | 61.6% |\\n| Multi-name     | 78.8%     | 49.1%  | 60.5% |\\n| Multi-name - GCN | 75.8%     | 47.2%  | 58.2% |\\n\\nTable 3: Performance comparison of the models on the test set of our experimental dataset.\\n\\n3.4. Results\\nTable 3 presents the main results of our experiments. The single-name model exhibits the highest precision at 80.3%, suggesting that when it predicts a speaker's name, it is correct 80.3% of the time. However, its recall is notably lower at 50.0%, indicating that it only identifies names for half of the speakers in the dataset. The multi-name model shows a marginal decrease in precision to 78.8% and a slight dip in recall at 49.1%. This reduction in precision and recall may suggest that introducing multiple names into the identification process complicates the model's ability to accurately predict the correct speaker names, possibly due to the increased complexity in distinguishing between multiple speakers within the same context.\\n\\nThe multi-name model enhanced with Graph Convolutional Networks (GCN) further decreases in performance, with a precision of 75.8% and a recall of 47.2%. This result suggests that the GCN component, contrary to expectations, does not necessarily impede the model's performance. Instead, it may provide a beneficial role in the context of the multi-name setting by capturing complex patterns and relationships between speaker identities which are not as effectively discerned when the GCN is removed.\\n\\nNote that, the recall scores are limited due to the fact that names of speakers are not always mentioned in the transcripts. Specifically, there are 71 speakers that we can found their names in the test transcripts, leading to the upper bound of 67.0% for the recall score. This means that our best model successfully found names for 71.4% of the speakers in the transcripts.\\n\\n4. Related Work\\nSpeakerID is an important task for automatic organization of dialogue contents such as TV programs, radio podcasts, and online meetings and has gained significant research efforts [4, 5, 6, 7, 8]. Most previous work on SpeakerID approaches the task via multi-model setting, where the input to the models involve both videos/images and transcripts of the dialogues. Other work [4, 9, 10, 11] focuses on the text-based setting, where the input to the model involves only the transcripts and text-based features of the dialogues. However, none of the previous work employs deep learning methods. Our work is the first work that employ pretrained language models [16, 17] for text-based SpeakerID.\\n\\n5. Conclusions\\nWe proposed a novel method to automatically obtain a large-scale dataset for SpeakerID and presented novel models using pretrained language models for the task. Experimental results show that our proposed models achieve great performance, demonstrating the effectiveness and quality of our proposed dataset and model for SpeakerID. This study not only proves the practicality and effectiveness of utilizing deep learning for text-based Speaker Identification but also paves the way for further exploration in the management and retrieval of dialogue content.\"}"}
{"id": "nguyen24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] F. Salmon and F. Vallet, \\\"An effortless way to create large-scale datasets for famous speakers.\\\" in *LREC*, vol. 14, 2014, pp. 348\u2013352.\\n\\n[2] F. Vallet, J. Uro, J. Andriamakaoly, H. Nabi, M. Derval, and J. Carrive, \\\"Speech trax: A bottom to the top approach for speaker tracking and indexing in an archiving context,\\\" in *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)*, 2016, pp. 2011\u20132016.\\n\\n[3] J. Burgess and J. Green, *YouTube: Online video and participatory culture*. John Wiley & Sons, 2018.\\n\\n[4] S. E. Tranter, \\\"Who really spoke when? finding speaker turns and identities in broadcast news audio,\\\" in *2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings*, vol. 1. IEEE, 2006, pp. I\u2013I.\\n\\n[5] J. Poignant, L. Besacier, and G. Qu\u00e9not, \\\"Unsupervised speaker identification in tv broadcast based on written names,\\\" *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, vol. 23, no. 1, pp. 57\u201368, 2014.\\n\\n[6] J. Poignant, H. Bredin, and C. Barras, \\\"Multimodal person discovery in broadcast tv at mediaeval 2015,\\\" in *MediaEval 2015*, 2015.\\n\\n[7] F. Vallet, J. Uro, J. Andriamakaoly, H. Nabi, M. Derval, and J. Carrive, \\\"Speech trax: A bottom to the top approach for speaker tracking and indexing in an archiving context,\\\" in *Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC'16)*. Portoro\u017e, Slovenia: European Language Resources Association (ELRA), May 2016, pp. 2011\u20132016. [Online]. Available: https://aclanthology.org/L16-1318\\n\\n[8] N. Le, H. Bredin, G. Sargent, M. India, P. Lopez-Otero, C. Barras, C. Guinaudeau, G. Gravier, G. B. da Fonseca, I. L. Freire et al., \\\"Towards large scale multimedia indexing: A case study on person discovery in broadcast news,\\\" in *Proceedings of the 15th International Workshop on Content-Based Multimedia Indexing*, 2017, pp. 1\u20136.\\n\\n[9] Y. Esteve, S. Meignier, P. Del\u00e9gile, and J. Mauclair, \\\"Extracting true speaker identities from transcriptions,\\\" in *Interspeech 2007*, 2007.\\n\\n[10] V. Jousse, S. Petit-Renaud, S. Meignier, Y. Esteve, and C. Jacquin, \\\"Automatic named identification of speakers using diarization and asr systems,\\\" in *2009 IEEE International Conference on Acoustics, Speech and Signal Processing*. IEEE, 2009, pp. 4557\u20134560.\\n\\n[11] M. Kucha\u0159ov\u00e1, S. \u0160kov\u00e1lov\u00e1, L. \u0160eps, and M. Boh\u00e1\u010d, \\\"Study on phrases used for semi-automatic text-based speakers names extraction in the czech radio broadcasts news,\\\" in *Text, Speech and Dialogue: 17th International Conference, TSD 2014, Brno, Czech Republic, September 8-12, 2014. Proceedings 17*. Springer, 2014, pp. 416\u2013423.\\n\\n[12] L. Deng and J. Platt, \\\"Ensemble deep learning for speech recognition,\\\" in *Proc. interspeech*, 2014.\\n\\n[13] Z. Zhang, J. Geiger, J. Pohjalainen, A. E.-D. Mousa, W. Jin, and B. Schuller, \\\"Deep learning for environmentally robust speech recognition: An overview of recent developments,\\\" *ACM Transactions on Intelligent Systems and Technology (TIST)*, vol. 9, no. 5, pp. 1\u201328, 2018.\\n\\n[14] U. Kamath, J. Liu, and J. Whitaker, *Deep learning for NLP and speech recognition*. Springer, 2019, vol. 84.\\n\\n[15] C. Wang, Y. Tang, X. Ma, A. Wu, D. Okhonko, and J. Pino, \\\"Fairseq S2T: Fast speech-to-text modeling with fairseq,\\\" in *Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing: System Demonstrations*. Suzhou, China: Association for Computational Linguistics, Dec. 2020, pp. 33\u201339. [Online]. Available: https://aclanthology.org/2020.aacl-demo.6\\n\\n[16] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \\\"BERT: Pre-training of deep bidirectional transformers for language understanding,\\\" in *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*. Minneapolis, Minnesota: Association for Computational Linguistics, Jun. 2019, pp. 4171\u20134186. [Online]. Available: https://aclanthology.org/N19-1423\\n\\n[17] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \\\"Roberta: A robustly optimized bert pretraining approach,\\\" *arXiv preprint arXiv:1907.11692*, 2019.\\n\\n[18] C. Zhu, Y. Liu, J. Mei, and M. Zeng, \\\"MediaSum: A large-scale media interview dataset for dialogue summarization,\\\" in *Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies*. Online: Association for Computational Linguistics, Jun. 2021, pp. 5927\u20135934. [Online]. Available: https://aclanthology.org/2021.naacl-main.474\\n\\n[19] T. N. Kipf and M. Welling, \\\"Semi-supervised classification with graph convolutional networks,\\\" in *International Conference on Learning Representations*, 2017. [Online]. Available: https://openreview.net/forum?id=SJU4ayYgl\\n\\n[20] Z. Du, S. Zhang, S. Zheng, and Z.-J. Yan, \\\"Speaker overlap-aware neural diarization for multi-party meeting analysis,\\\" in *Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing*. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 7458\u20137469. [Online]. Available: https://aclanthology.org/2022.emnlp-main.505\\n\\n[21] M. V. Nguyen, V. D. Lai, A. Pouran Ben Veyseh, and T. H. Nguyen, \\\"Trankit: A light-weight transformer-based toolkit for multilingual natural language processing,\\\" in *Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations*. Online: Association for Computational Linguistics, Apr. 2021, pp. 80\u201390. [Online]. Available: https://aclanthology.org/2021.eacl-demos.10\\n\\n[22] L. Yujian and L. Bo, \\\"A normalized levenshtein distance metric,\\\" *IEEE transactions on pattern analysis and machine intelligence*, vol. 29, no. 6, pp. 1091\u20131095, 2007.\\n\\n[23] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga, A. Desmaison, A. Kopf, E. Yang, Z. DeVito, M. Raison, A. Tejani, S. Chilamkurthy, B. Steiner, L. Fang, J. Bai, and S. Chintala, \\\"Pytorch: An imperative style, high-performance deep learning library,\\\" in *Advances in Neural Information Processing Systems 32*. Curran Associates, Inc., 2019, pp. 8024\u20138035. [Online]. Available: http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf\\n\\n[24] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, \\\"Transformers: State-of-the-art natural language processing,\\\" in *Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations*. Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: https://aclanthology.org/2020.emnlp-demos.6\"}"}
