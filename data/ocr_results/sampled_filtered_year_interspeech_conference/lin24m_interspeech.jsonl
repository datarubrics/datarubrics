{"id": "lin24m_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This paper presents MinSpeech, a speech corpus of Southern Min (also known as Hokkien), to propel research in dialect speech recognition. Despite the linguistic and cultural importance of Southern Min, there is still a notable scarcity of publicly accessible speech corpus for this dialect. MinSpeech provides 2237 hours of unlabeled audio and 1778 hours of labeled audio, sourced diversely and encompassing various contexts. Mandarin text is employed as labels to enable cross-linguistic alignment and transformation. Using this corpus, we have developed baseline systems, including supervised models (Kaldi Chain and Conformer) and two self-supervised models (Wav2vec 2.0 and HuBERT). These systems were assessed on an automatic speech recognition (ASR) task to the Southern Min dialect. Experiments illustrate that the corpus offers practical assistance and resources for speech processing of this dialect. MinSpeech dataset is available at [https://minspeech.github.io/](https://minspeech.github.io/).\\n\\n**Index Terms**: corpus, Southern Min dialect, ASR\\n\\n### 1. Introduction\\n\\nSpeech recognition is a crucial domain within speech processing. In recent years, deep neural network models, bolstered by vast amounts of training data and high-quality test sets [1, 2], have demonstrated exceptional performance in this field. However, the majority of existing speech recognition corpora focus on high-resource languages, leading to a significant data scarcity for low-resource languages. Inspired by the goals of the IARPA BABEL program [3, 4, 5, 6], researchers have undertaken numerous initiatives to address this challenge. In this work, we specifically focus on the low-resource language of Southern Min dialect.\\n\\nSouthern Min is predominantly spoken in southern Fujian and Taiwan, and serves as the mother tongue for many overseas Chinese. Despite its importance, the Southern Min dialect faces the threat of extinction due to a declining number of younger speakers [7, 8, 9]. Recent studies have incorporated Southern Min into natural language processing (NLP) and speech processing technologies in order to preserve and promote this dialect [10, 11, 12, 13]. Unfortunately, there is a scarcity of resources available for study. Previous research has only yielded a 1.5-hour speech corpus [14], which significantly impedes the progress of developing Southern Min speech recognition systems.\\n\\nSelf-Supervised Speech Representation Learning (SSL) is a highly effective method for addressing the challenge of low-resource ASR. By utilizing unlabeled speech data, it effectively alleviates the issue of labeled data scarcity [15, 16, 17, 18, 19, 20, 21, 22, 23]. In recent research, two self-supervised models, Wav2vec 2.0 [15] and HuBERT [16], have demonstrated high performance on various corpora and languages. Wav2vec 2.0 utilizes contrastive learning and achieves high recognition accuracy with only 10 minutes of labeled data. In contrast, HuBERT is based on mask reconstruction and has achieved the best results on several downstream tasks.\\n\\nIn this work, we constructed a large-scale Southern Min corpus with labeled audio to address the problem of scarce labeled data, and provided unlabeled audio for training self-supervised models. The corpus is composed of 36 TV programs covering a wide range of topics and styles, such as history, mythology, family, culture, education, with a high degree of diversity and coverage. We use Mandarin text as labels to facilitate alignment and transformation with corpora in other languages. Our baseline models were trained using supervised data, which included Kaldi Chain [24] and Conformer [25] models. Furthermore, we pre-trained and fine-tuned Wav2vec 2.0 and HuBERT with our corpus to obtain two SSL baseline models. Performance tests were conducted on the ASR task, confirming that the results indicate the significant contribution of our corpus to enhancing Southern Min recognition.\\n\\nThe main contributions of this paper are as follows:\\n\\n- We introduce, for the first time, a large Southern Min corpus named MinSpeech, comprising 2237 hours of unlabeled audio and 1778 hours of labeled audio, with high diversity and coverage across various topics and styles.\\n- We propose a methodology for creating the corpus and training baseline ASR systems of Southern Min dialect.\\n\\n### 2. MinSpeech Corpus\\n\\nIn this section, we describe the production process (Fig.1) and the distinctive characteristics of the MinSpeech corpus.\"}"}
{"id": "lin24m_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1. Corpus Production Process\\n\\n2.1.1. Screening and Downloading Videos\\n\\nWe began with identifying target topics spanning history, mythology, family, culture, education, criminal investigation, economics, emotions, and comedy. We selected videos from YouTube that have human-generated subtitles. The topics of the videos were aligned with our chosen keywords. These captions were employed as text labels for the audio files. We then selected 20 diverse TV dramas as the unlabeled audio, and 16 TV dramas as the labeled audio. All videos were downloaded and converted into mono 16kHz 16bit format audio files.\\n\\n2.1.2. Audio Segmentation\\n\\nYouTube video caption files with the extension \\\".vtt\\\" contain the start and end time of each sentence and the corresponding label. For labeled audio files, we cropped the corresponding audio files based on the timestamps in the subtitle files. For unlabeled audio files, we employed the silero-vad toolkit [26] to segment the audio files according to specific cutting rules:\\n\\n\u2022 Segmentation was permitted when the mute time exceeded 0.5 seconds.\\n\u2022 The time cutoff for segment boundaries was 0.1 seconds.\\n\u2022 The length of each segment was limited to less than 15 seconds.\\n\\n2.1.3. Label Generation\\n\\nUsing the format specification of captions, we extracted each sentence's start and end time and the corresponding text content. We numbered each subtitle and iterated through each numbered subtitle file, then cropped the corresponding audio files according to the timestamps and mapped the audio files to the subtitle text to form labeled data. To enhance the quality of the corpus, we implemented the following rules for filtering and processing the data:\\n\\n\u2022 Exclude subtitle data with multiple lines indicating simultaneous speech from multiple individuals.\\n\u2022 Exclude data with English in the subtitle text.\\n\u2022 Exclude data where the audio file's duration is less than 0.5 seconds.\\n\u2022 Remove any special characters from the subtitle text.\\n\u2022 Convert punctuation marks in subtitle text to spaces.\\n\\n2.1.4. Audio Validation\\n\\nTo assess the quality of the recordings, we utilized two screening criteria: signal-to-noise ratio (SNR) and signal energy. Each audio file must meet both criteria to be considered acceptable.\\n\\n2.2. Corpus Details\\n\\n2.2.1. Corpus Segmentation Information\\n\\nTable 1 provides an overview of the segmentation information for the MinSpeech unlabeled corpus. The corpus consists of 1291273 sentences, equivalent to a total duration of 2237 hours. The data was compiled from diverse sources. These sources include 20 TV dramas covering various topics spanning history, mythology, family, culture, education, criminal investigation, economics, emotions, and comedy. This diversity ensures that models trained on the corpus are exposed to various linguistic contexts and scenarios, thereby enhancing their adaptability and generalization capabilities. The corpus was segmented by randomly partitioning it into training and evaluation sets.\\n\\n| Split       | Utterances | Duration (hr) |\\n|-------------|------------|---------------|\\n| Train       | 1033009    | 1790.40       |\\n| Dev         | 258264     | 446.60        |\\n| Total       | 1291273    | 2237.00       |\\n\\nTable 1: Segmentation information of the MinSpeech unlabeled corpus\\n\\nTable 2 illustrates the distribution and characteristics of the MinSpeech labeled corpus, which comprises a total of 2759550 audio files extracted from 16 TV dramas. The corpus spans a cumulative 1778.43 hours, providing a substantial corpus for training and evaluation purposes. The topics covered are similar to those in the MinSpeech unlabeled corpus. We randomly selected two test sets from the MinSpeech labeled corpus that do not overlap with the training subset, Dev (6.38 hours) and Test (19.21 hours), to evaluate the generalization ability and performance of the model.\\n\\n2.2.2. Utterance Length Distribution\\n\\nTables 3 and 4 show the sentence length distribution information of the MinSpeech corpus. As can be seen from the table, both unlabeled corpus and labeled corpus are dominated by short sentences, accounting for most of the MinSpeech corpus. The characteristics of this sentence length distribution are consistent with the situation that the input audio of real-world speech tasks is usually short speech, indicating that the MinSpeech corpus has high practicality and representativeness.\\n\\n3. Baseline\\n\\nIn this section, we detail the baseline of three distinct speech recognition models using MinSpeech: the Kaldi Chain model [24], the Conformer model [25], and the SSL-Transformer. The Kaldi Chain model is a DNN-HMM-based model that features...\"}"}
{"id": "lin24m_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Distribution of the MinSpeech labeled corpus\\n\\n| Split     | Utterances | Duration (hr) |\\n|-----------|------------|---------------|\\n| Train     | 2719550    | 1738.84       |\\n| Dev       | 30000      | 20.38         |\\n| Test      | 30000      | 19.21         |\\n| Total     | 2759550    | 1778.43       |\\n\\nTable 3: MinSpeech unlabeled corpus length distribution information\\n\\n| Length(s) | Utterances | Proportion(%) |\\n|-----------|------------|---------------|\\n| Train/Dev |            |               |\\n| 0-5       | 542037     | 52.47         |\\n| 5-10      | 335088     | 32.44         |\\n| >10       | 155884     | 15.09         |\\n\\nA neural network using one-third of the original frame rate for their output layer [24]. The Conformer is an end-to-end model that efficiently captures local and global dependencies of audio sequences by combining a convolutional neural network and the Transformer architecture. The SSL-Transformer means that the encoders of the speech SSL models adopt the Transformer. We chose two mainstream pre-training methods, including Wav2vec 2.0 and HuBERT. Next, we used the MinSpeech labeled corpus to fine-tune the SSL models to improve their generalization ability.\\n\\n3.1. Evaluation Metrics\\n\\nThe models were evaluated using standardized character error rates (CERs) computed on the test set. Neither the Wenet Baseline System nor the SSL-Transformer Baseline System uses a language model.\\n\\n3.2. Kaldi Baseline System\\n\\nThe Kaldi toolkit [27] was used to implement the Chain model to maximize the log-likelihood of the correct sequence, rather than classifying frame by frame. Initially, the GMM-HMM model was used to get the alignment. The Chain model was subsequently optimized using cross-entropy and LF-MMI criteria. The model structure consists of six layers of convolutional neural network (CNN), ten layers of TDNN-F, one layer of Attention, one layer of TDNN-F, one fast-lstmp-layer, one layer of TDNN-F, and one fast-lstmp-layer. The input features include a 40-dimensional FBank and a 100-dimensional i-vector. Only the SpecAugment [28] technique was utilized to ensure comparability with other systems, and velocity/volume perturbations were avoided.\\n\\n3.3. Wenet Baseline System\\n\\nWe use the Wenet toolkit [29] to train the Conformer model. The FBank features were configured with a frame length of 25 ms, a frameshift of 10 ms, and a Mel-frequency cepstrum coefficient of 80. The Conformer encoder consists of 12 layers, each with a self-attention module, a convolutional module, and two feed-forward network modules. The Conformers have four attention heads, 2048 linear units, a CNN module kernel size of 15, and an output size of 256. The Transformer decoder is composed of 6 layers, each with four attention heads and 2048 linear units. The Adam optimizer is used during training with a learning rate of 0.002. Additionally, the warmup scheduler is employed with a warmup step of 25000. The objective function is hybrid CTC/Attention with the CTC weight set to 0.3 and the label smoothing weight to 0.1. For decoding, a beam search algorithm finds the most probable output sequences, keeping the 20 most probable sequences. Finally, the last 30 optimal checkpoints are averaged to obtain the final model.\\n\\n3.4. SSL-Transformer Baseline System\\n\\nThe SSL systems were built using the SSL-Transformer approach with Wav2vec 2.0 [15] and HuBERT [16] models. The pre-training phase of the experiments utilized the fairseq [30] toolkit, while the fine-tuning phase utilized HuggingFace's Transformers toolkit [31]. Wav2vec 2.0 employs a contrastive learning approach to learn the latent structure of the audio by maximizing the mutual information of the audio segments and their context. HuBERT utilizes a masked speech modeling technique to learn the underlying structure of audio by predicting the number of randomly masked segments. This approach enables the learning of high-level audio semantics by predicting audio features that are randomly masked. The pre-training phase framework is similar to that of [15, 16], where the base and large models are trained separately, and the pre-training data is sourced from our 2237-hour unlabeled Southern Min corpus. The following are the details of the fine-tuned experimental framework:\\n\\n- Encoder: The encoder, responsible for mapping the input audio features into high-dimensional representations for subsequent decoding, is taken from the speech SSL model trained on our Southern Min corpus.\\n- Connectionist Temporal Classification (CTC): We used CTC as the loss function.\\n- Hybrid CTC/Attention model: In this model, we still use the speech SSL model trained on our corpus as the encoder, which is subsequently connected to a decoder. The decoder uses an attention mechanism to align the correspondence between audio features and text labels. We used the Transformer-based GPT-2 as the decoder, which consists of 8 layers of Transformer with 1024 hidden units, 16 attention heads and 3072 feed-forward network units per layer. During the pre-training phase, we utilized the Adam optimizer with a learning rate of 0.0005 and weight decay of 0.01. Additionally, we implemented a warmup period for the first 32000 steps. For the fine-tuning, we optimized the learning rate using Adam and a three-state learning rate approach. Specifically, we warmed up the learning rate for the first 10% of updates, held it constant for the next 40%, and linearly decayed it thereafter. The update frequency was set to 10, the layer discard\"}"}
{"id": "lin24m_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"rate was set to 0.1, and the mask probability was set to 0.5. Decoding is accomplished through a hybrid CTC/Attention loss optimization with a CTC weight of 0.3. A beam search algorithm is utilized to identify the most probable output sequences, retaining the top 20 sequences and expanding them at each time step.\\n\\n4. Experiments\\n\\n4.1. Different Toolkits Results\\n\\nIn this section, we present the outcomes of our baseline experiments using three distinct toolkits. Table 5 shows the performance of various models across different toolkits evaluated on both the Dev and Test sets. The results demonstrate the effectiveness of HuBERT-large model, which outperformed other models on the Test set with a CER of 20.21%. The model's superior performance can be attributed to its robust SSL speech representation capabilities and efficient architecture design.\\n\\nThese baseline experiments provide a foundational benchmark for future improvements within speech processing and natural language understanding.\\n\\n| Toolkit Model          | Dev  | Test  |\\n|------------------------|------|-------|\\n| Kaldi Chain            | 27.92| 27.86 |\\n| Wenet Conformer        | 25.44| 25.26 |\\n| HuggingFace HuBERT-large | 20.18| 20.21 |\\n\\n4.2. Baseline for SSL Models of MinSpeech\\n\\n4.2.1. CTC Fine-tuning Performance\\n\\nWe present the benchmarking results of four pre-trained models (Wav2vec-base, Wav2vec-large, HuBERT-base, and HuBERT-large) tailored explicitly for the Southern Min dialects using the proposed corpus. The performance of these models fine-tuned with the CTC loss function is summarized in Table 6.\\n\\nThe results demonstrate the efficacy of these models in the speech recognition task for Southern Min dialects. Particularly noteworthy is the HuBERT-large model, which stands out with the lowest CERs of 21.42% and 21.22% on the development (Dev) and test (Test) sets, respectively. This indicates the robust generalization and adaptation abilities of HuBERT-large, contributing to its accurate speech recognition performance in Southern Min dialects.\\n\\nInterestingly, both the Wav2vec-large and HuBERT-base models demonstrate similar performance, with CERs of approximately 24% on the development set and slightly lower on the test set. This suggests that while model size is a factor, other elements such as model architecture, data representation, and training methodology also play crucial roles in determining superior performance in speech recognition tasks.\\n\\n4.2.2. Hybrid CTC/Attention Fine-tuning Performance\\n\\nWe separately use speech SSL models trained on our Minnan corpus as the encoder, backed by a transformer-based GPT-2 as the decoder, and employ hybrid CTC/attention loss to fine-tune the model. Table 7 presents the results of fine-tuning the models using the hybrid CTC/Attention. The performance of each model is assessed on both the development (Dev) and test (Test) sets. The results demonstrate improved speech recognition performance using the hybrid CTC/Attention loss compared to fine-tuning with CTC alone. Specifically, compared with CTC fine-tuning, the absolute values of CER of Wav2vec-base, Wav2vec-large, HuBERT-base and HuBERT-large models using hybrid CTC/Attention on the test set are respectively reduced by 1.03%, 1.45%, 1.31% and 1.01%. It suggests that the hybrid CTC/Attention approach better captures the alignment relationship between speech and text, thereby improving speech recognition accuracy.\\n\\nThe Wav2vec-base model exhibits competitive performance, achieving a CER of 25.23% on the development set and 25.18% on the test set. While these results are promising, they indicate potential for further refinement to enhance the accuracy. Scaling up the model architecture with the Wav2vec-large leads to notable improvements, with CER values of 22.61% on the development set and 22.53% on the test set. It demonstrates the effectiveness of increasing model capacity in reducing error rates. The HuBERT-base model exhibits competitive performance with a CER of 22.62% on the development set and 22.57% on the test set. Notably, the HuBERT-large model outperforms its counterparts, achieving significantly lower CER values of 20.18% on the development set and 20.21% on the test set. This can be attributed to its larger capacity and more sophisticated representations, emphasizing the importance of model scalability in capturing intricate dialectal nuances.\\n\\n| Model          | Dev  | Test  |\\n|----------------|------|-------|\\n| Wav2vec-base   | 26.34| 26.21 |\\n| Wav2vec-large  | 24.15| 23.98 |\\n| HuBERT-base    | 23.91| 23.88 |\\n| HuBERT-large   | 21.42| 21.22 |\\n\\n| Model          | Dev  | Test  |\\n|----------------|------|-------|\\n| Wav2vec-base   | 25.23| 25.18 |\\n| Wav2vec-large  | 22.61| 22.53 |\\n| HuBERT-base    | 22.62| 22.57 |\\n| HuBERT-large   | 20.18| 20.21 |\\n\\n5. Conclusion\\n\\nIn this work, we introduce MinSpeech, consisting of 2237 hours of unlabeled audio and 1778 hours of labeled audio. To our knowledge, this corpus stands as the most exhaustive collection available to date, serving as an invaluable asset for the advancement of research and the practical implementation of ASR technologies.\"}"}
{"id": "lin24m_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. Radford, J. W. Kim, and X. et al., \\\"Robust speech recognition via large-scale weak supervision,\\\" in Proceedings of the 40th International Conference on Machine Learning, ser. Proceedings of Machine Learning Research, A. Krause, E. Brunskill, K. Cho, B. Engelhardt, S. Sabato, and J. Scarlett, Eds., vol. 202. PMLR, 23\u201329 Jul 2023, pp. 28492\u201328518.\\n\\n[2] L. Barrault, Y.-A. Chung, and M. et al., \\\"Seamlessm4t-massively multilingual & multimodal machine translation,\\\" arXiv preprint arXiv:2308.11596, 2023.\\n\\n[3] J. Cui, X. Cui, and R. et al., \\\"Developing speech recognition systems for corpus indexing under the iarpa babel program,\\\" in 2013 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2013, pp. 6753\u20136757.\\n\\n[4] M. J. Gales, K. M. Knill, and R. et al., \\\"Speech recognition and keyword spotting for low-resource languages: Babel project research at cued,\\\" in Fourth International workshop on spoken language technologies for under-resourced languages. International Speech Communication Association (ISCA), 2014, pp. 16\u201323.\\n\\n[5] L.-M. Lam-Yee-Mui, W. B. Kheder, and L. et al., \\\"Multilingual models with language embeddings for low-resource speech recognition,\\\" in 2nd Annual Meeting of the ELRA/ISCA SIG on Under-resourced Languages. ISCA, 2023, pp. 83\u201387.\\n\\n[6] J.-Y. Hsu, Y.-J. Chen, and H.-y. Lee, \\\"Meta learning for end-to-end low-resource speech recognition,\\\" in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7844\u20137848.\\n\\n[7] S.-H. Ting and J. Z.-M. Teng, \\\"Chinese teenagers' perceptions of vitality of hokkien chinese in penang, malaysia,\\\" International Journal of the Sociology of Language, vol. 2021, no. 272, pp. 185\u2013217, 2021.\\n\\n[8] W. D. W. Gonzales, \\\"Interactions of sinitic languages in the philippines: Sinicization, filipinization, and sino-philippine language creation,\\\" in The palgrave handbook of Chinese language studies. Springer, 2022, pp. 369\u2013408.\\n\\n[9] W.-V. T. Chiung, \\\"Tone change in taiwanese: age and geographic factors,\\\" University of Pennsylvania Working Papers in Linguistics, vol. 8, no. 1, p. 5, 2003.\\n\\n[10] P.-J. Chen, K. Tran, and Y. et al., \\\"Speech-to-speech translation for a real-world unwritten language,\\\" arXiv preprint arXiv:2211.06474, 2022.\\n\\n[11] Y.-H. Chou, K. Chang, and W. et al., \\\"Evaluating self-supervised speech models on a taiwanese hokkien corpus,\\\" in 2023 IEEE Automatic Speech Recognition and Understanding Workshop. IEEE, 2023, pp. 1\u20137.\\n\\n[12] S.-E. Lu, B.-H. Lu, and L. et al., \\\"Exploring methods for building dialects-Mandarin code-mixing corpora: A case study in Taiwanese hokkien,\\\" in Findings of the Association for Computational Linguistics: EMNLP 2022, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Abu Dhabi, United Arab Emirates: Association for Computational Linguistics, Dec. 2022, pp. 6287\u20136305.\\n\\n[13] P. Bai, Y. Zhou, and Z. et al., \\\"Improving chinese pop song and hokkien gezi opera singing voice synthesis by enhancing local modeling,\\\" in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023, pp. 3302\u20133312.\"}"}
