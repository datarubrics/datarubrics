{"id": "labrak24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zero-Shot End-To-End Spoken Question Answering In Medical Domain\\n\\nYanis Labrak1,2, Adel Moumen1, Richard Dufour1,3, Mickael Rouvier1\\n\\n1LIA - Avignon University, France\\n2Zenidoc, France\\n3Nantes Universit\u00e9, \u00c9cole Centrale Nantes, CNRS, LS2N, UMR 6004, F-44000 Nantes, France\\n\\nyanis.labrak@univ-avignon.fr, adel.moumen@univ-avignon.fr, richard.dufour@univ-nantes.fr, mickael.rouvier@univ-avignon.fr\\n\\nAbstract\\nIn the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts.\\n\\nIndex Terms\\nspoken question answering, large language model, medical, zero-shot, whisper, ssl\\n\\n1. Introduction\\nSpoken Question Answering (SQA) aims to identify the correct answer from spoken documents or texts in response to a given spoken query. Unlike many other spoken language understanding tasks, such as speech summarization, which primarily focus on semantic comprehension at the utterance level, SQA demands advanced comprehension and reasoning over extensive audio content. In addition to grasping the question and understanding the global context within the audio, it requires capturing nuanced details to accurately select the correct answer, often involving the utilization of out-of-context information. As a result, SQA poses a significant challenge due to its multifaceted nature.\\n\\nTraditionally, SQA methods comprise a cascade of an Automatic Speech Recognition (ASR) system to transcribe the audio question followed by a Language Model (LM). The LM takes a prompt and the automatic transcription as input to predict the correct answer among a list of options. However, ASR errors introduce noise into the LM input, leading to performance degradation and information loss, despite community efforts [1, 2, 3] to mitigate these issues and enhance robustness to transcription errors. Consequently, the cascade of stages cannot match the performance of a single-stage model based on speech due to inherent information loss [4].\\n\\nThe emergence of Large Language Models (LLMs) like Bloom [5] or LLaMa 2 [6] represents a significant advancement in question-answering systems. However, these models require extensive parameter scaling, further complicating the challenge of running separate models for each stage. For instance, the ASR models are already large (e.g., Whisper Medium with 769M parameters and Large V2 with 1.55B parameters), necessitating significant hardware resources for each stage. Consequently, there is a growing interest in directly extracting information from speech to preserve maximal information while minimizing hardware requirements.\\n\\nSeveral architectures, such as Whisper [7], CLAP [8], and SpeechT5 [9], have proposed unifying textual and audio modalities using encoder-decoder models. Notably, autoregressive approaches based on LLMs, exemplified by SpeechGPT [10], have emerged. These models rely on textual prompts to encode speech signals into discrete units.\\n\\nWe propose a novel end-to-end audio-text entailment strategy for zero-shot multiple-choice question answering tasks, focusing on the medical domain. Inspired by zero-shot classification methods in textual Natural Language Processing (NLP) [11, 12] and computer vision [13, 14], our approach leverages the model's capacity to identify modalities that entail each other. Our contributions include:\\n\\n\u2022 An innovative audio-text entailment approach for zero-shot spoken multiple-choice question answering tasks.\\n\u2022 A new SQA dataset tailored to the medical domain.\\n\u2022 A zero-shot performance comparison of 4 existing state-of-the-art end-to-end models.\\n\u2022 An in-depth analysis of the disposition of the information required for the SQA task within speech encoder layers.\\n\u2022 A public release of all the code and data on GitHub and Hugging Face 1\\n\\n2. Medical spoken question answering\\nIn this section, we define the SQA task (Section 2.1) and present the open benchmark constructed from established medical datasets initially in textual format (Section 2.2). Additionally, we describe the audio prompt format (Section 2.3) and the SQA evaluation protocol (Section 2.4).\\n\\n2.1. Definition\\nWe focus on multiple-choice SQA within the medical domain. Each instance comprises an audio question followed by four possible spoken responses, denoted as $(q, o, c, a)$. Here, $q$ represents the question, $o$ denotes the options (labeled A to D), $c$ indicates the correct answer and $a$ encapsulates the audio containing both the question and options. Questions are structured\\n\\n1https://huggingface.co/SpokenMedicalQA\"}"}
{"id": "labrak24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"as single-turn interactions, devoid of dialogue. This evaluation relies solely on the model's internal knowledge without external information or span extraction. The primary objective is to assess end-to-end model performance in understanding and accurately choosing the correct answer from spoken input.\\n\\n2.2. Tasks collection and description\\n\\nRecent years have seen significant progress in SQA datasets, such as Clotho-AQA [15], Spoken-SQuAD [16], and LibriSQA [17]. However, these datasets do not specifically target the healthcare domain or rely solely on audio inputs. The absence of SQA datasets in the medical domain hampers the development of question answering systems tailored to healthcare contexts. To address this gap, we propose synthesizing an audio dataset from existing textual multiple-choice question answering (MCQA) corpora. Our approach involves using Text-To-Speech (TTS) technology on these MCQA textual datasets to generate synthetic audios, leveraging advancements in TTS models that increasingly resemble human speech quality [18, 19]. We utilized the OpenAI TTS API to synthesize speech based on the questions and available options. The speakers were alternated through the 6 available voices to introduce diversity and realism into the dataset. The resulting audio files were sampled at 16,000 Hz and converted to WAV mono channel format.\\n\\nOur reference texts were sourced from three open-source textual MCQA corpora in English, all relevant to healthcare, featuring single possible answers and a four-option format. Note that only the test data are detailed here, as the proposed approaches operate under zero-shot conditions.\\n\\nMMLU [20] comprises exam questions spanning 57 subjects, including those relevant to healthcare. We focused on six healthcare-related subjects already evaluated in MedPaLM-2 [21]: college biology, college medicine, anatomy, professional medicine, medical genetics, and clinical knowledge. The dataset includes a test set of 1,089 questions, totaling 8 hours and 39 minutes of synthesized audio.\\n\\nMedQA [22] integrates questions formatted similarly to the US Medical License Exam (USMLE), covering diverse medical topics. We exclusively utilized the test set, comprising 1,273 questions amounting to 21 hours and 22 minutes of audio.\\n\\nMedMCQA [23] consists of questions with four options each, extracted from Indian medical entrance examinations (AI-IMS/NEET). It encompasses 2,400 healthcare topics across 21 medical subjects, with 4,183 questions for the validation which are used as test ones since it is unavailable to the public [24]. The test set comprises 17 hours and 40 minutes of audio.\\n\\nOur final benchmark encompasses 8 SQA tasks (including 6 from MMLU) derived from these 3 synthesized datasets. Table 2 summarizes the audio duration distribution according to the different labels available in the test set.\\n\\n|            | MMLU | MedQA | MedMCQA | Total # Doc. |\\n|------------|------|-------|----------|--------------|\\n| A          | 1h50 | 5h55  | 3h41     | 13h28        |\\n| B          | 1h54 | 5h08  | 4h31     | 11h33        |\\n| C          | 1h50 | 5h49  | 3h57     | 11h37        |\\n| D          | 3h03 | 4h28  | 3h30     | 11h03        |\\n| Total      | 8h39 | 21h22 | 17h40    | 47h41        |\\n\\n2.3. Audio prompt format\\n\\nWe standardized all textual MCQA datasets and synthesized them into audio format. These audio MCQAs serve as prompts for the studied and proposed SQA systems. Following experimentation with various formats and careful listening to the resulting audio outputs, we identified an effective format exemplified below:\\n\\nPrompt Format\\n\\nA 39-year-old woman, with a history of thyroidectomy and primary hyperparathyroidism presents for surgical evaluation for a right adrenal mass.\\n\\nPreoperatively, which of the following medications should she receive to prevent a hypertensive emergency intraoperatively?\\n\\nOption A: Atenolol\\nOption B: Labetolol\\nOption C: Nifedipine\\nOption D: Phenoxybenzamine\\n\\nThe correct answer is Option D.\\n\\n2.4. Evaluation metric\\n\\nThe evaluation of multi-choice SQA with a single correct answer resembles a multi-class classification task. The performance is here assessed for each task using Accuracy, which measures the proportion of correctly predicted answers compared to the total number of questions. A prediction is considered accurate if it exactly matches the ground-truth answer, otherwise, it is classified as incorrect. Choosing the accuracy enables direct comparison with previous works on textual datasets [25, 26].\\n\\n3. Studied and proposed methods\\n\\nThis section outlines the zero-shot approaches studied for SQA. Firstly, we introduce baseline models with cascade systems (Section 3.1). Then, we present models integrating our end-to-end audio-text entailment approach (Section 3.2).\\n\\n3.1. Baseline cascade approaches\\n\\nOur baseline models involve a two-stage process: transcription of audio inputs into text using an ASR module, followed by its processing with an LLM to select the correct answer to posed questions. We conducted experiments with various models to assess the impact of different ASR and LLM configurations on SQA performance. In the ASR stage, we compared the performance using the reference transcription (Oracle) against Whisper Small, Medium, and Large V2 ASR models to identify potential transcription error propagation issues. Subsequently, in the LLM stage, we compared the performance of an LLM similar in size to Whisper Large V2 (1.5 billion parameters), named Phi 1.5, against larger models based on the LLaMa 2 architecture, configured with 7B and 13B parameters, to assess the scalability of performance with model size. In total, we investigated 12 cascade system combinations.\\n\\nDuring the second step of inference, the LLM predicts the next token based on the input prompt, generating probabilities for each token in the vocabulary. To ensure relevance, the vocabulary is filtered to include only relevant tokens (in this case, choice letters) corresponding to the expected answer options. This approach prevents the model from generating irrelevant tokens or hallucinations [27].\"}"}
{"id": "labrak24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Accuracy (in %) of the zero-shot cascade methods. Highest value in bold and second best is underlined.\\n\\n| Method       | Clinical KG | Medical Genetics | Anatomy | Pro Medicine | College Biology | College Medicine | MedQA | MedMCQA | Avg. |\\n|--------------|-------------|------------------|---------|--------------|----------------|------------------|-------|---------|------|\\n| Phi 1.5      | 31.3        | 31.6             | 19.3    | 20.6         | 29.2           | 28.9             | 27.7  | 31.2    | 28.4 |\\n| Whisper Small| 26.8        | 24.0             | 31.9    | 27.6         | 25.0           | 23.1             | 25.5  | 25.9    | 26.2 |\\n| Whisper Medium| 27.9      | 20.0             | 35.6    | 27.6         | 25.7           | 24.9             | 25.4  | 25.4    | 26.6 |\\n| Whisper Large V2 | 31.7 | 19.0             | 34.1    | 24.6         | 26.4           | 26.0             | 27.6  | 26.2    | 27.0 |\\n| Llama 2 7B   | 21.5        | 30.0             | 18.5    | 18.4         | 25.7           | 20.8             | 27.7  | 32.1    | 24.3 |\\n| Whisper Small| 29.4        | 31.0             | 25.2    | 33.5         | 31.9           | 31.2             | 29.9  | 30.7    | 30.3 |\\n| Whisper Medium| 30.6        | 39.0             | 25.2    | 35.3         | 37.5           | 29.5             | 29.5  | 31.1    | 32.2 |\\n| Whisper Large V2 | 31.7 | 38.0             | 26.7    | 33.5         | 29.9           | 31.8             | 28.7  | 30.8    | 31.4 |\\n\\n2. Zero-shot entailment-based approaches\\n\\nNumerous studies [11, 28] have underscored the advantages of leveraging Natural Language Inference (NLI) for textual zero-shot entailment and classification tasks. However, except for CLAP [29] and Pengi [30], based on contrastive learning and prefix-tuning respectively, a limited adaptation of such methodologies has been observed in speech-related literature, particularly with large-scale pre-trained audio models like Whisper and SpeechGPT. Our proposed zero-shot audio-text entailment method is integrated into the four previously mentioned models, aiming to assess the likelihood of a textual sequence matching an audio recording. In our setup, the audio contains the question and options, while the text represents classes A to D.\\n\\nFor Whisper [7], we utilize audio features and request individual log probabilities for each letter using the format: `<|startoftranscript|> [A]`. The predicted class is determined by the highest average log probability. To comply with Whisper's 30-second limit for audio segments, we truncate segments beyond this duration to capture only the question and options. For SpeechGPT [10], we populate the model's context in a prompt filled with speech units obtained from HuBERT [31] representations discretized using k-means clustering on 1,000 clusters. We then request the generation of one additional token to the model. Subsequently, we filter the vocabulary to retain only the log probabilities corresponding to letters A to D, as described earlier in Section 3.1. Pengi [30] undergoes minimal changes in the model, audio representation, and prompt format, maintaining a similar procedure. The approach is slightly adapted for the CLAP model [29], a dual encoder architecture trained with contrastive language-audio pre-training. Here, individual encoders process both speech and text. Given an audio sample (a) and a list of classes (o), we identify the best match among all pairs by calculating the cosine distance between their vector representations. The pair with the closest distance is considered the predicted match.\\n\\n4. Results\\n\\nIn this section, we examine the zero-shot condition performance on our SQA tasks using first the baseline cascade models (Section 4.1), and then our entailment approach across various end-to-end models (Section 4.2).\\n\\n4.1. Zero-shot cascade approaches\\n\\nTable 4 outlines the transcription performance, measured in Word Error Rate (WER), of Whisper ASR versions (Small, Medium, and Large V2) across various SQA tasks. Generally, Whisper Large V2 shows improved WER performance, except in MMLU Anatomy, where Whisper Medium performs better.\\n\\nTable 4: Transcription performance (in WER) on each SQA task. Best result in bold and second best is underlined.\\n\\n| Tasks       | Whisper S | Whisper M | Whisper L-V2 |\\n|-------------|-----------|-----------|--------------|\\n| MMLU        | 5.45      | 4.21      | 3.30         |\\n| Clinical KG | 6.19      | 4.59      | 4.31         |\\n| Medical Genetics | 4.90  | 2.68      | 3.50         |\\n| Anatomy     | 5.66      | 4.68      | 4.54         |\\n| Pro Medicine| 4.54      | 2.91      | 2.66         |\\n| College Biology | 26.02 | 25.54     | 24.74        |\\n| College Medicine | 7.50  | 6.21      | 5.84         |\\n| MedQA       | 7.99      | 6.33      | 6.10         |\\n| MedMCQA     | 8.53      | 7.14      | 6.87         |\\n\\nTable 1 displays the accuracy performance of studied LLM-based zero-shot cascade methods using Whisper automatic transcriptions on multiple SQA tasks. Interestingly, the Whisper model with the lowest WER might not always be the optimal choice in a cascade approach, indicating a lack of direct correlation between WER and SQA accuracy. Conversely, SQA performance appears to depend on LLM size, with larger models yielding higher accuracy. Notably, there is an 11.67% difference between Phi 1.5 and LLaMa 2 13B in Whisper Medium results, highlighting the significant advantage of scaling up LLMs. Except for Phi 1.5, all models show improved performance with transcriptions compared to Oracle. This enhancement, particularly in LLaMa 2 architectures, may be attributed to their better adaptability to speech normalization formats, reduced punctuation, and increased noise.\\n\\nFurthermore, with LLaMa 2, Whisper Medium transcriptions emerge as the top performers. Notably, LLaMa 13B demonstrates a 1.95% overall accuracy gain over Whisper Large V2 and a 2.54% improvement over Whisper Small. Similar trends are observed in the 7B model, with increases of 0.8% over Large V2 and 1.9% over Small. The performance of the LLaMa 2 13B model in a zero-shot scenario with Whisper Medium transcriptions shows promising results.\\n\\n4.2. Zero-shot end-to-end models' capabilities\\n\\nTable 3 outlines the accuracy performance of zero-shot end-to-end models using our entailment method on our multiple-choice SQA benchmark. While the overall average accuracy remains similar across models, specific models demonstrate...\"}"}
{"id": "labrak24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3: Accuracy (in %) of the zero-shot end-to-end models applying our entailment method. Highest value in bold and second best is underlined, excluding SpeechGPT + Oracle (model aligned with reference transcriptions).\\n\\n| Model                  | MMLU | Clinical KG | Medical | Genetics | Anatomy | Pro | Medicine College | Biology | College Medicine | MedQA | MedMCQA | Avg. |\\n|------------------------|------|-------------|---------|----------|---------|-----|------------------|---------|------------------|-------|----------|------|\\n| Whisper Small          | 24.1 | 31.0        | 20.0    | 17.6     | 25.0    | 20.2| 27.7             | 30.6    | 24.5             |       |          |      |\\n| Whisper Medium         | 30.6 | 20.0        | 17.8    | 42.6     |         |     |                  |         |                  |       |          |      |\\n| Whisper Large V2       | 27.5 | 24.0        | 26.7    | 20.2     | 20.1    | 19.6| 25.8             | 27.4    | 23.9             |       |          |      |\\n| CLAP Unfused           | 26.8 | 23.0        | 24.4    | 37.1     | 29.2    |     |                  | 32.9    | 23.1             | 19.7  | 27.0    | 23.1 |\\n| CLAP Large General     | 29.4 | 21.0        | 23.7    | 44.5     | 25.7    | 34.1| 21.1             | 20.3    |                  | 27.5  | 24.3    |     |\\n| CLAP Fused             | 21.5 | 30.0        | 18.5    | 18.4     | 25.7    | 20.8| 27.7             | 32.0    | 24.3             |       |          |      |\\n| Pengi Base             | 24.9 | 26.0        | 32.6    | 21.3     | 19.4    | 24.8| 24.0             | 24.4    | 24.7             |       |          |      |\\n| Base No Text Encoder   | 26.8 | 26.0        | 25.2    | 20.2     | 22.2    | 20.8| 24.3             | 25.9    | 23.9             |       |          |      |\\n| SpeechGPT E2E          | 28.3 | 23.0        | 29.6    | 17.6     | 21.5    | 27.2| 26.4             | 23.4    | 24.6             |       |          |      |\\n| SpeechGPT Oracle       | 36.2 | 32.0        | 27.4    | 35.7     | 29.9    | 34.1| 24.4             | 27.2    | 30.8             |       |          |      |\\n\\nProficiency in particular tasks, with none consistently outperforming others across all tasks. Notably, Whisper Medium showcases competitive zero-shot performance, surpassing cascade setups with Phi 1.5 despite having approximately half the parameters. CLAP's contrastive modeling outperforms Phi 1.5 but falls short of LLaMa 2 7B. Impressively, despite its smaller size\u2014153M parameters in its base form and 193M in its larger form\u2014CLAP performs remarkably well, being 14.7 times smaller than Whisper Large V2 combined with Phi 1.5 and 44.3 times smaller with LLaMa 2 7B. SpeechGPT encounters challenges in zero-shot tasks from speech, contrasting its performance with text (Oracle), highlighting difficulties in directly handling speech modality representations, which need to be addressed in the future, with a better alignment approach.\\n\\nNotably, Whisper, especially Whisper Medium, occasionally outperforms cascade configurations with Phi 1.5 in zero-shot scenarios. Specific tasks exhibit varying levels of difficulty for different models; for instance, MedMCQA yields high results with Whisper Small and CLAP Fused, while MMLU College Medicine favors Whisper Medium, CLAP Unfused, and CLAP Large General. SpeechGPT generally underperforms across most tasks, except for MMLU Anatomy and MedQA, where it outperforms most other models. Despite the small performance improvement over cascade systems, which is linked to the zero-shot setting, E2E systems can be enhanced by scaling with better quality SQA data and increasing the number of parameters to see if they follow scaling laws similar to LLMs.\\n\\n5. Analysis of encoder layers\\n\\nThis section presents an extensive analysis to pinpoint the critical location of information crucial for SQA tasks within the layers encoding the audio signal. To conduct this analysis, we extracted a subset of the MedMCQA training set consisting solely of audio sequences shorter than 30 seconds, which comprised 97.56% of the data, resulting in 120 hours of spoken data. This subset was partitioned into training and validation sets using an 80%/20% ratio, yielding 95 hours and 23 hours, respectively.\\n\\nOur experimental approach involves fine-tuning audio encoders and introducing an intermediate trainable layer of equal size to the number of encoder layers. This intermediate layer selects information from the encoder's layers through a weighted sum of their representations when feeding the classification head. The objective of this weighted encoder layers approach is to analyze the necessity of specific layers for executing the SQA task while enhancing model understanding.\\n\\nAs depicted in Figure 1, illustrating cumulative weights across encoder layers, Whisper models exhibit a propensity to concentrate information in the final layers, aligning with prior research findings [32]. This indicates that these audio-based models effectively utilize the last layer to represent textual information, possibly due to heavy reliance on the decoder.\\n\\nFigure 1: Cumulative weights according to encoder layers.\\n\\nIn contrast, Wav2Vec [33] and Data2Vec [34] primarily rely on a single intermediate layer, specifically the 15th and 21st layers, respectively. However, HuBERT [31] and WavLM [35] adopt a different strategy, integrating information from a broader range of layers. HuBERT integrates data from 12 layers, while WavLM incorporates information from 4 layers distributed across various regions of the encoder.\\n\\n6. Conclusion\\n\\nThis study introduces a novel synthetic Spoken Question Answering (SQA) dataset tailored specifically to the medical domain. We conducted zero-shot comparative analyses of end-to-end speech methodologies using a new entailment technique against cascade speech transcription and LLM module. Our experiments and analysis demonstrate the effectiveness of our end-to-end approach, yielding performances comparable to those achieved by cascade models of similar sizes. Moving forward, we aim to explore the utilization of speech alignment techniques with LLMs to enhance end-to-end question answering performance, with a particular emphasis on improving outcomes in low-resource domains such as healthcare. Our research faced multiple constraints. Using limited speaker variety for synthetic audio may reduce accuracy compared to natural speech, affecting response precision. Simplifying task formulation lacks genuine human interaction dynamics but enables metric-based assessments, enhancing model reproducibility and cost efficiency. Finally, our study neglects multilingual contexts, highlighting the need for additional exploration in diverse linguistic settings.\"}"}
{"id": "labrak24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgements\\n\\nThis work was performed using HPC resources from GENCI-IDRIS (Grant 2024-AD011015344 and Grant 2022-AD011013061R2). This work was financially supported by ANR MALADES (ANR-23-IAS1-0005) and Zenidoc.\\n\\n8. References\\n\\n[1] C.-H. Lee, Y.-N. Chen, and H.-Y. Lee, \u201cMitigating the impact of speech recognition errors on spoken question answering by adversarial domain adaptation,\u201d in ICASSP, 2019, pp. 7300\u20137304.\\n\\n[2] C.-H. Lee, S.-L. Wu, C.-L. Liu, and H. Yi Lee, \u201cSpoken SQuAD: A Study of Mitigating the Impact of Speech Recognition Errors on Listening Comprehension,\u201d in Interspeech, 2018, pp. 3459\u20133463.\\n\\n[3] C. You, N. Chen, and Y. Zou, \u201cKnowledge distillation for improved accuracy in spoken question answering,\u201d in ICASSP, 2021, pp. 7793\u20137797.\\n\\n[4] H. Inaguma, K. Duh, T. Kawahara, and S. Watanabe, \u201cMultilingual end-to-end speech translation,\u201d in ASRU, 2019, pp. 570\u2013577.\\n\\n[5] T. L. Scao, A. Fan, C. Akiki, and et al., \u201cBloom: A 176b-parameter open-access multilingual language model,\u201d 2023.\\n\\n[6] H. Touvron, L. Martin, K. Stone, and et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d 2023.\\n\\n[7] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d 2022.\\n\\n[8] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, \u201cLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,\u201d in ICASSP, 2023.\\n\\n[9] J. Ao, R. Wang, L. Zhou, and et al., \u201cSpeechT5: Unified-modal encoder-decoder pre-training for spoken language processing,\u201d in ACL, 2022, pp. 5723\u20135738.\\n\\n[10] D. Zhang, S. Li, X. Zhang, and et al., \u201cSpeechGPT: Empowering large language models with intrinsic cross-modal conversational abilities,\u201d in EMNLP, 2023, pp. 15,757\u201315,773.\\n\\n[11] K. Halder, A. Akbik, J. Krapac, and R. Vollgraf, \u201cTask-aware representation of sentences for generic text classification,\u201d in COLING, 2020, pp. 3202\u20133213.\\n\\n[12] M. P\u00e0mies, J. Llop, F. Multari, N. Duran-Silva, C. Parra-Rojas, A. Gonzalez-Agirre, F. A. Massucci, and M. Villegas, \u201cA weakly supervised textual entailment approach to zero-shot text classification,\u201d in EACL, 2023, pp. 286\u2013296.\\n\\n[13] Y. Du, J. Li, T. Tang, W. X. Zhao, and J.-R. Wen, \u201cZero-shot visual question answering with language model feedback,\u201d in Findings of the ACL, Toronto, Canada, 2023, pp. 9268\u20139281.\\n\\n[14] O.-B. Mercea, L. Riesch, A. S. Koepke, and Z. Akata, \u201cAudio-visual generalised zero-shot learning with cross-modal attention and language,\u201d in CVPR, June 2022, pp. 10,553\u201310,563.\\n\\n[15] S. Lipping, P. Sudarsanam, K. Drossos, and T. Virtanen, \u201cClotho-aqa: A crowdsourced dataset for audio question answering,\u201d 2022.\\n\\n[16] C.-H. Lee, S.-L. Wu, C.-L. Liu, and H.-y. Lee, \u201cSpoken squad: A study of mitigating the impact of speech recognition errors on listening comprehension,\u201d in Interspeech, pp. 3459\u20133463, 2018.\\n\\n[17] Z. Zhao, Y. Jiang, H. Liu, Y. Wang, and Y. Wang, \u201cLibrisqa: Advancing free-form and open-ended spoken question answering with a novel dataset and framework,\u201d 2023.\\n\\n[18] J. Kim, J. Kong, and J. Son, \u201cConditional variational autoencoder with adversarial learning for end-to-end text-to-speech,\u201d in ICML, vol. 139, 2021, pp. 5530\u20135540.\\n\\n[19] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in NeurIPS, 2020.\\n\\n[20] D. Hendrycks, C. Burns, S. Basart, A. Zou, M. Mazeika, D. Song, and J. Steinhardt, \u201cMeasuring massive multitask language understanding,\u201d in ICLR, 2021.\\n\\n[21] K. Singhal, T. Tu, J. Gottweis, and et al., \u201cTowards expert-level medical question answering with large language models,\u201d 2023.\\n\\n[22] D. Jin, E. Pan, N. Oufattole, W.-H. Weng, H. Fang, and P. Szolovits, \u201cWhat disease does this patient have? a large-scale open domain question answering dataset from medical exams,\u201d 2020.\\n\\n[23] A. Pal, L. K. Umapathi, and M. Sankarasubbu, \u201cMedmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering,\u201d in Proceedings of the Conference on Health, Inference, and Learning, vol. 174, 2022, pp. 248\u2013260.\\n\\n[24] C. Wu, W. Lin, X. Zhang, Y. Zhang, Y. Wang, and W. Xie, \u201cPmc-llama: Towards building open-source language models for medicine,\u201d 2023.\\n\\n[25] H. Nori, N. King, S. M. McKinney, D. Carignan, and E. Horvitz, \u201cCapabilities of gpt-4 on medical challenge problems,\u201d 2023.\\n\\n[26] Y. Labrak, A. Bazoge, E. Morin, P.-A. Gourraud, M. Rouvier, and R. Dufour, \u201cBiomistral: A collection of open-source pretrained large language models for medical domains,\u201d 2024.\\n\\n[27] P. Liang, R. Bommasani, T. Lee, and et al., \u201cHolistic evaluation of language models,\u201d Transactions on Machine Learning Research, 2023.\\n\\n[28] B. Romera-Paredes and P. Torr, \u201cAn embarrassingly simple approach to zero-shot learning,\u201d in ICML, vol. 37, 2015, pp. 2152\u20132161.\\n\\n[29] Y. Wu, K. Chen, T. Zhang, Y. Hui, T. Berg-Kirkpatrick, and S. Dubnov, \u201cLarge-scale contrastive language-audio pretraining with feature fusion and keyword-to-caption augmentation,\u201d in ICASSP, 2023, pp. 1\u20135.\\n\\n[30] S. Deshmukh, B. Elizalde, R. Singh, and H. Wang, \u201cPengi: An audio language model for audio tasks,\u201d in NeurIPS, 2023.\\n\\n[31] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Trans. Audio, Speech and Lang. Proc., vol. 29, p. 3451\u20133460, 2021.\\n\\n[32] H. Yang, J. Zhao, G. Haffari, and E. Shareghi, \u201cInvestigating Pre-trained Audio Encoders in the Low-Resource Condition,\u201d in Interspeech, 2023, pp. 1498\u20131502.\\n\\n[33] A. Baevski, H. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: a framework for self-supervised learning of speech representations,\u201d in Proceedings of the 34th International Conference on Neural Information Processing Systems, ser. NIPS\u201920. Red Hook, NY, USA: Curran Associates Inc., 2020.\\n\\n[34] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, \u201cdata2vec: A general framework for self-supervised learning in speech, vision and language,\u201d in ICML, vol. 162, 2022, pp. 1298\u20131312.\\n\\n[35] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao, J. Wu, L. Zhou, S. Ren, Y. Qian, J. Wu, M. Zeng, X. Yu, and F. Wei, \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\"}"}
