{"id": "fu23b_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. Babu, C. Wang, A. Tjandra et al., \u201cXls-r: Self-supervised cross-lingual speech representation learning at scale,\u201d in Proc. Interspeech, 2022.\\n\\n[2] A. Khurana, A. Laurent, and J. Glass, \u201cMagic dust for cross-lingual adaptation of monolingual wav2vec-2.0,\u201d in Proc. ICASSP, 2022.\\n\\n[3] J. Zhao and W. Zhang, \u201cImproving automatic speech recognition performance for low-resource languages with self-supervised models,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022.\\n\\n[4] J. Zuluaga-Gomez, A. Prasad, I. Nigmatulina et al., \u201cHow does pre-trained wav2vec 2.0 perform on domain-shifted asr? an extensive benchmark on air traffic control communications,\u201d in Proc. SLT, 2022.\\n\\n[5] B. Thomas, S. Kessler, and S. Karout, \u201cEfficient adapter transfer of self-supervised speech models for automatic speech recognition,\u201d in Proc. ICASSP, 2022.\\n\\n[6] A. Baevski, Y. Zhou, A. Mohamed et al., \u201cWav2vec2.0: A framework for self-supervised learning of speech representations,\u201d in Proc. NeurIPS, 2020.\\n\\n[7] W. Hsu, B. Bolte, Y. Tsai et al., \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, 2021.\\n\\n[8] S. Chen, C. Wang, Z. Chen et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022.\\n\\n[9] A. Mohamed, H. Lee, L. Borgholt et al., \u201cSelf-supervised speech representation learning: A review,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022.\\n\\n[10] A. Baevski and A. Mohamed, \u201cEffectiveness of self-supervised pre-training for asr,\u201d in Proc. ICASSP, 2020.\\n\\n[11] S. Yang, P. Chi, Y. Chuang et al., \u201cSuperb: Speech processing universal performance benchmark,\u201d in Proc. Interspeech, 2021.\\n\\n[12] Y. Chung, Y. Zhang, W. Han et al., \u201cW2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,\u201d in Proc. ASRU, 2021.\\n\\n[13] Y. Zhang, D. Park, W. Han et al., \u201cBigssl: Exploring the frontier of large-scale semi-supervised learning for automatic speech recognition,\u201d IEEE Journal of Selected Topics in Signal Processing, 2022.\\n\\n[14] C. Wang, Y. Wu, S. Liu et al., \u201cUnispeech at scale: An empirical study of pre-training method on large-scale speech recognition dataset,\u201d arXiv preprint arXiv:2107.05233, 2021.\\n\\n[15] A. Radford, J. Kim, T. Xu et al., \u201cRobust speech recognition via large-scale weak supervision,\u201d arXiv preprint arXiv:2212.04356, 2022.\\n\\n[16] J. Li, Y. Wu, Y. Gaur et al., \u201cOn the comparison of popular end-to-end models for large scale speech recognition,\u201d in Proc. Interspeech, 2020.\\n\\n[17] A. Gulati, J. Qin, C. Chiu et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d in Proc. Interspeech, 2020.\\n\\n[18] L. Fu, X. Li, R. Wang et al., \u201cScala: Supervised contrastive learning for end-to-end speech recognition,\u201d in Proc. Interspeech, 2022.\\n\\n[19] W. Hsu, A. Sriram, A. Baevski et al., \u201cRobust wav2vec 2.0: Analyzing domain shift in self-supervised pre-training,\u201d in Proc. Interspeech, 2021.\\n\\n[20] C. Zan, L. Ding, L. Shen et al., \u201cOn the complementarity between pre-training and random-initialization for resource-rich machine translation,\u201d in Proc. COLING, 2022.\\n\\n[21] S. Bucci, A. D'Innocente, Y. Liao et al., \u201cSelf-supervised learning across domains,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[22] H. Li, Z. Xu, G. Taylor et al., \u201cVisualizing the loss landscape of neural nets,\u201d in Proc. NeurIPS, 2018.\\n\\n[23] V. Soto, O. Siohan, M. Elfeky et al., \u201cSelection and combination of hypotheses for dialectal speech recognition,\u201d in Proc. ICASSP, 2016.\\n\\n[24] A. Arunkumar, V. Sukhadia, and S. Umesh, \u201cInvestigation of ensemble features of self-supervised pretrained models for automatic speech recognition,\u201d in Proc. Interspeech, 2022.\\n\\n[25] C. Tang, Y. Wang, X. Chen et al., \u201cExploring effective fusion algorithms for speech based self-supervised learning models,\u201d in Proc. NCMMSC, 2022.\\n\\n[26] T. Wu, T. Hsu, C. Li et al., \u201cThe efficacy of self-supervised speech models for audio representations,\u201d in Proc. HEAR (NeurIPS 2021 Competition), 2022.\\n\\n[27] Y. Sun and L. Fu, \u201cStacking ensemble learning for non-line-of-sight detection of global navigation satellite system,\u201d IEEE Transactions on Instrumentation and Measurement, 2022.\\n\\n[28] M. Yurochkin, M. Agarwal, S. Ghosh et al., \u201cBayesian nonparametric federated learning of neural networks,\u201d in Proc. ICML, 2019.\\n\\n[29] S. Singh and M. Jaggi, \u201cModel fusion via optimal transport,\u201d in Proc. NeurIPS, 2020.\\n\\n[30] D. Qiu, Q. Li, Y. He et al., \u201cLearning word-level confidence for subword end-to-end asr,\u201d in Proc. ICASSP, 2021.\\n\\n[31] A. Baevski, W. Hsu, Q. Xu et al., \u201cData2vec: A general framework for self-supervised learning in speech, vision and language,\u201d in Proc. ICML, 2022.\\n\\n[32] G. Hinton, O. Vinyals, and J. Dean, \u201cDistilling the knowledge in a neural network,\u201d in Proc. NeurIPS, 2015.\\n\\n[33] K. Huang, T. Feng, Y. Fu et al., \u201cEnsemble knowledge distillation of self-supervised speech models,\u201d in Proc. ICASSP, 2023.\\n\\n[34] S. Cao, Y. Kang, Y. Fu et al., \u201cImproving streaming transformer based asr under a framework of self-supervised learning,\u201d in Proc. Interspeech, 2021.\\n\\n[35] R. Takashima, L. Sheng, and H. Kawai, \u201cInvestigation of sequence-level knowledge distillation methods for ctc acoustic models,\u201d in Proc. ICASSP, 2019.\\n\\n[36] C. Talnikar, T. Likhomanenko, R. Collobert et al., \u201cJoint masked cpc and ctc training for asr,\u201d in Proc. ICASSP, 2021.\\n\\n[37] C. Wang, Y. Wu, Y. Qian et al., \u201cUnispeech: Unified speech representation learning with labeled and unlabeled data,\u201d in Proc. ICML, 2021.\\n\\n[38] J. Bai, B. Li, Y. Zhang et al., \u201cJoint unsupervised and supervised training for multilingual asr,\u201d in Proc. ICASSP, 2022.\\n\\n[39] N. Keskar, D. Mudigere, J. Nocedal et al., \u201cOn large-batch training for deep learning: Generalization gap and sharp minima,\u201d in Proc. ICLR, 2017.\\n\\n[40] H. Wang, M. Yurochkin, Y. Sun et al., \u201cFederated learning with matched averaging,\u201d in Proc. ICLR, 2020.\\n\\n[41] Z. Yao, D. Wu, X. Wang et al., \u201cWenet: Production oriented streaming and non-streaming end-to-end speech recognition toolkit,\u201d in Proc. Interspeech, 2021.\\n\\n[42] L. Fu, S. Li, Q. Li et al., \u201cUfo2: A unified pre-training framework for online and offline speech recognition,\u201d in Proc. ICASSP, 2023.\\n\\n[43] V. Panayotov, G. Chen, D. Povey et al., \u201cLibrispeech: An asr corpus based on public domain audio books,\u201d in Proc. ICASSP, 2015.\\n\\n[44] J. Yu, W. Han, A. Gulati et al., \u201cDual-mode asr: Unify and improve streaming asr with full-context modeling,\u201d in Proc. ICLR, 2021.\\n\\n[45] A. Graves, S. Fernandez, F. Gomez et al., \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proc. ICML, 2006.\"}"}
{"id": "fu23b_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nSelf-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates.\\n\\nIndex Terms: Automatic speech recognition, model fusion, optimal transport, self-supervised learning\\n\\n1. Introduction\\n\\nRecently, Self-Supervised Learning (SSL) has emerged as a successful paradigm to address the issue of label scarcity in low-resource Automatic Speech Recognition (ASR) tasks, e.g. multiple languages [1\u20133] and domain shift [4, 5]. It usually pre-trains a representation model on numerous unlabeled utterances, and then fine-tunes the model with a relatively small amount of labeled speech [6\u201312]. Nevertheless, the gains achieved by the pre-training might diminish when the amount of downstream labeled dataset increases [13\u201315]. Thus, there would be a dilemma in many resource-rich industry applications \u2013 Shall we train a Supervised Learning (SL) ASR model from scratch or fine-tune the pre-trained representation model? In general, SL models are optimized to perform well when large amounts of labeled and in-domain speech are available [16\u201318]; and SSL models are considered to have good generalization via pre-training on numerous unlabeled utterances [19\u201321]. While the performance gaps in Word/Character Error Rate (WER/CER) between the SL and SSL ASR models become small in resource-rich settings, we believe that empirically, the two models would contain diverse or even complementary abilities due to the training process being quite different. Based on this idea, as analyzed in Sec. 3.1, we first verify the potential of fusing SL and SSL models according to 1) Recognition accuracy: We roughly estimate the upper bound of model fusion via picking out the best hypothesis (HYP) of different models for each test sample, which implies a large improvement room on fusing these models; and 2) Optimization property: We study the optimization process of the two models via loss landscape [22], which qualitatively indicates the SSL and SL models' advantages in generalization and in-domain task learning, respectively.\\n\\nExisting model fusion methods mainly focus on how to aggregate the constituent models' output hypotheses or latent features. For example, Selection-Based Fusion (SBF) method (see Fig. 1(a)) was proposed to select the best hypothesis from different models via the designed criteria, e.g. confidence score [23]. Ensemble-Based Fusion (EBF) method (see Fig. 1(b)) was explored to combine the latent features of multiple encoders pre-trained in different frameworks [24\u201327]. However, both the SBF and EBF methods would inevitably suffer high computational cost at test time since each utterance is processed through all of these encoders. A naive solution might be aggregating all the models into a single one by directly averaging the model parameters. However, since the weight parameters of SL and SSL models are not one-to-one corresponded, such direct averaging is ineffective and may even damage the well-trained models. To circumvent this issue, matching weight before averaging was investigated in the computer vision domain [28, 29]. For example, Yurochkin et al. [28] proposed the Bayesian nonparametric matching method to align and average the image classification models on different edge devices for federated learning. Singh...\"}"}
{"id": "fu23b_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"et al. [29] explored the fusion of image classification models by leveraging optimal transport to align each layer's weight. However, discussion about the weight-based fusion for ASR models is quite rare, which might be more challenging than the image classification tasks [28, 29] since speech signals are sequential values [7].\\n\\nTo combine the SL and SSL models without increasing the inference cost, we propose an Optimal Transport based Fusion (OTF) method, which fuses the two ASR models into the same architecture and improves the performance in the following way (see Fig. 1(c)). First, inspired by the work of [29], we adopt optimal transport to softly align each layer's weights of the SL and SSL models. Specifically, a layer-wise transport map is estimated via minimizing the cost of transferring the distribution of the SL model's weight to the SSL model. Then, the input and output parameters of the SL model's weight are aligned by multiplying with the transport maps of preceding and current layers, respectively. Finally, to enhance the ASR performance, the aligned SL model is averaged with the SSL model and moderately fine-tuned on labeled data to obtain the fused model.\\n\\nExtensive experiments on different datasets show that OTF effectively fuses the SL and SSL models with obvious WER/CER reductions. Our main contributions are summarized as follows:\\n\\n\u2022 To the best of our knowledge, this is the first work exploring the fusion of SL and SSL models for speech recognition.\\n\u2022 We propose a novel approach, named OTF, which unifies SL and SSL ASR models efficiently without incurring an extra computational cost in inference.\\n\u2022 We verify the effectiveness of OTF, with discussion, on English and Chinese datasets with large WER/CER reductions compared with the individual models and baseline methods.\\n\\n2. Related Work\\n\\nModel fusion. In recent years, it has been shown favorable to fuse different models to enhance the ASR performance. Although the SBF method was explored to pick the better hypothesis of two ASR models without re-training, it would result in a suboptimal solution since the criteria used for hypothesis selection might not be accurate for each utterance [23, 30]. More recently, the EBF method was proposed to tightly couple two pre-trained models. Arunkumar et al. [24] investigated an ensemble model to combine the outputs of the last layer of HuBERT [7] and wav2vec2 [6] fine-tuned ASR models. Tang et al. [25] explored the combination of the multi-layer latent features of wav2vec2 [6] and data2vec [31]. Wu et al. [26] proposed an ensemble framework, with a combination of ensemble techniques to fuse SSL speech models' embedding. However, both the SBF and EBF methods would suffer high computational cost at test time with aggregating all the models' parameters. In contrast, our proposed OTF fuses the two ASR models into the same architecture via weight alignment, which will not incur an extra cost during inference.\\n\\nMulti-task learning. Another way to incorporate auxiliary abilities to the ASR model is multi-task learning. For example, distillation tasks [32] were added with ASR tasks to transfer the teacher model's (e.g. SSL model's) knowledge to the student model (e.g. SL model) [33\u201335]. The existing methods usually assume the teacher model is superior to the student model, while the performance gaps of the SL and SSL models might be small in resource-rich settings. The joint of SL and SSL training losses for ASR tasks were studied in [36\u201338]. However, as mentioned in [38], it might be difficult to balance the SSL and SL components systematically. Differently, our work focuses on weight-based fusion, which is more efficient to inherit the abilities of the individual models. Moreover, the multi-task learning method might also complement the moderate fine-tuning in the proposed OTF, which will be investigated in our future work.\\n\\n3. Our Proposed Approach\\n\\n3.1. Advantages for fusing SL and SSL ASR models\\n\\nComplementarity in recognition accuracy. Although the gaps in WER/CER of the SL and SSL ASR models are small in resource-rich settings, we find that the recognition errors of the two models might be different for a certain utterance. The potential gain by the fusion of SL and SSL models can be estimated via picking the best hypothesis among the individual models for each test utterance to calculate the error rate performance. It can be regarded as an approximated upper bound of model fusion, as shown in Table 1-2. Numerically, compared with the best among SL and SSL models, the relative WER/CER reduction of the upper bound can lead up to 21% for the English and Chinese models. This implies a large potential for improvement for the model fusion as we expected.\\n\\nComplementarity in optimization property. To find out the advantages of SL and SSL models with abundant speech labels, we analyze the optimization properties of the two models via the visualization of loss landscape [22]. Specifically, given the weights of an SL/SSL ASR model $\\\\theta$ and its initialized weights $\\\\theta_0$ before fine-tuning, we plot the loss landscape $L(\\\\theta(\\\\alpha))$ of the validation dataset, with $\\\\alpha$ the coefficient for the weights' linear interpolation $\\\\theta(\\\\alpha) = (1-\\\\alpha)\\\\theta_0 + \\\\alpha\\\\theta$; and with $L$ the ASR loss function used for fine-tuning. As shown in Fig. 2, the curves of the loss landscape show that each of SL and SSL has its advantage in the optimization process: 1) The flat minima points of SSL models present a better generalization than the SL models' sharp minima points [39]; and 2) The Chinese SL model achieves a better minima (and lower CERs on Chinese Cantonese test sets in Table 2) than the SSL model when large amounts of in-domain labeled speech is available. The results qualitatively indicate the SSL and SL models' complementarity in generalization and in-domain task learning, respectively.\\n\\n1Details about the models are shown in Sec. 4.1.\"}"}
{"id": "fu23b_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our work aims to fuse the SL and SSL ASR models into a single model with the same architecture while taking advantage of both models. To achieve this, we propose our method OTF, which matches similar weights in each layer for model aggregation (see Fig 3). Instead of searching over the space of permuting models, OTF softly aligns one model\u2019s weight to another model according to the cost (e.g. Euclidean distance) across the two weights\u2019 mutation matrices, transport mapping matrices are calculated to match similar weights in each layer for model aggregation (see Fig 3). Instead of searching over the space of permuting models, OTF softly aligns one model\u2019s weight to another model according to the cost (e.g. Euclidean distance) across the two weights\u2019 mutation matrices, transport mapping matrices are calculated to match similar weights in each layer for model aggregation.\\n\\nFor the SL model\u2019s weights, the transport mapping is calculated as\\n\\n\\\\[\\n\\\\hat{W}_l = \\\\sum_{m} \\\\alpha_{lm} W_{lm},\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\n\\\\alpha_{lm} = \\\\frac{\\\\exp(-D_{lm})}{\\\\sum_{n} \\\\exp(-D_{ln})},\\n\\\\]\\n\\nand\\n\\n\\\\[\\nD_{lm} = \\\\sqrt{\\\\frac{1}{2} \\\\sum_{i} \\\\sum_{j} (W_{li} - \\\\tilde{W}_{li}) (W_{mj} - \\\\tilde{W}_{mj})^2}.\\n\\\\]\\n\\nHere \\\\(\\\\hat{W}_l\\\\) is the target weights, \\\\(\\\\tilde{W}_l\\\\) is the source weights, and \\\\(D_{lm}\\\\) is the cost matrix. The target weights usually come from the SL model, and the source weights come from the SSL model. The transport mapping \\\\(\\\\alpha_{lm}\\\\) is the optimal solution to the linear programming problem.\\n\\nFor the two languages, we set the epoch number in the moderate fine-tuning of the proposed OTF as 10 for both languages. We use 1k-hour English and 10k-hour Mandarin-Cantonese unlabeled speech datasets to fine-tune the SL model, and then fine-tune the SSL model using 1k-hour English and 2.6k-hour Mandarin-Cantonese labeled data successively. Both the pre-training and fine-tuning are optimized with a mini-batch of 96 and using the same learning rate scheduler. The model consists of 2 convolutional sub-sampling layers and 12 Conformer blocks with dimension 512 for the encoder. The decoder is a 6-layer LSTM network with 512 hidden units. For the acoustic model, the input to the model is Mel-spectrograms of each utterance, pre-processed with a 25ms window size and 10ms step size. The 80-dimensional Mel-spectrograms are used for test-Mandarin and test-Cantonese. The 80-dimensional Mel-spectrograms are used for test-other.\\n\\nWe use the threshold of 0.3 as the threshold for testing the performance of OTF in both offline and online modes.\\n\\n### Table 1: WER performance of English systems and relative WER improvement of OTF over SSL model (in brackets)\\n\\n| Method            | WER (%) | Relative Improvement |\\n|-------------------|---------|----------------------|\\n| SL                | 12.7    | 0.0                  |\\n| SBF [23]          | 10.1 (+1.9) | 0.0                  |\\n| EBF [24]          | 9.4    | 0.0                  |\\n| OTF               | 7.9 (9.1)    | 0.0                  |\\n| SSL [42]          | 9.0 (12.4) | 0.0                  |\\n\\n### Table 2: CER performance of Chinese systems and relative CER improvement of OTF over SSL model (in brackets)\\n\\n| Method            | CER (%) | Relative Improvement |\\n|-------------------|---------|----------------------|\\n| SL                | 12.7    | 0.0                  |\\n| SBF [23]          | 10.1 (+1.9) | 0.0                  |\\n| EBF [24]          | 9.4    | 0.0                  |\\n| OTF               | 7.9 (9.1)    | 0.0                  |\\n| SSL [42]          | 9.0 (12.4) | 0.0                  |\\n\\nThe WER and CER of the ASR model are evaluated on the original test-clean dataset and only 2.6k-hour data are labeled, with a ratio of Mandarin to Cantonese 10:1. For LibriSpeech, the WER performance of the ASR model is evaluated on the original test-clean dataset.\"}"}
{"id": "fu23b_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Decoding. Following the representative SSL works [6, 7, 31], we evaluate the performance of all models using the Connectionist Temporal Classification (CTC) beam search decoder [45] with 10 the decoding beam size. Note that no language model is applied in our experiments.\\n\\n4.2. Main experiment\\n\\nBaseline methods. To evaluate the performance of our method on the fusion of SL and SSL models, we implement the SOTA EBF method via combining the outputs of the two models' encoders [24], and the existing SBF method based on the average word confidence [23] as our baseline methods.\\n\\nSystem performance. As shown in Table 1-2, we compare OTF with the performance of the SL and SSL models, and the baseline methods in offline and online modes. Compared with the SL and SSL models, OTF achieves a large improvement with lower WERs/CERs. Numerically, compared with the better of SL and SSL models, OTF yields relative WERs/CERs reductions up to 9.1% and 12.4% in English and Chinese, respectively. Although SBF and EBF are effective in model fusion, they suffer from a high inference cost with aggregating the two models' parameters. Besides, we find the baseline methods can cause a worse performance in some test sets. We infer 1) the confidence scoring of SBF might be not accurate enough for hypothesis selections; and 2) it would be challenging for EBF to align the features of two adequately-trained but diverse models. Compared with the baselines, our method consistently achieves the best performance in both the English and Chinese scenarios. Moreover, since each layer of the individual models' weights are fused into a single one, OTF is more efficient without increasing the model size than the baseline methods [23, 24].\\n\\n4.3. Ablation study and discussion\\n\\nTo further evaluate the effectiveness of OTF, we analyze the proposed method from the following three perspectives.\\n\\n1) Ablation study. To analyze the effect of weight alignment and moderate fine-tuning in OTF, we compare our proposed method with: direct averaging of the SL and SSL models; SL models with moderate fine-tuning; and SSL models with moderate fine-tuning (see Table 3-4).\\n\\n(a) Weight alignment. Before fine-tuning, we find that direct averaging of the two models will cause the recognition results to be all empty with 100% error rate. Although the performance can be substantially improved after fully fine-tuning on labeled data, the performance is inferior to the better one of the individual models. We infer the direct averaging of the two different models would damage the well-trained models and lead to a suboptimal solution. Differently, our method averages the layer-wise weights via alignment, which can still partly recognize the speech even without fine-tuning. We also find that the averaging after alignment for the ASR tasks achieves a larger performance degradation compared with the image classification tasks [29]. We infer it is more difficult to align the weights of sequence-to-sequence ASR models than the instance classification models. However, the proposed OTF still achieves the best performance than other methods with limited fine-tuning steps.\\n\\n(b) Moderate fine-tuning. As for the moderate fine-tuning in OTF, we compared SL and SSL models with the same fine-tuning to test if the performance improvement of our method is caused by more training steps. Since the SL and SSL are adequately trained, the performance is almost the same before and after the moderate fine-tuning. The experimental results show that our method fuses the two models effectively, and can obtain a better initialization to be fine-tuned for performance enhancement.\\n\\n2) Visualization of loss landscape. From the perspective of the optimization property, we plot the loss landscape of OTF, as shown in Fig. 2. The curves of loss landscape show that our method achieves the flattest and lowest minimum points than both the SL and SSL models. It implies that OTF fuses the two models and incorporates both of the individual advantages in generalization and in-domain optimization as we expected.\\n\\n3) Performance on small labeled datasets. As so far, we have verified the effectiveness of OTF on abundant datasets, i.e. one thousand hours and more. However, we still wonder about the performance when only a small amount of labeled dataset is available. Here, we assume that only a 100-hour train-clean subset of Librispeech [43] and a 50-hour Cantonese dataset are labeled. As shown in Table 5-6, the relative improvement of OTF decreases when there is a large performance gap between the two models. We infer that if there is a large gap between the two constituent models, the better one will dominate the results of model fusion. Nevertheless, our method can still improve the performance of the fused models with small labeled datasets.\\n\\n5. Conclusions\\n\\nWe proposed a novel weight-based fusion method for SL and SSL ASR models via optimal transport, which improved the recognition performance without increasing the inference cost. Extensive experiments on English and Chinese dataset were conducted to verify the effectiveness of the proposed method. However, models with the same architecture are used to test the performance, the fusion of heterogeneous models will be further investigated in our future work.\"}"}
