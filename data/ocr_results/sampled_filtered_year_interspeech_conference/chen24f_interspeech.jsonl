{"id": "chen24f_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detecting Empathy in Speech\\nRun Chen1, Haozhe Chen1, Anushka Kulkarni1, Eleanor Lin1, Linda Pang1, Divya Tadimeti1, Jun Shin1, Julia Hirschberg1\\n1 Columbia University, USA\\nrchchen@cs.columbia.edu, hc3295@columbia.edu, ajk2256@barnard.edu, eml2221@columbia.edu, sp4049@columbia.edu, dt2760@columbia.edu, js5810@columbia.edu, julia@cs.columbia.edu\\n\\nAbstract\\nEmpathy is the ability to understand another's feelings as if we were having those feelings ourselves. It has been shown to increase people's trust and likability. Much research has been done on creating empathetic responses in text in conversational systems, yet little work has been done to identify the acoustic-prosodic speech features that can create an empathetic-sounding voice. Our contributions include 1) collection of a new empathy speech dataset, 2) identifying interpretable acoustic-prosodic features that contribute to empathy expression and 3) benchmarking the empathy detection task.\\n\\nIndex Terms: empathy, computational paralinguistics, speech processing\\n\\n1. Introduction\\nMuch research has been done in the past 15 years on creating empathetic responses in text, facial expressions and gestures in conversational systems. However, little has been done to identify the speech features that can create an empathetic-sounding voice.\\n\\nEmpathy is the ability to understand another's feelings as if we were having those ourselves [1]. It can take several forms: cognitive empathy or \u201cperspective-taking\u201d, being able to put yourself in another's place \u2013 a particularly useful skill for managers; emotional empathy, actually feeling another person's emotion, also called \u201cemotional contagion\u201d, which can be overwhelming; and compassionate empathy \u2013 understanding another's pain as if we are having it ourselves and taking action to mitigate problems producing it [2, 3]. This third category has been found especially useful in dialogue systems and robots, since empathetic behavior can encourage users to like a speaker more, to believe the speaker is more intelligent, to actually take the speaker's advice, and to want to speak with the speaker longer and more often. Compassionate empathy can also be used to improve success in health-care advice-giving, as well as in negotiations and conflict resolution. Even when humans know that they are dealing with a computer system, if that system behaves empathetically, users will still like and trust it more [4].\\n\\nProducing empathetic responses requires first identifying a user's emotions to understand the need for such responses as well as the type of emotion the user is expressing and the reason for that emotion. Much research has been done to recognize the user's emotion and its cause from the user's words and sometimes from their speech. Much has also been done to create the appropriate emotional content of the system response \u2014 in some research projects also to provide appropriate facial expressions and gestures. But very little work has been done to discover what vocal cues can be used to create an empathetic-sounding voice. For empathy is more than simple agent emotional responses: to encourage users to connect with a conversational agent, that agent must present itself as empathetic even before the user expresses a need.\\n\\nOur goal is to identify the acoustic-prosodic as well as lexical aspects of speech that convey empathy \u2014 beyond merely producing appropriate emotion to address a user's particular issue or entraining to the user. We collect a new dataset of empathetic videos. We compare empathetic speech segments with neutral ones for changes in pitch, intensity, voice quality and speaking rate. We report empathetic lexical categories, specificity and readability levels. We also study how the speech features interact with the lexical content through ML modeling.\\n\\n2. Related work\\nPrevious work focused on developing multimodal avatars to produce feelings of engagement with the user, using different forms of listening behavior: backchannels, turn-ending identification, gestures, eyebrow raising, and other facial expressions. These include [5]'s Rea, a conversational agent; [6] and [7]'s Virtual Laboratory Exercise Agents, created to improve daily exercise interactions; [8] and [9]'s Rapport Agents with human-like listening behavior including backchannels, turn-ending identification, smiles and nods; [10]'s Greta, used to evaluate different methods of combining emotional facial expressions to produce empathy; [11]'s Jade Semantics Agents which added more empathetic emotions beyond happy and sad in email messages to the mix; [12]'s development of more rapport-building strategies using non-verbal behavior.\\n\\nWhile text-based empathetic chatbots have been created to detect and address users' negative emotions [13] and generate empathetic responses [14], little work has been done focusing on the speech aspect of empathy. Multimodal approaches incorporating text, audio, and speaker information have proven effective in predicting session-level empathy ratings [15, 16]. For turn-level empathy, [17] discovered that both pitch and intensity (loudness) were lower for both male and female speakers in empathetic speech than in neutral speech on their collected corpus of empathy and emotion labels on Italian call center conversations. More recent studies have investigated empathy in Cantonese [18], and Japanese [19], yet no publicly available speech dataset in English has been released.\\n\\nIn this work, we aim to identify empathetic speech using both acoustic-prosodic and lexical features of English YouTube video data. We have collected a large number of these and annotated segments in them as empathetic, neutral, or anti-empathetic in what is said and how it is said, as well as many other features of the videos to identify which are most watched and liked as well as different stages of empathetic speech.\\n\\nIn contrast to previous empathy studies where training data...\"}"}
{"id": "chen24f_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"were confidential, our dataset is sourced from publicly available\\nvideo platform and will be made accessible for future research.\\n\\n3. Dataset\\n3.1. Data Collection\\nWe have collected an empathetic dataset consisting of 346 En-\\nglish videos and about 53 hours in total.\\n\\nThe key dataset\\nstatistics are summarized in Table 1. These were manually col-\\nlected from Youtube through keyword searches, such as \\\"em-\\npathy\\\" and \\\"empathetic training\\\" from 2020 to 2022. They\\ninclude empathy training videos, acted therapy sessions, TV\\nshows, movies, interviews and TED Talks. The videos comprise\\n38% spontaneous and 62% acted speech. We identify metadata\\nfrom video platform APIs, including video and channel infor-\\nmation and viewer likes and comments. We also annotate addi-\\ntional information such as video category, speaker number and\\ngender, language, intended audience, and emotions expressed.\\n\\nEach video is rated by at least three expert annotators as \\\"em-\\npathetic\\\", \\\"neutral\\\" or \\\"anti-empathetic\\\" and taken the majority\\nvote.\\n\\n| Language | Count | Length | Category | Speakers | Topics |\\n|----------|-------|--------|----------|----------|--------|\\n| English  | 346   | 3s to 1.5h | 79% Empathetic | 38% Female | Social Work, Relationship, Therapy, Interview, Parenting, Workplace |\\n|          |       |         | 17% Neutral | 34% Male | |\\n|          |       |         | 2% Anti-empathetic | 27% Both | |\\n\\nTable 1: Empathetic Dataset Summary\\n\\n3.2. Data Annotations\\nTo gain better understanding of the empathetic speech, we select\\na subset of 65 videos for diarization and annotation for further\\nanalysis. Using the audio we obtain from the Youtube API,\\nwe first transcribe and diarize using pyannote\\ndiarization model. However, as the quality of the transcripts and alignments\\nrequire further manual correction, we re-align the transcripts us-\\nning the Praat interface shown in Figure 1. These videos were\\nannotated between 2023 and 2024 by 10 annotators and verified\\nby least one different annotator.\\n\\nThe manual re-alignment and annotation resulted in 1718\\nsegments with time stamps, transcripts, speakers, empathetic la-\\nlabs (\\\"empathetic\\\" or \\\"neutral\\\") and empathetic stages. We de-\\ndefine a segment as a natural sentence uttered by a speaker, poten-\\ntially shorter than a speaker turn. Each segment was sampled at\\n16k Hz, and we excluded any with music or noisy backgrounds\\nto ensure audio quality.\\n\\nWe also annotate four stages of empathetic behavior, a sim-\\nplification of empathy practices in therapy such as therapeu-\\ntic empathy system of empathetic attunement, attitude/stance,\\ncommunication, and technical/conceptual knowledge [20, 21]\\n\\n| Stage | Examples |\\n|-------|----------|\\n| 1 | \u201cHey, we all do.\u201d |\\n| 2 | \u201cWhen does Katherine come out in play?\u201d |\\n| 3 | \u201cKatherine who has a lot of hurt and unevolved feel-\\nings, I\u2019m taking your words.\u201d |\\n| 4 | \u201cThere\u2019s a kahuna principle, it\u2019s all about where we\\nget right energy to and our attention to ...so Katie is\\nbigger than life but Katherine gets a little bit of time,\\nso she can be just as evolved and happy and content.\u201d |\\n\\nTable 2: Examples of four stages of empathy at the segment-\\nlevel annotations from an interview between a therapist and\\nKaty Perry.\\n\\nSegment-level annotation yields 771 empathetic and 947 neu-\\ntral segments. The average length of a segment is 3.01 seconds\\n(empathetic 3.74 sec and neutral 2.43 sec). We use these man-\\nually annotated segments for developing empathy classification\\nmodels.\\n\\n4. Empathy Analysis\\nTo investigate the role of text and speech in conveying empathy,\\nwe employ significance tests on interpretable lexical and speech\\nfeatures. Specifically, we conduct unpaired t-tests on these fea-\\ntures extracted from 771 empathetic segments and 947 neutral\"}"}
{"id": "chen24f_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Speech Analysis\\n\\nWe extract a set of 12 acoustic-prosodic features representing the pitch, energy, voice quality and speaking rate of speakers: pitch mean, minimum, maximum, and standard deviation, intensity mean, minimum, maximum, and standard deviation, jitter, shimmer, harmonics-to-noise ratio (HNR), and speaking rate. These features are extracted with praat [22] and parseltongue [23] tools on default parameter settings. The speaking rate is measured in words per second from human-annotated transcripts. Additionally, we obtain 384 low-level features identified using the Interspeech 2009 (IS09) ComParE Challenge OpenSMILE baseline feature set, a standard benchmark feature set for many computational paralinguistic tasks [24, 25]. The OpenSMILE feature size is comparable to RoBERTa textual embeddings dimensions, preventing the model from ignoring speech information in the training.\\n\\nWe run independent t-tests for speech features extracted from the empathetic segments against those from the neutral segments. We apply Bonferroni correction to the p-values to control for errors in multiple testings. In Table 3, most acoustic-prosodic features are significantly different.\\n\\nThe empathetic speech is significantly lower in pitch minimum, mean and standard deviation, consistent with our expectation of a typical lower, flatter \u201ctherapist tone\u201d. The empathetic speech also has significantly lower minimum, mean and maximum intensities but higher standard deviations in intensities. This corresponds to a quieter, softer but more varied speech. Higher jitter and shimmer are usually associated with the breathiness of a calming voice. A lower speaking rate can also help to convey the empathetic message that one hears and understands the other. These results all align with our expectation that a comforting and soothing empathetic speaker typically features a lower, softer and slower voice.\\n\\nWe train a random forest (RF) classifier using the 12 acoustic-prosodic features, to distinguish between empathetic and neutral speech segments. The empathetic segments are downsampled to create a balanced set. With an 80/20 train/test split, the RF model achieves 0.540 accuracy and 0.587 F1 score. After model fitting, the Gini importance for the classifier identifies pitch mean and intensity standard deviation as the most crucial features (both about 0.11), although overall the normalized importance scores distribute approximately uniformly. These findings are consistent with our earlier t-test results, which highlighted pitch and intensity as the most significant indicator of empathy.\\n\\n4.2. Lexical Analysis\\n\\nWe further investigate lexical features associated with empathy, including significant LIWC dictionary categories [26], lexical diversity [27], concreteness scores [28], hedging frequencies [29, 30] and readability scores [31, 32]. Although we are able to identify a few lexical features that are characteristic of our empathetic dataset, the textual content itself alone may not be sufficient for us to understand or convey empathy, in contrast to the speech analysis in the previous section 4.1.\\n\\nThe LIWC dictionary categories [26] range from linguistic dimensions, psychological processes, personal concerns to spoken categories. In our analysis, we pinpoint specific LIWC lexical categories that exhibit notable frequency changes in empathetic speech, including assent, informal, anx, feel, tentat, negemo, cause. This suggests that when expressing empathy, individuals tend to express agreement, speak informally, emphasize the perceptual process of feeling, utilize vocabulary associated with tentative and causation cognitive processes, and discuss negative emotions like anxiety more frequently. These linguistic choices align with the empathetic goal of understanding and connecting with the other person's feelings.\\n\\nThe empathetic text has slightly lower lexical diversity. The averaged type to text ratio for empathetic and neutral segments are 0.141 and 0.170, respectively. [27]'s Measure of Textual Lexical Diversity (MTLD) for empathetic and neutral segments are 43.04 and 49.37, respectively. This could be attributed to the fact that empathy is typically manifested through the process of generalization and abstraction from the specific circumstances that give rise to the emotions of the other speaker, a phenomenon often observed in Stage 3 of empathetic responses.\\n\\nThe hedge phrase frequencies are very similar between empathetic and neutral speech, though empathetic segments have slightly lower frequencies. Relational hedges [29], which distance the speaker's relation to the propositional content, occur with a frequency of 0.00686 in empathetic speech and 0.00701 in neutral speech. The most common relational hedges for empathy include words in the LIWC cognitive processes category, such as \u201cknow\u201d, \u201cfeel\u201d and \u201cthink\u201d. Propositional hedges [31], which introduce uncertainty into the propositional content itself, appear at a rate of 0.00456 in empathetic speech and 0.00509 in neutral speech. The most common propositional hedges for empathy are \u201clike\u201d, \u201cabout\u201d, \u201creally\u201d and \u201ckind of\u201d.\\n\\nWe speculate that empathetic speakers may employ clearer and less ambiguous language when presenting their advice to their interlocutors, a strategy we observe in Stage 4.\\n\\nSimilarly, the concreteness scores [28] are comparable for empathetic and neutral speech. The empathetic segments averaged unigram score is 1.81 (std 0.68) and bigram score 3.18 (std 0.79), whereas the neutral segments averaged unigram score is 1.87 (std 0.72) and bigram score 3.11 (std 0.96). However, as the difference is minimal, such similarity in concreteness, as well as the frequency of hedge words between empathetic and neutral speech highlights the crucial role of acoustic-prosodic cues in conveying empathy.\"}"}
{"id": "chen24f_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lower readability scores indicate the complexity of empathetic speech. The Flesch Reading Ease scores [31] for empathetic and neutral transcripts are 29.97 and 63.06, respectively, indicating that empathetic speech is significantly more challenging to read and comprehend. The Dale-Chall Readability score [32] for empathetic segments are 8.35, which corresponds to a text level understandable by 11th or 12th-grade students. In contrast, neutral segments have a higher score of 6.98, matching a 7th or 8th-grade student level. This suggests that empathy utterances are more difficult to understand, as empathetic speakers often demonstrate their understanding by deepening or adding complexity to their interlocutors' experience, a strategy we observe in Stage 3.\\n\\n5. Empathy Classification and Results\\n\\nTo assess the impact of speech cues, we conduct an ablation study by comparing model performance with and without textual and speech information. We fine-tune a pretrained roberta-base model on our dataset [33]. Addressing the class imbalance between empathetic and neutral, we downsample empathetic data, resulting in a balanced dataset of equal number of empathetic and neutral segments. We then divide the data into training and validation sets with a 80/20 split ratio with StratifiedGroupKFold (n_splits=5).\\n\\nBaseline \u201cRoBERTa\u201d: The baseline textual model is a pretrained roberta-base RobertaForSequenceClassification model, with tokenized transcripts as input, finetuned for binary classification with lr=2e-5, batch size=16, for 20 epochs.\\n\\n\u201cRoBERTa+openSMILE\u201d: The multimodal model combines signals from both text and speech (Figure 2). Each segment transcript is encoded with a pretrained roberta-base encoder and passed through a pretrained roberta-base model with frozen parameters. The 384 dimensional IS09 openSMILE feature vector representing the speech signal goes through 6 fully connected layers, each followed by a ReLU activation and 0.1 dropout. Then outputs from both text and speech are concatenated and fed into 8 fully connected layers, each followed by a ReLU activation and 0.1 dropout, except the last output layer. The model is trained with AdamW optimizer (lr=2e-5, eps=1e-8), batch size = 8, epochs = 10.\\n\\nAll models are trained on Tesla T4 GPU.\\n\\n| Model               | Val. Acc | F1 score |\\n|---------------------|----------|----------|\\n| RoBERTa             | 0.528    | 0.603    |\\n| RoBERTa + openSMILE | 0.781    | 0.840    |\\n| RandomForest        | 0.540    | 0.587    |\\n\\nTable 4: Model performance on the empathetic/neutral binary classification task. Accuracy and F1 score on the held-out validation set.\\n\\nThe classification results are summarized in Table 4. The RoBERTa text-only model achieves 0.528 accuracy and 0.603 F1 score, with empathy class accuracy of 0.545 and neutral class accuracy of 0.496. The RoBERTa + openSMILE model performance peaked at epoch 3 with accuracy 0.781 and 0.840 F1 score, among with the empathy class accuracy 0.881 and neutral 0.591. The RandomForest results are copied from Section 4.1 for comparison.\\n\\nWe observe a huge performance improvement in a model utilizing both text and speech. This experiment demonstrates that speech features play a valuable role in enhancing the model's ability to predict empathy. It underscores that text alone may not be sufficient to convey empathy, emphasizing the need of integrating acoustic-prosodic information into conversational agents, as misalignment between text and speech expression often leads to ineffective or even sarcastic responses.\\n\\n6. Conclusions\\n\\nWe have collected a new empathy corpus of English empathetic videos. Our analysis on this dataset reveals distinctive characteristics of empathetic voices and texts. Empathetic voices tend to be lower, softer and slower, compared to neutral speech; and empathetic texts are emotion-based, less diverse and slightly more complex. These results are useful in guiding the development of empathetic conversational agents. We benchmark the empathy classification task with the RoBERTa model. The classification results underline the importance of speech in conveying empathy beyond the text. As we are releasing the dataset to the public, the research community can use our collected data to train their own models for tasks such as empathy detection and empathetic text-to-speech synthesis.\\n\\nIn the future, we plan to identify acoustic-prosodic and lexical features associated with different stages of empathy for more fine-grained analysis that could enhance training empathetic chatbots as well as therapists in their practice. We also plan to incorporate other modalities which are currently not utilized in our models to investigate how facial expressions and gestures in the videos cooperate with speech to convey empathy.\\n\\nFurthermore, we have been collecting and annotating additional empathy data in Mandarin, with the aim of conducting similar analyses as we have with our English dataset. As one may speculate that empathy expression may vary with different language and cultures, this expansion will enable us to explore cross-linguistic and cross-cultural dimensions of empathy expression.\"}"}
{"id": "chen24f_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgements\\n\\nThis research is being developed with funding from the Columbia Center of AI Technology (CAIT) research award and the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.\\n\\n8. References\\n\\n[1] R. F. Baumeister and K. D. Vohs, Encyclopedia of social psychology. Sage, 2007, vol. 1.\\n[2] M. L. Healey and M. Grossman, \u201cCognitive and affective perspective-taking: evidence for shared and dissociable anatomical substrates,\u201d Frontiers in neurology, vol. 9, p. 491, 2018.\\n[3] J. L. Goetz, D. Keltner, and E. Simon-Thomas, \u201cCompassion: an evolutionary analysis and empirical review.\u201d Psychological bulletin, vol. 136, no. 3, p. 351, 2010.\\n[4] G. M. Lucas, J. Boberg, D. Traum, R. Artstein, J. Gratch, A. Gainer, E. Johnson, A. Leuski, and M. Nakano, \u201cGetting to know each other: The role of social dialogue in recovery from errors in social robots,\u201d in 2018 13th ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2018, pp. 344\u2013351.\\n[5] J. Cassell, \u201cEmbodied conversational agents: Representation and intelligence in user interfaces,\u201d AI Mag., vol. 22, no. 4, p. 67\u201383, oct 2001.\\n[6] T. W. Bickmore and R. W. Picard, \u201cEstablishing and maintaining long-term human-computer relationships,\u201d ACM Trans. Comput.-Hum. Interact., vol. 12, no. 2, p. 293\u2013327, jun 2005. [Online]. Available: https://doi.org/10.1145/1067860.1067867\\n[7] T. Bickmore, D. Schulman, and L. Yin, \u201cMaintaining engagement in long-term interventions with relational agents,\u201d Applied artificial intelligence : AAI, vol. 24, pp. 648\u2013666, 07 2010.\\n[8] J. Gratch, N. Wang, J. Gerten, E. Fast, and R. Duffy, \u201cCreating rapport with virtual agents,\u201d in Intelligent Virtual Agents, C. Pelachaud, J.-C. Martin, E. Andr\u00e9, G. Chollet, K. Karpouzis, and D. Pel\u00e9, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg, 2007, pp. 125\u2013138.\\n[9] L. Huang, L.-P. Morency, and J. Gratch, \u201cVirtual rapport 2.0,\u201d Intelligent Virtual Agents, pp. 68\u201379, 2011.\\n[10] R. Niewiadomski, E. Bevacqua, M. Mancini, and C. Pelachaud, \u201cGreta: An interactive expressive eca system,\u201d in Proceedings of The 8th International Conference on Autonomous Agents and Multiagent Systems - Volume 2, ser. AAMAS '09. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2009, p. 1399\u20131400.\\n[11] M. Ochs, C. Pelachaud, and D. Sadek, \u201cAn empathic virtual dialog agent to improve human-machine interaction,\u201d in Proceedings of the 7th International Joint Conference on Autonomous Agents and Multiagent Systems - Volume 1, ser. AAMAS '08. Richland, SC: International Foundation for Autonomous Agents and Multiagent Systems, 2008, p. 89\u201396.\\n[12] R. Zhao, T. Sinha, A. Black, and J. Cassell, \u201cSocially-aware virtual agents: Automatically assessing dyadic rapport from temporal patterns of behavior,\u201d in Intelligent Virtual Agents, 2016.\\n[13] A. Ghandeharioun, D. McDuff, M. Czerwinski, and K. Rowan, \u201cEmma: An emotion-aware wellbeing chatbot,\u201d in International Conference on Affective Computing and Intelligent Interaction, September 2019. [Online]. Available: https://www.microsoft.com/en-us/research/publication/emma-an-emotion-aware-wellbeing-chatbot/\\n[14] H. Rashkin, E. M. Smith, M. Li, and Y.-L. Boureau, \u201cTowards empathetic open-domain conversation models: A new benchmark and dataset,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Florence, Italy: Association for Computational Linguistics, Jul. 2019, pp. 5370\u20135381. [Online]. Available: https://aclanthology.org/P19-1534\\n[15] J. Gibson, D. Can, B. Xiao, Z. E. Imel, D. C. Atkins, P. Georgiou, and S. S. Narayanan, \u201cA Deep Learning Approach to Modeling Empathy in Addiction Counseling,\u201d in Proc. Interspeech 2016, 2016, pp. 1447\u20131451.\\n[16] T. Tran, Y. Yin, L. Tavabi, J. Delacruz, B. Borsari, J. D. Woolley, S. Scherer, and M. Soleymani, \u201cMultimodal analysis and assessment of therapist empathy in motivational interviews,\u201d in Proceedings of the 25th International Conference on Multimodal Interaction, ser. ICMI '23. New York, NY, USA: Association for Computing Machinery, 2023, p. 406\u2013415. [Online]. Available: https://doi.org/10.1145/3577190.3614105\\n[17] F. Alam, M. Danieli, and G. Riccardi, \u201cAnnotating and modeling empathy in spoken conversations,\u201d Comput. Speech Lang., vol. 50, no. C, p. 40\u201361, jul 2018. [Online]. Available: https://doi.org/10.1016/j.csl.2017.12.003\\n[18] D. Tao, T. Lee, H. Chui, and S. Luk, \u201cCharacterizing Therapist\u2019s Speaking Style in Relation to Empathy in Psychotherapy,\u201d in Proc. Interspeech 2022, 2022, pp. 2003\u20132007.\\n[19] Y. Saito, Y. Nishimura, S. Takamichi, K. Tachibana, and H. Saruwatari, \u201cSTUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent,\u201d in Proc. Interspeech 2022, 2022, pp. 5155\u20135159.\\n[20] M. Fuller, E. Kamans, M. van Vuuren, M. Wolfensberger, and M. D. de Jong, \u201cConceptualizing empathy competence: a professional communication perspective,\u201d Journal of business and technical communication, vol. 35, no. 3, pp. 333\u2013368, 2021.\\n[21] B. D. Jani, D. N. Blane, and S. W. Mercer, \u201cThe role of empathy in therapy and the physician-patient relationship,\u201d Complementary Medicine Research, vol. 19, no. 5, pp. 252\u2013257, 2012.\\n[22] P. Boersma and D. Weenink, \u201cPraat: doing phonetics by computer [Computer program],\u201d Version 6.2.14, retrieved 24 May 2022 http://www.praat.org/, 1992-2022.\\n[23] Y. Jadoul, B. Thompson, and B. de Boer, \u201cIntroducing Parselmouth: A Python interface to Praat,\u201d Journal of Phonetics, vol. 71, pp. 1\u201315, 2018.\\n[24] B. Schuller, S. Steidl, and A. Batliner, \u201cThe interspeech 2009 emotion challenge,\u201d in Interspeech 2009, 2009.\\n[25] F. Eyben, M. W\u00f6llmer, and B. Schuller, \u201cOpensmile: the munich versatile and fast open-source audio feature extractor,\u201d in Proceedings of the 18th ACM international conference on Multimedia, 2010, pp. 1459\u20131462.\\n[26] J. W. Pennebaker, R. Booth, R. L. Boyd, and M. Francis, Linguistic Inquiry and Word Count: LIWC2015, Austin, TX, Sep. 2015.\\n[27] P. M. McCarthy and S. Jarvis, \u201cMtld, vocd-d, and hd-d: A validation study of sophisticated approaches to lexical diversity assessment,\u201d Behavior research methods, vol. 42, no. 2, pp. 381\u2013392, 2010.\\n[28] M. Brysbaert, A. B. Warriner, and V. Kuperman, \u201cConcreteness ratings for 40 thousand generally known english word lemmas,\u201d Behavior research methods, vol. 46, pp. 904\u2013911, 2014.\\n[29] A. Prokofieva and J. Hirschberg, \u201cHedging and speaker commitment,\u201d in 5th Intl. Workshop on Emotion, Social Signals, Sentiment & Linked Open Data, Reykjavik, Iceland, 2014.\\n[30] E. F. Prince, J. Frader, C. Bosk et al. \u201cOn hedging in physician-physician discourse,\u201d Linguistics and the Professions, vol. 8, no. 1, pp. 83\u201397, 1982.\\n[31] J. P. Kincaid, R. P. Fishburne Jr, R. L. Rogers, and B. S. Chissom, \u201cDerivation of new readability formulas (automated readability index, fog count and flesch reading ease formula) for navy enlisted personnel,\u201d Naval Technical Training Command Millington TN Research Branch, Tech. Rep., 1975.\\n[32] E. Dale and J. S. Chall, \u201cA formula for predicting readability: Instructions,\u201d Educational research bulletin, pp. 37\u201354, 1948.\\n[33] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. Stoyanov, \u201cRoberta: A robustly optimized bert pretraining approach,\u201d arXiv preprint arXiv:1907.11692, 2019.\"}"}
