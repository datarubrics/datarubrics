{"id": "xu22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] S. Chu, S. Narayanan, and C.-C. J. Kuo, \u201cEnvironmental sound recognition with time\u2013frequency audio features,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 17, no. 6, pp. 1142\u20131158, 2009.\\n\\n[2] C. Mydlarz, J. Salamon, and J. P. Bello, \u201cThe implementation of low-cost urban acoustic monitoring devices,\u201d Applied Acoustics, vol. 117, pp. 207\u2013218, 2017.\\n\\n[3] L. Lu, H.-J. Zhang, and H. Jiang, \u201cContent analysis for audio classification and segmentation,\u201d IEEE Trans. Speech Audio Process., vol. 10, no. 7, pp. 504\u2013516, 2002.\\n\\n[4] M. A. T. Turan and E. Erzin, \u201cMonitoring infant\u2019s emotional cry in domestic environments using the capsule network architecture,\u201d in ISCA Interspeech, 2018, pp. 132\u2013136.\\n\\n[5] C. Ji, T. B. Mudiyanselage, Y. Gao, and Y. Pan, \u201cA review of infant cry analysis and classification,\u201d Eurasip Journal on Audio, Speech, and Music Processing, vol. 2021, no. 1, pp. 1\u201317, 2021.\\n\\n[6] L. Abou-Abbas, C. Tadj, and H. A. Fersaie, \u201cA fully automated approach for baby cry signal segmentation and boundary detection of expiratory and inspiratory episodes,\u201d J. Acoust. Soc. Amer., vol. 142, no. 3, pp. 1318\u20131331, 2017.\\n\\n[7] R. D. Kent and A. D. Murray, \u201cAcoustic features of infant vocalic utterances at 3, 6, and 9 months,\u201d J. Acoust. Soc. Amer., vol. 72, no. 2, pp. 353\u2013365, 1982.\\n\\n[8] M. Petroni, A. S. Malowany, C. C. Johnston, and B. J. Stevens, \u201cClassification of infant cry vocalizations using artificial neural networks (anns),\u201d in IEEE Int. Conf. Acoust., Speech, Signal Process. (ICASSP), vol. 5, 1995, pp. 3475\u20133478.\\n\\n[9] A. S. Warlaumont, D. K. Oller, E. H. Buder, R. Dale, and R. Kozma, \u201cData-driven automated acoustic analysis of human infant vocalizations using neural network tools,\u201d J. Acoust. Soc. Amer., vol. 127, no. 4, pp. 2563\u20132577, 2010.\\n\\n[10] S. Yeh, G. Chao, B. Su, Y. Huang, M. Lin, Y. Tsai, Y. Tai, Z. Lu, C. Chen, T. Tai et al., \u201cUsing attention networks and adversarial augmentation for styrian dialect continuous sleepiness and baby sound recognition.\u201d in ISCA Interspeech, 2019, pp. 2398\u20132402.\\n\\n[11] M. K. Ebrahimpour, S. Schneider, D. C. Noelle, and C. T. Kello, \u201cInfantnet: A deep neural network for analyzing infant vocalizations,\u201d arXiv preprint arXiv:2005.12412, 2020.\\n\\n[12] G. M. Pretzer, L. D. Lopez, E. A. Walle, and A. S. Warlaumont, \u201cInfant-adult vocal interaction dynamics depend on infant vocal type, child-directedness of adult speech, and timeframe,\u201d Infant Behavior and Development, vol. 57, p. 101325, 2019.\\n\\n[13] T. N. Maghfira, T. Basaruddin, and A. Krisnadhi, \u201cInfant cry classification using cnn\u2013rnn,\u201d in Journal of Physics: Conference Series, vol. 1528, no. 1. IOP Publishing, 2020, p. 012019.\\n\\n[14] C. Ji, M. Chen, B. Li, and Y. Pan, \u201cInfant cry classification with graph convolutional networks,\u201d arXiv preprint arXiv:2102.02909, 2021.\\n\\n[15] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2016, pp. 770\u2013778.\\n\\n[16] B. Schuller, S. Steidl, A. Batliner, P. B. Marschik, H. Baumeister, F. Dong, S. Hantke, F. B. Pokorny, E.-M. Rathner, K. D. Bartl-Pokorny et al., \u201cThe interspeech 2018 computational paralinguistics challenge: Atypical & self-assessed affect, crying & heart beats,\u201d in ISCA Interspeech, 2018, pp. 122\u2013126.\\n\\n[17] P. B. Marschik, F. B. Pokorny, R. Peharz, D. Zhang, and et al., \u201cA novel way to measure and predict development: a heuristic approach to facilitate the early detection of neurodevelopmental disorders,\u201d Current neurology and neuroscience reports, vol. 17, no. 5, p. 43, 2017.\\n\\n[18] S. Sabour, N. Frosst, and G. E. Hinton, \u201cDynamic routing between capsules,\u201d in Proc. of NIPS, vol. 30, 2017.\\n\\n[19] F. Anders, M. Hlawitschka, and M. Fuchs, \u201cComparison of artificial neural network types for infant vocalization classification,\u201d IEEE/ACM Trans. Audio, Speech, Language Process., vol. 29, pp. 54\u201367, 2020.\\n\\n[20] J. Li, M. Hasegawa-Johnson, and N. L. McElwain, \u201cAnalysis of acoustic and voice quality features for the classification of infant and mother vocalizations,\u201d ELSEVIER Speech Commun., vol. 133, pp. 41\u201361, 2021.\\n\\n[21] X. Wei, Y.-X. Li, L. Zhong, and J.-B. Liang, \u201cDistinguishing infant cry from adult voice based on spectrum analysis,\u201d in 2012 International Conference on Audio, Language and Image Processing. IEEE, 2012, pp. 333\u2013336.\\n\\n[22] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, and et al., \u201cAn image is worth 16x16 words: Transformers for image recognition at scale,\u201d in International Conference on Learning Representations, 2021.\\n\\n[23] M. Chen, X. He, J. Yang, and H. Zhang, \u201c3-d convolutional recurrent neural networks with attention model for speech emotion recognition,\u201d IEEE Signal Process. Lett., vol. 25, no. 10, pp. 1440\u20131444, 2018.\\n\\n[24] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, and et al., \u201cAttention is all you need,\u201d in Proc. of NIPS, vol. 30, 2017, pp. 5998\u20136008.\\n\\n[25] H.-Y. Zhou, C. Lu, S. Yang, and Y. Yu, \u201cConvnets vs. transformers: Whose visual representations are more transferable?\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 2230\u20132238.\\n\\n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proceedings of NAACL-HLT, 2019, pp. 4171\u20134186.\\n\\n[27] J. L. Ba, J. R. Kiros, and G. E. Hinton, \u201cLayer normalization,\u201d arXiv preprint arXiv:1607.06450, 2016.\\n\\n[28] F. Eyben, M. W\u00f6llmer, and B. Schuller, \u201cOpensmile: the munich versatile and fast open-source audio feature extractor,\u201d in Proceedings of the 18th ACM MM, 2010, pp. 1459\u20131462.\\n\\n[29] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I. H. Witten, \u201cThe weka data mining software: an update,\u201d ACM SIGKDD explorations newsletter, vol. 11, no. 1, pp. 10\u201318, 2009.\\n\\n[30] M. Freitag, S. Amiriparian, S. Pugachevskiy, N. Cummins, and B. Schuller, \u201caudeep: Unsupervised learning of representations from audio with deep recurrent neural networks,\u201d The Journal of Machine Learning Research, vol. 18, no. 1, pp. 6340\u20136344, 2017.\\n\\n[31] M. Schmitt and B. Schuller, \u201cOpenxbow: introducing the pasau open-source crossmodal bag-of-words toolkit,\u201d The Journal of Machine Learning Research, vol. 18, 2017.\\n\\n[32] P. Tzirakis, S. Zafeiriou, and B. W. Schuller, \u201cEnd2you\u2013the imperial toolkit for multimodal profiling by end-to-end learning,\u201d arXiv preprint arXiv:1802.01115, 2018.\"}"}
{"id": "xu22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition\\n\\nHai-tao Xu1, Jie Zhang1,2, Li-rong Dai1\\n\\n1University of Science and Technology of China (USTC), Hefei, China\\n2State Key Laboratory of Acoustics, Institute of Acoustics, Chinese Academy of Sciences, China\\n\\nxuht06@mail.ustc.edu.cn, jzhang6@ustc.edu.cn, lrdai@ustc.edu.cn\\n\\nAbstract\\nCrying is the main way for babies to communicate with the outside world. Analyzing cry enables not only the identification of babies' needs/thoughts they want to express, but also the prediction of potential diseases. In general, it is much more difficult to recognize special needs and emotions from infant cry than adults, because infant cry does not contain any linguistic information and the emotional expression is not as rich as adults. In this work, we focus on the time-frequency characteristics of infant crying signals and propose a differential time-frequency log-Mel spectrogram features based vision transformer (ViT) approach for infant cry recognition (ICR). We first calculate the deltas of log-Mel spectrogram of infant crying sounds over time frames and frequencies, respectively. The log-Mels and deltas are then combined as a 3-D feature representation and fed into the ViT model for cry classification. Experimental results on the CRIED database show the superiority of the proposed system over comparison methods and that the combination of log-Mels, the time-frame delta and frequency-bin delta achieves the best performance. The proposed method is further validated on a self-recorded dataset.\\n\\nIndex Terms: infant cry recognition, three-dimensional log-Mels, vision transformer, transfer learning, differential.\\n\\n1. Introduction\\nRecently, with the advance in deep learning and large-scale datasets, automatic audio classification obtains an increasing attention, which can be applied into acoustic scene analysis [1], urban sound environment recognition [2] and audio stream segmentation [3, 4], to list a few. These tasks also appear in the yearly DCASE competition. The focus of this paper is on the infant cry recognition (ICR), which is also one of potential applications of audio classification.\\n\\nIt was reported that every year nearly 130 million babies are born all over the world [5]. For experienced parents, doctors, and nurses, they can partially understand the needs and thoughts of babies based on their cries, but this is very difficult for new parents. ICR therefore becomes rather demanding for new parents, as the newborn cannot express their thoughts, needs or emotions as naturally as adults, which are only conveyed by crying. In addition to understanding the daily needs of infants, some diseases can also be predicted through crying, as the cry of a sick baby contains some unique characteristics that are different from a healthy baby [5, 6]. If parents can understand the baby's crying, they can take better care of their children and judge whether the baby has a disease in order to make a timely treatment.\\n\\nIn [7], infant vocalic utterance durations, changes in fundamental frequency (F0) and formant frequencies, and variations of source excitations in the vocal tract for infant vocalic utterances were analyzed, revealing that infant vocalizations often have a sudden F0 shift, vocal fry and noise segments. In [8], a fully feed-forward neural network was used as the classification model and mel-frequency cepstrum coefficient (MFCC) as input features. In [9], the one-second spectrogram features that were extracted from a self-recorded dataset were fed into hybrid self-organizing maps (SOM), which were then fine-tuned as a supervised single-layer perceptron. In [10], the support vector machine (SVM) was employed for ICR on the BabySounds dataset. In [11], a convolutional neural network (CNN) with inception was applied to ICR on a day-long home LENA audio dataset [12]. In [13], a CNN-RNN based model was proposed using spectrogram features on a Dunstan Baby Language dataset. In [14], the Resnet50 [15] was used to extract high-level features, which were then input into a graph convolutional network. Moreover, the Computational Paralinguistics Challenge (ComParE) [16] was organized in InterSpeech2018, which was dedicated to recognizing infant vocalizations (e.g., 'crying', 'fussing' and 'neutral') on the CRIED dataset [17], which provides standardized conditions for objective performance evaluation. In [4], a CapsuleNet model [18] was designed with spectrogram as feature inputs. Anders et al. [19] compared various neural networks for ICR, including convolutional, recurrent and fully-connected networks as well as combinations of thereof. Li et al. [20] employed a CNN with self-attention (CNSA) with multiple feature groups. It can thus be concluded that one can make efforts for the ICR problem from two perspectives, i.e., feature extraction and classifier design.\\n\\nIt was shown that a representative feature has a more dominant influence on the ICR performance than the choice of classifiers [19]. Most existing works are based on the extraction of a single feature, e.g., log-Mels, MFCC, or a simple combination of time-domain and frequency-domain features. As the infant cry signals are mainly composed of expiration and inspiration segments [6], where principal frequency components range from 1500 Hz to 3000 Hz, while the adult speech signals are dominated by frequency components below 800 Hz [14, 21], the simple use of MFCC or log-Mels might lead to an information loss in the representation of crying signals.\\n\\nIn order to construct an ICR-specific feature, we investigate differential time-frequency feature representations in this work. To do this, we first extract log-Mel spectrogram from cry signals. Then, we calculate the deltas of log-Mels across time frames and frequency bins, respectively, which are called first-order time-frequency deltas. In order to extract a more fruitful representation, one can also compute second-order differential features. However, due to the limited length of this paper, we only report the results of the best feature combination. In the future work, we will further explore these two kinds of features.\"}"}
{"id": "xu22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1: An overview of the proposed vision transformer (ViT) and transformer encoder based infant cry recognition (ICR) framework.\\n\\nInitial log-Mels. The extracted log-Mel spectrogram and the first-order time-frequency deltas are concatenated as a 3-D feature representation, which is input into a vision transformer (ViT) [22] for classification. The effectiveness of the proposed ICR system is validated on both the public CRIED database and a self-recorded dataset. It is shown that the combination of initial log-Mels, the first-order time-frame delta and the first-order frequency-bin delta obtains the best performance. The rest of this paper is arranged as follows. Section 2 introduces the proposed ICR system. Section 3 presents the experimental setup and results, followed by concluding remarks in Section 4.\\n\\n2. Methodology\\n\\n2.1. Feature extraction\\n\\nIn [23], it was shown that the calculated deltas and delta-deltas of log-Mels as personalized features are capable of effectively reflecting emotional information and reducing irrelevant emotional effects, such as the speakers, contents, and environment. Therefore, we consider differential time-frequency log-Mel spectrograms as features for ICR in this work. The proposed feature extraction method is shown in the left part of Figure 1. As the frequency components of cry signals mainly range from 1500 Hz to 3000 Hz [14, 21], at the pre-processing stage we use a high-pass filter to remove the speech and low-frequency noise signals contained in the input crying audio data as in [4], which can reduce the data redundancy for facilitating model classification. Besides, we apply a voice activity detector (VAD) to remove the silent segments.\\n\\nFor feature extraction, we first calculate the log-Mel spectrogram of the input audio files, which is denoted by \\\\(X \\\\in \\\\mathbb{R}^{F \\\\times T}\\\\) with \\\\(F\\\\) and \\\\(T\\\\) denoting the number of frequency bins and time frames, respectively. The corresponding differential log-Mels (i.e., first-order deltas) over the time axis and frequency axis at time-frequency bin \\\\((f, t)\\\\) are respectively given by\\n\\n\\\\[\\n\\\\Delta_T X(f, t) = \\\\frac{\\\\sum_{n=1}^{N} n \\\\left( X(f, t+n) - X(f, t-n) \\\\right)^2}{\\\\sum_{n=1}^{N} n^2},\\n\\\\]\\n\\n\\\\[\\n\\\\Delta_F X(f, t) = \\\\frac{\\\\sum_{n=1}^{N} n \\\\left( X(f+n, t) - X(f-n, t) \\\\right)^2}{\\\\sum_{n=1}^{N} n^2},\\n\\\\]\\n\\nwhere \\\\(N\\\\) represents the length of the differential window. Then, we concatenate \\\\(X, \\\\Delta_T X\\\\) and \\\\(\\\\Delta_F X\\\\) to form a three-dimensional feature tensor (i.e., 3-D log-Mels) as\\n\\n\\\\[\\nY = (X, \\\\Delta_T X, \\\\Delta_F X) \\\\in \\\\mathbb{R}^{F \\\\times T \\\\times C}.\\n\\\\]\\n\\nwhere \\\\(C\\\\) denotes the number of feature channels (which is set to be 3 in this work). The obtained 3-D feature tensor is then fed into the ViT based classification network. For brevity, we only present the calculation of first-order time-frame and frequency-bin differential features. The second-order time-frequency differential features can be calculated similarly as above, e.g., \\\\(\\\\Delta^2_T T X\\\\), \\\\(\\\\Delta^2_T F X\\\\), \\\\(\\\\Delta^2_F F X\\\\) and \\\\(\\\\Delta^2_F T X\\\\).\\n\\nWith different differential features used in (3), one can construct different tensors. In the second-order case, in principal we have five combinations in total, which will be compared in the experimental section.\\n\\n2.2. ViT-based ICR classifier\\n\\nThe ViT was firstly proposed in [22] for image classification, which follows the original transformer architecture [24]. It was shown in [25] that it can be well transferred to various downstream tasks after fine-tuning the pre-trained model on ImageNet. Besides, given a similar pre-training performance on ImageNet, ViT has fewer parameters, which is thus more likely to avoid overfitting on downstream tasks. This is rather important for infant cry recognition (ICR), as currently the public ICR datasets are generally small scaled. In addition, the infant cry audio still has temporal information, which can be learn by transformer-based ViT explicitly. This is why we choose ViT as the classifier in this work.\\n\\nFor the classic transformer, the token sequence is required as a two-dimensional matrix. However, the infant cry signals are represented as a 3-D log-Mels feature. Therefore, we first need to reshape the feature data using an embedding layer in prior to the transformer encoder, i.e., flattening \\\\(Y \\\\in \\\\mathbb{R}^{F \\\\times T \\\\times C}\\\\) into a sequence of 2-D patches \\\\(Y_p \\\\in \\\\mathbb{R}^{M \\\\times (P^2 \\\\times C)}\\\\) with \\\\(P\\\\) denoting the size of each feature patch and \\\\(M = \\\\frac{FT}{P^2}\\\\) the number of patches, which also serves as the effective input sequence length for the transformer. The transformer uses a constant latent vector size \\\\(D\\\\) throughout its layers, where \\\\(D = P \\\\times P \\\\times C\\\\). Following this, the embeddings are fed into the transformer encoder. The transformer first applies a linear projection to the flattened patches, followed by a layer normalization (LN) layer, and then a multi-head self-attention (MSA) layer. This is repeated for a certain number of layers, each of which also includes a feed-forward network (FFN) layer. Finally, the output of the last layer is fed into a classification head, which is a multi-layer perceptron (MLP) with dropout and normalization layers.\\n\\nThe classification head aims to predict the class label of the input patch sequence, which is achieved by applying a linear transformation followed by a softmax function. The output of the classification head is the probability distribution over the possible class labels, indicating the likelihood of each label. The loss function used for training is the cross-entropy loss, which measures the discrepancy between the predicted probability distribution and the actual label distribution. The model parameters are updated using backpropagation and an optimizer, such as Adam, to minimize the loss function.\\n\\nThe ViT based ICR classifier is trained on the labeled datasets, and its performance is evaluated on both the public CRIED database and a self-recorded dataset. The effectiveness of the proposed ICR system is validated on both the public CRIED database and a self-recorded dataset. It is shown that the combination of initial log-Mels, the first-order time-frame delta and the first-order frequency-bin delta obtains the best performance. The rest of this paper is arranged as follows. Section 2 introduces the proposed ICR system. Section 3 presents the experimental setup and results, followed by concluding remarks in Section 4.\"}"}
{"id": "xu22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Following BERT [26], a learnable embedding $Y_{\\\\text{class}}$ is prepended to the feature sequence along with the added 1-D positional embeddings $E_{\\\\text{pos}}$ to form the patch embedding $h_0$, which is given by\\n\\n$$h_0 = [Y_{\\\\text{class}}; Y_1^{p}E; Y_2^{p}E; \\\\ldots; Y_M^{p}E] + E_{\\\\text{pos}},$$\\n\\nwhere $E \\\\in \\\\mathbb{R}^{(P^2 \\\\times C) \\\\times D}$ and $E_{\\\\text{pos}} \\\\in \\\\mathbb{R}^{(M+1) \\\\times D}$. The patches $h_0$ are then input into the transformer encoder.\\n\\nThe transformer encoder stacks $L_{\\\\text{encoder}}$ blocks, which are shown at the right-hand side of Figure 1. Each encoder block mainly consists of: 1) Layernorm (LN) [27], 2) a multi-headed self-attention (MSA) layer and 3) an MLP block. The Layernorm [27] was shown to be effective in case the batch size is small, which is given by\\n\\n$$y = x - E[x] \\\\sqrt{\\\\text{Var}[x] + \\\\epsilon} \\\\cdot \\\\gamma + \\\\beta,$$\\n\\nwhere $E[x]$ is the mean, $\\\\text{Var}[x]$ is the variance, $\\\\epsilon$ is to guarantee the positiveness of the square root, $\\\\gamma$ is the gain and $\\\\beta$ is the bias, respectively. Note that the involved parameters are trainable.\\n\\nThe multi-headed self-attention [24] is defined as\\n\\n$$\\\\text{Multi-head}(Q,K,V) = \\\\text{concat}(\\\\text{head}_1, \\\\ldots, \\\\text{head}_h) W_O,$$\\n\\nwhere $W_O$ is a weighting matrix and the heads are given by\\n\\n$$\\\\text{head}_i = \\\\text{Attention}(Q W_Q i, K W_K i, V W_V i),$$\\n\\nwhere the attention is calculated by\\n\\n$$\\\\text{Attention}(Q,K,V) = \\\\text{softmax}(Q K^T \\\\sqrt{d_k}) V,$$\\n\\nwhere $Q,K,V$ consist of multiple $q,k,v$ vectors, respectively, which are obtained by the linear transformation layer at each time step using the input sequence $x_t$.\\n\\nThe MLP block includes two linear layers for dimensional transformation, a Gaussian error linear unit (GELU) for activation function, and two dropout steps for alleviating the occurrence of overfitting. Note that the output shape through the transformer encoder remains unchanged. We extract the one-dimensional vector of the class token, which is passed through the MLP head to resolve the final classification. For more details on the ViT, we refer to [22].\\n\\n### 3. Performance evaluation\\n\\n#### 3.1. Datasets\\n\\nFor performance evaluation, we use the CRIED database that was developed by Marschik et al. [17], which consists of 5587 vocalizations of 20 healthy infants (10 females and 10 males) ranging from 4 to 16 weeks old and is recorded at a bi-weekly interval. All vocalizations were extracted from sequences of up to 5 minutes in duration when the infants were awake and lying in a supine position in a cot. The sampling frequency is 32 kHz and the durations of audio files are from 0.4 up to 41 seconds. Two experts in the field of early speech-language development annotated those vocalizations based on the audio-video clip as 'Neutral/Positive', 'Fussing' or 'Crying'. The partition of the dataset follows the ComParE contest [16], which is shown in Table 1. Besides, in order to further validate the effectiveness of the proposed method, we recruit some new moms as volunteers to record and annotate the new babies' cry using mobile phones in indoor environments as a self-recorded dataset, which consists of 1097 crying sounds. The dataset includes six classes: 'Diaper' (8 samples in training, 7 samples in testing), 'Hungry' (240 samples/training, 226 samples/testing), 'Sleepy' (3 samples/training, 3 samples/testing), 'Uncomfortable' (15 samples/training, 14 samples/testing), 'Pain' (283 samples/training, 280 samples/testing), and 'Hug' (9 samples/training, 9 samples/testing). The sampling frequency is 44.1 kHz and the durations of audio files range from 23 up to 195 seconds. As the involved moms simply label the recorded sounds based on their own experience (somehow lack of experience). For both datasets, all audio signals are first downsampled to 16 kHz.\\n\\n#### 3.2. Configuration\\n\\nFor feature extraction, we extract log-Mels spectrogram (i.e., $X$) from the pre-processed audio files, where the number of Mel-filter banks is set to be 224, the window length is 512 and the size of hop is 256, respectively. For the extracted log-Mels feature, we truncate or pad the time-frame dimension to be 224, based on which the time/frequency deltas are calculated. The length of the differential window is set as $N = 2$. As such, the dimension of $X, \\\\Delta T X$ and $\\\\Delta F X$ is $224 \\\\times 224$ and the dimension of the tensor $Y$ is $224 \\\\times 224 \\\\times 3$. For the ViT model, we set the patch size to be $16 \\\\times 16$, so that the vector size $D$ becomes 768. The numbers of repetitions of the encoder block and the heads of the multi-head attention module are both 12.\\n\\nAs comparison, the ComParE contest proposed several off-ficial baseline systems. For example, the OPENSMILE method uses the OPENSMILE toolbox [28] to calculate 6373 features, which are derived from various functionals over low-level descriptor (LLD) contours and then input into the SVM classifier [29]. The AUDEEP uses recurrent sequence to sequence autoencoders based unsupervised representation learning for feature extraction and SVM for classification based on the AUDEEP toolkit [30]. The OPENXBOW uses bag-of-audio-words (BoAW) as features and also SVM as classifier based on the OPENXBOW toolkit [31]. In addition, it is also provided an end-to-end baseline system, which uses a CNN for feature extraction from the raw audio signal and a recurrent neural network (RNN) with gated recurrent units for classification (implemented using the End2You [32] toolkit). Furthermore, in order to validate the benefit of pre-training for the ICR problem, we use a public pre-training model and fine-tune on the considered datasets, which can be downloaded from the website [3].\\n\\n#### 3.3. Results\\n\\nFor performance evaluation, we calculate the unweighted average recall (UAR) and accuracy (Acc). The UAR sometimes is more useful than Acc, especially for unbalanced multi-class classification problems. At first, we show the experimental results on the CRIED dataset in Table 2. Clearly, the combination of $(X, \\\\Delta T X, \\\\Delta F X)$ based ViT model outperforms the comparison approaches and achieves the best performance, and\\n\\n3https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-vitjx\"}"}
{"id": "xu22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The performance comparison on the CRIED database.\\n\\n| Model         | Features            | UAR [%] | Acc [%] |\\n|---------------|---------------------|---------|---------|\\n| END2YOU [32]  | CNN-based           | 70.8    |         |\\n| OPENSMILE [28]| ComParE             | 75.6    | 82.6    |\\n| OPENXBOW [31] | ComParE             | 76.9    | 84.2    |\\n| AUDEEP [30]   | AE-based            | 74.4    | 83.5    |\\n| CapsuleNet [4]| Spectrogram         | 71.6    | 86.1    |\\n| CNSA [20]     | Hybrid feature      | 77.96   | 86.72   |\\n| ResNet50 [15]| (X, \u2206T X, \u2206F X)    | 77.7    | 90.1    |\\n| ViT (X, X, X)| (X, \u2206T X, \u2206F X)    | 66.9    | 88.5    |\\n|               | (X, \u2206T X, \u2206^2F X)  | 77.4    | 87.8    |\\n|               | (X, \u2206F X, \u2206^2F X)  | 76.4    | 90.7    |\\n|               | (X, \u2206T X, \u2206F X)    | 78.5    | 90.8    |\\n| ViT + Fine-tune| (X, \u2206T X, \u2206F X)  | 81.2    | 93.4    |\\n\\nNeutral Fussing Crying\\n\\nTrue Labels\\n\\nPredicted Labels\\n\\n98.76 0.87 0.37\\n\\n4.76 74.38 20.86\\n\\n5.15 24.26 70.59\\n\\nConfusion matrix\\n\\nFigure 2: The confusion matrix of UAR on the CRIED database.\\n\\nFine-tuning can further improve the UAR by 2.7%. Comparing (X, \u2206T X, \u2206^2T X) and (X, \u2206F X, \u2206^2F X), we can see that time-frame and frequency-bin differential features are equivalently important for ICR as they obtain a similar performance. Comparing (X, \u2206T X, \u2206^2F X) or (X, \u2206F X, \u2206^2F X) to (X, \u2206T X, \u2206F X) or (X, \u2206F X, \u2206^2F X), it reveals that on the basis of first-order time-frame (or frequency-bin) differential features, a homoplasmic second-order time-frequency differential feature is more informative than the alloplasmic counterpart, as the latter in general achieves a better performance, particularly in UAR. This is due to the fact that a homoplasmic second-order time-frequency differential feature lacks differential information in the frequency dimension and alloplasmic counterpart is extracted based on the first-order differential feature. More importantly, the first-order time-frame and frequency-bin differential feature is extracted from the original log-mel spectrogram, which can thus represent the differential information across time frames and frequencies simultaneously. Besides, we add an ablation in order to show if only using differential spectrograms improve the performance. We input (X, X, X) (i.e., directly copy spectrograms) into ViT and (X, \u2206T X, \u2206F X) into ResNet50 [15] as shown in the Table 2. It shows that time-frequency differential feature achieves a better performance in both UAR and Acc. Compared to ResNet50 [15], ViT can improve the UAR by 0.8% and the Acc by 0.7%. Furthermore, we show the confusion matrix obtained by the ViT + Fine-tune in Figure 2. As the Neutral cry most commonly happens in practice and has the largest number of audio streams in the CRIED dataset, it obtains the highest recognition accuracy. Fussing and Crying are much more confusing in the sense of classification, as it is even difficult to make a clear discrimination between them and give an accurate label by an expert. This can also be interpreted using the spectrograms and first-order differential features of cry signals in Figure 3, from which we can clearly see that the Neutral signal has a more different frequency-bin differential feature from the others and the Fussing and Crying signals have more similarities in both spectrograms and first-order differential log-Mels. Finally, the experimental results on the self-recorded dataset is shown in Table 3.\\n\\nDue to the small size, the unbalanced class size and the label reliability (this is the most important reason indeed), the performance is much lower than that in Table 2. Similarly, it shows that the combination of (X, \u2206T X, \u2206F X) is the best option for time-frequency differential spectrogram features.\\n\\n4. Conclusion\\n\\nIn this paper, we investigated differential time-frequency log-Mel feature representations and applied the ViT model for the ICR problem. The original log-Mel spectrogram, first-order time-frame and frequency-bin differential log-Mels were concatenated as a 3-D feature, which was shown to be the best combination of differential time-frequency features. We found that including a pre-training model is also helpful for ICR. In principal, it is easiest to discriminate a Neutral cry from other categories. ICR is a quite challenging topic as it is hard for both human and machine to make a clear standardization.\\n\\n5. Acknowledgement\\n\\nThanks to K. D. Bartl-Pokorny, C. Einspieler, P. B. Marschik, F. B. Pokorny and D. Zhang from the Research Unit iDN - interdisciplinary Developmental Neuroscience, Division of Pediatrics, Medical University of Graz, Austria, for sharing the database. Also thanks to Liping Bao for discussion.\"}"}
