{"id": "sarabia23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning\\nMiguel Sarabia, Elena Menyaylenko, Alessandro Toso, Skyler Seto, Zakaria Aldeneh, Shadi Pirhosseinloo, Luca Zappella, Barry-John Theobald, Nicholas Apostoloff, Jonathan Sheaffer\\n\\nAbstract\\nWe present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60\u00b0 on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on direct-to-reverberant ratio estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43\u00b0 on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.\\n\\n1. Introduction\\nHumans can infer a great amount of information from their acoustic environment, including determining the spatial location of sound sources, estimating the size of a room, and estimating the room reverberance [1,2]. To develop machine learning algorithms with the same level of acoustic spatial awareness, we require a large and diverse dataset with annotations for each task. To the best of our knowledge, no publicly-available dataset has the scale, and diversity needed to train general models for multiple spatial audio tasks. This lack of large-scale datasets limits the applicability of modern machine learning techniques to spatial audio.\\n\\nIn this paper, we introduce Spatial LibriSpeech, a spatially augmented version of LibriSpeech [3] with optional noise from the Microsoft Deep Noise Suppression Challenge 2021 [4]. Spatial LibriSpeech augments the LibriSpeech and Microsoft Deep Noise Suppression samples by simulating how they would be perceived by a microphone array in various synthetic rooms. Spatial LibriSpeech contains over 650 hours of spatial audio with labels for source position, speaking direction, room acoustics, and room geometry (refer to Section 2 for a review of spatial audio tasks for which these labels are useful). Our goal is for Spatial LibriSpeech to be the main training dataset for spatial audio applications.\\n\\nTo create Spatial LibriSpeech, we first generated 8,952 synthetic rooms, which were used to obtain room impulse responses (RIRs) that were convolved and scaled with LibriSpeech samples. The RIRs are modeled on the Zylia microphone array, a 19-channel spherical microphone array. This array facilitates a means for extracting a third-order ambisonics representation up to a frequency of 3080Hz [5]. For completeness, we also provide synthesized, full-bandwidth first-order ambisonics [6], which are aliasing free. Either of these formats may be used to simulate a wide variety of arrays [7]. We describe the dataset generation process in more detail in Section 3.1. A key advantage of convolving existing speech samples with synthetic RIRs for spatial audio dataset generation is that we can ensure our dataset spans a variety of acoustic room properties and room sizes. For instance, full-band T30 values range from 145ms to 2846ms, and room floor area ranges from $13.3m^2$ to $277.4m^2$. Section 3.2 describes diversity statistics of Spatial LibriSpeech.\\n\\nWhile Spatial LibriSpeech has the potential to be used to train models for multiple tasks, in this paper we focus on some of the most fundamental spatial audio detection tasks: i) 3D source localization, ii) source distance, iii) third-octave narrow-band direct-to-reverberant ratios (DRRs), and iv) third-octave narrow-band T30s. All models share the same architecture and training regime (presented in Section 4). When training spatial audio models, there is a choice of using the microphone array signals as inputs or converting the microphone array signals to ambisonics to obtain device-agnostic models. In our work, models consume first-order ambisonics.\\n\\nOur models achieve a median absolute error of 6.60\u00b0 in 3D source localization, 0.43m in distance estimation, 2.74dB in DRR estimation, and 90.66ms in T30 estimation. Since our training dataset is composed of simulated acoustics, we verify the transferability of our models by testing them on two evaluation datasets: TUT Sound Events 2018 [8], and ACE Challenge [9] (Section 4.1). We find that fine-tuning is beneficial for ACE Challenge but unnecessary for TUT Sound Events 2018, and we plot the responses of our models on both evaluation datasets showing that ACE responses are at the tail of the Spatial LibriSpeech responses (see Section 4.2). Lastly, we show that, for single-task models, using a version of Spatial LibriSpeech made by uniformly sampling 10% of the dataset yields the same performance as the full dataset (cf. Section 4.3).\\n\\n2. Background\\nWe start by reviewing spatial audio tasks to highlight the large number of potential use cases for Spatial LibriSpeech. We divide audio tasks into three categories: source parameter estimation, environment parameter estimation, and spatial processing. Source parameter estimation tasks are concerned with understanding audio sources. Examples include estimation of: source localization [25], source distance [26], and speaking direction [27]. Environment parameter estimation tasks involve understanding the environment where the audio is produced.\"}"}
{"id": "sarabia23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of existing datasets. ROOMS refers to the number of unique acoustic environments in the dataset, ROOMCONFIG to the number of physical configurations per environment (e.g. different source receiver positions), CHANNELS to the availability of spatial information from a microphone array or other encoding: N - N channels mic array, B - Binaural, XOA - Xth order ambisonics, DATATYPES: sIR - Simulated IR, rIR - Recorded IR, sA - Simulated Audio, rA - Recorded Audio.\\n\\nLABELS: P - Position, SD - Speaking direction, R - Room acoustics, G - Room geometry, O - Other.\\n\\n| Dataset        | ROOMS | ROOMCONFIG | CHANNELS | DATATYPES | LABELS |\\n|----------------|-------|------------|----------|-----------|--------|\\n| dEchorate [10] | 11    | 180        | 5        | sIR, rIR  | P      |\\n| Arni [11]      | 1     | 21         | 3        | rdOA, 4th OA rIR, P, R |\\n| GIR [12]       | 1     | 2,951      | 1        | rIR P, O  |\\n| EasyCom [13]   | 1     | 50         | 4        | B rA P, R, G |\\n| CoupledRooms   | 2     | 101        | 4        | 1st OA rIR P, R, G |\\n| DCASE2021 task 3 [15] | 13    | 1,184\u20136,480 | 4, 1st OA rIR, rA P |\\n| BUT ReverbDB   | 8     | 155        | 1        | rIR P, R, G |\\n| SBSBRIR [17]   | 1     | 180 x 2\u00b0   | B        | rIR P |\\n| BIRD [18]      | 12,500| 8          | 2        | sIR P, R |\\n| Kemar BRIRs    | 1     | 50         | B        | rIR P, G |\\n| Motus [20]     | 1     | 3,320      | 32       | 4th OA rIR P, R, G |\\n| Aachen IR database [21] | 4 | 17 | B | rIR P, R, G, O |\\n| ACE Challenge  | 7     | 10         | 2 - 32   | rIR P, R |\\n| DIHRA [22]     | 2     | 62         | 3        | rA, sA P, G |\\n| Voice-Home     | 12    | 24         | 8        | rIR, rA P, G |\\n| Sweet-Home     | 4     | 7          | 1        | rA |\\n| Microsoft DNS 2001 [4] | 14,576| 1 | 1 | sIR, rIR R |\\n| TUT Sound Events 2018 [8] | 5 | 487-4,366 | 8, 1st OA sA, rA P, G |\\n| Spatial LibriSpeech | 8,952 | 20 | 19, 1st OA sA P, SD, R, G |\\n\\nExample tasks include estimation of DRR, material absorption and scattering, and room volume. Finally, spatial processing tasks involve the transformation of audio signals with information extracted from the acoustic environment. Example tasks include de-reverberation, beamforming, and audio source separation.\\n\\nTable 1 contains a summary of the main differences between 18 published spatial audio datasets in terms of the number of environments, number of physical configurations per environment, recorded/simulated channels, and data-types and labels included in the dataset. We found only two datasets that include over 50k unique configurations across all environments: BIRD and DCASE2021 Task 3. However, BIRD features only two microphone channels, and DCASE2021 Task 3 includes only position labels. Spatial LibriSpeech is the only dataset to feature over 200k unique configurations, labels for a large number of acoustic tasks, and both mic-domain audio and ambisonics. This lack of diversity and labels limits the applicability of existing datasets to modern machine learning techniques such as multi-task learning or contrastive representation learning.\\n\\n3. The Spatial LibriSpeech Dataset\\n\\nIn this section, we describe the generation of Spatial LibriSpeech and its defining characteristics.\\n\\n3.1. Generation Methodology\\n\\nOur pipeline to generate Spatial LibriSpeech consists of three steps: parametric room generation, room impulse response simulations, and mixing.\\n\\nParametric room generation. We start by defining a realistic set of conditions for commonly encountered living spaces following [36]. These consist of a predefined distribution of reverberation times and room shapes and sizes. The materials associated with the room surfaces are chosen from a database of typical construction materials, for which the absorption and scattering coefficients as a function of frequency are available.\\n\\nRoom impulse response simulations. We use a geometrical acoustic solver, which includes a high fidelity model of the room that accounts for the directivity of the microphones (including diffraction effects from the array body). The room is populated with several acoustic objects including the recording device placed at a randomized but bounded position, several sources surrounding the device with a randomized looking direction and directivity function. When sources are intended to represent speech, their directivity is computed using a boundary element code using a set of artificially generated models. The direct path from each source to the microphone array, the early reflections, and the late reverberations are used to assemble each impulse response.\\n\\nMixing. Once we have computed RIRs for each configuration of the simulated rooms, we mix them with LibriSpeech samples by randomly assigning them to sources in every simulated room. We then remove leading and trailing silence from each sample, convolve the sample with the selected RIR and scale the output to a random active speech level (ASL) between 85dB-ASL and 100dB-ASL at mouth reference point.\\n\\nFinally, we save the resulting audio samples and store the relevant labels from both the room simulation and LibriSpeech. For the distractor noise we follow the same process, except we assign a random source, different to the main signal source, and set the signal strength to a random signal-to-noise ratio (SNR) between 10dB and 40dB.\\n\\n3.2. Dataset Characteristics\\n\\nThe main characteristics of Spatial LibriSpeech are summarized in Table 2. The dataset consists of two splits: TRAIN which is derived from LibriSpeech's train-clean-100 and train-clean-360 subset, and TEST which is derived from test-clean. The audio is sampled at 16kHz.\\n\\nSpatial LibriSpeech includes the following labels: source localization (azimuth and elevation), speaking direction, room volume, surface and floor area, voice directivity identifier, narrow-band C50, DRR, EDT, T20, and T30 [38]; and all\\n\\n2 Silence is defined as any leading or trailing sound before a 100ms segment at more than 1% volume.\\n3 Mouth reference point is the point on the reference axis 25 mm in front of the lip plane ITU-T P.581 [37].\"}"}
{"id": "sarabia23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Main characteristics of Spatial LibriSpeech.\\n\\n|                  | TRAIN | TEST  |\\n|------------------|-------|-------|\\n| Total duration   | 573h 13m 12s | 85h 29m 20s |\\n\\nFor distance regression, we report the median absolute error in meters. As DRR and T30 are 20-dimensional vectors representing the third-octave frequencies between 100Hz and 8kHz, we use 0.5s for azimuth and elevation, 4.0s for distance, and 20h for T30 and DRR estimation. We use 8 NVIDIA Tesla V100s GPUs and 375GiB of RAM.\\n\\nFor 3D source localization, we report the median absolute error of each model. We report both the median absolute error as well as the Pearson correlation across frequencies. For distance regression, we report the median absolute error in meters. For T30 and DRR estimation, we report the median Pearson correlation, both for the best performing models.\\n\\nWe verify that our models do not overfit to the virtual rooms by checking that the performance of models trained with Spatial LibriSpeech transfer to two established evaluation datasets: ACE Challenge [9] and TUT Sound Events 2018 [8]. All models share the same architecture.\\n\\nWe report both the median absolute error and the Pearson correlation across frequency bins for the best performing models. We also report the IQR across predictions.\\n\\nFor each prediction, we report the relation of the best performing model across frequency bin for T30, T30 RAIN, and T30 ESTimation, and distance, and 21.2M gradient updates for T30 and DRR estimation.\\n\\nWe trained several neural networks for: i) 3D source localization (one for azimuth, and another for elevation), ii) source distance, iii) DRR, and iv) T30. We chose these tasks since they are representative of the source and environment parameter estimation spatial audio tasks and be-\"}"}
{"id": "sarabia23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Performance contextualization for different tasks.\\n\\nRANDOM refers to the performance of the model described in Section 4 without any training. S.L.S indicates the performance of our baseline (see Table 3). EXTERNAL refers to external benchmarks. The comparison with external benchmark may not be fair, as they were trained with different architectures, training regimes, and datasets closer to the evaluation dataset.\\n\\n\\\\* Model was fine-tuned on ACE Dev.\\n\\n| Metric | RANDOM | S.L.S. | EXTERNAL |\\n|--------|--------|--------|----------|\\n| ANSIM [8] median abs. 3D angle error (eq. 1) | 88.49\u00b0 8.19\u00b0 \u223c4.3\u00b0 [39] | 88.55\u00b0 5.80\u00b0 \u223c7.4\u00b0 [39] | 88.53\u00b0 12.43\u00b0 \u223c4.3\u00b0 [39] |\\n| T30 ACE [9] mean error | -712.43ms 156.44ms | 22.1ms [28] | |\\n| DRR | 9.84dB 4.04dB | 0.81dB [28] | |\\n\\n4.1. Performance and transferability\\n\\nTable 3 shows the performance of our models on all four tasks when evaluated against the test set of Spatial LibriSpeech, TUT Sounds Events 2018 [8], and ACE Challenge Eval set [9]. All models were trained using Spatial LibriSpeech only.\\n\\nOur results show that the difference in performance between the test set of Spatial LibriSpeech and the performance on TUT Sound Events 2018 is small, +5.83\u00b0 and +0.17m worse on REAL than Spatial LibriSpeech, though better on RESIM than on Spatial LibriSpeech (-0.90\u00b0 and -0.18m). We find that our models do not exploit spurious correlations between the speech level and the distance labels, instead incorporating information from reverberations to make accurate predictions. This is evidenced by the median absolute error of ANSIM (an anechoic subset of TUT Sound Events 2018) being 4.76m.\\n\\nLooking at the performance differences between Spatial LibriSpeech and ACE for DRR and T30 regression we find that the transfer penalty is higher (+11.76dB for DRR, and +66.66ms for T30). We also investigate fine-tuning the last three layers (the MLP) with the ACE Challenge Dev set for 20 epochs, and find that the median absolute error improves significantly for both T30 and DRR (resulting in 2.07dB and 14.91ms gaps with the Spatial LibriSpeech test set).\\n\\nFor additional context into these results, Table 4 compares the performance of our models against the performance of a randomly initialized version of our model as well as the reported performance of recent benchmarks [28, 39]. In general, benchmarks obtain better results, as they use more complex architectures and train on data specifically modeled after the evaluation sets. We hypothesize that the performance of our model is close to the benchmarks due to the diversity of Spatial LibriSpeech, since we did not carry out an architecture or hyper-parameter search. Furthermore, note that none of these benchmarks published their training sets. In contrast, LibriSpeech was primarily designed for training spatial audio models.\\n\\n4.2. Visualization of Dataset Representations\\n\\nPlotting the representations from our models trained on Spatial LibriSpeech further illustrates the transferability to real-world test data seen in Section 4.1. Figure 1 shows UMAP plots of embeddings extracted from representations before the MLP block of the network.\\n\\nWe see that the model trained for 3D source localization (Figure 1a) yields overlapping representations for Spatial LibriSpeech and TUT Sounds Events 2018, while the model trained on DRR (Figure 1b) yields representations on ACE at the tail of the Spatial LibriSpeech representations indicating the need for a small amount of fine-tuning of the MLP block on the target data.\\n\\n4.3. Training with a smaller subset of Spatial LibriSpeech\\n\\nWe explore the performance of models trained with just 10% of Spatial LibriSpeech, which we sample uniformly, maintaining the diversity of the dataset. We find that these models tend to perform worse on the test set of Spatial LibriSpeech than models trained with the full dataset, for instance the best median absolute error on 3D sound localization is 0.39\u00b0 higher, and 1.36ms higher for T30 regression on the 10% models. However, when looking at performance on external baselines, models trained with 10% of the training data tend to perform better, for example the best median absolute error on 3D sound localization on TUT Sound Events 2018 was 0.88\u00b0 lower, and the median Pearson correlation on T30 regression on ACE Challenge was 0.12 higher. Still, all performance differences were smaller than the IQR between models. Based on these results, we recommend researchers prototype with the smaller version of the dataset, which will be available as a separate download. Additionally, we release the full Spatial LibriSpeech dataset to enable researchers to explore other tasks, such as ablations of acoustic conditions or representation learning.\\n\\n5. Conclusion & Further Work\\n\\nWe have presented Spatial LibriSpeech, a spatial audio dataset for multiple spatial audio tasks and representation learning. We have shown the utility of our dataset with a simple convolutional network trained on Spatial LibriSpeech, the performance of which transfers to established baselines with minimal intervention, and that the results are close to the state-of-the-art, despite a less sophisticated architecture.\\n\\nWe intend to use Spatial LibriSpeech for a number of other tasks, such as denoising, or room identification. Another interesting line of research is whether we can further improve performance using representation learning to train a single model to regress to many of the spatial audio tasks with the same embedding. We look forward to the community using Spatial LibriSpeech to accelerate research in spatial audio.\"}"}
{"id": "sarabia23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] M. Ohuchi, Y. Iwaya, Y. Suzuki, and T. Munekata, \\\"A comparative study of sound localization acuity of congenital blind and sighted people,\\\" Acoust. Sci. and Tech., vol. 27, no. 5, pp. 290\u2013293, 2006.\\n\\n[2] S. Hameed, H. Pakarinen, K. Valde, and V. Pulkki, \\\"Psychoacoustic cues in room size perception,\\\" J. of the Audio Eng. Soc., 2004.\\n\\n[3] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \\\"Librispeech: An ASR corpus based on public domain audio books,\\\" in ICASSP. IEEE, 2015.\\n\\n[4] C. K. Reddy, H. Dubey, K. Koishida, A. Nair, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, \\\"INTERSPEECH 2021 Deep Noise Suppression Challenge,\\\" in Interspeech, 2021.\\n\\n[5] D. Pinardi and A. Farina, \\\"Metrics for evaluating the spatial accuracy of microphone arrays,\\\" in I3DA. IEEE, 2021.\\n\\n[6] M. J. Gerzon, \\\"Periphone (with height sound reproduction),\\\" in Audio Eng. Soc. Convention. AES, 1972.\\n\\n[7] H. Gamper, M. R. Thomas, L. Corbin, and I. Tashev, \\\"Synthesis of Device-Independent Noise Corpora for Realistic ASR Evaluation,\\\" in Interspeech, 2016.\\n\\n[8] S. Adavanne, A. Politis, J. Nikunen, and T. Virtanen, \\\"Sound event localization and detection of overlapping sources using convolutional recurrent neural networks,\\\" J. of Sel. Top. in Signal Process., vol. 13, no. 1, 2019.\\n\\n[9] J. Eaton, N. D. Gaubitch, A. H. Moore, and P. A. Naylor, \\\"Estimation of Room Acoustic Parameters: The ACE Challenge,\\\" Trans. on Audio, Speech, and Lang. Process., vol. 24, 2016.\\n\\n[10] D. Di Carlo, P. Tandeitnik, C. Foy, N. Bertin, A. Deleforge, and S. Gannot, \\\"dEchorate: a calibrated room impulse response dataset for echo-aware signal processing,\\\" J. of Audio Speech Music Proc., vol. 39, 2021.\\n\\n[11] T. McKenzie, L. McCormack, and C. Hold, \\\"Dataset of spatial room impulse responses in a variable acoustics room for six degrees-of-freedom rendering and analysis.\\\" arXiv, 2021.\\n\\n[12] R. Rust, A. Xydis, K. Heutschi, N. Perraudin, G. Casas, C. Du, J. Strauss, K. Eggenschwiler, F. Perez-Cruz, F. Gramazio, and M. Kohler, \\\"A data acquisition setup for data driven acoustic design,\\\" Build. Acoust., vol. 28, no. 4, pp. 345\u2013360, 2021.\\n\\n[13] J. Donley, V. Tourbabin, J.-S. Lee, M. Broyles, H. Jiang, J. Shen, M. Pantic, V. K. Ithapu, and R. Mehra, \\\"Easycom: An augmented reality dataset to support algorithms for easy communication in noisy environments.\\\" arXiv, 2021.\\n\\n[14] T. McKenzie, S. J. Schlecht, and V. Pulkki, \\\"Acoustic analysis and dataset of transitions between coupled rooms,\\\" in ICASSP. IEEE, 2021, pp. 481\u2013485.\\n\\n[15] A. Politis, S. Adavanne, D. Krause, A. Deleforge, P. Srivastava, and T. Virtanen, \\\"A dataset of dynamic reverberant sound scenes with directional interferers for sound event localization and detection,\\\" in Detect. and Classif. of Acoust. Scenes and Events Workshop, 2021.\\n\\n[16] I. Szoke, M. Skacel, L. Mosner, J. Paliesek, and J. Cernocky, \\\"Building and evaluation of a real room impulse response dataset,\\\" J. of Sel. Top. in Signal Process., vol. 13, no. 4, 2019.\\n\\n[17] D. Satongar, Y. W. Lam, and C. Pike, \\\"Measurement and analysis of a spatially sampled binaural room impulse response dataset,\\\" in Int. Congr. on Sound and Vib., 2014.\\n\\n[18] F. Grondin, J.-S. Lauzon, S. Michaud, M. Ravanelli, and F. Michaud, \\\"BIRD: Big Impulse Response Dataset.\\\" arXiv, 2020.\\n\\n[19] C. Mittag, S. Werner, M. Bohme, and F. Klein, \\\"Dataset of Binaural Room Impulse Responses at Multiple Recording Positions, Source Positions and Orientations in a Real Room,\\\" in DAGA. DEGA Akustik, 2017.\\n\\n[20] G. Gotz, S. J. Schlecht, and V. Pulkki, \\\"A dataset of higher-order ambisonic room impulse responses and 3d models measured in a room with varying furniture,\\\" in I3DA. IEEE, 2021.\\n\\n[21] M. Jeub, M. Schafer, and P. Vary, \\\"A binaural room impulse response database for the evaluation of dereverberation algorithms,\\\" in Int. Conf. on Digit. Signal Process. IEEE, 2009.\\n\\n[22] M. Ravanelli, L. Cristoforetti, R. Gretter, M. Pellin, A. Sosi, and M. Omologo, \\\"The DIRHA-ENGLISH corpus and related tasks for distant-speech recognition in domestic environments,\\\" in ASRU. IEEE, 2015.\\n\\n[23] N. Bertin, E. Camberlein, E. Vincent, R. Lebarbenchon, S. Peillon, \u00b4E. Lamand \u00b4e, S. Sivasankaran, F. Bimbot, I. Illina, A. Tom, S. Fleury, and E. Jamet, \\\"A French corpus for distant-microphone speech processing in real homes,\\\" in Interspeech, 2016.\\n\\n[24] M. Vacher, B. Lecouteux, P. Chahuara, F. Portet, B. Meillon, and N. Bonnefond, \\\"The Sweet-Home speech and multimodal corpus for home automation interaction,\\\" in LREC, 2014.\\n\\n[25] P.-A. Grumiaux, S. Kiti\u00b4c, L. Girin, and A. Gu\u00b4erin, \\\"A survey of sound source localization with deep learning methods,\\\" J. of the Acoust. Soc. Am., vol. 152, no. 1, 2022.\\n\\n[26] T. Gburrek, J. Schmalenstroeer, A. Brendel, W. Kellermann, and R. Haeb-Umbach, \\\"Deep neural network based distance estimation for geometry calibration in acoustic sensor networks,\\\" in EuEusipco, 2021.\\n\\n[27] K. Ahuja, A. Kong, M. Goel, and C. Harrison, \\\"Direction-of-Voice (DoV) Estimation for Intuitive Speech Interaction with Smart Devices Ecosystems,\\\" in UIST. ACM, 2020.\\n\\n[28] N. J. Bryan, \\\"Impulse response data augmentation and deep neural networks for blind room acoustic parameter estimation,\\\" in ICASSP, 2020.\\n\\n[29] S. Pelzer and M. Vorl\u00a8ander, \\\"Inversion of a room acoustics model for the determination of acoustical surface properties in enclosed spaces,\\\" Proc. of Meet. on Acoust., vol. 19, no. 1, 2013.\\n\\n[30] P. Srivastava, A. Deleforge, and E. Vincent, \\\"Blind room parameter estimation using multiple multichannel speech recordings,\\\" in Workshop on Appl. of Signal Process. to Audio and Acoust., 2021.\\n\\n[31] K. Kinoshita, M. Delcroix, H. Kwon, T. Mori, and T. Nakatani, \\\"Neural network-based spectrum estimation for online wpe dereverberation.\\\" in Interspeech, 2017.\\n\\n[32] J. Heymann, L. Drude, and R. Haeb-Umbach, \\\"Neural network based spectral mask estimation for acoustic beamforming,\\\" in ICASSP, 2016.\\n\\n[33] E. Tzinis, Z. Wang, and P. Smaragdis, \\\"Sudo rm -rf: Efficient networks for universal audio source separation,\\\" in Int. Workshop on Mach. Learn. for Signal Process. IEEE, 2020.\\n\\n[34] Z. Gong, P. Zhong, and W. Hu, \\\"Diversity in machine learning,\\\" IEEE Access, vol. 7, 2019.\\n\\n[35] A. Baevski, W.-N. Hsu, Q. Xu, A. Babu, J. Gu, and M. Auli, \\\"data2vec: A general framework for self-supervised learning in speech, vision and language,\\\" in ICML, 2022.\\n\\n[36] C. D\u00edaz and A. Pedrero, \\\"The reverberation time of furnished rooms in dwellings,\\\" Appl. Acoustics, vol. 66, no. 8, 2005.\\n\\n[37] ITU-T P.581, \\\"Use of head and torso simulator for hands-free and handset terminal testing,\\\" International Telecommunication Union, standard, Jul. 2022.\\n\\n[38] ISO 3382-1, \\\"Measurement of room acoustic parameters \u2014 Part 1: Performance spaces,\\\" International Organization for Standardization, standard, Jun. 2009.\\n\\n[39] C. Schymura, B. B\u00f6ninghoff, T. Ochiai, M. Delcroix, K. Kinoshita, T. Nakatani, S. Araki, and D. Kolossa, \\\"PILOT: Introducing Transformers for Probabilistic Sound Event Localization,\\\" in Interspeech, 2021.\\n\\n[40] F. Jacobsen, \\\"Active and reactive sound intensity in a reverberant sound field,\\\" J of Sound and Vib., vol. 143, no. 2, 1990.\\n\\n[41] B. Rafaely, Fundamentals of spherical array processing. Springer, 2015, vol. 8.\\n\\n[42] L. McInnes, J. Healy, and J. Melville, \\\"UMAP: Uniform manifold approximation and projection for dimension reduction.\\\" arXiv, 2018.\"}"}
