{"id": "saito24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nWe present SRC4VC, a new corpus containing 11 hours of speech recorded on smartphones by 100 Japanese speakers. Although high-quality multi-speaker corpora can advance voice conversion (VC) technologies, they are not always suitable for testing VC when low-quality speech recording is given as the input. To this end, we first asked 100 crowdworkers to record their voice samples using smartphones. Then, we annotated the recorded samples with speaker-wise recording-quality scores and utterance-wise perceived emotion labels. We also benchmark SRC4VC on any-to-any VC, in which we trained a multi-speaker VC model on high-quality speech and used the SRC4VC speakers' voice samples as the source in VC. The results show that the recording quality mismatch between the training and evaluation data significantly degrades the VC performance, which can be improved by applying speech enhancement to the low-quality source speech samples.\\n\\nIndex Terms: speech corpus, smartphone-recorded, crowdsourcing, annotation, voice conversion\\n\\n1. Introduction\\nVoice conversion (VC) is a technology for transforming the voice characteristics of source speech into those of target speech while keeping the phonetic content of the source speech unchanged [1]. VC enables expressions beyond the physical constraints of the human voice and enriches speech communication through social applications, such as speech-to-speech translation [2] and customer service [3]. As a result of developments in machine learning techniques [4, 5, 6] and well-designed speech corpora [7, 8], deep neural network (DNN)-based VC [9], which trains a VC model on a speech corpus including multiple speakers' voice samples, has become the mainstream of VC research and improved the quality of converted voices, an essential factor in VC performance evaluation.\\n\\nThe robustness of a trained VC model towards degraded speech input (i.e., degradation robustness [10]) is another crucial factor in real-world VC applications. Namely, models must be capable of high-quality VC even if the source speech, target speech, or both are degraded due to the recording environment and transmission channel. The degradation robustness can be improved by using artificially generated noisy speech when training the VC model or by cascading speech enhancement and VC models in the inference [11]. However, conventional degradation-robust VC work used only artificially degraded speech for VC evaluation. One primary reason is the lack of publicly available speech corpora recorded by devices owned by end-users with adequate annotations, which hinders validating the degradation robustness of state-of-the-art VC technologies.\\n\\nTo facilitate research on degradation-robust VC models, we present SRC4VC (Smartphone-Recorded Corpus for Voice Conversion), a new multi-speaker speech corpus for validating the degradation robustness of a VC model. The corpus consists of 11 hours of smartphone-recorded speech uttered by 100 crowdsourced Japanese speakers. Through crowdsourcing, the recorded samples were annotated with speaker-wise recording-quality scores and utterance-wise perceived emotion labels to the recorded samples. This design makes it possible to evaluate the robustness of a DNN-based VC model towards actual degraded speech input. We also benchmark SRC4VC on any-to-any VC, in which a multi-speaker VC model is trained on high-quality speech and used the SRC4VC speakers' voice samples as the source speech in VC. Our contributions are as follows:\\n\\n\u2022 We construct a new, publicly available corpus designed for evaluating the performance of VC from end-users' source speech input. Our corpus is open-sourced for research purposes only from https://y-saito.sakura.ne.jp/sython/Corpus/SRC4VC/index.html.\\n\u2022 We analyze the smartphone-recorded speech samples and discuss the results of speech quality assessments using DNNs and crowdsourcing.\\n\u2022 We present the results of an any-to-any VC experiment and show that, for VC using an end-user's voice as the source speech, the recording-quality mismatch can be addressed by simply introducing speech enhancement as preprocessing, rather than training a VC model with data augmentation.\\n\\n2. Construction of SRC4VC\\n2.1. Core design\\nWe designed SRC4VC so that it includes diverse voice samples from various actual smartphones and advances various VC tasks, such as emotional VC [12] and singing VC [13]. Specifically, the corpus includes 100 speakers who each recorded 52 voice samples categorized into the following four subsets: 10 read-aloud, 30 expressive, 10 conversational, and two singing samples. To examine non-parallel any-to-any VC, we randomly selected sentences for each speaker to record from the subsets except for singing.\\n\\nRead-aloud subset: We randomly selected 10 sentences per speaker from the Recitation324 subset of ITA [14], which contains 324 phonetically balanced sentences. This subset aims to record neutral, reading-style samples.\\n\\nExpressive subset: We used JVNV [15], which covers six emotions (Angry, Disgust, Fear, Happy, Sadness, Surprise). We randomly selected five sentences for each emotion category and asked the speakers to read the presented sentences while expressing the corresponding emotion.\\n\\nConversational subset: We used STUDIES [16] and CALLS [17] consisting of teacher-student and operator-customer dialogues, respectively. Although these corpora contain human-annotated emotion labels, we did not present the labels to the speakers.\"}"}
{"id": "saito24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"belts to the speakers because the ground-truth emotion for actual conversation generally cannot be defined. Instead, we allowed the speakers to express their own emotions by interpreting the presented sentences.\\n\\nSinging subset:\\nWe used two Japanese copyright-free songs: \u201ckatatsumuri\u201d (child-song) and \u201cShining star\u201d (J-POP).\\n\\nWe asked the speakers to sing only the first chorus of these two songs because singing entire songs without retakes is difficult for non-professionals.\\n\\n2.2. Voice recording by crowdworkers\\nWe crowdsourced smartphone-recorded voice samples using the above-mentioned four subsets. We collected the samples by macrotask crowdsourcing [18], where crowdworkers are employed by clients taking the workers' skills and experiences into consideration. We describe the recording process in detail below.\\n\\nPreparing voice recording platform:\\nWe created a webpage for voice recording that can be accessed via a smartphone. The webpage contained instructions for recording for each subset, a recording start/stop button, and text with the pronunciation of the words to be spoken. The recording function was implemented using Web Audio API [186].\\n\\nRecording speakers' voice samples:\\nWe recruited speakers through crowdsourcing and required them to understand the purpose of this project (i.e., collecting smartphone-recorded voice samples), have a smartphone, and record their voice in a quiet environment. We asked the speakers to record their voice using smartphones and our web-based recording platform. The speakers recorded 52 samples under the four subsets described in Section 2.1 in their own rooms. They could listen to reference singing voice samples when recording the \u201cSinging\u201d subset. We allowed the speakers to rerecord their voice if they made a mistake in the recording. After the recording process, the speakers submitted their recordings to a web server owned by the authors. The sampling rate was 48 kHz. With our manual validation of recorded data, we asked some of the speakers to rerecord misrecorded samples.\\n\\n2.3. Annotation\\nWe crowdsourced speaker-wise recording quality scores and utterance-wise perceived emotion labels.\\n\\nSpeaker-wise recording quality scores:\\nWe conducted recording quality rating of the collected voice samples and constructed the annotation sets as follows. Allowing sample overlap among sets, we made 400 sets containing 25 read-aloud utterances for each. The final speaker-wise recording quality scores were calculated by taking the average of 100 obtained scores for each speaker.\\n\\nUtterance-wise perceived emotion labels:\\nWe conducted emotion classification tests of the recorded \u201cExpressive\u201d and \u201cConversational\u201d subsets. The purpose was to investigate how well the speakers recruited by crowdsourcing were able to express the instructed or situation-oriented emotion in their speech. For each of the 30 expressive and 10 conversational samples, five different crowdworkers annotated the perceived emotion from seven options (i.e., the six emotions and Neutral).\\n\\nTable 1:\\nCrowdsourcing setup: the numbers of recruited crowdworkers and the amounts of paid rewards (per crowdworker). \u201cRec,\u201d \u201cEmo,\u201d \u201cexpr,\u201d and \u201cconv\u201d in this table denote recording, emotion, expressive, and conversational, respectively.\\n\\n| Task                        | # workers | Rewards |\\n|-----------------------------|-----------|---------|\\n| Voice rec.                  | 100       | $33.2   |\\n| Rec quality MOS test        | 400       | $0.44   |\\n| Emo labeling (expr)         | 500       | $1.31   |\\n| Emo labeling (conv)         | 500       | $0.44   |\\n\\nFigure 1:\\nHistogram of SNRs averaged over speakers.\\n\\n3. Corpus Analysis\\n3.1. Crowdsourcing setting\\nWe used Lancers [3] as the crowdsourcing platform. The overall period of the voice recording ran 2023. Table 1 lists the number of crowdworkers and the amounts of paid rewards per crowdworker. The recording quality ratings were conducted on the basis of five-scale mean opinion score (MOS) test on a scale of 1 (very bad) to 5 (very good).\\n\\n3.2. Corpus specification\\nOur SRC4VC consists of 1,000 read-aloud, 3,000 expressive, 1,000 conversational, and 200 singing voice samples recorded by 100 speakers, the total durations of which are 1.46, 7.16, 1.66, and 0.87 hours, respectively.\\n\\nFigure 1 shows the histogram of averaged priori signal-to-noise ratio (SNR) per speaker. We estimated SNRs based on the Ephraim-Malah algorithm [23] implemented in Essentia [24]. The minimum, median, and maximum values of the estimated SNRs with the speaker IDs are 22.2 dB (044), 32.5 dB (017), and 47.3 dB (064), respectively. These results indicate that our SRC4VC covers wider SNR range (approximately 25 dB) of recorded samples.\\n\\n3.3. Speaker distribution\\nWe discuss the distributions of SRC4VC speakers regarding gender, age, locale, and used device. The ratio of male to female speakers was 37 to 63, and the histogram of speaker ages is shown in Figure 2. The youngest and oldest ages with the speaker IDs were 21 (085) and 68 (009), respectively. Figure 3 maps the speakers' locales. Our SRC4VC covers 34 of 47 prefectures in Japan as the speaker locales. Sixty-one percent of the speakers used an iPhone as their recording device, with models ranging from SE to 15 Pro. These results demonstrate that our SRC4VC corpus consists of speech recorded by diverse speakers on various smartphones.\\n\\n3https://www.lancers.jp/\"}"}
{"id": "saito24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: List of existing open-sourced multi-speaker speech corpora related to SRC4VC. \u201cLang.\u201d and \u201cDur.\u201d denote covered languages and total durations in hours, respectively.\\n\\n| Corpus     | Speaking styles | Lang. | Dur. | # of speakers | Recording quality         |\\n|------------|-----------------|-------|------|---------------|---------------------------|\\n| VCTK [7]   | Reading w/ various accents | En    | 44   | 109           | Studio-recorded           |\\n| DDS [19]   | Reading (from VCTK [7] and DAPS [20]) | En    | 2000 | 48            | Device-rerecorded         |\\n| JVS [21]   | Reading, Whisper, Falsetto | Ja    | 30   | 100           | Studio-recorded           |\\n| CPJD [22]  | Reading w/ dialects | Ja    | 7    | 22            | Device-recorded           |\\n| SRC4VC     | Reading, Expressive, Conversational, Singing | Ja    | 11   | 100           | Smartphone-recorded       |\\n\\n3.4. Comparison with existing corpora\\n\\nTable 2 lists existing open-sourced multi-speaker speech corpora related to SRC4VC. Although SRC4VC is smaller than existing de-facto standard corpora for developing high-quality multi-speaker VC technologies, such as VCTK [7] and JVS [21], it covers various speech degradations that rarely occur in controlled and supervised studio recording. DDS [19] includes aligned parallel high-quality speech recordings taken from VCTK [7] and DAPS [20] and their rerecorded versions with various conditions by combining diverse acoustic environments and devices. In contrast, SRC4VC contains actual end-users' voice samples recorded in their own acoustic environments using smartphones. The design of SRC4VC was inspired by that of CPJD [22], in which the developers collected parallel speech data of Japanese dialects by crowdsourcing. As discussed in Section 3.3, our SRC4VC covers speakers from a wider area than those of CPJD (21 dialects).\\n\\n3.5. Annotation results\\n\\nWe analyze the results of the annotations for recording quality and emotion labels.\\n\\nTable 3: SRCC between human-annotated recording quality and each NISQA score. \u201cNat.\u201d denotes Naturalness.\\n\\n| Noisiness | Coloration | Discontinuity | Loudness |\\n|-----------|------------|---------------|----------|\\n| 0.15       | 0.67       | 0.36          | 0.54     |\\n\\nTable 4: Percentages of agreed emotional utterances in \u201cExpressive\u201d (expr) and \u201cConversational\u201d (conv) subsets.\\n\\n| Subset   | Ang | Dis | Fea | Hap | Sad | Sur | Neu |\\n|----------|-----|-----|-----|-----|-----|-----|-----|\\n| expr     | 14.6| 17.8| 14.4| 16.7| 15.7| 17.3| 0.35|\\n| conv     | 0.45| 0.29| 0.08| 0.59| 0.56| 0.28| 1.08|\\n\\nRecording quality scores: We computed the Spearman\u2019s rank correlation coefficient (SRCC) between the human-annotated recording quality MOS values and NISQA scores [25], which quantify the quality of input speech in terms of Noisiness, Coloration, Discontinuity, and Loudness [26, 27], in addition to Naturalness. Table 3 lists the results. The \u201cColoration\u201d and \u201cDiscontinuity\u201d scores moderately correlated with the human-annotated MOS values. These results indicate that dominant speech degradation factors in SRC4VC are frequency response and non-linear distortion.\\n\\nEmotion labels: We investigated the inter-annotator agreement of the perceived emotion labels. Specifically, we calculated the percentages of \u201cagreed emotional utterances.\u201d Namely, if more than two annotators assigned the same perceived emotion label to one utterance, we regarded it as the agreed emotional utterance. Table 4 lists the results. The distribution of agreed emotion labels for the \u201cExpressive\u201d subset was close to uniform except for \u201cNeutral.\u201d In contrast, the percentage of agreed emotion labels for the \u201cConversational\u201d subset was generally low, with the highest percentage of 1% for the \u201cNeutral\u201d emotion. These results suggest that the SRC4VC speakers could express and utter the indicated emotion to a certain extent, though they were not necessarily professional actors.\\n\\n4. Any-to-Any VC Experiment\\n\\n4.1. Experimental conditions\\n\\nOur experiment aims to investigate whether the recording-quality mismatch between the training data and evaluation data actually worsens VC performance and what kind of approaches can mitigate the mismatch.\\n\\nDatasets: We used JVS [21] for building an any-to-any VC model. The training, validation, and test sets included 86 (jvs001\u2013jvs086), 4 (jvs087\u2013jvs090), and 10 (jvs091\u2013jvs100) speakers, respectively. We used the parallel100 subset of JVS including 100 parallel speech utterances for each speaker as the training and validation sets. The test set, which was used as reference speech samples of unseen target speakers in any-to-any VC, was the nonpara30 subset of JVS including 30 non-parallel speech utterances for each speaker. Therefore, the trained VC model was evaluated on the task of converting 100 SRC4VC speakers\u2019 samples into those of the 4 JVS test speakers. Although our SRC4VC corpus contains the \u201cSinging\u201d subset, we excluded it from the evaluation data. The reason was that the VC task, i.e., zero-shot singing VC using a model trained\"}"}
{"id": "saito24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5: Objective evaluation results (NISQA Naturalness).\\n\\n\\\"DA\\\" and \\\"SE\\\" denote data augmentation and speech enhancement, respectively.\\n\\n| Methods                  | Q1   | Q2   | Q3   | Q4   | Avg. |\\n|--------------------------|------|------|------|------|------|\\n| Baseline (B)             | 2.36 | 2.50 | 2.38 | 2.41 | 2.41 |\\n| B + DA (noise)           | 2.39 | 2.53 | 2.41 | 2.47 | 2.45 |\\n| B + DA (reverb)          | 2.33 | 2.46 | 2.48 | 2.50 | 2.44 |\\n| B + DA (band)            | 2.34 | 2.51 | 2.43 | 2.51 | 2.45 |\\n| B + SE (Demucs)          | 2.39 | 2.50 | 2.42 | 2.43 | N/A  |\\n| B + SE (Miipher)         | 2.49 | 2.60 | 2.51 | 2.50 | 2.52 |\\n\\nWe observed that the relatively high-quality recorded speakers (i.e., Q3 and Q4) benefited from the data augmentation, while Q1 and Q2 suffered from the naturalness degradation when inappropriate data augmentations for them (i.e., reverb or band) were adopted. The optimal strategy was using Miipher for speech enhancement. These results suggest that, for VC using an end-user's voice as the source speech, the recording-quality mismatch can be addressed by simply introducing speech enhancement as preprocessing, rather than training a VC model with data augmentation.\\n\\nTable 6: Results of naturalness and similarity MOS tests.\\n\\n| Methods                  | Naturalness | Similarity |\\n|--------------------------|-------------|------------|\\n| Baseline (B)             | 2.54        | 2.17       |\\n| B + DA (noise)           | 2.59        | 2.18       |\\n| B + DA (reverb)          | 2.66        | 2.22       |\\n| B + DA (band)            | 2.62        | 2.17       |\\n| B + SE (Demucs)          | 2.53        | 2.17       |\\n| B + SE (Miipher)         | 2.74        | 2.21       |\\n\\n\\\"Baseline + SE (Miipher)\\\" achieved the highest naturalness, whose score was statistically significant compared to that of \\\"Baseline\\\" ($p < 0.05$). This further demonstrates the effectiveness of speech enhancement preprocessing.\\n\\n5. Conclusion\\n\\nWe presented SRC4VC, a new multi-speaker speech corpus for validating the degradation robustness of a VC model. The results of any-to-any VC experiments demonstrated that the recording-quality mismatch between the training and evaluation data significantly worsened the VC performance, and applying speech enhancement to the low-quality source speech samples improved the performance. In the future, we intend to develop a method to adapt a VC model trained with clean speech to an end-user's degraded speech input. We also continue to enlarge SRC4VC so that it includes speech degradations caused by transmission channels (e.g., live VoIP calls covered by the test subsets of NISQA corpus [25]).\\n\\nAcknowledgements:\\n\\nThis research was conducted as joint research between LY Corporation and Saruwatari-Takamichi Laboratory of The University of Tokyo, Japan. This work was...\"}"}
{"id": "saito24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] B. Sisman, J. Yamagishi, S. King, and H. Li, \u201cAn overview of voice conversion and its challenges: From statistical modeling to deep learning,\u201d *IEEE/ACM Transactions on Audio, Speech, and Language Processing*, vol. 29, pp. 132\u2013157, 2021.\\n\\n[2] F. Biadsy, R. J. Weiss, P. J. Moreno, D. Kanvesky, and Y. Jia, \u201cParrotron: An end-to-end speech-to-speech conversion model and its applications to hearing-impaired speech and speech separation,\u201d in Proc. INTERSPEECH, Graz, Austria, Sep. 2019, pp. 4115\u20134119.\\n\\n[3] H. Okano, Y. Okada, N. Wakatsuki, and K. Zempo, \u201cEffect of voice imitation using voice conversion by avatar on customer service in virtual environments,\u201d in Proc. VRST, Christchurch, New Zealand, Jun. 2023.\\n\\n[4] T. Kaneko and H. Kameoka, \u201cCycleGAN-VC: Non-parallel voice conversion using cycle-consistent adversarial networks,\u201d in Proc. EUSIPCO, Rome, Italy, Sep. 2018, pp. 2114\u20132118.\\n\\n[5] H. Kameoka, L. Li, S. Inoue, and S. Makino, \u201cSupervised determined source separation with multichannel variational autoencoder,\u201d *Neural Computation*, vol. 31, no. 9, pp. 1891\u20131914, Sep. 2019.\\n\\n[6] V. Popov, I. Vovk, V. Gogoryan, T. Sadekova, M. S. Kudinov, and J. Wei, \u201cDiffusion-based voice conversion with fast maximum likelihood sampling scheme,\u201d in Proc. ICLR, Virtual Conference, Apr. 2022.\\n\\n[7] J. Yamagishi, C. Veaux, and K. MacDonald, \u201cCSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit,\u201d https://doi.org/10.7488/ds/2645, 2019.\\n\\n[8] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibriTTS: A corpus derived from LibriSpeech for text-to-speech,\u201d in Proc. INTERSPEECH, Graz, Austria, Sep. 2019, pp. 1526\u20131530.\\n\\n[9] S. Desai, E. V. Raghavendra, B. Yegnanarayana, A. W. Black, and K. Prahallad, \u201cVoice conversion using artificial neural networks,\u201d in Proc. ICASSP, Taipei, Taiwan, Apr. 2009, pp. 3893\u20133896.\\n\\n[10] T.-h. Huang, J.-h. Lin, and H.-y. Lee, \u201cHow far are we from robust voice conversion: a survey,\u201d in Proc. SLT, Virtual Conference, 2021, pp. 514\u2013521.\\n\\n[11] C.-Y. Huang, K.-W. Chang, and H.-Y. Lee, \u201cToward degradation-robust voice conversion,\u201d in Proc. ICASSP, Singapore, May 2022, pp. 6777\u20136781.\\n\\n[12] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cEmotional voice conversion: Theory, databases and ESD,\u201d *Speech Communication*, vol. 137, pp. 1\u201318, 2022.\\n\\n[13] W.-C. Huang, L. P. Violeta, S. Liu, J. Shi, and T. Toda, \u201cThe Singing Voice Conversion Challenge 2023,\u201d in Proc. ASRU, Taipen, Taiwan, Dec. 2023.\\n\\n[14] J. Koguchi, I. Kanai, Y. Oda, T. Saito, and M. Morise, \u201cITA Corpus,\u201d https://github.com/mmorise/ita-corpus, 2021.\\n\\n[15] D. Xin, J. Jiang, S. Takamichi, Y. Saito, A. Aizawa, and H. Saruwatari, \u201cJVNV: A corpus of Japanese emotional speech with verbal content and nonverbal expressions,\u201d *IEEE Access*, vol. 12, pp. 19 752\u201319 764, Feb. 2024.\\n\\n[16] Y. Saito, Y. Nishimura, S. Takamichi, K. Tachibana, and H. Saruwatari, \u201cSTUDIES: Corpus of Japanese empathetic dialogue speech towards friendly voice agent,\u201d in Proc. INTERSPEECH, Incheon, South Korea, Sep. 2022, pp. 5155\u20135159.\\n\\n[17] Y. Saito, E. Iimori, S. Takamichi, K. Tachibana, and H. Saruwatari, \u201cCALLS: Japanese empathetic dialogue speech corpus of complaint handling and attentive listening in customer center,\u201d in Proc. INTERSPEECH, Dublin, Ireland, Aug. 2023, pp. 5561\u20135565.\\n\\n[18] E. Simperl, \u201cHow to use crowdsourcing effectively: Guidelines and examples,\u201d *LIBER Quarterly*, vol. 25, no. 1, pp. 18\u201339, Aug. 2015.\\n\\n[19] H. Li and J. Yamagishi, \u201cDDS: A new device-degraded speech dataset for speech enhancement,\u201d in Proc. INTERSPEECH, Incheon, South Korea, Sep. 2022, pp. 2913\u20132917.\\n\\n[20] G. J. Mysore, \u201cCan we automatically transform speech recorded on common consumer devices in real-world environments into professional production quality speech?\u2014a dataset, insights, and challenges,\u201d *IEEE Signal Processing Letters*, vol. 22, no. 8, pp. 1006\u20131010, Aug. 2015.\\n\\n[21] S. Takamichi, R. Sonobe, K. Mitsui, Y. Saito, T. Koriyama, N. Tanji, and H. Saruwatari, \u201cJSUT and JVS: Free Japanese voice corpora for accelerating speech synthesis research,\u201d *Acoustical Science and Technology*, vol. 41, no. 5, pp. 761\u2013768, Sep. 2020.\\n\\n[22] S. Takamichi and H. Saruwatari, \u201cCPJD Corpus: Crowdsourced parallel speech corpus of Japanese dialects,\u201d in Proc. LREC, Miyazaki, Japan, May 2018, pp. 434\u2013437.\\n\\n[23] Y. Ephraim and D. Malah, \u201cSpeech enhancement using a minimum-mean square error short-time spectral amplitude estimator,\u201d *IEEE Transactions on Audio, Speech, and Signal Processing*, vol. 32, no. 6, pp. 1109\u20131121, Dec. 1984.\\n\\n[24] D. Bogdanov, N. Wack, E. G\u00f3mez, S. Gulati, P. Herrera, O. Mayor, G. Roma, J. Salamon, J. Zapata, and X. Serra, \u201cESSENTIA: an open-source library for sound and music analysis,\u201d in Proc. ISMIR, Curitiba, Brazil, Nov. 2013, pp. 493\u2013498.\\n\\n[25] G. Mittag, B. Naderi, A. Chehadi, and S. M\u00f6ller, \u201cNISQA: A deep cnn-self-attention model for multidimensional speech quality prediction with crowdsourced datasets,\u201d in Proc. INTERSPEECH, Brno, Czech Republic, Sep. 2021, pp. 2127\u20132131.\\n\\n[26] M. Waltermann, *Dimension-based Quality Modeling of Transmitted Speech*. Springer, 2012.\\n\\n[27] N. C\u00f4t\u00e9, V. Gautier-Turbin, and S. M\u00f6ller, \u201cInfluence of loudness level on the overall quality of transmitted speech,\u201d in Proc. AES Convention, New York, U.S.A., Oct. 2007.\\n\\n[28] J. Lin, Y. Y. Lin, C.-M. Chien, and H.-Y. Lee, \u201cS2VC: A framework for any-to-any voice conversion with self-supervised pre-trained representations,\u201d in Proc. INTERSPEECH, Brno, Czech Republic, Sep. 2021, pp. 836\u2013840.\\n\\n[29] W.-C. Huang, S.-W. Yang, T. Hayashi, H.-Y. Lee, and T. Toda, \u201cS3PRL-VC: Open-source voice conversion framework with self-supervised speech representations,\u201d in Proc. AAAI SAS Workshop, Virtual Conference, Feb. 2022.\\n\\n[30] A. v. d. Oord, Y. Li, and O. Vinyals, \u201cRepresentation learning with contrastive predictive coding,\u201d *arXiv*, vol. abs/1807.03748, 2018.\\n\\n[31] I. Loshchilov and F. Hutter, \u201cSGDR: Stochastic gradient descent with warm restarts,\u201d in Proc. ICLR, Toulon, France, Apr. 2017.\\n\\n[32] \u2014\u2014, \u201cDecoupled weight decay regularization,\u201d in Proc. ICLR, New Orleans, U.S.A., May 2019.\\n\\n[33] J. Kong, J. Kim, and J. Bae, \u201cHiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d in Proc. NeurIPS, Vancouver, Canada, Dec. 2020.\\n\\n[34] D. Kingma and B. Jimmy, \u201cAdam: A method for stochastic optimization,\u201d in *arXiv preprint arXiv:1412.6980*, 2014.\\n\\n[35] A. D\u00e9fossez, G. Synnaeve, and Y. Adi, \u201cReal time speech enhancement in the waveform domain,\u201d in Proc. INTERSPEECH, Shanghai, China, Oct. 2020, pp. 3291\u20133295.\"}"}
