{"id": "yang23r_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG\\n\\nKai Yang1, Zhuang Xie2, Di Zhou3, Longbiao Wang1, Gaoyan Zhang1\u2217\\n\\n1Tianjin key Laboratory of Cognitive Computing and Application, Tianjin University, China\\n2School of Software, Henan University, China\\n3Japan Advanced Institute of Science and Technology, Japan\\nkaiyang@tju.edu.cn, zhuangxiecs@163.com, zhoudi@jaist.ac.jp, zhanggaoyan@tju.edu.cn\\n\\nAbstract\\nAuditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s-30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people\u2019s attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the \u03b4 and \u03b2 bands have high activity in attention tasks.\\n\\nIndex Terms: auditory attention detection, electroencephalography, real-life scenarios, common spatial pattern, brain-computer interface\\n\\n1. Introduction\\nThe latest Global Burden of Disease (GBD) study shows that the burden of hearing loss due to aging is increasing over time, and the global demand for assistive listening devices is growing [1]. Assistive listening devices, such as hearing aids and cochlear implants, can restore hearing in hearing-loss patients. Although these hearing devices have been improved over the past few decades, including the use of more advanced speech enhancement, directional beamforming, and noise suppression technologies [2], the most advanced hearing devices still do not work well in \u201ccocktail party\u201d [3] scenarios when multiple people are speaking at the same time. In such a scenario, normal-hearing people can easily distinguish and track the sound source of interest, while ignoring other sources. However, people with hearing impairments often have difficulty participating in conversations. While advanced speech enhancement algorithms can suppress background noise and enhance a speaker from a mix of speech, they often do not know which speaker to enhance. Researchers proposed extracting attention-related information from the brain to determine the attended speaker [4, 5, 6]. This problem is commonly known as Auditory Attention Detection (AAD). AAD algorithms can be integrated with speech separation technology, miniature EEG sensors, and intelligent gain systems in neuro-steered hearing devices [7] to selectively amplify the attended source, which will improve the quality of life of hearing-loss people.\\n\\nPrevious AAD researches have been based on the physiological basis that the brain can track speech envelope [4, 5]. Envelope tracking is reflected as a phase-locking effect between the neural signal and the speech envelope. Moreover, in multi-speaker scenarios, envelope tracking is enhanced for attended envelopes than unattended envelopes [4, 5, 8, 9]. Based on these findings, some researchers perform AAD by stimulus reconstruction (SR) methods [6, 10]. The SR methods construct a linear decoder to reconstruct speech envelopes from the recorded brain signals, such as magnetoencephalography (MEG) or electroencephalography (EEG). By comparing the correlation between the reconstructed envelopes and the actual envelopes of different speakers, the speaker with a higher correlation is identified as the attended one. The detection accuracy of the linear SR method is in the range of 82%-89% for a 60 s decision window. Such a long decision window is not suitable for realistic applications in hearing devices. However, as the decision window length decreases, especially below 10 s, the detection accuracy of the SR method will drop sharply [11]. Alickovic et al. [12] used canonical correlation analysis (CCA) to improve the accuracy of AAD. CCA algorithm finds the optimal linear transform to apply to both the stimulus envelopes and neural signals to reveal correlations between them. Using the 60 s decision window, the detection accuracy reaches about 90% on different datasets. Similarly, the accuracy performs poorly on shorter decision windows, with accuracies of about 58% and 68% for 1 s and 5 s [13].\\n\\nSR and CCA are both linear mapping methods. To improve detection accuracy, some researchers have proposed constructing deep neural network (DNN) models that can extract non-linear features to perform AAD. de Taillez et al. [14] used a simple Fully Connected Network (FCN) model to reconstruct the speech envelope from EEG signals and decode the attended speaker by correlation analysis. The accuracy of AAD is about 96.7% and 67.8% for decision windows of 60 s and 2 s. In [15], the authors proposed a convolutional neural network (CNN) model that uses EEG data and speech envelope features as input and implicitly computes the similarity between the EEG signals and the corresponding speech envelopes. The AAD accuracy is about 81% under a 10 s decision window. Cai et al. [16, 17] used the attention mechanism in neural networks to construct classification models with the accuracy of about 80%-88% for 2 s decision window and about 79%-84% for 1 s decision window. All the above studies employ clean speech envelopes as the model input, but this is not reasonable for practical applications because only mixed speech signals are available. Therefore, Vandecappelle et al. [18] used a CNN model to decode the locus of auditory attention (left/right) without knowledge of the speech envelopes, and the results show that the accuracy\"}"}
{"id": "yang23r_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A study used a multi-task learning model, in which the direct AAD classification algorithm, the principle of CSP is to solve a set of optimal spatial filters by diagonalizing matrices. The original signals are projected through filters into a lower-dimensional subspace so that the variance difference between the two classes of signals is the largest. Suppose the variance difference between the two classes of signals is $\\\\lambda$. Therefore, the discriminant model is:\\n\\n$$w = \\\\arg \\\\max \\\\left( \\\\sum_{t=1}^{N_{\\\\text{t}}} y_{1,t} (x_{1,t} - \\\\mu_{1})^T (x_{1,t} - \\\\mu_{1}) - \\\\sum_{t=1}^{N_{\\\\text{t}}} y_{2,t} (x_{2,t} - \\\\mu_{2})^T (x_{2,t} - \\\\mu_{2}) \\\\right)$$\\n\\nwhere $\\\\mu_{1}$ and $\\\\mu_{2}$ are the class feature means.\\n\\nThe commonly used bias is $\\\\log(\\\\frac{d_{\\\\lambda}(1)}{d_{\\\\lambda}(0)})$. The algorithm finds the optimal vector $v$ such that similar samples are as far away as possible [24]. The algorithm can classify the samples by $v$. Then we can choose a bias to classify the samples. The bias is $v^T(x - \\\\mu)$. Therefore, the feature vector $f$ is classified into class $C_1$ if $v^T(x - \\\\mu) > 0$, and into class $C_2$ if $v^T(x - \\\\mu) < 0$.\\n\\nFurther transforming Equation (4) into:\\n\\n$$w = \\\\arg \\\\max \\\\left( \\\\sum_{t=1}^{N_{\\\\text{t}}} y_{1,t} (x_{1,t} - \\\\mu_{1})^T (x_{1,t} - \\\\mu_{1}) - \\\\sum_{t=1}^{N_{\\\\text{t}}} y_{2,t} (x_{2,t} - \\\\mu_{2})^T (x_{2,t} - \\\\mu_{2}) \\\\right)$$\\n\\nIt is necessary to compute correlations over longer time windows. It is known that the brain, which does not require envelope features and avoids overfitting and the results may be poorer when changing different subjects' data, deep learning methods have a high risk of overfitting. In addition, we conducted experiments in different frequency bands to explore the contribution of EEG frequency bands in AAD. The CSP method extracts spatial features based on transient lateralization effects in scenarios instead of laboratory settings. The CSP method exploits the need to compute correlations over longer time windows. It is known that the brain, which does not require envelope features and avoids overfitting and the results may be poorer when changing different subjects' data, deep learning methods have a high risk of overfitting. In addition, we conducted experiments in different frequency bands to explore the contribution of EEG frequency bands in AAD.\"}"}
{"id": "yang23r_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3. Baseline AAD models\\n\\nIn this study, we compare three different AAD methods, including the classical SR method \\\\[6, 26\\\\], CCA method \\\\[12, 27\\\\], and a binary classification method \\\\[18\\\\] using advanced CNN. The SR method reconstructs the speech envelopes from the EEG signals by constructing linear decoders. The correlations between the reconstructed envelopes and the actual envelopes are calculated to detect the subjects' attention.\\n\\nCCA combines a spatio-temporal backward model (i.e. SR, mapping the EEG to the envelopes) and a temporal forward model (mapping the envelopes to the EEG) to make their output maximally correlated. CCA finds the best transformation matrices $W_1$ and $W_2$ for EEG signals $X_1$ and speech envelopes $X_2$. The columns of $X_1W_1$ are mutually uncorrelated, as are the columns of $X_2W_2$, while pairs of columns taken from both (\u201ccanonical correlate pairs\u201d) are maximally correlated \\\\[27\\\\]. The Pearson correlation coefficients between these pairs are defined as the canonical correlation coefficients. The attended speaker is identified by classifying the difference between the canonical correlation coefficients of the competing speakers using a classifier.\\n\\nThe CNN binary classification method uses EEG as input to predict the directional focus of subjects' attention. It consists of a convolutional layer with 5 kernels (kernel size: channels $\\\\times 17$) and two fully connected (FC) layers. The convolutional layer uses the rectified linear unit (ReLU) activation function and average pooling. The two FC layers have 5 and 2 units, respectively, followed by sigmoid activation. Loss is calculated using the cross-entropy loss function.\"}"}
{"id": "yang23r_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: AAD performance using different methods. Boldface indicated the best result.\\n\\n| Method | Window(s) |\\n|--------|-----------|\\n|        | 1 2 5 10 20 30 |\\n| SR [6] | 54.9 56.9 59.8 62.7 68.4 74.4 |\\n| CCA [12] | 55.3 58.7 62.8 65.5 71.8 77.5 |\\n| CNN [18] | 80.9 82.3 83.1 83.2 83.6 84.6 |\\n| CSP-SVM 1 | 80.1 81.5 82.0 84.6 85.0 84.6 |\\n| CSP-SVM 2 | 81.3 83.1 84.8 87.5 87.8 87.5 |\\n| CSP-LDA | 80.8 83.5 86.1 87.6 87.2 87.5 |\\n\\nFigure 1: AAD performance of different methods in sitting and walking states.\\n\\nCNN achieve accuracies of over 80% on 1 s window and the CSP achieves the highest accuracy on all decision windows. Paired t-tests were conducted to compare the performance of different methods. The results of the significance test show that the CSP-SVM 2 and CSP-LDA significantly outperform the CNN method ($t = 3.471, p < 0.05; t = 3.702, p < 0.05$), and there is no significant difference between CSP-SVM 1 and CNN ($t = 0.036, p = 0.973$). This indicates that the linear CSP filtering method meets or even exceeds the method using advanced neural networks. In addition, the accuracy of CSP-SVM 2 with the linear kernel is significantly better than that of CSP-SVM 1 with RBF kernel ($t = 7.621, p < 0.01$). There is also no significant difference between CSP-SVM 2 and CSP-LDA ($t = 0.413, p = 0.696$). However, LDA is faster to compute and requires fewer computational resources compared to SVM. Therefore, we used the LDA classifier in the subsequent analysis.\\n\\nWe further tested the AAD accuracy of the four methods in different states (sitting and walking), and the results are shown in Figure 1. It is found that the AAD accuracy in the sitting state is significantly higher than the accuracy in the walking state among all methods. The CSP method reached 85.13% and 76.56% accuracies for the 1 s decision window in the sitting and walking state, which shows that the CSP method can decode subjects' attention stably and accurately in real-life scenarios.\\n\\n4.2. The impact of different frequency bands on AAD\\n\\nTo investigate the contribution of different frequency bands in AAD, we further filtered the EEG data into different sub-bands, i.e. \u03b4 band (1-4 Hz), \u03b8 band (4-8 Hz), \u03b1 band (8-12 Hz), and \u03b2 band (12-30 Hz) to train the CSP filters and decode the directional focus of attention. The results are shown in Figure 2, and it can be found that the \u03b4 band has the highest decoding accuracy in the sitting state, but performs the worst in the walking state. The \u03b2 band contributes the most in the walking state, and the decoding accuracy even exceeds the sitting state in some decision windows. We conjecture that selective attention during sitting would work by increasing the gain of the low-frequency EEG signal for the attended speech [8, 6]. In contrast, during walking, subjects need to focus their attention more, the \u03b4 band is suppressed and the \u03b2 band dominates. Averaging the results of sitting and walking reveals that the \u03b2 band continues to contribute the most in the AAD task, consistent with the results obtained in previous studies [33, 34].\\n\\n5. Conclusions\\n\\nIn this study, we used a CSP linear filtering method to detect the directional focus of attention in real-life scenarios and achieved a decoding accuracy of 81.3% on a 1 s decision window. We distinguished the AAD accuracy of subjects in different behavioral states (sitting and walking). Although the accuracy is slightly lower in the walking state, it still shows potential for application. Experiments with sub-bands reveal significant contributions from \u03b4 and \u03b2 bands. Compared with traditional linear mapping methods and advanced CNN models, the CSP method is more accurate, responsive, robust, and requires fewer computational resources, which lays the foundation for realistic applications of neuro-steered hearing aids. Future research can be extended to extract EEG signals with a higher signal-to-noise ratio in living environments to improve AAD accuracy and explore AAD in scenes with more speakers.\\n\\n6. Acknowledgements\\n\\nThis work was supported by the National Natural Science Foundation of China (NO.61876126) and the Innovation Fund of Tianjin University (No. 2023XSU-0026).\"}"}
{"id": "yang23r_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] L. Haile, A. Orji, P. Briant, J. Adelson, A. Davis, and T. Vo, \u201cUpdates on hearing from the global burden of disease study,\u201d Innovation in Aging, vol. 4, no. Suppl 1, p. 808, 2020.\\n\\n[2] S. Haykin and K. R. Liu, Handbook on array processing and sensor networks. John Wiley & Sons, 2010.\\n\\n[3] E. C. Cherry, \u201cSome experiments on the recognition of speech, with one and with two ears,\u201d The Journal of the Acoustical Society of America, vol. 25, no. 5, pp. 975\u2013979, 1953.\\n\\n[4] N. Ding and J. Z. Simon, \u201cEmergence of neural encoding of auditory objects while listening to competing speakers,\u201d Proceedings of the National Academy of Sciences, vol. 109, no. 29, pp. 11 854\u201311 859, 2012.\\n\\n[5] N. Mesgarani and E. F. Chang, \u201cSelective cortical representation of attended speaker in multi-talker speech perception,\u201d Nature, vol. 485, no. 7397, pp. 233\u2013236, 2012.\\n\\n[6] J. A. O\u2019sullivan, A. J. Power, N. Mesgarani, S. Rajaram, J. J. Foxe, B. G. Shinn-Cunningham, M. Slaney, S. A. Shamma, and E. C. Lalor, \u201cAttentional selection in a cocktail party environment can be decoded from single-trial eeg,\u201d Cerebral cortex, vol. 25, no. 7, pp. 1697\u20131706, 2015.\\n\\n[7] S. Geirnaert, S. Vandecappelle, E. Alickovic, A. de Cheveigne, E. Lalor, B. T. Meyer, S. Miran, T. Francart, and A. Bertrand, \u201cElectroencephalography-based auditory attention decoding: toward neurosteered hearing devices,\u201d IEEE Signal Processing Magazine, vol. 38, no. 4, pp. 89\u2013102, 2021.\\n\\n[8] N. Ding and J. Z. Simon, \u201cNeural coding of continuous speech in auditory cortex during monaural and dichotic listening,\u201d Journal of Neurophysiology, vol. 107, no. 1, pp. 78\u201389, 2012.\\n\\n[9] E. M. Z. Golumbic, N. Ding, S. Bickel, P. Lakatos, C. A. Schevon, G. M. McKhann, R. R. Goodman, R. Emerson, A. D. Mehta, J. Z. Simon et al., \u201cMechanisms underlying selective neuronal tracking of attended speech at a \u201ccocktail party\u201d,\u201d Neuron, vol. 77, no. 5, pp. 980\u2013991, 2013.\\n\\n[10] M. J. Crosse, G. M. Di Liberto, A. Bednar, and E. C. Lalor, \u201cThe multivariate temporal response function (mtrf) toolbox: a matlab toolbox for relating neural signals to continuous stimuli,\u201d Frontiers in Human Neuroscience, vol. 10, p. 604, 2016.\\n\\n[11] W. Biesmans, N. Das, T. Francart, and A. Bertrand, \u201cAuditory-inspired speech envelope extraction methods for improved eeg-based auditory attention detection in a cocktail party scenario,\u201d IEEE Transactions on Neural Systems and Rehabilitation Engineering, vol. 25, no. 5, pp. 402\u2013412, 2016.\\n\\n[12] E. Alickovic, T. Lunner, F. Gustafsson, and L. Ljung, \u201cA tutorial on auditory attention identification methods,\u201d Frontiers in Neuroscience, p. 153, 2019.\\n\\n[13] S. Geirnaert, T. Francart, and A. Bertrand, \u201cFast eeg-based decoding of the directional focus of auditory attention using common spatial patterns,\u201d IEEE Transactions on Biomedical Engineering, vol. 68, no. 5, pp. 1557\u20131568, 2020.\\n\\n[14] T. de Taillez, B. Kollmeier, and B. T. Meyer, \u201cMachine learning for decoding listeners\u2019 attention from electroencephalography evoked by continuous speech,\u201d European Journal of Neuroscience, vol. 51, no. 5, pp. 1234\u20131241, 2020.\\n\\n[15] G. Ciccarelli, M. Nolan, J. Perricone, P. T. Calamia, S. Haro, J. O\u2019sullivan, N. Mesgarani, T. F. Quatieri, and C. J. Smalt, \u201cComparison of two-talker attention decoding from eeg with nonlinear neural networks and linear methods,\u201d Scientific Reports, vol. 9, no. 1, p. 11538, 2019.\\n\\n[16] S. Cai, P. Li, E. Su, and L. Xie, \u201cAuditory attention detection via cross-modal attention,\u201d Frontiers in Neuroscience, vol. 15, p. 652058, 2021.\\n\\n[17] S. Cai, E. Su, L. Xie, and H. Li, \u201cEeg-based auditory attention detection via frequency and channel neural attention,\u201d IEEE Transactions on Human-Machine Systems, vol. 52, no. 2, pp. 256\u2013266, 2021.\\n\\n[18] S. Vandecappelle, L. Deckers, N. Das, A. H. Ansari, A. Bertrand, and T. Francart, \u201cEeg-based detection of the locus of auditory attention with convolutional neural networks,\u201d Elife, vol. 10, p. e56481, 2021.\\n\\n[19] Z. Zhang, G. Zhang, J. Dang, S. Wu, D. Zhou, and L. Wang, \u201cEeg-based short-time auditory attention detection using multi-task deep learning,\u201d in INTERSPEECH, 2020, pp. 2517\u20132521.\\n\\n[20] U. Talukdar, S. M. Hazarika, and J. Q. Gan, \u201cAdaptation of common spatial patterns based on mental fatigue for motor-imagery bci,\u201d Biomedical Signal Processing and Control, vol. 58, p. 101829, 2020.\\n\\n[21] C. Zhang and A. Eskandarian, \u201cA computationally efficient multi-class time-frequency common spatial pattern analysis on eeg motor imagery,\u201d in 2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC). IEEE, 2020, pp. 514\u2013518.\\n\\n[22] M. Yan, Z. Lv, W. Sun, and N. Bi, \u201cAn improved common spatial pattern combined with channel-selection strategy for electroencephalography-based emotion recognition,\u201d Medical Engineering & Physics, vol. 83, pp. 130\u2013141, 2020.\\n\\n[23] B. Blankertz, R. Tomioka, S. Lemm, M. Kawanabe, and K.-R. Muller, \u201cOptimizing spatial filters for robust eeg single-trial analysis,\u201d IEEE Signal Processing Magazine, vol. 25, no. 1, pp. 41\u201356, 2007.\\n\\n[24] M. Ko\u0142odziej, A. Majkowski, and R. J. Rak, \u201cLinear discriminant analysis as eeg features reduction technique for brain-computer interfaces,\u201d Przeglad Elektrotechniczny, vol. 88, no. 3, pp. 28\u201330, 2012.\\n\\n[25] L. Wang, Support vector machines: theory and applications. Springer Science & Business Media, 2005, vol. 177.\\n\\n[26] L. Straetmans, B. Holtze, S. Debener, M. Jaeger, and B. Mirkovic, \u201cNeural tracking to go: auditory attention decoding and saliency detection with mobile eeg,\u201d Journal of Neural Engineering, vol. 18, no. 6, p. 066054, 2022.\\n\\n[27] A. de Cheveign\u00e9, D. D. Wong, G. M. Di Liberto, J. Hjortkj\u00e6r, M. Slaney, and E. Lalor, \u201cDecoding the auditory brain with canonical component analysis,\u201d NeuroImage, vol. 172, pp. 206\u2013216, 2018.\\n\\n[28] A. Delorme and S. Makeig, \u201cEeglab: an open source toolbox for analysis of single-trial eeg dynamics including independent component analysis,\u201d Journal of Neuroscience Methods, vol. 134, no. 1, pp. 9\u201321, 2004.\\n\\n[29] M. Plechawska-Wojcik, M. Kaczorowska, and D. Zapala, \u201cThe artifact subspace reconstruction (asr) for eeg signal correction. a comparative study,\u201d in Information systems architecture and technology: proceedings of 39th international conference on information systems architecture and technology\u2013ISAT 2018: part II. Springer, 2019, pp. 125\u2013135.\\n\\n[30] T. C. Ferree, \u201cSpherical splines and average referencing in scalp electroencephalography,\u201d Brain Topography, vol. 19, pp. 43\u201352, 2006.\\n\\n[31] O. Ledoit and M. Wolf, \u201cA well-conditioned estimator for large-dimensional covariance matrices,\u201d Journal of Multivariate Analysis, vol. 88, no. 2, pp. 365\u2013411, 2004.\\n\\n[32] N. Das, J. Zegers, T. Francart, A. Bertrand et al., \u201cLinear versus deep learning methods for noisy speech separation for eeg-informed attention decoding,\u201d Journal of Neural Engineering, vol. 17, no. 4, p. 046039, 2020.\\n\\n[33] J. R. Kerlin, A. J. Shahin, and L. M. Miller, \u201cAttentional gain control of ongoing cortical speech representations in a \u201ccocktail party\u201d,\u201d Journal of Neuroscience, vol. 30, no. 2, pp. 620\u2013628, 2010.\\n\\n[34] A.-L. Giraud and D. Poeppel, \u201cCortical oscillations and speech processing: emerging computational principles and operations,\u201d Nature Neuroscience, vol. 15, no. 4, pp. 511\u2013517, 2012.\"}"}
