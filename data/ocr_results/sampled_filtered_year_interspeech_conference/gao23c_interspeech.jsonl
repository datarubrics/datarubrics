{"id": "gao23c_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] \u201cWho: 1 in 4 people projected to have hearing problems by 2050,\u201d who.int, March 2021. [Online]. Available: https://www.who.int/news/item/02-03-2021-who-1-in-4-people-projected-to-have\\n\\n[2] R. O. Cornett, \u201cCued speech,\u201d American annals of the deaf, pp. 3\u201313, 1967.\\n\\n[3] S. K. Liddell and R. E. Johnson, \u201cAmerican sign language: The phonological base,\u201d Sign language studies, vol. 64, no. 1, pp. 195\u2013277, 1989.\\n\\n[4] C. Valli and C. Lucas, Linguistics of American sign language: An introduction. Gallaudet University Press, 2000.\\n\\n[5] W. C. Stokoe Jr, \u201cSign language structure: An outline of the visual communication systems of the american deaf,\u201d Journal of deaf studies and deaf education, vol. 10, no. 1, pp. 3\u201337, 2005.\\n\\n[6] \u201cFind your cued language,\u201d cuedspeech.org. [Online]. Available: https://cuedspeech.org/learn/find-your-cued-language/\\n\\n[7] L. Liu and G. Feng, \u201cA pilot study on mandarin chinese cued speech,\u201d American Annals of the Deaf, vol. 164, no. 4, pp. 496\u2013518, 2019.\\n\\n[8] L. Liu, G. Feng, X. Ren, and X. Ma, \u201cObjective hand complexity comparison between two mandarin chinese cued speech systems,\u201d in 2022 13th International Symposium on Chinese Spoken Language Processing (ISCSLP). IEEE, 2022, pp. 215\u2013219.\\n\\n[9] V. Attina, D. Beautemps, M.-A. Cathiard, and M. Odisio, \u201cA pilot study of temporal organization in cued speech production of french syllables: rules for a cued speech synthesizer,\u201d Speech Communication, vol. 44, no. 1-4, pp. 197\u2013214, 2004.\\n\\n[10] V. Attina, M.-A. Cathiard, and D. Beautemps, \u201cTemporal measures of hand and speech coordination during french cued speech production,\u201d in Gesture in Human-Computer Interaction and Simulation: 6th International Gesture Workshop, GW 2005, Berder Island, France, May 18-20, 2005, Revised Selected Papers 6. Springer, 2006, pp. 13\u201324.\\n\\n[11] L. Liu, G. Feng, D. Beautemps, and X.-P. Zhang, \u201cResynchronization using the hand preceding model for multi-modal fusion in automatic continuous cued speech recognition,\u201d IEEE Transactions on Multimedia, vol. 23, p. 292\u2013305, 2020.\\n\\n[12] L. Liu, G. Feng, and D. Beautemps, \u201cAutomatic dynamic template tracking of inner lips based on clnf,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, p. 5130\u20135134.\\n\\n[13] \u2014\u2014, \u201cAutomatic temporal segmentation of hand movements for hand positions recognition in french cued speech,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, p. 3061\u20133065.\\n\\n[14] L. Liu, T. Hueber, G. Feng, and D. Beautemps, \u201cVisual recognition of continuous cued speech using a tandem cnn-hmm approach.\u201d in Interspeech, 2018, p. 2643\u20132647.\\n\\n[15] N. Aboutabit, D. Beautemps, and L. Besacier, \u201cHand and lip desynchronization analysis in french cued speech: Automatic temporal segmentation of hand flow,\u201d in 2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings, vol. 1. IEEE, 2006, pp. I\u2013I.\\n\\n[16] L. Liu, G. Feng, D. Beautemps, and X.-P. Zhang, \u201cA novel resynchronization procedure for hand-lips fusion applied to continuous french cued speech recognition,\u201d in 2019 27th European Signal Processing Conference (EUSIPCO). IEEE, 2019, pp. 1\u20135.\\n\\n[17] M. Tang, X. Wu, P. Agrawal, S. Pongpaichet, and R. Jain, \u201cIntegration of diverse data sources for spatial pm2. 5 data interpolation,\u201d IEEE Transactions on Multimedia, vol. 19, no. 2, pp. 408\u2013417, 2016.\\n\\n[18] Y. Sun, M. Liu, and M. Q.-H. Meng, \u201cActive perception for foreground segmentation: An rgb-d data-based background modeling method,\u201d IEEE Transactions on Automation Science and Engineering, vol. 16, no. 4, pp. 1596\u20131609, 2019.\\n\\n[19] J. Wang, Z. Tang, X. Li, M. Yu, Q. Fang, and L. Liu, \u201cCross-modal knowledge distillation method for automatic cued speech recognition,\u201d p. 2986\u20132990, 2021.\\n\\n[20] S. Sankar, D. Beautemps, and T. Hueber, \u201cMultistream neural architectures for cued speech recognition using a pre-trained visual feature extractor and constrained ctc decoding,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 8477\u20138481.\\n\\n[21] L. Liu and L. Liu, \u201cCross-modal mutual learning for cued speech recognition,\u201d arXiv preprint arXiv:2212.01083, 2022.\\n\\n[22] H. Sloetjes, \u201c305ELAN: Multimedia Annotation Application,\u201d in The Oxford Handbook of Corpus Phonology. Oxford University Press, 05 2014. [Online]. Available: https://doi.org/10.1093/oxfordhb/9780199571932.013.019\\n\\n[23] P. Boersma, \u201cPraat: doing phonetics by computer [computer program],\u201d http://www.praat.org/, 2011.\\n\\n[24] X. Su, X. Yan, and C.-L. Tsai, \u201cLinear regression,\u201d Wiley Interdisciplinary Reviews: Computational Statistics, vol. 4, no. 3, pp. 275\u2013294, 2012.\\n\\n[25] D. E. King, \u201cDlib-ml: A machine learning toolkit,\u201d The Journal of Machine Learning Research, vol. 10, pp. 1755\u20131758, 2009.\\n\\n[26] \u2014\u2014, \u201cDlib-models,\u201d 2020. [Online]. Available: https://github.com/davisking/dlib-models\\n\\n[27] Google, \u201cMediapipe,\u201d 2020. [Online]. Available: https://github.com/google/mediapipe\\n\\n[28] C. Feichtenhofer, H. Fan, J. Malik, and K. He, \u201cSlowfast networks for video recognition,\u201d in Proceedings of the IEEE/CVF international conference on computer vision, 2019, pp. 6202\u20136211.\\n\\n[29] I. Loshchilov and F. Hutter, \u201cSgdr: Stochastic gradient descent with warm restarts,\u201d arXiv preprint arXiv:1608.03983, 2016.\"}"}
{"id": "gao23c_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus\\n\\nLufei Gao\u2020, Shan Huang\u2020, Li Liu\\n\\n1 Shenzhen Research Institute of Big Data, China\\n2 The Hong Kong University of Science and Technology (Guangzhou), China\\navrillliu@hkust-gz.edu.cn\\n\\nAbstract\\n\\nCued Speech (CS) is a multi-modal visual coding system combining lip reading with several hand cues at the phonetic level to make the spoken language visible to the hearing impaired. Previous studies solved asynchronous problems between lip and hand movements by a cuer-dependent piecewise linear model for English and French CS. In this work, we innovatively propose three statistical measures on the lip stream to build an interpretable and generalizable model for predicting hand preceding time (HPT), which achieves cuer-independent by a proper normalization. Particularly, we build the first Mandarin CS corpus comprising annotated videos from five speakers including three normal and two hearing impaired individuals. Consequently, we show that the hand preceding phenomenon exists in Mandarin CS production with significant differences between normal and hearing impaired people. Extensive experiments demonstrate that our model outperforms the baseline and the previous state-of-the-art methods.\\n\\nIndex Terms: Mandarin Cued Speech, Asynchronous multimodality, Hand preceding time, Hearing-impaired\\n\\n1. Introduction\\n\\nHearing-impairment is one of the social-concerned issues worldwide. According to the World Health Organization (WHO), more than 1.5 billion people worldwide are affected by some form of hearing impairment [1], presenting significant challenges for both society as a whole and the communities affected by this issue.\\n\\nCued Speech (CS) is a visual coding system for the spoken language invented by Cornett in 1967 [2]. CS differs from sign language (SL) [3, 4, 5] in that it follows the rules of a normal language system, using hand gestures to code phonemes and supplementing the limitations of lip reading.\\n\\nCurrently, CS has been adapted to about 65 languages and dialects all over the world [6]. Liu et al. proposed a Chinese cued speech system, specifically for Mandarin cuers [7, 8]. In practice, we have found that for adults either normal hearing or hearing-impaired, who master Chinese Pinyin, it only takes 24 hours for them to master the rules of Mandarin CS and use it for simple expressions. Similar to other languages, hand cues in Mandarin CS also tend to reach the target state before the lip cues during expression. In the literature, it is called hand preceding phenomenon or lip-hand asynchronous problem [9, 10, 11]. Comprehending and dissecting this phenomenon holds great significance in understanding the cognitive principles of CS expression, improving automatic CS recognition [12, 13, 14], and developing realistic CS synthesis systems.\\n\\nThere are three main contributions in this work:\\n\\n- A new multi-cuer mandarin CS corpus is developed with accurate manual annotation for the lip and hand movements. It contains CS videos produced by three normal cuers and two hearing loss cuers. As far as we know, this is the first Mandarin CS corpus that contains data from hearing-loss people which is incredibly valuable for the academic research of CS.\\n- A novel method is proposed to address the lip-hand multi-modal asynchronous problem in the CS system and to deal with cuer adaptation. The method utilizes three lip stream measures to achieve lip-hand alignment for temporal segmentation and selects proper strategies for cuer normalization.\\n- Extensive experimental results show that our proposed resynchronization method archives better performance compared with the previous methods concerning the resynchronization effect, interpretability, and generalization ability for multiple cuers. Besides, we discuss the hearing-impaired performance of their mandarin CS expression with some noteworthy observations.\\n\\n2. Related Work\\n\\n2.1. Hand preceding phenomenon\\n\\nThe temporal organization of hand and speech coordination during French CS production was studied in [9, 10]. It was found that hand movement reaches the target position 239 ms before the onset of the acoustic vowel and hand shape formation accounts for a large fraction of hand transitions [9]. This relationship is not occasional but was also found on other proficient CS cuers [10]. The study [15] measured a 144.5 ms long HPT for syllables extracted from French continuous sentences and affirmed the importance of the instant at which the hand reaches the target position.\\n\\n2.2. Lip-hand resynchronizations in CS recognition\\n\\nTo resolve the issue of lip-hand asynchrony, Liu et al. [11, 13, 16] analyzed the correlation between HPTs and their lip target temporal positions, where a piece-wise linear function is used for HPM. It is presumed that the HPT remains constant until the turning point is achieved, after which it quickly declines. As the model is based on limited English/French CS videos, it is difficult to generalize to new cuers producing Mandarin CS, thus less generalizable.\\n\\nAsynchronous multi-modal fusion is a fast-growing research area [17, 18]. Prior researches have explored the use of neural networks to tackle this issue in CS. In [19, 20], RNNs trained with a CTC loss were adopted to address the...\"}"}
{"id": "gao23c_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"asynchronous problem. In [21], a transformer based on cross-modal mutual learning was proposed by achieving feature fusion based on the aligned modalities with the long-time dependencies. However, the black-box methods lack interpretability, which limits our ability to identify the specific factors related to the issue.\\n\\nIn the following sections, we will describe the corpus construction and define three measurements on the acoustic stream and employ statistical-based display methods to address the asynchronous issue, thereby enhancing both the interpretability and generalization ability of the multi-modal fusion process.\\n\\n3. Methodology\\n\\n3.1. Mandarin CS Corpus\\n\\n3.1.1. Data collection\\n\\nTo collect the Mandarin CS videos, five volunteers, including three normal hearing (2 female and 1 male, named NF1, NF2, and NM1) and two hearing-impaired (1 female and 1 male, named DF1 and DM1) learned to use Mandarin CS in advance within 24 hours of training. A 1000-sentence Chinese corpus consisting of 5 themes was constructed for normal-hearing cuers, while a smaller corpus consisting of simpler sentences was provided for the hearing impaired cuers. Each sentence contains 4 to 27 words, 10.6 on average. The volunteers recorded 1000 videos at home, using the front-facing cameras on their personal mobile phones with the following standards: 1) white background with no decoration; 2) 720 HD at 30 fps; 3) turning off the camera mirror effect. Finally, we collected 3000 CS videos by the normal and 979 by the hearing-impaired cuers.\\n\\n3.1.2. Data annotation\\n\\nEach video contains two annotation documents of temporal segments corresponding to phonemes, one based on hand gestures and the other based on acoustics. Hand annotations were completed by ELAN [22], while acoustic annotations by Praat [23]. The annotations were all manually done and double-checked.\\n\\n3.2. Hand preceding analysis\\n\\nFor an input video, the audio signal and visual signal are regarded as the lip stream and hand stream, respectively. Due to the fact that one vowel corresponds to one syllable in Mandarin, only vowel segments are taken into our consideration. The required measurements related to the analysis are drawn in Fig. 1. Specifically, HPT is defined as the time difference between two instants, i.e., the hand target instant \\\\( t_{\\\\text{mid}}^i \\\\) and lip target instant \\\\( T_{\\\\text{mid}}^i \\\\) of the \\\\( i \\\\)-th vowel in a sentence. The target instants are the mid-time points between the start and the end time of the vowel segments extracted from temporal annotations. Therefore, we denote the \\\\( i \\\\)-th HPT by \\\\( \\\\Delta^i = t_{\\\\text{mid}}^i - T_{\\\\text{mid}}^i \\\\), where \\\\( N \\\\) is the number of syllables in a sentence.\\n\\n3.2.1. Lip vowel to end\\n\\nLip vowel to end (LVE) refers to the time difference between the lip target instant of a vowel and the sentence end. It is observed that HPT decreases sharply as the cuer is about to finish a sentence. This phenomenon can be explained from the perspective of cognitive function in the brain. The articulatory process of pronunciation lags behind the hand gestures during the expression, but as the expression is nearing completion, the two streams will gradually approach synchronization. Therefore, we define the variable that measures the time difference between the sentence end and the lip target instant of a vowel segment as lip vowel to end (LVE), denoted by \\\\( \\\\delta^i \\\\). The mapping relationship from \\\\( \\\\delta^i \\\\) to \\\\( \\\\Delta^i \\\\) is written as \\\\( \\\\hat{\\\\Delta}^i = f_0(\\\\delta^i) \\\\).\\n\\n3.2.2. Lip vowel interval\\n\\nPauses within an utterance may play a similar role for HPT at the end of the sentence. In other words, HPT decreases sharply to a small value when there is a large time gap between two syllables. Besides, a Mandarin syllable may have zero, one or two consonants preceding its vowel. When the hand moves from a vowel position to the next, a hand-shape transition occurs during Mandarin CS production if it is decorated with two consonants. The transition can increase the time duration between two syllables, thus lengthening the HPT. Therefore, we define the variable that measures the time difference between two adjacent lip target instants as lip vowel interval (LVI), denoted by \\\\( \\\\alpha^i \\\\). The last LVI of a sentence is set to the maximum value among previous LVI, i.e. \\\\( \\\\max_{0 < i < N - 1} \\\\alpha^i \\\\). The mapping relationship from \\\\( \\\\alpha^i \\\\) to \\\\( \\\\Delta^i \\\\) is written as \\\\( \\\\hat{\\\\Delta}^i = f_1(\\\\alpha^i) \\\\).\\n\\n3.2.3. Lip vowel duration\\n\\nAnother factor that may influence \\\\( \\\\Delta^i \\\\) is the amount of time that is spent pronouncing the vowel. Since half of the vowel segment is included in the HPT measurement, if the vowel is pronounced longer, HPT will be prolonged accordingly. Thereby, we refer to this measurement as lip vowel duration (LVD) and denote it by \\\\( \\\\beta^i \\\\). The mapping from \\\\( \\\\beta^i \\\\) to \\\\( \\\\Delta^i \\\\) is denoted by \\\\( \\\\hat{\\\\Delta}^i = f_2(\\\\beta^i) \\\\).\\n\\n3.3. Cuer normalization\\n\\nThe corpus adopted in this work consists of five cuers with different ages, genders, hearing conditions, and proficiencies of CS production. Besides, the number of words and the expression speed vary substantially across different videos. Hence, selecting appropriate normalization is necessary in order to address subject differences, i.e. the issue of cuer adaptation.\"}"}
{"id": "gao23c_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Z-score normalization is applied. The normalized variables are denoted by \\\\( \\\\Delta' \\\\) and \\\\( \\\\beta' \\\\), respectively. The descriptive statistics in the unit of milliseconds for each cue, average on all cues, average on normal cues, and average on deaf cues are shown in Table 1. The total number of vowel segments is listed in the last column of the table.\\n\\n| Cuers     | HPT   | LVD   | Syllable number |\\n|-----------|-------|-------|----------------|\\n| NF1       | 242   | 338   | 79             |\\n| NF2       | 246   | 389   | 94             |\\n| NM1       | 352   | 464   | 16             |\\n| DF1       | 154   | 365   | 96             |\\n| DM1       | 164   | 397   | 102            |\\n| NORMAL    | 280   | 397   | 121            |\\n| DEAF      | 159   | 369   | 99             |\\n| ALL       | 253   | 390   | 117            |\\n\\n*N/D stands for normal/deaf; F/M stands for female/male. NF1 represents \u201cNormal Female 1\u201d, NF2 represents \u201cNormal Female 2\u201d, and so forth. NORMAL refers to the corpus subset containing NF1, NF2 and NM1. DEAF refers to the corpus subset containing DF1 and DM1. ALL refers to the corpus containing five cues.\\n\\nLog-scale normalization is used for LVI as it shows a left-skewed distribution, denoted by \\\\( \\\\alpha' = \\\\log \\\\alpha \\\\). Due to the different lengths and number of characters among videos, the statistical distribution of LVE is also skewed to left. So log-scale normalization is suggested for LVE as well, denoted by \\\\( \\\\delta' = \\\\log \\\\delta \\\\).\\n\\nFig. 2 shows the linear regressions (LRs) \\\\( f_0(\\\\delta') \\\\), \\\\( f_1(\\\\alpha') \\\\) and \\\\( f_2(\\\\beta') \\\\) based on ALL. Due to the presence of a distinct inflection point towards the end of the sentence, a piece-wise LR is employed for \\\\( f_0 \\\\), where the segment point is denoted by \\\\( \\\\gamma \\\\). It is observed that HPT is dominated by LVE when \\\\( \\\\delta' \\\\) is smaller than \\\\( \\\\gamma \\\\). Otherwise, when the lip target instant is away from the sentence end, we propose a linear combination of \\\\( f_1(\\\\alpha') \\\\) and \\\\( f_2(\\\\beta') \\\\) to predict HPT. As in Eq. (1), \\\\( f(\\\\cdot) \\\\) is a segmentation function taking in the normalized LVE, LVI, and LVD as input variables to calculate HPT, where \\\\( \\\\lambda_1 + \\\\lambda_2 = 1 \\\\) and \\\\( \\\\lambda_1 > 0, \\\\lambda_2 > 0 \\\\). Intuitively, if LVI is larger for a syllable in a sentence, it is logical to assume that \\\\( f_1(\\\\alpha') \\\\) carries a greater weight and the same holds true when LVD is larger. Therefore we define \\\\( \\\\lambda_1 \\\\) and \\\\( \\\\lambda_2 \\\\) as in Eq. (2), where \\\\( \\\\bar{\\\\alpha} \\\\) and \\\\( \\\\bar{\\\\beta} \\\\) are the mean values of \\\\( \\\\alpha \\\\) and \\\\( \\\\beta \\\\) in a sentence, respectively.\\n\\nIn the following experiments, \\\\( \\\\gamma \\\\) is empirically set to -34 and LR analyses are conducted for ALL, NORMAL and DEAF separately, named A-LR, N-LR and D-LR for short, respectively.\\n\\n### 4. Experimental Setup\\n\\n#### 4.1. Data pre-processing\\n\\nTo evaluate the effectiveness of the proposed HPT model, we conduct the vowel recognition experiment and calculate the average distance between the hand coordinates at the target time point and the predicted time point. Therefore, video clips containing vowels are segmented based on the lip annotations, hand annotations and the predicted hand target instants. The regions of interest (ROIs) of lip and hand are then extracted from each clip using the facial landmark detector included in the dlib library [25, 26] and hands solution in mediapipe [27], respectively. Once the key points of the lips and hands have been extracted, a 140 \\\\( \\\\times \\\\) 140 square is obtained around the lips, while a 320 \\\\( \\\\times \\\\) 320 square is obtained around the hands, with the exception of DM1, whose hand square is set to 400 \\\\( \\\\times \\\\) 400 due to larger hand size. To do the vowel recognition, the training and testing sets are divided randomly in a 4:1 ratio based on sentences.\\n\\n#### 4.2. Vowel recognition\\n\\nWe utilize the Slowfast video classification network [28] as the backbone for feature extraction and concatenate the lip and hand features for multi-modal feature fusion. The fused feature is then classified using a two-layered multilayer perceptron into 16 vowel categories.\\n\\nThe network is initialized randomly and trained using the SGD optimizer with an initial learning rate of 0.01, which is adjusted using the CosineAnnealing policy [29]. The mini-batch size is set to 64, and the entire network is trained for 100 epochs.\\n\\n#### 4.3. Metrics\\n\\nThree metrics are adopted to evaluate the performance of HPT predictions. The first is mean squared error (MSE) between the HPT predictions and their ground truth in the normalized scale, denoted by \\\\( e_{HPT} \\\\). The second is mean hand coordinate distances (MHCD) between the hand coordinates at \\\\( T_{mid_i} \\\\) and \\\\( t_{mid_i} - \\\\hat{\\\\Delta}_i \\\\), denoted by \\\\( d_{HPT} = \\\\frac{1}{N} \\\\sum_{i=0}^{N-1} \\\\sqrt{(x_i - \\\\hat{x}_i)^2 + (y_i - \\\\hat{y}_i)^2} \\\\), where \\\\( (x_i, y_i) \\\\) and \\\\( (\\\\hat{x}_i, \\\\hat{y}_i) \\\\) are the hand coordinates of two time instants. The third is the vowel recognition accuracy, defined as the ratio of correctly recognized vowels to the total number of vowels.\\n\\n### 5. Results and Discussion\\n\\nThe hand temporal segmentation is done through five HPT calculations: ground truth by hand annotations, named GT; baseline by audio annotations, named audio-based; mean HPT value\"}"}
{"id": "gao23c_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: The results of vowel recognition.\\n\\n| LR data     | Cuers       | Mono-Modal | Multi-Modal Fusion (Lip + Hand) |\\n|-------------|-------------|------------|---------------------------------|\\n| Lip Hand    | Ground truth | Audio-based | LVE                             |\\n|             |             | Mean-based  | LVE + LVI + LVD                 |\\n| ALL         | NF1         | 86.52%     | 81.19%                          |\\n|             | NF2         | 93.91%     | 77.16%                          |\\n|             | NM1         | 81.57%     | 71.79%                          |\\n|             | DF1         | 86.20%     | 80.54%                          |\\n|             | DM1         | 76.59%     | 73.18%                          |\\n|             | Average     | 86.06%     | 76.78%                          |\\n| Normal      |             |            |                                 |\\n|             | NF1         | 87.33%     | 76.71%                          |\\n|             | NF2         | 81.58%     | 77.01%                          |\\n| DEAF        |             |            |                                 |\\n|             | Average     | 81.58%     | 77.01%                          |\\n\\n*N/D stands for normal/deaf; F/M stands for female/male. NF1 represents \u201cNormal Female 1\u201d, NF2 represents \u201cNormal Female 2\u201d, and so forth.\\n\\nFigure 3: Comparison of MSE and MHCD: (a) MSE across different corpus subsets calculated on different LRs. (b) Boxplot of MHCD across different cuers calculated on different LRs.\\n\\nThe vowel recognition results are shown in Table 2. LVE + LVI + LVD achieves the closest accuracy to GT. It even exceeds GT for NF2 by A-LR, reaching as much as 98.39% accuracy rate, which may be due to inaccurate manual annotations. The results of MSE and MHCD are shown in Fig. 3. Comparing with mean-based, LVE performs better on both metrics, while LVE + LVI + LVD leads to the minimum MSE and MHCD. Besides, Fig. 3(b) shows that LVE + LVI + LVD has better stability.\\n\\nTo visualize the correcting effect of the method, polar coordinates are used to represent hand position, where the center point of the lip is set to be the coordinates origin. The spatial distributions of hand positions at the predicted hand target instants are shown in Fig. 4. LVE + LVI + LVD has a corrective effect on the hand positions at target instants by revising the HPT values. Intuitively, a better hand temporal segment corresponds to a more separable hand position distribution. It is obvious that the separability across five positions becomes more and more remarkable from (b) to (d). In summary, the proposed method can improve the performance of hand temporal segmentation by comprehensive evaluation.\\n\\n5.2. Comparison between normal and hearing-impaired\\n\\nAs shown in Table 1, the statistic characteristics are considerably different between NORMAL and DEAF. A noteworthy observation is that the mean HPT of DEAF is significantly lower than that of NORMAL. This may be because hearing-impaired subjects have less accurate speech perception, leading to their hand movements arriving closer in time to the lip target instant. Moreover, female cuers generally exhibit smaller mean HPT values than male cuers, indicating that women's language proficiency extends to cued speech.\\n\\nIn Fig. 3(a), the MSE of NORMAL by A-LR are almost the same as NORMAL by N-LR. But the MSE reduction in DEAF by D-LR is significant. This is reasonable as the amount of DEAF data occupies less than 25% in the complete corpus, so that there is a larger bias compared with the NORMAL group. This finding also gives a hint that it may be better to distinguish between different hearing conditions when developing a CS recognition system.\\n\\n6. Conclusions\\n\\nWe propose a novel re-synchronization method for the lip-hand asynchronous problem during CS production. A multi-cuer mandarin CS corpus is collected to build the statistical-based model. Extensive experiments illustrate the efficiency of our proposed method and the differences that exist between normal and hearing-impaired cuers are discussed. In the future, in addition to expanding the corpus, we will use the interpretable model to provide semi-supervised signals to train a pre-trained model for automatic CS segmentation and recognition.\"}"}
