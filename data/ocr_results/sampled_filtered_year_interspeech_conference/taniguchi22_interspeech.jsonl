{"id": "taniguchi22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation\\n\\nShuta Taniguchi, Tsuneo Kato, Akihiro Tamura, Keiji Yasuda\\n\\n1 Doshisha University, Japan, 2 MINDWORD Co. Ltd., Japan\\n\\ntskato@mail.doshisha.ac.jp\\n\\nAbstract\\n\\nIn the training programs of human simultaneous interpreters, trainee speech is transcribed into text for quality assessment. Despite the challenges that interpreter speech contains disfluencies such as hesitations, filled pauses, interruptions, and self-repairs, automatic speech recognition (ASR) is expected to be introduced to save labor of transcription. In the training programs, source language text can be utilized for ASR because the training materials are prepared in advance. Thus, we propose a Transformer-based end-to-end ASR with an auxiliary input of a source language text toward transcribing simultaneous interpretation. Because a sufficient amount of human interpreter speech with source language text is not available for training the model, we conducted the initial evaluation of the model by simulating speech with source language text by changing the inputs and outputs of large-scale corpora for developing end-to-end speech translation (ST). Our proposed model significantly reduced word error rates (WERs) for four ST datasets: MuST-C English speech - Dutch text, English speech - German text, CoVoST 2 English speech - Japanese text, and our original TED-based English speech - Japanese text dataset.\\n\\nIndex Terms: automatic speech recognition, multimodal inputs, Transformer\\n\\n1. Introduction\\n\\nTrainees take various interpretation tests in the training programs of human simultaneous interpreters. Trainers rate the quality of the trainees' interpretation using transcribed texts to dispense with playing the speech repeatedly and to prevent their subjective impression of speech from affecting the score. Introduction of today's fast-evolving ASR is expected to automatically transcribe interpreter speech. The ASR will face challenges of disfluencies such as hesitations, filled pauses, interruptions, and self-repairs. Meanwhile, the ASR for the training programs of simultaneous interpreters has an advantage of having access to source language text because the test materials are prepared in advance. We aim to improve its accuracy by using source language text as an auxiliary input.\\n\\nMultimodal machine learning is actively studied in the context of deep learning. Having access to multiple modalities makes inference robust and provides complementary information [1]. In the ASR field, audiovisual speech recognition (AV-ASR), which incorporates visual information from facial videos into ASR, has long been studied, and many studies have shown that the visual information contributes to noise robustness [2, 3]. AV-ASR now renews attention on how to incorporate two modalities effectively in an end-to-end manner [4, 5].\\n\\nTextual input is more popular in combination with visual tasks such as visual question answering [6, 7] than audio tasks. Joint learning of visual and textual information with Transformer [8] flexibly captures the alignment between elements of an input text and regions in an image input with its attention mechanism, and it is even sensitive to syntactic relations such as associations between verbs and image regions that correspond to their arguments. Transformer-based vision-and-language models are extended to a unified Transformer encoder-decoder model that handles multiple tasks and domains in a single model [9]. The unified Transformer model created a new application of generating live video comments based on visual and textual contexts [10]. As for a combination of speech and textual input, multimodal emotion recognition based on para-linguistic features from speech and linguistic features from transcribed text was proposed [11]. While exploring effective para-linguistic features based on wav2vec 2.0 [12] features, they extracted linguistic features based on Transformer [8].\\n\\nIn this paper, we propose an end-to-end multimodal speech recognition with audio and source language text inputs based on Transformer architecture. Our model is not an explicit error correction model but a Transformer-based encoder-decoder model that integrates two modalities in the middle of the processing and whose model parameters are optimized in an end-to-end manner. Training the Transformer-based end-to-end model requires a vast amount of speech from simultaneous interpretation with source language text that is currently nonexistent. Thus, we simulate the speech with source language texts by changing the inputs and outputs of large-scale corpora for developing ST. To be exact, we treated the correct speech translation as the source language text and evaluated the ASR's accuracies with and without the source language text.\\n\\n2. Transformer-based multimodal ASR with auxiliary input of source language text\\n\\n2.1. Model structure\\n\\nOur model consists of two Transformer encoders for inputting audio and source language text and one Transformer decoder for outputting recognized words. Figure 1 illustrates the model architecture. From left to right, the blocks aligned in three vertical lines represent the source language text encoder, the audio encoder and the decoder. The colors shading the blocks represent the types of Transformer blocks. The gray blocks have a multi-head self-attention and a feed-forward network followed by a residual connection, layer normalization, and dropout with no source-target attention. The orange and green blocks have one and two multi-head source-target attentions, respectively, in between the multi-head self-attention and the feed-forward network. Every Transformer block in the decoder has a source-target attention to the output of the audio encoder. Additionally, one Transformer block (the green one) in the decoder and one Transformer block (the orange one) in the audio encoder have a source-target attention to the output of the source language text encoder.\"}"}
{"id": "taniguchi22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1.1. Source language text encoder\\nThe source language text encoder on the left side of Figure 1 accepts a tokenized source language text \\\\( S = \\\\{s_1, s_2, \\\\ldots, s_L\\\\} \\\\), embeds the tokens into a series of \\\\( d_M \\\\)-dimensional vectors \\\\( \\\\bar{S} \\\\), and adds positional encoding. The process is expressed as follows:\\n\\n\\\\[\\n\\\\bar{S} = \\\\text{TextEmbed}(S) \\\\quad (1)\\n\\\\]\\n\\n\\\\[\\nX_0^S = \\\\bar{S} + \\\\text{PositionEnc} \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( X_0^S \\\\), an \\\\( L \\\\times d_M \\\\) matrix, denotes an input into the first encoder block. The \\\\( i \\\\)-th Transformer encoder block in the \\\\( N_T \\\\) stacks (\\\\( 1 \\\\leq i \\\\leq N_T \\\\)) receives an output of the previous block and outputs \\\\( X_i^S \\\\), a matrix of the same size.\\n\\n\\\\[\\nX_i^S = \\\\text{TfEncBlk}(X_{i-1}^S) \\\\quad (3)\\n\\\\]\\n\\nAs Figure 1 shows, a standard Transformer encoder block \\\\( \\\\text{TfEncBlk}(\\\\cdot) \\\\) consists of a multi-head self-attention and a fully-connected feed-forward network, each followed by an add & norm layer.\\n\\n2.1.2. Audio encoder\\nThe audio encoder in the middle of Figure 1 accepts a time series of acoustic features \\\\( A = \\\\{a_1, a_2, \\\\ldots, a_M\\\\} \\\\), extracts local temporal patterns in the acoustic features with a 2D convolutional neural network (2D-CNN), and adds positional encoding. The process is expressed as follows:\\n\\n\\\\[\\n\\\\bar{A} = \\\\text{Conv2D}(A) \\\\quad (4)\\n\\\\]\\n\\n\\\\[\\nX_0^A = \\\\bar{A} + \\\\text{PositionEnc} \\\\quad (5)\\n\\\\]\\n\\nwhere \\\\( X_0^A \\\\), an \\\\( M \\\\times d_M \\\\) matrix, is an input into the first encoder block. The \\\\( j \\\\)-th Transformer encoder block in the \\\\( N_A \\\\) stacks (\\\\( 1 \\\\leq j \\\\leq N_A \\\\)) outputs \\\\( X_j^A \\\\) in the same way as the source language text encoder except that the \\\\( M_A \\\\)-th encoder block has a source-target attention to the output of the source language text encoder.\\n\\n\\\\[\\nX_j^A = \\\\begin{cases} \\n\\\\text{TfEncBlk}(X_{j-1}^A) & \\\\text{if } j \\\\neq M_A \\\\\\\\\\n\\\\text{TfEncAttnBlk}(X_{j-1}^A, X_{N_T}^S) & \\\\text{if } j = M_A \\n\\\\end{cases} \\\\quad (6)\\n\\\\]\\n\\nThe \\\\( M_A \\\\)-th encoder block has a multi-head source-target attention in between a multi-head self-attention and a fully-connected feed-forward network, each followed by an add & norm layer, as shown in Figure 1.\\n\\n2.1.3. Decoder\\nThe decoder on the right side of Figure 1 autoregressively outputs recognized tokens. The decoder receives previous output tokens \\\\( \\\\hat{W}_0:t-1 = \\\\{w_0, \\\\hat{w}_1, \\\\ldots, \\\\hat{w}_{t-1}\\\\} \\\\), \\\\( w_0 = \\\\langle s \\\\rangle \\\\), embeds them into a series of \\\\( d_M \\\\)-dimensional vectors \\\\( \\\\bar{W}_0:t-1 \\\\), and adds positional encoding as follows:\\n\\n\\\\[\\n\\\\bar{W}_{0:t-1} = \\\\text{TextEmbed}(\\\\hat{W}_{0:t-1}) \\\\quad (7)\\n\\\\]\\n\\n\\\\[\\nZ_{0:t} = \\\\bar{W}_{0:t-1} + \\\\text{PositionEnc} \\\\quad (8)\\n\\\\]\\n\\nwhere \\\\( Z_{0:t} \\\\), a \\\\((t-1) \\\\times d_M\\\\) matrix, denotes the \\\\( t \\\\)-th input into the first decoder block. The \\\\( k \\\\)-th Transformer decoder block in the \\\\( N_D \\\\) stacks (\\\\( 1 \\\\leq k \\\\leq N_D \\\\)) receives an output of the previous block with an attention to the output of the audio encoder and then outputs \\\\( Z_k^t \\\\) except that the \\\\( M_D \\\\)-th decoder block has an additional source-target attention to the output of the source language text encoder.\\n\\n\\\\[\\nZ_k^t = \\\\begin{cases} \\n\\\\text{TfDecAttnBlk}(Z_{k-1}^t, X_{N_A}^A) & \\\\text{if } k \\\\neq M_D \\\\\\\\\\n\\\\text{TfDec2AttnBlk}(Z_{k-1}^t, X_{N_T}^S, X_{N_A}^A) & \\\\text{if } k = M_D \\n\\\\end{cases} \\\\quad (9)\\n\\\\]\\n\\nThe final fully-connected layer with a softmax function autoregressively outputs the most probable word.\\n\\n\\\\[\\np(V) = \\\\text{Softmax}(\\\\text{Linear}(Z_{N_D}^t)) \\\\quad (10)\\n\\\\]\\n\\n\\\\[\\n\\\\hat{w}_t = \\\\arg\\\\max_{v \\\\in V} p(V) \\\\quad (11)\\n\\\\]\\n\\nwhere \\\\( V \\\\) and \\\\( p(V) \\\\) stand for vocabulary and posterior probabilities, respectively.\\n\\n2.2. Training\\nThe model is trained with inputs of audio and source language text and outputs of correct transcription in an end-to-end manner. The training is conducted in teacher forcing, which always provides a sequence of correct tokens as inputs into the decoder, minimizing a cross entropy error expressed as follows:\\n\\n\\\\[\\nL = -\\\\sum_{t=1}^{T} \\\\log p(w_t|w_0:t-1, S, A) \\\\quad (12)\\n\\\\]\\n\\n3. Experiments\\nWe evaluated our model in two steps. First, we investigated which blocks in the decoder and the audio encoder should attend to the source language text encoder with a development set. Then, we compared the performance of our model with a baseline model that had no source language text encoder. We measured performance based on the word error rate (WER).\"}"}
{"id": "taniguchi22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Word error rate w.r.t. decoder block MD with attention to source language text encoder for MuST-C En-Nl development set.\\n\\n| M  | A  | WER [%] |\\n|----|----|---------|\\n| 1) | 1  | 12.7    |\\n| 2) | 1  | 12.2    |\\n| 3) | 1  | 12.4    |\\n| 4) | 1  | 11.4    |\\n| 5) | 1  | 11.4    |\\n| 6) | 1  | 11.9    |\\n\\nTable 2: Word error rate w.r.t. audio encoder block MA with attention to source language text encoder for MuST-C En-Nl development set.\\n\\n| M  | A  | WER [%] |\\n|----|----|---------|\\n| 1) | 4  | 12.4    |\\n| 2) | 4  | 12.7    |\\n| 3) | 4  | 11.7    |\\n| 4) | 4  | 11.5    |\\n| 5) | 4  | 11.4    |\\n| 6) | 4  | 12.1    |\\n\\nTable 3: Word error rate w.r.t. increased or decreased numbers of blocks with attention to source language text encoder for MuST-C En-Nl development set.\\n\\n| M  | A  | WER [%] |\\n|----|----|---------|\\n| 1) | -  | 12.3    |\\n| 2) | 4  | 11.8    |\\n| 3) | 4  | 11.4    |\\n| 4) | 12| 4  | 12.2    |\\n| 5) | 4  | 11.4    |\\n| 6) | 12| 1:6    | 11.7    |\\n\\nTable 4: Total time (upper) and numbers of sentences (lower) of train, development, and test sets of four datasets.\\n\\n| Dataset | Train (h) | Development (h) | Test (h) | Train (k) | Development (k) | Test (k) |\\n|---------|-----------|-----------------|---------|-----------|-----------------|---------|\\n| MuST-C  | 434.2     | 2.5             | 4.1     | 248.3     | 1.4             | 2.6     |\\n| TED-EN-JP | 400.0   | 2.5             | 4.1     | 229.7     | 1.4             | 2.6     |\\n| CoVoST 2 | 208.5     | 1.5             | 3.1     | 133.6     | 1.0             | 2.0     |\\n| En-Ja   | 364.0     | 26.0            | 25.0    | 289.4     | 15.5            | 15.5    |\\n\\n3.1. Datasets\\n\\nWe evaluated the models with four ST datasets: two translated languages in MuST-C [13], English speech and its Japanese translation in CoVoST 2 [14], and our original TED-EN-JP dataset. These datasets are composed of original speech, transcription text, and translation text. As combinations of languages in the same language family, we used English speech as audio input and Dutch and German translations as source language text from MuST-C corpus. As a combination of languages in different language families, we used English speech as audio input and Japanese translations as source language text from CoVoST 2 corpus. Furthermore, we created our original TED-EN-JP dataset based on how MuST-C was created [13].\\n\\nWe created our TED-EN-JP dataset as follows.\\n\\nData download\\n\\nWe downloaded 628 hours of TED speech in English, a transcription of the speech comprised of 315k sentences, and their Japanese translation.\\n\\nText alignment\\n\\nWe split paragraphs of the transcription and the translation into sentences, respectively, and aligned the sentences with Vecalign [15]. As the paragraphs were time-stamped for captioning, we paired every transcription paragraph with a translation paragraph which has the closest time stamp. We merged successive translation paragraphs corresponding to a transcription paragraph if necessary, and checked if each paragraph-level pair was consistent with the sentence-level alignment. Inconsistent paragraph pairs were filtered out.\\n\\nAudio to text alignment\\n\\nWe aligned speech data with the transcription paragraphs using the Gentle forced aligner separately from the text alignment, filtering out speech data that had less than 96% word accuracy. Lastly, we extracted a product set of two sets of the text alignment and audio-to-text alignment as the TED-EN-JP dataset. After the purification process, the TED-EN-JP dataset was comprised of 213 hours of English speech and 136k sentences of transcription and translation in Japanese. The amount of extracted data was smaller than other language versions of MuST-C because we applied the additional time-stamp filter. We randomly chose 1,000 sentences as a development set and 2,000 sentences as a test set.\\n\\nThe sizes of the four datasets are listed in Table 4. Note that the development and test sets of MuST-C En-Nl and En-De are the same, while the development and test sets of TED-EN-JP are different from those of MuST-C En-Nl and En-De.\\n\\n3.2. Experimental setup\\n\\nIn the first step, we move a Transformer block that has a source-target attention to the source language text encoder in the decoder and audio encoder, respectively with the development set of MuST-C En-Nl dataset and search for the best combination in terms of a WER. Furthermore, we investigate how the performance changes when the number of Transformer blocks in the decoder and audio encoder increases or decreases. In the second step, we evaluate our model and the baseline model with the four datasets.\\n\\nOur model has six source language text encoder blocks \\\\( N_T = 6 \\\\), twelve audio encoder blocks \\\\( N_A = 12 \\\\) and six decoder blocks \\\\( N_D = 6 \\\\). The baseline model has the same numbers of audio encoder blocks and decoder blocks. We set the dimension \\\\( d_M \\\\) of the embedding vectors in the source language text encoder, audio encoder, and decoder to 256. We set the heads in the multi-head attentions to 4, and set the dimension of the vector in the feed-forward network in each Transformer encoder/decoder block to 2,048. The input acoustic features are 80-dimensional log Mel filterbank outputs sampled at 10 ms intervals and their temporal resolution is reduced to one-fourth by using the 2D-Convolutional network.\\n\\nWe tokenized the transcription and source language text of MuST-C and TED-EN-JP datasets into 5,000 subwords using SentencePiece unigram model. [16]. We tokenized those of the CoV oST 2 dataset into 150 subwords. We trained the models with a batch size of 64 and with early stopping with a patience of five epochs at a maximum of 70 epochs. We trained the models with SpecAugment [17]. We used Espnet2 toolkit [18].\"}"}
{"id": "taniguchi22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6: Example of recognized texts improved by proposed model for each MuST-C En-Nl, MuST-C En-De, and TED-EN-JP dataset. Words corrected by proposed model and corresponding words in source language text are underlined.\\n\\n**A) MuST-C En-Nl**\\n\\nReference text: today there are 23 million\\n\\nSource language text input: vandaag zijn dat er 23 miljoen\\n\\nRecognized text by baseline model: there are 23 billion\\n\\nRecognized text by proposed model: today there are 23 million\\n\\n**B) MuST-C En-De**\\n\\nReference text: my mom was the youngest of her 10 kids\\n\\nSource language text input: meine mutter war das j\u00a8ungste ihrer zehn kinder\\n\\nRecognized text by baseline model: my mom was the young as the pretend kid\\n\\nRecognized text by proposed model: my mom was the youngest of her 10 kids\\n\\n**C) TED-EN-JP**\\n\\nReference text: but i think electricity is preferable for cars and trucks\\n\\nSource language text input: \u3067\u3059\u304c\u79c1\u306f\u8eca\u3084\u30c8\u30e9\u30c3\u30af\u306b\u306f\u96fb\u529b\u304c\u597d\u307e\u3057\u3044\u3068\u8003\\n\\nRecognized text by baseline model: but i think a electricity is profitable for cars and drugs\\n\\nRecognized text by proposed model: but i think a electricity is preferable for cars and trucks\\n\\nFigure 2: Attention weights from decoder to source language text encoder in proposed model for samples listed in Table 6. Horizontal and vertical axes list source language tokens and recognized tokens, respectively. Yellow cells represent higher attention weights close to 1, while navy ones represent lower weights close to 0. Corresponding source language tokens or tokens close to those are attended to for corrected output tokens with proposed model.\\n\\n3.3. Experimental Results\\n\\nWhen moving the audio encoder and the decoder blocks with a source-target attention to the output of the source language text encoder, the best combination we found was \\\\( M_A = 11 \\\\) and \\\\( M_D = 4 \\\\). Tables 1 and 2 show how a WER changes with \\\\( M_A \\\\) fixed at 11 and variable \\\\( M_D \\\\) (Table 1) and with \\\\( M_D \\\\) fixed at 4 and variable \\\\( M_A \\\\) (Table 2), respectively. Table 1 shows that the minimal WER was obtained at \\\\( M_D = 4 \\\\) or \\\\( M_D = 5 \\\\) and that the best performance was not achieved with an attention at an earlier block or the final block. Table 2 shows that the minimal WER was obtained at \\\\( M_A = 11 \\\\) and that the best performance was also not achieved with an attention at an earlier block or the final block. These results suggest that the source language text information is best incorporated at a later block of the audio encoder and the decoder.\\n\\nTable 3 shows how a WER changes with increased or decreased numbers of blocks with an attention to the source language text encoder in the audio encoder and the decoder. Conditions 1) and 2) show that the WER increased without an attention in the decoder or in the audio encoder. Conditions 4), 5), and 6) show that the WER did not further decrease by making all the blocks have an attention to the source language text encoder.\\n\\nTable 5 shows the WERs of the baseline model and our model with \\\\( M_A = 11 \\\\) and \\\\( M_D = 4 \\\\) for the test sets of the four datasets. Our model decreased WERs for all the test sets. The error reduction rates were greater for Dutch and German text input from the same language family than the Japanese text input from a different language family.\\n\\nTable 6 lists examples of recognized texts improved by our model for MuST-C En-Nl, MuST-C En-De, and TED-EN-JP datasets. Our model corrected words misrecognized with the baseline model in accordance with the source language text. Figure 2 illustrates the attention weights from the decoder to the source language text encoder in our model for the examples listed in Table 6. The corresponding source language tokens or tokens close to those were attended to for the corrected output tokens.\\n\\n4. Conclusions\\n\\nWe proposed a Transformer-based end-to-end ASR with an auxiliary input of source language text toward transcribing simultaneous interpretation, and we verified the effectiveness of the auxiliary input by simulating speech and source language text data with corpora for developing end-to-end ST. Our experiments indicated that our model significantly reduced WER compared to the baseline model without the auxiliary input with all four datasets. The results also showed that source language text information should be incorporated into the audio encoder and the decoder in the late blocks, respectively. We will evaluate our model using simultaneous interpretation with source language text in the next step.\"}"}
{"id": "taniguchi22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. References\\n\\n[1] T. Baltrusaitis, C. Ahuja, and L.-P. Morency, \u201cMultimodal machine learning: A survey and taxonomy,\u201d IEEE Trans. Pattern Anal. Mach. Intell., vol. 41, no. 2, pp. 423\u2013443, feb 2019. [Online]. Available: https://doi.org/10.1109/TPAMI.2018.2798607\\n\\n[2] G. Potamianos, C. Neti, G. Gravier, A. Garg, and A. W. Senior, \u201cRecent advances in the automatic recognition of audio-visual speech,\u201d in Proc. IEEE, 2003, pp. 1306\u20131326.\\n\\n[3] J. Ngiam, A. Khosla, M. Kim, J. Nam, H. Lee, and A. Y. Ng, \u201cMultimodal deep learning,\u201d in ICML 2011, 2011, pp. 689\u2013696.\\n\\n[4] H. Wang, F. Gao, Y. Zhao, and L. Wu, \u201cWaveNet with cross-attention for audiovisual speech recognition,\u201d IEEE Access, vol. 8, pp. 169160\u2013169168, 2020.\\n\\n[5] F. Tao and C. Busso, \u201cEnd-to-end audiovisual speech recognition system with multitask learning,\u201d IEEE Transactions on Multimedia, vol. 23, pp. 1\u201311, 2020.\\n\\n[6] J. Lu, D. Batra, D. Parikh, and S. Lee, \u201cViLBERT: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks,\u201d in Advances in Neural Information Processing Systems, vol. 32, 2019.\\n\\n[7] L. H. Li, M. Yatskar, D. Yin, C.-J. Hsieh, and K.-W. Chang, \u201cVisualBERT: A simple and performant baseline for vision and language,\u201d arXiv preprint arXiv:1908.03557, 2019.\\n\\n[8] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d in Advances in Neural Information Processing Systems 30, 2017, pp. 5998\u20136008.\\n\\n[9] R. Hu and A. Singh, \u201cUniT: Multimodal multitask learning with a unified transformer,\u201d in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2021, pp. 1439\u20131449.\\n\\n[10] S. Ma, L. Cui, D. Dai, F. Wei, and X. Sun, \u201cLivebot: Generating live video comments based on visual and textual contexts,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 33, no. 01, 2019, pp. 6810\u20136817.\\n\\n[11] M. R. Makiuchi, K. Uto, and K. Shinoda, \u201cMultimodal emotion recognition with high-level speech and text features,\u201d in 2021 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU). IEEE, 2021.\\n\\n[12] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Advances in Neural Information Processing Systems, vol. 33, 2020, pp. 12449\u201312460.\\n\\n[13] R. Cattoni, M. A. D. Gangi, L. Bentivogli, M. Negri, and M. Turchi, \u201cMuST-C: A multilingual corpus for end-to-end speech translation,\u201d Computer Speech & Language, vol. 66, pp. 1\u201314, 2021.\\n\\n[14] C. Wang, A. Wu, J. Gu, and J. Pino, \u201cCoV oST 2 and massively multilingual speech translation,\u201d in Proc. Interspeech 2021, 2021, pp. 2247\u20132251.\\n\\n[15] B. Thompson and P. Koehn, \u201cVecalign: Improved sentence alignment in linear time and space,\u201d in Proc. EMNLP 2019, 2019, pp. 1342\u20131348.\\n\\n[16] T. Kudo and J. Richardson, \u201cSentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d in Proc. EMNLP 2018, 2018, pp. 66\u201371.\\n\\n[17] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 2613\u20132617.\\n\\n[18] S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y. Unno, N. Enrique Yalta Soplin, J. Heymann, M. Wiesner, N. Chen, A. Renduchintala, and T. Ochiai, \u201cESPnet: End-to-End Speech Processing Toolkit,\u201d in Proc. Interspeech 2018, 2018, pp. 2207\u20132211.\"}"}
