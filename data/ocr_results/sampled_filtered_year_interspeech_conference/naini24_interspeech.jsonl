{"id": "naini24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThere are several applications for speech-emotion recognition (SER) systems in areas such as security and defense and healthcare. SER systems have achieved high performance when they are trained and tested in similar conditions. However, the performance often drops in more realistic and diverse conditions. Most existing SER datasets are too controlled and do not capture complex scenarios relevant to practical applications. This paper presents the White House tapes speech emotion recognition (WHiSER) corpus, which includes distant speech with real emotions from conversations in the Oval Office in 1972. This dataset is unique because it combines natural emotional expressions with various background noises, making it a perfect tool to test and improve SER models. Its real-world complexity and authenticity make the WHiSER corpus an excellent corpus for advancing emotion recognition technology, offering insights into how human emotions can be accurately recognized in complex environments.\\n\\nIndex Terms: emotional dataset, speech emotion recognition, expressive speech.\\n\\n1. Introduction\\n\\nThe development of speech-emotion recognition (SER) systems has opened novel opportunities in diverse domains such as healthcare, customer satisfaction, security and defense, education, and entertainment [1\u20133]. SER systems must accurately detect emotional behaviors regardless of the context, target application, and recording conditions. The effectiveness of these systems relies on the quality and diversity of the datasets used to train the models. It is important to have challenging datasets to test the reliability of SER systems in complex environments that resemble real-world conditions. The existing emotional corpora that capture real human interactions are often recorded using high-quality recording devices with a noiseless environment [4]. However, most of the real-world applications involve different types of background acoustic noise [5, 6]. Therefore, we need more diverse SER datasets that capture the challenges that SER systems are expected to face if deployed in real applications.\\n\\nOver the years, researchers have created many emotional speech datasets. Initial datasets relied mainly on actors reading a predefined set of sentences portraying targeted categorical emotions (happiness, anger, sadness) [7, 8]. However, research indicates that these emotions tend to be too exaggerated and do not represent the expressions observed in everyday human interactions [9]. A more natural approach is to simulate conversations between individuals to create more natural interactions [4, 10]. However, it is difficult, time-demanding, and expensive to collect a large database with this method. Few studies explored speech datasets generated from uncontrolled settings such as TV shows [11, 12], but these sources often resulted in biased emotional content because of the focus of the show (colloquial conversations in family-oriented programs, negative expressions in conflict-based shows). Recently, large emotional speech corpora have been developed by annotating the data available in media-sharing platforms [4,13]. Even though these corpora provide adequate resources to develop strong SER systems, the recordings are still clean. We need emotional datasets recorded under real-time conditions including distance speech, reverberation, and background noise.\\n\\nThis paper introduces the White House tapes speech emotion recognition (WHiSER) corpus, obtained from the Nixon's Tapes recordings [14]. From 1971 to 1973, President Nixon recorded conversations in the Oval Office, which included authentic emotional interactions under varied recording conditions. The recordings were declassified and archived. They include distant speech, reverberation, and low signal-to-noise ratio (SNR), capturing relevant realistic conditions for SER systems. We rely on the pipeline proposed in the affective naturalistic database consortium (AndC) to select and annotate emotional recordings. The corpus comprises 6 hours and 21 minutes of data specifically annotated for emotional attributes (arousal, valence, and dominance), and categorical emotions (anger, sadness, happiness, surprise, fear, disgust, contempt, and neutral). The WHiSER database is intended to be used as an independent test set, in the context of the generalization of SER systems in complex environments. Its use can facilitate advances in unsupervised domain adaptation for SER [15\u201317]. All the data and the emotional labels are released on our GitHub repository. 1https://github.com/msplabresearch/WHiSER\\n\\n2. Related Work\\n\\nBuilding the right infrastructure to develop computational models for emotions is critical. A common approach in early emotional databases is to record actors instructed to read sentences portraying target emotions. Several emotional datasets are developed using this approach, including the Emo-DB [8], RAVDESS [18], TESS [19], CREMA-D [20], and the Chen Bimodal [21] databases. However, Devillers et al. [22] and Batliner et al. [23] highlighted that such acted emotions do not accurately reflect real-life emotional complexity, suggesting that performances in acted scenarios do not match real-world application accuracies. The IEMOCAP [24] and MSP-IMPROV [25] databases were created to feature conversations in dyadic interactions, moving away from the scripted readings of previous datasets. Although these datasets tackled the problem of exaggerated emotions by presenting more natural dialogues, the recordings still involve actors. Other datasets such as VAM [26], SEMAINE [27], and TUM-AVIC [28] were built, sourcing natural interactions from TV shows and call centers. While...\"}"}
{"id": "naini24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"they moved towards capturing genuine interactions, the emotional content in these datasets became significantly skewed, reflecting the specific contexts of these interactions rather than the range of emotional content observed in everyday human interactions. The MSP-Podcast [4] corpus relied on recordings obtained from audio-sharing websites, enriching the emotional context beyond what acted datasets offer. Podcasts are often recorded with close-talking microphones, providing clean audio. There is a pressing need for emotional datasets that capture naturalistic recordings resembling the conditions expected to exist if SER systems are deployed in real applications. This observation is the motivation for the WHiSER dataset, which provides a valuable resource to comprehensively assess and enhance the capabilities of SER systems.\\n\\n3. The WHiSER Corpus\\n\\nThe WHiSER dataset consists of speech files obtained from the declassified President Nixon\u2019s Oval Office recorded conversations from 1971 to 1973, referred to as Nixon\u2019s tapes [14]. These recordings cover significant events and meetings in key locations such as the Oval Office, Cabinet Room, the President\u2019s Old Executive Office Building, and Aspen Lodge at Camp David. It also includes recordings of the White House telephone calls. The recorded conversations in the dataset provide emotionally rich natural interactions using distant microphones, which makes it an ideal resource for emotional recognition tasks. We follow the pipeline suggested in the affective naturalistic database consortium (AndC) to select and annotate emotional recordings. This section details the process.\\n\\n3.1. Pipeline and Data Annotation\\n\\nThe data collection pipeline\u2019s preprocessing stage is divided into two main phases: the audio preparation phase and the filtering phase. In the audio preparation phase, the raw audio collections are transformed into a consistent format using the Librosa toolbox [29] (16kHz, 16-bit, and single-channel). These recordings are then processed by a voice activity detector (VAD) to identify speech segments. In the filtering phase, the recordings undergo several filtering steps to enhance speech quality. This step includes automatic algorithms to determine the presence of music, using the approach proposed in Lee et al. [30], and noise, using the noise estimator proposed by Nicolson and Palival [31]. We considered 5dB SNR as a lower threshold for removing samples that are too noisy. Only the utterances that satisfy all predefined criteria are considered in the subsequent stages. In total, this set includes 121,482 sentences.\\n\\nIn natural conversations, most of the sentences are emotionally neutral. Therefore, the AndC pipeline involves automatically retrieving samples with machine-learning tools that are likely to convey emotional reactions. We collected emotionally rich samples from the full set of speech files in two stages. Initially, we employed four speech-emotion recognition models based on preference learning on emotional attributes. These models were initialized with the wav2vec2 large model [32] from the HuggingFace [33] library and were fine-tuned with the MSP-Podcast dataset, as detailed in [17]. From these chosen samples, we manually eliminated any unintelligible samples, regardless of their emotional content, ending up with 1,427 speech files to be annotated with emotional labels. In the second stage, we utilized 12 speech-emotion recognition models designed to predict attribute scores, which helped us select emotionally rich samples. These models were based on the state-of-the-art self-supervised learning (SSL) models (wavLM [34], wav2vec2 [32], HuBERT [35], Data2vec [36]), fine-tuned and trained with the MSP-Podcast dataset, as explained in [37]. We also removed any unintelligible or duplicate samples from the first stage, resulting in 4,000 speech files to be annotated with emotional labels. The WHiSER corpus incorporates all samples from both stages, totaling 5,427 speech files.\\n\\nThe selected utterances are annotated with emotional labels using a perceptual evaluation. During this phase, annotators label the utterances with the emotional dimensions of arousal (calm versus active), valence (negative versus positive), and dominance (weak versus strong), using a seven-point Likert scale. We provide self-assessment manikins (SAMs) [38] as visual references to help annotators accurately assess these dimensional attributes. The annotations include primary categorical emotion, which is the class that most accurately represents the emotional content of the utterance, choosing from a list of eight primary emotions: anger (A), sadness (S), happiness (H), surprise (U), fear (F), disgust (D), contempt (C), and neutral (N). The annotators also provide secondary emotions, including all the emotional classes perceived in the audio. In addition to the eight options, the list for the secondary emotions includes amused, frustrated, depressed, concerned, disappointed, excited, confused, and annoyed. The corpus was annotated by 33 student workers at anonymous. Each speaking turn was annotated by at least five annotators. We use the plurality rule to obtain consensus for primary emotions. For each emotional attribute, we estimate the average scores assigned to each sentence by the annotators, using this value as the consensus score.\\n\\n3.2. Size of the WHiSER corpus\\n\\nThe WHiSER corpus consists of 5,427 speech files with a total duration of 6 hours 21 minutes. The duration of speech samples within a dataset is a critical factor for the extraction of emotional content and subsequent labeling. Our dataset was developed to include speech files ranging from 3 to 11 seconds, as shown in Figure 3(b). This range was chosen based on previous studies indicating the challenges in perceiving the emotional content from very short audio clips [39]. Likewise, attributing a single emotional label to a long file leads to label noise as emotion can fluctuate within the sentence. We split longer speaking turns into smaller parts to fall within our target duration. Figure 3(b) reveals that the majority of samples are concentrated within the 3 to 6-second window.\\n\\n3.3. Emotional Content Analysis\\n\\nThe sentences in the corpus were selected based on their predicted content in terms of emotional attributes. Figure 1(a), 1(b), and 1(c) show the distribution of emotional attribute scores for arousal, valence, and dominance, respectively. For each attribute, we observe a Gaussian distribution centered around four, which is the central point in the seven-point Likert scale. This distribution captures the expected emotional expressions displayed in daily human interactions, where extreme expressions are less common. The distribution presents a significant number of samples around the scores of three and five. Sridhar and Busso [40] observed a higher level of uncertainty in\"}"}
{"id": "naini24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 1: Emotional distribution across different classes.\\n\\n| Emotion   | Percentage |\\n|-----------|------------|\\n| Happiness | 22.7%      |\\n| Sadness   | 12.5%      |\\n| Anger     | 13.7%      |\\n| Neutral   | 64.3%      |\\n\\nThe emotional distribution within a dataset can significantly affect its efficacy as a training or testing resource. Figure 2(a) shows the distribution of primary emotions across the dataset. The dataset showcases a substantial portion of 'Neutral' emotional data, with 64.3% instances. This neutrality mirrors the authentic emotional representation of day-to-day conversations, which often skew towards a non-extreme emotional display.\\n\\nFigure 2(b) plots the placement of the sentences assigned to each emotional class in the arousal-valence space. This figure illustrates the variability of the emotional content included within the same classes. We notice that even within the class 'Neutral,' we observe a range of emotional expressions perceived in the speech. Figure 2(a) shows that the dataset exhibits an almost even distribution among 'Happiness,' 'Sadness,' and 'Anger.' This balanced representation of major emotions makes the dataset an effective tool for testing SER models and provides the ability to distinguish between different emotional states without bias. The frequencies of these categorical emotions validate the dataset's potential to serve as a balanced benchmark for SER systems, ensuring they are well-versed in detecting and interpreting a spectrum of emotions that are commonplace in real-world interactions.\"}"}
{"id": "naini24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Concordance correlation coefficient (CCC) for arousal, valence, and dominance. The symbol *\u2020 indicates that the result is a significant improvement over the result with the symbol *, which is a significant improvement over the results without any symbol.\\n\\n|                | CCC LR | CCC ADA | CCC DAL |\\n|----------------|--------|---------|---------|\\n| **Arousal**    |        |         |         |\\n| wavLM          | 0.299  | 0.396   | 0.391   |\\n| wav2vec2       | 0.301  | 0.391   | 0.384   |\\n| HuBERT         | 0.292  | 0.385   | 0.377   |\\n| **Valence**    |        |         |         |\\n| wavLM          | 0.392  | 0.441   | 0.446   |\\n| wav2vec2       | 0.395  | 0.457   | 0.461   |\\n| HuBERT         | 0.383  | 0.450   | 0.453   |\\n| **Dominance**  |        |         |         |\\n| wavLM          | 0.338  | 0.369   | 0.379   |\\n| wav2vec2       | 0.326  | 0.371   | 0.376   |\\n| HuBERT         | 0.317  | 0.365   | 0.361   |\\n\\nTable 2: Comparison of within and cross corpus performance for 4 primary emotions classification. The table reports the average F1 (Macro), UAR (%) values. (\u2217 indicates that the model is significantly better than the other three methods)\\n\\n|                | F1 Within | UAR Within | F1 UAR | F1 Within | UAR Within | F1 UAR | F1 Within | UAR Within | F1 UAR |\\n|----------------|-----------|-------------|--------|-----------|-------------|--------|-----------|-------------|--------|\\n| wavLM          | 0.613     | 63.9        | 0.621  | 64.2      | 0.627       | 65.3   | 0.676     | 69.2        | 0.692  |\\n| wav2vec2       | 0.592     | 61.7        | 0.601  | 61.9      | 0.609       | 62.6   | 0.652     | 66.5        | 0.672  |\\n| HuBERT         | 0.608     | 62.8        | 0.604  | 62.7      | 0.617       | 63.4   | 0.661     | 67.4        | 0.684  |\\n\\nAs adversarial domain adaptation (ADA). To understand the best performance that could be achieved on the WHiSER corpus, we considered within-corpus settings as the fourth method. For the within-corpus settings, we considered a five-fold cross-validation of the corpus, to train the LR models directly on the WHiSER corpus using a downstream head. In each fold, we use data from four partitions as a train set and one as a test set. For wav2vec2, we fine-tuned the LR model by removing the top 12 transformer layers, which retains performance with fewer parameters [42]. The SER architecture for both fine-tuning and training steps involves two layers with 1,024 nodes each, layer normalization, and ReLU activation. For emotional attributes, the average concordance correlation coefficient (CCC) from the three attributes (arousal, valence, and dominance) served as the loss function while fine-tuning the models. For training the SER model, we use three different models, one for each attribute, where the weights are individually set to optimize the performance of the target attribute. For emotional classes, we used the cross-entropy loss for fine-tuning and training stages (a four-class problem). ADA employs a 128-node layer for task and domain classifiers with ReLU activation, while DAL includes two 256-node layers with ReLU and linear activations for the task classifier. We utilize an EC2 g5.4xlarge instance for fine-tuning the models with an NVIDIA A10G GPU with an Adam optimizer at a learning rate of 10e-5. For all other tests, we use an NVIDIA GeForce RTX 3090 GPU.\\n\\nTable 1 shows the results for the emotional attributes, reporting performance in terms of CCC. We evaluate if the results are statistically significant by dividing the WHiSER corpus data into 20 subsets of similar size. Then, we conducted a two-tailed t-test over the 20 subsets. We defined statistical significance at a p-value < 0.05. When we compare the LR results with the ADA and DAL approaches, we observe significant improvements by using UDA strategies. However, the table shows that the within-corpus setting achieves significantly better results than all other cases. There are opportunities to explore better UDA strategies to bridge this gap.\\n\\nTable 2 shows the classification performance for categorical emotions. We reported the results using F1-score (macro) and unweighted average recall (UAR) metrics. Similar to the experiments with emotional attributes, we observed a significantly better result in the within-corpus setting compared to all other cases. In most cases, UDA strategies produced significantly better performance compared to the LR model. Overall, among the three SSL methods considered, wavLM produced better results in most cases with few exceptions in both experiments.\\n\\n5. Conclusions\\n\\nThe paper presented the White House tapes speech emotion recognition (WHiSER) corpus consisting of authentic and diverse emotional content from President Nixon's Oval Office recordings. The corpus closely aligns with real-world challenges faced by SER systems when deployed in practical applications. The WHiSER corpus, with its natural emotional expressions and varied background noises, produces a challenging testing scenario for SER models. Through the detailed analysis of the dataset's emotional content and the evaluation of SER models using this corpus, we have underscored the potential of the WHiSER corpus to serve as an invaluable asset for both testing and improving emotion recognition systems. By incorporating scenarios that feature distant speech, reverberation, and low signal-to-noise ratios, the WHiSER corpus bridges the gap between controlled laboratory conditions and the nuanced, unpredictable nature of real-life settings. Future work will aim to explore innovative approaches to harness the full potential of the WHiSER corpus in advancing deployable SER technology.\\n\\n6. References\\n\\n[1] C. Busso, M. Bulut, and S. Narayanan, \u201cToward effective automatic recognition systems of emotion in speech,\u201d in Social emotions in nature and artifact: emotions in human and human-computer interaction, J. Gratch and S. Marsella, Eds. New York, NY , USA: Oxford University Press, Nov. 2013, pp. 110\u2013127.\\n[2] J. Acosta, \u201cUsing emotion to gain rapport in a spoken dialog system,\u201d Ph.D. dissertation, University of Texas at El Paso, El Paso, TX, USA, December 2009.\\n[3] C.-C. Lee, K. Sridhar, J.-L. Li, W.-C. Lin, B.-H. Su, and C. Busso, \u201cDeep representation learning for affective speech signal analysis and processing: Preventing unwanted signal disparities,\u201d IEEE Signal Processing Magazine, vol. 38, no. 6, pp. 22\u201338, Nov. 2021.\\n[4] R. Lotfian and C. Busso, \u201cBuilding naturalistic emotionally balanced speech corpus by retrieving emotional speech from existing podcast recordings,\u201d IEEE Transactions on Affective Computing, vol. 10, no. 4, pp. 471\u2013483, October-December 2019.\\n[5] Z. Zhang, F. Ringeval, J. Han, J. Deng, E. Marchi, and B. Schuller, \u201cFacing realism in spontaneous emotion recognition from speech: Feature enhancement by autoencoder with LSTM neural networks,\u201d in Interspeech 2016, San Francisco, CA, USA, September 2016, pp. 3593\u20133597.\\n[6] C. Huang, , G. Chen, H. Yu, Y. Bao, and L. Zhao, \u201cSpeech emotion recognition under white noise,\u201d Archives of Acoustics, vol. 38, no. 4, pp. 457\u2013463, 2013.\\n[7] M. Liberman, K. Davis, M. Grossman, N. Martey, and J. Bell, \u201cEmotional prosody speech and transcripts,\u201d Philadelphia, PA, USA, 2002, Linguistic Data Consortium.\"}"}
{"id": "naini24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
