{"id": "tapo24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Leveraging Speech Data Diversity to Document Indigenous Heritage and Culture\\n\\nAllahsera Tapo 1, \u00b4Eric Le Ferrand 2, Zoey Liu 3, Christopher Homan 1, Emily Prud\u2019hommeaux 2\\n\\n1 Rochester Institute of Technology, USA\\n2 Boston College, USA\\n3 University of Florida, USA\\n\\n{aat3261,cmhvcs}@rit.edu, {leferran,prudhome}@bc.edu, liu.ying@ufl.edu\\n\\nAbstract\\n\\nThe majority of the world's 7,000 languages lack a standardized writing system. In this paper we consider one such language, Bambara, which is rarely written down but is widely spoken in Mali and neighboring countries. We explore the task of using automatic speech recognition (ASR) to transcribe culturally significant recordings focused on two domains: archival linguistic and anthropological fieldwork and contemporary oral histories performed by griots, the traditional Mande history keepers. We describe our two 6.5-hour corpora then experiment with different data configurations and multi-stage tuning from pretrained multilingual models within two neural ASR architectures. We find that while the diversity in content, style, and recording quality across the two corpora presents challenges, their commonalities can sometimes be leveraged to improve ASR accuracy. We note, however, that the diverse qualities of these corpora diminish their utility for cross-domain ASR training.\\n\\nIndex Terms: linguistic diversity, cultural heritage, under-resourced languages, low-resource speech recognition\\n\\n1. Introduction\\n\\nIn the past decade, the development of deep neural architectures has yielded tremendous improvements in performance for a wide range of NLP tasks. What is rarely noted in the papers reporting these results is that these advances are possible for only the tiny percentage of the world's 7,000 languages that have sufficient resources to train such large models. While it would be relatively straightforward \u2013 though laborious and expensive \u2013 to create training resources for some of these languages, at least half of the world's languages lack an established writing system [1]. The lack of written resources for these languages makes it nearly impossible to train robust NLP models of any kind. It's easy to assume that this is a problem only for endangered languages (e.g., languages indigenous to the Americas) or for non-standard \\\"dialects\\\" of languages with long histories of writing (e.g., Arabic of the Maghreb region). In fact, many of these languages are spoken by millions or tens of millions of people [2, 3].\\n\\nAs a result, much of the important cultural and historical data from the past century held by these communities is stored only in audio format. Thoroughly documenting and preserving the heritage of these communities entails producing written versions of these recordings. Manually generating transcriptions, however, can be challenging for speakers who, while typically fully literate in a government-supported high-resource language, do not have experience with or access to an agreed-upon writing system for their own language. Automatic speech recognition (ASR) has the potential to help facilitate the transcription process and to support the preservation of these valuable elements of these communities' heritage.\\n\\nIn this paper, we consider one such language, Bambara, a language spoken by 16 million people in Mali and neighboring countries that lacks a widely-accepted writing system. We focus on using ASR to transcribe two types of recordings that are common in these types of speech communities, that hold cultural significance, but that are quite different from each other: archival linguistic and anthropological fieldwork interviews from the mid-twentieth century and performances by professional community storytellers, in this case griots, the oral historians of the Mande people. We attempt to answer three questions: (1) what accuracy can we expect from \\\"off the shelf\\\" state-of-the-art architectures on data collected in these unusual speech domains? (2) is one domain more challenging than the other for these architectures? and (3) can we leverage data from one domain to improve accuracy in the other domain?\\n\\nUsing two different state-of-the-art neural ASR architectures that support fine-tuning from multilingual models \u2013 Wav2Vec2 XLSR [4, 5] and Whisper [6] \u2013 we compare ASR accuracy for each type of data when training on different combinations of the two corpora. We find that while including data from both corpora during training, whether jointly or in a multi-stage fine-tuning scenario, improves accuracy within Wav2Vec2 XLSR, including out-of-domain data during training degrades performance substantially within Whisper.\\n\\nThese results have implications for researchers using ASR to document indigenous cultural artifacts and for ASR development...\"}"}
{"id": "tapo24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ers working in very low-resource settings. First, though it might\\nseem intuitive that adding acoustic training data of any kind in\\nthe target language (in our case, enough to double the size of\\nthe corpus) will improve ASR accuracy, the impact of including\\nadditional data from a different domain within that language ap-\\npears to be unpredictable. Researchers using ASR to generate\\nwritten documentation of primarily oral artifacts should care-\\nfully consider the similarity of their target data to existing avail-\\nable training data. Second, our findings point to weaknesses in\\nboth Wav2Vec2 XLSR and Whisper in low-resource settings.\\nWhile Whisper outperforms Wav2Vec2 XLSR for both our cor-\\npora, Whisper appears to be much more fragile in the face of\\nout-of-domain data.\\n\\nTo support research on using ASR for heritage preservation,\\nwe will make our corpora available to researchers who have\\ncompleted their institution's human subjects protection train-\\ning. This unique combination of different domains \u2013 distinct\\nbut both culturally valuable \u2013 will be a valuable resource to\\ndevelopers of ASR technologies for supporting language doc-\\numentation in settings where access to new data is limited.\\n\\n2. Prior Work\\nLinguists have long recognized the potential of ASR to break\\nthrough the \\\"transcription bottleneck\\\" [7, 8], or the barrier pre-\\nsented by the lack of time and expertise required to transcribe\\nrecordings collected as fieldwork or as documentation of impor-\\ntant cultural practices. Only recently, however, have technolog-\\nical hurdles and error rates decreased to the point where ASR\\nis a viable option for language documentation. Several research\\ngroups have investigated a range of approaches to incorporat-\\ning ASR into the endangered language documentation pipeline\\n[9, 10, 11, 12, 13]. Meanwhile, there has been interest in build-\\nning ASR corpora for more widely spoken but under-resourced\\nlanguages like Bambara. A number of ASR corpora compara-\\ntible in size to ours have been collected for languages with similar\\nprofiles \u2013 widely spoken but not always taught in schools \u2013 but\\nin nearly all cases, they consist read speech recorded in a studio\\nor professional news broadcasts [14, 15, 16, 17] rather than the\\nmore challenging domains of fieldwork or performance, making\\nthem quite different from our corpora.\\n\\nAlthough there is linguistic research on the Bambara lan-\\nguage [18, 19] and a small amount of work on machine transla-\\ntion [20, 21], prior work on ASR for Bambara is limited. Van\\nvan der Westhuizen et al. (2021) [22] explored the utility of\\nincluding acoustic data from several other under-resourced lan-\\nguages, both related and unrelated, to improve ASR accuracy\\nfor Bambara. The authors used a professionally transcribed 8-\\nhour corpus of Bambara radio broadcasts, and they trained mod-\\nels using a hybrid DNN within the Kaldi ASR toolkit [23]. They\\nfound that including additional speech data for any one or any\\ncombination of the several other available languages yielded\\nimprovements over monolingual training, reducing WER from\\naround 43.5% to as low as 37.8%. Our work is quite distinct\\nin that we are fine-tuning massive multilingual models within\\nfully neural architectures and that we are exploring a monolin-\\ngual setting consisting of speech data from the more challenging\\ndomains of spontaneous performances and interviews. We are\\nunable to compare our own approach to this prior work since\\nwe do not have access to the dataset. Nevertheless these results,\\nin which adding data from other languages improves WER, are\\ninteresting in light of our own results which sometimes show\\npatterns that conflict with this prior work.\\n\\n3. Data\\nAround 16 million people speak Bambara, mostly in Mali,\\nwith 5 million first-language speakers. It is a member of\\nthe Mande family of languages, which also includes Maninka,\\nJula, Mandinka, and several other languages. Together, Mande\\nlanguages are spoken by nearly 30 million people in western\\nAfrica. Although writing systems have been proposed over the\\nyears, Bambara is chiefly a spoken language. It is not currently\\nan official language of government or education in Mali, al-\\nthough a new constitution is expected to make all Malian lan-\\nguages official languages. Many, if not most, speakers of Bam-\\nbara do not read or write the language, relying instead on French\\n\u2013 currently the official language of government and education\\nin Mali \u2013 for written communication. Today, when people do\\nwrite Bambara, they typically use the Latin alphabet with a few\\nadditional characters borrowed from the International Phonetic\\nAlphabet, allowing for a fairly reliable one-to-one correspon-\\ndence between sound and symbol. Less frequently, Bambara is\\nwritten in N\u2019ko, an indigenous writing system developed in the\\n1940s for the Mande languages [24], or in the Arabic script.\\n\\nUnlike most prior work on ASR for African languages [15,\\n16, 17] we focus on culturally significant recordings rather than\\nread speech or radio broadcasts. We use two distinct corpora\\ncollected in different settings decades apart to explore the utility\\nof ASR for cultural documentation. Table 1 provides summary\\ninformation about each corpus. All data was collected and/or\\ntranscribed with the approval of the participating institutions'\\nIRB or local equivalent.\\n\\n3.1. Fieldwork\\nThe fieldwork corpus consists of recordings of interviews with\\n8 Bambara speakers captured by linguists, journalists, and an-\\nthropologists in the 1970s through the 1990s. These record-\\nings, which touch upon both historical and contemporary issues\\nin Mali and West Africa, were transcribed in stages over the\\nlast ten years by several Bambara-speaking linguists at the Cen-\\ntre national de la recherche scientifique (CNRS). Given the age\\nof the recordings, little information is available about the envi-\\nronment in which the recordings were made or the equipment\\nused to make the recordings. The speech files were originally\\nrecorded on magnetic tape and digitized within the last decade.\\n\\nCode-switching between French and Bambara is quite com-\\nmon. Utterances rendered completely in French were removed\\nfrom the corpus. Utterances combining French and Bambara\\nwere retained. The transcription system used for this data in-\\ncluded tone markings on vowels, where applicable. Since tone\\nwas not rendered in the griots corpus transcriptions, diacritics\\nindicating tone were removed from the fieldwork transcripts.\\n\\n3.2. Griots\\nThe second corpus consists of recordings of performances by\\ngriots, the traditional history keepers and storytellers of the\\nMande people. In most communities in Mali and in much of\\n\\nTable 1: Characteristics of the two corpora.\\n\\n|                  | Griots Fieldwork | French Fieldwork |\\n|------------------|------------------|------------------|\\n| audio (HH:MM)    | 06:36 00:43      | 06:26 00:44      |\\n| utterance count  | 9,847 1,095      | 8,942 994        |\\n| token count      | 64,958 5,214     | 67,262 3,860     |\\n| type count       | 28,101 3,185     | 28,684 2,407     |\"}"}
{"id": "tapo24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"West Africa, each ethnic group has a hierarchy of social classes, which includes, among others, nobility and griots. Each noble family is associated with a specific griot family who preserves their history, culture, and traditions across generations. Although traditional community structures have changed, the role of the griots is still relevant, and their practice of guarding and sharing the culture of their communities remains vibrant.\\n\\nThirty griots were recruited to participate in our data collection on location in Mali. Audio and video recordings of the 1-hour performances, which took place indoors and outdoors in front of an audience of around 30 Bambara speakers, were made by a professional audio technician and a videographer. Five Malian doctoral candidates in linguistics transcribed 10 hours from the full 30-hour corpus. Two non-adjacent 10-minute segments of each 1-hour recording were selected for transcription. The transcribers were trained to use ELAN for transcription of speech and in our internally defined guidelines for how to render Bambara using the Latin alphabet with supplemental characters from IPA. Each transcriber transcribed roughly 2 hours of speech. In most cases, the transcribers did not have extensive prior experience writing or reading Bambara. We note that some transcribers produced more accurate transcripts with more reliable timestamps than others. While we do hope to harmonize the quality of these transcripts in future work, we consider this variability in transcription quality to be a feature rather than a bug. Non-professional transcription is common in community-driven data collection efforts; our corpus is therefore a realistic representative of the kind of data that is likely to be collected under similar circumstances.\\n\\n### 4. Method\\n\\nOur research goals are to (1) calculate the accuracy of two state-of-the-art architectures \u201coff the shelf\u201d for the two different corpora; (2) to determine whether one domain (fieldwork vs. griot performance) is more challenging than the other; (3) to leverage data from one domain to improve accuracy in the other domain.\\n\\nFor each corpus, 10% of the utterances were randomly selected to serve as a test set, with the remaining 90% serving as training data (see Table 1). We explore the two training sets alone and in various combinations within the two architectures of interest. As noted earlier, we use two state-of-the-art neural ASR architectures that permit fine-tuning from a large multilingual model: Wav2vec2 XLSR [4, 5], with and without an n-gram language model for decoding, and Whisper [6].\\n\\nIn Wav2vec2 XLSR, latent speech representations are learned in an unsupervised manner from the raw audio of a wide range of languages using a convolutional feature encoder. Training is carried out via a transformer architecture that, like BERT [26], solves a contrastive task to predict masked outputs, in this case from the feature encoder. The resulting model is then fine-tuned to a particular language in a supervised fashion using labeled data for that language. For our wav2vec training, we used Wav2Vec2 XLSR-53 with the hyperparameters specified in the Hugging Face Wav2Vec2 XLSR-53 tutorial.\\\\(^1\\\\)\\n\\nThis multilingual model was trained on 56,000 hours of speech from 53 languages. No languages from the Mande family or its parent family, Niger-Congo, are among the 53 languages used.\\n\\nTo decode with a language model, we built a trigram LM with modified Kneser-Ney smoothing from the transcripts of the audio data using the KenLM toolkit [27]. The CTC alpha and beta LM parameters were left at their default values of 0.5 and 1.5.\\n\\n1. https://huggingface.co/blog/fine-tune-xlsr-wav2vec2\\n\\nWhisper takes a different approach, using supervised, rather than unsupervised, pre-training over log-Mel spectrogram, rather than raw audio, representations, within an established encoder-decoder Transformer architecture. The multilingual model we use, Whisper Medium, was trained with multiple objectives: voice activity detection, ASR, and speech translation. Whisper Medium was trained on 680,000 hours of labeled data, including 117,000 hours from 96 languages that are not English. No Mande or Niger-Congo languages were included in training. When training Whisper models, we fine-tune from this medium multilingual acoustic model using the hyperparameters specified in the Hugging Face Whisper tutorial.\\\\(^2\\\\)\\n\\nRecall that both corpora were segmented by utterance. Rather than padding each utterance with silence to arrive at the required 30-second segments for training within Whisper, we concatenated utterances to create training samples ranging between 20 and 30 seconds. Note that Whisper decoding is auto-regressive and thus no language model is used during decoding.\\n\\nWe train models using the following training data conditions within each architecture, and we test each of the two test sets with each relevant data condition. We first train models on each of the two corpora individually and on the two corpora together, and test these models on both test sets. We then carry out a two-stage fine-tuning process in which the first round of fine-tuning includes both corpora, while the second stage of fine-tuning includes only data from the target domain of the test data. Table 2 provides details about the various configurations.\\n\\nAll experiments were run on a single node with an Intel(R) Xeon(R) Gold 6150 CPU @ 2.70GHz with 36 CPUs. Depending on the computational resources required, we used one or two A100 with 40 GB of memory, using two A100s whenever we trained a model on both corpora combined.\\n\\n### 5. Results\\n\\nThe output of each model on the test set for each corpus is evaluated with the standard ASR metrics of word error rate (WER) and character error rate (CER). Lower values represent improvements in accuracy. Results are presented in Table 3. Whisper outperforms Wav2Vec2 XLSR by a wide margin when trained on exclusively in-domain data, reducing WER by 36.55% on average for the griots data, and by 14.73% for the fieldwork data. Unsurprisingly, for both architectures, training exclusively on in-domain data yields lower error rates than training exclusively on out-of-domain data. In-domain training leads to an average 46.11% WER reduction (48.32% for CER) over out-of-domain training for the griots data, and a 59.85% WER reduction (64.52% for CER) on average for fieldwork. Interestingly, Whisper performance degrades much more when trained on out-of-domain data compared to Wav2Vec2 XLSR.\\n\\n2. https://huggingface.co/blog/fine-tune-whisper\"}"}
{"id": "tapo24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Word error rate (WER) and character error rate (CER) for each corpus test set under the various architecture and training data combinations. A dash in a two-stage fine-tuning cell indicates that the test data was not tested with that particular setting given the low likelihood of this situation in real-world settings.\\n\\n| ASR Architecture | Training data configuration | Griots test data | Fieldwork test data |\\n|-----------------|-----------------------------|-----------------|---------------------|\\n|                 | WER CER                     | WER CER         |                     |\\n| wav2vec         | GRIOTS                      | 0.442 0.175     | 0.789 0.472         |\\n|                 | FIELDWORK                   | 0.687 0.298     | 0.542 0.289         |\\n|                 | BOTH                        | 0.437 0.174     | 0.523 0.28          |\\n|                 | 2STAGE: GRIOTS              | 0.416 0.166     | -                   |\\n|                 | 2STAGE: FIELDWORK           | -                | 0.522 0.28          |\\n|                 | w/ LM                       | WER CER         |                     |\\n|                 | GRIOTS                      | 0.409 0.169     | 0.754 0.466         |\\n|                 | FIELDWORK                   | 0.654 0.292     | 0.497 0.276         |\\n|                 | BOTH                        | 0.406 0.168     | 0.489 0.272         |\\n|                 | 2STAGE: GRIOTS              | 0.389 0.16      | -                   |\\n|                 | 2STAGE: FIELDWORK           | -                | 0.485 0.271         |\\n| Whisper         | GRIOTS                      | 0.27 0.149      | 0.826 0.523         |\\n|                 | FIELDWORK                   | 0.739 0.364     | 0.443 0.323         |\\n|                 | BOTH                        | 0.404 0.173     | 0.553 0.34          |\\n|                 | 2STAGE: GRIOTS              | 0.385 0.171     | -                   |\\n|                 | 2STAGE: FIELDWORK           | -                | 0.488 0.294         |\\n\\nMore research is needed in this area. The results suggest that while Whisper performs very well when trained on exclusively in-domain data, it is very sensitive to domain differences. When even a small portion of the training data is from a different domain, dramatic degradations in performance emerge, suggesting that Whisper may be overfitting to the domain of the test data. We leave further investigations of what acoustic or linguistic factors give rise to these performance discrepancies for future work.\\n\\nWe note that even though the two corpora are similar in size, their respective results given the same training settings are not directly comparable, given that their data content is different. That said, we offer some qualitative observations. Although the number of speakers is smaller for the fieldwork data, which would lead to less speaker variation, WER results are substantially higher under all training conditions for the fieldwork data. We expect that this is due to the quality of this audio relative to the professionally recorded griot data. The fieldwork data was recorded in the field decades ago, potentially with poorer equipment and in unpredictable settings; data loss may also have occurred during the digitization process.\\n\\n### Discussion & Future Work\\n\\nThis study investigates ASR for two small Bambara corpora from diverse domains that are representative of the kind of data typically available for orally transmitted languages. Our results demonstrate that modern ASR architectures can yield error rates well within the range of utility for language documentation [10] for both archival and contemporary culturally significant recordings. This shows promise for leveraging spoken language technology to support the preservation of memory and heritage. We see, however, that training with additional data from an out-of-domain corpus does not reliably yield accuracy improvements. We cannot currently say whether the lack of an expected boost in accuracy is due to differences in the quality of the audio, the content of the audio, or perhaps changes in pronunciation or vocabulary over time.\\n\\nWhile Whisper yields stronger results with in-domain training, it is very sensitive to changes in domain, even in the two-stage fine-tuning scenario. This is quite surprising given that the two domains, while diverse, involve the same language with the same writing conventions. Whisper users may gain no benefit from including even large amounts of data if the domain of the additional data diverges substantially from the target domain. These results raise some questions about Whisper, which was shown to be significantly more accurate than wav2vec on a large number of popular English benchmarks [6]. The superiority of Whisper is not consistently observed in other \u201chead to head\u201d comparisons. English child speech, for instance, appears to be more accurately recognized with wav2vec [28]. Prior work on under-resourced languages also shows that Whisper is frequently outperformed by Wav2Vec2 XLSR and even a hybrid statistical-neural model [29, 30]. Could these differences be due to problems with domain mismatch as well?\\n\\nIn our future work, we plan to investigate other types of data augmentation, particularly methods that enhance or degrade audio quality, in order to improve cross-domain performance. In addition, since there are still several hours of untranscribed recordings for both corpora, we will explore how the unlabeled data can be leveraged in designing alternative training configurations for ASR. Finally, we hope to probe the issue of domain-related performance weaknesses in Whisper more thoroughly and to explore alternative ways of incorporating out-of-domain data in Whisper training.\"}"}
{"id": "tapo24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. Acknowledgements\\n\\nThis work was supported in part by Google Creative Lab and a Google PhD Fellowship awarded to the first author, Allahsera Tapo. We would like to thank Bakary Diarrassouba, Valentin Vydrin, and the staff at RobotsMali.\\n\\n8. References\\n\\n[1] D. M. Eberhard, G. F. Simons, and C. D. Fennig, *Ethnologue: Languages of the World*. Twenty-seventh edition. Dallas, Texas: SIL International, 2024.\\n\\n[2] G. Adda, S. St \u00a8uker, M. Adda-Decker, O. Ambouroue, L. Be- sacier, D. Blachon, H. Bonneau-Maynard, P. Godard, F. Ham- laoui, D. Idiatov et al., \u201cBreaking the unwritten language barrier: The bulb project,\u201d *Procedia Computer Science*, vol. 81, pp. 8\u201314, 2016.\\n\\n[3] F. Coulmas, *Writing and society: An introduction*. Cambridge University Press, 2013.\\n\\n[4] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in *Advances in Neural Information Processing Systems*, vol. 33, 2020, pp. 12 449\u201312 460.\\n\\n[5] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised cross-lingual representation learning for speech recognition,\u201d in *Proceedings of Interspeech*, 2021, pp. 2426\u20132430.\\n\\n[6] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d *arXiv preprint arXiv:2212.04356*, 2022.\\n\\n[7] F. Seifart, N. Evans, H. Hammarstr \u00a8om, and S. C. Levinson, \u201cLanguage documentation twenty-five years on,\u201d *Language*, vol. 94, no. 4, pp. e324\u2013e345, 2018.\\n\\n[8] N. P. Himmelmann, \u201cMeeting the transcription challenge,\u201d *Language Documentation and Conservation*, vol. 15, 2018.\\n\\n[9] V . Gupta and G. Boulianne, \u201cSpeech transcription challenges for resource constrained indigenous language Cree,\u201d in *Proceedings of the 1st Joint Workshop on Spoken Language Technologies for Under-resourced languages (SLTU) and Collaboration and Computing for Under-Resourced Languages (CCURL)*, 2020, pp. 362\u2013367.\\n\\n[10] E. Prud'hommeaux, R. Jimerson, R. Hatcher, and K. Michelson, \u201cAutomatic speech recognition for supporting endangered language documentation,\u201d *Language Documentation and Conservation*, vol. 15, pp. 491\u2013513, 2021.\\n\\n[11] Z. Liu, J. Spence, and E. Prud'hommeaux, \u201cEnhancing documentation of Hupa with automatic speech recognition,\u201d in *Proceedings of the Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages*, 2022, pp. 187\u2013192.\\n\\n[12] E. Le Ferrand, S. Bird, and L. Besacier, \u201cLearning from failure: Data capture in an Australian aboriginal community,\u201d in *Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, 2022, pp. 4988\u20134998.\\n\\n[13] M. Higgins, R. Barker, J. Simpson, and D. Jurafsky, \u201cAutomated speech tools for helping communities process restricted-access corpora for language revival efforts,\u201d in *Fifth Workshop on the Use of Computational Methods in the Study of Endangered Languages*, 2022, p. 41.\\n\\n[14] S. S. Juan, L. Besacier, B. Lecouteux, and M. Dyab, \u201cUsing resources from a closely-related language to develop ASR for a very under-resourced language: A case study for iban,\u201d in *Proceedings of INTERSPEECH*, Dresden, Germany, September 2015.\\n\\n[15] H. Gelas, L. Besacier, and F. Pellegrino, \u201cDevelopments of Swahili resources for an automatic speech recognition system,\u201d in *SLTU - Workshop on Spoken Language Technologies for Under-Resourced Languages*, Cape-Town, Afrique Du Sud, 2012. [Online]. Available: http://hal.inria.fr/hal-00954048\\n\\n[16] E. Gauthier, L. Besacier, S. V oisin, M. Melese, and U. P. Elingui, \u201cCollecting resources in sub-saharan african languages for automatic speech recognition: a case study of wolof,\u201d in *10th Language Resources and Evaluation Conference (LREC 2016)*, 2016.\\n\\n[17] C. Sikasote and A. Anastasopoulos, \u201cBembaspeech: A Speech Recognition Corpus for the Bemba Language,\u201d in *Proceedings of AfricaNLP*, Online, April 2021. [Online]. Available: https://arxiv.org/pdf/2102.04889.pdf\\n\\n[18] V . Vydrin, K. Maslinsky, J.-J. M\u00b4eric, and A. Rovenchak, \u201cCorpus bambara de r\u00b4ef\u00b4erence,\u201d 2011.\\n\\n[19] V . Vydrin, \u201cV owel elision and reduction in bambara,\u201d *Italian Journal of Linguistics*, vol. 32, no. 1, pp. 103\u2013124, 2020.\\n\\n[20] A. A. Tapo, M. Leventhal, S. Luger, C. M. Homan, and M. Zampieri, \u201cDomain-specific mt for low-resource languages: The case of bambara-french,\u201d 2021.\\n\\n[21] D. I. Adelani, J. O. Alabi, A. Fan, J. Kreutzer, X. Shen, M. Reid, D. Ruiter, D. Klakow, P. Nabende, E. Chang, T. Gwadabe, F. Sackey, B. F. P. Dossou, C. C. Emezue, C. Leong, M. Beuk- man, S. H. Muhammad, G. D. Jarso, O. Yousuf, A. N. Rubungo, G. Hacheme, E. P. Wairagala, M. U. Nasir, B. A. Ajibade, T. O. Ajayi, Y . W. Gitau, J. Abbott, M. Ahmed, M. Ochieng, A. Aremu, P. Ogayo, J. Mukiibi, F. O. Kabore, G. K. Kalipe, D. Mbaye, A. A. Tapo, V . M. Koagne, E. Munkoh-Buabeng, V . Wagner, I. Abdul- mumin, A. Awokoya, H. Buzaaba, B. Sibanda, A. Bukula, and S. Manthalu, \u201cA few thousand translations go a long way! leveraging pre-trained models for african news translation,\u201d 2022.\\n\\n[22] E. van der Westhuizen, T. Padhi, and T. Niesler, \u201cMultilingual training set selection for asr in under-resourced malian languages,\u201d in *International Conference on Speech and Computer*. Springer, 2021, pp. 749\u2013760.\\n\\n[23] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in *IEEE 2011 workshop on automatic speech recognition and understanding*, no. CONF. IEEE Signal Processing Society, 2011.\\n\\n[24] D. W. Oyler, \u201cRe-inventing oral tradition: The modern epic of souleymane kant \u00b4e,\u201d *Research in African Literatures*, vol. 33, no. 1, pp. 75\u201393, 2002. [Online]. Available: http://www.jstor.org/stable/3820930\\n\\n[25] H. Brugman, A. Russel, and X. Nijmegen, \u201cAnnotating multimedia/multi-modal resources with elan.\u201d in *LREC*, 2004, pp. 2065\u20132068.\\n\\n[26] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in *Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)*, Jun. 2019, pp. 4171\u20134186.\\n\\n[27] K. Heafield, \u201cKenlm: Faster and smaller language model queries,\u201d in *Proceedings of the sixth workshop on statistical machine translation*, 2011, pp. 187\u2013197.\\n\\n[28] A. Barcovschi, R. Jain, and P. Corcoran, \u201cA comparative analysis between conformer-transducer, whisper, and wav2vec2 for improving the child speech recognition,\u201d in *2023 International Conference on Speech Technology and Human-Computer Dialogue (SpeD)*. IEEE, 2023, pp. 42\u201347.\\n\\n[29] R. Jimerson, Z. Liu, and E. Prud'Hommeaux, \u201cAn (unhelpful) guide to selecting the best asr architecture for your under-resourced language,\u201d in *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)*, 2023, pp. 1008\u20131016.\\n\\n[30] A. Williams, A. Demarco, and C. Borg, \u201cThe applicability of wav2vec2 and whisper for low-resource maltese asr,\u201d in *Proc. 2nd Annual Meeting of the ELRA/ISCA SIG on Under-resourced Languages (SIGUL 2023)*, 2023, pp. 39\u201343.\"}"}
