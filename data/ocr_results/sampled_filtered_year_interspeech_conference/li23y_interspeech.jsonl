{"id": "li23y_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] D. Bhattacharyya, R. Ranjan, F. Alisherov, M. Choi et al., \u201cBiometric authentication: A review,\u201d International Journal of u-and e-Service, Science and Technology, vol. 2, no. 3, pp. 13\u201328, 2009.\\n\\n[2] E. Variani, X. Lei, E. McDermott, I. L. Moreno, and J. Gonzalez-Dominguez, \u201cDeep neural networks for small footprint text-dependent speaker verification,\u201d in IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014, pp. 4052\u20134056.\\n\\n[3] D. Snyder, D. Garcia-Romero, D. Povey, and S. Khudanpur, \u201cDeep neural network embeddings for text-independent speaker verification.\u201d in INTERSPEECH, vol. 2017, 2017, pp. 999\u20131003.\\n\\n[4] L. Li, Y. Chen, Y. Shi, Z. Tang, and D. Wang, \u201cDeep speaker feature learning for text-independent speaker verification,\u201d in INTERSPEECH, 2017.\\n\\n[5] O. M. Parkhi, A. Vedaldi, and A. Zi, \u201cDeep face recognition,\u201d in Proc. British Machine Vis. Conf. (BMVC), 2015.\\n\\n[6] J. Deng, J. Guo, X. Niannan, and S. Zafeiriou, \u201cArcFace: Additive angular margin loss for deep face recognition,\u201d in CVPR, 2019.\\n\\n[7] W. H. Sumby and I. Pollack, \u201cVisual contribution to speech intelligibility in noise,\u201d The journal of the acoustical society of america, vol. 26, no. 2, pp. 212\u2013215, 1954.\\n\\n[8] S. Partan and P. Marler, \u201cCommunication goes multimodal,\u201d Science, vol. 283, no. 5406, pp. 1272\u20131273, 1999.\\n\\n[9] E. Z. Golumbic, G. B. Cogan, C. E. Schroeder, and D. Poeppel, \u201cVisual input enhances selective speech envelope tracking in auditory cortex at a \u201ccocktail party\u201d,\u201d Journal of Neuroscience, vol. 33, no. 4, pp. 1417\u20131426, 2013.\\n\\n[10] C. C. Chibelushi, F. Deravi, and J. S. Mason, \u201cA review of speech-based bimodal recognition,\u201d IEEE transactions on multimedia, vol. 4, no. 1, pp. 23\u201337, 2002.\\n\\n[11] S. O. Sadjadi, C. S. Greenberg, E. Singer, D. A. Reynolds et al., \u201cThe 2019 NIST audio-visual speaker recognition evaluation,\u201d in Odyssey, 2020, pp. 259\u2013265.\\n\\n[12] S. O. Sadjadi, C. Greenberg, E. Singer, L. Mason, and D. Reynolds, \u201cThe 2021 NIST speaker recognition evaluation,\u201d arXiv preprint arXiv:2204.10242, 2022.\\n\\n[13] G. Sell, K. Duh, D. Snyder, D. Etter, and D. Garcia-Romero, \u201cAudio-visual person recognition in multimedia data from the IARPA Janus program,\u201d in IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2018, pp. 3031\u20133035.\\n\\n[14] J. Alam, G. Boulianne, L. Burget, M. Dahmane et al., \u201cAnalysis of ABC submission to NIST SRE 2019 CMN and VAST challenge.\u201d in Odyssey, 2020, pp. 289\u2013295.\\n\\n[15] S. Shon, T.-H. Oh, and J. Glass, \u201cNoise-tolerant audio-visual online person verification using an attention-based neural network fusion,\u201d in IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2019, pp. 3995\u20133999.\\n\\n[16] Y. Qian, Z. Chen, and S. Wang, \u201cAudio-visual deep neural network for robust person verification,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 1079\u20131092, 2021.\\n\\n[17] R. Tao, R. K. Das, and H. Li, \u201cAudio-visual speaker recognition with a cross-modal discriminative network,\u201d in INTERSPEECH, 2020, pp. 2242\u20132246.\\n\\n[18] L. Li, R. Liu, J. Kang, Y. Fan, H. Cui, Y. Cai, R. Vipperla, T. F. Zheng, and D. Wang, \u201cCN-Celeb: multi-genre speaker recognition,\u201d Speech Communication, vol. 137, pp. 77\u201391, 2022.\\n\\n[19] S. Pigeon and L. Vandendorpe, \u201cThe M2VTS multimodal face database (release 1.00),\u201d in International Conference on Audio- and Video-Based Biometric Person Authentication. Springer, 1997, pp. 403\u2013409.\\n\\n[20] K. Messer, J. Matas, J. Kittler, J. Luettin, G. Maitre et al., \u201cXM2VTSDB: The extended M2VTS database,\u201d in Second international conference on audio and video-based biometric person authentication, vol. 964. Citeseer, 1999, pp. 965\u2013966.\\n\\n[21] C. Sanderson, \u201cThe VidTIMIT database,\u201d IDIAP, Tech. Rep., 2002.\\n\\n[22] C. McCool, S. Marcel, A. Hadid, M. Pietik \u00a8ainen et al., \u201cBi-modal person recognition on a mobile phone: using mobile phone data,\u201d in ICMEW. IEEE, 2012, pp. 635\u2013640.\\n\\n[23] A. Chowdhury, Y. Atoum, L. Tran, X. Liu, and A. Ross, \u201cMSU-AVIS dataset: Fusing face and voice modalities for biometric recognition in indoor surveillance videos,\u201d in ICPR. IEEE, 2018.\\n\\n[24] M. Marras, P. A. Mar \u00b4\u0131n-Reyes, J. J. Lorenzo Navarro, M. F. Cas- trill\u00b4on Santana, and G. Fenu, \u201cAveRobot: an audio-visual dataset for people re-identification and verification in human-robot interaction,\u201d ICPRAM, 2019.\\n\\n[25] K. S. Jones, K. Walker, C. Caruso, J. Wright, and S. Strassel, \u201cWeCanTalk: A new multi-language, multi-modal resource for speaker recognition,\u201d in LREC, 2022.\\n\\n[26] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cV oxCeleb: A large-scale speaker identification dataset,\u201d in INTERSPEECH, 2017, pp. 2616\u20132620.\\n\\n[27] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cV oxceleb2: Deep speaker recognition,\u201d in INTERSPEECH, 2018, pp. 1086\u20131090.\\n\\n[28] J. Tracey and S. Strassel, \u201cV AST: A corpus of video annotation for speech technologies,\u201d in LREC, 2018.\\n\\n[29] Y. Fan, J. Kang, L. Li, D. Wang et al., \u201cCN-Celeb: a challenging Chinese speaker recognition dataset,\u201d in IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2020, pp. 7604\u20137608.\\n\\n[30] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, \u201cJoint face detection and alignment using multitask cascaded convolutional networks,\u201d IEEE signal processing letters, vol. 23, no. 10, pp. 1499\u20131503, 2016.\\n\\n[31] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cECAPA-TDNN: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d arXiv preprint arXiv:2005.07143, 2020.\\n\\n[32] M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell et al., \u201cSpeechBrain: A general-purpose speech toolkit,\u201d arXiv preprint arXiv:2106.04624, 2021.\\n\\n[33] J. Deng, J. Guo, E. Ververas, I. Kotsia, and S. Zafeiriou, \u201cRetinaface: Single-shot multi-level face localisation in the wild,\u201d in CVPR, 2020, pp. 5203\u20135212.\\n\\n[34] N. Br \u00a8ummer and J. Du Preez, \u201cApplication-independent evaluation of speaker detection,\u201d Computer Speech & Language, vol. 20, no. 2-3, pp. 230\u2013275, 2006.\\n\\n[35] J. Kittler, N. Poh, O. Fatukasi, K. Messer, K. Kryszczuk, J. Richiardi, and A. Drygajlo, \u201cQuality dependent fusion of intramodal and multimodal biometric experts,\u201d in Biometric Technology for Human Identification IV, vol. 6539. SPIE, 2007, pp. 18\u201331.\\n\\n[36] N. Br \u00a8ummer and E. De Villiers, \u201cThe bosaris toolkit: Theory, algorithms and code for surviving the new DCF,\u201d arXiv preprint arXiv:1304.2865, 2013.\"}"}
{"id": "li23y_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nAudio-visual person recognition (AVPR) has received extensive attention. However, most datasets used for AVPR research so far are collected in constrained environments, and thus cannot reflect the true performance of AVPR systems in real-world scenarios. To meet the request for research on AVPR in unconstrained conditions, this paper presents a multi-genre AVPR dataset collected 'in the wild', named CN-Celeb-AV. This dataset contains more than 420k video segments from 1,136 persons from public media. In particular, we put more emphasis on two real-world complexities: (1) data in multiple genres; (2) segments with partial information. A comprehensive study was conducted to compare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the results demonstrated that CN-Celeb-AV is more in line with real-world scenarios and can be regarded as a new benchmark dataset for AVPR research. The dataset also involves a development set that can be used to boost the performance of AVPR systems in real-life situations. The dataset is free for researchers and can be downloaded from http://cnceleb.org/.\\n\\nIndex Terms: audio-visual, multi-genre, person recognition, dataset\\n\\n1. Introduction\\n\\nBiometric recognition is an automatic process of measuring and analyzing human biometrics and authenticating personal identity [1]. Voice and face are among the most popular biometrics, partly because they can be collected remotely and non-intrusively. In the past few years, with the emergence of deep learning and data accumulation, performance of the two biometric recognition techniques, i.e., speaker recognition (SR) and face recognition (FR), has been remarkably improved and a wide range of applications has been fostered [2, 3, 4, 5, 6].\\n\\nDespite the impressive progress, either SR or FR suffers from their respective practical difficulties. For audio-based SR, challenges are content variation, channel discrepancy, additive noise, spontaneous speaking styles, and even the change in physiological status. For video-based FR, challenges can be from varied illumination, changed position, and accident occlusion. Researchers have a long-term journey to combine the two modalities, primarily motivated by the fact that audio and visual information are complementary, as has been demonstrated by some psychological experiments [7, 8, 9].\\n\\nFor instance, with the assistance of facial information, humans could do significantly better on auditory perception tasks than if only audio signals were available. Take another example, when a person wears a mask, the face is obscured, which increases both false acceptance and false rejection of face recognition; while this mask has a slight effect on the voice, making speaker recognition more suitable for person authentication. A straightforward idea is to integrate the complementary information of audio and visual modalities to construct an audio-visual person recognition (AVPR) system, that is supposed to be more robust, especially in unconstrained conditions [10]. To answer such requirement for multi-modality person recognition, NIST kicked off an audio-visual challenge [11] in SRE 2019, and keep the effort in SRE 2021 [12].\\n\\nExisting AVPR research takes two approaches. The hybrid approach trains SR and FR models separately and combines their decisions by either score fusion [13, 14] or embedding integration [15]. An advantage of this approach is that the SR and FR models can be built using techniques and databases developed in their respective fields, though the shortcoming is that the intrinsic relation between the two modalities is not fully utilized. The second approach is multi-modal joint modeling. For example, Qian et al. [16] designed audio-visual neural nets based on either feature-level or embedding-level concatenation. Tao et al. [17] proposed a cross-modal discriminative neural net to learn shared audio-visual embeddings, and use them to enhance a score-fusion AVPR.\\n\\nAll the above research reported promising performance. For example, Qian et al. [16] reported an EER reduction from 3.04 (V) and 1.62 (A) to 0.55 (A+V) on the VoxCeleb1 test set. However, all these studies are based on relatively constrained data, for which we mean (1) the data is clean and involves limited variations; (2) the information of the two modalities is fully exposed. We argue that the results based on this type of data cannot reflect real-life complexity, e.g., in unconstrained environments where information of one or two modalities is corrupted or lost.\\n\\nTo facilitate AVPR research in solving real-world challenges, we publish a new AVPR dataset named CN-Celeb-AV in this paper. The new dataset follows the principles of CN-Celeb [18] in data collection but contains both audio and visual data. The entire dataset consists of two parts, one is the 'full-modality' part that involves full AV information and one is the 'partial-modality' part that involves a large proportion of video segments whose audio or visual modality is corrupted or missing. The entire dataset covers 11 different genres in real-world scenarios (as in CN-Celeb1/2) and contains more than 420k video segments from 1,136 persons (e.g., Chinese celebrities, video bloggers, and amateurs). We hope that these distinct properties permit CN-Celeb-AV a suitable benchmark evaluation set for AVPR with real-world complexity.\"}"}
{"id": "li23y_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison of existing AVPR datasets. * means information is unavailable.\\n\\n| Dataset       | # of Spks | # of Segments | # of Hours | Genre       | Uncertainty | Status |\\n|---------------|-----------|---------------|------------|-------------|-------------|--------|\\n| M2VTS [19]    | 37        | *             | *          | indoor      | constrained | Private |\\n| XM2VTS [20]   | 295       | *             | *          | indoor      | constrained | Not Free |\\n| VidTIMIT [21] | 43        | 430           | 0.5        | indoor      | constrained | Public  |\\n| MOBIO [22]    | 150       | 28,800        | 61         | indoor      | semi-constrained | Public |\\n| MSU-A VIS [23]| 50        | 2,260         | 3          | indoor      | semi-constrained | Public |\\n| AveRobot [24] | 111       | 2,664         | 5          | indoor      | semi-constrained | Public |\\n| WeCanTalk [25]| 202       | 3,199         | *          | telephone   | semi-constrained | Private |\\n| VoXCeleb1 [26]| 1,251     | 153,516       | 352        | mostly      | interview   | Public  |\\n| VoXCeleb2 [27]| 6,112     | 1,128,246     | 2,442      | mostly      | interview   | Public  |\\n| JANUS (CORE) [13]| 360     | 1,593         | *          | multi genres| unconstrained | Public  |\\n| VAST (SID) [28]| 300       | *             | *          | multi genres| unconstrained | Private |\\n| CN-Celeb-AV (Ours) | 1,136 | 420,055       | 669        | multi genres| unconstrained | Public  |\\n\\n2. Review of AVPR datasets\\n\\nThis section presents a brief review of existing AVPR datasets and compares them with CN-Celeb-AV. The main information is summarized in Table 1, and the following are some details.\\n\\nEarly work of AVPR focused on constrained conditions where speech content, face pose, environment, and devices are strictly controlled. The representative datasets in this stage involve clear faces and clean voices, such as M2VTS [19], XM2VTS [20], and VidTIMIT [21]. Further studies considered semi-constrained conditions, in which faces may be occluded and voices may be corrupted by noise or non-target speech, but the recording environment and speech content are often controlled. Typical datasets in this stage include MOBIO [22], MSU-A VIS [23], AveRobot [24] and WeCanTalk [25].\\n\\nRecently, the research focus has shifted to unconstrained conditions. A key feature of recognition tasks in these conditions is that the recording environment and device are fully unconstrained, and the target person may be unaware of being recorded. VoXCeleb [26, 27] is a typical dataset with such features. The data was collected from YouTube videos recorded by diverse devices in various conditions for different purposes. A key issue of VoXCeleb is that the videos are mostly from interview programs which are a relatively simple genre and not fully 'in the wild'.\\n\\nCN-Celeb-AV approaches real-world complexities in two aspects: it involves data from multiple genres, and audio and visual information may be partially available. Some recent datasets involve multi-genre data, e.g., JANUS Multimedia dataset [13] and VAST (SID) [28], but they are either small or in private status.\\n\\n3. CN-Celeb-AV\\n\\n3.1. Data description\\n\\nThe purpose of the CN-Celeb-AV dataset is to evaluate the true performance of AVPR techniques in unconstrained conditions and provide a standard benchmark for AVPR research. All the data was collected from Bilibili, a popular Chinese public media. In total, it contains more than 420k video segments (669 hours) from 1,136 people (mostly Chinese celebrities) and covers 11 genres as in CN-Celeb [29].\\n\\nSpecifically, CN-Celeb-AV is composed of two parts. The first 'full-modality' part is the audio-visual version of the audio-only CN-Celeb1 [29] dataset. Most of the data in this part contains both audio and visual information. It is split into a development set and an evaluation set, which involve 689 people and 197 people respectively, following the original split of CN-Celeb1 [29]. The two sets are denoted by CNC-AV-Dev-F and CNC-AV-Eval-F respectively, where 'F' means 'full modality'.\\n\\nThe second 'partial-modality' part is a set of newly collected data that involves a large proportion of video segments whose audio or visual information is corrupted or fully lost. For example, the face and/or the voice of the target person may disappear shortly, be corrupted by noise, or even be fully unavailable. We denote this set of data by CNC-AV-Eval-P, indicating that it is an evaluation set, and with partial AV information. It involves 308k video segments (427 hours) collected from 250 people. The duration of each video segment is 5 seconds, and the number of genres of each person is more than 3.\\n\\nTable 2 presents the data profile of CN-Celeb-AV, and Table 3 presents the data distribution over genres. Note that the persons in CNC-AV-Eval-P were removed from CNC-AV-Dev-F and CNC-AV-Eval-F, which is the reason why the data in these two sets are smaller than those in the corresponding development and evaluation sets of CN-Celeb1.\\n\\nTable 2: The data profile of CN-Celeb-AV\\n\\n|               | CNC-AV-Dev-F | CNC-AV-Eval-F | CNC-AV-Eval-P |\\n|---------------|--------------|--------------|--------------|\\n| # of Genres   | 11           | 11           | 11           |\\n| # of Persons  | 689          | 197          | 250          |\\n| # of Segments | 93,973       | 17,717       | 308,365      |\\n| # of Hours    | 199.70       | 41.96        | 427.75       |\\n\\nTable 3: The distribution over genres of CN-Celeb-AV\\n\\n| Genre   | # of Spks | # of Segments | # of Hours |\\n|---------|-----------|---------------|------------|\\n| Overall | 1,136     | 420,055       | 669.41     |\\n| Advertisement | 45 | 3,888       | 5.44       |\\n| Drama   | 136       | 7,118         | 7.70       |\\n| Entertainment | 555 | 40,728      | 59.98      |\\n| Interview | 907 | 155,462    | 261.97     |\\n| Live Broadcast | 269 | 57,984      | 84.62      |\\n| Movie   | 66        | 2,405         | 3.01       |\\n| Play    | 104       | 6,485         | 8.27       |\\n| Recitation | 45 | 3,457       | 5.99       |\\n| Singing | 384       | 24,752        | 43.98      |\\n| Speech  | 214       | 61,503        | 109.25     |\\n| Vlog    | 158       | 56,273        | 79.20      |\"}"}
{"id": "li23y_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Features of CN-Celeb-A V\\n\\nAs have been shown in Table 1, CN-Celeb-A V possesses several desired features that make it suitable for AVPR research to tackle real-world challenges.\\n\\n- Almost all the video segments involve real-world uncertainties, e.g., content, noise, channel, overlap, speaking style variations in voices and pose, illumination, expression, resolution, and occlusion in faces.\\n- There is a large proportion of single-speaker multi-genre data from many media files, allowing cross-genre and cross-session tests. These tests match real-world situations.\\n- Information is partially observed in some video segments, especially in CNC-A V-Eval-P, making it suitable for testing the performance of AVPR systems in real-life complex conditions, which is the situation where AV multi-modal techniques are expected to get the most value.\\n\\n3.3. Collection pipeline\\n\\nThe collection pipeline of CNC-A V-Dev-F and CNC-A V-Eval-F is the same as in CN-Celeb1 [29]. For CNC-A V-Eval-P, the situation is more complex as video segments with partial information need to be collected, leading to an increased burden on both automatic collection and human annotation. We therefore designed a simplified pipeline that made the data collection highly efficient.\\n\\nCNC-A V-Eval-P was collected in three steps: (1) candidate videos were manually selected; (2) an automatic process to extract candidate segments; (3) human check to confirm valid segments. This process is much faster than a purely human-based annotation and also avoids potential errors caused by a purely automated process. We highlight that human check is important in our case: because the multi-genre data is very complex, the automatic process often makes mistakes. We also developed a user-friendly crowd-sourcing platform to assist with video annotation and human check. The source code of this platform will be published on the dataset webpage to help readers collect their own data. The illustration of the collection pipeline is presented in Figure 1 and the corresponding steps are summarized as follows.\\n\\n- **Step 1. Manual video selection.**\\n  - Select a person of interest (POI) who can be a celebrity or an uploader on Bilibili (a platform similar to YouTube), and then select multiple videos of the person as candidate videos. Annotate the genre type of each video.\\n  - For each video, grab a clear face image of the POI (POI face). Further select 10 short speech segments, each being 1-second long and containing the clear voice of the POI. Merge the 10 short segments into a 10-second POI speech. The POI face and POI speech are used in STEP 2 to select video segments containing the POI. Note that all the work in this step is performed by humans.\\n\\n- **Step 2. Automatic data processing.**\\n  - Divide each candidate video into short segments of 5 seconds. Each segment contains 5 images and 500 speech frames.\\n  - For each segment, use the MTCNN model [30] to perform face detection and alignment, and then use the InsightFace model [6] to perform face verification. During the verification, the POI face and the face in each image are compared, and the maximum cosine distance is used as the detection score.\\n  - For each segment, use the ECAPA-TDNN model [31] from the SpeechBrain toolkit [32] to perform speaker verification, using the POI speech obtained in Step 1 as the enrollment speech.\\n  - Delete segments whose detection scores fall in the lowest 15% with both the audio and visual modalities. The deleted segments are shown as yellow blocks in the annotation platform. All the rest segments are shown as white.\\n\\n- **Step 3. Human check with online auxiliary system.**\\n  - Human annotators check each segment and label them as accepted (shown as green) or rejected (shown as red). The acceptance criterion is that humans can tell POI's existence with information from at least one modality.\\n  - To reduce human workload, an auxiliary system was designed to label low-confidence segments (shown as yellow) and update the threshold for yellow segments after each human annotation.\\n  - Double-check all the accepted segments by a senior annotator to guarantee data quality.\\n\\nIt is worth mentioning that in STEP 2, the audio and visual modalities are processed in parallel and symmetrically, thus avoiding bias towards a particular modality. As far as we know, many existing datasets did not consider this modality bias.\\n\\n4. Experiments\\n\\n4.1. Data\\n\\nIn this section, we conduct AVPR experiments with two CN-Celeb-A V evaluation sets and other two popular datasets: MOBIO [22] and VoxCeleb1 [26]. We choose these two datasets for two reasons, one is that they have been widely used by researchers, and the other is that they released official trials, making the comparison easy. For the two comparative datasets, we use the official trials defined by the data provider. For CNC-A V-Eval-F/P, we choose video segments from a genre without too much complexity (such as interview and speech) for each POI. The POI face and speech of this video are used as enrollment data for that POI, and all the remaining video segments are used for testing.\\n\\nWe argue that this trial design is in line with real-world\"}"}
{"id": "li23y_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scenarios where users are often enrolled in a constrained condition while tested in unconstrained environments. Due to the large amount of test utterances in CNC-A V-Eval-P, we construct the negative pairs for this evaluate set by a sampling approach. Specifically, for each POI, we randomly sample negative pairs three times the positive pairs. The trials of the 4 datasets are listed in Table 4.\\n\\nTable 4: The profile of 4 evaluation sets\\n\\n| Dataset      | # of Spks | # of Target | # of Nontarget |\\n|--------------|-----------|-------------|----------------|\\n| MOBIO        | 58        | 6,090       | 187,530        |\\n| V oxCeleb1-O | 40        | 18,860      | 18,860         |\\n| CNC-A V-Eval-F | 197    | 17,693      | 53,079         |\\n| CNC-A V-Eval-P | 250    | 308,365     | 924,802        |\\n\\n4.2. Model setting\\n\\n4.2.1. Speaker verification\\n\\nWe employ the ECAPA-TDNN model [31] in the SpeechBrain toolkit [32] for speaker verification. The model is trained with V oxCeleb1.dev and V oxCeleb2. All the speech data are first preprocessed by V AD and then fed into ECAPA-TDNN to extract speaker embeddings. The cosine similarity is used to score the trials.\\n\\n4.2.2. Face verification\\n\\nTo perform face verification, we sample images every 25 frames. For each sampled image, RetinaFace [33] is used to perform face detection and InsightFace [6] is used to extract face embeddings, and verification scores are based on cosine similarity. Since there are multiple face images in both enrollment and test videos, a pooling scheme is required to make full use of these images, either at the embedding level or the score level.\\n\\nFor MOBIO and V oxCeleb1.O, only the target person may appear in the video. We therefore simply average the face embeddings of all the sampled images to represent a video for either enrollment or test.\\n\\nFor CNC-A V-Eval-F/P, the POI face is used for enrollment. During the test, multiple faces may appear in the test video. We compare the person vector to the face embedding of every sampled image, and the maximum cosine similarity is used as the verification score.\\n\\n4.2.3. System fusion\\n\\nSimple score fusion is used to fuse the audio and visual modalities. Calibration is a standard technique to perform such fusion. In a nutshell, calibration maps raw scores produced by a decision system to log-likelihood ratios (LLRs) [34]. The LLRs have a clear probabilistic interpretation, making them theoretically suitable for combining decisions from different systems [35]. A CLLR-based calibration routine implemented in the BOSARIS toolkit [36] is used to perform calibration. Once the scores of the visual and audio streams are calibrated, we simply average them to get the final score.\\n\\n4.3. Results\\n\\nThe results in terms of EER(%) and minDCF($P_{tar}$= 0.1) are reported in Table 5. Firstly, we can see that both single-modal and multi-modal systems achieve good performance on the MOBIO and V oxCeleb1 datasets. This is expected since the information is almost complete in these two datasets and the interference is limited. In contrast, the performance on the two CNC-A V-Eval datasets is much worse, especially with the visual modality. This is also not surprising, as the data in these datasets is more complex, e.g., multiple faces may occur in the same frame and the target face may be small or occluded. Since all these complexities occur in real-life conditions, the results indicate that the present person recognition techniques are still far from perfect, either with audio or visual clues.\\n\\nSecondly, it can be observed that the performance of multi-modal systems is consistently better than single-modal systems on all the datasets, demonstrating the benefit of multi-modal processing. However, even with the A V models, the performance of the two CNC-A V-Eval datasets are still poor, suggesting further research.\\n\\nTable 5: Results on different evaluation sets.\\n\\n|              | Audio Visual Fusion |\\n|--------------|---------------------|\\n| MOBIO        | 2.48/0.146          |\\n| V oxCeleb1-O | 1.04/0.057          |\\n| CNC-A V-Eval-F | 14.98/0.458         |\\n| CNC-A V-Eval-P | 16.78/0.451         |\\n\\nIn the last experiment, we test the value of the released development set CNC-A V-Dev-F. We simply use this set to train two LDA models that project the audio and visual embeddings respectively. For the audio stream, it reduces the dimension of speaker embeddings from 192 to 128, and for the visual stream, it reduces the dimension of face embeddings from 512 to 128. The projected embeddings are then used to perform tests as in the previous experiment. The results are shown in Table 6. It can be seen that significant performance gains were obtained, with both the two single-model systems and the fusion system. This demonstrated the value of CNC-A V-Dev-F.\\n\\nTable 6: Results on the two CNC-AV evaluation datasets with LDA trained on the CNC-AV development set.\\n\\n|              | Audio Visual Fusion |\\n|--------------|---------------------|\\n| CNC-A V-Eval-F | 10.63/0.357         |\\n| CNC-A V-Eval-P | 13.97/0.390         |\\n\\n5. Conclusion\\n\\nWe introduced CN-Celeb-A V, a free multi-modal dataset for audio-visual person recognition research. This dataset consists of one development set and two evaluation sets. The two evaluation sets were designed to represent test conditions with full-modality and partial-modality information respectively. We compared the two evaluation sets with two existing A V datasets, MOBIO and V oxCeleb1. Experimental results demonstrated that CN-Celeb-A V represents more challenging real-life situations, and simply employing and combining the current SOTA speaker and face recognition models cannot achieve satisfactory results in this condition. Finally, we verified that the development set of CN-Celeb-A V can be used to improve the performance of A VPR system in real-life conditions.\"}"}
