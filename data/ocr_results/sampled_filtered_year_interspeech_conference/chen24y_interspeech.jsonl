{"id": "chen24y_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] B. Martinez, P. Ma, S. Petridis, and M. Pantic, \u201cLipreading using temporal convolutional networks,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2020, pp. 6319\u20136323.\\n\\n[2] J. S. Chung and A. Zisserman, \u201cLip reading in the wild,\u201d in Asian Conference on Computer Vision (ACCV). Springer, 2017, pp. 87\u2013103.\\n\\n[3] S. Yang, Y. Zhang, D. Feng, M. Yang, C. Wang, J. Xiao, K. Long, S. Shan, and X. Chen, \u201cLrw-1000: A naturally-distributed large-scale benchmark for lip reading in the wild,\u201d in IEEE International Conference on Automatic Face & Gesture Recognition (FG). IEEE, 2019, pp. 1\u20138.\\n\\n[4] P. Ma, Y. Wang, J. Shen, S. Petridis, and M. Pantic, \u201cLip-reading with densely connected temporal convolutional networks,\u201d in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2021, pp. 2857\u20132866.\\n\\n[5] T. Afouras, J. S. Chung, A. Senior, O. Vinyals, and A. Zisserman, \u201cDeep audio-visual speech recognition,\u201d IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 44, no. 12, pp. 8717\u20138727, 2018.\\n\\n[6] A. Ephrat, I. Mosseri, O. Lang, T. Dekel, K. Wilson, A. Hassidim, W. T. Freeman, and M. Rubinstein, \u201cLooking to listen at the cocktail party: a speaker-independent audio-visual model for speech separation,\u201d ACM Transactions on Graphics, vol. 37, no. 4, pp. 1\u201311, 2018.\\n\\n[7] A. Nagrani, J. S. Chung, and A. Zisserman, \u201cVoxceleb: a large-scale speaker identification dataset,\u201d in INTERSPEECH. ISCA, 2017.\\n\\n[8] J. Chung, A. Nagrani, and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d in INTERSPEECH. ISCA, 2018.\\n\\n[9] A. J. Goldschen, O. N. Garcia, and E. D. Petajan, \u201cContinuous automatic speech recognition by lipreading,\u201d in Motion-Based Recognition. Springer, 1997, pp. 321\u2013343.\\n\\n[10] S. Petridis and M. Pantic, \u201cDeep complementary bottleneck features for visual speech recognition,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2016, pp. 2304\u20132308.\\n\\n[11] Y. M. Assael, B. Shillingford, S. Whiteson, and N. De Freitas, \u201cLipnet: End-to-end sentence-level lipreading,\u201d arXiv preprint arXiv:1611.01599, 2016.\\n\\n[12] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in International Conference on Machine learning (ICML), 2006, pp. 369\u2013376.\\n\\n[13] M. Cooke, J. Barker, S. Cunningham, and X. Shao, \u201cAn audio-visual corpus for speech perception and automatic speech recognition,\u201d The Journal of the Acoustical Society of America, vol. 120, no. 5, pp. 2421\u20132424, 2006.\\n\\n[14] S. Jeon, A. Elsharkawy, and M. S. Kim, \u201cLipreading architecture based on multiple convolutional neural networks for sentence-level visual speech recognition,\u201d Sensors, vol. 22, no. 1, p. 72, 2021.\\n\\n[15] B. Shillingford, Y. Assael, M. W. Hoffman, T. Paine, C. Hughes, U. Prabhu, H. Liao, H. Sak, K. Rao, L. Bennett et al., \u201cLarge-scale visual speech recognition,\u201d arXiv preprint arXiv:1807.05162, 2018.\\n\\n[16] P. Ma, S. Petridis, and M. Pantic, \u201cEnd-to-end audio-visual speech recognition with conformers,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2021, pp. 7613\u20137617.\\n\\n[17] P. Ma, S. Petridis, and M. Pantic, \u201cVisual speech recognition for multiple languages in the wild,\u201d Nature Machine Intelligence, vol. 4, no. 11, pp. 930\u2013939, 2022.\\n\\n[18] P. Ma, A. Haliassos, A. Fernandez-Lopez, H. Chen, S. Petridis, and M. Pantic, \u201cAuto-avsr: Audio-visual speech recognition with automatic labels,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[19] C. Chen, D. Wang, and T. F. Zheng, \u201cCn-cvs: A mandarin audio-visual dataset for large vocabulary continuous visual to speech synthesis,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[20] Z. Gao, S. Zhang, I. McLoughlin, and Z. Yan, \u201cParaformer: Fast and accurate parallel transformer for non-autoregressive end-to-end speech recognition,\u201d in INTERSPEECH. ISCA, 2022.\\n\\n[21] J. Deng, J. Guo, Y. Zhou, J. Yu, I. Kotsia, and S. Zafeiriou, \u201cRetinaface: Single-stage dense face localisation in the wild,\u201d arXiv preprint arXiv:1905.00641, 2019.\\n\\n[22] A. Bulat and G. Tzimiropoulos, \u201cHow far are we from solving the 2d & 3d face alignment problem?(and a dataset of 230,000 3d facial landmarks),\u201d in IEEE International Conference on Computer Vision (ICCV), 2017, pp. 1021\u20131030.\\n\\n[23] T. Kudo and J. Richardson, \u201cSentencepiece: A simple and language independent subword tokenizer and detokenizer for neural text processing,\u201d in Conference on Empirical Methods in Natural Language Processing (EMNLP), 2018, pp. 66\u201371.\\n\\n[24] K. Kim, F. Wu, Y. Peng, J. Pan, P. Sridhar, K. J. Han, and S. Watanabe, \u201cE-branchformer: Branchformer with enhanced merging for speech recognition,\u201d in IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 84\u201391.\\n\\n[25] J. G. Fiscus, \u201cA post-processing system to yield reduced word error rates: Recognizer output voting error reduction (rover),\u201d in IEEE Workshop on Automatic Speech Recognition and Understanding Proceedings. IEEE, 1997, pp. 347\u2013354.\\n\\n[26] Y. Peng, S. Dalmia, I. Lane, and S. Watanabe, \u201cBranchformer: Parallel mlp-attention architectures to capture local and global context for speech recognition and understanding,\u201d in International Conference on Machine Learning (ICML). PMLR, 2022, pp. 17 627\u201317 643.\\n\\n[27] J. Nozaki and T. Komatsu, \u201cRelaxing the Conditional Independence Assumption of CTC-Based ASR by Conditioning on Intermediate Predictions,\u201d in INTERSPEECH, 2021, pp. 3735\u20133739.\\n\\n[28] J. Lee and S. Watanabe, \u201cIntermediate loss regularization for ctc-based speech recognition,\u201d in IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP). IEEE, 2021, pp. 6224\u20136228.\\n\\n[29] M. Burchi and R. Timofte, \u201cAudio-visual efficient conformer for robust speech recognition,\u201d in IEEE/CVF Winter Conference on Applications of Computer Vision, 2023, pp. 2258\u20132267.\\n\\n[30] D. Wu, B. Zhang, C. Yang, Z. Peng, W. Xia, X. Chen, and X. Lei, \u201cU2++: Unified two-pass bidirectional end-to-end model for speech recognition,\u201d arXiv preprint arXiv:2106.05642, 2021.\\n\\n[31] B. Zhang, D. Wu, Z. Peng, X. Song, Z. Yao, H. Lv, L. Xie, C. Yang, F. Pan, and J. Niu, \u201cWenet 2.0: More productive end-to-end speech recognition toolkit,\u201d arXiv preprint arXiv:2203.15455, 2022.\"}"}
{"id": "chen24y_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThe first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers. The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task. This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction. It also summarizes the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at http://cnceleb.org/competition.\\n\\nIndex Terms: CNVSRC, Visual speech recognition, Lip reading, Chinese VSR\\n\\n1. Introduction\\n\\nVisual Speech Recognition (VSR), commonly called lip reading, is a technology that utilizes lip movements to infer speech content. It has broad applications, including public surveillance, support for elderly and disabled individuals, and fake video detection. Traditionally, VSR has been primarily focused on recognizing isolated words or phrases. For example, Martinez et al. [1] developed a model that extracts visual features using 3D convolution, ResNet-18, and a multi-scale temporal convolutional network (MS-TCN). This was further enhanced by simple average pooling and a softmax layer for inferring word posteriors, resulting in commendable performance on the LRW [2] and LRW-1000 datasets [3], which are the largest publicly available benchmark datasets for unconstrained isolated word lip-reading in English and Mandarin, respectively. Ma et al. [4] adopted a similar architecture but introduced a densely connected temporal convolutional network (DC-TCN) to achieve improved performance.\\n\\nRecently, research in lip reading has progressed beyond word and phrase recognition to focus on large vocabulary continuous visual speech recognition (LVC-VSR), a more challenging task but with more realistic merit. Significant advancements have been made in English benchmarks, partly attributable to the availability of large-scale English visual-speech datasets such as LRS2 [5], LRS3 [5], AVSpeech [6], and VoxCeleb [7, 8]. While earlier studies addressed this issue using hand-crafted features and sequential models like HMM [9] or RNN [10], a significant breakthrough occurred with the end-to-end approach, which processes raw video frames and generates a word sequence. LipNet [11] is perhaps the first end-to-end model, integrating spatiotemporal convolution layers and bi-directional gated recurrent unit (BGRU), trained using the connectionist temporal classification (CTC) loss [12]. The model underwent testing on GRID, a dataset with limited grammar and vocabulary [13]. A similar architecture was adopted by Jeon et al. [14], with their approach involving the integration of multiple CNN-based streams into the feature extraction process. While promising, the GRID corpus is limited in its ability to capture real-world complexity due to the constrained grammar and vocabulary of the sentences. A more challenging task involves large vocabulary continuous visual speech recognition based on datasets gathered from online media repositories like YouTube. A seminal work [5] introduced the first comprehensive visual speech datasets LRS2 and LRS3, along with the first transformer-based system trained with either the CTC loss (TM-CTC) or the sequence-to-sequence loss (TM-seq2seq). In a parallel endeavor, Google [15] developed an end-to-end model (CNN/BLSTM backbone and CTC loss) that transcribes videos into phone sequences and utilizes an FST-based decoder to obtain word sequences. The research was expanded upon in a subsequent study [16], which introduced a hybrid CTC/Attention model utilizing a ResNet/Conformer encoder and a Transformer-based language model (LM). This work was further developed by incorporating time-masking data augmentation and an auxiliary reconstruction loss in a subsequent publication [17].\\n\\nIn addition to employing complex structures, a simple yet effective approach is increasing the training data volume. However, a major challenge arises from the dearth of well-labeled data. One promising strategy to address this issue involves utilizing a pre-trained automatic speech recognition (ASR) model to transcribe unlabelled videos. This methodology was extensively explored in [18], wherein a ResNet/Conformer model was trained on both unlabelled datasets like VoxCeleb2 [8] and AVSpeech [6], as well as text-labelled datasets such as LRW [2], LRS2 [5], and LRS3 [5]. The study revealed significant performance enhancements with auto-labelled data, and further improvements were observed with increased incorporation of unlabelled data.\\n\\nFor VSR on Chinese data, the research progress has been significantly impeded by the scarcity of data resources. Despite the presence of LRW-1000 [3] as the sole large-scale dataset in Chinese, it consists solely of isolated words. In 2023, the release of the CN-CVS dataset [19] marked the debut of the first large-scale continuous visual-speech dataset in Chinese, thereby presenting an opportunity to propel research in Chinese VSR.\"}"}
{"id": "chen24y_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This also motivated the first Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2023), hosted as a special session on NCMMSC 2023 to estimate the performance boundary of existing LVC-VSR techniques with Chinese data and attracting research interest in this domain. The CN-CVS [19] dataset was used as the primary training data, supplemented by two additional datasets - CNVSRC-Single and CNVSRC-Multi - introduced by the organizers to facilitate system development and evaluation. The organizers also published the model and code of the baseline systems so that the participants could use them as references when developing their systems. This paper summarises the challenge, emphasizing the predominant findings gleaned from the submitted systems.\\n\\nThe structure of the rest of this paper is outlined as follows: Section 2 introduces the tasks and data of the challenge. In Section 3, a comprehensive description of the baseline system is provided, covering the model structure, training strategies, and performance evaluation. Section 4 reports the challenge result and summarizes the representative technologies utilized by the participants. Lastly, the paper is concluded in Section 5.\\n\\n2. Tasks and Data\\n\\n2.1. Tasks\\n\\nThe CNVSRC 2023 challenge encompasses two distinct tasks: Single-speaker VSR (T1) and Multi-speaker VSR (T2). Task T1 emphasizes the performance of large-scale tuning for a specific speaker, whereas T2 focuses on the fundamental performance of the system for non-specific but registered speakers, i.e., speakers seen in the data for system development. In both tasks, the system is fed with silent facial videos featuring a single individual, and it is required to generate the spoken content in written form.\\n\\nEach task is further categorized into a 'fixed track' and an 'open track'. The fixed track permits the use of data and additional resources that have been agreed upon by the organizing committee. Conversely, the open track allows participants to employ any resources except the evaluation set.\\n\\nCharacter Error Rate (CER) was used as the main metric to evaluate VSR performance, formulated as follows:\\n\\n\\\\[\\n\\\\text{CER} = \\\\frac{S + D + I}{N}\\n\\\\]\\n\\nwhere \\\\(S\\\\), \\\\(D\\\\), and \\\\(I\\\\) represent the number of substitutions, deletions, and insertions in the output transcription, respectively. \\\\(N\\\\) is the number of characters in the ground truth transcription.\\n\\n2.2. Data profile\\n\\nCNVSRC 2023 utilized the CN-CVS dataset [19] in addition to two supplementary datasets: CNVSRC-Single and CNVSRC-Multi, which served as the development and evaluation data for the single-speaker VSR task (T1) and multi-speaker VSR task (T2), respectively. All the data was transcribed into text. Table 1 presents the data profile of the three datasets. Note that both CNVSRC-Single and CNVSRC-Multi were split into a development set and an evaluation set. The development data was transparent to the participants (including the video, audio, and text), while the text and audio of the evaluation data were kept secret during the entire challenge process. The participants could use the development data in any way, e.g., freely splitting it into a subset for model fine-tuning and a subset for model validation/selection.\\n\\n| Dataset    | Train | Dev | Eval | Dev | Eval |\\n|------------|-------|-----|------|-----|------|\\n| CN-CVS     |       |     |      |     |      |\\n| CNVSRC-Single |      |     |      |     |      |\\n| CNVSRC-Multi |      |     |      |     |      |\\n\\n2.2.1. CN-CVS\\n\\nThe CN-CVS dataset [19] comprises visual-speech data from over 2,557 speakers, totalling more than 300 hours of videos. It encompasses various scenarios, including news broadcasts and public speeches. So far, CN-CVS is the largest open-source Chinese visual-speech dataset. This dataset was used as the primary training data for the CNVSRC 2023 challenge. Note that the original publication of CN-CVS is for the video-to-speech synthesis (VTS) task and thus does not involve text transcription. To support the CNVSRC 2023 challenge, we labelled the video with a semi-automatic pipeline that involves ASR transcription and human check, as will be presented shortly.\\n\\n2.2.2. CNVSRC-Single\\n\\nThe CNVSRC-Single dataset, designed for a single-speaker VSR task (T1), is obtained from a broadcaster's online channel. It includes over 800 speech videos of that broadcaster, with a cumulative duration of over 100 hours. The pipeline used in [19] was employed for the collection and processing.\\n\\n2.2.3. CNVSRC-Multi\\n\\nThe CNVSRC-Multi dataset was designed as the development/evaluation data for the multi-speaker VSR task (T2). It encompasses two scenarios: reading in a recording studio and speeches downloaded from the internet.\\n\\nIn the recording studio scenario, facial videos of speakers were captured from three different camera angles (0\u00b0, 30\u00b0, 60\u00b0), while their speech audio was recorded using a high-quality microphone. The speakers were prompted a sentence at each time via a computer screen and were asked to read the sentence clearly with a neutral emotion. The video data from the front camera was used in CNVSRC 2023, which was transcoded to 25 frames per second (FPS) and scaled to an appropriate size. A total of 23 speakers participated in the recording, each reading 1,000 sentences. In the speeches from the internet scenario, videos of public speeches from 20 speakers were collected from the internet, again following the same collection and processing pipeline as [19]. To ensure the integrity of the collected videos from the internet, a face recognition tool was employed to check whether there is only one face in each video frame and if the face is the target face. A manual check was then conducted to double-check that each extracted video only contained the target face.\\n\\n2.3. Text annotation\\n\\nTo generate text transcriptions, a Paraformer-based ASR system [20] was employed to transcribe the speech of all the videos. Furthermore, the manual check was conducted to ensure that the CER of the transcriptions remains below 2%.\\n\\n3. Baseline System\\n\\nLeveraging the state-of-the-art model used in Auto-A VSR [18], we trained two baseline systems, one for the single-speaker VSR task (T1) and the other for the multi-speaker VSR task (T2). The model structure and training strategies of the baseline systems are described in detail in Section 3. Section 4 reports the challenge result and summarizes the representative technologies utilized by the participants. Lastly, the paper is concluded in Section 5.\"}"}
{"id": "chen24y_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Training Details of the Pretraining (P1 & P2) and Fine-tuning (FT) steps when constructing the baseline systems.\\n\\n| Experiment | P1 | P2 | FT (Single-Speaker) | FT (Multi-Speaker) |\\n|------------|----|----|---------------------|--------------------|\\n| Initialize | Random | P1 Saved Model | P2 Saved Model | P2 Saved Model |\\n| Warmup Epochs | 5 | 5 | 2 | 2 |\\n| Learning Rate | 0.0002 | 0.001 | 0.0003 | 0.0002 |\\n| Training Epochs | 75 + Early stop | 75 | 80 | 80 |\\n| Saved Model | Top 10 average | Last 10 epochs average | Last 5 epochs average | Last 5 epochs average |\\n\\nVSR task (T1) and the other for the multi-speaker VSR task (T2). Only the datasets provided in this challenge were used. In other words, these systems conform to the specifications of the fixed tracks.\\n\\n3.1. Model structure\\n\\nThe model structure is duplicated from Auto-A VSR [18]. Specifically, it comprises three components: visual frontend, encoder, and decoder. As in [18], the visual frontend adopts ResNet18 as its backbone, except that the first 2D-CNN layer is replaced with a 3D-CNN layer to capture local spatiotemporal correlation. The encoder adopts a Conformer structure with 12 layers, while the decoder employs a Transformer structure with 6 layers.\\n\\nUpon receiving the input video data, the visual frontend performs initial local spatiotemporal feature extraction. Subsequently, the Conformer encoder further extracts context-dependent features. A projection layer and a transformer decoder are employed to predict the output class labels. The entire model was trained with the joint CTC/Attention loss, where the CTC loss is back-propagated through the projection layer, while the Attention loss is back-propagated through the decoder. Refer to [18] for details.\\n\\n3.2. Data pre-processing\\n\\nThe provided datasets of the challenge include the faces of the target speakers, so a pre-processing pipeline was designed to extract the lip region. Note that the pipeline was equally applied to both the training data (CN-CVS) and the development/evaluation data (CNVSRC-Single and CNVSRC-Multi).\\n\\nInitially, we utilized RetinaFace [21] to detect the facial regions in each frame. Subsequently, FAN [22] was employed to extract facial landmarks, with which each detected face was aligned with a mean reference face. Finally, we extracted the lip region through the alignment for each video frame, which served as the input to the visual frontend. The modelling units are subword tokens, and the tokenizer was trained using the SentencePiece [23] tool in the unigram mode. During the training of this tokenizer, only the text data from CN-CVS was utilized. Subsequently, we employed this tokenizer to process the CN-CVS, CNVSRC-Single, and CNVSRC-Multi datasets, obtaining the token sequences used to train the recognition models.\\n\\n3.3. Training strategy\\n\\nWe followed a two-step process to build the baseline systems. Firstly, we performed pre-training with the CN-CVS dataset. Subsequently, the development set was split into an \u2018adaptation set\u2019 and a \u2018validation set\u2019, with a ratio of 8:1 for the single-speaker dataset and 3:1 for the multi-speaker dataset. The adaptation set was used to fine-tune the pre-trained model, while the validation set was used to select the appropriate checkpoint. The training process was summarized in Table 2, and the details are as follows.\\n\\nThe initial training phase (P1) selected CN-CVS videos with a duration of less than 4 seconds to train an \u2018easy model\u2019. The maximum number of training epochs was 75, and early stopping was triggered if the model\u2019s performance on the validation set started to drop. Once the training stopped, we selected the top 10 models based on their accuracy on the validation set, averaged their parameters to obtain the P1 model.\\n\\nNext, a full pre-training phase (P2) was evoked using the complete CN-CVS dataset, and the training was conducted for 75 epochs. Note that a warmup stage of 5 epochs was designed, by which the learning rate was gradually increased from 0 to 0.001. The average of the models of the last 10 epochs was used as the P2 model, i.e., the pre-trained model.\\n\\nThe fine-tuning step started from the P2 model and ran 80 epochs, including 2 epochs of warmup. This process is the same for the models trained for the single-speaker task and the multi-speaker task, but there are indeed some differences. Besides the learning rate (see Table 2), the most notable difference is that for the single-speaker model, we randomized the parameters of the classification layer of the P2 model, to provide sufficient space for the adaptation with the large amount of single-speaker data. The average of the models of the last 5 epochs was used as the final model, for both the single-speaker task and the multi-speaker task.\\n\\n3.4. Performance\\n\\nThe performance of the baseline models was evaluated on the respective validation set and evaluation set for both the single-speaker task and multi-speaker task, using the TorchMetrics tool.\\n\\nTable 3: Performance of the baseline systems.\\n\\n| Task               | Valid | Eval |\\n|--------------------|-------|------|\\n| T1: Single-speaker VSR | 48.57% | 48.60% |\\n| T2: Multi-speaker VSR   | 58.77% | 58.37% |\\n\\n4. CNVSRC 2023 Report\\n\\n4.1. Leaderboard\\n\\nCNVSRC 2023 received 10 valid submissions from 6 teams. Most teams chose to submit their results to the single-speaker task, suggesting that single-speaker VSR is more suitable for the current stage of technical development, and multi-speaker VSR is over-challenging. The leaderboard is reported in Table 4.\\n\\nOverall, T237 achieved the best performance in 3/4 of the tasks & tracks, and their results outperformed the baseline systems by a large margin. Their proposed system consists of a ResNet-3D visual frontend, an E-Branchformer encoder [24], and a Transformer decoder. The Chinese characters were used.\"}"}
{"id": "chen24y_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Leaderboard of CNVSRC2023. Team ID and CER are reported.\\n\\n| Task       | Fixed Track | Open Track |\\n|------------|-------------|------------|\\n| Baseline   | 48.60%      | 58.37%     |\\n| Rank 1     | T237 34.76% | T237 41.05%|\\n| Rank 2     | T266 38.09% | T244 53.68%|\\n| Rank 3     | T290 39.47% |            |\\n| Rank 4     | T238 40.52% |            |\\n| Rank 5     | T267 41.62% |            |\\n\\nas the modelling units, and multiple data augmentation methods, including speed perturbation, random rotation, and horizontal flipping, were applied during training. Additionally, a ROVER-based system fusion [25] was performed during the inference procedure.\\n\\n4.2. Technical summary\\n\\nWe summarize here the promising techniques demonstrated by the results of the submissions, highlighting the most effective methods in **bold font**.\\n\\n4.2.1. Data pre-processing\\n\\nMany teams adhered to the baseline system for data pre-processing. A notable observation is that T237 extracted lip regions of varying sizes and resolutions as inputs to their model and discovered that **larger regions (lip + mouth around)** yielded clear performance improvement.\\n\\n4.2.2. Data augmentation\\n\\nThe participants extensively used data augmentation techniques, including random erase, random crop, random flip, and adaptive time masking. Notably, **speed perturbation** and **generative data augmentation** were reported to yield exceptionally remarkable results. Speed perturbation adjusts the speed of the original videos by a factor ranging from 0.9 to 1.1. T237 and T238 observed notable enhancements in model performance (4.86% relative improvement) by applying speed perturbation. Generative data augmentation involves generating speech-driven lip videos, hence producing extra video-text training pairs. T238 utilized facial images from CNVSRC-Single and speech from CNVSRC-Single to create an extra set of video-text pairs and reported a relative CER reduction of 6.98%.\\n\\n4.2.3. Model structure\\n\\nMost of the participating teams adopted the model architecture of the baseline systems, though some teams chose a more complicated backbone to pursue better performance. For instance, T237 achieved superior results using a **ResNet3D** structure. Moreover, T237 employed two advanced encoder structures: Branchformer [26] and **E-Branchformer** [24]. All these advanced structures lead to notable performance improvements. T266 introduced an **inner CTC residual module** [27, 28, 29] that resides in the Conformer block of the encoder. This module back-propagated a CTC loss through the shallow layers thus facilitating more effective parameter updates for the shallow layers of the model. Furthermore, taking inspiration from [30, 31], T266 utilized a **bi-transformer** structure to construct the decoder. This modification enhances the model's ability to capture contextual information from both the past and future segments.\\n\\n4.2.4. Modeling units\\n\\nMost participating teams used Chinese characters as the modelling units, and showed better performance than the subword tokens used by the baseline systems. In addition to subword tokens, T238 used **phonemes** as supplementary modelling units and designed a separate decoder for phoneme recognition. This approach achieved performance improvement, for which a hypothesis is that phonemes contain less semantic variation and thus are more closely related to lip movement, which may stabilize the training, especially in the early stage.\\n\\n4.2.5. Cross-modality modeling\\n\\nSome teams designed various approaches to leverage the cross-modal dependency. T290 invented an ASR-VSR joint system that forces the video representations to predict not only the text labels but also the speech representations produced by the middle layer of the ASR system. Following the same inspiration, T244 trained an audio-visual recognition system where the ASR and VSR have their respective independent encoders and decoders, and an AVSR decoder is constructed on top of the fused ASR and VSR features.\\n\\n4.2.6. Decoding strategy\\n\\nSeveral teams integrated RNN-based or Transformer-based language models to enhance the decoder, and mild performance improvement was reported. Moreover, system fusion was widely employed by teams to improve the performance of their systems.\\n\\n5. Conclusion\\n\\nThis paper comprehensively details the inaugural Chinese Continuous Visual Speech Recognition Challenge (CNVSRC 2023). A key motivation of the challenge is to investigate the performance bound of VSR under the present data resource, e.g., 300 hours of training data from 2,557 speakers. The overall results suggest that the performance is far from satisfactory, even for the single-speaker scenario where about 100 hours of video is available for one person. The poor performance is certainly attributed to the lack of data, but whether it is related to the special linguistic properties of Chinese, e.g., the ubiquitous homophones, is unknown.\\n\\nBased on these technical reports from participants, we have summarized the key techniques that might be crucial in constructing Chinese VSR systems. The most effective methods, ordered by their merit in terms of CER reduction: **Chinese characters as modelling units**, **rich data augmentation**, **fully 3D-CNN visual frontend**, **cross-modality modelling**, **system fusion**. Leveraging the technical insights provided by the participants, we have established a cutting-edge benchmark for Chinese LVC-VSR. We aspire that these resources will strengthen the burgeoning field of LVC-VSR research.\"}"}
