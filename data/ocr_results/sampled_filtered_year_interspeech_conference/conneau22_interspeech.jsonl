{"id": "conneau22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d in Proc. of NeurIPS, 2020.\\n\\n[2] Q. Xu, A. Baevski, T. Likhomanenko, P. Tomasello, A. Conneau, R. Collobert, G. Synnaeve, and M. Auli, \u201cSelf-training and pre-training are complementary for speech recognition,\u201d in ICASSP 2021. IEEE, 2021, pp. 3030\u20133034.\\n\\n[3] A. Baevski, W.-N. Hsu, A. Conneau, and M. Auli, \u201cUnsupervised speech recognition,\u201d NeurIPS 2021, 2021.\\n\\n[4] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. Auli, \u201cUnsupervised cross-lingual representation learning for speech recognition,\u201d in Proc. of Interspeech, 2021.\\n\\n[5] A. Babu, C. Wang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal, K. Singh, P. von Platen, Y. Saraf, J. Pino et al., \u201cXls-r: Self-supervised cross-lingual speech representation learning at scale,\u201d arXiv preprint arXiv:2111.09296, 2021.\\n\\n[6] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMls: A large-scale multilingual dataset for speech research,\u201d in Proc. of Interspeech, 2020.\\n\\n[7] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d in Proc. of ACL, 2021.\\n\\n[8] C. Wang, A. Wu, and J. Pino, \u201cCovost 2 and massively multilingual speech-to-text translation,\u201d arXiv, 2020.\\n\\n[9] J. Hu, S. Ruder, A. Siddhant, G. Neubig, O. Firat, and M. Johnson, \u201cXtreme: A massively multilingual multi-task benchmark for evaluating cross-lingual generalisation,\u201d in ICML, 2020.\\n\\n[10] S. Ruder, N. Constant, J. Botha, A. Siddhant, O. Firat, J. Fu, P. Liu, J. Hu, G. Neubig, and M. Johnson, \u201cXtreme-r: Towards more challenging and nuanced multilingual evaluation,\u201d in EMNLP, 2021.\\n\\n[11] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, \u201cBert: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proc. of NAACL, 2019.\\n\\n[12] Y.-A. Chung, Y. Zhang, W. Han, C.-C. Chiu, J. Qin, R. Pang, and Y. Wu, \u201cW2v-bert: Combining contrastive learning and masked language modeling for self-supervised speech pre-training,\u201d arXiv preprint arXiv:2108.06209, 2021.\\n\\n[13] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00b4an, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, \u201cUnsupervised cross-lingual representation learning at scale,\u201d ACL 2020, 2019.\\n\\n[14] A. Bapna, C. Cherry, Y. Zhang, Y. Jia, M. Johnson, Y. Cheng, S. Khanuja, J. Riesa, and A. Conneau, \u201cmslam: Massively multilingual joint pre-training for speech and text,\u201d in arXiv, 2022.\\n\\n[15] M. J. F. Gales, K. M. Knill, A. Ragni, and S. P. Rath, \u201cSpeech recognition and keyword spotting for low-resource languages: Belief project research at cued,\u201d in Spoken Language Technologies for Under-Resourced Languages, 2014.\\n\\n[16] T. Alum \u00a8ae, D. Karakos, W. Hartmann, R. Hsiao, L. Zhang, L. Nguyen, S. Tsakalidis, and R. Schwartz, \u201cThe 2016 bbn georgian telephone speech keyword spotting system,\u201d in ICASSP, 2017.\\n\\n[17] A. Ragni, Q. Li, M. J. F. Gales, and Y. Wang, \u201cConfidence estimation and deletion prediction using bidirectional recurrent neural networks,\u201d in SLT, Athens, 2018.\\n\\n[18] H. Inaguma, J. Cho, M. K. Baskar, T. Kawahara, and S. Watanabe, \u201cTransfer learning of language-independent end-to-end asr with language model fusion,\u201d in ICASSP, 2019.\\n\\n[19] R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber, \u201cCommon voice: A massively-multilingual speech corpus,\u201d in LREC, 2020.\\n\\n[20] M. Rivi`ere, A. Joulin, P.-E. Mazar\u00b4e, and E. Dupoux, \u201cUnsupervised pretraining transfers well across languages,\u201d in ICASSP, 2020.\\n\\n[21] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in ICASSP, 2015.\\n\\n[22] J. Iranzo-S \u00b4anchez, J. A. Silvestre-Cerd `a, J. Jorge, N. Rosell \u00b4o, A. Gim \u00b4enez, A. Sanchis, J. Civera, and A. Juan, \u201cEuroparl-st: A multilingual corpus for speech translation of parliamentary debates,\u201d in ICASSP 2020, 2020.\\n\\n[23] M. A. Di Gangi, R. Cattoni, L. Bentivogli, M. Negri, and M. Turchi, \u201cMust-c: a multilingual speech translation corpus,\u201d in NAACL, 2019.\\n\\n[24] E. Salesky, M. Wiesner, J. Bremerman, R. Cattoni, M. Negri, M. Turchi, D. W. Oard, and M. Post, \u201cThe multilingual tedx corpus for speech recognition and translation,\u201d arXiv preprint arXiv:2102.01757, 2021.\\n\\n[25] J. Valk and T. Alum\u00a8ae, \u201cV oxlingua107: a dataset for spoken language recognition,\u201d in Proc. of SLT, 2020.\\n\\n[26] A. W. Black, \u201cCmu wilderness multilingual speech dataset,\u201d in ICASSP 2019, 2019.\\n\\n[27] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \u201cGLUE: A multi-task benchmark and analysis platform for natural language understanding,\u201d in ICLR, 2019.\\n\\n[28] A. Wang, Y. Pruksachatkun, N. Nangia, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, \u201cSuperglue: A stickier benchmark for general-purpose language understanding systems,\u201d arXiv preprint arXiv:1905.00537, 2019.\\n\\n[29] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, \u201cXlnet: Generalized autoregressive pretraining for language understanding,\u201d arXiv, vol. abs/1906.08237, 2019.\\n\\n[30] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d JMLR, 2019.\\n\\n[31] A. Conneau, R. Rinott, G. Lample, A. Williams, S. R. Bowman, H. Schwenk, and V. Stoyanov, \u201cXnli: Evaluating cross-lingual sentence representations,\u201d in EMNLP 2018, 2018.\\n\\n[32] P. Lewis, B. O\u02d8guz, R. Rinott, S. Riedel, and H. Schwenk, \u201cMlqa: Evaluating cross-lingual extractive question answering,\u201d arXiv preprint arXiv:1910.07475, 2019.\\n\\n[33] J. H. Clark, E. Choi, M. Collins, D. Garrette, T. Kwiatkowski, V. Nikolaev, and J. Palomaki, \u201cTydi qa: A benchmark for information-seeking question answering in typologically diverse languages,\u201d TACL, 2020.\\n\\n[34] S. Yang, P. Chi, Y. Chuang, C. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G. Lin, T. Huang, W. Tseng, K. Lee, D. Liu, Z. Huang, S. Dong, S. Li, S. Watanabe, A. Mohamed, and H. Lee, \u201cSUPERB: speech processing universal performance benchmark,\u201d Interspeech, 2021.\\n\\n[35] S. Evain, M. H. Nguyen, H. Le, M. Z. Boito, S. Mdhaffar, S. Al-isamir, Z. Tong, N. Tomashenko, M. Dinarelli, T. Parcollet et al., \u201cTask agnostic and task specific self-supervised learning from speech with lebenchmark,\u201d arXiv, 2021.\\n\\n[36] N. Goyal, C. Gao, V. Chaudhary, P.-J. Chen, G. Wenzek, D. Ju, S. Krishnan, M. Ranzato, F. Guzm\u00b4an, and A. Fan, \u201cThe flores-101 evaluation benchmark for low-resource and multilingual machine translation,\u201d in TACL, 2021.\\n\\n[37] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d arXiv preprint arXiv:2205.12446, 2022.\\n\\n[38] Y. Jia, M. T. Ramanovich, Q. Wang, and H. Zen, \u201cCVSS corpus and massively multilingual speech-to-speech translation,\u201d arXiv preprint arXiv:2201.03713, 2022.\\n\\n[39] D. Gerz, P.-H. Su, R. Kusztos, A. Mondal, M. Lis, E. Singhal, N. Mrk \u02c7si\u00b4c, T.-H. Wen, and I. Vuli \u00b4c, \u201cMultilingual and cross-lingual intent detection from spoken data,\u201d arXiv preprint arXiv:2104.08524, 2021.\"}"}
{"id": "conneau22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"XTREME-S: Evaluating Cross-lingual Speech Representations\\nAlexis Conneau\u25b3, Ankur Bapna\u25b3, Yu Zhang\u25b3, Min Ma\u25b3, Patrick von Platen\u2663, Anton Lozhkov\u2663, Colin Cherry\u25b3, Ye Jia\u25b3, Clara Rivera\u25b3, Mihir Kale\u25b3, Daan Van Esch\u25b3, Vera Axelrod\u25b3, Simran Khanuja\u25b3, Jonathan H. Clark\u25b3, Orhan Firat\u25b3, Michael Auli2, Sebastian Ruder\u25b3, Jason Riesa\u25b3, Melvin Johnson\u25b3\\n\\nGoogle Research\\nHugging Face\\nMeta AI\\n\\n{aconneau, ankurbpn, ngyuzh, ruder, riesa, melvinp}@google.com; patrick@huggingface.co\\n\\nAbstract\\nWe introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in \\\"universal\\\" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible through the HuggingFace platform.\\n\\n1. Introduction\\nIn the past two decades, the exploding amount of content on the Internet has led to a pressing urgency to build systems that can understand text, speech, and videos in all of the world's approximately 6,900 languages. Making speech technology available in all languages is especially important to give speakers of under-represented languages an equal voice on the Internet, and the possibility to make their content and culture known outside of their language cluster. Building speech systems for such a large number of languages is especially challenging but recent advances in self-supervised learning (SSL) present great opportunities to achieve this goal.\\n\\nSpeech pre-training techniques like wav2vec 2.0 [1] have emerged as the predominant approach for automatic speech recognition (ASR) and direct speech-to-text translation (ST), and have made speech models much more data efficient: ASR models can be learnt with as little as a few hours of labeled data [2, 3]. Multilingual pre-training helps build better representations for languages that lack unannotated data, and thus enables the same data-efficient strategies for low-resource languages. Approaches like XLS-R [4, 5], for example, have shown particularly strong results on several tasks, including ASR on BABEL and multilingual LibriSpeech, and AST on CoVoST-2. Following a recent trend in natural language processing, the speech community has made these multilingual pre-trained models publicly available to accelerate research in multilingual speech understanding.\\n\\nTo support this rapid development and to make better speech technology available in all languages of the world, the community requires high-quality datasets and a unified evaluation benchmark that is shared across researchers and practitioners. There has been significant progress in the past few years towards building publicly available multilingual evaluation datasets for speech understanding [6, 7, 8]. Many research studies have, however, designed models on different tasks, and evaluated on a small and often disparate set of languages. This makes comparisons across methods difficult, slows down the development of multilingual representations, and hinders the evaluation of the generalization capabilities of such pre-trained models. The goal of this paper is to structure the evaluation of multilingual speech representation learning.\\n\\nTo address these issues and incentivize the rapidly-evolving research on general-purpose multilingual speech representation learning, we introduce XTREME-S, the Cross-lingual Transfer Evaluation of Multilingual Encoders for Speech benchmark. XTREME-S builds on top of the XTREME series of evaluation benchmarks for text understanding, with XTREME [9] and XTREME-R [10], which specialize in the evaluation of multilingual text representations and have helped the community improve multilingual language understanding, with impressive performance improvements on a variety of tasks.\\n\\nXTREME-S is meant to be a more exhaustive, thorough and complete evaluation of learned speech representations. It covers 102 diverse languages spanning more than 10 language families and includes four different task families: recognition, translation, classification and retrieval. The seven downstream tasks of XTREME-S also cover various domains, from read-speech to parliamentary speech. It also includes a new general-purpose massively multilingual evaluation dataset dubbed Fleurs in all of the 102 languages.\\n\\n2. Related work\\nMultilingual representations\\nSelf-supervised learning methods like BERT [11], wav2vec 2.0 [1] or w2v-BERT [12] have been extended to the cross-lingual setting through mBERT [11], XLM-R [13] or XLS-R [4, 5]. These methods demonstrate the effectiveness of multilingual understanding in improving low-resource language representation through unsupervised cross-lingual transfer from higher-resource languages. Combined with the few-shot learning capability of wav2vec 2.0 [2], strong self-supervised speech representations can be built in low-resource languages, enabling training speech recognition systems with just a few hours of labeled data. XLS-R models demonstrate data-efficient capabilities in both speech recognition and speech translation for low-resource languages. Recently, mSLAM [14] built a pre-trained multilingual model for both speech and text, leading to strong improvements on speech translation and even better data efficiency in low-resource languages. mSLAM is evaluated on text downstream tasks from XTREME [9] and tasks from our new XTREME-S benchmark.\\n\\nMultilingual speech evaluation\\nThere has been a signif...\"}"}
{"id": "conneau22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"significant body of work on building trusted multilingual evaluation datasets for speech. IARPA introduced BABEL [15] for evaluating speech models in low-resource languages. This dataset has been widely used in the speech community and covers real-world conversational telephone speech in 17 African and Asian low-resource languages. Recent work revived this dataset with different preprocessing [16, 17, 18, 4]. The CommonVoice effort [19] offers a wide coverage of speech recognition data in more than 70 languages, with read speech of Wikipedia and other sentences. CommonVoice has been used namely for phoneme recognition [20]. The Multilingual LibriSpeech [6] dataset extends the classical LibriSpeech task [21] to seven other European languages. VoxPopuli builds semi-supervised learning data from European Parliament session [7] in 23 languages, and includes speech transcriptions and translations for 16 languages, as well as speech-to-speech translations. With more than 400k hours of unlabeled speech, VoxPopuli is also used as a public pre-training corpus [5, 14].\\n\\nIn speech-to-text translation, CoVoST-2 [8] has become one of the go-to datasets for multilingual evaluation, covering 21 language directions into English and English into 15 languages. Europarl-ST [22], Must-C [23] and mTEDX [24] also provide common evaluation of speech translation. LangID can be evaluated using VoxLingua107 [25] on YouTube data in 107 languages, and CMU Wilderness [26] on New Testament data in 700+ languages. Fleurs is a new multilingual speech understanding evaluation dataset in 102 languages.\\n\\n3. XTREME-S\\n\\nIn this section, we describe the design decisions we made that led to the choice of tasks, domains and languages for our benchmark. Then we describe task families and their corresponding datasets.\\n\\n3.1. Design principles\\n\\nGiven XTREME's goal of providing an accessible benchmark for the evaluation of cross-lingual transfer learning on a diverse and representative set of tasks and languages, we select the tasks and languages that make up the benchmark based on the following principles:\\n\\n- **Task difficulty**: Tasks should be sufficiently challenging that they are not saturated by the strongest existing baselines. The data should also be representative of the challenges faced by practitioners, under the constraint that the data should be publicly accessible.\\n\\n- **Diversity**: We aim for task, domain and language diversity. Tasks should be diverse and cover several domains to provide a reliable evaluation of model generalization and robustness to noisy naturally-occurring speech in different environments. Languages should be diverse to ensure that models can adapt to a wide range of linguistic and phonological phenomena. Language coverage should not be unnecessarily large so as to avoid cumbersome evaluations. We note that the tasks are focused particularly on linguistic aspects of speech, while nonlinguistic/paralinguistic aspects of speech relevant to e.g. speech synthesis or voice conversion are not evaluated.\\n\\n- **Data efficiency**: The training sets of XTREME-S range from a few hours to a few hundred hours of labeled data per language. This is a few-shot setting suited for low-resource understanding. XTREME-S strongly encourages data-efficient self-supervised representation learning.\\n\\n- **Training efficiency**: Tasks should be trainable with a reasonable amount of time (few days) and compute (few GPUs). We enforce that constraint by having datasets focused on few-shot learning (e.g. Fleurs or MLS). This is to make the benchmark accessible, in particular to practitioners working under resource constraints. We also minimize the number of required fine-tuning runs where we can, for instance by encouraging multilingual fine-tuning over monolingual fine-tuning.\\n\\n- **Monolingual data**: Unlabeled speech is available publicly through corpora already used in past work (e.g. MLS, VoxPopuli, CommonVoice). Unlabeled text data is available in all languages, for instance, through Common Crawl data as in the mC4 dataset. Speech data is however not abundant for all languages, so multilinguality is important to build strong representations for those languages.\\n\\n- **Accessibility**: Each task should be available under a permissive license that allows the use and redistribution of the data for research purposes. When needed, we provide scripts to download and easily reproduce the preprocessing steps. Tasks have also been selected based on their usage by pre-existing multilingual pre-trained models, for simplicity.\\n\\n- **Reproducibility**: We encourage submissions that leverage publicly available speech and text datasets. Users should detail which data they use. In general, we encourage settings that can be reproduced by the community, but also encourage the exploration of new frontiers for speech representation learning.\"}"}
{"id": "conneau22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Task               | Corpus      | Train | Dev  | Test  | Lang. | Fine-tune | Eval | Task Metric | Domain        |\\n|--------------------|-------------|-------|------|-------|-------|-----------|------|-------------|---------------|\\n| Speech recognition | FLEURS      | 999h  | 122h | 293h  | 102   | Multi     | 1    | CER         | Read-speech   |\\n|                    | MLS         | 80h   | 10h  | 10h   | 8     | Multi     | 1    | WER         | Read-speech   |\\n|                    | VoxPopuli   | 1300h | 240h | 240h  | 14    | Multi     | 1    | Euro Parl   |               |\\n| Speech translation | CoVoST-2    | 566h  | 144h | 153h  | 21    | Multi     | 1    | BLEU        | Read-speech   |\\n| Speech classification | FLEURS    | 999h  | 122h | 293h  | 102   | Multi     | 1    | LangID Acc. | Read-speech   |\\n|                    | Minds-14    | 2h    | 1h   | 1h    | 14    | Multi     | 1    | Intent Cl.  | E-banking     |\\n| Speech retrieval   | FLEURS      | 49h   | 6h   | 14h   | 5     | Either    | 1/5  | Mining P@K | Read-speech   |\\n\\nTable 1: Characteristics of the datasets in XTREME-S. We report the number of hours for each train, dev and test set, and the number of languages. We specify the type of fine-tuning (monolingual or multilingual), which coincides with the number of fine-tuning runs. We also include the task, the metric and the speech domain.\\n\\n### 3.2.2. Speech Translation (ST)\\n\\nFor speech translation, we use all the 21 language pairs into English from the CoVoST-2 dataset.\\n\\n### 3.2.3. Speech classification\\n\\nFor speech classification, we include LangID and intent classification. After hyperparameter tuning, we encourage reporting the average result over 5 random seeds.\\n\\nFleurs-LangID\\n\\nWe use Fleurs as a LangID dataset by using the same train, dev and test splits as used for ASR. We report over classification accuracy over the 102 languages.\\n\\nMinds-14\\n\\nMINDS-14 [39] is an intent classification task from spoken data. It covers 14 intents extracted from the e-banking domain, with spoken examples in 14 language varieties. We merge monolingual datasets into a single multilingual dataset, with a 30-20-50% train-dev-test split.\\n\\n### 3.3. Languages\\n\\nOur 102 languages cover various language families and geographical locations, from Western Europe/Americas, Eastern Europe, Central-Asia, Middle-East, North-Africa, Sub-Saharan Africa, South Asia, South-East Asia to CJK languages. We have 36 languages covered by at least two evaluation datasets. The language coverage provides a good estimate of the generalization ability of multilingual models.\\n\\n### 4. Results\\n\\nIn this section, we describe our baselines and the corresponding results. We also comment on the specificities of each downstream task and offer remarks on how results can be improved.\\n\\n#### 4.1. Baselines\\n\\nWe present two baselines. The first is a 600M parameter speech-only pre-trained wav2vec-BERT model trained on 429k unlabeled data in 51 languages from VoxPopuli, MLS, CommonVoice and BABEL, similar to XLS-R. The second is the 600m parameter mSLAM speech-text pre-trained model that leverages the same speech data, as well more than 10TiB of unlabeled text data from mC4 and some ASR supervision. More details on these baselines, including fine-tuning details can be found in [14]. For some tasks, we also report results of the XLS-R models from [5]. If capacity constraints become an issue, we encourage practitioners to use same-capacity apples-to-apples comparisons with the smaller XLS-R (0.3B) and w2v-bert-51 (0.6B) models.\\n\\n#### 4.2. Speech recognition\\n\\nIn Table 2, we report average unit and word error rates on Fleurs, MLS and VoxPopuli. Pre-trained models obtain strong performance across domains and on both high-data regimes datasets.\"}"}
{"id": "conneau22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Table of results for XTREME-S.\\n\\n4.3. Speech translation\\n\\nFor most low-resource languages, only a couple of hours are available as supervision. Specifically, w2v-bert-51 (0.6B) obtains 13.4 and mSLAM obtains 15.6 average BLEU on low-resource languages, 35.6 and 36.3 on high-resource languages. Overall, those models obtain 20.4 and 20.6 average BLEU respectively on all languages (see Table 2). On this dataset, only one multilingual fine-tuning run is done to simplify the evaluation.\\n\\n4.4. Speech classification\\n\\nWe report our baselines on the two speech classification datasets in Table 2. We see that the mSLAM model obtains the best performance overall. Each of these datasets only require a single fine-tuning run; we build a multilingual training set from Minds-14 to reduce its inherent variance.\\n\\nOn Minds-14, mSLAM obtains around 86.9% accuracy, and 73.3% accuracy on Fleurs LangID, while w2v-bert-51 (0.6B) obtains 82.7 and 71.4 respectively. We note that on Fleurs-LangID, speakers are different between train sets and dev/test sets. Avoiding overfitting on speaker ID for the LangID task is essential for obtaining good performance.\\n\\n5. Discussion\\n\\nIn this section, we discuss several components of the XTREME-S benchmark.\\n\\nOn test sets:\\nTest sets are available in open-source and are not hidden to the public. We trust practitioners to perform all hyperparameter search and checkpoint selection on the dev set, and eventually report performance on the test set. Results are however double-checked through the submission of the predictions of the model for each task.\\n\\nOn speech data:\\nWe encourage the community to use similar unlabeled speech datasets across submissions when possible to encourage apple-to-apple comparisons across models. We do encourage submissions that also use different unlabeled speech, although preferably only in the case where there is a substantial difference (e.g. much smaller or much larger, or from more diverse sources, or using TTS-augmented data etc). Additional unlabeled speech data can be used for pre-training but also for self-training and other methods.\\n\\nOn text data:\\nThe mC4 and Wikipedia datasets should cover all the languages of the XTREME-S benchmark, including low-resource ones. We encourage the use of these datasets for learning language models, for training text-augmented speech models, or using TTS augmentation for example. We hope the community can also develop smarter ways to adapt these very large unlabeled text datasets to each particular task and domain through filtering methods.\\n\\nOn language modeling:\\nThe use of language model decoding is allowed. When using LMs, results should also be reported without LM fusion for comparison. The dataset and the type of LM used should be explicitly detailed in submissions and papers for reproducibility. When doing smart filtering of unlabeled text data, the technique should be explained clearly and the data released in open-source when possible.\\n\\nOn the average score:\\nWe weight differently each task of the XTREME-S benchmark. Speech recognition and translation each have a weight of 40%, and speech classification has a weight of 20%. The average score is computed in the following way:\\n\\n$$\\\\text{Average Score} = 0.4 \\\\times (100 - \\\\text{Fleurs} + \\\\text{MLS} + \\\\text{VoxPopuli}) + 0.4 \\\\times \\\\text{CoV oST-2} + 0.2 \\\\times (\\\\text{F-LID} + \\\\text{M-14})$$\\n\\nThis is to give more importance to the core recognition and translation tasks.\\n\\n6. Conclusion\\n\\nWe presented XTREME-S, an evaluation benchmark meant to evaluate the generalization ability of multilingual speech pretrained models. The benchmark consists of four key task types: recognition, translation, classification and retrieval. In total, XTREME-S covers 102 languages with various language families, from high-resource to low-resource, and different scripts. Tasks cover several domains and data regimes, from a few hours of supervision to more than a thousand hours, and are all directly open-sourced and made easily accessible. We presented two baselines: one speech-only pretrained model and one speech-text pretrained model that obtain strong results on each task. We also built a new dataset named Fleurs, in 102 languages, covering many low-resource languages. We hope XTREME-S will enable the community to build better speech representations in many languages, and enable rapid access to data-efficient speech technology for all the world's languages.\"}"}
