{"id": "rennie22_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] J. Vettin and D. Todt, \\\"Laughter in conversation: Features of occurrence and acoustic structure,\\\" Journal of Nonverbal Behavior, vol. 28, no. 2, pp. 93\u2013115, 2004.\\n\\n[2] J. Raclaw and C. E. Ford, \\\"Laughter and the management of divergent positions in peer review interactions,\\\" Journal of Pragmatics, vol. 113, pp. 1\u201315, 2017.\\n\\n[3] H. Kangasharju and T. Nikko, \\\"Emotions in organizations: Joint laughter in workplace meetings,\\\" The Journal of Business Communication (1973), vol. 46, no. 1, pp. 100\u2013119, 2009.\\n\\n[4] B. Schuller, S. Steidl, A. Batliner, F. Burkhardt, L. Devillers, C. M\u00fcller, and S. Narayanan, \\\"Paralinguistics in speech and language\u2014state-of-the-art and the challenge,\\\" Computer Speech & Language, vol. 27, no. 1, pp. 4\u201339, 2013.\\n\\n[5] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. R. Scherer, F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi, M. Mortillaro, H. Salamin, A. Polychroniou, F. Valente, and S. Kim, \\\"The INTERSPEECH 2013 Computational Paralinguistics Challenge: Social Signals, Conflict, Emotion, Autism,\\\" in INTERSPEECH 2013: 14th Annual Conference of the International Speech Communication Association, Lyon, France, Aug. 2013. [Online]. Available: https://hal.sorbonne-universite.fr/hal-02423147\\n\\n[6] B. Schuller, S. Steidl, A. Batliner, P. B. Marschik, H. Baumeister, F. Dong, S. Hantke, F. B. Pokorny, E.-M. Rathner, K. D. Bartl et al., \\\"The interspeech 2018 computational paralinguistics challenge: atypical and self-assessed affect, crying and heart beats,\\\" 2018.\\n\\n[7] A. Vinciarelli, P. Chatziioannou, and A. Esposito, \\\"When the words are not everything: The use of laughter, fillers, back-channel, silence, and overlapping speech in phone calls,\\\" Frontiers in ICT, vol. 2, 2015. [Online]. Available: https://www.frontiersin.org/article/10.3389/fict.2015.00004\\n\\n[8] R. Gupta, K. Audhkhasi, S. Lee, and S. Narayanan, \\\"Detecting paralinguistic events in audio stream using context in features and probabilistic decisions,\\\" Computer Speech & Language, vol. 36, pp. 72\u201392, 2016.\\n\\n[9] G. Hagerer, N. Cummins, F. Eyben, and B. W. Schuller, \\\"Did you laugh enough today?\u2014deep neural networks for mobile and wearable laughter trackers.\\\" in INTERSPEECH, 2017, pp. 2044\u20132045.\\n\\n[10] G. Gosztolya, B. Andr\u00e1s, T. Neuberger, and T. L\u00e1szl\u00f3, \\\"Laughter classification using deep rectifier neural networks with a minimal feature subset,\\\" Archives of Acoustics, vol. 41, no. 4, pp. 669\u2013682, 2016.\\n\\n[11] B. Schuller, S. Steidl, A. Batliner, A. Vinciarelli, K. Scherer, F. Ringeval, M. Chetouani, F. Weninger, F. Eyben, E. Marchi et al., \\\"The interspeech 2013 computational paralinguistics challenge: Social signals, conflict, emotion, autism,\\\" in Proceedings INTERSPEECH 2013, 14th Annual Conference of the International Speech Communication Association, Lyon, France, 2013.\\n\\n[12] R. Brueckner and B. Shulter, \\\"Social signal classification using deep blstm recurrent neural networks,\\\" in 2014 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2014, pp. 4823\u20134827.\\n\\n[13] R. Gupta, K. Audhkhasi, S. Lee, and S. S. Narayanan, \\\"Paralinguistic event detection from speech using probabilistic time-series smoothing and masking.\\\" in Interspeech, 2013, pp. 173\u2013177.\\n\\n[14] L. Kaushik, A. Sangwan, and J. H. Hansen, \\\"Laughter and filler detection in naturalistic audio,\\\" 2015.\\n\\n[15] M. G\u00f3sy, \\\"Bea\u2014a multifunctional hungarian spoken language database,\\\" Phonetician, vol. 105, pp. 50\u201361, 2013.\\n\\n[16] C. Cieri, D. Miller, and K. Walker, \\\"From switchboard to fisher: Telephone collection protocols, their uses and yields,\\\" in Eighth European Conference on Speech Communication and Technology, 2003.\\n\\n[17] R. J. Volkema, G. and H. Ronald, \\\"The influence of cognitive-based group composition on decision-making process and outcome,\\\" Journal of Management Studies, vol. 35, no. 1, pp. 105\u2013121, 1998.\\n\\n[18] R. Lenain, J. Weston, A. Shivkumar, and E. Fristed, \\\"Surfboard: Audio feature extraction for modern machine learning,\\\" 2020.\\n\\n[19] H. Inaguma, K. Inoue, M. Mimura, and T. Kawahara, \\\"Social signal detection in spontaneous dialogue using bidirectional lstm-ctc.\\\" in Interspeech, 2017, pp. 1691\u20131695.\\n\\n[20] L. S. Kennedy and D. P. Ellis, \\\"Laughter detection in meetings,\\\" 2004.\\n\\n[21] M. T. Knox and N. Mirghafori, \\\"Automatic laughter detection using neural networks.\\\" in Interspeech, 2007, pp. 2973\u20132976.\\n\\n[22] M. T. Knox, N. Morgan, and N. Mirghafori, \\\"Getting the last laugh: Automatic laughter segmentation in meetings,\\\" in Ninth Annual Conference of the International Speech Communication Association, 2008.\\n\\n[23] S. Skansi, Introduction to Deep Learning: from logical calculus to artificial intelligence. Springer, 2018.\\n\\n[24] A. Fern\u00e1ndez, S. Garc\u00eda, F. Herrera, and N. V. Chawla, \\\"Smote for learning from imbalanced data: Progress and challenges, marking the 15-year anniversary,\\\" Journal of Artificial Intelligence Research, vol. 61, pp. 863\u2013905, 2018.\"}"}
{"id": "rennie22_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset\\n\\nGordon Rennie 1, Olga Perepelkina 2, Alessandro Vinciarelli 1\\n\\n1 University of Glasgow, G12 8QQ Glasgow, United Kingdom\\n2 Intel, Santa Clara, US\\n\\nGordon.Rennie@glasgow.ac.uk, Olga.Perepelkina@intel.com, Alessandro.Vinciarelli@glasgow.ac.uk\\n\\nAbstract\\n\\nLaughter is a common paralinguistic vocalization that has been shown to be used for controlling the flow of a conversation, nullifying previous statements, and managing conversations on delicate topics. Already there have been concerted efforts to develop methods for automatically detecting laughter in speech. Many of these studies use artificial datasets and report their model performance using the AUC metric. This paper replicates previous work on laughter detection on those artificial datasets and then extends them by validating the methods on a larger and more naturalistic dataset made up of 60 spontaneous conversations (120 speakers and roughly 12 hours of material in total) with the best performing model achieving an AUC of 90.39 \u00b1 1.10 (precision=13.99 \u00b1 4.09, recall=76.36 \u00b1 12.00, F1=23.06 \u00b1 4.99). The paper then goes on to discuss the shortcomings with the current standard comparison metric in the field of AUC and suggests alternatives which may aid in the comparison and understanding of method\u2019s effectiveness.\\n\\nIndex Terms: speech recognition, human-computer interaction, computational paralinguistics\\n\\n1. Introduction\\n\\nThis paper presents attempts to automatically detect laughter in naturalistic audio conversations. It builds on previous works in two manners. Firstly, it uses a larger and more naturalistic data corpus than most current papers in the field to further validate the methods created in previous studies. Secondly, it displays the issue with the current standard comparison metric used commonly in the field of laughter detection and suggests metrics which may be better suited to the task of displaying the effectiveness of laughter detection methods and paralinguistic detection methods more generally.\\n\\nParalanguage, specifically laughter, serves a multitude of purposes from nullifying a previous statement and helping control the flow of conversation [1], to being used to manage delicate conversation topics such as divergences in opinion [2], and can be used in meetings to create collegiality or end a discussion on a topic [3].\\n\\nIf computers are to engage in effective verbal communication with human's it is necessary for them to have an understanding of paralanguage given the central role it plays in human-to-human communication. This issue has long been recognised in the field of human-computer interaction and strides towards developing tools for computers to understand paralanguage have already been made. The INTERSPEECH challenge exemplifies this quickly growing field of research with research into recognising: age, gender, and affect in 2010 [4]; social signals, conflict, emotion and autism in 2013 [5]; and atypical and self-assessed affect, crying and heart beats in 2018 [6] to mention only a few challenges.\\n\\nPrevious studies have attempted to automatically detect laughter with varying degrees of success. A large amount of the previous work carried out in the field uses the SSPNet vocalisation corpus [7] (SVC) including: [8], [9], and [10]. The SVC was corpus created from the SSPNet Mobile corpus (SMC) [7] corpus. It was created for the 2013 Interspeech Challenge [5] and has been widely used in the field since to enable comparison across different methodologies. The most common metric used is the AUC-ROC due to this being used in the original challenge [11]. Results from these studies can be seen in Table: 1\\n\\nTable 1: Previous Studies Results for Laughter Detection in SSPNet Vocalisation Corpus\\n\\n| Study     | Model Type                      | AUC (in %) |\\n|-----------|---------------------------------|------------|\\n| [8]       | Context independent DNN         | 80.9       |\\n|           | Window-wise feature statistics + DNN | 86.4       |\\n|           | Minimum Mean Squared Error Filter | 94.2       |\\n| [9]       | LSTM RNN                        | 92.24      |\\n| [12]      | BLSTM                           | 94         |\\n| [13]      | DNN + Probabilistic Smoothing   | 93.3       |\\n\\nOther datasets used include: conversational telephony speech databases [14], UT-opinion [14], and the BEA Hungarian Spoken Language Database used by [10] and created by [15].\\n\\nAn issue with the SVC is that each audio clip is guaranteed to contain at least one laughter or filler event between 1.5s and 8.5s during each 11 second clip. The curated nature of the dataset aids in the task of laughter detection but reduces the validity of the experiments in relation to real world audio.\\n\\nSimilarly, the conversational telephony speech (CTS) database is created using a switchboard and fisher technique, there are limited details of the creation of the corpus used in [14] but a general overview of the technique can be found here [16]. Unfortunately, due to the lack of detail on the creation of the CTS database in [14], it is impossible to discern how naturalistic/spontaneous the audio elicited was.\\n\\nHowever, [14] also created a second corpus - the UT-opinion corpus. In this corpus participants were interviewed on 10 different topics by a researcher. A sub-set of the full UT-opinion corpus was used in their study as a naturalistic audio test set used to evaluate the system trained on the CTS dataset. This sub-corpus contained 1.45 hours of conversational data with 45 unique speakers, 164 laughter and 981 filler events. Although naturalistic audio the setting where the audio was created, an interview, is far from a natural conversation.\"}"}
{"id": "rennie22_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dataset produces a useful backdrop but fails to fully capture a spontaneous conversation. This dataset is also comparatively small, both in terms of total audio time and the number of target paralinguistic events.\\n\\nAnother corpus of note is the BEA Hungarian corpus, specifically the conversation module of the overall database. This module is created by three participants: an interviewer, a subject, and a third person. The interviewer chooses a topic of conversation from a wide variety, examples include: keeping pets in apartments, subway construction, and marriage vs co-habitation. This module has a total of 75 conversations with an average duration of 16 minutes and contains 332 laughter and 321 speech segments. The use of the interviewer to guide the conversation limits how spontaneous the speech of participants was and the total number of laughter events is low compared to other corpora.\\n\\nThe study presented here builds on these previous studies by first replicating the previous studies using the SVC. This replication to enable comparison with previous studies. The study then extends these results by applying the same techniques to the SSPNet Mobile corpus (SMC) [7]. The SMC is the parent data corpus of the SVC, with the clips used in the SVC being extracted from the SMC. The SMC represents a more difficult challenge given its size and challenges the BEA Hungarian corpus for quality in terms of the SMC's data generation method, two participants having a conversation independent of any experimenter influence, and quantity in terms of total number of paralinguistic events (SMC has around 6 times as many laughter events). Details of the SMC are in the section: 2.2.\\n\\nAfter examining the effectiveness of the best detection methods currently used in the field the paper then selected the two most effective methods and compared them using further metrics of precision, recall, and F1. This was done to demonstrate issues with using AUC as the standard comparison metric for laughter detection and to suggest better metrics to use in the future.\\n\\nThe rest of this article is organized as follows: Section 2 describes the data used in the experiments, Section 3 details the feature extraction, model architecture, and post-processing methods used to detect laughter, Section 4 reports on the results of the various detection methods on both datasets and the final Section 5 draws the conclusions.\\n\\n2. Data\\n\\nThis section describes the two corpora used in this paper. The SVC and the SMC. With the SVC being extracted from the SMC some details are omitted from the description of the SVC and are covered in the description of the SMC.\\n\\n2.1. SVC\\n\\nThe SVC was created by selecting clips from the SMC. Clips were taken from all 60 conversations in the SMC and so participant information remains the same and is detailed in the next section. The SVC consists of 2763 audio clips of non-overlapping speech. Each clip is 11 seconds in length and contains at least one laughter or filler event between 1.5 and 9.5 s. In total there are 1158 laughter events (Male=475, Female=683) in the corpus with a total duration of 18 min 11 sec of laughter in the corpus and an average laughter duration of 0.94 s \u00b10.70.\\n\\nThe full corpus length is 506 min 33 s. Meaning laughter composes 3.59% of the total audio data.\\n\\nThis corpus was used to enable the comparison of the methods used in this study against previous studies. Table: 1 shows the results from previous studies that used the SVC and their model's performance using the AUC statistic. This statistic was selected as it was the original metric used in the 2013 challenge [5] and has been used widely in the literature since.\\n\\n2.2. SMC\\n\\nThe SMC is composed of 60 phone conversations. 120 participants created the data through spontaneous speech centered around 'The winter survival task' [17]. In this scenario participants debate which items from a predefined list which can increase the chances of survival after a plane crash in a polar environment. The creators of the corpus selected the winter survival task as few people have knowledge of how to survive an extreme scenario such as the one presented to participants in the task. Indeed only one participant involved in the creation of the corpus had any experience meaning it is likely that the outcomes of the conversations tended to depend on social and psychological aspects rather than on the experience/knowledge of either speaker.\\n\\nThe participant gender divide was 57 male, 63 female and all participants were native English speakers. Participants' age ranged from 18-64. Most were current students or staff at university with only 16 being former students. To avoid the effects that seniority of position may have on the conversation participants were unaware of their conversation partner's background.\\n\\nThe total duration of the audio is 12 hours, 41 minutes and 55 seconds meaning the average per call was 12 minutes and 42 seconds per call. The total amount of laughter events in the corpus is 1,804. Participants laughed across the entirety of their conversation, on average, 15.03 \u00b113.80, the high standard deviation shows there is variability between speakers. Controlling for conversation time reduces this variation with speakers laughing per minute, on average, 3.57 \u00b13.38, with a maximum of 22.46 and a minimum of 0. These statistics once again show, on a frequency level, that participants highly differed in laughter events.\\n\\nThis paper's reported statistics on the dataset differs from previous paper's using the SMC due to the treatment of overlapping speech segments. In the SMC there were 5790 segments which contained overlapping speech. There is an independent recording of each participant's speech meaning there are clean audio recordings for each speaker's vocalization during overlapping speech. Therefore when overlapping speech occurred the individual recordings were used for feature extraction. This increased the total number of laughs as in the original studies two speakers laughing at the same time was counted as one laugh event whereas in the current paper the laughs were counted separately. This increased both number of laughs and the total time of the corpus. After the overlapping speech correction had been carried out laughter made-up 2.85% of the full corpus.\\n\\n3. Laughter Detection Method\\n\\nEight detection methods were used in this study. These methods differed in three ways, the underlying model architecture, the input feature vector and the post-processing carried out on the model outputs.\\n\\n3.1. Feature Extraction\\n\\nFeature vectors were extracted from audio with a sample rate of 44,100Hz, a window of 1024 samples (roughly 23.22ms) and a time step of 512 samples (roughly 11.61ms). Both of these\"}"}
{"id": "rennie22_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"values are standard in the literature [8] and no fine-tuning was attempted in this study. Each resulting window had the following features extracted from it: the first 13 Mel Frequency Cepstral Coefficients (MFCC), the signal intensity, the Root Mean Square sample values and the fundamental frequency contour (features were extracted using the Python library Surfboard v0.2.0 [18]). This resulted in each window creating a feature vector of dimension 16.\\n\\nBefore training and testing each feature was normalised to a scale from 0-1. The normalization function was adjusted based on the training data feature values and applied to both the training and the testing data.\\n\\n3.2. Input Feature Vector Including Time Dependant Information\\n\\nTime dependant information is recognised as key for identifying paralinguistic events in speech [8] [19] [20] [21] [22]. In this study feature vectors was expanded to include the $\\\\Delta$ of each of the 16 features. This $\\\\Delta$ was the difference between the current frame's values and the immediately previous frame's. This means detection methods using the time dependant information had feature vectors with a dimension of 32.\\n\\n3.3. Post-Processing\\n\\nIt was observed that many spurious frames were classified as laughter, spurious detection was characterised by spikes in the output posterior probability. To remove these spikes some detection methods applied a Hamming window convolution to the output of the models, the hamming window was 51 frames wide corresponding to around 600ms, just over half the average duration of a laughter event.\\n\\n3.4. Model Architecture\\n\\nThe following section outlines the two main underlying architectures of the detection methods used.\\n\\n3.4.1. Feedforward Neural Network Architecture\\n\\nAll neural networks were created using the Python library Keras (version 2.5.0). In the present study the first four laughter detection models shared a similar neural network architecture with only the input layer differing between them. The initial model had 16 input nodes (32 in the case of detection methods using vectors expanded with $\\\\Delta$ values). There were two hidden layers, both with 8 nodes each, these hidden layers use a Relu activation [23]. Finally, there is a single output node which uses Sigmoid activation [23].\\n\\nIn all methods using the neural network architecture, class weighting was used to increase the calculated loss for incorrect predictions on feature vectors of the target class during training. This was done as the datasets in both the SVC and the SMC are highly imbalanced. Utilising class weights ensures that during training, mistakes in classifying the underrepresented class (in this case laughter) are penalized more heavily. Class weights were calculated based on the distribution of each class in the training set.\\n\\n3.4.2. LSTM network Architecture\\n\\nIn the LSTM portion of the work all detection methods once again shared a similar network architecture with only the input size differing between them. The LSTM networks was composed of 1 input layer, 3 hidden layers and one output layer.\\n\\nThe input layer for detection methods which did not include the time dependant information the model took the target feature vector and the nine immediately prior feature vectors, meaning it had 160 inputs. For detection methods involving the time dependant information there were double the input nodes (n=320). The first two hidden layers were composed of 25 densely connected standard neural network nodes while the third layer was composed of 15 LSTM nodes all using RELU activation. The output layer contained a single output node which used Sigmoid activation.\\n\\n3.5. Training and Testing\\n\\nA k-fold approach was used for creation of training and testing sets, 6 folds were created by splitting the data at a conversational level, each training set contained 50 conversations and each testing set contained 10. This was to ensure that no speaker was represented in both the training and testing dataset. The average number of laughs in a training set per fold was 1503 (SD=63.54) with a average total duration of laughter of 1084.84 s (SD=64.55). The average total audio time per training set was 679 minutes and 25.5 s (SD=14 minutes 51 s). The average number of laughs in a test set was 300.67 (SD=63.54) with a average total duration of laughter of 216.97 s (SD=64.55s). The average total audio duration of a test set was 135 minutes 53 s (SD=14 minutes 51 s). To improve upon the reliability of the results each detection method experiment was repeated three times, each time initialising random underlying models, before training and testing. Results are presented as the average over 6 folds and 3 repetitions.\\n\\n4. Results and Discussion\\n\\nTable: 2 shows the results of the eight detection methods on the SVC and SMC data corpus.\\n\\n| Detection Method          | SVC (AUC% \u00b1 SD) | SMC (AUC% \u00b1 SD) |\\n|---------------------------|------------------|-----------------|\\n| Neural Network            | 83.49 \u00b1 2.71     | 84.68 \u00b1 1.02    |\\n| NN with Delta             | 83.50 \u00b1 3.09     | 84.19 \u00b1 1.81    |\\n| NN with Hamming window    | 89.76 \u00b1 2.93     | 90.39 \u00b1 1.10    |\\n| NN with Hamming window and Delta | 89.35 \u00b1 3.23 | 89.75 \u00b1 1.25    |\\n| LSTM                      | 86.43 \u00b1 3.30     | 85.42 \u00b1 2.33    |\\n| LSTM with Delta           | 86.95 \u00b1 3.02     | 84.18 \u00b1 2.18    |\\n| LSTM with Hamming window  | 92.50 \u00b1 3.32     | 90.11 \u00b1 1.56    |\\n| LSTM with Hamming window and Delta | 92.86 \u00b1 2.72 | 89.19 \u00b1 1.96    |\\n\\nThe results in this study on the SVC mirror previous studies where the inclusion of time dependant information improved results with both LSTM based network architectures and post processing methods which rely on time dependant information yielding the best results [8] [9] [12] [14]. Results in this study approach the gold standard seen in the field so far on the SVC with the use of the LSTM network, extended feature vectors and the post processing Hamming window applied reaching close to the current maximum that the authors are aware of 94.2% [8]. In the case of the SMC the best result came from using the NN architecture, with the original feature vector and the post processing Hamming window applied.\\n\\nDespite the increase in difficulty from the SVC to the SMC the results show that approaches developed and refined on the SVC are effective on the SMC. This adds validation to the current automatic laughter detection methods in the field. It is important to note that both standard neural networks and LSTM\"}"}
{"id": "rennie22_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"based neural networks achieved similar levels of AUC performance and had similar ROC curves as shown in Figure 1. This would suggest that both architectures have a similar effectiveness. However, upon further investigation it became obvious that the two detection schemes differed in their ability and manner of laughter detection.\\n\\n**Figure 1:** ROC curves across 6 folds and 3 repetitions for method 3 (left) and method 5 on the SVC.\\n\\nTables 4 and 3 show the precision, recall, and F1 scores for each detection method on both the SMC and SVC datasets. A clear difference can be seen between the two underlying model architectures with NN based detection methods achieving relatively high recall with poor precision and LSTM based detection methods the opposite. On its own this is an important point as it shows the inadequacy of the descriptive powers of the AUC metric and the need for the field to adopt better metrics, such as precision, recall, and F1.\\n\\nA potential cause for the difference in performance on precision and recall can be seen by comparing the laughter events that each detection method detects and misses. Each feature vector was tagged with the original event it belonged to in the corpus. If each event which contained at least one feature vector classified as laughter by the detection method is considered as detected the comparison between detected and missed events can be made. Taking the best F1 result for each architecture on the SVC (methods 3 and 5) a potential cause of the difference in architecture can be seen. Detection method 5 detected events which were on average $1.02 \\\\pm 0.73$ s long whereas the average duration of missed laughter events was $0.69 \\\\pm 0.54$ s. Detection method 3 detected events which were on average $0.97 \\\\pm 0.71$ s long and missed events which were $0.50 \\\\pm 0.40$ s long. Although the average duration of detected events is relatively similar the difference in the methods is shown by what they miss. The average missed event for an LSTM based method is longer (by $0.19$ s) than for the NN based method. This suggests LSTMs find detecting shorter events more difficult and suggests why the difference in precision and recall is seen.\\n\\n**Table 3:** Average Precision, Recall, and F1 for each detection method on the SVC. Bold highlights best F1. HM = Hamming Window\\n\\n| Detection Method | Precision $\\\\pm$ Std | Recall $\\\\pm$ Std | F1 $\\\\pm$ Std |\\n|------------------|---------------------|-----------------|--------------|\\n| Neural Network   | 11.70 $\\\\pm$ 2.33   | 71.64 $\\\\pm$ 6.64 | 19.98 $\\\\pm$ 3.41 |\\n| NN with $\\\\Delta$ | 11.47 $\\\\pm$ 2.85   | 72.51 $\\\\pm$ 7.54 | 19.60 $\\\\pm$ 4.20 |\\n| NN with HM       | 18.29 $\\\\pm$ 4.91   | 76.53 $\\\\pm$ 7.68 | 29.04 $\\\\pm$ 5.97 |\\n| NN with HM and $\\\\Delta$ | 17.75 $\\\\pm$ 5.78 | 76.77 $\\\\pm$ 9.28 | 28.12 $\\\\pm$ 7.45 |\\n| Long-Short Term Memory | 65.84 $\\\\pm$ 12.80 | 21.81 $\\\\pm$ 9.93 | 30.56 $\\\\pm$ 11.18 |\\n| LSTM with $\\\\Delta$ | 67.04 $\\\\pm$ 13.93 | 21.51 $\\\\pm$ 11.31 | 29.57 $\\\\pm$ 11.08 |\\n| LSTM with HM    | 78.28 $\\\\pm$ 20.91 | 14.23 $\\\\pm$ 9.48 | 22.75 $\\\\pm$ 13.38 |\\n| LSTM with HM and $\\\\Delta$ | 84.87 $\\\\pm$ 10.16 | 13.72 $\\\\pm$ 11.34 | 21.33 $\\\\pm$ 14.66 |\\n\\n**Table 4:** Average Precision, Recall, and F1 for each detection method on the SMC. Bold highlights best F1. HM = Hamming Window\\n\\n| Detection Method | Precision $\\\\pm$ Std | Recall $\\\\pm$ Std | F1 $\\\\pm$ Std |\\n|------------------|---------------------|-----------------|--------------|\\n| Neural Network   | 9.75 $\\\\pm$ 1.93     | 74.95 $\\\\pm$ 10.34 | 17.13 $\\\\pm$ 3.09 |\\n| NN with $\\\\Delta$ | 10.40 $\\\\pm$ 2.45    | 72.05 $\\\\pm$ 13.56 | 17.86 $\\\\pm$ 3.60 |\\n| NN with HM       | 13.99 $\\\\pm$ 4.09    | 76.37 $\\\\pm$ 12.00 | 23.06 $\\\\pm$ 4.99 |\\n| NN with HM and $\\\\Delta$ | 14.47 $\\\\pm$ 4.57 | 73.29 $\\\\pm$ 15.84 | 22.99 $\\\\pm$ 3.92 |\\n| Long-Short Term Memory | 59.18 $\\\\pm$ 13.56 | 6.67 $\\\\pm$ 4.89  | 11.30 $\\\\pm$ 7.60 |\\n| LSTM with $\\\\Delta$ | 56.52 $\\\\pm$ 19.44  | 2.88 $\\\\pm$ 1.95  | 5.37 $\\\\pm$ 3.49 |\\n| LSTM with HM    | 78.70 $\\\\pm$ 22.97  | 2.99 $\\\\pm$ 2.99  | 5.58 $\\\\pm$ 5.40 |\\n| LSTM with HM and $\\\\Delta$ | 62.49 $\\\\pm$ 40.45 | 5.42 $\\\\pm$ 0.63  | 1.07 $\\\\pm$ 1.23 |\\n\\n**5. Conclusions** This paper demonstrated laughter detection in line with cutting edge results on the SMC (in terms of %AUC). To the authors' knowledge the SMC is the largest (in terms of number of laughter paralinguistic events) and most naturalistic conversational (in terms of the setting in which the data was gathered) dataset in the literature. The paper shows there were minimal differences in various detection methods' performance on the SVC compared with their performance on the SMC thus lending validation and reliability to the work in the field to date.\\n\\nThe paper then examined the metrics commonly used in the field and showed shortcomings in AUC. When reading the current state of the art papers in the field of automatic detection of laughter the AUC metric unfortunately suggests that both LSTM and neural network based architecture perform with similar effectiveness. This paper shows this not to be the case and suggests that the AUC metric is not fit for purpose for evaluating performance. This is because of the common issue in the field of highly imbalanced datasets. In these highly imbalanced cases, correctly identifying the rare target events is the goal and comparison and reporting of models should enable a reader to understand how well a detection method performs on detecting laughter in the soup of conversation.\\n\\nPrecision, recall, and F1 give a much better indication of how well methods perform on laughter detection. As such this paper suggests that AUC is a poor metric for examining the effectiveness of a given detection method and papers in the field should include further metrics such as precision, recall, and F1. Future work will focus on attempting to improve the detection results in terms of precision, recall, and F1. Methods such as data augmentation, over-sampling of the minority class using for example the SMOTE technique [24], could improve the training of models. Ensemble models could be used to exploit the advantages of both architecture bases, using LSTMs to detect longer events and NN to detect shorter. This could be done by either having multiple models classifying concurrently and a composite score being taken or having follow up models which filter results. Finally, networks can be extended by increasing the number of nodes/layers. Here relatively small networks were created to decrease training time and enable as many methods to be experimented with as possible. In future the most promising detection methods could be built using larger networks with more layers and nodes per layer.\\n\\n**6. Acknowledgements** The work in this article was supported by United Kingdom Research and Innovation through the UKRI Centre for Doctoral Training in Socially Intelligent Artificial Agents (EP/S02266X/1).\"}"}
