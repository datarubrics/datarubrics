{"id": "turetzky24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] A. Baevski, Y. Zhou, A. Mohamed, and M. Auli, \u201cwav2vec 2.0: A framework for self-supervised learning of speech representations,\u201d Advances in neural information processing systems, vol. 33, pp. 12 449\u201312 460, 2020.\\n\\n[2] P. K. Rubenstein, C. Asawaroengchai, D. D. Nguyen, A. Bapna, Z. Borsos, F. d. C. Quitry, P. Chen, D. E. Badawy, W. Han, E. Kharitonov et al., \u201cAudiopalm: A large language model that can speak and listen,\u201d arXiv preprint arXiv:2306.12925, 2023.\\n\\n[3] C. Wang, Y. Wu, Y. Qian, K. Kumatani, S. Liu, F. Wei, M. Zeng, and X. Huang, \u201cUnispeech: Unified speech representation learning with labeled and unlabeled data,\u201d in International Conference on Machine Learning. PMLR, 2021, pp. 10 937\u201310 947.\\n\\n[4] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: An ASR corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[5] J. Kahn, M. Rivi\u00e8re, W. Zheng, E. Kharitonov, Q. Xu, P.-E. Mazar\u00e8, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen et al., \u201cLibri-light: A benchmark for ASR with limited or no supervision,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7669\u20137673.\\n\\n[6] C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux, \u201cVoxpopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation,\u201d arXiv preprint arXiv:2101.00390, 2021.\\n\\n[7] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28 492\u201328 518.\\n\\n[8] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., \u201cScaling speech technology to 1,000+ languages,\u201d arXiv preprint arXiv:2305.13516, 2023.\\n\\n[9] Y. Zhang, W. Han, J. Qin, Y. Wang, A. Bapna, Z. Chen, N. Chen, B. Li, V. Axelrod, G. Wang et al., \u201cGoogle usm: Scaling automatic speech recognition beyond 100 languages,\u201d arXiv preprint arXiv:2303.01037, 2023.\\n\\n[10] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023.\\n\\n[11] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar et al., \u201cVoicebox: Text-guided multilingual universal speech generation at scale,\u201d Advances in neural information processing systems, vol. 36, 2024.\\n\\n[12] C. K. Reddy, H. Dubey, V. Gopal, R. Cutler, S. Braun, H. Gamper, R. Aichner, and S. Srinivasan, \u201cIcassp 2021 deep noise suppression challenge,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6623\u20136627.\\n\\n[13] A. Defossez, G. Synnaeve, and Y. Adi, \u201cReal time speech enhancement in the waveform domain,\u201d arXiv preprint arXiv:2006.12847, 2020.\\n\\n[14] Y. Dissen, F. Kreuk, and J. Keshet, \u201cSelf-supervised Speaker Diarization,\u201d in Proc. Interspeech 2022, 2022, pp. 4013\u20134017.\\n\\n[15] H. Yadav and S. Sitaram, \u201cA survey of multilingual models for automatic speech recognition,\u201d arXiv preprint arXiv:2202.12576, 2022.\\n\\n[16] T. Likhomanenko, Q. Xu, V. Pratap, P. Tomasello, J. Kahn, G. Avidov, R. Collobert, and G. Synnaeve, \u201cRethinking evaluation in ASR: Are our models robust enough?\u201d arXiv preprint arXiv:2010.11745, 2020.\\n\\n[17] L. Campbell, \u201cEthnologue: Languages of the world,\u201d 2008.\\n\\n[18] A. Petrov, E. La Malfa, P. Torr, and A. Bibi, \u201cLanguage model tokenizers introduce unfairness between languages,\u201d Advances in Neural Information Processing Systems, vol. 36, 2024.\\n\\n[19] A. W. Black, \u201cCmu wilderness multilingual speech dataset,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 5971\u20135975.\\n\\n[20] V. Pratap, Q. Xu, A. Sriram, G. Synnaeve, and R. Collobert, \u201cMls: A large-scale multilingual dataset for speech research,\u201d arXiv preprint arXiv:2012.03411, 2020.\\n\\n[21] A. Conneau, M. Ma, S. Khanuja, Y. Zhang, V. Axelrod, S. Dalmia, J. Riesa, C. Rivera, and A. Bapna, \u201cFleurs: Few-shot learning evaluation of universal representations of speech,\u201d in 2022 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2023, pp. 798\u2013805.\\n\\n[22] S. Izre\u2019el, B. Hary, and G. Rahav, \u201cDesigning cosih: the corpus of spoken Israeli Hebrew,\u201d International Journal of Corpus Linguistics, vol. 6, no. 2, pp. 171\u2013197, 2001.\\n\\n[23] J. Azogui, A. Lerner, and V. Silber-Varod, \u201cThe open university of Israel map task corpus (matacop),\u201d 2016.\\n\\n[24] M. Marmorstein and N. Matalon, \u201cThe huji corpus of spoken Hebrew: An interaction-oriented design of a corpus,\u201d 2022.\\n\\n[25] O. Sharoni, R. Shenberg, and E. Cooper, \u201cSaspeech: A Hebrew single speaker dataset for text to speech and voice conversion,\u201d in Proc. Interspeech, 2023.\\n\\n[26] Y. Marmor, K. Misgav, and Y. Lifshitz, \u201cIVRIT. AI: A comprehensive dataset of Hebrew speech for AI research and development,\u201d arXiv preprint arXiv:2307.08720, 2023.\\n\\n[27] C. Commons, \u201cCreative commons attribution 4.0 international public license.\u201d\\n\\n[28] S. Team, \u201cSilero vad: Pre-trained enterprise-grade voice activity detector (VAD), number detector and language classifier,\u201d 2021.\\n\\n[29] J. Sohn, N. S. Kim, and W. Sung, \u201cA statistical model-based voice activity detection,\u201d IEEE signal processing letters, vol. 6, no. 1, pp. 1\u20133, 1999.\\n\\n[30] V. Pratap, A. Tjandra, B. Shi, P. Tomasello, A. Babu, S. Kundu, A. Elkahky, Z. Ni, A. Vyas, M. Fazel-Zarandi et al., \u201cScaling speech technology to 1,000+ languages,\u201d arXiv preprint arXiv:2305.13516, 2023.\\n\\n[31] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[32] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100, 2020.\\n\\n[33] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Granger, and M. Auli, \u201cfairseq: A fast, extensible toolkit for sequence modeling,\u201d arXiv preprint arXiv:1904.01038, 2019.\\n\\n[34] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\\n\\n[35] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu, and R. Pang, \u201cConformer: Convolution-augmented Transformer for Speech Recognition,\u201d in Proc. Interspeech 2020, 2020, pp. 5036\u20135040.\\n\\n[36] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin, \u201cAttention is all you need,\u201d Advances in neural information processing systems, vol. 30, 2017.\\n\\n[37] C. Kim, D. Gowda, D. Lee, J. Kim, A. Kumar, S. Kim, A. Garg, and C. Han, \u201cA review of on-device fully neural end-to-end automatic speech recognition algorithms,\u201d in 2020 54th Asilomar Conference on Signals, Systems, and Computers. IEEE, 2020, pp. 277\u2013283.\"}"}
{"id": "turetzky24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"HEBDB: a Weakly Supervised Dataset for Hebrew Speech Processing\\n\\nArnon Turetzky1, Or Tal1, Yael Segal-Feldman2, Yehoshua Dissen2, Ella Zeldes1, Amit Roth1, Eyal Cohen2, Yosi Shrem2, Bronya R. Chernyak2, Olga Seleznova2, Joseph Keshet2, Yossi Adi1\\n\\n1The Hebrew University of Jerusalem, Israel\\n2Technion, Israel Institute of Technology\\narnon.turetzky@mail.huji.ac.il\\n\\nAbstract\\n\\nWe present HEBDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HEBDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HEBDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HEBDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.\\n\\nIndex Terms: Automatic Speech Recognition, Speech Benchmark, Hebrew Speech Technologies\\n\\n1. Introduction\\n\\nSpoken language technologies have seen a great leap in performance following the success of deep neural networks consisting of large-scale models [1, 2, 3] and datasets [4, 5, 6]. This includes Automatic Speech Recognition (ASR) [7, 8, 9], Text-to-Speech (TTS) [10, 11], speech enhancement [12, 13], speaker diarization [14], to name a few.\\n\\nA fundamental requirement in the success of the aforementioned models is optimization using large-scale datasets [1]. For instance, when considering ASR, Whisper [7] was trained using \u223c700k hours of speech utterances and Google USM was trained over \u223c12M hours of speech recordings. As for TTS, both VALL-E and VoiceBox were trained over 60k hours of speech. As a result, such big performance advancements are mainly kept for high-resource language in which large-scale datasets can be found.\\n\\nOne approach to mitigate the performance gaps between high-resource and low-resource languages is to train speech models considering multi-lingual setups [7, 8]. The authors in [15] empirically demonstrated the benefit of training multi-lingual spoken language processing models, while the authors in [8] specifically demonstrated the benefit of low-resource languages. Although such performance improvements are interesting and important, directly training models over large-scale benchmarks still achieve superior performance [16].\\n\\nThe Hebrew language is among the low-resource languages explored in prior work [7, 8]. The Hebrew language is being spoken by roughly 9 million people worldwide [17]. Besides the lack of large-scale datasets in Hebrew, the language syntax and structure impose some inheriting challenges, such as: (i) using non-Latin letters, which sets it apart from many languages; (ii) Traditional Hebrew has diacritics (\u201cNikud\u201d) while modern Hebrew writing rarely uses them. Such discrepancies impose a critical challenge on ASR and TTS systems which need to learn non-trivial pronunciations that directly affect word meanings. Such differences are not presented in writings but sound differently, and can be distinguished mainly based on context. For instance the word \u201cpita\u201d can have two meanings: bread and seduced based on the context; (iii) Hebrew is a morphologically rich language, with common use of prefixes and suffixes to modify words\u2019 meanings and to add prepositions. This property makes tokenization difficult and less efficient, especially under the multi-lingual setup [18].\\n\\nIn this study, we present HEBDB, a weakly supervised spontaneous speech dataset in the Hebrew language. HEBDB is comprised of \u223c2500 hours of in-the-wild natural speech consisting of a numerous number of speakers and diverse topics and vocabulary. We release both the raw recordings together with a pre-processed and weakly transcribed version. Additionally provide a transcription confidence score for each of the data samples, which can be used to develop strategies for fine-tuning considering different supervision qualities. In releasing this dataset, our goal is to advance research and development of Artificial Intelligence (AI) based tools for spoken language processing directly developed for the Hebrew language. To further enhance the development of such tools, we provide two baseline systems: (i) a self-supervised model; and (ii) a fully supervised ASR model. Full dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.\\n\\nThe paper is structured as follows. We start by reviewing datasets and speech processing tools directly dedicated to the Hebrew language in Section 2. Next, in Section 3 we provide a detailed description of HEBDB, its curation, statistics, pre-processing, and supervision quality assessment. In Section 4, we describe the baseline systems and compare their performance to current open-source tools. We conclude the paper in Section 5, where we outline future work along this research direction.\\n\\n2. Related work\\n\\nSpoken Hebrew benchmarks.\\n\\nAs Hebrew is considered a low-resource language, public spoken benchmarks hardly exist. Previous efforts in constructing datasets in Hebrew were either released under a multi-lingual benchmark [19, 20, 8, 21] or relatively small [22, 23, 24, 25]. The authors in [22] established the Corpus of Spoken Israeli Hebrew (CoSIH) with the...\"}"}
{"id": "turetzky24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Details & Statistics of HEBDB\u2019s raw recordings. We report the list of sources, sampling rates, number of channels, total duration (in hours), and indication of single or multiple speakers in a given source. Both sampling rates and channels are reported as a percentage.\\n\\n| Source                  | Sample rate (kHz) | # Channels | Duration (h) | Single / Multiple speakers |\\n|-------------------------|-------------------|------------|--------------|----------------------------|\\n| Geekonomy               | 44.1: 60%; 48: 40% | Stereo: 67%; Mono: 33% | 1146 | Multiple speakers          |\\n| Osim History            | 44.1: 95%; 48: 3%; 22.05: 2% | Stereo: 93%; Mono: 7% | 270  | Mostly single speaker     |\\n| The Dor Kahn Experience  | 44.1: 100%        | Stereo: 100% | 218          | Multiple speakers          |\\n| Yo! The podcast         | 44.1: 25%, 48: 75% | Stereo: 25%; Mono: 75% | 295          | Multiple speakers          |\\n| Good Question           | 16: 100%          | Stereo: 100% | 163          | Multiple speakers          |\\n| Yad vashem              | 48: 100%          | Stereo: 100% | 492          | Multiple speakers          |\\n\\nThe goal of compiling a large database of recordings of spoken Israeli Hebrew in order to facilitate and enhance research in the field. Next, the authors in [23] released The Map Task Corpus (MaTaCoP) of Hebrew dialogues. The authors in [24] collected naturally occurring speech and interaction in Modern Hebrew via telephone conversations during the years 2020\u20132021 and released the HUJI Corpus of Spoken Hebrew (HUJICorpus). More recently, the authors in [25] released SASPEECH, a high-quality single-speaker Hebrew dataset which is designed to enhance Hebrew speech synthesis research. Although all of these prior works are important and valuable, the provided benchmarks are relatively small. CoSIH contains approximately 12.3 hours of speech, the MaTaCoP corpus contains approximately 5.3 hours, the HUJICorpus has approximately 3.8 hours, and SASPEECH which is the largest one contains approximately 30 hours of speech. The most relevant concurrent work to ours is the great work done by [26], which released a dataset denoted as ivrit.ai. The authors released approximately 3300 hours of speech from local podcasts and provided the first large-scale dataset in Hebrew. We would like to state that the proposed benchmark is orthogonal to the release of ivrit.ai. We believe the community should leverage as many high-quality publicly available datasets as possible to close the gap between low- to high-resource languages. Additionally, unlike ivrit.ai, we release two baseline systems (SSL and supervised one) for speech processing and ASR.\\n\\n3. HEBDB dataset\\n\\nHEBD contains natural dialogues of spontaneous speech. It is comprised of both testimonies from World War II survivors and five podcasts covering a wide range of subjects and speakers. While the testimonies provide firsthand accounts of historical events, the majority of our dataset consists of podcasts covering diverse topics such as economy, politics, sports, culture, science, history, and music, to name a few. This combination of personal narratives and informative discussions offers a rich and varied resource for analysis and interpretation.\\n\\nWe provide two versions of the dataset: raw and pre-processed. The raw version contains over 2584 hours of in-the-wild audio in varying sample rates, recorded channels, and number of speakers. Table 1 provides a detailed description of this version including statistics. We release this version to allow researchers and practitioners to explore different pre-processing alternatives and methods.\\n\\nThe pre-processed version contains roughly 1690 hours of audio, down-sampled, segmented into multiple files, and automatically transcribed. This version is better suited for training acoustic models as is. We optimize and evaluate the proposed baseline systems using the pre-processed version only. In the next subsection, we provide a detailed description of the pre-processing pipeline used.\\n\\nBoth versions of HEBDB corpus are released under the very permissive of CC BY 4.0 license [27].\\n\\n3.1. Pre-processing\\n\\nThe raw recordings are constructed from full podcast episodes and testimonies and, hence, contain long audio sources and plenty of non-speech segments, e.g. music, environmental sounds, silence, etc. Such in-the-wild conditions make model optimization challenging and require a pre-processing step. To handle that, we apply the following pre-processing pipeline to the raw version of HEBDB. We first resample all the audio recordings to 16kHz, mono recordings, using julius [1] python package. Next, we apply a Voice Activity Detection (VAD) model to partition the waveform to sentences and discard empty and noisy parts. Lastly, we automatically transcribe the segmented speech utterances using a pre-trained ASR model.\\n\\nVoice activity detection and speech segmentation.\\n\\nWe use the silero-vad [28] to perform voice activity detection over the 16KHz audio files. Unlike traditional VAD models [29] that are based on Digital Signal Processing heuristics, the \u2018silero-vad\u2019 is a learning model based on convolutional and LSTM layers [30]. We specifically chose the silero-vad as it provides superior performance to other publicly available VAD models and was found beneficial in prior work [26, 25]. In general, VAD relies on frame-wise activity classification. Following that, to properly segment the audio into sentences, we need to calibrate a classification threshold over the model\u2019s frame-wise confidence scores and define a minimal duration of silence between activated segments. We follow this process as we wish to have a minimal number of words in each segment while keeping its length to fit in a processing unit memory. Specifically, we use a confidence threshold of 0.5 to filter out activated segments with a minimum duration of 1 second, separating audio segments by a minimal silence duration of 100 ms and padding both sides of the segmented audio with 30 ms of silence.\\n\\nTranscriptions.\\n\\nWe provide weak supervision in the form of\\n\\n1https://github.com/adefossez/julius\\n2https://github.com/snakers4/silero-vad\"}"}
{"id": "turetzky24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: Details of HEBDB\u2019s pre-processed version. We report the total duration (in hours) for each source together with statistics of the processed utterances (in seconds).\\n\\n| Source             | Duration (h) | Mean / Min / Max / Med (s) |\\n|--------------------|--------------|----------------------------|\\n| Geekonomy          | 99.8         | 4.7 / 1.0 / 282.1 / 3.2    |\\n| Osim History       | 22.39        | 4.4 / 1.1 / 152.2 / 3.1    |\\n| DKE                | 18.14        | 3.8 / 1.1 / 107.6 / 2.9    |\\n| Yo! the podcast    | 14.22        | 3.3 / 1.1 / 115.7 / 2.5    |\\n| Good Question      | 7.5          | 5.0 / 1.1 / 175.5 / 3.2    |\\n| Yad Vashem         | 6.74         | 5.3 / 1.0 / 361.1 / 4.4    |\\n\\nFigure 1: Score level histogram of the data filtering process. We use a threshold of 0.3 which filters roughly 13% of the data.\\n\\nWe leverage the pre-trained Whisper large-v2 (1.55B) version to transcribe all the segmented data. Although Whisper supports transcription in specific languages it might output non-Hebrew characters not limited to Latin. Analyzing the frequencies of character across our train set, most of the non-Hebrew chars were found in 1%, hence we removed those samples from our data. Additionally, Whisper might output a \\\\(<\\\\text{RTL}>\\\\) token, as this token is not relevant to the acoustics of the speech utterance we simply remove it from the text. For better alignment between acoustics and written text, we converted numbers and dates to words using the num2words package.\\n\\nLastly, Hebrew has 5 special letters with final form, we experimented with normalizing the final instances to regular ones and found it to be beneficial. Hence, we adjust the 5-gram LM provided by the MMS to normalize accordingly.\\n\\nStatistics. After the prepossessing step, we are left with \\\\(\\\\sim 1690\\\\) hours of speech partitioned into varied length segments with the vast majority of the segmented files having less than 10 seconds. Table 2 shows a further subdivision of processed audio with respect to each source separately. Figure 2 depicts a box plot for processed instances quartile distributions over audio duration in seconds and the number of transcribed words with respect to each source, discarding outliers.\\n\\nNotice, that the pre-processing step did not affect all sources equally. For instance, the Yad vashem source was reduced from 492 hours to 67.4, this is due to bad recording conditions and long silences at the beginning or end of the files.\\n\\n3.2. Data filtering To further enhance the reliability of our transcripts, we employ a forced aligner using an alternative model, specifically the MMS model [30]. This model requires the input text to be in transliterated Latin script for accurate alignment. We achieved this using the Uroman package, which romanizes text from most languages into the Latin alphabet. However, Uroman\u2019s performance dips with non-diacritized text, prompting us to first diacritize all transcriptions using the UNIKUD package, a tool specifically designed to add necessary diacritical marks to Hebrew text, thus ensuring higher transliteration accuracy.\\n\\nWe utilized the forced aligner to generate a confidence score for each utterance, calculated by averaging the confidence scores of individual words. These utterance-level scores were used to filter out lower-quality data, aiming to train our models on high-quality data only. The confidence scores for Hebrew utterances were notably lower on average than for English, with a mean score of 0.417 and a std of 0.11. We set a threshold of 0.3 for the confidence scores to determine the data quality cutoff.\\n\\nInitially, our dataset comprised \\\\(\\\\sim 1690\\\\) hours of speech. After applying the threshold for filtering, we retained 1470 hours of speech with a mean score of 0.447 and a std of 0.08, considered as reliable transcripts for training. Figure 1 presents a histogram of the forced aligner scores.\\n\\n4. Baseline system\\n\\n4.1. Implementation details\\n\\nWe provide two baseline systems together with HEBDB. The first one is an SSL model, namely HuBERT [31]. The second\"}"}
{"id": "turetzky24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model is a fully supervised one, namely Conformer [32]. Both models were optimized using HEBDB and evaluated on the Hebrew subset from the Fleurs benchmark [21]. Both models were evaluated with and without a language model (LM). We use 5-gram LM provided by the MMS project [6].\\n\\nHuBERT. We train a HuBERT-base with $\\\\sim 95$M for two iterations following the standard recipe for 'pretrain' outlined in the fairseq framework [33]. In the first iteration, we utilize 100 clusters generated from 10% of the data using KM-clustering on MFCC features. The model is trained on 4A5000 GPUs, using gradient accumulation to match the original recipe's specifications of 250k training steps across 32 GPUs. For the second iteration, we increase the number of clusters to 500 and use representations obtained from the 6th layer, still utilizing gradient accumulation but for 400k training steps. After HuBERT pre-training, we employ the connectionist temporal classification (CTC) loss [34] for ASR fine-tuning for 150K steps.\\n\\nConformer. The Conformer used is based on the model introduced by Gulati et al [35] trained with the CTC loss and a character tokenizer using 4A40 GPUs. The model is similar to the large model in [35] with $\\\\sim 100$M parameters. The Conformer model's hyper-parameters are as follows: convolution kernel size 31, n-heads 8, hidden-dim 512, 17 layers, and dropout 0.1. The model is fed Mel-spectrograms of 80 filters, with a window size of 25 ms and stride of 10 ms. We employ time and frequency masking as augmentation techniques during training. We utilize a Noam optimizer [36] with 10,000 warmup steps. Our batch size is measured in audio length, consisting of utterances with lengths ranging from 1 to 30 seconds, cumulatively not exceeding 300 seconds per batch. Finally, we trained for a total of 160k steps for the full training and 140k for the filtered training.\\n\\n4.2. Results\\nWe compare the previously mentioned baseline systems to both Whisper [7] and MMS [8] family of models. We consider Whisper using 39M, 74M, 244M, 769M, and 1.55B parameter models. For MMS we consider models trained on 61 languages and 1,107 languages. All MMS models contain 1.5B parameters. Notice, that although Whisper and MMS models are multi-lingual, both were optimized over significantly larger datasets.\\n\\nTable 3 presents the Word-Error-Rates (WER) results computed over the Fleurs [21] benchmark. When considering comparison to Whisper models the provided baseline systems reach comparable or superior performance up until model size of 769M parameters. When scaling the model size to 1.55B, Whisper provides better performance while being between $\\\\sim 15$ times bigger. Although providing worse performance, we believe such models and results are interesting and valuable to the community as these could be important for use cases where performance can be compromised over significantly smaller models [37]. When comparing to MMS, the Conformer model trained on HEBDB provides comparable performance while HuBERT was found to be superior. Notice, that both Conformer and HuBERT are significantly smaller than the MMS model ($\\\\sim 15$x smaller). When comparing HuBERT and Conformer, results suggest HuBERT provides superior performance (8-9 absolute points) with and without LM decoding.\\n\\n| Model                      | #param | LM | WER |\\n|----------------------------|--------|----|-----|\\n| Whisper                    | 39M    |    | 71.6|\\n| Whisper                    | 74M    |    | 61.7|\\n| Whisper                    | 244M   |    | 44.4|\\n| Whisper                    | 769M   |    | 33.1|\\n| Whisper                    | 1.55B  |    | 27.1|\\n| MMS (61 lang)              | 1.5B   | \u2713  | 66.6|\\n| MMS (61 lang)              | 1.5B   | \u2713  | 41.5|\\n| MMS (1,107 lang)           | 1.5B   | \u2713  | 40.0|\\n| MMS (1,107 lang)           | 1.5B   | \u2713  | 67.1|\\n| HEBDB-HuBERT               | 95M    | \u2713  | 33.9|\\n| HEBDB-HuBERT               | 95M    | \u2713  | 41.6|\\n| HEBDB-HuBERT (w. df)       | 95M    | \u2713  | 33.6|\\n| HEBDB-Conformer            | 110M   | \u2713  | 43.0|\\n| HEBDB-Conformer            | 110M   | \u2713  | 49.5|\\n| HEBDB-Conformer (w. df)    | 110M   | \u2713  | 48.4|\\n| HEBDB-Conformer (w. df)    | 110M   | \u2713  | 41.9|\\n\\nNext, we evaluate the effect of the data filtering. We train both HuBERT and Conformer models using the filtered version as presented in Section 3.2. Notice, under this setup, the SSL, and pretraining part of HuBERT were still optimized using the whole dataset, while we modify only the fine-tuning part to use the filtered version only. Results are presented in Table 3 (bottom rows). Results suggest that the data filtering provides a small improvement for both HuBERT and the Conformer models. This suggests that overall in our dataset there is enough signal from the weak supervision to construct a performing acoustic model, however, there is still room for improvement in data quality assessment. We hope the speech community will adopt and develop such techniques in future research.\\n\\n5. Conclusion & future work\\nIn this work, we present HEBDB, a weakly supervised dataset in the Hebrew language, aimed at improving the development of AI-based speech processing tools directly dedicated to Hebrew. To further enhance the development of speech processing tools for Hebrew, we additionally provide two baseline systems, a self-supervised one and a fully supervised ASR acoustic model. Both HEBDB and the pre-trained models are released under the CC BY 4.0 license. We hope the community will adopt such datasets and baseline systems together with other efforts in the field to advance the automatic development of speech-processing tools in Hebrew.\\n\\nFor future work, we plan to extend this dataset and provide higher-quality annotations in the form of transcriptions, speaker annotations, etc. Additionally, we plan to provide a subset of high-fidelity recordings which will be used to develop systems for generative tasks such as text-to-speech and voice conversion in Hebrew.\\n\\nAcknowledgements\\nThis research work was supported by the Israel Innovation Authority, grant number 78563.\"}"}
