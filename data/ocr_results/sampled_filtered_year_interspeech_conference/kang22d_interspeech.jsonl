{"id": "kang22d_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nSpeech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (ESC) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%.\\n\\nIndex Terms\\nSpeech Emotion Recognition, Multi-task Learning, Phoneme Recognition, Gender Recognition\\n\\n1. Introduction\\nEmotions, an \\\"implicit channel\\\" that transmits explicit messages [1], play a vital role in human communication. Speech is an important carrier of emotion and the easiest way to record information completely. Modern people usually communicate by phone or through APPs. Unfortunately, existing technologies lack standards for speech emotion information. If designed a unified SER framework that automatically recognizes the human emotional state categories (ESC), such as happy, sad, anger, and its emotional intensity scales (EIS) expressed in natural speech, the intelligence systems can better perceive human thoughts and have better interactivity.\\n\\nRecently, deep-learning-based approaches show great performance in extracting hidden speech information, including age [2], facial expression [3] and emotion, etc. The improvement of network architecture with recurrent neural networks (RNN) and attention mechanism [4, 5], or analyzing and fusing the textual modal information [6, 7] has demonstrated impressive results in solving SER problem. However, since emotion is a complex psychological process, a single task may not be sufficient to capture complicated emotional features.\\n\\nUsing a transfer model for different tasks can also improve accuracy. Because it essentially uses external knowledge to make the model more robust. To capture emotional information, researchers combine the power of automatic speech recognition (ASR) with natural language processing (NLP) into the SER network structure to improve model performance [8, 9]. However, their solutions are to fuse the information of different tasks in the last few layers (high-level features) of the network, which makes the information of different tasks difficult to communicate with each other. In fact, the information of different tasks is intrinsically correlated with each other. In SER, the speed and the way of speaking will affect the expression of human emotions. Therefore, a unified model will comprehensively consider these factors to give a reasonable inference.\\n\\nMulti-task learning (MTL) uses a shared backbone model to simultaneously optimize multiple objectives in different tasks. The advantage comes from side information and cross-regularization of different tasks. At the same time, joint optimization also brings challenges [10]. In SER, MTL provides an idea for the model to learn multimodal information simultaneously [9, 11, 12]. However, these methods use only one dataset for training and testing, which leads to a lack of model generalization.\\n\\nUnsupervised learning utilizes multiple datasets to generate high-quality speech features [13, 14] for SER. However, this method does not fully exploit hidden information in these datasets.\\n\\nThe released SER datasets have: (1) content-dependent (such as IEMOCAP [15]) or content independent (such as RAVDESS [16], ESD [17], CREMA-D [18]); (2) contain EIS (such as IEMOCAP, CREMA-D, RAVDESS) or not (CASIA [19]); (3) lack a unified standard for the ESC in each dataset. To find this unifying criterion, we took inspiration from psychologists' theories of emotion. One of the most well-known theories is the emotional wheel proposed by Psychologist Robert Plutchik. He theorized 8 emotions with 24 \\\"primary\\\", \\\"secondary\\\", and \\\"tertiary\\\" dyads [20]. Later, E. Cambria et al. [21] updated his theory and created the hourglass of emotions - he divided emotion into 4 dimensions, but reversed the position of each emotion, such that the intensity of emotion on each dimension could be a numerical value. These theories give us an idea to create a calculable metric for speech emotions. And the main contributions of our works are as follows:\\n\\n\u2022 This paper proposed SpeechEQ, using a multi-scale unified metric, SpeechEQ Metric (SEQM), that unifies all frameworks. It can be trained with an MTL framework to simultaneously perform two emotion recognition tasks, ESC and EIS, and two auxiliary tasks of phoneme recognition and gender recognition;\\n\u2022 We build a Mandarin SER dataset - SpeechEQ dataset (SEQD) to demonstrate that using this metric, the emotion recognition accuracy of each Mandarin SER dataset can be improved;\\n\u2022 The effectiveness of this method is also verified on the public English SER dataset IEMOCAP;\"}"}
{"id": "kang22d_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Methodology\\n2.1. Multiscale Unified Metric\\nIn order to unify different frameworks into a unified standard, we need a metric. Inspired by the emotional hourglass theory, we divide human emotions into 8 ESC. Then, use a numerical value to represent the intensity of the emotion: a value from 1 to 4 represents the EIS from low to high. Finally, very weak emotions with low EIS values ranging from 0 to 1 are defined as the 9th category \u2013 Neutral. We name it, the SpeechEQ metric (SEQM), shown in Figure 1. With this metric, multiple datasets can be fused into a Multiscale Unified Dataset (MsUD) \u2013 a label unification mechanism.\\n\\n![Figure 1: the SpeechEQ Metric for SER.](image)\\n\\nIn modeling, since emotion categories are independent of each other, but the discrimination of emotional intensity is vague, sometimes indistinguishable, our proposed model will classify emotions into 9 categories and regress emotional intensity. To create the gold label for training, if the EIS is divided into 3 levels (i.e., low, medium, high), the gold label will be set to 1.5, 2.5, and 3.5 (neutral is 0). If with 2 levels, set them to 2 and 3 respectively. If emotional intensity has been recorded in other forms, rescale to the range of 1 to 4.\\n\\n2.2. SpeechEQ Dataset\\nTo test the effectiveness of the SEQM, we constructed the SpeechEQ Dataset (SEQD), a content-independent Mandarin SER dataset, to help us improve the overall model performance in Mandarin SER tasks. This dataset contains a total of 2.3 hours of speech, 1648 audio clips from 20 speakers (10 males and 10 females) at a sampling rate of 16kHz and a precision of 16 bits. It was recorded with a Huawei phone in a medium-sized conference room at a signal-to-noise ratio (SNR) of about 20dB. The speaker is about 20cm from the microphone, sitting in the center of the room. See Table 1 for details (Neutral is 73).\\n\\n| Trust | Joy | Anticipation | Anger | Disgust | Sadness | Surprise | Fear |\\n|-------|-----|--------------|-------|---------|---------|----------|------|\\n| Low   | 62  | 59           | 60    | 72      | 66      | 61       | 61   |\\n| Medium| 62  | 63           | 65    | 77      | 73      | 77       | 63   |\\n| High  | 70  | 59           | 60    | 71      | 67      | 62       | 70   |\\n\\nTo build this dataset, each speaker independently wrote 3 to 5 sentences describing the emotion for each of the 25 emotions in the SEQM and performed it independently in the tone of talking to someone in everyday conversation. Then, each utterance was judged independently by three judges. If more than one judge judges the recorded speech emotion to be inaccurate, the speaker is notified to re-record until all three judges agree that the emotion is matched.\\n\\n2.3. Model Structure\\nFor the speech feature extraction, the structure of ECAPA-TDNN [22] has efficient design structures such as Res2Net [23] and Squeeze Excitation blocks (SE) [24]. However, it is originally for the speaker recognition task, which results in a smaller receptive field. SER requires more frames to understand contextual information for more comprehensive reasoning, which needs a larger receptive field.\\n\\n![Figure 2: Network topology of the SpeechEQ framework.](image)\\n\\nIn Figure 2, in the backbone model, all structures are the same with ECAPA except Res-BiGRU (the residual structure of bidirectional GRU) in the red background, which has been added to SE-Res2Block that extends the receptive field. The backend model is changed to the structure of MTL. In our SER framework, in addition to predicting the category and intensity of emotions, the model also predicts the phonemes in speech and the gender of the speaker as auxiliary tasks.\\n\\n2.4. Training and Inference\\nIn the training phase, the generated predictions can have access to the labels via blue, red, orange, and green paths to generate loss functions respectively. For each utterance in the training set, the blue path has access to the gold phoneme sequences labels $y_p$ translated from text labels, the red path to the ground-truth gender labels $y_g$, the orange path to the gold ESC labels $y_e$, and the green path to the gold EIS labels $y_{eis}$. The predicted labels are $\\\\hat{y}_p$, $\\\\hat{y}_g$, $\\\\hat{y}_e$, and $\\\\hat{y}_{eis}$. The loss functions are CTC Loss, Focal Loss, CCC Loss, and Attentive Stat Pooling + BN.\"}"}
{"id": "kang22d_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ye and the green path to the gold EIS labels yeis.\\n\\nFor phoneme sequence prediction (blue task), the Connectionist Temporal Classification (CTC) \\\\[25\\\\] as the metric be-\\n\\nThus, the CTC loss for phoneme recognition task is:\\n\\n\\\\[ L_{\\\\text{CTC}} = - \\\\sum_{t} \\\\log P(y_t | \\\\hat{y}_t) \\\\]\\n\\nwhere the sum ranges over all time steps. The predicted phoneme probability vectors \\\\( \\\\hat{y}_t \\\\) and the gold phoneme labels \\\\( y_t \\\\) are compared.\\n\\nFor regression of EIS (green task), the Lin's Concordance Correlation Coefficient (CCC) \\\\[27\\\\] is used. Let \\\\( L_{\\\\text{EIS}} \\\\) denote either \\\\( g \\\\) loss (FL) \\\\[26\\\\] with a tunable parameter\\n\\n\\\\[ \\\\beta \\\\]\\n\\nfor gender, or \\\\( * \\\\) loss for ESC and gender recognition tasks is in Equation 2, the sign\\n\\n\\\\[ L_{\\\\text{EIS}} = -\\\\sum_{t} \\\\log P(y_t | \\\\hat{y}_t) \\\\]\\n\\nwhere \\\\( \\\\sum \\\\) ranges over all time steps. The predicted phoneme probability vectors \\\\( \\\\hat{y}_t \\\\) and the gold phoneme labels \\\\( y_t \\\\) are compared.\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nonto the range of 0 to 4, a clipping function is designed to prevent the value from exceeding the value range, a clipping function is designed to\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\\n\\nDuring training, these values can be captured and masked by setting them to a fake value of -1 when building the gold labels.\\n\\nFor unspecified EIS labels by setting a mask, i.e., temporarily clip the value to the range of 0 to 4,\"}"}
{"id": "kang22d_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 3: Ablation study of the proposed model on Mandarin datasets compared with baseline model.\\n\\n| Experiment                        | CASIA Acc | ESD Acc | SEQD Acc  | MSE    |\\n|-----------------------------------|-----------|---------|-----------|--------|\\n| Baseline                          | 93.27%    | 89.43%  | 55.83%    | 64.24% |\\n| w/o MTL                           | 88.52%    | 86.76%  | 46.71%    | 52.64% |\\n| SpeechEQ                          | 96.45%    | 93.25%  | 66.14%    | 86.91% |\\n| w/o MTL                           | 95.32%    | 92.33%  | 62.47%    | 85.45% |\\n| w/o MSUD                          | 93.15%    | 90.46%  | 52.24%    | 67.20% |\\n| w/o MTL w/o MSUD                  | 92.58%    | 89.67%  | 51.18%    | 55.01% |\\n\\nIf the number of test samples for each class is the same (W A=UA), we denote it as Acc. Finally, the average score of each cross-validation result is taken as the final performance score. For datasets that do not have EIS labels (i.e., CASIA and ESD), it is trained using the label ignore mechanism.\\n\\nExperiment 2:\\nFor IEMOCAP, we select 4 emotions (i.e., angry, happy+excited, sad, and neutral) \u2013 Table 2. The experimental setup is the same as the cited system in Table 4. We use the \u201cdominance\u201d value as its EIS, and rescale them from a range of 1 to 5, to a range of 1 to 4. Then, randomly split the train/test into a 4:1 ratio and perform a 5-fold cross-validation. The MsUD is built by important emotions of RA VDESS, CREMA-D, ESD (English part). The phoneme recognition for English uses Librispeech [30]. Transcripts can be directly translated into phonemes as gold labels via the dictionary CMUDict [31] (dictionary size is 85, with 84 phonemes and 1 silence). Unspecified EIS labels are trained with the label ignore mechanism. The test metrics are the same as in Experiment 1.\\n\\n3.2. Hyper-parameters\\nFor the speech feature, all datasets will be resampled to the sample rate at 16kHz and precision of 16 bits, and use the 80-dimensional Mel Filter-Banks feature extracted using the librosa package with frame size 25ms, hop size 10ms with Hamming window. The channel parameter $C$ in the convolutional layers for the proposed network is 256. The dimension of the bottleneck in the SE-Block and the attention module is set to 128. The scale dimensions in the Res2Block are set to 8. The tunable parameter $\\\\gamma$ in FL is 10.\\n\\nIn the first stage, the parameters $\\\\alpha$, $\\\\beta$, $\\\\eta$ of the combined loss are initially set to be 1, 0.1, 0.1 respectively for fast convergence of each task. All networks are optimized with the Adam optimizer [32] with a learning rate of 1e-4 and weight-decay of 1e-5. The mini-batch size for training is 32.\\n\\nIn the fine-tune stage, both of combined loss parameters $\\\\beta$ and $\\\\eta$ gradually decreased from 0.1 to 0.01. Furthermore, the learning rate decreased from 1e-4 to 1e-6.\\n\\n3.3. Evaluation Results\\n\\nExperiment 1:\\nThe baseline model of CNN+RNN+Attention shares the same backbone model with [11]. Ablation studies are performed on the SpeechEQ framework with or without MTL or MSUD compared to the baseline model. The results are presented in Table 3, and the EIS value distribution of Neutral, Low, Medium and High for training and testing set of SpeechEQ are shown in the Figure 3a, 3b. Experimental results demonstrate that the feature extractor of SpeechEQ outperforms the baseline model with or without the help of MSUD and MTL. Although adding auxiliary tasks of gender recognition and phoneme recognition improves the accuracy, training models on only a single dataset have limited improvement. Without the use of MTL, the accuracy of the model is significantly improved after adding a MSUD. Combining the two approaches, the accuracy of the model is further improved on both models. From this result, it can be seen that training the model with either MSUD or MTL, or their combination, improves the accuracy of each dataset. Meanwhile, the regression task of EIS will also be more convergent.\\n\\nExperiment 2:\\nCompares the evaluation accuracy of training with or without MSUD and MTL, as well as other cited systems trained and tested only with IEMOCAP (results copied directly from the reference) in Table 4. It is proved that the SpeechEQ framework is also applicable to the English dataset: using MSUD (with external data and labels) and MTL can significantly improve the ESC accuracy of the model, and further improve the state-of-the-art results by about 2%. At the same time, the EIS regression task is more convergent.\\n\\n4. Conclusions\\nIn this paper, we propose SpeechEQ, a framework for unifying emotion recognition tasks: (1) it unifies all SER frameworks with a multi-scale unified metric - SpeechEQ Metric; (2) it can be trained on an MTL framework to perform not only ESC classification and EIS regression, but also auxiliary tasks of gender recognition and phoneme recognition. Subsequently, we construct a Mandarin SER dataset - SEQD, to demonstrate the effectiveness of our method in improving model performance. Finally, two experiments demonstrate the effectiveness of our method on both Mandarin and English SER tasks.\\n\\n5. Acknowledgement\\nThis paper is supported by the Key Research and Development Program of Guangdong Province under grant No.2021B0101400003. Corresponding author is Jianzong Wang from Ping An Technology (Shenzhen) Co., Ltd (jzwang@188.com).\"}"}
{"id": "kang22d_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6. References\\n\\n[1] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz, and J. G. Taylor, \u201cEmotion recognition in human-computer interaction,\u201d IEEE Signal processing magazine, vol. 18, no. 1, pp. 32\u201380, 2001.\\n\\n[2] S. Si, J. Wang, J. Peng, and J. Xiao, \u201cTowards speaker age estimation with label distribution learning,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 4618\u20134622.\\n\\n[3] S. Si, J. Wang, X. Qu, N. Cheng, W. Wei, X. Zhu, and J. Xiao, \u201cSpeech2video: Cross-modal distillation for speech to video generation,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2021.\\n\\n[4] D. Li, J. Liu, Z. Yang, L. Sun, and Z. Wang, \u201cSpeech emotion recognition using recurrent neural networks with directional self-attention,\u201d Expert Systems with Applications, vol. 173, p. 114683, 2021.\\n\\n[5] Z. Peng, Y. Lu, S. Pan, and Y. Liu, \u201cEfficient speech emotion recognition using multi-scale cnn and attention,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 3020\u20133024.\\n\\n[6] W. Chen, X. Xing, X. Xu, J. Yang, and J. Pang, \u201cKey-sparse transformer for multimodal speech emotion recognition,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 6897\u20136901.\\n\\n[7] P. Singh, R. Srivastava, K. Rana, and V. Kumar, \u201cA multimodal hierarchical approach to speech emotion recognition from audio and text,\u201d Knowledge-Based Systems, vol. 229, p. 107316, 2021.\\n\\n[8] S. Padi, S. O. Sadjadi, D. Manocha, and R. D. Sriram, \u201cMultimodal emotion recognition using transfer learning from speaker recognition and bert-based models,\u201d arXiv preprint arXiv:2202.08974, 2022.\\n\\n[9] X. Cai, J. Yuan, R. Zheng, L. Huang, and K. Church, \u201cSpeech emotion recognition with multi-task learning,\u201d in IEEE Conference of the International Speech Communication Association (INTERSPEECH), vol. 2021, 2021.\\n\\n[10] M. Crawshaw, \u201cMulti-task learning with deep neural networks: A survey,\u201d arXiv preprint arXiv:2009.09796, 2020.\\n\\n[11] Y. Li, T. Zhao, and T. Kawahara, \u201cImproved end-to-end speech emotion recognition using self attention mechanism and multitask learning.\u201d in IEEE Conference of the International Speech Communication Association (INTERSPEECH), 2019, pp. 2803\u20132807.\\n\\n[12] A. Nediyanchath, P. Paramasivam, and P. Yenigalla, \u201cMulti-head attention for speech emotion recognition with auxiliary learning of gender recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 7179\u20137183.\\n\\n[13] S. E. Eskimez, Z. Duan, and W. Heinzelman, \u201cUnsupervised learning approach to feature analysis for automatic speech emotion recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 5099\u20135103.\\n\\n[14] S. Si, J. Wang, H. Sun, J. Wu, C. Zhang, X. Qu, N. Cheng, L. Chen, and J. Xiao, \u201cVariational information bottleneck for effective low-resource audio classification,\u201d in Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2021, p. 31.\\n\\n[15] C. Busso, M. Bulut, C.-C. Lee, A. Kazemzadeh, E. Mower, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIemocap: Interactive emotional dyadic motion capture database,\u201d Language resources and evaluation, vol. 42, no. 4, pp. 335\u2013359, 2008.\\n\\n[16] S. R. Livingstone and F. A. Russo, \u201cThe ryerson audio-visual database of emotional speech and song (ravdess): A dynamic, multimodal set of facial and vocal expressions in north american english,\u201d PloS one, vol. 13, no. 5, p. e0196391, 2018.\\n\\n[17] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cEmotional voice conversion: Theory, databases and esd,\u201d Speech Communication, vol. 137, pp. 1\u201318, 2022.\\n\\n[18] H. Cao, D. G. Cooper, M. K. Keutmann, R. C. Gur, A. Nenkova, and R. Verma, \u201cCrema-d: Crowd-sourced emotional multimodal actors dataset,\u201d IEEE transactions on affective computing, vol. 5, no. 4, pp. 377\u2013390, 2014.\\n\\n[19] J. T. F. L. M. Zhang and H. Jia, \u201cDesign of speech corpus for mandarin text to speech,\u201d in The Blizzard Challenge, 2008.\\n\\n[20] R. Plutchik, \u201cA general psychoevolutionary theory of emotion,\u201d in Theories of emotion. Elsevier, 1980, pp. 3\u201333.\\n\\n[21] E. Cambria, A. Livingstone, and A. Hussain, \u201cThe hourglass of emotions,\u201d in Cognitive behavioural systems. Springer, 2012, pp. 144\u2013157.\\n\\n[22] B. Desplanques, J. Thienpondt, and K. Demuynck, \u201cEcapa-tdnn: Emphasized channel attention, propagation and aggregation in tdnn based speaker verification,\u201d arXiv preprint arXiv:2005.07143, 2020.\\n\\n[23] S.-H. Gao, M.-M. Cheng, K. Zhao, X.-Y. Zhang, M.-H. Yang, and P. Torr, \u201cRes2net: A new multi-scale backbone architecture,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 43, no. 2, pp. 652\u2013662, 2019.\\n\\n[24] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132\u20137141.\\n\\n[25] A. Graves, S. Fern\u00e1ndez, F. Gomez, and J. Schmidhuber, \u201cConnectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks,\u201d in Proceedings of the 23rd international conference on Machine learning, 2006, pp. 369\u2013376.\\n\\n[26] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r, \u201cFocal loss for dense object detection,\u201d in Proceedings of the IEEE international conference on computer vision, 2017, pp. 2980\u20132988.\\n\\n[27] I. Lawrence and K. Lin, \u201cA concordance correlation coefficient to evaluate reproducibility,\u201d Biometrics, pp. 255\u2013268, 1989.\\n\\n[28] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, and O. Nieto, \u201clibrosa: Audio and music signal analysis in python,\u201d in Proceedings of the 14th python in science conference, vol. 8. Citeseer, 2015, pp. 18\u201325.\\n\\n[29] H. Bu, J. Du, X. Na, B. Wu, and H. Zheng, \u201cAishell-1: An open-source mandarin speech corpus and a speech recognition base-line,\u201d in 20th Conference of the Oriental Chapter of the International Coordinating Committee on Speech Databases and Speech I/O Systems and Assessment (O-COCOSDA). IEEE, 2017, pp. 1\u20135.\\n\\n[30] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an asr corpus based on public domain audio books,\u201d in IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[31] J. Kominek and A. W. Black, \u201cThe cmu arctic speech databases,\u201d in Fifth ISCA workshop on speech synthesis, 2004.\\n\\n[32] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d arXiv preprint arXiv:1412.6980, 2014.\\n\\n[33] Z. Zhao, Z. Bao, Z. Zhang, N. Cummins, H. Wang, and B. Schuller, \u201cAttention-enhanced connectionist temporal classification for discrete speech emotion recognition,\u201d Proceedings of the IEEE International Speech Communication Association (INTERSPEECH), 2019.\\n\\n[34] Y. Xu, H. Xu, and J. Zou, \u201cHgfm: A hierarchical grained and feature model for acoustic emotion recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6499\u20136503.\\n\\n[35] M. Xu, F. Zhang, and W. Zhang, \u201cHead fusion: improving the accuracy and robustness of speech emotion recognition on the iemocap and ravdess dataset,\u201d IEEE Access, vol. 9, pp. 74 539\u201374 549, 2021.\"}"}
