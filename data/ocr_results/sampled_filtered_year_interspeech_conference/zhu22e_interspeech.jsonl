{"id": "zhu22e_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Filler Word Detection and Classification: A Dataset and Benchmark\\nGe Zhu1\u21e4, Juan-Pablo Caceres2, Justin Salamon2\\n1University of Rochester, 2Adobe Research\\nge.zhu@rochester.edu {caceres,salamon}@adobe.com\\n\\nAbstract\\nFiller words such as 'uh' or 'um' are sounds or words people use to signal they are pausing to think. Finding and removing filler words from recordings is a common and tedious task in media editing. Automatically detecting and classifying filler words could greatly aid in this task, but few studies have been published on this problem to date. A key reason is the absence of a dataset with annotated filler words for model training and evaluation. In this work, we present a novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K annotations of other sounds that commonly occur in podcasts such as breaths, laughter, and word repetitions. We propose a pipeline that leverages VAD and ASR to detect filler candidates and a classifier to distinguish between filler word types. We evaluate our proposed pipeline on PodcastFillers, compare to several baselines, and present a detailed ablation study. In particular, we evaluate the importance of using ASR and how it compares to a transcription-free approach resembling keyword spotting. We show that our pipeline obtains state-of-the-art results, and that leveraging ASR strongly outperforms a keyword spotting approach. We make PodcastFillers publicly available, in the hope that our work serves as a benchmark for future research.\\n\\nIndex Terms: filler word detection, speech disfluency, keyword spotting\\n\\n1. Introduction\\nSpeech disfluencies, such as filler words, stuttering, repetitions and corrections, are common in spontaneous speech [1]. Of all disfluencies, filler words, especially 'uh's and 'um's, are the most common [2]. For content creators working on, e.g., podcasts or video interviews, manually finding and editing filler words in video and audio recordings requires significant time and effort. Automatically detecting filler words accurately has the potential to significantly speed up speech content creation workflows. Such a filler word detection system must be able to both localize filler words in time and classify them correctly.\\n\\nPrevious work has focused on detecting and removing speech disfluencies from text transcripts [3\u20136], some also incorporating acoustic features [7]. In some cases, the transcripts are produced via Automatic Speech Recognition (ASR) [8\u201310]. In this scenario it is up to the ASR to transcribe the filler words, which requires training an ad-hoc ASR with filler words in its vocabulary. This is computationally intensive and challenging since ASR systems are often trained on spoken text corpora which do not contain any filler words, and thus cannot detect them reliably. Furthermore, adding a new filler word to the vocabulary would require re-training the ASR model.\\n\\nMore recently, several data driven methods have been proposed to detect speech disfluencies directly from audio in telephone conversations [11, 12] and naturalistic recordings [13\u201315]. Sheikh et al. proposed StutterNet [14], a time-delay neural network (TDNN) to classify repetitions, blocks, prolongations and interjections, the latter being another term for filler words. They used the UCLASS dataset [16], which is designed for stutter classification. This poses some challenges, since the dataset was recorded in a controlled environment, and exclusively contains speech by people who stutter. This, along with the small size of the dataset (\u21e04K sentences), means it is unclear whether the results would generalize to recordings of spontaneous speech more broadly. To tackle this issue, Kourkounakis et al. [15] expanded the training data by creating a synthesized stutter dataset, LibriStutter, by inserting repetitions or interjections in between non-stuttered speech from a subset of LibriSpeech [17]. The interjections, however, were taken from UCLASS, presenting the same aforementioned challenge.\\n\\nSalamin et al. [18] trained Hidden Markov Models to segment laughter, fillers, speech and silence from spontaneous speech on the SSPNet Vocalization Corpus, which is also a small-scale dataset. Lea et al. [12] created a large speech disfluency dataset, SEP-28K, from podcasts with people who stutter, and used it to build a stutter detector. As before, there is a generalization challenge given the audio data are specific to people who stutter. Also, the dataset is annotated at the clip level, so it does not provide precise timestamps for filler words and cannot be used to evaluate detection accuracy at a fine temporal resolution.\\n\\nThe closest study to our work is by Das et al. [1], who proposed a disfluency repair system aiming at removing filler words and long pauses. They trained a convolutional recurrent neural network (CRNN) for filler word segmentation (detection and classification) applied directly to audio recordings. They used two speech datasets, Switchboard speech data [19] with transcripts and Automanner [20]. A key limitation of the approach noted by the authors is that it was unable to distinguish filler words such as 'uh' or 'um' from real parts-of-speech, returning false-positives for actual words that sound similar to or contain filler words, such as \\\"um-brella\\\". Also, Kaushik et al. [13] found that the mismatch between training on telephony speech and testing on naturalistic recordings hurts filler classification accuracy. The test set for evaluating the methods presented by Das et al. [1] only contains 20 speech samples, once again making it hard to draw generalizable conclusions.\\n\\nIn this paper, we address the data scarcity challenge for filler word detection by creating the largest annotated dataset of filler words published to date, PodcastFillers, which we make publicly available online 1. We propose an efficient workflow for generating annotation candidates in continuous speech recordings that leverages a robust Voice Activity Detection (VAD) model and an off-the-shelf ASR, and annotate over 85K filler word candidates. The resulting dataset spans 145 hours of speech from over 350 speakers coming from 199 public podcast episodes, and has 35K annotated filler words and 50K annotations of other speech events that are common in podcasts.\\n\\n1podcastfillers.github.io\"}"}
{"id": "zhu22e_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"I want a flight to Boston on Friday.\"}"}
{"id": "zhu22e_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ASRC \u2227 1st Stage: filler candidate detection\\n2nd Stage: filler classification\\nEvent level Labels:\\n0.2~0.3s           Filler\\n0.6~1.1s           Words\\n\\nFigure 2: Proposed two-stage filler detection and classification.\\n\\na 5 sec clip (for context), and highlighted in the interface. Anno-\\ntators had to determine whether the highlighted candidate was a\\nfiller word or not, and based on that select one of the five filler\\nlabels or eight non-filler labels. Each candidate was annotated\\nby two people, or three when the first two disagreed. Out of all\\ncandidates labeled \u2018uh\u2019 or \u2018um\u2019 in the dataset, 98.4% and 96.4%\\nrespectively had at least two annotators agree on the label.\\n\\n3. Filler Detection Pipeline\\n\\nWe propose a filler detection pipeline with two variants: the first\\nleverages ASR, while the second does not, which is relevant for\\ndeployment scenarios where ASR is not available. The pipeline\\nis depicted in Fig. 2. In the first stage, the input audio is passed\\nthrough the VAD model to find voice regions. The first pipeline\\nvariant also runs the audio through ASR to discard regions with\\ntranscribed words. The second variant passes straight to the next\\nstage. In the second stage, the remaining candidate time regions\\nare passed through a classification model that produces labeled\\nevents with a start time, end time, and a label. Moving forward,\\nwe shall refer to the first pipeline as AVC-FillerNet (for ASR +\\nVAD + Classifier), and the second as VC-FillerNet (no ASR).\\n\\nBy skipping the ASR, VC-FillerNet is computationally lighter,\\nbut runs the risk of detecting parts of actual words as fillers [1].\\n\\nOur goal is to train a robust multi-class classifier to detect\\nfillers given a short snippet of audio. Given the label distri-\\nbution in PodcastFillers, we opted to discard labels with 3K\\nor less annotations and consolidate other labels, producing five\\nnew labels, each with ample training data: \u2018filler\u2019 (\u2018uh\u2019+\u2018um\u2019),\\n\u2018words\u2019 (\u2018regular words\u2019 and \u2018repetitions\u2019), \u2018laughter\u2019, \u2018music\u2019,\\nand \u2018breath\u2019. Ultimately, we only care about the detection ac-\\ncuracy for the \u2018filler\u2019 class, and we expect this consolidation to\\nlead to a more robust classifier. In Section 5 we also evaluate\\nour ability to classify \u2018uh\u2019 and \u2018um\u2019 as two separate classes.\\n\\nWe use wav2vec [28] embeddings computed with a 10 ms\\nhop size as input to the model. Wav2vec was pretrained on\\nover 960 hours of speech, providing a robust representation for\\nclassification of speech-like sounds. During training we apply\\ntime and \u201cfrequency\u201d masking to the embeddings via SpecAug-\\nment [29], and optimize a cross entropy loss.\\n\\nSince our goal is to detect specific short utterances in an\\naudio stream, the task can be viewed as a keyword-spotting\\n(KWS) problem where our keyword is the joint set of \u2018uh\u2019 and\\n\u2018um\u2019. With this in mind, we adapt a lightweight KWS model\\nbackbone architecture, TC-ResNet8 [30], for efficient classifi-\\ncation. TC-ResNet8 only has around 100k parameters, mak-\\ning it suitable for low-latency inference. It applies 1D convo-\\nlutions along the temporal axis and spans the entire frequency\\nrange in every layer, achieving strong performance even with a\\nsmall number of layers. Because the filler candidates in AVC-\\nFillerNet are normally short segments, we can train an\\nevent classifier to directly predict the event label for the entire input\\nsegment. On the contrary, for VC-FillerNet, the filler candidates\\nare usually long sequences of voice, so we train a\\nframe classifier to predict frame-level labels at a fine temporal resolution,\\ne.g., every 100 ms. To get frame-level predictions, we adapt\\nthe TC-ResNet8 backbone by adding an LSTM layer. Similar\\nto Filler-CRNN [1], we then group contiguous frames with the\\nsame predicted label into an event. The final output of both\\nthe event-level classifier and frame-level classifier are discrete\\nevents with a start time, end time, and a label. We compare the\\ntwo approaches as part of our ablation study.\\n\\n4. Experimental Design\\n\\n4.1. Data split and training\\n\\nFor our experiments, we split the PodcastFillers dataset into\\ntrain, validation, and test sets with 173, 6, 20 episodes respec-\\ntively, while ensuring each subset remains gender-balanced.\\nThe audio is downsampled from 44.1 kHz to 16 kHz for compu-\\ntational efficiency. We train our proposed models on the train-\\ning set, tune hyper-parameters and the VAD threshold on the\\nvalidation set, and report performance on the test set. To train\\nthe event classifier, AVC-FillerNet, we use 1 s input clips with\\nthe labeled filler candidate placed at the center of the clip. The\\nmodel produces a single prediction for the input, which is com-\\npared to the ground truth label. To train the frame classifier,\\nVC-FillerNet, we use 1 s input clips where the filler candidate\\ncan appear anywhere in the clip. The model produces per-frame\\npredictions that are compared to the ground truth events\\n(which have start/end times) frame-by-frame during training.\\n\\n4.2. Baselines\\n\\nWe compare our systems with two strong baselines: a neural-\\nnetwork-based method, Filler-CRNN [1], and a forced-aligner-\\nbased method, Gentle [31]. The input to Filler-CRNN is log-\\nmel with 128 bins computed from 1 s clips. The original Filler-\\nCRNN architecture yielded weak performance in our experi-\\nments, so we fine tune it (number of layers, kernel size, pool-\\ning) on PodcastFillers for a stronger baseline. With Gentle, we\\nfirst apply pre-trained Kaldi [32] acoustic models developed on\\nthe Fisher English corpus [33] to generate syllable tokens. We\\ncompare the tokens with the ASR transcript: filler words are de-\\ntected if the inconsistent regions between the two are re-aligned\\nby inserting \u2018um\u2019 or \u2018uh\u2019 into the transcript.\\n\\n4.3. Evaluation metrics\\n\\nWe compute segment-based and event-based metrics (Precision,\\nRecall, F1) using sed eval [34] for the \u2018filler\u2019 class, to evalu-\\nate detection accuracy and localization accuracy respectively.\\nSegment-based metrics map the system output and ground truth\\nto a fixed time grid for comparison. Event-based metrics com-\\npare the estimated sound events and the ground truth events di-\\rectly. A predicted event is considered a true positive if it over-\\nlaps with a ground truth event that has the same label\\nand\\nits\\nonset and offset are within a threshold (slack) from the refer-\\nce event\u2019s onset and offset (200 ms in this work).\"}"}
{"id": "zhu22e_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.4. Ablations\\n\\nWe run ablation studies to understand the impact of each of the two stages of our proposed pipeline, VAD and the filler classifier. For VAD, we vary the activation threshold from 0.1\u20130.9: the lower the threshold the more candidates will be passed to the second stage. For the classifier, we compare different input features (log-mel with 64 bins and wav2vec) and architectures (event classifier and frame classifier). We evaluate both the AVC-FillerNet and VC-FillerNet pipelines in all ablations. The output events from each model are converted to frame-level likelihoods, which are compared to the reference annotations to produce Precision-Recall (PR) curves, which elucidate the trade-off between precision and recall.\\n\\n5. Results\\n\\n5.1. Ablation studies\\n\\nWe start by analyzing the influence of the VAD activation threshold, using the PodcastFillers validation set, and wav2vec as the input feature. The results are shown in Fig. 3(a). The best PR-curve is obtained using the lowest threshold (0.1), and as we increase the threshold there is a notable decrease in recall. Interestingly, the precision remains consistent regardless of the VAD activation threshold. This suggests that the filler classifier is robust at rejecting false positives with lower VAD likelihoods, and so a low VAD threshold maximizes recall by ensuring we do not miss soft filler words, without compromising on precision.\\n\\nFollowing this, we fix the VAD threshold to 0.1 in the second ablation study where we compare different input features and classifier backbones, shown in Fig. 3(b). Wav2vec consistently outperforms log-mel as the input feature, confirming that this model, trained on a large speech corpus, yields a discriminative representation for filler word classification. For the AVC-FillerNet pipeline, the event classifier is only marginally better than the frame classifier. In contrast, since the VC-FillerNet pipeline cannot leverage ASR to determine the precise timing of filler candidates, the frame classifier outperforms the event classifier in this pipeline due to its superior temporal accuracy.\\n\\nMost importantly, we see that across both ablation studies, AVC-FillerNet clearly outperforms VC-FillerNet. By leveraging ASR, AVC-FillerNet produces tight temporal boundaries around filler candidates, and dramatically reduces the number of candidates passed to the classifier. This reduces the chances of producing false positives, leading to a boost in precision.\\n\\n5.2. Comparison to baselines\\n\\nWe compare AVC-FillerNet and VC-FillerNet with two baselines, Filler-CRNN and Gentle (described in Sec 4.2), and report results the PodcastFillers test set as shown in Tab. 1. For our pipelines, we use the optimal VAD threshold of 0.1 as determined by the ablation study on the validation set. We see that AVC-FillerNet significantly outperforms all the other systems for all metrics. Gentle yields higher precision than VC-FillerNet and Filler-CRNN, but has the lowest recall among all the systems. We speculate this may be improved by leveraging acoustic models trained with filler words in Gentle.\\n\\n5.3. Filler detection & classification with fine granularity\\n\\nFinally, we evaluate our systems' ability to separately detect 'uh' and 'um' filler words. We re-train the classifier with the two labels as separate classes, and evaluate our systems on the PodcastFillers test set, as shown in Tab. 2. We see that 'um' is easier to classify, especially for VC-FillerNet. We speculate that this is because 'um's are typically longer than 'uh's, providing the classifier with more signal to leverage for inference.\\n\\n6. Conclusion\\n\\nIn this work we presented PodcastFillers, a large dataset of podcasts with annotated filler words. The dataset was created by bootstrapping a VAD model and a commercial ASR system to generate filler candidates that were annotated via crowdsourcing. We proposed ASR-based and ASR-free filler detection and classification pipelines, AVC-FillerNet and VC-FillerNet. Our experiments showed that AVC-FillerNet achieves state-of-the-art results, significantly outperforming existing filler word detection systems, and that leveraging ASR outperforms a key-word spotting approach for filler word detection. Through ablation studies, we evaluated the impact of our design choices on system performance. We hope the PodcastFillers dataset, our proposed filler detection and classification pipeline, and our experimental results serve as a benchmark for future research.\"}"}
{"id": "zhu22e_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] S. Das, N. Gandhi, T. Naik, and R. Shilkrot, \u201cIncrease apparent public speaking fluency by speech augmentation,\u201d in ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2019, pp. 6890\u20136894.\\n\\n[2] K. Womack, W. McCoy, C. O. Alm, C. Calvelli, J. B. Pelz, P. Shi, and A. Haake, \u201cDisfluencies as extra-propositional indicators of cognitive processing,\u201d in Proceedings of the workshop on extra-propositional aspects of meaning in computational linguistics, 2012, pp. 1\u20139.\\n\\n[3] N. Bach and F. Huang, \u201cNoisy bilstm-based models for disfluency detection.\u201d in INTERSPEECH, 2019, pp. 4230\u20134234.\\n\\n[4] F. Wang, W. Chen, Z. Yang, Q. Dong, S. Xu, and B. Xu, \u201cSemi-supervised disfluency detection,\u201d in Proceedings of the 27th International Conference on Computational Linguistics, 2018, pp. 3529\u20133538.\\n\\n[5] P. Jamshid Lou, P. Anderson, and M. Johnson, \u201cDisfluency detection using auto-correlational neural networks,\u201d in Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP2018). Brussels, Belgium: Association for Computational Linguistics, 2018, pp. 4610\u20134619.\\n\\n[6] S. Wang, W. Che, Q. Liu, P. Qin, T. Liu, and W. Y. Wang, \u201cMulti-task self-supervised learning for disfluency detection,\u201d in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 34, no. 05, 2020, pp. 9193\u20139200.\\n\\n[7] V. Zayats and M. Ostendorf, \u201cGiving attention to the unexpected: Using prosody innovations in disfluency detection,\u201d in Proceedings of NAACL-HLT, 2019, pp. 86\u201395.\\n\\n[8] J. Ferguson, G. Durrett, and D. Klein, \u201cDisfluency detection with a semi-markov model and prosodic features,\u201d in Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2015, pp. 257\u2013262.\\n\\n[9] H. Hassan, L. Schwartz, D. Hakkani-T\u00fcr, and G. Tur, \u201cSegmentation and disfluency removal for conversational speech translation,\u201d in Fifteenth Annual Conference of the International Speech Communication Association, 2014.\\n\\n[10] P. A. Heeman, R. Lunsford, A. McMillin, and J. S. Yaruss, \u201cUsing Clinician Annotations to Improve Automatic Speech Recognition of Stuttered Speech,\u201d in Proc. Interspeech 2016, 2016, pp. 2651\u20132655.\\n\\n[11] R. Gupta, K. Audhkhasi, S. Lee, and S. S. Narayanan, \u201cParalinguistic event detection from speech using probabilistic time-series smoothing and masking.\u201d in Interspeech, 2013, pp. 173\u2013177.\\n\\n[12] C. Lea, V. Mitra, A. Joshi, S. Kajarekar, and J. P. Bigham, \u201cSep-28k: A dataset for stuttering event detection from podcasts with people who stutter,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6798\u20136802.\\n\\n[13] L. Kaushik, A. Sangwan, and J. H. Hansen, \u201cLaughter and filler detection in naturalistic audio.\u201d International Speech and Communication Association, 2015.\\n\\n[14] S. A. Sheikh, M. Sahidullah, F. Hirsch, and S. Ouni, \u201cStutterNet: Stuttering detection using time delay neural network,\u201d in 2021 29th European Signal Processing Conference (EUSIPCO). IEEE, 2021, pp. 426\u2013430.\\n\\n[15] T. Kourkounakis, A. Hajavi, and A. Etemad, \u201cFluentNet: End-to-end detection of stuttered speech disfluencies with deep learning,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 2986\u20132999, 2021.\\n\\n[16] P. Howell, S. Davis, and J. Bartrip, \u201cThe university college london archive of stuttered speech (uclass),\u201d Journal of Speech, Language, and Hearing Research, vol. 52, pp. 556\u2013569, 2009.\\n\\n[17] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, \u201cLibrispeech: an ASR corpus based on public domain audio books,\u201d in 2015 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2015, pp. 5206\u20135210.\\n\\n[18] H. Salamin, A. Polychroniou, and A. Vinciarelli, \u201cAutomatic detection of laughter and fillers in spontaneous mobile phone conversations,\u201d in 2013 IEEE International Conference on Systems, Man, and Cybernetics. IEEE, 2013, pp. 4282\u20134287.\\n\\n[19] J. J. Godfrey, E. C. Holliman, and J. McDaniel, \u201cSwitchboard: Telephone speech corpus for research and development,\u201d in Acoustics, Speech, and Signal Processing, IEEE International Conference on, vol. 1. IEEE Computer Society, 1992, pp. 517\u2013520.\\n\\n[20] M. I. Tanveer, R. Zhao, K. Chen, Z. Tiet, and M. E. Hoque, \u201cAutoManner: An automated interface for making public speakers aware of their mannerisms,\u201d in Proceedings of the 21st International Conference on Intelligent User Interfaces, 2016, pp. 385\u2013396.\\n\\n[21] G. Chen, S. Chai, G.-B. Wang, J. Du, W.-Q. Zhang, C. Weng, D. Su, D. Povey, J. Trmal, J. Zhang, M. Jin, S. Khudanpur, S. Watanabe, S. Zhao, W. Zou, X. Li, X. Yao, Y. Wang, Z. You, and Z. Yan, \u201cGigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio,\u201d in Proc. Interspeech 2021, 2021, pp. 3670\u20133674.\\n\\n[22] J. Salamon, D. MacConnell, M. Cartwright, P. Li, and J. P. Bello, \u201cScaper: A library for soundscape synthesis and augmentation,\u201d in 2017 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA). IEEE, 2017, pp. 344\u2013348.\\n\\n[23] C. Veaux, J. Yamagishi, K. MacDonald et al., \u201cSuperseded-cstr-vctk corpus: English multi-speaker corpus for cstr voice cloning toolkit,\u201d 2016.\\n\\n[24] S. Hershey, D. P. Ellis, E. Fonseca, A. Jansen, C. Liu, R. C. Moore, and M. Plakal, \u201cThe benefit of temporally-strong labels in audio event classification,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 366\u2013370.\\n\\n[25] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 776\u2013780.\\n\\n[26] B. McFee, C. Raffel, D. Liang, D. P. Ellis, M. McVicar, E. Battenberg, and O. Nieto, \u201clibrosa: Audio and music signal analysis in python,\u201d in Proceedings of the 14th python in science conference, vol. 8, 2015, pp. 18\u201325.\\n\\n[27] Y. Chen, H. Dinkel, M. Wu, and K. Yu, \u201cVoice Activity Detection in the Wild via Weakly Supervised Sound Event Detection,\u201d in Proc. Interspeech 2020, 2020, pp. 3665\u20133669.\\n\\n[28] S. Schneider, A. Baevski, R. Collobert, and M. Auli, \u201cwav2vec: Unsupervised Pre-Training for Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 3465\u20133469.\\n\\n[29] D. S. Park, W. Chan, Y. Zhang, C.-C. Chiu, B. Zoph, E. D. Cubuk, and Q. V. Le, \u201cSpecAugment: A Simple Data Augmentation Method for Automatic Speech Recognition,\u201d in Proc. Interspeech 2019, 2019, pp. 2613\u20132617.\\n\\n[30] S. Choi, S. Seo, B. Shin, H. Byun, M. Kersner, B. Kim, D. Kim, and S. Ha, \u201cTemporal convolution for real-time keyword spotting on mobile devices,\u201d arXiv preprint arXiv:1904.03814, 2019.\\n\\n[31] R. Ochshorn and M. Max Hawkins, \u201cGentle: A robust yet lenient forced aligner built on kaldi.\u201d https://lowerquality.com/gentle/.\\n\\n[32] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P. Motlicek, Y. Qian, P. Schwarz et al., \u201cThe kaldi speech recognition toolkit,\u201d in IEEE 2011 workshop on automatic speech recognition and understanding, no. CONF. IEEE Signal Processing Society, 2011.\\n\\n[33] C. Cieri, D. Miller, and K. Walker, \u201cThe fisher corpus: A resource for the next generations of speech-to-text.\u201d in LREC, vol. 4, 2004, pp. 69\u201371.\\n\\n[34] A. Mesaros, T. Heittola, and T. Virtanen, \u201cMetrics for polyphonic sound event detection,\u201d Applied Sciences, vol. 6, no. 6, p. 162, 2016.\"}"}
