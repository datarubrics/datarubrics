{"id": "leduc24_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. Acknowledgement\\n\\nWe thank Bao Tran at Chubb Canada and Linh Nguyen from Bucknell University for helping the initial annotation. We appreciate David Thulke at RWTH Aachen University and AppTek GmbH for his precious feedback. We thank Ralf Schl\u00fcter at RWTH Aachen University for supporting computing resource to conduct experiments.\\n\\n9. References\\n\\n[1] M. Kameyama, G. Kawai, and I. Arima, \u201cA real-time system for summarizing human-human spontaneous spoken dialogues,\u201d in Proceeding of Fourth International Conference on Spoken Language Processing. ICSLP\u201996, 1996.\\n\\n[2] R. Jain, A. Jangra, S. Saha, and A. Jatowt, \u201cA survey on medical document summarization,\u201d arXiv preprint arXiv:2212.01669, 2022.\\n\\n[3] Y. Song, Y. Tian, N. Wang, and F. Xia, \u201cSummarizing medical conversations via identifying important utterances,\u201d in Proceedings of the 28th International Conference on Computational Linguistics, 2020, pp. 717\u2013729.\\n\\n[4] K. Le-Duc, \u201cVietmed: A dataset and benchmark for automatic speech recognition of Vietnamese in the medical domain,\u201d in Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), 2024, pp. 17 365\u201317 370.\\n\\n[5] A. B. Abacha and D. Demner-Fushman, \u201cOn the summarization of consumer health questions,\u201d in Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019, pp. 2228\u20132234.\\n\\n[6] V.-H. Nguyen, T.-C. Nguyen, M.-T. Nguyen, and N. X. Hoai, \u201cVnds: A vietnamese dataset for summarization,\u201d in 2019 6th NAFOSTED Conference on Information and Computer Science (NICS), 2019, pp. 375\u2013380.\\n\\n[7] N. Minh, V. H. Tran, V. Hoang, H. D. Ta, T. H. Bui, and S. Q. H. Truong, \u201cViHealthBERT: Pre-trained language models for Vietnamese in health text mining,\u201d in Proceedings of the Thirteenth Language Resources and Evaluation Conference, N. Calzolari, F. B\u00e9chet, P. Blache, K. Choukri, C. Cieri, T. Declerck, S. Goggi, H. Isahara, B. Maegaard, J. Mariani, H. Mazo, J. Odijk, and S. Piperidis, Eds. Marseille, France: European Language Resources Association, Jun. 2022, pp. 328\u2013337. [Online]. Available: https://aclanthology.org/2022.lrec-1.35\\n\\n[8] E. Pavlick, M. Post, A. Irvine, D. Kachaev, and C. Callison-Burch, \u201cThe language demographics of Amazon Mechanical Turk,\u201d Transactions of the Association for Computational Linguistics, vol. 2, pp. 79\u201392, 2014. [Online]. Available: https://aclanthology.org/Q14-1007\\n\\n[9] M. Li, T. Shi, C. Ziems, M.-Y. Kan, N. F. Chen, Z. Liu, and D. Yang, \u201cCoannotating: Uncertainty-guided work allocation between human and large language models for data annotation,\u201d 2023.\\n\\n[10] J. Choi, E. Lee, K. Jin, and Y. Kim, \u201cGpts are multilingual annotators for sequence generation tasks,\u201d 2024.\\n\\n[11] S. Latif, M. Usama, M. I. Malik, and B. W. Schuller, \u201cCan large language models aid in annotating speech emotional data? uncovering new frontiers,\u201d 2023.\\n\\n[12] S. Wang, Y. Liu, Y. Xu, C. Zhu, and M. Zeng, \u201cWant to reduce labeling cost? GPT-3 can help,\u201d CoRR, vol. abs/2108.13487, 2021. [Online]. Available: https://arxiv.org/abs/2108.13487\\n\\n[13] A. Meyer, J. Riese, and T. Streichert, \u201cComparison of the performance of gpt-3.5 and gpt-4 with that of medical students on the written german medical licensing examination: Observational study,\u201d JMIR Medical Education, vol. 10, p. e50965, 2024.\\n\\n[14] C.-Y. Lin, \u201cRouge: A package for automatic evaluation of summaries,\u201d in Text summarization branches out, 2004, pp. 74\u201381.\\n\\n[15] N. L. Tran, D. M. Le, and D. Q. Nguyen, \u201cBartpho: Pre-trained sequence-to-sequence models for Vietnamese,\u201d 2022.\\n\\n[16] L. Phan, H. Tran, H. Nguyen, and T. H. Trinh, \u201cVit5: Pre-trained text-to-text transformer for Vietnamese language generation,\u201d 2022.\\n\\n[17] M.-T. Nguyen, H.-D. Nguyen, T.-H.-N. Nguyen, and V.-H. Nguyen, \u201cTowards state-of-the-art baselines for Vietnamese multidi,spersed summarization,\u201d in 2018 10th International Conference on Knowledge and Systems Engineering (KSE), 2018, pp. 85\u201390.\\n\\n[18] L. Phan, T. Dang, H. Tran, T. H. Trinh, V. Phan, L. D. Chau, and M.-T. Luong, \u201cEnriching biomedical knowledge for low-resource language through large-scale translation,\u201d in Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, 2023, pp. 3131\u20133142.\\n\\n[19] Y. Chen, Y. Liu, L. Chen, and Y. Zhang, \u201cDialogsum: A real-life scenario dialogue summarization dataset,\u201d in Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 2021, pp. 5062\u20135074.\\n\\n[20] W. Kryscinski, N. S. Keskar, B. McCann, C. Xiong, and R. Socher, \u201cNeural text summarization: A critical evaluation,\u201d in Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP). Association for Computational Linguistics, 2019.\\n\\n[21] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz et al., \u201cHuggingface\u2019s transformers: State-of-the-art natural language processing,\u201d arXiv preprint arXiv:1910.03771, 2019.\\n\\n[22] M. Lewis, Y. Liu, N. Goyal, M. Ghazvininejad, A. Mohamed, O. Levy, V. Stoyanov, and L. Zettlemoyer, \u201cBart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension,\u201d 2019.\\n\\n[23] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d The Journal of Machine Learning Research, vol. 21, no. 1, pp. 5485\u20135551, 2020.\\n\\n[24] A. Conneau, K. Khandelwal, N. Goyal, V. Chaudhary, G. Wenzek, F. Guzm\u00e1n, E. Grave, M. Ott, L. Zettlemoyer, and V. Stoyanov, \u201cUnsupervised cross-lingual representation learning at scale,\u201d arXiv preprint arXiv:1911.02116, 2019.\\n\\n[25] A. Roberts, H. W. Chung, A. Levskaya, G. Mishra, J. Bradbury, D. Andor, S. Narang, B. Lester, C. Gaffney, A. Mohiuddin, C. Hawthorne, A. Lewkowycz, A. Salcianu, M. van Zee, J. Austin, S. Goodman, L. B. Soares, H. Hu, S. Tsvyashchenko, A. Chowdhery, J. Bastings, J. Bulian, X. Garcia, J. Ni, A. Chen, K. Keane, J. H. Clark, S. Lee, D. Garrette, J. Lee-Thorp, C. Raffel, N. Shazeer, M. Ritter, M. Bosma, A. Passos, J. Maitin-Shepard, N. Fiedel, M. Omernick, B. Saeta, R. Sepassi, A. Spiridonov, J. Newlan, and A. Gesmundo, \u201cScaling up models and data with t5x and seqio,\u201d 2022.\\n\\n[26] J. Heek, A. Levskaya, A. Oliver, M. Ritter, B. Rondepierre, A. Steiner, and M. van Zee, \u201cFlax: A neural network library and ecosystem for JAX,\u201d 2023. [Online]. Available: http://github.com/google/flax\\n\\n[1964]\"}"}
{"id": "leduc24_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Real-time Speech Summarization for Medical Conversations\\n\\nKhai Le-Duc\u22171,2,6, Khai-Nguyen Nguyen\u22173, Long Vo-Dang4, Truong-Son Hy5,6\\n\\n1University of Toronto, Canada\\n2University Health Network, Canada\\n3College of William and Mary, United States\\n4University of Cincinnati, United States\\n5Indiana State University, United States\\n6FPT Software AI Center, Vietnam\\n\\nduckhai.le@mail.utoronto.ca, knguyen07@wm.edu, TruongSon.Hy@indstate.edu\\n\\nAbstract\\n\\nIn doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online.\\n\\nIndex Terms: speech recognition, speech summarization, AI for healthcare, LLM\\n\\n1. Introduction\\n\\nIn real-world conversations, the volume of information grows significantly in tandem with speaking rates, leading to information overload. Remembering every detail discussed, especially medical information, is beyond human capability. Yet, doctors and patients frequently make decisions by prioritizing crucial information and its significance. Consequently, the adoption of real-time speech summarization (RTSS) system is emerging as an effective approach to tackle this issue.\\n\\nCompared to pre-recorded speech summarization, RTSS research has very little literature [1]. Besides, in industry settings, to the best of our knowledge, there is currently no RTSS system deployed for real-world applications.\\n\\nIn terms of medical domain, according to the latest survey by [2] and to the best of our knowledge, there is only one publicly available dataset for medical conversation summarization [3]. This dataset consists of written text in the Chinese language and was crawled from an online healthcare service provider. However, no speech summarization dataset for medical conversations is publicly available.\\n\\nRTSS systems proposed by [1] constantly update and revise the current summary state in the course of a dialogue using additional components, such as flexible recognizer of utterance units, utterance lookahead-er, and information overrider.\\n\\nTo tackle all the problems above, we propose a new approach to RTSS system for medical conversations. Our contribution are as follows:\\n\\n\u2022 We propose the first deployable RTSS system for real-world applications.\\n\u2022 We introduce VietMed-Sum\u2014the first speech summarization dataset for real-world medical conversations, to the best of our knowledge.\\n\u2022 We conduct the first attempt to leverage ChatGPT and human annotators collaboratively to create gold standard and synthetic summaries for medical conversations.\\n\u2022 We present baseline results on our dataset using various state-of-the-art models.\\n\\nAll code, data (English-translated and Vietnamese) and models are published online.\\n\\n2. Real-time Speech Summarization System\\n\\n2.1. Previous Designs\\n\\nRTSS system proposed by [1] has 3 major additional components: flexible recognizer of utterance units, utterance lookahead-er, and information overrider. Flexible recognizer of utterance units automatically split real-time Automatic Speech Recognition (ASR) transcript into segments with random lengths, while utterance lookahead-er seeks additional context from subsequent words generated by ASR, and information overrider continuously updates the summary in response to the latest contextual changes.\\n\\nWe conducted surveys and gathered feedback from engineers and found that these extra components not only extend both inference and training time but also complicate RTSS systems, making it challenging for engineers to deploy and maintain them effectively. Furthermore, the\"}"}
{"id": "leduc24_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"summary is constantly updated after each utterance generated by the ASR system. This results in increased computational costs when compared to a scenario where a solid summary is generated after a set of utterances. The continuously updated summary creates confusion as users are unable to keep track, given the uncertainty of when a summary is completed.\\n\\n2.2. Our Design\\n\\nIn contrast, our approach is much simpler. Our design generates a local summary after every N utterances of speech within a conversation and a global summary after the end of a conversation. Every local summary is generated using the corresponding local context of N utterances, without the need to continuously update the new context generated by real-time ASR utterances. Meanwhile, the global summary serves as an \u201coverrider\u201d using the context of the entire conversation.\\n\\n2.3. Balance for System Delay\\n\\nRTSS system by [1] generates a summary with a delay of one utterance. A large number of delayed utterances results in longer waiting time for users to receive the generated summary. Conversely, a low number of delayed utterances means that the context necessary for accurate summaries is missing, making summarization unnecessary. After analyzing the context within the VietMed corpus [4] and conducting our internal user survey, we found that setting N = {4, 5} (or a maximum of around 30 seconds) strikes a suitable balance. This ensures that each summary includes an adequate amount of context without keeping users waiting excessively.\\n\\n3. Data\\n\\n3.1. Labeling strategy\\n\\nWe used GPT-3.5 Turbo (or ChatGPT) to generate summaries for every transcript in our dataset, which we refer to as GPT-annotated summaries. We then split the dataset into two subsets: Gold standard (GOLD) set and the Synthetic (SYN) set. On the GOLD set, we performed human editing where the human annotator edit the GPT summary according to the annotation guideline, while on the SYN set, we did not. More information on GPT annotation is on section 4.\\n\\n3.2. Data Collection\\n\\nReal-world dataset (REAL): We choose the VietMed dataset [4], a real-world medical ASR dataset in Vietnamese, for annotating summaries. This choice is driven by the fact that VietMed currently stands as the world's largest and most generalizable publicly-available medical ASR dataset.\\n\\nSimulated dataset (SIM): To make the dataset more generalizable and to extend the scale of the existing VietMed dataset, we used extra medical text data. We simulated real-world conversations by imitating the speaking style found in the VietMed corpus. This includes incorporating hesitations, disfluencies, and stuttering words at a rate similar to that of VietMed utterances. Pseudo Python code for simulation is in the Appendix.\\n\\nOur GOLD set contains gold standard data from REAL and SIM, while SYN contains GPT summaries from the extra medical text mentioned above.\\n\\n4. GPT for Annotation\\n\\n4.1. Motivation\\n\\nTo the best of our knowledge, existing prominent Vietnamese summarization datasets, such as VietNews [6] and FAQSum [7], utilize their titles and abstracts as summaries. Our dataset, however, lacks these pre-made summaries which would traditionally require human annotation. However, finding high-quality annotators for either low-resource languages like Vietnamese or medical domain is hard [8].\\n\\nIn recent years, there has been an increasing focus on utilizing Large Language Models (LLMs) for annotation [9, 10, 11]. Experimental results from [12] showed that fully GPT-3 labeling can outperform fully human labeling in low-budget settings. GPT has shown to have adequate medical knowledge [13]. Furthermore, it achieves reasonable performance as an annotator in sequence generation tasks in Vietnamese [10].\\n\\n4.2. GPT Setup\\n\\nWe used GPT-3.5 Turbo to generate GPT summaries. Full setup details are in the Appendix.\\n\\n4.3. Cost-Efficiency Evaluation\\n\\nFollowing the design from [10, 12], we evaluated the performance difference of human-annotated summaries versus GPT.\\n\\nTable 1 shows the statistics of our dataset. To construct our VietMed-Sum dataset, we keep the original split of REAL by [4] as 5-5-6 hours for the corresponding train-dev-test set. We split our SIM set with a ratio of 8:1:1 for the corresponding train-dev-test, while the entire SYN is used for training. Acquisition and annotation of medical dataset is challenging and costly, resulting in medical summarization datasets typically being smaller compared to those in the general domain. Compared to other public medical written text summarization dataset, such as MeQSum corpus of summarized consumer health questions [5], our dataset has 23 times more summaries. Besides, compared to the Chinese medical text summarization dataset by [3], ours is half the size.\\n\\n3.5. English-translated VietMed-Sum\\n\\nWe also introduce VietMed-Sum-en, the English version of VietMed-Sum which was translated using Google Translate. Results are in the Appendix.\\n\\n4. GPT for Annotation\\n\\n4.1. Motivation\\n\\nTo the best of our knowledge, existing prominent Vietnamese summarization datasets, such as VietNews [6] and FAQSum [7], utilize their titles and abstracts as summaries. Our dataset, however, lacks these pre-made summaries which would traditionally require human annotation. However, finding high-quality annotators for either low-resource languages like Vietnamese or medical domain is hard [8].\\n\\nIn recent years, there has been an increasing focus on utilizing Large Language Models (LLMs) for annotation [9, 10, 11]. Experimental results from [12] showed that fully GPT-3 labeling can outperform fully human labeling in low-budget settings. GPT has shown to have adequate medical knowledge [13]. Furthermore, it achieves reasonable performance as an annotator in sequence generation tasks in Vietnamese [10].\\n\\n4.2. GPT Setup\\n\\nWe used GPT-3.5 Turbo to generate GPT summaries. Full setup details are in the Appendix.\\n\\n4.3. Cost-Efficiency Evaluation\\n\\nFollowing the design from [10, 12], we evaluated the performance difference of human-annotated summaries versus GPT.\"}"}
{"id": "leduc24_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: ROUGE on FAQSum on two budgets: $2.5 and $5. ViT5 is trained on the data from each method. GPT \\\\(\\\\rightarrow\\\\) Human-reuse refers to two-step finetuning on the 250 human summaries from the $2.5-budget setting while GPT \\\\(\\\\rightarrow\\\\) Human-new refers to two-step finetuning on new 250 human summaries.\\n\\n|                | R-1      | R-2      | R-L      |\\n|----------------|----------|----------|----------|\\n| $2.5 250 Human Summaries | 60.73    | 45.35    | 55.67    |\\n| 6k GPT Summaries          | 56.69    | 40.13    | 50.61    |\\n| $5 500 Human Summaries   | 62.85    | 47.19    | 57.50    |\\n| 6k GPT \\\\(\\\\rightarrow\\\\) Human-reuse | 62.88    | 47.81    | 57.67    |\\n| 6k GPT \\\\(\\\\rightarrow\\\\) Human-new    | 63.45    | 47.65    | 57.49    |\\n\\nTable 3: ROUGE scores on FAQSum on 6k human summaries, 6k GPT summaries and the previously mentioned two-step fine-tuning process of ViT5.\\n\\n|                | R-1      | R-2      | R-L      |\\n|----------------|----------|----------|----------|\\n| 6k Human Summaries | 65.80    | 50.82    | 60.83    |\\n| 6k GPT Summaries          | 56.69    | 40.13    | 50.61    |\\n| 6k GPT \\\\(\\\\rightarrow\\\\) 250 Human-reuse | 62.88    | 47.81    | 57.67    |\\n| 6k GPT \\\\(\\\\rightarrow\\\\) GPT      | 56.93    | 39.81    | 50.14    |\\n\\nTable 4 shows the ROUGE scores on our GOLD test set for the baseline models fine-tuned on the combination of GOLD local and global summaries. ViT5, ViPubmedT5, and ViT5-vietnews consistently outperforms the BARTpho variants. Results from Table 5 shows the models have a noticeable drop in performance. On the local summaries, the ROUGE scores from the BARTpho variants are much lower than that of the ViT5 variants. Conversely, when fine-tuned on the global summaries, both variants of BARTpho performs much better than the ViT5 variants except ViT5-vietnews, probably because...\"}"}
{"id": "leduc24_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Experimental results on VietMed-Sum\u2019s GOLD test set of each model fine-tuned on local + global summaries of GOLD.\\n\\nTable 5: Results on VietMed-Sum\u2019s Global and Local subset of GOLD test set. Each baseline model is fine-tuned on the global summaries (above) and the local summaries (below) of GOLD. ViT5-vietnews was previously fine-tuned on other Vietnamese abstractive summarization dataset.\\n\\nTable 6: Experimental results on VietMed-Sum\u2019s GOLD test set of each baseline model fine-tuned on SYN, (SYN + GOLD) and SYN \u2192 GOLD. + refers to concatenating the datasets while \u2192 refers to two-step fine-tuning. Bolded text refers to two best performing models.\\n\\nTable 7: Experimental results on ASR transcripts of the two best performing models from Section 6.2. Full table with all results is in the Appendix.\\n\\nWe report the results of our baseline models on the ASR transcripts on Table 7. The performance of the models is worse than that on VietMed-Sum, which we attribute to the noisy nature of the text generated by the ASR models. Nevertheless, the ROUGE scores remain fairly reasonable, which is proof to our model\u2019s robustness.\\n\\nTable 8: Results for human evaluation on 50 samples. Scores range from 1 (worst) to 5 (best). GOLD is the baseline which has all scores of 5. ViT is the best model for ROUGE scores.\"}"}
