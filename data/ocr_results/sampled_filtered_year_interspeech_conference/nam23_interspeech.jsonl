{"id": "nam23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Disentangled Representation Learning for Multilingual Speaker Recognition\\nKihyun Nam1\u2020, Youkyum Kim1\u2020, Jaesung Huh2, Hee-Soo Heo3, Jee-weon Jung4, Joon Son Chung1\\n1Korea Advanced Institute of Science and Technology, South Korea\\n2University of Oxford, United Kingdom\\n3Naver Corporation, South Korea\\n4Carnegie Mellon University, USA\\njoonsc@kaist.ac.kr\\n\\nAbstract\\nThe goal of this paper is to learn robust speaker representation for bilingual speaking scenario. The majority of the world's population speak at least two languages; however, most speaker recognition systems fail to recognise the same speaker when speaking in different languages.\\n\\nPopular speaker recognition evaluation sets do not consider the bilingual scenario, making it difficult to analyse the effect of bilingual speakers on speaker recognition performance. In this paper, we publish a large-scale evaluation set named VoxCeleb1-B derived from VoxCeleb that considers bilingual scenarios.\\n\\nWe introduce an effective disentanglement learning strategy that combines adversarial and metric learning-based methods. This approach addresses the bilingual situation by disentangling language-related information from speaker representation while ensuring stable speaker representation learning. Our language-disentangled learning method only uses language pseudo-labels without manual information.\\n\\nIndex Terms: speaker recognition, real conversation, bilingual speaking, disentangled representation learning\\n\\n1. Introduction\\nAn estimated 60 to 75 percent of the world's population speaks at least two languages [1]. While somebody is speaking in a foreign language, it has been observed that the person's voice sounds different from when speaking in their mother tongue [2]. With recent trends in globalisation, it has become easier to encounter multilingual scenarios. Therefore, the focus on multilingual speaker recognition has become more important [3\u20137].\\n\\nWhile the performance of speaker recognition systems has improved significantly due to recent advances in deep learning [8\u201315] and the availability of large-scale datasets [16,17], the state-of-the-art systems fail easily under the language mismatch condition. The popular speaker recognition evaluation sets do not consider bilingual scenarios, making it difficult to analyse their effect on speaker recognition performance. There are a few evaluation datasets that consider bilingual scenarios; however, they are collected from controlled environments like phone-call platform [3] or contain only limited languages [6]. The recent VoxCeleb Speaker Recognition Challenge (VoxSRC) [18] contains some bilingual speakers; however, their evaluation datasets remain private. Hence, to the best of our knowledge, there is no large-scale public evaluation set that takes bilingual speakers into account.\\n\\nTo this end, we publish a large-scale bilingual evaluation set derived from VoxCeleb1 [16], focusing on bilingual speaking problems. We call this test protocol VoxCeleb1-B. To increase the scale and the diversity compared to the VoxSRC challenge test set [18], we expand the number of bilingual trials and the number of languages, resulting in a total of 808,574 trials and 15 languages. Moreover, for the first time, we release the manually annotated language labels of VoxCeleb1. More details of VoxCeleb1-B and the language labels are given in Section 2.\\n\\nPrevious literature finds that a speaker's identity information is intertwined with various factors including accent [19], gender [20,21], age [21], nationality [21], emotion [22,23], and spoken language [24]. Using the proposed evaluation protocol, we observe that the existing speaker recognition models do not generalise well to bilingual speakers. We suppose that the mismatched prosodic characteristics from bilingual speakers' different languages significantly affect the performance of the speaker recognition models.\\n\\nTo resolve the language-dependent problem, traditional methods on multilingual speaker recognition have mostly utilised combination of probabilistic linear discriminant analysis and scoring functions based on a standard backbone system such as the i-vectors [5,7,25]. However, these methods do not ensure language-invariant speaker representations. Other studies [15,26\u201333] have proposed two types of disentangled representation learning methods, namely adversarial learning-based method and metric learning-based method, which isolate nuisance attributes from the speaker representation. Adversarial learning-based method disturbs convergence of non-speaker discriminator, while metric learning-based method minimises distance or similarity between speaker-relevant and non-speaker representations. For adversarial learning-based method, some studies [15,26\u201328] utilise the gradient reversal layer (GRL).\\n\\nAlthough GRL has shown performance improvement in disentanglement of the target information, we find through our experiments that it frequently causes unstable training and is sensitive to hyperparameters. On the other hand, some studies [29,31\u201333] propose metric learning-based methods to minimise correlation between speaker representation and non-speaker representations. [29] utilises mean absolute Pearson's correlation (MAPC) minimisation and [31] uses cosine similarity (COS) minimisation. [32,33] employ mutual information minimisation. However, since [29,32,33] perform domain adaptation for different domains, there is no guarantee that they will perform well in the goal of this work, namely intra-domain disentangled representation learning.\\n\\nWe evaluate the existing methods and our method on the evaluation set which reflects real-world bilingual scenario unlike [31] which conducts experiments on a simulated dataset. In this work, we propose an effective disentangled representation learning, which weakens the language-dependent information that resides in the speaker representation. The proposed learning strategy combines GRL and MAPC minimisation objective, which overcomes unstable learning and effectively learns language-disentangled speaker representation. The neural network consists of a main speaker recognition model and a spoken language recognition model. During training, language-disentangled learning leverages language pseudo-labels extracted from a spoken language recognition model.\"}"}
{"id": "nam23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Statistics of the VoxCeleb1 test sets, VoxSRC validation sets and VoxCeleb1-B.\\n\\n| Test set         | VoxCeleb1 cl. | VoxCeleb1-E cl. | VoxCeleb1-H cl. | VoxSRC 2020 Val | VoxSRC 2021 Val | VoxCeleb1-B |\\n|------------------|---------------|-----------------|-----------------|-----------------|-----------------|-------------|\\n| # of trials      | 37,611        | 579,818         | 550,894         | 263,486         | 60,000          | 808,574     |\\n| (Pos. / Neg.)    | (18,802 / 18,809) | (289,921 / 289,897) | (275,488 / 275,406) | (131,743 / 131,743) | (29,969 / 30,031) | (404,287 / 404,287) |\\n| Cross-lingual    | \u2717             | \u2717               | \u2717               | \u2717               | \u2713               | \u2713           |\\n\\nFigure 1: Overview of the training strategy. The coloured parts of the network are updated during each training procedure. Note that the Gradient Reversal Layer (GRL) is only activated during speaker embedding training procedure.\\n\\n2. Bilingual speaker recognition test set\\n\\nWe publish a large-scale bilingual speaker recognition evaluation protocol derived from VoxCeleb1 dataset [16], which is one of the widespread benchmark evaluation datasets in the recent speaker recognition field. Most of existing evaluation sets do not focus on the bilingual scenarios. Our speaker recognition evaluation set contains 808,574 trials in total. Half of the trials are intra-speaker cross-lingual and the remaining trials are inter-speaker monolingual.\\n\\n2.1. Obtaining language labels\\n\\nTo simulate the bilingual scenarios with VoxCeleb1 dataset, it is necessary to have the language labels of the utterances in the test set. We utilise language annotations of VoxCeleb1 dataset from VoxSRC 2021 [18] which are manually checked after obtaining language pseudo-labels of the utterances by using a Spoken Language Recognition (SLR) model pre-trained on VoxLingua107 [34] dataset. VoxLingua107 dataset contains 6,628 hours of speech that are divided into 107 languages.\\n\\nAssuming that a single speaker speaks only one language in a video, one audio sample is randomly sampled for each video. 15 languages including English, French, Hindi, German, Spanish, Italian, Afrikaans, Portuguese, Dutch, Korean, Urdu, Swedish, Russian, Chinese, and Arabic are annotated by annotators of various nationalities. Out of 153,516 utterances in the VoxCeleb1 dataset, 883 utterances, whose language could not be recognised by annotators, have been excluded from the proposed evaluation list.\\n\\n2.2. VoxCeleb1-B Evaluation list\\n\\nSpeaker verification evaluation protocol consists of positive and negative trials. Each trial involves an enrollment utterance and a test utterance. The trial type is decided based on whether the enrollment and the test utterances have the same speaker identity. To evaluate the robustness of speaker recognition models in the bilingual scenarios, we propose an evaluation protocol named VoxCeleb1-B, which simulates language-mismatch scenarios with a large amount of cross-lingual trials. Using the speaker and language annotations, we generate 404,287 intra-speaker cross-lingual trials and inter-speaker monolingual trials each. The number of speakers for each language and the number of samples per speaker are limited to 1,000 and 15, respectively, to avoid bias towards more frequent languages.\\n\\nTable 1 shows the statistics of existing evaluation lists derived from VoxCeleb1, and the proposed VoxCeleb1-B. The three original VoxCeleb1 test sets and the VoxSRC 2020 [35] validation set are expected to contain very few cross-lingual positive trials, whereas the VoxSRC 2021 [18] contains some cross-lingual trials. VoxCeleb1-B is explicitly designed to contain a large number of cross-lingual trials.\\n\\n3. Language-disentangled learning\\n\\nIn this section, we describe the proposed language-disentangled representation learning strategy. Our training framework is inspired by [15,36,37] and summarised in Figure 1. The network consists of a speaker embedding network that includes a speaker feature extractor and a speaker embedding layer, a speaker classifier, and a language classifier. The speaker embedding network follows the existing speaker models [38, 39] while the language classifier is attached for the purpose of a language discriminator.\\n\\nThe speaker embedding network produces frame-level embeddings $z_i$ from the input mel-spectrogram data $x_i \\\\in \\\\mathbb{R}^{T \\\\times F}$ ($1 \\\\leq i \\\\leq N$), where $T$, $F$, and $N$ are the number of frames, frequency bins, and the size of mini-batch, respectively. To derive an utterance-level vector $e_i^{S}$ from the frame-level embeddings $z_i$, we adopt Attentive Pooling Layer (APL) which includes self-attentive pooling (SAP) [40] or attentive statistics pooling (ASP) [41] as a speaker embedding layer.\\n\\nThe speaker embedding vector $e_i^{S}$ is passed as an input feature to both the speaker classifier and the language classifier, which consist of one and three fully-connected layers, respectively. We obtain the language feature vector, $e_i^{L}$, from the output of the second fully-connected layer in the language classifier. For the language classifier, the GRL is placed at the front of the language classifier and is activated in speaker embedding training step.\\n\\nThe training process of our framework alternates between two phases for the data from the same mini-batch: (1) language discriminator training, and (2) speaker embedding training. In the first phase, we train the language discriminator to recognise the spoken language from $e_i^{S}$. In the second phase, the speaker recognition network is trained to classify speakers, while intentionally trained to poorly recognise spoken languages.\"}"}
{"id": "nam23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.1. Language discriminator training\\nIn this step, we train the language classifier, while freezing the speaker recognition network. This approach can be interpreted to train the language recognition for the latest state of the speaker representation vector $e_i^S$ that has been extracted by the speaker embedding layer. The objective function $L_{\\\\text{lang}}$ of the language classifier is a categorical cross-entropy loss. In Figure 1a, the parts of the network coloured in blue are optimised by $L_{\\\\text{lang}}$.\\n\\n3.2. Speaker embedding training\\nIn this step, we train the speaker recognition network with language-disentangled representation learning. The language classifier's parameters are not updated at this stage. The total loss function to train the language-disentangled speaker recognition model can be formulated as follows.\\n\\n$$L_{\\\\text{total}} = L_{\\\\text{spk}} + L_{\\\\text{de}}$$ (1)\\n\\nwhere $L_{\\\\text{spk}}$ is an objective function for the speaker recognition and $L_{\\\\text{de}}$ is an objective function of the disentangled representation learning. For $L_{\\\\text{spk}}$, we can utilise objective functions such as softmax loss, prototypical loss, and contrastive loss, which have been employed in the previous works [38, 39]. For prototypical loss and contrastive loss, we exclude the speaker classifier since these losses are directly derived from the speaker embedding vectors rather than speaker logits. For $L_{\\\\text{de}}$, we can apply objective functions from two types of learning methods, namely adversarial learning-based method and metric learning-based method. In this work, we select gradient reversal layer as an adversarial learning-based method, and metric learning-based methods include cosine similarity minimisation and mean absolute Pearson's correlation minimisation. The details of each method are as follows.\\n\\n**Gradient Reversal Layer (GRL).** Gradient reversal layer inverts the gradient value of target loss function to opposite sign for disturbing the convergence of the target loss function. In our work, the target loss function is $L_{\\\\text{lang}}$.\\n\\n**Cosine similarity (COS) minimisation.** This method minimises cosine similarity between speaker embedding vector $e_i^S$ and language feature vector $e_i^L$.\\n\\n**Mean Absolute Pearson's Correlation (MAPC) minimisation.** This method minimises mean absolute Pearson's correlation [29] between speaker embedding vector and language feature vector. In our work, $L_{\\\\text{corr}}$ can be formulated as follows.\\n\\n$$L_{\\\\text{corr}} = \\\\frac{1}{F} \\\\sum_{i=1}^{N} \\\\sum_{j=1}^{F} \\\\frac{|\\\\text{Cov}(e_{i,j}^S, e_{i,j}^L)|}{\\\\sigma(e_{i,j}^S) \\\\cdot \\\\sigma(e_{i,j}^L)}$$ (2)\\n\\nwhere $\\\\text{Cov}(\\\\cdot)$ is the covariance and $\\\\sigma(\\\\cdot)$ is the standard deviation. $F$ is the dimensionality of the embedding vector $e_i$.\\n\\n**Ours.** We propose an effective disentangled representation learning that consists of GRL and MAPC minimisation. The total loss function $L_{\\\\text{total}}$ of our method can be formulated as follows.\\n\\n$$L_{\\\\text{total}} = L_{\\\\text{spk}} + L_{\\\\text{corr}} + \\\\lambda L_{\\\\text{lang}}$$ (3)\"}"}
{"id": "nam23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We would like to thank Icksang Han and Bong-Jin Lee for helpful comments.\\n\\nTable 2: Equal Error Rates (EER) and minimum Detection Cost Function (minDCF) on (a) VoxSRC 2021 validation set, VoxCeleb1-B, (b) VoxCeleb1-E test set, and VoxSRC 2020 validation set. Accuracy of Spoken Language Recognition (SLR Acc.) is computed on VoxCeleb1-B.\\n\\nAs a result, we verify that existing disentangled representation learning strategies applied to cross-domain tasks do not guarantee robust language-disentangled speaker representation learning. This highlights the drawback of GRL, which tends to induce unstable training despite its effectiveness in removing the language information from the speaker representation.\\n\\nThe use of pseudo-labels significantly performance improvements under bilingual scenarios. This indicates that it can be cost-effective to use pseudo-labels of specific models. This highlights the drawback of GRL, which tends to induce unstable training despite its effectiveness in removing the language information from the speaker representation.\\n\\nLanguage-disentangled speaker representation. To verify whether the language information is separated from the speaker representation, we evaluate a spoken language recognition model trained from scratch with the speaker embedding vector extracted from each model as input data. The structure of the spoken language recognition model is the same as the language classifier described in Section 3. As shown in Table 2a (SLR Acc.), our method shows lower spoken language recognition performance than the baselines of two models. This highlights that the proposed method successfully resolves the bilingual problem. Our learning strategy disentangles language information from the speaker representation in order to make the embeddings robust to cross-lingual trials. Our proposed learning strategy shows significant performance improvements under bilingual scenarios, while remaining effective on existing test sets.\\n\\nWe have developed strategies to train speaker embeddings that are effective on the other evaluation sets. Especially, in the case of ResNet-S, our method shows the best performance on VoxSRC validation sets and VoxCeleb1-B, respectively, while the performance degrades on the other evaluation sets. Furthermore, ResNet-S and ResNet-L exhibit significant performance improvements of 33\\\\% and 12\\\\% on VoxCeleb1-B and VoxCeleb1-E, respectively, from the mean and the standard deviation on every evaluation set including VoxCeleb1-B.\\n\\nWe observe that the metric learning-based methods, COS and MAPC minimisation, outperform the baselines on VoxCeleb1. However, we observe that the metric learning-based methods, COS and MAPC minimisation, outperform the baselines on VoxCeleb1. Nonetheless, the proposed learning strategy works successfully, showing significant performance improvements in bilingual scenarios. This demonstrates that the proposed learning strategy overcomes the limitations of existing disentangled representation learning methods and facilitates robust language-disentangled speaker representation learning.\\n\\nResults on cleaned version of VoxCeleb1 test sets and VoxSRC 2020 validation set. The proposed learning method on ResNet-L outperforms the baselines and all existing methods on most of evaluation sets except for spoken language recognition. This illustrates that the proposed learning strategy works successfully, showing significant performance improvements in bilingual scenarios. This demonstrates that the proposed learning strategy overcomes the limitations of existing disentangled representation learning methods and facilitates robust language-disentangled speaker representation learning.\"}"}
{"id": "nam23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] G. Vince, \u201cThe amazing benefits of being bilingual,\u201d BBC, 2016.\\n\\n[2] B. Lee and D. V. L. Sidtis, \u201cThe bilingual voice: Vocal characteristics when speaking two languages across speech tasks,\u201d Speech, Language and Hearing, vol. 20, no. 3, pp. 174\u2013185, 2017.\\n\\n[3] C. Cieri, L. Corson, D. Graff, and K. Walker, \u201cResources for new research directions in speaker recognition: the mixer 3, 4 and 5 corpora.\u201d in Proc. Interspeech. Citeseer, 2007, pp. 950\u2013953.\\n\\n[4] M. Akbacak and J. H. Hansen, \u201cLanguage normalization for bilingual speaker recognition systems,\u201d in Proc. ICASSP, vol. 4. IEEE, 2007, pp. IV \u2013257.\\n\\n[5] L. Ferrer, H. Bratt, L. Burget, H. Cernocky, O. Glembek, M. Graciarena, A. Lawson, Y. Lei, P. Matejka, O. Plchot et al., \u201cPromoting robustness for speaker modeling in the community: the prism evaluation set,\u201d in Proceedings of NIST 2011 workshop. Citeseer, 2011, pp. 1\u20137.\\n\\n[6] D. Reynolds, E. Singer, S. O. Sadjadi, T. Kheyrkhah, A. Tong, C. Greenberg, L. Mason, and J. Hernandez-Cordero, \u201cThe 2016 nist speaker recognition evaluation,\u201d MIT Lincoln Laboratory Lexington United States, Tech. Rep., 2017.\\n\\n[7] P. Matejka, O. Novotn\u00b4y, O. Plchot, L. Burget, M. D. S\u00e1nchez, and J. Cernock\u00b4y, \u201cAnalysis of score normalization in multilingual speaker recognition.\u201d in Proc. Interspeech, 2017, pp. 1567\u20131571.\\n\\n[8] K. He, X. Zhang, S. Ren, and J. Sun, \u201cDeep residual learning for image recognition,\u201d in Proc. CVPR, 2016, pp. 770\u2013778.\\n\\n[9] D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur, \u201cX-vectors: Robust dnn embeddings for speaker recognition,\u201d in Proc. ICASSP. IEEE, 2018, pp. 5329\u20135333.\\n\\n[10] J.-W. Jung, H.-S. Heo, I.-H. Yang, H.-J. Shim, and H.-J. Yu, \u201cA complete end-to-end speaker verification system using deep neural networks: From raw signals to verification result,\u201d in Proc. ICASSP. IEEE, 2018, pp. 5349\u20135353.\\n\\n[11] M. Ravanelli and Y. Bengio, \u201cSpeaker recognition from raw waveform with sincnet,\u201d in IEEE Spoken Language Technology workshop. IEEE, 2018, pp. 1021\u20131028.\\n\\n[12] D. Snyder, D. Garcia-Romero, G. Sell, A. McCree, D. Povey, and S. Khudanpur, \u201cSpeaker recognition for multi-speaker conversations using x-vectors,\u201d in Proc. ICASSP. IEEE, 2019, pp. 5796\u20135800.\\n\\n[13] J. Snell, K. Swersky, and R. Zemel, \u201cPrototypical networks for few-shot learning,\u201d in NeurIPS, vol. 30, 2017.\\n\\n[14] P. Khosla, P. Teterwak, C. Wang, A. Sarna, Y. Tian, P. Isola, A. Maschinot, C. Liu, and D. Krishnan, \u201cSupervised contrastive learning,\u201d in NeurIPS, vol. 33, 2020, pp. 18 661\u201318 673.\\n\\n[15] J. Kang, J. Huh, H. S. Heo, and J. S. Chung, \u201cAugmentation adversarial training for self-supervised speaker representation learning,\u201d in IEEE Journal of Selected Topics in Signal Processing, 2022.\\n\\n[16] A. Nagrani, J. S. Chung, W. Xie, and A. Zisserman, \u201cVoxceleb: Large-scale speaker verification in the wild,\u201d Computer Speech & Language, vol. 60, p. 101027, 2020.\\n\\n[17] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxCeleb2: Deep Speaker Recognition,\u201d in Proc. Interspeech, 2018, pp. 1086\u20131090.\\n\\n[18] A. Brown, J. Huh, J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxsrc 2021: The third VoxCeleb speaker recognition challenge,\u201d arXiv preprint arXiv:2201.04583, 2022.\\n\\n[19] D. Raj, D. Snyder, D. Povey, and S. Khudanpur, \u201cProbing the information encoded in x-vectors,\u201d in IEEE Automatic Speech Recognition and Understanding workshop. IEEE, 2019.\\n\\n[20] C. Luu, P. Bell, and S. Renals, \u201cLeveraging Speaker Attribute Information Using Multi Task Learning for Speaker Verification and Diarization,\u201d in Proc. Interspeech, 2021, pp. 491\u2013495.\\n\\n[21] C. Luu, S. Renals, and P. Bell, \u201cInvestigating the contribution of speaker attributes to speaker separability using disentangled speaker representations,\u201d in Proc. Interspeech, 2022.\\n\\n[22] J. Williams and S. King, \u201cDisentangling style factors from speaker representations.\u201d in Proc. Interspeech, 2019, pp. 3945\u20133949.\\n\\n[23] R. Pappagari, T. Wang, J. Villalba, N. Chen, and N. Dehak, \u201cx-vectors meet emotions: A study on dependencies between emotion and speaker recognition,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7169\u20137173.\\n\\n[24] S. Maiti, E. Marchi, and A. Conkie, \u201cGenerating multilingual voices using speaker space translation based on bilingual speaker data,\u201d in Proc. ICASSP. IEEE, 2020, pp. 7624\u20137628.\\n\\n[25] A. Misra and J. H. Hansen, \u201cSpoken language mismatch in speaker verification: An investigation with nist-sre and crss bi-ling corpora,\u201d in IEEE Spoken Language Technology workshop. IEEE, 2014, pp. 372\u2013377.\\n\\n[26] Z. Meng, Y. Zhao, J. Li, and Y. Gong, \u201cAdversarial speaker verification,\u201d in Proc. ICASSP. IEEE, 2019, pp. 6216\u20136220.\\n\\n[27] W. Xia, J. Huang, and J. H. Hansen, \u201cCross-lingual text-independent speaker verification using unsupervised adversarial discriminative domain adaptation,\u201d in Proc. ICASSP. IEEE, 2019, pp. 5816\u20135820.\\n\\n[28] D. Xin, Y. Saito, S. Takamichi, T. Koriyama, and H. Saruwatari, \u201cCross-lingual speaker adaptation using domain adaptation and speaker consistency loss for text-to-speech synthesis.\u201d in Proc. Interspeech, 2021, pp. 1614\u20131618.\\n\\n[29] W. H. Kang, S. H. Mun, M. H. Han, and N. S. Kim, \u201cDisentangled speaker and nuisance attribute embedding for robust speaker verification,\u201d IEEE Access, vol. 8, pp. 141 838\u2013141 849, 2020.\\n\\n[30] Y. Kwon, S. W. Chung, and H. G. Kang, \u201cIntra-class variation reduction of speaker representation in disentanglement framework,\u201d in Proc. Interspeech, vol. 2020, 2020, pp. 3231\u20133235.\\n\\n[31] F. Tong, S. Zheng, H. Zhou, X. Xie, Q. Hong, and L. Li, \u201cDeep Representation Decomposition for Rate-Invariant Speaker Verification,\u201d in Proc. Speaker Odyssey, 2022, pp. 228\u2013232.\\n\\n[32] L. Yi and M.-W. Mak, \u201cDisentangled speaker embedding for robust speaker verification,\u201d in Proc. ICASSP. IEEE, 2022, pp. 7662\u20137666.\\n\\n[33] S. H. Mun, M. H. Han, M. Kim, D. Lee, and N. S. Kim, \u201cDisentangled speaker representation learning via mutual information minimization,\u201d in Proc. APSIPA ASC. IEEE, 2022.\\n\\n[34] J. Valk and T. Alum\u00e4e, \u201cVoxlingua107: a dataset for spoken language recognition,\u201d in IEEE Spoken Language Technology workshop. IEEE, 2021, pp. 652\u2013658.\\n\\n[35] A. Nagrani, J. S. Chung, J. Huh, A. Brown, E. Coto, W. Xie, M. McLaren, D. A. Reynolds, and A. Zisserman, \u201cVoxsrc 2020: The second VoxCeleb speaker recognition challenge,\u201d arXiv preprint arXiv:2012.06867, 2020.\\n\\n[36] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \u201cAdversarial discriminative domain adaptation,\u201d in Proc. CVPR, 2017, pp. 7167\u20137176.\\n\\n[37] J. S. Chung, J. Huh, and S. Mun, \u201cDelving into VoxCeleb: environment invariant speaker recognition,\u201d in Proc. Speaker Odyssey, 2020.\\n\\n[38] J. S. Chung, J. Huh, S. Mun, M. Lee, H.-S. Heo, S. Choe, C. Ham, S. Jung, B.-J. Lee, and I. Han, \u201cIn Defence of Metric Learning for Speaker Recognition,\u201d in Proc. Interspeech, 2020, pp. 2977\u20132981.\\n\\n[39] Y. Kwon, H.-S. Heo, B.-J. Lee, and J. S. Chung, \u201cThe ins and outs of speaker recognition: lessons from voxsrc 2020,\u201d in Proc. ICASSP. IEEE, 2021, pp. 5809\u20135813.\\n\\n[40] W. Cai, J. Chen, and M. Li, \u201cExploring the Encoding Layer and Loss Function in End-to-End Speaker and Language Recognition System,\u201d in Proc. Speaker Odyssey, 2018, pp. 74\u201381.\\n\\n[41] K. Okabe, T. Koshinaka, and K. Shinoda, \u201cAttentive Statistics Pooling for Deep Speaker Embedding,\u201d in Proc. Interspeech, 2018, pp. 2252\u20132256.\\n\\n[42] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin, N. Gimelshein, L. Antiga et al., \u201cPytorch: An imperative style, high-performance deep learning library,\u201d in NeurIPS, vol. 32, 2019.\\n\\n[43] D. P. Kingma, J. Ba, Y. Bengio, and Y. LeCun, \u201cAdam: A method for stochastic optimization,\u201d in Proc. ICLR, 2015.\\n\\n[44] O. Sadjadi, C. Greenberg, E. Singer, D. Reynolds, L. Mason, and J. Hernandez-Cordero, \u201cThe 2018 nist speaker recognition evaluation,\u201d in Proc. Interspeech, 2019.\"}"}
