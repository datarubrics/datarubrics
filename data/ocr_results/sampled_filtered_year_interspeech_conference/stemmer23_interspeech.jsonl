{"id": "stemmer23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[2] B. Wrede and E. Shriberg, \u201cSpotting \u201chot spots\u201d in meetings: Human judgments and prosodic cues,\u201d in INTERSPEECH, 2003.\\n\\n[3] A. Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Morgan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, and C. Wooters, \u201cThe icsi meeting corpus,\u201d in 2003 IEEE International Conference on Acoustics, Speech, and Signal Processing, 2003. Proceedings. (ICASSP \u201903)., vol. 1, 2003, pp. I\u2013I.\\n\\n[4] I. Mccowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos, M. Kronenthal, G. Lathoud, M. Lincoln, A. Lisowska Masson, W. Post, D. Reidsma, and P. Wellner, \u201cThe ami meeting corpus,\u201d Int\u2019l. Conf. on Methods and Techniques in Behavioral Research, 01 2005.\\n\\n[5] V. Rajan, A. Brutti, and A. Cavallaro, \u201cConflictnet: End-to-end learning for speech-based conflict intensity estimation,\u201d IEEE Signal Processing Letters, vol. 26, no. 11, pp. 1668\u20131672, 2019.\\n\\n[6] J. Gillick, W. Deng, K. Ryokai, and D. Bamman, \u201cRobust laughter detection in noisy environments,\u201d in Interspeech 2021, 08 2021, pp. 2481\u20132485.\\n\\n[7] S. wen Yang, P.-H. Chi, Y.-S. Chuang, C.-I. J. Lai, K. Lakhotia, Y. Y. Lin, A. T. Liu, J. Shi, X. Chang, G.-T. Lin, T.-H. Huang, W.-C. Tseng, K. tik Lee, D.-R. Liu, Z. Huang, S. Dong, S.-W. Li, S. Watanabe, A. Mohamed, and H. yi Lee, \u201cSUPERB: Speech Processing Universal PERformance Benchmark,\u201d in Proc. Interspeech 2021, 2021, pp. 1194\u20131198.\\n\\n[8] M. Filippone, S. Kim, F. Valente, and A. Vinciarelli, \u201cPredicting the conflict level in television political debates: an approach based on crowdsourcing, nonverbal communication and gaussian processes,\u201d in MM 2012 - Proceedings of the 20th ACM International Conference on Multimedia, 10 2012.\\n\\n[9] J. Godfrey, E. Holliman, and J. McDaniel, \u201cSwitchboard: telephone speech corpus for research and development,\u201d in ICASSP-92: 1992 IEEE International Conference on Acoustics, Speech, and Signal Processing, vol. 1, 1992, pp. 517\u2013520 vol.1.\\n\\n[10] C. Busso, M. Bulut, C.-C. Lee, E. A. Kazemzadeh, E. M. Provost, S. Kim, J. N. Chang, S. Lee, and S. S. Narayanan, \u201cIemocap: interactive emotional dyadic motion capture database,\u201d Language Resources and Evaluation, vol. 42, pp. 335\u2013359, 2008.\\n\\n[11] K. Laskowski, \u201cModeling vocal interaction for text-independent detection of involvement hotspots in multi-party meetings,\u201d in 2008 IEEE Spoken Language Technology Workshop, 2008, pp. 81\u201384.\\n\\n[12] D. Makhervaks, W. Hinthorn, D. Dimitriadis, and A. Stolcke, \u201cCombining acoustics, content and interaction features to find hot spots in meetings,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 8054\u20138058.\\n\\n[13] B. Schuller, B. Vlasenko, F. Eyben, M. W\u00f6llmer, A. Stuhlsatz, A. Wendemuth, and G. Rigoll, \u201cCross-corpus acoustic emotion recognition: Variances and strategies,\u201d IEEE Transactions on Affective Computing, vol. 1, no. 2, pp. 119\u2013131, 2010.\\n\\n[14] T. X. Z. X. Zhang S, Liu R, \u201cDeep cross-corpus speech emotion recognition: Recent advances and perspectives.\u201d Frontiers in Neuro Robotics, November 2021.\\n\\n[15] S. Parthasarathy and C. Busso, \u201cSemi-supervised speech emotion recognition with ladder networks,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 2697\u20132709, 2020.\\n\\n[16] R. Milner, M. A. Jalal, R. W. M. Ng, and T. Hain, \u201cA cross-corpus study on speech emotion recognition,\u201d in 2019 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2019, pp. 304\u2013311.\\n\\n[17] B. Wrede, S. Bhagat, R. Dhillon, and E. Shriberg, \u201cMeeting recorder project: Hot spot labeling guide, ICSI technical report TR-05-004,\u201d Tech. Rep., 2005.\\n\\n[18] J. Eggink, \u201cKrippendorff\u2019s alpha,\u201d 2023, MATLAB Central File Exchange. Retrieved March 4, 2023. [Online]. Available: https://www.mathworks.com/matlabcentral/fileexchange/36016-krippendorff-s-alpha\\n\\n[19] J. Kahn, M. Rivi`ere, W. Zheng, E. Kharitonov, Q. Xu, P. E. Mazar\u00b4e, J. Karadayi, V. Liptchinsky, R. Collobert, C. Fuegen, T. Likhomanenko, G. Synnaeve, A. Joulin, A. Mohamed, and E. Dupoux, \u201cLibri-light: A benchmark for asr with limited or no supervision,\u201d in ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 7669\u20137673, https://github.com/facebookresearch/libri-light.\\n\\n[20] M.-J. Caraty and C. Montaci`e, Detecting Speech Interruptions for Automatic Conflict Detection. Springer International Publishing, 2015, pp. 377\u2013401. [Online]. Available: https://doi.org/10.1007/978-3-319-14081-0\\n\\n[21] W. Kirch, Ed., Pearson's Correlation Coefficient. Dordrecht: Springer Netherlands, 2008, pp. 1090\u20131091.\\n\\n[22] W.-N. Hsu, B. Bolte, Y.-H. H. Tsai, K. Lakhotia, R. Salakhutdinov, and A. Mohamed, \u201cHubert: Self-supervised speech representation learning by masked prediction of hidden units,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 29, pp. 3451\u20133460, 2021.\\n\\n[23] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, M. Funtowicz, J. Davison, S. Shleifer, P. von Platen, C. Ma, Y. Jernite, J. Plu, C. Xu, T. Le Scao, S. Gugger, M. Drame, Q. Lhoest, and A. Rush, \u201cTransformers: State-of-the-art natural language processing,\u201d in Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations. Online: Association for Computational Linguistics, Oct. 2020, pp. 38\u201345. [Online]. Available: https://aclanthology.org/2020.emnlp-demos.6\"}"}
{"id": "stemmer23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach\\nGeorg Stemmer1, Paulo Lopez Meyer2, Juan Del Hoyo Ontiveros3, Jose A. Lopez4, Hector A. Cordourier Maruri5, Tobias Bocklet6\\n\\n1, 6 Intel Corp., Germany\\n2, 3, 5 Intel Corp., Mexico\\n4 Intel Corp., USA\\nfirstname.lastname@intel.com\\n\\nAbstract\\nSpeech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach.\\n\\nIndex Terms: emotion recognition, human-computer interaction, computational paralinguistics.\\n\\n1. Introduction\\nSpeech does not only convey words but also paralinguistic information, e.g., a speaker's emotional state. This information typically gets lost in the transcript of a conversation. A computer's ability to understand a human conversation, or to communicate with humans, could be significantly improved by enriching transcripts using speech emotion recognition (SER). Various practical applications can be imagined, e.g., more comprehensive meeting summaries, detecting communication problems, or improved human-computer interaction. Even though SER has been studied for decades [1], very few such applications exist today. We believe that one of the reasons is a scarcity of research on SER for natural, not-acted data, where the experimental setup has been designed to match an actual application.\\n\\nThe goal of the work described in this paper was to create a baseline system for enriching transcripts of technical meetings with emotional information extracted from the speech signal. More specifically, the system detects emotional hotspots which are defined as regions of increased speaker involvement [2].\\n\\nWe evaluate the accuracy of the proposed system on two popular corpora, the ICSI Meeting Corpus [3], and the AMI Meeting Corpus [4], created by recording and annotating project meetings at research institutes in US and Europe. Due to the realistic setup when collecting the datasets, emotional events, including hotspots, occur relatively rarely. The sparseness of events, combined with the limited size of the corpora, increases the risk of overfitting a model if trained on the meeting corpora themselves. The models may learn invisible biases, e.g., caused by the correlation of speakers or discussion topics with emotional events. To ensure that evaluation results reflect a model's ability to detect emotional events, and nothing else, we follow a cross-corpus approach where the test corpora are not used for training. According to [2], hotspots are related to the emotional dimension of 'activation', or 'arousal'. Therefore, our approach is based on a set of models trained for different types of labels but generate scores that are expected to correlate with the arousal of a speaker. Hotspot detection is performed by combining models developed and published by others for conflict intensity estimation [5], laughter detection [6], and emotion classification [7]. These models have been trained on political discussions from the SSPNet Conflict Corpus [8], telephony conversations from the Switchboard corpus [9], and fine-tuned for acted emotions contained in the IEMOCAP data set [10]. Models are combined using a linear interpolation of scores, where the weights are optimized on a development partition of the ICSI meeting corpus. We also investigate the adaptation of the conflict intensity estimation model.\\n\\n1.1. Related work\\nFor a more detailed acoustic-prosodic analysis of the hotspots in the ICSI meeting corpus, we refer the reader to [2]. An early work for automatic hotspot detection in the ICSI meeting corpus has been described by Laskowski [11]. The author leveraged the available reference speech and laughter segmentations to extract low-level vocal activity features, which are fed into a classifier. More recently, automatic hotspot detection for the ICSI meeting corpus has been investigated by Makhervaks et al. in [12]. Similar to our work, a score combination is proposed to detect the hotspots. Prosodic features are extracted from the speech signal and combined with speaker activity, lexical, and laughter count features. This is in contrast to our work where we do not use any manual transcription, lexical or prosodic feature extraction, or even use the ICSI meeting corpus for training. The authors [12] mention that manual transcriptions and the lack of validation on an independent meeting corpus make it difficult to judge whether the performance figures reported could be achieved in a real application.\\n\\nApplying a cross-corpus approach to address the issue of generalization in SER has already been proposed by others, see, e.g., [13] for one of the earliest works in this area. A recent literature review on this subject has been published by Zhang et al. in [14]. Most of the results are for acted emotions, which are difficult to transfer to real-world applications [1]. To best of our knowledge, hotspot detection in meetings has not been explored using a cross-corpus approach. In cross-corpus SER, different methods have been proposed to reduce the mismatch between training and test corpora. One popular research direction is the use of transfer learning techniques.\"}"}
{"id": "stemmer23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"rection is to improve the generalization of the models, e.g., by multi-task learning with large amounts of unlabeled data [15]. Other approaches leverage transfer learning to improve models pre-trained on mismatched data, see e.g., [16]. The semi-supervised adaptation procedure described in this paper is comparable to the latter, except that we had to adjust the method to deal with the reference labels' sparsity by including the scores of the pre-trained model.\\n\\n1.2. Contributions\\n\\nThe main contributions of the paper are the following: To our knowledge, this paper is the first to report hotspot detection results for ICSI and AMI meeting corpora using a cross-corpus approach. It demonstrates that robust hotspot detection on natural, realistic data can be performed with a simple shallow ensemble of various models trained on highly mismatched data. In addition, we introduce an adaptation method for the conflict estimation model that is robust to the highly imbalanced label distribution. Finally, we provide a new labeling of a subset of the AMI meeting corpus.\\n\\n2. Data\\n\\n2.1. ICSI Meeting Corpus\\n\\nThe ICSI meeting corpus [3] is a collection of 75 natural, i.e., not-acted, meetings at the ICSI research institute, totaling about 72 hours (including silence). All meetings were in the English language. A significant proportion of speakers is non-native. Further details on speaker demographics, recordings setup, transcription can be found in [3]. Here we focus on how the data has been prepared for our experiments. The meetings have been recorded with desktop and headset microphones; each headset channel corresponds to a speaker. For each channel, an utterance segmentation is provided. Hotspot labels are provided for each speaker and each segment. Hotspots can have different intensity levels (hotness): the three primary levels are 'lukewarm', 'warm', and 'hot'. Each label contains additional information, e.g., valence, which is ignored here. Details can be found in [2, 17].\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the segment is assigned the 'hotter' label. Segments that overlap with a hotspot, but are not a hotspot, are removed. Overlap is defined as having overlapping start/end times or a gap of less than 0.5 seconds. As concise segments often corresponded to noises, we removed all segments with a duration of <1 second. The average duration of the remaining segments is 10 seconds.\\n\\nAs we found the audio quality of the desktop microphone channels unacceptable, we mixed all headset microphone channels into a single channel. The following procedure ensures no contradicting labels when mapping the speaker-specific hotspot labels to the mixed channel: All hotspot segments that overlap in time get merged. If the intensity level of those segments differs, the"}
{"id": "stemmer23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model parameters, are optimized on the AMI corpus. However, the same values optimized for the development partition of ICSI are applied when testing with the AMI corpus. Thus, AMI is used as an independent test corpus. The corresponding label distributions are shown in Tab. 3. Interestingly, we have a much higher percentage of hotspots in the AMI corpus than in the ICSI corpus; 39% of all segments receive a hotspot label. This may be due to the raters having a higher sensitivity for hotspots or other reasons, like the AMI meeting participants being generally more 'involved'. We plan to investigate this as soon as we have completed labeling a more significant proportion of the corpus.\\n\\nTable 3: Label distributions for the prepared AMI data.\\n\\n| Test | # Meetings | # Segments | # hotspots | Total duration [h] |\\n|------|------------|------------|------------|-------------------|\\n|      | 26         | 433        | 169        | 3.6               |\\n\\n2.3. Training Corpora\\n\\nThe conflict intensity estimation model has been trained on the SSPNET Conflict Corpus [8]. The corpus comprises 12 hours of French political debates, recorded on television. The recordings were cut into 30-second long segments and assigned a conflict intensity score by averaging the estimates provided by ten different raters who were not speaking French.\\n\\nThe laughter detection model has been trained on the Switchboard Corpus [9], a collection of 240 hours of transcribed, spontaneous telephony conversations in English. Each conversation is about five minutes long. The transcripts contain laughter annotations including detailed start and end times.\\n\\nThe emotion classification model has been pretrained on the Libri-Light [19] corpus, a collection of 60k hours of audiobooks in English. For finetuning, the IEMOCAP dataset [10] has been employed. IEMOCAP contains about 12 hours of read and spontaneous English speech from ten actors. The acted emotions have been annotated with ten different categorical labels. For creating the emotion classification model the popular setup of just four emotion classes as targets has been deployed: sadness, happiness+excitement, anger, and neutral.\\n\\n3. Experimental Setup\\n\\nICSI and AMI meeting corpora are not used for model training. Instead, we re-use three different models developed and published by others for other data: A conflict intensity estimator, a laughter detector, and an emotion classifier. The following sections provide a short overview of these models, focusing on how we applied them to the meeting datasets.\\n\\n3.1. Conflict Intensity Estimation\\n\\nConflict detection in human communication involves identifying speech with opposite or negative verbal cues [20]. Rajan et al. [5] introduced a convolutional-recurrent architecture with an attention mechanism for estimating the intensity of conflict in a conversation. The work achieved state-of-the-art results for the SSPNet Conflict Corpus at the time of publication. The end-to-end network, which they call ConflictNET, directly processes a 30-second segment of speech samples. Feature extraction is replaced by several convolutional layers which learn the relevant speech properties from the training data. An attention layer enables the network to learn acoustic events which do not span the whole input segment. The network is trained to maximize the Pearson Correlation Coefficient [21], i.e., a linear correlation between the single output neuron and a conflict intensity measure that, after scaling, ranges between $-1$ and $+1$. We leverage the code published by the authors [1]. The following changes were applied to the code: First, the input signal is not downsampled but the full 16 kHz sampling rate is used, doubling the input segment size. Second, the input signal's Root Mean Square (RMS) amplitude is not normalized anymore. Third, when applying ConflictNet to the ICSI corpus, segments with a length of less than 30 seconds are not zero-padded but repeated until the whole input segment is filled with audio samples.\\n\\n3.2. Laughter Detection\\n\\nFor laughter detection we applied a model published by Gillick et al. [6]. It is based on a ResNet-18 architecture which processes 128-dimensional Mel spectrograms as features. The model has been trained on data from the Switchboard corpus, which contains annotations for laughter. We directly downloaded the model trained and shared by the authors [2]. We processed all segments of the meeting corpora using the default parameter settings, i.e., a detection threshold of $0.5$ and a minimum laughter length of $0.2$ seconds. Preliminary experiments indicated that there is some potential in tuning these parameters for our test corpora, which is something we are going to investigate in the future.\\n\\n3.3. Emotion Classification\\n\\nFor the emotion classification we downloaded a pretrained large model with HuBERT topology [22] from Huggingface [3]. The model has been fine-tuned over the 4-class (neutral, sad, angry, happy) IEMOCAP dataset with a leave-one-session-out training strategy. The development of this component leveraged the s3prl open source toolkit [4].\\n\\n3.4. Evaluation Measures\\n\\nAs we deal with a detection problem, we evaluate a model by computing False Accept Rates (FAR) and False Rejection Rates (FRR) for a reasonable range of detection thresholds. FRR represents the proportion of false negatives, i.e., segments in a test set that are scored below the threshold but are a hotspot. FAR is the proportion of false positives, i.e., segments scored equal to or above the threshold but are not a hotspot. Equal Error Rate (EER) is the error rate where FAR and FRR are equal. The lower the EER, the better the system. We also report the Unweighted Average Recall (UAR), which is the average of the proportions of the true positives, i.e., hotspot segments that are scored above the threshold, and true negatives, i.e., segments that are scored below the threshold and which are not a hotspot. The higher the UAR, the better the system. The UAR reported for a given model is always the highest UAR that could be achieved for all detection thresholds.\\n\\n1. https://github.com/smartcameras/ConflictNET\\n2. https://github.com/jrgillick/laughter-detection\\n3. https://huggingface.co/facebook/hubert-large-ll60k\\n4. https://github.com/s3prl/s3prl/tree/main/s3prl/downstream/emotion\"}"}
{"id": "stemmer23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.5. Model Ensemble\\nIf we consider each of the models for conflict intensity estimation, laughter detection, and emotion classification as sources of useful information about the arousal level of an utterance, it makes sense to try to combine them to get improved results. In preliminary experiments we tried different ways to combine the models. Ultimately, it worked best to normalize each model's scores by subtracting a model-specific mean and dividing by a model-specific standard deviation. The normalized scores of the different models are then combined by linear interpolation. The optimal interpolation weights are determined on the ICSI development set using an exhaustive search that tries out all interpolation weights between $-1.0$ and $1.0$ in steps of $0.25$. For six different scores $966 \\\\approx 0.5$, $M$ combinations are evaluated, and the one that leads to the highest UAR is selected. Our experiments showed that many weight combinations lead to very similar UARs. We also tried minimizing EER instead of maximizing the UAR, which made no difference.\\n\\n3.6. Semi-Supervised Model Adaptation\\nWe investigated whether adapting the ConflictNET model to the ICSI development set can improve results. As the hotspots rarely occur, simply re-training or finetuning using existing labels impairs the model. Therefore we first scored the ICSI development set with the pre-trained ConflictNET model. Then we heuristically modified the scores depending on the labels. If a segment was labeled as 'lukewarm', we increased the score by 1. For segments labeled 'warm', we increased the score by 2. Finally, for segments labeled 'hot', we increased by 3. We then finetuned the ConflictNET model on the modified scores. This semi-supervised method ensures the model does not deviate significantly from the pre-training, but can adapt to the hotspots in the ICSI development set.\\n\\n4. Results and Discussion\\nThe results for the different models, individually and in combination, are shown in Tab. 4.\\n\\n|                         | ICSI Test | AMI Test |\\n|-------------------------|-----------|----------|\\n| EER [%]                 | 23.4      | 20.9     |\\n| UAR [%]                 | 77.3      | 72.1     |\\n| Model comb.             |           |          |\\n| Conflict                | 25.6      | 75.1     |\\n| Laughter                | 35.8      | 64.2     |\\n| Happy                   | 31.9      | 69.0     |\\n| Sad                     | 59.8      | 50.0     |\\n| Angry                   | 45.5      | 55.2     |\\n| Neutral                 | 65.1      | 50.0     |\\n| All                     | 23.4      | 77.3     |\\n| Confl. adapt. to ICSI Dev. | 22.0  | 79.2     |\\n| All with adapt. Confl.  | 20.9      | 79.5     |\\n\\nThe conflict intensity score has the lowest EER from all individual scores, with laughter being the second best. From the emotion classes, only 'happy' seems to provide helpful information for hotspot detection. The combination of all scores provides the lowest EERs on both test corpora. Adapting the conflict intensity estimation to the ICSI development set helps for the ICSI testset but does not seem to reduce the EER on the AMI testset \u2013 this indicates that the ConflictNET model may learn corpus-specific information during adaptation, e.g. speakers. On the other hand, the adapted model does not perform much worse on the AMI corpus indicating that overfitting could be avoided.\\n\\nWhen considering the proposed approach for an actual application, one is interested mainly in the FAR vs. FRR at a specific detection threshold. The Detection Error Tradeoff (DET) curve in Fig. 1 is the best combined model from the last row of Tab. 4. For a FAR of around 10%, the system misses about 30% of the ICSI hotspots and about 45% of the AMI hotspots, which means that the error rate is still too high for most real-world applications.\\n\\nFor a more detailed analysis the confusion table Tab. 5 is presented, which evaluates the recognition rates for the original labels in the ICSI test set at the detection threshold that minimizes EER. As expected from listening to the files, the 'lukewarm' hotspots, equally classified as 'no hotspot' or 'hotspot', are a significant source of errors.\\n\\n| Reference | Original label | Hotspot Detected [%] |\\n|-----------|----------------|----------------------|\\n| no        | yes            | 81.6 18.4            |\\n| no hotspot| none           | 49.3 50.7            |\\n| 'lukewarm'| 'warm'         | 21.1 78.9            |\\n| 'hot'     |                | 12.5 87.5            |\\n\\n5. Conclusions and Next Steps\\nWe have shown that a cross-corpus approach is effective in detecting emotional hotspots in natural meetings. The results are surprisingly good, given the high training and test data mismatch. However, error rates are too high for an actual application. For future research, we will finalize the labeling of the AMI corpus to confirm our findings and publish the labels. We will start applying methods that increase generalization, e.g., ladder networks [15], and plan to include features derived from an automatic speech recognizer.\\n\\n6. References\\n[1] A. Batliner, B. Schuller, D. Seppi, S. Steidl, L. Devillers, L. Vidrascu, T. Voigt, V. Aharonson, and N. Amir, The Automatic Recognition of Emotions in Speech. Berlin, Heidelberg:\"}"}
