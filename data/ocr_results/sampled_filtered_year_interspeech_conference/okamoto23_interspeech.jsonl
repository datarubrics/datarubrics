{"id": "okamoto23_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] J. F. Gemmeke., D. P. W. Ellis., D. Freedman, A. Jansen, W. Lawrence, R. C. Moore, M. Plakal, and M. Ritter, \u201cAudio set: An ontology and human-labeled dataset for audio events,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2017, pp. 776\u2013780.\\n\\n[2] Y. Sudo, K. Itoyama, K. Nishida, and K. Nakadai, \u201cEnvironmental sound segmentation utilizing Mask U-Net,\u201d in Proc. IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2019, pp. 5340\u20135345.\\n\\n[3] T. Ochiai, M. Delcroix, Y. Koizumi, H. Ito, K. Kinoshita, and S. Araki, \u201cListen to what you want: Neural network-based universal sound selector,\u201d in Proc. INTERSPEECH, 2020, pp. 1441\u20131445.\\n\\n[4] I. Kavalerov, S. Wisdom, H. Erdogan, B. Patton, K. Wilson, J. L. Roux, and J. R. Hershey, \u201cUniversal sound separation,\u201d in Proc. IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 2019, pp. 175\u2013179.\\n\\n[5] M. Delcroix, J. B. V\u00e1zquez, T. Ochiai, K. Kinoshita, Y. Ohishi, and S. Araki, \u201cSoundbeam: Target sound extraction conditioned on sound-class labels and enrollment clues for increased performance and continuous learning,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 31, pp. 121\u2013136, 2023.\\n\\n[6] J. H. Lee, H.-S. Choi, and K. Lee, \u201cAudio query-based music source separation,\u201d in Proc. International Society for Music Information Retrieval (ISMIR), 2019, pp. 878\u2013885.\\n\\n[7] E. Tzinis, S. Wisdom, J. R. Hershey, A. Jansen, and D. P. W. Ellis, \u201cImproving universal sound separation using sound classification,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 96\u2013100.\\n\\n[8] O. Slizovskaia, L. Kim, G. Haro, and E. Gomez, \u201cEnd-to-end sound source separation conditioned on instrument labels,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 306\u2013310.\\n\\n[9] Y. Okamoto, S. Horiguchi, M. Yamamoto, K. Imoto, and Y. Kawaguchi, \u201cEnvironmental sound extraction using onomatopoeic words,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022, pp. 221\u2013225.\\n\\n[10] X. Liu, H. Liu, Q. Kong, X. Mei, J. Zhao, Q. Huang, M. D. Plumbley, and W. Wang, \u201cSeparate what you describe: Language-queried audio source separation,\u201d in Proc. INTERSPEECH, 2022, pp. 1801\u20131805.\\n\\n[11] K. Kilgour, B. Gfeller, Q. Huang, A. Jansen, S. Wisdom, and M. Tagliasacchi, \u201cText-driven separation of arbitrary sounds,\u201d in Proc. INTERSPEECH, 2022, pp. 5403\u20135407.\\n\\n[12] H.-W. Dong and N. Takahashi, Y. Mitsufuji, J. McAuley, and T. Berg-Kirkpatrick, \u201cClipsep: Learning text-queried sound separation with noisy unlabeled videos,\u201d in Proc. International Conference on Learning Representation (ICLR), 2023.\\n\\n[13] C. D. Kim, B. Kim, H. Lee, and G. Kim, \u201cAudioCaps: Generating captions for audios in the wild,\u201d in Proc. the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, 2019, pp. 119\u2013132.\\n\\n[14] K. Drossos, S. Lipping, and T. Virtanen, \u201cClotho: an audio captioning dataset,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020, pp. 736\u2013740.\\n\\n[15] M. Wu, H. Dinkel, and K. Yu, \u201cAudio caption: Listen and tell,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 830\u2013834.\\n\\n[16] Q. Kong, Y. Xu, T. Iqbal, Y. Cao, W. Wang, and M. D. Plumbley, \u201cAcoustic scene generation with conditional sampleRNN,\u201d in Proc. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019, pp. 925\u2013929.\\n\\n[17] Y. Okamoto, K. Imoto, S. Takamichi, R. Yamanishi, T. Fukumori, and Y. Yamashita, \u201cOnoma-to-wave: Environmental sound synthesis from onomatopoeic words,\u201d APSIPA Transactions on Signal and Information Processing, vol. 11, e13, 2022.\\n\\n[18] X. Liu, T. Iqbal, Z. Turab, J. Zhao, Q. Huang, M. D. Plumbley, and W. Wang, \u201cConditional sound generation using neural discrete time-frequency representation learning,\u201d in Proc. IEEE International Workshop on Machine Learning for Signal Processing (MLSP), 2021, pp. 1\u20136.\\n\\n[19] P. Chen, Y. Zhang, M. Tan, H. Xiao, D. Huang, and C. Gan, \u201cGenerating visually aligned sound from videos,\u201d IEEE Transactions on Image Processing, vol. 29, pp. 8292\u20138302, 2020.\\n\\n[20] Y. Zhou, Z. Wang, C. Fang, T. Bui, and T. L. Berg, \u201cVisual to sound: Generating natural sound for videos in the wild,\u201d in Proc. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018, pp. 3550\u20133558.\\n\\n[21] F. Kreuk, G. Synnaeve, A. Polyak, U. Singer, A. D\u00b4efossez, J. Copet, D. Parikh, Y. Taigman, and Y. Adi, \u201cAudioGen: Textually guided audio generation,\u201d in Proc. International Conference on Learning Representation (ICLR), 2023.\\n\\n[22] H. Liu, Z. Chen, Y. Yuan, X. Mei, X. Liu, D. Mandic, W. Wang, and M. D. Plumbley, \u201cAudioLDM: Text-to-audio generation with latent diffusion models,\u201d arXiv preprint arXiv:2301.12503, 2023.\\n\\n[23] D. Yang, J. Yu, H. Wang, W. Wang, C. Weng, Y. Zou, and D. Yu, \u201cDiffsound: Discrete diffusion model for text-to-sound generation,\u201d arXiv preprint arXiv:2207.09983, 2022.\\n\\n[24] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, X. Yin, and Z. Zhao, \u201cMake-an-audio: Text-to-audio generation with prompt-enhanced diffusion models,\u201d arXiv preprint arXiv:2301.12661, 2023.\\n\\n[25] Y. Luo and N. Mesgarani, \u201cConv-TasNet: Surpassing ideal time\u2013frequency magnitude masking for speech separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 27, no. 8, pp. 1256\u20131266, 2019.\\n\\n[26] J. Devlin, M. Chang, K. Lee, and K. Toutanova, \u201cBERT: Pre-training of deep bidirectional transformers for language understanding,\u201d in Proc. of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1, 2019, pp. 4171\u20134186.\\n\\n[27] L. Liu, H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and J. Han, \u201cOn the variance of the adaptive learning rate and beyond,\u201d in Proc. International Conference on Learning Representation (ICLR), 2020, pp. 1\u201313.\\n\\n[28] E. Vincent, R. Gribonval, and C. F \u00b4evotte, \u201cPerformance measurement in blind audio source separation,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 14, no. 4, pp. 1462\u20131469, 2006.\"}"}
{"id": "okamoto23_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nIn conventional studies on environmental sound separation and synthesis using captions, datasets consisting of multiple-source sounds with their captions were used for model training. However, when we collect the captions for multiple-source sounds, it is not easy to collect detailed captions for each sound source, such as the number of sound occurrences and timbre. Therefore, it is difficult to extract only the single-source target sound by the model-training method using a conventional captioned sound dataset. In this work, we constructed a dataset with captions for a single-source sound named CAPTDURE, which can be used in various tasks such as environmental sound separation and synthesis. Our dataset consists of 1,044 sounds and 4,902 captions. We evaluated the performance of environmental sound extraction using our dataset. The experimental results show that the captions for single-source sounds are effective in extracting only the single-source target sound from the mixture sound.\\n\\nIndex Terms\\n\\ntarget sound extraction, environmental sound, sound event detection, natural language processing\\n\\n1. Introduction\\n\\nEnvironmental sounds are indispensable to enhance the sense of presence and immersion in media content such as movies and games. One way to prepare a desired single-source sound is to obtain it from a sound database [1]. However, the required environmental sounds may not always exist in the database. Moreover, there are many multiple-source sounds on the Internet, but obtaining only the single-source target sound is not easy.\\n\\nTo obtain only the single-source target sound, environmental sound extraction and separation methods have been proposed [2, 3, 4, 5, 6, 7] for extracting only the single-source target sound from a multiple-source sound. For example, a method has been proposed with which a sound event class is specified and only sounds corresponding to the sound event class are extracted [2, 3, 5, 8]. To extract sounds more accurately than the sound event class, the method using onomatopoeic words that transcribes the characteristics of the sound to be extracted has been proposed [9]. However, since onomatopoeic words are language and culture dependent, the dataset for model training needs to be modified in accordance with the user's native language. To avoid dependence on language and culture, a method of environmental sound extraction and separation using captions for sound has also been proposed [10, 11, 12]. With this method, the characteristics of the multiple-source target sound are expressed by captions and the multiple-source target sound can be extracted. This method enables us to extract the multiple-source target sound, which includes some sound event classes corresponding to the caption. The conventional environmental sound-extraction method using captions is based on a dataset [13, 14, 15] in which captions are assigned to multiple-source sounds.\\n\\nTable 1: Examples of captions in CAPTDURE. The sound sources are underlined.\\n\\n| #Sources | Caption |\\n|----------|---------|\\n| Single   | \u2022 One or two keyboards continue to be pressed slowly with a light, high-touch sound. |\\n|          | \u2022 The sound of the buttons on the keyboard being pressed one by one quite slowly. |\\n|          | \u2022 The electronic tone of the digital alarm clock is high-pitched and continues to sound slowly to gradually faster. |\\n|          | \u2022 The bright, high-pitched alarm clock bells are ringing. |\\n| Multiple | \u2022 Keyboard typing and mouse clicking noises are continually heard. |\\n|          | \u2022 The sound of a toaster rang out as if to counteract the sound of typing on a small keyboard. |\\n|          | \u2022 The alarm clock is drowned out by the loud noise of the hair dryer. |\\n|          | \u2022 The alarm clock beeps at equal intervals, with one final sound to close the lock. |\\n\\nEnvironmental sound synthesis methods, which artificially generate sounds, have also been proposed to obtain the required sound [16, 17, 18, 19, 20]. Recently, there are some studies of environmental sound synthesis using captions [21, 22, 23, 24]. However, no captioned sound datasets represent only a single-source sound, so it is difficult to fine-tune the generated single-source sound. We need the caption corresponding to a single-source sound to control synthesized environmental sound finely.\\n\\nIn this paper, we constructed a dataset with captions for sound shown in Table 1 for single sound sources that can be used in various tasks that involve environmental sounds. The dataset is called CAPTDURE (CAPTioned sound Dataset of single soURcEs), pronounced as \u201ccapture.\u201d As an example of the use of the dataset, we conduct an experiment on environmental sound extraction using captions. In our experiment, we show that the use of captions for sound assigned to a single-source sound is effective in extracting only target sounds in a multiple-source sound.\\n\\nThe rest of the paper is organized as follows. In Sec. 2, we describe the creation of CAPTDURE. In Sec. 3, we discuss our...\"}"}
{"id": "okamoto23_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2: List of recording equipment\\n\\n| Recording equipment | Software    | Microphone      | Audio interface |\\n|---------------------|-------------|-----------------|-----------------|\\n| Apple/Macbook Pro (16-inch 2019) | Audacity    | SHURE/MX150B/O-XLR | Roland/Rubix24 |\\n| TASCAM/DR-44WL       | Built-in    | Built-in        | Built-in        |\\n| Apple/iPhone SE     | PCM recording | Built-in       | Built-in        |\\n| Huawei/dtab d01-G   | AudioRec    | Built-in        | Built-in        |\\n\\nTable 3: Recorded sound event classes\\n\\n| Sound event class       | Description            | #Subclasses | #Clips | Duration [s] |\\n|------------------------|------------------------|-------------|--------|--------------|\\n| Keyboard               | Sound of keyboard typing | 5           | 60     | 465.0        |\\n| Door                   | Sound of door opening and closing | 4           | 48     | 279.0        |\\n| Mouse                  | Sound of mouse clicking | 7           | 84     | 600.0        |\\n| Water tap running      | Sound of water tap running | 5           | 60     | 431.0        |\\n| Dryer operating        | Operating sound of dryer | 6           | 252    | 1,746.0      |\\n| Ventilation fan        | Operating sound of ventilation fan | 3           | 36     | 284.0        |\\n| Door lock              | Sound of door locking  | 6           | 48     | 245.0        |\\n| Intercom               | Sound of intercom      | 5           | 56     | 350.0        |\\n| Door knock             | Sound of door knock    | 6           | 72     | 430.0        |\\n| Microwave              | Operating sound of microwave | 4           | 48     | 329.0        |\\n| Toaster                | Operating sound of toaster | 5           | 60     | 465.0        |\\n| Cutlery                | Sound of hitting cutlery | 7           | 52     | 362.0        |\\n| Clock                  | Operating sound of alarm clock | 5           | 60     | 420.0        |\\n| Fan                    | Operating sound of fan | 3           | 108    | 779.0        |\\n| Total                  |                        | 71           | 1,044  | 7,185.0      |\\n\\nFigure 1: Histograms of average appropriateness score experiments on environmental sound extraction using language query. Finally, we summarize and conclude this paper in Sec. 4.\\n\\n2. Creation of CAPTDURE\\n\\n2.1. Design of CAPTDURE\\n\\nThe CAPTDURE dataset consists of the following contents.\\n\\n- **Single-source sounds**: We recorded a total of 1,044 single-source sounds, consisting of 14 types of daily sound events, as shown in Table 3. For each sound event, we recorded approximately 40 to 100 sounds while changing the recording conditions, such as recording equipment. The length of each sound was set to 5 to 9 seconds. Each sound event class was divided into subclasses in accordance with differences in timbre and pitch.\\n\\n- **Multiple-source sounds**: We created a total of 1,044 multiple-source sounds using the recorded single-source sounds. We selected two sounds of different sound event classes from the recorded sounds and mixed the sounds so that the signal-to-noise ratio (SNR) was 0 dB. We padded with zero the shorter sound and aligned the sound length of the two sounds.\\n\\n- **Captions for single-source sounds**: We collected a total of 4,902 captions (3 or more captions per single-source sound). We describe the details of the captions collection in Sec. 2.3.\\n\\n- **Captions for multiple-source sounds**: We collected a total of 3,132 captions (3 captions per multiple-source sound).\\n\\n- **Appropriateness score for each caption**: We asked crowdworkers to score the appropriateness level for captions transcribed by others. The appropriateness scores enable us to evaluate captions on the basis of the judgment of others. We describe the details of the appropriateness scores in Sec. 2.4.\\n\\n- **Worker ID**: CAPTDURE includes anonymized IDs of workers who gave captions and appropriateness scores.\\n\\nThis dataset is freely available online.\\n\\n2.2. Recording environment and setup\\n\\nThe environmental sounds included in CAPTDURE were recorded in two types of soundproof rooms. The first soundproof room is a length of 3.1 m, a width of 5.4 m, and a height of 2.7 m with a reverberation time ($T_{60}$) of 0.2 s. The second soundproof room is a length of 4.9 m, a width of 4.0 m, and a height of 2.5 m with a reverberation time ($T_{60}$) of 0.2 s. Some environmental sounds, such as \u201cwater tap running\u201d and \u201cdoor\u201d that are difficult to record in a soundproof room, were recorded at locations other than these rooms. Table 2 lists the equipment used for the sound recording. The sampling frequency was 48 kHz, and the quantization bit rate was 16 bits. The microphone was set at a distance of approximately 0.3 to 0.5 m from the sound source being recorded.\\n\\n2.3. Captions collection\\n\\nWe used Lancers, which is a crowdsourcing service in Japan, to collect captions for sounds from Japanese crowdworkers. In the pre-experiment, we tried using Amazon Mechanical Turk (MTurk). However, comparing Lancers and MTurk, we confirmed that the quality of crowdworkers was higher in Lancers. The collection of captions was conducted by presenting five sounds to one worker. In the pre-experiment, the five sounds presented to the crowdworkers were from different sound event classes. However, the caption of each sound mainly consisted of the caption of the sound class it belongs to. Therefore, the five sounds presented to the crowdworkers consisted of the same sound event class, and we instructed the crowdworkers to write a different sentence as the caption given to each sound. Crowdworkers were also instructed to avoid providing only with the type of sound, such as \u201cthe sound of the microwave,\u201d or only with onomatopoeic words, such as /k a N k a N/.\\n\\nWe collected a total of 4,902 captions (3\u20135 captions per single-source sound) for the single-source sounds. Moreover, we collected a total of 3,132 captions (3 captions per multiple-source sound) for the multiple-source sounds. In addition to the captions collected in Japanese, CAPTDURE also contains\"}"}
{"id": "okamoto23_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4: Statistics of train, validation, and test set of CAPTDURE\\n\\n|                      | #Clips | #Captions | #Words (en) / #Characters (ja) |\\n|----------------------|--------|-----------|-------------------------------|\\n| **Single-source sound** |        |           |                               |\\n| train                | 795    | 3,774     | 10.53 (en) / 22.80 (ja)       |\\n| validation           | 82     | 374       | 10.62 (en) / 22.91 (ja)       |\\n| test                 | 167    | 501       | 10.43 (en) / 23.05 (ja)       |\\n| **Multiple-source sound** |        |           |                               |\\n| train                | 795    | 2,385     | 16.83 (en) / 36.47 (ja)       |\\n| validation           | 82     | 246       | 16.60 (en) / 35.53 (ja)       |\\n| test                 | 167    | 754       | 17.79 (en) / 38.06 (ja)       |\\n\\nCaptions translated into English using the DeepL API.\\n\\n2.4. Captions evaluation\\n\\nWe asked crowdworkers to score the level of appropriateness for captions transcribed by others. We present pairs of audio samples and the corresponding caption to a crowdworker. The crowdworker then gives an appropriateness score for each caption. The appropriateness score is on a five-point scale, 1 (very inappropriate) to 5 (very appropriate), for each caption by those who did not create the caption. The appropriateness scores for each caption were collected from more than three crowdworkers.\\n\\nFigure 1 shows the histogram of each collected appropriateness score calculated as the average per sound for single and multiple-source sounds. Most of the captions were given an appropriateness score of 3 or higher. This fact indicates that we were able to collect many appropriate captions to express each sound.\\n\\n2.5. Data splitting\\n\\nWe split the single- and multiple-source sounds of each subclass as approximately 7:1:2 for the train, validation, and test sets. The captions corresponding to each sound are also included in each set. The statistics of each set are shown in Table 4. We artificially split the sounds of each set to balance the number of words/characters of captions.\\n\\n3. Experiments\\n\\nTo demonstrate the efficiency of CAPTDURE, we evaluated the performance of environmental sound extraction using caption.\\n\\n3.1. Network architecture\\n\\nTo extract the sound that corresponds to caption \\\\( l \\\\) from multiple-source sound \\\\( x \\\\), we trained a neural network based on Conv-TasNet [25]. The \\\\( T \\\\)-length multiple-source sound \\\\( x \\\\in \\\\mathbb{R}^{1 \\\\times T} \\\\) is fed to the encoder, which consists of a one-dimensional (1-D) convolution layer as follows:\\n\\n\\\\[\\nW = \\\\text{Encoder}(x) \\\\in \\\\mathbb{R}^{C \\\\times T}'.\\n\\\\] (1)\\n\\nAt the same time, the sound caption \\\\( l \\\\) is fed to the Bidirectional Encoder Representations from Transformers (BERT) [8]. In this experiment, we used the pre-trained English and Japanese BERT models. The vector obtained by BERT is compressed by linear transformation \\\\( \\\\text{Linear} \\\\) to the \\\\( C \\\\)-dimension as follows:\\n\\n\\\\[\\no = \\\\text{Linear}(\\\\text{BERT}(l)) \\\\in \\\\mathbb{R}^C.\\n\\\\] (2)\\n\\nThe soft-mask \\\\( M \\\\) is calculated from the obtained matrix \\\\( W \\\\) and vector \\\\( o \\\\) as\\n\\n\\\\[\\nM = \\\\text{Separator}(W \\\\odot [o, o, \\\\ldots, o]^T) \\\\in (0, 1)^{C \\\\times T'},\\n\\\\] (3)\\n\\nwhere \\\\( \\\\odot \\\\) denotes the Hadamard product and \\\\( \\\\text{Separator}(\\\\cdot) \\\\) is \\\\( K \\\\)-stacked 1-D convolutional layers. Finally, the Hadamard product of soft-mask \\\\( M \\\\) and \\\\( W \\\\) is fed to the decoder, which consists of a 1-D convolution layer, to obtain the target signal corresponding to the caption as follows:\\n\\n\\\\[\\n\\\\hat{y} = \\\\text{Decoder}(M \\\\odot W) \\\\in \\\\mathbb{R}^{1 \\\\times T}.\\n\\\\] (4)\\n\\nThe network was trained to minimize the L1 norm between target sound \\\\( y \\\\) and estimated sound \\\\( \\\\hat{y} \\\\).\\n\\n3.2. Dataset construction\\n\\nWe randomly selected three single sounds with different sound event classes from CAPTDURE to create a mixture sound. The training and validation sets consisted of 2,385 and 246 mixture sounds, respectively. We used three randomly selected captions for each sound for our experiments.\\n\\nWe constructed two evaluation datasets using the test set of single-source sounds from CAPTDURE: the inter-event-class dataset and the intra-event-class dataset. Each sound in those datasets is a mixture sound of a target sound and an interference sound, but those are from different sound event classes in the inter-event-class dataset and from the same sound event class in the intra-event-class dataset. The signal-to-interference ratio was varied by \\\\{\u221210, \u22125, 0, 5, 10\\\\} dB. Each evaluation set consisted of 501 mixture sounds for each SNR.\\n\\n8https://huggingface.co/bert-base-uncased\\n9https://huggingface.co/cl-tohoku/\\nbert-base-japanese\"}"}
{"id": "okamoto23_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 6: SDRi [dB] for extracted signals\\n\\n| SNR      | Dataset                      | Model-training method                                      |\\n|----------|------------------------------|------------------------------------------------------------|\\n|          | Inter-event-class dataset    | Training using multiple-source caption (ja)                 |\\n|          |                              | 5.12 \u00b1 4.22, 4.39 \u00b1 4.04, 3.93 \u00b1 4.21, 0.93 \u00b1 4.92, \u22121.85 \u00b1 5.95 |\\n|          |                              | Training using multiple-source caption (en)                 |\\n|          |                              | 4.67 \u00b1 4.03, 3.88 \u00b1 3.88, 2.43 \u00b1 4.04, 0.17 \u00b1 4.71, \u22122.78 \u00b1 5.67 |\\n|          |                              | Training using single-source caption (ja)                   |\\n|          |                              | 7.28 \u00b1 5.19, 6.26 \u00b1 5.02, 4.63 \u00b1 5.18, 2.09 \u00b1 5.71, \u22121.08 \u00b1 6.54 |\\n|          |                              | Training using single-source caption (en)                   |\\n|          |                              | 6.42 \u00b1 4.84, 5.14 \u00b1 4.66, 3.13 \u00b1 4.89, 0.40 \u00b1 5.54, \u22122.91 \u00b1 6.40 |\\n|          | Intra-event-class dataset    | Training using multiple-source caption (ja)                 |\\n|          |                              | 3.22 \u00b1 2.78, 2.49 \u00b1 2.37, 1.11 \u00b1 2.31, 0.63 \u00b1 2.71, \u22121.01 \u00b1 3.08 |\\n|          |                              | Training using multiple-source caption (en)                 |\\n|          |                              | 3.09 \u00b1 2.50, 2.38 \u00b1 2.11, 1.15 \u00b1 2.15, 0.86 \u00b1 2.74, \u22120.86 \u00b1 3.90 |\\n|          |                              | Training using single-source caption (ja)                   |\\n|          |                              | 4.36 \u00b1 3.60, 3.24 \u00b1 3.38, 1.36 \u00b1 3.36, 0.63 \u00b1 3.01, \u22121.33 \u00b1 3.66 |\\n|          |                              | Training using single-source caption (en)                   |\\n|          |                              | 4.47 \u00b1 3.09, 3.23 \u00b1 2.41, 1.39 \u00b1 2.43, 0.70 \u00b1 2.91, \u22121.48 \u00b1 3.27 |\\n\\n#### 3.3. Training and evaluation setup\\n\\nTable 5 lists the experimental conditions and parameters used for this experiment. In this experiment, we compared two types of model training methods as follows:\\n\\n- **Training using multiple-source captions**: This model training method uses the captions corresponding to the multiple-source sounds. This method is equivalent to the model training method using the conventional dataset.\\n- **Training using single-source captions**: This model training method uses captions corresponding to single-source sounds.\\n\\nIn the evaluation, we evaluate the sound extraction performance using the evaluation dataset created in Sec. 3.2 for each model training method.\\n\\nTo evaluate each model-training method, we used signal-to-distortion ratio improvement (SDRi) [28] as an evaluation metric, which is defined as the difference between the SDR of the target sound to the mixture sound and that of the target sound to the extracted sound. We evaluated in terms of the SDRi on each evaluation dataset.\\n\\n#### 3.4. Experimental results\\n\\nTable 6 lists the SDRi on each evaluation dataset. The training using single-source captions enables us to extract only the target sound from a mixture sound compared with the training using multiple-source captions. Moreover, from the results of the intra-event-class dataset, the caption for a single-source sound is effective to extract only the target sound, even when the interference sound from the same sound event class appears in the mixture sound.\\n\\nFigure 2 shows the spectrogram of the extracted sounds using single-source captions. We used four audio samples in the inter-event-class dataset with 0 dB. We observed that the training using multiple-source captions left a significant amount of non-target sounds. The training using single-source captions extracted only the single-source target sound. On the other hand, the model trained using single-source captions could extract only the single-source target sound. Thus, the captions for a single-source source are effective in extracting only the single-source target sound.\\n\\n#### 4. Conclusion\\n\\nWe constructed a dataset with captions for a single-source sound that can be used in various tasks that use environmental sounds. We recorded a total of 1,044 single-source sounds and collected a total of 4,902 captions for recorded single-sound sources. The experimental results indicate that the use of sound captions assigned to a single-sound source is effective in extracting only the target sounds in mixture sounds. Verifying our dataset's effectiveness for other tasks involving environmental sounds is necessary.\"}"}
