{"id": "liu22d_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] G. K. Chambers, C. Curtis, C. D. Millar, L. Huynen, and D. M. Lambert, \u201cDNA fingerprinting in zoology: past, present, future,\u201d Investigative genetics, vol. 5, no. 1, pp. 1\u201311, 2014.\\n\\n[2] B. Stonehouse, Animal marking: recognition marking of animals in research. Macmillan International Higher Education, 1978.\\n\\n[3] T. Trnovszky, P. Kamencay, R. Orjesek, M. Benco, and P. Sykora, \u201cAnimal recognition system based on convolutional neural network,\u201d Advances in Electrical and Electronic Engineering, vol. 15, no. 3, pp. 517\u2013525, 2017.\\n\\n[4] M. A. Bee and H. C. Gerhardt, \u201cIndividual voice recognition in a territorial frog (rana catesbeiana),\u201d Proceedings of the Royal Society of London. Series B: Biological Sciences, vol. 269, no. 1499, pp. 1443\u20131448, 2002.\\n\\n[5] C. L. Ting Yuan and D. Athiar Ramli, \u201cFrog sound identification system for frog species recognition,\u201d in International Conference on Context-Aware Systems and Applications. Springer, 2012, pp. 41\u201350.\\n\\n[6] N. Dave, \u201cFeature extraction methods LPC, PLP and MFCC in speech recognition,\u201d International journal for advance research in engineering and technology, vol. 1, no. 6, pp. 1\u20134, 2013.\\n\\n[7] J. C. Bezdek, S. K. Chuah, and D. Leep, \u201cGeneralized k-nearest neighbor rules,\u201d Fuzzy Sets and Systems, vol. 18, no. 3, pp. 237\u2013256, 1986.\\n\\n[8] H. V. Koops, J. Van Balen, and F. Wiering, \u201cA deep neural network approach to the LifeCLEF 2014 bird task,\u201d CLEF2014 Working Notes, vol. 1180, pp. 634\u2013642, 2014.\\n\\n[9] V. Tiwari, \u201cMFCC and its applications in speaker recognition,\u201d International journal on emerging technologies, vol. 1, no. 1, pp. 19\u201322, 2010.\\n\\n[10] B. P. T \u00b4oth and B. Czeba, \u201cConvolutional neural networks for large-scale bird song classification in noisy environment.\u201d in CLEF (Working Notes), 2016, pp. 560\u2013568.\\n\\n[11] E. S \u00b8as \u00b8maz and F. B. Tek, \u201cAnimal sound classification using a convolutional neural network,\u201d in 2018 3rd International Conference on Computer Science and Engineering (UBMK). IEEE, 2018, pp. 625\u2013629.\\n\\n[12] Y. Li and Z. Wu, \u201cAnimal sound recognition based on double feature of spectrogram in real environment,\u201d in 2015 International Conference on Wireless Communications & Signal Processing (WCSP). IEEE, 2015, pp. 1\u20135.\\n\\n[13] V. Svetnik, A. Liaw, C. Tong, J. C. Culberson, R. P. Sheridan, and B. P. Feuston, \u201cRandom forest: a classification and regression tool for compound classification and QSAR modeling,\u201d Journal of chemical information and computer sciences, vol. 43, no. 6, pp. 1947\u20131958, 2003.\\n\\n[14] C. Y. Yeo, S. Al-Haddad, and C. K. Ng, \u201cDog voice identification (ID) for detection system,\u201d in 2012 Second International Conference on Digital Information Processing and Communications (ICDIPC). IEEE, 2012, pp. 120\u2013123.\\n\\n[15] P. Senin, \u201cDynamic time warping algorithm review,\u201d Information and Computer Science Department University of Hawaii at Manoa Honolulu, USA, vol. 855, no. 1-23, p. 40, 2008.\\n\\n[16] T. Pellegrini, \u201cDeep-learning-based central african primate species classification with mixup and specaugment,\u201d in Inter-speech, 2021.\\n\\n[17] C. Li, X. Ma, B. Jiang, X. Li, X. Zhang, X. Liu, Y. Cao, A. Kannan, and Z. Zhu, \u201cDeep speaker: an end-to-end neural speaker embedding system,\u201d arXiv preprint arXiv:1705.02304, 2017.\\n\\n[18] L. Wan, Q. Wang, A. Papir, and I. L. Moreno, \u201cGeneralized end-to-end loss for speaker verification,\u201d in 2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2018, pp. 4879\u20134883.\\n\\n[19] J.-w. Jung, S.-b. Kim, H.-j. Shim, J.-h. Kim, and H.-J. Yu, \u201cImproved rawnet with feature map scaling for text-independent speaker verification using raw waveforms,\u201d arXiv preprint arXiv:2004.00526, 2020.\\n\\n[20] H. Luo, Y. Shen, F. Lin, and G. Xu, \u201cSpoofing speaker verification system by adversarial examples leveraging the generalized speaker difference,\u201d Security and Communication Networks, vol. 2021, 2021.\\n\\n[21] H. Wu, X. Li, A. T. Liu, Z. Wu, H. Meng, and H.-y. Lee, \u201cImproving the adversarial robustness for speaker verification by self-supervised learning,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 30, 2022.\\n\\n[22] W. M. Campbell, D. E. Sturim, and D. A. Reynolds, \u201cSupport vector machines using GMM supervectors for speaker verification,\u201d IEEE signal processing letters, vol. 13, no. 5, pp. 308\u2013311, 2006.\\n\\n[23] A. Kanagasundaram, R. Vogt, D. Dean, S. Sridharan, and M. Mason, \u201cI-vector based speaker recognition on short utterances,\u201d in Proceedings of the 12th Annual Conference of the International Speech Communication Association. International Speech Communication Association, 2011, pp. 2341\u20132344.\\n\\n[24] M. J. Owren, \u201cHuman voice in evolutionary perspective,\u201d Acoust. Today, vol. 7, no. 24, pp. 10\u20131121, 2011.\\n\\n[25] W. T. Fitch, B. De Boer, N. Mathur, and A. A. Ghazanfar, \u201cMonkey vocal tracts are speech-ready,\u201d Science advances, vol. 2, no. 12, p. e1600723, 2016.\\n\\n[26] M. Ravanelli and Y. Bengio, \u201cSpeaker recognition from raw waveform with sincnet,\u201d in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 1021\u20131028.\"}"}
{"id": "liu22d_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism\\n\\nPeng Liu1, Songbin Li1, Jigang Tang1\\n\\n1 Institute of Acoustics, Chinese Academy of Sciences, Beijing, China\\nlisongbin@mail.ioa.ac.cn\\n\\nAbstract\\nPrimates are facing a serious survival crisis. Tracking the range of animal activities and population changes is of great significance for efficient animal protection. Primates are highly alert and inaccessible to humans so that it is difficult to track animals through direct observation, DNA fingerprinting, or marking methods. Primate recognition based on animal calls has the advantages of wide monitoring range, low equipment cost, and good concealment. In this work, we propose an effective macaque speech feature extraction structure, and innovatively propose a feature fusion mechanism to effectively obtain the feature representation of each call. Furthermore, we construct a public open source macaque voiceprint verification dataset. The experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations. The equal error rate (EER) of our macaque voiceprint verification algorithm reaches 6.19%.\\n\\nIndex Terms: Deep learning, voiceprint verification, macaque recognition\\n\\n1. Introduction\\nVarious climate elements, such as pollution and global warming, have led to a sharp decline in the population of many wild animals. Animal protection has become a continuous concern. In complex wild environments, effective identification of individuals in animal populations is important for studying animal population information and its dynamic changes. Therefore, in this work, we take macaques as the research object and construct an individual voiceprint verification algorithm that can be effectively used for individual animal recognition.\\n\\nAt present, the most commonly used individual animal recognition methods include direct observation, DNA fingerprint [1], marking [2], image recognition [3] and call recognition methods [4]. Because macaques mostly live and breed in dense forests, which have strong shelter, it is difficult to recognize them directly through images. And macaques are highly vigilant and hard for humans to approach, thereby it is also difficult to implement direct observation, DNA fingerprint and marking methods. However, the calls of macaques can spread far in dense forests. And calls collection equipment has the advantages of low cost, good concealment, and wide detection range. Thus, recognize macaques through their calls is a feasible solution.\\n\\nSince there is no available dataset of primate calls, the research on primate voiceprint is facing great obstacles. To address this issue, we collect the audio and video of 144 macaques and construct a dataset. Then we propose a macaque voiceprint verification algorithm as shown in Fig. 1. This paper aims to verify the feasibility of macaque voiceprint recognition and improve the EER of macaque voiceprint verification based on deep learning. Experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations.\\n\\n2. Related Works\\nTo the best of our knowledge, we did not find any of the comparable studies on primate voiceprint verification. Therefore, we summarize animal voiceprint related studies in this section. Most of the existing animal voiceprint researches focus on the recognition of animal species, and few studies focus on the recognition of individual animals. For instance, Yuan et al. [5] proposed a frog species recognition algorithm based on linear prediction coefficient (LPC) features [6] and k-nearest neighbour (KNN) [7]. Koops et al. [8] used a three-layer DNN structure to realize bird species recognition based on mel frequency cepstral coefficients (MFCC) [9]. Toth et al. [10] using fixed-size spectrogram and AlexNet to recognize bird species with background noise, the best mean average precision is 42.6%. Emre et al. [11] collected 875 voices of 10 animals as a dataset, and proposed a neural network composed of 3 convolution layers and 3 fully connected layers. The accuracy rate reached 75%. Li et al. [12] proposed a dual feature combination composed of projection features and local binary mode variance, combined with random forest (RF) [13] to recognize 40 species of animals and achieve an accuracy of 80% with a signal-to-noise ratio of less than 10dB. Yeo et al. [14] used dog barking MFCC combined with dynamic time warping algorithm [15] to recognize 5 dogs, with an average accuracy rate of 75.5%. Pellegini [16] achieved the classification of four primate species and a background category with forest sound events using a standard 10-layer CNN. The unweighted average recall reached 92.5%.\\n\\nIn general, the current animal voiceprint related research is still in its infancy. On the one hand, from the perspective of speech preprocessing, most of the existing animal calls recognition algorithms use MFCC, LPC, spectrogram, etc., and lack further exploration of speech features. From the perspective of the call model, many algorithms are based on machine learning models, such as KNN, RF, and lack relevance to frontier research in the field of speech signal processing. From the perspective of the scope of application, most of the existing methods are based on closed datasets, which can be equivalent to multi-classification tasks. However, the research on human voiceprint has currently made great progress, and both animal voiceprints and human voiceprints are characteristics of sound, thus, human voiceprint research can be used for reference in the study of animal individual recognition. On the other hand, deep learning has brought new progress to human voiceprint research [17, 18, 19, 20, 21]. Speaker verification algorithms\"}"}
{"id": "liu22d_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 1: Illustration of proposed macaque voiceprint verification network. The overall structure of the algorithm can be divided into three modules: feature extraction, feature correlation, and classification modules. Such as Deep Speaker [17], LGE2E [18], and RawNet2 [19] have achieved better performance than classic GMM [22] and i-vector [23]. However, compared with human speech, the semantic information of macaque calls is monotonous [24, 25], resulting in similar features in different macaque calls. We compare the collected macaque audio data with human speech. As shown in Fig. 2, the sounds of macaque and human exhibit good periodicity in a short period of time. The corresponding spectrograms of macaque and human are shown in Fig. 3. It can be seen that there are significant differences in energy at different frequencies, which may result in human speaker recognition methods that may not be fully applicable to individual recognition of macaques. When we apply the speaker verification algorithm to macaque voiceprint verification, we get much worse results than the human speaker dataset. Therefore, it is necessary to propose an effective algorithm to overcome the above-mentioned problems.\\n\\nFig. 2: Comparison of the waveforms of macaque and human. (left): macaque, (right): human.\\n\\nFig. 3: Comparison of the spectrograms of macaque and human. (left): macaque, (right): human.\\n\\n3. The Proposed Method\\n\\nThe neural network architecture proposed in this work is shown in Fig. 1. We divide the overall structure into three modules: feature extraction, feature correlation, and classification modules corresponding to the backbone module, channel fusion mechanism (CFM), and GRU-FC. For the backbone network, we proposed a SincNet-ResNet architecture, which refers to RawNet2 [19], as shown in Fig. 4. The CFM is shown in the green box of Fig. 1.\\n\\n3.1. Backbone structure\\n\\nThe backbone consists of a learnable band-pass filter SincBlock [26], six ResBlocks, a Conv layer and two TransformBlock. Among them, a ResBlock consists of a one-dimensional convolution residual unit and a pooling layer to extract frame-level feature representation from the original call data. Conv is a convolution operation layer. Feature map scaling (FMS) [19] is used to scale the filter axis of feature maps. Besides, the TransformBlock is designed to added multi-head to enhance the feature extraction capabilities. After inputting the original calls, the SincBlock converts the time domain information into frequency domain information, and performs the six ResBlocks to reduce the feature dimension while extracting features. To facilitate the multi-head design of the Transform Layer, a 1-dimensional convolution with a kernel size of 1 is used to perform channel conversion on the output characteristics of the ResBlock. Finally, the two-dimensional feature map is output as the input of the feature correlation module.\\n\\n3.2. Channel fusion mechanism\\n\\nThe output features of the backbone can be represented by $F = [f_1, f_2, ..., f_T]$, $f \\\\in \\\\mathbb{R}^n$ where $n$ represents the frame-level feature dimension, and $T$ is the number of frames. By grouping $F$ with a stride of 1, the feature group representation is obtained:\\n\\n$$FG = ([f_1, f_2, ..., f_c], ..., [f_T, f_1, f_2, ..., f_{c-1}])$$\\n\\nwhere $c$ represents the number of frames in each feature group. By grouping feature maps, we first proposed channel weighted fusion, a new frame-level feature representation mechanism. The channel fusion mechanism (CFM) is shown in the dotted\"}"}
{"id": "liu22d_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Layer Input: 48400 samples Output Shape\\n\\nSincBlock {\\n  Sinc(251, 1, 128)\\n  MaxPool\\n  BN\\n  LeakyReLU\\n} \\\\times 1 (128, 16050)\\n\\n\\\\times 2 (256, 22)\\n\\nConv Conv(1, 1, 32) (256, 32)\\n\\nTransform\\n{  \\n  MHA\\n  FC\\n  Dropout\\n  FC\\n  Dropout\\n} \\\\times 2 (256, 32)\\n\\nFigure 4: The backbone structure of our proposed method.\\nNumbers denoted in Conv and Sinc refers to filter length, stride, and number of filters.\\n\\n\\\\begin{equation}\\n  \\\\text{FG}' = g(f(\\\\text{FG}_i)) = \\\\omega_2(\\\\omega_1(\\\\text{FG}_i) + b_1) + b_2\\n\\\\end{equation}\\n\\nm = \\\\text{MaxP}(\\\\text{FG}'_i, c)\\n\\na = \\\\text{AvgP}(\\\\text{FG}'_i, c)\\n\\n\\\\omega_i = \\\\sigma(m + a)\\n\\nf^*_i = \\\\text{tp}(\\\\text{FG}'_i) \\\\cdot \\\\omega_i\\n\\\\end{equation}\\n\\nwhere \\\\(f\\\\) and \\\\(g\\\\) respectively represent the two-layer fully connected structure mapping function, and the parameters are \\\\(\\\\omega_1, b_1\\\\) and \\\\(\\\\omega_2, b_2\\\\) respectively; \\\\(\\\\text{MaxP}\\\\) and \\\\(\\\\text{AvgP}\\\\) represent the maximum pooling and average pooling, \\\\(\\\\sigma\\\\) denotes the Sigmoid function, \\\\(\\\\text{tp}\\\\) represents transpose, and \\\\(\\\\cdot\\\\) means dot multiplication. Finally, the \\\\(i\\\\)-th feature group \\\\(\\\\text{FG}'_i\\\\) is mapped to the feature \\\\(f^*_i\\\\), which represents the feature of the \\\\(i\\\\)-th frame after fusion, where \\\\(\\\\text{FG}'_i \\\\in \\\\mathbb{R}^{c \\\\times n}, f^*_i \\\\in \\\\mathbb{R}^n\\\\). In this paper, the correlation degree of two vectors is measured by cosine distance.\\n\\n4. EXPERIMENTS\\n\\n4.1. Dataset\\n\\nWe in cooperation with the primate research center of Kunming Institute of Zoology, Chinese Academy of Sciences, spent 21 days on recording and manual labeling to constructed a Macaque Voiceprint Verification Dataset, termed MVVD. The detailed description is shown below.\\n\\nThe MVVD dataset can be downloaded from: https://github.com/PengLiu2022/MacaqueDataset.\\n\\n4.1.1. Dataset collection\\n\\nThe MVVD dataset consists of audio and video data. The audio data is recorded by a dedicated microphone connected to a computer with a sampling rate of 44.1kHz and the video is recorded by a professional camera. The microphone and the camera are set up simultaneously for dataset recording. The collected data is derived from 144 macaques with unique number aged 0-2 years. For each macaque, we record data of 5-30 minutes.\\n\\nTo explore the possibility of macaque voiceprint verification, we selected four macaques and randomly selected a segment of their calls. The comparison of spectrograms among different macaque individuals is shown in Fig. 5. As can be seen from the Fig. 5, the frequency distribution of spectrograms is quite different, which provides the possibility for individual recognition.\\n\\nFigure 5: Comparison of the spectrograms of four macaques.\\n\\n4.1.2. Dataset labelling\\n\\nEach audio data processing procedure is as follows: A) mark the exact start and end times of each macaque call manually based on the collected original audio waveform; B) cut the macaque call segment according to the marked period time to remove the silent segment and the noise segment; C) put the call segments belonging to each macaque under a folder named after their unique number.\\n\\nNote that we do not directly use the voice activity detection (VAD) method to label start and end points in audio segments. At present, the accuracy and robustness of the VAD algorithm are not satisfying. It may also mark the noise as a macaque's call, which will affect the research results based on this dataset. Human ears can distinguish macaque calls very well, so noise fragments are filtered out when labelling.\\n\\n4.1.3. Dataset characteristics\\n\\nThe total duration of the recorded original voice is 2,143.11 minutes and remains 171.35 minutes valid duration after the manually labelling. The total number of macaque call segments is 18,309. We only cut and split the audio data, and do not perform any operations on the collected video data. The video data can be developed in the future. The basic training set (MacaqueT) is composed of the first 100 macaques, and the remaining macaques make up the basic evaluation set (MacaqueE). The detailed information of the audio dataset can be seen in Table 1.\"}"}
{"id": "liu22d_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Basic information of MVVD.\\n\\n|                | Training set | Testing set | Total         |\\n|----------------|--------------|-------------|---------------|\\n| Macaque number | 100          | 44          | 144           |\\n| Total duration | 1609.26      | 533.85      | 2143.11       |\\n| Effective duration | 137.54      | 33.81       | 171.35        |\\n| Average duration/s | 0.5609 | 0.5639      | 0.5615        |\\n| Number of calls | 14712        | 3597        | 18309         |\\n\\nWe have performed a statistical experiment of duration distribution information on the audio in the training set and the evaluation set respectively, as shown in Fig. 6. As can be seen from the figure, most of the macaque call segments last from 0.3s to 0.5s.\\n\\nFigure 6: The statistical duration distribution of calls. (a) For training set. (b) For evaluation set.\\n\\n4.2. Experimental setup\\n\\nBased on the basic training set (MacaqueT), we designed three variant training sets with different audio lengths: MacaqueT1, MacaqueT5 and MacaqueT10. Signals are resampled at 16 kHz. The corresponding sampling points are 5200 (0.32 s), 24,000 (1.5 s) and 48,000 (3 s). MacaqueT5 (or MacaqueT10) extends from MacaqueT1. The expansion strategy is to randomly select 5 (or 10) call segments from the same macaque corpus for splicing, and sample 24000 (or 48,000) audio points after splicing.\\n\\nThere are 3,597 macaque calls in the last 44 macaques corpus. We designed 5 testing sets with different audio lengths: MacaquesE1, MacaquesE2, MacaquesE3, MacaquesE4 and MacaquesE5. MacaquesE1 contains raw data of 44 macaques, including 3564 calls. MacaquesE2, MacaquesE3, MacaquesE4, MacaquesE5 splice each macaque corpus in the testing set according to the number of 2, 3, 4 and 5 segments. Based on the above five testing sets, we constructed 80,000 audio pairs for voiceprint verification test, of which the number of positive and negative pairs is 40,000.\\n\\nTo compare the effects of different training sets on verification performance, we conducted experiments on the three training sets and five testing sets. Due to the fixed length of model input, the model obtained by using MacaquesT1 training set can only be tested on MacaquesE1 testing set. The experimental results with our baseline model SincNet-ResNet are shown in Table 2. T1, T5, and T10 indicate the results achieved with respect to MacaquesT1, MacaquesT5 and MacaquesT10, respectively.\\n\\nWe can see that our baseline model yields the best performance under testing set MacaquesE3 and training set MacaquesP10. Therefore, the following experiment will be based on MacaqueT10 training set.\\n\\n4.3. Performance analysis and discussion\\n\\nComparison with state-of-the-art human voiceprint verification algorithms.\\n\\nVoiceprint verification algorithms have evolved from multi-classification under closed datasets to embedded feature representation. Based on the development of deep learning in the field of human voiceprint verification, we tried four state-of-the-art algorithms to test the application effect of the human voiceprint verification model on the macaque datasets, and compared them with our results. This comparative experiment is based on the MacaquesT10 training set and the MacaquesE3 testing set.\\n\\nThe CAP, LGE2E, and DeepSpeaker algorithms use MFCC features as input, and RawNet2 uses raw audio data as input. During training and testing, the duration of the call was compressed/expanded to 3 seconds. The results are shown in Table 3, from which we can determine that our method achieved the best performance and improved EER by 3.96% compared with suboptimal methods.\\n\\nTable 2: Results of a comparison of 3 training sets and 5 testing sets of our baseline method.\\n\\n| Testing set | T1/EER (%) | T5/EER (%) | T10/EER (%) |\\n|-------------|------------|------------|-------------|\\n| MacaquesE1  | 16.9       | 15.53      | 16.53       |\\n| MacaquesE2  | 8.93       | 9.93       |             |\\n| MacaquesE3  | 8.88       | 8.01       |             |\\n| MacaquesE4  | 11.69      | 8.78       |             |\\n| MacaquesE5  | 12.04      | 9.95       |             |\\n\\nTable 3: Result of a comparison results with the four state-of-the-art methods.\\n\\n| Training set | Testing set | Model   | EER(%) |\\n|--------------|-------------|---------|--------|\\n| MacaquesT10  | MacaquesE3  | RawNet2 | 10.33  |\\n| MacaquesT10  | MacaquesE3  | LGE2E   | 10.15  |\\n| MacaquesT10  | MacaquesE3  | DeepSpeaker | 13.21  |\\n| MacaquesT10  | MacaquesE3  | CAP     | 12.74  |\\n| MacaquesT10  | MacaquesE3  | our     | 6.19   |\\n\\n5. Conclusions\\n\\nIn this paper, we have verified the performance of different human voiceprint verification models on macaque verification. Experimental results verified the feasibility of macaque voiceprint verification, and we proposed the channel fusion mechanism (CFM) for the correlation of frame-level features, which effectively reduced the EER of macaque voiceprint verification. However, the natural environment is much more complicated. The noise under various natural conditions, including wind, rain, mixed sounds of animals, etc., will have a significant impact on the performance of the algorithm. How to make the algorithm better deal with the interference of the natural environment is a major direction that needs to be studied in future.\\n\\n6. Acknowledgements\\n\\nThis work was supported in part by the Important Science and Technology Project of Hainan Province under Grant ZDKJ2020010, and in part by Frontier Exploration Project Independently Deployed by Institute of Acoustics, Chinese Academy of Sciences under Grant QYTS202015 and Grant QYTS202115.\"}"}
