{"id": "yang24d_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"7. References\\n\\n[1] M. \u0141ajszczak, G. Cambara, Y. Li, F. Beyhan, A. van Korlaar, F. Yang, A. Joly, \u00c1. Mart\u00edn-Cortinas, A. Abbas, A. Michalski et al., \u201cBase tts: Lessons from building a billion-parameter text-to-speech model on 100k hours of data,\u201d arXiv preprint arXiv:2402.08093, 2024.\\n\\n[2] Z. Jiang, J. Liu, Y. Ren, J. He, Z. Ye, S. Ji, Q. Yang, C. Zhang, P. Wei, C. Wang et al., \u201cBoosting prompting mechanisms for zero-shot speech synthesis,\u201d in The Twelfth International Conference on Learning Representations, 2023.\\n\\n[3] M. Le, A. Vyas, B. Shi, B. Karrer, L. Sari, R. Moritz, M. Williamson, V. Manohar, Y. Adi, J. Mahadeokar et al., \u201cVoicebox: Text-guided multilingual universal speech generation at scale,\u201d Advances in neural information processing systems, vol. 36, 2024.\\n\\n[4] K. Shen, Z. Ju, X. Tan, Y. Liu, Y. Leng, L. He, T. Qin, S. Zhao, and J. Bian, \u201cNaturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers,\u201d arXiv preprint arXiv:2304.09116, 2023.\\n\\n[5] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech 2: Fast and high-quality end-to-end text to speech,\u201d arXiv preprint arXiv:2006.04558, 2020.\\n\\n[6] C. Wang, S. Chen, Y. Wu, Z. Zhang, L. Zhou, S. Liu, Z. Chen, Y. Liu, H. Wang, J. Li et al., \u201cNeural codec language models are zero-shot text to speech synthesizers,\u201d arXiv preprint arXiv:2301.02111, 2023.\\n\\n[7] Y. Wang, D. Stanton, Y. Zhang, R.-S. Ryan, E. Battenberg, J. Shor, Y. Xiao, Y. Jia, F. Ren, and R. A. Saurous, \u201cStyle tokens: Unsupervised style modeling, control and transfer in end-to-end speech synthesis,\u201d in International conference on machine learning. PMLR, 2018, pp. 5180\u20135189.\\n\\n[8] Y. Ren, M. Lei, Z. Huang, S. Zhang, Q. Chen, Z. Yan, and Z. Zhao, \u201cProsospeech: Enhancing prosody with quantized vector pre-training in text-to-speech,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7577\u20137581.\\n\\n[9] J. Za\u00efdi, H. Seut\u00e9, B. van Niekerk, and M.-A. Carbonneau, \u201cDaftexprt: Cross-speaker prosody transfer on any text for expressive speech synthesis,\u201d arXiv preprint arXiv:2108.02271, 2021.\\n\\n[10] J. Yamagishi, C. Veaux, and K. MacDonald, \u201cCSTR VCTK Corpus: English multi-speaker corpus for CSTR voice cloning toolkit (version 0.92),\u201d 2019.\\n\\n[11] H. Zen, V. Dang, R. Clark, Y. Zhang, R. J. Weiss, Y. Jia, Z. Chen, and Y. Wu, \u201cLibritts: A corpus derived from Librispeech for text-to-speech,\u201d arXiv preprint arXiv:1904.02882, 2019.\\n\\n[12] Y. Shi, H. Bu, X. Xu, S. Zhang, and M. Li, \u201cAishell-3: A multi-speaker mandarin tts corpus and the baselines,\u201d arXiv preprint arXiv:2010.11567, 2020.\\n\\n[13] T. Guo, C. Wen, D. Jiang, N. Luo, R. Zhang, S. Zhao, W. Li, C. Gong, W. Zou, K. Han et al., \u201cDidispeech: A large scale mandarin speech corpus,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 6968\u20136972.\\n\\n[14] K. Zhou, B. Sisman, R. Liu, and H. Li, \u201cSeen and unseen emotional style transfer for voice conversion with a new emotional speech dataset,\u201d in ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2021, pp. 920\u2013924.\\n\\n[15] A. Adigwe, N. Tits, K. E. Haddad, S. Ostadabbas, and T. Dutoit, \u201cThe emotional voices database: Towards controlling the emotion dimension in voice generation systems,\u201d arXiv preprint arXiv:1806.09514, 2018.\\n\\n[16] M. Kim, S. J. Cheon, B. J. Choi, J. J. Kim, and N. S. Kim, \u201cExpressive text-to-speech using style tag,\u201d arXiv preprint arXiv:2104.00436, 2021.\\n\\n[17] Z. Guo, Y. Leng, Y. Wu, S. Zhao, and X. Tan, \u201cPrompttts: Controllable text-to-speech with text descriptions,\u201d in ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023, pp. 1\u20135.\\n\\n[18] D. Yang, S. Liu, R. Huang, G. Lei, C. Weng, H. Meng, and D. Yu, \u201cInstructtts: Modelling expressive tts in discrete latent space with natural language style prompt,\u201d arXiv preprint arXiv:2301.13662, 2023.\\n\\n[19] S. Ji, J. Zuo, M. Fang, Z. Jiang, F. Chen, X. Duan, B. Huai, and Z. Zhao, \u201cTextrolspeech: A text style control speech corpus with codec language text-to-speech models,\u201d arXiv preprint arXiv:2308.14430, 2023.\\n\\n[20] G. Sun, Y. Zhang, R. J. Weiss, Y. Cao, H. Zen, A. Rosenberg, B. Ramabhadran, and Y. Wu, \u201cGenerating diverse and natural text-to-speech samples using a quantized fine-grained vae and autoregressive prosody prior,\u201d in ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6699\u20136703.\\n\\n[21] G. Sun, Y. Zhang, R. J. Weiss, Y. Cao, H. Zen, and Y. Wu, \u201cFully-hierarchical fine-grained prosody modeling for interpretable speech synthesis,\u201d in ICASSP 2020-2020 IEEE international conference on acoustics, speech and signal processing (ICASSP). IEEE, 2020, pp. 6264\u20136268.\\n\\n[22] R. Huang, Y. Ren, J. Liu, C. Cui, and Z. Zhao, \u201cGenerspeech: Towards style transfer for generalizable out-of-domain text-to-speech synthesis,\u201d arXiv preprint arXiv:2205.07211, 2022.\\n\\n[23] A. Radford, J. W. Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever, \u201cRobust speech recognition via large-scale weak supervision,\u201d in International Conference on Machine Learning. PMLR, 2023, pp. 28492\u201328518.\\n\\n[24] Y. Wu, X. Tan, B. Li, L. He, S. Zhao, R. Song, T. Qin, and T.-Y. Liu, \u201cAdaspeech 4: Adaptive text to speech in zero-shot scenarios,\u201d arXiv preprint arXiv:2204.00436, 2022.\\n\\n[25] D. Lee, Z. Tian, L. Xue, and N. L. Zhang, \u201cEnhancing content preservation in text style transfer using reverse attention and conditional layer normalization,\u201d arXiv preprint arXiv:2108.00449, 2021.\\n\\n[26] Y. Yi, L. He, S. Pan, X. Wang, and Y. Xiao, \u201cProsodyspeech: Towards advanced prosody model for neural text-to-speech,\u201d in ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022, pp. 7582\u20137586.\\n\\n[27] A. Gulati, J. Qin, C.-C. Chiu, N. Parmar, Y. Zhang, J. Yu, W. Han, S. Wang, Z. Zhang, Y. Wu et al., \u201cConformer: Convolution-augmented transformer for speech recognition,\u201d arXiv preprint arXiv:2005.08100, 2020.\\n\\n[28] B. D. T. C. Ltd, \u201caidatatang200zh, a free Chinese Mandarin speech corpus.\u201d\\n\\n[29] J. Kong, J. Kim, and J. Bae, \u201cHifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis,\u201d Advances in Neural Information Processing Systems, vol. 33, pp. 17022\u201317033, 2020.\\n\\n[30] X. Wang, S. Takaki, and J. Yamagishi, \u201cNeural source-filter waveform models for statistical parametric speech synthesis,\u201d IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 28, pp. 402\u2013415, 2019.\\n\\n[31] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y. Liu, \u201cFastspeech: Fast, robust and controllable text to speech,\u201d Advances in neural information processing systems, vol. 32, 2019.\\n\\n[32] S. Chen, C. Wang, Z. Chen, Y. Wu, S. Liu, Z. Chen, J. Li, N. Kanda, T. Yoshioka, X. Xiao et al., \u201cWavlm: Large-scale self-supervised pre-training for full stack speech processing,\u201d IEEE Journal of Selected Topics in Signal Processing, vol. 16, no. 6, pp. 1505\u20131518, 2022.\\n\\n[33] H. Bai, R. Zheng, J. Chen, M. Ma, X. Li, and L. Huang, \u201cA3t: Alignment-aware acoustic and text pretraining for speech synthesis and editing,\u201d in International Conference on Machine Learning. PMLR, 2022, pp. 1399\u20131411.\"}"}
{"id": "yang24d_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis\\n\\nQian Yang1,\u2217, Jialong Zuo1,\u2217, Zhe Su1,\u2217, Ziyue Jiang1, Mingze Li1, Zhou Zhao\u2020, Feiyang Chen2, Zhefeng Wang2, Baoxing Huai2\\n\\n1 Zhejiang University, 2 Huawei Cloud\\nqyang1021, jialongzuo, suzhe, ziyuejiang, limingze, zhaozhou @zju.edu.cn, chenfeiyang2, wangzhefeng, huaibaoxing@huawei.com\\n\\nAbstract\\nWe introduce an open source high-quality Mandarin TTS dataset MSceneSpeech (Multiple Scene Speech Dataset), which is intended to provide resources for expressive speech synthesis. MSceneSpeech comprises numerous audio recordings and texts performed and recorded according to daily life scenarios. Each scenario includes multiple speakers and a diverse range of prosodic styles, making it suitable for speech synthesis that entails multi-speaker style and prosody modeling. We have established a robust baseline, through the prompting mechanism, that can effectively synthesize speech characterized by both user-specific timbre and scene-specific prosody with arbitrary text input. The open source MSceneSpeech Dataset and audio samples of our baseline are available at https://speechai-demo.github.io/MSceneSpeech/.\\n\\nIndex Terms: Speech Dataset, Expressive Speech Synthesis, Prosody Transfer\\n\\n1. Introduction\\nSpeech synthesis is designed to empower machines with the capability to produce voices that are indistinguishable from those of humans, and it is an indispensable component of AGI (Artificial General Intelligence) human-machine interaction. In the current speech synthesis domain [1, 2, 3, 4, 5, 6], the generated speech is no longer satisfied with merely synthesizing low-noise, high-definition outputs, instead, there is a shift of focus towards generating speech that is more natural, rhythmic, and expressive.\\n\\nContemporary high-expressive speech synthesis [1, 2] implicitly learns prosodic modeling through large volumes of data, attempting to expand the prosodic distribution by extending the distribution of data collected in the wild. Nonetheless, the quality of in-the-wild data is uneven, with complex and varied distribution patterns that do not necessarily align with the prosodic distribution found in everyday conversation. Furthermore, the absence of prosodic labeling makes it challenging to exert control over the prosody.\\n\\nConsequently, we have recorded MSceneSpeech, a high-quality Mandarin TTS dataset meticulously organized by speaker identities and scene labels, expertly crafted to enhance the modeling and transformation of prosody. MSceneSpeech contains around 15 hours of high-quality audio recorded by professionals. The scripts for each scenario have been thoughtfully curated to align with the contextual backdrop, and the accompanying audio recordings are delivered with expressive nuance, transcending mere recitation. The dataset encompasses a diverse assortment of four scene categories: Chat, News, QA (Question and Answer), and Storytelling, with each category showcasing a range of distinct speakers. Leveraging the MSceneSpeech dataset for fine-tuning enables synthesis models to adeptly execute style-related tasks, such as multi-style speech synthesis, and cross-speaker style transfer.\\n\\nIn the realms of style transfer and multi-style speech synthesis, numerous exceptional studies have emerged, demonstrating the capability to generate audio of superior quality across an array of stylistic nuances [2, 3, 7, 8, 9]. However, when it comes to cross-speaker style transfer, there are scant examples of work that proficiently handles the dual challenges of style transfer and voice adaptation. Thus, we have also proposed a robust baseline that is capable of generating multiple styles while performing voice adaptation. Both style transfer and voice adaptation are based on arbitrary reference audio, making the process extremely convenient and unconstrained.\\n\\nWe utilize a timbre reference speaker module, supplemented by a prompt-based style transfer module, which explicitly extracts style information from the reference audio of the desired style and integrates it into the audio generation module. Our main contributions can be summarized as follows:\\n\u2022 We release MSceneSpeech dataset that comprises diverse real-life scenario recordings, addressing the limitation of existing datasets to exert control over the prosody.\\n\u2022 We present a robust baseline, with a prompt-based prosody module and a reference timbre module, able to perform voice adaptation with different prosodic patterns across various real-life scenes.\\n\\n2. Related work\\n2.1. Expressive Speech Synthesis Corpus\\nOver the past few years, many TTS datasets such as VCTK [10], LibriTTS [11], AISHELL 3 [12], and DiDiSpeech [13] have been released, making a significant contribution to speech synthesis tasks. The aforementioned datasets primarily consist of reading-style data, which, despite promoting the research of high-quality speech synthesis, exhibit limitations in terms of style coverage and prosody diversity. Therefore, some prosody-rich speech datasets are constructed for expressive speech tasks, covering a variety of speaking styles and domains. For emotion, ESD [14] and EmoV-DB [15] are designed for voice adaptation tasks and emotional TTS tasks. For style, ST TTS [16] presents a style-tagged TTS dataset utilizing a short phrase or word representing the style of an utterance. Several recent studies [17, 18, 19] are proposed to control speech style through natural language prompts which are manually annotated or conducted by automatic description creation pipeline based on speech attribute labeler and large language models. Unfortu-\"}"}
{"id": "yang24d_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"nately, these datasets are either commercially unavailable (not open source) or rely on style descriptions labeled based on natural language. In light of this, we release MSceneSpeech dataset, organized by speaker identities and scene labels, aiming to provide an open-source data resource encompassing diverse real-life scenario recordings.\\n\\n2.2. Style Transfer in Text-to-Speech\\nStyle transfer has been studied for decades in the TTS community, with the aim of transferring the style (e.g., prosody and emotion) from a reference utterance to the synthesized target speech. GST-Tacotron [7] introduces global style tokens to learn linguistic-agnostic prosodic features for various style control and transfer. Some works [20, 21] further study a way to include a hierarchical, fine-grained prosody representation. GenerSpeech [22] proposes a multi-level style adaptor to efficiently model a large range of style conditions. Daft-Exprt [9] and MegaTTS [2] introduce corresponding mechanisms to tackle the cross-speaker style transfer task. The former utilizes a gradient reversal layer to penalize the prosody encoder, while the latter adopts a latent code language model to fit the distribution of prosody. However, disentangled style control is rarely considered in existing methods, which means independently controlling timbre and other style attributes such as prosody. MSceneSpeech, on the other hand, presents a promising baseline model that transfers only the vocal style from a reference speaker prompt and generates prosodic audio samples consistent with the scene reference prompt.\\n\\n3. MSceneSpeech Dataset\\nIn this section, we present MSceneSpeech, a novel high-quality monolingual Mandarin speech dataset that encompasses a diverse range of daily scenarios. Containing rich prosody, it serves as an important resource for synthesizing expressive audio under various scenarios.\\n\\n3.1. Data recording and annotation\\nBefore recording the audio, we meticulously selected scenes and carefully chose texts that were highly pertinent to each respective scenario. These were then provided to professional recording artists who, based on the content of the texts and scenario, delivered the content through interpretative performances rather than mere recitation. This approach was taken to achieve a better prosodic effect. During the recording process, we required a clear distinction in the recording style for the same person across different scenarios, while the performance method for different individuals within the same style should be consistent.\\n\\nAfter the audio recording was completed, we conducted verification of the text annotations. Although the recording artists were instructed to pronounce according to the annotations during recording, there were still instances where the text was read incorrectly or additional paralinguistic elements were added. To address this, we initially utilized Whisper [23] to transcribe the recorded audio. Then, if there was a significant discrepancy between the transcribed text and the annotated text, we proceeded to manually compare and correct the text.\\n\\n3.2. Data Processing\\nIn our data processing workflow, we controlled the duration of audio clips to fall within the 5-10 seconds range, optimizing them for ease of use. Our criteria for splitting audio were initially based on the presence of traditional stopping indicators, including periods, question marks, and other punctuation marks that can be considered as a stop of a sentence. The audio was primarily segmented at periods, provided that the resulting clips fell within a duration of 5 to 10 seconds. In the absence of these stopping signs, commas and other punctuation were considered for splitting under the same time constraints. If neither punctuation was suitable, we opted to split at the nearest available punctuation.\\n\\nAfter splitting, due to potential inaccuracies in alignment, we implemented Automatic Speech Recognition (ASR) proofreading, removing any segments where text similarity fell below 80%. This filter led to the exclusion of approximately 5% of the audio files. We further made refinements manually, removing a small amount of sentences with alignment issues.\\n\\nAfter curating the dataset, we divide it into training and testing subsets. The testing subset is created by selecting one speaker and their corresponding recordings from each scene, while the remaining data are allocated to the training set.\\n\\n3.3. Dataset Statistics\\nWe gather statistics on the dataset and present them in Table 1. The dataset comprises 4 distinct scenes, and the aggregate duration of the audio clips amounts to a total of 14.7 hours, with a mean audio length of 9.1 seconds. We argue that prosodic features are predominantly found in variables such as duration and pitch, so a noticeable variation in these metrics across different scenes shows the rich prosody in our dataset. What's more, this variability introduces a challenge to model prosody and timbre separately.\\n\\nMoreover, we compare our dataset with other famous multi-speaker datasets [12, 13], focusing on the metrics of speaker mean, variance, and skew. Notably, our dataset exhibits greater pitch variance within the same speaker, indicating a richer diversity in prosody.\"}"}
{"id": "yang24d_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: The overall architecture of Our Baseline. Duration, pitch, and energy are extracted from the prompt (In training: unmasked part; In inference: reference speech). It serves as conditions for their respective predictors. And losses are calculated only on the masked part.\\n\\n4. Proposed baseline\\n\\n4.1. Model overview\\n\\nIn this section, we introduce our proposed baseline. The overall architecture of our baseline is illustrated in Fig. 2. The Linguistic Encoder and Style Adaptive Encoder are built upon FastSpeech2 [5] and are responsible for converting raw text into a sequence of phoneme-based linguistic features. Our Timbre Encoder is built upon Adaspeech 4 [24], with several modifications for better adaptive voices. Specifically, a set of base vectors, which are randomly initialized and referred to as Speaker Basis Vectors, store timbre-relevant variables. We perform cross-attention between the Speaker Basis Vectors and the transformer encoder outputs of the SPK-Mel to obtain the final speaker representations. A detailed description of our Prosody-Related Module will be provided in the subsequent section.\\n\\n4.2. Prosody-Related Modules\\n\\nWe argue that prosody mostly exists in duration, pitch, and energy. It is a highly variant attribute with both local and long-term dependencies, changing rapidly over time and only has a weak correlation to text [2]. This observation inspires us to condition prosody with a prompting mechanism. This approach offers a stronger method for conditioning prosody compared to previous techniques that rely on implicitly extracting weaker prosodic features or employing Conditional LayerNorm [25].\\n\\nIn practice, to realize the prompt mechanism, we utilize Masked Prosody Prediction (MPP), which involves masking a portion of the ground-truth duration, pitch, energy and predicting the masked values conditioning on the unmasked part. Thus, the predictors are trained to emulate the prosody presented in the reference prompt, mimicking its duration, pitch, and energy patterns. To address the issue of entangled timbre and prosody, we explicitly model prosody through its components: duration, pitch, and energy. This approach contrasts with previous work [8, 22, 26] that relied on implicit modeling of prosody.\\n\\nWhen choosing between different predictor architectures, we find the original predictor architecture in FastSpeech 2 [5] performs poorly in prosody transfer, mainly because it relies solely on convolutions, providing it with a limited receptive field. To address this limitation, we employ conformer layers [27] for its ability to capture global info. Such a feature has shown to be particularly effective in the process of prosody transfer in our ablation study.\\n\\n4.3. Training and inference procedures\\n\\nIn this part, we outline our training and inference procedures, which primarily differ in the setting of prosody-related modules. The speaker representations are extracted by timbre encoder, and mel-spectrograms are generated by the diffusion decoder conditioned on both the speaker and prosody representations. During training, we mask 60% of the mel-level duration, pitch, and energy ground-truths extracted from original waveform, calculating the loss for these masked portions. During inference, we concatenate the prompted duration, pitch, and energy with zeros, mirroring the masked portions during training.\\n\\nDuring the supervised fine-tuning stage in MSceneSpeech, we fix our Linguistic Encoder and Style Adaptive Encoder to preserve linguistic information learned from the pre-train dataset. Though we finetune other module parameters to learn rich prosody, it is crucial that the steps are controlled in a limited range so as not to break the generalization and adaptation ability trained from the pre-train dataset.\\n\\n5. Experiment\\n\\n5.1. Experimental setup\\n\\nWe pre-train our baseline on a mixed public dataset that includes both Chinese and English speech, consisting the training set of aidatatang 200zh [28], AISHELL-3 [12], DiDiSpeech [13], and VCTK [10] datasets. This aggregated dataset comprises approximately 400 hours of speech from around 2,000 speakers. After that, we perform supervised fine-tuning on our 15-hour MSceneSpeech dataset and a few hours of internal speech corpus consisting of specific speakers. For both pre-train and fine-tuning datasets, we convert text sequences to phoneme sequences using our internal grapheme-to-phoneme tool. All audio files are first resampled to 16 kHz. Subsequently, we extract mel-spectrograms using an FFT size of 1024, a hop size of 256, and a window size of 1024. The mel-spectrograms generated by our model are then converted into audio samples using HiFi-GAN [29], complemented by an additional pre-trained NSF module depicted in [30], combining pitch information during waveform generation.\\n\\nFor the effective evaluation of both subjective and objective experiments, we conducted separate experiments for voice adaptation and prosody transfer. The experiments of the voice adaptation are presented in Section 5.4, while the experiments of prosody transfer can be found in Section 5.5.\\n\\nIn our experiments, masking 60% of the mel-level duration, pitch, and energy ground-truth yielded the best results. The code and models are available on GitHub.\"}"}
{"id": "yang24d_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2:\\nSubjective evaluation on speech quality (MOS-Q) and the objective (ASV-Score) and subjective (MOS-S) evaluation on speaker consistency.\\n\\n| Method          | MOS-Q (\u2191)         | MOS-S (\u2191)         | ASV-Score (\u2191) |\\n|-----------------|-------------------|-------------------|---------------|\\n| Adaspeech 4     | 3.88 \u00b1 0.04       | 3.85 \u00b1 0.04       | 0.881         |\\n| A3T             | 3.86 \u00b1 0.05       | 3.76 \u00b1 0.04       | 0.866         |\\n| Our Baseline    | 3.91 \u00b1 0.04       | 4.03 \u00b1 0.05       | 0.884         |\\n\\nOur experiments focused on the results in Mandarin, however, since our model was also pre-trained on English datasets, we have included the model\u2019s English synthesis capabilities on our demonstration page\u2021. Additionally, to test zero-shot ability of our baseline, we perform zero-shot style transfer to ESD [14] datasets also in our demo page.\\n\\n5.2. Model Configuration\\nThe Linguistic Encoder and Style-Adaptive Encoder are composed of multiple Feed-forward Transformer blocks [31], with positional encoding. The dimension of the speaker embedding is set to 256. The dimension and size of the Speaker Basis Vectors are configured to 128 and 2000, respectively. The duration, pitch, and energy predictors all use the same Conformer architecture but differ in layer numbers. In the Diffusion Decoder, we stack 20 layers of convolution with kernel size 3 and dilation factor 1 at each layer.\\n\\n5.3. Evaluation Methods\\nFor subjective evaluations, we measure three types of Mean Opinion Score (MOS): MOS-Q to assess audio quality, MOS-S for speaker consistency, and MOS-P for prosody consistency, conducted using Amazon Mechanical Turk. For objective evaluation of speaker similarity, we calculate the similarity score using the pre-trained Speaker Verification model WavLM-large [32] and call it ASV-Score. To evaluate prosody similarity, we adopt the methodology in FastSpeech2 [5]. This involves 1) Extracting pitch and energy metrics from generated and reference speech; 2) Calculating statistical descriptors like mean, standard variation, skewness, and kurtosis for each metric; 3) Getting the differences between generated and reference speech across the dataset, then averaging them.\\n\\n5.4. Performance on speaker consistency\\nVoice adaptation is not the innovative aspect of our baseline, therefore, in the experiments, we only focus on comparing our timbre module with our backbone models, including A3T [33] and Adaspeech 4 [24].\\n\\nIn the experiments of our baseline, we set the prosody reference to be identical to the speaker reference, focusing solely on the results related to voice adaptation. All systems were trained using the same dataset configurations and parameter settings as described in section 5.1.\\n\\nAs shown in Table 2, Our baseline showcases its capability in voice adaptation is on par with existing expert voice adaptation models, while our baseline remains its ability in style transfer.\\n\\n\u2021https://speechai-demo.github.io/MSceneSpeech/\\n\\nTable 3:\\nSubjective evaluation on speech quality (MOS-Q), prosody consistency (MOS-P) and speaker consistency (MOS-S).\\n\\n| Method          | MOS-Q | MOS-P | MOS-S |\\n|-----------------|-------|-------|-------|\\n| Our Baseline    | 0.00  | 0.00  | 0.00  |\\n| CNN-Predictor   | -0.78 | -0.57 | -0.15 |\\n| Atten-Predictor | -1.23 | -0.47 | -0.45 |\\n| Parallel-SPK    | -1.10 | -0.56 | -0.54 |\\n| Addall-SPK      | -0.26 | -0.58 | +0.01 |\\n| NS2-Prompting   | +0.01 | -0.06 | -0.37 |\\n\\nTable 4:\\nObjective evaluation on speaker consistency (ASV-Score) and prosody consistency on pitch difference.\\n\\n| Method          | ASV-Score | Pitch Mean | Pitch Std | Pitch Skew | Pitch Kurt |\\n|-----------------|-----------|-----------|-----------|-----------|-----------|\\n| Our Baseline    | 0.833     | 11.48     | 7.31      | 0.53      | 3.64      |\\n| CNN-Predictor   | 0.831     | 22.80     | 27.23     | 0.73      | 5.10      |\\n| Atten-Predictor | 0.739     | 66.50     | 21.52     | 2.23      | 5.58      |\\n| Parallel-SPK    | 0.774     | 27.55     | 16.85     | 0.88      | 5.10      |\\n| Addall-SPK      | 0.845     | 11.55     | 9.18      | 0.68      | 4.98      |\\n| NS2-Prompting   | 0.756     | 11.50     | 4.81      | 0.82      | 5.67      |\\n\\n5.5. Ablation Study and Prosody Transfer Analysis\\nWe further conduct ablation studies to verify the effectiveness of each carefully designed module for prosody transfer. We focus on three key design choices: 1) Predictor architecture\u2014either original CNN-based (CNN-Predictor) or CNN with single attention (Atten-Predictor); 2) Speaker representation\u2014either no shuffling speaker approach (Parallel-SPK) or incorporating speaker embedding throughout all parts of the model (Addall-SPK); 3) Prosody modeling\u2014using an implicit prompting mechanism as per NaturalSpeech2 [4] (NS2-Prompting).\\n\\nWe perform the subjective evaluation of speech quality, prosody consistency, and speaker consistency on Table 3 and objective evaluation of prosody consistency. We put energy evaluation on prosody consistency in the demo page and only show pitch on Table 4. Key findings in subjective and objective metrics include: 1) CNN-Predictor shows poor prosody transfer with a slight drop in speaker consistency, and Atten-Predictor struggles to perform in both timbre and prosody transfer; 2) Parallel-SPK underperforms due to train-test mismatch, and Addall-SPK boosts speaker consistency but affects prosody diversity negatively; 3) NS2-Prompting entangles timbre and prosody, harming speaker consistency.\\n\\n6. Conclusion\\nIn this paper, we introduce MSceneSpeech, an open source high-quality Mandarin expressive TTS dataset, featuring a diverse range of real-world scenarios, and aims to advance the synthesis of speech with richer prosody and to facilitate style transfer and synthesis within specific everyday scenarios. Furthermore, we introduce a strong baseline, capable of synthesizing audio with the corresponding timbre and prosody based on any given timbre reference audio and prosody reference audio. Experimental results show our baseline\u2019s superior performance in both speaker voice adaptability and multiple prosody transferability.\"}"}
