{"id": "liu22t_interspeech", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"8. References\\n\\n[1] T. J. Park, N. Kanda, D. Dimitriadis, K. J. Han, S. Watanabe, and S. Narayanan, \u201cA review of speaker diarization: Recent advances with deep learning,\u201d Computer Speech and Language, vol. 72, p. 101317, 2022.\\n\\n[2] H. Aronowitz, W. Zhu, M. Suzuki, G. Kurata, and R. Hoory, \u201cNew advances in speaker diarization,\u201d in International Speech Communication Association (Interspeech), 2020.\\n\\n[3] T. Baltru\u0161aitis, C. Ahuja, and L.-P. Morency, \u201cMultimodal machine learning: A survey and taxonomy,\u201d IEEE transactions on pattern analysis and machine intelligence, vol. 41, no. 2, pp. 423\u2013443, 2018.\\n\\n[4] J. S. Chung, A. Nagrani, and A. Zisserman, \u201cVoxceleb2: Deep speaker recognition,\u201d in International Speech Communication Association (Interspeech), 2018.\\n\\n[5] R. Gao and K. Grauman, \u201cVisualvoice: Audio-visual speech separation with cross-modal consistency,\u201d in 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2021, pp. 15490\u201315500.\\n\\n[6] W. Kang, B. C. Roy, and W. Chow, \u201cMultimodal speaker diarization of real-world meetings using d-vectors with spatial features,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 6509\u20136513.\\n\\n[7] I. McCowan, J. Carletta, W. Kraaij, S. Ashby, S. Bourban, M. Flynn, M. Guillemot, T. Hain, J. Kadlec, V. Karaiskos et al., \u201cThe ami meeting corpus,\u201d in Proceedings of the 5th international conference on methods and techniques in behavioral research, vol. 88. Citeseer, 2005, p. 100.\\n\\n[8] Y. Ding, Y. Xu, S.-X. Zhang, Y. Cong, and L. Wang, \u201cSelf-supervised learning for audio-visual speaker diarization,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 4367\u20134371.\\n\\n[9] J. Roth, S. Chaudhuri, O. Klejch, R. Marvin, A. Gallagher, L. Kaver, S. Ramaswamy, A. Stopczynski, C. Schmid, Z. Xi et al., \u201cAva active speaker: An audio-visual dataset for active speaker detection,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp. 4492\u20134496.\\n\\n[10] J. S. Chung, J. Huh, A. Nagrani, T. Afouras, and A. Zisserman, \u201cSpot the conversation: speaker diarisation in the wild,\u201d in International Speech Communication Association (Interspeech), 2020.\\n\\n[11] J. S. Chung and A. Zisserman, \u201cLip reading in profile,\u201d 2017.\\n\\n[12] E. Zhongcong Xu, Z. Song, C. Feng, M. Ye, and M. Z. Shou, \u201cAva-avd: Audio-visual speaker diarization in the wild,\u201d arXiv e-prints, pp. arXiv\u20132111, 2021.\\n\\n[13] R. Ahmad, S. Zubair, H. Alquhayz, and A. Ditta, \u201cMultimodal speaker diarization using a pre-trained audio-visual synchronization model,\u201d Sensors, vol. 19, no. 23, p. 5163, 2019.\\n\\n[14] R. Ahmad, S. Zubair, and H. Alquhayz, \u201cSpeech enhancement for multimodal speaker diarization system,\u201d IEEE Access, vol. 8, pp. 126671\u2013126680, 2020.\\n\\n[15] R. Tao, Z. Pan, R. K. Das, X. Qian, M. Z. Shou, and H. Li, \u201cIs someone speaking? exploring long-term temporal features for audio-visual active speaker detection,\u201d in Proceedings of the 29th ACM International Conference on Multimedia, 2021, pp. 3927\u20133935.\\n\\n[16] B. Kulis et al., \u201cMetric learning: A survey,\u201d Foundations and Trends\u00ae in Machine Learning, vol. 5, no. 4, pp. 287\u2013364, 2013.\\n\\n[17] W. Ge, \u201cDeep metric learning with hierarchical triplet loss,\u201d in Proceedings of the European Conference on Computer Vision (ECCV), 2018, pp. 269\u2013285.\\n\\n[18] B. Castellano, \u201cPysceneDetect: Video scene cut detection and analysis tool.\u201d [Online]. Available: https://github.com/Breakthrough/PySceneDetect\\n\\n[19] A. Dutta and A. Zisserman, \u201cThe VIA annotation software for images, audio and video,\u201d in Proceedings of the 27th ACM International Conference on Multimedia, ser. MM \u201919. New York, NY, USA: ACM, 2019. [Online]. Available: https://doi.org/10.1145/3343031.3350535\\n\\n[20] S. Zhang, X. Zhu, Z. Lei, H. Shi, X. Wang, and S. Z. Li, \u201cS3fd: Single shot scale-invariant face detector,\u201d in Proceedings of the IEEE international conference on computer vision, 2017, pp. 192\u2013201.\\n\\n[21] N. Ryant, K. Church, C. Cieri, A. Cristia, J. Du, S. Ganapathy, and M. Liberman, \u201cThe Second DIHARD Diarization Challenge: Dataset, Task, and Baselines,\u201d in International Speech Communication Association (Interspeech), 2019, pp. 978\u2013982.\\n\\n[22] H. Bredin, R. Yin, J. M. Coria, G. Gelly, P. Korshunov, M. Lavechin, D. Fustes, H. Titeux, W. Bouaziz, and M.-P. Gill, \u201cpyannote.audio: neural building blocks for speaker diarization,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, May 2020.\\n\\n[23] M. Ravanelli and Y. Bengio, \u201cSpeaker recognition from raw waveform with sincnet,\u201d in 2018 IEEE Spoken Language Technology Workshop (SLT). IEEE, 2018, pp. 1021\u20131028.\\n\\n[24] N. Dawalatabad, M. Ravanelli, F. Grondin, J. Thienpondt, B. Desplanques, and H. Na, \u201cEcapa-tdnn embeddings for speaker diarization,\u201d 2021, arXiv:2104.01466.\\n\\n[25] J. Deng, J. Guo, N. Xue, and S. Zafeiriou, \u201cArcface: Additive angular margin loss for deep face recognition,\u201d in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019, pp. 4690\u20134699.\\n\\n[26] C. Lea, R. Vidal, A. Reiter, and G. D. Hager, \u201cTemporal convolutional networks: A unified approach to action segmentation,\u201d in European Conference on Computer Vision. Springer, 2016, pp. 47\u201354.\\n\\n[27] J. Hu, L. Shen, and G. Sun, \u201cSqueeze-and-excitation networks,\u201d in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 7132\u20137141.\\n\\n[28] D. Snyder, G. Chen, and D. Povey, \u201cMUSAN: A Music, Speech, and Noise Corpus,\u201d 2015, arXiv:1510.08484v1.\\n\\n[29] T. Ko, V. Peddinti, D. Povey, M. L. Seltzer, and S. Khudanpur, \u201cA study on data augmentation of reverberant speech for robust speech recognition,\u201d in IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017, pp. 5220\u20135224.\\n\\n[30] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in Int. Conf. Learn. Represent., 2014, pp. 1\u201315.\"}"}
{"id": "liu22t_interspeech", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MSDWILD: MULTI-MODAL SPEAKER DIARIZATION DATASET IN THE WILD\\n\u2020\\n\\nTao Liu1, \u2020Shuai Fan2, Xu Xiang2, Hongbo Song2, Shaoxiong Lin1, Jiaqi Sun1, Tianyuan Han1, Siyuan Chen1, Binwei Yao1, Sen Liu1, Yifei Wu1, \u2021Yanmin Qian1, \u2021Kai Yu1\\n\\n1 MoE Key Lab of Artificial Intelligence, AI Institute, X-LANCE Lab, Shanghai Jiao Tong University\\n2 AISpeech Ltd, Suzhou China\\n\\n{liutaw, Johnson-Lin, jotaro, hty19980927, chensiyuan925, yaobinwei, sen.liu, yifei.wu, yanminqian, kai.yu}@sjtu.edu.cn, {shuai.fan, xu.xiang, hongbo.song}@aispeech.com\\n\\nAbstract\\nSpeaker diarization in real-world acoustic environments is a challenging task of increasing interest from both academia and industry. Although it has been widely accepted that incorporating visual information benefits audio processing tasks such as speech recognition, there is currently no fully released dataset that can be used for benchmarking multi-modal speaker diarization performance in real-world environments. In this paper, we release MSDWild, a benchmark dataset for multi-modal speaker diarization in the wild. The dataset is collected from public videos, covering rich real-world scenarios and languages. All video clips are naturally shot videos without over-editing such as lens switching. Audio and video are both released. In particular, MSDWild has a large portion of the naturally overlapped speech, forming an excellent testbed for cocktail-party problem research. Furthermore, we also conduct baseline experiments on the dataset using audio-only, visual-only, and audio-visual speaker diarization.\\n\\nIndex Terms: speaker diarization, multi-modality, audio-visual\\n\\n1. Introduction\\nSpeaker diarization [1, 2] identifies the talkers and their talking duration, solving the problem of 'who spoke when.' Inspired by multi-modal complementarity [3] and its success in several audio-visual tasks [4, 5], multi-modal speaker diarization [6, 7, 8] takes advantage of both audio and visual modalities, exploring multi-modal fusion and further improving the performance.\\n\\nHowever, existing multi-modal speaker diarization datasets [7, 9, 10] are constrained in meetings, TV programs, or movie sources, which are recorded cooperatively and contain a mismatch to the in-the-wild scenario. In the scenario, people are talking spontaneously with frequent switching and sudden interrupting, leading to a naturally overlapped speech during the talking. Besides, distances from the talking face to the camera vary, resulting in various face resolutions and recording distances. In addition, not all faces are front, and plenty of them are side faces. Lastly, natural interference also exists, including various noises and room reverberations.\\n\\n\u2020These authors contributed equally to this work.\\n\u2021Yanmin Qian and Kai Yu are the corresponding authors.\\n\u2217The dataset is available at https://x-lance.github.io/MSDWILD.\\n\\nAMI AVA AVA-AVD MSDWild (Ours)\\n\\nFigure 1: Several examples from AMI [7], AVA-AVD [12], and our MSDWild methods. Compared with existing datasets from constrained sources (e.g., AMI is from meetings, AVA-AVD is from movies), our dataset contains daily casual conversation in the wild, with at least two speakers talking in turn. The bottom of our examples is the speaker diarization timeline; different colors represent different speakers.\\n\\nIn this paper, our objective is to collect a multi-modal speaker diarization dataset in those in-the-wild settings. To achieve this, we build a collecting pipeline. Compared with collecting pipeline [10], we have two major differences. First, we use the keyword 'VLog' to gather informal talks rather than 'panel debate' or 'discussion,' which are often used in conjunction with formal talks. Second, to filter out videos with over two speakers talking in turn, we do it manually without using any pre-trained audio-visual algorithms (e.g., SyncNet [11]). We think those algorithms may lead to dataset bias. Our dataset examples, compared with AMI [7] and AVA-AVD [12], are shown in Figure 1.\\n\\nActive speaker detection, which judges whether the audio matches the face, is a significant sub-task for audio-visual speaker diarization methods [13, 14, 10, 8, 12]. Most of those methods train on a separate audio-visual dataset (e.g., VoxCeleb2 [4]), not directly on the audio-visual speaker diarization dataset, which leads to a mismatch between the sub-task and the final inference performance. Our dataset also provides videos with cropped faces, labeled with speaking or not speaking, for training audio-visual sub-task.\"}"}
{"id": "liu22t_interspeech", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1: Comparison with existing audio-visual speaker diarization datasets. overlapped: The overlapped speech rate per video.\\n#speakers: The min/average/max speaker number per video.\\n#SC: The average speaker changes times per minute.\\n#noise: Whether the videos contain the noise.\\n#continuous: Whether video scenes are continuous without fast camera switching.\\n\\n| Dataset         | Source      | #videos | Duration | Speech% | Overlapped | #speakers | #SC  | Noise | Continuous |\\n|-----------------|-------------|---------|----------|---------|------------|-----------|------|-------|------------|\\n| AMI [10]        | Meeting     | 170     | 100h     | 80.91%  | 13.57%     | 3 / 4 / 5 | 7.8  |       |            |\\n| VoxConverse [10]| TV show     | 448     | 63h50m   | 90.7%   | 3.6%       | 1 / 5.6 / 21 | 3.28 |       |            |\\n| AVA-AVD [12]    | Movie       | 351     | 29h15m   | 45.95%  | 4.4%       | 2 / 7.7 / 24 | 9.6  |       |            |\\n| MSDWild (Ours)  | Daily Conversation | 3143  | 80h3m    | 91.29%  | 14.01%     | 2 / 2.7 / 10 | 11.8 |       |            |\\n\\nOur contributions are three-fold. First, we release a multi-modal speaker diarization dataset in the wild, MSDWild. This dataset contains over 3000 video clips with 80 hours. Besides, these datasets can be used for training and testing simultaneously. Second, extensive analyses are conducted, including detailed dataset metrics and comparisons with existing datasets. Third, we also conduct several audio-only, visual-only, and audio-visual baseline methods on our dataset. For audio-visual methods, we also investigate fusion strategies. We hope that MSDWild can provide a real in-the-wild testbed for the speaker diarization community. Besides, MSDWild is also suitable for exploring the ability of multi-modal audio-visual fusion for speaker diarization, better solving the problem of 'who spoke when.'\"}"}
{"id": "liu22t_interspeech", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Comparison with VoxCeleb2 [4], a celebrity interview dataset, talking face in our dataset has more casual head gestures and has an extra not-speaking status. Those talking faces are used to train an active speaker detection task, a sub-task for multi-modal speaker diarization.\\n\\n4. Dataset description\\n\\nOur dataset, MSDWild, contains 3143 video clips with 84 labeled hours. Daily casual conversation occupies the majority of our dataset. Compared with the formal conversation: news report or debate, the daily casual conversation has three features: frequently talking in turn, various head gestures, and various background noises or room reverberations.\\n\\nIn multi-modal speaker diarization, we find that the algorithm performances are extremely affected by the speaker number. So we separate videos with speaker numbers over than four to form a many-talker set. The rest, with speaker numbers from two to four, forms a few-talker set. Furthermore, we randomly divide the few-talker set into two sets: a training set and a testing set. Due to its limited size, the many-talker set is only used for testing. Therefore, our dataset forms three sets: few-talker training set, few-talker testing set, and many-talker testing set.\\n\\nThe number of video clips in those three sets is 2476, 490, and 177. The corresponding labeled duration is 69.09, 10.58, and 4.51 hours. However, the many-talker set's average utterance number and the overlapped speech ratio are significantly greater than the few-talker set. The many-talker set has much more speech alternations and overlapped speeches. The detailed dataset metrics are listed in Table 2.\\n\\n5. Experiments\\n\\n5.1. Evaluation metrics\\n\\nWe report two metrics: DER (Diarization Error Rate) and JER (Jaccard Error Rate). DER is the summary of missed speech (MS) time, false alarm (FA) time, and speaker error (SE) time to the reference time. We also calculate errors in overlapped speech part (OVL). JER, initially proposed by DIHARD [21], is the average of each speaker's MS and FA rate. Compared with DER, JER is more strict when some speakers dominate the conversation. A 0.25-second forgiving collar is used for all metrics, and overlapped speeches are taken into account.\\n\\n5.2. Audio-only method\\n\\nWe run Pyannote [22], a publicly available open-source toolkit for speaker diarization, on our dataset. Pyannote uses a standard audio-only pipeline that includes an SincNet-based [23] model for voice activity detection, ECAPA-TDNN [24] for embedding extraction, and agglomerative clustering for speaker clustering.\\n\\n5.3. Visual-based pipeline\\n\\nVisual-only and audio-visual share the same pipeline [13, 14, 8]: face detection, face tracking, and active speaker detection. We adopt S3FD [20] as the face detection method. In the face tracking stage, we use IoU tracking for faces in adjacent video frames and use Arcface [25] to cluster different tracks when they belong to the same speaker. Because those videos exist audio not belonging to the talking faces, we need active speaker detection methods to capture the correspondence between audio and lip motion. An example is shown in Figure 3. We investigate three methods: visual-only, two-stream audio-visual, and fused audio-visual. Before illustrating those methods in detail, we first describe the experiment setup.\\n\\n5.3.1. Experiment setup\\n\\nVisual representation. The inputs of our visual encoder are videos with cropped faces. First, we transform the RGB image to gray-scale in order to save the computation, and the frame rate of the video is converted to 25 Hz. So the input formula is \\\\( (T_v, W_v, H_v) \\\\), representing video frame time, width, and height. Both \\\\( W_v \\\\) and \\\\( H_v \\\\) use 112 here. Followed by [15], we also use 3D Conv and ResNet-18 to encode each frame into a 512-dimensional embedding. Finally, TCN [26] is followed to capture the inter-frame relation.\\n\\nAudio representation. The sample rate of our audios is 16kHz. We first convert audios into MFCC (Mel Frequency Cepstral Coefficient) features, with the window length: 25 ms, the window step: 10 ms, and the number of cepstrum: 13. Then SENet [27] is followed to encode audio features.\\n\\nData augmentation. The visual data augmentation uses random horizontal flip, random cropping, random rotation, and random sampling low resolution: 32 \u00d7 32, 64 \u00d7 64, or 96 \u00d7 96.\\n\\n1https://github.com/pyannote/pyannote-audio\"}"}
{"id": "liu22t_interspeech", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3: Audio-visual Speaker Diarization Pipeline. Best view in color. First, the face detection and face clustering stage are to identify the speaker, solving the problem 'who'. Then, audio-visual active speaker detection is to figure out the speaking period of each talking face, solving the problem 'when'. Waveform in green color represents the speaking period of ID 0 in this example.\\n\\nThe audio data uses MUSAN [28] and room reverberation [29] for data augmentation.\\n\\n5.3.2. Implementation of visual-based pipeline\\n\\nVisual-only method. We solely employ visual representation to train a binary classification network for each video frame in the visual-only method: zero for labeling not-speaking and one for labeling speaking. The binary cross-entropy loss is used to optimize this network.\\n\\nTwo-stream audio-visual method. Two-stream [11, 8] uses a parallel feature extraction network for individual modalities. Two-stream processes segment-level input and outputs an embedding for the whole segment. Longer input duration will lead to a coarse boundary, and shorter input causes lower accuracy. So there exists a trade-off in choosing the segment duration. The segment duration, used in our experiment, is 0.4 and 1 second (for $T_a$ and $T_v$). Contrastive loss is utilized to optimize this network during training, and L2 distance is employed as the similarity metric.\\n\\nFused audio-visual method. The fused architecture [15] implicitly models the relation between multi-modalities by training a weighted network. We use concatenation to fuse multi-modalities here. The input length of the fused model differs from the two-stream model in that it is trained and assessed at the frame level. The fused model classifies the talking state for each video frame, but the segment-level model only produces one outcome. As a result, the fused technique has a better time resolution and can better model inter-frame relations. Same as the visual-only method, the cross-entropy loss is also utilized here to optimize this network. During inference, video frames with a probability over 0.5 are regarded as speaking while others are not speaking.\\n\\nTable 3: The baseline results of audio-only, visual-only, and audio-visual methods on the few-talker val set.\\n\\n| Method                  | MS FA SE OVL DER JER |\\n|-------------------------|----------------------|\\n| Audio-only [22]         | 5.5 3.16 13.3 5.05 21.96 61.0 |\\n| Visual-only             | 9.13 12.47 1.71 3.12 23.32 41.22 |\\n| Audio-visual(Two-stream) [11, 8] | 8.49 14.3 1.8 3.66 24.59 44.71 |\\n| Audio-visual(Fused) [15] | 7.27 4.0 0.93 3.26 12.2 35.01 |\\n\\nTable 4: The baseline results of audio-only, visual-only, and audio-visual methods on the many-talker val set.\\n\\n| Method                  | MS FA SE OVL DER JER |\\n|-------------------------|----------------------|\\n| Audio-only [22]         | 12.64 5.54 24.97 12.34 43.15 84.28 |\\n| Visual-only             | 11.72 34.54 7.45 8.09 53.71 62.71 |\\n| Audio-visual(Two-stream) [11, 8] | 14.6 31.08 6.91 7.9 52.6 63.7 |\\n| Audio-visual(Fused) [15] | 14.2 7.45 4.2 6.59 25.86 54.79 |\\n\\n5.4. Result and analysis\\n\\nTable 3 and Table 4 show the final result. First, the overall DER and JER show that our dataset is challenging, especially in the many-talker condition. Second, the two-stream audio-visual methods perform even worse compared with the audio-only method. The fused audio-visual method improves 9.76% and 17.29% absolutely in the few-talker and many-talker val set, respectively. Besides, audio-visual methods can improve speaker diarization performance in speaker error (SE) rate and error rate in overlapped speech part (OVL), especially in many-talker condition. However, they perform poorly in missed speech (MS) and false alarm (FA). Those show potential in audio-visual methods, and more efficient methods remain to be explored.\\n\\nThird, compared with audio-only methods, methods with visual modality, including visual-only and audio-visual ones, improve greatly in JER, which shows that visual modality can better solve the speaker diarization in the wild. This is because there is a high variance in the speaker time in the wild, and the JER metric can capture the variance.\\n\\n6. Conclusion\\n\\nIn this paper, we propose MSDWild: a novel multi-modal speaker diarization dataset in the wild. The dataset contains spontaneously daily conversations on 'unconstrained' conditions. We also test several baseline methods for speaker diarization. However, there exist multi-modal methods to be explored, and experiments reveal that there is still potential for improvement, particularly in the multi-talker situation.\\n\\n7. ACKNOWLEDGEMENTS\\n\\nThis work was supported by State Key Laboratory of Media Convergence Production Technology and Systems Project (No. SKLMCPTS2020003), Shanghai Municipal Science and Technology Major Project (2021SHZDZX0102), National Natural Science Foundation of China (Grant No. 92048205), and Suzhou Science and Technology Planning Project (No. ZXT2020003).\"}"}
